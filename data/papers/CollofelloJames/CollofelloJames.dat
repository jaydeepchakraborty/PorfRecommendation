The Journal of Systems and Software 82 (2009) 1568–1577

Contents lists available at ScienceDirect

The Journal of Systems and Software
journal homepage: www.elsevier.com/locate/jss

Understanding the effects of requirements volatility in software engineering by
using analytical modeling and software process simulation
Susan Ferreira a,*, James Collofello b, Dan Shunk c, Gerald Mackulak c
a

Industrial and Manufacturing Systems Engineering Department, The University of Texas at Arlington, Arlington, TX 76019, USA
Computer Science and Engineering Department, Arizona State University, Tempe, AZ 76019, USA
c
Industrial Engineering Department, Arizona State University, Tempe, AZ 76019, USA
b

a r t i c l e

i n f o

Article history:
Available online 19 March 2009
Keywords:
Requirements volatility
Software process modeling
Requirements engineering risk

a b s t r a c t
This paper introduces an executable system dynamics simulation model developed to help project
managers comprehend the complex impacts related to requirements volatility on a software development project. The simulator extends previous research and adds research results from an empirical survey, including over 50 new parameters derived from the associated survey data, to a base model. The
paper discusses detailed results from two cases that show signiﬁcant cost, schedule, and quality impacts
as a result of requirements volatility. The simulator can be used as an effective tool to demonstrate the
complex set of factor relationships and effects related to requirements volatility.
Ó 2009 Elsevier Inc. All rights reserved.

1. Requirements volatility introduction
Requirements volatility refers to growth or changes in requirements during a project’s development lifecycle. There are multiple
aliases commonly associated with or related to the phenomenon of
requirements volatility. These terms include requirements change,
requirements creep, scope creep, requirements instability, and
requirements churn among others. Costello (1994) provides a relatively detailed set of metrics for requirements volatility. Other
simple metrics for requirements volatility deﬁne it as the number
of additions, deletions, and modiﬁcations made to the requirements set per time unit of interest (per week, month, phase,
etc.). Requirements volatility, in its various forms, surfaces as a frequent and high impact risk in numerous empirical studies performed to identify risk factors or to understand variables leading
to a project’s success or failure (examples include Boehm, 1991;
Curtis et al., 1988; Houston, 2000; Jones, 1994; Känsälä, 1997;
Moynihan, 1997; Ropponen, 1999; Ropponen and Lyytinen, 2000;
Schmidt et al., 2001; The Standish Group, 1995; Tirwana and Keil,
2006).
Changes to a set of requirements can occur at multiple points
during the development process (Kotonya and Sommerville,
1998). These changes can take place ‘‘while the requirements are
being elicited, analyzed and validated and after the system has
gone into service”. Past philosophy dictated that requirements
had to be ﬁrm by the completion of the requirements phase and
* Corresponding author. Tel.: +1 817 272 1332; fax: +1 817 272 3406.
E-mail addresses: ferreira@uta.edu (S. Ferreira), collofello@asu.edu (J. Collofello),
dan.shunk@asu.edu (D. Shunk), mackulak@asu.edu (G. Mackulak).
0164-1212/$ - see front matter Ó 2009 Elsevier Inc. All rights reserved.
doi:10.1016/j.jss.2009.03.014

that requirements should not change after this time. This view is
now understood to be unrealistic (Reifer, 2000). Kotonya and Sommerville (1998) discuss that requirements change is unavoidable.
They also indicate that requirements changes do not necessarily
imply that poor requirements engineering practice was utilized
as requirements changes could be the result of a combination of
factors. The term ‘‘requirements engineering” refers to the processes required to generate and maintain the software requirements throughout the duration of the project.
Concern for the effects of requirements volatility is not usually
associated with the front end of the process, for example, during
requirements deﬁnition. Volatility during the requirements deﬁnition phase is expected because this is when requirements are being
created. However, once the design process begins, the impact of
requirements change is progressively greater due to the additional
investment in time and effort as the project continues to generate
artifacts and complete required tasks. Additions or modiﬁcations
may need to be made to previously generated or in process project
artifacts and additional time investment or scrapped effort can
result. Due to the additional unplanned effort, severe consequences
can potentially occur, including signiﬁcant cost and schedule overruns, and at times, cancelled projects. The impact of changing
requirements during later phases of a project and approaches for
assessing the impacts of these changes has been well documented
(Yau et al., 1978, 1986, 1988; Yau and Kishimoto, 1987; Yau and
Liu, 1988). In an agile software development environment, changes
are welcomed throughout the development process (Beck et al.,
2001). The target of this article is not agile type projects but more
traditional development type projects.

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

2. The need for a requirements volatility assessment tool
The effects of requirements volatility have been discussed in the
literature for some time. However, little empirical research has
been carried out on the topic of requirements volatility that considered the factors involved and the integrated quantitative effects
of requirements volatility on factors related to key project management indicators (cost, schedule, and quality). A relatively small
number of studies consider requirements volatility and its associated effects, especially in a manner integrated with other software
project management factors. These existing studies primarily fall
into a few major research method categories: survey or software
assessment based research (Jones, 1998, 1994; Lane, 1998; Nidumolu, 1996; Zowghi et al., 2000; Zowghi and Nurmuliani, 2002),
interviews and case studies (Javed et al., 2004; Loconsole and Börstler, 2005, 2007; Nurmuliani et al., 2004; Zowghi and Nurmuliani,
1998), regression analysis (Stark et al., 1999), reliability growth
model (Malaiya and Denton, 1999), analytic hierarchy process
analysis (Finnie et al., 1993), and simulation models (Houston,
2000; Lin and Levary, 1989; Lin et al., 1997; Madachy et al.,
2007; Madachy and Tarbet, 2000; Pfahl and Lebsanft, 2000; Smith
et al., 1993).
The existing simulation models discussed in the literature were
developed and tailored for one organization, have a limited view of
requirements volatility or requirements engineering, or do not include requirements engineering processes considered in concert
with the rest of the lifecycle or other critical project factors. A paucity of the literature exists on process modeling and simulation
work performed in requirements engineering, an area now receiving more focused attention because of the impact that it has on the
rest of the systems and software engineering lifecycle.
The limited research and relative importance of requirements
volatility as a risk and the relatively sparse level of requirements
engineering process modeling led the researchers to more analysis
and examination of these areas. A system dynamics process model

1569

simulator, the Software Project Management Simulator (SPMS)
that includes data which is stochastically based on industry survey
data distributions, was then developed as part of a doctoral dissertation (Ferreira, 2002). SPMS illustrates a software business model
that considers the effects of requirements volatility on a software
project’s key management parameters: cost, schedule, and quality.
SPMS presents a more comprehensive and detailed view of the
researched areas than previous models. The development of the
SPMS simulator and associated results developed in this paper
are discussed in this journal article.
3. Requirements volatility tool development process
This section of the paper brieﬂy discusses the research method
used to develop the simulation model. Key research questions addressed in the initial research study include: (1) Which software
factors are affected by requirements volatility? (2) How can these
factors and the uncertainty associated to these factors be modeled?
and (3) What is the project management impact of requirements
volatility? Fig. 1 provides a summary view of the processes used
during the research effort. Starting with the ﬁgure’s top left and
top right sides and ﬂowing down, the ﬁgure illustrates that two efforts (one per ﬁgure side) were initiated concurrently and these efforts ﬂowed into the development of the software process
simulator discussed in this paper.
A rigorous review of the requirements engineering and requirements volatility related literature was performed. Various process
and information models were created to represent and assist in
analysis and synthesis of the knowledge gained during the literature review. Requirements engineering process models and workﬂows, an information model, and a causal model were developed
prior to the simulator development. Relevant factors and associated relationships were identiﬁed based on analysis of the literature review material and discussions with software engineering
experts. Further analysis of the captured information led to the

Fig. 1. Research method.

1570

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

development of a causal model (Fig. 2). The causal model is important because it illustrates the cause and effect relationships between software development factors related to requirements
volatility and includes hypothesized relationships between factors.
The causal model was iteratively developed based on fundamental
factor relationships (for example, job size and overall project effort), researcher industry and academic experience, and hypothesized relationships. The ﬁgure highlights (blue shading) factors
and associated relationships (blue lines) that were further explored
during the research effort. This causal model is expected to evolve
over time as more relationships are explored and understood.
One of the proposed solutions to address the identiﬁed research questions was to develop a software process simulator.
A simulator was selected because it provides a tool for software
project managers and researchers to perform ‘‘what if” analyses,
and enables users to examine the risk of various levels of
requirements volatility and determine project outcomes. A simulator can represent the complexity of relationships between large
quantities of interrelated factors and effectively illustrates the effects and impact of requirements volatility. A simulator with a
graphical icon format was chosen to represent project factors
and relationships. The previously developed causal model was
used to develop the simulator. A subset of the previously generated causal model relationships and factors were selected and
modeled. A subset of the causal model relationships and factors
was chosen because follow-on work needed to be limited in
length to allow key factor data to be collected or these areas
were already modeled in the pre-existing simulator that was extended to create the new simulator, SPMS. Concepts and constructs from all of the generated process and information
models contributed to the creation of the simulator’s workﬂows
and other model sectors.
Joint research was performed with Daniel Houston to characterize four deterministic software process simulators using statistical
design of experiments (Houston et al., 2001). Results of this experimentation work fed into Dan Houston’s development of the Software Process Actualized Risk Simulator (SPARS) (Houston, 2000).
The SPMS research simulation model evolved from Houston’s
SPARS model. The SPARS model also represents an adaptation, as
it reuses or modiﬁes large portions from Abdel-Hamid and Madnick’s model (1991) and Tvedt’s model (1996) and then extends
the consolidated model to incorporate effects of a number of risk
factors. The SPARS model was selected because of its comprehensive software project management scope and updates for more
modern development practices. Reusing components from SPARS
facilitated the development of the SPMS model in that common
constructs did not need to be recreated and the model used previously validated simulator components. As part of the research effort, the SPARS model was modiﬁed to eliminate some
unnecessary factors and extended to create the research model,
SPMS.
As the initial simulator sector designs were generated, walkthroughs were conducted with an initial set of individuals who
were familiar with the research. Once an initial version of the model containing the key constructs was completed, a secondary walkthrough of the model was held with four reviewers outside of the
research group. These reviewers included representatives from
industry and academia that were currently performing research
in requirements engineering and/or software process modeling
and simulation. Following the model walkthroughs, the simulator
was modiﬁed to incorporate reviewer comments and suggestions.
The model was then ready to include quantitative data.
Many of the model variables required data that was not available in the literature. In order to populate these model parameters,
a survey was developed and administered to collect the needed
data. The Project Management Institute’s Information Systems

Speciﬁc Interest Group (PMI-ISSIG) sponsored the survey by providing the host site for the web-based survey and sending notiﬁcations about the survey to its members. Although PMI-ISSIG was the
primary target population for the survey, one mailing was sent to
individual Software Engineering Institute (SEI) Software Process
Improvement Network (SPIN) group contacts within the United
States and to individual professional contacts. Three hundred
twelve software project managers and other software development
personnel submitted responses for the survey.
Survey results indicated that 78% of the respondents experienced some level of requirements volatility on their project. The
survey ﬁndings highlight that requirements volatility can increase
the job size dramatically, extend the project duration, cause major
rework, and affect other project variables such as morale, schedule
pressure, productivity, and requirements error generation. Fig. 3
shows an example of requirements volatility related data from
the survey. The histogram in the ﬁgure depicts how requirements
change affected the job size as a percent of the original job size. In
the vast majority of cases, requirements volatility increased the job
size. However, one can see that there were also cases where there
was no change or a net decrease in the job size. Survey respondents
had an average of 32.4% requirements volatility related job size increases. Other captured volatility effects included signiﬁcant increases in project rework and reduced team morale. The survey
also captured effects from schedule pressure. As the resource effort
increases due to requirements volatility (to address job size additions and rework), schedule pressure also increases. The survey
data showed increases in requirements error generation as the
schedule pressure increases. These effects cause consequences
leading to impacts on key project management indicators such as
cost, schedule, and quality. More details on the survey ﬁndings
showing primary and secondary effects of requirements volatility
are addressed in Ferreira (2002).
Statistical analysis of the survey responses allowed the generation of stochastic distributions for many of the simulation model’s
requirements volatility and requirements engineering factors and
relationships. The model’s stochastic inputs are primarily generated using either empirical discrete distributions derived from analyzing histograms of the data or are generated using the inverse
transform method (Abramowitz and Stegun, 1964). The selection
of the type of distribution depended on the survey analysis results.
Using these stochastic inputs, the simulation model’s random variates use a random number as an input to generate the desired distribution. Based on the sampling frequency, the distributions are
sampled once per run or are sampled continuously throughout a
run to be drawn as necessary.
Once the derived stochastic factor distributions and relationships were added to the simulator, veriﬁcation and validation
(V&V) exercises were performed. Model validation determines
whether a model is a ‘‘useful or reasonable representation of the
system” (Pritsker et al., 1997). Veriﬁcation checks that the simulation model runs as intended. Matko et al. (1992) indicate that modeling work is not an exact science since the real system is never
completely known. Given this situation, the validation and veriﬁcation effort primarily focused on building conﬁdence in the model
as a reasonable representation of the system and in its usefulness
in the provision of results. The overall approach for veriﬁcation and
validation included tests of the structure and behavior of the model. This strategy follows a framework of guidelines presented by
Richardson and Pugh (1981) that builds conﬁdence in the model
and its results. The tests focus on suitability and consistency
checks. The model veriﬁcation and validation activities were performed by the model developer, and various software process
and project experts.
Additional veriﬁcation work was performed in order to understand differences between the SPMS and SPARS model when the

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

1571

Fig. 2. Causal model.

risk factors are not actualized. Differences between the models in
this case were relatively small. More information about this veriﬁcation work is discussed in Ferreira et al. (2003).
4. Assessment tool capability and use
SPMS illustrates researched effects of requirements volatility
and includes requirements engineering extensions to the SPARS

software project management model. The SPARS model was modiﬁed to eliminate some unnecessary factors and extended to create
the research model. Major additions to the base model include the
researched results for the effects of requirements volatility and signiﬁcant extensions to add and support the requirements engineering portions of the software development lifecycle. SPMS
encompasses the requirements engineering through test phases
of the software development lifecycle. Requirements volatility

1572

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

Fig. 3. Distribution of requirements volatility related percent change in job size.

starts and is stochastically simulated during the project’s development and test phases. The model workﬂows also cover the entry of
change requests, change request analysis and review activities, and
their disposition (accepted, rejected/deferred).

Table 1
Model classiﬁcation.
Purpose
Scope
Model
approach

Planning, understanding, process improvement
Medium to large size project, short to long duration, one product/
project team
Continuous, mixed mode (stochastic and deterministic variables),
iThinkTM simulation tool

SPMS demonstrates causal model effects of requirements volatility. Survey ﬁndings showing the impact of requirements volatility on increasing software project job size (this occurs a majority of
the time), increased rework, and lowered staff morale are represented in the model using stochastic relationships derived during
the survey data analysis. Over 50 new parameters that used distributions derived from the survey data were added to the model. In
addition to these survey drawn distributions, a signiﬁcant number
of distributions were reused from other sources, or parameters
were modeled using single point inputs that could be changed by
a user. The effects of lowered morale on requirements engineering
productivity and requirements error generation are represented in
the model. Schedule pressure effects on requirements error generation were also studied as part of the survey data analysis and were
added to pre-existing schedule pressure effects in the model.
Among other survey data used in the simulator, the model includes
requirements defect detection effectiveness for various software
development activities or milestones and relative work rates of
requirements volatility related activities compared to their normal
work rate. Other model contributions include the addition of a
requirements engineering staff type and requirements engineering
support activities which were added to pre-existing simulator
development and test personnel and activities.
Table 1 presents a view of the model’s classiﬁcation, according
to the characterization framework from Kellner et al. (1999). The
typical model audience is expected to be software development
project managers or researchers seeking to gain an understanding
of requirements engineering and requirements volatility and its effects integrated with other software project risks. The model is relatively complex, given its purpose, and assumes a sophisticated

Table 2
Model sector descriptions.
Sector name

Description

Change request work ﬂow

Stochastic change request entry over 10 intervals. Incoming change request (CR) analysis, change request control board (CCB) review,
and disposition
Requirements generation and review process including requirements error and defect rework. Stochastic entry of requirements
volatility related project scope changes and rework over 10 intervals
Work product ﬂow through the development lifecycle, from design and code through testing and rework of design and code errors.
Work is pulled from development and test activities for requirements volatility related rework and reduction and for rework of
requirements defects
A support sector that calculates the amount of product to add to the product cycle for requirements volatility additions based on
survey data
A support sector that calculates the amount of product to pull from the development and test work for requirements volatility related
rework based on survey data
A support sector that calculates the amount of product to pull from development and testing work ﬂow activities for requirements
related reductions. Includes policy choice that deﬁnes where to remove product
Entry and summation of requirements engineering, developer, and tester planned stafﬁng proﬁle information
Allocation of requirements engineer effort to requirements engineering activities based on activity priority

Requirements work ﬂow
Development and test work ﬂow

Requirements change additions
Requirements change rework
Requirements change reductions
Planned stafﬁng
Requirements engineer effort
allocation
Developer and tester effort
allocation
Requirements quality management
Development and test quality
management
Actual stafﬁng
Attrition and replacement
Planning
Control
Adjustment of job effort
Productivity inputs
Productivity
Progress measure
Senior management commitment

Allocation of developer and tester effort to project activities based on activity priority
Generation and detection of requirements errors and defects
Generation and detection of design and code errors and defects
Entry and exit of staff based on planned stafﬁng proﬁles, assimilation of new staff, attrition, and replacements. Entry of contract
personnel and organizationally experienced personnel handled separately for the different staff groups
Attrition (including attrition due to low morale) and replacement calculations for requirements engineers and the grouped set of
developers and testers
Calculation of requirements engineer and the grouped developer and tester work force levels needed based on willingness to hire
Calculation of effort perceived still needed to complete project, effort shortages, schedule pressure, and assumed requirements
engineer, developer, tester productivity. Calculations to determine start of development and testing
Adjustment of job effort based on requirements volatility related additions, rework, and reductions as well as from underestimation
Productivity inputs and calculation of project activity productivity based on productivity multipliers
Work rate modiﬁcation due to effort remaining, effort required, and staff exhaustion. Generation of two staff type productivity
multipliers (including learning, communication overhead, staff experience, and morale)
Calculation of requirements engineering and development and testing progress. Modiﬁcation of currently perceived job size based on
requirements volatility and discovered work due to underestimation
Adjustment of stafﬁng and schedule multipliers based on senior management commitment. This is the only sector that was not
modiﬁed from the original SPARS model

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

user that is educated on the use of simulation models and software
development project management. The model is practically based,
relying on a signiﬁcant and proven foundation of software project
management and simulation research.
The model is segmented into 20 sectors. The sectors are organized into convenient and logical groupings of related factors. Table 2 provides an overview of the model sectors with a brief
description of each in order to give the reader an introduction to
the model’s scope. An example of one sector excerpted from the
model is shown in Fig. 4. The view illustrates the requirements
engineering work ﬂow sector. The connections on the right of the
sector ﬂow into or out of the development and test work ﬂow sector (sector not shown).
The requirements work ﬂow sector encompasses a normal
product work ﬂow. Requirements are generated and reviewed during the normal product work ﬂow. The normal requirements work

1573

ﬂow begins at the initiation of the requirements engineering phase,
at the To_Be_Worked stock. The To_Be_Worked stock is initially
populated with the estimated starting job size. The work then
ﬂows through the Generated_Reqts, Reqts_Awaiting_Review,
Reviewed_Reqts stocks, and then into the development process
as an inﬂow. The requirements generation activities are aggregated, encompassing requirements elicitation, analysis, negotiation, and initial requirements management. Once reviewed, the
requirements product is dispositioned and defective product containing requirements errors is removed from the normal work ﬂow
and becomes part of the requirements error rework work ﬂow, to
be reworked before ﬂowing into the development and test activities. Requirements defects caught post the requirements phase
come back into the requirements defect rework work ﬂow to be reworked. The reworked product then ﬂows back into the development and test activities. Additions to job size due to volatility

Fig. 4. SPMS simulator requirements engineering work ﬂow sector.

1574

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

and underestimation ﬂow into the normal work ﬂow through the
course of the project. The lower half of the sector includes the
requirements change rework work ﬂow. Rework due to requirements volatility is drawn from the development and test work
ﬂows and is worked though the requirements change rework work
ﬂow. This work ﬂow contains separate requirements error related
activities.
The model allows the user to enter detailed inputs related to
their project. Other data is automatically extracted from the stochastic distributions derived from the survey data. Table 3 provides
a listing of a subset of the new inputs added to the model to provide a ﬂavor for the types of data required from the user. These inputs can be input per run or a set of runs. Table 4 identiﬁes the new

Table 3
New factor user inputs (subset only).
Factor

Input quantity

Quantity of experienced requirements
engineers (REs), per interval
Quantity of new requirements
engineers, per interval
Quantity of experienced personnel allocated
to training new reqts engineers (%)
RE transfer in day (days)
Experienced organization RE transfer quantity
Time delay to transfer REs off project (days)
Max quantity of new RE hires per experienced
staff (staff/staff)
Time for organization experienced REs (but not
project experienced) to be productive (days)
Project day that person in org is scheduled to
come onto project (day)
Quantity of RE staff experienced in the organization who
are to be transferred on project (staff)
Requirements review adjustment policy – adjusts
review effort (boolean switch)
Reworked requirements review adjustment policy
(boolean switch)
Requirement error bad ﬁx fraction (%)
Requirements defect bad ﬁx fraction (%)
Nominal change request analysis productivity
(function points/person-day)
Nominal requirements generation productivity
(function points/person-day)
Nominal requirements review productivity
(function points/person-day)
Requirements volatility (boolean switch)

10 (1 for each of 10
intervals)
10 (1 for each of 10
intervals)
1
1
1
1
1
1
1
1
1 Selection
1 Selection
1
1
1
1
1
1 Selection

survey distributions added to the model. Data from the survey is
extracted from stochastic distributions with timing as deﬁned in
Table 4. ‘‘1 Selection per run” means that a value is pulled from
the stochastic distribution one time per run and used throughout
the run. While the data listed in Tables 3 and 4 do not include all
the new or modiﬁed factors in the model it does provide a perspective of the type of data that the user can enter and use.
5. Assessment tool results
The simulator was used to run two cases for the purpose of
comparing them. Each case was setup for 100 runs apiece. The
set of 100 runs for each case was selected for convenience as the
modeling tool allows additional runs, if desired. The two cases
are as follows: (1) A baseline case without the requirements volatility actualized [baseline] and (2) a case with the requirements
volatility related factors actualized [reqts volatility]. The data in
the square brackets corresponds to the case identiﬁer in later ﬁgures. Actualizing the requirements volatility risk for the second
case allows the model to represent the stochastic researched effects of requirement volatility. These include survey-based effects
related to requirements additions, changes, and modiﬁcations as
well as entering change requests. Also included, among the other
effects, are morale, and related schedule pressure effects on morale. The initial job size was estimated to be 400 function points.
The original schedule estimate (planned duration) was deﬁned to
be 408 days.
Figs. 5–8 depict the differences, at the completion of the projects, between the baseline runs (no requirements volatility considered) and runs with the other case where the requirements
volatility risk is actualized for various summary outputs. The model allows additional detailed outputs, if desired. Box plots were
used to represent the data because the simulation results were
positively skewed given the tendency for higher project size, cost,
duration, and released defects (among other outputs) with the
actualization of the requirements volatility risk. The box plots provide a graphical display of the center and the variation of the data,
allowing one to see the symmetry of the data set (Ott, 1988). With
the exception of the ﬁnal project size (Fig. 5), the baseline results
show a small level of variability because some of the model parameters (e.g. requirements engineering process factors) were modeled
stochastically. Therefore, even when the requirements volatility
risk is not actualized, some variability in results will appear.

Table 4
New survey data distributions.
Factor

Selection frequency

Quantity of requirements change requests, per interval (intervals 1–10)
Change request time span with requirements volatility (ratio of duration)
Determination of change request acceptance/deferral due to schedule pressure (ratio)
Relative requirements defect rework productivity (ratio)
Relative requirements generation rework productivity (ratio)
Relative requirements change error rework productivity (ratio)
Relative requirements change requirement review productivity (ratio)
Relative design and code requirements defect rework productivity (ratio)
Percentage of perceived job size increased due to requirements volatility, per interval (%)
Percentage of perceived job size reworked due to requirements volatility, per interval (%)
Percentage of perceived job size reduced due to requirements volatility, per interval (%)
Requirements review effectiveness (%)
Design and code requirement defect detection effectiveness (%)
Design and code review requirement defect detection effectiveness (%)
Test requirement defect detection effectiveness (%)
Reworked requirements review effectiveness (%)
Reworked requirements design and code defect detect effectiveness (%)
Requirements error multiplier for schedule pressure (ratio)
Requirements error multiplier for morale (ratio)

10 Selections per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
10 Selections per run
10 Selections per run
10 Selections per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run
1 Selection per run

(1 selection for each interval)

(1 selection for each interval)
(1 selection for each interval)
(1 selection for each interval)

1575

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

Fig. 5. Project size box plots.

Fig. 7. Project duration box plots.

Fig. 8. Project released defect density box plots.
Fig. 6. Project cost box plots.

Fig. 5 illustrates the difference in project size between the baseline case and the requirements volatility case. The baseline size is
400 function points. The average size for the requirements volatility case is 499.9 function points with a standard deviation of 82.0,
and a range from 382.0 to 788.3 function points over the 100 runs.
The runs that have values below the baseline 400 function points
(e.g. 382.0) can be explained by the fact that requirements changes
do not always add additional project scope, but can be made to remove scope or unnecessary requirements. However, 93 of the 100
runs had a ﬁnal project size greater than the base case of 400 functions points with relatively large scope added in the course of the
project. This scope addition has a signiﬁcant negative ripple effect
on the project cost (effort in person-days) and project duration. Table 5 contains the detailed statistics for each case including average, median, standard deviation calculations and the minimum
and maximum run values.
Fig. 6 presents the results for cost. Cost is considered to be human effort consumed during the project in person-days. This cost is
equivalent to effort and does not include other project costs. The
effort or cost presented in the ﬁgure encompasses the cumulative
effort for the requirements engineering through test activities.

Table 5
Case study simulation results.
Output/Case

Median

Std. Dev.

Minimum

Maximum

Size (function points)
Baseline
400
Reqts volatility
499.9

Average

400
485.7

0
82.0

400
382.0

400
788.3

Cost (effort in person days)
Baseline
4107.6
Reqts volatility
6208.2

4119.0
5968.9

35.9
1398.1

3969.5
3988.5

4155.5
11196.1

Duration (days)
Baseline
Reqts volatility

452.0
608.5

4.3
132.8

442.0
444.0

461.0
1101.0

Released defect density (defects per function point)
Baseline
0.753
0.753
0.013
Reqts volatility
0.894
0.886
0.075

0.726
0.747

0.792
1.170

452.4
634.0

The detailed statistics are presented in Table 5. As mentioned
earlier, the baseline results do show a small level of variability because some of the model parameters are represented stochastically. The average for the requirements volatility case is over
2000 person-days more than the average for the baseline case.

1576

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

Considering that the baseline cost is an average of 4107.6 person
days, this is a very large number for a program manager to justify
as it represents more than a 50% increase in resource costs. The
cost range for the requirements volatility case is very large as well.
The requirements volatility case has a maximum of 11,196.1 person-days. This represents a very signiﬁcant difference from the
baseline maximum.
Fig. 7 displays the results for project duration in days. The duration
encompasses the requirements engineering through test activities for
the project. The detailed statistics for the two cases are shown in Table
5. It takes an average of 452.4 days to complete the project for the
baseline case. The average for the requirements volatility case is signiﬁcantly higher at an average of 634.0 days. As in the case of the cost,
the range for the requirements volatility case has a wide span.
Fig. 8 presents the project released defect density box plots. The
data represents the defects released post the test process and is in
units of defects per function point. The quantity of released defects
is also signiﬁcantly higher for the requirements volatility case. The
baseline case has an average of 0.753 defects per function point.
The average for the requirements volatility case is 0.894. The range
span for this case is also relatively large.
For the case that include the risk of requirements volatility, the
box plots for each parameter have a wide span. The wide span in outcomes indicates a large potential for unpredictable results as well as
considerable impact on project results. A comparison of the model
results for projects with no requirements volatility and those with
requirements volatility show signiﬁcant differences in the box plots.
One can clearly see the potential impact of requirements volatility
on key project management parameters such as cost (effort), schedule, and quality. Differences between the baseline and requirements volatility case results can be attributed to the addition (or
reduction) in project scope caused by requirements volatility, cause
and effect relationships between factors, and the stochastic representation of a number of the factors.

6. Summary and conclusions
Key research questions were addressed as part of the research. These questions include: (1) Which software factors are
affected by requirements volatility? (2) How can these factors
and the uncertainty associated to these factors be modeled?
and (3) What is the project management impact of requirements
volatility? A causal model was developed and used to evaluate
which software factors are affected by requirements volatility.
A survey was administered to collect information for a subset
of the factors identiﬁed in the causal model. This allowed the
researchers to verify the relationships and quantify the level of
the relationships. A simulator was chosen as the means to model
how the factors relate to other project factors and to represent
the uncertainty associated to the factors using stochastic distributions derived from survey data. The SPMS model assists in
understanding and evaluating the program management impact
of requirements volatility.
This simulator can help an interested user to better understand
the requirements engineering process and the impact of requirements volatility via its process-oriented workﬂows and comprehensive scope. The factor relationships represented in the model
represent a signiﬁcant contribution. This work expands our understanding of the requirements engineering process and the effects of
requirements volatility. The rigorous research to both understand
and leverage the previous foundation of knowledge in requirements engineering and requirements volatility and the thorough
nature of the survey and analysis of this valuable data to assess factor relationships and populate the simulator with empirical data is
a signiﬁcant contribution.

In addition to simulating the effects of requirements volatility,
SPMS offers the ability to simulate various project scenario combinations starting with the requirements engineering portion of the
lifecycle. The model offers a robust set of parameters that capture
various facets of the project including requirements errors and defects, requirements defect density, defect containment effectiveness for various milestones, and other parameters integrated
with a very comprehensive development and test related set of factors and relationships. Each of the model distributions can be easily calibrated and tailored to a speciﬁc organization’s environment.
All the other factors are also setup to be readily modiﬁable to reﬂect an organization’s historical data.
Survey data used in the simulator captures information on factors and relationships not previously available from a wide population in the software industry and allows modeling variables
stochastically given the large quantity of survey responses. Many
of the model variables were modeled stochastically to allow the
user to assess project outcomes probabilistically. The model results
quantitatively illustrate the potential for signiﬁcant cost, schedule,
and quality impacts related to requirements volatility.
Software project managers can use this tool to better understand the effects of requirements volatility and this risk in concert
with other common risks. The simulator can be used to assess potential courses of action to address the risk of requirements
volatility.

7. Future research
Additional research in the area of requirements engineering, as
it continues to evolve, is expected to provide a continuous stream
of new ideas, perspectives, and information that can allow for the
development of richer models and that can represent different facets of understanding in this under-represented yet critical research
area. More work needs to be done to model the impact of requirements engineering related processes and policies so that project
managers and software development personnel are more aware
of the impact of the decisions they make during this phase and
how the rest of the lifecycle may be affected by their choices. This
work can lead to the further identiﬁcation of best practices and
generate insights valuable to managing software development projects in the future.
As this and other simulators that incorporate requirements
engineering processes and relationships continue to evolve, additional experimentation with the models may prove valuable in
identifying common factors and relationships. The research model
presented in this paper is relatively large and complex. Sensitivity
analysis can assist in the identiﬁcation of inﬂuential factors in this
and other models. The results of further experimentation may be
used to ‘‘prune” insigniﬁcant factors from the model so that a more
efﬁcient model can result (Houston et al., 2001). One of the beneﬁts
of a simpler model includes a shortened training and learning
ramp-up time. Core concepts that make the most difference in results can be emphasized. Since the model is less complex it becomes easier to understand and may perhaps be more popular
given the reduced time to understand and then populate the model
with organizational and project speciﬁc data. Maintenance time
may also be reduced because the model is smaller and it is easier
to ﬁnd and ﬁx problems.
Agile processes which welcome requirements changes present
another valuable area to study. As these processes continue to mature and more quantitative results are available, simulating different types of agile processes, (e.g. XP, Scrum, etc.) would be of
interest to determine the project management ramiﬁcations related to cost, schedule, and quality as compared to more traditional
approaches.

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568–1577

References
Abdel-Hamid, T., Madnick, S., 1991. Software Project Dynamics: An Integrated
Approach. Prentice-Hall, Englewood Cliffs, NJ.
Abramowitz, M., Stegun, I.A. (Eds.), 1964. Handbook of Mathematical Functions,
Applied Mathematics Series 55. National Bureau of Standards, Washington, DC.
Beck, K., Beedle, M., van Bennekum, A., Cockburn, A., Cunningham, W., Fowler, M.,
Grenning, J., Highsmith, J., Hunt, A., Jeffries, R., Kern, J., Marick, B., Martin, R.,
Mellor, S., Schwaber, K., Sutherland, J., Thomas, D., 2001. Principles Behind the
Agile Manifesto. Retrieved 11.6.2008 from: <http://www.agilemanifesto.org/
principles.html>.
Boehm, B.W., 1991. Software risk management: principles and practices. IEEE
Software 8 (1), 32–41.
Costello, R.J., 1994. Metrics for Requirements Engineering. Master of Science Thesis,
California State University, Long Beach.
Curtis, B., Krasner, H., Iscoe, N., 1988. A ﬁeld study of the software design process for
large systems. Communications of the ACM 31 (11), 1268–1287.
Ferreira, S., 2002. Measuring the Effects of Requirements Volatility on Software
Development Projects, Ph.D. Dissertation, Arizona State University.
Ferreira, S., Collofello, J., Shunk, D., Mackulak, G., Wolfe, P., 2003. Utilization of
Process Modeling and Simulation in Understanding the Effects of Requirements
Volatility in Software Development. In: Proceedings of the 2003 Process
Simulation Workshop (ProSim 2003).
Finnie, G.R., Witting, G.E., Petkov, D.I., 1993. Prioritizing software development
productivity factors using the analytic hierarchy process. Journal of Systems
and Software 22 (2), 129–139.
Houston, D.X., 2000. A Software Project Simulation Model for Risk Management,
Ph.D. Dissertation, Arizona State University.
Houston, D.X., Ferreira, S., Collofello, J.S., Montgomery, D.C., Mackulak, G.T., Shunk,
D.L., 2001. Behavioral characterization: ﬁnding and using the inﬂuential factors
in software process simulation models. Journal of Systems and Software 59 (3),
259–270.
Javed, T., Maqsood, M., Durrani, Q.S., 2004. A study to investigate the impact of
requirements instability on software defects. ACM SIGSOFT Software
Engineering Notes 29 (3), 7.
Jones, C., 1994. Assessment and Control of Software Risks. PTR Prentice-Hall, Inc.,
Englewood Cliffs, NJ.
Jones, C., 1998. Estimating Software Costs. McGraw-Hill, New York.
Känsälä, K., 1997. Integrating risk assessment with cost estimation. IEEE Software
14 (3), 61–67.
Kellner, M.I., Madachy, R., Raffo, D.M., 1999. Software process modeling: why?
what? how? Journal of Systems and Software 46 (2–3), 91–105.
Kotonya, G., Sommerville, I., 1998. Requirements Engineering: Processes and
Techniques. John Wiley and Sons, Ltd..
Lane, M.S., 1998. Enhancing software development productivity in Australian ﬁrms.
In: Proceedings of the Ninth Australasian Conference on Information Systems
(ACIS ’98), vol. 1, pp. 337–349.
Lin, C.Y., Abdel-Hamid, T., Sherif, J.S., 1997. Software-engineering process
simulation model (SEPS). Journal of Systems and Software 38 (3), 263–277.
Lin, C.Y., Levary, R.R., 1989. Computer aided software development process design.
IEEE Transactions on Software Engineering 15 (9), 1025–1037.
Loconsole, A., Börstler, J., 2005. An industrial case study on requirements volatility
measures. In: Proceedings of the 12th Asia-Paciﬁc Software Engineering
Conference (APSEC ’05), 8 p.
Loconsole, A., Börstler, J., 2007. Are size measures better than expert judgment? An
industrial case study on requirements volatility. In: Proceedings of the 14th
Asia-Paciﬁc Software Engineering Conference (APSEC ’07), pp. 238–245.
Madachy, R., Tarbet, D., 2000. Initial experiences in software process modeling.
Software Quality Professional 2 (3), 1–13.
Madachy, R., Boehm, B., Lane, J., 2007. Assessing hybrid incremental processes for
SISOS development. Software Process: Improvement and Practice 12 (5), 461–
473.
Malaiya, Y.K., Denton, J., 1999. Requirements volatility and defect density. In:
Proceedings of the 10th International Symposium on Software Reliability
Engineering, pp. 285–294.
Matko, D., Zupancic, B., Karba, R., 1992. Simulation and Modeling of Continuous
Systems: A Case Study Approach. Prentice-Hall International Ltd., Great Britain.
Moynihan, T., 1997. How experienced project managers assess risk. IEEE Software
14 (3), 35–41.
Nidumolu, S.R., 1996. Standardization, requirements uncertainty and software
project performance. Information and Management 31 (3), 135–150.
Nurmuliani, N., Zowghi, D., Fowell, S., 2004. Analysis of requirements volatility
during software development life cycle. In: Proceedings of the 2004 Australian
Software Engineering Conference (ASWEC ’04), pp. 28–37.
Ott, L., 1988. An Introduction to Statistical Methods and Data Analysis. PWS-KENT
Publishing Company.
Pfahl, D., Lebsanft, K., 2000. Using simulation to analyze the impact of software
requirements volatility on project performance. Information and Software
Technology 42 (14), 1001–1008.
Pritsker, A. Alan B., O’Reilly, Jean J., LaVal, David K., 1997. Simulation with Visual
SLAM and AweSim. System Publishing Company, West Lafayette, IN.
Reifer, D.J., 2000. Requirements management: the search for Nirvana. IEEE Software
17 (3), 45–47.
Richardson, George, P., Alexander, L., Pugh III, 1981. Introduction to System
Dynamics Modeling with DYNAMO. The MIT Press, Cambridge, MA.

1577

Ropponen, J., 1999. Risk assessment and management practices in software
development. Chapter 8 in Beyond the IT Productivity Paradox. John Wiley
and Sons. pp. 247–266.
Ropponen, J., Lyytinen, K., 2000. Components of software development risk: how to
address them? A project manager survey. IEEE Transactions on Software
Engineering 26 (2), 98–112.
Schmidt, R., Lyytinen, K., Keil, M., Culle, P., 2001. Identifying software project risks:
an international Delphi study. Journal of Management Information Systems 17
(4), 5–36.
Smith, B.J., Nguyen, N., Vidale, R.F., 1993. Death of a software manager: how to avoid
career suicide through dynamic software process modeling. American
Programmer 6 (5), 10–17.
The Standish Group, 1995. The Chaos Report. Obtained from <http://www.
standishgroup.com/chaos.html>.
Stark, G.E., Oman, P., Skillicorn, A., Ameele, A., 1999. An examination of the effects of
requirements changes on software maintenance releases. Journal of Software
Maintenance: Research and Practice 11 (5), 293–309.
Tirwana, A., Keil, M., 2006. Functionality risk in information systems development:
an empirical investigation. IEEE Transactions on Engineering Management 53
(3), 412–425.
Tvedt, J.D., 1996. An Extensible Model for Evaluating the Impact of Process
Improvements on Software Development Cycle Time. Ph.D. Dissertation,
Arizona State University.
Yau, S.S., Collofello, J.S., MacGregor, T., 1978. Ripple effect analysis of software
maintenance. In: Proceedings of the IEEE Computer Society’s Second International
Computer Software and Applications Conference (COMPSAC ’78), pp. 60–65.
Yau, S.S., Kishimoto, Z., 1987. A method for revalidating modiﬁed programs in the
maintenance phase. In: Proceedings of 11th IEEE International Computer
Software and Applications Conference (COMPSAC ‘87), pp. 272–277.
Yau, S.S., Liu, C.S., 1988. An approach to software requirement speciﬁcations. In:
Proceedings of 12th International Computer Software and Applications
Conference (COMPSAC ‘88), pp. 83–88.
Yau, S.S., Nicholl, R.A., Tsai, J.P., 1986. An evolution model for software maintenance.
In: Proceedings of 10th IEEE International Computer Software and Applications
Conference (COMPSAC ‘86), pp. 440–446.
Yau, S.S., Nicholl, R.A., Tsai, J.P., Liu, S.S., 1988. An integrated life-cycle model for software
maintenance. IEEE Transactions on Software Engineering 14 (8), 1128–1144.
Zowghi, D., Nurmuliani, 1998. Investigating requirements volatility during software
development: research in progress. In: Proceeding of the Third Australian
Conference on Requirements Engineering (ACRE98), pp. 38–48.
Zowghi, D., Nurmuliani, 2002. A study of the impact of requirements volatility on
software project performance. In: Proceedings of the Ninth Asia-Paciﬁc
Software Engineering Conference (ASPEC ’02), pp. 3–11.
Zowghi, D., Offen, R., Nurmuliani, 2000. The impact of requirements volatility on the
software development lifecycle. In: Proceedings of the International Conference
on Software Theory and Practice (IFIP World Computer Congress), pp. 19–27.

Susan Ferreira is an Assistant Professor and the founding Director of the Systems
Engineering Research Center (SERC) at The University of Texas, Arlington (UTA).
Before joining UTA, Dr. Ferreira worked as a systems engineer in the Defense
industry on complex software intensive systems. Her industry background includes
work for Lockheed Martin, General Dynamics/Motorola, and Northrop Corporation.
Her teaching and research interests are related to systems engineering (SE) and
include requirements engineering, SE process modeling and simulation, lean SE, SE
return on investment, SE cost estimation, and system of systems engineering.
James Collofello is currently Associate Dean for the Engineering School and Professor of Computer Science and Engineering at Arizona State University. He received
his Ph.D. in Computer Science from Northwestern University. His teaching and
research interests lie in the software engineering area with an emphasis on software project management, software quality assurance and software process modeling. In addition to his academic activities, he has also been involved in applied
research projects, training and consulting with many large corporations over the
last 25 years.
Dan Shunk is the Avnet Professor of Supply Network Integration in Industrial Engineering at Arizona State University. He is currently pursuing research into collaborative commerce, global new product development, model-based enterprises and
global supply network integration. He won a Fulbright Award in 2002-2003, the 1996
SME International Award for Education, the 1991 and 1999 I&MSE Faculty of the Year
award, the 1989 SME Region VII Educator of the Year award, chaired AutoFact in 1985,
and won the 1982 SME Outstanding Young Engineer award. Dr. Shunk studied at
Purdue where he received his Ph.D. in Industrial Engineering in 1976.
Gerald Mackulak is an Associate Professor of Engineering in the Department of
Industrial, Systems and Operations Engineering at Arizona State University. He is a
graduate of Purdue University receiving his B.Sc., M.Sc., and Ph.D. degrees in the
area of Industrial Engineering. His primary area of research is simulation methodology with a focus on model abstraction, execution speed and output analysis. He
has authored over 75 articles, served as an associate editor for two simulation
journals and continues to guide research in simulation efﬁciency.

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996

A Design-based Model for the Reduction of Software Cycle Time
Ken W . Collier, Ph.D.
Assistant Professor
Computer Scienceand Engineering
Northern Arizona University, Flagstaff, AZ
Ken.Collier@nau.edu

JamesS. Collofello, Ph.D.
Professor
Computer Science
Arizona StateUniversity, Tempe, AZ
James.Collofello@Asu.Edu
advantage of emerging technologies [ 11. Although much
has been written about cost estimation, effort estimation
and productivity and process improvement, little research
has been directly aimed at reducing software cycle time.
The proposed model is based on a systematic and
rigorous analysis of the software process and identification
of product factors that affect cycle-time, as well as the
impact of design decisions on those factors. This
design-based model attempts to reduce software
development time by iteratively and strategically
restructuring the design of a software system. In order to be
practical, such a model must provide significant benefits at
a minimaI cost. It must be able to be implemented simply
without requiring an excess of special training. It must be
applicable to as many different types of software systemsas
possible, and it must provide measurablebenefits. It should
also work in concert with, instead of as a replacement for,
existing models and methodologies.
This paper is divided into three major sections. The
first section provides motivation for the design-based
model presented in the paper. It does so by demonstrating
the impact of software design decisions on cycle-time,
software quality, and scheduling options. The second
section details the model and provides theoretical
justification for an algorithmic approach to schedule
refinement. The model also provides some guidance for
restructuring the design to further shorten cycle time.
Finally, the results of an evaluation of the model are
provided. Although these results are inconclusive, they
suggest that the model has promise and meets the
requirementspreviously established.

Abstract
This paper presents a design-based soj?ware cycle time
reduction model that can be easily implemented without
replacement of existing development paradigms or design
methodologies, The research results suggest that there are
many cycle-time factors that are influenced by design
decisions. If manipulated carefully it appears that an
organization can reduce cycle-time and improve quality
simultaneously. The preliminary results look promising
and it is expected that further experimentation will support
the use of this model. This paper will analyze the basis for
the model proposed here, describe the model’s details, and
summarize the preliminary results of the model.

Introduction
Many organizations have relatively mature, effective
software-development processes in place and have
employed talented software engineers and managers to
implement these processes. In such organizations, it is
appropriate to question whether each softwaredevelopment effort is performed at peak efficiency and
effectiveness.
One of the primary goals of any organization is to
release high quality software in as little time as possible.
Therefore, an increase in software-development efficiency
may be manifest as a reduction in development time.
However, simply reducing development time is not good
enough. It is important that these reduction efforts
maintain high levels of both process and product quality.
This research involves the examination and control of
factors that have an impact on software development time.
More specifically, this paper presents a model for reducing
the development cycle time by restructuring software
design. Reduction of development time, together with
increasing quality and productivity, is the goal of many
software development organizations. The benefits are
numerous including: extended market life; increased
market share; higher profit margins; and the ability to take

Software Cycle Time Factors
Design DecisionsAffect Cycle Time and Software
Quality
Early in this research project, a systematic analysis of
cycle-time factors and issues was conducted. The details of
this analysis are outlined in [2]. However, an overview of
733

1060-3425/96$5.00 0 1996 IEEE

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29)
1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996

scheduling decisions that increase the chance that a good
schedule will be found. The software design and
scheduling process can be viewed as a process of making
decisions to maximize the chance of finding the best
accessibleschedule,given all scheduling constraints [2].
Currently, this attempt at finding a best accessible
schedule is a function of intelligent decision-making,
intuition, and the application of design and scheduling
heuristics. An approach that is highly subject to the
abilities and expertise of the software engineers and project
managers. An ideal scheduling methodology should be a
prescriptive process that encourages the result of a good
scheduleregardlessof expertise and ability.

the results of that study will serve to motivate the designbasedmodel proposed in this paper.
First, the software development cycle was examined to
establish a comprehensive set of factors impacting cycletie. These factors were divided into process factors and
product factors. Process factors include: software reuse,
requirements change, risk management, productivity,
personnel availability, software tools, equipment resources,
method maturity, verification and validation techniques,
quality assurance techniques, integration strategies, and
integration schedule. Product factors include: subsystem
dependencies, module reuse, subsystem complexity,
subsystem independence, subsystem risk, subsystem size,
and subsystemgeneralization.
Notably, most of these factors are impacted by design
decisions. This recognition led to a set of cycle-time
improving design goals including: maximize reuse; design
independent subsystems:design simple subsystems;ensure
completeness; localize risk: design to minimize risk
occurrence; ensure correct design: design for low coupling;
design for high cohesion; design to maximize
independence; design unique subsystems; and design for
flexible scheduling.
A fortunate, and unexpected side-effect of this analysis
is that these are the same design goals that improve design
quality. This analysis showed that design decisions impact
both design quality and cycle-time, and that these goals are
not mutually exclusive. Moreover, the analysis revealed
that improving cycle-time through design decisions is
likely to improve quality and vice versa

An Overviewof’Project Scheduling
In the late 1950s two computer-based scheduling
systems were developed to aid in the scheduling of large
engineering projects. Both are based on task dependency
networks. The critical-path method (CPM) was developed
by Du Pont and Remington Rand. The CPM method is a
deterministic scheduling strategy that is based on the best
estimate of task-completion time. The program evaluation
and review technique (PERT) is a similar method and was
developed for the US Navy. The PERT method used
probabilistic time estimates [3].
In the CPM approach, a task-dependencynetwork is a
directed acyclic graph (DAG), which is developed from
two basic elements: activities (tasks) and events
(completion of tasks). An activity cannOt be started until
its tail event has been reached due to the completion of
previous activities. An event has not been reached until all
activities leading to it have been completed.
Associated with each activity is an estimated time to
completion. The critical path is the path through the DAG
that represents the longest time to project completion. To
help calculate critical path and identify critical activities a
set of parameters is established for each activity including:
duration, earliest start time, latest start time, earliest ftish
time, latest finish time, and float. Float refers to the slack
time between earliest start and latest start. All activities on
the critical path have a total float of 0. This reflects the
idea that critical-path activities must be completed on time
in order to keep the project on schedule [3].
PERT scheduling uses many of the same ideas as CPM.
However, instead of task completion being a “most likely”
estimate, it is a probabilistic estimate. The estimator
predicts an optimistic estimate, o, and a pessimistic
estimate, p, of task completion. The most likely time, m,
falls somewhere between those values. The time estimates
are assumed to follow a beta distribution. The expected
time is given as te = (o + 4m + p)/6. The expected times are
calculated for each activity in the task-dependency
network, and the critical path is then determined as in
CPM [31.

Design DecisionsImpact Development Schedule
Scheduling decisions play a critical role in reducing
cycle-time. Therefore, it is appropriate to analyze the
factors that constrain scheduling options. The architectural
design of a system dictates subsystem dependencies,
thereby dictating the possible scheduling strategies. Prior
to software design, implementation scheduling possibilities
are relatively unbounded by product decisions, i.e., all
schedulesare accessible.
Selecting a design alternative can be viewed as a
reduction in the set of accessible schedules [2]. There are
many decisions that further constrain the set of accessible
schedules.Staffmg, resource allocation, risk analysis, and
identifying high-priority functionality, all constrain the set
of accessible schedules. Therefore, making design
decisions that home in on the optimal schedule is a cycletime reduction goal.
It is unrealistic to suggest that one can always arrive at
the best possible design or schedule. Moreover, there is no
way of knowing when the best design or schedulehas been
developed. A more practical goal is to make design and

732

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29)
1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International

Shortening the Critical Path

Conference on System Sciences - 1996

increases to simulate the dependency, thereby increasing
the task completion time. This in turn increases the
likelihood of defects being introduced into the code. As
defects increase, the debugging time increases.
When resource dependencies are violated, the cost
dependsupon the type of resource. If two tasks require the
use of some limited-access hardware device, then
dependency violation might require the purchase of a
secondsuch device. The cost is monetary. If two tasks both
require some very specialized expertise that few team
members possess, then dependency violation means
training other programmers. The cost in this case is in
terms of tune spent learning and retraining.
Many factors contribute to the completion time of
programming tasks. Each programming task represents a
cycle of subactivities that includes detailed design, coding,
unit testing, integration testing, and system testing; all of
which are time-consuming. Additionally, there are
process-management and control activities involved in
each task.
One must decide whether the benefits of violating a
dependency outweigh the costs. Furthermore, it is
unreasonable to expect that all dependencies share the
same violation cost. The stronger or more complex the
dependency,the greater the cost of violation. Dependency
strength may be caused by the degree of coupling between
modules or simply by the amount of accessof the module’s
componentsby other modules.
Consider two tasks, A and B, in which B is dependent
upon the completion of A. Dependency-violation cost can
be viewed as the addition of some percentage of the
completion time of A to B’s completion tune. This
percentagereflects the amount of A that must be simulated
in order to complete task B before A is actually complete.
A violation cost approaching 100% reflects the idea that
task A is being simulated in its entirety, which defeats the
purpose of violating the dependency. Conversely, an
estimated violation cost approaching 0% reflects the notion
that B’s dependency on A is artificial and both could be
performed in parallel with little consequence.
A dependencyclassification scheme is proposed to help
determine dependencies that are good candidates for
violation. If a dependency violation cost is estimated to
range between 0% and 25%, then the dependency is
classified as a weak dependency.If the cost is in the range
26%-50%, then the dependencyis moderate. If the cost is
in the range 5 l%-75%, then the dependency is strong;
while 76%-100% violation cost would be considered very

Software development time is measured by the critical
path through the project’s task dependency network.
Hence, all cycle-time reduction efforts can be viewed as
efforts to shorten the critical path. There are basically two
ways to shorten the critical path: Shorten the completion
time of tasks on the critical path, or, remove activities from
the critical path
The first option can be achieved either by simplifying
the task or by increasing the productivity of the work team
assigned to the task. Task simplification can be
accomplished by either dividing the task into simpler
concurrent tasks or by eliminating unnecessarywork from
the task. Productivity can be increased by improving:
management techniques, resources, or development
techniques; or by allocating additional resources [4,5,6].
Although it is not a primary topic of this paper, the
maximization of productivity is fundamental to cycle-time
reduction.
Violating Task Dependencies
The second approach to eliminating critical-path
activities implies the violation of task dependencies.This
goal requires an understanding of the types of relationships
between task dependencies.Tasks can be divided into two
general subcategories: programming
tasks and
nonprogramming tasks (e.g., training, technical reviews,
etc.). Programming tasks correspond to design components
in a software system, and may be low-level modules, or the
integrations of multiple modules into subsystems.
Therefore, dependenciesbetween programming tasks are
connected to dependencies between the components in a
design. There are three types of dependencies between
tasks:
Data Dependency - If module A imports information
that module B exports, then module A is data
dependentupon module B .
Functional Dependency - If module A requires
functionality provided by module B to complete its own
function, then A is functionally dependentupon B.
Resource Dependency - If the completion of module A
requires resources that are currently allocated to
module B , then A is resource dependentupon B .
There is some cost involved in dependency violation.
Otherwise, the ideal scheduling approach would be to
complete all tasks in parallel and then integrate all at one
time. Of course, as Fred Brooks observed, project
scheduling and management is not this simple [7].
For programming tasks, the dependency violation cost
occurs in the form of scaffolding (i.e., test drivers and code
stubs) to simulate me parts of one module upon which
another depends. As data and functional dependenciesare
violated, the amount of required scaffold development

strong.

Accurately estimating the cost of violating
dependenciesis a topic for future research. However, for
programming tasks that are functionally or data dependent
upon other tasks, the cost is primarily in the creation of
code scaffolding. In this case, the estimated cost can be
733

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29)
1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996

simple.

derived from the software cost models used to estimate the
initial task durations. Resource dependencies and
nonprogramming tasks are not likely to be so simple.
From these ideas on critical path shortening, a set of
scheduling goals for the reduction of cycle-time form the
basis for the model presentedin this paper:
1. Violate low cost dependencies to increase concurrent
development whenever cost effective.
2. Transform high cost dependencies into low cost
dependencies.
3. Reduce task-completion time by simplifying the task.
4. Reduce task-completion time by dividing it into
concurrent subtasks.
by increasing
5. Reduce task-completion time
productivity.

l

additional personnel resources thereby limiting the
degree of concurrency in the schedule.
l

The previous section was devoted to project scheduling
and identifying the general strategies for shortening the
critical path in a schedule. This section examines the
impact of design decisions on scheduling outcomes.
Moreover, identifying connections between design and
scheduling provides a set of techniques for achieving the
critical-path-shortening goals previously identified.
In general a software design and its development
schedule are closely connected due to the fact that design
components dictate the work tasks in the development
schedule. It is useful to identify other, more subtle,
design-scheduleconnections.
Design dependencies determine schedule dependencies

- Any degree of coupling between two modules in the
design translates into a dependency between the
corresponding tasks in the schedule.
l

Design independence determines task concurrency

-

Modules in a design that are uncoupled can be
developed in parallel, given no other constraints and
assuming that they are not resource dependent.
*

Development learning curves affect productivity and
productivity
affects task completion time - Design

components that require programmers to learn special
skills will take longer to implement than those
components for which programmers are already
trained.
l
So&are reuse afSects task-completion time - Code
reuse is almost certainly faster than designing,
developing, and testing code from scratch.
In general, a design with complex components that are
highly dependent upon one another will result in a highly
serial schedulewith long task-completion times and excess
staffmg needs. Conversely, a design with simple,
independent components will result in a highly concurrent
schedule with short task completion times and lower
required staffing.
The schedule-improvement goals previously listed are
impacted by design decisions. Task dependencystrength is
determined by intermadular dependencies (coupling). Task
completion time is affected by maduEe complexity. Task
concurrency is affected by intramadular dependencies
(cohesion).
Clearly, the coupling and cohesion heuristics play a
role in determining the critical path length in a schedule.
Additionally, information hiding, data abstraction, data
localization, and fan-in/fan-out will affect the de
pendencies in a development schedule. This is further
evidence that existing quality metrics are coincident with
the goal of cycle-time reduction. Furthermore, these
existing heuristics can provide the necessarymechanisms
for improving the development cycle time by a judicious
refinement of the design.

The Relationship Between Scheduleand Design

l

Component complexity in design determines task
stafing - Highly complex components may require

A Cycle Time Reduction Model

Component
complexity
in
design
determines
task-completion time - Modules that are complex will

In [21 a link is made between the factors that affect

take longer to implement and test than modules that are

Architectural Design

Figure 1 - Current Design Cycle

734

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29)
1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International

development cycle time and software design decisions.
This lii combined with the link between design and
scheduling provide sufficient motivation for a design-based
cycle-time reduction model. The model proposed is based
on the convergence of the ideas presented in previous
sections. This model aims to shorten cycle time by
shortening the implementation phase through design and
schedule refinement.

Conference on System Sciences - 1996

sacrificing product quality. This model addressessome of
the problems with current iterative design models. Critical
path length is the driving metric for determining when it is
cost-effective to continue design refmement or when to
move on to development. Furthermore, because critical
path is used to reflect the improvement of each iterative
design refinement, the benefits of the iterative process are
measurable and can be compared directly to the cost of
refinement. For example, if it takes five programmer days
to conduct a single design iteration and the critical path is
shortened by only three programmer days, then it is clearly
not cost-effective to continue refining the design. In this
case, the method helps designers determine when to stop
designing. The approach taken by this model is to

Model Description
Current state of the art in software design dictates an
iterative process of design and refmement to converge on a
design of the highest quality. Figure 1 reflects this notion.
Under this model the development schedule is deferred
until design is complete. There are some drawbacks to this
design model. First, widely used design-based metrics do
not provide a mechanism for determining when it is
cost-effective to continue design refmement and when to
stop refming and move to the next phase. Second, the
current design model is not prescriptive. It is difficult to
determine which parts of the design deserve refmement at
any iteration of the cycle. Finally, although this iterative
design process is the state of the art, it is not necessarily
the state of the practice. It remains the tendency of many
designers to adopt the first design that is generated. This
may be becausethe benefits of iteratively improving design
are difficult to quantify.
The model proposed in this paper is also iterative in
nature. However, development scheduling becomes an
integral part of the design cycle. In this model the
designers develop an initial design and then iteratively
work to develop the best schedule (i.e., shortest critical
path) that can be implemented using that design. Then the
design is refined according to standard practices.
Following refinement, a new “best schedule” is developed
for the improved design and so on until the schedule does
not continue to improve. This model is graphically
representedin figure 2.
The ultimate goal of this cycle-time reduction model is
a design that improves development time without

selectively apply design-improving
techniques to key
components in the design. The aim of this approach is to

minimize the effort and maximize the benefit.

How The DesignModel Works
This design cycle follows five basic phases:
1. Initial Design - During this phase the system is
designed using existing methods and techniques. At
this point in the process, the model does not differ from
current design models. In fact, this cycle-time reduction
design model may be thought of as a design
metamodel, since it does not replace popular design
techniques and paradigms but is symbiotic with
existing methods.
2. Initial Schedule - The initial scheduling phase in this
model employs current state-of-the-art scheduling and
effort estimation techniques.
3. Schedule Refinement - The schedule-refmement phase
of this design model is the point at which the schedule
is iteratively analyzed and refined in an effort to
shorten the critical path without altering the design.
4. Problem Identification
- Now that the schedule has
been refined sufficiently, this phase serves the purpose
of identifying those system components that, if
improved, represent a significant
cycle-time
improvement.

Architectural Design

Figure 2 - Cycle Time Design Model

735

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29)
1060-3425/96 $10.00 © 1996 IEEE

~_~~~
-~~

~~~~~~~~
~~

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - I996
5. Design Refinement - During this phase efforts are made

to improve those tasks that were identified as
problematic by the previous phase.
This cycle is iterated using the critical path length as a
cycle-time improvement metric. For each iteration of the
design cycle, the critical path will reflect the benefit of the
refinement efforts. Ideally, the architectural design cycle
should halt as soon as the critical path becomes stable or
when the benefits at each iteration do not justify the effort
We do not propose to replace existing state-of-the-art
design and scheduling methods in this model. Instead, the
remainder of this section will focus on the development of
methods for accomplishing steps 3,4 and 5.
Schedule Refinement
The third phase in this model deserves closer
inspection. Ideally, this phase will reveal the optimal
schedule for the current version of the design. It has been
observed that the task dependenciesalong the critical path
in a schedule can, potentially, be violated. A brief foray
into graph theory will motivate an algorithmic approach to
the problem of schedule optimization.
The scheduling problem can be temporarily simplified
by stating it as a graph-manipulation problem restricted by
a set of rules. Given a directed acyclic graph (DAG) in
which each vertex v has a weight that is represented by
wt(v), the weight of the entire graph W is:
/ Vi

W = X wt(vi) ;3cVvi {c is a critical path, vi is a vertex
is on C}
The burden of a vertex v, b(v) is the sum of the

weights of all immediate predecessorsof v, which are on a
critical path. The prehistory of v, ph(v) is the sum of the
weights of all predecessorsof v, which are on the longest
serial path from the initial vertices to v. These are likely to
be the predecessorsof v that are on a critical path. The
object is to reposition vertices in the graph in order to
achieve the smallest weight (i.e., shortest critical path).
However, there are rules for moving vertices:
1. Never disconnect the goal vertex from its predecessors.
This is equivalent to eliminating subsystemsfrom the
f& integration.
2. Vertices are repositioned by disconnecting them from
their critical-path predecessors.This reflects the notion
of violating task dependencies.
3. Whenever a vertex v is disconnected from its critical
path predecessors,these predecessorsare reconnected
to all of v’s immediate successors.This is to prevent the
graph from becoming disconnected.
4. Whenever a vertex v is disconnected from its
critical-path predecessors,its weight is adjusted by the
expression: wt(v) = wt(v) + (m x b(v)). In this
expression m representsthe percentageof v’s dependent
tasks that must be simulated in order to develop and

736

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29)
1060-3425/96 $10.00 © 1996 IEEE

test v (i.e., scaffolding). This reflects the cost of
violating task dependencies.
Some observations can be made from this view of
schedule optimization. Fist, it is not likely to be beneficial
to reposition slack path vertices. Second, a path that is
critical at one point in the optimization process may cease
to be critical at some future time. Third, there may be
multiple critical paths in the graph at any given time.
These conditions and others that are not so obvious must
be addressedby any strategy that is to be used to solve this
problem.
An algorithmic strategy might implement a cycle of
vertex repositioning followed by recalculating critical-path
length. Such an algorithm must be able to determine which
vertex to reposition at each iteration and when to stop.
The selection of a vertex for repositioning must be
conducted in light of a cost-benefit analysis. The benefit is
measured as a reduction in W. The cost is represented by
the increase in wt(v) due to dependency violation. The
vertex that produces the greatest benefit and least cost is
the prime candidate.
Some additional observations can be made about the
structure of critical paths in this problem space. It is
possible that the critical path in a DAG of significant size
will not be a single linear sequenceof vertices and edges.
Multiple parallel critical paths may require repositioning
multiple vertices before seeing a reduction in W.
Furthermore, if the critical path diverges at one task and
then reconverges later (i.e., becomes temporarily parallel),
vertices prior to this divergence or following the
reconvergenceshould be manipulated if possible in order
to realize immediate benefits.
The trouble with each of these scenarios is that they
obscure the benefits of repositioning certain candidate
vertices. The value of W may remain steady over several
iterations before it begins to decrease.Rather than halting
when no further decreasein critical path length is seen, it
may be more beneficial to halt when an increase is
detected.
These observationsmotivate a simple greedy algorithm
that iteratively repositions critical path vertices based on a
cost-benefit analysis and recalculates the value of W,
halting when the value of W increases. This algorithm
always identifies the vertex that appears to promise the
greatest immediate reduction in W. Greedy algorithms are
often used in optimizing problems such as this. However,
most greedy algorithms accept a “good” solution rather
than guaranteeing an optimal solution. This algorithm is
no exception. The problem of finding an optimal solution
lies in the probability of reaching a local minimum It is
possible that the repositioning of a vertex will temporarily
increase W in order to realize a greater decreaseon future
iterations. Unfortunately, guaranteeing an optimal solution
to graph-shortening for a graph of any complexity is a hard

Proceedings of the 29th Annual Hawaii International

problem which cannot be solved in polynomial time and is
NP-complete [8,9, lo].

Conference on System Sciences -

1996

conditions and its use may result in a loss of accuracy. The
remainder of the discussion of this model will use .25 as a
value for m. However, until an empirically established
upper bound value is established one may prefer to use
actual violation cost estimates.

Calculating Dependency Violation Cost
In determining the net benefit of dependency violation,
m represents the percentage of simulation of the tasks on

An Algorithm for Schedule Refinement

which another task relies. This requires that up to 100% of
the fast task must be simulated in order to complete the
second task. Therefore, the cost of violating a dependency
lies between 0% and 100% of the duration of the first task.
Hence, for a single dependency the value of m is in the
range [O.O,l.O]. The value of m increases as the number of
tasks on which the target task is dependent increases and
may exceed 1.O.
Furthermore, whenever a task dependency is eliminated
there are likely to be hidden costs due to added
communication requirements, inaccuracies in estimating,
etc. As m approaches 1 .O the likelihood increases that these
hidden costs will cause the dependency violation cost to
exceed the benefit. Therefore, it is useful to establish a
maximum value for m above which a dependency should
not be violated.
Empirically defining an upper bound value for m is
extremely difficult if not impossible. However, during the
development of the model presented here, fifty different
schedules and scheduling scenarios were examined. The
purpose of these examinations was to identify the behaviors
and consequences of task dependency violation. These
efforts led to the observation that .25 appears to be a
conservatively reasonableupper boundary value for m [ 11.
It was consistently observed that whenever the
estimated cost of violating a dependency was below 50%,
the violation resulted in a decreasein critical path length.
In fact, many critical path decreaseswere observed for cost
estimates as high as 80%. However, providing for hidden
costs, it is deemed prudent to take a highly conservative
approach to selecting an upper bound value for m. It was
observed during these exercises that 100% of the cases in
which the value for m was below 30% resulted in a
shortening of the critical path. Compensating for hidden
costs, this model uses an upper bound of 25% for m. This
choice has the added benefit of allowing the cycle-time
reduction model to focus only on weak dependencies as
candidates for violation. Assuming inaccuracies in the
estimates of dependency violation cost, .25 represents a
worst-case value within that range. Fixing m at .25
simplifies the algorithm since categorizing dependency
strength is easier than accurately estimating the cost of
dependencyviolation.
This use of .25 as a fixed value for m in the cycle-time
reduction model can easily be replaced by a more empirical
value or by the actual violation cost estimate. The selection
of a fixed value serves primarily to simplify the use of the
model. This value was selected under ultraconservative

The graph theory concepts of the previous section form
the basis for a schedule refmement algorithm that attempts
to reposition critical path tasks. Both CPM and PERT
represent a project schedule as a DAG in which vertices
represent development activities and milestones, while
edges represent dependenciesbetween tasks. The duration
of each task corresponds to the weight of a vertex, wt(v),
and the critical path duration corresponds to W. The
prehistory of a task t, ph(t), is the sum of the durations of
all tasks preceding t along the critical path. The burden of
a task t, b(t), is the sum of the durations of all tasks that
immediately precede t.
The process of refining the schedule is, in essence, a
process of selectively violating task dependencies in a
cost-effective manner. It is assumed that the schedule
follows a typical CPM or PERT format, in which the
information for each task includes: earliest start date (ES),
latest start date (LS), earliest finish date (El?), latest ftish
date (LF), and duration (D). Furthermore, it is assumed
that the dependencies between tasks in a schedule have
been categorized using a scheme similar to the one
previously described. The following strategy takes the
conservative approach of violating only weak dependencies
and assumes the worst case within that range (i.e., m =
.25). Given more information, the procedure can be
modified as is necessary.
The algorithm is as follows:
1. Build a set D of weak dependencieson the critical path.
These are the primary candidates for violation.
2. For each dependency i-+j E D, where task i is
dependent on task j, calculate ph(i) and b(i). Remove
from D any dependencies whose cost outweighs i’s
prehistory, wt(i) + ph(i) 5 wt(i) + .25b(i).
3. For each dependency i+j E D, calculate the net
benefit, n(i) = wt(i) - .25b(i), of removing the
dependent task from the critical path and placing it in a
slack path. Remove any dependenciesfrom D that have
no net benefit, n(i) < 0. D now contains the fmal set of
candidates for elimination.
4. Select the dependency i+j E D, for which n(i) is the
greatest, remove the dependent task from the critical
path, and replace it on a slack path using the following
rules:
l
If i has no successorsand j becomes disconnected
from all successors, then do not violate this
dependency since j will never become integrated

737

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29)
1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International

Conference on System Sciences - 1996

2. Identify dependenciesthat, if violated, would produce
the greatestreduction in the critical-path length.
3. For the identified tasks, examine the potential for either
simplifying or decomposing their corresponding design
components and estimate the effort required to do so.
Retain those that have the greatest benefit for the least
effort4. For the identified dependencies, evaluate the effort
required to weaken the dependency and evaluate the
potential for success.Retain those that have the greatest
benefit for the least effort.
5. From the retained tasks and dependencies, select the
one that has the greatest overall net benefit in terms of
critical-path shortening.
6. Ideally, one task or dependencywill he refined on each
design-cycle iteration. However, it may be that by
performing simple refinements to several tasks and/or
dependencies,a major shortening of the critical path
will be experienced on one iteration. These decisions
must be determined based on individual project
scenarios.
Problem identification relies upon the talents of
software engineers to make good decisions. These
guidelines may easily be supplemented with additional,
project-specific information to increase their benefit.
Future work in validating cost and benefit estimating
techniques wilI further strengthen this phase in the model.

into the system without an additional integration
step. In this case, i is the final system integration.
This is not likely to be cost-effective.
l
Connect j to all immediate successorsof i and
update i’s ES, LS, EF, and LF values. This reflects
the change in the integration strategy for i andi (i
can now be developed concurrently with /I.
* Update i’s duration by a factor of .25, D = .2.50
l
All additional constraints that affect the
development schedule should be maintained (e.g.,
resource allocation, risk prioritization, etc.). It is
impossible to define rules for maintaining these
constraints. Therefore, it is the responsibility of
the scheduler to ensure that they are not violated.
5. Becausea new critical path may emerge as a result of
task repositioning, steps l-4 are repeated until the
schedule reaches a stable state (i.e., no changes can be
made to the schedule) or until the critical path begins to
increase.
At best, the result of this algorithm is an optimal
schedule for the given design under the given constraints.
At the least, the resulting schedule will not be any worse
than the original schedule.
There are additional
scheduling-improvement
techniques that have not been addressed here, such as
resource-leveling, which may help to shorten the critical
path [3]. Used in conjunction with such techniques, the
model describedhere offers a simple yet powerful means of
reducing development time. It is based on observation,
intuition, and the mathematical manipulation of task
networks and CPM components. A major benefit to this
scheduling method is that it is highly automatable once
tasks have been identified and a preliminary schedule is
developed.

Design Refinement
After identifying the most beneficial task or
dependency for refinement, it is necessary to quickly
identify the cause of the problem and resolve it. If the
target of refinement is a dependency, then the aim is to
weaken that dependency to make its dependent task a
candidate for repositioning. Assuming the dependency is
not resource dependent, it may be that efforts should be
made to decreasecoupling between corresponding design
components using existing design heuristics and
principles.
If the target of the refmement effort is task duration,
reducing the complexity of corresponding components
might be accomplished through simplification or
subdivision. Subdivision implies that the module has poor
cohesion. Ideally, system modules should be functionally
cohesive. Therefore, the designer should work to reduce
cohesion and divide the system component into multiple,
independentsubcomponents.

Problem Identification
Once the development schedule has been refined there
are two ways to further shorten the critical path: either
remove tasks from the critical path, or shorten the
completion time of tasks on the critical path. The only
remaining critical tasks are those with high
dependency-violation costs or weak dependencies for
which there is no net benefit in removing them from the
critical path.
This phase in the model attempts to shorten the
duration of a target task; or to prescribe design refinements
that will result in weaker dependencies. The approach
taken is to identify the design component that, if refined
will result in the greatestcycle-time benefit.
This approach is heuristic rather than algorithmic.
General guidelines are as follows:
1. Identify tasks in the schedule that have the greatest
impact on the critical path.

Preliminary Results
This model cannot guarantee a reduction in actual
cycle-time since many things can happen between software
design and product release. However, preliminary results
suggest that the model has promise in achieving the
738

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29)
1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International

Conference on System Sciences - 1996

1
Actual Size
Actual Effort
DPS
.........................
..........................................................................
.....................................................................
............................................................
j
2
Actual Size
Actual Effort
Educated
:;
.........................
..........................................................................
....................................................................
.............................................................
3
Actual Size
COCOMO
DPS
4
Actual Size
COCOMO
Educated
I
I
I
5
Actual Size
Putnam
DPS
.........................
..........................................................................
............................................................................................................................
i
6
Actual Size
Putnam
Educated
.........................
..........................................................................
............................................................................................................................
;
7
Expected Size
COCOMO
DPS
8
Expected Size
COCOMO
Educated
j
I
f
9
Expected
Size
.........................
............................
............................................
......................................................................................
;
10
Expected
Size
I.........................
I...........................
.............................................
I....................................................................
Table 1: Demonstration

following results:
1. Significantly reduce the estimated development time of
a software product at the design phase. For the
purposes of this effort, any reduction of estimated
development time of 5% or more will be considered
significant.
2. Maintain or improve the quality of the initial design
(i.e., the model will not reduce design quality).
3. Help to focus design-refinement efforts on beneficial
design components.
Although empirical validation of this model is virtually
impossible, demonstration of the model on a variety of
software designs under a variety of conditions yields
encouraging results. The model was applied to live
software systems ranging in size from 736 source lines of
code (SLOC) to 6,309 SLOC, and ranging in quality.
Three factors were identified as affecting the initial
schedule of a particular software design: size estimation
technique; effort estimation technique; and scheduling
technique. For size estimation in this analysis actual size
and expected size were used. For effort estimation, actual
efforts were used in addition to the COCOMO and Putnam
models [ll, 121. In developing initial schedules, two
approaches were used: dependency preserving scheduling
(DPS) and educated scheduling. DPS refers to preserving
all intermodular dependencies from the design on the
corresponding development tasks. Educated scheduling
refers to more common scheduling approaches in which
task dependencies are determined by functionality, risk,
resources,etc. Table 1 shows the variety of combinations of
size estimation, effort estimation, and scheduling

Strategies

approach. By combining each of these factors with each of the
five software designs, forty-two different design-scheduling
scenarios were used to observe the effects of the model (in
some cases,certain combinations were infeasible). Table 2
shows the statistical results of these 42 demonstrations.
The data is fairly scattered. However, in all but one of the
demonstration cases, a significant (i.e., 5% or greater)
reduction in estimated development time was observed.
It is notable that the design refinements in each of the
demonstration scenarios contributed as much or more to
improving quality as to improving estimated cycle-time.
The quality of each of the poor quality designs improved,
while the high quality designs did not experience any loss
in quality. The details of this demonstration of
effectiveness can be found in [ 11. It should be noted that
the demonstration scenarios are all small programs, and it
remains to be seen how well this model scales to largescale software systems.

Summary and Conclusions
Current design cycles do not provide mechanisms for
determining on which parts of the system to focus
refmement efforts, or for determining when to stop
iterating. This model provides a solution to both problems.
Through a schedule analysis, the model guides the
designer to focus refinement efforts on the most beneficial
system components. By using the critical path as a metric
of cycle-time improvement, this model helps determine
when to halt the design cycle. Additionally, this model

Table 2: Results of 42 Model Demonstrations

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29)
1060-3425/96 $10.00 © 1996 IEEE

Proceedings

of the 29th Annual Hawaii

Bibliography

provides a mechanism for the designer to determine which
techniques to apply in order to resolve targeted problems.
In this manner, the cycle-time reduction model helps the
designer selectively apply design-improving techniques to
key components in the system in order to cost-effectively
improve the development cycle time. Applying Laws of
Pareto to cycle time, 20% of the software design will
account for 80% of the cycle time. The goal of this model
is to help software engineers focus their energy on the key
20% for improvement by identifying system components
that most significantly impact the critical path length.
There are a number of other benefits to this cycle time
reduction model:
l
It is not a replacement methodology. This design
approach allows software engineers to continue using
state-of-the-art design and scheduling methods.
e

This method provides

l

Using the critical path as a metric, the impact of design
refinements is clear.
This method combines established practices. Popular
scheduling methods, and
design heuristics,
cost-estimation techniques are supported in this model.

0

design-improvement

111 Collier, K.W. “A Design-based Model for the Reduction of
Software Cycle Time”,
University, 1993.

Annroach, McGraw-Hill

l

New York 1983.

[41 Bisant, D.B. and J.R. Lyle, “A Two-Person Inspection
Method to Improve Programming Productivity”,
IEEE
Transactions on Sofhyare Engineering vol. 15 no. 10, 1989,
pp. 1294-1304.
Boehm, B., M.H. Penedo, D.E. Stuckle, R.D. Williams and
A.B Pyster, “A Software Development Environment for
Improving Productivity”, Computer, vol. 17 no. 6, 1984, pp.
3042.

feedback.

Dart, S.A., R.J. Ellison and P.H. Feiler, “Software
Development Environments”, Computer, vol. 20 no. 11,
1987, pp. 18-28.
I71 Brooks, F.P. The Mvthical Man-Month: Essavs on Software
Enaineering, Addison-Wesley, Massachusetts, 1982.

@I Boctor, F.F. “Some Efficient Multi-heuristic

Procedures for
Resource Constrained Project Scheduling”,
European
Journal of Operational Research, vol. 49, 1990, pp. 3-13.

The method provides a metric for measuring cycle-time
reduction. Critical path length offers a quantitative

resource

State

[31 Dieter, G.E. Engineering Design: A Materials and Process

This method provides motivation for iterative design
refinement. Currently, it is unclear how much benefit is

The method allows for fixed

Arizona

Time Reduction”,
Proceedings: International
Phoenix
Conference on Computers and Communications, March 2831,1995, pp. 302-309.

PI Khattab, M.M. and F. Choobineh, “A New Approach for

measure of the effects of the model on estimated
cycle-time reduction.
l

Ph.D. Dissertation,

121 Collier, K.W. and J.S. Collofello, “Issues in Software Cycle

gained by placing energy into design iteration. This
method provides this information in the form of units
of time saved.
l

Conference on System Sciences - 1996

International

Project Scheduling With a Limited Resource”, International
Journal of Production Research, vol. 29 no. 1, 1991, pp.
185-198.

allocations.

Resource constraints that limit scheduling flexibility
are accommodated by this model. Furthermore, if
resources are not fmed, this model provides useful
information for determining resource requirements.

[lo] Khattab, M.M. and F. Choobineh, “A New Heuristic for
Project Scheduling With a Single Resource Constraint”,
Computers and Industrial Engineering, vol. 20 no. 3, 1991,
pp. 381-387.

Unrealistic schedules are identified early by this
model. As the designs are refined for shorter cycle

[ll]

times, the feasibility of initial scheduling estimates
becomes apparent allowing for contingency plming
and deadline renegotiation early in the product life
cycle.
Due to the small size of the software designs that were
used to evaluate the model, the usefulness of the model on
medium or large software designs is unclear. Further
empirical evidence is required to make any conclusive
statements about the model’s effectiveness. However,
initial results are encouraging. This design-based cycletime reduction model can only improve as cost/effort
estimation models become more accurate. Furthermore,
this model is easily adaptable to organizational
idiosyncrasies and to changing technologies,

Boehm, B. “Software Engineering Economics”, IEEE
Transactions on Sofiare
Engineering vol. SE-10 no. 1,
1984, pp. 4-21.

[12] Putnam, L.H. and W. Myers, Measures for Excellence:
Reliable Software on Time, Within Budget, Yourdon Press,
New Jersey, 1992.
[13] Yourdon, E. Mana&c!
Hall, New Jersey,

740

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29)
1060-3425/96 $10.00 © 1996 IEEE

1989.

the Structured Techniques, Prentice-

Using Simulation to Facilitate the Study of Software Product Line Evolution1
Yu Chen†, Gerald C. Gannod‡2, James S. Collofello†, and Hessam S. Sarjoughian†
†
‡
Dept. of Computer Science and Engineering
Division of Computing Studies
Arizona State University – Tempe Campus
Arizona State University – Polytechnic Campus
Tempe AZ 85287, USA
Mesa AZ 85212, USA
{yu_chen, gannod, collofello, sarjoughian}@asu.edu
Abstract
A product line approach is a disciplined methodology
for strategic reuse of source code, requirement
specifications, software architectures, design models,
components, test cases, and the processes for using the
aforementioned artifacts. Software process simulation
modeling is a valuable tool for enabling decision making
for a wide variety of purposes, ranging from adoption
and strategic management to process improvement and
planning. In this paper, discrete event simulation is used
to provide a framework for the simulation of software
product line engineering.
We have created an
environment that facilitates strategic management and
long-term forecasting with respect to software product
line development and evolution.

1. Introduction
A software product line is defined as a set of softwareintensive systems sharing a common, managed set of
features that satisfy the specific needs of a particular
market segment or mission and are developed from a
common set of core assets in a prescribed way [8]. A
product line approach is a disciplined methodology for
strategic reuse of source code, requirement specifications,
software architectures, design models, components, test
cases, and the processes for using the aforementioned
artifacts. Software product line engineering promises
large-scale productivity gains, shorter time-to-market,
higher product quality, increased customer satisfaction,
decreased development and maintenance cost [8].
However, those benefits are not guaranteed under all
situations, and they are affected by many factors such as
the initiation situation, the adoption and evolution
approaches, the market demands, and the available
resources. The goal of this research is to develop a
1
2

simulator that facilitates software product line decision
making at an early stage by providing time and cost
estimates under various situations.
In this paper, discrete event simulation theory and
Constructive
Product
Line
Investment
Model
(COPLIMO) [2] are used to create an environment that
facilitates strategic management and long-term
forecasting with respect to software product line
development and evolution. Specifically, the simulator
facilitates the study of the effect of a number of process
decisions, including choice of evolution approach, upon
factors such as effort and time-to-market. The simulator
not only gives statistical results at the end of the
simulation, but also visually presents how major product
line engineering activities progress and interact over
time. The simulator is built upon DEVSJAVA [1], a
general-purpose Java-based discrete event simulation
framework. The tool is extensible and allows other
simulation frameworks and cost models to be used.
The remainder of this paper is organized as follows.
Section 2 presents background information. Section 3
describes the simulation model and the simulator.
Results are discussed in Section 4. Section 5 contains
related work and Section 6 draws conclusions and
suggests future investigations.

2. Background
This section describes background information on
Software Product Lines, software process simulation,
DEVSJAVA [1], and COPLIMO [2].

2.1. Software product lines
Software product line development involves three
essential activities: core asset development, product
development, and management [8].
Core asset

This material is based upon work supported by the National Science Foundation under grant No. CCR-0133956.
Contact author.

development (domain engineering) involves the creation
of common assets and the evolution of the assets in
response to product feedback, new market needs, etc.
Product development (application engineering) creates
individual products by reusing the common assets, gives
feedback to core asset development, and evolves the
products.
Management includes technical and
organizational
management,
where
technical
management is responsible for requirement control and
the coordination between core asset and product
development activities.
There are two main software product line adoption
approaches: big bang and incremental [10]. With the big
bang approach, core assets are developed for a whole
range of products prior to the creation of any individual
product. With the incremental approach, core assets are
incrementally developed to support the next few
upcoming products. In general, the big bang approach
has a higher return on investment but involves more
risks, while the incremental approach has lower entry
costs but higher total costs. The four common software
product line adoption situations are: independent,
project-integration, reengineering-driven, and leveraged
[10]. Under the independent situation, a product line is
created without any pre-existing products. Under the
project-integration situation, a product line is created to
support both existing and future products. Under a
reengineering-driven scenario, a product line is created
by reengineering existing legacy systems. And the
leveraged situation is where a new product line is created
based on some existing product lines.
Some common product line evolution strategies are:
infrastructure-based, branch-and-unite, and bulkintegration [10]. The infrastructure-based strategy does
not allow deviation between the core assets and the
individual products, and requires that new common
features be first implemented into the core assets and
then built into products. Both the branch-and-unite and
the bulk-integration strategies allow temporal deviation
between the core assets and the individual products. The
branch-and-unite strategy requires that the new common
features be reintegrated into the core assets immediately
after the release of the new product, while the bulkintegration strategy allows the new common features to
be reintegrated after the release of a group of products.

2.2. Simulation
A software process is a set of activities, methods,
practices, and transformations that people use to develop
and maintain software and associated products, such as
project plans, design documentations, code, test cases,
and user manuals [4]. Adopting a new software process is

expensive and risky, so software process simulation
modeling is often used to reduce the uncertainty and
predict the impact.
Software process simulation
modeling can be used for various purposes and scopes,
and have been supported by many technologies [3]. The
software product line process simulator described in this
paper is for long-term organization strategic
management, and is implemented in DEVSJAVA [1], a
Java implementation of the Discrete Event System
Specification (DEVS) modeling formalism [1].
The external view of a DEVSJAVA model is a black
box with input and output ports. A model receives
messages through its input ports and sends out messages
via its output ports. Ports and messages are the means
and the only means by which a model can communicate
with the external world. A DEVSJAVA model is either
“atomic” or “coupled”. An atomic model is undividable
and generally used to build coupled models. A coupled
model consists of input and output ports, a finite number
of (atomic or coupled) models, and couplings. The
couplings link model ports together and are essentially
message channels. They also provide a simple way to
construct hierarchical models.
To execute atomic and coupled models, DEVSJAVA
uses distinct atomic and coupled simulators that support
incremental simulation model development. These
simulators can execute in alternative settings (i.e.,
sequential, parallel, or distributed). An important feature
of the DEVS framework is the ability for models to
seamlessly execute either in logical or (near) real-time.
Furthermore, due to its availability of its source code and
object-oriented design, DEVSJAVA can be extended to
incorporate domain-specific (e.g., Software Product Line)
logic and semantics.

2.3. COPLIMO
In the simulator, COMPLIMO [2] is used as the cost
model to provide cost estimates. COPLIMO is a
COCOMO II [9] based model for software product line
cost estimation, and has a basic life cycle model and an
extended life cycle model. The basic life cycle model has
two sub-models: a development model for product line
creation and a post-development model for product line
evolution. Although the basic life cycle model has many
simplification assumptions, it is thought to be good
enough for early stage product line trade-off
considerations [2]. The basic model also can be easily
extended to the extended life cycle model, which allows
products have different parameter values instead of the
same values. In the implementation, the cost model is
designed as a plug-in model, thus other cost models can
be plugged in to meet other needs.

3. Approach
A simulation framework and a software cost model
were used to develop the simulator.
Although
DEVSJAVA [1] and COMPLIMO [2] are currently used,
they can be replaced by other suitable simulation
frameworks and cost models. This section presents the
abstract software product line engineering model, the
specifics of the simulation models, the assumptions, and
the simulation tool.

products are developed by reusing the existing core
assets, and existing products are updated after the change
of the core assets. Figure 3.1 depicts the process flow.
Costs associated with this approach include core asset
development costs, new product development costs, and
existing product maintenance costs. Compared to the big
bang approach, the incremental approach has higher
product development costs because of the incompleteness
of the core assets, and extra product maintenance costs as
the result of a short-term planning penalty.

3.1. Abstract product line model

Figure 3.2. SPL evolution approaches
Figure 3.1. SPL adoption approaches
Software product line engineering typically involves a
creation phase and an evolution phase [10]. Currently
the simulator provides two options (big bang and
incremental) for the creation stage and two options
(infrastructure-based and branch-and-unite) for the
evolution stage. In the following, we will discuss the
costs associated with those cases in detail.
With the big bang adoption approach, core assets are
first developed to meet the requirements for a whole
range of products. Products are then developed by
reusing the core assets [10]. Figure 3.1 illustrates the
process flow. Costs associated with this approach
include core asset development costs and new product
development costs.
With the incremental adoption approach, the core
assets are incrementally developed to meet the
requirements of the next few upcoming products, new

With the infrastructure-based product line evolution
strategy, the process for building a new product is the
following: core assets are updated by incorporating new
common requirements, and the new product is developed
and existing products are updated. Figure 3.2 shows the
process flow. The COPLIMO [2] basic life cycle model
assumes that a change to a product causes the same
percentage of change on reused code, adapted code, and
product unique code. So if the change rate caused by
new product requirements is α, then the costs for one
product development iteration include the costs of
maintaining the core assets with a change rate of α, the
costs of developing the new product, and the costs of
maintaining existing products with a change rate of α.
With the branch-and-unite product line evolution
strategy, the process for building a new product is the
following: the new product is developed, core assets are
updated to incorporate new common features, and
existing products are updated (including the newly

created product). Figure 3.2 illustrates the process flow.
If α is the change rate caused by new product
requirements, then the costs for one product development
iteration include the costs of developing a new product
with (1-α) percentage of the reuse rate, the costs of
maintaining the core assets with a change rate of α, and
the costs of maintaining existing products (including the
newly created one) with a change rate of α. Product
maintenance costs are higher in this case because it has
one more product to update, the newly created one. The
new product is first created with new features that are not
supported by the core assets, then after the core assets are
updated to incorporate the new features the new product
needs to be updated to keep consistent with the core
assets. The new product development costs are also
higher with this approach, because of the lower reuse
rate.

project cannot be started until the requested resources are
granted from the Employee Pool. If the number of
employees in the employee pool is not less than the
requested number of employees, the requested amount of
employees will be granted. Otherwise, if the number of
available employees meets the minimum employee level
(a model parameter, between 5/8 and 1), then the number
of available employees will be granted. In that case, a job
can be started with fewer resources but longer
development time. In other cases, the employee pool will
not grant any resources until enough resources are
returned.

3.2. Model development
Twelve DEVSJAVA [1] models were developed to
model software product line engineering activities.
Figure 3.3 shows a UML diagram depicting the
hierarchical structure of the model.
Some time
constraints are imposed in the simulator: for each atomic
model, jobs are processed one by one in FIFO order (or in
combination with some priority).
The PLPEF (Product Line Process Experimental
Frame) is the top level coupled model and contains a
Product Line Process instance and an Experimental
Frame instance.
The Product Line Process models software product
line engineering activities. It contains an instance of
Technical Management, Core Asset Development, and
Employee Pool, and a finite number of Product
Development instances.
The number of Product
Development models to be included depends on the
number of projected products in the product line. The
Product Line Process receives market demands and
dispatches them to Technical Management. It sends out
requirements (generated by Technical Management and
Maintenance Requirement Generator) and development
reports (generated by Core Asset Development and
Product Development), which can be used for process
monitoring.
The Employee Pool models human resource
management. It receives resource request and resource
return messages, and sends out reply messages to grant
resources.
Currently, Employee Pool manages the
resource requests in either a pure FIFO manner or a
FIFO manner where new development jobs are given
higher priority. Before starting any development activity,
a resource request must be sent to Employee Pool. A

Figure 3.3. Model hierarchical structure
The Product Development models the application
engineering activity. It has a Development instance for
product creation and inter-product synchronization
(development instance), a Development instance for
inner-product maintenance (maintenance instance), and a
Maintenance Requirement Generator instance. When the
Product Development gets the first requirement, the
development instance starts product creation, once that is
done, the Maintenance Requirement Generator sends out
a requirement to maintain the product for N years (the
number of years in the product life cycle), which starts
the maintenance instance.
After N years, the
Maintenance Requirement Generator sends out a stop
message, which stops the maintenance activity and the
acceptance of new development requirements.
The Development models a general software
engineering activity. When a new requirement is
received, Development sends a resource request to the
Employee Pool, waits for the reply from the Employee
Pool, starts developing activity when the resources are
granted, then returns resources to the Employee Pool and
sends a report to Technical Management upon
completion. The Development model will stop accepting
new requirements when it receives a stop message on its
stop port, which means the product reaches the end of its
life cycle and needs to be phased out.
The Maintenance Requirement Generator models
product maintenance requirement generation. Once a
new product is released, it sends out a requirement to
maintain the product for N years (the number of years in

the product life cycle), and sends a stop message when
the product reaches the end of the product life cycle.
The Core Asset Development models domainengineering processes. Currently, it is modeled in the
same way as the Development model. The domain
engineering is not modeled as the same as the application
engineering, because in practice technical management
often collects the core asset feedback from product
development and issues the core asset requirements in the
context of product development.
Table 3.1. Behavior of technical management

Stage
Creation

Approach
Big bang

Incremental

Evolution

Infrastructure
-Based

Branch-andUnite

Activities
1. Create core assets if they
do not exist
2. Create new product by
fully reusing core assets
1. Increase core assets if
necessary
2. Create new product by
fully reusing core assets
3. Update existing products
1. Update core assets
2. Create new product by
fully reusing core assets
and updating existing
products (excluding the
newly created product)
1. Create new product by
partially reusing core
assets
2. Update core assets
3. Update existing products
(including newly created
product)

The Technical Management models requirement
generation and control as well as the coordination
between core asset and product development. It receives
market demands (which are processed in FIFO order),
generates requirements for core asset or product
development, and accepts development reports. Which
requirements will be generated, when will they be
generated, and where they will be sent depend on the
selected product line adoption and evolution approaches.
Technical Management coordinates core asset
development and product development activities through
keeping the requirement generation in a certain order.
Table 3.1 summaries the behavior of Technical
Management according to the given strategies.
The Experimental Frame consists of a Market
Demand instance and a Transducer instance. It feeds
Product Line Process with inputs and receives Product
Line Process’ outputs.

The Market Demand models the demands for new
products from the market. It sends out a new product
request after a certain interval, which can be set through
the model parameter, “interval”.
The Transducer observes product line engineering
activities for a certain amount of time. During the
observation period, it receives development requirements
and reports, and tracks the generating and finishing time
of each requirement. At the end of a simulation run it
will output some statistical information to a file.

3.3. Assumptions
In the simulator, we made a number of assumptions as
follows.
1. All the employees have the same capability and can
work on any project.
2. If task B needs to make use of the results from task
A, task B cannot start until task A is finished.
3. Product maintenance starts right after the release of
the product and the maintenance activity holds the
staff until the product is phased out.
The assumptions made by the COPLIMO [2] basic
life cycle model are:
4. All products in the product line have the same size,
the same fractions of reused (black-box reuse) code,
adapted (white-box reuse) code, and product unique
code, and the same values for cost drivers and effort
modifiers.
5. For each product in the product line, the size, the
values of cost drivers, and the values of effort
modifiers remain constant over time. For each
product, the fractions of reused code, the fractions of
adapted code, the fractions of product unique code
remain constant during the time when core assets
stay the same.
6. A change to a product will cause the same
percentage of change to reused code, adapted code,
and product unique code.
Assumption 2 states that concurrency between
interdependent tasks is not supported in the current
version, which will be supported to some extent in the
future. Assumption 4 can be relaxed by using COPLIMO
[2] extended life cycle model, which allows products to
have different parameter values. Assumption 5 can be
relaxed by allowing users to specify the change trend.
Assumption 6 can also be relaxed by allowing users to
specify the change rate on different portions of the
products. Because COPLIMO is currently used as the
underlying cost model, its assumptions are adopted in the
simulator.
If another cost model is used, these
assumptions would be replaced by those made by the new
cost model.

Figure 3.4 Simulation tool in execution

3.4. Simulation tool
The simulation tool was developed in Java and runs in
the DEVSJAVA [1] environment. Figure 3.4 shows the
user interface. The upper part of the interface shows the
current running model and its package, which are
“PLPEF” and “productLineProcess”, respectively. The
middle part shows the models and their hierarchical
relationships.
The bottom part contains execution
control components. The “step” button allows running
the simulator step by step, the “run” button allows
executing the simulator to the end, and the “restart”
button allows starting a new simulation run without
quitting the system. The “clock” label displays the
current simulation time in the unit of months, and
selecting the “always show couplings” checkbox will
allow couplings between models to be displayed. The
simulation speed can be manipulated at run time to allow
execution in near real-time or logical time (slower/faster
than real-time).

Figure 3.4 shows that at time 27.783, Core Asset
Development is idle, Products 1 is under initial
development, Product 2 and 3 are waiting for resources,
and Products 4 and 5 are in planning. The messages tell
that Product 2 just received requested resources and
Product 3 just sent out a resource request. Because of the
lack of resources, the Employee Pool cannot grant the
requested resources and is waiting for more resources to
be returned, which in turn puts Product 3 in wait.
Technical Management is idle, Market Demand
generates new product requirement in every 12 months.
Finally, and Transducer is observing the simulation.
This case shows a situation where limited resources cause
development activity delay.
At the end of each simulation run, a result table is
generated similar to Table 3.2. The table has two
sections that are divided by a blank line. The top section
lists the created products, their first release time (FR),
time-to-market (TTM), initial development effort (DPM),
initial development time (DT), accumulated development
and
maintenance
effort
(APM),
accumulated

TPM
2810.1
FT
220.1
APM
153.2
TR
20
ATTM
44.9

4. Results
In this section some simulation cases are presented to
illustrate the use of the simulator and to demonstrate the
analytical capability of the simulator.

4.1. Overview
The inputs to the simulator include general
parameters and product (core asset) parameters. The
general parameters are used to describe software product
line process attributes and organization characteristics.
These parameters include the maximum number of
products that will be supported by the product line, the
number of products that will be created during the
creation stage, the product line adoption and evolution
approaches, the number of employees in an organization,
and the market demand intervals. The product (core
asset) parameters are primarily determined by the
employed cost model (COPLIMO, in this case). These
parameters include the size of the product (core assets) in
source lines of code, fraction of product unique code,
fraction of reused code, fraction of adapted code,
percentage of design modified, percentage of code
modified, and percent of integration required for
modified software. In addition, a number of parameters
related to reuse and maintenance are included, such as
software understanding of product unique code, software
understanding of adapted code, unfamiliarity with

1
2
3
4
5
6
7

12
12
12
12
12
12
12

50
40
30
50
50
50
50
















Branch-and-unite

AT FR
50.5
3
159.0
4
159.0
4
159.0
4
149.6
3
140.3
2

Infrastructure-based

APM
651.6
443.0
443.0
443.0
424.2
405.4

Incremental

DT
27.8
20.3
20.3
20.3
20.3
20.3

Big bang

core
p01
p02
p03
p04
p05

DPM
582.2
217.7
217.7
217.7
217.7
217.7

Single product only

Table 4.1. Scenarios

TTM
27.8
48.1
36.1
44.4
43.8
52.1

Resources

Table 3.2. Simulation Result

FR
27.8
48.1
48.1
68.4
79.8
100.1

Market Demand Interval

product unique code, unfamiliarity with adapted code,
and average change rate caused by new market demands.
To study the effect of resources, adoption approaches,
and evolution approaches on software product line
engineering, we ran the simulator seven times using the
same basic parameter values. Accordingly, we varied the
number of resources, the type of adoption approach, and
type of evolution approach.

Scenario

development and maintenance time (AT), and the
number of finished requirements (FR). The bottom
section summarizes the total product line evolution effort
(TPM), the time when all the requirements are finished
(FT), the average annual development effort (APM), the
number of requirements generated (TR), and the average
time-to-market (ATM). The unit of effort is personmonths and the unit of time is months.




4.2. Effect of resources
The inputs to Scenarios 1, 2 and 3 only differ in the
values of number of employees (50, 40, and 30,
respectively). The results differ in time-to-market, as
shown in Figure 4.1. At the beginning, there is little
difference, as time progresses, the gap of time-to-market
between
resource-constrained
and
non-resourceconstrained scenarios increases. The reason is that as
more products are developed, more resources for product
maintenance are required, thus less resources are left for
new product development, which may increase the
resource waiting time and in turn result in a longer timeto-market. The effort associated with Scenario 3 is
smaller than the other two cases. That is because in
Scenario 3, when Product 10 is released at 202.69,
Product 1 and 2 are already phased out (at the time
168.1). Accordingly, no effort is needed to update those
two products due to the change of the core.

4.3. Effect of adoption approach
The inputs to Scenarios 1 and 4 only differ in product
line adoption approach (big bang and incremental,
respectively). The results differ in time-to-market, as
shown in Figure 4.2. As specified by the inputs, the core

Time-to-Market

100
80
60
40
20
0

1

2

3

4

5

6

7

Products

numEmp=30

numEmp=40

8

9

10

numEmp=50

Time-to-Market

Figure 4.1. Effect of resources
60
50
40
30
20
10
0

1

2

3

4

5

6

7

Products

Big Bang

8

9

10

9

10

Incremental

Figure 4.2. Effect of adoption approach
Time-to-Market

100
80
60
40
20
0

1

2

3

4

5

6

Products

Branch-and-unite

7

8

Infrastructure-based

Figure 4.3. Effect of evolution approach
Time-to-Market

120
100
80
60
40
20
0

1

2
Big_Inf

3

4
Big_Bra

5

6

Products
Inc_Inf

7

8
Inc_Bra

9

10
Tra

Figure 4.4. Effect of combined adoption and evolution approach

assets are developed in two steps with the incremental
approach. The first increment happens right before the
development of Product 1, and implements half of the
core assets. The second increment happens right before
the development of Product 4, and implements the rest of
the core assets. For the first three products, the
incremental approach appears to have shorter time-tomarket, mainly because fewer core assets means less time
is required for asset development. As request for Product
4 comes, with the incremental approach, the development
of the new product can not be started until the rest of the
core assets have been implemented. So, we see a big
jump in time-to-market from Product 3 to 4. The
incremental approach results in higher total effort
(6173.36) than the big band approach (5338.47). That is
the nature of its process flow.

4.4. Effect of evolution approach
The inputs to Scenarios 1 and 5 differ in product line
evolution approach (infrastructure-based and branch-andunite, respectively). Figure 4.3 shows the comparison of
the results in time-to-market. As specified by the inputs,
the evolution stage starts from Product 7. For Product 7,
the branch-and-unite approach has smaller time-tomarket because the product gets developed earlier and
does not have to wait for core asset updates. For the later
products, the branch-and-unite approach results in longer
time-to-market because it requires extra effort to rework
new products and imposes more task dependencies, thus
reducing concurrency. The total effort of the branchand-unite approach (5340.37) is only a slightly higher
than the infrastructure-based approach (5338.47). That
is because when Product 9 and 10 are released, some
early products have already been phased out, so the costs
for updating existing products are reduced.

4.5. Effect of adoption and evolution approaches
A situation an organization might face is the need to
determine which software development and evolution
approaches best fit its goals. Scenarios 1 and 4 – 7 show
the alternatives the organization might have. Scenario 7
is the case where a traditional software development
approach (single product only) is taken, where products
are created and evolved independently.
Figure 4.4 shows the comparison of the results in
time-to-market. As we can see, the big bang with
infrastructure-based approach has the shortest average
time-to-market, and the incremental with branch-andunite approach has the longest average time-to-market.
The traditional approach has the shortest time-to-market
for the first two products, the longest time-to-market on

the third product, then its time-to-market stays between
the incremental and the big bang approaches, afterwards
its time-to-market starts climbing dramatically but still
stays in between the branch-and-unite and infrastructurebased approaches. In our experiment, the reuse rates are
not very high (30% for both black-box and white-box
reuse) and the product is relatively small (100KSLOC),
so the traditional product development time is only
slightly longer (about 5 months) than product line
engineering approaches. In the case of branch-and-unite
evolution approach, the dependencies imposed by that
approach overweighs the benefits of reusing the core
assets. The total effort of Scenario 1 and 4-7 are
5338.47, 5340.37, 6173.36, 6194.03, and 8760.55,
respectively. As we have expected, the traditional
approach requires considerably more effort. By largescale reuse, product line approaches generally result in
smaller code size to development and maintain. Thus,
the total effort on creating and evolving the products in a
product line is smaller.

4.6. Validation of model and results
Several steps have been taken to verify and validate
the model. First, the results of the simulator have been
compared with the results of COCOMO II [9] to make
sure the mathematic calculations are correct, and the
results are the same (ignoring rounding errors). Second,
the results of the simulator have been compared with the
common knowledge about the product line, and we feel
the results confirm to the common knowledge. Third, a
initial small set of experts have reviewed the simulation
results, and they feel that the results are consistent with
what have been observed in the real world and the
abstract model reflects the real process flow at a high
level. In future investigations, we plan to continue
soliciting expert feedback and compare simulation results
with real product line data.

5. Related work
Cohen [7] presents an approach for making a software
product line investment determination. The approach
uses three factors to justify software product line
investment:
applications,
benefits,
and
costs.
Applications include the number of projected products in
the product line, the time they will be developed, and
their annual change traffic; benefits consists of the
tangible and intangible goals the organization wishes to
achieve through a product line approach; costs are the
life cycle costs associated with core assets and individual
products. Costs are affected by some factors, such as

costs of reuse, degree of reuse, and core assets change
rate. Our cost estimation method is consistent with the
Cohen approach but provides more capabilities.
Regnell et al. use a simulator to study a specific
market-driven requirement management process [5].
The goal of simulator is to help in exploring bottleneck
and overload situations in the requirement engineering
process, investigating which resources are needed to
handle a certain frequency of new requirements, and
analyzing process improvement proposals. The specific
process is modeled using queuing network and discrete
event simulation [6]. Our simulator also uses discrete
event simulation, but its purpose is to study life cycle
issues for a product family instead of a portion of a
software engineering process for a single product.
Riva and Delrosso recently discussed issues related to
software product family evolution [11]. They state that a
product family typically evolves from a copy-and-paste
approach to a mature software platform. They point out
some issues that harm the family evolution, such as
organization bureaucracy, dependencies among tasks,
slower process of change, and the new requirements that
can break the architectural integrity. Their notion of
product family used in that paper is different from the
definition of a product line [8]. Creating a product
family by copy-and-paste is not a product line approach,
because the product line approach emphasizes a
disciplined strategic reuse, not opportunistic reuse. A
product line is actually a product family that has already
evolved to a mature software platform. Our simulation
results also show that in some cases dependencies
imposed by product line approaches result in slower
market response than the traditional software
engineering approach.

6. Conclusions and future investigations
Software product line engineering promises of reduced
cost while still supporting differentiation makes adoption
and continued use of the associated approaches attractive.
However, in order to make appropriate planning,
decision tools are necessary. In this paper, we described
a simulator that is intended to support early stage
decision-making. The simulator provides both static and
dynamic information for the selected software product
line engineering process. The statistical result generated
at the end of the simulation can be used for trade-off
analysis. Stepping through the simulator helps analyzing
product line processes, uncovering problems, and
improving the understanding of software product line
evolution.
Currently the simulation tool supports the study of
independent product line initiation using big bang or

incremental product line adoption approaches and
infrastructure-based or branch-and-unite product line
evolution strategies. Our future investigations include
providing estimates for other software product line
initiation
situations and approaches, allowing
concurrency between inter-dependent tasks to some
extent, providing probabilistic demand intervals,
incorporating other cost models, and removing a number
of the simplification assumptions. Furthermore, we plan
to validate the model by comparing the results with real
product line data and getting more expert feedback.
Also, we want to combine the simulator with an
optimization model, so users can specify their end-goal
criteria and then allow the simulator to search for the
best results.

7. References
[1] B.P. Zeigler and H.S. Sarjoughian, “Introduction to DEVS
Modeling & Simulation with JAVA(TM): Developing
Component-based
Simulation
Models”,
2003,
http://www.acims.arizona.edu/SOFTWARE/software.shtml.
[2] B. Boehm, A.W. Brown, R. Madachy, and Y. Yang, “A
Software Product Line Life Cycle Cost Estimation Model”,
USC, June 2003
[3] M. I. Kellner, R. J. Madachy, and D. M. Raffo, “Software
Process Modeling and Simulation: Why, What, How”, The
Journal of Systems and Software, April 1999, pp. 91-105.
[4] M. Paulk, et al., “Key Practices of the Capability Maturity
Model”, Version 1.1, Tech. Rept. CMU/SEI-93-TR-25,
Software Engineering Institute, Feb 1993.
[5] M. Höst, B. Regnell, et al, “Exploring Bottlenecks in
Market-Driven Requirements Management Processes with
Discrete Event Simulation”, The Journal of Systems and
Software, Dec 2001, pp. 323-332.
[6] J. Banks, J.S. Carson, and B.L. Nelson, Discrete-Event
System Simulation, 2nd Ed., Prentice Hall, Aug 2000.
[7] S. Cohen, “Predicting When Product Line Investment Pays”,
Proceedings of the Second International Workshop on Software
Product Lines: Economics, Architectures, and Implications,
Toronto Canada, 2001, pp. 15--18.
[8] P. Clements and L.M. Northrop, Software Product Lines -Practices and Patterns, Addison-Wesley, Aug 2001
[9] B. Boehm, B. Clark, E. Horowitz, C. Westland, R.
Madachy, and R. Selby, “Cost Models for Future Software Life
Cycle Processes: COCOMO 2.0,” Annals of Software
Engineering Special Volume on Software Process and Product
Measurement,
Science Publishers, Amsterdam, The
Netherlands, 1995, pp. 45 - 60.
[10] K. Schmidt and M. Verlage, “The Economic Impact of
Product Line Adoption and Evolution”, IEEE Software, Jul/Aug
2002, pp. 50-57.
[11] C. Riva and C.D. Rosso, “Experiences with Software
Product Family Evolution”, Proceedings of International
Workshop on Principles of Software Evolution, Helsinki
Finland, Sep 2003, pp. 161-169.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 1

What Makes Free/Libre Open
Source Software (FLOSS)
Projects Successful?

An Agent-Based Model of FLOSS Projects
Nicholas P. Radtke, Arizona State University, USA
Marco A. Janssen, Arizona State University, USA
James S. Collofello, Arizona State University, USA

Abstract
The last few years have seen a rapid increase in the number of Free/Libre Open Source Software (FLOSS)
projects. Some of these projects, such as Linux and the Apache web server, have become phenomenally
successful. However, for every successful FLOSS project there are dozens of FLOSS projects which never
succeed. These projects fail to attract developers and/or consumers and, as a result, never get off the
ground. The aim of this research is to better understand why some FLOSS projects flourish while others
wither and die. This article presents a simple agent-based model that is calibrated on key patterns of data
from SourceForge, the largest online site hosting open source projects. The calibrated model provides
insight into the conditions necessary for FLOSS success and might be used for scenario analysis of future
developments of FLOSS. [Article copies are available for purchase from InfoSci-on-Demand.com]
Keywords:	

Agent-Based Model; Emergent Properties; FLOSS; Open Source; Prediction Success;
Simulation

Although the concept of Free/Libre Open
Source Software (FLOSS) has been around
for many years, it has recently increased in
popularity as well as received media attention,
not without good reason. Certain characteristics
of FLOSS are highly desirable: some FLOSS
projects have been shown to be of very high
quality (Analysis of the Linux Kernel, 2004;
Linux Kernel Software, 2004) and to have low

defect counts (Chelf, 2006); FLOSS is able to
exploit parallelism in the software engineering
process, resulting in rapid development (Kogut
& Metiu, 2001); FLOSS sometimes violates
Brooks’ law (Rossi, 2004), which states that
“adding manpower to a late software product
makes it later” (Brooks, 1975); and FLOSS
development thrives on an increasing user- and
developer-base (Rossi, 2004).

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global
is prohibited.

2 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009

As open source has become a prominent
player in the software market, more people and
companies are faced with the possibility of using open source products, which often are seen
as free or low-cost solutions to software needs.
However, choosing to use open source software
is risky business, partly because it is unclear
which FLOSS will succeed. To choose an open
source project, only to find it stagnates or fails in
the near future, could be disastrous, and is cited
as a concern by IT managers (T. Smith, 2002).
Accurate prediction of a project’s likelihood to
succeed/fail would therefore benefit those who
choose to use FLOSS, allowing more informed
selection of open source projects.
This article presents an initial step towards
the development of an agent-based model that
simulates the development of open source projects. Findings from a diverse set of empirical
studies of FLOSS projects have been used to
formulate the model, which is then calibrated
on empirical data from SourceForge, the largest
online site hosting open source projects. Such a
model can be used for scenario and sensitivity
analysis to explore the conditions necessary for
the success of FLOSS projects.

BACKGROUND
There have been a limited number of attempts
to simulate various parts of the open source
development process (Dalle & David, 2004).
For example, Dalle and David (2004) use agentbased modeling to create SimCode, a simulator that attempts to model where developers
will focus their contributions within a single
project. However, in order to predict the success/failure of a single FLOSS project, other
existing FLOSS projects, which are vying for a
limited pool of developers and users, may need
to be considered. This is especially true when
multiple FLOSS projects are competing for a
limited market share (e.g., two driver projects
for the same piece of hardware or rival desktop
environments such as GNOME and the KDE).
Wagstrom, Herbsleb, and Carley (2005) created
OSSim, an agent-based model containing us-

ers, developers, and projects that is driven by
social networks. While this model allows for
multiple competing projects, the published
experiments include a maximum of only four
projects (Wagstrom et al., 2005). Preliminary
work on modeling competition among projects
is currently being explored by Katsamakas and
Georgantzas (2007) using a system dynamics
framework. By using a population of projects,
it is possible to consider factors between the
projects, e.g., the relative popularity of a project
with respect to other projects as a factor that
attracts developers and users to a particular
project. Therefore, our model pioneers new
territory by attempting to simulate across a
large landscape of FLOSS with agent-based
modeling.
Gao, Madey, and Freeh (2005) approach
modeling and simulating the FLOSS community via social network theory, focusing on
the relationships between FLOSS developers.
While they also use empirical data from the
online FLOSS repository SourceForge to calibrate their model, they are mostly interested
in replicating the network structure and use
network metrics for validation purposes (e.g.
network diameter and degree). Our model attempts to replicate other emergent properties
of FLOSS development without including the
complexities of social networking. However,
both teams consider some similar indicators,
such as the number of developers working on
a project, when evaluating the performance of
the models.
In addition, there have been attempts to
identify factors that influence FLOSS. These
have ranged from pure speculation (Raymond’s
(2000) gift giving culture postulates) to surveys
of developers (Rossi, 2004) to case studies using data mined from SourceForge (Michlmayr,
2005). Wang (2007) demonstrates specific
factors can be used for predicting the success
of FLOSS projects via K-Means clustering.
However, this form of machine learning offers
no insight into the actual underlying process
that causes projects to succeed. Therefore, the
research presented here approaches simulating

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global
is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 3

the FLOSS development process using agentbased modeling instead of machine learning.
To encourage more simulation of the
FLOSS development process, Antoniades, Samoladas, Stamelos, Angelis, and Bleris (2005)
created a general framework for FLOSS models.
The model presented here follows some of the
recommendations and best practices suggested
in this framework. In addition, Antoniades
et al. (2005) developed an initial dynamical
simulation model of FLOSS. Although the
model presented here is agent-based, many of
the techniques, including calibration, validation, and addressing the stochastic nature of
the modeling process, are similar between the
two models. One difference is the empirical
data used for validation: Antoniades et al.’s
(2005) model uses mostly code-level metrics
from specific projects while the model presented
here uses higher project-level statistics gathered
across many projects.

IDENTIFYING AND SELECTING
INFLUENTIAL FACTORS
Factors which are most likely to influence the
success/failure of FLOSS must first be identified
and then incorporated into the model. Many
papers have been published in regards to this,
but most of the literature simply speculates on
what factors might affect the success and offers
reasons why. Note that measuring the success
of a FLOSS project is still an open problem:
some metrics have been proposed and used but
unlike for commercial software, no standards
have been established. Some possible success
indicators are:
•	
•	
•	
•	
•	
•	

Completion of the project (Crowston,
Howison, & Annabi, 2006)
Progression through maturity stages (Crowston & Scozzi, 2002)
Number of developers
Level of activity (i.e., bug fixes, new feature
implementations, mailing list)
Time between releases
Project outdegree (Wang, 2007)

•	

Active developer count change trends
(Wang, 2007)

English and Schweik (2007) asked eight
developers how they defined success and failure
of an open source project. Answers varied for
success, but all agreed that a project with a lack
of users was a failure. Thus having a sufficient
user-base may be another metric for success.
Papers that consider factors influencing
success fall into two categories: those that look
at factors that directly affect a project’s success
(Michlmayr, 2005; Stewart, Ammeter, & Maruping, 2006; S. C. Smith & Sidorova, 2003) and
those that look for factors that attract developers
to a project (and thus indirectly affect the success
of a project) (Bitzer & Schröder, 2005; Rossi,
2004; Raymond, 2000; Lerner & Tirole, 2005).
A few go a step further and perform statistical
analyses to discover if there is a correlation
between certain factors and a project’s success/
failure (Lerner & Tirole, 2005; Michlmayr,
2005), and Kowalczykiewicz (2005) uses
trends for prediction purposes. Wang (2007)
demonstrates that certain factors can be used
for accurate prediction using machine learning
techniques. Koch (2008) considers factors affecting efficiency after first using data envelopment analysis to show that successful projects
tend to have higher efficiencies.
In general, factors affecting FLOSS projects fall into two categories: technical factors
and social factors. Technical factors are aspects
that relate directly to a project and its development and are typically both objective and easy to
measure. Examples of technical factors include
lines of code and number of developers.
The second category is social factors.
Social factors pertain to aspects that personally
motivate individuals to engage in open source
development/use. Examples of social factors
include reputation from working on a project,
matching interests between the project and the
developer/user, popularity of the project with
other developers/users, and perceived importance of the code being written (e.g., core versus
fringe development (Dalle & David, 2004)).
Most of the social factors are subjective and

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global
is prohibited.

4 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009

rather difficult, if not impossible, to measure.
Despite this, it is hard to deny that these might
influence the success/failure of a project and
therefore social factors are considered in the
model. Fortunately, the social factors being
considered fall under the domain of public
goods, for which there is already a large body
of work published (e.g., Ostrom, Gardner, &
Walker, 1994; Jerdee & Rosen, 1974; Tajfel,
1981; Axelrod, 1984; Fox & Guyer, 1977).
Most of this work is not specific to FLOSS, but
in general it explores why people volunteer to
contribute to public goods and what contextual
factors increase these contributions.
The findings of this literature are applied
when designing the model, as are findings
from publications investigating how FLOSS
works, extensive surveys of developers asking
why they participate in FLOSS (e.g., Ghosh,
Krieger, Glott, & Robles, 2002), and comments and opinions of FLOSS users (e.g., T.
Smith, 2002).

INITIAL MODEL
The model universe consists of agents and
FLOSS projects. Agents may choose to contribute to or not contribute to, and to consume
(i.e. download) or not consume FLOSS projects.
At time zero, FLOSS projects are seeded in

the model universe. These initial projects vary
randomly in the amount of resources that will
be required to complete them. At any time,
agents may belong to zero, one, or more than
one of the FLOSS projects. The simulation is
run with a time step (t) equal to one (40 hour)
workweek.
Table 1 contains the properties of agents.
Table 2 contains the properties of projects.
At each time step, agents choose to produce
or consume based on their producer and consumer numbers, values between 0.0 and 1.0 that
represent probabilities that an agent will produce
or consume. Producer and consumer numbers
are statically assigned when agents are created
and are drawn from a normal distribution. If
producing or consuming, an agent calculates
a utility score for each project in its memory,
which contains a subset of all available projects.
The utility function is shown in Box 1.
Each term in the utility function represents a weighted factor that attracts agents to
a project, where w1 through w5 are weights that
control the importance of each factor, with 0.0
≤ w1,w2,w3,w4,w5 ≤ 1.0 and Σ5i=1 wi = 1.0. Factors
were selected based on both FLOSS literature
and our own understanding of the FLOSS development process. Keeping it simple, a linear
utility equation is used for this version of the
model. The first term represents the similarity
between the interests of an agent and the direc-

Table 1. Agent properties
Property

Description

Type/Range

Consumer number

Propensity of an agent to consume (use)
FLOSS.

Real [0.0, 1.0]

Producer number

Propensity of an agent to contribute to
(develop) FLOSS.

Real [0.0, 1.0]

Needs vector

A vector representing the interests of the
agent.

Each scalar in vector
is real [0.0, 1.0]

Resources number

A value representing the amount of work
an agent can put into FLOSS projects on
a weekly basis. A value of 1.0 represents
40 hours.

Real [0.0, 1.5]

Memory

A list of projects the agent knows exist.

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global
is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 5

Table 2. Project properties
Property

Description

Type/Range

Current resources

The amount of resources or work being
contributed to the project during the current time interval.

Real

Cumulative resources

The sum, over time increments, of all
resources contributed to the project.

Real

Resources for completion

The total number of resources required to
complete the project.

Real

Download count

The number of times the project has been
downloaded.

Maturity

Six ordered stages a project progresses
through from creation to completion.

{planning, pre-alpha,
alpha, beta, stable,
mature}

Needs vector

An evolving vector representing the
interests of the developers involved in the
project.

Each scalar in vector
is real [0.0, 1.0]

Integer

Box 1.
utility = w1 ⋅ similarity (agentNeeds, projectNeeds )
+ w2 ⋅ currentResourcesnorm
+ w3 ⋅ cumulativeResourcesnorm
+ w4 ⋅ downloadsnorm
+ w5 ⋅ f (maturity )

tion of a project; it is currently calculated using
cosine similarity between the agent’s and project’s needs vectors. The second term captures
the current popularity of the project and the
third term the size of the project implemented
so far. The fourth term captures the popularity of a project with consumers based on the
cumulative number of downloads a project has
received. The fifth term captures the maturity
stage of the project. Values with the subscript
“norm” have been normalized (e.g., downloadsnorm is a project’s download count divided by
the maximum number of downloads that any
project has received). The discreet function
f
maps each of the six maturity stages into a
value between 0.0 and 1.0, corresponding to the
importance of each maturity stage in attracting
developers. Since all terms are normalized, the

(1)

utility score is always a value between 0.0 and
1.0. Both consumers and producers use the
same utility function. This is logical, as most
FLOSS developers are also users of FLOSS.
For consumers that are not producers, arguably
the terms represented in the utility function are
still of interest when selecting a project. There
is relatively little research published on users
compared to developers of FLOSS, so it is unclear if selection criteria are different between
the two groups.
It is possible that some of the terms included in the utility function are redundant or
irrelevant. Part of the model exploration is to
determine which of these factors are relevant.
See the Calibrating the Model and Results
sections below.

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global
is prohibited.

6 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009

Agents use utility scores in combination
with a multinominal logit equation to probabilistically select projects. The multinominal logit
allows for imperfect choice, i.e., not always
selecting the projects with the highest utility.
There is no explicit formulation of communication between agents included in the
model; implicitly it is assumed that agents
share information about other projects and thus
agents know characteristics of projects they are
not currently consuming/producing. At each
time step, agents update their memory. With a
certain probability an agent will be informed
of a project and add it to its memory, simulating discovering new projects. Likewise, with
a certain probability an agent will remove a
project from its memory, simulating forgetting
about or losing interest in old projects. Thus,
over time an agent’s memory may expand and
contract.
Projects update their needs vector at each
iteration using a decaying equation, where the
new vector is partially based on the project’s
previous vector and partially on the needs vectors of the agents currently contributing to the
project. An agent’s influence on the project’s
vector is directly proportional to the amount
of work the agent is contributing to the project
with respect to other agents working on the same
project. This represents the direction of a project
being influenced by the developers working on
it. Finally, project maturity stages are computed
based on percent complete threshold values.

VALIDATION METHOD
Creating a model that successfully predicts
the success or failure of FLOSS projects is
a complicated matter. To aid in the iterative
development process, the model is first calibrated to reproduce a set of known, emergent
properties from real world FLOSS data. For
example, Weiss (2005) surveyed the distribution
of projects at SourceForge in each of six development categories: planning, pre-alpha, alpha,
beta, stable, and mature. Therefore, the model
will need to produce a distribution of projects

in each stage similar to that measured by Weiss.
In addition, two other emergent properties were
chosen to validate the initial model:
•	
•	
•	

Number of developers per FLOSS project.
Number of FLOSS projects per developer.
By creating a model that mimics a number
of key patterns of the data, confidence is
derived about the model.

CALIBRATING THE MODEL
The model has a number of parameters that
must be assigned values. A small subset of these
can be set to likely values based on statistics
gathered from surveys or mined from FLOSS
repository databases. For the remaining parameters, a search of the parameter space must be
performed to find the combination that allows
the model to most closely match the empirical
data. Since an exhaustive search is not practical,
the use of genetic algorithms from evolutionary
computation is used to explore the parameter
space (Kicinger, Arciszewski, & De Jong, 2005).
This is done as follows: an initial population
of model parameter sets is created randomly.
The model is run with each of the parameter
sets and a fitness score is calculated based on
the similarity of the generated versus empirical data. The parameter values from these sets
are then mutated or crossed-over with other
parameter sets to create a new generation of
model parameter sets, with a bias for selecting
parameters sets that resulted in a high fitness;
then the new generation of parameter sets are
evaluated and the process repeated. In this
case, a genetic algorithm is being used for a
stochastic optimization problem for which it
is not known when a global optimum is found.
Genetic algorithms are appropriate for finding
well-performing solutions in a reasonably brief
amount of time. Reviewing the values of the
best performing parameters will help identify
which factors are important/influential in the
open source software development process.

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global
is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 7

The fitness function chosen for the genetic
algorithm is based on the sum of the square of
errors between the simulated and empirical
data, as shown in Box 2.
Since there are three fitness values calculated, one per empirical data set, the three
fitness values are averaged to provide a single
value for comparison purposes.

RESULTS
Since the model includes stochastic components,
multiple runs with a given parameter set were
performed and the results averaged. In this case,
four runs were performed for each parameter set
after initial experimentation showed very low
standard deviations even with small numbers
of runs. The averaged model results were then
compared to the empirical data.
As empirical investigations of FLOSS
evolution note, it takes approximately four
years for a project of medium size to reach

a mature stage (Krishnamurthy, 2002). Thus,
the model’s performance was evaluated by
running the model for 250 time steps, with a
time step of one week, for a total simulated
time equivalent of a little over five years. All
metrics were gathered immediately following
the 250th time step.
The averaged data (over 4 runs) from the
simulator’s best parameter set, along with the
empirical data, is shown in Figs. 1, 2, and 3.
Figure 1 shows the generated percentage of
projects in each maturity stage is a similar shape
to the empirical data, with the main difference
being the highs are too high and the lows are too
low in the simulated data. This disparity may be
a result of initial model startup conditions. At
time 0, the model starts with all projects in the
planning stage. This is obviously different than
SourceForge, where the projects were gradually
added over time, not all at once in the beginning.
While the model does add new projects each
time step, with a growth rate based on the rate of
increase of projects at SourceForge, it may take

Box 2.

fitness = 1 −

sum of square of errors
maximum possible sum of square of errors

(2)

Figure 1. Percentage of FLOSS projects in maturity stages. Empirical data from (Weiss, 2005)

Maturity Stages
45
40

Percent

35
30
25
20

Sim Average
Emp Value

15
10
5
0
Plan
ning

Prealpha

Alpha

Beta

Stable Mature

Maturity Stage

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global
is prohibited.

8 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009

be corrected via additional experimentation
with parameters.
Table 3 contains the average fitness scores
for each of the emergent properties for the top
performing parameter set. These values provide
a quantitative mechanism for confirming the
visual comparisons made above: the maturity
stage fitness score is indeed lower than the other
two properties. The combined fitness is simply
the mean of the three fitness scores, although this
value could be calculated with uneven weights
if, say, matching each property was prioritized.
Doing so would affect how the genetic algorithm
explored the parameter space. It may be the case
that certain properties are easy to reproduce
in the model and work over a wide range of
parameter sets, in which case these properties
may be weighted less than properties that are
more difficult to match. Properties which are
always matched should be discarded from the
model for evolution purposes as they do not
discriminate against different parameter sets.
Finally, examining the evolved utility
weights of the top 10 performing parameter
sets provides insight into what factors are
important in the model for reproducing the
three properties examined. Table 4 contains
the averages and standard deviations for each

more than 250 time steps for maturity stages to
stabilize after the differing initial condition. At
the end of the simulation run, just short of 60%
of the projects were created sometime during
the simulation while the remaining 40% were
created at time 0.
As shown in Figure 2, the number of developers per projects follows a near-exponential
distribution and the simulated data is similar,
especially for projects with fewer than seven
developers. Note that the data in Figure 2
uses a logarithmic scale to help with a visual
comparison between the two data sets. Beyond
seven developers, the values match less closely,
although this difference is visually amplified as
a result of the logarithmic scale and is actually
not as large as it might initially appear. Since
there are few projects with large numbers of
developers in the empirical data, the higher
values may be in the noise anyhow and thus
focus should be on the similarity of the lower
numbers.
Figure 3 shows the number of projects per
developer is a relatively good match between
the simulated and empirical data, with the main
difference being the number of developers working on one project. It is likely that this could

Figure 2. Percentage of projects with N developers. Empirical data from (Weiss, 2005)

Developers per Project
6

ln(percent)

4
2
0

Sim Average
Emp Value

-2
-4
-6
-8
0

5

10

15

20

25

30

35

40

Developers per Project

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global
is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 9

Figure 3. Percentage of developers with N projects. Empirical data from (Ghosh et al., 2002)

Projects per Developer
40
35

Percent

30
25
20

Sim Average
Emp Value

15
10
5
0
0

1

2

3

4-5 6-7 8-1 11- 16- >20
0
15 20

Projects per Developer

Table 3. Averaged fitness scores for the best
Emergent Property

Fitness Score

Maturity stage

0.9679

Devs per project

0.9837

Projects per dev

0.9938

Combined

0.9818

Table 4. Utility function weights from the best
10 param
Weight

Mean

Std.
Dev.

w1 (similarity)

0.1849

0.1137

w2 (current resources)

0.3964

0.1058

w3 (cumulative resources)

0.0003

0.0003

w4 (downloads)

0.0022

0.0039

w5 (maturity)

0.4163

0.1534

of the weights. It appears that the cumulative
number of resources and download counts are
not important in reproducing the examined properties in the model. This conclusion is reached
by observing these weight’s small values (low
mean and small variance) in comparison to the
other weights (high means and larger variance).

Unfortunately, the high variance of the remaining three weights makes it difficult to rank them
in order of importance. Rather, the conclusion is
that similarity, current resources, and maturity
are all important in the model.
Another interesting set of values evolved by
the system are the parameters for the producer
and consumer numbers. While the producer
and consumer numbers are drawn from normal
distributions bounded by 0.0 and 1.0 inclusive,
neither the mean nor standard deviations of these
distributions are known. Therefore, these values
are evolved to find the best performing values.
Table 5 contains the evolved mean and standard deviation for the producer and consumer
numbers averaged from the top 10 parameter
sets. Notice that the mean producer number is
very high at 0.9801 and very stable across the
top 10 parameter sets, with a standard deviation
of 0.0079. Likewise, the standard deviation is
relatively low at 0.1104 and also stable with a
standard deviation of 0.0101. This indicates that
the top performing model runs had agents with
high propensities to develop. In other words having most agents produce frequently (i.e., most
agents be developers) produces better matching
of the empirical data. This is in alignment with
the notion that FLOSS is a developer-driven

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global
is prohibited.

10 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009

Table 5. Evolved producer/consumer number
distributions parameters

Producer/Consumer Number

Producer number
Consumer
number

Parameter statistics
from top 10 parameter sets
Mean

Std. Dev.

Mean

0.9801

0.0079

Std. Dev.

0.1104

0.0101

Mean

0.6368

0.1979

Std. Dev.

0.3475

0.3737

process. The evolved consumer number mean
is much lower and standard deviation is much
higher compared to the producer number.
Neither one of these parameters is particularly
stable, i.e., both have large standard deviations
over the top 10 parameter sets. This indicates
that the consumer number distribution has
little effect on matching the empirical data for
the top 10 parameter sets. Note that this is in
alignment with the evolved weight for downloads approaching 0.0 in the utility functions.
Consumers are not the driving force in matching
the empirical data in the model.

DISCUSSION
Once developers join a project, it is likely that
they will continue to work on the same project
in the future. This is especially evident in the
case of core developers, who typically work
on a project for an extended period of time.
Currently, the model attempts to reproduce
this characteristic by giving a boost (taking
the square root) of the utility function for
projects worked on in the previous time step.
In effect, this increases the probability of an
agent selecting the same projects to work on in
the subsequent time step. Improvements to the
model might include adding a switching cost
term to the utility function, representing the
extra effort required to become familiar with
another project. Gao et al. (2005) address this

issue by using probabilities based off data from
SourceForge to determine when developers continue with or leave a project they are currently
involved with in their FLOSS model.
The model’s needs vectors serve as an
abstraction for representing the interests and
corresponding functionalities of the agents and
projects respectively. Therefore, the needs vector is at the crux of handling the matching of
developers’ interests with appropriate projects.
For simplicity, initial needs vector values are
assigned via a uniform distribution, but exploration of the effects of other distributions may be
interesting. For example, if a normal distribution
is used, projects with vector components near
the mean will have an easy time attracting agents
with similar interests. Projects with vector components several standard deviations from the
mean may fail to attract any agents. A drawback
of a normal distribution is that it makes most
projects similar; in reality, projects are spread
over a wide spectrum (e.g., from operating
systems and drivers to business applications
and games), although the actual distribution is
unknown and difficult to measure.
Currently, needs vectors for projects and
agents are generated independently. This has
the problem of creating projects which have
no interest to any agents. An improvement
would be to have agents create projects; when
created, a project would clone its associated
agent’s needs vector (which would then evolve
as other agents joined and contributed to the
project). This behavior would more closely
match SourceForge, where a developer initially registers his/her project. By definition,
the project matches the developer’s interest at
time of registration.
For simplicity’s sake, currently the model
uses a single utility function for both producers and consumers. It is possible that these two
groups may attach different weights to factors
in the utility function or may even have two
completely different utility functions. However,
analysis of the model shows that developers are
the driving force to reproduce the empirical
data. Exploration of a simplified model without

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global
is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 11

consumers may show that concerns about using
multiple utility functions are irrelevant.
One final complication with the model is
its internal representations versus reality. For
example, a suggested strategy for success in
open source projects is to release early and
release often (Raymond, 2000). Using this
method to determine successful projects within
the model is problematic because the model
includes no concept of releasing versions of
software. Augmenting the model to include a
reasonable representation of software releases
is non-trivial, if possible at all. Likewise, it is
difficult to compare findings of other work on
conditions leading to success that map into this
model. For example, Lerner and Tirole (2005)
consider licensing impacts while Michlmayr
(2005) consider version control systems,
mailing lists, documentation, portability, and
systematic testing policy differences between
successful and unsuccessful projects. Unfortunately, none of these aspects easily map into the
model for comparison or validation purposes.

emergent properties for validation purposes, the
model could move into the realm of prediction.
In this case, it would be possible to feed real-life
conditions into the model and then observe a
given project as it progresses (or lack of progresses) in the FLOSS environment.

CONCLUSION

Brooks, F. P. (1975). The mythical man-month:
Essays on software engineering. Reading, MA:
Addison-Wesley.

A better understanding of conditions that
contribute to the success of FLOSS projects
might be a valuable contribution to the future of
software engineering. The model is formulated
from empirical studies and calibrated using
SourceForge data. The calibrated version produces reasonable results for the three emergent
properties examined. From the calibrated data,
it is concluded that the similarity between a
developer and a project, the current resources
going towards a project, and the maturity stage
of a project are important factors. However, the
cumulative resources and number of downloads
a project has received are not important in
reproducing the emergent properties.
The model presented here aids in gaining
a better understanding of the conditions necessary for open source projects to succeed. With
further iterations of development, including
supplementing the model with better data-based
values for parameters and adding additional

REFERENCES
Analysis of the linux kernel. (2004). Research report.
(Coverity Incorporated)
Antoniades, I., Samoladas, I., Stamelos, I., Angelis,
L., & Bleris, G. L. (2005). Dynamical simulation
models of the open source development process.
In S. Koch (Ed.), Free/open source software development (pp. 174–202). Hershey, PA: Idea Group,
Incorporated.
Axelrod, R. (1984). The evolution of cooperation.
New York: Basic Books.
Bitzer, J., & Schröder, P. J. (2005, July). Bug-fixing
and code-writing: The private provision of open
source software. Information Economics and Policy,
17(3), 389-406.

Chelf, B. (2006). Measuring software quality: a study
of open source software. Research report. (Coverity
Incorporated)
Crowston, K., Howison, J., & Annabi, H. (2006,
March/April). Information systems success in free
and open source software development: Theory
and measures. Software Process: Improvement and
Practice, 11(2), 123–148.
Crowston, K., & Scozzi, B. (2002). Open source
software projects as virtual organizations: competency rallying for software development. In IEE
proceedings software, 49, 3–17).
Dalle, J.-M., & David, P. A. (2004, November 1).
SimCode: Agent-based simulation modelling of
open-source software development (Industrial Organization). EconWPA.
English, R., & Schweik, C. M. (2007). Identifying
success and tragedy of FLOSS commons: A preliminary classification of Sourceforge.net projects.

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global
is prohibited.

12 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009
In FLOSS ’07: Proceedings of the first international
workshop on emerging trends in FLOSS research and
development (p. 11). Washington, DC, USA: IEEE
Computer Society.
Fox, J., & Guyer, M. (1977, June). Group size and
others’ strategy in an n-person game. Journal of
Conflict Resolution, 21(2), 323–338.
Gao, Y., Madey, G., & Freeh, V. (2005, April). Modeling and simulation of the open source software
community. In Agent-Directed Simulation Conference (pp. 113–122). San Diego, CA.
Ghosh, R. A., Krieger, B., Glott, R., & Robles, G.
(2002, June). Part 4: Survey of developers. In Free/
libre and open source software: Survey and study.
Maastricht, The Netherlands: University of Maastricht, The Netherlands.
Jerdee, T. H., & Rosen, B. (1974). Effects of opportunity to communicate and visibility of individual
decisions on behavior in the common interest. Journal
of Applied Psychology, 59(6), 712–716.
Katsamakas, E., & Georgantzas, N. (2007). Why most
open source development projects do not succeed?
In FLOSS ’07: Proceedings of the first international
workshop on emerging trends in FLOSS research and
development (p. 3). Washington, DC, USA: IEEE
Computer Society.
Kicinger, R., Arciszewski, T., & De Jong, K. A.
(2005). Evolutionary computation and structural
design: A survey of the state of the art. Computers
and Structures, 83(23-24), 1943-1978.
Koch, S. (2008). Exploring the effects of SourceForge.net coordination and communication tools
on the efficiency of open source projects using data
envelopment analysis. In S. Morasca (Ed.), Empirical
Software Engineering: Springer.
Kogut, B., & Metiu, A. (2001, Summer). Opensource software development and distributed innovation. Oxford Review of Economic Policy, 17(2),
248-264.
Kowalczykiewicz, K. (2005). Libre projects lifetime
profiles analysis. In Free and open source software
developers’ European meeting 2005. Brussels,
Belgium.
Krishnamurthy, S. (2002, June). Cave or community?:
An empirical examination of 100 mature open source
projects. First Monday, 7(6).

Lerner, J., & Tirole, J. (2005, April). The scope of
open source licensing. Journal of Law, Economics,
and Organization, 21(1), 20–56.
Linux kernel software quality and security better
than most proprietary enterprise software, 4-year
Coverity analysis finds. (2004). Press release. (Coverity Incorporated)
Michlmayr, M. (2005). Software process maturity and
the success of free software projects. In K. Zielinski
& T. Szmuc (Eds.), Software engineering: Evolution and emerging technologies (p. 3-14). Krakow,
Poland: IOS Press.
Ostrom, E., Gardner, R., & Walker, J. (1994). Rules,
games and common pool resources. Ann Arbor, MI:
University of Michigan Press.
Raymond, E. S. (2000, September 11). The cathedral and the bazaar (Tech. Rep. No. 3.0). Thyrsus
Enterprises.
Rossi, M. A. (2004, April). Decoding the “Free/
Open Source(F/OSS) Software puzzle” a survey of
theoretical and empirical contributions (Quaderni
No. 424). Dipartimento di Economia Politica, Università degli Studi di Siena.
Smith, S. C., & Sidorova, A. (2003). Survival of opensource projects: A population ecology perspective. In
ICIS 2003. Proceedings of international conference
on information systems 2003. Seattle, WA.
Smith, T. (2002, October 1). Open source: Enterprise ready – with qualifiers. theOpenEnterprise. (http://www.theopenenterprise.com/story/
TOE20020926S0002)
Stewart, K. J., Ammeter, A. P., & Maruping, L. M.
(2006, June). Impacts of license choice and organizational sponsorship on user interest and development
activity in open source software projects. Information
Systems Research, 17(2), 126–144.
Tajfel, H. (1981). Human groups and social categories: Studies in social psychology. Cambridge, UK:
Cambridge University Press.
Wagstrom, P., Herbsleb, J., & Carley, K. (2005). A
social network approach to free/open source software
simulation. In First international conference on open
source systems (pp. 16–23).
Wang, Y. (2007). Prediction of success in open source
software development. Master of science dissertation,
University of California, Davis, Davis, CA.

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global
is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 13

Weiss, D. (2005). Quantitative analysis of open
source projects on SourceForge. In M. Scotto & G.
Succi (Eds.), Proceedings of the first international

conference on open source systems (OSS 2005) (pp.
140–147). Genova, Italy.

Nicholas P. Radtke is a PhD candidate in computer science at Arizona State University. His research
focuses on understanding and modeling free/libre open source software engineering processes.
Marco A. Janssen is assistant professor on formal modeling of social and social-ecological systems within
the School of Human Evolution and Social Change at Arizona State University. He is also the associate
director of the Center for the Study of Institutional Diversity. His formal training is within the area of
operations research and applied mathematics. His current research focuses on the fit between behavioral,
institutional and ecological processes. In his research he combines agent-based models with laboratory
experiments and case study analysis. Janssen also performs research on diffusion processes of knowledge
and information, with applications in marketing and digital media.
James S. Collofello is currently computer science and engineering professor and associate dean for the
Engineering School at Arizona State University. He received his PhD in computer science from Northwestern
University. His teaching and research interests lie in the software engineering area with an emphasis on software quality assurance, software project management and software process modeling and simulation.

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global
is prohibited.

A System Dynamics Software Process Simulator for
Staffing Policies Decision Support
Dr. James Collofello
Dept. of Computer Science and Engineering
Arizona State University
Tempe, Arizona 85287-5406
(602) 965-3190
collofello@asu.edu
Ioana Rus, Anamika Chauhan
Dept. of Computer Science and Engineering
Arizona State University

Dan Houston
Honeywell, Inc. and Dept. of Industrial and
Management Systems Engineering
Arizona State University
Douglas Sycamore
Motorola
Communication Systems Divsions
Scottsdale, Arizona
Dr. Dwight Smith-Daniels
Department of Management
Arizona State University

Abstract
Staff attrition is a problem often faced by software
development organizations. How can a manager plan
for the risk of losses due to attrition? Can policies for
this purpose be formulated to address his/her specific
organization and project?
Proposed was to use a software development
process simulator tuned to the specific organization,
for running "what-if" scenarios for assessing the
effects of managerial staffing decisions on project's
budget, schedule and quality. We developed a system
dynamics simulator of an incremental software
development process and used it for analyzing the
effect of the following policies: to replace engineers
who leave the project, to overstaff in the beginning of
the project or to do nothing, hoping that the project
will still be completed in time and within budget.
This paper presents the simulator, the experiments
that we ran, the results that we obtained and our
analysis and conclusions.
Introduction
We used process modeling and simulation for
estimating the effect on the project cost, schedule and
rework of different staffing policies. Assessment of
such managerial decisions could be also done by other
methods like formal experiments, pilot projects, case
studies, and experts opinions and surveys. Modeling
and simulation is preferred because it does not have
the disadvantages of the other methods (like possible
irrelevance in the case of formal experiments or pilot
projects, interfering with the real process and taking a

long time for case studies or subjectivity for experts
opinions and surveys).
System Dynamics Modeling
A software process model represents the process
components (activities, products, and roles), together
with their attributes and relationships, in order to
satisfy the modeling objective. Some modeling
objectives are to facilitate a better human
understanding and communication, and to support
process improvement, process management,
automated guidance in performing the process, and
automated process execution [6]. In addition, we
demonstrate how process modeling experimentation
can be used to investigate alternatives for
organizational policy formulation.
There are many modeling techniques developed
and used so far, according to the modeling goal and
perspective. The paradigm chosen for this research was
system dynamics modeling. Richmond considers
system dynamics as a subset of systems thinking.
Systems thinking is "the art and science of making
reliable inferences about behavior by developing an
increasingly deep understanding of underlying
structure" [11]. Systems thinking is, according to
Richmond, a paradigm and a learning method.
System dynamics is defined as "the application of
feedback control systems principles and techniques to
modeling, analyzing, and understanding the dynamic
behavior of complex systems" [2]. System dynamics
modeling (SDM) was developed in the late 1950's at
M.I.T. SDM is based on cause-effect relationships that
are observable in a real system. These cause-effect
relationships constantly interact while the computer
model is being executed, thus the dynamic

interactions of the system are being modeled, hence its
name. A system dynamics model can contain
relationships between people, product, and process in
a software development organization. The most
powerful feature of system dynamics modeling is
realized when multiple cause-effect relationships are
connected forming a circular relationship, known as a
feedback loop. The concept of a feedback loop reveals
that any actor in a system will eventually be affected
by its own action. Because system dynamics models
incorporate the ways in which people, product, and
process react to various situations, the models must
be tuned to the organizational environment that they
are modeling.
SDM is a structural approach, as opposed to other
estimation models like COCOMO and SLIM, that are
based on correlation between metrics from a large
number of projects.
The automated support for developing and
executing SDM simulators enables handling the large
complexity of a software development process which
can not be handled by a human mental model.
Building and using the model results in a better
understanding of the cause-effect relationships that
underlie the development of software.
A simulator to be used for estimating, predicting,
and tracking a project requires a quantitative modeling
technique. SDM utilizes continuous simulation
through evaluation of difference and differential
equations. These equations implement both the
feedback loops that model the project structures as
flows, as well as the rates that model the dynamics of
these flows. Thus, application of an SDM model
requires an organization to define its software
development processes and to identify and collect
metrics that characterize these processes.
Simulation enables experimenting with the model
of the software process, without impacting on the real
process. The effects of one factor in isolation can be
examined, which cannot be done in a real project. The
outcome of the SD simulation has two aspects: an
intellectual one, consisting of a better understanding of
the process and a practical one, consisting of
prediction, tracking, and training.
SDM was applied to the software development
process for the first time by Tarek Abdel-Hamid and
Stuart Madnick [2]. Their model captures the
managerial aspects of a waterfall software life cycle. It
was the starting point for many subsequent models of
the entire process [15], [14] or parts of it [5] and [9]
that have been successfully used for resource
management [8], [13], process reengineering [4],
project planning, and training [12].
Abdel-Hamid [1] studied the impact of turnover,
acquisition, and assimilation rates on software project
cost and schedule. He found that these two response
variables can be significantly affected by an

employment time of less than 1000 days, by a hiring
delay less than or greater than 40 days, or by an
assimilation delay greater than 20 days. We take this
discussion a step further into the practical realm by
focusing on the turnover issue and demonstrate how
SDM can be used in decision support for strategies
that address staff turnover.
Description of System Dynamics Model
Our system dynamics modeling tool was
developed using the ithink simulation software [7].
The model incorporates four basic feedback loops
comprised of non-linear system dynamic equations.
The four feedback loops were chosen because they
encompass the factors that are typically the most
influential in software projects [16].
All of these feedback loops begin and end at the
object labeled, Schedule and Effort, which is the
nucleus of the system. This object represents a
project schedule and the effort in person hours to
complete a schedule plan. See Figure 1.
The first feedback loop represents the staffing profile
of a project (refer to the loop “Schedule and Effort”,
“Staffing Profile”, “Experience Level”, and
“Productivity” on Figure 1). The staffing profile
affects productivity based on the number of engineers
working on a project, the domain expertise of the
engineers, and amount of time an engineer participates
on a project.
The second feedback loop models the
communication overhead (refer to the loop “Schedule
and Effort”, “Staffing Profile”, “Communication
Overhead”, and “Productivity” on Figure 1). The
more people on a project will result in an increase in
communication overhead among the team members
and thus, decreases the productivity efficiency [14]
[15].
The third feedback loop takes into consideration
the amount of defects generated by the engineers
during the design and coding phases of an increment,
which translates into rework hours (refer to the loop
“Schedule and Effort”, “Staffing Profile”, “Experience
Level”, “Defect Generated, and “Rework Hours” on
Figure 1).
The tool also models the impact of
domain expertise on defect generation. An engineer
with less domain expertise generates more defects than
an engineer with a higher degree of domain expertise.
The fourth feedback loop models the schedule
pressure associated with the percentage of work
complete per the schedule (refer to the loop “Schedule
and Effort”, “Schedule Pressure”, and “Productivity”
on Figure 1). The farther behind schedule the greater
the schedule pressure. As schedule pressure increases,
engineers will work more efficiently and additional
hours, increasing productivity toward completing the
work. However, if schedule pressure remains high and

engineers are working many hours of overtime, they
begin to generate more defects and eventually an
exhaustion limit is reached. Once the exhaustion

limit is reached, productivity will decrease until a
time period such that the engineers can recoup and
begin working more productively again.

Schedule
Pressure

Schedule
and
Effort

Rework
Hours

Productivity
Defects
Generated

Staffing
Profile
Communication
Overhead
Experience
Level

Figure 1. Basic Feedback Loops of the Model
The model inputs are summarized in Table 2
familiar. For a summary of the experts’ credentials,
The model outputs are provided in the form of
refer to Table 1.
graphs that plot the work remaining in each
Each expert (or evaluator) was asked to review the
increment, current staff loading, total cost, percent
tool from a Project Leader position, a Software
complete, and quality costs. Figure 2 is an example
Developer position, and to assess the tool's technical
of the output that shows the graph for Staff Loading
characteristics. During the simulations, the experts
and Total Cost plots.
were to observe the output for valid trend data and for
The model was validated by two methods, expert
a level of comfort with the accuracy of the results.
opinion and reproduction of actual project results. Six
The demo given to each expert was identical. After
experts from Motorola and very experienced software
completing the demo, the experts were given an
professionals, examined the simulator model and
opportunity to run different scenarios with the
expressed their confidence in its ability to model the
supplied data or scenarios with their own project data.
general behavior of a software project. In addition,
Finally, they were given an opportunity to compare
one of the experts used the model to accurately
the results to any project manage tools they use.
reproduce the results of a project with which he was

Table 1. Background Summary of Each Expert
Expert #1

Expert #2

Expert #3

Expert #4

Expert #5

Expert #6

Title: Principal Software Engineer
Degrees: M.S. and Ph.D. in Computer Science
Years of Experience: 13 Years
Responsibilities: Project Leader, Systems Engineer, Software Engineer, and
Proposal Development.
Other Pertinent Information: Former Assistant Professor of Computer
Science at Arizona State University, Tempe, AZ.
Title: Principal Software Engineer
Degrees: Ph.D. in Computer Science
Years of Experience: 25+ Years
Responsibilities: Project Leader
Other Pertinent Information: Former Assistant Professor of Computer
Science at Arizona State University, Tempe, AZ.
Title: Software Engineer
Degrees: B.S in Computer Science
Years of Experience: 11 years
Responsibilities: Software Development - Currently developing a ManMachine Interface.
Special Awards:
Special Achievement Award, 1987 - Naval Avionics Center
Exceptional Performance, 1991 - Motorola GSTG, CSO
Engineering Award, 1993 - Motorola GSTG
Title: Software Engineer
Degrees: M.S. in Computer Science; M.B.A. in Management
Years of Experience: 33 Years
Responsibilities: Developing software for a medium size aerospace project.
Other Pertinent Information: A retired Air Force Lieutenant Colonel.
Title: Chief Software Engineer
Degrees: B.S. in Electrical Engineering
Years of Experience: 39 Years
Responsibilities: Software Process and Quality
Special Awards: Dan Nobel Fellow Award (Motorola); IEEE contributor
Other Pertinent Information:
Author: Motorola GED's Software Process Manual.
Instructor: Motorola GED Project Leader Training.
Chair: Motorola GED Software Metrics Working Group.
Chair: Motorola GED Software Training Program Working Group.
Member: Motorola GED Software Defect Prevention Working Group.
Three Patents.
Developing software fault occurrence and prediction model.
Title: Engineering Metrics Manager
Degrees: B.S. in Electrical Engineering
Years of Experience: 35 Years
Responsibilities: Coordinate collection and analysis of Engineering metrics.

Overall, the tool fared extremely well against the
evaluation process and the experts were quite pleased
with the results. There were two particularly exciting
highlights that occurred during the reviews with
expert #3 and expert #5 that are worth noting.

The first highlight was seeing the results from
simulating expert #3's Man-Machine Interface project,
which was 80% completed. Before simulating the
project, some actual historical data along with some
remaining estimated data to complete the ManMachine Interface project was entered into the
simulator. After simulating the project with actual

and estimated data,
Currently
the simulator tracked
there is an updated
Number of increments
3
within a couple of
version of the
Productivity
percentage points of
simulator tool that
Engineer 1 (domain inexperienced)
.8
expert #3's actual and
has been
Engineer 2 (domain experienced)
1.0
predicted results.
incorporated into a
Engineer 3 (very domain
1.2
The second
corporate training
experienced)
highlight was
program. The
Defect generation rates
simulating three sets
simulator is being
Engineer 1 (domain inexperienced)
.0300/hr
of project data
used to
Engineer 2 (domain experienced)
.0250/hr
supplied by expert #5.
demonstrate an
Engineer 3 (very domain
.0225/hr
He generated these
incremental
experienced)
three scenarios with
development
Defect detection
the aid of the
approach over a
% of defects found in peer reviews
80%
COCOMO tool for
waterfall
% of defects found in integration test
20%
the three engineering
development
Percent of schedule allocated to rework
10%
activities: Detail
approach, peer
Defect removal costs
Design, Code and
review
Found in peer reviews
2 hr/defect
Unit Test, and
effectiveness,
Found in integration testing
10 hr/defect
Integration. After
schedule
simulating the three
compression
scenarios, expert #5 concurred that the simulator
concept, mythical man-month perception, and the
produced believable results for all three scenarios [14].
90% completion syndrome.
Description of the experiment
Attrition is clearly detrimental to a software
development project. When an experienced developer
leaves a project prior to its completion, then a project
manager faces the question of whether to replace the
departing individual or to forego the expense of
replacement and make other adjustments, such
dropping functionality, slipping the schedule, and
rearranging assignments. The answer to the question
of replacement may turn on factors such as the
experience of the developer, the percent completion of
the project, the number of engineers on the project, the

time required for hiring or transfer, and the attrition
rate of the organization.
Replacement is costly, but may be required to
keep a project on schedule. It leads one to wonder,
can the dilemma presented by attrition be resolved by
yet another alternative, that of staffing a project with
more than the necessary number of development
engineers? Are there project situations in which it is
economically feasible, or even desirable, to mitigate
the risk of attrition by overstaffing? The experiment
described here was motivated by these questions and
the results offer an indication of the desirability of
staffing policies which include the overstaffing option.

Table 2. Model Inputs

Number of 1:
engineers
Total Staff Load
Time of1: attrition50.00
Attrition2:rate 2932.12

Low Value
10 2: Total Cost
At the end of increment 1
10%

High Value
20
At the end of increment 2
30%
2

1:
2:

25.00
1466.06

1

2
1

1
1:
2:

1

2

2

0.00
0.00
0.00

Budget: Page 1

500.00

1000.00

1500.00

2000.00

Hours

Figure 2. Sample Output for Staff Loading and Total Cost
were used with corresponding productivity factors
The experiment was conducted for an incremental
(Table 2). The departing engineers were assumed to
project, consisting of three equal and non-overlapping
be very experienced, while the replacements were
increments, with a total effort estimate of 9000 personassumed to be inexperienced. The learning rate is
hours. This is roughly equivalent to ten engineers
such that inexperienced and experienced engineers
completing an increment every eight weeks. Three
advance one experience level with each increment
levels of application domain experience
completed, until they become very experienced.
(inexperienced, experienced, and very experienced)
〈 Time in the project at which attrition occurs
Three strategies were considered:
〈 Attrition rate
〈 No replacement after attrition
The model’s response variables recorded for this
〈 Replace engineers as they leave
experiment were:
〈 Overstaff at the beginning of the project at the
〈 Project duration relative to the estimated schedule
same level as the attrition
〈 Project cost
These strategies were considered for combinations
〈 Rework cost (hrs)
of three factors:
Each strategy was evaluated for two values of each
〈 Number of engineers on the project
of the three factors (Table 3).

Table 3. Factors for Studying Staffing Strategies for Attrition

Time of
Attrition
(End of
Number of
Run Engineers Increment)
1
2
3
4
5
6
7
8
9
10
11
12
13
14
15

10
10
10
10
10
10
10
10
10
20
20
20
20
20
20

1
1
1
1
1
1
2
2
2
1
1
1
2
2
2

Attrition
Rate

Action:
Replace
(Y/N) or
Overstaff

Duration
Relative to
Estimated
Schedule

10
10
10
30
30
30
30
30
30
10
10
10
10
10
10

N
Y
O
N
Y
O
N
Y
O
N
Y
O
N
Y
O

1.00
.97
.96
1.09
.98
.95
1.04
.98
.91
1.10
1.08
1.08
1.09
1.09
1.08

Project
Cost
($1000)
675.10
683.68
701.10
558.30
646.32
692.17
641.10
670.57
738.58
682.70
705.32
741.60
713.90
716.11
772.40

Rework
Cost (hr)
771
789
822
724
816
902
778
789
918
833
859
924
851
851
940

Experimental results
The data in Table 4 reveals the trends found in the simulation results.
Table 4. Results from Simulating Staffing Strategies for Attrition
When the number of engineers working on a
project is small and attrition rate is low, the effect of
attrition is not very visible. For example, consider
Runs 1, 2, and 3 in Table 4. In this case, the number
of engineers is at the low level (10) and the attrition
rate is also at the low level (10%). The results of Run
1 indicate that the project is completed on time
without replacement or overstaffing. The fact that the
project completes early or on time in all three of these
runs suggests that attrition does not have a significant
effect if the number of engineers is low and the
attrition rate is low.
As both the number of engineers working on a
project and the attrition rate increase, the effects of
attrition becomes more visible. Consider Runs 4, 5,
and 6 in Table 4. In Run 4, the estimated schedule
is overrun by 9%, but Runs 5 and 6 show early
completion. Also, the effects of attrition are more
visible when attrition occurs at the end of increment 1
than when it occurs at the end of increment 2. In Run
4 when attrition occurs at the end of increment 1, the
project overruns the schedule by 9%, whereas in Run
7 when it occurs at the end of increment 2, the

schedule overrun is only 4%. Also, in Run 6, the
schedule underrun is 5%, whereas in Run 9, it is 9%.
These runs also indicate that attrition costs more
when it occurs later in the project.
The results in Table 4 may also be viewed as an
evaluation of three staffing strategies for attrition in
five sets of the three factors. Each set is comprised of
(number of engineers, increment number ending at
time of attrition, and attrition rate). Because we are
considering three factors in this experiment, the
orthogonal relationships of these five sets of factors are
illustrated in the cube of Figure 3.
When the staffing strategy for attrition is compared
within each set of factors, desirable strategies can be
identified based on cost and project duration against
the estimated schedule. These results are summarized
in Figure 4, which adds the desirable strategies to the
cube of Figure 3. In Figure 4, N represents the
strategy of no replacement and no overstaffing, R
represents the replacement strategy, and O represents
the overstaffing strategy. In some situations either of
two strategies is desirable, depending on whether one
wishes to minimize cost or complete the project on

schedule (or in the
N for cost
(20,2,10)
case,
(10,2,30)
minimize
the
R for schedule
Increment no.
schedule overrun).
For
e x a m p l e , completed
point (10,1,10) in at time of
Figure 4 represents a attrition
N for cost
(20,2,10)
project in which the
O for schedule
ratio of estimated
effort to the number of
Attrition rate
engineers is high (say
finish about 3%
(10,1,30)
early but will add
N for cost
about 1% to the cost
R for schedule
of the project.
Overstaffing
to
anticipate
this
(10,1,10)
attrition will allow
(20,1,10) R
N
No. of engineers
the
project
to
complete about 4%
early and will increase the project cost about 4%. The

As another example, in the case of 20 engineers on
the project and two people leave at the end of
increment 2 (point (20,2,10) in Figure 4), other
schedule regardless of the chosen course of action,
but overstaffing (O) will limit the overrun. Neither

900 hours) and one
person leaves at the
end of Increment 1.
In this scenario, the
project can finish
on-time without
replacing
the
person. Replacing
the person will
allow the project to
most desirable
strategy for this set
of factors, then, is
no action (N)
because it allows
the project to finish
on time at the
lowest cost.

strategies are indicated. The project will overrun the
estimated
replacing nor overstaffing (N) remains the best option
for minimizing cost.

Figure 3. Relationships of Factor Sets in Table 2.

Figure 4. Desirable Staffing Strategies for Attrition under Each Set of Factors
most influential factors for modeling software
dynamics and, more importantly, their degrees of
Conclusions and Future Research
influence on project outcomes.
Our research grew out of the concern for having
strategies to address attrition in software development
organizations. We also wished to examine the ability
of a more abstract simulator, composed of four process
feedback loops, to provide realistic data for supporting
the formulation of staffing policies.
This experiment suggests several implications for
software project staffing in response to attrition. In
general, no action for attrition is the least expensive
choice and overstaffing is the most expensive choice.
The choice of no action is indicated when schedule
pressure is high and cost containment is a priority.
The replacement strategy has the advantage of
alleviating the exhaustion rate and concomitant
increases in attrition. Even though overstaffing is the
more expensive option, it can have the very desirable
effect of minimizing project duration. Thus, this
strategy should be considered for projects in which
completion date has been identified as the highest
priority. Such a circumstance might occur if delivery
of a commercial application is required during a
market "window of opportunity," or if the contract for
a custom application has a penalty attached to late
delivery. The cost of overstaffing must be weighed
with the other factors in the project and process
simulation can provide data for this decision.
Regarding process modeling objectives and the use
of SDM, this experiment has demonstrated the use of
a software development process simulator, one which
models the major dynamic influences in a project.
The software practitioners who examined the results
produced by this experiment found them to be both
acceptable and realistic. This model is capable of
supporting designed experimentation and providing
data for supporting staffing decisions.
This group continues to research in software
development process simulation. In addition to the
questions of attrition, the group continues
investigating into questions related to project
management training, project risk assessment, and
product quality. In terms of process modeling issues,
further research continues to identify and validate the

References
[1] Abdel-Hamid, Tarek, A Study of Staff Turnover,
"Acquisition, and Assimilation and Their Impact on
Software Development Cost and Schedule", Journal of
Manager Information Systems, Summer 1989, vol. 6, no.
1, pp. 21-40.
[2] Abdel-Hamid, Tarek and Stuart E. Madnick, Software
Project Dynamics An Integrated Approach, PrenticeHall, Englewood Cliffs, New Jersey, 1991.
[3] Abdel-Hamid, Tarek, "Thinking in Circles", American
Programmer, May 1993, pp. 3-9.
[4] Bevilaqua, Richard J. and D.E. Thornhill, "Process
Modeling", American Programmer, May 1992, pp. 3-9.
[5] Collofello, James S., J. Tvedt, Z. Yang, D. Merrill, and
I. Rus, "Modeling Software Testing Processes",
Proceedings of Computer Software and Applications
Conference (CompSAC'95), 1995.
[6] Curtis, Bill, M. I. Kellner and J. Over, "Process
Modeling", Communications of the ACM, 35(9), Sept.
1992, pp. 75-90.
[7]ithink Manual, High Performance Systems, Inc.,
Hanover, NH, 1994.
[8] Lin, Chi Y., "Walking on Battlefields: Tools for
Strategic Software Management", American Programmer,
May, 1993, pp. 34-39.
[9] Madachy Raymond, "System Dynamics Modeling of
an Inspection-based Process", Proceedings of the
Eighteenth International Conference on Software
Engineering, Berlin, Germany, March 1996.

[10] Richardson, George P. and Alexander L. Pugh III,
Introduction to System Dynamics Modeling with
DYNAMO, The M.I.T. Press, Cambridge, MA, 1981.
[11] Richmond, Barry, "System Dynamics/Systems
Thinking: Let's Just Get On With It", International
System Dynamics Conference, Sterling, Scotland, 1994.
[12] Rubin, Howard A., M. Johnson and Ed Yourdon,
"With the SEI as My Copilot Using Software Process
"Flight Simulation" to Predict the Impact of Improvements
in Process Maturity", American Programmer, September
1994, pp. 50-57.
[13] Smith, Bradley J, N. Nguyen and R. F. Vidale, "Death
of a Software Manager: How to avoid Career Suicide

through Dynamic Software Process Modeling", American
Programmer, May 1993, pp. 11-17.
[14] Sycamore, Douglas M., Improving Software Project
Management Through System Dynamics Modeling,
Master of Science Thesis, Arizona State University, 1996.
[15] Tvedt, John D., An Extensible Model for Evaluating
the Impact of Process Improvements on Software
Development Cycle Time, Ph.D. Dissertation, ASU,
1996.
[16] Beagley, S.M., “Staying the Course with the Project
Control Panel.” American Programmer (March 1994)
29-34.

ProSim'05
The 6th International Workshop on
Software Process Simulation and Modeling
14-15 May 2005
Saint Louis, Missouri, USA

ICSE 2005 Co-located Event

Editors:

Dietmar Pfahl,
David Raffo,
Ioana Rus,
Paul Wernick

Printed by Fraunhofer IRB, Stuttgart, Germany, 2005, ISBN 3-8167-6761-3
The copyright of each paper remains with the respective authors.

Table of Contents
ProSim'05 - The 6th International Workshop on
Software Process Simulation and Modeling
Message from the Steering Committee ..................................................................................... vii
Workshop Organizers ................................................................................................................. ix
Program Committee .................................................................................................................... xi
Part 1: Keynotes
Keynote 1: Estimating the Risk of Releasing Software .................................................................. 3
Brendan Murphy
Microsoft Research, UK
Keynote 2: Open Market Software Development ........................................................................... 3
David M. Weiss
Avaya Labs, USA
Keynote 3: Learning from Agile Software Development ............................................................... 3
Mary Shaw
Carnegie Mellon University, Pittsburgh, USA
Part 2: Research Papers
Section I. Process Modeling – Focus on Methodology
Model Support for Simulation-Based Training Games: From Behavioral Modeling to User
Interactions ..................................................................................................................................... 9
Gustavo Olanda Veronese, COPPE – UFRJ, Brazil
Márcio Oliveira Barros, DIA – UNIRIO, Brazil
Cláudia Maria Lima Werner, COPPE – UFRJ, Brazil
Self-Organized Development in Libre Software: a Model based on the Stigmergy Concept ...... 16
Gregorio Robles, Universidad Rey Juan Carlos, Spain
Juan Julian Merelo, Universidad de Granada, Spain
Jesus M. Gonzalez-Barahona, Universidad Rey Juan Carlos, Spain
Understanding Team Forming in Software Development ............................................................ 26
Silvia T. Acuña, Universidad Autónoma de Madrid, Spain
Marta Gómez, Universidad San Pablo-CEU, Spain
Natalia Juristo, Universidad Politécnica de Madrid, Spain
Section II. Process Modeling – Focus on Model Implementation
Modeling Recruitment and Role Migration Processes in OSSD Projects .................................... 39
Chris Jensen, UC Irvine/Institute for Software Research, USA
Walt Scacchi, UC Irvine/Institute for Software Research, USA
Size Measurement of Embedded Software System Families ....................................................... 48
Sebastian Kiebusch, University of Leipzig, Germany
Bogdan Franczyk, University of Leipzig, Germany
Andreas Speck, University of Jena, Germany

iii

Evaluating the Impact of a New Technology Using Simulation: The Case for Mining Software
Repositories .................................................................................................................................. 57
David Raffo, Portland State University, USA
Tim Menzies, Portland State University, USA
A Software Process Simulation Model of Extreme Programming ............................................... 63
Marco Melis, Universita' di Cagliari, Italy
Ivana Turnu, Universita' di Cagliari, Italy
Alessandra Cau, Universita' di Cagliari, Italy
Giulio Concas, Universita' di Cagliari, Italy
Section III. Process Simulation Modeling – Focus on Methodology
DEVS-based Software Process Simulation Modeling: Formally Specified, Modularized, and
Extensible SPSM .......................................................................................................................... 73
Keungsik Choi, Korea Advanced Institute of Science and Technology, Korea (South)
Doo-Hwan Bae, Korea Advanced Institute of Science and Technology, Korea (South)
TagGon Kim, Korea Advanced Institute of Science and Technology, Korea (South)
Towards an Agile Development Method of Software Process Simulation .................................. 83
Niniek Angkasaputra, Fraunhofer IESE, Germany
Dietmar Pfahl, Fraunhofer IESE, Germany
Section IV. Process Simulation Modeling – Focus on Model Implementation
Software Process and Business Value Modeling .......................................................................... 95
Ray Madachy, USC Center for Software Engineering and Cost Xpert Group, USA
A Software Product Line Process Simulator .............................................................................. 102
Yu Chen, Arizona State University, USA
Gerald C. Gannod, Arizona State University, USA
James S. Collofello, Arizona State University, USA
Simulation Models Applied to Game-Based Training for Software Project Managers ............. 110
Alexandre Ribeiro Dantas, COPPE – UFRJ, Brazil
Márcio de Oliveira Barros, DIA – UNIRIO, Brazil
Cláudia Maria Lima Werner, COPPE – UFRJ, Brazil
Towards Interactive Systems Usability Improvement through Simulation Modeling ................ 117
Nuria Hurtado, University of Cádiz, Spain
Mercedes Ruiz, University of Cádiz, Spain
Jesús Torres, University of Seville, Spain
Economic Analysis of Integrated Software Development and Consulting Companies ............. 126
Charbel Noujeim, University of Karlsruhe, Germany
Jörg Sandrock, University of Karlsruhe, Germany
Christof Weinhardt, University of Karlsruhe, Germany
Part 3: Experience Reports
Implementing Generalized Process Simulation Models ............................................................. 139
David Raffo, Portland State University, USA
Umanatha Nayak, Portland State University, USA
Wayne Wakeland, Portland State University, USA
Simulating Problem Report Flow in an Integration Process ...................................................... 144
Dan Houston, Honeywell, USA

iv

Part 4: Position Papers
A Conceptual Model of the Software Development Process .....................................................
Diana Kirk, University of Auckland, New Zealand
Ewan Tempero, University of Auckland, New Zealand
People Applications in Software Process Modeling and Simulation .........................................
Ray Madachy, USC Center for Software Engineering, USA
Goal-oriented Composition of Software Process Patterns ..........................................................
Jürgen Münch, Fraunhofer IESE, Germany
Towards an Interactive Simulator for Software Process Management under Uncertainty ........
Thomas Birkhoelzer, University of Applied Science, Germany
Christoph Dickmann, Siemens Medical Solutions, Germany
Juergen Vaupel, Siemens Medical Solutions, Germany
Joerg Stubenrauch, University of Applied Science, Germany
Effective Resource Allocation for Process Simulation: A Position Paper .................................
Mohammad S. Raunak, University of Massachusetts at Amherst, USA
Leon J. Osterweil, University of Massachusetts at Amherst, USA
Understanding Open Source and Agile Evolution through Qualitative Reasoning ....................
Juan C. Fernández Ramil, The Open University, UK
Andrea Capiluppi, The Open University, UK
Neil Smith, The Open University, UK
Teaching by Modeling instead of by Models .............................................................................
Thomas Birkhoelzer, University of Applied Science, Germany
Emily Oh Navarro, University of California, Irvine, USA
André van der Hoek, University of California, Irvine, USA
A Simulation Model for Global Software Development Project ................................................
David Raffo, Portland State University, USA
Siri-on Setamanit, Portland State University, USA

155

160
164
169

175

179

185

189

Author Index ............................................................................................................................. 197

v

vi

Message from the Steering Committee
We would like to welcome you to St. Louis, USA, for the 6h International Workshop on Software
Process Simulation and Modeling (ProSim’05).
Since 1998 ProSim has been a successful international workshop that has show-cased the leading
research in the Software Process Simulation and Modeling domain. Moreover, it has become one
of the regular international meetings of the Software Process community. Participants have come
from Europe, Asia, South America, North America, Africa, and Australia/New Zealand. With
respect to structure, the workshop is a combination of keynote and paper presentations, themed
sessions, and panel discussions.
The goal of this workshop is to bring together academics and practitioners interested in the area of
software process modeling and simulation and in important industrial issues related to cost
estimation and business process design. ProSim’05 continues the tradition set in previous
workshops of serving as an international forum for presenting current research themes and
applications, and for discussing various approaches to discover underlying similarities at both the
applied and theoretical levels. In particular, this workshop will solicit research dealing with both
the application of software process simulation research in addressing real-world problems, as well
as advances being made which provide the foundation for Software Process and Software Process
Simulation Modeling in the future.
We would like to thank the members of the ProSim’05 program committee, as well as the session
chairs and all who have helped to set up this conference and made the co-location with ICSE’05
possible.

Dietmar Pfahl
Fraunhofer IESE, Germany
pfahl@iese.fraunhofer.de

Ioana Rus
Fraunhofer Center, USA
irus@fc-md.umd.edu

David Raffo
Portland State University, USA
davidr@sba.pdx.edu

Paul Wernick
University of Hertfordshire, UK
P.D.Wernick@herts.ac.uk

vii

viii

Workshop Organizers
Steering Committee
Dietmar Pfahl
Fraunhofer IESE, Germany
David Raffo
Portland State University, USA
Ioana Rus
Fraunhofer Center, USA
Paul Wernick
University of Hertfordshire, UK

Web-Page Design
Siri-on Setamanit
Portland State University, USA

Proceedings Design
Niniek Angkasaputra and Stephan Thiel
Fraunhofer IESE, Germany

ix

x

Program Committee
Thomas Birkhoelzer, University of Applied Sciences Konstanz, Germany
James Collofello, Arizona State University, USA
Paolo Donzelli, University of Maryland, USA
Volker Gruhn, University of Leipzig, Germany
Ross Jeffery, UNSW and NICTA, Australia
Dan Houston, Honeywell, USA
Marc Kellner, Software Engineering Institute, CMU, USA
Marek Leszak, Lucent Technologies Bell Labs, Germany
Ray Madachy, University of Southern California, Los Angeles, USA
Bob Martin, Software Management Consulting, USA
Jürgen Münch, Fraunhofer IESE, Germany
Leon Osterweil, University of Massachusetts, USA
Dewayne Perry, University of Texas, Austin, USA
Dietmar Pfahl, Fraunhofer IESE, Germany
Antony Powell, University of York, UK
David Raffo, Portland State University, USA
Juan F. Ramil, The Open University, UK
Günther Ruhe, University of Calgary, Canada
Mercedes Ruiz Carreira, Escuela Superior de Ingenieria, Cadiz, Spain
Ioana Rus, Fraunhofer Center Maryland, USA
Walt Scacchi, University of California, Irvine, USA
Thomas Thelin, Lund University, Sweden
Paul Wernick, University of Hertfordshire, UK

xi

xii

Part 1

Keynotes

1

2

Keynote 1:
“Estimating the Risk of Releasing Software”
by Brendan Murphy, Microsoft Research, UK
Abstract:
In the ideal world all commercial software would be fault free but, unfortunately, there is no
known process and or tools that can guarantee such software. Therefore to improve the overall
quality of Windows software a number of studies have been performed to understand the
relationship between software development attributes (such as size, complexity, churn) and
subsequent failures affecting end customers. This talk will describe the results of some of these
studies, the process used to develop Windows 2003 operating system and how the results of the
studies are being fed into models to predict the risk associated with releasing versions of the
Windows operating System.

Keynote 2:
“Open Market Software Development”
by David M. Weiss, Avaya Labs, USA
Abstract:
Two critical issues in a software development organization are how work is assigned to developers
and how developers are compensated. Although these may sound like organizational issues, they
are closely linked to technical issues, especially architectural issues. A development process such
as software product line engineering, which often focuses on a common architecture for a product
line, provides an opportunity to change the usual ways that work is assigned and developers are
compensated. Open market software development is a proposal for making such a change,
allowing developers more freedom to choose their work assignments and compensating them
based on the value of their work.

Keynote 3:
“Predicting Value from Design”
by Mary Shaw, Carnegie Mellon University, Pittsburgh, USA
Abstract:
Early design decisions in software projects profoundly affect both the properties and the costs of
the eventual implementation. It is much easier and cheaper to change these decisions during
design than after implementation has yielded running code. Improvements in our ability to predict
properties of an implementation without actually inspecting the code would enable software
designers to better understand the consequences of early decisions and would facilitate
comparison of design alternatives to a degree not currently possible. This talk will discuss some
code-free predictive evaluation techniques and the challenges of harnessing them to provide a
unified framework for reasoning about the overall value that should arise from a design.

3

4

Part 2

Research Papers

5

6

Section I

Process Modeling – Focus on
Methodology

7

8

Model Support for Simulation-Based Training Games: From Behavioral
Modeling to User Interactions
Gustavo Olanda Veronese1, Marcio Oliveira Barros2, Cláudia Maria Lima Werner1
1 COPPE – UFRJ
Systems and Computer Engineering
Program
Caixa Postal 68511 - CEP 21945-970
Rio de Janeiro – RJ – Brazil
{veronese, werner}@cos.ufrj.br

2 DIA – UNIRIO
Av. Pasteur 458, Urca
Rio de Janeiro – RJ – Brazil
Voice: 55 21 2244-5613
marcio.barros@uniriotec.br

processes and integrated tools to support the
organization of development activities to build
such games. Moreover, current game development
tools, such as game templates and engines, are not
easily integrated to simulation models.
Ideally, simulation-based educational games
should be easily adapted to distinct training
demands. From a software designer perspective,
the development of adaptable training games
involves the identification of common aspects that
constitute this kind of software, the formal
representation of such aspects, and the composition
of games based on these representations (preferably
reused from formerly developed games).
Therefore, in order to reduce the effort invested
in the development of simulation-based games,
there is a need to provide formal representations for
common aspects involved in game development
and a composition technique to build games from
these representations.
In this paper, we present a framework that
supports the development of simulation-based
training games. A lightweight development
process, a set of models, and a game composition
strategy compose the framework. Currently, the
proposed models focus on the development of
single-user, two-dimensional games, where the
trainee interacts with model elements through a
graphical interface. To analyze the usefulness of
the proposed framework, a training game in the
field of software project management was
developed as a case study. The game presents a
project to the player, where he must execute
management tasks, such as controlling project
execution, staffing, and planning.
This paper is organized in three sections. The
first section comprises this motivation. Section 2
presents the fundamentals of the proposed

Abstract
Current trends in education point to an
increasing use of simulation-based environments
for learning purposes. However, the industry still
lacks well-documented techniques, models, and
processes to organize the development of
simulation-based educational games. In this paper,
we propose a framework to support the
development of educational games based on
simulation models. The framework is composed by
a lightweight process and a set of models that
emphasize (i) the separation of activities regarding
distinct aspects of a game; and (ii) the reuse of
artifacts that were built to represent such aspects
throughout game development. A software project
management training game was modeled as a case
study of the proposed approach.

1. Motivation
Simulation-based games are drawing the
attention of many researchers [3][4][5], who are
analyzing their usage as a supporting technology
for conventional methods of adult training, among
other applications. It has been observed that adults
tend to learn better if they can experiment, make
mistakes, and apply the theories that they have
learned in practical situations. Simulation-based
instructional environments allow trainees to play an
active role in the learning process, providing a
virtual representation for situations with which the
trainees can interact and apply their knowledge
without the risks associated to the real world [1].
Even though many researchers agree on the
relevance of simulation-based games to educational
purposes, there is still a lack of well-documented

9

approach. Section 3 describes the software project
management game that was modeled as case study
to evaluate the proposed framework. Finally,
section 4 presents conclusions and future
perspectives of this work.

based educational game requires building the
behavioral, story and graphical models.
The skills required to build the behavioral
model
(domain
knowledge,
mathematical
modeling, logical modeling, among others) seem
very distinct from the skills that are needed to
develop the graphical model (visualization, spatial
organization, drawing, etc). So, we assume that
several persons will be involved in the
development of an educational game. The
separation of distinct game aspects in different
models supports work distribution among the game
development team. To coordinate the work
performed by these distinct groups, we propose a
lightweight game development process.

2. Model Support for Simulation-based
Training Games
In a training game, the characters and objects
with which the trainees can interact represent
concepts of the knowledge area that is under
exploration in the educational context. These
elements can have their behavior controlled by
simulation. By behavior, we mean how these
elements respond to changes in the modeled system
and to user interactions. Educational games usually
use graphical effects to enrich their interactivity
and fantasy, trying to capture user attention and
motivation. From this view, we identify three main
aspects that should be covered while developing a
simulation-based game: a simulation component, a
story that contextualizes what is being learned, and
interaction elements.
In this paper, we propose a model-driven game
development framework that divides a training
game project into these three major aspects. A
distinct model maps each of these aspects:

Figure 1 – Overview of the proposed game
development process

(i)

a behavioral model describes how the
elements within a domain behave over time,
according to user interactions;
(ii) a story model maps the mathematical or
logical formulations presented in the
behavioral model to elements pertaining to the
game domain, such as objects and characters;
(iii) a graphical model depicts the elements
composing the story model, presenting them
to the user and controlling user interaction
with the story model.

Five steps compose the proposed process, as
presented in Figure 1. The first one corresponds to
the elaboration of a short textual description for the
desired game, describing player’s goals, domain
elements involved in the game (characters and
objects), environments where the game story takes
place, and possible states that can be assumed by
game elements.
The three central activities of the proposed
process have the objective of designing the aspects
into which the game was divided: (i) simulation
definitions; (ii) story definitions; and (iii) graphical
definitions. First, the development team describes
the simulation model. Next, they build the story
model. Finally, the team builds the graphical
model. Such models are addressed in detail in the
following subsections.
Since distinct persons tend to participate in the
development of each model, these activities can run
in parallel. However, this requires the definition of
interfaces among the model layers, to reduce
integration effort. The game textual description
supports these definitions, providing a common
view of the desired game to be shared among
developers.

The framework’s game composition strategy
receives one model for each of these aspects and
integrates their references. The composition
strategy resembles a layered architecture, where the
behavioral model simulator represents the core
layer. A second layer manages the story model,
commanding simulation steps upon the behavioral
model, capturing results calculated by the
simulator, and mapping these results to domain
level, user recognizable states for the characters
and objects that compose the story. Finally, the
third and upper layer handles user interactions,
translating user actions to story level events,
capturing story elements and presenting them to the
user. Therefore, the development of a simulation-

10

The last process step regards the composition
strategy. In this activity, the models built by the
preceding activities are integrated and executed. In
the execution phase, two roles take place: the
trainee (game player), and the facilitator, whose
main function is to elucidate trainee’s doubts and
questions.
In the following subsections, we address each of
the models used in the framework in detail. To
support the presentation, we discuss the models in
the light of our project management educational
game case study.

Class properties are positioned in its middle
section, while behavior is presented in the lower
section.

2.1 The Simulation Model
The simulation model is the core of the game. It
defines the elements that take part in the problem
domain addressed by the game and the relevant
relationships among these elements. The model
describes, through mathematical formulations, the
rules that drive each domain element behavior. The
simulation model is described in the System
Dynamics Metamodel notation [3], an extension of
the System Dynamics modeling language in which
models are described in a high level representation,
closer to object oriented concepts.
This metamodel allows one to design domain
elements as classes, each composed by properties
and behavior. Properties are relevant numeric data
about the class, while class behavior is denoted by
a set of equations describing rules of how class
instances react to changes in the model. An
instance is a specific occurrence of a class in a
model. Each class instance can have different
values for the properties defined in its class. Class
level relationship constructs define how underlying
instances are linked to each other when the model
is under execution. Although we have defined
instances at this point, the instances involved in the
game are handled in the story model.
In our project management case study, problem
domain elements include the project, its activities,
and the developers composing the project team.
Activities are classified as analysis, design, coding,
tests, and inspection activities. Relationships
among these elements include the developer
assigned to accomplish an activity, the activities
that compose a project, and the activities that must
be completed before start of another activity.
Figure 2 depicts the Developer and Activity
classes defined in a graphical notation of the
simulation model. The notation resembles UML
diagrams, though simulation model diagrams
present system dynamics related elements, such as
stocks and rates, instead of attributes and methods.
Each three-sectioned rectangle represents a class,
whose name is presented in the upper section.

Figure 2 – Simulation Model Diagram
The directional association between the Activity
and Developer classes allows a developer to be
assigned to accomplish an activity. The relationship
only allows instance from these classes to be linked
to each other, but behavior equations can use these
links to represent the behavior of developers
executing an activity. The auto-relationship named
as Precedence defines the execution order of
project activities, since it describes which activities
precede each activity.

2.2 The Story Model
The story model is built atop the simulation
model. Its major objective is to map the abstract
equations that compose the simulation model into
real world concepts that are easily recognizable by
the trainee. To accomplish this mapping, the story
model assigns descriptive attributes to each
simulation model element, such as the name and
curriculum of a developer, the details on how an
activity should be developed, among others. Such
attributes are not relevant for the simulation, that is,
they do not impact on the model behavior1, but
they are helpful to facilitate the interactions
between the trainee and the simulation model.
Instead of handling with equations, the trainee
interacts with elements that can be observed in the
problem domain (a developer with a name, an
activity with procedures for its execution, and so
on).
In addition, the interactive nature of games
required some changes on how the simulation is
conducted by the System Dynamics Metamodel
simulator. In the original simulator, interactions
between the modeler and the model are limited to
1 In case they could affect model behavior, such attributes
should be addressed in the simulation model and would not be
considered descriptive attributes.

11

an initial setup. Once model parameters are set, the
remaining simulation steps are automatically run,
without further intervention from the user.
However, trainees in educational games are
frequently adjusting the underlying model by
making decisions that concern game elements.
These decisions, such as changing the developer
assigned to an activity or the number of hours that
a developer is asked to work per day, change model
values and structure, and thus have to be accounted
for in the remaining simulation cycles. To support
structural changes in the relationships among
model elements after the simulation cycles have
started, we have changed the simulator
implementation.
The story model is responsible for mapping user
interactions with story elements into structural
changes in the simulation model. The model
associates a set of actions with each domain
element. User interactions can trigger these actions.
Each action is described as a sequence of primitive
structural operations that affect the underlying
behavioral model. Such operations include
changing the value of a property, creating a new
domain element (such as a developer or an
activity), removing an element from the model,
breaking a relationship between two elements, or
creating a new relationship among such elements.
The new simulator handles these operations by
changing the simulation model structure, storing
the values calculated in previous simulation cycles
according to the former structure, and calculating
future cycles with the new one.
A state machine controls the actions that can be
enacted upon a domain element by the trainee in a
given moment. This state machine is defined in the
story model and presents the relevant states for
each domain element that can be perceived by the
trainee. Each state is associated with a set of
actions, and state transitions are triggered by
changes in the simulation model variables.
Transitions are defined as boolean expressions
based on the value of simulation variables.

case study. It dictates how each developer will
react to changes in the simulation model in
consequence of user interactions. When the project
starts, developers are available to execute any
activity. They automatically pass to the “Idle” state
when hired. After being assigned to some activity
under execution, they pass to the “Working” state.
Depending on the developer’s exhaustion level, its
state can alternate among “Working”, “Tired” or
“Exhausted”. Developers only return to the
“Available” state if they are fired. So, three
information given by simulation variables
determine a developer’s state: allocation to the
project, exhaustion level, and association to one or
more activities under execution. Other game
elements, such as project activities, have distinct
and independent state machines.
After defining new structures, like action and
attributes, to be embedded in simulation classes,
and state machines for each class, there is a setup
step where the story modeler should initialize all
instances in the game. For example, instances of
“Activity” class are named “Analysis”, “Design”,
“Inspection”, “Coding”, and “Testing”, all of them
with distinct values for the “function points”
property.

2.3 The Graphical Model
The
graphical
model
contains
visual
representations (images and animations) for story
elements. Decorative objects (such as desks, chairs,
and pictures) are added to the game. Story level
state machines are associated to presentation level
state machines, whose goal is to reflect visually the
changes occurred in a story element. So, story-level
state transitions trigger actions in the graphical
layer, such as moving or disposing an image. By
using this strategy, it is possible, for instance, to
distinguish a tired developer from a developer that
is not exhausted: sometimes, the tired developer
will be sleeping on the desk, asking for coffee, and
so on.
Figure 4 shows all possible graphical states that
a developer can assume, when he/she is in the
working state. Some graphical states can be
represented by an animation (as listening to music)
and some can be static images (as thinking). The
switch among graphical states occurs according to
probability distribution stated in the model.
The graphical model also defines interaction
environments (such as offices, laboratories, and
rooms), which are linked by navigation paths
associated to story objects. Finally, the graphical
model maps user level interactions, such as mouse
movements or the activation of mouse buttons
upon the visual representation of a story element,
into story level actions. This completes the cascade

Figure 3 – Developers’ state machine
Figure 3 shows an example of a state machine
designed to control developer interactions in the

12

of user interactions, from mouse actions to changes
in the behavioral model structure, through the
three-layered game architecture.

bottom of the screen). Computers, chairs, desks,
printers, and telephones are just decorative objects.
In the illustrated scenario, the player is changing
the developer assigned to an activity. Activity
execution can be analyzed by looking at the board.
In this project, the two parallel threads of activity
execution represent two distinct use cases under
development. Concluded activities are marked with
“C” (analysis and design for both use cases). Balls
with triangles indicate on-going activities.
Activities marked with the encircled square are
disabled (in this case, inspections). Paused
activities, represented by parallel rectangles inside
a circle, are waiting for the conclusion of precedent
tasks.
Some relevant variables are continuously
displayed on the control panel during the game
session. The right-hand area of the control panel
shows the elapsed time since the project started and
the budget consumed until this moment. These
variables are chosen in the graphical model and
should help the player in taking decisions. The
game ends when either the project is concluded
successfully or allocated time/budget are consumed
(implying project failure).
Although the simulation model was reused from
another application, we had to adapt it to support
the story-level state machines that were required by
the Developer domain element. Basically, some
transitions in the developer’s state machine needed
to query information that was not exposed by its
related domain model class. For instance, we
included a boolean variable to inform whether the
developer was allocated to some project activity or
not. This variable allowed us to create the human
resources office, were available developers waiting
to be hired are presented during the game.
Moreover, we realized that more states, either in
the story level or in the interface would improve
user awareness about what is happening during the
game. So, the game designer has to make a tradeoff
between awareness and simulation complexity,
since each new state in the story or presentation
layers requires several transitions to be included
and mapped into the simulation model.
While developing the case study, certain
difficulties were introduced because the proposed
models are curretly developed in common text
editors: simulation models are written according to
the metamodel notation [2], and the story and
graphical models are based on a particular XML
schema. Some defects could only found during
execution. Most of these defects were traced to
syntax problems that could be found during model
development, thus informing the game designer
earlier than the test phase. We plan to prevent such
errors by developing specialized model editors with
rule verifiers.

Graphical states

45%
Story state
Working

45%

10%

Figure 4 – Graphical states for the state
“Working”

3. A Project Management Training
Game
To evaluate the usefulness of the framework and
its models, we have developed a project
management game using the proposed approach.
The game starts with a textual description of an
academic system to be developed in a given time
and with limited budget. The corresponding
simulation model is a representation of the real
system, developed and used by a Brazilian
graduation program. In the proposed problem, the
manager should use the waterfall process model.
After an initial project presentation, the manager
can see a human resources room, where developers
are available to integrate the staff. By clicking on a
developer, the player can evaluate if it is worth to
hire him, based on characteristics including skills
on some project activities and hourly cost.
Figure 5 shows a screenshot extracted during a
game session. It represents the laboratory where
developers work. The player can interact with
developers and activities (the later are shown in the
board placed on the wall), change the current
environment (by clicking in the door), and execute
generic commands in a control panel (on the

13

Figure 5 – Project Management Training Game: Laboratory Environment
simulation model. Thus, several behavioral models
for the same domain may share the same story
model. This allows the preparation of several
instructional lessons, each with a proper model
addressing a distinct issue, using the same story and
graphical layers.
Although the approach has been tested in a
specific game, an effort should be made in
performing experiments to evaluate its feasibility in
other problems. Also, future research should focus in
developing mechanisms for the semi-automated
analyses of game events occurred during training
sessions. These analyses should permit assessing the
efficacy of learning experiences based on this kind
of simulation games. They should also address the
issues that were not fully understood by the trainee
for more focused lessons.

4. Conclusions and Future Work
In this paper, we presented an approach to
organize the development of simulation-based
educational games. Such approach is based on a
lightweight development process, which is centered
in three models addressing distinct game aspects
(behavior, story, and presentation). The separation of
game concerns into these models is supposed to
allow an easier game development. The evolution of
one aspect does not directly affect the other two. For
example, designers can change characters
configuration and problem modeling by changing
story and graphical model (if necessary). No work
ought to be done in the simulation model.
We believe that one of the main benefits of the
approach is that it allows designers to focus on
domain aspects, saving development time, which
otherwise would be wasted in specific programming
level problems, such as event handling, graphics
rendering, and others. The game execution engine is
responsible for controlling all these common aspects
of simulation games. Another benefit is that the
proposed models are not dependent of a specific

References
[1] Ahdell, R. e Andresen, G. (2001). Games and
simulation in workplace e-learning. Master’s thesis,
Norwegian University of Science and Technology,
Norway.

14

[2] Barros, Márcio O.; Werner, Cláudia M.L.; Travassos,
Guilherme H. (2001) "From Models to Metamodels:
Organizing and Reusing Domain Knowledge in System
Dynamics Model Development", in Proceedings of the
19th International Conference of the System Dynamics
Society, Atlanta, USA
[3] Groβler, A. (2000). Methodological issues of using
business simulators in teaching and research, in
Proceedings of 18th International conference of The
System Dynamics Society, 18, Bergen, Norway.

[4] Oh, E., Baker, A., and van der Hoek, A. (2004).
Teaching software engineering using simulation games, in
Proceedings of the 2004 International Conference on
Simulation in Education, San Diego, California.
[5] Spector, J. M. (2000). System dynamics and interactive
learning environments: Lessons learned and implications
for the future. Simulation & Gaming, 31(4):509-516.

15

Self-Organized Development in Libre Software: a
Model based on the Stigmergy Concept
Gregorio Robles
Universidad Rey Juan Carlos
grex@gsyc.escet.urjc.es

Juan Julian Merelo
Universidad de Granada
jmerelo@geneura.ugr.es

Abstract— Libre (free, open source) software projects are
lately getting increasing attention from the research community; for instance, several studies have focused on the inner
working of some successful projects. However, there is still
little emphasis on trying to explain the landscape of libre
software development at large, maybe due to the distribution
of developers, to the (in many cases) non-compulsory nature
of their relationships, and to the extreme importance of
motivation to attract resources to a project. In this paper
we model the relationships among developers (with each
other and the projects they decide to put work in) with
the behavior of some social insects performing large-scale
works. Speciﬁcally, we apply the concept of stigmergy, which
considers that communication (by means of stimulus) does
not happen directly among entities (in our case developers),
but through changes in the environment. Stigmergy makes
an autocatalytic reaction, of the same kind that the one
observed in bazaar-like self-organized libre software projects,
possible. We will build a model based upon these ideas, test it
against quantitative data and results from previous research,
and provide results of a simulation. Our conclusion is that
the libre software development can indeed be modeled as a
stigmergic phenomenon, in terms of allocation of developers
to projects, and in the further evolution of those projects.
An important consequence of this fact is that the individual
productivity of developers would be not as important as the
total production of a community. This would mean that the
exploitation of stigmergic mechanisms would be more efﬁcient
for increasing the output of a project than actions oriented
towards increasing productivity of individuals.

Keywords: Process modeling and simulation, libre
(free/open) source software, self-organization, software
evolution
I. I NTRODUCTION
One of the most surprising characteristics of libre software development is that it does not seem to obey many
rules of the ‘classical’ software engineering development
process. There are no (or few) pre-deﬁned requirements,
no detailed design, and a lack of inter-process documentation [1]. In addition, libre software projects are not organized in a clear, predeﬁned hierarchical structure, where a
central authority shows the way to go. But despite all of
that, they are capable of delivering useful, mature and (in
many cases) high quality pieces of software. This can only
be explained by assuming they feature self-regulation and
self-organization.
Most of these circumstances have been known for many
years [2], and there has been plenty of research activity
The work of Gregorio Robles and Jesus M. Gonzalez-Barahona has
been funded in part by the European Commission, under the CALIBRE
CA, IST program, contract number 004337, by the Universidad Rey Juan
Carlos under project PPR-2004-42 and by the Spanish CICyT under
project TIN2004-07296. We thank the anonymous reviewers of the ProSim
2005 workshop for their comments and suggestions.

Jesus M. Gonzalez-Barahona
Universidad Rey Juan Carlos
jgb@gsyc.escet.urjc.es

around those topics [3]–[6]. However, there is still no good
model on how libre software is produced as a whole. Maybe
because the inherent difﬁculties of dealing with many
projects, each with different contexts, research effort has
been focused on single, usually successful, libre software
projects.
In this paper, we are interested in the complete landscape of libre software development. We propose a model
based on stigmergy, a concept that helps to explain how
some social insects perform large-scale works. Stigmergy
assumes that communication between individuals happens
through stimuli caused by changes in the environment,
and not by directly interchanging information. Stigmergy
explains how autocatalytic reactions are possible: the same
kind of behavior observed in bazaar-driven self-organized
libre software projects.
Based on this stigmergy concept, we have designed a
model for studying the evolution of the libre software
landscape, and of the projects in it. The model deals with
how developers are allocated to projects (or how developers
decide to join projects), and how that impacts on the relative
evolution of those projects. We have also implemented a
system for the simulation of the model (which has been
calibrated using data from previous studies), and veriﬁed
its output comparing its results to the results of the research
on real libre software projects.
This paper is, then, a ﬁrst step towards understanding
the social and computer-mediated interactions that yield,
as a ﬁnal product, a libre software project, and, as such,
it can be used to improve those interactions in order to
produce software products more efﬁciently, even in a nonlibre software environment.
The structure of this paper is as follows: Next, social
insects and the concept of stigmergy are presented (section
II. After that, the application of stigmergy to libre software
development is discussed in section III, including the
description of a model for libre software development;
the next section IV indicates how the model has been
veriﬁed. The next section (V) discusses the model, its
implications and its limitations. To ﬁnish the main part
of the paper, some conclusions are presented in section
VI. After that there are three appendices, detailing the
conceptual model, the implementation of a simulation for
it, and the veriﬁcation of the model comparing the results
of simulations with real data about projects.
II. S ELF - ORGANIZATION

THROUGH STIGMERGY

In the late 1950s the French biologist P.P. Grassé realized, while studying the construction of termites nests, that

16

some behaviors which lead to collective coordination were
consequence of the effects produced in the local environment by previous behaviors (usually of other termites). He
called this phenomenon stigmergy1 [7].
Grassé observed that when termites build their nest, they
start randomly, without any coordination. Once a certain
point of activity is reached in an areal, it becomes a
signiﬁcant stimulus for other termites which then start to
collaborate in that same area, leading to the construction
of the nest.
Stigmergy is observed mainly in social insects, such as
termites, ants and some kinds of spiders. Their activity
does not depend on the direct interactions with other insectworkers, but on the structure of the environment. Individual
behavior is controlled and guided by previous work, i.e.
the changes in the environment have a direct impact on
the self-organization and coordination of the colony. An
insect creates, with its activity, a structure which stimulates
other members of the colony, causing them to perform other
speciﬁc activities.
One of the ways stigmergy is observed in social insects is
by means of chemical marking, depositing pheromones in
speciﬁc places (for instance where food is found). Several
deposits in the same place have an additive effect, causing
in the end an autocatalytic reaction2. Stigmergy is, therefore, a coordination mechanism, characterized by the lack
of a priori planning and explicit or direct communication
between entities. Information exchange is done through
changes in the environment, which usually have only local
effects, and can therefore exert inﬂuence only nearby where
they were produced.
Stigmergy has already been recognized as a method for
studying cognitive processes, such as software development
by Susi [8], who compared different theoretical frameworks
for artifact-mediated collaborative activity. In the case of
software development, artifacts are on-line communication
media (wikis, mailing lists, and the source code itself).
In our study we apply the stigmergy concept to libre
software development. Although in general ant algorithms
have been used for optimization in several contexts, our
research is focused on the process itself, rather than on
optimality. The aim of this work is to verify whether
developers have, when they develop software, a behavior
comparable to that of stigmergic insects.
Just to introduce with some more detail stigmergy in
social insects, we introduce now a model for the behavior
of ants when looking for food. In the next section, a similar
model will be proposed for libre software development.
Figure 13 show the paths followed by ants in their search
for food. Ants are depicted as small points at the end of
their path. The food and the nest are signaled in both parts
of the ﬁgure. The picture on the left shows the way ants
proceeded before ﬁnding the food. Once an ant ﬁnds food,
it uses pheromones to “mark” the way to the corresponding
location.
1 From

’ (mark/sign) and ‘
’ (work)
the Greek ‘
an autocatalytic reaction a product of the reaction also acts as a
reactant, modifying the speed of the reaction
3 This ﬁgure has been taken from the book ‘Swarm Intelligence: From
Natural to Artiﬁcial Systems” by Bonabeau et al. [9]
2 In









	












Since the moment when food is found by the ﬁrst ant
to the moment shown in the picture on the right, the
pheromone path gets stronger and stronger because of the
transit of other ants (auto-catalysis), while it gets optimized
by a partially random behavior (which is explained afterwards in detail). The randomness explains why even when
there is a (possibly optimal) way to the food, there are
still some ants which follow an alternate path. This can be
observed by the existence of the other gray paths on the
right picture that are different from the main path.

Figure 1. Optimal path from the nest to the food. Ants solve this problem
by using stigmergy.

The logic of every entity (in this case an ant) involved in
the process is as follows. At the beginning, any ant looks in
the environment for information about food, i.e. smelling
pheromones that have been deposited by other ants. An
ant can follow a certain pheromone mark, depending on
whether a random function is below (or above) a given
threshold. The threshold depends of course on the intensity
of the stimulus: the more intense, the higher (or lower) the
threshold value will be, and the more likely the ant will
follow it.
Once the ant moves, it can ﬁnd food or not. If food
is found, the ant takes some of it and goes back to the
nest, secreting pheromones on its way. If no food is found,
the ant just moves on and gets new information from the
environment. In other words, the process restarts from the
beginning. The whole process can be represented as a ﬂow
chart (see ﬁgure 2).
This model assumes that the whole system is composed
of a set of ants, but that any individual is treated as a single
entity. It also assumes that the system is changing dynamically: new stimuli appears in the environment while ants
ﬁnd food and take it to their nest. Pheromones have also
an evaporation coefﬁcient, which causes old information to
become less and less important with time.
III. M ODELING LIBRE

SOFTWARE DEVELOPMENT

Before presenting the stigmergic model for libre software
development, we introduce the basic assumption that will
lead to it. We consider that developers modify the environment by producing “development activity” (i.e. source
code). The production of code is the ﬁrst step towards
building software systems, whose presence may be seen
as a stimuli for other developers to work on them. The
larger a project (in source lines of code) is, the more likely
it will be that a random developer collaborates on it. This

17

Developers are the active agents in the model. They
can be identiﬁed by their name, and their primary
characteristics are their current location in the software
map, and their technical skills and experience. Skills
and experience will depend on the application domain,
and may change in time by using or developing
software for that application domain. Developers can
move from cell to cell (moving to new application
domains) in the software map, but they can only
participate in a project in the cell where they are
located.
The dynamic part of the model is provided by developers,
as described in ﬁgure 3. The ﬂow diagram contains several
paths which depend on decisions taken by the developer.
Those decisions depend on several factors that will be
exposed in the following subsections. The high-level characteristics of the model will be presented in more detail
in Appendix I, while the implementation details of these
decisions will be discussed in Appendix II.
•

Figure 2. Stigmergic algorithm used by ants to ﬁnd the optimal way to
food from the nest.

is so because large projects get more attention, and may
target the needs of more developers (among other reasons).
With this assumption in mind, our hypothesis is that
an autocatalytic process, similar to those found in the
behavior of colonies of social insects, will be found.
Source code and activity around source code are in our
model the pheromones while libre software projects are the
environment.
The model for developers should be, in principle, more
complex than the one for ants, since the latter make use
of ‘genetic’ knowledge which is quite different in nature
to the one used by developers need to create software.
For instance, we have to consider program comprehension
aspects, such as the learning process and the acquisition
of experience. Both will be different when a developer
collaborates on a project or not.
Our model is built up on three classes: map (Map),
projects (Project) and developers (Ant). The high-level
details of these classes are:
• The map is conceptualized as a two dimensional
matrix of a given size, n, where each cell corresponds
to a kind of software. Cells contain projects and developers. The length of the map, n, is much smaller than
the number of developers, N . Any cell may contain
as many projects as have been created by developers
in it. In summary, this matrix is an abstraction of a
software map
• Projects are identiﬁed by their name, and their primary
characteristic is their size (in number of lines of
code). They contain a list of developers which may
be active or not on the project. Projects are located in
the corresponding cell, according to their application
domain and hence cannot change their position. Our
model assumes that the larger a project is, the more
stimulating it will be for developers to work on it.

Figure 3. Stigmergic algorithm used to model libre software developers
in each turn.

The working of the proposed algorithm is as follows.
Developers work in turns. The ﬁrst action in every turn is
to determine the position in the map where the developer
will be located. Once that position is known for the current
turn, it will be checked whether the developer ﬁnds (gets
to know) a project in it.
If the developer ﬁnds an already existing project, he
may collaborate on it, learning (gaining experience) and
improving his skills. The amount of work performed will
depend both on previous experience and skills, as well
as on the time devoted to it. But if the developer, even
ﬁnding a project, decides not to work on it, he may still
learn from it by using its software. In any case, learning
by using happens at a slower rate than by collaborating in
code production. On the other hand, if the developer does

18

not ﬁnd a project, he may decide to create a new one.
All these behaviors are modeled as probabilistic functions, in which a random value has to be above a threshold
that may depend on parameters that have to be calibrated
carefully. Thresholds are normalized to one and may have
maximum/minimum values to avoid that a path becomes
impossible to follow. For further details, see Appendix II.
IV. VALIDATING

AND VERIFYING THE MODEL

The parameters and thresholds that have been used in the
model have to be conﬁgured properly in order to obtain a
realistic model. Data from previous research studies, and
surveys performed on libre software developers have been
used for this purpose. So, for instance, the mean time spent
on a project by libre software developers has been taken
from several surveys, and productivity has been measured
in SLOC, in order to be comparable with the application
of the COCOMO cost estimation model [10].
Veriﬁcation of the model has been performed against
results obtained by other research studies. We can classify
these results in two sets: facts about developer contributions, and evidences about the software produced. In both
cases, with the former calibration, our model shows similar
trends to those obtained in these studies.
Regarding developer contributions we have observed
whether the model shows a mean involvement, as reported
by Hertel [11]; inequality patterns for developer contribution to projects, as found by Koch et al. [12]; whether
projects are led throughout the lifetime of the project
by several generations of core groups [13]; and whether
the number of developers in projects follows a power-law
distribution [14]. Other contribution patterns as stated by
Mockus et al. on Apache and Mozilla [15], have also been
compared with the results we have obtained.
Regarding the size of the developed software in the
model, we check whether the average size of software
projects remains constant in time, and if we obtain a
power-law for project size as some studies on GNU/Linux
distributions have thrown [16]. Finally, we look at the
software evolution patterns that our model provides and
compare it to the current state of the art [17], [18] in this
respect.
Veriﬁcation of the model has been performed against
results obtained by other research studies. We can classify
these results in two sets: facts about developer contributions, and evidences about the software produced.
Our implementation assumes that the matrix is quadratic
and that its size does not change in time. In order to see
how the size of the matrix affects the results, we have run
our simulation for several values that range from 100 cells
(10x10) to 2500 cells (50x50).
The number of developers is another dimension which
we have to set, specially as the relation matrix size/number
of developers will give an idea of the concentration of developers and probably affect the results. Thus, we simulated
with values for number of developers that go from 50 to
10,000. The number of turns (weeks) was set to 750, around
15 years.
In the following subsections, we will compare the results
thrown by our simulation and compare it to evidences from
previous research studies.

A. Mean time per week
Hertel et al. [11] report from a survey made to 141 Linux
kernel developers that the mean time these developers
devote to developing libre software is in the mean 12.37
hours per week.
For all the values for the matrix size and the number
of developers with which we want to validate our model,
the mean time devoted to development laid between 11.4
hours/week and 11.9 hours/week. The statistical mean for
probability function of time is 11.71 hours/week.
B. Number of developers and age of a project
Krishnamurthy [22] made an analysis of around 100
mature libre software projects. He stated that the number
of developers working on a project was correlated to the
age of the project. An analysis of our simulation shows that
this is a common trend.
C. Distribution of work
Mockus et. al [15] reported that certain libre software
projects as Apache and Mozilla rely on a small group
of developers (called the ‘core’) who control the code
base. This group is responsible for around 80% of the
contributions. Other research studies have shown this trend
for other projects, as for instance Koch for GNOME [12].
From a general point of view, Ghosh et al. found that this
happens to be in general for all libre software [23], [24].
Devs
50
50
75
75
100
100
100
500
5000

Map Size
100
2500
100
2500
100
625
2500
100
100

Mean
0.74
0.53
0.73
0.50
0.75
0.62
0.5
0.70
0.69

2nd Quintile
0.71
0.44
0.71
0.40
0.72
0.56
0.41
0.69
0.64

4th Quintile
0.91
0.82
0.91
0.80
0.91
0.86
0.81
0.91
0.88

Table I
G INI C OEFFICIENT FOR THE PROJECTS

A way of measuring the inequality in contributions is
by using a classical index used in economics for wealth
distribution: the Gini coefﬁcient [25] which is tightly related to the Pareto distribution. Table I gives the results
from our simulation for the various parameters we have
used (“Devs” being the number of developers and “Map
Size” the number of cells). The mean of Gini coefﬁcients
goes from 0.5 for a setting where there is a low developer
concentration to values around 0.75 similar to the ones
reported for SourceForge projects4 . The table shows also
values for the second and fourth quintile to give insight
about the distribution of data. It can be observed that the
distribution is always skewed to values nearer to 1.
D. Power-law of contributors
The distribution of developers among projects has been
reported to follow a power-law in a study performed on
more than 50,000 projects [14].
4 See

19

http://libresoft.urjc.es:9080/libresoft/25

Figure 4 is the result of plotting the results from our
model for several runs (the ones done with 100 cells). On
the vertical axis we can see the number of developers (in
log scale) while the horizontal axis (also in log scale) gives
the projects.
10
5000 developers 100 slots
1000 developers 100 slots
500 developers 100 slots
100 developers 100 slots
75 developers 100 slots
50 developers 100 slots

possibly due to the same issues as discussed in the previous
subsection.
F. Mean size remains constant
Another ecology study performed on GNU/Linux distributions in time threw as results that the mean size of the
packages remained constant in time [16].
We haven’t observed in the simulations that the mean
size of the projects remain constant in time for any of the
parameters considered. In our model, what remains constant
for all the cases considered is surprisingly the median.
But the mean size has a tendency to grow even if new
projects are created by developers. This could imply that
the creation factor is too high and too few projects are
created in our simulation or that having an existing project
in a cell makes it very difﬁcult to have other projects there
too.
G. Initial size

1
1

10

Figure 4.

100

1000

Number of developers in log-log scale.

The ﬁgure shows that for all curves we have got two
different zones, both with the shape of a power-law (linear
in log-log scale). Interesting is the existence of a breaking
point between them, meaning that there exists a vacuum of
projects with an intermediate number of developers in our
model. Once projects achieve certain size, they accelerate
themselves enormously. This, of course, does not comply
to any study performed on libre software, at least not in this
way throwing out such a gap. It seems that this effect in the
simulation depends on the concentration of developers, so
probably letting the size of the software map grow in time
may make this curve grow smoothly as a unique power-law.
E. Power-law of project sizes
Gonzalez-Barahona et al. demonstrated from a study on
a GNU/Linux distribution that the sizes of the projects also
follow a power-law curve [26].

We also wanted to check if the assumption that the
ﬁrst contribution should be statistically bigger in terms of
SLOC is meaningful or not. Therefore we looked at the
sizes of packages for the Debian GNU/Linux distribution
in time [16]. In Debian 2.0 168 out of 1096 (15%), in
Debian 2.1 217 out of 1551 (14%), in Debian 2.2 446 out
of 2611 (17%) and in Debian 3.0 739 out of 4579 (16%)
are under our mean initial value (400 SLOC), which makes
our assumption hard to stay.
But looking at the model, the ﬁrst quintile gives for
different runs with different parameters 295 SLOC, 290
SLOC, 282 SLOC, 283 SLOC. So this result throws out
that our model is correct, or even that the 400 SLOC that
we have considered as initial mean value is too few.
H. Generations
A study performed on large libre software found that the
leading core groups identiﬁed by Mockus et al. [15] where
not stationary in time, but that generations of core groups in
time could be identiﬁed [13]. In other words, the ones that
started the project do not have to be part of it all the time
and newcomers take it over and make the project evolve.

100
5000 developers 100 slots
1000 developers 100 slots
500 developers 100 slots
100 developers 100 slots
75 developers 100 slots
50 developers 100 slots

Devs
100
100
100
50
50
50
75
75
75

10

Map
100
625
2500
100
625
2500
100
625
2500

# Proj
118
559
97
109
255
5
111
492
23

% Proj
39.6%
36.68%
2.69%
37.46%
20.88%
0.22%
38.01%
34.72%
0.77%

% Founder
4.95%
27.58%
44.83%
9.6%
40.02%
49.92%
5.95%
33.96%
57.76%

% 1st Third
54.43%
49.1%
54.9%
49.22%
54.44%
54.81%
54.68
51.78%
59.52%

Table II
L OOKING FOR GENERATIONS IN THE CORE GROUP. C ONTRIBUTION OF
THE FOUNDER AND OLDEST THIRD OF DEVELOPERS TO THE PROJECT.
1
1

Figure 5.

10

100

1000

Size of projects in log-log scale.

Figure 5 plots projects in the horizontal (logarithmic)
axis and project sizes in the vertical (logarithmic) axis.
The resulting curve is clearly power-law for all the cases
plotted with a distorting side-effect at the end of the curve

Table II gives the result of looking if we can ﬁnd such
a behavior in our model. We have therefore taken projects
that have a number of developers bigger than 6 (see column
% Proj to see how many projects are affected by this
choice) and have seen the percentage of the code that has
been contributed by the project founder (% Founder) and by

20

oldest contributors (% 1st third). The mean values that are
shown in the table reﬂect that the proportion of work made
by the founder and the oldest third is in fact important, but
by far not predominant. It should be reminded that a stable
core group from the beginnings would give values above
80% as discussed in previous subsections.
I. Software growth
There have been a few studies that have investigated
the system growth in libre software applications, being
specially signiﬁcant the one performed by Godfrey and Tu
on the Linux kernel which showed that it threw a super-liner
growth [17] apparently breaking one of Lehman’s Laws on
Software Evolution [18].

B. Limits

300000

250000

200000

150000

100000

50000

0
150

Figure 6.

getting help from peers is more likely, and expectations
about success (joining a project which is already a success)
are higher.
Probably this is also the reason for the high rate of
abandoned or one-developer projects. If a project is small,
it is going to have a hard time attracting new developers,
which means that no new resources are available for development, that growth is slow or that visibility will also be
small and that in turn makes the project not more attractive
to developers... On the contrary, if a project reaches a
certain size, more and more people know about it, and will
eventually consider joining it. This is perfectly consistent
with the power-law distribution found in the size of libre
software projects and the rich-get-richer effect, for instance.

200

250

300

350

400

450

500

Growth in number of SLOC for some projects.

Figure 6 shows that for the largest projects from one of
our analysis, no such super-liner growth can be identiﬁed.
At most, projects show a linear rate with some trends of
super-linearity for some time. Of course, in this case the
number of case studies in the libre software world allows
us not to make conclusions about if super-linearity is a
common trend or only achievable by Linux because of its
special circumstances as Godfrey points out [17].
V. D ISCUSSION
After presenting the model, we will discuss in this
section some of its possible uses, its limits, and some other
general considerations about its implications in the software
engineering ﬁeld.
A. Development attracts developers
The effect that is in the core of our model could be
summarized as “development attracts developers”. This is
by no means a strange factor of social interactions: activity
is usually an attractor for more activity. This helps to
explain from the growth of cities to the success (or not)
of cafeterias in a given area. In our case, the attraction by
successful libre software projects is clear. Many developers
prefer to collaborate in a mature, large and well known
project than in a small, hardly known, obscure one. This is
reasonable from many points of view. It is more likely that
the developer get knowledge about a well-known project,
his expectations about becoming known himself are higher,
it is more likely that he has some kind of relationship
with a current developer (since there are more of them),

One of the ﬁrst questions that arises when presenting a
new model is under what conditions it fails to be reasonable
or plausible. In this regard, we have to highlight that the
main drawback of our model is that it considers no direct
communication mechanisms between developers, but this
kind of communication does exist in real libre software
projects, and is in fact quite important. However, it is also
important to notice that the model matches quite well real
data, as is shown in appendices. Which would mean, from
other point of view, that maybe that direct communication
channels have little inﬂuence on how developers choose
projects, and on how the evolution of human resources
available to a project shapes its evolution.
Going further in the limits of the model, it is convenient
to notice that there are many factors, very important in
the low-level understanding of a project, which are not
considered. Some examples are those related to the “external relations” of the project: documentation, blogs, support
sites, web sites, etc. All of them have a clear impact on the
visibility of the project, and therefore on the attraction of
new developers. However, they are not considered in the
model.
C. Applicability in proprietary domains
Software development goes beyond libre software and as
such it would be interesting to study whether this model is
applicable to non-libre (proprietary) software development
environments.
Since our model emphasizes the availability of the source
code, and in fact considers it as an attraction factor, the
case of proprietary software is basically not considered
at all at least for traditional in-house developments in
companies. In this sense, the model may be helpful to
explain how “development communities” work for this
kind of companies, although they are usually built upon
partnership contracts among companies, many similarities
could be observed.
But some parts of the model could be used to understand
and even to estimate whether it could be convenient to
distribute as libre software some system when a given
company wants to quickly promote a large development
community around it. Besides the explicit consideration
that the software company should try to attract developers to enter an autocatalytic reaction, other questions
could be simulated with this model. In this regard, many

21

companies that want to invest in a software developed
by the community do not know if the effort they should
devote to the project is by hiring developers from this
community or to contract new developers that should
integrate themselves into the project. The former provide
probably over a system-wide knowledge and have already
experience and expertise on the project, while the latter
add new developers to the project (which makes the project
more attractive in our model) although they require over a
software comprehension phase to gain understanding over
the system and become productive.
In recent years, some large software companies have
started to look at the libre software phenomenon and
to adapt their strategies to it even if not releasing their
software under a libre software license. Their objective
is to leverage the libre software development process as
described in this paper (autocatalytic process, to beneﬁt
from external contributions, etc.) without losing the control
of the project that copyright law grants them5 . It is difﬁcult
to know the impact of the license used by a software project
on the willingness of new developers to join in. In any case,
licenses have different implications which can be seen as
positive or negative for different developers, which in our
model could increase or decrease their particular thresholds
to join a project.
D. Social and cultural factors
Another limitation that our model has is that we consider
developers being too homogeneous. In this sense we only
take into account skills for characterizing developers and
hope that the statistical considerations introduced in the
model will make heterogeneity arise among developers and
the model more realistic. However, some other factors such
as geography or cultural afﬁnities could also be important
for the selection of a project to join in and of course the
barrier of entry for a project will be different depending
on these factors. For instance, it seems reasonable that if
there is a large group of developers of a certain project in
a geographical area, new developers in that area are more
likely to join that one. Or that language could impose barriers for entering a give project. However, we do not know
about studies which show whether this actually happen.
Our model is global in that sense: any developer with a
certain set of skills is equally likely to join a given project,
with independence of where he lives, which languages he
speaks, or any other peculiarity he may have.
Considering after the discussion in the previous paragraph, that the pool of developers available in a certain
domain is global, the model will only work when there is
a certain number of them. If there are too few, no stigmergic
process is possible. We have not studied that lower bound,
but it is probably above the usual number of developers in a
given domain that most companies have hired. This would
be another reason why these stigmergic processes does not
happen in proprietary in-house software environments.
E. Stigmergy, the bazaar and the cathedral
The classical model for libre software compares it to a
bazaar as there is a lack of formal rules, hierarchies and
5 This is the case, for instance, of Microsoft’s Shared Source Initiative
or the Java Community created around Java by SUN.

mechanisms for “imposing” what an individual developer
will do. In opposition, we ﬁnd the traditional way software
is developed where processes, roles and tasks are speciﬁed
and which has been compared to the way cathedrals were
built centuries ago [2].
If we consider our stigmergy model in the context of
the bazaar, it shows some common properties. The model
behaves in a certain way as a bazaar, with no formal rules,
no formal hierarchy, and no task-imposition mechanisms
(in our case, which project a developer joins). However,
our model is more appropriate than the bazaar model when
comparing it to the cathedral. This is because although the
bazaar model explains concisely the exchange of code and
knowledge in a non-controlled environment, its analogy
lacks of a very important element: a ﬁnal (tangible) artifact
produced by all those interactions as we have in the case
of software (its source code). This is not the case for the
cathedral model as at the end of the process we have,
obviously, a cathedral.
And that is precisely the reason why we think our
model is a better analogy for the development in libre
software environments as stigmergy may also lead social
insects to build complex structures. This is the case for
termite nests which could in fact be considered literally
as cathedrals made of clay. This means that with the
stigmergic model we have a complete analogy that allows
to compare the construction of cathedrals as it was done by
humans several centuries ago (and which follows process
similar to traditional in-house software development) with
the construction of termite nests (which obeys to behaviors
that can be found in the development of libre software
systems). In addition, our model illustrates that developers
need to be stimulated in order to make such a development
process happen while the bazaar analogy does not consider
how the bazaar comes to be.
In other words, our model explains how even if the ﬁnal
product is the same (cathedrals, ie. a software product), the
process that has made it possible is completely different.
It is actually not about what is produced but how it is
produced.
F. Information hiding
Finally, we cannot help but enter into a historical debate
about how information should be managed in software
development processes: the classical Brooks [19] vs. Parnas [20] argument about information hiding. The latter
proposed in the early seventies that there should be some
design and implementation decisions that software developers should hide in one place from the rest of the program
which was criticized by the former.
Our model is certainly more oriented towards Brooks’
position. Information (source code) should not be hidden
as it is the stimulus that makes an autocatalytic reaction
possible and the whole software development phenomenon
successful.
Interestingly enough, the 20th anniversary edition of the
“Mythical Man-Month” Brooks states that this was one of
the seldom errors in the original version. Probably from
the pure software development methodology point of view,
information hiding is more appropriate and is a practice
that should be followed. But, as we can see from the model

22

presented in this paper, software engineering sometimes has
to consider other factors as development does not happen
in a vacuum. Software projects that intend to be based on
stigmergy should manage information hiding in a balanced
way to assure software quality and developer attraction at
the same time.
VI. C ONCLUSIONS
This paper is a ﬁrst step towards understanding the
social and computer-mediated interactions that yield, as a
ﬁnal product, a libre software project. We have therefore
proposed and veriﬁed an analogy with how social insects
perform large-scale works by means of indirect communication entities that act as stimulants.
Our primary conclusion is that libre software development can indeed be modeled as a stigmergic one, at
least with respect to how developer effort is allocated to
projects, and to how this affects in the evolution of projects
themselves. The model we have proposed shows patterns
similar to those reported in real world studies on libre
software, although ﬁner calibration and further discussion
on the variables and threshold values is necessary. This
means, for instance, that the individual productivity may
not be as important as the total production as a community;
or that stigmergic mechanisms could be used in order to
increase productivity at the project level.
On the other hand, our model raises a set of interesting
questions the we have brieﬂy discussed. We have seen that
development attracts developers and that this activates a
rich-get-richer effect which makes successful projects even
more successful and attractive to new developers. We then
have discussed the limitations that our model exhibits and
have presented our doubts about if this a stigmergic process
could be applicable in proprietary environments because
of the lack of critical mass and the fact that developers
may have different threshold values for more restrictive
licenses. Finally, we have indicated how stigmergy gives a
clearer analogy of some of the factors that characterize libre
software development than analogies that are commonly
used and we have discussed how our model ﬁts into the
classical information hiding debate.
Future research could focus on studying how the modiﬁcation of some of the parameters may affect the development of software in the libre software world. This could
be the case of a company wanting to hire developers for a
libre software project. An interesting extension could be to
study how other ‘communities’ present in libre software
development (i.e. translators, documenters, etc.) can be
included in a model like the one presented.
R EFERENCES
[1] W. Scacchi, “Free and open source development practices in the
game community.” IEEE Software, vol. 21, no. 1, pp. 59–66, 2004.
[2] E. S. Raymond, “The cathedral and the bazar,” First Monday, vol. 3,
no. 3, 2000.
[3] R. Ferenc, I. Siket, and T. Gyimóthy, “Extracting facts from open
source software.” in Proceedings of the International Conference in
Software Maintenance, Chicago, IL, USA, 2004, pp. 60–69.
[4] I. Samoladas, I. Stamelos, L. Angelis, and A. Oikonomou, “Open
source software development should strive for even greater code
maintainability,” Communications of the ACM, vol. 47, no. 10,
October 244.

[5] J. W. Paulson, G. Succi, and A. Eberlein, “An empirical study of
open-source and closed-source software products,” Transactions on
Software Engineering, vol. 30, no. 4, April 2004.
[6] D. German, “An empirical study of ﬁne-grained software modiﬁcations,” in Proceedings of the International Conference in Software
Maintenance, Chicago, IL, USA, 2004.
[7] P.-P. Grassé, “La reconstruction du nid et les coordinations interindividuelles chez bellicositermes natalensis et cubitermes sp. la
théorie de la stigmergie: Essai d’interpretation du comportement des
termites constructeurs.” Insectes Sociaux, no. 6, pp. 41–81, 1959.
[8] T. Susi and T. Ziemke, “Social cognition, artefacts and stigmergy: A
comparative analysis of theoretical frameworks for the understanding
of artefact-mediated collaborative activity,” Journal of Cognitive
Systems Research, no. 2, pp. 273–290, 2001.
[9] E. Bonabeau, M. Dorigo, and G. Theraulaz, Swarm Intelligence:
From Natural to Articial Systems. Oxford University Press, Inc.,
1999.
[10] B. Boehm, Software Engineering Economics. Prentice Hall, 1981.
[11] G. Hertel, S. Niedner, and S. Hermann, “Motivation of software
developers in open source projects: An internet-based survey of
contributors to the Linux kernel,” Research Policy, vol. 32, no. 7,
pp. 1159–1177, 2003.
[12] S. Koch and G. Schneider, “Effort, cooperation and coordination
in an open source software project: GNOME,” Information Systems
Journal, vol. 12, no. 1, pp. 27–42, 2002.
[13] J. M. Gonzalez-Barahona and G. Robles, “Unmounting the ”code
gods” assumption,” in Proceedings of the Fourth International
Conference on eXtreme Programming and Agile Processes in
Software Engineering, 2003. [Online]. Available: http://libresoft.dat.
escet.urjc.es/html/downloads/xp2003-barahona-roble%s.pdf
[14] K. Healy and A. Schussman, “The ecology of open-source software
development,” University of Arizona, USA, Tech. Rep., Jan. 2003.
[15] A. Mockus, R. T. Fielding, and J. D. Herbsleb, “Two case studies
of Open Source software development: Apache and Mozilla,” ACM
Transactions on Software Engineering and Methodology, vol. 11,
no. 3, pp. 309–346, 2002.
[16] J. M. Gonzalez-Barahona, G. Robles, M. Ortuño-Perez, L. RoderoMerino, J. Centeno-Gonzalez, V. Matellan-Olivera, E. CastroBarbero, and P. de-las Heras-Quirós, Free/Open Source Software
Development. Idea Group Inc., 2004, ch. Analyzing the anatomy of
GNU/Linux distributions: methodology and case studies (Red Hat
and Debian).
[17] M. W. Godfrey and Q. Tu, “Evolution in Open Source software:
A case study,” in Proceedings of the International Conference on
Software Maintenance (ICSM 2000), San Jose, California, 2000, pp.
131–142.
[18] M. Lehman, J. Ramil, P. Wernick, and D. Perry, “Metrics and laws of
software evolution - the nineties view,” in Proceedings of the Fourth
International Software Metrics Symposium, Albuquerque, NM, USA,
1997.
[19] F. P. Brooks, The Mythical Man-Month. New York: Addison Wesley
Longman Inc., 2001.
[20] D. L. Parnas, “On the criteria to be used in decomposing systems
into modules,” Communications of the ACM, vol. 15, pp. 1053–1058,
1972.
[21] R. A. Ghosh, R. Glott, B. Krieger, and G. Robles, “Survey of
developers (free/libre and open source software: Survey and study),”
International Institute of Infonomics. University of Maastricht, The
Netherlands, Tech. Rep., June 2002.
[22] S. Krishnamurthy, “Cave or community? an empirical examination
of 100 mature open source projects.” First Monday, vol. 7, no. 6,
2002.
[23] R. A. Ghosh, G. Robles, and R. Glott, “Software source code
survey (Free/Libre and Open Source Software: Survey and Study),”
International Institute of Infonomics. University of Maastricht, The
Netherlands, Tech. Rep., June 2002.
[24] R. A. Ghosh and V. V. Prakash, “The orbiten free software survey,”
FirstMonday, vol. 5, no. 7, May 2000.
[25] C. Gini, On the Measure of Concentration with Espacial Reference
to Income and Wealth. Cowles Commission, 1936.
[26] J. M. Gonzalez-Barahona, M. A. Ortuño Perez, P. de las
Heras Quiros, J. Centeno Gonzalez, and V. Matellan Olivera, “Counting potatoes: The size of Debian 2.2,” Upgrade Magazine, vol. II,
no. 6, pp. 60–66, Dec. 2001.

23

A PPENDIX
I. The conceptual model
In this appendix we will introduce the high-level characteristics of our model.
A. Getting a (new) position in the map
The developer gets at ﬁrst a random position in the
software map. This position may change at the beginning of
every turn. This is done following a probabilistic function
that depends on several parameters, which are explained
below.
There is a natural dullness to remain on the same
position, given by a minimum value for the threshold. There
should be a maximum possible value for the threshold,
so there should always be a chance of moving to another
places in the software map.
threshold = f (dullness, skills, experience)
Hence a developer moves to a new (random) position if:
random value(0, 1) > threshold
B. Finding a project
Once a developer is on a given position, he starts looking
for projects. Some parameters will make ﬁnding a project
more probable: being a large project, a huge number of
developers participating and the time the project exists.
All of them make the project more easy to ﬁnd, up to
a minimum threshold value that assures no project is
known by anyone anytime. We will hence assume that
the threshold depends on an initial value which will give
the probability of ﬁnding a project even being small, with
few developers and new and that will decrease as these
parameters change until a minimum value is achieved:

D. Creating a project
In this case we assume that the developer has a natural
inclination to create a new project, which depending on the
implementation will be high or low. Natural inclination will
be affected by skills and experience on other places in the
software map, specially if they are located near the current
one.
threshold = f (inclination, skills, experience)
In our model a developer creates new project if:
random value(0, 1) > threshold
E. Amount of work (code) performed
By coding a developer produces lines of code for a
project. The amount of work that is done depends on
the time that is committed to do it as well as on the
skills and experience of the developer. A developer acquires
experience by working on a project. Experience is measured
in terms of source lines of code and depends on the
application domain (i.e. the position in the software map).
SLOC = f (skills, experience, time)
F. Learning rate
Learning can be done by submitting code to a project,
or just by “using” a project. “Using” should be understood
in the advanced user way, so that possibly reading the mail
archives, submitting bugs and reading even technical documentation is comprised. By learning a developer acquires
skills.
skills = f (usingordeveloping)
The function for skills as well as the one for experience
could not be simply additive, but have also into account a
decreasing of both parameters as time passes.

threshold = f (size, developers, age)
Hence a developer knows (ﬁnds) a project if:
random value(0, 1) > threshold
C. Participating in a project
Skills and experience a developer has on a position are
going to be the parameters that affect the calculation of
the threshold value. Again we will have a maximum value
which will make it possible that even without skills and
experience a developer works on a project, as well as a
minimum value which implies that working on a project
may not happen even having high values of skills and experience. A minimum value could be selected proportional
to vacation time or similar.
threshold = f (skills, experience)
Thus a developer collaborates on a project if:
random value(0, 1) > threshold

24

A PPENDIX
II. Implementation of the model
The stigmergy model presented in this paper has been
implemented with a Python-based program called pyStigmergy6. The main implementation details are explained and
discussed in this appendix.
A. Getting a (new) position in the map
The natural dullness to remain on the same position
(used as a minimum value) has been set to 0.7, while the
maximum has been given a value of 0.99. Choosing 0.99
means that if turns are weekly, a developer would at least
change his position in the mean every two years.
random value(0, 1) > 0.7 +

skills experience
+
100
10000

The normalization values have been put to 100 for skills
and 10000 for experience. More insight about these values
(and hence their normalization) is given in subsection F.
B. Finding a project
The maximum value of the threshold for ﬁnding a project
has been set to 0.8, which in fact means that ﬁnding a
project is by default difﬁcult. The minimum value has been
set to 0.01.
Size, number of developers and age have been normalized into formulas to give number that range from 0 to
circa 6. All formulas are monotonic increasing and have the
property that they new code/developers when the project is
small affects the visibility more than when the project is
already large.
size =

ln(project size)
2
√

numDev =

project numDev
2

age = project age ∗ 0.05
Age has been taken to be linear and measured in weeks
with a factor of 0.05. Age is given by the number of turns
a project has been worked on by at least one developer,
being also a proxy for a release in the ‘release often’ libre
software fashion [2].
As we can see from the next equation, the three parameters are treated equally by summing them up and by
dividing them by a value that is dependent on the constant.
The 64 has been taken as a normalization value and is the
result of 43 , which is a sort of mean of all parameters.
random value(0, 1) > 0.8 −
6 The

size + numDev + age
(1 − 0.8) ∗ 64

program is libre software, licensed under the GNU General
Public License (GNU GPL), and can be downloaded from http://
libresoft.urjc.es.

C. Participating in a project
We suppose that the maximum value is given by a
constant that has been set to 0.5, meaning that a newcomer
has the same probability of working on the project than
not doing it. Skills and experience (properly weighted in
order to become normalized) will lower the threshold up
to a minimum value of 0.01.
skills experience
+
random value(0, 1) > 0.5 −
1000
50000
D. Creating a project
We assume that developers are not very inclined to create
a new project, so we have put a constant threshold value
of 0.8 that should be surpassed. Although the conceptual
model discusses other parameters, we have not considered
them for our implementation.
random value(0, 1) > 0.8
E. Amount of work (code) performed
The amount of work performed depends on the time
devoted to a project. Based on the FLOSS survey [21]
which was answered by over 2000 respondents, we have
chosen a probabilistic function that descends linearly from
1 hour to 40 hours. The maximum involvement is 40
hours/week. Involvement is calculated randomly from 0 to
1 and then transformed to the previous probability function
by means of the following equation:
√
time = 40 − 1600 − 1600 ∗ involvement
The production of the developer depends on its skills and
experience in this position. The maximum value for skills
* experience is 50.
Finally the worked done (measured in SLOC) is given
by the multiplication of all the parameters. As the multiplication of skills and experience as well as time have both
an upper bound, the maximum amount of SLOC that can
be produced by a developer in a single week has also an
upper limit: 2000 SLOC7 .
worked = round(skills ∗ experience ∗ time + 0.5)
Creating a project works differently. We have supposed
a random amount of code is added at ﬁrst because of the
programming structure and an initial size that the project
has to have before being released:
worked = rand(0, 1) ∗ 800
This means that we assume that contributing a new
project has in mean 400 SLOC more than a contribution to
an existing project.
F. Learning rate
If a developer works (submits code) to or uses a project,
his skills get increased following this equation:
skills = skills + f actor ∗ rand(0, 1)
where the value of the factor is 2 for developing and 0.5
for “using” the software.
7 Although this is the maximum value and it may be obtained only under
certain, very speciﬁc circumstances it appears to be very big (compared
for instance to the estimation given by the COCOMO model [10]). In
section ?? we discuss these parameters.

25

Understanding Team Forming in Software Development
Silvia T. Acuña

Marta Gómez

Natalia Juristo

Escuela Politécnica Superior
Universidad Autónoma de Madrid
silvia.acunna@uam.es

Escuela Politécnica Superior
Universidad San Pablo-CEU
mgomez.eps@ceu.es

Facultad de Informática
Universidad Politécnica de Madrid
natalia@fi.upm.es

certain task, goal or objective, engaged in frequent face-toface interaction to execute a task, while the individuals are
(mutually) interdependent on each other with regard to the
outcome of the task and its execution” (Katzenbach &
Smith, 2001).
We attempt to give an understanding of what factors influence performance differences among development teams,
arising, as a function of their composition, in terms of individual personality factors, task characteristics and team
behaviour. We have two goals: a) we characterise the two
software development strategies now in use: agile process
models and heavy-weight process models according to task
features and team behaviour aspects and b) we set out a
cross-experiment to empirically study what type of people/
people combinations (team) feel more at home with and
perform better in different software process types.
Most of the work on team forming in the field of psychology analyses the relationships between personality factors
and task characteristics (Molleman et al., 2002; Barry &
Stewart, 1997; Hackman & Oldham, 1980). Research has
also been done towards understanding the effects of multifunctionality in teams (Molleman & Slomp, 1999). It is
accepted that team performance cannot be reliably predicted from team personality composition and task characteristics alone, but rather depends on the interactive effects
of both team personality/task characteristics composition
and team climate characteristics. Teamwork is affected by
what preferences and perception the team members have as
to how the tasks should be performed within the team, that
is, of what the climate is like. Team performance depends
on the personality distribution within a team and the individual team members’ ability to handle and work in a particular type of climate.
To define team forming heuristics for the software process,
we need to establish relationships between individual factors (personality and KSA), aspects of the tasks and the
team climate to improve the performance and satisfaction
of the people working together.
Psychologists collect data to ascertain what team forming
strategies are applied in a particular domain. These studies
are non-intrusive and call for measurements to be taken of
climate, personality factors, KSA and task characteristics
before, during and after software project development. This
will provide data that could be used for exploration, conjecture and to infer correlations for the variables to be considered in software development team forming.
This paper is structured as follows. Section 2 describes related work on individual factors in the software process.
Section 3 presents approaches to team forming from the
field of social psychology. Section 4 describes the compo-

Abstract
People are a fundamental and critical concern in software
development success or failure. There is research that takes
this aspect into account and incorporates people into the
software process. This is done by analysing people individually and establishing relationships with the activities
that are performed within the project. Then again there is
agreement on the fact that these people work together to
perform interdependent development tasks, and these group
interrelationships are complex. This leads to the need to
examine software developer team forming.
This article presents on-going research into the understanding of the complex structure of software development team
forming. First, we present the components of this structure.
Second, we characterise the current two development
strategies, agile and heavy-weight, based on given team
behaviour components. Finally, we set out a crossexperiment to empirically examine the relationships between these team behaviour components and the different
types of software process. The importance of studying team
forming in software development is that if we can find out
what sociological and psychological factors improve development team performance, managers will be able to use
this knowledge to form better teams for each way of developing software.
Social psychology is now researching team forming considering a series of factors that have been found to affect
team performance. On the one hand, this work analyses
team member personality factors and knowledge, skills and
abilities (KSA) and, on the other, these analyses are interrelated with task characteristics. This is because team performance cannot be reliably predicted from team personality composition alone, but rather depends on the interactive
effects of both team personality composition and team task
characteristics. Moreover, this relationship involves other
factors, like interactions between people, including conflict,
cohesion and climate.
To be able to apply the results of this research to software
development, however, data need to be collected that indicate how people/task/team behaviour relationships influence software development team performance and satisfaction to find heuristic rules on team forming.
Keywords: Software process modelling; Agile and heavyweight software processes; Team forming; Team personality; Task characteristics; Team climate
1. INTRODUCTION
This article is about teams and the collective effort of individuals working together towards a common goal; more
specifically, teams that develop software. Here, a team is
defined as “a number of individuals brought together for a

26

Citing DeMarco and Lister (1999), “Most software development projects fail because of failures with the team running them”. We need to take a step further and examine the
development team, its interrelations and characteristics to
better ascertain what factors influence team performance in
software development.

nents, aspects and values of the team behaviour and effectiveness model. Section 5 describes research into team
forming in software development. Section 6 characterises
the two development process types: agile and heavy
weight, based on task features and team behaviour, and
describes the cross-experiment run to fit the best people/team behaviour to each software process type. Conclusions are set out in section 7.

3. PSYCHOLOGICAL STUDIES ON TEAMWORK
From research in the field of psychology into team behaviour and team forming in particular, we can say that there
are three main approaches: KSA approach, structural contingency theory and team climate inventory theory.

2.

INDIVIDUAL FACTORS IN SOFTWARE
DEVELOPMENT
People are considered as one of the most important and
critical factors within software development with regard to
project success or failure (DeMarco & Lister, 1999). In
actual fact there is widespread recognition that software
process productivity and efficiency is critically dependent
on human and social factors (Boehm et al., 2000). A number of studies have been conducted on how to bring people
into software development. Most of this research examines
the individual qualities of the people involved in the software process, considering personality factors and the competencies required according to the characteristics of the
task to be performed, etc.
Acuña and Juristo (2004) have incorporated behavioural
competencies and capabilities into the software process,
defining the capability/people and capability/role relationships within a development project. These relationships are
the basis for determining the capabilities of the development team members to then assign people to role performance, depending on their capabilities and the capabilities
required by roles. Turley and Bieman (1995) also discussed
the standard skills and personality traits of software engineers and roles in software engineering. Some studies have
used standard personality tests, such as the “Sixteen Personality Factor Questionnaire” (Moore, 1991), to define
personality profiles of application programmers, application systems analysts, technical programmers and software
services managers. Other studies, such as the one by
Wynekoop and Walz (2000), try to find the personality
traits of software engineers empirically by developing a
model of six personality traits applied to software development. Kellner et al. (1999) described how the interrelationships between the human, technical and economic aspects of software projects could be modelled and simulated.
Finally, Koontz and O’Donnell (1972) defined five basic
principles for software personnel management that provide
guidelines for improving the staffing situation of software
companies to achieve production goals.
This is research into individual factors of unquestionable
importance. Indeed, these factors are part of the process
areas of levels 2 and 3 of the People-CMM (Curtis et al.,
2001). Nevertheless, while formalised mechanisms for performing these activities are a must, they are not sufficient,
as people work together, and this team aspect has a decisive
influence on the results of software production, which is
basically a social activity.

3.1. Knowledge, Skills and Abilities (KSA)
Groups usually exist for a reason, and in organisational
teams, this reason is to accomplish an organisationally
relevant goal. The team can only accomplish this goal if the
individual members are qualified enough to contribute to
this goal. In other words, the individual members should
have the necessary KSAs to be able to work on one or more
of the tasks that need completion if the goal is to be
achieved. But task-related KSAs alone are not sufficient.
Individuals also require interpersonal skills for working
with others in a group, and knowledge of, for example, the
group’s norms such as proper behaviour. The possession of
KSAs by individuals in groups is an almost unwritten assumption, since, as an individual is in a group, he or she
must have the required KSAs for functioning in a group. It
would make little sense to have individuals in a football
team who cannot “pass the ball to a team mate”. Some
studies have been conduct towards gaining an understanding of the effects of multifunctionality in teams, that is, the
number of different tasks a worker has mastered (Molleman
& Slomp, 1999; Van den Beukel & Molleman, 2002).
3.2. Structural Contingency Theory
Structural contingency has been much researched at organisational level and is now being adapted to team level (Hollenbeck et al., 2002). This theory essentially addresses task
characteristics and their match with the team. It is a sociotechnical approach, considers both the social and the technical parts and its aim is to match the relevant questions at
team level.
Research conducted by Molleman is based precisely on this
theory, suggesting the approach of a people-to-team fit by
task characterisation (Molleman et al., 2002). This approach proposes characterising the people/task relationship.
In other words, the personality traits of the people (conscientiousness, emotional stability, openness to experience,
etc.) and the characteristics of the tasks (autonomy, interdependence, etc.) that will lead to a positive correlation in
the team results should be determined. This approach is
supported by the work of Molleman and establishes a relationship of moderation: the task characteristics moderate
the people-team fit.
Other researchers have also pointed out that the task plays a
moderator role with regard to of team member characteris-

27

Personality factors determine personal preferences, opinions, attitudes, values and characteristics. In short, everybody has a different personality topology, which is what
makes this person a different individual. Studies conducted
in the field of psychology have converged on a range of
five personality factors. These five personality factors are
covered by the Five Factor Personality Inventory, also
known as the “Big Five” (Digman, 1990; Barrick & Mount,
1991; Hendriks, 1997), which has come to be the dominant
topology in a lot of research, including the field of team
forming.
The Big Five questionnaire identifies the five fundamental
dimensions of human personality:
- Extraversion, inherent in a confident and enthusiastic view of many, mainly interpersonal, aspects of
life.
- Agreeableness, altruistic concern and emotional
support for others.
- Conscientiousness, proper to a perseverant, scrupulous and responsible behaviour.
- Emotional Stability, a broad-spectrum trait including
characteristics such as the ability to deal with the
negative effects of anxiety, depression, irritability or
frustration.
- Openness to Experience, especially intellectual
openness to new ideas, values, feelings and interests.
Research conducted so far in social psychology point to
three primary personality traits of the Big Five as being
relevant for effective team operation and results (Molleman
et al., 2002): conscientiousness, emotional stability and
openness to experience. However, we will use the five personality factors to determine which combinations should be
taken into account in agile and heavy-weight software development team forming.

tics and performance. Hackman and Oldham (1980) discussed the extent to which interpersonal skills contribute to
team performance depending on whether group tasks call
for more or less interpersonal relations. Another factor is
growth needs if the task is not routine and offers a learning
opportunity. Molleman and Slomp (1999) suggest that
some people may feel uncomfortable if the tasks are ambiguous, whereas others may consider them a challenge.
3.3. Team Climate Inventory Theory
Team climate is supported by Anderson and West’s Team
Climate Inventory (1994). There are two possible approaches for achieving the people-team fit. The first would
establish people’s climate preferences individually and the
second approach would characterise the team climate
through the perceptions of the team members. In both
cases, the technique used is the team climate inventory
questionnaire (Anderson & West, 1998; Anderson & West,
1999).
The first team climate inventory theory approach matches
people to teams according to what climate preferences they
have. It is concerned with defining the climate aspects involved in task performance and selecting people who have
preferences for the defined climate aspects. This approach
aims to select people who would feel more comfortable
within the team by investigating what preferences the person in question has by asking questions like “do you like
discussing ideas openly in the work team?”
The second team climate inventory approach fits people to
the team according to climate characterisation (Burch &
Anderson, 2004). A relationship of mediation is established, that is, what is the effect or to what extent is climate
mediating the person-team fit. In this case, team climate is
characterised by asking questions like “does the project
manager give you the chance to discuss and exchange ideas
openly, etc.?” Having determined the perceptions on climate, the preferences of each team member can then also
be analysed to fit the person to the team.

PEOPLE:
• Personality (Big Five )

PRODUCTO:
PRODUCT:

• KSA’s
-Skills -Abilities)
(Knowledge-Skills-Abilities
(Knowledge

4.

TEAM BEHAVIOUR AND EFFECTIVENESS
MODEL
Team forming and team behaviour is composed of four
basic components: a) people, b) tasks, c) team behaviour
and d) outputs. Figure 1 shows the relationships between
these components, as well as the primary aspects of each
one. These relationships are very dynamic in terms of variety and change, which makes the group structure open and
complex. Let us examine each of these components, as well
as their aspects/subaspects and associated values.

• CLIMA
(TSI)
CLIMATE
(Preferences)

TEAM BEHAVIOUR:
• CLIMATE
CLIMA (TCI)
(Perceptions)

• Calidad
Product del
Quality
producto
• Time
• Tiempo

• Conflict
• Cohesion

TASK:

PEOPLE:

• Interdependence

Satisfacción
• Satisfaction

• Autonomy
• Routine

DEVELOPMENT PROCESS

OUTPUTS

Figure 1. Team Behaviour and Effectiveness Model

Other individual factors that play a role in team forming are
individuals’ group KSAs:

4.1. People
Most of the research conducted appraises the individual
aspects of the people involved in team forming. Specifically, it analyses the required personality factors and KSAs
depending on the characteristics of the task to be performed, as well as preferences for a team climate.

-

28

Knowledge on how to work in teams and achieve a
common goal.
Skills linked to specific experiences like people’s
expertise in and strategy for relating within the
group and with other groups.

-

and vice versa. Therefore, mutual interdependence
calls for communication, cooperation and collective
decision making within the team. The possible values for team task interdependence are: Interdependence and Independence.
ii) Autonomy: this refers how much freedom a team
has to make decisions on goals (“what”), work
methods (“how”), delivery planning (“when”) and
work distribution among team members (“who”;
Molleman, 2000). The possible values for this aspect are: Autonomous and Reliant. Autonomous
teams are less controlled and their work is usually
less structured. If there are a lot of people making
decision autonomously in the team tasks, this is an
indication that the team members have a lot of freedom to develop different activities depending on
their personality (Barry & Stewart, 1997). Consequently, both team task interdependence and autonomy will moderate how the employee characteristics
are related to the team results.
iii) Routine: this refers to the level of non-routineness of
the group task. Non-routine tasks are characterised
by their uncertainty with regard to operational procedures (how to perform the task), task requirements
(what is needed to perform the task), task distribution (who works on what part of the task) and environmental demands (what is the goal/objective of
the group), as well as the likelihood of change in
these elements, and unique problems that can appear
as a result of these aspects of uncertainty (Breaugh,
1985; Evans & Fischer, 1992). The possible values
for this aspect are: Routine and Creative.

Abilities, such as the ability to reason, synthesise,
analyse, needed to achieve personal and team goals.

For the people component, another aspect that plays a role
in team forming, apart from personality and KSA aspects,
is what preferences each team member has on the work
climate, that is, what environment he or she likes to work
in. The Team Climate for Innovation Questionnaire is used
for this purpose (Anderson & West, 1998; Anderson &
West, 1999). This questionnaire can be used to select people with similar preferences and is, therefore, known by the
acronym TSI (Team Selection Inventory) (Anderson &
Burch, 2003).
The Team Climate for Innovation Questionnaire is also
used to determine the real climate in development teams.
This aspect is analysed later under the team behaviour
component discussed in section 4.3. Both aspects, preferences for a work climate and perception of the team work
climate, measure four subaspects that will also be detailed
in section 4.3.
4.2. Task
Task is understood as “a set of specifications identifying
the goal that is to be achieved and the procedures that an
individual or group may employ when attempting to
achieve it” (Steiner, 1972). Task characteristics influence
employee performance. There are several task topologies:
Steiner (1972), McGrath (1984), etc. For example, Hackman and Oldham’s topology (1980) is one of the most
widely accepted for individual employee well-being and
directed at motivating aspects of the task:
- Skill variety: the different skills necessary to accomplish a job.
- Task identity: the degree to which an individual
completes a “whole” product or piece of work rather
than just a small part.
- Task significance: the impact that the work has on
the lives of others.
- Autonomy: the independence the employee (or
group) has in planning and doing the job.
- Feedback from the job itself: the manner in which
the job provides feedback about the employee (or
group) performance.
Several pieces of research (Breaugh, 1985; Evans &
Fischer, 1992; Molleman et al., 2002; Hollenbeck et al.,
2002) have revealed that the task characteristics that influence teamwork results are: autonomy, interdependence and
routine.

4.3. Team Behaviour
Several papers (Tuckman, 1965; Steiner, 1972; McGrath,
1984; West, 1990; West & Anderson, 1996; Anderson &
West, 1998; Larson & LaFasto, 2001) point to the following team processes as being variables involved in team
forming relationships:
i) Cohesiveness: a measure for a group’s entitavity (or
groupness). It is the result of all forces acting on the
members to remain in the group. A team’s cohesion
is an indicator of its strength. It is an essential characteristic of teams, since team members are more
willing to collaborate if the ties that bind them to the
team are stronger. The possible values for this aspect are: Cohesive and Cooperative.
ii) Conflict: is a struggle, clash or state of opposition
between opposing forces, ideas or interests. This is
indicative of the number of interpersonal, intragroup
or intergroup behaviours that can be considered a
conflict. Wherever a number of individuals work together in a group towards a common goal, some
form and level of disagreement or animosity is
likely to occur, irrespective of fate. The possible

i) Interdependence: task interdependence refers to a
situation in which the process and result of one task
affects the process and result of other tasks. This interdependence is reciprocal, which means that people (employees) are mutually interdependent. Mutual interdependence means that decisions on
“what”, “how” and “when” taken by one team
member affect the decisions of his or her colleagues

29

member participation or Participative climate, as opposed to a Directed work climate.
• Support for innovation: support provided by the team
for innovative ideas. So, a teamwork climate can be defined as tending towards innovation or as an Innovative
climate, as opposed to a Conservative work climate.
• Team vision: how clearly the team defines goals. Therefore, a teamwork climate can be said to tend towards
taking a team view or Unanimous climate, as opposed
to a Compartmentalised climate.
• Task orientation: how much effort the team puts into
achieving excellence in what it does. Accordingly, a
teamwork climate can be considered to be demanding
and committed to quality or Climate for Quality and
Excellence, as opposed to a Conformist Climate.

values for this aspect are: Compulsion and Negotiation.
iii) Cooperation: willingness of an individual to put in
effort, either mental or physical, on behalf of the
group towards achieving the group’s goal, consisting of behaviour that yields maximal joint profit for
all the parties involved. Cooperation is the strength
of a team derived from pooling the efforts of a number of individuals together to work towards a common goal or task.
iv) Communication: is essential to any team, as a means
of distributing information between team members.
Communication is the process by which members
generate cohesiveness in the form of shared meaning structures.
There are many other team processes, and there is no definite list. As the focus here is on the match between people/team and the different ways of developing software, we
have selected two team behaviour aspects, namely, Cohesion and Conflict, which, according to Yang & Tang
(2004), are most closely linked to these matches as regards
better or worse team performance. Admittedly, the selection might appear to be somewhat arbitrary, and there is a
lot to be said for including other team behaviour aspects,
but we chose a limited number of team behaviour aspects
for practical reasons (length of the questionnaire and efficiency of the model).
Another team behaviour aspect that also has an impact on
the team is team climate. As mentioned earlier, team climate refers, on the one hand, to the preferences as regards
the work setting and, on the other, to the perceptions on
what “life” in the group is like. For example, trust in the
team, conflict resolution ability, that is, capability for removing barriers to goal attainment, problem-solving ability, maintenance of a productive interaction between team
members and the capability to overcome obstacles standing
in the way of team effectiveness are all elements of the
climate that mediate between people and the activities they
perform in a team and team effectiveness. To be effective
team members, therefore, people need an understanding of
internal and external team dynamics, as well as of other
group types.
The team behaviour component considers the second subaspect of team climate, that is, what perception each team
member has of the climate in which he or she is working or
has worked. The Team Climate for Innovation Questionnaire is known in this case by the acronym TCI (Team
Climate Inventory).
The climate questionnaire, as both TSI and TCI, was developed to measure four aspects considered essential for
effective team operation and propensity to innovation:
• Participative safety: how much trust that participating
team members feel within the group when explaining
their opinions and ideas. Accordingly, there can be said
to be a teamwork climate with a greater tendency to

The aspects described above and their possible values are
set out in Table 1 for each of the three components (People,
Task and Team Behaviour) of the development process.
The aspects can then be decomposed into subaspects. The
aspects referring to the task features and the real teamwork
climate will be used to characterise the two ways of developing software now in use: agile and heavy weight processes (section 6). All the aspects in Table 1 are being
measured to determine what impact they have on product
quality and team productivity, as well as on the satisfaction
of the team members. In other words, these aspects have
been included in the cross-experiment described in section
6 to empirically study what types of people and people
combinations (teams) provide a better fit for each software
process type.
4.4. Outputs
Team performance refers to the extent to which team goals,
such as productivity, delivery time and product quality or
services, are achieved. According to Hackman and Oldham
(1980), performance is also related to aspects that are not
explicitly part of the team goals, such as job satisfaction or
team feasibility. The factors that are usually measured and
assessed in the final result are: satisfaction and performance. There are two sides to team effectiveness:
• Explicit aspects or formal performance, which refer to
how the team goals are achieved (to what extent, delivery time and product and services quality).
• Non-explicit aspects, which are related to other points,
such as job satisfaction and team feasibility.
Effectiveness is manifest at both group and member level
from the viewpoint of satisfaction. Satisfaction mainly reflects the affective side of personal results. At work team
level, it refers to fulfilling social needs, as well as the aspiration to belong to the group.

30

COMPONENT

ASPECT/ SUBASPECT

Personality (Big Five)

PEOPLE

KSA
Climate (Preferences or TSI)
• Participative safety
• Support for innovation
• Team vision
• Task orientation

TASK

TEAM
BEHAVIOUR

Autonomy
Interdependency between
team tasks
Task routineness
Cohesion
Conflict
Climate (Perceptions or TCI)
• Participative safety
• Support for innovation
• Team vision
• Task orientation

We find then that there is no study that covers all the aspects that social psychology has shown to be relevant for
software development team forming. Neither do the analysed studies take into account climate, which is a factor
that is involved in effective and efficient software development team forming.

VALUES

-

Extraversion
Agreeableness
Conscientiousness
Emotional Stability
Openness to Experience
Knowledge
Skills
Abilities

6.

CHARACTERISING DIFFERENT
DEVELOPMENT STRATEGIES BASED ON
TEAM BEHAVIOUR
There are now two opposing philosophies on the software
process and, therefore, on how to work and cooperate during development: agile methods (Beck et al., 2001; Fowler,
2001; Jeffries et al., 2001) and the heavy-weight or traditional software development process models (IEEE, 1997;
ISO/IEC, 2002). Agile methods are put forward as peopleoriented adaptable models, whereas the traditional approaches are activity- and product-driven.
It is to be expected that neither approach is better than the
other in absolute terms, but one will outperform the other
under certain circumstances. These process types are not
exclusive, and there is a spectrum of process types, ranging
from the pure heavy-weight models, through mixed heavyweight (for example, small documentation and roles hierarchy preestablished by the project manager), mixed lightweight (decision-making participation but more demanding
documentation) processes, to pure agile methods.
Psychological studies of Structural Contingency Theory
defend that team performance very much depends on the
task. However, the development task features will vary
depending on whether an agile or a heavy-weight process is
in place. For example, processes application and people
assignation, documentation, etc., are more Reliant in a
heavy-weight process and more Autonomous for agile
methods.
We want to empirically study what type of people, people
combinations and team behaviours at climate level better
match each software process. For this purpose, first we
describe the heavy-weight and agile methods according to
particular development features (Table 2). Second, considering the descriptions for each of these development features (Table 2), we characterise the two process types according to the psychological team behaviour criteria discussed earlier. In particular, we consider the aspects: Interdependence, Autonomy and Routineness for the Task component and Cohesiveness, Conflict and real teamwork Climate (TCI) within the Team Behaviour component (specified in Table 1). Table 3 shows the values of the aspects
analysed for the agile and heavy-weight process models.

- Participative / Directed
- Innovative / Conservative
- Unanimous / Compartmentalised
- Quality / Conformist
- Autonomous/ Reliant
- Interdependence / Independence
- Routine/Creative
- Cohesive / Cooperative
- Compulsion / Negotiation
- Participative / Directed
- Innovative / Conservative
- Unanimous / Compartmentalised
- Quality / Conformist

Table 1. Components, aspects and values considered in software
development team forming

5.

SOFTWARE DEVELOPMENT TEAM
BEHAVIOUR RELATED WORK
There has been little research into group aspects applied to
software development. There are studies that use a standard
test, like the “Myers-Briggs Type Indicator” (Rutherford,
2001; Bostrom & Kaiser, 1981; Teague, 1998; Hardiman,
1997), to determine the guidelines for team success according to software engineer personality types. There is another
study that determines the connection between abilities and
personality traits and team performance (White & Leifer,
1986). These studies omit KSA from the individual factors
and confidence, conflict, climate, etc., from the group factors.
There are also quantitative team forming methods based on
a quantitative abilities model (Burdett & Li, 1995), but they
do not consider the softer aspects, such as team member
personality factors and actual team personality.
Another team forming method is based on analysing re
quired and available skills (Zakarian & Kusiak, 1999), but
it does not indicate how to evaluate people’s skills. Neither
does it consider task and group characteristics.
Zuser and Grechening (2003) propose the use of a questionnaire based on abilities and personality traits that provide the team with information during development on finished software projects to improve team performance. The
team is built according to the team forming phases of
Tuckman’s model: forming, storming, norming, performing
and adjourning (Tuckman, 1965). This study combines
individual personality and group factors only, and omits
KSAs and task characteristics.

31

DEVELOPMENT
FEATURES

AGILE METHODS
(Beck, 1999; Beck et al., 2001; Fowler, 2001; Jeffries et al., 2001)

HEAVY-WEIGHT METHODS
(IEEE, 1997; ISO/IEC, 2002)

REQUIREMENTS
MANAGEMENT

Short requirements (story cards) are written for a product feature
or characteristic. They have a 10-to-20 day term, that is, the
scope of the requirements will be developed within this time
period. Story cards are used to estimate priorities, scope and
development time.

They are based on the elicitation and in-depth analysis of the
user requirements from which the functional and non-functional
system requirements are defined.

DEVELOPMENT
MODEL

Non-systematic development model, sharing an incremental
organisational model, based on small deliveries with short cycles. Development by short iterations. Short-term planning for
each iteration.

CHANGE
MANAGEMENT

Adaptable, acceptation of change as an inherent part of the
development process.

CUSTOMER
RELATIONS

PEOPLE
ASSIGNATION

PROCESS
APPLICATION

DOCUMENTATION

The client works with the development team and in the same
physical space throughout the whole project. The client closely
cooperates in the project. The client gets a better view of the real
project status.
People-oriented process. People are not considered replaceable
parts. Support should be given to the actual members of the
development team and not the generic process roles. As it is an
adaptable process, a very effective team of creative, highly
qualified developers who are good a communicating, highly
sociable and highly adaptable to changes and work well as a
team is needed. Self-managed or autonomous teams decide on
their own technical work and its planning.
The processes are accepted, the whole team should be actively
committed to or involved in applying the processes. This means
that developers and management are in the same decisionmaking position.
Fewer management activities. Development is directly code
driven, suggesting that the important part of the documentation
is the source code and demanding a minimum and little documentation for a particular activity.

Systematic development model, in which project development is
ordered organisationally: estimation, planning, analysis, design,
implementation, verification and validation, installation. More
orderly development, where iterations are more comprehensive
and usually affect more than one stage. Medium- and long-term
planning, normally covering the whole project and gradually
adjusted as the project progresses.
Predictive, where changes should be evaluated to determine
whether are not they are addressed in this development process
or future versions.
The client and development team have a contractual relationship. The client or user participates in defining system requirements and the acceptance of some of the project stages. However, the real project status is unknown to the client.
Task- or activity-oriented process. The important thing is the
activity under development, not who is doing it. People should
rotate to prevent any delays caused by new members joining.
People are allocated to particular roles and are not usually interchangeable. A good understanding between team members is
also important.
The processes are usually established, normally by the management, disregarding the opinion and appraisal of development
team members, as well as the fitness to the project type under
development.
More management activities. More bureaucracy. Much more
extensive and specific documentation for each task or development activity.

Table 2. Description of agile and heavy-weight description according to software development features
DEVELOPMENT
FEATURES

REQUIREMENTS
MANAGEMENT
DEVELOPMENT
MODEL
CHANGE
MANAGEMENT
CUSTOMER
RELATIONS

METHOD

TASK

COHESION AND CONFLICT

CLIMATE (TCI)

Agile
Heavy-weight
Agile
Heavy-weight
Agile
Heavy-weight
Agile

Independence
Interdependence
Independence (continuous)
Interdependence (stagewise)
Autonomous (adaptable)
Reliant (structured)
Interdependence & Creative
Independence & Routine
(planned)
Autonomous (multifunctional)

Cohesive &Negotiation
Cooperative & Negotiation
Cohesive & Negotiation
Cooperative & Compulsion
Negotiation
Compulsion
Cohesive & Negotiation

Participative
Directed
Unanimous
Compartmentalised
Innovative
Conservative (accommodating)
Cohesive

Heavy-weight

Reliant (hierarchical)

Cooperative & Compulsion

Agile
Heavy-weight
Agile

Autonomous
Reliant
Autonomous

Cohesive
Cooperative
Cooperative

Participative, Innovative, Unanimous & Quality
Directed, Conservative, Compartmentalised & Quality
Participative & Innovative
Directed & Conservative
Participative

Heavy-weight

Reliant

Cohesive

Directed, Conservative & Quality

Heavy-weight
Agile

PEOPLE ASSIGNATION
PROCESS
ASSIGNATION
DOCUMENTATION

Cooperative & Negotiation
Cohesive & Negotiation

Compartmentalised

Table 3. Appraisal of agile and heavy-weight methods by team behaviour

The computing students are taking different subjects, all
related to one of the stages that take place in a software
project or with the full software project. However, not all
the projects are developed according to the same development philosophy, that is, some projects follow pure or
mixed heavy-weight processes, whereas other perform agile or mixed agile processes.
Many aspects are considered to have an influence on team
performance, effectiveness and satisfaction. However, we

After analysing how development teams are organised in
software processes and examining the team behaviour aspects from social psychology, which are considered important in forming development teams to get better performance, we considered collecting data to analyse and advance
software engineering at three Spanish universities. There
are basically two reasons for doing this: a) it is not intrusive
and b) it is not costly.

32

the criteria of document evaluation). Team member satisfaction at the end of the project is measured by means of a
special-purpose questionnaire used in social psychology.

have taken into account: the personality of each team
member; knowledge, skills and abilities on how to do
teamwork, preferences and perceptions of the teamwork
environment or climate in which the software project is
developed, and, finally, the level of cohesion and conflict
achieved by the team. These aspects have been selected,
because Yang & Tang (2004) consider them to have a significant impact on the above-mentioned team response
variables, where, however, they were measured in separate
ways rather than as an integral approach as here. From
these aspects and their relationships measured in the crossexperiment, we will be able to find out what type of people/people combinations (team) are better for the different
software development types.
They are all aspects examined in the field of social psychology. Therefore, to be able to determine how much each
of these aspects is involved in the results achieved by the
team, we are taking measurements on each one using questionnaires prepared to get findings on how to relate people
individually and at team level depending on the software
process model, as well as to see the results on the product
(quality and development time) and the people (job satisfaction within the team). For this purpose, specialists in
social psychology are running a cross-experiment on the
students, which, additionally, are helping in data analysis,
as well as their interpretation:
The cross-experiment is divided into three parts:
1. Preliminary phase or PRE
2. Development phase or DURING
3. Final phase or POST
and the questionnaires will be designed according to this
division, as set out in Table 4. A detail of the items of the
team climate inventory (TCI) questionnaire (Anderson &
West, 1998; Anderson & West, 1999) used to measure the
real team climate in development teams is shown in Figure
2.

Participative safety
We share information generally in the team rather than keeping it to ourselves.
People keep each other informed about work-related issues in the team.
There are real attempts to share information throughout the team
People feel understood and accepted by each other.
Support for innovation
Assistance in developing new ideas is readily available.
The team is open and responsive to change.
People in the team are always searching for fresh, new ways of looking at problems.
Team vision
How clear would you need to be about what your team objectives were?
How far would you need to be in agreement with these objectives?
To what extent would you need to think they were useful and appropriate objectives?
Task orientation
Would there be a real concern among team members that the team should achieve the
highest standards of performance?
Would the team have clear criteria, which members would try to meet in order to achieve
excellence as a team?
Would you and your colleagues monitor each other so as to maintain a higher standard of

Figure 2. Detail of the team climate inventory (TCI)

questionnaire
The aim is to cover the range of possible processes from
pure heavy-weight to pure agile processes. What we are
relating for each of these development types are people
aspects (personality, KSA and climate preferences) along
with the combinations of people in each team and the product quality, development time and team member satisfaction. The types of things we expect to find is how one type
of person behaves individually and at team level in an agile
process and determine the person/team behaviour match to
each software process.
7. CONCLUSIONS
Social psychology has not yet managed to come up with an
answer to the question of how to form teams to optimise
the final result. Additionally, the research carried out in
social psychology on the subject of team performance is
specialised in a field or task type. This means that to gain
knowledge about software development team forming, we
need to conduct specialised studies in our area.
This is precisely the goal of the cross-experiment that we
are conducting. Measures are taken through the questionnaires on aspects and subaspects that affect each team behaviour model component (Table 1). This way we can
characterise people according to personality factors (Big
Five) and KSAs, as well as team behaviour through two
climate levels, preferences (TSI) and perceptions (TCI). On
the other hand, we characterise the different software process models, agile and heavy-weight, considering the extreme cases, pure agile and pure heavy weight processes,
although the cross-experiment is also planned on projects
that lie somewhere in-between (mixed heavy-weight and
mixed light-weight methods). The software processes are
characterised considering particular features of the task
(interdependence, autonomy and routineness) and the aspects that have an impact on the team behaviour component
(cohesion, conflict and climate perceived in the team
(TCI)), matching their possible values to software process
features, agile and heavy weight (Table 2 and Table 3).
Having completed these characterisations, we look at what

CROSS-EXPERIMENT PHASES
MEASURED
ASPECTS

Personality(Big
Five)
KSA
Climate (TSI)
Climate (TCI)
Interdependence,
Autonomy and
Routineness
Conflict and Cohesion

PRE

DURING

POST

X

X
X

X
X
X

X

Table 4. Cross-experiment phases and aspects

All the measures that are taken from the start to the end of
the cross-experiment are independent variables, except the
level of Satisfaction, which like Performance, is a dependent or response variable. Performance includes the development time (measured as a function of software products
delivery date compliance) and product quality (defined by

33

Curtis B, Hefley WE, Miller S. People Capability Maturity
Model (P-CMM) Version 2.0. Maturity Model CMU/SEI-2001MM-001. Carnegie Mellon University, Software Engineering
Institute, 2001.
DeMarco T, Lister T. Peopleware: Productive Projects and
Teams (2nd ed). Dorset House, New York, 1999.
Digman JM. Personality structure: Emergence of the five-factor
model. Annual Review of Psychology 1990; 41:417-440.
Evans BK, Fischer DG. A hierarchical model of participatory
decision-making, job autonomy, and perceived control. Human
Relations 1992; 45:1169-1189.
Fowler M. Is design dead? Proceedings of the XP2000,
http://www.martinfowler.com/articles/designDead.html,
http://www.refactoring.com/, 2001.
Hackman JR, Oldham GR. Work Redesign. Addison-Wesley,
Reading, 1980.
Hardiman LT. Personality types and software engineers. IEEE
Computer 1997; 301(10):10–10.
Hendriks AA. The construction of Five-Factor Personality Inventory (FFPI), Dissertation, University of Groningen, Groningen, The Netherlands, 1997.
Hollenbeck JR, Moon H, Ellis APJ, West BJ, Ilgen DR,
Sheppard L, Porter COLH, Wagner III JA. Structural contingency theory and individual differences: examination of external
and internal person-team fit. Journal of Applied Psychology
2002; 87(3):599-606.
IEEE Standard for Developing Software Life Cycle Processes,
IEEE Standard 1074-1997.
ISO/IEC International Standard: Information Technology. Software Life Cycle Processes, Amendment 1, ISO/IEC Standard
12207-1995/Amd. 1-2002.
Jeffries R, Anderson A, Hendrikson C. Extreme Programming
Installed. Addison-Wesley, 2001.
Katzenbach J, Smith D. The Discipline of Teams: A Mindbookworkbook for Delivering Small Group Performance. John Wiley
& Sons, 2001.
Kellner MI, Madachy RJ, Raffo DM. Software Process Simulation Modelling: Why? What? How? Journal of Systems and Software 1999; 46:91-105.
Koontz H, O’Donnell C. Principles of Management: An Analysis
of Managerial Functions. McGraw-Hill, 1972.
Larson C, LaFasto FMJ. When Teams Work Best: 6.000 Team
members and leaders tell what it takes to excel. Sage Publications, 2001.
McGrath JE. Groups: Interaction and Performance. PrenticeHall, Englewood Cliffs, NJ, 1984.
Molleman E, The modalities of self-management: the “must”,
“may”, “can” and “will” of local decision making, The International Journal of Operations and Production Management 2000;
20.
Molleman E, Nauta A, Jehn KA. Person-Job Fit Applied to
teamwork: A Multi-Level Approach. Proceeding of the 6th International Workshop on Teamworking, Scholl of Technology and
Society 2002.
Molleman E, Slomp J. Functional flexibility and team performance. International Journal of Production Research 1999;
37:1837-1858.
Moore E. Personality characteristics of information systems
professionals. Proceedings of the Conference on SIGCPR 1991;
140–155.

type of people, both individually (personality factors gathered using the Big Five) and at team level (climate preferences (TSI)) better fit in with either software process type
and, therefore, determine the person/team behaviour match
for each software process. The cross-experiment is now
yielding preliminary findings, and we see how sure and
innovative people with the ability to reason, synthesise and
work together as a team and who prefer a unanimous and
innovative climate perform better in agile processes when
the team is composed of 66% of such members and the
other 44% are diligent people with knowledge of teamwork
but who prefer a compartmentalised climate. On the other
hand, teams with diligent and sure people who are good at
teamwork if reserved for given activities and prefer a comfortable, compartmentalised, quality climate perform better
in heavy-weight processes when the team has 53% of such
members and the remainder of the membership are innovative people who prefer a cohesive and participative climate.
REFERENCES
Acuña ST, Juristo N. Assigning people to roles in software projects. Software: Practice and Experience 2004; 34:675-696.
Anderson N, Burch GJ. The Team Selection Inventory. ASE.
NFER-Nelson, Slough, 2003.
Anderson N, West M. The Team Climate Inventory. ASE.
NFER-Nelson, Windsor, 1994.
Anderson N, West M. Measuring climate for work group innovation: Development and validation of the team climate inventory.
Journal of Organizational Behaviour 1998; 19:235-258.
Anderson N, West M. The Team Climate Inventory: User’s
Guide, 2nd ed. ASE. NFER-Nelson, Windsor, 1999.
Barrick MR, Mount MK. The Big Five personality dimensions
and job performance: A meta-analysis. Personnel Psychology
1991; 44:1-26.
Barry B, Stewart GL. Composition, process and performance in
self-managed groups: The role of personality. Journal of Applied
Psychology 1997; 82:62-78.
Beck K. Extreme Programming Explained: Embrace Change.
Addison-Wesley, Reading, 1999.
Beck K, Beedle M, Cockburn A, Cunnimgham W, Fowler M et
al., Agile Manifesto, http://agilemanifesto.org/, 2001.
Boehm BW, Abts C, Brown WA, Chulani S, Clark BK,
Horowitz E, Madachy R, Reifer DJ, Steece B. Software Cost
Estimation with COCOMO II. Upper Saddle River: Prentice Hall
PTR, 2000.
Bostrom RP, Kaiser KM. Personality differences within systems
project teams: Implications for designing solving centers. Proceedings of the Eighteenth Annual ACM SIGCPR Conference
1981; 248–285.
Breaugh JA. The measurement of work autonomy. Human Relations 1985; 38:551-570.
Burch GJ, Anderson N. Measuring person-team fit: Development
and validation of the team selection inventory. Journal of Managerial Psychology 2004; 19(4):406-426.
Burdett G, Li R-Y. A quantitative approach to the formation of
workgroups. Proceedings of the ACM SIGCPR Conference
1995; 202–212.

34

Rutherfoord RH. Using personality inventories to help form
teams for software engineering class projects. SIGCSE-Bulletin
2001; 33(3):76–76.
Steiner ID. Group process and productivity, New York: Academic Press, 1972.
Teague J. Personality type, career preference and implications
for computer science recruitment and teaching. Proceedings of
the Third Australasian Conference on Computer Science Education 1998; 155–63.

West MA, Anderson N. Innovation in top management teams.
Journal of Applied Psychology 1996; 81:680-693.
White K, Leifer R. Information systems development success:
Perspectives from project team participants. MIS Quarterly
1986; 10(3):215–23.
Wynekoop J, Walz D. Investigating traits of top performing software developers. Information Technology & People 2000;
13(3):186-195.
Yang, H-L, Tang J-H. Team structure and team performance in
IS development: a social network perspective. Information &
Management 2004; 41:335-349.
Zakarian A, Kusiak A. forming teams: An analytical approach.
IIE Transactions 1999; 31:85–97.
Zuser W, Grechening T. Reflecting skills and personality internally as means for team performance improvement. Proceedings
of the 16th Conference on Software Engineering Education and
Training, IEEE Computer Society, 2003.

Tuckman B. Developmental sequence in small groups. Psychological Bulletin 1965; 63:384–399.
Turley R, Bieman J. Competencies of exceptional and nonexceptional software engineers. The Journal of Systems and Software
1995; 28(1):19–38.
Van den Beukel AL, Molleman E. Downsides of multifunctionality in team-based-based work. Personnel Review 2002; 31:482494.
West MA. The social psychology of innovation in groups. In M.
A. West, J. L. Farr (Eds.), Innovation and Creativity at Work.
Wiley, 1990.

35

36

Section II

Process Modeling – Focus on
Model Implementation

37

38

Modeling Recruitment and Role Migration Processes in OSSD Projects
Chris Jensen and Walt Scacchi
Institute for Software Research
Bren School of Information and Computer Sciences
University of California, Irvine
Irvine, CA USA 92697-3425
{cjensen, wscacchi}@ics.uci.edu

Abstract
Socio-technical processes have come to the
forefront of recent analyses of the open source
software development (OSSD) world. Though
there many anecdotal accounts of these processes,
such narratives lack the precision of more formal
modeling techniques, which are needed if these
processes are going to be systematically analyzed,
simulated, or re-enacted. Interest in making these
processes explicit is mounting, both from the
commercial side of the industry, as well as among
spectators who may become contributors to OSSD
organization. Thus, the work we will discuss in this
paper serves to close this gap by analyzing and
modeling recruitment and role transition processes
across three prominent OSSD communities whose
software development processes we've previously
examined: Mozilla.org, the Apache community, and
NetBeans.

Figure 1. An “onion” diagram representation of an
open source community organizational hierarchy
processes. What guidance is provided is often
directed at recruitment- initial steps to get people in
the door. Guidance for attaining more central roles
is often characterized as being meritocratic,
depending on the governance structure of the
community. Nevertheless, these development roles
and how developers move between them seems to
lie outside of the traditional view of software
engineering, where developers seem to be limited
to roles like requirements analyst, software
designer, programmer, or code tester, and where
there is little/no movement between roles (except
perhaps in small projects).

Keywords: Project recruitment, membership,
process modeling, open source, Mozilla, Apache,
NetBeans

Introduction

Christie and Staley (2000) argue that social and
organizational processes, such as those associated
with moving between different developer roles in a
project, are important in determining the outcome
of software development processes. In previous
studies, we have examined software development
processes within and across OSSD communities
(Jensen and Scacchi, 2005, Scacchi 2002, 2004,
2005). Here, we take a look at two related sociotechnical processes used in OSSD as a way of
merging
the
social/cultural
and
technical/developmental
OSSD
activities.
Specifically, we’ll focus on the recruitment and
migration of developers from end-users or
infrequent contributors towards roles more central
to the community, like core developer, within
projects such as the Mozilla, Apache community,
and NetBeans projects. Such processes characterize
both the hierarchy of roles that OSS developers
play (cf. Gacek and Arief 2004), as well as how
developers move through or become upwardly
mobile within an OSSD project (Sim and Holt
1998).
While anecdotal evidence of these

In recent years, organizations producing both open
and closed software have sought to capitalize on
the perceived benefits of open source software
development (OSSD) methodologies.
This
necessitates examining the culture of prominent
project communities in search of ways of
motivating developers. Although the ensuing
studies have provided much insight into OSSD
culture, missing from this picture was the process
context that produced the successes being
observed. Ye and Kishida (2003) and Crowston
and Howison (2005) observe that community
members gravitate towards central roles over time
represented with “onion” diagrams such as in
figure 1. These depictions indicate a similar
number of layers in organizational hierarchies
across communities, but do not suggest how one
might transition between layers and what roles are
available at each layer.
Much like their
development processes, OSSD communities
typically provide little insight into role migration

39

frustrated with an outstanding issue within the bug
repository and submit a patch, themselves.

processes exists, the lack of precision in their
description serves as a barrier to community entry,
continuous improvement, and process adoption by
other organizations. The goal of our work here
thus serves to provide process transparency through
explicit modeling of such processes in ways that
may enable increased community participation,
more widespread process adoption, and process
improvement.

The next task is to recruit others to accept the patch
and incorporate it into the source tree. Recruitment
of patch review is best achieved through emailing
reviewers working on the module for which the
patch was committed or reaching out to the
community via the Mozilla IRC chat.
By
repeatedly
demonstrating
competency
and
dedication writing useful code within a section of
the source, would-be developers gain a reputation
among those with commit access to the current
source code build tree.
Eventually, these
committers recommend that the developer be
granted access by the project drivers. In rare cases,
such a developer may even be offered ownership of
a particular module if s/he is the primary developer
of that module and it has not been blocked for
inclusion into the trunk of the source tree1.

In the remaining sections, we outline details
aboutrecruitment and role migration as membership
processes as found while examining each of these
three OSSD project communities. At the
ProSim’05 Workshop we will present a variety of
semi-structured and formal models that enable
more rigorous analysis and simulated re-enactment
using tools and techniques we have previously
developed and employed (cf. Noll and Scacchi
2001, Jensen and Scacchi 2005)

Once a project contributor is approved as a source
code contributor, there are several roles available to
community members. Most of these are positions
requiring greater seniority or record of
demonstrated
accomplishments
within
the
community. As module developers and owners
establish themselves as prominent community
members, other opportunities may open up. In
meritocratic fashion (cf. Fielding 1999), developers
may transition from being a QA module contact to
a QA owner. Similar occasions exist on the project
level for becoming a module source reviewer.

Membership Processes in Mozilla.org
Developer recruitment in Mozilla was difficult at
the start. The opening of the Netscape browser
source code offered developers a unique
opportunity to peek under the hood of the once
most dominant Web browser in use. Nevertheless,
the large scale of the application (multi-million
source lines of code) and the complex/convoluted
architecture scared developers away.
These
factors, combined with the lack of a working
release and the lack of support from Netscape led
one project manager to quit early on (Mockus, et.
al, 2002). However, with the eventual release of a
working product, the Mozilla project garnered
users who would become developers to further the
cause.

Super-reviewers attain rank by demonstrating
superior faculty for discerning quality and effect of
a given section of source on the remainder of the
source tree. If a reviewer believes that s/he has
done this appropriately, s/he must convince an
existing
super-reviewer
of
such
an
accomplishment. This super-reviewer will propose
the candidate to the remainder of the superreviewers. Upon group consensus, the higher rank
is bestowed on the reviewer (Mozilla Code Review
FAQ, 2005). The same follows for Mozilla drivers,
who determine the technical direction of the project
per release.

The Mozilla Web site lists several ways for
potential developers and non-technical people to
get involved with the community (Getting Involved
with Mozilla.org, 2005). The focus on quality
assurance and documentation reflects a community
focus on maturing, synchronizing, and stabilizing
updates to the source code base. Technical
membership roles and responsibilities currently
listed include bug reporting, screening, confirming,
and fixing, writing documentation, and contacting
sites that do not display properly under Mozilla.
Compared to more central roles, these activities do
not require deep knowledge of the Mozilla source
code or system architecture, and serve to allow
would-be contributors to get involved and
participate in the overall software development
process.

Community level roles include smoke-test
coordinator, code sheriff, and build engineer,
although no process is prescribed for such
transitions. As individual roles, they are held until
vacated, at which time, the position is filled by
appointment from the senior community members
and Mozilla Foundation staff. Role hierarchy and a
flow graph of the migration process for
transitioning from reviewer to super-reviewer are
provided in figure 2 as an example of those we

When bugs are submitted to the Bugzilla, they are
initially assigned to a default developer for
correction. It is not uncommon for community
developers and would-be developers to become

1

40

https://bugzilla.mozilla.org/show_bug.cgi?id=18574

ASF membership follows the same process as PMC
membership- nomination and election by a
majority vote of existing ASF members.

have modeled for this community. In the flow
graph, rectangles refer to actions, whereas ovals
refer to resources created or consumed by the
associated action, as determined by the direction of
the arrow linking the two. Transitions from one
role to another are depicted with a dashed arrow
from an action performed by one role to the title of
another. We have also used dashed lines to
differentiate social or role transitioning activities
and
resources
from
strictly
technical,
developmental resources.

ASF members may run for office on the ASF board
of directors, as outlined by the ASF bylaws
(Bylaws of the Apache Software Foundation,
2005). Accordingly, the offices of chairman, vice
chairman, president, vice president, treasurer (and
assistant), and secretary (and assistant) are elected
annually. A flow graph of the role migration
process appears in figure 3.

Membership Processes in the Apache
Community

Although, there is one path of advancement in the
Apache community, there are several less formal
committees that exist on a community (as opposed
to project) scale. These include the conference
organizing committee, the security committee, the
public relations committee, the Java Community
Process (JCP) committee, and the licensing
committee. Participation in these committees is
open to all committers (and higher ranked
members) and roles are formalized on an as-needed
basis (e.g. conference organization).
Noncommitters may apply for inclusion in specific
discussion lists by sending an email to the board
mailing alias explaining why access should be
granted. Thus, processes associated with these
committees are ad hoc and consist of one step.

Role migration in the Apache community is linear.
The Apache Software Foundation (ASF) has laid
out a clear path for involvement in their
meritocracy. Individuals start out as end-users
(e.g., Web site administrators), proceed to
developer status, then committer status, project
management committee (PMC) status, ASF
membership, and lastly, ASF board of directors
membership (How the ASF Works, 2005). Much
as in advancement in the Mozilla community,
Apache membership is by invitation only. As the
name suggests, the Apache server is comprised of
patches submitted by developers. These patches
are reviewed by committers and either accepted or
rejected into the source tree.

Membership
Processes
NetBeans.org Community

In addition to feature patches, developers are also
encouraged to submit defect reports, project
documentation, and participate on the developer
mailing lists. When the PMC committee is
satisfied with the developer’s contributions, they
may elect to extend an offer of “committership” to
the developer, granting him/her write access to the
source tree.
To accept committership, the
developer must submit a contributor license
agreement, granting the ASF license to the
intellectual property conveyed in the committed
software artifacts.

in

the

Roles in the NetBeans.org community for
developing the Java-based NetBeans interactive
development environment are observable on five
levels of project management (Oza, et. al 2002) just
as in Apache. These range from users to source
contributors, module-level managers, project-level
managers, and community-level managers. The
NetBeans community’s core members are mostly
Sun Microsystems employees, the community’s
primary sponsor, and are subject to the
responsibilities set on them by their internal
organizational hierarchy. As such, (and unlike the
cases of Apache and Mozilla), not all roles are
open to volunteer and third-party contributors.
Non-Sun employed community members wanting
to participate beyond end-usage are advised to start
out with activities such as quality assurance (QA),
internationalization, submitting patches, and
documentation (Contributing to the NetBeans
Project, 2005). As in the case with Mozilla, until
they have proven themselves as responsible, useful,
and dedicated contributors, developers must submit
their contributions to developer mailing lists and
the issue repository, relying on others with access
to commit the source. However, unlike Mozilla,
developers are also encouraged to start new
modules.

PMC membership is granted by the ASF. To
become a PMC member, the developer/committer
must be nominated by an existing ASF member
and accepted by a majority vote of the ASF
membership participating in the election (Fielding,
et. al, 2002).
Developers and committers
nominated to become PMC members have
demonstrated commitment to the project, good
judgment in their contributions to the source tree,
and capability in collaborating with other
developers on the project. The PMC is responsible
for the management of each project within the
Apache community. The chair of the PMC is an
ASF member elected by his/her fellow ASF
members who initially organizes the day-to-day
management infrastructure for each project, and is
ultimately responsible for the project thereafter.

41

NetBeans project.. One of the three is appointed by
Sun. The community at large elects the other two
members of the governance board. These elections
are held every six months, beginning with a call for
nominations by the community management.
Those nominees that accept their nomination are
compiled into a final list of candidates to be voted
on by the community. A model of the product
development track role migration process is shown
in figure 4.

While the community was more liberal with
module creation early in the project’s history, as
the community has matured, additions to the
module catalogue have become more managed to
eliminate an abundance of abandoned modules.
Also as in Mozilla, developers are subjected to the
proving themselves before being granted committer
status on a portion of the source tree. Additionally,
they may gain module owner status be creating a
module or taking over ownership of an abandoned
module that they have been the primary committer
for.
With module ownership comes the
responsibility to petition the CVS manager to grant
commit access to the source tree to developers,
thereby raising their role status to “committer.”

Discussion
In both NetBeans and Mozilla, recruitment consists
of listing ways for users and observers to get
involved. Such activities include submitting defect
reports, test cases, source code and so forth. These
activities require a low degree of interaction with
other community members, most notably decision
makers at the top of the organizational hierarchy.
Our observation has been that the impact of
contributions trickles up the organizational
hierarchy whereas socio-technical direction
decisions are passed down. As such, activities that
demonstrate capability in a current role, while also
coordinating information between upstream and
downstream (with respect to the organizational
hierarchy) from a given developer are likely to
demonstrate community member capability at
his/her current role, and therefore good candidates
for additional responsibilities.

Rising up to the project-level roles, the Sunappointed CVS source code repository manager is
responsible for maintaining the integrity of the
source tree, as well as granting and removing
developer access permissions. In contrast, the
release manger’s role is to coordinate efforts of
module owners to plan and achieve timely release
of the software system.
Theoretically, any
community member may step in at any time and
attempt to organize a release. In practice, this
rarely occurs. Instead, most community members
passively accept the roadmap devised by Sun’s
NetBeans team. In the latter case, the previous
release manager puts out a call to the community to
solicit volunteers for the position for the upcoming
cycle. Assuming there are no objections, the
(usually veteran) community member’s candidacy
is accepted and the CVS manager prepares the
source tree and provides the new release manager
permissions accordingly. Alternatively, a member
of Sun may appoint a member of their development
team to head up the release of their next
development milestone.

Recruitment and role migration processes aren’t
something new; since they describe the actions and
transition passages involved in moving along
career paths. Like career paths described in
management literature (e.g., Lash and Sein 1995),
movement in the organizational structure may be
horizontal or vertical. Most large OSSD project
communities are hierarchical, even if here are few
layers to the hierarchy and many members exist at
each layer.

At the community-management level, the
community managers coordinate efforts between
developers and ensures that issues brought up on
mailing lists are addressed fairly. At the inception
of the NetBeans project, an employee of CollabNet
(the company hosting the NetBeans Web portal)
originally acted as community manager and liaison
between CollabNet and NetBeans. However, it
was soon transferred to a carefully selected Sun
employee (by Sun) who has held it since. As
community members have risen to more central
positions in the NetBeans community, they tend to
act similarly, facilitating and mediating mailing list
discussions of a technical nature, as well as
initiating and participating in discussions of project
and community direction.

In the communities we have examined, we found
different paths (or tracks) towards the center of the
developer role hierarchy as per the focus of each
path.
Paths we’ve identified include project
management (authority over technical issues) and
organizational management (authority over
social/infrastructural issues). Within these paths,
we see tracks that reflect the different foci in their
software processes.
These include quality
assurance roles, source code creation roles, and
source code versioning roles (e.g. cvs manager, cvs
committer, etc), as well as role paths for usability,
marketing, and licensing. There are roles for
upstream development activities (project planning-these are generally taken up by more senior
members of the community. This is due in part that
developers working in these roles can have an

Lastly, a committee of three community members,
whose largely untested responsibility is to ensure
fairness within the community, governs the

42

impact on the system development commensurate
with the consequences/costs of failure, and require
demonstrated skills to ensure the agents responsible
won’t put the software source code into a state of
disarray).

these processes may be well understood (e.g.,
project management processes like scheduling or
staffing), while others are often treated as “one-off”
or ad hoc in nature, executing in a variety of ways
in each instantiation. The purpose of our
examination and modeling study of recruitment and
role migration processes is to help reveal how these
socio-technical processes are intertwined with
conventional software development processes, and
thus constrain or enable how software processes are
performed in practice. In particular, we have
examined and modeled these processes within a
sample of three OSSD projects that embed the Web
information infrastructure. Lastly, we have shown
where and how they interact with existing software
development processes found in our project
sample.

In comparison to traditional software development
organizations, tracks of advancement in open
source communities are much more fluid. A
developer contributing primarily to source code
generation may easily contribute usability or
quality assurance test cases and results to their
respective community teams. This is not to suggest
that a module manager of a branch of source code
will automatically and immediately gain core
developer privileges, responsibilities, and respect
from those teams.
However, industrial
environments tend towards rigid and static
organizational hierarchies with highly controlled
growth at each layer.

References
Bylaws of the Apache Software Foundation,
available
online
at
http://www.apache.org/foundation/bylaws.html
accessed 7 February 2005

The depiction of role hierarchies in open source
communities as concentric, onion-like circles
speaks to the fact that those in the outer periphery
have less direct control or knowledge of the
community’s current state and its social and
technical direction compared to those in the inner
core circle. Unlike their industrial counterparts,
open source community hierarchies are dynamic.
Although changes in the number of layers stabilizes
early in the community formation, the size of each
layer (especially the outer layers) is highly
variable. Evolution of the organizational structure
may cause or be caused by changes in leadership,
control, conflict negotiation, and collaboration in
the community, such as those examined elsewhere
(Jensen and Scacchi 2005b). If too pronounced,
these changes can lead to breakdowns of the
technical processes.

Christie, A. and Staley, M. “Organizational and
Social Simulation of a Software Requirements
Development
Process”
Software
Process
Improvement and Practice 2000; 5: 103–110
(2000)
Contributing to the NetBeans Project, available
online
at
http://www.netbeans.org/community/contribute/
accessed 7 February 2005
Coward, Anonymous.
“About Firefox and
Mozilla” Comment on Slashdot.org forum “Firefox
Developer on Recruitment Policy,” available online
at
http://developers.slashdot.org/comments.pl?sid=13
7815&threshold=1&commentsort=0&tid=154&tid
=8&mode=thread&cid=11527647, 31 January,
2005.

As a general principle, meritocratic role migration
processes such as those we have observed consist
of a sequence of establishing a record of
contribution in technical processes in collaboration
with other community members, followed by
certain “rights of passage” specific to each
community. For Apache, there is a formal voting
process that precedes advancement. However, in
the Mozilla and NetBeans communities, these are
less formal.
The candidate petitions the
appropriate authorities for advancement or
otherwise volunteers to accept responsibility for an
activity. These authorities will either accept or
deny the inquiry.

Crowston, K. and Howison, J. 2005. The Social
Structure of Free and Open Source Software
Development, First Monday, 10(2). February.
Online
at
http://firstmonday.org/i8ssues/issue10_2/crowston/i
ndex.html
Elliott, M., The Virtual Organizational Culture of a
Free
Software
Development
Community,
Proceedings of the Third Workshop on Open
Source Software, Portland, Oregon, May 2003.

Conclusion
Social or organizational processes that affect or
constrain the performance of software development
processes
have
had
comparatively
little
investigation. This is partially because some of

Fielding, R., Shared Leadership in the Apache
Project. Communications ACM, 42(4), 42-43, 1999.

43

Proceedings--Software, 149(1), 24-39, February
2002.

Fielding, R., Hann, I-H., Roberts, J and Sandra
Slaughter, S. “Delayed Returns to Open Source
Participation: An Empirical Analysis of the Apache
HTTP Server Project,” Presented at the Conference
on Open Source: Economics, Law, and Policy,
Toulouse, France June 2002.

Scacchi, W.,
Free/Open Source Software
Development Practices in the Computer Game
Community, IEEE Software, 21(1), 59-67,
January/February 2004.

Gacek, C. and Arief, B., The Many Meanings of
Open Source, IEEE Software, 21(1), 34-40,
January/February 2004.

Scacchi, W.,
Socio-Technical
Interaction
Networks in Free/Open Source Software
Development Processes, in S.T. Acuña and N.
Juristo (eds.), Software Process Modeling, 1-27,
Springer Science+Business Media Inc., New York,
2005.

Getting Involved with Mozilla.org, Web page
available
online
at
http://www.mozilla.org/contribute/ 3 November
2004
How the ASF works, available online
http://www.apache.org/foundation/how-itworks.html, accessed 7 February 2005

Sim, S.E. and Holt, R.C., The Ramp-Up Problem in
Software Projects: A Case Study of How Software
Immigrants Naturalize, Proc. 20th Intern. Conf.
Software Engineering, Kyoto, Japan, 361-370,
1998.

at

Jensen, C. and Scacchi, W., Process Modeling
Across the Web Information Infrastructure,
Software Process Improvement and Practice, to
appear, 2005.

Ye, Y. and Kishida, K. Towards an Understanding
of the Motivation of Open Source Software
Developers, Proc. 25th Intern. Conf. Software
Engineering, Portland, OR, 419-429, 2003.

Jensen, C. and Scacchi, W. Collaboration,
Leadership, Control, and Conflict Negotiation
Processes in the NetBeans.org Open Source
Software Development Community. working
paper, Institute for Software Research, March 2005
Lash, P.B. and Sein, M.K. Career Paths in a
Changing IS Environment: A Theoretical
Perspective, Proc. SIGCPR 1995, 117-130.
Nashville, TN
Mockus, A., Fielding, R., and Herbsleb, J. “Two
Case Studies of Open Source Software
Development: Apache and Mozilla,” ACM
Transactions on Software Engineering and
Methodology, 11(3):309-346, 2002
Mozilla Code Review FAQ, available online at
http://www.mozilla.org/hacking/code-reviewfaq.html, accessed 7 February 2005
Noll, J. and Scacchi, W., Specifying ProcessOriented Hypertext for Organizational Computing,
J. Network and Computer Applications, 24(1), 3961, 2001.
Oza, M. Nistor, E. Hu, X., Jensen, C., Scacchi, W.
“A First Look at the NetBeans Requirements and
Release Process.” June 2002, updated February
2004
available
online
at
http://www.isr.uci.edu/~cjensen/papers/FirstLookN
etBeans/.
Scacchi, W., Understanding the Requirements for
Developing Open Source Software Systems, IEE

44

Quality Assurance
Module Peer

QA Contact

QA Owner

Volunteer Tester

Reviewer

Super Reviewer

Smoke Test Coordinator

Development
Developer

Module Peer

Module Owner

Bugzilla Component Owner

Smoke Test Coordinator

Reviewer

Super Reviewer

Source Build
Code Sherrif

Build Engineer

Project/Community Management
QA Owner

Module Owner

Code Sherrif

Build Engineer

Reviewer

Super Reviewer

Drivers
Mozilla Staff

Super Reviewership
Accept Nomination
for Super Reviewership

Reviewer
Performance
Evidence

Demonstrate Performance by Assessing quality,
effect of submitted patches, enhancements

Super
Reviewership
Request

Assess Reviewer’s Performance

Request Consideration
for Super Reviewership

Reviewer
Recommendation

Super Reviewer

Nomination

Recommend Reviewer for Super Reviewership
Positive
Consensus

Reviewer
Recommendation

Discuss Candidate’s Performance

Negative
Consensus

Figure 2. Role hierarchy and super reviewership migration in the Mozilla community

45

Grant Super Reviewership

Development
End User

Developer

Committer

PMC Member

ASF Member

ASF Board Member

PMC Chair

Download, install, configure, and use the system
Submit defect reports
Submit feature requests
Submit questions and answers on use of the system on user mailing lists

Submit feature patches to developer mailing lists
Submit defect reports to developer mailing lists
Submit project documentation to developer mailing lists
Participate in discussions on developer mailing lists

Committership
Selected defect/
enhancement

Enhancement
requests, defect list

Source patch

Submit patch to committers
Select defect/enhancement to patch

Write, revise, test patches
Merit

Developer
Nomination
message

Become nominated for committership

Vote
results

Committership
advancement
notification

Receive majority vote of PMC members
for membership nomination

Assess merit

License agreement
Community Bylaws, docs
Mailing lists

Committer
Signed license
agreement

Read/understand license agreement, community policies
Submit license agreement (CLA)
Read developer documentation
Join project commit diff mailing lists

Figure 3. Role hierarchy and committership migration in the Apache community, highlighting the sequence of a
developer becoming a committer

46

Development
Observer

Module
Developer

User

Module
Contributor

Module
Maintainer

Release Manager
CVS Manager
(Sun Staffed)

Quality Assurance
Observer

Developer
(Tester)

User

QA
Contributor

QA “Module”
Maintainer

Release Manager

User Interface
Observer

UI Developer/
Tester

User

UI “Module”
Maintainer

UI Contributor

Release Manager

Community Web Portal
Web Content
Developer

Observer

Web “Module”
Maintainer
(Sun Staffed)

Web Content
Contributor

Marketing
Observer

Developer
(Promoter)

Sun Marketing
Analyst

Sun Marketing
Manager

Sun Executive

Governance
Contributor

Maintainer

CVS Manager
(Sun Staffed)

Release Manager

Community Manager
(Sun Staffed)

Governance Board Member

Web Team Membership
Locate NetBeans related
articles, news

Insight on
appropriateness

NetBeans news,
articles

Web Content Developer

Request commit access
to localized areas of the site

Read existing newsletters to
glean article appropriateness

Observer

Submit NetBeans related articles
and translations to Web team mailing list

Write newsletter, article content

Coordinate with developers, contributors
to assemble article, newsletter
content submissions

Existing
newsletters
, articles

Translate existing site content
to another language

Translated
site content
NetBeans news,
article content
NetBeans
newsletter,
featured article

Web Content Contributor
Submit request for news
to community members

Distribute newsletters to mailing lists

Figure 4. Role hierarchy and Web Team membership migration in the NetBeans open source community

47

Size Measurement of Embedded Software System Families
Sebastian Kiebusch
Information Systems Institute
Faculty of Economics and
Management
University of Leipzig
Germany
kiebusch@wifa.uni-leipzig.de

Bogdan Franczyk
Information Systems Institute
Faculty of Economics and
Management
University of Leipzig
Germany
franczyk@wifa.uni-leipzig.de

Andreas Speck
Commercial Information Systems
Faculty of Economics and
Business Administration
University of Jena
Germany
andreas.speck@uni-jena.de

In contrast to this evolution, in these days not much
software is reused but developed in a proprietary way for
each type of car again.
First positive reuse concepts may be found in the
application object technologies or concepts such as
frameworks or components [18]. However, the large
number of software systems with large variability and
different variants demands techniques for reuse like the
concept of software system families (SSF) [4]. Within
this framework a SSF is a “… set of software- intensive
systems sharing a common, managed set of features that
satisfy the specific needs of a particular market segment
or mission and that are developed from a common set of
core assets” [5].
To improve the possibilities of reusing software
resources we develop the approach of Process Family
Engineering in Service- Oriented Applications (PESOA)
for the automotive and the electronic business
(eBusiness) domain. Our work in the eBusiness domain is
not an issue of this automotive focused paper. However,
it is the central theme in [9], [10] and [11]. The main idea
of the PESOA- approach is to expand the concept of SSF
by a domain independent process model for a more
detailed addressing of variable and common assets.1
As a result of the rapid software inclusion in
automotive control units, embedded software systems are
expected to account for up to 10% of the overall costs of
a car [4]. Therefore, the software development and
maintenance costs are now a considerable factor for an
automobile, but appropriate cost models are highly
insufficient [4].
Several approaches access the areas of processes, SSF or
embedded systems in the automotive domain [1, 3, 6, 7,
17]. However, none of these exemplary methods
measures the unadjusted size or estimates the costs of
process oriented SSF in the automotive industry.

Abstract
Embedded software systems have become the driving
force in many areas of technology like the automotive
industry. Control functions of cars, driver assistance as
well as systems for information and entertainment are
accomplished by software driven control units. Due to
the high complexity and development effort of embedded
systems, these resources have to be reused. Software
system families are a promising solution to gain a cost
reduction by reusing common software assets in
different variants of an automobile. To support the
economic management of this development approach we
need software metrics to estimate the effort of building
embedded software system families. Techniques of size
measurement and cost estimation for software system
families are highly insufficient in general and do not
exist for the automotive domain. Therefore, this article
describes a conglomerate of innovative metrics to
measure the size of a system family orientated software
development. These size metrics analyze a real- time and
a process focused perspective of embedded software
system families in the automotive domain. A combination
of both viewpoints describes the unadjusted size of
software driven control units to indicate and estimate
their development costs.

1. Introduction
Today more than 98% of all microprocessors are used in
embedded systems which all are free progammable [4].
Additionally to this broad base we observe an enormous
increase of embedded software systems in several
domains. The automotive industry is an outstanding
sector to visualize this accelerated growth of technology.
In these days, up to 90% of new functions and
innovations in a car are enabled by embedded software
technologies [4].

1

48

Further information about the PESOA- techniques are
downloadable at: www.peosa.org and www.kiebusch.de

For this reason we introduced the Process- FamilyPoints (PFP) analysis to enable a size measurement and
effort estimation of embedded SSF. According to this the
PFP metrics are an effort indication system to estimate
the development costs of process oriented SSF in
embedded control units of automobiles.

criterions
real time
functions
common soft
(germ. EGW)
common hard
(germ. EGH)

2. Real time measurement
In general real time systems differ from business
information systems by the special consideration of the
dimension of time [14]. An embedded real time system is
always a part of a well- specified larger system, which
consists of mechanical subsystems and often of a manmachine interface [13]. The control units of the
automotive domain are an example application of these
embedded real time systems and also the main point of
the following investigations.

variable

common

input
output
1
2
3 or more

-

-

X

X

-

X

-

-

X

-

X

-

X

X

-

1 to 2

3 to 5

6 or more

low
average
high

average
average
high

high
high
high

2.3 Real time transformation
The categorized and complexity weighted real time
functions have to be converted into the temporary size
measure of unadjusted PFP. This virtual size measure is
comparable to unadjusted Function Points [19], the Mark
II Function Point Index [20] and the Common Software
Measurement International Consortium (COSMIC)
functional size unit (Cfsu) [6].
As a matter of principle hard real time functions are
more expensive to develop than soft real time functions
because of validation and security aspects [12]. Beside
this fact we have to identify if the real time functions are
embedded in variable assets, which are less costly than
their counterparts in common assets [12]. This is due to
higher quality and security standards of common assets
for the reason that they contain base functionalities which
are reused in every product of the SSF. Additionally we
have to separate among a horizontal perspective where

functional
validation
hard
soft
(yes)
(no)

X

common

Table 2: Complexity matrix to rate real time functions

Table 1: Classification of real time functions in embedded SSF

real time
functions
variable soft
(germ. EVW)
variable hard
(germ. EVH)

variable

Depending on the Function Point Analysis (FPA) and
other sizing methods, the complexity of the categorized
real time functions has to be weighted. Within this
context the amount of input and output signals are an
excellent complexity indicator of real time functions [12].
The metric in table 2 classifies the real time complexity
according to the explained signal structure. All boundary
values of this table are derived out of a Simulink2 model
description of a motor control unit from DaimlerChrysler
as an example of an embedded real time SSF in [15]. This
tabular metric will be calibrated for a universal usage by
other SSF focused real time control units.

No consistent classification of real time systems and their
functional requirements is available in the actual
technical literature. In many cases you will find a variant
rich categorization of real time systems like in [13] and
[14]. However, the following criteria enable a general
economic categorization of embedded real time systems:
• Hard timing constrains are imposed and must be
validated as well as guaranteed. A missed hard timing
constrain is equal to a breakdown of the wellspecified larger system.
• Soft timing constrains require a less rigorous
validation or guarantee and are not imposed. To miss a
soft timing constrain does not affect the functionality
of the well- specified larger system.
We consider real time systems in SSF. Therefore, it is
essential to classify the reuse of real time functions if they
are a part of common or variable assets in a SSF.
Table 1 demonstrates a classification metric in order
to categorize the real time functions of embedded SSF.
Instead of other functional sizing methods this PFP
categorization focuses the viewpoint of software
developers according to the characteristics of the
automotive domain.

asset reuse

functional
validation
hard
soft
(yes)
(no)

2.2 Real time complexity weighting

2.1 Real time categorization

criterions

asset reuse

2

49

Platform for multidomain simulation and model- based design
of time- varying systems.

instance of a SSF is reflected by the amount of
instantiated products (PA).

variable real time functions are encapsulated and a
vertical viewpoint where one real time function can be a
part of variable as well as of common assets at the same
time.
To realize a high compatibility with other functional
sizing methods we assign our horizontal variable real
time categories to the complexity dependant
transformation factors of the FPA data perspective {5; 7;
10; 15}. Within this activity we allocate the soft functions
(EVW) to the lower and the hard functions (EVH) to the
higher values as you can see in table 3.
Horizontal variable real time functions are reused
relying on their product independent implementation
frequency (IH, germ. Implementierungshäufigkeit).
Hence the amount of unadjusted PFP for an EVW or an
EVH is inversely proportional to their individual IH.
Historical experiences in SSF development vary between
different organizations. In consequence complexity
dependant correction factors for variability (KV, germ.
Korrekturfaktor- Variabilität) are required to supplement
the EVW and EVH conversion factors. If the complexity
repercussions are not sufficiently covered by the
conversion factors, an empirical determined KV
substitutes the standard KV value. A detailed derivation
of the standard value and the adjustment action as well as
the updating procedure is explained in [10].

Table 4: Transforming complexity weighted, horizontal
common real time functions

complexity
low
(germ. g)
average
(germ. m)
high
(germ. h)

low
(germ. g)
average
(germ. m)
high
(germ. h)

EVH

KVg ×5
IH
KVm ×7
IH
KVh ×10
IH

KVg ×7
IH
KVm ×10
IH
KVh ×15
IH

purpose
How high is the
share of
common signals
in this vertical
variable real
time function?

PA
KG m ×15
PA
KG h ×23
PA

PA
KG m ×23
PA
KG h ×35
PA

calculation

interpretation

B
G=
C

0≤ G ≤

B = Amount of
common signals
in this function.
C = Amount of
all signals in
this function.

Closer to ½
means more
common signals
in this vertical
variable
function.3

1
2

Now we have to exclude G from the variable
transformation quotients in table 3 like shown in the
initial part of the generic formula 2. Subsequently we
have to add the product of G and the common conversion
quotients from table 4 to the previous calculation
according to the last part of the generic formula 2.

The determination of horizontal common real time
functions is basically prearranged by the transformation
of horizontal variable assets. Depending on the more
expensive development of common assets, they are
assigned to higher conversion values which are extended
about two new factors. This extrapolation is executed by
formula 1 which represents the original FPA- conversion
factors thru a cubic function.
1
1
7
y = ×x 3 − × x 2 + × x + 3
6
2
3

KG g ×15

Table 5: Common signals in a vertical variable real time
function

unadjusted PFP
EVW

EGH

KG g ×10

As already mentioned, we have to convert also vertical
variable real time functions. These functions are realized
as a mixture of variable and common assets with a
predominance of variability. Therefore, we have to
determine the quota of common signals in vertical
variable real time functions like it is defined in table 5.

Table 3: Transforming complexity weighted, horizontal
variable real time functions

complexity

unadjusted PFP
EGW

KVg / m / h ×conversion factorvar ⎞
⎛
⎜ (1−G )×
⎟
IH
⎝
⎠
⎛ KG g / m / h ×conversion factorcom ⎞
+ ⎜ G×
⎟
PA
⎝
⎠

(2)

By using formula 2 to join the transformation quotients
from table 3 and table 4, we are able to convert
complexity weighted vertical variable real time functions
into the size measure of unadjusted PFP.
The transformation of vertical common real time
functions is prefixed by the prior procedures. Hence we
have to identify the share of variable signals (V) in a

(1)

The transformation quotients in table 4 enable the final
calculation of unadjusted PFP for horizontal common real
time functions with consideration of complexity (low/
average/ high), validation (soft/ hard) and historical
experiences in developing common assets by a correction
factor (KGg/m/h). The reuse of common assets in every

3

50

The maximum value of G is ½ because a vertical variable real
time function is usually characterized by a majority of
variable (C-B) and a minority of common signals (B).

PFP- metrics considers the reuse of common and variable
assets in SSF.

vertical common real time function which is realized by
table 6.
Table 6: Variable signals in a vertical common real time
function

purpose
How high is
the share of
variable signals
in this vertical
common real
time function?

calculation

interpretation

B
V=
C

B = Amount of
variable signals
in this function.
C = Amount of
all signals in
this function.

3 Process measurement
A process has to be identified as a sequence of events
with a definite beginning and a distinct ending. The
activities within the process progression transform an
object to a desired achievement [1]. The focus of this
chapter is on internal software process flows instead of
external development and maintenance processes which
are examined in the PFP macro analysis [12].

1
0≤V≤
2

Closer to ½
means more
variable signals in
this vertical
variable function.

3.1 Process categorization

Referring to the determination of V we have to separate
this contingent of variable signals from the conversion
quotients of table 4 and multiply them with the
transformation quotients of table 3. The subsequent
merging of these two terms is demonstrated in a general
viewpoint by formula 3.
KG g / m / h ×conversion factorcom ⎞
⎛
⎜ (1− V )×
⎟
PA
⎝
⎠
⎛ KVg / m / h ×conversion factorvar ⎞
+ ⎜ V×
⎟
IH
⎝
⎠

Independent from the level of detail, a process is
characterized by the factors of input, processing and
output [16].
The PFP matrix in table 7 separates process functions
depending on if they are a part of the variability or
commonalities in an embedded SSF. In addition, taking in
the consideration of locality, this separates them whether
the input or the output factor is inside or outside the
application boundary (Distinction among internal and
external functionalities as well as demarcation of the
software which then is measured [11]).
All processes must be logical coherent and defined
from the view of the developer.

(3)

The described categorization, complexity weighting and
transformation of embedded real time functions enables
an unadjusted size measurement for automotive control
units. This measurement method takes account of asset
reuse, validation effort, functional complexity and
historical experiences from the perspective of a software
developer. A practical validation of this PFP step as well
as a comparison to the FPA is executed at
DaimlerChrysler Research and Technology at the present
moment. The first results are very promising since the

3.2 Process complexity weighting
The process complexity matrix is based on a number of
generic process elements in order to support different
modeling techniques. Nodes, operators and edges

Table 7: Process functions to measure the size of a SSF

asset reuse
criterions
common

X

-

-

-

X

X

X

-

-

X

X

-

X

-

X

-

-

X

X

-

X

X

-

-

-

X

-

-

X

X

-

X

-

X

X

-

-

X

X

-

-

X

-

X

X

X

-

-

variable
process functions
variable internal process (PVI,
germ. Prozess- Variabel- Intern)
variable unidirectional process (PVU,
germ. Prozess- Variabel- Unidirektional)
variable bidirectional process (PVB,
germ. Prozess- Variabel- Bidirektional )
common internal process (PGI,
germ. Prozess- Gemeinsam- Intern)
common unidirectional process (PGU,
germ. Prozess- Gemeinsam- Unidirektional)
common bidirectional process (PGB,
germ. Prozess- Gemeinsam- Bidirektional)

locality
output
input
outside
inside
boundary boundary

input
outside
boundary

51

output
inside
boundary

Table 9. Transforming complexity weighted, horizontal
variable process functions

characterize the universal intersecting set of conventional
process models. For this reason and to ease the PFP
complexity weighting, process nodes and operators are
summarized together as well as analyzed jointly with
their related edges.

complexity

Table 8: Complexity matrix to rate process functions

nodes
edges
1 to 5
6 to 8
9 or more

1 to 3

4 to 5

6 or more

low
average
high

average
average
high

high
high
high

The numerical restrictions in table 8 are extracted out of a
database with 43 State Machines of the Unified Modeling
Language (UML) and 13 UML Activity Diagrams [15].
These processes describe an embedded software system
to control a gasoline engine unit from DaimlerChrysler.
An additional calibration of this metric is necessary and
will be done during further investigations to make this
table universally valid for the entire domain of embedded
automotive control units.

PVI

low
(germ, g)

KVg×5
IH

average
(germ, m)

KVm×7
IH

high
(germ, h)

KVh×10
IH

unadjusted PFP
PVU
15
KVg×5
16
IH
5
KVm×8
16
IH
3
KVh×12
16
IH

PVB
KVg×7
IH

KVm×10
IH
KVh×15
IH

Within the high requirements of quality and modular
interfaces for a generic component implementation, the
horizontal common processes are extremely critical. For
this reason the complexity dependant conversion factors
for PGB, PGU and PGI are higher than their counterparts
which transform the horizontal variability.
In terms of going forward, the conversion factors for
horizontal common processes rely also on the real time
oriented conversion factors for commonalities:
• Like the relationship between a PVI and the variable
real time perspective, the PGI conversion values are
identical to the lowest factors for a real time oriented
transformation of common assets {10; 15; 23}.
• The consideration of common interface and gateway
requirements increases the implementation size of a
PGB. For this reason a PGB matches the manner of the
highest real time transformation values for
commonalities {15; 23; 35}.
• A single real time focused complement for the PGU
concept does not exist because a PGU is a PGI- PGBmixture. Therefore, it is necessary to interpolate
additional conversion values by formula 1.
The transformation quotients in table 10 enable the final
calculation of unadjusted PFP for horizontal common
process functions in SSF with a consideration of the same
aspects like in the transformation metric for variable
processes.

3.3 Process transformation
A horizontal process describes variable or common
functionalities of a software product in a homogeneous
manner. These horizontal processes can possibly be the
consequence of an optional variability capsulation inside
a process orientated SSF.
The variable PFP transformation factors for PVI,
PVU and PVB are related to the variable real time
oriented PFP conversion values to achieve an internal
cohesion of the PFP analysis:
• A PVI is comparatively easy to develop because of its
completely internal implementation and therefore,
attached to the lowest factors for a real time oriented
variability conversion {5; 7; 10}.
• The realization of a PVB considers the critically
requirements of variable gateways and interfaces.
Consequentially the PVB conversion values are similar
to the three highest real time focused transformation
factors for variable assets {7; 10; 15}.
• The abstract of a PVU is a PVI- PVB- mixture and
does not correspond with a single real time oriented
concept. Hence the PVU conversion factors have to be
created by using the linear independent, cubic formula
1 for an interpolation.
The matrix in table 9 expresses nine conversion quotients
for PVI, PVU and PVB which are partly derived from
formula 1. Therefore, this transformation table takes
account of complexity (low/ medium/ high), locality
(bidirectional/ unidirectional/ internal), asset reuse (IH)
and historical experiences in SSF orientated development
(KVg/m/h).

Table 10. Transforming complexity weighted, horizontal
common process functions

complexity

52

PGI

low
(germ. g)

KGg×10
PA

average
(germ. m)

KGm×15
PA

high
(germ. h)

KGh×23
PA

unadjusted PFP
PGU
3
KGg×12
16
PA
9
KGm×18
16
PA
7
KGh×28
16
PA

PGB
KGg×15
PA
KGm×23
PA
KGh×35
PA

Table 12: Calculating the proportion of variable elements in a
vertical common process

A vertical process describes variable and common
functionalities of a software product in a heterogeneous
manner. To count these combined processes we separate
them in variable and common assets with dependence on
the strategy: divide et impera - divide and conquer.
Vertical variable processes contain a preponderance
of variable functions. Consequently the table 9 for
calculating horizontal variability is also the main
foundation to count vertical variable processes.
According to this heterogeneity of vertical variable
assets it is necessary to determine the share of common
elements (G) in this vertical variable processes like it is
illustrated in table 11.

purpose
How high is
the share of
variable nodes
and edges in
this vertical
common
process?

How high is
the share of
common nodes
and edges in
this
vertical
variable
process?

calculation
B
G=
C

B = Amount of
common
elements in this
process.
C = Amount of
all elements in
this process.

interpretation
0≤ G ≤

B
V=
C

B = Amount of
variable
elements in this
process.
C = Amount of
all elements in
this process.

interpretation
0≤ V ≤

1
2

Closer to ½
means more
variable elements
in this vertical
common process.

The reuse of formula 3 allows the concluding conversion
of vertical common processes in embedded software
systems. After the identification of variable elements
through V, it is essential to remove these nodes and edges
from the calculations of table 10 like it is shown in the
initial section of formula 3. For a complete representation
of a vertical common process it is indispensable to add
the variable elements by using the transformations
quotients from table 9 as it is displayed in the last fraction
of the generic formula 3.

Table 11: Calculating the proportion of common elements in a
vertical variable process

purpose

calculation

1
2

Closer to ½
means
more
common
elements in this
vertical variable
process.

3 Exemplary case study
To complete and summarize the previous theoretical
investigations we apply these PFP metrics to a restricted
practical case study. Figure 1 represents an embedded
real time function to administer the air condition by an
engine control unit. An embedded process to regulate the
throttle valve is additionally illustrated in figure 2.

Formula 2 enables beside the discussed real time
conversion also the final transformation of vertical
variable processes in embedded control units. The
common nodes and edges which are represented via G
have to be removed from the variable oriented
calculations of table 9 like in the first fraction of the
mentioned equation. Afterwards these excluded elements
must be counted like horizontal commonalities based on
table 10. This is accomplished by the second part of
formula 2.
As the opposite of vertical variability, vertical
common processes include a majority of common
functions. Table 10 is a metric to count horizontal
commonalities and also the main component to measure
vertical common processes.
At the beginning of this transformation we have to
clarify the share of variable nodes and edges (V) in
vertical common processes according to the model in
table 12.

Figure 1: Exemplary real time function from an embedded SSF

The real time function from figure 1 has to be
transformed into the temporary size measure of
unadjusted PFP. Beside this diagram we need the
following basic information to use the discussed PFP
micro analysis in this measurement situation:
• Soft timing constrains;
• Vertical variability with two variable (highlighted
gray) and six common signals;
• High complexity in dependence on the signal structure
and table 2;

53

• Quota of variable elements is V = 2/8 = 0,25 according
to table 6;
• Standardized correction factors (KGg/m/h = 1; KVg/m/h =
1) are used because of unavailable historical
experiences;
• Variable elements are present in two instances of a
SSF including tree products.
The size of this embedded vertical common, soft real time
function accounts seven unadjusted PFP according to
formula 3.4

the brief example above we would count 17 unadjusted
PFP.
The sum of all unadjusted PFP describes a size
measure of an embedded control unit and can be used as a
coarse effort indicator for a SSF development [8]. Finally
we have a size measurement system and therefore a base
to build an effort estimation tool for embedded SSF in the
automotive sector.

4 Conclusion and further research
The metrics in this article are independent from the
development techniques of process orientated SSF and
accomplish an unadjusted size measurement for
embedded control units in the automotive domain.
To establish an ISO/ IEC 14143 compatible
measurement system was not the goal of our PFP
approach. This is justified in that we need to analyze the
viewpoint of a software developer instead of the
consumer perspective in the automotive sector.
Furthermore we appreciate the flexibility to extend our
approach with a PFP macro analysis which is not
compliant to the generic rules of functional sizing in ISO/
IEC 14143 but helpful for accurate cost estimation.

Figure 2: Exemplary process function from an embedded SSF

For a subsequent conversion of the process function in
figure 2 we need the following aspects to apply the
explained PFP micro analysis:
• External input “revolution” (germ. Motordrehzahl) and
external
output
“throttle
position”
(germ.
Drosselklappenöffnung);
• Horizontal variability modeled in an encapsulated
variable asset;
• Average complexity according to the amount of nodes/
edges and table 3;
• Standardized correction factors (KVg/m/h = 1) are
reused because of unavailable historical experiences;
• Implementation of the variable process in one instance
of the SSF.
The size of this embedded horizontal variable,
bidirectional process embraces ten unadjusted PFP in
dependence on table 9.5
The PFP sums of all measured real time functions and
processes in an embedded automotive control unit
describe the unadjusted size of a previously defined
counting scope6 [11]. For a counting scope in the size of

KG h ×23 ⎞ ⎛
KVh ×10 ⎞
⎛
⎟ + ⎜ V×
⎟
PA ⎠ ⎝
IH ⎠
⎝
1×23
1×10
=7
= (1 − 0, 25) ×
+ 0, 25 ×
3
2

4 unadjusted PFP= ⎜ (1− V)×

KVm ×10 1×10
=
= 10
IH
1

5

unadjusted PFP =

6

“Application independent border which can be embrace more
or less functionality as a single software program” [11].

Figure 3: The PFP approach to estimate the effort in process
oriented SSF

54

additional functionality that was not specified in the
requirements but identified during development.
The additional documentation process is an integral
part of the PFP approach in order to retrace prior
calculations and to support future effort estimations.
To validate the PFP effort estimation system we
developed a PFP counting tool and apply our approach
currently at a SSF oriented project with DaimlerChrysler
Research and Technology and in an eBusiness project
seminar at the University of Leipzig. The first results in
terms of a comparison between the PFP analysis and
other sizing methods such as the FPA or COSMIC FFP
are very promising. The PFP approach covers the
characteristics of SSF much better than the FP or
COSMIC FFP analysis particularly with regard to the
special aspects of reuse, processes and general system
characteristics.
During the validation process we will derive
regression lines for both domains in order to forecast the
effort of developing process oriented SSF. With the
associated two dimensional ([quality-] adjusted PFP;
development effort) diagram we are able to predict
software development costs after the very early project
phase of asset scoping.

Figure 3 illustrates the entire PFP concept to estimate the
effort for process oriented SSF in multiple domains. The
reason that the PFP analysis focuses the eBusiness as well
as the automotive sector is that both domains are
characterized by processes. Therefore, a process
orientation increases the efficiency in a domain specific
SSF development and reduces the costs of software
products in these exemplary sectors.
Within the first PFP step we identify the type of count
which can be a development or a reuse project as well as
a count for a single product of the SSF [9]. The following
PFP action which is illustrated in figure 3 defines the
application boundary and the counting scope to
demarcate the process oriented SSF [11].
An unadjusted size measure is calculated by the
eBusiness oriented PFP micro analysis in figure 3. Within
this framework we categorize, weight and transform a
data as well as a process oriented view. In [9], [10] and
[11] detailed description of these PFP parts may be
found.
The real time and process oriented PFP modules
(categorization,
complexity
weighting
and
transformation) with a grey emphasis are elucidated in
this article and determine an unadjusted size measure for
embedded SSF in the automotive sector.
A completion of the domain specific PFP micro
analysis is to be gained by the PFP macro analysis which
considers external influences on development costs with
soft characteristics. The PFP macro analysis allows a
flexible modification and extension of all influencing
factors. Therefore, this part of the PFP analysis is
versatile applicable and could be used as a potentional
substitution of the FPA or Mark II adjustment
procedures.
Inside of the PFP macro analysis we examine twenty
domain independent influences according to the
approaches of FPA, Mark II and in allusion to the Object/
Data Point Method. Subsequently we look at fifteen
domain specific system characteristic for the eBusiness as
well as for the automotive domain. After these obligatory
steps it is possible to calculate adjusted PFP as a size
measure which is comparable to other adjusted functional
size measures. Alternatively we can determine quality
adjusted PFP by regarding quality aspects in dependence
on ISO/ IEC 9126. With this action we loose our
compatibility to FPA and Mark II but realize a more
precise measure for an exact effort estimation in process
oriented SSF.
The procedure of estimating SSF development costs is
executed before (initial calculation), in between (interim
calculation) and after (final calculation) the software
engineering project according to figure 3. This enables
the consideration of SSF evolution between variable and
common assets. Furthermore it is possible to detect

References
[1] Abran, A. Functional Size Measurement for Real Time and
Embedded Software. In: Fourth International Symposium
and Forum on Software Engineering Standards, Curitiba
May 1999.
[2] Allgaier, H. J., Segmentierung der Auftragsabwicklung:
Modellanalyse einer Gestaltungskonzeption, PhD- Thesis,
Technische Universität München, München 1994.
[3] Beuche, D. Composition and Construction of Embedded
Software Families. Dissertation, Otto-von-GuerickeUniversität, Magdeburg 2003.
[4] Broy, M. Automotive Software Engineering. In:
Proceedings of the 25th International Conference on
Software Engineering (ICSE’03), Portland, May 2003.
[5] Clements, P., Northrop, L. Software Product Lines:
Practices and Patterns. Addison- Wesley, Boston et al.
2002.
[6] Commom Software Measurement International Consortium
(Ed.), COSMIC- FFP: Measurement Manual 2.2. Québec
2003.
[7] Diaz- Herrera, J., L., Madisetti, V., K. Embedded Systems
Product Lines. In Proceedings of Software product lines,
CSE Workshop. Limerick, June 2000.
[8] Kamm, C., Siedersleben, J., Schick, D., Saad, A.
Systematische Aufwandsschätzung für Software im
Fahrzeug. In: OBJEKTspektrum Nr. 6, November/
Dezember 2004, pp. 60-64.
[9] Kiebusch, S. An approach to a data oriented size
measurement in Software- Product- Families. In: Abran,
A., Bundschuh, M., Dumke, R., Ebert, C., Zuse, H. (Hrsg.)

55

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]

[20]

Metric News: Journal of the GI- Interest Group on
Software Metrics, Vol. 9, Nr. 1, August 2004, pp. 60-67.
Kiebusch, S. Franczyk, B. Functional size measurement of
processes in Software- Product- Families. In: Proceedings
of the 2nd Software Measurement European Forum, Rome,
March 2005, pp. 161-172.
Kiebusch, S. Towards a Function-Point oriented analysis
of process focused Software-Product-Families. In:
Proceedings of the Net.ObjectDays 2004: 5th Annual
International Conference on Object- Oriented and Internetbased Technologies, Concepts, and Applications for a
Networked World, Erfurt, September 2004, pp. 147-152.
Kiebusch, S., Richter, E., Weiland, J. Metriken: Definition
und Validierung. PESOA TR x/ 2005, Universität Leipzig,
To be published at: http://www.pesoa.org, Leipzig 2005.
Kopetz, H. Real- Time Systems: Design Principles for
Distributed Embedded Applications. Kluwer Academic
Publishers, Boston et al. 1997.
Lauber, R., Göhner, P. Prozessautomatisierung 1:
Automatisierungssysteme und -strukturen, Computer- und
Bussysteme für die Anlagen- und Produktautomatisierung,
Echtzeitprogrammierung und Echtzeitbetriebssysteme,
Zuverlässigkeits- und Sicherheitstechnik. 3. Edition,
Springer- Verlag, Berlin et al. 1999.
Richter, E., Schnieders, A., Weiland, J. Prozessanalyse
und –modellierung in der Domäne Automotive. PESOA TR
07/ 2005, DaimlerChrysler Research and Technology and
Hasso- Plattner- Institut, http://www.pesoa.org, Ulm and
Potsdam 2005.
Scholz, R., and Vrohlings, A. Prozeß- StrukturTransparenz. In: Gaintanides, M., Scholz, R., Vrohlings,
A., and Raster, M. (Ed.), Prozeßmanagement: Konzepte,
Umsetzungen und Erfahrungen des Reengineerings,
München 1994, pp. 37-56.
Sholom, C., Zubrow, D., Dunn, E. Case Study: A
Measurement Program for Product Lines. Technical Note,
CMU/ SEI-2004-TN-023, Software Engineering Institute,
Carnegie Mellon University, Pittsburgh 2004.
Speck, A. Reusable Industrial Control Systems. Industrial
Informatics. Special Section of the IEEE Transactions on
Industrial Electronics Society, 50(3), pp 412-418, 2003.
The International Function Point Users Group (Ed.)
Function Point Counting Practices Manual: Release 4.2.
Clarkston 2004.
United Kingdom Software Metrics Association (Ed.) MK II
Function Point Analysis: Counting Practices Manual
Version 1.3.1. Kent 1998.

56

Evaluating the Impact of a New Technology
Using Simulation: The Case for Mining
Software Repositories
David Raffo, Member IEEE, Tim Menzies, Member IEEE
Abstract— Adopting new technologies on a
development process can be a risky endeavor. Will the
project accept the new technology? What will be the
impact? Far too often the project is asked to adopt the
new technology without planning how it will be applied
on the project or evaluating the technology’s potential
impact. In this paper we provide a case study evaluating
one new technology. Specifically we assess the merits of
defect detectors learned from software repositories of
static code measures (Halstead and McCabe). Using
process simulation, we find situations where the use of
such detectors is useful and situations where the use of
such detectors is useless for large-scale NASA projects
that utilize a process similar to the IEEE 12207 systems
development lifecycle.

the probability of false alarms while at the same
time reducing the amount of code that must be
manually inspected in order to achieve these
results. Given these improvements, it seems
possible that this new technology would be
“ready for prime time”. Software Process
Simulation (SPS) was used to assess whether this
was in fact the case. More importantly, SPS was
used to identify under what conditions the
technology was “ready for prime time” and
under what conditions it was not.
Although a variety of methodologies where
used to conduct the evaluation of this new
technology including the Process Tradeoff
Analysis Method (PTAM) [2], general
simulation methods regarding sensitivity analysis
and portions of the Broad Range Sensitivity
Analysis approach (BRSA) [3]. We believe the
main contribution of this paper is a practical one
dealing with technology planning, application
and adoption. We see that this case study
provides strong evidence of the benefit of
applying process simulation in industrial
settings.
As mentioned, section 2 (background) will
provide an overview of the process simulation
model and approach used in this case study as
well as recent research advances by Menzies et
al. In section 3, we provide an overview of the
case study performed. Due to space constraints,
we have only presented two scenarios in this
paper – the first scenario showing an
improvement in overall project performance and
the section scenario showing and overall
reduction in performance. In section 4, we
present case study results for the two scenarios
selected. Finally, we conclude in section 5.

Index
Terms—Software
Process
Simulation,
Technology Adoption, Technology Evaluation, Data
Mining

I. INTRODUCTION
Good research results are wasted unless there
is a compelling business case to use them.
Without such a case, a project manager may not
be convinced that they should, for instance,
reallocate scarce resources to implement a new
technology on their project. The aim of this
paper is to offer one example of a business case
evaluating the impact of a new technology
currently under development using software
process simulation.
Process simulation was selected as a means to
evaluate this new technology because it captures
the details associated with how the new
technology would actually be applied in the
field. Moreover, process simulation allows for
extensive sensitivity analysis which enables
different scenarios and special case conditions to
be checked. This kind of analysis gave the
developers of the technology realistic feedback
about their targeted field of use and their
envisioned approach for roll out of the tool.
The new technology selected for this
evaluation was defect detectors learned from
software repositories of static code measures
(Halstead and McCabe). As will be discussed in
the background section, recent research advances
by Menzies et al. [1] have significantly improved
the probability of detection of defects, lowered

II. BACKGROUND
A. Process Simulation and the Model Used for
this Study
Process simulation is commonly used in many
industries including manufacturing and service
operations to address these kinds of issues. In

57

Figure 1 – IEEE 12207 Process Simulation Model with IV&V Layer

recent years, process simulation has been applied
to software development processes1. The key
advantage to process simulation is that these
models can capture the details associated with
the development process and provide a
systematic approach for incorporating metrics
data and creating the necessary process level
predictions along multiple measures of
performance [4], [5], [6]. Simulation modeling
tools (e.g. Arena, Extend, Stella, etc.) simplify
conducting sensitivity (or ”what if”) analyses. As
a result, process simulation models can explicitly
capture localized changes to the process made by
implementing a new tool, technology or method
and then predict the overall project-level
impacts.
In this study, we employ process simulation to
assess the impact of applying learned fault
detectors under multiple possible operational
scenarios. The specific process simulation model
used for this study is a model of the IEEE 12207
systems development process [7]. This process is
representative of the process used on large-scale
NASA and US Department of Defense (DoD)
projects. The model contains industry standard
benchmark data from [8] for largescale systems
development. Moreover, the model has been
tuned using a data set from 8 NASA projects

over 100 KSLOC in size. Predictions made with
the model provide similar accuracy to those
obtained using COCOMO I (i.e. predictions were
within 30% of actual values, more than 70% of
the time).
Figure 1 shows a top-level view of the software
development model used for this study. As can
be seen in that figure, the main life cycle phases
of the IEEE 12207 process are:
x Process implementation
x System and software requirements analysis
x Software architecture and, detailed design
x Software coding and unit testing
x Software and system integration planning
x Integration and qualification testing
x Integration and acceptance support
Figure 1 also shows that we have augmented
IEEE 12207 with an additional IV&V layer that
models the actions of external consultants
auditing software artifacts.
B. The New Technology: Learned Defect
Detectors
Particular measures can be assessed by
combining them to form detectors and then
assessing each specific detector using: (1) PD:
the probability of defecting faults (a.k.a. recall),
(2) Accuracy, (3) The effort associated with the
detector triggering, and (4) The probability of
false alarm (PF). Ideally, a detector has a high
probability of detection and a low false alarm
rate but, in practice, increasing one also increases
the other.
The art of data mining is finding some way to
trade-off competing PDs and PFs. Numerous
data mining algorithms have been developed that

1

See the proceedings of the ProSim International
Workshops, at http:
//www.prosim.pdx.edu/ and special issues of the
Journal of Systems
and Software (Vol 46, No 2/3, Vol. 47, No. 9 and Vol, 59,
No. 3), and the
international journal of Software Process: Improvement and
Practice, Vol 5,
No. 2/3, Vol. 7, No. 3/4 and Vol 9, No. 2) on this topic.

58

achieve this goal. Elsewhere Menzies et,al. have
compared several widely-used methods [1] and
concluded that Bayesian methods usually outperform decision-tree methods, at least for the
data sets studied here.
A stratification effect was also seen in those
studies; i.e. learning on specialized subsets (e.g.
just examples from a particular sub-sub-system)
is often preferred to learning from the entire
system.
Those studies also saw a previously
unreported effect. It is well known that the
efficacy of the learned theories increases as the
number of training examples increases.
However, surprisingly, that increase tapers off
remarkably early for the NASA defect data. In
fact, given a binary classification problem (ie.
“true” means “defects > 0” and “false” means
“defects=0”), no statistically significant increase
was seen after training from only 50 examples.
Taken together, these results suggest that it
would be relatively simple and quick to learn
specialized detectors for local domains from data
collected just from, say, a sub-sub-system. The
rest of this paper uses process simulation to see
the effects of this policy.
Before turning to the business case
simulations, we need to state some underlying
assumptions. One important issue is the meaning
of PD (probability of detection). Our research
results show PDs learned from defects logs
containing issue reports from multiple sources:
inspections, software tests, hardware tests,
formal method results, etc. As a result of this
extensive checking, we assert that the PDs
shown above are equal to the percentage of
defects ever found in the system. Another kind
of PD, would be that seen in data miners
executing on just the issue reports seen in the
most recent manual inspections. In this case, the
PD changes to a smaller value (which we denote
PD0) since our learners might only find a certain
percentage of the defects contained in the
inspection logs which, in turn, is some
percentage of the total number of defects.
Further research is required to determine the
actual value of the PD0 learned from just
inspection logs. While we await that data, in this
paper, we present two scenarios. In Scenario I,
we assume PD refers to a percentage of the total
number of errors; i.e. the “IV&V situation”
where we are learning from rich defect logs. In
Scenario II, we will assume PD0 i.e. the “V&V
situation” where we are only learning from the
results of inspections.

Scenario I’s conclusions will endorse using
defect detectors while Scenario II’s conclusions
will be more negative. We also make several
other assumptions, based on the results of [1, 9
and 10]:
A1: Based upon the early plateau effect [1], we
will use 50 as the number modules needed
for data miners to learn defect detectors at
the sub-sub-system level.
A2: Effort associated with our detectors being
triggered is linearly proportional to line of
code; i.e. Effort = PD + 5% . . . 10%.
A3: For our detectors, PF  10%.
A4: PD associated with the detectors; i.e. PD =
40. . .50%.
A5: Assumption A4 assumes, in turn, that defect
detectors are learned from data divided
below the sub-system level.
In principle, these assumptions can be checked
and adjusted as necessary. This is one of the
strengths of process simulation. Our current
process model has been built and repeatedly
checked over the last three years.
III. BUSINESS IMPLICATIONS OF DEFECT
DETECTORS
A. Baseline Model Results
Baseline performance was predicted in
terms of development effort (or cost), effort
devoted to rework, IV&V effort, project
duration, corrected defects, and escaped (or
delivered) defects. Our baseline of the AS-IS
process presumes:
1.
The project is 100,000 lines of code.
2.
Figure 1 shows the assumed software
process: i.e. IEEE 12207+IV&V model.
3.
Full Fagan inspections [11] are done at
all development phases- including at the
coding phase. These manual inspections
find 40% to 60% of the total defects.
The actual baseline performance for the ASIS process (without using data miners applied to
defect detectors as part of IV&V activities) can
be seen in Figure 2.
B. Scenario I: Defect Detectors and IV&V
For the first TO-BE process scenario, we apply
the defect detectors as part of an independent
verification and validation (IV&V) step after
coding and code inspections are complete. In this
TO-BE scenario, defect detectors are utilized in
IV&V work as follows:
x Defect logs and code modules that have
completed code inspections and other forms
of testing are sent to IV&V.

59

Total Size
(KLOC)

Baseline
Average
99.79
Std Dev
4.00
Case 1: detcap IV&V = 0
Average
99.79
Std Dev
4.00
Deltas
Case 2: detcap IV&V = 0.01
Average
99.79
Std Dev
4.00
Deltas
Case 3: detcap IV&V = 0.02
Average
99.79
Std Dev
4.00
Deltas
Case 4: detcap IV&V = 0.03
Average
99.79
Std Dev
4.00
Deltas
Case 5: detcap IV&V = 0.04
Average
99.79
Std Dev
4.00
Deltas
Case 6: detcap IV&V = 0.05
Average
99.79
Std Dev
4.00
Deltas
Case 7: detcap IV&V = 0.10
Average
99.79
Std Dev
4.00
Deltas

Total +
Effort IV&V
(PM)

Total
Effort
(PM)

Total
Rework
Effort
(PM)

Total
Duration
(Month)

Average
Duration

Total
Defect
Corrected

Total
Latent
Defects

Code
Inspection
Effort
(PM)

781.41
27.66

781.41
27.66

160.56
6.95

32.81
1.43

28.51
1.22

5,907.08
257.00

507.81
22.00

9.67
0.29

782.57
27.67
-1.16 Ż

781.41
27.66
0.00

160.56
6.95
0.00

32.85
1.43
-0.05

28.51
1.22
0.00

5,907.08
257.00
0.00

507.81
22.00
0.00 Ż

10.83
0.30
-1.16

782.06
27.67
-0.65 Ż

780.90
27.65
0.51

160.05
6.93
0.51

32.85
1.43
-0.04

28.50
1.22
0.00

5,928.55
258.00
-21.47

505.79
22.00
2.02 Ż

10.83
0.30
-1.16

781.27
27.66
0.14 Ż

780.11
27.64
1.30

159.29
6.89
1.27

32.83
1.43
-0.03

28.49
1.22
0.02

5,931.95
258.00
-24.87

502.40
21.00
5.41 Ż

10.83
0.30
-1.16

780.48
27.64
0.93 Ż

779.32
27.63
2.09

158.53
6.86
2.03

32.82
1.43
-0.01

28.47
1.22
0.03

5,935.34
258.00
-28.26

499.00
21.00
8.81 Ż

10.83
0.30
-1.16

779.68
27.64
1.72 Ż

778.52
27.62
2.88

157.77
6.83
2.79

32.80
1.43
0.00

28.46
1.22
0.05

5,938.73
258.00
-31.65

495.61
21.00
12.20 Ż

10.83
0.30
-1.16

778.89
27.63
2.51 Ż

777.73
27.61
3.67

157.01
6.80
3.55

32.79
1.43
0.02

28.44
1.21
0.06

5,942.13
259.00
-35.05

492.22
21.00
15.59 Ż

10.83
0.30
-1.16

774.94
27.58
6.47 Ż

773.78
27.57
7.63

153.21
6.63
7.35

32.72
1.43
0.09

28.37
1.21
0.14

5,959.10
259.00
-52.02

475.24
20.00
32.57 Ż

10.83
0.30
-1.16

Fig 2. Operational Scenario I: Using Defect Detectors in IV&V Mode

x

x

Defect detectors are learned on the logs and
then applied to 100% of the code. Once the
logs are in a format suitable for the learners,
this can be done automatically and quickly
(a mere matter of seconds). In our
simulations we make the conservative
assumption that preparing the input logs
takes two person days(or 16 hours of effort)
for a large 100 KSLOC project.
The defect detectors identify code modules
that are likely to contain defects. Since the
code modules will have gone through code
inspections and other assessment measures
during project level V&V, many of the
modules that are identified will already be
known to contain defects. These modules
will not be looked at again. Instead, the
defect detectors will be used to identify
modules where no defects were found
during their initial code inspections, but
whose characteristics indicate that these
modules are likely to have defects.

The modules that trigger the detectors are
then reinspected. The re-inspection rate’s
upper bound is PF; i.e. 10% (from [A5]).
To assess the impact of learning data miners
for defect detectors, then using them in IV&V
mode, the next parameter required is an estimate
of the percentage of the escaped defects that will
be found using the above procedure. At present,
more research is necessary to empirically
determine this percentage. While we await those
results, we can use the process simulation model
to identify the minimum percentages required in
order to break even (where expenses equal
benefits). Moreover, the process simulation
model can help assess the risk of applying the
defect detectors by examining the worst-case
scenario (i.e. when no additional defects are
detected).
The results of these tests are shown in Figure 2
and, for the purposes of this discussion, we focus
on the cells marked with a black triangle (Ż).
These cells show the difference between the
baseline data and the results from Scenario I. As
x

60

Total Size
(KLOC)
Baseline
Average
99.79
Std Dev
4.00
Case 2: detcap IV&V = 0.48
Average
99.79
Std Dev
4.00
Deltas
Case 1: detcap IV&V = 0.47
Average
99.79
Std Dev
4.00
Deltas

Total
Effort
(PM)

Total
Rework
Effort (PM)

Total
Duration
(Month)

Average Duration

Total Defect
Corrected

Total
Latent
Defects

Code
Inspection
Effort (PM)

781.41
27.66

160.56
6.95

32.81
1.43

28.51
1.22

5,907.08
257.00

507.81
22.00

9.67
0.29

780.24
27.52
1.17 Ż

163.24
7.07
-2.68

32.74
1.43
0.07

28.44
1.21
0.07

5,895.33
257.00
11.75

519.56
22.00
-11.75 Ż

5.52
0.14
4.15

781.53
27.54
-0.13 Ż

164.58
7.13
-4.02

32.78
1.43
0.03

28.47
1.22
0.03

5,889.45
256.00
17.63

525.43
22.00
-17.62 Ż

5.42
0.14
4.24

Fig 3. Operational Scenario II: Using Defect Detectors in V&V Mode.

can be seen, using defect detectors breaks-even
(i.e. the Delta goes from negative to positive) if
the above approach can detect an additional 1 to
2% of the latent defects in the code and starts
showing a positive benefit in both effort and
latent defects at 3%. The worst case is that an
additional 1.16 person months would be
expended doing inspections that do not find any
new defects.
Moreover, if 5% or 10% of the latent defects
are found, the quality of the code would be
improved by an average 15.5 and 32.5 defects
respectively and an average 2.5 and 6.5 person
months of effort respectively could be saved. To
repeat, the minimum performance target for
defect detectors to be beneficial for IV&V would
be 3% additional defects detected and the
maximum exposure would be 1.16 person
months of effort. Based on our commercial work
with companies exploring re-inspections of their
code, we are confident that the 3% additional
defects threshold can be easily surpassed.
However, in this forum, we cannot publish
supportive evidence for this claim since it is
based on proprietary data.

A minimum set of code was inspected. From
assumption A3, 50 modules per sub-subsystem, or 11.5% of the code, would be
required to achieve plateau performance.
x Using the defect logs from these inspections
and the inspected modules, defect detectors
were learned using data miners and applied
to the rest of the code.
x Modules were identified as likely candidates
for defects.
x Only those portions of the code that are
tagged as likely ”hot spots” were inspected.
Under this scenario, the defect detectors would
select 61.5% of the code for further inspection.
This would result in 38.5% reduction in
inspection effort (approximately 3.7 personmonths) and inspection schedule savings.
However, process simulation shows that the
savings in inspection effort would not offset the
increase in defect detection and rework costs
associated with finding these defects later in the
development process. Figure 3 shows the
baseline results and results of having an expected
defect detection capability of 47% which will
cause the process using defect detectors during
project V&V to break even on effort (but have an
overall poorer quality).
If we are learning on an inspection log
containing 50% of all defects (the expected
case), then such a 47% overall defect detection
rate is only possible if a data miner can learn
near-perfect detectors with a PD of 98% (i.e.
50% * 98% = 47%). Since this is highly unlikely
(to say the least), the conclusion of Scenario II
must be to doubt the value of defect detectors for
improving V&V.
x

C. Scenario II: Defect Detectors and
Inspection-based V&V
Scenario II is the case where analysts have a
much weaker training set; i.e. only the results of
internal inspections. This is the PD0 case where
the data miners find only some the defects in
defect logs which contain only some of the
project defects. One situation where this could
happen would be when a team declines to wait
for the IV&V team to report issues. Instead,
they use their own experience of, say, their code
inspections to build defect detectors. For
example:

61

REFERENCES

IV. CONCLUSION
[1]

Process simulation is a powerful tool for
conducting what-if queries on software
processes. We have shown above two such whatifs: in Scenario I we did not know the impact of
applying data miners to IV&V yet, in Figure 7,
we could still identify the break even point
where such mining was useful.
More generally, we see this kind of analysis as
being very beneficial for more than just assessing
defect detectors learned from software
repositories. The above process simulation is an
example of a general technological assessment
process where we can:
x Identify the conditions under which
application of a new technology would be
beneficial.
x As importantly, we can identify situations
when applying this technology would not be
beneficial.
x We can have performance benchmarks or
criteria that vendors of a new technology
would need to achieve in order for an
organization to consider investing and
adopting their technology.
x We can diagnose problems associated with
implementing a new tool or technology and
identify new and creative ways to apply the
technology to the benefit of the organization
(and the vendors)
x Finally, we can do all this before the
technology is purchased or applied and
therefore can save scarce resources available
for process improvement.

[2]

[3]

[4]

[5]
[6]

[7]

[8]

ACKNOWLEDGMENT

[9]

This research was conducted at West
Virginia University, Portland State University,
partially sponsored by the NASA Office of
Safety and Mission Assurance under the
Software Assurance Research Program led by
the NASA IV&V Facility. Reference herein to
any specific commercial product, process, or
service by trade name, trademark, manufacturer,
or otherwise, does not constitute or imply its
endorsement by the United States Government.

[10]

[11]

62

T. Menzies and J. DiStefano and A. Orrego
and R. Chapman, “Assessing Predictors of
Software Defects", Proceedings, workshop
on Predictive Software Models, Chicago",
2004,
Available
from
http://menzies.us/pdf/04psm.pdf
D.M. Raffo, “Modeling software processes
quantitatively and assessing the impact of
potential process changes of process
performance,” May 1996, Ph.D. thesis,
Manufacturing and Operations Systems.
Wakeland, Martin, and Raffo, “Using
Design of Experiments, Sensitivity Analysis,
and Hybrid Simulation to Evaluate Changes
to a Software Development Process: A Case
Study”, Software Process: Improvement and
Practice, Vol 9, No 2, 2004
[7] D. Raffo and M. Kellner, “Impact of
potential process changes: A quantitative
approach to process modeling,” in Elements
of Software Process Assessment and
Improvement, K. El Emam and N.
Madhavji, Eds. 199, IEEE Computer
Society.
G.A. Hansen, Automating Business Process
Reengineering, Prentice Hall, 1997.
M. Laguna and J. Marklund, Business
Process Modeling, Simulation, and Design,
Pearson Prenctice Hall, 2004.
Institute of Electrical and Inc. Electronics
Engineers, “Iso/iec 12207 standard for
information technology - software lifecycle
process,” 1998.
C. Jones, Applied Software Measurement
(second edition), McGraw Hill, 1991.
T. Menzies, J. Di Stefano, K. Ammar, K.
McGill, P. Callis, R. Chapman, and Davis J,
“When can we test less?,” in IEEE
Metrics’03,
2003,
Available
from
http://menzies.us/pdf/03metrics.pdf.
Tim Menzies and Justin S. Di Stefano,
“How good is your blind spot sampling
policy?,” in 2004 IEEE Conference on High
Assurance Software Engineering, 2003,
Available
from
http://menzies.us/pdf/03blind.pdf.
M. Fagan, “Design and code inspections to
reduce errors in program development,”
IBM Systems Journal, vol. 15, no. 3, 1976.

1

A Software Process Simulation Model of Extreme
Programming
Marco Melis, Student Member, IEEE Ivana Turnu, Student Member, IEEE Alessandra Cau
and Giulio Concas
Abstract—In this paper we present a simulation model
that we have developed to evaluate the applicability and
eﬀectiveness of Extreme Programming (XP) process. The
XP process has been modelled and a simulation executive
has been written, enabling to simulate XP software development activities. The model follows an object-oriented approach, and has been implemented in Smalltalk language,
following an XP process itself. It will be able to vary the
usage level of some XP practices and to simulate how all the
project entities evolve consequently. Here we present some
results concerning the simulation of an XP process varying
the adoption level of the Pair Programming practice.

I. Introduction
XTREME Programming (XP) is a new lightweight
methodology of software development which has become very popular in recent years, and which has been
recently reviewed [1]. The eﬀectiveness of XP practices,
however has still to be quantitatively assessed. Works that
report quantitative results are still very scarce and propose
empirical studies which are extremely costly and virtually
impossible to be performed at a large degree of completeness. Therefore, the use of simulation to estimate and
verify the eﬀectiveness of a particular development process
represents an alternative solution.
In this paper we present a simulation model we have
developed in order to evaluate the applicability and eﬀectiveness of XP process.
A simulation executive has been written, enabling to
simulate XP software development activities. The model
follows an object-oriented approach, and has been implemented in Smalltalk language, following XP process itself.
It will be able to vary the usage level of some original XP
practices, which are still present also in the second edition
of Beck’s book [1], such as Real Customer Involvement
(On-site customer), Pair Programming, Test-First Programming, Continuous Integration, and to simulate how
all the project entities evolve consequently.
Presently, the modules simulating the planning and code
production activities are fully operative while others are
under development. We are currently able to simulate the

inﬂuence of Pair Programming and Test Driven Development practices. Here we present some results concerning
the simulation of these modules.
The remainder of the paper is organized as follows: in
section II we give some information on the Software Process Simulation approach; in section III an overview of the
Extreme Programming and of the Pair Programming practice in particular is reported. Section IV is aimed to expose
some important related works on the simulation modeling
of XP, while in sections V and VI we describe our model.
Sections VII and VIII report data and results obtained
from our simulator.

E

Department of Electrical and Electronic Engineering
Università di Cagliari, Cagliari, 09123, Italy
Phone: (+39) 070-6755774, email: marco.melis@diee.unica.it
Department of Electrical and Electronic Engineering
Università di Cagliari, Cagliari, 09123, Italy
Phone: (+39) 070-6755774, email: ivana.turnu@diee.unica.it
Department of Electrical and Electronic Engineering
Università di Cagliari, Cagliari, 09123, Italy
Phone: (+39) 070-6755774, email: alessandra.cau@diee.unica.it
Department of Electrical and Electronic Engineering
Università di Cagliari, Cagliari, 09123, Italy
Phone: (+39) 070-6755781, email: concas@diee.unica.it

II. Software Process Simulation
Software Process Simulation (SPS) is becoming increasingly popular in the software engineering community, both
among academics and practitioners [2]. In fact, new and
innovative software engineering techniques are being developed constantly, so a better understanding of them is
useful to evaluate their eﬀectiveness and to predict possible problems. Simulation can provide information about
these issues avoiding real world experimentation, which is
too costly in terms of time and money. This ﬁeld of SPS
has attracted growing interest over the last twenty years,
but only in recent years it is beginning to be used to address several issues concerning the strategic management
of software development and process improvement support.
It can also help project managers and process engineers to
plan changes in the development process. The development of a simulation model is an inexpensive way to collect information when costs, risks and complexity of the
real system are very high.
In order to create a connection between real world and
simulation results, it is usual to combine empirical ﬁndings
and knowledge from real processes. In general, empirical
data are used to calibrate the model, and the results of
the simulation process are used for planning, design and
analyzing real experiments. A SPS model usually focuses
on speciﬁc portions of the software development process,
although it may represent the entire life cycle of the development process.
A model is at any rate an approximation and a simpliﬁcation of the real system, and the model developer has
to investigate in order to identify and model the aspects
of the software process that are relevant to addressing the
issues and questions he is studying.

63

2

III. An Extreme Programming Overview
In recent years Extreme Programming (XP) [3] is the agile methodology that has received most attention. It rejects
the pomp and ceremony of traditional waterfall methodologies and the use of complex tools and solutions in favor of
people doing the simplest things that could possibly work.
It is deﬁned by a set of practices that embody four fundamental values: communication, feedback, courage and
simplicity. XP is based on the famous twelve practices of
Planning Game, Small Releases, Metaphor, Simple Design,
Testing, Refactoring, Pair Programming, Continuous Integration, Collective Code Ownership, On Site Customer,
Don’t Burn Out (40-Hour Work Week), and Coding Standards. These practices have been recently reviewed by the
original proposer itself – Kent Beck – in the second edition
of the ﬁrst XP book [1].
A. More on Pair Programming
Pair Programming practice states that any production
code must be created by a pair of developers working together at the same computer. Here we report some quantitative studies conducted to assess the validity and eﬃciency of this practice.
An experiment conducted by Nosek at Temple University studied 15 full-time, experienced programmers working for 45 minutes on a challenging problem, important
to their organization, in their own environment, and with
their own equipment. Five worked individually, ten worked
collaboratively in ﬁve pairs, at the same conditions. The
pairs completed the task 40% more quickly and eﬀectively
by producing better algorithms and code in less time [4].
Laurie Williams – University of Utah – conducted a controlled experiment on 28 students working in pair programming and 13 students in individual programming. She has
found that paired programmers spent, on average, 15%
more time to complete two projects than the solo programmers spent to complete one, suggesting that pair programming is 40-50% faster than solo programming. On the
other hand, pair programmers produced 15% fewer defects
than the individuals and implemented the same functionalities in fewer lines of code [5].
Both the above experiments and many others have found
that people learn signiﬁcantly more about the system and
about software development, also giving better information
ﬂow and team dynamics.
IV. Related work
In spite of the great diﬀusion of Extreme Programming
(XP) in academic and industrial ﬁeld, only recently the
ﬁrst attempts of XP processes simulation have appeared,
all using the System Dynamics approach. Here we cite
some signiﬁcant contribution.
In [6] Cao proposes a system dynamic simulation model
to investigate the applicability and eﬀectiveness of agile
methods and to examine the impact of agile practices on
project performance in terms of quality, schedule, cost, customer satisfaction.

Misic et al. [7] investigate the possibility of using system dynamics to model, and subsequently simulate and
analyze, the software development process of the XP software development process. In particular, they consider
the eﬀects of four practices of this methodology: pair programming, refactoring, test-driven development, and small
developmental iterations.
In [8] Kuppuswami et al. propose a system dynamics
simulation model of the XP development process to show
the constant nature of cost of change curve that is one
of the most important claimed beneﬁts of XP. They also
describe the steps to be followed to construct a cost of
change curve using the simulation model.
One of the most relevant works was perhaps made by
Kuppuswami et al. [9]. They developed a system dynamics simulation model to analyze the eﬀects of the XP
practices on software development eﬀort. The developed
model was simulated for a typical XP project of the size
of 50 User Stories and the eﬀects of the individual practices were computed. The results indicated a reduction in
software development cost by enhancing the usage levels
of individual XP practices.
V. The Simulation Modeling Approach
We decided to implement our model following a discrete
event approach. In this section we give some insights about
this technique and some explanation on how our model has
evolved from the initial choice.
The main reason why we chose the Discrete Event Simulation (DES) is that inside a DES model each entity is well
identiﬁed and is characterized by a number of attributes
whose values can be possibly changed by the execution of
some speciﬁc events. So, we can inspect the status of each
model entity at each time step of the simulation, giving us
the possibility to better understand the evolution of the
process during a simulation run.
In a DES the simulation is advanced to the time of the
next signiﬁcant event. The execution of each event possibly triggers the creation of other subsequent events. The
approach of jumping between signiﬁcant points in time
is more eﬃcient and allows models to be evaluated more
quickly.
One other advantage of the DES approach is that it allows us to deﬁne a stochastic model and perform Monte
Carlo simulations, taking into account of the intrinsic risk
and uncertainty of real projects. For example, the estimated eﬀort required to implement each User Story and
its priority are not ﬁxed, and should be better modelled
using appropriate random distributions.
However, as we were deeply deﬁning our model, we observed that many attributes could be better modelled with
time-continuous functions. This has led us to choose a
model that can be seen as an hybrid of Discrete Event and
System Dynamics models.
In fact, we use integration rates (typical of System Dynamics) which updates entity attributes at time steps driven by
event execution (Discrete Event). For example, the rate at
which a developer implements a speciﬁc task increases with

64

M. Melis: A SPS MODEL OF XP

3

her skill level, that will vary continuously with her cumulated experience on the project. Nevertheless, our model
updates developers’ experience levels at discrete steps, in
correspondence of each development session closure. So,
we can conclude that we are following an hybrid modeling
approach (see more details in Martin and Raﬀo [10].)

TABLE I
Input parameters and output variables of the model.

VI. Model Description
The model is characterized by several activities (Release
Planning, Development Session, etc). The inputs to these
activities are entities (User Stories, Integrated code, etc)
that are modiﬁed and created by other instances of activities. The class diagram in ﬁgure 1 shows the relationships
among the high-level entities of the XP process model. The
activities are eventually composed of sub-activities such as
the user story estimation activity. Each activity is executed by one or more actors of the process. The identiﬁed

Input parameters

Output variables

Number of initial USs
Number of developers
Mean and standard deviation
of initial USs estimation
Initial Team velocity

Number of ﬁnal USs
Defect density
Number of Classes, methods,
DSIs

Number of Iterations per Release
Typical iteration duration

..
.
Each modelled entity

The time granularity of our model is that of a development session, which is typically of a couple of hours. The
equations that regulate the variations on the model entities and the execution of each activity have been taken
from existing models, empirical data and, where necessary,
from authors assumptions. About the statistical distributions used in the model we have mostly chosen gaussian
and log-normal functions.
In table I we report the input parameters needed to start
a simulation and the output variables that can be obtained
from the simulation. Moreover, the model is characterized
by a number of inner parameters that have been mainly
taken by existing models and
A. Model Dynamics

Fig. 1. Class diagram of some of the high-level model entities.

actors are the Team, made up of Developers, the Customer and the Manager, as shown in ﬁgure 2. Each
actor has some attributes, which vary in time, and can
perform a number of actions. These actions can be performed in cooperation with other actors (two developers
working in pair-programming) in order to carry out a particular activity (see section VI-A.1).

Fig. 2. Extract from the class diagram of the identiﬁed XP actors.

The project starts with an initial number of User
Storyes (USs), which identify the main requirements of
the project and represent a preliminary evaluation of the
project’s size. These USs are prioritized by the Customer
and subsequently estimated by the Team using values
taken from statistical distributions.
This estimate is aﬀected by a stochastic error which is
decreased by the overall experience of the Team on the
project. When an US estimation exceeds a certain limit (a
portion of the Iteration capacity) it will be splitted into
two or more USs. The next phase consists of choosing the
USs which will be implemented for the next Release and
consequently assigned to a speciﬁc Iteration.
During an Iteration, design and development of the
scheduled USs is performed. This activity produces the
source code required to implement the functionality described by each US. The produced code is characterized
by size (in terms of number of classes, methods and locs)
and quality (number of defects).
The time actually spent to implement each US is affected by the estimation error and by the velocity of the
developers who have worked on it1 . In addition, it is inﬂuenced also by adoption levels of Pair Programming and
Test Driven Development (TDD) practices.
In some cases not all the planned USs are completed
within an Iteration. These USs are planned again and
1 Developers are statistically diﬀerent from each other in terms
of initial skill and initial velocity. These attributes increases in accordance to the experience gained on the project.

65

4

implemented in next Iterations. Moreover, the Customer can write new USs and possibly report problems
that he has found after each release of the system.
After the end of each Release the Customer can report a number of problems he/she has found on the project
released up to that moment. This number is related to the
defect density of the system. These reports are planned
by the Team as the other USs (ProblemReportStory
(PR-US)) each of which has an associated US aﬀected by
the problem founded by the Customer. The implementation of each PR-US has the eﬀect of reducing the number
of defects of the related US.
For the sake of brevity, we only report an informal description of the Development Session activity.
A.1 Development Session
During a development session, characterized by a certain duration taken from a statistical distribution, a Developer develops the code relating to a particular User
Story. In an XP project the development session would
be normally performed by two developers working together
at a single computer (Pair Programming). However, this
practice is rarely adopted completely. For this reason, our
model has an input parameter called Pair Programming
Adoption that indicates the percentage of usage of this
practice. This parameter gives the probability at which a
Development Session will be performed by two developers
instead of one.
A Development Session performed in pair programming is more eﬃciently than a “solo-programming” in
terms of the time needed to implement a single User
Story, defects injected (due to the continuous review
made by the pair developer [5]), learning eﬃciency (the
developer skill increases faster if one works together with
another developer [11], [12]).
In particular we have made the assumption that the velocity of a pair of Developers is given by the average
of the velocities of the two developers involved, increased
by 40%, as found in some empirical researches [5]). Moreover, in these studies it has been found that the defects
injected during a pair-session is less than that of a “soloprogramming”. So, we model this behavior imposing that
the maximum number of bugs injected during a Development Session activity is dictated by the best developer
of the pair in terms of skill.
In an XP project Developers should write the code
with the relating unit tests. Also in this case the model
has a parameter called Tdd Adoption that accounts for the
level of adoption of this practice. This parameter decreases
the velocity of the development session and the number of
the defects injected [13].
At the end of a Development Session activity, new
code is produced and existing code is modiﬁed, introducing
inevitably a certain number of defects. The level of these
changes are aﬀected by stochastic variables inﬂuenced by
both Developers’ attributes (experience and skill) and
the usage levels of individual XP practices (Testing and
Pair Programming).

VII. Verification and Validation of the Model
One of the major problems in process simulation is the
eﬀective calibration and validation of the developed simulator. In order to reach this goal, data sets gathered in real
projects are needed. However, these data are diﬃcult to
obtain for several reasons. The greater part of real projects
are developed inside privately-owned companies that, for
obvious reasons, are generally reluctant to publish data
regarding their inner development process.
Also, it is diﬃcult to ﬁnd companies that develop software using XP and systematically collect information regarding their development process. Moreover, in the case it
happens, it is not guaranteed that the level of detail of the
information collected is suﬃcient for a proper simulation
calibration and validation. We can cite two XP projects
where tracking activity has been conducted systematically
and whose data are available at a suﬃcient level of detail:
Repo Margining System [14] and Market Info [15].
In order to calibrate the parameters of the simulation
model, we have used some input variables (see table I)
coming from the Repo Margining System project [14], such
as the number of developers, the release duration and so on.
Also, we have used the project and process data gathered
during the ﬁrst iteration. We then simulated the evolution
of the project starting from the second iteration.
With these input parameters a number of simulation
runs have been performed. Then, we have iteratively calibrated the model parameters in order to better ﬁt the
ﬁnal results of the real project. In table II the simulation
outputs are compared with the ones taken from the Repo
Margining System case study.
TABLE II
Comparison between simulation results averaged on 200 runs
and the Repo Margining System case study. Standard
deviations are reported in parenthesiss. A story point
corresponds to 30 minutes of work.

Output variable
Total days of Development
Number of User Stories
Estimated Eﬀort [Story points]
Actual Eﬀort [Story points]
Number of Releases
Iterations per Release
Developed Classes
Developed Methods
DSI

Simulation

Real Project

60,3 (22,8)
28,9 (7,45)
478,2 (183,3)
811,3 (296,4)
2,4 (0,8)
2,7 (0,3)
245 (108,2)
1073 (475)
15646 (6922)

60
29
474
793
2
3
251
1056
15543

A conceptual model validation has been done interviewing some individuals familiar with the XP process itself.
The proposed approach was presented, and its various concepts – roles, activities and artifacts – were explained in
detail. The collected feedback on our approach was positive.
Also, we performed an event validation process comparing the sequence of the events produced by the simulation

66

M. Melis: A SPS MODEL OF XP

5

with those of a real XP process.
As regards the veriﬁcation of the correctness of the simulator, we implemented the system using pair-programming.
Following this practice, a continuous review was made by
the pair-developer diminishing, in this way, the probability of introducing errors during the implementation of the
simulator. In addition, we covered all the functionalities
implemented with unit and acceptance tests, enabling an
automatic and continuous veriﬁcation of the correctness of
the system.

TABLE III
Comparison between simulation results averaged on 200 runs
obtained varying the usage level of PP (TDD level =
100%). Standard deviations are reported in parenthesis.

Output variable
Working days
Released USs
Defects/KDSI
KDSI

VIII. Results
We are developing the simulator following the XP
methodology and using XPSwiki [16], that is an XP project
management tool developed by our research group, in order to plan and monitor the process development.
In the ﬁrst release of our project, the simulator allowed
to simulate the Release and Iteration Planning activities
and the code production phase where an user story is
converted into production code by the Team. In the
second development release we implemented the possibility to vary the level of usage of Pair Programming
from maximum (100% = Fully Adopted ) to minimum level
(0% = Not adopted ), while in the third release we introduced the inﬂuence of Test Driven Development. In next
releases we will model in more detail the already implemented system and the inﬂuence of other XP practices.
The present model has been calibrated using data from
a real project. The case study we have chosen is Repo
Margining System (see section VII), which has been performed following the XP process. In this research we investigate the following question:
How would have been the simulated project if
the team had not followed the Pair Programming
(PP) practice at all?

(TDD = 100%)

51,1 (23,6)
28,7 (7,6)
23,0 (5,3)
21,5 (10,2)

60,3 (22,8)
28,9 (7,5)
19,7 (4,5)
15,6 (6,9)

TABLE IV
Results of the two-sided t-test (α = 0.05) of the two
samples obtained with PP=0% and PP=100% (TDD=100%).

Hypothesis B: The number of working days needed to
complete the same number of functionalities using PP
is diﬀerent from that without PP (H0 : tnoP P = tP P ,
H1 : tnoP P = tP P ).

We have inspected the two project conditions (PP= 0%
and PP= 100%) in terms of total days needed to complete the same number of functionalities, residual defect
density and ﬁnal Delivered Source Instructions (DSI). For
each of the conditions we have performed 200 simulation
runs. Results are reported in table III.
The results with PP = 100% (2nd column) are the same
as those reported in table II, which have been obtained

PP level = 100%

(TDD = 100%)

simulating the Repo Margining System project.
Looking at the 1st column (table III), it can be seen
how some outputs of the same simulated project have varied only excluding the use of Pair Programming. We have
found that not using Pair Programming at all, the duration of the project (in terms of working days) decreases by
15%. The number of User Stories remains quite the same
in both cases. Instead, the defect density increases by 17%
when we exclude the use of PP. In addition, the use of PP
decreases the number of DSI by 27%.
Consequently, we can say that the use of the Pair Programming practice increases the total cost of development
(working days), but it is repaid by an increase of the quality
of the project (in terms of defect density) and by a better
design, in terms of fewer lines of code per User story (0,75
KDSI/US against 0,54 KDSI/US).
These results are quite in agreement with those found in
[5] and [4], as previously described in section III-A.
In addition we have performed a two-sided t-test (α =
0.05). The test results (table IV) have conﬁrmed our hypotheses (A, B and C) with a statistical signiﬁcance of
95%.

According to what we have found in literature (see section
III-A) we have formulated our hypotheses:
Hypothesis A: The residual defect density of the project
using the PP practice is diﬀerent from that obtained
without PP (H0 : dnoP P = dP P , H1 : dnoP P = dP P ).

Hypothesis C: The number of lines of code needed
to implement the same number of functionalities using PP is diﬀerent from that without PP
(H0 : DSI noP P = DSI P P , H1 : DSI noP P = DSI P P ).

PP level = 0%

tscore

tcrit

H0

P − value

HypA

6,87

1.97

rejected

0.00

HypB

3,98

1.97

rejected

0.00

HypC

6,75

1.97

rejected

0.00

We have also performed the same experiment varying
the adoption of Pair Programming when TDD is not used.
We have found (table V) that not using Pair Programming at all, the duration of the project decreases by 12%.
The number of User Stories remains quite the same in both
cases. Instead, the defect density increases by 16% when
we exclude the use of PP. In addition, the use of PP decreases the number of DSI by 28%.
Analyzing the ﬁrst column of the tables III and V we can
see how keeping the usage level of PP constant and varying
from 0% to 100% the usage level of TDD, the working days
increases by 14%, the defects density decreases by 18%,

67

6

TABLE V
Comparison between simulation results averaged on 200 runs
obtained varying the usage level of PP (TDD level = 0%).
Standard deviations are reported in parenthesis.
Output variable
Working days
Released USs
Defects/KDSI
KDSI

PP level = 0%

PP level = 100%

(TDD = 0%)

(TDD = 0%)

45,0 (23,2)
28,8 (7,9)
28,0 (5,3)
18,0 (8,2)

51,1 (19,1)
28,8 (7,6)
24.1 (6,0)
13,0 (6,1)

on what has been empirically found up to now, but many
other issues have to be better investigated.
We are planning to improve the current simulator modeling other practices and activities of the software development process, with emphasis on XP. Another important
stage of our research will be the validation of the model
using other real projects and experiments.
X. Acknowledgements
This work was supported by MAPS (Agile Methodologies for Software Production) research project, contract/grant sponsor: FIRB research fund of MIUR, contract/grant number: RBNE01JRK8.

while the number of User Stories remains quite the same.
Other interesting results can be observed looking at
the 2nd column of both tables. Starting from the Repo
Margining results (PP=100% and TDD =100%), we have
excluded the use of TDD. It can be noticed that the use
of TDD increases the duration of the project by 18%. In
other words, the Repo Margining project would have been
concluded 15% of the time before the actual end time if
TDD was not used. The extra time taken by TDD could
be attributed to the time needed to develop test cases. In
addition, test cases lead to an increment of the source instructions of the project, as conﬁrmed by the simulation
results (+ 20%). On the other hand, the simulation produced a decrease in defect density by 18% when TDD is
used. These ﬁgures are quite in agreement to those found
empirically in [17].
The extreme case is when we simultaneously vary the
usage of both the practices. From TDD=PP=0% to
TDD=PP=100% we have obtained an increase in time by
34%, a decrease by 30% of the defect density and a decrease
in DSIs by 13%. However, these results are not conﬁrmed
by any empirical ﬁndings because we haven’t found related
studies in literature, so we cannot validate this last experiment.
All the results illustrated here have shown a statistical
signiﬁcance (α = 0, 05) after having performed a two-sided
t-test.
IX. Conclusions and Future Work
In this paper we have presented a simulation model of
XP process that we have developed in order to evaluate
the eﬀectiveness of this methodology.
We have calibrated our model using some results obtained from a real project. Then we have simulated the
variation of the usage level of two keys XP practices: Pair
Programming and Test Driven Development.
We have found that increasing the usage of such practices the defect density of the project signiﬁcantly decreases. On the other hand, the results have shown an
increase on the number of days needed to implement the
same functionalities.
Let us note that our model is not a complete representation of the intrinsic complexity of these practices and of
the development process itself. We have based our model

References
[1] Kent Beck and Cynthia Andres, Extreme Programming Explained: Embrace Change- Second Edition, Addison-Wesley,
2004.
[2] Marc I. Kellner, Raymond J. Madachy, and David M. Raﬀo,
“Software process simulation modeling: Why? What? How?,”
The Journal of Systems and Software, vol. 46, no. 2–3, pp. 91–
105, Apr. 1999.
[3] Kent Beck, Extreme Programming Explained: Embrace Change,
Addison-Wesley, 1999.
[4] John T. Nosek, “The case for collaborative programming,”
Commun. ACM, vol. 41, no. 3, pp. 105–108, 1998.
[5] Alistair Cockburn and Laurie Williams, “The costs and beneﬁts
of pair programming,” in Proceedings of the First International
Conference on Extreme Programming and Flexible Processes in
Software Engineering (XP2000), Cagliari, Sardinia, Italy, June
2000.
[6] Lan Cao, “A modeling dynamics of agile software development,”
in Companion of 19th Annual ACM SIGPLAN Conference on
Object-Oriented Programming, Systems, Languages, and Applications (OOPSLA). 2004, pp. 46–47, ACM Press.
[7] Vojislav B. Misic, Hudson Gevaert, and Michael Rennie, “Extreme dynamics: Modeling the extreme programming software
development process,” in Proceedings of ProSim04 workshop on
Software Process Simulation and Modeling, 2004, pp. 237–242.
[8] S. Kuppuswami, K. Vivekanandan, and Paul Rodrigues, “A
system dynamics simulation model to ﬁnd the eﬀects of xp on
cost of change curve.,” in XP2003 Conference Proceedings, 2003,
pp. 54–62.
[9] S. Kuppuswami, K. Vivekanandan, Prakash Ramaswamy, and
Paul Rodrigues, “The eﬀects of individual xp practices on software development eﬀort,” SIGSOFT Softw. Eng. Notes, vol. 28,
no. 6, pp. 6–6, 2003.
[10] Robert H. Martin and David Raﬀo, “A model of the software
development process using both continuous and discrete models.,” Software Process: Improvement and Practice, vol. 5, no.
2-3, pp. 147–157, 2000.
[11] K. Vivekanandan, The Eﬀects of Extreme Programming on Productivity, Cost of Change and Learning Eﬃciency, Ph.D. thesis, Doctor of Philosophy in Computer Science and Engineering,
2004.
[12] D. Sanders, “Student perceptions of the suitability of extreme
and pair programming,” in Proceedings of XP Universe Conference, Raleigh, NC, 2001.
[13] Ivana Turnu, Marco Melis, Alessandra Cau, Michele Marchesi,
and Alessio Setzu, “Introducing TDD on a free-libre open source
software project: a simulation experiment,” in Proceedings of
Qute Swap workshop on QUantitative TEchniques for SoftWare
Agile Processes, 2004.
[14] Klondike Team,
“Tracking-aworkingexperience,” Published on: http://www.communications.xplabs.com/paper20012.html, 1900.
[15] PierGiuliano Bossi, “Extreme programming applied: a case in
the private banking domain,” in Proceedings of OOP2003, 2003.
[16] Sandro Pinna, Simone Mauri, Paolo Lorrai, Michele Marchesi,
and Nicola Serra, “XPSwiki: an Agile Tool Supporting the
Planning Game,” in XP2003 Conference Proceedings, 2003, pp.
104–113.

68

M. Melis: A SPS MODEL OF XP

7

[17] Boby George and Laurie Williams, “An initial investigation of
test driven development in industry,” in Proceedings of the 2003
ACM symposium on Applied computing. 2003, pp. 1135–1139,
ACM Press.

69

70

Section III

Process Simulation Modeling –
Focus on Methodology

71

72

DEVS-based Software Process Simulation Modeling: Formally Specified,
Modularized, and Extensible SPSM ∗
KeungSik Choi, Doo-Hwan Bae, TagGon Kim
Department of EECS,
Korea Advanced Institute of Science and Technology, Daejon 305-701, Korea
{kschoi, bae}@se.kaist.ac.kr, tkim@ee.kaist.ac.kr
Abstract
This paper proposes DEVS (Discrete Event System
Specification)-based software process simulation modeling method which is a formally specified, modularized, and
extensible simulation modeling approach. The proposed approach adopts DEVS formalism, a general purpose discrete
event modeling and simulation framework, to the software process simulation modeling domain. This approach
enables us to clearly understand the software process simulation model by formal specification, and provides explicit
extension point to extend the simulation model for a specific purpose.
This approach also provides naturally hybrid software process simulation modeling environment, which
embeds DESS (Differential Equation System Specification) and DTSS (Discrete Time System Specification)
into DEVS formalism. This hybrid approach overcomes
some limitations of system dynamics simulation models, such as difficulties of controlling process execution,
representing explicit process activity, and modeling inherent uncertainty.

1. Introduction
Many researchers have investigated why Software Process Simulation Models (SPSMs) are not widely used in industry and proposed several approaches. Raffo [1] argued
that industry is not using SPSMs to their full advantages,
because process models are difficult to build and maintain.
As a solution, he proposed designing a Generalized Software Process Simulation Model (GPSM) that can be tailored quickly to a particular project scenario and deployed
∗

This work was supported by the Ministry of Information & Communication, Korea, under the Information Technology Research Center
(ITRC) Support Program.

73

rapidly. He also reviewed three possible research areas applicable to GPSM: Modularization, Software Product-Line,
and Cognitive Pattern. Angkasaputra and Pfahl [2] proposed
to make SPSMs more agile by taking benefits from agile methods and design patterns to reduce product delivery
time and budget. Ruiz [3] developed a Reduced Dynamic
Model (RDM) which simplified Abdel-Hamid and Madnick’s model [4] to easily learn and understand the model
and to use the model at the initial phases of a project where
the available or known information about the project is little.
Although these approaches alleviate some of the difficulties in using SPSMs in industry, there are other obstacles to
apply SPSMs, especially system dynamics models, in software development projects. Traditional SPSMs are difficult
to understand and extend, because most of researches concentrate on developing simulation models and displaying
simulation results. For example, the stock flow diagrams
of system dynamics models have complicated arrows, circles, rectangles, etc., which make it difficult to understand
the process activity controls and variable interactions. Furthermore, those models don’t have clear specifications and
are rarely verified. Because simulation models are also software systems, we need a clear specification of the simulation model. Liu [5] claimed that to improve the maintainability we have to enhance the structural and behavioral
specifications.
In an attempt to answer the aforementioned arguments
we propose a DEVS (Discrete Event System Specification)
[6]-based software process simulation modeling technique
which is a formally specified, modularized, and extensible
simulation modeling approach. With this approach, we can
clearly specify and verify the simulation model and extend
the model using Object-Oriented framework provided by
the DEVS simulation engine [7]. Another benefit of this approach is this technique can solve the disadvantages of system dynamics models such as difficulties in controlling activity sequencing, describing discrete process steps, representing attributes of individual entities, and modeling un-

certainties [11].
The structure of this paper is as follows. In Section 2,
we briefly introduce the DEVS formalism and the simulation environment. Section 3 describes the proposed DEVSbased software process simulation modeling method and
demonstrates how to extend and tailor the simulation model.
Section 4 compares existing hybrid simulation approaches
and proposed DEVS-based SPSM in various aspects. Section 5 summarizes the main results of this paper and gives a
plan for future work.

2. Background

in

in1
in2

out1
M1

out

in

M2

out

out2

Coupled Model

Figure 1. Coupled DEVS model

The behaviors represented by four functions of the
atomic model are as follows:
• An atomic model can stay only in one state at any time
• The maximum time to stay in one state without external event is determined by ta(s) function

2.1. DEVS formalism
DEVS is a general formalism for discrete event system
modeling based on set theory [6]. It allows representing any
system by three sets and four functions: Input Set, Output
Set, State Set, External Transition Function, Internal Transition Function, Output Function, and Time Advanced Function. DEVS formalism provides the framework for information modeling which gives several advantages to analyze
and design complex systems: Completeness, Verifiability,
Extensibility, and Maintainability [7]. DEVS can also approximate continuous systems using numerical integration
methods. Thus, simulation tools based on DEVS are potentially more general than other tools including continuous simulation tools [8]. With those properties, we applied
DEVS formalism to software process simulation modeling.
DEVS has two kinds of models to represent systems.
One is an atomic model and the other is a coupled model
which can specify complex systems in a hierarchical way
[6]. A DEVS model processes an input event based on its
state and condition, and it generates an output event and
changes its state. Finally, it sets the time during which the
model can stay in that state. An atomic DEVS model is defined by the following structure [6]:
M = hX, Y, S, δext , δint , λ, tai
where:
• X is the set of input values,
• Y is the set of output values,
• S is the set of states,
• δext : Q × X → S is the external transition function,
where Q = {(s, e)|s ∈ S, 0 ≤ e ≤ ta(s)} is the total
state set, e is the time elapsed since last transition

• When an atomic model is in a state (0 ≤ e ≤ ta(s)), it
changes its state by δext function if it gets an external
event
• If possible remaining time in one state is passed
(e = ta(s)), it generates output by λ function and
changes the state by δint function
DEVS coupled model is constructed by coupling DEVS
models. Through the coupling, the output events of one
model are converted into input events of other models. In
DEVS theory, the coupling of DEVS models defines new
DEVS models (i.e., DEVS is closed under coupling) and
then complex systems can be represented by DEVS in a hierarchical way [6]. Figure 1 shows a coupled DEVS model.
M1 and M2 are DEVS models. The M1 model has two input ports (”in1” and ”in2”) and one output port (”out”). The
M2 model has one input port (”in1”) and two output ports
(”out1” and ”out2”). They are connected by input and output ports (e.g., ”out” port of M1 is connected to ”in” port
of M2) internally, which is called Internal Coupling (IC).
The M1 model is connected by external input, ”in” port of
Coupled Model, to ”in1” port, which is called External Input Coupling (EIC). The M2 model is connected to output
port ”out” of Coupled Model (e.g., ”out1” port of M2 is
connected to ”out” port of Coupled Model), which is called
External Output Coupling (EOC). According to the closure
property, the coupled model in Figure 1 can be used as a
DEVS model and it can be coupled with other DEVS models.

2.2. Simulation environment
The DEVSimHLA [7] is a C++ based DEVS simulation environment which is integrated with Microsoft Visual Studio .NET. It, therefore, provides the advantages of
Object-oriented framework such as encapsulation, inheritance, etc. The DEVSimHLA coordinates the event sched-

• δint : S → S is the internal transition function,
• λ : S → Y is the output function,
• ta : S → R+
0,∞ is the set positive reals bet. 0 and ∞

74

ules of atomic models in a system and provides classes and
APIs for simulation.
With DEVS formalism and DEVSimHLA, we can get
several advantages. First, we can specify the systems mathematically which gives straight forward verification method.
Second, we can model hierarchical and modularized systems which enhance understandability and extensibility.
Third, we can reuse simulation models by inheritance.

3. DEVS-based software process simulation
modeling approach
This section describes how to model software development process using DEVS formalism. We develop a generic
and simplified software process model to estimate the cost
and duration of a project based on the initial information on
the project such as project size and project duration. We will
show the overall architecture of the proposed simulation
model, and how to formally specify the software process
simulation model using DEVS formalism. We also demonstrate how to extend and tailor the DEVS-based software
process simulation model for a specific purpose. Finally, we
will emphasize how our approach provides naturally hybrid
simulation environment.

3.1. Overview
The purpose of this simulation modeling approach is to
develop a formally specified, modularized, and extensible
simulation model to make full advantages of SPSMs. We
analyze published system dynamics models through literatures and books, and identify several limitations as follows
[11]:
• Understanding and maintaining simulation models is
difficult because of no clear specification
• Extending simulation models is difficult because of no
explicit reuse mechanism and extending points provided
• Describing process steps is difficult because of no explicit mechanism to control the activity sequences
• Representing error prone modules or variable productivity of developers is difficult because of no individual entities and entity attributes
• Modeling uncertainties inherent in estimates of model
parameters is difficult
We referenced system dynamics models provided by
Vensim simulation tool [9] and Abdel-Hamid and Madnick
[4], and simplified it for our purpose. We assume that we already analyzed all the dynamic interactions of software process variables using such as Causal-Loop Diagram (CLD).

75

Identify main variables

Encapsulate related
variables in an atomic model

CLD/
SD model

Define messages among
atomic models

Make hierarchically
coupled models

Coupling
Diagram

Specify using DEVS formalism

State transition Diagram,
DEVS specification,
Logical computation table

Implement the SPSM

DEVSimHLA

Figure 2. Overall approach of DEVS-based
software process simulation modeling

Figure 2 shows our approach of proposed DEVS-based
software process simulation modeling. First, we identify
main variables which drive the simulation result variables
such as an effort and a duration in software process simulation models. For example, we identified that the most important variable is a workflow rate which is determined by
many factors such as productivity, total workforce, synergy,
and communication overhead. We then construct an atomic
model by encapsulating closely related variables centered
on the main variable identified in the previous step, and
identify interaction points (variables) among atomic models. There are two kinds of interaction points. One is a variable which is calculated by other atomic models and updated by input messages. The other is a variable which is
computed in this model and becoming an output message.
After constructing atomic models and interaction points,
we define messages which transmit simulation information
among atomic models. We then couple the models hierarchically. Hierarchical modeling enables us to modularize
the simulation model and makes it simple and understandable.
When atomic and coupled models are defined, and their
interactions are identified, we specify the simulation models using four methods which include a coupling diagram,
a state transition diagram, DEVS specification, and a logical computation table. These methods we propose provide
a clear, verifiable, understandable, and extensible specification for the software process simulation model. Finally, we
implement the specification using the DEVSimHLA environment [7].

3.2. Overall architecture of the DEVS-based software process simulation model
Figure 3 illustrates the overall architecture of the simulation model, which is composed of a DevelopmentPhase
and an ExperimentalFrame model. It shows the most basic structure of the software development project. We design this to make the model simple and easy to extend. The
DevelopmentPhase model can represent the whole software
development life-cycle and also can represent each phase
of the software development life-cycle. The ExperimentalFrame model can be reused and extended for a specific simulation purpose.
The DevelopmentPhase model represents any phase of
the software development life-cycle. It can be extended to
any of the life-cycle such as requirements, design, or implementation. It also can be a Waterfall or Incremental lifecycle by coupling the DevelopmentPhase model each other.
The WorkToBeDone, WorkDone, and Rework models are
like a level variable in stock flow diagram. The WorkToBeDone model stores the amount of work to be done
and sends it to the Work model. The WorkDone model integrates the workflow rate to compute the work done, and the
Rework model computes the amount of rework to do again
and sends the rework rate to the WorkToBeDone model.
The ExperimentalFrame model plays a role of a measurement system or observer like an oscilloscope in electronics. It generates inputs to the observed system, and accepts and analyzes the experiment data. It is a system that
interacts with the system of interest to obtain the data of interest under specified conditions. In this simulation model,
it generates WorkToDo message, which is an initial work
to do. The WorkToDo message will be processed by the
DevelopmentPhase model and sends the simulation data,
as a WorkMonitoring message, to the ExperimentalFrame
model.
The TimeIntervalGenerator model in the ExperimentalFrame is an executive which drive the simulation execution.
It generates a Time event in a small-enough constant-time
interval to make the WorkToBeDone model change its state
and generate the WorkToDo message. The TimeIntervalGenerator model enables the project variables to dynamically interact each other, which allows this model to become
a naturally hybrid simulation model. The naturally hybrid
simulation approach will be discussed in Section 4
The WorkMonitoring message contains simulation variables which are stored and analyzed by the SimAnalysis
model in the ExperimentalFrame model. This message includes level variables, rate variables, and auxiliary variables
in system dynamics representation. These variables are dynamically updated through the feedback loop, which shows
the effects of complex dynamic software development process.

76

The Done message makes this simulation model stop.
The WorkToBeDone model generates the Done message
when the work is done. The interaction point of the DevelopmentPhase model (e.g., Time, Done, and WorkMonitoring) depicted in a small rectangle in Figure 3 is called an
input or an output port. This is an explicit extension point
in DEVS-based software process simulation model. For example, we can instantiate the DevelopmentPhase model to
a Requirements model and a Design model, and then connect the Done port of the Requirements model to the WorkIn
port of the Design model. The model extension issues will
be further discussed in Section 3.4.

3.3. Formal specification using DEVS formalism
As we mentioned before, we use four methods (coupling
diagram, state transition diagram, DEVS specification, and
logical computation table) to specify software process simulation model. Using these methods we can develop a clear,
verifiable, understandable, and extensible specification for
a software process simulation model.
The coupling diagram shows the internal structure of the
coupled model and the flow of simulation data. The state
transition diagram specifies how the state changes as the
system responds to its different inputs. This is the explicit
mechanism to control the activity sequences. Based on the
input and the state of the model we can change the behavior of the model. The DEVS specification defines the behavior of the model with 3 state sets and 4 functions. This
provides the sound framework to verify the behavior of the
simulation model, because the formal specification gives
clear interpretation of that. The logical computation table
describes how to calculate the project variables when the
model changes its state by external input or internal time
expiration.
Figure 4 shows the coupling diagram of the
Work Coupled model which shows the internal structure of the Work model in Figure 3. The Work Coupled
model comprises ScheduleMng, WorkforceMng, and
WorkflowRateMng model. The ScheduleMng model calculates the remained schedule time to complete the job and
based on this it calculates the required workflow. The required workflow can be KLOC/day or FP (Function
Point)/day, etc. The required workflow is sent to the WorkforceMng model which returns the indicated workforce.
The indicated workforce means required workers to complete the job based on the schedule. Based on this indicated workforce, the ScheduleMng model determines the
amount of overwork. The WorkforceMng sends the total workforce and the WorkflowRateMng model calculates the workflow rate and work quality. The workflow rate means work done per day (e.g., KLOC/day or
FP/day).

SW_Project_Management_Simulation_System
DevelopmentPhase
WorkIn

WorkToDo
TotalWork

WorkToDo

WorkToBeDone

WorkflowRate_Quality
WorkflowRate_Quality_In
WorkDone

Work

Time Done

WorkflowRate_Quality_Out

Workflow_Rework_Rate

Workflow_Rework_Rate
Rework

WorkflowRate_Quality_In
Time

Done

WorkMonitoring

Done

Time

WorkMonitoring
ExperimentalFrame

WorkToDo

Done

Done

Time

WorkMonitoring

ProjDone

SimAnalysis

TimeIntervalGenerator

WorkToDo

ProjStop

Figure 3. Overall architecture of DEVS-based SPSM

Figure 5 depicts the state transition diagram of the OvertimeCalc atomic model. This diagram shows that the initial state is a Wait state and it stays infinitely until it receives external input message. The model changes the state
to the GetIndicatedWorkforce state when it receives external input message, ScheduleTime RequiredWorkflow, and
then the model changes the state to the OvertimeCalc state
in response to the IndicatedWorkforce input. In the OvertimeCalc state, the model generates the Overtime output
message and changes its state to the Wait state with 0 second delay.

WorkToDo

Work_Coupled
WorkToDo

ScheduleMng
WorkToDo

WorkflowCalc

RequiredWorkflow
RequiredWorkflow

ScheduledTime_RequiredWorkflow

WorkforceMng

ScheduledTime_RequiredWorkflow
IndicatedWorkforce

OvertimeCalc

IndicatedWorkforce
TotalWorkforce

Table 1 shows the DEVS specification of the OvertimeCalc model. It represents the same information with the
state transition diagram but defines it more declaratively.

Overtime

IndicatedWorkforce
Overtime
TotalWorkforce

Overtime

Table 2 shows the logical computation table of the OvertimeCalc model. This table specifies the type of the variables and the equations. When this model receives a ScheduledTime RequiredWorkflow message in the Wait state, it
saves the input variables, and when it receives an IndicatedWorkforce message in the GetIndicatedWorkforce state,
it calculates the NormalWorkflow, SchedulePressure, and
Overtime variables. When this model receives the external
inputs again by the feedback structure after a constant-time
interval (e.g., one hour or one day), the variables are dynamically recalculated. Through this mechanism, we imple-

WorkflowRateMng
WorkflowRate_Quality

WorkflowRate_Quality

Figure 4. Coupling diagram of Work coupled
model

77

OvertimeCalc
(0 sec)

Overtime

Wait
(infinity)

δext (Wait, Scheduled- Save ScheduledTime and
Time RequiredWorkflow) RequiredWorkflow
δext (GetIndicatedWorkforce, Calculate NormalWorkflow,
SchedulePressure, Overtime
IndicatedWorkforce)

IndicatedWorkforce

Table 2. Logical computation table of OvertimeCalc model

GetIndicatedWorkforce

ScheduledTime_RequiredWorkflow

Logical description

State transition function

(infinity)

Figure 5. State transition diagram of OvertimeCalc model

ment feedback loop of the system dynamics. We don’t specify the detailed variable types and equations in Table 2 because they are not our focus in this paper.

3.4. Extending and tailoring the simulation model
The model in Figure 3 is a generic and simplified one.
It can estimate the effort and duration for intermediate size
(21.3 Man-Month, 8 Months) [10] project. Based on this
simulation model, we can extend this model rapidly and
with low cost, because the base model has a clearly verified specification and explicit interfaces (ports or extension
points) for extension.
For example, Figure 6 extends the base model to Waterfall life-cycle model by coupling the DevelopmentPhase
model to each other. The Waterfall model starts when it re-

OvertimeCalc = hX, Y, S, δext , δint , λ, tai

X={ScheduledTime RequiredWorkflow, IndicatedWorkforce}
Y ={Overtime}
S={Wait, GetIndicatedWorkforce, Overtime Calc}
δext (Wait, ScheduledTime RequiredWorkflow) =
GetIndicatedWorkforce
δext (GetIndicatedWorkforce, IndicatedWorkforce)=
Overtime Calc
δint (Overtime Calc) = Wait
λ(Overtime Calc) = Overtime
ta(Wait) = ta(GetIndicatedWorkforce) = Infinity
ta(Overtime Calc) = 0

ceives WorkIn input which is the initial project estimation
size and it ends when it receives the ProjDone message. The
Requirements phase model does the job and the model outputs the Done message when it completes the job, and this
message is becoming an input message of the Design phase
model. Of course, we have to modify the variables and the
dynamic equations of each model to apply the characteristics of each phase. The ExperimentalFrame model is also
reused and extended to store and analyze the project information of each phase.
Another example is shown in Figure 7. In this example,
we add a ResourcePool model which manages shared human resources. If one organization has limited resources
and performs two projects simultaneously, we suffer resource conflicts. Proj A and Proj B are modified version
of the DevelopmentPhase model in Figure 3. The WorkforceMng model in the Proj A is modified to request new
workers to the organization’s shared resource pool and it
receives allocated workers. We reuse other models as it is
except the WorkforceMng model. The ExperimentalFrame
model is extended to store and analyze the project information of two projects.

Waterfall_Lifycycle_Model
Waterfall

Requirements
WorkIn

Done WorkIn Design

WorkIn

Time

WorkMonitoring

Time

Done
Time

Coding
Done WorkIn

Time
WorkMonitoring

Time

ProjDone

WorkMonitoring

WorkToDo

ExperimentalFrame

Table 1. DEVS specification of OvertimeCalc
model

WorkMonitoring

WorkMonitoring
Done

Done

ProjDone

Figure 6. Extended model: Waterfall model

78

Multi_Project_Management_Simulation_System

DEVS

Allocated_Hiring_Transfer_Rate

DTSS

DESS

ResourcePool

Allocated_Hiring_Transfer_Rate

Hiring_Transfer_Rate_Request

Hiring_Transfer_Rate

Allocated_Hiring_Transfer_Rate

Proj_A

Proj_B
Allocated_Hiring_Transfer_Rate

Allocated_Hiring_Transfer_Rate
WorkforceMng

Time Time

WorkforceMng

Hiring_Transfer_Rate

Hiring_Transfer_Rate
WorkIn

small-enough
constant time
constant time interval
interval

Hiring_Transfer_Rate

Done

WorkMonitoring

Done_A

WorkMonitoring_A

WorkMonitoring

Time

Done

WorkMonitoring_B

Figure 8. Formalisms embedding
WorkIn

Done_B

ExperimentalFrame
WorkToDo_A

WorkToDo_B

Figure 7. Extended model: Multi-project
model

4. Naturally hybrid simulation modeling approach
We described DEVS-based software process simulation
modeling approach, which overcomes some limitations of
published system dynamics models: difficulties in understanding and extending simulation models, difficulties in
describing discrete process steps, difficulties in representing individual entities and entity attributes of a variable,
and difficulties in modeling uncertainties. The four methods
for specifying software process simulation model provide
a clear and understandable specification, and we can extend the simulation model through explicit extension points
(ports). We also showed how to model discrete process steps
in Section 3.4. Moreover, we can easily represent individual
entities and entity attributes, and uncertainties because the
proposed simulation model is basically a discrete event simulation model.
Because our approach incorporates feedback loop mechanism of system dynamics and represents discrete event, the
DEVS-based software process simulation modeling technique is a hybrid simulation modeling approach. We named
this approach to ”Naturally hybrid simulation modeling approach” because DEVS formalism can naturally embed Discrete Time System Specification (DTSS) and Differential
Equation System Specification (DESS), which will be discussed further in this section.

4.1. Characteristics of DEVS-based naturally hybrid simulation modeling approach
In this section, we provide theoretical backgrounds on
how to model continuous systems in DEVS representation,

79

and describes the characteristics of DEVS-based naturally
hybrid simulation modeling approach.
Traditionally, differential equations have been solved
with numerical integration in discrete time. In formal terms,
Differential Equation System Specification has been embedded into Discrete Time System Specification [6]. The
word ”embedded” in this context means that any system in
DESS can be simulated by DTSS. Of course, errors may
be introduced in the DTSS approximation of the DESS
model, but it is tolerable if the discrete time is small enough.
Moreover, any DTSS can be simulated by the DEVS by
constraining the time advance to be constant. Therefore, if
we constrain the time advance of the DEVS to be smallenough constant-time, we can model DESS with DEVS formalism. This formalism embedding illustrated in Figure 8
makes DEVS-based software process simulation modeling
technique to be a naturally hybrid simulation modeling approach. We coined ”naturally hybrid” because this simulation modeling environment naturally includes both discrete
and continuous simulation modeling techniques.
We also can easily model uncertainties in model parameters. For example, if we want to make a stochastic human
resource transfer model, we can extend the DEVS formalism to a stochastic DEVS. The output function of DEVS, λ,
can be extended like this:
λ̃ : S × [0, 1] → Y
Therefore, the output is characterized by a possibility distribution of [0, 1]. We can apply this to the external transition
function and internal transition function as well.

4.2. Comparisons of published hybrid simulation
approaches and DEVS-based SPSM
System dynamics models describe the interaction between project factors, but do not easily represent discrete
process steps. On the other hand, discrete event models may
not have enough events to represent feedback loops accurately [11]. Many researches, therefore, have tried to combine discrete event simulation and continuous simulation to

Martin [11]

Lakey [12]

DEVS-based SPSM

Evaluate potential process
changes
Extend
Discrete event models are
combined in system dynamics framework
Process activities: DES

Project estimation & Project
management
Extend
Feedback
mechanisms
are approximated in discrete process
Process activities: DES

Project environment: SD

Divide a discrete activity
into multiple sub-activities
and iterate multiple times

Calculates duration, total effort, errors which are
passed out to system dynamics model

Product size and quality are
passed on to next activity

Project estimation & project
management
DEVSimHLA
System dynamics models are implemented with
DEVS which embeds DESS
Process activities: DES
Implement feedback mechanism by constraining time
advance into small-enough
constant-time interval
Information on the work,
process control events, and
simulation results data are
passed on to next activity

Continuous model

System dynamics model
(human resource, manpower allocation, productivity) passes out the data to
discrete event model

Each sub-activity (feedback
loops of development, review, rework) iterates multiple times to incorporate dynamic feedback loops while
calculating a number of
equations for product, process, project factors

Limitations

Duration time of each activity can not be dynamically
calculated

Coarse approximation of
system dynamics model

Timing issues

Each activity computes the
duration time but advances
the clock only by the specified delta time

Time advance doesn’t mean
anything, schedule model in
each activity calculates calendar weeks

Author

Application

Implementation tool

Basic approach

Discrete event model

Workflow, human resource,
manpower allocation, productivity, work quality,
etc. are dynamically calculated

No limitation

Time advance is constrained
to small-enough constanttime interval, duration time
of each activity is dynamically calculated

Table 3. Comparison of hybrid simulation modeling approaches

model software process more realistically. Table 3 compares
existing hybrid simulation approaches with our approach.
Martin et al. [11] develop a combined model that represents the software development process as a series of
discrete process steps executed in a continuously varying project environment to evaluate the potential process
changes. The process activities are represented in discrete
event model and the project environments, such as human
resource, manpower allocation, and productivity, are modeled in system dynamics. In their approach, the discrete
event models are combined in the system dynamics framework as shown in Figure 9. To support hybrid model, they
advance the clock by the specified delta time while preserving the discrete scheduling time computed by discrete
event model. At each delta time increment, they integrate
all necessary equations until the next scheduled event time

80

is reached. The limitation of this approach is that the duration time of each activity is not calculated dynamically.
In most system dynamics approach, the activity time is dynamically influenced by many factors such as pressure, fatigue, and workforce, but this approach calculates the duration time of each activity once before executing the activity.
Lakey [12] proposes a hybrid simulation model for
project estimation and management. The feedback loops
are incorporated in the discrete event process by dividing a single discrete event process block into multiple
iterations so that certain factors are allowed to change several times within the discrete activity. The development
process has discrete four major activities as shown in Figure 9, and each of the activity incorporates four discrete
sub-blocks which represent feedback loops of system dy-

Lakey's approach

Martin's approach
HR

Preliminary
Design

MP Alloc

Feedback loop
approximation:
5-times iterations

Detailed
Design

DES
Plan

Discrete Process

QA

Test

SD Framework

DES Framework

DEVS-based software process simulation modeling
(Naturally hybrid simulation modeling approach)
Requirements

Design

Implementation

DES
SD in DEVS

DES

SD in DEVS

Test
DES

SD in DEVS

SD in DEVS

Figure 9. Characteristics of each hybrid simulation approach

namics. Within each of the sub-blocks are a number of
equations that relate the product, process, and project factors to output parameters: Schedule, size, quality, manpower, overhead, skill level, tool support, process maturity, and functional growth. The limitation of this approach
is that this may not fully represent overall dynamic feedback relationships in software development project because
it approximates feedback loops by limited five iterations in each process activity.
Martin et al. [11] advance the clock at a small steady
increment and integrate all necessary equations to implement system dynamics concept, and Lakey [12] iterates Development, Review, and Rework sub-activities five times in
each activity to incorporate dynamic feedback loops. Martin combines discrete event model into the system dynamics
framework, and Lakey approximates feedback loop in discrete event model. On the other hand, our approach embeds
DESS and DTSS into DEVS formalism, and implements
each activity phase (e.g., Requirements) with system dynamics model with DEVS as shown in Figure 9. This simulation modeling environment naturally includes both discrete and continuous components.

81

5. Conclusion and Future Work
We proposed DEVS-based software process simulation
modeling technique which is a formally specified, modularized, and extensible simulation modeling approach. We
modularized the simulation model by encapsulating closely
related variables in one atomic model, and used four methods to specify a software process simulation model. Our approach enables us to make a clear, verifiable, understandable, and extensible specification for a software process
simulation model. DEVS-based software process simulation modeling technique also overcomes some limitations
of existing system dynamics models such difficulties as describing discrete process steps, controlling the activity sequences, and modeling uncertainties, by providing naturally
hybrid simulation modeling environment.
This simulation modeling approach is unique in that it
adopts DEVS formalism, a general purpose modeling and
simulation framework, to a software process simulation
modeling domain and implements a naturally hybrid simulation environment by embedding DESS and DTSS into
DEVS formalism. We can implement the same system dynamics models as models developed by system dynamics

modeling tools (Vensim, Extend, or iThink) with discrete
event simulation technique, and at the same time we can utilize the advantages of discrete event simulation because this
technique is fundamentally a discrete event simulation one.
We have described a base model using DEVS-based software process simulation modeling technique and shown
some examples of extended model. We, however, haven’t
experimented in industrial environment. We have plan to experiment a Waterfall life-cycle model in industrial setting,
which will makes this simulation modeling approach more
concrete.

References
[1] D. Raffo, G. Spehar, and U. Nayak, ”Generalized Simulation Models: What, Why and How?”, ProSim ’03, 2003.
[2] N. Angkasaputra and D. Pfahl, ”Making Software Process Simulation Modeling Agile and Pattern-based”, ProSim ’04, 2004.
[3] M. Ruiz, I. Ramos, and M. Toro, ”A simplified model of software
project dynamics”, The Journal of Systems and Software, Elsevier,
2001, pp. 299-309
[4] T. Abdel-Hamid and S. Madnick, Software Project Dynamics: An Integrated Approach, Prentice-Hall, Englewood Cliffs, NJ, 1991
[5] Ling Liu, ”EVOLVE: adaptive specification techniques for objectoriented software evolution”, Thirty-First Hawaii International
Conference, 1998
[6] B. Zeigler, H. Pracehofer, and TagGon Kim, Theory of Modeling and
Simulation, Second Edition, Academic Press, New York, 2000
[7] TagGon Kim, DEVSimHLA v2.2.0 Developer’s Manual, Korea Advanced Institute of Science and Technology (KAIST), 2004
[8] E. Kofman, M. Lapadula, and E. Pagliero, ”PowerDEVS:A DEVSBased Environment for Hybrid System Modeling and Simulation,
Technical Report LSD0306, LSD, Universidad Nacional de Rosario,
2003
[9] Vensim Modeling Guide, Ventana Systems, Inc., 2004
[10] B. Boehm, Software Engineering Economics, Prentice-Hall, Englewood Cliffs, NJ, 1981
[11] R.H. Martin and D. Raffo, ”A Model of the Software Development Process Using Both Continuous and Discrete Models”, Software Process Improvement And Practice, John Wiley & Sons, NJ,
2000, pp. 147-157
[12] Peter B. Lakey, ”A Hybrid Software Process Simulation Model for
Project Management”, ProSim ’03, 2003.

82

Towards an Agile Development Method of Software Process Simulation
Niniek Angkasaputra, Dietmar Pfahl
Fraunhofer Institute for Experimental Software Engineering
Kaiserslautern, Germany
{angkasa, pfahl}@iese.fraunhofer.de
processes more flexible, and thus more efficient. To
tackle this goal, we followed a three-step approach.
First, we selected an existing SPS modeling
method that provides sufficiently mature process
guidance. As a point of reference, we focused on an
existing, well-documented and mature SPS modeling
process offered by the IMMoS (Integrated
Measurement, Modeling, and Simulation) framework
[10, 11]. Although IMMoS focuses on the
development of continuous SPS models, i.e., System
Dynamics (SD) models, most of the elements of
IMMoS can also be used for the development of other
types of SPS models, e.g., discrete-event models.
Second, we identified agile software development
as one source of inspiration for making IMMoS more
flexible and thus more efficient and cost-effective [4].
We reviewed existing survey reports to understand the
principles and characteristics of agile methods known
in the area of software engineering (SE). Inspired by
Abrahamsson’s characterization scheme of agile
software development methods [1], we analyzed to
what extent IMMoS implicitly exhibits corresponding
characteristics. Based on this analysis we reformulated and slightly enhanced IMMoS to make its
agility more explicit and complete. To reflect these
changes we assigned the name Agile-IMMoS to the
enhanced SPS modeling method.
Third, we chose one agile method that implements
most of the common agile principles, i.e., Extreme
Programming, and used it as a model for proposing
further enhancements of Agile-IMMoS through
adoption of suitable agile practices which are not yet
considered in the various activities of the SPS
modeling process.
The remainder of this paper is organized as
follows. Section 2 briefly presents the SPS modeling
guidance offered by IMMoS and discusses to which
extent agility is implicitly present. Section 3 gives an
overview of existing agile methods, summarizes the
commonalities of agile development processes, and
lists their practices. Section 4 presents Agile-IMMoS,
the result of enhancing IMMoS towards an agile SPS
modeling method. Section 5 summarizes existing
related work. Section 6 presents conclusions and
plans for future work.

Abstract
Process simulation modeling has been used for
various purposes in software engineering. To become
even more widely used, in particular in industrial
environments, an effective and efficient modeling
method is required. IMMoS (Integrated Measurement,
Modeling, and Simulation) offers such a method.
Once a valid software process simulation model has
been developed, it is important to maintain the model,
i.e., to keep it up-to-date with the current software
development process. This requires a simulation
modeling process that shortens delivery time and is
responsive to changes of the modeled process. One
way to achieve this is to make simulation modeling
agile. This paper describes our work on enhancing
IMMoS to make it an agile method, and by adopting
the practices of a suitable agile software development
method.
1 BACKGROUND AND MOTIVATION
To be successful, companies try to satisfy their
customers by delivering quality software on time and
within budget. As software development happens in
very dynamic environments, flexibility of the
development processes is an important success factor
in this endeavor.
During the last two decades, process simulation
modeling has been used for various purposes in
software engineering, i.e., strategic management,
planning, control and operational management,
understanding, and process improvement [9]. Model
validity is an important success criterion of all
software process simulation (SPS) projects. To be
cost-effective, a valid SPS model must not only be
able to adequately reflect the modeled system at a
particular point in time, but over long time periods.
This implies that an SPS model has to be kept up-todate with the current software process, i.e., the
modeled system. Taking into account that software
processes may be instable, the SPS modeling process
must be able to produce and re-adjust SPS models in a
short delivery time. A study in the early 1990s found
that development of SPS models is very timeconsuming [8]. Unfortunately, this is still the case to
date. In order to overcome this problem, we started to
investigate possibilities to make SPS modeling

83

SDM of phase 1, and the Enhanced SDMs of phases 2
and 3.
IMMoS Process Model. The IMMoS process
model defines the activities that are (repeatedly)
performed in the various phases. Each activity is
described by eleven attributes: activity ID, name,
(involved) role, input, output, entry condition, exit
condition, description (of tasks to be performed),
(useful) methods and techniques, (special) guidelines,
and (provided) materials and tools. The IMMoS
process activities are associated to the SDM lifecycle
as follows:
• Phase 0 (pre-study) consists of the following
activities: 1) first contact, 2) characterization, 3)
management briefing, 4) identification of SDM
users, 5) problem definition, 6) technical feasibility
check, and 7) planning and contract.
• Phase 1 (initial model development) consists of the
following activities: 1) technical briefing, 2)
definition of dynamic hypotheses, 3) definition of
causal diagram, 4) review of the causal diagram
(verification 1), 5) implementation of initial SDM,
6) review of the initial SDM (verification 2), and
7) test of the initial SDM (validation 1).
• Phase 2 (model enhancement) consists of the
following activities: enhancement of the initial
SDM and test of the enhanced SDM (validation 2).
Note that the activity enhancement of the initial
SDM basically repeats the activities in Phase 1.
• Phase 3 (model application and maintenance)
consists of the activity application and maintenance
of the SDM. The user applies the SDM to find new
policies that solve the problem(s) at hand. This
includes experimenting with parameter values and
model structures. If modification of the SDM is
necessary, then activities similar to Phase 2 are
repeated.

2 THE IMMoS METHODOLOGY
The
following
problems
motivated
the
development of the IMMoS methodology [11]:
• Lack of a commonly accepted and sufficiently
detailed SPS modeling process;
• Lack of precise guidelines on how to define the
SPS problem statement in a goal-oriented way;
• Lack of reuse of information from software
engineering (SE) models that typically exist in
software organizations, i.e., static quantitative
models, and static process models;
• Lack of integration of SPS modeling with
established static modeling methods to create
synergy effects.
IMMoS addresses all of these problems. In this
section, the IMMoS methodology is briefly described
to give an overview1.
2.1 The IMMoS Framework
The framework of IMMoS process guidance for
SD-based SPS modeling consists of the following
models:
IMMoS Phase Model. Four phases are defined to
structure the SDM2 lifecycle: Phase 0 – pre-study to
identify SDM users and define the SDM modeling
goal, Phase 1 – development of an initial SDM that
reproduces the system’s reference behavior, Phase 2 –
enhancement of the initial SDM to make it suitable for
the intended problem analysis/solution, and Phase 3 –
application and maintenance of the enhanced SDM. It
is important to note that these phases can be iterated.
IMMoS Role Model. This model defines the
minimal set of roles that are important in an SDM
development project: SDM Customer (C) as the
sponsor of the SDM development project, SDM User
(U) as the future user of the SDM in the customer’s
organization, SDM Developer (D) as the responsible
person for the technical development, Facilitator (F)
as a support for establishing contacts and planning as
well as arranging meetings, Moderator (M) for
preparing and guiding workshops and meetings, and
SE Subject Matter Expert (E) as source of relevant SE
information needed for SDM building.
IMMoS Product Model. This model provides a
complete list of work and end products in the different
phases. Among the products defined, several occur in
any SDM modeling process: SDM Dynamic
Hypotheses Definition (i.e., Reference Mode and Base
Mechanisms), Causal Diagram, and SDM (i.e., Initial
Flow Graph and Mathematical Equations). IMMoS
distinguishes three SDM maturity levels: the Initial

1

2

2.2 The Agility of IMMoS
Based on our experience from many SPS modeling
projects, we found that the following five
requirements should be fulfilled by SPS modeling
methods:
R1: Involvement of domain experts is required in
the definition of a valid model that is close to
the real system.
R2: The validation task is iterated until a model is
considered to be satisfying the system’s
reference mode.
R3: Dynamic user requirements should be
responded to quickly and with the right details.
R4: The development process should better be
performed top-down, i.e., starting from a very
abstract and simple model to a more detailed
and complex model.
R5: Involvement of a variety of roles is required,
not only from the simulation model developers
but also from the customer’s management.

A more detailed summary can be found in [11]. The
complete work has been published in [10].
SDM is an abbreviation for ”SD model“.

84

IMMoS was developed based on the fundamental
assumption that continuous alteration of an SDM
should be expected and managed in order to keep it
consistent with the change-prone software processes
that it intends to represent. Due to this underlying
assumption, IMMoS fulfils these requirements to a
high degree.
The IMMoS methodology fosters communications
between SDM developers, SE experts and key
stakeholders of the customer’s organization (R1 and
R5). This is realized through the definition of
different roles in the IMMoS role model and their
involvement in the activities defined by the IMMoS
process model.
The IMMoS process model defines activities that
are iterated until a valid SDM is delivered. It also
defines review activities that do not only aim at
finding faults in work products early (R2). The
reviews also give the opportunity to capture change
requests (R3).
Furthermore,
the
IMMoS
phase
model
distinguishes several development stages that SDMs
undergo during development, from initial/simple
models to enhanced/complex models (R4).
In software engineering, the following agile
principles are known for providing better support for
the customer and timely delivery of the desired
software [2]:
• P1: Individuals and interactions over processes
and tools
• P2: Working software over comprehensive
documentation
• P3: Customer collaboration over contract
negotiation
• P4: Responding to change over following a plan

process guidance for SDM development is
documented in a rather monolithic way, giving the
(not intended) impression that processes and tools are
more important than the interaction between the
individuals that assume the various roles described in
the method. Thus, additional work is required to
enhance IMMoS and to make the agility of IMMoS
more explicit and practical. This will be done by reformulating the modeling guidance of IMMoS to
make it more compact, flexible, transparent, and
supportive towards agility.
3 AN OVERVIEW OF AGILE METHODS
In this section, we first summarize common
characteristics of agile (software development)
processes. Then we present existing agile methods in
SE, and list the practices that they entail, indicating
which of the four agile principles each practice mainly
focuses on.
3.1 Characteristics of Agile Processes
Through a process of deduction from various
discussions of other authors (some of them authors of
agile methods) about the characteristics of agile
software processes, Abrahamsson et al. [1]
summarized the following important characteristics
that make a software development method agile:
• Incremental (C1): Development happens in small
releases with rapid development cycles in order to
deliver executable programs fast and to get
immediate feedback for enhancement.
• Cooperative (C2): Development stresses the
importance of customer and developers working
constantly together with close communication.
• Adaptive (C3): Development is able to react to last
moment changes.
• Straightforward (C4): The method itself is easy to
learn and to modify and it is sufficiently
documented.

In this list, the word “over” stands for “is/are at
least as important as”. It implies that the expressions
on the left hand side and on the right hand side of the
word “over” should be kept in balance. The four
principles have been implemented in various agile
software development methods and several positive
effects have been reported [1].
The degree to which IMMoS addresses the
requirements R1 to R5 indicates that it implicitly
supports to some extent the agile principles listed
above. First, IMMoS keeps processes, tools, and
contract negotiation balanced with communication
and collaboration between the SDM developer and the
partners in the customer organization (P3). Second,
IMMoS balances comprehensive documentation with
demonstrating valid work and end products to the
customer during review (P2). Third, IMMoS balances
adherence to the project plan with responsiveness to
change requests from the customer (P4).
Nevertheless, IMMoS cannot yet be considered a
fully practical agile method for SPS model
development. For example, agile principle P1 is not
sufficiently well implemented. Currently, IMMoS

In Section 4, we will use characteristics C1 to C4
as our main points of reference for the transformation
of IMMoS into Agile-IMMoS.
3.2 Agile Methods and their Practices
In our attempt to find an agile method that could
serve as a reference for enhancing Agile-IMMoS
towards more agility, we identified ten candidate
methods from four main sources: Seven agile methods
described in [1], one (i.e., Agile Software Process) in
[5], one (i.e., Agile Modeling) in [3], and another one
(i.e., Lean Software Development) in [12]. We then
associated the practices of each agile method to one of
the agile principles they mainly focus on (see Table
1).
We briefly summarize the essentials of the agile
methods listed in Table 1 in the next paragraphs.

85

Table 1. Agile methods with their practices clustered with regards to agile principles
Agile Principles
P2: Working
P3: Customer
software
collaboration

P1: Individuals &
interactions

Agile Methods
Dynamic Systems
Development Method
(DSDM)

• DSDM teams must be
empowered to make
decisions
• A collaborative and
cooperative approach
shared by all
stakeholders is
essential

Scrum

• Product Backlog
•
• Daily Scrum meeting •
•
• Holistic diversity
•
strategy
•
• Reflection workshops •
•
•

Crystal Family

Extreme Programming
(XP)

• Metaphor
• Pair programming
• Collective code
ownership
• Coding standards

Adaptive Software
Development (ASD)
Pragmatic
Programming (PP)
Agile Software Process •
(ASP)

Feature-driven
Development (FDD)

•

Agile Modeling (AM)

•

•
•

•
•

Lean Software
Development (LSD)

•
•
•

• Frequent delivery of
•
product
• Iterative and incremental
development
•
• Changes are reversible
• Requirements are
•
baselined at a high level
• Testing is integrated
throughout the lifecycle

•
•
•
•
•
•
•

P4: Responding
to change

Active user
involvement is
imperative
Fitness for business
purpose
A collaborative and
cooperative approach
shared by all
stakeholders is
essential
Sprint planning meeting • Sprint planning
• Sprint procedure
meeting
Effort estimation
• Sprint review
meeting
Sprint Backlog
• Sprint review meeting
Staging
• User Viewings
Revision and review
Monitoring
Parallelism and flux
Methodology-tuning
technique
Small releases
• Planning game
• Refactoring
Simple design
• On-site Customer
Continuous integration
Continuous testing
40-hour week
Iterative development • Customer focus group
reviews
Feature-based planning

• Rigorous testing
•
• Incremental & iterative
development
People-centered
• Time-based process
process
• Concurrent and
asynchronous process
• Incremental delivery and
iterative process
• Distributed software
process
Individual Class
• Domain Object
(Code) Ownership
Modeling
Feature teams
• Developing by Feature
Progress Reporting
• Inspection
• Regular builds
• Configuration
management
Teamwork, e.g., model • Iterative and incremental •
with others, collective modeling, e.g., apply the
ownership
right artifacts, create
several models in
Motivation: e.g.,
model to communicate parallel
Productivity, e.g.,
• Simplicity, e.g., create
apply modeling
simple content
standards, reuse
• Validation, e.g.,
existing resources
consider testability
• Documentation, e.g.,
discard temporary
models
Amplify learning
• Eliminate waste
•
Empower the team
• Deliver fast
See the whole

86

User-centered design

Teamwork, e.g., active
stakeholder
participation

Build integrity in

The Dynamic Systems Development Method
(DSDM) suggests fixing time and resources and then
adjusting the amount of functionality accordingly
instead of fixing the amount of functionality in a
product and then adjusting time and resources
accordingly. The items listed in the table are more in
the form of DSDM principles than practices.
Scrum is an approach for managing the systems
development process in a constantly changing
development environment.
Crystal Family consists of a set of different
methods to be selected and provides suggestions on
choosing an appropriate methodology for a project
based on its size and criticality. The method is open
for any development practices and tools.
Extreme Programming (XP) has the novelty of
being based on the way the individual practices are
collected and lined up to function with each other.
Adaptive Software Development (ASD) offers
solutions for the development of large and complex
systems, in particular. The method encourages
incremental and iterative development with constant
prototyping.
Pragmatic Programming (PP) introduces a set of
programming practices. The PP method itself
comprises a collection of short programming tips that
focus on day-to-day problems.
The Agile Software Process (ASP) method focuses
on delivering products incrementally over time. The
ASP is enacted iteratively with a fixed cycle-time.
The Feature-driven Development (FDD) method
focuses on the design and building phases. FDD
consists of a set of best practices, emphasizes quality
aspects throughout the process, and includes frequent
and tangible deliveries along with accurate monitoring
of project progress.
Agile Modeling (AM) applies the idea of agile,
rapid development to software system modeling. Its
key focus is on modeling practices and cultural
principles. The method encourages developers to
produce sufficiently advanced models to support acute
design needs and documentation purposes. The items
listed in the table are categorical practices, each of
which contains several AM practices (given as
examples).
Lean Software Development (LSD) aims at
utilizing the same high level principles as guidelines
or thinking tools for increasing software production
efficiency. The items listed in the table are LSD’s
principles, which are guideposts for devising
appropriate practices for a particular environment.
LSD provides various tools for converting these
principles into agile software development practices.
Based on the information contained in Table 1, we
selected Extreme Programming (XP) created by Beck
[6] as the candidate for improving Agile-IMMoS.
This was due to the fact that XP seems to cover the
agile principles most comprehensively. Moreover, XP

is a lightweight methodology for small to medium
sized teams developing software in the face of vague
or rapidly changing requirements. This perfectly
matches the context of SPS modeling. Also, XP is the
most documented one of the various agile methods,
and many successful applications have been reported
[1]. In Section 4, we discuss to which extent relevant
XP practices are yet reflected by Agile-IMMoS, and
we identify simulation modeling activities that can be
further improved by adopting XP practices not yet
comprised in Agile-IMMoS.
4 AGILE-IMMoS
The enhancement of IMMoS, which aims at
bringing its agile potentials to the surface, yielded
Agile-IMMoS. Major modifications relate to the
IMMoS process, phase, and product models. The
IMMoS role model remains unchanged, but the
mapping of roles to activities has been re-adjusted and
presented more compactly.
Compared to traditional IMMoS, in Agile-IMMoS
the characteristics of agile processes (cf. Section 3.1)
are addressed more explicitly:
• Due to modifications to the IMMoS process and
phase models, Agile-IMMoS becomes a more
straightforward (C4) and adaptive (C3) SPS
modeling methodology.
• Due to enhancements of the IMMoS product
model, Agile IMMoS becomes a more incremental
(C1) SPS modeling methodology.
• Finally, a more comprehensive description of how
the existing IMMoS roles interact during SPS
model development makes Agile-IMMoS a more
cooperative (C2) SPS modeling methodology.
More detailed presentations of the models
contained in Agile-IMMoS give Sections 4.1 to 4.4
below.
In order to further improve Agile-IMMoS with
regards to agility, we analyzed the practices advocated
by the agile SE method XP with regards to their
relevance for inclusion in Agile-IMMoS. We then
check whether relevant practices are already reflected
in Agile-IMMoS. For relevant practices that are not
yet reflected we describe how they should be included
into Agile-IMMoS (cf. Section 4.5).
4.1 Agile-IMMoS Process Model
In Agile-IMMoS, the following modifications to
the existing process model were made:
• The overview presentation of the IMMoS process
model was altered and simplified.
• The activities in IMMoS Phase 0 were now
capsulated as organizational activities, since they
concern organizational aspects such as project
orientation and planning until contract preparation.

87

Validation of
the SDM
Technical
Briefing/RePlanning

Verification 2
(SDM)

Organizational
Activities

Start

Implementation
of SDM

iterative

Definition of
Dynamic
Hypotheses

End

Verification 1
Construction
(Causal
of Causal
Diagram)
Diagram

Development Activities

Figure 1. Process model overview of Agile-IMMoS

SDM
Agreement

SDM
Customer
Sheet

First Contact

Project Plan

Characterization

SDM Goal
Definition

Management
Briefing Materials

Management
Briefing

SDM
Customer
Sheet

Management
Briefing Minutes

SE Models

SDM
Baseline

Problem
Definition
Project Logfile
Feasibility
Check

Planning
and
Contract

SE Models

Development
Contract

Project Plan
Reference
Mode

Spearmint v7.3 ©FhG-IESE
Artifact

Technical Briefing /
Re-Planning
Definition of
Dynamic
Hypotheses
Construction
of Causal
Diagram

Identification
of SDM Users

SDM Goal
Definition

Technical Briefing
Materials

Activity
Links:
Activity–produces/
consumes–Artifact

(a)

Verification 1
(Causal
Diagram )

Technical Briefing
Minutes
SDM Base Mechanism

Causal
Diagram

Verification
Report 1

Implementation
of SDM

SDM

Verification 2
(SDM )

Verification
Report 2

Validation of
the SDM

Validation
Report

(b)

Figure 2. Process model of Agile-IMMoS: organizational activities (a) and development activities (b)
• The activities in IMMoS Phases 1 were generalized
and packaged into a set of development activities
that are iterated for developing and maintaining
defined SDM releases. As a consequence, the
activities in IMMoS Phases 1, 2 and 3 were
collapsed.

process model is shown in Figure 1. The process starts
with organizational activities followed by the SDM
development activities, which are iterative. The
amount of iterations and the length of any iteration
depend on the SDM release plan.
The Agile-IMMoS process model now supports an
iterative development process more explicitly with
short cycles and clearly defined milestones. Short
development cycles facilitate early risk and problem
identification and enable rapid correction and

In this way, the activities are no longer tied
directly to the phase model (i.e., SDM lifecycle). The
new overview presentation of the Agile-IMMoS

88

verification. Explicit milestones help to better plan
and review development progress. Moreover the
number of different types of SDM product has been
reduced.

of the (problematic) dynamic behavior of one or
more system parameters observed in reality.
• Phase 2: Enhanced SDM for singular problem
solving such as strategic management as well as
process improvement and technology adoption.
• Phase 3: Enhanced SDM for any repeated use, for
the same purpose such as management planning,
control
and
operational
management,
understanding, as well as training and learning.
This phase likely involves maintenance of the
model to keep it valid compared to the current real
system.

4.1.1 Organizational Activities
Figure 2 (a) presents the product flow of the
organizational activities in Agile-IMMoS, which
consists of seven activities. The descriptions of these
activities are identical to those in the traditional
IMMoS process model, except that the activity
‘Planning and Contract’ now comprises an additional
task related to the planning the SPS model release. A
typical product release plan is described in Section
4.3. For one SDM development project, the overall
organizational activities are normally required to be
performed only once.

4.3 Agile-IMMoS Product Model
Several intermediate and end products produced by
Agile-IMMoS activities are shown in Figure 2. The
Agile-IMMoS product model defines these products
and provides templates to support the developers.
Among the products, a core set of products that are
typical for any SDM modeling project are listed
below:
a. Base Mechanism and Reference Mode, both
constituting the Dynamic Hypotheses of the SDM;
b. Causal Diagram;
c. SDM, consisting of the SDM Flow Graph and its
underlying Mathematical Equations, as well as
(optionally) a Graphical User Interface.

4.1.2 Development Activities
Figure 2 (b) shows the product flow of the
development activities in Agile-IMMoS. Again, it
consists of seven activities: Technical Briefing / RePlanning, three activities for constructing the main
SDM products (Definition of Dynamic Hypotheses,
Construction of Causal Diagram, and Implementation
of SDM), and three V&V activities (Verification 1:
Causal Diagram, Verification 2: SDM, and Validation
of the SDM).
The number of activities is kept small to obtain a
short development cycle. The milestones, which are
used to plan and review development progress, are
“start”, “verification 1”, “verification 2”, “validation”,
and “finish/delivery”.
Typically, development activities are iterated
several times in any phase of the SDM lifecycle (cf.
Section 4.2), and apply to any SDM release (cf.
Section 4.3).

These products are developed in small releases to
get an incremental development that makes it possible
to deliver a meaningful model fast and to get
immediate feedback for correction and enhancement.
Each phase can have several releases. Often, releases
are determined based on the complexity of the SDM
(i.e., from low to high complexity) as illustrated in
Figure 3 (a), but there can also be other
considerations, e.g., constraints in availability of
experts or data regarding certain aspects of the
problem to be resolved. SDM releases are initially
planned during the activity ‘Planning and Contract’ of
organizational activities. During SDM development,
re-planning can be done during activity ‘Technical
Briefing / Re-Planning’.
For any product release, iterations of development
activities are planned and performed as illustrated in
Figure 3 (b). Ideally, an iteration will not take longer
than two calendar weeks. The exact number of
iterations varies depending on the novelty and/or
difficulty of the problem, the number of proposed
solution alternatives, the experience and skill levels of
the developers with the simulation approach, and the
availability of subject matter experts and prospective
users from the customer organization.

4.2 Agile-IMMoS Phase Model
Different
from
the
traditional
IMMoS
methodology, SDM lifecycle phases in Agile-IMMoS
are not directly associated with process activities (cf.
Section 4.1). Instead, the Agile-IMMoS phase model
structures the SDM lifecycle into three phases, which
are linked to the SDM goal definition. An SDM goal
consists of five elements: scope, purpose, role,
dynamic focus, and environment. Using the set of SPS
modeling purposes proposed in [9], Agile-IMMoS
phases can be linked to SDM goal definitions as
follows:
• Phase 1: Initial SDM, to reproduce the reference
mode. A reference mode is an explicit description

89

(a)

(b)
Figure 3. SDM release planning

4.4 Agile-IMMoS Role Model

4.5 Agile Practices in Agile-IMMoS

The role types and their descriptions do not change
from the IMMoS role model (cf. Section 2.1 for the
list of roles); only a new summary representation is
added to the Agile-IMMoS method to give a sense of
the degree of involvement to the persons assuming the
various roles in a modeling project.

In order to further improve Agile-IMMoS with
regards to agility, we first analyzed the twelve
practices advocated by the agile SE method XP with
regards to their relevance for inclusion in AgileIMMoS. Then we checked whether relevant practices
are yet implicitly or explicitly reflected in AgileIMMoS. The results are presented in Table 3.

Table 2
(a) Role involvement in organizational activities
O1: First Contact
O2: Characterization
O3: Management Briefing
O4: Identification of SDM Users
O5: Problem Definition
O6: Feasibility Check
O7: Planning and Contract

C
9
9
9
9
9
9
9

U D
9
9
99
9
99
99
9

Table 3. Coverage of XP practices in Agile-IMMoS

F M E

XP Practice

9
9

9

Metaphor
Pair programming
Collective code ownership
Coding standards
Small releases
Simple design
Continuous integration
Continuous testing
40-hour week
Planning game
On-site customer
Refactoring

9

(b) Role involvement in development activities
C U D F M E
9
9

D1: Technical Briefing / Re-Planning 9 9 9 9
D2: Definition of Dynamic Hypotheses
9999
D3: Construction of Causal Diagram
9
D4: Verification 1 (Causal Diagram)
9999
D5: Implementation of SDM
999
D6: Verification 2 (SDM)
9
D7: Validation of the SDM
9999

9
9

Agile-IMMoS
Framework
Relevant
Reflected
yes
yes
yes
no
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
yes
no
yes
no
yes
yes
yes
yes

It turned out that all practices are relevant. Nine of
them are already reflected in the Agile-IMMoS
framework. Three practices are relevant but not yet
reflected (light gray shaded cells).
‘Metaphor’ is implicitly reflected through the
Agile-IMMoS
product
‘Dynamics
Hpotheses
Definition (containing ‘Base Mechanisms’ and
‘Reference Mode’). ‘Collective code ownership’ is
the standard practice in SDM development. ‘Coding
standards’ are supported by the SD modeling tools.
‘Small releases’ are supported by the Agile-IMMoS
phase and product models, and the related guidelines
in the process model. ‘Simple design’ is supported by
the guidelines for the definition of base mechanisms
provided by the development activity ‘Definition of

9

Although involvement of people from the
customer organization is important, it should not be
excessive in order to avoid being counter-productive.
People are involved only to some degree, which is
expressed in three shades of color intensity: the darker
the cell shadow, the more intensively the role is
involved in terms of effort. Table 2 (a) and (b) shows
the role involvement in the Agile-IMMoS
organizational activities (O1-O7) and development
activities (D1-D7), respectively.

90

Dynamic Hypotheses’. ‘Continuous integration’ is
facilitated through the functionality of standard SD
development tools which allows for developing and
executing sub-models either separately or combined.
‘Continuous testing’ is facilitated through the early
availability of reference modes for validation testing.
’On-site customer’ is supported by the Agile-IMMoS
role and process models. ‘Refactoring’ is addressed
by the guidelines for developing ‘Causal Diagrams’ in
the development activity ‘Construction of Causal
Diagram’.
Table 4 shows in which activities of the AgileIMMoS process model the four agile practices that are
not yet properly reflected, should be included
(possibly after adaptation).

efforts have been made to provide SPS modeling
techniques that make certain steps of SPS modeling
more efficient and cost-effective. These techniques
include templates [7], generic models [13], patternbased generators [14], and building blocks [15]. All of
these techniques could be adapted and integrated into
Agile-IMMoS process activities.
6 CONCLUSION AND FUTURE WORK
Agile-IMMoS offers an agile development
approach to SPS modeling with System Dynamics.
The Agile-IMMoS modeling guidance consists of a
process model, phase model, product model (incl.
release concept), and role model, which are arranged
in such a way that agile values are put upfront. In
particular, this arrangement enables flexibility in
defining and adjusting project plans and model scopes
in the form of SDM releases.
Based on our analyses, we conclude that AgileIMMoS exhibits all characteristics of an agile method,
and supports all agile principles. Moreover, when
taking XP as a reference method from the area of
software engineering, we found that nine out of
twelve practices are reflected by Agile-IMMoS. The
three remaining practices can easily be added to the
method.
Furthermore, Agile-IMMoS provides certain
potential practices that can be applied to the
development activities. These practices were adapted
from the agile method Extreme Programming, in
addition to the methods and techniques, special
guidelines, as well as materials and tools that were
already in IMMoS.
In the current version, Agile-IMMoS covers all
important SPS modeling activities from project
definition (i.e., organizational activities) to SDM
development (i.e., development activities). It does not
yet address issues related to the training of SPS model
users.
Our future work will focus on the evaluation of
Agile-IMMoS with regards to efficiency and costeffectiveness in SPS modeling project using System
dynamics. We also intend to adapt and formalize
Agile-IMMoS to support the development of discreteevent SPS models.

Table 4. XP practices to be introduced into AgileIMMoS
XP Practice
Pair Programming
40-hour week
Planning Game

Agile-IMMoS Activities
Organizational Development
D2, D3, D5
O7
D1
O7
D1

‘Pair programming’, one of the most popular XP
practices, advocates the idea of having two
programmers working together using a single
computer and giving each other immediate feedback.
This practice could easily be encouraged by explicitly
including it as a recommended technique for all roles
involved in development activities D2, D3, and D5
(Definition of Dynamic Hypotheses, Construction of
Causal Diagram, Implementation of SDM). The
technique could not only apply to the role SDM
Developer, but expand to roles SDM User and SE
Subject Matter Expert.
’40-hour week’ advocates a strict risk management
with regards to time and effort consumption. This can
be reflected in Agile-IMMoS activites O7 and D1
(Planning and Contract, Technical Briefing / RePlanning) by adding the 40-hour week rule to the
guidelines applying to the re-/planning tasks.
‘Planning game’ advocates the idea of bringing
together customers, subject matter experts, and
modeler for the purpose of defining releases that
provide the maximal business value. Again, this can
be added as a specific technique to Agile-IMMoS
activities O7 and D1.

7 ACKNOWLEDGEMENT
The authors would like to thank Sonnhild
Namingha from the Fraunhofer Institute for
Experimental Software Engineering (IESE) for the
editorial review of the first version of this article.

5 RELATED WORK
By making IMMoS more agile, we tried to provide
a comprehensive SPS modeling methodology that
helps increase the acceptance of process simulation in
software industry by making it more efficient and
cost-effective. In the literature, we have not found
other proposals aiming at the same target that are
equally comprehensive. However, several useful

8 REFERENCES
[1] Abrahamsson, Pekka; Salo, Outi; Ronkainen, Jussi;
Warsta, Juhani: “Agile software development methods:
Review and Analysis”. Technical Research Centre of
Finland, VTT Publications 478, available online:

91

http://www.inf.vtt.fffpdf/publications/2002/P478.pdf,
Espoo, Finland, 2002.
[2] Agile Alliance: “Agile Manifesto”, source: official
Website http://www.agilemanifesto.org.
[3] Ambler, Scott W.: Agile Modeling: Effective Practices
for Extreme Programming and the Unified Process,
John Wiley & Sons, Inc. , New York, 2002.
[4] Angkasaputra, Niniek; Pfahl, Dietmar: “Making
Software Process Simulation Modeling Agile and
Pattern-based”. In: Proceedings of the 5th Process
Simulation Modelling Workshop (ProSim-2004),
Edinburgh, Scotland, 24-25 May 2004, pp. 222-227.
[5] Aoyama, Mikio: “Agile Software Process and Its
Experience”. In: Proceedings of the 20th International
Conference on Software Engineering (ICSE), Kyoto,
Japan, IEEE Computer Society, Washington, DC,
USA, 1998, pp. 3-12.
[6] Beck, Kent: Extreme Programming
Embrace Change, Addison-Wesley, 2000.

Explained:

[7] Berling, Tomas; Andersson, Carina; Höst, Martin;
Nyberg, Christian: "Adaptation of a Simulation Model
Template for Testing to an Industrial Project". In:
Proceedings of the 4th Process Simulation Modelling
Workshop (ProSim-2003), Portland, USA, May 2003.
[8] Keller, Lucien; Harrell, Charles; Leavy, Jeff: “The
Three Reasons Why Simulation Fails”. In: Industrial
Engineering, Vol. 23, No. 4, IE, April 1991, pp. 27-31.
[9] Kellner, Marc I.; Madachy, Raymond J.; Raffo, David
M.: “Software process simulation modelling: Why?
What? How?” In: The Journal of Systems and
Software, Vol. 46, Elsevier, USA, 1999.
[10] Pfahl, Dietmar: An Integrated Approach to SimulationBased Learning in Support of Strategic and Project
Management in Software Organisations. PhD Theses in
Experimental Software Engineering, Fraunhofer IRB
Verlag, Stuttgart, 2001.
[11] Pfahl, Dietmar, Ruhe, Guenther: “IMMoS: A
Methodology for Integrated Measurement, Modelling,
and Simulation”. In: International Journal of Software
Process: Improvement and Practice, Wiley, Vol. 7,
2003, pp. 189-210.
[12] Poppendieck, Mary: “Lean Software Development”.
In: C++ Magazine, Fall 2003.
[13] Raffo, D.; Spehar, G.; Nayak, U.: "Generalized
Simulation Models: What, Why and How?". In:
Proceedings of the 4th Process Simulation Modelling
Workshop (ProSim-2003), Portland, USA, 3-4 May
2003.
[14] Schuetze, M.; Riegel, J.P.; Zimmermann, G.: “A
Pattern-Based Application Generator for Building
Simulation”. In: Proceedings of the 6th European
Conference held jointly with the 5th ACM SIGSOFT
international symposium on Foundations of Software
Engineering, Zurich, Switzerland, 1997, pp. 468-482.
[15] Valentin, E.; Verbraeck, A.: “Simulation Using
Building Blocks”. In: Proceedings of AI, Simulation
and Planning in Highly Automated Systems, 2002, pp.
65-71.

92

Section IV

Process Simulation Modeling –
Focus on Model Implementation

93

94

1

Software Process and
Business Value Modeling
Ray Madachy, Senior Member, IEEE



simulation can be used to reason about software value
decisions. It can help find the right balance of activities that
contribute to stakeholder value with other constraints such as
cost, schedule or quality goals.
A value-oriented approach provides explicit guidance for
making products useful to people by considering different
people’s utility functions or value propositions. The value
propositions are used to determine relevant measures for given
scenarios.
Two major aspects of stakeholder value are addressed here.
One is the business value to the development organization
stemming from software sales. Another is the value to the
end-user stakeholder from varying feature sets and quality.
Production functions relating different aspects of value to their
costs were developed and are included in the integrated
model.

Abstract — Business value attainment should be a key
consideration when designing software processes. Ideally they
are structured to meet organizational business goals, but it is
usually difficult to integrate the process and business
perspectives quantitatively. This research uses modeling and
simulation to assess tradeoffs for business case analysis. An
improved model relates the dynamics between product
specifications, investment costs, schedule, software quality
practices, market size, license retention, pricing and revenue
generation.
The system dynamics model allows one to
experiment with different product strategies, software processes,
marketing practices and pricing schemes while tracking financial
measures over time. It can be used to determine the appropriate
balance of process activities to meet goals. Examples are
developed for varying scope, reliability, delivery of multiple
releases, and determining the quality sweet spot for different
time horizons. Results show that optimal policies depend on
various stakeholder value functions, opposing market factors and
business constraints. Future model improvements are also
identified.

A. Relation to Previous Work
An early version of the model in this research is described
in [3]. Its focus was on the impact of quality on sales, market
dynamics and financial measures with the software process
largely stubbed out. A primary improvement has been the
addition of a major sector in the model for software process
and product. It covers the development process including
effort, schedule and product quality in terms of defect levels.
Previously the staffing profile and expected quality were
derived externally and manually input. Neither could be
impacted dynamically during the course of a simulation run.
Now the software process and product sector is capable of
producing the internal dynamics between job size, effort,
schedule, required reliability and quality. Input parameters
can also be modified interactively by the user during the
course of a run and the model responds to the midstream
changes. Additional investment options were also added to
the finance sector. The model has been enhanced overall to be
more general-purpose and user-interactive.
Also added to the model are value-based production
functions relating the market value to the cost of different
feature sets and to the cost of reliability. It is shown in the
examples how these constructs can be used to help quantify
the value of different strategies. The model can now be used
to assess the effects of combined strategies by varying the
scope
and
required
reliability
independently
or
simultaneously.

Index Terms—Software process modeling and simulation,
Value-based software engineering, Software business value,
Software quality, System dynamics

I. INTRODUCTION AND BACKGROUND
Software-related decisions should not be extricated from
business value concerns. Unfortunately, software engineering
practice and research frequently lacks a value-oriented
perspective. Value-Based Software Engineering (VBSE)
integrates value considerations into current and emerging
software engineering principles and practices [1]. This
research addresses the planning and control aspect of VBSE to
manage the value delivered to stakeholders. Techniques to
model cost, schedule and quality are integrated with business
case analysis to allow tradeoff studies in a commercial
software development context. Business value is accounted
for in terms of return-on-investment (ROI) of different
product and process strategies.
It is a challenge to tradeoff different software attributes,
particularly between different perspectives such as business
and software development. Software process modeling and
Ray Madachy is with the University of Southern California Center for
Software Engineering, University of Southern California, Los Angeles, CA
90089 USA and Cost Xpert Group, San Diego, CA 92109 USA (e-mail:
madachy@usc.edu).

95

2
cumulative effort

II. MODEL OVERVIEW

staffing rate

The system dynamics model represents a business case for
commercial software development. The user inputs and
model factors can vary over the project duration as opposed to
a static model. It can be used dynamically before or during a
project. Hence it is suitable for “flight simulation” training or
actual project usage to reflect actuals to-date.
The sectors of the model and their major interfaces are
shown in Fig. 1. The software process and product sector
computes the staffing profile and quality over time based on
the software size, reliability setting, and other inputs. The
staffing rate becomes one of the investment flows in the
finances sector, while the actual quality is a primary factor in
market and sales. The resulting sales are used in the finance
sector to compute various financial measures.
Fig. 2 shows a diagram of the software process and product
sector. It dynamically calculates effort, schedule and defects.
The staffing rate over time is calculated with a version of
Dynamic COCOMO [5] using a variant of a Rayleigh curve
calibrated to the COCOMO II cost model at the top level. The
project effort is based on the number of function points and
the reliability setting. There are also some parameters that
determine the shape of the staffing curve.

estimated total effort
manpower buildup parameter

~

Function Points

effort multiplier
~

Reliability Setting
defect density
defects

actual quality
~

defect generation rate

defect removal rate

Fig. 2. Software process and product sector
There is a simple defect model to calculate defect levels
used in the market and sales sector to modulate sales. Defect
generation is modeled as a co-flow with the software
development rate, and the defect removal rate accounts for
their finding and fixing. See [4] for more background on
these standard flow structures for effort and defects.
Fig. 3 shows the market and sales sector accounting for
market share dynamics and software license sales. The
perceived quality is a reputation factor that can reduce the
number of sales if products have many defects (see the next
section).

Product Specifications

Software
Process and
Product

learning function

start staff

Product Quality

active licenses

Market and
Sales

new license selling rate

license expiration rate

potential market share

license expiration fraction

~

market size multiplier

potential market
share rate change

Sales Revenue

change in perceived quality

perceived quality

Staffing Rate

Finances

potential market share
increase due to new product

market share delay
~

delay in adjusting perceptions

Fig. 1. Model sectors and major interfaces

~

actual quality

Fig. 3. Market and sales sector
The market and sales model presented herein is a
simplification of a more extensive being used in industry that
accounts for additional marketing initiatives and software
license maintenance sales.
The finance sector is shown in Fig. 4. It includes cash
flows for investment and revenue. Investments include the
labor costs for software development, maintenance and
associated activities.
Revenue is derived from the number of license sales. Sales
are a function of the overall market size and market share
percentage for the software product. The market share is
computed using a potential market share adjusted by

96

3
perceived quality. The additional market share derivable from
a new product is attained at an average delay time. More
details of the model are provided in [4].

modulate sales based on the perceived quality reputation. A
bad quality reputation takes hold almost immediately with a
buggy product (bad news travels fast), and takes a long time to
recover from in the market perception even after defects are
fixed. This phenomenon is represented with asymmetrical
information smoothing as shown in Fig. 5 with a variable
delay in adjusting perceptions.
The graph in Fig. 5 shows a poor quality release at time=3
years with a followup release to the previous quality level.
While the perceived quality quickly plummets after the bad
release, it rises much more slowly even when the actual
quality has improved.

~

~

Staffing Rate

other investments

cumulative investment
~

investment rate

cash flow
cumulative revenue

ROI

1:
2:

revenue generation rate

average license price

1: perceived quality
100 1
2

2: current indicator of quality
1

2

2

1

1

new license selling rate
1:
2:

Fig. 4. Finance sector
A. Quality Modeling and Value Functions
For simplification, software reliability as defined in the
COCOMO II model [5] is used as a proxy for all quality
practices. It models the tradeoff between reliability and
development cost. There are four different settings of
reliability from low to very high that correspond to four
development options. The tradeoff is increased cost and
longer development time for increased quality.
This
simplification can be replaced with a more comprehensive
quality model (see Conclusions and Future Work).
The resulting quality will modulate the actual sales relative
to the highest potential. A lower quality product will be done
quicker; it will be available on the market sooner but sales will
suffer from poor quality. The mapping between reliability and
the relative impact to sales from continuing Delphi surveys is
now captured as a production function and used in the model
with the latest refined numbers.
Collectively there are two value-based production functions
in the model to describe value relationships (they are
illustrated in the first applied example). A market share
production function addresses the organizational business
value of product features. The business value is quantified in
terms of added potential market share attainable by the
features. The relationship assumes that all features are
implemented to the highest quality. Since the required
reliability will impact how well the features actually work, the
relationship between reliability costs and actual sales and is
needed to vary the sales due to quality.
The value function for actual sale attainment is relevant to
two classes of stakeholders. It describes the value of different
reliability levels in terms of sales attainment, and is essentially
a proxy for user value as well. It relates the percent of
potential sales attained in the market against reliability costs.
Illustrations of the production functions are shown in the next
section.
The market and sales sector also has a provision to

1:
2:

2

50

0
0.00

Page 1

1.25

2.50
Years

3.75

5.00

quality

Fig. 5. Perceived quality trends with high and low quality
product deliveries

III. APPLIED EXAMPLES
Several representative business decision scenarios are
demonstrated in this section. The first one demonstrates the
ability to dynamically assess combined strategies for scope
and reliability. The second example looks at strategies of
multiple releases of varying quality. Finally the model is used
to determine a process sweet spot for reliability.
A. Dynamically Changing Scope and Reliability
The model can be used to assess the effects of individual
and combined strategies for overall scope and reliability. This
example will show how it can be used to change product
specifications midstream as a re-plan. Static cost models
typically do not lend themselves to re-plans after the project
starts, as all factors remain constant through time. This
dynamic capability can be used in at least two ways by a
decision-maker:
x assessing the impact of changed product
specifications during the course of a project
x before the project starts, determining if and how late
during the project specifications can be changed
based on new considerations that might come up.
Three cases are simulated: 1) an unperturbed reference
case, 2) a midstream descoping of the reference case and 3) a
simultaneous descoping and lowered required reliability.
Such descoping is a frequent strategy to meet time constraints
by shedding features.

97

4
The market share production function in Fig. 6 relates the
potential business value against the cost of development for
different feature sets. The actual sales production function
against reliability costs is shown in Fig. 7, and it is applied
against the potential market capture. The four discrete points
correspond to required reliability levels of low, nominal, high
and very high. Settings for the three cases are shown in both
production functions.
Fig. 8 shows a sample control panel interface to the model.
The primary inputs for product specifications are the size in
function points (also called scope) and required reliability.
The number of function points is the size to implement given
features. The size and associated cost varies as the number of
features to incorporate.
The reliability settings on the control panel slider are the
relative effort multipliers to achieve reliability levels from low
to very high. These are input by user via the slider for
“Reliability Setting”. The attainable market share derived
from the production function in Fig. 6 is input by the user on
the slider “Potential Market Share Increase”.
Fig. 8 also shows the simulation results for the initial
reference case. The default case of 700 function points is
delivered with nominal reliability at 2.1 years with a potential
20% market share increase. This project is unperturbed
during its course and the 5 year ROI of the project is 1.3.

Added Market Share Percent

25%

100%
High

Percent of Potential Sales

90%

Core
Features

0%
4

6

Case 2
Low
1

1.1

1.2

1.3

Fig. 7. Sales production function and reliability

5%

2

Reference Case
and Case 1

50%

Relative Effort to Achieve Reliability

10%

0

Nominal
60%

0.9

Case 1 and Case 2
(550 Function Points)

High Payoff
Features

70%

30%

Reference Case
(700 Function Points)

15%

Required Reliability
Settings

80%

40%

Features with
Diminishing Returns

20%

Very High

8

Cost ($M)

Fig. 6. Market share production function and feature sets

98

Case 1 in Fig. 9 illustrates the initial case perturbed at .5
years to descope low-ROI features (see Fig. 6 and Fig. 7 for
the points on the production function). The scope goes down
to 550 function points and the staffing profile adjusts
dynamically for it. The schedule is reduced by a few months.
In this case the potential market share increase is lowered by
only two percentage points to 18%. With lower development
costs and earlier delivery the ROI increases substantially to
2.2.
A combined strategy is modeled in Fig. 10 for Case 2. The
scope is decreased the same as before in Case 1 (Fig. 9) plus
the reliability setting is lowered from nominal to low. Though
overall development costs decrease due to lowered reliability,
the market responds poorly. This case provides the worst
return of the three options and market share is lost instead of
gained.
In Case 2 there is an early hump in sales due to the initial
hype of the brand new product, but the market soon discovers
the poor quality and then sales suffer dramatically. These
early buyers and others assume the previous quality of the
product line and are anxious to use the new, “improved”
product. Some may have pre-ordered and some are early
adopters that always buy when new products come out. They
are the ones that find out about the lowered quality and the
word starts spreading fast.
A summary of the three cases is shown in Table I. Case 1
is the best business plan to shed undesirable features with
diminishing returns. Case 2 severely hurts the enterprise
because quality is too poor.

5

Case

Delivered
Size
(Function
Points)
700

Reference
Case:
Unperturbed
Case 1:
Descope
Case 2:
Descope and
Lower
Reliability

Fig. 8. Sample control panel and reference case (unperturbed)
1: staffing rate

2: market share

3: ROI

15
35
3

1:
2:
3:

1
1
2

2

3

2
8
23
1

1:
2:
3:

3

2

2

3
3

3
0
10
-1

1:
2:
3:

0.00

1
2.00

1.00

1
3.00

Page 1

1
4.00

5.00

Years

Delivery
Time
(Years)

Final
Market
Share

1.0

4.78

2.1

28%

1.3

550

1.0

3.70

1.7

28%

2.2

550

.92

3.30

1.5

12%

1.0

1: revenue generationÉ
5
20
2
6

2: cumulative revenue

3: ROI

4: cumulative investÉ

1
3

1: staffing rate
1:
2:
3:

2: market share

1:
2:
3:
4:

3
10
1
3

3

2
3

3
8
23
1

1:
2:
3:
4:

3

1
0
-1
0

1

2

Page 2

3

4

4

2

1
2

2

0.00
3

4

1

1

2

4

3: ROI

15
35
3

1

1:
2:
3:

ROI

B. Multiple Releases
This example shows a more realistic scenario for
maintenance and operational support.
Investments are
allocated to ongoing maintenance and the effects of additional
releases of varying quality are shown.
The reference case contains two product rollouts at years 1
and 3, each with the potential to capture an additional 10% of
the market share. These potentials are attained because both
deliveries are of high quality as seen in Figs. 11-12.
A contrasting case in Figs. 13-14 illustrates the impact if
the second delivery has poor quality yet is fixed quickly (Fig.
5 shows the quality trends for this case). This results in a
change of revenue from $11.5 M to $9.6M, ROI from 1.3 to
0.9.
This example is another illustration of the sensitivity of the
market to varying quality. Only one poor release in a series of
releases may have serious long term consequences.
1:
2:
3:
4:

Fig. 9. Case 1 – descoping of low ROI features at time = .5
years

TABLE I
CASE SUMMARIES
Delivered
Cost
Reliability ($M)
Setting

1.25

2.50
Years

3.75

5.00

financials

2
3

3

1:
2:
3:

0.00
Page 1

2

0
10
-1
1.00

1
2.00

1
3.00

2

1
4.00

Fig. 11. Reference case financials for two high quality
product deliveries

5.00

Years

Fig. 10. Case 2 – descoping of low ROI features and
reliability lowering at time = .5 years

99

6

1:
2:
3:
4:

1: active licenses
3000
1100
110
40

1:
2:
3:
4:

1500
650
65
25

2: new license selling É

3: license expiration rÉ

4: potential market shÉ

3

2

4

1
4

4

3

2

1
3

1

1:
2:
3:
4:

0
200
20
10

2

4

1
2

3

0.00

1.25

2.50
Years

Page 1

3.75

5.00

sales and market

Fig. 12. Reference case sales and market for two high quality
product deliveries
1:
2:
3:
4:

1: revenue generationÉ
5
10
2
6

2: cumulative revenue

3: ROI

4: cumulative investÉ

4

2

1:
2:
3:
4:

3
5
1
3

3

1
0
-1
0

1

4

1

1

2

Bad Quality Loss = Maximum Potential Revenue with Same
Timing – Revenue.

4

3

1:
2:
3:
4:

3

4

3

1

2

2

0.00

1.25

2.50
Years

Page 2

3.75

5.00

financials

Fig. 13. Financials for high and low quality product deliveries
1:
2:
3:
4:

1: active licenses
3000
1000
100
40

2: new license selling É

example. A more comprehensive risk analysis would consider
probability distributions to obtain a range of results.
Probability is considered constant for each case and is not
explicitly used in the calculations. Only the costs (or losses)
are determined.
A set of runs is performed that simulate the development
and market release of a new 80 KSLOC product. The product
can potentially increase market share by 30%, but the actual
gains depend on the level of quality. Only the highest quality
will attain the full 30%. Other parameterizations are an initial
total market size = $64M annual revenue, the vendor has 15%
initial market share, and the overall market doubles in 5 years.
A reference case is needed to determine the losses due to
inferior quality. The expected revenues for a sub-quality
delivery must be subtracted from the maximum potential
revenues (i.e. revenue for a maximum quality product
delivered at a given time). The latter is defined as delivering a
maximum quality product at a given time that achieves the full
potential market capture. The equation for calculating the loss
due to bad quality is

3: license expiration rÉ

4: potential market shÉ

3

4

The loss due to market delay is computed keeping the
quality constant. To neutralize the effect of varying quality,
only the time of delay is varied. The loss for a given option is
the difference between the revenue for the highest quality
product at the first market opportunity and the revenue
corresponding to the completion time for the given option
(assuming the same highest quality). It is calculated with
Market Delay Cost = Maximum Potential Revenue –
Revenue.

2

1:
2:
3:
4:

4

1500
600
60
25

4

1

2

1
3

1

1:
2:
3:
4:

0
200
20
10

2

0.00
Page 1

2

3

4

1
3

1.25

2.50
Years

3.75

5.00

sales and market

Fig. 14. Sales and market for high and low quality product
deliveries
C. Finding the Sweet Spot
This example derived from [4] shows how the value-based
product model can support software business decision-making
by using risk consequence to find the quality sweet spot with
respect to ROI. The following analysis steps are performed to
find the process sweet spot:
x vary reliability across runs
x assess risk consequences of opposing trends:
market delays and bad quality losses
x sum market losses and development costs
x calculate resulting net revenue to find process
optimum.
The risk consequences are calculated for the different
options. Only point estimates are used for the sake of this

Fig. 15 shows the experimental results for an 80 KSLOC
product, fully compressed development schedules and a 3year revenue timeframe for different reliability options. The
resultant sweet spot corresponds to reliability=high. The total
cost consisting of delay losses, reliability losses and
development cost is minimum at that setting for a 3-year time
horizon. Details of the intermediate calculations for the loss
components are provided in [4].
The sweet spot depends on the applicable time horizon,
among other things. The horizon may vary due for several
reasons such as another planned major upgrade or new
release, other upcoming changes in the business model, or
because investors mandate a specific timeframe to make their
return.
The experiment was re-run for typical time horizons of 2, 3
and 5 years using a profit view (the cost view is transformed
into a profit maximization view by accounting for revenues).
The results are shown in Fig. 16.
The figure illustrates that the sweet spot moves from
reliability equals low to high to very high. It is evident that
the optimal reliability depends on the time window. A shortlived product (a prototype is an extreme example) does not
need to be developed to as stringent reliability as one that will

100

7
live in the field longer.

x

$35

development cost
market delay loss
bad quality loss
total cost

$30

Cost (Millions)

internal feedback on product initiatives from an
organizational planning and control entity to the
software process.
A more comprehensive model would consider long term
product evolution and periodic upgrades. Another related
aspect to include is general maintenance by adding explicit
activities for operational support.
The product defect model can be enhanced with a dynamic
version of COQUALMO [6] to enable more constructive
insight into quality practices. This would replace the current
construct based on the single factor for required software
reliability.
Other considerations for the model are in the market and
sales sector. The impact of different pricing schemes and
varying market assumptions on initial sales and maintenance
can all be explored. Some of these provisions are already
accounted for in a proprietary version of the model.
The model application examples were run with idealized
inputs for sake of demonstration, but more sophisticated
dynamic scenarios can be easily handled to model real
situations. For example discrete descopings were shown, but
in many instances scope will exhibit continuous or fluctuating
growth over time.
More empirical data on the relationships in the model will
also help identify areas of improvement. Assessment of
overall dynamics includes more collection and analysis of
field data on business value and quality measures from actual
software product rollouts.

$25
$20
$15
$10
$5
$0
Low

Nominal

High

Very High

Software Reliability

Fig. 15. Calculating reliability sweet spot (3-year timeframe)
$180
$160

2 year time horizon
3 year time horizon
5 year time horizon

Profit (Millions)

$140
$120
$100
$80
$60
$40

REFERENCES

$20

[1]

$0
Low

Nominal

High

Very High

[2]

Software Reliability

[3]

Fig. 16. Reliability sweet spot as a function of time horizon
[4]

IV. CONCLUSIONS AND FUTURE WORK

[5]

It is crucial to integrate value-based methods into the
software engineering discipline. To achieve real earned value,
business value attainment must be a key consideration when
designing software products and processes. This work shows
several ways how software business decision-making can
improve with value information gained from simulation
models that integrate business and technical perspectives.
The model demonstrates a stakeholder value chain whereby
the value of software to end users ultimately translates into
value for the software development organization. It also
illustrates that commercial process sweet spots with respect to
reliability are a balance between market delay losses and
quality losses. Quality does impact the bottom line.
The model can be elaborated to account for feedback loops
to generate revised product specifications (closed-loop
control). This feedback includes:
x
external feedback from user to incorporate new features

[6]

101

B. Boehm, L. Huang: “Value-based software engineering: A case study”,
IEEE Software, Vol. 20, No. 2, 2003
B. Boehm, L. Huang, A. Jain, R. Madachy, “Reasoning about the ROI of
software dependability: the iDAVE Model”, IEEE Software, Vol. 21,
No. 3, 2004
R. Madachy, “A software product business case model”, Proceedings of
the 5th International Workshop on Software Process Simulation and
Modeling, 2004
R. Madachy, Software Process Dynamics, IEEE Computer Society
Press, Washington D.C., 2005 (to be published)
B. Boehm, C. Abts, W. Brown, S. Chulani, B. Clark, E. Horowitz, R.
Madachy, D. Reifer, B. Steece, Software Cost Estimation with
COCOMO II, Prentice-Hall, 2000
S. Chulani, B. Boehm, “Modeling software defect introduction and
removal: COQUALMO (COnstructive QUALity MOdel)”, USC-CSE
Technical Report 99-510, 1999

A Software Product Line Process Simulator
Yu Chen, Gerald C. Gannod, and James S. Collofello

Abstract—A software product line is a set of softwareintensive systems sharing a common, managed set of features
that satisfy the specific needs of a particular market segment
or mission, and are developed from a common set of core assets
in a prescribed way. A software product line approach
promises shorter time-to-market and decreased life cycle cost.
However, those benefits are not guaranteed under every
situation and are affected by many factors, such as number of
available employees, market demands, reuse rate, process
maturity, and product line adoption and evolution approaches.
Before initiating a software product line, an organization needs
to evaluate available process options in order to see which one
best fits its goals. The aim of this research is to develop a
software product line process simulator that can predict the
cost for a selected software product line process and provide
useful information for cost-benefit analysis.

A

I. INTRODUCTION

software product line is a set of software-intensive
systems sharing a common, managed set of features
that satisfy the specific needs of a particular market segment
or mission. A defining characteristic of product lines is that
products are developed from a common set of core assets in
a prescribed way [1]. A software product line approach
promises shorter time-to-market, higher product quality, and
decreased life cycle cost [1]. However, those benefits are
not guaranteed under every situation and are affected by
many factors including the number of available employees,
market demands, reuse rate, process maturity, and the
selected product line adoption and evolution approaches.
Before initiating a software product line, an organization
needs to evaluate available process options in order to see
which one best fits its goals. The aim of this research is to
develop a software product line process simulator that can
predict the cost for a selected software product line process
and provide useful information for cost-benefit analysis.
Several techniques have used simulation to study software
processes, including [2]. However, to our best knowledge,
this is the first simulator in literature to address software
product line process issues.
In this paper, an architecture-based software product line
process simulator is presented.
The simulator uses
Microsoft Project [3] to define an organization's product line
development process. DEVSJAVA [4] is used as the
This material is based upon work supported by the National Science
Foundation under career grant No. CCR-0133956.
Yu Chen, Dept. of Computer Science and Engineering, Arizona State
University - Tempe Campus, Tempe AZ 85287, USA, yu_chen@asu.edu.
Gerald C. Gannod, Division of Computing Studies, Arizona State
University - East Campus, Mesa AZ 85212, USA, gannod@asu.edu.
James S. Collofello, Dept. of Computer Science and Engineering,
Arizona State University - Tempe Campus, Tempe AZ 85287, USA,
collofello@asu.edu.

modeling and simulation formalism and COPLIMO [5] is
used as the underlying cost model. The simulator is meant
to be used after the high-level product line and product
architectures have been defined. The inputs to the simulator
include a product line life cycle project plan at the
component granularity level, available resources, product
demands, and cost model parameter values. These inputs
are at a level that allows for organizational level product
line planning. The outputs from the simulator provide
estimates of the first release time, initial development effort,
life cycle effort for each product, life cycle effort for the
whole product line, and resource usage rates. By varying
the inputs and comparing the outputs, a manager can make
decisions about whether a product line approach should be
used, and given that one is used, what strategies should be
adopted, and what the potential resource allocations should
be.
In our previous work, we developed an early stage
software product line simulator [6] meant to be used when
the product line architecture and system features are still
vague. As such, it makes some simplification assumptions
about the uniformity of the product size, change rate, reuse
rate, etc. Also, it only supports a limited number of product
line adoption and evolution strategies. The architecturebased simulator presented in this paper is meant to be used
when the product line architecture, feature model, and
product map are well-defined. It removes many simplifying
assumptions made by the previous simulator, supports more
general product line processes, and provides more
information in the simulation results.
The remainder of the paper is organized as follows.
Section 2 presents background information and related
work. Section 3 describes the approach and the simulator.
Results are discussed in Section 4. Section 5 draws
conclusions and suggests future investigations.
II. BACKGROUND AND RELATED WORK
This section describes background and related work on
software product lines and product line cost models.
A. Software Product Lines
Software product line development involves three
essential activities: core asset development, product
development, and management. Core asset development
(domain engineering) involves the creation of common
assets and the evolution of the assets in response to product
feedback, new market needs, etc. Product development
(application engineering) creates individual products by
reusing the common assets, gives feedback to core asset
development, and evolves the products. Management
includes technical and organizational management, where
technical management is responsible for requirement control

102

and the coordination between core asset and product
development.
A software product line can be initiated under several
situations: independent, project-integration, reengineeringdriven, and leveraged [7]. Under the independent situation,
a product line is created without any pre-existing products.
Under the project-integration situation, a product line is
created to support both existing and future products. Under
a reengineering-driven scenario, a product line is created by
reengineering existing legacy systems. And the leveraged
situation is where a new product line is created based on
some existing product lines.

exclusive. For instance, a product line can be adopted via a
proactive approach and then evolved through a reactive
approach.

Fig. 2. Reactive approach process flow

Fig. 1. Proactive approach process flow

There are two main software product line development
approaches: proactive and reactive. With the proactive
approach, core assets are developed first to support future
products. With the reactive approach, core assets are
incrementally created when new requirements arrive.
Depending on the degree of planning-ahead, the proactive
approach can be classified into big bang and incremental
approach [7]. With the big bang approach, core assets are
developed for a whole range of products prior to the
creation of any individual product. With the incremental
approach, core assets are incrementally developed to
support the next few upcoming products. Fig. 1 shows the
process flow of the proactive approaches. Some common
reactive approaches are: infrastructure-based, branch-andunite, and bulk-integration [7]. The infrastructure-based
approach does not allow deviation between the core assets
and the individual products, and requires that new common
features be first implemented into the core assets and then
built into products. Both the branch-and-unite and the bulkintegration approaches allow temporal deviation between
the core assets and the individual products. The branch-andunite strategy requires that the new common features be
reintegrated into the core assets immediately after the
release of the new product, while the bulk-integration
strategy allows the new common features to be reintegrated
after the release of a group of products. Fig. 2 shows the
process flow for the infrastructure-based and branch-andunite approach.
These approaches are not mutually

B. Product line cost models
Several software product line cost estimation approaches
[5], [10], [11] have been proposed. Bockle et al. discussed
software product line adoption scenarios and presented a
product line cost model [10]. Among the seven adoption
scenarios, only two of them, developing a single product
line without pre-existing products, are considered by this
paper. Their cost model takes organization costs into
account, which is not considered in this work. Boehm et al.
proposed COPLIMO [5], a COCOMO II [12] based model,
for software product line cost estimation. COPLIMO has a
basic life cycle model, which consists of a product line
development cost model and an annualized postdevelopment extension.
This simulator uses COPLIMO [5] as the underlying cost
model. However, in the implementation, the cost model is
designed as a plug-in model, thus allows other cost models
to be used as well. The early stage simulator [6] uses the
COPLIMO basic life cycle model. To allow more detailed
modeling, the basic life cycle model is extended by using
COCOMO II [12] and is used by this architecture-based
simulator.
III. APPROACH
This section presents the overview of our approach, the
software product line process model, and the simulation
tool.
A. Overview
Before initiating a software product line, an organization
needs to evaluate available process options in order to see
which one best fits its goals. Fig. 3 shows the process
evaluation steps and where the simulator fits into the
process. Software product line process is first defined by
using architecture definition, then the process definition is
simulated, and the simulation results are analyzed. The
current tools used for these steps are Microsoft Project [3],

103

the simulator discussed here, and Microsoft Excel,
respectively. The cost, time, and resource usage estimates
provided by the simulator can also be used to refine the
process definition. For instance, the outputs can be
reinterpreted into process definition to provide a detailed
project schedule plan.

Fig. 3. Process evaluation

development process is one of the following types: new
development, adoption, and reuse. New development
means development without reuse, adoption refers to whitebox reuse, and reuse means black-box reuse. Module
maintenance represents module modification activities.
Some attributes associated with the models are shown in
Fig. 5. The attributes include standard Microsoft Project
parameters (e.g. UID, WBS, and predecessorLinks),
parameters needed by the cost model [5] (e.g. dm, cm, and
im), and simulator specific parameters (e.g. processType).
The temporal relationships among processes are described
by
``predecessorLinks'',
which
is
a
list
of
``predecessorLink''. If Process A has a ``predecessorLink''
that points to Process B, then Process A can not start until
Process B is finished. Each process can have resources
associated with it. For this study, only the human resources
to conduct the processes are considered.
Description
Unique identification
Work breakdown structure code
Process start time
Process finish time
A list of links to the predecessors
The type of the process as described above
The target of the process, such as the name of
a product line, product, and module
Process duration in months
tdev
Effort in person-months
pm
The number of resources
units
The sum of scale drivers
scaleFactor
Project schedule
sced
scedCompression Project schedule compression
Product life span in years
lifeSpan
The product of effort multipliers
eaf
Source line of code
sloc
Maintenance change factor
mcf
Percentage of design modification
dm
Percentage of code modification
cm
Percent of Integration Required for Modified
im
Software
Percentage of reuse effort due to software
su
understanding
programmer unfamiliarity with software
unfm
percentage of reuse effort due to assessment
aa
and assimilation
Attribute
UID
WBS
startTime
finishTime
predecessorLinks
processType
dest

Fig. 4. Process meta-model

B. Software product line process model
The organizational level software product line
development process was abstracted into a model as shown
in Fig. 4. This model categorizes processes into three levels
product line, product, and module. A product line process
consists of several product processes. Each product process
is one of the following types: product demand, product
development, product maintenance, product enhancement,
and annual maintenance. Product demand represents
external new product requirements. Product development
refers to initial product creation. Product maintenance
represents product modification activity, and annual
maintenance refers to annually planned product
maintenance.
Product enhancement models the
maintenance activity that is to improve the product
functionality and involves major product change. It is not
considered as product maintenance because it often involves
different cost calculation method. A product process has
several module processes. Each module process is either a
module development or maintenance process. A module

Fig. 5. Attribute description

To define a software product line process of this kind,
inputs from market analysis, high-level feature model and
product map, and product line development and evolution
strategies are needed. Market analysis results tell what kind
of products will be needed in the future and when they will
be needed. The feature model and product map allows
decomposing products into high-level modules and
recognizing reusable modules.
The product line
development and evolution strategies provide guidance on
how to create and evolve core assets as well as products and
in which order. This software product line process is
mainly for organizational level management control, so only
one level module process is provided. If more detailed
management is needed, the process can serve as process
architecture and allow further module level process
refinement.

104

C. Simulation tool
The simulator is implemented in Java using the
DEVSJAVA [4] formalism. It uses Microsoft Project [3] as
the process definition tool and COPLIMO [5] as the cost
model. The input is a process definition (in XML format)
that conforms to the previously discussed process meta-

model. In the process definition, only the resources for the
product line level processes need to be specified. The
simulator will use the cost model to assign resources to the
lower level processes.

Fig. 6. Simulation tool in execution

Fig. 7. Simulation results

105

The simulator user interface is depicted in Fig. 6. The
upper part of the interface identifies the current running
model and its package ("PLPEF" and "productLineProcess",
respectively). The large middle area shows the models and
their hierarchical relationships. The bottom of the window
contains execution control components. The "step" button
allows running the simulator step by step, the "run" button
allows executing the simulator to the end, and the "restart"
button allows starting a new simulation run without quitting
the system. The "clock" label displays the current simulation
time in the unit of months, and selecting the "always show
couplings" checkbox will allow couplings between models
to be displayed. The simulation speed can be manipulated at
run time to allow execution in near real-time or logical time
(slower/faster than real-time).
At the end of each simulation run, a result table is
generated similar to Fig. 7. The table has three sections.
The product section provides data related to individual
products including initial source line of code (ISLOC), first
release time (FRT), time-to-market (TTM), initial
development effort (IDE), initial development time (IDT),
accumulated development and maintenance effort (ADE),
accumulated development and maintenance time (ADT),
and accumulated process waiting time (AWT). The product
line section summarizes the product line related statistics,
which include accumulated development and maintenance
effort (ADE), accumulated development and maintenance
time (ADT), product line life span (LS), average annual
development effort (AADE), average time-to-market
(ATTM), and accumulated process waiting time (AWT). In
the table, the unit of effort is person-months and the unit of
time is months. The resource section provides resource
usage data including percentage of the time the resource
pool is in the wait stage for the lack of resources (PWT),
average resource usage rate (AUR), minimum resource
usage rate (MinUR) , and maximum resource usage rate
(MaxUR). Comparing with the early stage simulator result,
this result adds two sections (Product Line and Resource)
and two fields (ISLOC and AWT) for the Product section.
The following assumptions are made in the simulation
model:
1. All the employees have the same capability and can
work on any project.
2. Every product (including core asset) has two
concurrent development processes: one for planned
maintenance and one for development and
unplanned maintenance. Each of the process
handles the requirements in FIFO order.
3. For each product, its fractions of new productspecific, adapted, and reused code stay relatively
constant across its life cycle.
Assumption 1 is a simplification of the reality, where
each employee has different skills and capability. The
purpose of this simulator is to give high-level cost and time
estimates, so the average is used to model employee
capability. If detailed modeling is needed, more Employee
Pool instances can be used to model employees with
different level of capabilities. Assumption 2 is based on
experience with software development in real development
organizations. In the case that there are more concurrent

processes for a product, more Development or Maintenance
instances can be used. Assumption 3 is made by the cost
model, COPLIMO [5], and can be changed if other cost
models are used.
IV. RESULTS
In this section a case study is presented to illustrate the
use and the analytical capability of the simulator. The data
shown is the result of executing the simulation system.
A. Overview
The target of the case study is a simulator product line.
Specifically, the case study models a product line of
software process simulators. Its feature model [13] is
depicted in Fig. 8. The feature model indicates that a
simulator must have an IO, Cost Model, and Simulation
Model. The IO can include a basic IO, and optionally Web
or GUI IO. The Cost Model can use either Model1 or
Model2. The Simulation Model can include Early Stage
model, or Later Stage model, or both. Fig. 9 provides the
name, description, and size information for each high-level
component within the product line. The product line
product map displayed in Fig. 9 shows the relationships
between the components and individual products. The
feature model, high-level component map, and product map
are the results of market analysis and high-level architecture
design. Based on this information and market demand
analysis, development strategies and decisions (such as
which components should be included in the core assets,
how and core assets should be developed, evolved, and
reused, and when the products should be developed and
maintained) can be represented by using the product line
process model described earlier.

Fig. 8. Feature model
Eight different strategies for building the product line
were defined into eight processes and then simulated. The
first seven strategies all use the product line approach, while
the last strategy uses traditional independent product
development approach. The product line strategies differ in
the number of employees, adoption approaches, and
evolution methods. They all use proactive approach (big
bang or incremental) for product line adoption and reactive
approach (infrastructure-based or branch-and-unite) for
product line evolution. For the big bang approach, the core
assets are fully developed before Product 1. For the
incremental approach, the core assets are first developed
before Product 1 and then increased before Product 4.

106

Name
C1
C2
C3
C4
C5
C6
C7
C8
C9
C10

Description
Basic IO - early stage
Web IO - early stage
GUI IO - early stage
Basic IO - later stage
Web IO - later stage
GUI IO - later stage
Simulation model - early stage
Simulation model - later stage
Cost model1
Cost model2

Percentage of Waiting Time for these strategies. We can see
that as the number of employees reduces the percentage of
waiting time increases, which explains the time-to-market
differences between these processes. Strategy 5 provides
sufficient resources for the product line development, so its
percentage of waiting time is 0. By comparing the results, a
manager can make decision on how many resources should
be allocated to the product line development.

Size
20k
30k
30k
20k
30k
30k
50k
50k
30k
30k

Prod10

Prod9

Prod8

Prod7

Prod6

Prod5

Prod4

Prod3

Prod2

Name

Prod1

Fig. 9. High-level components

9 9 9 9 9 9 9 9 9 9
9 9
9 9 9 9 9 9
9
9
9
9
C3
9 9 9
9 9
C4
9 9
9 9
C5
9
9
C6
C7 9 9 9 9 9 9 9 9 9 9
9 9 9
9 9
C8
C9 9 9 9 9 9 9
C10
9 9 9 9
C1
C2

Fig. 12. Effect of resources on time-to-market

Fig. 10. Product map

Parameter
Number of employees
Big bang
Incremental
Infrastructure based
Branch and unite
Independent development

Strategy
1 2 3 4 5 6 7 8
50 55 60 90 90 90 90 90

9
9
9 9 9
9
9
9 9 9 9 9
9 9
9

Fig. 11. Strategies

B. Effect of resources
Strategies 1, 2, 3 and 5 differ in the number of employees
(50, 55, 60, and 90, respectively) in the context of
incremental adoption and infrastructure-based evolution.
The results of their simulations differ in time-to-market, as
shown in Fig. 12, which was generated using data similar to
that shown in Fig. 7. For the first three products, there is no
difference in time-to-market, because the available
resources are sufficient for all the three cases. Starting from
Product 4, we see more resource-constrained strategies have
longer time-to-market than less resource-constrained ones.
When the request for Product 4 comes, more resources are
needed for increasing the core assets while some resources
are still held by new product development activities. For the
processes that do not have enough resources, their core asset
incremental activities are put on hold until more resources
are made available, which in turn delays development of
later products. Another reason is that as more products are
developed, more resources are used for maintenance and
fewer resources are available for new product development.
In the case that there are not enough resources, processes
with fewer resources need to wait longer to start the
development or maintenance activities. Fig. 13 shows that

Fig. 13. Effect of resources on resource usage

Figure 13 shows that there is a downward trend of timeto-market after Product 4, because the resource request peak
happens when the core assets need to be increased. After
that resource requests tend to decline. In our previous study
[6] on the effect of resources, the time-to-market had an
increasing trend in the context of big bang adoption and
infrastructure-based evolution. That is because after the
initial core development, the resource request peak
gradually comes when more products are under maintenance
while new products need to be developed. In general, lack
of resources will result in longer time-to-market, but the
trend of time-to-market over the time depends on the
characteristics of the individual processes, such as adoption
and evolutions strategies, reuse rates, and the frequency of
market demands.
C. Effect of evolution approaches
Strategies 4 and 6 differ in product line evolution
approach (infrastructure-based and branch-and-unite,
respectively). Fig. 14 shows the comparison of the results
with respect to time-to-market. The inputs specify that the
evolution stage starts from Product 7. For Product 7, the

107

branch-and-unite approach has lower time-to-market
because it can start product development without waiting for
core asset update. For the later products, the branch-andunite approach results in longer time-to-market because new
product development can not be started until the previous
product development has been finished and the core assets
have been updated. This reduces concurrency, and explains
the reason that the average resource usage for the branchand-unite approach is less than the infrastructure-based
approach, as shown in Fig. 15. The results also indicate that
the branch-and-unite approach results in more effort than
the infrastructure-based approach, due to the extra rework
required on the new products after the core updates.

Fig. 14. Effect of evolution approaches on time-to-market

can see, the big bang with infrastructure-based approach has
the shortest average time-to-market, and the traditional
approach has the longest average time-to-market. The
traditional approach has the shortest time-to-market for the
first two products, because it does not incur overhead from
core asset creation. For the first three products, the big bang
approaches have the highest time-to-market, because of the
core asset development overhead. For Product 4, the
incremental adoption has the longest time-to-market due to
the core increase. For Product 6 through 10, the traditional
approach results in the longest time-to-market, because of
the higher development effort and lack of resources. By
large-scale reuse, product line approaches generally result in
smaller code size to development and maintain. Thus, the
total effort and time on creating and evolving the products
in a product line is smaller. Fig. 16, except the branch-andunite cases, is consistent with the second hypothesis [14]
proposed by Knauber et al., which says that after some
initial investment, the time-to-market per product decreases
down to a certain minimum and significantly below the
respective time-to-market in the case of independent
development approach. For the branch-and-unite evolution
strategies, although the time-to-market is still below the
traditional approach, it has an increasing trend, which is
caused by the dependencies imposed by this approach. This
is consistent with Riva and Delrosso's observation from the
product line development. They pointed out that some
issues, such as organization bureaucracy and dependencies
among tasks, can harm family evolution [15].

Fig. 15. Effect of evolution approaches on resource usage

D. Effect of combined approaches
Before developing a family of products, an organization
needs to evaluate the available strategic options and
determine which one best fits its goals. Strategies 4 - 8 show
the alternatives an organization might have. Strategy 8 is the
case for traditional software development (independent
development) where products are created and evolved
independently.
Fig. 16 shows the comparison of the alternatives in timeto-market. During the adoption stage (from Product 1 to 6),
the time-to-market differences are mainly caused by the
adoption approaches; during the evolution stage (from
Product 7 to 10), the differences are mostly influenced by
the evolution strategies. Because the transition from the
adoption stage to the evolution phase happens between
Product 6 and 7, the time-to-market transitions among
different approaches occur between Products 6 and 7. As we

Fig. 16. Effect of combined approaches on time-to-market

Fig. 19 compares the product line initial development
efforts (including initial product development effort and
core asset development and increment effort) between big
bang and incremental product line adoption and traditional
approach. The big bang adoption involves highest initial
development effort and lowest total effort. The traditional
approach requires the lowest initial effort and highest total
effort. The incremental adoption requires less initial effort
but higher total effort than the big bang adoption. The
figure suggests that there should be at least three products to
make the product line approach appealing. These results are
consistent with the product line investment curve described
by Schmid and Verlage [7]. The results also confirms the
first product line hypotheses [14] formulated by Knauber et
al., which states that after some initial investment, the effort
that is needed to develop a new product decreases
significantly below the effort that is required to build the

108

same product using independent development approach.

the simulator and comparing their results, an organization
can evaluate the alternative processes.
Sensitivity analysis has been conducted using the
simulator. The results show that when reuse rate is high
enough, resources are sufficient, and demands come at
appropriate frequency, software product line processes
result in lower life cycle costs, lower resource usage, and
shorter average time-to-market. The results also show that
the big bang adoption approach requires higher initial costs
but lower total costs, compared to the incremental approach.
In general, we feel the results of the simulator confirm
common knowledge on software product lines [7]. In the
future, we plan to solicit expert feedback and compare
simulation results with real product line data.
In addition, we plan on incorporating other cost models
into the simulator, using XPDL [16] as the process
definition language, and combining optimization models
with the simulator.

Fig. 17. Effect of combined approaches on resource usage

REFERENCES
[1]
[2]

[3]
[4]
Fig. 18. Effect of combined approaches on accumulated development effort

[5]

[6]

[7]
[8]
[9]
[10]
Fig. 19. Initial development effort comparison
[11]

V. CONCLUSIONS
Software product line approach requires higher up-front
investment and results in increased process complexity. In
order to reduce investment risk and ease process
management, decision and process tools are necessary. In
this paper, we described a simulator that is intended to
support post-architecture software product line process
impact estimation. It uses DEVSJAVA [4] as the modeling
and simulation formalism, COPLIMO [5] as the cost model,
and Microsoft Project [3] as the process definition tool. The
simulator can provide both dynamic and static information
for the selected process. Stepping through the simulator
allows tracing product line process and uncovering hidden
problems. The static results generated at the end of the
simulation gives time, cost, and resource estimates for the
selected process. By running different processes through

[12]

[13]
[14]

[15]
[16]

109

P. Clements and L. M. Northrop, Software Product Lines: Practices
and Patterns. Boston MA U.S.A.: Addison-Wesley, Aug 2001.
M. Host, B. Regnell, J. N. O. Dag, and J. Nedstam, “Exploring
bottlenecks in market-driven requirements management processes
with discrete event simulation,” Systems and Software, vol. 59, pp.
323–332, 2001.
Microsoft Corporation, “Microsoft project,” Jan 2005. [Online].
Available: http://office.microsoft.com/en-us/FX010857951033.aspx
B. P. Zeigler and H. S. Sarjoughian, “Introduction to DEVS modeling
& simulation with Java,” Aug 2003.
B. Boehm, A. W. Brown, R. Madachy, and Y. Yang, “A software
product line life cycle cost estimation model,” in ISESE ’04:The 2004
International Symposium on Empirical Software Engineering
(ISESE’04). IEEE Computer Society, 2004, pp. 156–164.
Y. Chen, G. C. Gannod, J. S. Collofello, and H. S. Sarjoughian,
“Using simulation to facilitate the study of software product line
evolution,” in 7th Intl. Workshop on Principles of Software Evolution,
Kyoto, Japan, Sep 2004.
K. Schmid and M. Verlage, “The economic impact of product line
adoption and evolution,” IEEE Software, vol. 19, no. 4, pp. 50 – 57,
Jul 2002.
C. W. Krueger, “Easing the transition to software mass
customization,” in the 4th International Workshop on Software
Product-Family Engineering, Bilbao Spain, Oct 2001, pp. 282–293.
J. Bosch, “Maturity and evolution in software product lines:
Approaches, artefacts and organization,” in The Second Software
Product Line Conference, San Diego, USA, Aug 2002, pp. 257–271.
G. Bockle, P. Clements, J. D. McGregor, D. Muthig, and K. Schmid,
“Calculating roi for software product lines,” IEEE Software, vol. 21,
pp. 23–31, May/Jun 2003.
S. Cohen, “Predicting when product line investment pays,” Software
Engineering Institute, Tech. Rep. CMU/SEI-2003-TN-017, Jul 2003.
B. W. Boehm, B. Clark, E. Horowitz, J. C. Westland, R. J. Madachy,
and R. W. Selby, “Cost models for future software life cycle
processes: COCOMO 2.0,” Annals of Software Engineering, vol. 1,
pp. 57–94, 1995. [Online]. Available:
citeseer.ist.psu.edu/boehm95cost.html
K. Czarnecki and U. W. Eisenecker, Generative programming:
methods, tools, and applications. Addison-Wesley, 2000.
P. Knauber, J. Bermejo, G. Bockle, J. C. S. do Prado Leite, F. van der
Linden, L. M. Northrop, M. Stark, and D. M. Weiss, “Quantifying
product line benefits,” in PFE ’01: Revised Papers from the 4th Intl.
Workshop on Software Product-Family Engineering. London, UK:
Springer-Verlag, 2002, pp. 155–163.
C. Riva and C. D. Rosso, “Experiences with software product family
evolution,” in Sixth International Workshop on Principles of Software
Evolution, Helsinki, Finland, Sep 2003.
W. M. Coalition, “Workflow process definition interface - XML
Process Definition Language,” Workflow Management Coalition,
Tech. Rep. WFMC-TC-1025, Oct 2002.

Simulation Models Applied to Game-Based Training for Software Project
Managers
Alexandre Ribeiro Dantas
COPPE UFRJ
Caixa Postal 68511
Rio de Janeiro, Brazil
alexrd@cos.ufrj.br

Márcio de Oliveira Barros
DIA UNIRIO
Av. Pasteur 458, Urca
Rio de Janeiro, Brazil
marcio.barros@uniriotec.br

Cláudia Maria Lima Werner
COPPE UFRJ
Caixa Postal 68511
Rio de Janeiro, Brazil
werner@cos.ufrj.br

By analyzing past situations and evaluating the
different paths that the project could have followed if
specific decisions were made, a student can enhance
his management skills. In this sense, case studies
analysis and role-play strategies are useful to
develop the students’ expertise. This proper
investment in project management education,
emphasizing a hands-on learning approach, seems to
play an important role in preventing future faulty
projects due to inadequate management.
In this work we propose a simulation-based game
as an educational instrument to provide learning-bydoing for project managers. A System Dynamics
[10] software project model describes how the game
will react to user decisions. A simulator and a game
machine complement the game. While the simulator
is responsible for executing model equations, the
game machine handles user interactions. Such
interactions are inserted into the simulation and the
game machine provides feedback results in a gamelike fashion, through clear goals, challenge, drama
and multimedia effects.
The remainder of this paper is organized as
follow. Section 2 discusses the challenges to
educational models for project management,
emphasizing the learning-by-doing approach.
Section 3 presents our approach to add game features
to simulation models. Section 4 discusses the
limitations of the modeling approach that we have
selected and the changes that were made to introduce
game-related functionalities into this simulator.
Section 5 provides qualitative evaluations about the
use of a game-based training model while section 6
presents our final considerations.

Abstract
Inadequate use of project management techniques
in software projects can be traced to the lack of
efficient education strategies for managers. In this
work, the learning-by-doing model is presented as
an approach for project management education.
This model requires an environment where students
can act as managers without the risks associated to
the failure of real software projects. The limitations
of simulation tools and pilot-projects are discussed
and a game-based training approach is proposed to
address project management hands-on education
requirements.

1. Introduction
Many studies have observed that the adoption of
project management techniques in software
development projects is usually deficient and
inadequate [17][23]. Such studies suggest that these
deficiencies may be a potential cause for so many
projects experiencing poor quality, schedule and
budget overruns. On the other hand, one possible
trace to inadequate use of project management can
be related to the lack of comprehension of its
essential practices and techniques.
Since project management is accepted as a
knowledge-intensive activity, personal expertise
drives its major success factors. However, the
common practice of promoting new managers from
the technical staff, due to their success when
applying technical skills, and the lack of efficient
project management education strategies contribute
to this scenario of inadequate management [17].
While novice managers usually take “fire-fighting”
actions based on intuitive decisions which can lead
to budget and schedule overruns, experienced
managers apply lessons learned from past projects,
even if these were unsuccessful ones (the learningby-error approach) [13].

2. Towards an experience-based learning
approach for project management
The pedagogical field, the art and the science of
teaching present diverse education models. Such
models were developed for specific contexts, ages,
perception and memory styles. They describe the

110

main interactions among the instructor, students,
supporting materials and teaching resources. In order
to analyze which education model would best fit
project
management
characteristics
and
requirements, we address some pedagogical models
deficiencies.
Most current education models are content-driven
and instructor-centric: a teacher is responsible for
selecting what students will learn, when and how the
learning process will be conducted. These models
are usually supported by expositive classes,
seminars, textbooks and tests [20]. Some studies
have shown that this approach is not adequate for
adult learning [13], since adults prefer to learn based
on experience. They must be motivated and learn
better what they can apply to solve their current
problems. Management education is strictly adult
learning and motivation is a strong factor for the
educational success. Experiencing faulty projects
due to insufficient or inadequate management, for
example, is one way of increasing adult’s motivation
and engagement to an artificial learning situation [8].
In addition, large-scale software projects have
complex nature. This nature is characterized by
dynamic complexity in feedback loops, non-linear
behavior, cause-and-effect relationships distant in
time and social influences [24]. Complex project
behavior usually cannot be estimated with precision
by human perception, intuition and mental models.
On the other hand, by living situations that require
specific decisions and analyzing the behavior
derived from choices previously made, the student
builds a library of behavioral patterns relating
decisions and their consequences on software
projects. Later, this knowledge will be the basis of
students’ managerial decisions. While analyzing a
decision, a manager usually seeks in his memory for
a similar situation in other projects or uses his
perception to capture current reality and mentally
predict its future state according to the available
alternatives [7].
Since software project behavior is not easily
derived from basic principles (the content to be
learned), content-driven educational approaches
should be complemented with techniques that exploit
learning-by-doing. Pilot-projects and simulation are
good examples. The use of simulations benefits from
reduced schedule, budget and risks constraints
inherent to real and even pilot projects. Moreover, a
simulation model can be configured to represent
distinct development situations that could only
happen in real large projects, with long schedules
and large teams. In this case, different models can be
quickly executed and analyzed by simulation with a
variety of educational, process evaluation or
improvement goals.
Thus, learning-by-doing is a strong requirement
for adult education models and a key issue for a

better project management educational approach.
However, a simulation is not necessarily a learning
environment. There is still a lack of foundation in
cognitive learning theory and a coupling of that
theoretical perspective with the practice of
instructional design [22].
The efficiency of simulation use in education is
yet to be well documented and established [11][22].
When conducting research on the effectiveness of
business simulators, issues beyond the simulator
should be considered, including the user
characteristics and the learning situation. There are
actually many subjective and intervenient variables
that can lead to biased empirical results, especially
when comparing different approaches such as books,
teachers and simulators (e.g., teacher’s quality,
book’s content). Similarities of approaches and
complementarities among them should be exploited,
too [12].
Business simulators (also known as ‘business
flight simulators’) are tools for an improved
experience-based decision support. There are many
examples of those flight-simulators in the industry
and technical literature [6][12][16][18][19][26].
Usually,
those
simulators
present
similar
characteristics: a business process model, a control
panel to initial conditions setup and key management
decisions (using input boxes, knobs, dials, etc), text
reports and graphs. Some empirical studies [16][18]
have addressed the use of simulation and modeling
in an educational context. These studies indicate that
a simulation model approach with role-play
scenarios is considered to be useful, providing
understanding about software projects behavior in
project management learning.
Two empirical studies were recently conducted
[2] to evaluate the use of simulation to support
decision-making on software project management.
Such studies illustrate some simulation drawbacks
for educational goals. Students from two different
universities (3 D.Sc. students, 26 M.Sc. students, 16
B.Sc. students and 4 B.Sc.) were asked to manage a
small project with a major objective of concluding it
in the lowest schedule as possible, while attending to
certain quality restrictions.
A project emulator (that is, a software that
dictates project behavior over time) was used to
represent the proposed project. The participants
interacted with the emulator, making decisions about
which developers should take part in the project
team, how many hours should each developer work
per day, if inspections should be included in the
development process, and which developer should
accomplish each project activity. Half of the
participants managed the project based on their own
experience. The remaining participants used
simulations to analyze their options and evaluate

111

their decisions before applying them in the project
emulator.
The results of these studies showed positive
correlation between subject experience, ability to
interpret simulation results and success in attending
to project objectives. This was an unexpected result
because modeling and simulation were supposed to
provide more help to inexperienced managers. It was
also observed that the lack of engagement had
negative influence on subjects’ performance.
In order to have an engaged learning-by-doing
experience, the “plan the flight and fly the plan”
approach usually found in business flight-simulators
may not be sufficient. A more active student
participation and control during the simulation
should be addressed instead of just setting up initial
model conditions and passively watching simulation
results. In an artificial learning situation, it is usually
difficult for a model analyst to trace model observed
results to intermediate behavior. A graphical
feedback of simulation results can perform better for
project behavior understanding and cause-effect
mapping, especially to novice managers. Simulation
tools lack the look-and-feel of a real project
development environment. To make modeling and
simulation more useful for inexperienced managers,
we shall look for better ways to present simulation
results.
A hands-on adult learning model for project
management requires an environment where students
can act as managers. In this sense, games can be
integrated to simulation models, adding fun,
challenge, visual effects and a more compelling
interaction model for students. Interaction is one of
the fundamental game characteristics for an active
learning construction and to avoid player's boredom
in the activity. With an active and flexible
participation, the player tastes the control of the
game, exploring its contents and seeking defined
goals, following his own way to overcome the
challenges. Usability, fidelity, multimedia feedback
and drama effects are also important issues to
provide the role-play experience to the player, acting
as a manager in a virtual software development
office.
Although playing is still considered to be the
opposite of working for many people and
organizations, digital games are increasingly being
adopted in serious-minded applications for
experience-based therapy, communication and
education [25]. The computer and video games
software sales in the United States grew four percent
in 2004 to U$ 7.3 billion [9]. Digital games are a
growing market to children and also to adults: the
average age of a game player is 36 years [9] while
the average task-force age is 39 years [20]. Thus,
soon all the task-force will consist of people who
have always lived in a digital world with strong

participation of games. According to the
Entertainment Software Association, more than half
of game players expect to be playing as much, or
more, as they do today 10 years from now [9].
The game-based educational approach can have
multiple targets such as individual and
organizational learning, prevision and decisionmaking
support,
process
evaluation
and
improvement. Similar approaches for software
engineering education can be found in the technical
literature, such as the SimSE tool [20], the SESAM
project [7] and for military training, such as the Full
Spectrum Command [15].
Finally, an education game approach requires
some investment in order to be built and prepared for
application in classes. In order to enhance the results
of such investment, a game machine should be
reusable across several distinct learning experiences.
In this work, we emphasize some important
requirements for a reusable education simulationbased game: simulation model flexibility through
reusable knowledge pieces, graphical user
interaction and feedback through multimedia
features. By introducing the model as a separated
component using pluggable pieces of knowledge,
different educational goals and situations can be
performed by trainees reusing the same game
machine and simulator.

3. Simulation models for an education
game machine
To evaluate the game-based learning approach,
we developed a simulation-based game, called The
Incredible Manager1. The game machine allows a
trainee to act as a project manager, being responsible
for planning, executing and controlling a software
project. The trainee’s goal is to complete a proposed
project within the cost and schedule established
during a planning phase and approved by the
project’s stakeholders. While planning and
controlling the software project, the trainee has to
make several decisions, including team selection,
allocation, effort dedicated to quality assurance,
budget and schedule control, among others.
Virtual characters, whose behavior is controlled
by a simulation model, represent project
stakeholders. They inspect the trainee’s project plan,
comparing it to a baseline plan based on the average
cost and effort required to build the project. The
stakeholders must approve the trainee’s plan in order
to authorize project execution. Such approval allows
the trainee to review a plan requiring much less
resources than should be expected to complete the
1

The game and other System Dynamics resources are available at
<http://reuse.cos.ufrj.br/riosim/>

112

project, but it also inhibits the trainee to prepare a
plan requiring much more resources than needed to
develop the project.
After having an approved project plan, the trainee
can start the project development. The development
runs in continuous turns, consuming project
resources requested by the project plan. The player
must be aware of the project behavior and take
corrective actions when necessary. Visual effects
and reports show the game characters’ state, such as
exhausted developers, late tasks, project without
funds, and so on. To avoid finishing the resources
before project completion, the player may need to
modify the original plan on the fly. According to
these decisions, different players can live the
experience of managing the same simulation model
of a project in different ways.
The simulation model describes the world and
relevant aspects of the software project presented to
the trainee. However, software development projects
are difficult to model since they are classified as
systems of complex dynamics [24]. Addressing these
difficulties, System Dynamics [10] is a modeling
discipline based on a holistic view to describe and
evaluate the visible behavior presented by a system.
Such behavior is determined by the structure of
elements that participate in the system and the
relationships among them, described in the model
through mathematical equations.
This modeling discipline has already been used in
the development of software project models [1],
which became a base for subsequent reviews and
extensions by other authors. One of these extensions
is the scenario-based project management paradigm
[3][4], which separates uncertain aspects from
known facts in project models. This separation
occurs by building distinct models (namely scenario
models) for each uncertain aspect that can influence
a software project, while the project itself is
described in a baseline model (or domain model) to
which the separate models will be integrated. Since
project behavior can be affected by unexpected
events, management actions and strategies, the
student may test its sensibility to combinations of
such elements, evaluating their impact over the
project success.
A proposed System Dynamics metamodel [4]
encapsulates the complexity of the traditional
constructors in an object-oriented fashion. Models
can be easily developed, modified, integrated and
expanded to embrace management knowledge from
the technical literature and practice. The metamodel
is the basis for building domain and scenario models.
A domain model defines categories of elements that
take part in the problem domain represented in the
model, their properties, behavior and relationships.
The domain model does not describe a model for a
specific problem, but for a knowledge area where

modeling can be applied. It is a domain description
that must be specialized to describe a particular
problem. Thus, from an abstract domain model it is
possible to create several operational project models.
Scenario models provide a library of generic
management events and theories that an instructor
can integrate to a project model and present to
management trainees during a simulation session.
Senior managers can develop scenario models
expressing experiences they have collected by
participating in several projects. These scenarios will
further allow less experienced managers to share
senior managers’ knowledge. We have a library of
about fifteen scenario models available for the
simulator. These scenarios include theories regarding
developers’ productivity and error generation rates
due to their experience, developers’ productivity due
to learning the application domain, effects of
overworking and exhaustion upon developers,
communication overhead, error propagation along
the activities that compose a project network, among
others.
DOMAIN +
SCENARIO
MODELS

SIMULATION
MACHINE

PLAYER

GAME
MACHINE

Figure 1. Game components and
interactions
Main game components and their interactions are
shown in Figure 1. The simulation model (composed
by a domain model and selected scenario models) is
an independent component that can be easily
exchanged according to the educational goals or
management knowledge desired. In fact, each
simulation model represents a game phase
(externally set in a configuration file). Phases are
independent files that can be used to create multiple
educational projects with increasing complexity or
peculiarities.
The high-level models (domain and scenarios)
can then be translated to traditional System
Dynamics constructors for computational simulation.
The System Dynamics metamodel translation and
simulation process, however, must present especial
features considering the requirements for a gamebased simulation. These features are presented and
discussed in the next section.
The game machine differs from a traditional
simulator display by presenting model behavior and
its changes over time in a multimedia format that
tends to be easy to understand by the students.
Moreover, their interaction with the simulation

113

process is made through an interface closer to the
real-world situation of managing a software project:
instead of handling with mathematical equations, the
trainee deals with iconographic representations of
developers, activities, and other elements that are
usually present in a real software project. As shown
in the qualitative evaluations that we conducted, this
enforces motivation and, therefore, enhances the
learning process.
Figure 2 shows the game office environment.
Color changes indicate the project network (in the
top right), schedule and funds (in the bottom left)
current states. Developers’ states are shown through
balloons and body expressions as they are idle, tired
or in panic. The player can pause the game, ask to
modify the project plan and visualize selected
developers and task details in the message area (in
the bottom right).

structural relationship; since the manager can change
the developer in charge of an activity (for many
reasons, from the availability of a more capable
developer to a request of the developer itself), the
model structure should be able to change during the
game session, without loss of previous simulated
values during the previous simulation steps.
To address these limitations, this work presents a
dynamic-structure compiler and simulator for models
based on the System Dynamics metamodel [3]. The
simulator starts by translating the project and
scenario models built according to the System
Dynamics metamodel to traditional System
Dynamics constructors (this process is handled by
the original metamodel compiler) [4][5]. Once the
models are translated to mathematical equations,
simulation steps are executed continuously until the
player performs an action. Upon receiving a player
action, the simulator performs the required structural
changes in the high-level project model
representation and translates the new model to the
traditional equations. A player action may trigger the
inclusion of new elements to the project model,
removal of existing elements, or modifications upon
the values of element properties. Once the structural
changes are applied and the new equations-based
model is generated, model simulation is restarted in
the next simulation steps with the new set of
equations, however without affecting the simulation
steps already accomplished and the values calculated
in these steps.

5. Qualitative evaluation

Figure 2 - The game office

To evaluate the game-based approach for project
management education, 24 people from 3 different
groups were invited to run The Incredible Manager
with a simulation model representing a software
project. The first group was composed by 7
participants from a project management graduation
class. The second was composed by 8 participants
from an industrial software development laboratory,
while the third was composed by 9 participants
(organized in 3 groups) from an undergraduate class.
A total of 11 B.Sc. students and 13 M.Sc./D.Sc.
students ran the game.
While performing the study, participants were
asked to play the role of a trainee using our game.
The training session included a simulation (running
the game) and a discussion session (presenting the
lessons learned). All participants were asked to fill in
two questionnaires. The first was filled in before the
training section and contained questions about
academic degree, personal experience and interest in
software development and project management. The
second questionnaire, filled in at the end of the
training session, contained qualitative questions

4. Dynamic-structure simulation
Most traditional System Dynamics simulators
provide a static-structure simulation: although the
behavior of domain elements changes over time,
their structure, that is, the relationships among these
elements, remain fixed from the first to the last
simulation step. Usually, model properties are
initially configured by an operator and the model
behavior is evaluated after all simulation steps are
executed.
This type of simulation is not well suited for
game-base simulations, where user interaction is a
fundamental characteristic. Such interactions
describe player actions to control the project. Like in
the real world, the manager should be able to hire
new team mates, ask them to work overtime due to
schedule pressure or increase the effort on design
inspections to not impact the product quality. In a
project management game, these user decisions may
require changing the way in which domain elements
relate to each other. For instance, we consider that
the developer selected to accomplish an activity is a

114

about the game-based educational approach and the
game itself.
The results of the qualitative questions for the
three different evaluations were very similar: 100%
approved the game-based model, felt that they
learned the lessons presented, and that they increased
their management skills. For 52.2% of the
participants, the training session was considered very
pleasant. Finally, 87.5% of the participants described
that the game experience raised their interest in
project management.
The remaining questionnaire results indicate that
the game-based simulation learning is motivating,
practical and fun for the participants. Challenge,
visual effects and time pressure are also viewed as
important factors for the engagement and
entertainment during the activity. The participants,
especially the novices, pointed out that the graphical
feedback and the possibility of practical simulation
of real project situations were very stimulating. The
diversity of choices and the uncertainty from the
model give a non-linear experience running the
game in different situations, exploring several
educational goals.
The most reported improvement points are related
to the simulation model evolution, including new
project development processes, psychological, social
and organizational aspects of uncertainty. Multiplayer support with different roles, interactions and
an intelligent integrated coach during game
execution are requested as future works.
The positive results achieved in the evaluation,
though, cannot support the effectiveness of the
game-based project management education. The
focus of this evaluation was mainly to gain
qualitative insights and future work directions. As
stated before, the design and execution of
experimental and quantitative studies using
simulations in education is a complex task due to
several intervenient variables to consider when
isolating and measuring the effectiveness of such
approach.

the motivation and engagement necessary for a better
artificial management training situation.
Concerning future works over the proposed
models for game-based simulation, there is a special
demand for graphical tools for model construction
and evolution. Model refinements and new scenarios
should include more realistic situations of software
development offices, projects and developers. Since
there must be a quantitative basis to these model
evolutions, new empirical studies should be
performed to gather these data.
Future research on software engineering gamebased learning works should involve multiple
disciplines, such as pedagogical issues over the
training environment and process, cognitive and
motivational issues, including usability, interaction,
perception and multimedia presentation of the game.

Acknowledgments
The authors would like to thank all the participants
involved in the studies and CAPES for the financial
investment in this work.

References
[1] Abdel-Hamid, T.K., Madnick, S.E., Software Project
Dynamics - An Integrated Approach, Prentice-Hall,
Englewood Cliffs, 1991.
[2] Barros, M.O., Werner, C.M.L., Travassos, G.H.,
“Supporting Risk Analysis on Software Projects”. The
Journal of Systems and Software, v. 70, n. 1-2, 2004, pp.
21-35.
[3] Barros, M. O.; Werner, C.M.L.; Travassos, G. H.,
“System Dynamics Extension Modules for Software
Process Modeling”. In: International Workshop on
Software Process Simulation and Modeling, Portland,
2003.
[4] Barros, M.O., Werner, C.M.L., Travassos, G.H., "A
system dynamics metamodel for software process
modeling". Software Process Improvement and Practice,
v.7, n.3-4, 2002.

6. Conclusions

[5] Barros M.O., “Hector – System Dynamics
Metamodel
Compiler”.
Available
at
<http://sety.cos.ufrj.br/riosim/tools/Hector/Hector.php>,
March 25, 2005.

Simulation-based games seem well suited to be
introduced in a learning-by-doing education model,
such as required by manager trainees. They give the
students an opportunity to experiment the
consequences of executing or neglecting important
project management functions, confront themselves
with complex issues that must be resolved during
project development, and test different approaches
and solutions to project management problems. The
lessons learned from the decision-making and the
cause-effect discussions over their own projects
become an important factor to improve and maintain

[6] Beer
Game
Resources,
Available
<http://web.mit.edu/jsterman/www/> March 25, 2005.

at

[7] Doyle, J.K., Ford, D.N., Radzicki, M.J., Tress, W.S.,
“Mental Models of Dynamic Systems”. Available at
<http://www.wpi.edu/Academics/Depts/SSPS/Research/Pa
pers/27.pdf>, March 25, 2005.
[8] Drappa, A., Ludewig, J., “Simulation in Software
Engineering Training”. In: Proceedings of the

115

Engineering,

Conclusions”. In: Proceedings of the European Software
Measurement Conference, Antwerp, 1998, pp. 305-313.

[9] Entertainment Software Association. Available at
http://www.theesa.com/facts/top_10_facts.php, March 25,
2005.

[18] Pfahl, D., Laitenberger, O., Dorsch, J., Ruhe, G., “An
Externally Replicated Experiment for Evaluating the
Learning Effectiveness of Using Simulations in Software
Project Management Education”. Empirical Software
Engineering, 8, 2003, pp. 67–395.

International Conference on
Limerick, 2000, pp. 199-208.

Software

[10] Forrester, J.W., Industrial Dynamics, Cambridge,
MA: The MIT Press, 1961.

[19] POWERSIM
Resources,
Available
<http://www.powersim.com/> March 25, 2005.

[11] Größler, A.,
“Methodological Issues of Using
Business Simulators in Teaching and Research”. In:
Proceedings of the Conference of the International System
Dynamics Society, Bergen, Norway, 2000.

[20] Prensky, M.,
McGraw-Hill, 2001.

[23] Standish Group, “The Chaos Chronicles”, The
Standish Group International, 2003.

[14] Knowles, M., Andragogy in Action, Jossey-Bass, San
Francisco, CA, 1984.
Redux.

Learning,

[22] Spector, J.M., Davidsen, P.I., “Constructing Learning
Environments Using System Dynamics”. Journal of
Courseware Engineering, v. 1, 1998, pp. 5-11.

[13] Klein, G., Sources of Power: How People Make
Decisions, Cambridge, MA: MIT Press, 1998.

Game

Game-Based

[21] SimSE
Online
Resources,
Available
at
http://www.ics.uci.edu/~emilyo/SimSE/, March 25, 2005.

[12] Größler, A., Notzon, I., Shehzad, A., “Constructing
an Interactive Learning Environment (ILE) to Conduct
Evaluation Experiments”. In: Proceedings of the
Conference of the International System Dynamics Society,
Wellington, New Zeland, 1999.

[15] Macedonia, M. Ender’s
Computer, v.2, 2005, pp. 95-97.

Digital

at

[24] Sterman, J.D., System Dynamics Modeling for Project
Management, MIT System Dynamics Group, Cambridge,
MA, USA, 1992.

IEEE

[25] Swartout, W., van Lent, M., “Making a Game of
System Design”. Communications of the ACM, v.46, n. 7,
2003, pp. 32-39.

[16] Maier, F.H, Strohhecker, J., “Do Management Flight
Simulators Really Enhance Decision Effectiveness”. In:
Proceedings of the Conference of the International System
Dynamics Society, Cambridge, 1996.

[26] VENSIM
Resources,
Available
<http://www.vensim.com/>, March 25, 2005.

[17] Mandl-Striegnitz, P.; Lichter, H., “A Case Study on
Project Management in Industry: Experiences and

116

at

Towards Interactive Systems Usability Improvement through Simulation
Modeling
Nuria Hurtado1, Mercedes Ruiz1, Jesús Torres2
Department of Computer Languages and Systems
University of Cadiz (Spain)
{nuria.hurtado, mercedes.ruiz}@uca.es
2
Department of Computer Languages and Systems
University of Seville (Spain)
jtorres@lsi.us.es

1

Thus, the software industry should realize that they
need to pay attention to usability from the early stages
of system development with the introduction of a User
Centered Design (UCD) approach.
Along these lines, different proposals have been
made, coming from both the Usability Engineering
(UE) and the Software Engineering (SE) fields, for the
setting out of methods, techniques and tools with the
aim of orienting developers as to which activities
should be carried out during the software development
process that may grant a previously established
usability level [2][5][6][7][11][16].
However, in spite of the social and economic
benefits that usability allows and yet despite strong
motivation within some organizations to practice and
apply effective SE and UE methods, there still exist
major gaps of understanding both between suggested
practice, and how software is actually developed in
industry, and between the best practices of each of the
fields. The existing UE methods are integrated in
development practices in a way that is more
opportunistic than systematic. As a result, product
quality is not as high as it could be, and rework is often
necessary [9].
Modeling and simulation techniques are considered
as valuable tools for the improvement of processes in
several areas of engineering. Since the early 90s
various simulation models have been developed to
respond to different questions related to the software
development process proving their usefulness in this
scope [13].
This paper presents an approach to the application
of modeling and simulation techniques to the User
Centered Design (UCD) process and usability
improvement. More precisely, it proposes the use of
dynamic simulation models for the improvement of

Abstract
Nowadays, usability has become an essential
contribution to the success of interactive systems and
is recognized as a quality attribute for software
products. This paper proposes the use of dynamic
simulation models for the improvement of interactive
systems usability through the application of a User
Centered Design (UCD) process and its integration
into the software development process. The simulation
model developed is used to experiment on the effect
that different levels of usability have over the behavior
of the UCD process in a specific kind of interactive
systems such as web site application development.

1. Introduction
Over the last few years, there has been an increase
in the amount of people using and depending on
computer technology. At the same time, due to the
growth and expansion of the internet, software systems
have increased their interaction degree. This implies an
ever-growing demand of more usable products.
For a long time, the importance of usability has
been neglected in the development of interactive
systems, and so it has been relegated to nothing else
than final product evaluation activities. It is important
to bear in mind that system usability is not only related
to user interface appearance but, mainly, to the way in
which the user can interact with the system and, hence,
to the overall structure of the system and the logic of
the business.
Usability increases customer satisfaction and
productivity, leads to customer trust and inevitably
results in tangible cost, savings and profitability [15].

117

interactive systems usability through the application of
a UCD process and its integration into the software
development process [10].
The proposed approach is intended to help
developers understand and improve the behavior of the
UCD process and its special features, reinforcing
motivation for a change in the development process of
organizations, and helping to bridge the existing gaps
between SE and UE.
For the purpose of this study, the simulation model
is used to determine the effect that different levels of
usability have over the UCD process behavior of a
specific kind of interactive systems development such
as web site design.
The structure of the paper is as follows. Section 2,
presents the concepts of usability and UCD in order to
set the scope of our study and we comment on the
process model that is eventually chosen to build a
simulation model. Section 3, presents a brief account
of the advantages of simulation models of software
processes that support the usefulness of the application
of these techniques to the UCD process. Section 4,
introduces the model development, as well as the
chosen simulation approach, a description of the model
and parts of it, the definition of scenarios for the
simulation and some simulation results. Section 5,
includes the main conclusions of this proposal and
future work to be carried out along these lines.

effectiveness, efficiency and satisfaction in a specified
context of use” [12].
It is necessary to point out that usability depends
strictly on the context of use, that is, on specific users
and work environment, and hence it is a quality not
inherent to software. Hence, it is deduced that in order
to develop a usable product it is not enough to
systematically apply any general instructions or
usability guidelines, but it is necessary to apply a UCD
process that allows for the integration of the user into
the development from the early stages of it, thus
permitting an extensive knowledge of the context of
use.
User Centred Design is an approach to interactive
systems design that specifically aims at making
systems more usable through the incorporation of the
user to the development process.
Amongst the benefits of the application of UCD
processes the ISO 13407:1999 [11] includes:
− Cost production reduction. Cost and development
time can be reduced, avoiding redesign and
reducing the number of later changes on the
product.
− Increase of user productivity and operational
efficiency of organizations.
− Improvement of the quality of the product and of
its appeal to users, resulting in a competitive
advantage.
− Making systems that are easier to use and learn,
thus reducing the cost of technical service,
training and maintenance.
− Increase of user satisfaction, which reduces
trouble and stress.

2. Usability and User Centered Design
The term usability is defined in norm ISO 9241-11
as “the degree to which a product may be used by any
given users to attain specific objectives with

Planning the Human
Centred Process

Undestanding and
Specfying the context
of use

Evaluating designs
against requirements

The system satisfies the
specified requirements

Specifying user and
organisation
requirements

Producing design
solutions

Figure 1. Interdependence of User Centred Design activities [11]

118

− Producing design solutions. Specific design
solutions must be carried out using some kind of
prototyping. Such prototypes are presented to users
and feedback is used to make design modifications.
− Evaluating design with respect to requirements.
Evaluation must be present at all stages of the life
cycle, with the intention of providing a feedback
that contributes to design improvement. It will also
determine whether the specified objectives have
been attained, and it will check the use of the
product in the long term.

2.1. Model of User Centered Process
As it has been already mentioned, there are different
methodological suggestions, coming from various
disciplines (UE and SE) for the development of
interactive systems based upon the user centered
approach. All these proposals aim at guiding
developers in proceeding in an organized way in order
to attain the usability of an interactive system during
its development, although how the integration of HCI
(Human-Computer Interaction) proposals into SE
Process Models should be carried out, is still under
research. The present work is centered in the process
model developed in the international standard ISO
13407:1999 [11] since it is considered to be the basic
reference in the development of user centered
processes by the HCI community. It is not linked to
any existent methods, and it complements any design
methods and lays down a user centered general
perspective that may be integrated into various
development processes according to each particular
context. All design activities introduced are applicable,
to a greater or lesser extent, to each of the system
development stages, although -previous to its
application- a user centered planning of the process
must be set up. Such planning must include, among
other things, the procedure for the integration of these
activities into the rest of the system development
activities (for example analysis, design and
evaluation). Such procedure will depend in each case
on the project in particular but it should always allow
for iteration. Nevertheless, the standard does not
specify how such integration must be done. Figure 1
shows the various activities of the UCD process and
interdependence among them.
The process describes four main design activities
centred on the user: understanding and specifying the
context of use, specifying user and organization
requirements, producing design solutions and
evaluating design against requirements. The process
implies the iteration of these activities until the system
satisfies the specified requirements. A brief
explanation of each activity follows:
− Understanding and specifying the context of use.
Identification should be made of the features of
potential users, the tasks they are going to perform
and the environment in which the system is going
to be used.
− Specifying user and organization requirements with
respect to use context description. Objectives must
be set identifying compromises and priorities
among the various requirements.

3. Modeling and Simulation for Software
Process Improvement
Simulation can help when it comes to make
decisions about questions related to process
improvement, because it helps predicting the effect
that a change would have in the process before it takes
place.
In this scope, the dynamic model introduced in [14]
is of great importance, being – along with AbdelHamid´s original model [1] - one of the dynamic
models that represents with greater detail the whole
software development process. In [14], a model for
showing the effect of making formal inspections on
cost, deadline and quality of projects is introduced.
Also, the use of simulation models to predict,
quantitatively, the impact of changes upon processes is
proposed in [20].
Most recent simulation models are especially
designed and oriented towards the evaluation of the
results of different measures for process improvement.
Various models have been developed in the scope of
the CMM model (Capability Maturity Model) among
which, the models proposed in [21] and [22] are worth
pointing out. [21] shows the application of a model to
predict software process perform in terms of effort,
staff, deadline and quality of the product. A Dynamic
Integrated Framework for Software Process
Improvement (DIFSPI) is developed in [22]. It offers a
methodology and a working environment that
combines both the advantages of traditional methods
and those of systems dynamics, thus allowing project
managers as well as members of the Software
Engineering Improvement Group (SEIG) to design and
evaluate new software process improvements.
[10] presents an initial approach to the application
of modeling and simulation techniques to the UCD
process.

119

4. UCD Process Modeling and Simulation
4.1. Simulation approach

−

There are several simulation model approaches
applicable to the study of the various aspects of the
software process. Among them, two main approaches
are worth pointing out: Continuous modeling and
discrete modeling.
The continuous simulation approach is based upon
the Systems Dynamics theory. It is useful when
systems contain variables that change in a continuous
manner with time. Continuous modeling of a process
represents the interaction among its key factors as an
ensemble of differential equations where time is
increased step by step.
The discrete simulation approach is based upon
queue systems. In the discrete simulation, time
advances when a discrete event takes place.
Since the purpose of this study is to model UCD
process mechanisms, we have chosen the continuous
simulation approach.

stages. These stages could correspond to the
analysis, design and evaluation phases of a classic
development life cycle.
Usability Level: This parameter determines
the level of usability of the project. The parameter
can take three different values, low level, medium
level and high level.
Web Proje ct Size

Web Application
Effort

Global Usability
Size

Usability tasks
Global Usability
Effort

Usability Effort

Usability size

Usability level
life cycle phas e

Final Usability
Effort

4.2. Model Development

Figure 2. Usability effort and usability size estimations

4.2. 1. Introduction to model development
The main aim of the developed model is to help
understand and improve the UCD process and its
integration into the overall software development
process, resulting in the improvement of system
usability. The process model established in ISO
13407:1999 [11], has been chosen to model and
simulate the UCD process. The computation of the
amount of tasks to be developed in each activity of the
UCD process, as well as the effort to be allocated to
each of them, have been adapted to the special scenario
of a web site design project [3].

−

The variables involved are the following:
Web
application
effort measured in
person_month has been estimated using the
estimation model called WEBMO [18][19]. This
model estimates the effort and duration of web
applications development projects as an adaptation
of the COCOMO II early design model [4]. The
effort equation used is the following:
9

Web applicatio n effort = A ∏ cd i (Web project size ) P1
i =1

4.2.2. Estimations of Usability Effort and Usability
Size
For the estimation of Usability Effort and Usability
Size, several input parameters and auxiliary variables
have been considered. Figure 2 shows the diagram for
the computation of these variables.

−

The input parameters involved are the following:
Web Project Size: measured in thousands of
Source Lines of Code (SLOC)
− Life Cycle Phase: This parameter determines
the development stage of the project that is going
to be simulated, namely, early, central or late

−

120

The constant A and the values for the power law
p1 will depend on which of the five application
domains is considered. The application domain
considered in this model is the one that
corresponds to the web portals domain. The
equation has also nine cost drivers cd, which have
been set to their nominal values in this case study.
Global usability effort: This value is
obtained using Web application effort
according to the conclusions of the research
carried out by Nielsen Norman Group in their
study of the best usability practices in web
development [17]. For the purpose of this model,
the obtained value has been considered as the
nominal value corresponding to a medium

−
−
−

usability level (level 2). The value will be
increased or decreased by a percentage law for the
usability levels: high (level 3) and low (level 1),
respectively.
Global
usability
size: measured in
thousands of SLOCs.
Usability tasks: measured in tasks.
Usability effort: Global effort for the
usability tasks.

The percentage of Usability Size variable that
it is necessary to perform on each activity will depend
on the Life cycle phase input parameter.
According to such parameter the initial values for the
various UCD activities will vary. These initial values
are represented by variables:
−
−
−

−

Finally, the values corresponding to the Usability
size and the Final usability effort variables
are obtained depending on the Usability level and
the Life
cycle
phase input parameters,

size of context of use.
Requirements size.
Design size.
Evaluation size.

In order to control the sequence for the activation of
each activity the model is based on the following
pattern: When a certain percentage of tasks is
completed on a particular activity, it will be possible to
start the next activity. This percentage is established
through a series of input parameters. Each of these
input parameters act upon the following auxiliary
variables that control the start of activities:

conforming to the proposed scenario.
4.2.3. UCD Process Modeling
Figure 3 shows a simplified flow and level diagram
of the developed model. Each of the activities of the
UCD process described in [11] has been represented as
a level variable. Level variables represent the number
of tasks that are performed on each of UCD activities,
namely:
−
−
−
−

Initial
Initial
Initial
Initial

−
−
−
−

Necessary
Necessary
Necessary
Necessary

context of use.
user´s requirements.
solutions.
evaluation.

Each activity gets started by the activation of the
corresponding flows. The flow is activated when the
number of performed tasks satisfies the percentage of
tasks established as necessary to be able to go on to the
next activity.

Specified context of use.
Specified user´s requirements.
Designed solutions.
Evaluated design solutions.

Usability level
life cycle phase

Usability s ize

Initial size of
context of use

Finished c ontext of
use specification

Specification rate of
context of use

context of use
effort

Initial
requirements size

Necessary c ontext
of use
Finished requirements
spec ification

Specified
context of use

Specification
requirements rate

Necessary user´s
requirements

Specified us er´s
requirements

Final Usability
Effort

Necessary
solutions
Finished solution
design

Design rate

Designed
solutions

desing effort

user specification
effort

Initial evaluation
size

Initial Design size

Evaluation rate

evaluation effort

Revision rate
revision effort

revision size

<Usability level>
<life c ycle phase>

Figure 3. Simplified flow and level diagram

121

Finished solution
evaluation

Evaluated
design
solutions

Initial
Phases

Initial
Phases

Central
Phases

Final
Phases

Final
Phases

Level 1

Initial
Phases

Final
Phases

Central
Phases

Level 2

Central
Phases

Level 3

Figure 4. Usability Tasks Global Distribution in the life cycle according to Usability Level

The flow of work, flows applying a development
rate between one activity and the next one. The
development rate will depend on the productivity and
dedication of the staff assigned to each one of the UCD
activities as well as on the effort corresponding to each
activity. Flow variables are as follows:
−
−
−
−

−

4.3. Model Simulation
4.3.1. Scenarios definition
According to ISO standard 13407:1999 [11], before
applying the UCD process it is necessary to plan it out,
in order to specify how user centred activities fit in the
overall development process.
To simulate the model three main scenarios have
been considered. These scenarios will be mainly driven
by the three usability levels considered. The
Usability level and Life cycle phase input
parameters will determine the values of usability
size and final usability effort variables. At
the same time, these variables will determine the initial
distribution of usability tasks to be performed as well
as the effort to be invested in them, setting the initial
situation of the scenarios. The distribution of usability
tasks in the life cycle, which reflects the assumed
scenario for the simulation is detailed in figure 4.

Specification rate of context of use.
Specification requirements rate.
Design rate.
Evaluation rate.
Revision rate: This rate will be affected by the
revision size variable that will agree with the

percentage of evaluated tasks that need to be reelaborated and will depend on Usability
level
and Life cycle phase input
parameters.
The final usability effort is distributed into each
activity resulting in the following effort variables:
−
−
−
−
−

Context of use effort.
Specification requirements effort.
Design effort.
Evaluation effort.
Revision effort

Level 3- Initial Phase
2

1

0

0

1

2

3

4
5
6
Time (Month)

7

8

9

Specified context of use : 3_1
Specified user´s requirements : 3_1
Designed solutions : 3_1
Evaluated design solutions : 3_1

10
tasks
tasks
tasks
tasks

Figure 5. Results for the Initial phase and Level 3 of Usability

122

Level 3- Central Phase
4

2

0

0

1

2

3

4
5
6
Time (Month)

7

8

9

Specified context of use : 3_2
Specified user´s requirements : 3_2
Designed solutions : 3_2
Evaluated design solutions : 3_2

10
tasks
tasks
tasks
tasks

Figure 6. Results for the Central phase and Level 3 of Usability

Usability level 1 would correspond to the situation
in which the usability methods and techniques are not
correctly applied from the early stages of development,
putting most of them off to the final stages. The initial
scenario given by level 3 corresponds to an ideal
situation in which usability activities would be taken
into account through the whole life cycle of the web
site project. Level 2 scenario defines an intermediate
situation.
Once the Usability Size variable has been
initialized, it is distributed -depending on the life cycle
phase- into each of the variables corresponding to the
number of UCD tasks that must be carried out in each
process activity. Values are allocated according to
those usability tasks that it is necessary to carry out in

each activity during the application of the UCD
process to the web design [2][3][8].
The revision size variable represents the
percentage of evaluated tasks that need to be reelaborated. This percentage increase as the level of
usability decreases and the life cycle phase increases,
since the number of errors encountered during
evaluation is notably increased when usability is not
taken into account since the early stages of
development. [8][9].
Finally, the distribution of Usability Effort into each
of the activities is carried out according to initial size
of tasks for each activity, taking also into account
revision tasks.

Level 3- Final Phase
4

2

0

0

1

2

3

4
5
6
Time (Month)

7

8

9

Specified context of use : 3_3
Specified user´s requirements : 3_3
Designed solutions : 3_3
Evaluated design solutions : 3_3

10
tasks
tasks
tasks
tasks

Figure 7. Results for the Final phase and Level 3 of Usability

123

Level 1- Final Phase
4

2

0
0

1

2

3

4
5
6
Time (Month)

7

8

9

Specified context of use : 1_3
Specified user´s requirements : 1_3
Designed solutions : 1_3
Evaluated design solutions : 1_3

10
tasks
tasks
tasks
tasks

Figure 8. Results for the Final phase and Level 1 of Usability

4.3.2. Simulation Results

5. Conclusions and Future Work

The model has been implemented using the
Vensim® simulation environment. As an example, a
simulation for a project of 11,000 SLOCs has been
performed. Figures 5, 6 and 7 show the behavior of
UCD activities for level 3 through the complete
development process. The graphics show how the
results reproduce the expected behavior from a
qualitative point of view.
In figure 5 we verify that all UCD activities are
involved in the initial phase. We can see that Use
Context Specification activities as well as User´s
Requirements Specification ones have a greater degree
of importance in this initial phase of the life cycle, in
which web site objectives are also planned and use
scenarios defined. The curve corresponding to Solution
Design represents the consideration of established
guidelines for web writing style, navigation and page
design as well as the design of early prototypes and
mock-ups, which must eventually be evaluated in this
phase for representative end users.
Figures 6 and 7 show how important design and
evaluation activities become when ever more
functional (and thus more complex) prototypes of the
site are developed, their evaluation being consequently
increased in complexity.
Figure 8 shows the behavior of the process for the
final stage and for level 1 of usability. It is interesting
to point out how development time is increased in
comparison with level 3. This is chiefly due to the
increase in the number of revision

This paper presents the results of the application of
simulation modeling to the UCD process. More
precisely, the developed dynamic model helps
visualize the behavior of UCD activities during the
development life cycle of web site portal development.
Furthermore, it provides a tool to experiment the
effects that the variations in the desired usability level
and the estimated initial size have upon the UCD
process evolution and behavior. Managers and
developers could benefit from it to make decisions in
order to improve the final product usability. The
developed model is also useful to experiment with
other types of software development projects.
The present paper contributes to justify the
usefulness that modeling and simulation techniques –
already validated in other software development
paradigms- have to understand and improve the UCD
process, setting a basis for its application in this scope.
Future research is oriented toward a deeper study of
the application of modeling and simulation techniques
to UCD integration into software development, as well
as to the identification of the special features of UCD
processes that help us model the specific aspects of
usability methods and techniques, that affect
interactive system usability both during the
development process and in the evaluations of the final
product once it is implanted.

124

de Diseño Centrado en el usuario”. Proceedings of the 5th
ADIS 2004 Workshop on Decision Support in Software
Engineering, University of Malaga. Published on CEUR-WS,
vol 120. November, 2004. Available at: <http://CEURWS.org/Vol-120/>

Acknowledgements
The
authors
wish
to
acknowledge
the
Interdepartmental Commission for Science and
Technology (Comisión Interministerial de Ciencia y
Tecnología) Spain, for subsidizing this research
through projects TIC 2003-369 and TIC 2001-1143C03-02

[11] ISO 13407:1999. Human-Centred Design Processes for
Interactive Systems. International Standard Organism, 1999.
[12] ISO 9241-11:1998. Ergonomic Requirements for Office
Work with Visual Display Terminals (VDTs) - Part 11:
Guidance on Usability. International Standard Organism,
1998.

6. References
[1] Abdel-Hamid, T., Madnick, S., Software Project
Dynamics: An Integrated Approach. Prentice-Hall,
Englewood Cliffs, NJ. 1991.

[13] Kellner, MI., Madachy, RJ., Raffo, DM., Software
Process Simulation Modeling: Why? What? How? The
Journal of Systems and Software, 46 (2/3). 1999. pp. 91-105.

[2] Bevan, N., UsabilityNet Methods for User Centred
Design. Human-Computer Interaction: Theory and Practice
(volume 1 of the Proceedings of HCI international 2003).
Lawrence Erlbaum Associates. 2003. pp. 434-438.

[14] Madachy, R., “A Software Project Dynamics Model for
Process Cost, Schedule and Risk Assessment”. Ph.D.
Dissertation. University of Southern California, Los Angeles,
CA. 1994.

[3] Bevan, N., “Usability Issues in web site design”.
Proceedings of UPA’98.Washington DC. June, 1998. pp. 2226.

[15] Marcus, A. “Return on Investment for usable UserInterface Design: Examples and Statistics”, Aaron Marcus &
Associates, Inc, California, February, 2002, pp. 1-24.

[4] Boehm, Barry W., Chris Abts, A. Winsor Brown, et al.
Software Cost Estimation with COCOMO II. Prentice Hall,
2000. pp. 51-55.

[16] Mayhew, D., The Usability Engineering Lifecycle,
Morgan Kaufmann, San Francisco, 1999.

[5] Daly-Jones, O., Bevan, N., Thomas, C., “Handbook of
user centred design”. EC Telematics Applications
Programme, Project IE 2016 INUSE, NPL Usability
Services, National Physical Laboratory, Queens Road,
Teddington, Middlesex, TW11 0LW, UK. January, 2001.

[17] Nielsen, J.; Gilutz, S.. “Usability
Investment”. Nielsen Norman Group, 2003.

Return

on

[18] Reifer, D. “Web development: estimating quick-tomarket software”, IEEE Software, Vol. 17, No. 6.. 2000.
pp.57–64

[6] Dix, A., Finlay, J., Abowd, G., Beale R.,. HumanComputer Interaction. Prentice Hall, Englewood Cliffs, NJ
(2nd edition).1998.

[19] Reifer, D. “Estimating Web Development Costs: There
Are Differences”. The Journal of Defense Software
Engineering. June, 2002.

[7] Ferré, X., “Integration of Usability Techniques into the
Software Development Process”. Proceedings of the
Workshop Bridging the Gaps Between Software Engineering
and Human-Computer Interaction. ICSE’03. (International
Conference on Software Engineering). Portland, Oregon.
May, 2003.

[20] Raffo, D., “Modeling Software Processes Quantitatively
and Assessing the Impact of Potencial Process Changes on
Process Performance”. Ph.D. Dissertation. Graduate School
of Industrial Administration, Carnegie Mellon University,
Pittsburg, PA. 1996.

[8] Folmer, E., Bosch, J., Cost Effective Development of
Usable Systems; Gaps between HCI and SE. Proceedings of
the Workshop Bridging the Gaps Between Software
Engineering and Human-Computer Interaction. ICSE’04.
(International Conference on Software Engineering).2004.

[21] Raffo, D., Kellner, MI., Chapter 16. “Modeling
Software Processes Quantitatively and Evaluating the
Performance of Process Alternatives”. En El Eman, K.,
Madhavji, N. (Eds.), Elements of Software Process
Assessment and Improvement. IEEE Computer Society
Press, Los Alamitos, CA. 1999.

[9] Harning, M.B, Vanderdonckt, J., Introduction to the
proceedings of the workshop “Closing the gaps: software
engineering and Human-Computer Interaction”. INTERACT
2003.

[22] Ruiz M., “Modelado y Simulación para la Mejora de los
Procesos Software”. Ph.D. Dissertation. Department of
Computer Languages and Systems. University of Sevilla,
2003. (In Spanish)

[10] Hurtado N., Ruiz M., Torres J., “Aplicación del
Modelado y Simulación de Sistemas Dinámicos al Proceso

125

1

Economic Analysis of Integrated Software
Development and Consulting Companies
C. Noujeim J. Sandrock and C. Weinhardt, Department of Information Management and Systems,
University of Karlsruhe


Abstract— The still growing use of Internet technologies and
corporate software shape the structure of the software industry.
While visions of the Computational Grid or service orientated
architectures promise systems and environments where users
merely access software in a plug-and-play-manner, experience
and empirical results point at the installation and integration
efforts for software rollout and integration projects.
Also economic practice blurs the boundaries between software
and service providers and different models of software vendor
and software integrator relationships have evolved. This paper
therefore addresses strategic issues of joint software vendor/
software integrator companies with the aim to understand the
underlying dynamics and complex feedback structures of such
companies. With the help of a System Dynamics simulation
model we analyze the complex interaction for different pricing,
quality and timing strategies for the software production process
in combination with several sales and software integration
strategies.
Index Terms— Software development, service company,
System Dynamics

I. INTRODUCTION

I

N economic research software is typically considered to be
an information product with distinct features such as lock-in
effects, strong network economics, minimal marginal
production (replication) costs and relative high creation and
distribution costs [1]. However, the distinction between a
product and a service is difficult to make, because the
purchase of a product is accompanied by facilitating services
including installation [2].
In economic practice, different models of software vendor
and software integrator relations have evolved. On one hand
independent software vendor (e.g. Siebel, Oracle) distribute
their products and third-party software integrator (e.g.
Accenture) complete the installation. On the other hand
companies such as IBM, Siemens or SAP/SAP-SI offer single
vendor turn-key solutions. Though the internet jeopardizes
and des-intermediates traditional value chains1 the latter
organization model seems to resurrect in today’s business life.
In this work, we consider an integrated software vendor/

software integrator company developing, marketing and
distributing a new software product. We analyze the
company’s economic development and performance over a
product-life-cycle of six and a half years.
Therefore we concentrate on the company’s profit,
measured in the cash flow over the relevant period. Revenue
of the company is influenced by the product attractiveness
offered by the company and hence determines profit and
market share. We consider the software product attractiveness
as a dynamic parameter affected by:
- Quality attractiveness: the quality attractiveness
is divided into software product and software
consultancy attractiveness and represents the
degree of accuracy of the software code and the
precision and excellence of the consulting service.
- Pricing attractiveness: the pricing attractiveness
applies also to both software and consultancy
pricing and foresees different pricing and licensing
schemes including the four pricing policies with
variable pricing, multi-dimensional pricing, the flat
rate cap pricing, and flat rate pricing.
- Timing attractiveness: the timing attractiveness
describes the effects of rapid time-to-market and
early-mover advantages and includes as well the
aspect of decreasing attractiveness for outdated
software and the maintenance release time.
Therefore we analyze different pricing, quality and timing
strategies. This should improve the management of such a
company and stimulate the dynamic perception of causalities
and effects of integrated software vendor/integrator
companies. In the paper we will discuss in particular the
effects of excessive sales, timing policies and segment
differentiation strategies when the company concentrates it
sales on a certain market.
II. RELATED WORK
We consider the software development process as a project
limited in time. And with System Dynamics, which is based
on the principal of system thinking and is a tool that allows us
to model business systems for today’s complex market [4], we
found an effective method to simulate and model software

1
According to M.E. Porter [3] a value chain can be understood as a
“collection of activities that are performed to design, produce, market, deliver,
and support” products and services.

126

2
business.2
The domain of software simulation and modeling with
System Dynamics offers a great variety of publications where
several different models are presented. Constituting the
fundamentals in the area of Software Project Management, T.
Abdel-Hamid and S. Madnick developed the first software
project simulation model with System Dynamics [5]. Their
model consists of four elements: human resource
management, software production, controlling and planning.
Remarkable is also the model for software development of J.
Duggan based on the Brooks’ law [6]. It shows that adding
extra manpower to a project can lead to an increased overall
delay in the project delivery time [7]. Furthermore J. R.
Madachy developed a dynamic model of an inspection-based
software lifecycle process to support quantitative evaluation
of the process [8, 9]. He built as well a business case model
for a commercial software enterprise relating the dynamics
between product development investments, software quality
practices, market size, license retention, pricing and revenue
generation [10]. In addition Cartwright and Shepperd
introduce a model analyzing the dynamic behavior from a
maintenance perspective [11].
These examples are the most considered ones in developing
the model described in the following.
III. SIMULATION MODEL OVERVIEW
The focal company produces, markets, and distributes
software products and consultancy services such as system
integration and support. The model represents a single
company facing stochastic, potentially unlimited demand and
consists of the five following sectors.
1.
2.
3.
4.
5.

Software Production
Software Integration and Consulting
Human Resources
Finance and Controlling
Software Market

A. Software Production
The Software Production sector examines software
development and maintenance. These processes take a certain
period of time constrained by the number of employees. The
project size influences directly the project development time
and the project complexity. Furthermore this sector considers
different strategies of timing and quality influencing the
software sales and company success.
- Timing Strategy models the effects that may
accrue when the management decides to reduce the
planed development time in order to accelerate the
production processes and increases the time to
market for the software product or a new release
keeping the quality of the relevant product
2

According to J. Sterman [4] this approach is “a set of conceptual tools
that enable us to understand the structure and dynamics of complex systems.”

-

unaffected.
Quality strategy: Here the management accepts the
quality erosion of the product in order to reduce
the development time.

Delays in the product delivery are not considered.
B. Software Consulting
The consulting sector describes the software support and
services considering human resources and quality. It shows
the impact of quality of software consulting on sales and vice
versa. It considers also the estimated time for integration
depending on the number of users in the enterprise and the
product complexity. The consulting capacity is furthermore
influenced by the number of enterprises (customers) which are
supported. Also the quality depends on the number of
customers and the productivity of the consultants. Therefore
by means of interaction with Human Resources sector the
module calculates the desired number of consultants.
It will show also the effects on sales, if the management
decides to reduce the quality in order to reduce costs due to
the consultants.
C. Human Resources
Human Resources sector is split in 3 parts:
1. Human Resources for software development: New
employees will be hired, if the expected deadline
can not be achieved with the actual number of
employees. However while the resources are
limited there are a maximum number of
employees, which is pre-defined. The model
calculates the appropriate number of the new
employees to be hired. However the new
employees must be trained in order to achieve the
same productivity of experienced employees. This
training process takes time and derogates slightly
the production process in this period. The hiring
and training time is modeled as a first order delay.
2. Human Resources for software maintenance: Very
similar to the first part, new employees will be
hired, if the deadline for software modification or
new release can-not be met.
3. Human Resources for consulting: The company
will hire new consultants, if the expected delivery
time of the support ser-vice exceeds the customers’
accepted de-livery time. Those new consultants,
like the new employees of the software
development, must be trained in order to achieve
the same productivity as the experienced
consultants. In addition avail-able human
resources are limited. Due toe the training of new
employees company’s average support quality
decrease with the number of new and
inexperienced employees. If than the productivity
of the consult-ants exceeds the demand, the

127

3
module will calculate and reduce the labor force
until a minimum number of consultants, which is
pre-defined. However the new consultants with
lower productivity will be first decruited. This
process is also modeled as a first-order delay.
Training time is assumed to be constant and voluntary quits
or retirements are not considered.
D. Finance and Controlling
The finance sector tracks the performance of the software
company. The cash flow is considered to be the most suitable
measure of a company's success and is directly calculated as
inflow minus out-flow. The inflow is the addition of the
software product inflow and the consulting inflow:
- Software product inflow equals the product of the
product price and the licenses sailed.
- Consulting inflow equals the product of the
consulting price and the consulting days.
Customers pay immediately after delivery/consultancy
without any price reduction or delay. The outflow covers
wages and all other laboratory costs, rents, taxes, etc.
This sector presents also several pricing strategies for the
software product including variable price model, multidimensional price model, flat rate cap price model and flat
rate. The pricing of the product depends on the size of the
customers (small, medium and large enterprises). We
considered in our model that the variable parameter is the
number of users in each customer company.
1.

2.

3.

4.

Variable price model: The price for the license is
direct proportional to the number of the users. So it
will give a constant average.
Multi-Dimensional price model: In this approach,
companies pay additionally to the basic price –
which is a fixed price – a variable license fee
which is directly proportional to the number of
users (employees of the company). Hence, the
average price per user will constantly decline with
the increasing number of users.
Flat Rate Cap model: The price will be constant
until a defined number of users and than a variable
element will be added. The outcome of this is also
that the aver-age price declines with the increasing
number of users.
Flat Rate model: In this model, the price is set as a
constant and does not depend on the number of
users. The average price will decline rapidly with
the increasing number of users.

Additionally it presents the same pricing strategies models
for the consultancy, however the integration efforts (days) are
the variable parameter.

E. Software Market
The relevant market is split up into three parts – small,
middle and large enterprises – in order to achieve an adequate
granularity for each part. The module considers the effects of
sales on market share which again impacts sales.
In order to simulate scenarios where the company already
exists on the market, we set for each market sector a number
of concurred customers using an older version of the software
product. Thereby we determine market share of the local
company prior of the new product launch and hence strongly
influence the “System Lock In”. Sales for the new product on
the other hand depend on its attractiveness and the “System
Lock In” of the company. Hax and Wilde characterize three
Business strategic options [12]:
- Best product – competition based upon product
economics: low cost or differentiation.
- Total customers satisfaction – competition based
upon customer economic: Reducing customer
costs or increasing their profits
- System Lock In – competition based upon system
economics: complementor Lock In, competitor
Lock out, proprietary standard.
The “System Lock In” effects are crucial in software
providers’ business strategy process. For instance Microsoft
Office has a strong “System Lock In” because of the immense
customer base and the ubiquity of office application it has
become a de facto standard within most companies. Hence
most documents must be authored or converted into Office
readable documents. Furthermore, “Switching costs” are
normally high in the Software Business and are considered in
our model: if customers acquiring the new product will not
switch to a competing vendor within the next 4 ½ years.
F. The Company Model
The model, which consists of 339 equations, elucidates the
dynamic relations and feedbacks between all its components.
In this connection the model simulates the impact of different
pricing strategies on sales. It provides also many scenarios,
e.g. the impact of timing on software development cost and
sales as well as sales on software support, quality and cost.
Besides it shows the impact of excessive product sales on
software support quality which influences the software
product sales. In this paper will discuss 3 scenarios:
1. Scenario 1: the management will set the price for
software product and for software consulting low,
avoiding here a “price war” scenario.
2. Scenario 2: the management will decide to bring
the product 6 months earlier to the Market.
3. Scenario 3: the management will decide to
concentrate on the middle and big enter-prices.
IV.

VALIDATION

The structure of the model is inferred from intensive
literature research including the above mentioned literature
and software development guidelines and standards such as VModel. The Human resource module for instance is based on

128

4
TABLE I
SENSITIVITY ANALYSIS
Value

Cost
Deviation

Accepted
Delay
Salary
Consultants
Price
a
Consultancy
b
Quality

Cash
Flow

RSD*

Deviation

Revenue
Consulting
Deviation
RSD

RSD

Revenue
Software
Deviation
RSD

2,5 month
3000; 4000
EUR / empl.

5,48%

0,90%

-25,90%

4,98%

-2,68%

0,33%

-5,79%

0,33%

6,09%

2,71%

-27,96%

10,50%

-2,80%

0,00%

-5,91%

0,00%

:

-0,48%

4,95%

-8,52%

25,15%

-1,81%

8,28%

-3,89%

8,36%

1
-5,57%
3,02%
-28,94%
25,45%
-11,94%
7,91%
-12,08%
Initial Employees
Production
40 empl.
-0,90%
0,83%
-10,03%
14,25%
-2,97%
3,72%
-3,18%
Training Time
Development
3 month
5,84%
0,05%
-27,30%
0,19%
-2,80%
0,00%
-5,91%
Salary
3000; 4000
Maintenance
EUR / empl
5,79%
0,98%
-27,17%
3,71%
-2,80%
0,00%
-5,91%
Initial Employees
Consultancy
40 empl.
0,00%
0,20%
0,01%
0,22%
0,00%
0,08%
0,00%
Price
c
Product
:
1,76%
4,71%
-9,49%
36,66%
-0,86%
13,59%
-2,27%
Employees
Maintenance
10 empl.
-1,27%
2,62%
-17,74%
35,32%
-5,86%
7,80%
-5,88%
Salary
3000; 4000
Development
EUR / empl.
5,74%
1,89%
-27,05%
7,32%
-2,80%
0,00%
-5,91%
Target Time to
market
24 month
-7,75%
2,56%
-77,08%
26,28%
-28,36%
0,56%
-25,26%
*
Relative Standard Deviation
a
Prices vary according to the target customer segment: for small sized [4000, 7000], medium sized [7000, 10000] and large companies [10000, 13000]
b
Quality varies between [0.8, 1] since the quality is bounded by 1.
c
Prices vary according to the target customer segment: for small sized [1000, 4000], medium sized [4000, 9000] and large companies [90000, 14000]

Brooks' law [7]. Furthermore structures of familiar models or
(sub-) models are considered and incorporate or modified
when applicable. Some parameters such as salary per
employee can be assessed by available empirical data or
market and industry reports.
Unfortunately, the structure of the model has not been
validated empirically with interviews; calibration of the model
is constrained by the model’s size and by the scarcity of
empirical evidences. Behavior reproduction tests or Turing
tests have not been carried out. The presented model therefore
must be considered as a preliminary model a starting point for
further empirical validation [13].
However, extreme behavior tests (e.g. production, time,
hiring times etc.) and intensive numerical sensitivity analysis
support the robustness of the model. The influence of most of
the initial parameters has been assessed within the symmetric
interval of -10% and +10% of the initial value in a univariate
sensitivity analysis; salaries for consultants, developers and
for the maintenance force have been studied with a
multivariate analysis. The impact of the parameter variation
accounts in general for at most 6% of the cost development,
3% of the license revenue and 6% of the consultancy revenue
deviation form the reference case after six years. However, the
targeted time to market of the software product has a stronger
influence on both cost and revenue – This case will be
discussed in Scenario 2.
Some results of the sensitivity analysis are depicted in

8,20%
3,88%
0,00%
0,00%
0,10%
9,16%
7,98%
0,00%
1,47%

Table 1.

V. SIMUALTION SCENARIOS
A. THE BASIC SCENARIO
In this scenario the management of the company decides to
develop a new software product and to bring it on the market
within two years. In the beginning the project size is fixed at
8000 (working/day). The company starts with 40 trained
employees and each of them has a productivity of 8
(hours/day). The model calculates the need of new resources
and hires them (figure 1). However new employees have a
limited productivity of 2.4 (hours/day) and need to be trained
in order to achieve the full productivity. The training process
takes three months. During this training process the
productivity of the trained employees also decreases because
they are training the new employees. Figure 2 depicts the
progress of the product development process. The product
marketing starts when 80% of the product development is
finished. The products maintenance starts immediately after
the product is been launched and the releases and updates are
scheduled in this scenario every twelve months. The company
has 50% of the market share. We model the concurrent price
of the software product and consultancy on the market as a

129

5
random geometric walk. The management decides to use for
software pricing a flat rate pricing model for the three market
sectors. The expected price equals the mean value of the upper
and lower bound of the chosen truncated normal distribution.
The management uses a variable pricing model for
consultancy services and similarly the average price, which is
the product of the price and the number consultancy days,
takes also in the mean value of the extremes of the random
function. This makes the average attractiveness of the product
price and the consultancy price around 0.5 (with 1 as
maximum). Due to the sales the company needs consultants to
integrate and support the product. The initial number of
trained consultants is set to 40 employees. Because of the
limited available resources on the market, we define the
maximal number of consultancy employees on 80. If new
consultants are needed, the module will calculate and hire
them. The training period is also three months. Figure 3 shows
how many consultants are needed in order to keep the quality
of consultancy 1 (with 1 as the highest quality) while figure 4
depicts the number of trained consultants (productivity = 8
hours/day). Needing to train the new employees (2.4
hours/day), which causes also production erosion, a quality
fall cannot be avoided (figure 5). The cash flow of this
scenario is shown in (figure 6) and the market share for the
three market sectors is presented in figure 7.
In figure 1 we can see that the model hires at the beginning
two new employees in order to bring the product on time
(time=24) as shown in figure 2. When the product is launched
the consulting begins. At the launch time of the product the
attractiveness of the product starts very high with 1. Then it
decreases as a Smooth Function with the time until a new
release is launched which affects the attractiveness in a
positive way. We see in figure 3 that the need for consultants
is much higher than 40 and exceeds also the maximum
number of resources. This is why 40 new consultants are hired
and the number of trained employees rises with time as shown
in figure 4. The quality of consultancy suffers from the lower
production (figure 5). The quality is modelled as a Smooth
Function, because bad quality needs time to recover. This
affects the cash flow and the Market Share.
-

-

Cash Flow at (time=78) is 7.32842 Million Euro
Market Share (time= 78) is 48.9953 % (this is due
to the lower attractiveness of the product at the end
of its life cycle).
Market Share Small enterprises = 48.7651%
Market Share Middle enterprises = 51.4553%
Market Share Large enterprises = 48.1969%

New Software Developers
20

15

10

5

0
0

6

12

18

24

30

36
42
Time (Month)

48

54

60

66

72

78

New Software Developers: Scenario Three
New Software Developers: Scenario Two
New Software Developers: Scenario One
New Software Developers: Basic Scenario

Fig. 1: New software employees
SW- Product
8,000

6,000

4,000

2,000

0
0

6

12

18

24

30

36
42
Time (Month)

48

54

60

66

72

78

54

60

66

72

78

54

60

66

72

78

"SW- Product": Scenario Three
"SW- Product": Scenario Two
"SW- Product": Scenario One
"SW- Product": Basic Scenario

Fig. 2: Software product development
Need for Consultants
600

450

300

150

0
0

6

12

18

24

30

36
42
Time (Month)

48

Need for Consultants : Scenario Three
Need for Consultants : Scenario Two
Need for Consultants : Scenario One
Need for Consultants : Basic Scenario

Fig. 3: Need for consultants
Trained Consultants
80

60

40

20

0
0

6

12

18

24

30

Trained Consultants : Scenario Three
Trained Consultants : Scenario Two
Trained Consultants : Scenario One
Trained Consultants : Basic Scenario

Fig. 4: Experienced consultants

130

36
42
Time (Month)

48

6
consultancy is around 1 (with 1 as maximum). This implies
that the sold licences are much higher than in Basic Scenario
(figure 8). In addition this implies that the need of consultants
is much higher (figure 3). This affects the quality of
consultancy (figure 5) which affects on its part the sales. This
is why the market share, which increased at the beginning of
the sales, decreases until the quality recovers (figure 7).
However, when the quality recovers the attractiveness of the
product due to time was lower this is why market share does
not increase again. This scenario shows that the strategy
chosen did not achieve its goal.

SW Produkt Quality Attraktiveness
1

0.75

0.5

0.25

0
0

6

12

18

24

30

36
42
Time (Month)

48

54

60

66

72

78

SW Produkt Quality Attraktiveness : Scenario Three
SW Produkt Quality Attraktiveness : Scenario Two
SW Produkt Quality Attraktiveness : Scenario One
SW Produkt Quality Attraktiveness : Basic Scenario

-

Fig. 5: quality attractiveness
Cash Flow
10 M

Cash flow is 3.38 Million Euro
The Market Share increased only to 56.9805%
Market Share Small enterprises = 56.2093%
Market Share Middle enterprises = 58.9246%
Market Share Large enterprises = 56.2093%

5M

Revenue Software
0

10 M

-5 M

7.5 M

-10 M

5M
0

6

12

18

24

30

36
42
Time (Month)

48

54

60

66

72

78
2.5 M

Cash Flow : Scenario Three
Cash Flow : Scenario Two
Cash Flow : Scenario One
Cash Flow : Basic Scenario

0
0

6

12

18

Fig. 6: Cash Flow

24

30

36
42
Time (Month)

48

54

60

66

72

78

Revenue Software : Scenario Three
Revenue Software : Scenario Two
Revenue Software : Scenario One
Revenue Software : Basic Scenario

Market Share
80

Fig. 8: Software revenue
70

SCENARIO 2:
60

50

40
0

6

12

18

24

30

36
42
Time (Month)

48

54

60

66

72

78

Market Share : Scenario Three
Market Share : Scenario Two
Market Share : Scenario One
Market Share : Basic Scenario

Fig. 7: Market Share

SCENARIO 1:
In Scenario 1 the management decides to use an aggressive
pricing strategy in order to capture a higher market share
(80%). They set the prices for software by using a variable
price model for small enterprises, a multi-dimensional price
model for middle enterprise and a flat rate cap model for the
large enterprises. The prices for consultancy are defined for
the three markets segment with variable pricing models. The
prices however are around the lower extreme of the
concurrent price random function. Here we don’t consider any
“game scenario”. While the prices are low, the pricing
attractiveness of the product for both software and

In this scenario the pricing strategy is the same as in the
Basic Scenario, which implies an average price attractiveness
of 0.5. Furthermore the management decides twelve months
after the beginning of the project to launch the product six
months earlier in order to get a higher market share due to a
very innovative product on the market. The model calculates
and hires the new developer as shown in figure 1. This curve
reach a maximum of 16 new employees at time = 13.
However due to training time and production erosion the
product is finished at time = 22 (figure 2). This new product,
which is released two months earlier than in the Basic
Scenario, has a very high timing attractiveness and a time
advantage. The time advantage of two months has four
positive feedbacks:
- The consulting of the product can begin two
months earlier. This influences the Cash Flow.
- The attractiveness of the product is higher due to
the product innovation.
- The product life cycle is also two months longer.
- The marketing of the product starts also in this
scenario at time=18 (when 80% of the product is
finished) (figure 9).

131

7

However, the investment for this product due to the higher
number of software developer is higher as shown in figure 9.
As the period between the marketing of the product, which
start at time 18, and its launch is smaller (two months), the
sales which happen in this period are less than the sales which
happen during the same period in the Basic Scenario. On the
other hand the consulting of the product begins earlier. This
means that at the beginning of the product the need of the
consultants is smaller as shown in figure 3 which also affects
the quality of the consultancy as shown in figure 5. Higher
timing attractiveness, consultancy quality attractiveness and a
longer product life cycle affect licenses sale and cash flow and
also Market Share. However approaching the issue of
reducing the time of development needs to be well studied and
is not very simple due to the project coordination difficulties.
Other work mentioned in the literature approaches this area.
Project development needs time and many resources and since
the company has amongst others limited resources, the
management can not reduce the development time as it wished
to. In this Scenario we show that the initial strategy of the
management to launch the product six months earlier failed.
This is due to Brooks’ law which says that adding additional
manpower to a software project often make it later, because of
the project coordination difficulties [7].
-

Cash Flow = 9.02003 Million Euro
Market Share = 52.607 %
Market Share Small enterprises = 51.4894%
Market Share Middle enterprises = 55.0758%
Market Share Large enterprises = 51.8039%
Marketing Start

1

0.75

0.5

0.25

TABLE II
PRICING FOR SCENARIO 3
Small
Middle
enterprises
enterprises
Software
Price

Consultancy
Price

Average
Software
Price
Attractiveness
Average
Consultancy
Price
Attractiveness
.

MultiDimensional
Fixed Price =
1000 Euro
Variable
Price=100
Euro
Variable
Price
Price = 1500
Euro/ Day
0.15

0.17

Large enterprises

Flat Rate
Price = 5500
Euro

Flat Rate
Price = 10500 Euro

Flat Rat
Price = 8500
Euro

Flat Rat
Price = 12000 Euro

0.73

0.72

0.5

0.33

almost constant, 60% of the initial small enterprise customers
decide to pay the high price which lies in average on the
higher extreme of the concurrent price Random Function. The
market share for the middle enterprises rises to 54.7323%
because of the attractiveness of the software price which lies
below the average price of the concurrent price (figure 11).
The market share for the large enterprises is 49.9408% (figure
12). This is by reason of the average price of software and
consultancy which is 0.525 and the lower time attractiveness
at the end of the product life cycle. The quality of consultancy
is better as in the Basic Scenario (Figure 5). This, the higher
software and consultancy price for small enterprises and the
higher number of licenses sold for the middle enterprises,
force up the cash flow to 8.5 Million. This Scenario shows
that due to the limited resources available for the software
company, especially the consultancy resources, the company
can not optimise the consultancy quality with such resources.
Concentrating the resources to a certain segment of the market
can raise the Cash Flow.

0
0

6

12

18

24

30

36
42
Time (Month)

48

54

60

66

72

78

Marketing Start : Scenario Three
Marketing Start : Scenario Two
Marketing Start : Scenario One
Marketing Start : Basic Scenario

Figure 9: Marketing start
SCENARIO 3:
In this scenario the management decides to concentrate on
middle and big enterprises. They set the price as in Table II:
As we see in table II the attractiveness of the price for small
enterprises is low and that is why the market share of the
small enterprises drops to 41.466% (figure 10). However, due
to the high switching cost and the System Lock In staying

132

-

Cash Flow = 8.50561 Million Euro
Market Share = 48.1635
Market Share Small enterprises = 41.466%
Market Share Middle enterprises = 54.7323%
Market Share Large enterprises = 48.9408%

8
Market Share Small Enterprises
80

70

60

50

40
0

6

12

18

24

30

36
42
Time (Month)

48

54

60

66

72

Market Share Small Enterprises : Scenario Three
Market Share Small Enterprises : Scenario Two
Market Share Small Enterprises : Scenario One
Market Share Small Enterprises : Basic Scenario

78
Percent
Percent
Percent
Percent

Fig. 10: Market Share for small enterprises
Market Share Middle Enterprises
80

70

increases the revenue. And finally we discussed the case when
the management decides to concentrate on a Market segment,
using different pricing strategies. Due to the limited resources
of the company, this procedure implies a higher market share
in the chosen sectors and a higher Cash Flow. However, we
didn’t discuss exclusively in this paper the cases of using
different pricing strategy, although this model is able to
analyse this.
The simulations for different pricing and licensing
scenarios reveal the complex feedback structures in the price
setting process and its impacts on the performance of the
integrated software and consulting company. Further research
efforts should be made towards validation and calibration of
(sub) models though the size of the model limits the value of a
carefully calibrated model. Empirical data for instance from
company or industry reports have not been incorporated in the
modelling process but would be a desirable enhancement.
Behaviour reproduction or Turing tests have not been carried
out but should increase the confidence in the model.

60

50

40
0

6

12

18

24

30

36
42
Time (Month)

48

54

60

66

72

Market Share Middle Enterprises : Scenario Three
Market Share Middle Enterprises : Scenario Two
Market Share Middle Enterprises : Scenario One
Market Share Middle Enterprises : Basic Scenario

78
Percent
Percent
Percent
Percent

Fig. 11: Market Share for Middle Enterprises
Market Share Large Enterprises
80

70

60

50

40
0

6

12

18

24

30

36
42
Time (Month)

48

54

Market Share Large Enterprises : Scenario Three
Market Share Large Enterprises : Scenario Two
Market Share Large Enterprises : Scenario One
Market Share Large Enterprises : Basic Scenario

60

66

72

78
Percent
Percent
Percent
Percent

Fig. 12: Market Share for large enterprises

VI. CONCLUSION
Those scenarios illustrate the independences between the
quality of software consultancy and the cash flow of the
product. Limited resources for the consultancy support of a
software product will decrease enormously the cash flow of a
company, if excessive sales happen. Furthermore the market
share can not achieve its growth target. We applied also
Brooks’ law on the software development and showed its
impacts on the company market share and cash flow. Bringing
a project earlier on the market costs more investment but

133

9

APPENDIX: THE COMPANY MODEL

134

10

REFERENCES
[1]
[2]
[3]
[4]
[5]
[6]
[7]
[8]

[9]
[10]
[11]

[12]
[13]

Shapiro, C.; Varian, H. R.: Information Rules: A Strategic Guide to the
Network Economy, Boston: Harvard Business School Press, 1998
Fitzsimmons, J. A.; Fitzsimmons, M. J.: Service management, 4. ed.,
Boston: McGraw-Hill, 2004
Porter, M. E.: Competitive Advantage, New York: Free Press, 1985
Sterman, J. D.: Business Dynamics, Boston: Irwin McGraw-Hill, 2000
Abdel-Hamid, T.; Madnick, S.: Software projects dynamics - an
integrated approach, Prentice-Hall, 1991
Duggan J. : System dynamics simulation for software development,
National University of Ireland, Galway 2004
Brooks F.P.: The mythical man-month, Addison-Wesley Publ., 1995
Madachy R.: A software project dynamics model for process, cost,
schedule, and risk assessment, PhD Thesis, University of Southern
California, Los Angeles, 1994
Madachy R.: System dynamics modeling of an inspection-based process,
IEEE Computer Society Press, Berlin, 1996
Madachy R.: A software product business case model, USC Center for
Software Engineering and Cost Xpert Group, Prosim 2004
Cartwright M; Shepperd M.: On building dynamic models of
maintenance behaviour, in Proc. 10th European Software Control and
metrics Conf. Shaker Publishing, Hertmonceux, 1999.
Hax A. C., Wilde D.L.: The Delta Project: Discovering new sources of
profitability in a networked economy, St. Martin’s Press, 2001
Vennix, J. A.: Group model building, Chichester: Wiley, 1996

135

136

Part 3

Experience Reports

137

138

Implementing Generalized Process Simulation Models
David M. Raffo+*, Umanatha Nayak*, and Wayne Wakeland&
+College of Engineering and Computer Science
*School of Business Administration
&
Systems Science Program
Portland State University
Portland, Oregon, USA

Abstract
Over the past decade, software process
simulation has been used by both researchers and
practitioners to assess the cost, quality and schedule
associated with projects. Although this technology
has been shown to have success in addressing
questions ranging from strategic management issues
to process improvement planning as well as
management training, this technology has not yet
been widely adopted. One of the inhibiting factors is
the cost of developing these models. In order to
reduce the cost associated with developing software
process simulation models, we have developed
generic building blocks that can be used to quickly
create software process models. In addition, our
software process simulation blocks can offer
important benefits in terms of improving the quality
and usability of these models. In this paper, we
present our generalized process simulation model
(GPSM) concept and blocks. We also present two
illustrative examples that apply the generalized
blocks to create software development lifecycle
models at two leading organizations.
Key Words: Software Process Modeling, Process
Reuse, Process Simulation, and Generic Blocks.

1

Introduction

Software process simulation (SPS) is an
emerging technology/methodology that can be
used in software project management to estimate
project-level performance, carry out ‘what-if
analyses’, and predict the impact of process
changes among others. Raffo et al. [1] state that
industry has yet to take full advantage of the
benefits offered by SPS models (SPSMs). They
cite two main reasons for this:
•
•

difficult to achieve. SPSMs may be developed at
different levels of scope and depth to suit an
organization’s needs. The higher the level of
detail and fidelity to the process, the greater
amount of effort required to build the SPSM.
Further, the simulation tools used to develop an
SPSM can have a significant impact on the
amount of effort and the degree of expertise
required. Most simulation tools are very general
and are not tailored to address software process
modeling requirements. These requirements
include such concepts as modularization, reuse,
and product families. The following research is
of particular interest:
•
•
•

information hiding by Ribo et al. [9],
the modularization concept by Parnas [2
– 6], and
the product family concept by Weiss et
al. [7].

Given the obstacles to the adoption of SPS
models by industry discussed above, and the
results from prior research, Raffo et al. [1]
believe a tool is needed that provides an
architecture and model structure in the form of
templates and basic model building blocks.
Such a tool would significantly reduce the
expertise required of the modeler, model
analysts, and model users. They propose a
generalized process simulation model (GPSM)
component
logic
with
the
following
modularization framework provided by Parnas
[2 - 6]:

Process models tend to be difficult to
build and maintain.
The metrics data for the process models
can be difficult to acquire (or may not
exist).

The general perception is that SPSMs are
costly to build and that timely results are

139

•

Partitioning the Simulation Process
Logic
- Reducing Logic (Complexity)
- Protecting Logic (Errors)

•

Changing the Simulation Process Logic
- Internal (High Changes)
- Interface (Low Changes)
- Grouping (Manageable
Aggregation)

•

Understanding the Simulation Process
Logic

1

-

Document Internal
Document Interface
Relevant Access

Raffo et al also adopt the “product family”
concept as defined by Weiss et al [7] within the
GPSM framework. The product family defines
the overall GPSM domains by the processes
being modeled, by the facilities that will be
required to promote the development of family
members, and by the ability to rapidly create
these members into usable simulation
applications that perform within their context.
Starting with this broad concept of GPSM,
this paper proposes a specific framework for
building software process simulation models by
making use of generic building blocks. First, we
discuss the different activities involved in
software development, and how these activities
can be partitioned into four software
development activity categories. Then, we
present example generic model blocks designed
to support the creation of SPSMs. Next we
discuss example high-level process/activity
blocks built using the GPSM generic blocks.
Finally, we briefly describe two models built
using a combination of GPSM process/activity
blocks and generic model blocks.

2

This block can be used to represent
requirements, design, coding, or other
forms of development activity.
Inspection – where an entity is being
reviewed or inspected to detect the
defects injected during the development
activity.
Rework – where an entity is being
reworked to correct the defects detected
during the inspection activity and a
likelihood of some bad-fix type of defects
being injected into it.
Testing – where an entity is being tested
to detect any defects that may have been
injected during development and rework
activities.

•

•

•

Classification of various SW development
activities into four distinct categories helps with
modularization, reduction of complexity,
protection of logic, and reuse of GPSM
components. We can further classify the Testing
activity into four sub categories namely: Identify
Anomalies, Verify Anomalies, Rework, and
Regression Testing.
•

Building GPSM Product Platform
•

This section addresses some of the
implementation-related issues of generalized
process simulation model (GPSM) concepts.
This work makes use of Extend™ from
ImagineThat, Inc. to implement these concepts
and addresses following concepts involved in
creating GPSM product platform:
•
•
•

Categorizing
software
development
activities,
Building generic GPSM blocks,
Building GPSM process/activity blocks.

Each of these will be discussed further in the
following sub-sections.
2.1

•

The Inspection activity can be further
divided into two or more low level activities
such as Inspection Preparation, and Hold
Inspection Meeting.
•

SW Development Activity Categories

We can classify various SW development
activities into following four categories:
•

Development – where an entity is being
worked on or developed and a likelihood
of some defects being injected into it.

Identify Anomalies – where an entity is
being tested on to detect the defects.
This testing may detect one or more
anomalies that point to one ore more
underlying
defects
during
the
verification process.
Verify Anomalies – where an anomaly
detected during Identify Anomalies
activity is traced to the underlying
software defect (one or more anomalies
might be caused by a single underlying
defect).
Regression Testing – where a reworked
entity is being tested using a subset of
test cases to make sure the entity
performs correctly as expected.

•

Inspection Preparation – where an
entity is being prepared for an
inspection activity.
Inspection Meeting – where a meeting
takes place to evaluate or inspect an
entity.

Defining these low-level
inspection
activities enables us in creating GPSM process

140

2

blocks for Inspection types like Walkthrough,
Desk Checking, and Full Fagan etc.
2.2

Building Generic GPSM Blocks

them. In addition to this, we can add other
GPSM process/activity blocks to this library as
the need arises. Figure 1 shows a screen shot of
a SW, Inspection (Formal) block while the SW,
Testing block is shown in Figure 2.

The generic GPSM blocks greatly reduce the
complexity involved in building software
process models. In order to manage these
generic blocks better and facilitate component
reuse, a new Extend™ library called SW,
Generic was created to hold these generic GPSM
blocks. Some of the new blocks created in this
library are:
•
•
•
•
•
•
•

Activity, Development,
Activity, Inspection,
Activity, Rework,
Activity, Identify Anomalies,
Activity, Verify Anomalies,
Activity, General,
SW, Resource Pool.

Figure 1: Screen shot of SW, Inspection (Formal)
block

3

The generic GPSM blocks work in
conjunction with SW, Resource Pool block, a
new block that handles the allocation, release,
and preemption of resources to various
activities.
2.3

Using these new GPSM libraries (i.e. SW,
Generic and SW, Process), it is much easier to
develop life-cycle models for real software
development projects. To demonstrate the ease
of use and reduction in cost, we built the
following lifecycle models using the new GPSM
libraries:
•

Building GPSM Process/Activity Blocks

Once the generic GPSM blocks were
designed and developed, the following GPSM
process blocks were created by utilizing generic
GPSM blocks from SW, Generic library making
use of hierarchical block concept of Extend™:
•
•
•

Building GPSM Models

SW, Testing,
SW, Joint Review,
SW, Inspection (Formal).

These new GPSM process/activity blocks were
placed in new GPSM library called SW, Process.
As discussed in the previous section, creation of
these blocks was necessitated by the fact that
some of the activities had multiple sub tasks in

•

A tailored IEEE12207 software process
with an IV&V process being used at
NASA,
An incremental development process used
at a leading software development firm.

In the first model (shown in Figure 3), we
added an IV&V process layer along with a
tailored IEEE12207 process model for largescale NASA projects. This model contains 86
process steps and has 3 levels of hierarchy to
support both systems and software development
projects. Using the generic blocks provided in
the GPSM environment, process steps might be
easily added or removed so that the model can
be easily tailored to specific company processes.

Figure 2: Screen shot of SW, Testing block

141

3

Figure 3: Screen shot of a tailored IEEE12207 software process lifecycle model

Figure 4: Screen shot of an incremental development process model at a leading SW firm

In addition, Figure 4 shows the model of an
incremental development process used at a
leading software development firm. This model
also has multiple layers of hierarchy all built
from the generic process blocks described
above. These two models confirmed that the
custom libraries significantly reduced the time to
build the models and hence reduced the cost.
Both the IEEE12207 software lifecycle process
and the incremental development lifecycle
models were built in two days rather than
several weeks that would have been required to
build such a model without the new GPSM
libraries.
Many favorable comments were provided by
users regarding the usefulness and appropriateness of GPSM. Some of them are:
•
•
•

•
•

Very easy to construct the model using
GPSM blocks compared to using regular
Extend™ blocks.
GPSM blocks hide the complexity from
the users.
Took a couple of hours to understand
how GPSM blocks work (of course
previous experience with Extend™
assumed).
Once we had the data, we required only
a couple of hours to populate the model.
Requires less memory and runs faster.

4

Conclusions

This work demonstrates that specific SPSMs
can be built very quickly when using the GPSM
framework – a set of generic software specific
building blocks. These generic GPSM blocks
enable modularization, reuse of components, and
creation of product families. The demonstration
included constructing models for a tailored
IEEE12207 software life-cycle process and an
incremental software development life cycle.
These models were built in hours rather weeks
and GPSM framework was very favorably
received by users who were well versed in
alternative methods.
The use of GPSM concepts and GPSM
building blocks makes it very easy to build SW
lifecycle models and leads to a substantial
reduction in time and cost of building SPSMs.
Once we build these models, we can run various
scenarios or use cases by changing various
model parameters. In addition, these models can
be used as templates and quickly adapted to
other projects within the firm. In future work,
we plan to add more blocks to the libraries and
build models for other software life cycles using
this GPSM framework.

142

4

Acknowledgements
This work has benefited from programming
work done by Bhuricha Sethanandha and testing
work carried out by Siri-on Setamanit whose
contribution is gratefully acknowledged. In
addition, we are grateful to NASA for
supporting this research effort under grant
NAG5-12739.

His active research areas include software process
simulation, biomedical simulation, and optimization
in the context of simulation. Wakeland completed his
Ph.D. in Systems Science at Portland State University
after earning B.S. Engr. and M. Engr. degrees at
Harvey Mudd College. His background also includes
twenty year of industrial management experience in
information technology and manufacturing.

References
Biographies

[1] Raffo, D.; Spehar, G.; Nayak, U., "Generalized
Process Simulation: What, Why and How?",
Proceedings of ProSim ’03 Workshop, May
2003.

David Raffo
Dr. Raffo is currently Associate Professor of
Computer Science and Business Administration at
Portland State University. He is also a Visiting
Scientist at the Software Engineering Institute. Raffo
completed his Ph.D. at Carnegie Mellon University.
Dr. Raffo’s research interests include: Software
Process Design, Financial Analysis of Software
Engineering Decisions, Process Simulation, and
Value Based Software Engineering. Dr. Raffo has
over forty refereed publications in the field of
software engineering and is co-Editor-in-Chief of the
international journal of Software Process:
Improvement and Practice. He has received research
grants from the National Science Foundation (NSF),
the Software Engineering Research Center (SERC),
NASA, IBM, Tektronix, Motorola and NorthropGrumman. Prior professional experience includes
programming as well as managing software
development and consulting projects at Arthur D.
Little, Inc. Dr. Raffo teaches a variety of courses in
Software Process and Operations Management.
Umanatha Nayak
Mr. Nayak has over eleven years of software
development experience working as a SW engineer,
senior systems Analyst and IT Consultant. He has
worked for companies like British Telecom, Pacific
Telecom, CenturyTel, Alaska Communications
Systems, and Mahindra British Telecom on a wide
range of systems. His simulation experience involved
using GPSM to simulate commercial software
development processes. Mr. Nayak is currently a
doctoral student in Portland State University with
research interests in software process modeling and
simulation. He has an MBA from Portland State
University with an emphasis on Management of
Innovation and Technology, and a B.E in Electronics
and Communications Engineering from Mangalore
University in India.

[2] Parnas, D. L. "On the Criteria To Be Used in
Decomposing
Systems
Into
Modules",
Communications of the ACM, Vol. 15, No. 12,
pp. 1053-1058, December 1972.
[3]

Parnas, D.L., "On the Design and Development
of Program Families'", IEEE Transactions on
Software Engineering, Vol. SE2, No. 1, March
1976, pp. 1-9.

[4]

Parnas, D. L., "Designing Software for Ease of
Extension and Contraction", IEEE Transactions
on Software Engineering, Vol. SE-5, No. 2,
March 1979, pp. 128-137.

[5]

Parnas, D. L., "The Modular Structure of
Complex Systems", IEEE Transactions on
Software Engineering, Vol. SE-11, No. 3,
March 1985, pp. 259-266.

[6]

Parnas, D. L., Clements, P.C., "A Rational
Design Process: How and Why to Fake It",
IEEE Transactions on Software Engineering,
Vol. SE-12, No. 2, February 1986, pp. 251-257.

[7]

Weiss, D. M.; Lai, C.T.R., "Software ProductLine Engineering: A Family-Based Software
Development Process", Addison-Wesley, 1999,
448, pgs., ISBN 0-201-69438-7.

[8]

Gardner, K., "Cognitive Patterns: ProblemSolving Frameworks for Object Technology",
Addison-Wesley, 1998, 250, pgs., ISBN 0-52164998-6

[9]

Ribó J. M.; Xavier Franch X. A precedencebased approach for proactive control in
software process modeling, ACM International
Conference Proceeding Series, Proceedings of
the 14th international conference on Software
engineering and knowledge engineering,
Ischia, Italy, 2002.

Wayne Wakeland
Dr. Wakeland is an Associate Professor of System
Science at Portland State University, where he
teaches a suite of courses on computer simulation.

143

5

Simulating Problem Report Flow in an Integration Process
Dan Houston, Ph.D.
Aerospace Systems Software CoE
Honeywell International, Inc.
dxhouston@ieee.org

switching contexts, produced changes in
managerial policies.
Clearly, the literature has recognized a wide
variety of ways in which simulation can support
SPI. Furthermore, the categories of simulation
purpose offered by Kellner et al. [1] can be
enhanced by suggesting several dimensions for
distinguishing simulation-supported SPI.
x Standards-based vs. business-driven. An
agenda for SPI, and therefore for simulation
support, may be guided by the use of an
industry standard, or it may be driven directly
by needs for improving the business results of
programs.
x Modeling scope. Madachy and Tarbet
distinguished five levels of model scope: a
portion of a development life cycle; a
development project; multiple, concurrent
projects; long-term product evolution; and
long-term organization.
x Retrospective vs. predictive. Retrospective
modeling can be performed either for the sake
of measuring savings from improvements, or
for the sake of understanding problems and
what might have been done differently. It has
all the advantages of hindsight, including data
for model calibration. Predictive modeling
looks ahead and generates new information for
decision-makers.
In terms of these categories, this case study
describes use of a model based on immediate
business need. The scope was a single phase,
integration, and the model was primarily
retrospective for the sake of understanding the
processing of integration problems.

Abstract
Software process improvement (SPI) has been a
discernible theme in the literature on software
process simulation. This literature has recognized
a wide variety of ways in which simulation can
support SPI. This case study describes one of those
ways. Very focused, retrospective modeling of
integration problem report flows provided insight
into integration dynamics. The insight gained, both
from modeling the recent development phase and
from modeling some alternative scenarios, clarified
the lessons learned and suggested a major
improvement for the next release cycle.
Keywords:
integration
simulation

Software
problem

process improvement;
processing; integration

1. Simulation and SPI Background
Software process improvement (SPI) has been a
discernible theme in the literature on software
process simulation. In their scheme for
characterizing software process simulation, Kellner
et al. [1] listed SPI as one of six purposeful
categories. Several studies have supported this
characterization.
In particular, several studies have related
simulation for SPI to industry standards. Christie
[2] and Raffo and Vandeville [3] both discussed
simulation in the context of CMM-based SPI.
Christie suggested a role for simulation in
progressing to each CMM level. Raffo and
Vandeville described the use of simulation in a
company that embraced the CMM as part of its
strategic process improvement goals. Stallinger [4]
described the development of a model of process
capability evolution based on ISO 15504.
In contrast to standards-based SPI deployment
of process simulation, some authors report the use
of smaller, more focused modeling efforts that
influence managerial decision-making. Madachy
and Tarbet [5] reported on the use of six small
models. Use of two of them in particular, a model
of Brooke’s Law and a model of personnel

2. Case Study Background
Industrial software development often takes
place under intense schedule pressure. In this type
of setting, large process improvement projects are
difficult to undertake; the time and effort required
for simulating a complete development process can
be prohibitive. As a result, SPI opportunities must
be studied quickly and the best improvements
selected
and
implemented
incrementally.
Consequently, improvement must often begin by

144

build became more stable, more testing could
be done, until a very unstable build was
produced.
x The number of problems logged for a build
was a linear function of the build test duration
(R2=.77).
x The types of problems found varied through
the program, with integration problems being
found first. The types of problems found in
integration shifted as functionality was added
and as early problems were cleared.
This study produced a number of
recommendations for improvements to the ITTS
tool, to build planning and management, and to
integration management.

focusing on a particular phase of concern. Such
was the case in an avionics system program.
An avionics system provides numerous
functions in areas that include flight planning, crew
displays, navigational aids, vertical and lateral
guidance, and aircraft system monitoring. These
functional subsystems must be integrated within
themselves, but must also integrate with other
aircraft systems such as flight controls and
communications.
The software for a new avionics system is
typically developed over a period of one to three
years. In developments for a large new commercial
aircraft, the system can be delivered incrementally,
each increment about 4 to 6 months apart. The
functionality for each increment is defined with the
customer. Depending on the size of the new
system, each increment may be managed as a
program consisting of development projects based
on product lines. At the procedural level, software
development processes can vary across product
lines, and therefore from one project to another
within a program due to factors such as
geographical
distribution,
automated
code
generation, software artifact reuse, technical
maturity of the team members, team stability,
product innovation, and team’s domain experience.
The complexity of an avionics system means
that the integration phase of development is a
major undertaking. In 2003, a study of the
integration process found that delays often
occurred due to scheduling lab time. The
integration process was improved by adding tool
support in the form of an application for scheduling
lab usage. This application, Integration Test
Session Support (ITSS) also captures test session
data in the form of integration problem reports
(IPRs) stored in a database and routed to the
appropriate assignee for investigation and
evaluation.

4. Problem Report Process and Model
At the same time that the integration study was
undertaken, the entire development process was
also being analyzed for opportunities to “lean it
out” and improve cycle time. Lean analysis works
from a product perspective to find places in the
product development in which product sits idle.
The wasted times may result from rework, from
poor planning, from queuing problems, and so
forth. All of these are opportunities for cycle time
improvement.
In order to take advantage of the ITSS data
study and relate it to the lean analysis, Christie’s
advice was taken. In his description of progress
toward Level 3, he states, “Simulation can be a
significant help here in identifying critical process
dependencies, weak links and production
bottlenecks.” [2] The ITSS data provided the
ability to trace IPRs from origination to disposition
so that the timing could be characterized.
The static data analysis indicated that
processing of some classes of problems was a
source of delays. The automation provided by ITSS
had streamlined the integration process, however,
its high degree of usability also facilitated a
substantial increase in the number of IPRs that had
to be processed. Although the problem report
processing is conceptually simple (Figure 1), the
numbers of testers (95) and investigators (91)
complicated the analysis. Thus, the effect of delays
was difficult to appreciate until the process was
modeled dynamically.
A simple, discrete event model of problem
report processing was produced from the ITSS data
analysis. The model, illustrated in Figure 2, used
one entity type to represent IPRs generated by
integration testers. The analysis had identified ten
testers who generated most of the IPRs. These ten
were modeled individually and the other testers
were aggregated in the model. The IPRs were
generated deterministically by using the ITSS data
directly. By generating squawks deterministically,
the model replicated exactly what happened in the

3. Preliminary Data Analysis
The first analysis of ITSS data was performed near
the end of the first release cycle during which it
was fully deployed. Test session data and IPRs
were analyzed in order to better understand the
integration activity. The study looked at
relationships among test session variables
(frequency and duration), builds, problems logged,
and testers. The following were among the lessons
learned from the original analysis of ITSS data.
x Builds were performed daily and the duration
of testing on sequential builds was weakly
autocorrelated. That is, the time spent testing a
build was correlated with the time spent testing
the previous build. This autocorrelation
suggested that build stability was a factor in
test duration. In other words, the more stable a
build was, the more it could be tested. As each

2

145

Problem
Report
Generation

Problem
Report
Queues

Problem
Report
Investigation

Defect?

Defect
Fixing

Fig. 1. Integration Problem Reporting Process
Due to a limitation in ITSS, IPR closure dates
were not recorded. Therefore, IPR investigation
durations had to be estimated from other data
sources. Using defect records and ITSS data,
investigation durations for all IPRs were found to
be exponentially distributed, so the investigation
durations for each IPR type were assumed to be
exponentially distributed.
During modeling, it became apparent that
performance of the Type A queue dominated the
processing. The Type Other queue, actually an
aggregation of 87 small queues, handled most of
the IPRs. The Type B, C, and D queues were found
to have relatively small backlogs, which occurred
only because the people handling them were
redirected to support Type A investigations.
Consequently, IPR Types B, C, and D have
artificially inflated investigation durations and their
backlogs could have been cleared quickly.
Given their queue sizes, The Type A and Other
investigation durations were modeled as
exponential distributions. However, the Type B, C,
and D investigation durations were not important,
so they were modeled simply as constants.
Once IPRs are investigated, the ones that are
not defects (“Unable to reproduce,” “Nonproblem,” or “Duplicate”) are routed to closure.
The IPRs determined to be defects are routed for
fixing only when not covered by an existing defect
report. As 48% of the defect IPRs did not already
have defect reports, a uniformly distributed random
number (U(0,1)) was used to select these entities.

process. It also reduced the burden of model
validation and it provided a baseline for evaluating
stochastic variations of the model.
Each IPR has a classification and a disposition,
which were treated as IPR attributes. Classification
designates the functionality in which the problem
occurred. It is assigned by the tester at the time an
IPR is written. IPRs are routed to investigators
based on the Classification attribute.
Disposition is the result of the IPR
investigation and is the reason for closing an IPR.
It may be “Defect,” “Unable to reproduce,” “Nonproblem,” or “Duplicate.” Because the modeling
was retrospective, the disposition was available for
all IPRs that had been closed at the time the model
was completed. The model’s second decision node
read the Disposition attribute and either routed the
IPR for closure or for fixing.
The simulation time of the model was in
workdays. The model included ITSS data on 3400
IPRs generated over 123 workdays. Though the
program actually ran about 133 workdays, the
model was sufficient to demonstrate the integration
delay problem and alternative cases prior to the end
of the program.
The ITSS data analysis identified four
investigation queues (IPR Types A, B, C, and D)
that were backlogged. However, that analysis could
not show how those four queues grew, so they were
modeled individually; all other problem
investigation queues were aggregated in the model
(IPR Type Other).

Fig. 2. IPRs Model Diagram

3

146

were equal, 24 per day. At Workday 60, the
generation rate doubled to 48 per day, but the
disposition rate only increased by a multiple of 1.6
to 39 per day. The aggregate lengths grew steadily
until Workday 108 when the generation rate began
to decrease. The program managers knew from
other reports that a backlog of IPRs was being
worked, but they didn’t have a clear understanding
of how it had grown.
Figure 4 shows the queue lengths of the four
significant queues, Types A, B, C and D. The Type
A IPRs queue aroused the greatest concern because
it was the longest single queue. Its length peaked at
138 problems on Workday 99, while the other
queue lengths had much lower peaks. But it also
was the most expensive queue because it was on
the project’s critical path [6]. For this reason, the
three resources investigating the Type B, C, and D
IPRs were redirected to investigating the Type A
IPRs. Thus, the services rates and the queue lengths
were longer than necessary. It was expected that
the Type B, C, and D IPR queues could have been
cleared in a short time, given attention by their
assigned resources.
The simulation provided a quick translation of
queue length to development time by allowing the
simulation to run beyond the current date until the
queues clear (Figure 5). The Type A IPRs queue
had an eleven day backlog, whereas the aggregated
queues had only a six day backlog. The Type C and
D IPR queues took longer to clear due to their
inflated service times.

Fix times were distributed Weibull(.942,
13.21), which has a mean of 13.5 workdays. The
rework cycle is modeled as multiple server, so
entities depart after their assigned delay.

4. Model Usage and Results
4.1 Base Case
The actual results of the integration problem
processing were considered the base case. The
model allowed us to study the workflow after
deployment of ITSS. We found that delays had
moved downstream in the integration process. ITSS
enabled more integration testing and easier logging
and assignment of IPRs. As a result, more IPRs
were being generated. Delays due to a lack of
coordination in integration testing and manual
handling of IPRs were significantly reduced. As a
consequence, the delays due to IPR investigation
became significant.
Figure 3 shows the number of IPRs opened
(top line) and closed (middle line) up until
Workday 123. Also, the “All Closed” line (bottom)
shows the number of IPRs and defects closed.
Therefore the vertical distance between the IPRs
Opened and IPRs Closed lines is the number of
IPRs awaiting investigation or being investigated.
Similarly, the vertical distance between the IPRs
Closed and All Closed lines is the number of
defects being fixed.
The overall problem generation and disposition
rates could be characterized, for the most part, as
discontinuous linear functions. Between Workdays
12 and 60, the IPR generation and disposition rates
.

Fig 3. Open IPRs, Closed IPRs, and Closed IPRs and Defects through Workday 123

4

147

Fig. 4. IPR Queue Lengths: Type A, B, C, and D IPRs

Fig. 5. IPR Queue Lengths Cleared
had been improved with ITSS and it was helpful to
see whether further improvement was justified, or
attention should be shifted to other development
phases. Some managers were in favor of additional,
easy integration improvement, while others saw
greater need to focus more effort on higher quality
earlier in the development process. The model
afforded an opportunity to look at the relative value
of three improvement alternatives, one with regard
to higher upstream quality, and two integration
improvements
x Better quality entering integration. Though the
management team had not settled on a
particular project for improving product
quality, project ideas were being produced in
the course of an assessment during the
program. Evidence gathered during the
assessment suggested that developers were
submitting work for integration prematurely
due to schedule pressure. This led management
to think that the number of IPRs could be
reduced in the next program by better
coordination among functional areas and by
requiring more cross-functional testing prior to
integration.
Other
quality-inducing
improvements could be made, but they could
be expensive. Depending on the combination

Although the Type A IPRs queue contained
over two weeks of investigation work for about two
months, the cost of the queue was recognized as
much more than two weeks of cycle time.
Reinertsen [6] demonstrates that the cost of a queue
increases exponentially with utilization: when
utilization is below 65%, the relative time in queue
is low, but it doubles from 60% to 80% utilization,
from 80% to 90% utilization, and from 90% to
95% utilization. As will be shown, both the time in
the system and the variation of the time in system
rise dramatically with 100% utilization. Due to the
nature of the problems in this queue, some of them
were recognized as blocking parts of integration
testing. Consequently, the costs of the queue were
(a) the measurable delay due a backlog, (b)
unpredictability of completion due to high time in
system variation, and (c) the unknown delays
produced by holding test-blocking IPRs in queue.
4.2. Alternative Cases
A significant advantage of a simulation model
is in considering alternative scenarios, that is what
might have been. In this case, thinking about
alternatives was important because the next release
was being planned as the current release was being
completed. In this program, the integration process

5

148

deviation in IPR TIS at any given time during the
program up to Workday 123. Both the mean TIS
and the TIS standard deviation remain low in all
cases through Workday 60. However, as queue
lengths increased, so did the mean TIS and the TIS
standard deviation.
These figures show that increasing the IPR
investigation effort for Type A IPRs had little
effect on both the mean TIS and TIS variation.
Better testing (“No Waste”) and a 25% IPR
reduction produced substantial improvements in
mean TIS and TIS variation. The best improvement
was produced by a 50% reduction in IPRs.
Prior to this study, managers thought that
further improvement to the integration process
would be inexpensive and would reduce integration
time. However, the results indicated that much
more substantial improvements would come from
improving the upstream development process so as
to improve product quality entering integration.
Therefore steps were taken to improve the
development cycle.
x Both customer and internal requirements were
reviewed earlier so that they could be clarified
before they were needed.
x More time was spent studying functional
dependencies and planning an incremental
development schedule based on these
dependencies.
x The large number of problems entering
integration produced unstable builds that were
difficult to investigate and fix. In order to
ensure build stability and satisfaction of build
exit criteria, functional increments were
planned for 4 weeks apart rather than 2 weeks
apart. Build planning included tracing the build
plan to requirements.
x In order to ensure readiness for integration,
guidelines for integration readiness were
revisited. Revised guidelines included focused
and rigorous pre-integration testing of
developer builds prior to submission for full
integration.
x A small change was made to ITSS, the
addition of IPR Closed Date. This allowed IPR
TIS to be measured directly so that the effects
of integration changes could be monitored in
subsequent programs.

of actions employed, it was expected that the
number of IPRs could be reduced by 25% to
50%. Therefore, these two cases were
modeled.
x Better integration testing. The ITSS data
analysis had found that 32% of the IPRs were
non-problems, duplicates, or could not be
reproduced.
The
analysis
had
also
demonstrated different capabilities among
integration testers. Taking these two facts
together, managers began to wonder if better
testing could eliminate the investigation time
due to unnecessary IPRs. If so, then integration
testing effectiveness could be improved by
having the best testers train and/or mentor
other testers. Although elimination of all nonessential IPRs was considered an ideal
situation that would be difficult to achieve, it
was also worth investigating in a model, so it
was modeled.
x More problem resolution capacity. A simple
way of decreasing delays was to provide more
IPR investigation capacity. This could be done
either by increasing the number of
investigators, or by using investigators more
efficiently. Given resource constraints, the
former approach was unlikely; the latter
approach was very feasible but required that
management have very timely information on
investigation queues so that work could be
assigned to reduce investigation delays. The
case of 25% more investigation capacity for
the Type A IPRs was chosen to represent the
ability to apply resources immediately as
needed to this critical queue.
Table 1 lists the cases modeled, the labels used
in the following figures, and a brief description of
how the modeling was done. For the alternatives to
the base case, the modeling description is a
modification to the base model.
Figures 6 and 7 provide comparative results of
the cases. Figure 6 is a line plot, one line per case,
of the mean IPR time in system. The IPR time in
system (TIS) is the duration from an IPR’s
generation to its disposition. Thus, TIS includes
both queue time and investigation time. The lines
in Figure 6 show how the mean TIS increased in
each case up to Workday 123.
Figure 7 is a similar plot of the standard

Case
Base
Better quality
entering integration
Better integration
testing
More problem
resolution capacity

Table 1. The Cases Modeled
Label
Modeling
Base Case
Replicated actual IPR flow.
25% Less IPRs
25% of IPRs were randomly selected and removed.
50% Less IPRs
50% of IPRs were randomly selected and removed.
No Waste
Removed non-problem, duplicate, and non-reproducible
IPRs from the flow.
+25% Invest
Reduced the expected Type A IPR resolution time such that
25% more IPRs could be resolved daily.

6

149

6

IPR Disposition Duration

5

4
Base Case
+25% Invest

3

25% Less IPRs
No Waste
50% Less IPRs

2

1

0
0

20

40

60

80

100

120

Workday

Fig. 6. Mean IPR Disposition Times for Five Cases

7

IPR Disposition Duration

6

5
Base Case

4

+25% Invest

3

25% Less IPRs

No Waste

50% Less IPRs
2

1

0
0

20

40

60

80

100

120

Workday

Fig. 7. Standard Deviations of IPR Investigation Times for Five Cases
working relationships. A simulation can
demonstrate how changes, that might require
something more from individuals, benefit a
development process.

x

The addition of full reporting capability to
ITSS was investigated in order to support
better integration management and more
problem resolution capacity. But due to its low
benefit, this work was postponed when it was
found that it would cost more than expected.
In addition to the specific program benefits
produced by the study, the simulation also
contributed to increased “practice pull” [7], that is
the active participation of software practitioners in
developing and adopting new practices. The lack of
a desire for process improvement often limits the
effectiveness of SPI initiatives. As individual
contributors, developers tend to optimize work
practices based on their own job scopes and closest

4.3 Model Verification, Validation, and
Evaluation
Richardson and Pugh’s [8] framework
(Appendix A) was used to verify, validate, and
evaluate the model. In addition to the verification
and validation exercises performed by the modeler
(dimensional consistency, traces, base case
reproduction, and so forth), the model was
validated and evaluated in discussions with both
internal process experts and product line managers.

7

150

The model was judged to be a good representation
of the problem reporting process, with the
exception of the last step for defect fixing. This
step was too abstract to describe the projected
management of defects beyond the date the data
was collected (Workday 123). In order to
adequately represent defect report processing,
defect management activities such as prioritization
would need to be added to the model. In spite of
this shortcoming, the model provided new insights
to the integration problem report process. Managers
realized that their existing reports were neither
timely enough nor focused enough to provide them
the information required for adequately managing
IPS queues.
Development and documentation of the model
required about 48 hours of effort over 7 days.

References
[1] Kellner, Marc, Raymond J. Madachy, and
David Raffo. “Software process Simulation
Modeling: Why? What? How?” Journal of Systems
and Software 46:2-3 (1999), 91-105.
[2] Christie, Alan. “Simulation in Support of
Process
Improvement.”
Software
Process
Simulation Modeling Workshop, ProSim 98. June
22-24, 1998, Silver Falls, Oregon.
[3] Raffo, David, and Joseph Vandeville. “Software
Process Simulation to Achieve Higher CMM
Levels.” Software Process Simulation Modeling
Workshop, ProSim 98. June 22-24, 1998, Silver
Falls, Oregon.
[4] Stallinger, Friedrich. “Software Process
Simulation to Support ISO/IEC 15504 Based
Software Process Improvement.” Software Process
Simulation Modeling Workshop, ProSim 99. June
27-29, 1999, Silver Falls, Oregon.
[5] Madachy, Ray, and Denton Tarbet. “Initial
Experiences in Software Process Modeling.”
Software Quality Professional 2:3 (June 2000) 1527.
[6] Reinertsen, Donald G. Managing the Design
Factory. Simon & Schuster, New York, 1997.
[7] Börjesson, Anna, and Lars Mathiassen.
“Successful Process Implementation.” IEEE
Software 21:4 (July/August 2004) 36-44.
[8] Richardson, George P., and Alexander L. Pugh
III. Introduction to System Dynamics Modeling
with DYNAMO. Cambridge, Massachusetts: The
M.I.T. Press, 1981.

5. Conclusions
The use of a very simple, focused simulation in
this project was a low cost way to demonstrate a
basic development problem that was difficult to see
clearly from existing reports. Also, the modeling
enabled quantification of the problems encountered
and suggested how much alternative types of
changes would be necessary for eliminating
integration delays. Finally, it provided a tool for
monitoring integration in subsequent releases.

Appendix A
Model verification, validation, and evaluation was performed according to the method of Richardson and
Pugh (1981), outlined in this table.

Verification

Validation

Evaluation

Structure
Dimensional consistency

Behavior
Parameter variability / sensitivity analysis

Extreme conditions in equations

Structural insensitivity

Structural adequacy (modeler review)
Face validity (expert review)

Traces
Statistical tests

Parameter validity
Appropriateness of model characteristics
for the intended audience (user review)

Case study: replication
Check for unexpected behavior
Check for process insights

8

151

152

Part 4

Position Papers

153

154

A Conceptual Model of the Software Development Process
Diana Kirk, Ewan Tempero
Department of Computer Science
University of Auckland
New Zealand
{d.kirk|e.tempero}@cs.auckland.ac.nz
Abstract

in the long term, provide a tool for predictions based on understanding.
The rest of this paper is organised as follows. In section 2 we use a brief history of the study of planetary motion to illustrate the difference between models that just provide prediction, those that provide fragmented understanding and those that provide holistic understanding. In section
3 we overview the kinds of work researchers are doing and
examine limitations. In section 4 we suggest what a conceptual model should provide to researchers and present an
overview of our model along wih a research roadmap. In
section 5 we summarise the paper.

We believe that all software development processes can
be described by a single abstraction that accounts for the
great variety of processes that exist, outcomes of interest and project-specific factors that occur in real-world
projects. The abstraction is best represented as a conceptual model, the components of which are theoretical constructs, or models, in their own right. We believe that such
a model will facilitate model building to solve individual
problems, provide support for empirical research and, in
the long term, provide a tool for predictions based on understanding. We are taking the first steps in building such a
model.

2 Science and Understanding
Rivett [18], when describing the status of model building
in the field of operations research in 1972, reminds us that,
throughout history, man has constantly searched for pattern
and generalisation. From around 700 BC, the Babylonians
measured and recorded the motions of the stars and planets,
analysed these, and were successful in forecasting planetary
events with great precision. Their recordings of hundreds of
years of planetary data enabled them to estimate the value
of the motion of the sun from the node with an error of only
five seconds. The same estimation, when made in the nineteenth century, yielded an accuracy of only seven seconds.
Although the Babylonians recorded events with care,
they made no attempt to theorise. The Greeks, on the other
hand, followed a different approach, and built first mechanical and then geometrical models of planetary motion in an
attempt to understand and explain. However, their models
were made up of a number of parts and the Greeks had no
success in unifying these. When applied to the Babylonian
data, the models were found to be incorrect [18].
Rivett notes that, in more recent times, Kepler proposed
three laws of planetary motion based on data that had been
collected by Tycho Brahe. He applied an elliptical model
to the motion of the planets and from this model produced
laws that appeared to work. No-one knew the fundamen-

1 Introduction
An ideal predictive model of the software development
process would encompass all kinds of processes, outcomes
of interest, and variations in project-specific factors. Such
a model would support researchers by providing a holistic
framework within which research findings might be positioned and assumptions exposed. Current predictive models are scoped to solve a particular problem on a particular process. These models use existing data as a basis
for predicting outcomes, and are thus aimed at understanding and predicting within the bounds imposed by that data.
These factors make it difficult to generalise to other problems and processes. If we are to improve our ability to
predict, we must first understand in a theoretical way the
relationships between process, product and people. Building models (theories), generating hypotheses from these and
applying empirical research to test the hypotheses is an accepted approach.
It is our position that all software development processes
can be described by a single abstraction and that such an
abstraction will facilitate model building to solve individual problems, provide support for empirical research and,
1

155

tal reason why the laws worked. And we notice that, as the
laws were based on planetary data, these laws could not predict the movements of other celestial objects, for example,
comets. Newton later brought some understanding to bear
on celestial motion when he postulated a force that acted
between all objects in the universe with mass. From this
understanding and unification of ideas from physics and astronomy, he was able to show that orbits for celestial objects, for example comets, were not only elliptical, but could
be hyperbolic and parabolic. He was thus able to predict
accurately for all celestial bodies, show that Kepler’s Laws
were a special case of Newton’s Laws and improve the accuracy of Kepler’s calculations.
Rivett summarises by stating that a model may be predictive without being explanatory, but an explanatory model is
always predictive. We also note that an inability to unify
fragmented models that relate to the same phenomenon is
an indication that understanding is not complete.
A theory is a model or framework for understanding.
Building theoretical models, generating hypotheses from
these and applying empirical research to test the hypotheses is an accepted approach to better understand cause and
effect relationships.

a certain rate and certain level of proficiency. If we don’t
know what were the human factors at play when the metrics were collected, we have no idea whether or not we may
apply the same metrics in another project.

3.2 Predictive modelling
Early predictive models aimed to facilitate the prediction of costs and durations for a given project. These cost
estimation models are equations linking costs to the size of
the software product to be delivered and a number of other
factors believed to influence costs, for example, developer
experience. The form of the model equation is inferred from
a statistical manipulation on a number of datasets collected
from real projects. A more recent example of the use of statistical techniques on large datasets is the attempt to isolate
factors that affect fault distribution in code [2, 7, 15, 9, 14].
The statistical literature warns us that in this modelling
paradigm we must be wary of confusing predictive capability with causation, and that we may apply predictions with
confidence only in circumstances similar to those present
during original data capture [5].

3.3 Empirical Research

3 Current Models
The last few years have seen the emergence of an interest in sound empirical research. It is generally agreed by
researchers that, if we are to progress as a professional discipline, it is now time to move away from the ‘analytical
advocacy research’ [6] with which the industry is familiar
and towards a more formal approach to experimentation.
Research into the software process can be categorised as
examining the inter-relationships between process, product
and people. For example, “Which inspection technique is
better?”, “Did the technique yield better results if the developers were experienced?”.
As pointed out by Carver et al. [4], there is a problem
with understanding what are the common assumptions arising in current empirical research efforts. It is difficult for
researchers to be sure that all possible explanations for results have been identified and that effects are, in fact, due
to the cause under investigation. This is a problem of internal validity. For example, in experiments involving process
and product, are we certain the human factors were held
constant? We remember to take account of experience and
skills, but are there any other factors that might confound
results, for example, motivation and ease of communications?

If we consider the research currently taking place in the
area of software process modelling, we can identify a situation similar to that described in the previous section. Models built by researchers are based on specific data sets and
are thus predictive but not explanatory, and a number of different modelling paradigms are used. There are three main
research groups and each carries out research from a different perspective.

3.1 Simulation Modelling
Simulation models aim to solve a number of different
problems, many of which are predictive in nature. Different
modelling paradigms are used, for example, discrete-event
and system dynamics, and modellers approach a problem
from the viewpoint of the paradigm selected. Models are
generally created for a specific company process and tend to
use metrics data from the target company for model formulation. The scope of application of the models is thus limited and cost of model creation high. Human factors are often included as part of the model architecture, for example,
as changing engineer motivation during long projects. However, some human aspects are often ‘hidden’ in the metrics
that populate the model. For example, a company’s metrics
database may contain a measure of ‘typical productivity’
or ‘average number of defects injected or found’, and these
metrics in fact ‘hide’ the fact that real people are coding at

3.4 Need for Holism
The limitations exhibited by the above research
paradigms can be summarised as relating to context, in that

156

the full context for the data supporting the model must be
known and results obtained are not applicable to different
contexts.
We suggest that these limitations might be mitigated by
the existence of a holistic framework within which to position research efforts. This viewpoint is supported by a
number of researchers from both within and without the
software engineering community, who reiterate the need to
build models to increase understanding [1, 8, 10, 13, 18].

Process
1

1..*
1

Product

1

Method transforms
Product

Activity

1

1 1
1

1

1

Method

1

-relativeEfficiency
-relativeContentEffectiveness
-relativeQualityEffectiveness
-productPreconditions
+transformProduct()

QualityModel
-definitionModel
-architectureModel
-designModel
1
-sourceModel
-integrationModel
ContentModel
-executablesModel -definitionModel
1
-architectureModel
-designModel
CostModel
-sourceMode
-definitionModel
-integrationModel
-architectureModel
-executablesModel
-designModel
-sourceModel
-integrationModel
-executablesModel

4 Conceptual Model
Our research is aimed at developing a conceptual model
for the software development process. The model captures
aspects of people, process and product into a framework
that will facilitate a theoretical approach to software development process research. In this section, we overview the
current status of our research.

Method
transformation
scaled by

1

EngineerMultiplier
-efficiencyScaleFactor
-contentEffectivenessScaleFactor
-qualityEffectivenessScaleFactor

Method
changes
Engineer

HumanFactorsModel

Engineer
-skills
-experience
-motivation

EnvironmentalFactors
-projectManagementStyle
-teamStructure
-toolSupport

Figure 1. Process model architecture

signs, code, etc. to be delivered to the development
organisation as an asset for use in later projects.

4.1 Objectives

• We note that any Activity comprises two parts, Method
(the task that is carried out) and EngineerMultiplier
(how well the Engineers carry out the task).

Our model should provide the following support to researchers [12]
• Choice of granularity in the process under study (the
whole process, a single process element, etc.).
• The ability to select process elements and to define
new kinds of elements.
• A means of capturing the deliverables-related objectives that are the targets for the process.
• A choice of models for the above objectives, for example, quality models.
• The ability to define new kinds of product-related objectives, for example, business value.
• Decoupling of human- and techology-related factors.

• A Method is defined by how it changes the Product and
what are its Product-related preconditions (for example, existence of design documents). This means that,
in addition to the traditional tasks, for example, ‘code
from design documents’, our model handles any task
that causes change to the Product. Tasks such as ‘test
first design’, ‘create a prototype based on a feature list’
or ‘code from prototype’ are valid Methods.
• The definition of Method permits tasks of any granularity. So, for example, ‘develop product from requirements’ or even ‘develop product’ are as valid as ‘carry
out design review’.

4.2 Architecture

• When an Engineer carries out a Method, the Engineer
is changed as a result. For example, his experience or
some skills are likely to increase.

In figure 1, we present an overview of the architecture
for our model, presented as a UML diagram. The main idea
is that any software development process involves a Product being changed in some way by application of a number
of Activities. Each Activity has two dimensions, a Method
that defines how the Product changes and a scale factor, EngineerMultiplier, that captures how the change is scaled according to human related factors. Each model component
is a model in its own right. An early version of the model
is described in [11]. We highlight some key aspects of the
model below.

• A Product comprises a number of Product models. At
the moment these relate to the conventional productrelated drivers i.e. ContentModel (‘how much is
there’), QualityModel (‘how good is it’) and CostModel (‘how much did it cost’). However, as we become aware of other objectives, for example, the need
to capture business value for product artifacts[3], we
can simply extend the model to encompass these.
• In our model, we view environmental factors as affecting how well the Engineers carry out tasks. A HumanFactorsModel defines what are the relevant factors and
how these are combined into our EngineerMultiplier.

• A Product comprises all artifacts that describe the software being produced. These include the software delivered to the end customer and all requirements, de-

157

4.3 Implementation

We note that the values for ‘InspectArchitecture’, ‘InspectDesign’ and ‘CodeWalkthrough’ have increased as expected (the original experiment was to ‘improve’ these activities).

Our first implementation of the model uses java, with
product and process definitions in an input .xml file. This
implementation was selected for convenience.

4.4.2 Example 2

4.4 Proof of Concept

The next study is based on work by Pfahl and Lebsanft [16].
The cited paper describes a simulation model to analyse
how much effort would be required to stabilise requirements
such as to achieve the most cost-effective outcome. A system dynamic model is implemented. Six experiments take
place, each with a different value for requirements effort
and the results on total effort studied.
For this study, the ProductModel includes ‘total number of requirements’, ‘number of correct requirements’ and
‘cost in person hours’. We identify two Methods (‘capture requirements’ and ‘implement requirements’) carried
out for each release cycle and implement these an appropriate number of times.
The efficacy values of interest are ‘Efficiency’ and ‘ContentEffectiveness’ (how many were correct). As the experiments assume constant efficacy values for ‘implement requirements’, we show the values for the ‘capture requirements’ Activity only for the six experiments described in
the paper (table 2).

As preliminary proof-of-concept feasibility study for our
model, we have captured two simulation modelling examples from the literature and are in the process of capturing
a third. Note that we are not re-creating the research, but
are aiming to confirm that we can represent a range of experiments with our model. We present an overview of these
examples below.
For each study, we first define the appropriate Product Model, then identify the Methods required for Product
transformation, and finally identify and calculate values for
the Efficiency and Effectiveness multipliers that represent
both relative Method efficacy and how well the Engineers
work with the Method.
4.4.1 Example 1
This proof-of-concept feasibility study is based on work by
Raffo, Vandeville and Martin [17]. This paper describes a
simulation model to examine quality outcomes when the review and inspection processes are improved. A state-based
model is implemented.
For this experiment, the Product Model includes ‘Number of remaining defects’ and ‘Number of discovered defects’. The Methods identified are listed in the first column
of table 1. The relevant efficacy factor is ‘QualityEffectiveness’ and the values obtained are summarised in table 1 for
the ‘as-is’ process and the ‘to-be’ process.

Table 2. Example 2 efficacy summary
Simulation Efficiency ContentEffectiveness
n1
2.806
.47
Baseline
1
.733
n3
.586
.832
n5
.262
.935
Optimal
.1814
.9615
n6
.151
.9709

Table 1. Example 1 efficacy summary
Activity
As-is
To-be
Analyse
1.01
1.01
InspectArchitecture
.276
.65
ResolveArchitectureDefects
1
1
DetailDesign
1.003 1.003
InspectDesign
.278
.66
ResolveDesignDefects
1
1
CodeAndUnitTest
1
1
CodeWalkthrough
.28
.66
ResolveCodeDefectsWalkthrough .91
.976
UnitTest
.63
.627
ResolveCodeDefectsUnitTest
.95
.867
IntegrationTest
.67
.607
ResolveIntegrationDefects
1
1

The assumption is that more time spent results in better
requirements. In the context of our model, we note that as
time spent in gathering requirements increases (n1 to n6),
efficiency decreases (more time is taken per requirement)
and effectiveness increases (they get more correct).
4.4.3 Contribution
In the two examples above, we represented two experiments
in our model. In experiment 2, the assumption made by
Pfahl and Lebsanft is that spending more time on requirements gathering results in better requirements. When we
capture the experiment in our model, the spending of more
time is represented as a decrease in Efficiency and an increase in Effectiveness. Moreover, in the context of our
model, we understand that efficiency and effectiveness may

158

be changed either by changing the Method or by changing
how well the Engineers work with the Method. We now
have a much larger scope for a business case. Rather than a
single solution ‘spend more time on requirements’, alternatives can be proposed to optimise efficiency and effectiveness by strategies of Method fine-tuning, training, personnel
skills, etc. A similar oservation results from experiment 1.

researchers over a period of time. A collaborative approach
is indicated.

References
[1] F. Anger. Directions for the NSF software engineering and
languages program. Software Engineering Notes, 24(4),
1999.
[2] V. R. Basili and B. T. Perricone. Software errors and complexity: An empirical investigation. Communications of the
ACM, 27(1), 1984.
[3] B. Boehm. Value-based software engineering. ACM SIGSOFT Software Engineering Notes, 28(2), 2003.
[4] J. Carver, J. V. Voorhis, and V. Basili. Understanding the
impact of assumptions on experimental validity. In Proceedings of the 2004 International Symposium on Empirical Software Engineering. The Institute of Electrical and Electronic
Engineers, Inc., 2004.
[5] T. Coladarci, C. D. Cobb, E. W. Minium, and R. C. Clarke.
Fundamentals of Statistical Reasonong in Education. John
Wiley and Sons, Ltd, USA, 2004.
[6] N. Fenton, S. L. Pfleeger, and R. Glass. Science and substance: A challenge to engineers. IEEE Software, July, 1994.
[7] N. E. Fenton and N. Ohlsson. Quantitave analysis od faults
and failures in a complex software system. IEEE Transactions on Software Engineering, 26(8), 2000.
[8] A. Fuggetta. Rethinking the modes of software engineering
research. Journal of Systems and Software, 46(2/3), 1999.
[9] L. Hatton. Reexamining the fault density-component size
connection. IEEE Software, March/April, 1997.
[10] C. Jones. Programming Productivity. McGraw-Hill, Inc,
1986.
[11] D. Kirk and E. Tempero. A flexible software process model.
Technical Report UoA-SE-2004-3, University of Auckland,
2004.
[12] D. Kirk and E. Tempero. Proposal for a flexible software
process model. In ProSim 04, 2004.
[13] J. Ludewig. Models in software engineering - an introduction. Software and Systems Modeling, 2(1), 2003.
[14] J. C. Munson and T. M. Khoshgoftaar. The detection of
fault-prone programs. IEEE Transactions on Software Engineering, 18(5), 1992.
[15] T. J. Ostrand and E. J. Weyuker. The distribution of faults
in a large industrial software system. In G. S. Avrunin and
G. Rothermel, editors, Proceedings of the ACM/SIGSOFT
International Symposium on Software Testing and Analysis,
pages 56–64. ACM, 2002.
[16] D. Pfahl and K. Lebsanft. Using simulation to analyse the
impact of software requirement volatility on project performance. Information and Software Technology, 42, 2000.
[17] D. M. Raffo, J. V. Vandeville, and R. H. Martin. Software
process simulation to achieve higher cmm levels. Journal of
Systems and Software, 46(2/3), 1999.
[18] P. Rivett. Principles of Model Building: The Construction
of Models for Decision Analysis. John Wiley and Sons, Ltd,
Bath, Great Britain, 1972.

4.5 Research Roadmap
The first task is to continue retrofitting experiments from
the various research groups into our model, aiming for as
much variation as possible, for example, experiments based
on different kinds of process and experiments in which process granularity varies from fine-grained to those in which
the whole development effort is treated as a single Activity.
In parallel with the above, we will investigate what is an
appropriate representation for our model. We are considering alternatives, for example, Alloy, to replace the existing
UML representation.
Once we are happy with both model and representation,
the model will be used for hypothesis generation and formal experimentation. For example, one hypothesis would
be that a single HumanFactors model is appropriate for all
tasks. The form of the model might then be researched
within the academic environment and later, once it is clear
what might be the relevant factors, ‘tested’ in industry.

5 Summary
It is our position that all software development processes
can be described by a single abstraction that accounts for
the great variety of processes that exist, outcomes of interest and project-specific factors that occur in real-world
projects. We believe that such a model will facilitate model
building to solve individual problems, provide support for
empirical research and, in the long term, provide a tool for
predictions based on understanding.
Current software development process models predict
without understanding and research efforts are fragmented.
We hypothesise that an integrated model of the software
development process is possible and that such a model
would support researchers by providing a holistic framework within which research findings might be positioned
and assumptions exposed. We present a candidate for such
a model and some initial proof-of-concept studies.
We are aware that this is an ambitious undertaking.
There are many reasons why such a model can not be contructed in entirety at the present time. For example, we are
not confident about how to best operationalise and measure
many software attributes, the industry is not agreed on what
is an appropriate quality model, etc. This is a task for many

159

1

People Applications in Software Process
Modeling and Simulation
Ray Madachy, Senior Member, IEEE


Abstract—Focusing on people is one of the highest leverage
opportunities to improve process performance. But people issues
have been under-represented in process modeling and
simulation, as much of the work has dealt with technical aspects.
Software processes will always be socio-technical by nature, and
due attention should be paid to the human aspects and people
dynamics. Simulation using system dynamics is useful to
represent and understand some people issues. This position
paper describes basic phenomenology and sample simulation
results for people considerations. Constructs are discussed for
important aspects including motivation, exhaustion, experience
and learning curves, training, hiring and retention,
communication, stakeholder collaboration, and workforce
dynamics at the project and macro levels. A short summary of
lessons learned about people issues is also provided.
Index Terms—People factors, Software process modeling,
Software process simulation, System dynamics

I. INTRODUCTION AND BACKGROUND
People rather than technology provide the best chance to
improve processes and execute successful projects. Software
processes will always be socio-technical by nature, and due
attention should be paid to the human aspects. Too many
proposed “silver bullets” for software engineering focus on
technical solutions while ignoring the people dynamics.
People must be available, they should have requisite skills and
experience for their jobs (including how to work with others),
they should be adequately motivated to perform, have
necessary resources, and have a good working environment
that fosters creativity and learning.
The focus of this work is on phenomena directly related to
people. It will cover some important personnel attributes, skill
development, training, hiring and retention, team issues, and
macro workforce dynamics. Even though many of the people
attributes are considered “soft factors”, it is still feasible and
meaningful to model them [1].
Simulation can also be used directly for training personnel
in a project flight simulation mode. However training is a
different class of application and not addressed in this paper.

A. An Application Taxonomy
People applications are differentiated from other
applications in software process modeling and simulation in
terms of their primary state variables. In system dynamics
models, these correspond to the levels (accumulations) in the
main chains of primary focus. People applications quantify
personnel levels and characteristics. Process and product
applications are centered around software artifacts, their
transformations and attributes through the process stages.
Project and organization applications generally revolve around
issues such as business case analysis, estimating, planning,
statusing, resource allocation, tracking business value, etc.
Decision structures are frequently embodied in the
applications regarding people, processes and products, etc.
There are however many connections and overlap between
the application areas. For example, modeling the allocation of
resources for personnel development would be an
organizational application.
This paper discusses only
applications where the primary focus is on people and their
attributes.
B. People Impacts
How important are people characteristics relative to other
factors? Our COCOMO II research [2] provides quantitative
relationships for major software project factors.
The
combined personnel factors provide the widest range of
software productivity impact against all other cost factors
according to the data. The total productivity range for the
people factors is 20.8, which denotes a 2008% variation in
productivity due to people attributes. Many of the other
factors are also influenced by human actions, so the overall
variation due to people is actually greater.
However, COCOMO assumes that people are reasonably
well-motivated and that other soft factors can be treated as
invariant. These assumptions do not hold true in too many
instances, so some people factors not currently represented in
COCOMO will be addressed. Most of these represent
measures that are harder to define and collect in the
COCOMO context, so this work shows some ways they can
start being quantified.

Ray Madachy is with the University of Southern California Center for
Software Engineering, University of Southern California, Los Angeles, CA
90089 USA and Cost Xpert Group, San Diego, CA 92109 USA (e-mail:
madachy@usc.edu).

160

II. APPLICATIONS
A few sample applications will be overviewed. Additional

2
aspects and more extensive modeling details are provided in
[1]. In the first section below we start with modeling the
number and kinds of people on a job, and then address other
important characteristics.
A. Project Workforce Modeling
Workforce modeling contains structures for hiring,
assimilation, training, and transferring of people off a project.
Separate levels can be used to represent different experience
pools. Frequently there are levels for new and experienced
people, since there are productivity differences between the
two. Having separate levels also allows one to model
experienced people being involved in training of new hires.
The training overhead on the part of the experienced people
gives them less time to devote to other development tasks. A
Brooks’ Law model in [1] describes this.
In the Abdel-Hamid integrated project model [3], there was
also a work force level needed parameter for the decision on
how many people are currently required for the project. The
level is limited partly on the ability to assimilate new people
onto the project. There is also a work force level sought
parameter, which is used in conjunction with the work force
level needed parameter. The gap between total work force
and work force level sought is used to control hiring and
transfer rates.
Acquiring good personnel is often very difficult, but is time
well-spent. Personnel must first be found and identified.
Then they have to be sold on a particular job, and negotiations
ensue until all parties reach a hiring agreement. The time
needed to hire new employees depends on the position levels,
organizational culture and processes, current market trends
etc.
A system dynamics delay structure is a convenient way to
model hiring delays. The delay time refers to the period from
when a position is open, normally through a job requisition
process, to when a new hire starts work. Representative
hiring delays are listed and discussed in [1].
Besides structures for modeling the workforce dynamics,
there should also be provisions for human characteristics that
impact productivity. Some of these important factors are
reviewed next.
B. Motivation and Overtime
Motivation can have tremendously varying impact over
time. It may even eclipse the effects of skill and experience
on projects. Motivation may wax and wane due to a multitude
of factors and impact productivity either way. People will
traverse through several up or down motivational phases
during a project.
Pressure to work overtime can greatly influence motivation
and overall productivity. Suppose that management pushes
for overtime in order to catch up on schedule, and that the
management edict can be expressed in terms of desired output
to normal output. We wish to model an effective overtime

multiplier for productivity. How should this relationship be
modeled?
Limits exist such that people will not continue to increase
their overtime past their saturation thresholds. Thus, there is a
value of desired output beyond which no further overtime will
be worked. Studies have indicated that a small amount of
aggressive scheduling will tend to motivate people to produce
more, but highly unreasonable goals will have the opposite
effect. The management challenge is to strike the right
balance.
A few people might work 80 hour weeks and even greater,
but a more reasonable limit might be a 60-hour workweek for
most industry segments. This puts an upper bound of
60/40=1.5 on the overtime factor. Additionally, each worker
will not do overtime to the same extent and may start/stop
their overtime periods at different times. Thus, we will model
the response of the aggregate system as a smooth curve. Such
functions are best done in terms of normalized variables, so
the independent variable will be the desired/normal output. A
relationship has been created that relates the productivity
overtime multiplier as a function of desired/normal output [1].
This function is easily used in a system dynamics model in
graphical or tabular form.
In reality different classes of organizations exhibit varying
productivity multiplier curves. For example, people in an
entrepreneurial startup situation are going to be much more
willing to put in overtime as opposed to a civil servant
position. Additionally the curves level off when extrapolated
further into the over-saturated zone, as they eventually show
degraded productivity as the expectation is set too high. For
many individuals, the overtime productivity curve will exhibit
a downward trend after a point. See [1] for these extensions
and refinements to the curve for selected industries.
C. Exhaustion and Burnout
Human nature dictates that the increased productivity
effects of overtime can only be temporary, because everyone
needs to “de-exhaust” at some point. This has been observed
across all industries. People start working a little harder with
some schedule pressure, but after several weeks fatigue sets in
and productivity drops dramatically. Then there is a recovery
period where people insert their own slack time until they’re
ready to work at a normal rate again.
The underlying assumptions of the exhaustion model from
[3] are:
x Workers increase their effective hours by decreasing
slack time or working overtime.
x The maximum shortage that can be handled varies.
x Workers are less willing to work hard if deadline
pressures persist for a long time.
x The overwork duration threshold increases or decreases
as people become more or less exhausted.
x The exhaustion level also increases with overwork.
x The multiplier for exhaustion level is 1 when people work
full 8 hour days, and goes over 1 with overtime. The

161

3
exhaustion increases at a greater rate in overtime mode up
to the maximum tolerable exhaustion.
x The exhaustion level slowly decreases when the threshold
is reached or deadline pressures stop with an exhaustion
depletion delay.
x During this time, workers don’t go into overwork mode
again until the exhaustion level is fully depleted.
A run of the exhaustion model with specific parameters
described in [1] shows increasing exhaustion up to a breaking
point. The actual fraction of man-days for the project rises as
the project falls behind schedule, which means that people
have less and less slack time during their day. The fraction
increases as the work rate adjustment kicks in. As the
exhaustion level accumulates, it affects the overwork duration
threshold such that the number of days people will work
overtime starts to decrease.
These general trends continue and productivity is increased
nearing the point of maximum exhaustion. When the
overwork duration threshold reaches zero the team cannot
continue at the same pace, so the de-exhausting cycle starts.
The actual fraction of man-days for the project and the
exhaustion level both slowly decrease.
The overwork
duration threshold begins to increase again, but a new
overwork cycle won’t start until the exhaustion level reach
zero. Alternative formulations for burnout dynamics are also
discussed in [1].
D. Learning
Learning is the act of acquiring skill or knowledge through
study, instruction or experience. In the context of a software
process, developers become more productive over the long
term due to their accumulated experience. The increase in
productivity occurs indefinitely, and a learning curve
describes the pattern of improvement over time.
A learning curve is expressed by an S-shaped productivity
function over time. The curve shows an initial period of slow
learning where the development process is being learned, then
a middle period of fast learning followed by a decreasing
slope portion that nearly levels out (the form of a typical Scurve). The reduced learning period is due to machine
limitations. Calibration of the curve for software development
is treated in [1].
The implications of learning while executing software
processes can be substantial when trying to plan effort and
staffing profiles. Unfortunately, few planning techniques used
in software development account for learning over the
duration of a project. The standard usage of COCOMO for
example assumes static values for experience factors on a
project.
At first glance, learning curves may be easily expressed as a
graph over time. But this simple depiction will only be
accurate when time correlates with cumulative output. As
Raccoon discusses [4], there are biases that mask learning
curves. There are also process disruptions for periods of time
that affect productivity.

Learning curves are traditionally formulated in terms of the
unit costs of production. The most widely used representation
for learning curves is called the Log-Linear learning curve
expressed by y=a(x)n, where a is the cost of the first unit, x is
the cumulative output, and n is the learning curve slope. The
learning curve formulas are implemented in system dynamics
feedback structures and discussed further in [1].
E. Communication
Good communication between people is necessary for
project success. Frequently it is cited as the most important
factor. Personnel capability factors inherently account for
team dynamics rather than assessing individual capabilities. If
a team doesn’t communicate well then it doesn’t matter how
smart and capable the constituents are. However, a great deal
of time is often consumed in communication.
As shown in a simple Brooks’ Law model [1], team size is
an important determinant of communication overhead and
associated delays. Communication overhead depends on the
number of people. The number of communication paths
increases proportionally to the (number of people)2. The
model shows that adding people increases communication
overhead and may slow the project down more. It takes more
effort and longer time to disseminate information on large
teams compared to small teams. This could have a profound
impact on project startup, since the vision takes much longer
to spread on a large team.
As the overall team size increases however, team
partitioning takes place and communication overhead is
reduced on a local scale. Partitioning is not trivial to model
and the communication overhead should be reformulated to
account for it [1].
F. Stakeholder Negotiation and Collaboration
Achieving early agreements between project stakeholders is
of tantamount importance. Reaching a common vision
between people and identifying their respective win
conditions are major precepts of the WinWin process [6].
Negotiation and collaboration processes can be difficult
though, and modeling the social aspects can provide insight.
A couple unique models have explored both the social and
organizational issues of requirements development.
An important work in this area is [7] that simulates the
organizational and social dynamics of a software requirements
development process. It looks at how the effectiveness of
people interactions affects the resulting quality and timeliness
of the output. The model has both continuous and discrete
modeling components.
While the modeling of the organizational processes uses a
discrete, event based approach, the social model was based on
continuous simulation. The social model gives each individual
three characteristics: their technical capability, their ability to
win other team members to their point of view (influence),
and the degree to which they are open to considering the ideas

162

4
of others (openness).
The model showed that the level of commitment or trust
escalates (or decreases) in the context of a software project.
Commitment towards a course of action does not happen all of
a sudden but builds over a period of time.
Another effort using system dynamics to model stakeholder
collaboration activities was performed in [8]. They simulated
the Easy WinWin requirements negotiation process to assess
issues associated with the social and behavioral aspects, and to
explore how the outcome is impacted. They revisited [7] and
retained the model for individual characteristics of technical
ability, ability to influence others and openness to influence
from others.
G. Macro Workforce Shortage
There is a seemingly ever-present shortage of skilled
software and IT personnel to meet market needs. Closing the
gap entails long-term solutions for university education,
ongoing skill development in industry and worker incentives.
Rubin studied the IT workforce shortage in the United States
[5] and mentions a destructive feedback loop whereby labor
shortages cause further deterioration in the market.
The degenerative feedback is a reinforcing process between
worker shortages, staff turnover and job retention.
Organizations try to alleviate the labor shortage by offering
salary increases to attract new hires. As more people job hop
for better compensation, the shortage problem is exacerbated
because more attrition and shorter job tenures cause lessened
productivity. Thus the market response adds fuel to the
destructive feedback loop. The increased compensation to
reduce the impact of attrition causes more attrition.
The feedback results in a greater need for people to develop
a fixed amount of software, in addition to the overall
increased demand for software development in the world
market. Considering the larger world picture illustrates
interactions between national and global economic trends.
A causal loop structure of the labor shortage feedback loop
has been created to describe the situation. The difference
between desired productivity and actual productivity (on a
local or aggregate level) manifests itself as a workforce
shortage. The shortage causes aggressive hiring such as
providing large starting salaries (or other hiring incentives) to
fill the gap. The increased compensation available to workers
causes higher attrition rates. The increased personnel turnover
causes learning overhead for new people to come up to speed
and training overhead on the part of existing employees to
train them. Both of these impact productivity negatively,
which further causes the workforce shortage.

of many processes and ultimate project success are dependent
on people working together.
Motivation is a key factor to achieve high productivity from
people. It can enable people to work very hard and be
creative. Yet people have limits on their amount of hard work
and long hours until burnout inevitably occurs. The right
balance has to be struck. Management needs to be very
careful about overtime and its converse slacktime.
Learning is an S-shaped phenomenon that can be
successfully modeled, yet plans usually don’t account for
individual and team learning curves. It is a very real
consideration that can be easily handled with system
dynamics.
Good communication between people is crucial, both
within a development team and with external stakeholders.
Individual capabilities aren’t as important as communicating
well as a team. Emphasis should be placed on creating an
environment for teams to gel. Team dynamics play a
particularly critical role when identifying and negotiating
stakeholder win conditions.
For external stakeholders,
expectations management is critical to make everyone a
winner. It won’t work without open, honest communication
and negotiation of win conditions to reach a shared vision
between people.
Workforce issues should be dealt with at the macro level
also. There are feedback dynamics that create and sustain a
shortage of skilled people. Without a long-term vision to
address the causes then the industry will continue to be
plagued with staffing problems. Modeling and simulation can
be used to help understand this and other workforce issues.
These are just some of the people phenomena that should be
better understood and accounted for on software projects. See
[1] for other applications (including using simulation for
personnel training), lessons learned, comparisons of different
modeling techniques for people considerations and directions
for future work.

REFERENCES
[1]
[2]

[3]
[4]
[5]
[6]
[7]

III. CONCLUSIONS
People are the single most important aspect to focus on to
improve productivity. More will be gained by concentrating
on people factors and human relations than technical methods.
In this sense agile approaches have it right. The effectiveness

[8]

163

R. Madachy, Software Process Dynamics, IEEE Computer Society
Press, Washington D.C., 2005 (to be published)
B. Boehm, C. Abts, W. Brown, S. Chulani, B. Clark, E. Horowitz, R.
Madachy, D. Reifer, B. Steece, Software Cost Estimation with
COCOMO II, Prentice-Hall, 2000
T. Abdel-Hamid, S. Madnick, Software Project Dynamics, Englewood
Cliffs, NJ, Prentice-Hall, 1991
L. Raccoon, “A learning curve primer for software engineers”, Software
Engineering Notes, ACM Sigsoft, January 1996
H. Rubin, “The United States IT Workforce Shortage (Version 3.0)”,
META Research Report, 1997
B. Boehm, A. Egyed, J. Kwan, D. Port, A. Shah, R. Madachy, “Using
the WinWin spiral model: A case study”, IEEE Computer, July 1998.
A. Christie, M. Staley, “Organizational and social simulation of a
software requirements development process”, Proceedings of ProSim
Workshop ‘99, Portland, OR, June 1999
F. Stallinger, P. Grünbacher, “System dynamics modelling and
simulation of collaborative requirements engineering”, Journal of
Systems and Software, December 2001

Goal-oriented Composition of Software Process Patterns
Jürgen Münch
Fraunhofer Institute for Experimental Software Engineering
Sauerwiesen 6, 67661 Kaiserslautern, Germany
Juergen.Muench@iese.fraunhofer.de
development (such as offshoring) has a growing impact
on software development processes. Besides these
aspects, process models need to be designed, described,
evolved, and introduced in a way that they are really used
in practice. This is a challanging task requiring
methodological, technological, and organizational support
for software developing organizations.
Two levels of abstraction can be distinguished: On the
one hand, generic processes capture project-independent
process knowledge that is relevant for certain application
domains (e.g., space software) and/or development
contexts (e.g., large projects). On the other hand, projectspecific process models describe activities for concrete
projects. Typically, project-specific models are derived
from generic models through tailoring.
Both generic models and project-specific models need
to be evolved: Process-specific models are often evolved
through refinement or replanning activities during the
course of the project. Generic processes can be evolved,
for instance, by including feedback from the execution of
projects that are in the scope of the generic model.
The tailoring of software process models to project
constraints requires an understanding of process
variations and knowledge about when to use which
variation. Typically, the following logical steps are
needed before the customization is performed [11]:

Abstract
The development of high-quality software or softwareintensive systems requires custom-tailored process
models that fit the organizational and project goals as
well as the development contexts. These models are a
necessary prerequisite for creating project plans that are
expected to fulfill business goals. Although project
planners require individual process models customtailored to their constraints, software or system
developing organizations also require generic processes
(i.e., reference processes) that capture projectindependent knowledge for similar development contexts.
The latter is emphazised by assessment approaches (such
as CMMI, SPICE) that require explicit process
descriptions in order to reach a certain capability or
maturity level. Among other concepts such as
polymorphism,
templates,
or
generator-based
descriptions, software process patterns are used to
describe generic process knowledge. Several approaches
for describing the architecture of process patterns have
already been published (e.g., [7]). However, there is a
lack of descriptions on how to compose process patterns
for a specific decelopment context in order to gain a
custom-tailored process model for a project. This paper
focuses on the composition of process patterns in a goaloriented way. First, the paper describes which
information a process pattern should contain so that it
can be used for systematic composition. Second, a
composition method is sketched. Afterwards, the results of
a proof-of-concept evaluation of the method are
described. Finally, the paper is summarized and open
research questions are sketched.

•
•

1. Introduction

•

Providing generic processes for organizations and
deriving custom-tailored software development processes
for projects is a challenging task: Typically, contexts and
technology are changing often, new state-of-the art
knowledge should be considered, conformance to
standards often needs to be guaranteed, experience from
past projects should be captured, and distributed

Possible process alternatives need to be elicited
and explicitly described.
Process alternatives need to be characterized and
constraints/rules on their use need to be
formulated. This requires a deeper understanding
of the appropriateness of the process alternatives
for different contexts and their effects in these
contexts.
Before the start of the project, a characterization
of the project context and its goal is necessary,
describing the information needed to select
process alternatives.

The approach described in this article is based on an
engineering-style development paradigm. This means that
planning is based on explicit experience from past

164

projects (i.e., models) and is done in a goal-oriented way.
The performance of development activities should adhere
to the plan and appropriate means should be used to
control and avoid plan deviations or to do replanning in a
systematic way. Finally, experience gained during project
execution should be analyzed and captured for future
projects. Due to the context-orientation of software
development (i.e., development activities are unique for a
specific context), models need to be customizable to
contexts and their scope of validity should be defined.
This means that the applicable contexts and the degree of
evaluation must be attached to a model.
Software process patterns can be seen as process
model fragments that capture relevant process
information for solving typical software or system
engineering problems in specific contexts. Patterns are a
reuse mechanism for software knowledge. The origins of
patterns lie in the building architecture domain [1]. In
software engineering, first patterns were used in the area
of software design [5]. Based on typical characteristics of
these domains, patterns seem to be appropriate for
capturing knowledge, at least knowledge about creative
activities. This could be seen as a hint that patterns are
also an appropriate means for capturing knowledge about
software development processes that typically also
include highly creative activities. Besides process
patterns, patterns are nowadays also used in other
software engineering areas such as requirements
engineering (e.g., requirements engineering patterns [8])
or organizational structuring (e.g., organizational patterns
[3][9]).
Applying software process patterns in the context of
project planning requires a set of patterns, a schema for
characterizing those patterns, an approach for selecting
appropriate patterns, an approach for composing the
selected patterns, and, finally, a technique for integrating
those patterns on the level of a process modeling notation.
This paper focuses on the composition. Approaches for
characterization and selection are described, for instance,
in [2],[6],[12]. An approach for integrating patterns (or
process fragments) on the level of a formal process
modeling language is described in [11].

Process
Combination

Process

Quality
Model

*

*

Process
Pattern
1
1
1

1
Process Pattern
Goal

Significance

Attribute

1

*
transforms

Scope
1
1

* characterizes*

1
Attribute
Transformation

*

1
1
Characterization
Vector

Figure 1. Process Pattern Architecture.
•
•

•

•

•

2. Process Patterns and Goals
We define a process pattern as a reusable fragment of
a process model or a combination of process models that
represents an activity or a set of activities, and that is
described together with the following information (see
Figure 1):

•

Characterization vector: a set of characterizing
attributes C and an assignment of values C(t) at a
certain point in time t.
Significance: information about the degree of
validation of the pattern (e.g., information about
the number of usages of the pattern in the context
defined by the characterization vector).
Process pattern goal: an atomic goal describes a
restriction on the attributes from the
characterization vector by using a < b, a <= b, a >
b, a >= b, a = b or a ∈ M. A complex goal is a
logic combination of atomic goals or complex
goals using the operators “&” (and), ”|” (or), “¬”
(negation), “⇒” (implication) and “⇔”
(equivalence). Process pattern goals can be used
to describe the goal that can be reached by a
pattern in a certain context. If the pattern is
composed of other patterns, the goal describes the
goal that can be reached by the composition. An
example goal is ((maximal defect rate < 0.8 %) &
(service level = high) & (test-effort = f(designcomplexity)))
Attribute
transformation:
An
attribute
transformation describes a change of attributes
that is caused by the application of a process
pattern. An example is reliability:=P reliability *
1.15, where p is the process pattern.
Quality models: models describing cause-effect
relations (e.g., a prediction model for test effort
based on design complexity) can be associated to
a pattern.
Process combination: Process patterns can be
combined from other process patterns using
composition operators (see below).

The description of pattern goals, attribute transformations
and quality models should ideally be based on empirical
evidence. Also, process simulation could be used to get
relevant knowledge about the effects of process patterns
in certain contexts.

165

start:
goal:

create
requirements
(StP)

eff = 0 & req_doc = incomplete
eff <= 700 & req_doc = verified

implies

squential combination
eff = 555 + 50 + 49 &
req_doc = verified
create
d-requirements
(OMT process)

create
traceability
matrix

eff_req = 50

eff = 654 &
req_doc = verified

perspectivebased
reading

eff_req = 49 &
req_doc = verified

parallel combination
eff_req = max { eff_e1, eff_e2} &
req_doc = complete
create class
diagrams

create
sequence
diagrams

eff_e1 = 555

eff_e2 = 405

eff _req = 555 &
req_doc = complete

Figure 2. Goal-oriented Composition of Process Patterns.

3. Process Pattern Composition
The starting point for a goal-oriented process pattern
composition are project goals that can be formulized
analogous to the process pattern goals. The attributes that
can be used in the goal definition are a subset of the
characterizing attributes of the process patterns and the
attributes describing the project context.
For the composition of process patterns, the following
composition operators have been defined: sequential
combination,
parallel
combination,
conditional
combination, and iterative combination. These operators
describe how process patterns can be composed and the
implications of the compositions on the attribute
mappings. The iterative combination can be defined
recursively by using the conditional combination:

2.

3.

while ( c ) t; ≡ if ( c ) t; while ( c ) t;
The overall composition method consists of the
following steps:
1. Composition of process patterns by replacement,
refinement, and augmentation. Based on an initial
process pattern, this step creates a combination of
process patterns. How process patterns can be
combined is constrained. Such constraints are, for
instance, that two process patterns can only be
combined if the same tool is used or that a logical
control flow sequence for a set of process patterns
is necessary because of necessary product flows.

Such constraints are described via so-called
composition networks that need to be established
for application domains (see [10] for further
details).
Verification of the process pattern combination by
using formal semantics. During this step, the
question is answered of whether the current
process pattern combination fulfills the project
goal (or sub-goals).
If the verification is successful, the formal process
descriptions of the process patterns need to be
integrated. The approach described in [11] and
[13], which also integrates the product flows
between the process patterns, could be used. This
step includes the transformation of processes from
the pattern level to the process instance level, i.e.,
the customization of the integrated process
patterns to the project environment. If the
verification is not successful, a new composition
needs to be chosen (i.e., step 1 starts again).

Verification and goal-oriented composition of process
patterns can now be defined in the following way: Let g
be a project goal and p a process pattern combination. Let
[p] denote the attribute transformation that is caused by
the process pattern combination p. Let s be the values of
all characterizing attributes at the beginning of the
project. Then we can define:

166

1.
2.

In addition, step 3 of the composition method has been
evaluated in several case studies. This is described in
detail in [10].

The verification of a goal is the proof that
[p](s) ⇒ g
The goal-oriented composition of process patterns
is the creation of a process pattern combination p
so that [p](s) ⇒ g

5. Conclusions and Future Research Topics

The details of the method are described in [10]. Figure
2 gives an example of the method: Here, the project goal
is the development of a verified requirements document
with an overall effort (eff) of less than 700 hours. The
starting conditions are that no effort has been consumed
and the requirements document is incomplete. Below the
patterns that are displayed using rectangles, the attribute
transformations are given. Subsequently, a parallel
combination and a sequential combination are applied and
the effects are calculated. Showing that the expression
“effort = 654 hours and requirements_document =
verified” implies the goal completes the planning and
demonstrates that a process combination has been found
that promises to be a good basis for a project plan (based
on the empirical evidence captured in the attribute
transformations of the process patterns). For simplicity
reasons, the attribute transformations in the example are
described on the instance level (i.e., effort is given in
absolute values). Typically, an additional mapping from
the process pattern level to the instance level is necessary.
For example, the effort needed for conducting a process
pattern could be given as a percentage value of the overall
effort, which is estimated during project planning. This
effort needed for conducting a pattern may also be
determined by a function on a value of the project
characterization vector.

This paper sketched a method for composing process
patterns in a goal-oriented way and showed initial
evaluation results. The application of the method has
shown that it is possible to provide mechanisms for
systematically applying and using process patterns during
project planning. Moreover, the method provides
mechanisms for verifying that a certain combination of
process patterns fulfills a project goal in a specific
context. It should be mentioned that as with all models,
the quality of the results heavily depends on the validity
(i.e., mainly the empirical basis) of the models.
Challenges for future research in this area are, for
instance,
•
•
•
•
•

4. Evaluation

the development of a graphical representation for
process pattern combinations (especially
mechanisms to visualize variability),
automated support for verifying process pattern
combinations against goals,
gaining experience about appropriate granularity
levels for process descriptions,
generating process documentations from process
pattern combinations,
integrating patterns for software development
activities and patterns/processes for system
development activities (e.g., development
activities in mechanical engineering).

Acknowledgement

The method has been applied in a case study that
focused on the planning of a large development effort for
an embedded automation system. 42 process patterns
were defined and applied. The goal of the evaluation was
to qualitatively show the applicability of the method and
to get feedback on how to improve the method. For the
project two alternative development tools (Rapsody and
StP) were considered that influenced many development
activities. Therefore, two clusters of process pattern
combinations were built. In the first cluster, 32 process
pattern combinations needed to be assessed with respect
to their suitability to fulfill the goal. In the second cluster,
15 alternatives needed to be assessed. In each cluster, less
than 5 verification steps were necessary. Several
combinations have been proven to be suitable. It was
possible to choose among the results depending on
priorities (e.g., minimize planned effort, optimize defect
density rate). The project was conducted according to the
plan using more than 20 students. The monitoring of the
project data showed high plan adherence (based on
qualitative judgement).

The author would like to thank Jens Heidrich from the
Fraunhofer Institute for Experimental Software
Engineering (IESE) who contributed to the
implementation of the method, and Sonnhild Namingha
from the Fraunhofer Institute for Experimental Software
Engineering (IESE) for reviewing the first version of the
article.

References
[1] Linda C. Alexander and Alan M. Davis. Criteria for
Selecting Software Process Models. In Proceedings of the
15th Annual International Computer Software &
Applications Conference, pp. 521-528. IEEE Computer
Society Press, Tokyo, 1991.
[2] Ralf Bergmann, Hector Muñoz-Avila, Manuela
Veloso and E. Melis. Case-based Reasoning applied to
Planning. In M. Lenz, B. Bartsch-Spörl, H.D. Burkhard

167

Montenegro, Ralf Kneuper, eds., Proceedings of the 4th
workshops of the section 5.1.1 (GI), Berlin, March 1997

and S. Wess, Eds., Case-Based Reasoning Technology:
From Foundations to Applications. Lecture Notes in
Artificial Intelligence 1400, Springer Verlag, 1998.
[3] James O. Coplien. A Development Process Generative
Pattern Language, chapter 13, pages 183--237. AddisonWesley, Reading, MA, 1995.
[4] Khaled El Emam, Nazim H. Mahavji and K.
Toubache. Empirically Driven Improvement of Generic
Process Models. In Proceedings of the 8th International
Software Process Workshop, 1993.
[5] Erich Gamma, Richard Helm, Ralph Johnson, John
Vlissides. Design Patterns – Elements of Reusable
Object-Oriented Software. Addison Wesley, 1995.
[6] Soeli T. Fiorini, Julio Cesar Sampaio do Prado Leite
and Carlos José Pereira de Lucena. Reusing Process
Patterns. 2nd International Workshop on Learning
Software Organizations (LSO), 2000.
[7] Mariele Hagen and Volker Gruhn, Process Patterns –
a Means to Describe Processes in a Flexible Way,
Proceedings of the 5th International Workshop on
Software Process Simulation and Modeling (ProSim
2004), Edinburgh, Scotland, United Kingdom, pp.50-56,
May 24-25, 2004.
[8] Kathrin Lappe et al., Requirements Engineering
Patterns – An Approach to Capturing and Exchanging RE
Experience. Final Report from the WGREP, DESY 2004,
Hamburg, Germany.
[9] M. Lipshutz, R. Creps und M. Simos. Organizational
domain modeling (ODM) tutorial, January 1997.
[10] Jürgen Münch. Muster-Basierte Erstellung von
Software-Projektplänen (in German). PhD Theses in
Experimental Software Engineering, Vol. 10, ISBN: 38167-6207-7, Fraunhofer IRB Verlag, 2002.
[11] Jürgen Münch. Transformation-based Creation of
Custom-tailored Software Process Models. Proceedings
of the 5th International Workshop on Software Process
Simulation and Modeling (ProSim 2004), Edinburgh,
Scotland, United Kingdom, pp.50-56, May 24-25, 2004.
[12] Rubén Prieto-Díaz und Peter Freeman. Classifying
Software for Reusability. IEEE Software, 4(1): 6-16,
IEEE Computer Society, January 1987.
[13] Markus Schmitz, Jürgen Münch und Martin Verlage.
Tailoring of large process models on the basis of MVP-L
(in German). In Günther Müller-Luschnat, Sergio

168

1

Towards an Interactive Simulator for Software
Process Management under Uncertainty
Thomas Birkhoelzer, Christoph Dickmann, Juergen Vaupel, Joerg Stubenrauch


Abstract—The management of a software producing
organization can be considered as controlling a complex system.
However, the dynamics and the parameters of this system are
often unknown, or just vaguely known in the best case.
Nevertheless, managers need to operate and decide in their daily
practice using “uncertain” or “not validated” information.
Therefore, a simulator is developed in this work, which
includes three essential building blocks to simulate an
environment of such uncertainties: 1. Parameters with random
deviations; 2. Time-varying parameters causing structural
different behavior; 3. Dynamics such that short-term effects are
opposite to long-term effects.
The purpose of such a simulator is to raise the awareness and
understanding of decisions and their associated effects, like the
investment in a specific process area and the effect on the
software development organization.
Index Terms—Capability maturity model, software process
simulation, uncertain process information.

I.

INTRODUCTION

A. Intention
ROM a system theoretic point of view, the management of
a software producing organization can be described as the
task of controlling the system of a “software producing
organization”. Such a system is governed by a set of internal
states (capabilities) and produces a set of outputs (business
measures). Management control means to balance these
outputs to achieve the business goals.
Process simulation and process simulators are used as tools
to gain understanding and knowledge of such systems by tests
and experiments in a virtual form [4], e.g. to evaluate
strategies [3], [5], [14], to compare process alternatives [6],
[12], [15], or for teaching and training [7], [10], [11], [13].
Software development management decisions can be more
solid if based on a system of quantitative measures. In reality,

F

Manuscript received February 3, 2005.
T. Birkhoelzer is with the Department of Electrical Engineering and Information Technology, University of Applied Science, Konstanz, Germany
(phone: +49 7531 206239; e-mail: birkhoelzer@fh-konstanz.de).
C. Dickmann is with Siemens Medical Solutions, Erlangen, Germany
(e-mail: christoph.dickmann@siemens.com).
J. Vaupel is with Siemens Medical Solutions, Erlangen, Germany
(e-mail: juergen.vaupel@siemens.com).
J. Stubenrauch is with the Department of Electrical Engineering and Information Technology, University of Applied Science, Konstanz, Germany
(e-mail: joerg.stubenrauch@gmx.de).

a mix of qualitative and quantitative business indicators can be
assumed to serve as a decision basis. However, the
quantitative analysis of software processes, which is necessary
in the context of such simulations, often reveals that there is
only very limited real-world quantitative information available
to build or verify the underlying models. Often, a lot of the
parameters in the simulation models need to be estimated or
derived indirectly. Validation of the parameters or the models
in general is often mentioned as one of the difficult or missing
tasks [8], [12]. Most often, this is not due to methodological
difficulties but due to missing data.
Yet, this uncertainty is not a flaw of the simulation
approach – although sometimes discovered in this context –
but a characteristic of software development management in
general: Software managers make decisions about processes
even without precise knowledge about the quantitative
mechanisms and effects. Comprehensive quantitative process
information is part of the maturity level four in CMM and
CMMI (Capability Maturity Model Integration), and only
about 20% of the reporting organizations have reached that
level [16]. This means that 80% of organizations operate
without such information being used in a comprehensive,
planned and sophisticated manner!
Moreover, due to the variability of software tasks and
constantly changing development conditions, it seems almost
inevitable to operate under a certain level of uncertainty: Even
the most precise data gathered for one project or time period
might not be valid for the next one. Furthermore, it normally
needs a longer learning period to build up the expertise that
allows for a qualification of the quantitative project measures
for a specific organization. This process of calibration for the
quantitative project measures is specific for each individual
organization and can therefore not be shared between different
organizations.
Therefore, this work presents a simulation approach that is
designed to mimic such an uncertain and changing
environment. Such a simulator is intended to raise the user’s
awareness for these effects, the underlying mechanisms and
their results. It can be used for training or teaching of current
or future stakeholders in software production, and help to
build up confidence in decisions to be taken, e.g. by
simulating best case and worst case scenarios.
B. Background
In [11], an interactive simulator was presented, which
allows a user to act in the role of a manager of a software

169

2
producing organization. The manager’s task is to best meet
strategic business goals by iteratively investing in and
improving software development processes. The simulation
model’s internal states represent capability levels of CMMI
key process areas [9]. The inputs in the model are abstract
“investments in key process areas” covering financial as well
as non-financial efforts. One investment point as abstract
fiscal unit is the effort necessary to raise the capability level of
the associated key process area by one level. The model’s
outputs are balanced scorecard [2] business measures that
provide the basis for the next process improvement investment
decision.
The resulting model is a non-linear, time-discrete, statespace system with x t
x1, t , , x n, t denoting the n-tuple of



u

the internal state variables, u t

input variables, and yt



, , u l , t



the l-tuple of



(1)

u i ,t  ¦ E ij  x j ,t  ¦ J ij  gate( x lij , t ,Q ij , P ij )  x j , t

(2)

¦D

(3)

j

y i ,t

1, t

y1, t , , ym, t  the m-tuple of output

variables:
x i , t 1 norm O i  x i , t  (O i  1)  s i ,t
s i ,t



j

ij

j

 x j ,t  ¦ F ij  gate( x lij ,t , V ij , U ij )  x j ,t
j

with
norm(arg )

­ 5 for
°
®arg for
° 0 for
¯

gate(arg ,Q , P )

arg ! 5
0 d arg d 5
arg  0

1
1 e

(4)

(5)

Q ( arg  P )

Equations (1) and (2) are separated to make the underlying
structure transparent: the term si ,t , consisting of the
investment input and the feedback from other states is the
driving force of the first order dynamics of (1). The
parameter Oi , with 0  Oi  1 , models the time constant of the
dynamics of the i-th process area, representing the “ease and
speed” of process improvements in this process area. For
values of Oi close to zero, the state reacts fast on changes

linear influence is expressed by the parameter E ij . The term

J ij  gate( x lij , t ,Q ij , P ij ) is used to model that the influence of
some process areas depends of a certain capability level of
another process area as a prerequisite. In CMMI, this is the
background of the assignment of process areas to maturity
levels within the staged representation: the process areas of
the lower maturity levels are prerequisites of the process areas
of the higher maturity level. The gating function
gate(arg ,Q , P ) defined by (5) translates this concept of a
prerequisite in a continuous mathematical form: for small
values of arg , the function value is close to zero, for large
values of arg , the function is close to one, for arg P the
function value is 0.5. Therefore, the parameters P ij express
the “switching values”, the parameters Q ij the slope of the
transitions.
Equation (3) has the same structure as (2) where D ij
denotes the strength of a linear influence and F ij of a nonlinear, gated one, of the j-th process area on the i-th business
measure. For modeling simplicity, all business measures are
additionally normalized to the range zero to one and such that
the value zero denotes the worst and the value one the best
case (e.g. business measure Malfunction = 1 represents “no
malfunctions at all”).
A difference equation as described by (1) needs initial
conditions. These describe the status of the organization at
simulation start. By specifying appropriate initial conditions,
different organizational starting points can be used for the
interactive simulation task, e.g. a rather mature organization or
a start-up organization with no capabilities at all.
In this form, the mathematical model is a strong abstraction
and simplification of real-world mechanisms. On purpose, it
reflects essential properties of the dynamics of a software
producing organization only, and does not model “real world”
mechanisms directly.
The model is implemented as an interactive simulator, i.e. at
each time step, the user can allocate the available budget
(which is derived based on the actual business measures) to
investments in key process areas during the next time period.

of si ,t . The closer the value of Oi is to 1, the slower is the
reaction. A parameter value Oi

0 would mean “no delay” or

“no dynamic” respectively. For Oi ! 1 the system would be
unstable, in the case 1  Oi  0 the state would oscillate.
In CMMI, the capability level of each process area is
characterized by a value from zero to five:, zero is the smallest
possible value (non-existing or incomplete respectively), five
the maximal one (optimizing or perfect). Therefore, the state
values xi , t are restricted as well by the range from zero to five
by the function norm( ) defined by (4).
In (2) the parameters E ij and J ij serve as weights for the
influence of the j-th process area on the i-th process area. A

170

Time-discrete
Simulation
Model

ut

yt

One cycle

Investments into CMMI
key process areas

Status shown by a
balanced scorecard

„Player“
Fig. 1. Interactive Simulator

3
II. BUILDING BLOCKS TO MIMIC UNCERTAINTY

E ij( actual )

A system as described in Section I.B and [11], with simple
first order time dependencies and few time invariant internal
feedbacks, i.e. constant dependencies between internal states,
is relatively “easy” to control, i.e. to manage. The direction of
effects (positive or negative) of decisions and strategies are
immediately visible in the outputs and do not change during
the simulation. Therefore, strategies can easily be probed,
evaluated and eventually corrected in few trials or time steps.
However, such a model does not include fundamental realworld behavior and thus may simplify more than is desired
even for theoretical or didactic reasons. There are at least
three effects, which add substantial realistic variability on
model effects.
A. Random parameter deviations
The model parameters are used to adapt the model to a
specific organization’s conditions. If sufficient data would
exist, this would be a standard parameter identification
problem. However, different organizations have different
parameter sets.
For a manager, who joins a new organization, chances are
good that this organization differs more or less from her or his
previous experiences. Thus, the parameters of the new
organization are different from his previous experiences and
difficult to estimate.
Such organizational uncertainties of software management
are mimicked in the model by random parameter deviations.
This is a known, standard modeling technique. The range of
deviation can be specified for each parameter. Each time a
new simulation round is started, a new parameter set is
chosen. This forces the user to watch closely the evolvement
of the model (or the organization respectively) and not to just
blindly rely on past recipes.
B. Time-varying parameters
Unlike an assembly line, a software producing organization
is not a time invariant system. The parameters of the software
production vary considerably between projects, each new
technology or methodology introduced will change some or
all of the process parameters. Such changes over time are
assumed to be monotonous for a certain time period: For
instance, introducing a more effective teamwork or workflow
environment will improve the support between processes. The
gradual and monotonous character of this change is due to the
gradual adoption process.
Thus, software management has to be aware that, even
within the same organization, data or experiences collected in
the past might not be valid in the future.
Therefore, the second building block to mimic the
uncertainties of software management is the introduction of
time-varying parameters.
Mathematically, this is modeled by a linear transition
between two sets of parameters:

ft

f t  E ij( Set1)  (1  f t )  E ij( Set 2)

f t 1  q t  'f

(6)

Moreover, f t is limited to the interval [0,1] . The parameter

sets can be specified in the model file, the transition speed 'f
modeling the speed of change can be configured in the
simulator. The transition can be turned on or off at any point
during the course of the simulation by the switch q t  ^0,1` .
C. Uncertain short–term dynamic effects
In (1), the model state dynamics are described by a first
order difference equation. The step response (i.e. the time
response after a step increase in the input) of such a first order
difference equation with si ,t regarded as input is shown in

Fig. 2. (Note that this is a simplification, since si ,t is usually
not constant due to state dependencies.)
Process improvements, however, can show a structural
different behavior in which the short-term dynamic effects of
decisions are opposite to the long-term effects. Consider the
example, where an investment, i.e. change, in a key process
area “disturbs” the routine in the short-term causing the
maturity to decline first. Only after a while, the expected and
intended positive effect (i.e. investment raises the capability
level) becomes visible. The most well-known example of such
behavior in Software Engineering is Brooks’ Law: “Adding
manpower to a late project makes it later” [1]. Due to similar
reasons as mentioned in [1] (change consumes resources), it is
expected that most of the complex processes or activities show
such a behavior.
As an additional effect, processes can slightly deteriorate in
the long-term compared to their maximal performance. This
might be accredited to the build-up of inflexibility and
bureaucracy.
Fig. 3 shows the step response of a system with these
properties, i.e. an initial undershoot followed by a slight
overshoot.
Such dynamics can be modeled by a third order difference
equation, i.e. equation (1) is replaced by the following
dynamics:
x i , t 1 norm(Oi , 0  x i , t  O i ,1  x i , t 1  Oi , 2  x i , t  2
(7)
 N i , 0  s i ,t  N i ,1  s i ,t 1  N i , 2  s i ,t  2 )
In case of such a dynamic, it is necessary to stick to a
strategy (an investment) even if it looks like a wrong strategy
during the initial steps due to the initial decrease (the
undershoot). However, once understood, this would be as easy
to cope with as the simple first order behavior: just assume the
opposite short-time effect for assessing a strategy, i.e. a
strategy would only be good, if it results in a short term
decrease.

171

4
Development, Technical Solution, and Organizational Process
Focus. These process areas highly depend on specific
knowledge and highly skilled persons. Therefore,
improvements require substantial resources from within the
process causing the initial decline in the step response. All
other process areas are modeled by first order difference
equations.

2,5

capability level

2
1,5

x
s

1
0,5
0
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55
time steps

Fig.2. Step response of a first order difference equation,

Oi

0.89 .

2,5

capability level

2
1,5

x
s

1
0,5
0
1 4 7 10 13 16 19 22 25 28 31 34 37 40 43 46 49 52 55
time steps

Fig. 3. Step response of a third order difference equation,

Oi 0
N i0

2.63, Oi1
0.15, N i1

2.3 Oi 2
0.327, N i 2

0.669,
0.176 .

Real world settings are more complicated due to a mix of
these two types of behavior. Some process area have a step
response as shown in Fig. 2, others as shown in Fig. 3. Then it
is more difficult to find adequate mitigation strategies because
it may not be clear whether a strategy is right despite initial
negative effects, or whether it is wrong for the whole decision
period.
Therefore, the third building block to mimic the
uncertainties of software management is the extension of the
dynamical model such that some process areas have a step
response similar to Fig. 2 and others similar to Fig. 3.

A. Investment in Requirements Development
The first scenario simulates an organization that has about
reached maturity level three. However, Requirements
Development has been neglected so far, i.e. there were no
direct investments in this process area yet. Note that the
process area has nevertheless reached a capability level of two
due to the support of other process areas.
As a next step in the build-up of the organization, the
customer orientation should be improved. Therefore,
investments in Requirements Development are set to three.
The result of this strategy after 10 time steps is shown in
Fig. 4. The perspectives Innovation (left, lower graph) and
Customers (middle, lower graph) show a characteristic ditch,
which is due to the evolution of Requirements Development
as shown in Fig. 5: first a drop in performance and then a
gradual improvement. This means, during the first time steps
(values in the middle of the chart graphs), the investment in
Requirements Development seemed to be a wrong strategy,
since the respective measures slumped initially (see Fig. 4).
Only by sticking to the strategy, the investments finally pay
off.

III. EXEMPLARY RESULTS
Below, the effects of time-varying parameters and higher
order dynamics as discussed in Section II.B and II.C are
demonstrated in two exemplary scenarios. The random
parameter deviations introduced in Section II.A primarily
serve to vary the task between simulation runs, not within one
simulation of the same scenario. Therefore, they are omitted
here.
For both scenarios, the structure of the simulation model
presented in [11] is used. It consists of 15 process areas
deduced from [9] and 27 business measures arranged in six
perspectives of a balanced scorecard [2]. Only the summary
scores of the six perspectives are shown here in order to avoid
a detail overload.
Three process areas are modeled by a third order difference
equation with parameters according to Fig. 3: Requirements

172

Fig. 4. Step response of a balanced scorecard summary view.

5
Section II.B. To exaggerate this effect for the presentation, the
parameters are changed in one time step only, i.e. 'f 1
in (6).
After this change, the management strategy, which was
previously successful, is not sustainable anymore: It results in
a small, constant decline, see Fig. 6. Without
countermeasures, the budget will be consumed and the
organization may go bankrupt.
To respond to these variations, the management changes its
strategy after 20 time steps: the investments in Organizational
Process Focus are increased to three and the investments in
Integrated Teaming decreased to three. With this new strategy,
another 30 time steps are simulated. Fig. 6 shows the complete
course after 50 time steps. The organization finally recovers,
but starts at a lower business level again. This is due to the
fact that the build-up of Organizational Process Focus has
considerable time delay even exhibiting an initial drop in the
capability level, see above.

Fig. 5. Step response of process area Requirements Development

Parameter
change

Strategy
change

Parameter
change

Strategy
change

Parameter
change

Strategy
change

IV. CONCLUSION

Parameter
change

Strategy
change

Parameter
change

Strategy
change

Parameter
change

Strategy
change

Fig. 6. Business measures of an evolving company.

B. From teams to organizations
The second scenario considers an organization that is a
small and highly innovative company. In such an
environment, Integrated Teaming (e.g. agile development and
pair programming) is a key success factor for quickly reacting
to changes in requirements and technical approaches. On the
other hand, improvements in the process area Organizational
Process Focus will cause less improvement in other process
areas in such a small organization than in a larger
organization.
Therefore, a management strategy with high investments in
Integrated Teaming (five investment points) and low
investments in Organizational Process Focus (one investment
point) is successful and stable, which is reflected in the
simulation by stable states and a stable budget.
Over time, such a business evolves towards a bigger
organization, mature markets, and legacy products. For such
an organization the relative weight (significance) of Integrated
Teaming decreases and the weight of Organizational Process
Focus increases. This is represented in the simulation by
appropriate changes of the parameters in the model.
Typically, this is a gradual change over many time steps,
i.e. a typically application of the mechanisms described in

In this work, extensions of a model for simulating overall
software development in a whole organization are discussed,
which are intended to address some of the real world
complexity of software management caused by the inherent
uncertainty of the effects of decisions. These extensions were
designed to mimic the following management problems:
x Random-organizational deviations make it difficult to rely
on management “recipes” that were valid in the past.
x Time-varying development conditions make it challenging
to extrapolate from past results for estimating future effects,
even within the same organization.
x Different short-term adaptations and behaviors (caused by
higher order dynamics) make it challenging to probe and
evaluate strategies on the basis of their current or short-term
effects only.
The intention of the simulator is to simulate the potential
effects in order to help understanding potential mitigation
conditions, not to propose a solution or answer to these
challenges but.
The results presented in Section IV indicate that the
simulator can be used as a tool to raise and sharpen the
awareness of the complexity of dependencies and variations
over time in software development organizations.
REFERENCES
[1]
[2]
[3]

[4]

[5]

173

F. Brooks, The Mythical Man Month, Addison-Wesley, 1995.
R. S. Kaplan and D. P. Norton, The Balanced Scorecard, President and
Fellows of Harvard College, 1996.
C. Lin, T. Abdel-Hamid, and J. Sherif, “Software-Engineering Process
Simulation Model (SEPS)”, Journal of Systems and Software 38,
Elsevier, New York, 1997, pp. 263-277.
M. L. Kellner, R. J. Madachy, and D. M. Raffo, “Software Process
Modeling and Simulation: Why? What? How?”, Journal of Systems and
Software 46, Elsevier, New York, 1999, pp. 91-105.
J. Williford and A. Chang, “Modeling the FedEx IT Division: A System
Dynamics Approach to Strategic IT Planning”, Journal of Systems and
Software 46, Elsevier, New York, 1999, pp. 203-211.

6
[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

D. M. Raffo and M. I. Kellner, “Modeling Software Processes
Quantitatively and Evaluating the Performance of Process Alternatives”,
in Elements of Software Process Assessment and Improvement, editors
K. E. Emam and N. Madhavji, IEEE Computer Society Press, Los
Alamitos, California, 1999, pp. 297 -341.
A. Drappa and J. Ludewig, “Simulation in Software Engineering
Training”, Proceedings of the 22nd International Conference on
Software Engineering, ICSE, Limerick, Ireland, June 2000, pp. 199-208.
D. M. Raffo and M. I. Kellner, “Empirical Analysis in Software Process
Simulation Modeling”, Journal of Systems and Software 53, Elsevier,
New York, 2000, pp. 31-41.
CMMI Product Team, “Capability Maturity Model Integration (CMMISE/SW/IPPD, V1.1), Continuous Representation”, Software Engineering
Institute, Pittsburgh, 2001.
D. Pfahl, M. Klemm, and G. Ruhe, “A CBT Module with Integrated
Simulation Component for Software Project Management Education and
Training”, Journal of Systems and Software 59, Elsevier, New York,
2001, pp. 283-298.
Th. Birkhölzer, L. Dantas, C. Dickmann, J. Vaupel, „Interactive
Simulation of Software Producing Organization's Operations based on
Concepts of CMMI and Balanced Scorecards“, Proceedings of the 5th
International Workshop on Software Process Simulation and Modeling
(ProSim 2004), Edinburgh, May 2004, pp. 123-132.
D. Pfahl, M. Stupperich, and T. Krivobokova, “PL-SIM: A Generic
Simulation Model for Studying Strategic SPI in the Automotive
Industry”, Proceedings of the 5th International Workshop on Software
Process Simulation and Modeling (ProSim 2004), Edinburgh, May
2004, pp. 149-158.
E. Oh Navarro and A. van der Hoek, “Software Process Modeling for an
Interactive, Graphical, Educational Software Engineering Simulation
Game”, Proceedings of the 5th International Workshop on Software
Process Simulation and Modeling (ProSim 2004), Edinburgh, May
2004, pp. 171-176.
F. Padberg, “A Comprehensive Simulation Study on Optimal Scheduling
for Software Projects” , Proceedings of the 5th International Workshop
on Software Process Simulation and Modeling (ProSim 2004),
Edinburgh, May 2004, pp. 177-185.
D. Raffo, U. Nayak, S. Setamanit, P. Sullivan, W. Wakeland, “Using
Software Process Simulation to Assess the Impact of IV&V Activities”,
Proceedings of the 5th International Workshop on Software Process
Simulation and Modeling (ProSim 2004), Edinburgh, May 2004,
pp.197-205.
CMM Appraisal Program, “Process Maturity Profile; Software CMM
2004 Mid-Year Update”, Available:
http://www.sei.cmu.edu/sema/profile_SW-CMM.html, 8/2004.

174

1

Effective Resource Allocation for Process
Simulation: A Position Paper
Mohammad S. Raunak and Leon J. Osterweil
University of Massachusetts at Amherst
{raunak, ljo}@cs.umass.edu


Abstract: We often simulate processes to be able to reason
about, forecast, and plan the best utilization of available
resources. As process programmers, we define resources to be
the agents that carry out tasks, and the tools and other entities
required by agents in order for them to be able to complete their
assigned work. Specifying these resources rigorously and
allocating them efficiently during process simulation or execution
is a non trivial problem. In this paper, we present many hard and
interesting issues related to resource management and propose
some solution approaches. In particular, we talk about an
auction based solution approach, which we feel fits well in
different types of process simulation.
Index Terms— allocation, process, resource, simulation

I. INTRODUCTION

process and supervising its execution. However, we have
always maintained that a clear, precise and complete
definition of the resources required by the process, and
flexible mechanisms for their allocation, are no less important
for process analysis and improvement. We argued the
importance of a separate resource manager and laid out a
modeling approach in earlier works [rodion99, lerner00]. Over
the last few years our experience with process modeling,
analysis, and execution have grown, drawing from our work
with processes in such diverse domains as digital government,
e-commerce, and medical services, as well as with software
process. These experiences have provided insights into some
more challenging issues and have pointed us toward some
promising approaches to address the challenges. In this paper,
we present many hard and interesting issues related to
resource management and propose some solution approaches.

O

ne of the primary reasons to execute or simulate
processes (and many other types of software) is to be
able to reason about, forecast, and plan, the best
utilization of available resources [kellner99]. Managing
resources well translates into better process management,
which in turn, ensures better quality in the final products or
services resulting from a process. From our perspective as
process programmers, we define resources to be the agents
that carry out tasks, and the tools and other entities required
by agents in order for them to be able to complete their
assigned work. Specifying these resources rigorously and
allocating them efficiently during process simulation or
execution is a non trivial problem.
Our previous work in process programming has focused on
specifying the coordination of activities, their execution
semantics and artifact flows [wise98, cass99, cass00]. All
these issues are undeniably crucial for properly capturing a
This research was partially supported by the Air Force Research
Laboratory/IFTD and the Defense Advanced Research Projects Agency under
Contract F30602-97-2-0032, by the U.S. Department of Defense/Army and
the Defense Advance Research Projects Agency under Contract DAAH01-00C-R231, and by the National Science Foundation under Grant No. CCR0204321. The U.S. Government is authorized to reproduce and distribute
reprints for Governmental purposes notwithstanding any copyright annotation
thereon. The views and conclusions contained herein are those of the authors
and should not be interpreted as necessarily representing the official policies
or endorsements, either expressed or implied of the Defense Advanced
Research Projects Agency, the Air Force Research Laboratory/IFTD, the U.S.
Dept. of Defense, the U. S. Army, The National Science Foundation, or the
U.S. Government
.

II. RESOURCE MANAGEMENT CHALLENGES
In our work we define a process as (largely) hierarchical
structure of tasks, called steps. Each step is to be executed by
an agent, whose characteristics are specified as part of the
process definition, and a set of auxiliary resources, also
specified as part of the process definition [cass00, wise98].
In our approach, the execution of such a process definition
entails the binding of a specific agent, and specific resources,
identified by means of a search through a resource repository,
to each process step, as it comes up for execution. An
important premise of our work is that resources, and indeed
agents, can be either humans or automated devices (either
hardware or software). Thus our process definitions can
rightly be viewed as specifications of how humans and
automated devices are to be coordinated. This view
particularly emphasizes the importance of effective
identification of the agents and resources to be selected for
binding to the various steps. Invariably the resources that are
available are in short supply and are the subjects of
contention. Thus, potential delays, bottlenecks, and resource
starvation can presumably be avoided or reduced by using
resource analysis to identify ways in which resource
contention can be alleviated (e.g. by causing tasks to execute
in parallel or by skillful sharing of resources).
Another way to achieve better process performance is to
schedule the resources based on some allocation strategies.
Researchers in operating systems, multi agent systems and

175

2
operations research have long looked at different allocation
algorithms for better utilization of resources in processes.
However, the challenges we face in software and a lot of other
processes are unique due to the diverse nature of resources we
need to deal with here. Resources like processor time, network
bandwidth or completely automated agents have strictly
predictable behavior that is relatively less complex to schedule
and reason about. The simultaneous presence of humans and
automated agents, and the potential diversity in the time
granularity of the required resources, make it inherently much
more difficult to specify them properly and allocate them
effectively with a coherent scheduling scheme.
There are three very important and intricately
interconnected tasks in the realm of resource management:
modeling the resources, identifying and retrieving the
resources and finally, scheduling the resources. The modeling
task usually takes place before simulation, potentially in
parallel with the process definition work. The modeling or
specification of resources, to a large extent, is dependent on
the domain for which a process is being programmed. In
software processes, for example, we have human agents like
programmers, designers, testers as well as non-human
resources like compilers, development environments, software
licenses etc. On the other hand, a medical emergency room
process includes resources like nurses, doctors, orderlies,
beds, medicines, and a whole variety of equipments and tools.
It is quite challenging to provide a common structure and
service architecture that would allow a process programmer to
model and allocate resources from these diverse domains
coherently.
Identifying and retrieving the resources require capabilities
to store resource information persistently and to be able to
query them with some rich query language. Specifying and
storing both resource types and instances adequately is not
always straight forward. The resource manager architecture
needs to be flexible enough to support processes from diverse
domains that deal with a wide ranging resource types.
Scheduling of resources is primarily required during run
time. The modeling and scheduling capabilities that we have
previously proposed in [rodion99] have proven effective in
programming processes with comparatively less complex
resource variety and requirements. Our previous approach
worked by statically defining the resource model and letting
process steps query that static model. However, we have
discovered that agents and resources often have dynamic
attributes which may change in real time during the execution
of the process. For example, in an emergency department
process of a hospital, the nurses (one type of agent/resource)
are frequently changing locations and any query based on the
propinquity of a resource requires the resource manager to
accommodate dynamically changing resources states.
Software, medical, and other processes require complex
specification of composite resources like design teams or
nursing pools which include instances of individual agents,
each of which might potentially need to be allocated to several
different tasks. While allocation of such resources can be done
by modeling capacity of these resources and to allow them to
be assigned to multiple tasks in parallel, a more accurate
model of such partial allocation would require time based

allocation of the agents to each task. In this case, an agent will
not be bound to a process step indefinitely; it will be acquired
by the step for a specific period of time.
Often we come across process programs where resources
are sought in some preferential order. For example, there may
be a designated agent for a task; however, other agents with
some particular skill can carry out the task in absence of the
designated agent. The process definition somehow needs to
specify the preferred resource in addition to specifying all the
attributes properly that would allow selection of substitute
resources that can fill in.
Our experiences with processes and resources have shown
that there are process definitions that require the execution to
block in case a resource is busy. Allowing such blocking calls
to the resource server during an execution or simulation of a
process gives rise to the whole dimension of potential
deadlocks and starvations.
An important requirement of resource management is the
ability to reserve resources for planning purposes. This service
requires look-ahead capabilities on the process execution
paths. As process programmers, we often need to deal with
complex processes with abundant exceptions. Such control
flow structures make it very difficult to predict future resource
requirements based on which proper reservation can be made.
A related requirement for a resource manager is to be able to
identify the level of resource redundancy required to ensure
safe completion of processes in case of a sudden surge in
activities.
Our ongoing work with medical processes has strongly
demonstrated the need for resource preemption capability in
the resource manager. Resources like doctors, nurses or even
hospital beds get preempted and assigned to a new instance of
a task with higher priority in an emergency department
process. We are convinced that such cases of higher priority
work preempting resources from a running low priority task is
not uncommon in software processes either. From the process
simulation or execution perspective, this brings in a whole
new level of complexity.

III. SOLUTION APPROACHES
In [lerner00], we described a resource model based on
resource classes, resource instances, their attributes and
relationships amongst them. Our subsequent experience with
process simulation work has shown that not all the elements of
that proposed framework are necessarily effective for resource
modeling and allocation in simulations. We thus propose a
new approach to resource modeling where entities are
primarily categorized as agents and non-agent resources. Our
contention is that the association between a process step and
its required resources is driven by constraints on the agent or
agents responsible for carrying out the task.
Resources usually have different types of constraints.
Amongst others, there are “requires” relationships between
resource entities which prevent assigning of a resource
without also assigning one or more other required resource.
This constraint can be addressed by having attributes
associated with the resources that would allow the resource

176

3
manager to query on a resource attribute to create compound
resource collections dynamically at run time.
The other important constraint on resources is brought
about by dynamically created composite objects. For example,
a programmer pair in XP pair programming practice or a
couple of nurses assigned to a ‘trauma patient’. We propose to
utilize ideas similar to “views” used by database researchers to
create abstract tables on the fly. The process programmer will
define resource objects and will provide definitions of the
types of composition allowed for these resource objects.
When a process step specifies a query for one of these
composite resource objects, a query processor will join
multiple objects to create composite resource on the fly and
the Resource Manager will retrieve them atomically.
We also propose the incorporation of timing constructs in
our process model, Little-JIL [wise98], in order to provide
important information regarding the time within which an
agent needs to start its work after being assigned to a task, and
the maximum time the agent is given to complete the task. In
some cases, we also envision the need for specifying the
minimum time required for a task for scheduling purposes.
The resource analysis study is often performed with the
objective of identifying the amount of redundancy required
for resources to successfully execute processes in case of a
surge of activity. We are in the process of collecting real life
data for a medical emergency department process to
investigate the characteristics of process behavior and
resource loads under such circumstances. We also plan to
explore process simulation with stochastic input model to
create such scenarios.
We will explore the allocation of resources using strategies
based on known scheduling algorithms from other areas like
operating systems for non-agent resources. However, we
propose a novel approach for the assignment of agents to
tasks. Agents and groups of agents will be presented with
specifications of tasks, time requirements and other resource
requirements related to them. Assignment of agents to tasks
is to be done by means of an auction. The agents will be
required to bid for assignment to these tasks, being
incentivized to bid as aggressively as possible, consistent with
private valuations based upon their need for work, desire for
advancement, or fear of layoffs. The final assignment of tasks
will be done in response to determination of the highest
bidder. Groups of agents in this scenario will be allowed to
communicate amongst themselves in a manner that is similar
to the way bidders may collude in different types of auctions
to reduce their bid. In our case, however, bidder collusion
will be aimed at coming up with the highest bid for a task.
We believe this idea of auctioning is going to be applicable
in different resource management issues. The preferential
ordering of resources that we have discussed earlier can be
achieved based on the bids placed by the different agents (and
other resources). We also feel that the requirements of
supporting dynamic creation of composite resource objects
can be aptly addressed with the utilization of combinatorial
auctions. There is a broad literature in Operations Research on
different auction formats and their effectiveness. We believe,
we can utilize those results and experiment with them in our
environment to discover their utility.

IV. RELATED WORKS
Different software process programming languages like
APEL[establier97], MVP-L[rombach93], ALF[canals94],
Statemate[harel90] and Process Weaver[fern93] have used
resource managers to facilitate process execution. However,
the modeling capabilities in such systems are restrictive and
do not provide support for scheduling.
There has been considerable work in operating systems
focusing on scheduling techniques of primarily hardware
resources [goyal96, shenoy98]. As mentioned earlier, the
domains of their work including required timing granularity,
differ significantly from resources that include both human
and automated agents as well as other entities.

V. CONCLUDING REMARKS
Modeling as well as scheduling of resources to reason about
effective process simulation and resource requirements is a
hard problem. There is ample literature in other areas of
computer and management sciences that look at the resource
management issues from different domain perspectives. It is
crucial for our community to take a critical look at the issues
of resource management, identify the unique challenges faced
by the process programmers while simulating or executing
processes, determine what approaches from other areas may or
may not work, and propose and evaluate new solution
approaches. In this paper, we have tried to motivate such
resource management research by pointing to some intricate
issues related to resources within a process environment. We
have also proposed an auction based solution approach, which
we feel will work well in different types of process simulation
and execution.
ACKNOWLEDGMENT
We would like to thank Barbara Lerner, Rodion
Podorozhny, Anoop Ninan and Joel Sieh for their earlier
attempts at developing some initial versions of resource
management service for process execution support.
REFERENCES
[canals94] Canals, G. et. al., ALF: A framework for building process-centered
software engineering environments. In Software Process Modeling and
Technology, pages 153-185Researhc Studies Press, Sommerset, England 1994
[cass99] Cass, A.G., Lerner, B. S., McCall, E. K., Osterweil, L. J., Sutton, Jr.,
S. M. and Wise, A. Logically central, physically distributed control in a
process runtime environment, Technical Report 99-65, University of
Massachusetts, Department of Computer Science, November 1999
[cass00] Cass, A.G., Lerner, B. S., McCall, E. K., Osterweil, L. J., Sutton, Jr.,
S. M. and Wise, A. Little-JIL/Juliette: A process definition language and
interpreter, In Proceedings of the 22nd International Conference on Software
Engineering, 754-757, Limerick, Ireland, June 2000
[establier97] Establier, J., Dami, S. and Amiour. A., APEL: A graphical yet
executable formalism for process modeling, In proceedings of Automated
Software Engineering, March 1997.
[fern93] Fernstrom, C. PROCESS WEAVER: Adding process support to
UNIX. In the second International conference on Software processes, pages
12-26, 1993 `

177

4
[goyal96] Goyal, P., Guo, X. and Vin, H. A Hierarchical CPU Scheduler for
Multimedia Operating Systems. Proceedings of the 2nd Symposium. on
Operating Systems Design and Implementation, pages 107-122, Seattle,
Washington, 1996.
[harel90] Harel, D. and Lachover, H. et. al. STATEMATE: A working
environment for the development of complex reactive systems. IEEE
Transactions on Software Engineering, vol 16, issue 4, April 1990.
[kellner99] Kellner, M. I., Madachy, R. J., and Raffo, D. M., Software
Process Simulation Modeling: Why? What? How? Journal of Systems and
Software, vol. 46, no. 2/3, 15 April, 1999
[lerner00] Lerner, B. S., Ninan, A. G, Osterweil L. J., Podorozhny R. M. ,
Modeling and Managing Resource Utilization in Process, Workflow, and
Activity Coordination, Technical Report, Department of Computer Science,
University of Massachusetts, Amherst, MA 01003, August 2000. (UM-CS2000-058)
[rodion99] Podorozhny, Rodion, Lerner, B. S. and Osterweil L. J., Modeling
Resources for Activity Coordination and Scheduling. 3rd Internation
conference on coordination models and languages, Amsterdam 1999
[rombach93] Rombach, H. and Verlage, M. How to assess a software process
modeling formalism from a project member’s point of view. In proceedings of
the Second International Conference on Software Processes, pages 147-159,
1993.
[shenoy98] Shenoy, P and Vin.H, CELLO: A Disk Scheduling Framework for
Next-Generation Operating Systems. Proceedings of the ACM SIGMETRICS,
1998.
[wise98] Wise, Alexander, Little-JIL 1.0 Language Report, Department of
Computer Science, University of Massachusetts, Amherst, MA 01003, April
1998. (UM-CS-1998-024)

178

Understanding Open Source and Agile Evolution through Qualitative
Reasoning
Juan C. Fernández-Ramil, Andrea Capiluppi, Neil Smith
Computing Department
The Open University
Walton Hall, Milton Keynes MK7 6AA, U.K.
(j.f.ramil, a.capiluppi, n.smith)@open.ac.uk
1. Introduction
The phenomenon of software evolution has been described in the literature [e.g., Lehman
1974; Lehman and Belady 1985] and several models of different nature [e.g., Aoyama 2002;
Capiluppi 2003; Lehman et al 2002; Rajlich and Bennett 2000; FEAST] have been proposed
to understand and explain the empirical observations. Some of these models purport to be
universally applicable to all software development processes. However, the models in the
literature were built mainly observing software developed in what has been the traditional
centrally-managed Waterfall development process or one its variants. Since these theories
were developed, software development methods have themselves evolved and now much
software is developed and evolved by using agile methods (such as XP [Beck 1999]) and in
open-source environments. It remains a question to be investigated what are the
characteristics of software evolution under these new paradigms and whether the existing
models form an accurate description of the evolution of software under these new regimes or
whether new models will be required. In previous work [e.g., Ramil & Smith 2002; Smith &
Ramil 2002, 2003; Smith et al 2004, 2005] the authors have illustrated how qualitative
reasoning methods, such a qualitative simulation, provide a useful way to examine the
general behaviour of models of software evolution. This extended abstract describes our
plans for further research into the topic.
2. Qualitative Reasoning
The vast majority of the research and development in the process simulation field has
focused in the use of quantitative techniques such as system dynamics and discrete-event
simulation, as exemplified by the majority of the contributions to the ProSim series of
previous workshops. In our work we have been pursuing a radically different approach to the
empirical study of the system dynamics of software evolution processes: the approach is
called qualitative reasoning. The approach is based on the observation that neither the
empirical data available for study of the dynamics of software evolution processes nor our
understanding of the relationships between variables is sufficiently precise as to use
traditional quantitative techniques in all their potential. Qualitative reasoning provides
advantages which we have discussed in previous papers (see references below). An issue that
emerges in all quantitative simulation modelling, particularly in deterministic approaches but
also in stochastic approaches, is the need to execute the models as precisely as possible,
calling for accurate empirical measures for calibration and model validation. Moreover,
classical simulation techniques require precise knowledge of the functional relationships
involving the variables being modelled (e.g., software size, effort, defect density and so on).
In the majority of software processes, with exception of those at the highest maturity levels,
these relationships are likely to be either known incompletely or unknown. Qualitative
reasoning, which includes techniques such as qualitative simulation and abstraction, does not
solve these problems completely but can ameliorate some of them.
There is an important sub-class of models: the models that can be mathematically
represented as a set of differential equations with coefficients expressed as real numbers.

179

These emerge in many simulation problems including the application of the widely used
system dynamics techniques [e.g. Lehman et al 2002]. However, in a qualitative simulation
technique such as QSIM [Kuipers 1994], the parameters of the differential equations do not
need to be specified as real numbers. It is sufficient to know whether they are positive or
negative. Mathematical functions in a QSIM model can be specified simply as monotonically
increasing and decreasing functions. That is sufficient for the QSIM simulation engine to
explore all the possible behaviours associated to a particular set of differential equations. The
output of the simulation is qualitative: it is expressed in terms of shapes and direction, not
precise trajectories. For example, a qualitative simulation may be able to tell us under which
conditions an important attribute, such as functional growth rate, displays sustained increase
(linearly, sub-linearly or super-linearly), steadiness or decrease (again, linearly, sub-linearly,
or super-linearly). Table 1 presents the set of symbols we have used for expressing the
simulation output [e.g., Janusz & Venkatasubramanian 1991]. The key idea is the abstraction
the observed behaviour.

Sign of First
Difference

Sign of 2nd
Difference

Symbol

Segment Name

dm/dt

~d2m/dt2

[0]

[0]



Constant

[+]

[0]

/

Linear growth

[]

[0]

\

Linear decrease

[+]

[+]



Superlinear increase

[+]

[]



Sublinear increase

[]

[]



Superlinear decrease

[]

[+]



Sublinear decrease

[0]

[+]

∪

Minimum reached

[0]

[]

∩

Maximum reached

Table 1 – Symbols used to present the qualitative simulation output using QSIM and the
corresponding signs for the first and second differences of the variable

One significant difference between qualitative process simulation and the classical
counterparts is the need to perform qualitative abstraction (QA) of the empirical data, in
order to be able to compare the simulation output to real world data and, in doing so, assess
the validity of the simulation model. Qualitative abstraction transforms a sequence of
measurements into a pattern, which can be codified as any sequence of the symbols presented
in table 1. The simplest QA algorithm obtains the first and second differences of a sequence
of measurements and extract the signs of the differences (+,–,0). Next, based on the possible
combinations presented in table 1, the algorithm identifies the corresponding sequence of
patterns. This difference between traditional quantitative and qualitative approaches is
illustrated by the diagrams in figure 1a and 1b below.
Metric Data

Metric Data

Qualitative
Abstraction

validation

validation
Simulation Models

Simulation

Simulation Output

Abstracted Trends

Simulation Models

Qualitative
Simulation

Simulation Output

Figures 1a and 1b – Model validation for classical quantitative simulation (left) and in qualitative
reasoning (right)

180

3. Empirical Data
The investigation that we are planning to undertake will start with empirical data
extraction from code repositories for which we have developed expertise. Previous research
has shown that records such as existing change log records, program headers and
configuration management offer a limited, but still suitable source in order to study the
software evolution. The measures upon which the investigation will be based are obtainable
from this type of sources.
As part of an empirical work, data extraction will be performed by using, or adapting,
existing tools, as well as creating new tools, specifically created for the extraction, parsing or
harvesting the statistical results out of the automatically extracted data. A dual approach is
used in this work when referring to collect and analyse data: a discrete approach is defined
when a software system is evaluated through the source code of its releases, while a sampling
approach is used in presence of a CVS repository, and source code is extracted based on
sampling dates (e.g. every week, or month). In this work we plan to extract several attributes
for each software system, taking measurements over releases (or sampled systems) of size in
number of files, files handled, average complexity and the level of complexity-control work.
We have accumulated experience in the use of a set of metrics to characterise the evolution
of software systems at different levels of abstraction: sub-systems, modules (or files or
classes) and functions (or procedures). The study of sub-systems have been mainly through
the characterization of the evolving folder structure of the source code, which can be seen as
a surrogate of the architecture of the system. It is when the behaviour of these is jointly
analysed that a richer view of the evolution of a system emerges. Through these
visualisations, for example, it has been found, for example, that in the Open Source context,
additional stages are likely to emerge in addition to those indicated in models in the literature
such as the one in [Rajlich and Bennett 2000]. In particular, they have identified a
hibernation stage in some Open Source projects in which for some time no evolution activity
is apparent, after which activity is regained. The use of qualitative simulation will help to
clarify and put these and other observations into a more solid basis. Access to empirical data
is essential in an investigation such as this. A London based software house in the domain of
web advertising, actively using Agile processes has provided access to the source code of one
of their systems. Moreover, open source projects are important sources of empirical data
which we plan also to exploit. There is now in the software evolution community a growing
interest and expertise in extracting metrics from these widely available data sources [e.g.
RELEASE, Fisher et al 2003]. This offers opportunities to share data-sets and expertise. We
also have access to data from at least 3 proprietary systems: the change log records of a large
information system in the telecommunication domain, change history records of an operating
system kernel and the program headers of a financial transaction system. These data sources
provide also input to the proposed research. Even if the proposed investigation focuses on the
Agile and open source paradigm and on the already available industrial records, given the
increasing popularity of the former and the richness of the latter, the research will generate
useful and relevant results and make progress in this area.
4. Research Questions
The general research question to be investigated is the following: what are the
characteristics of software evolution under Open Source and Agile paradigms? The detailed
research questions to be investigated in our future work include the following:
1. to which degree the empirically observed evolutionary trends in attributes – such
as size, structural and functional complexity of software – can be successfully
modelled by existing and new qualitative simulation models of the evolution
process and
2. what is the support from validated qualitative simulation models to existing
empirical generalisations such as the Rajlich and Bennett’s staged model of the

181

software life cycle and the laws of software evolution, and to other alternatives
which may emerge?
3. what are the theoretical and practical implications of the answers to questions 1
and 2?
The plan for the investigation of the above questions requires at least the following steps:
a) data extraction from existing records of evolving systems
b) construction and validation of qualitative simulation process models, both
generic, and specialised to particular software process domains (e.g. traditional
non-agile, agile, open source). The models can be inspired in existing empirical
generalisations or in direct observations and insight
c) refinement of the exiting set of empirical generalisations regarding long-term
evolution, including Rajlich and Bennett’s model, Lehman’s laws of software
evolution and measures of their empirical support
d) experimentation with the models and systematic analysis of the empirical
generalisations, in order to generate a compendium of good practice [e.g.
Lehman & Ramil 2001], including recommendations for long-term management
of the software process
e) interaction, when possible, with key developers of one collaborator company in
order to validate and contrast the output of the data extraction, the empirical
generalizations that may emerge from the models
The desired deliverables for our future research will include:
a. a set of qualitative simulation process models specialised to traditional
proprietary processes (Waterfall-life processes), Open Source and Agile
processes
b. a refined set of empirical generalisations regarding long-term evolution,
including refined versions of the Rajlich and Bennett’s model, Lehman’s laws of
software evolution
c. a compendium of good practice, including recommendations for long-term
management of the software process
d. an evaluation of existing theory and models of software evolution.
The above represent our long term plans over a research period of two years or so,
provided that adequate funding is provided. If no funding is secured, then we plan to still
make progress in the topic but a slower rate.
5. Related Work
[Suarez et al 2002] applied qualitative simulation techniques to the software process but
with focus on new development, not on long term software evolution as in our approach. The
Open Source domain has been studied using traditional direct visualization of trends based on
quantitative metric data extracted from a single Open source system [e.g. Capiluppi et al
2004, Capiluppi et al 2005]. To the knowledge of the present authors, our work is the first
attempt to analyse the growth trends of a large sample of Open Source and Agile systems
using qualitative reasoning techniques.
Exploration of activity levels in software evolution has been the focus of several
quantitative simulation models using system dynamics [e.g., Lehman et al 2002]. This
modelling effort has been inspired in data and observations of proprietary software. Research
efforts specifically involving simulation of some aspects of Open Source development and
evolution has also taken place. Examples include [Madey et al 2002], who used the existing
SWARM agent-based simulation tools, and [Dalle & David 2004], who built their own agent
platform, SimCode. This type of models was used to investigate questions related to the
amount of effort allocated to OSS projects and whether a significant attraction of new
developers can be achieved or not in the evolution of the project. This research shares with
our work the focus on product characteristics (e.g. size and complexity) and on evolution.
However their approach is bottom-up, involving rather complex models, while our approach

182

is top-down, starting with a simple theory or explanation of the phenomenon and much
simpler models.
6. Final Remarks
Our previous results in the application of qualitative simulation and qualitative abstraction
to empirical data derived from software evolution processes have been already reported in
two journal papers [Ramil & Smith 2002; Smith et al 2005] and several workshop
presentations [Smith & Ramil 2002, 2003; Smith et al 2004]. Our future work will build
upon results achieved, expanding them and extending them to the Agile domain. In the short
term (next few months) we plan to continue work in empirical data extraction and in the
building of qualitative simulation models for Open Source processes. If we are given the
opportunity, we hope to be able to report on our most recent work, including new work we
included in [Smith et al 2005], at the coming ProSim 2005 workshop. We welcome the
feedback of the community with regards to our plans. We are also currently preparing a
proposal to request funds from the UK EPSRC in order to be able to work in the long term
goals and deliverables.
7. Acknowledgements
We gratefully acknowledge Dr. Helen Sharp, from the Computing Dept., The Open
University, through whom we have been able to get access to the source code of an evolving
system, developed under the Agile paradigm.
8. References
[Aoyama 2002] Aoyama M, Metrics and Analysis of Software Architecture Evolution with
Discontinuity, Proc. 5th Intl. Workshop on Principles of Software Evolution, IWPSE 2002,
Orlando, FL: 103 – 107
[Beck 1999] Beck K (1999); Extreme Programming Explained: Embrace Change, Addison-Wesley, 224
pp.
[Capiluppi 2003] Capiluppi A., Models for the Evolution of OS Projects, Proc. ICSM, Amsterdam, 22
– 26 Sept. 2003, pp. 65 - 74.
[Capiluppi et al 2004] Capiluppi A., Morisio M. & Ramil J.F., Structural Evolution of An Open Source
System: A Case Study, Proceedings of the 12th International Workshop on program Comprehension
(IWPC), June 24-26, 2004, Bari, Italy, pp. 172 - 182 .
[Capiluppi et al 2004b] Capiluppi A., Morisio M. & Ramil J.F., The evolution of source folder
structure in actively evolved open source systems, Proceedings of the 10th International
Symposium on Software Metrics, Sept. 11-17, Chicago, Illinois, pp. 2 - 13
[Capiluppi et al 2005] Capiluppi A., Faria A.E. & Ramil J.F., Exploring the Relationship between
Cumulative Change and Complexity in an Open Source System, 9th European Conference on
Software Maintenance and Reengineering (CSMR), Manchester, UK, March 21-23, 2005
[Dalle & David 2004] Dalle J.M., David P.A. 2004. SimCode: Agent-based Simulation Modelling of
Open-Source
Software
Development,
available
online
at
http://opensource.mit.edu/papers/dalledavid2.pdf <as of Feb. 2005>
[FEAST] FEAST, Feedback, Evolution And Software Technology, Dept. of Computing, Imperial
College, http://www.doc.ic.ac.uk/~mml/feast/ <as of Feb. 2005>
[Fisher et al 2003] Fischer M, Pinzger M and Gall H (2003); “Populating a Release History Database
from Version Control and Bug Tracking Systems”, Proc. ICSM 2003, 22-26 Sept, Amsterdam,: 23
– 32
[Glass 2003] Glass RL, 2003, Facts and Fallacies of Software Engineering, Addison Wesley
Professional, 224 pp.
[Janusz and Venkatasubramanian 1991] Janusz, M. and V. Venkatasubramanian, (1991) Automatic
generation of qualitative description of process trends for fault detection and diagnosis. Engng
Applic. Artif. Intell., 4, pp. 329-339.
[Kuipers 1994] Kuipers, B. (1994) Qualitative Reasoning: Modeling and Simulation with Incomplete
Knowledge, Cambridge, Massachusetts: MIT Press.
[Lehman 1974] Lehman M.M., “Programs, Cities, Students, Limits to Growth?”, Inaugural Lecture, in
Imperial College of Science and Technology Inaugural Lecture Series, v. 9, 1970, 1974, pp. 211 –
229. Also in Programming approach, Gries D (ed.), Springer Verlag, 1978, pp. 42 – 62. Reprinted
as Chapter 7 in [Lehman and Belady 1985].

183

[Lehman and Belady 1985] Lehman MM and Belady L (1985); Program Evolution – Processes of
Software Change, Acad. Press, London, 1985.
[Lehman and Ramil 2001] Lehman MM and Ramil JF (2001); “Rules and Tools for Software
Evolution Planning and Management”, Annals of Software Engineering, vol. 11, special issue on
Software Management, 2001, pp. 15 – 44.
[Lehman et al 2002] Lehman MM, Kahen G and Ramil JF, Behavioural Modelling of Long lived
Evolution Processes– Some Issues and an Example, J. of Software Maintenance and Evolution,
spec. issue on Separation of Concerns, vol. 14, 2002, pp. 335 – 351
[Madey et al 2002] Madey GR, Freeh VW, Tynan RO. Agent-Based Modeling of Open Source using
Swarm, in Proc. of Americas Conference on Information Systems (AMCIS 2002), Dallas, Texas,
August.
[Rajlich and Bennett 2000] Rajlich VT and Bennett KH (2000); “A Staged Model for the Software
Life Cycle”, IEEE Computer, 33(7): 66 – 71.
[Ramil & Smith 2002] Ramil J.F. & Smith N., Qualitative Simulation of Models of Software
Evolution, Journal of Software Process: Improvement and Practice, vol. 7, 2002, pp. 95 – 112.
[RELEASE] RELEASE, REsearch Links to Explore and Advance Software Evolution,
http://labmol.di.fc.ul.pt/projects/release/ <as of Feb. 2005>
[Smith & Ramil 2002] Smith N. & Ramil J.F., Qualitative Simulation of Software Evolution Processes,
Proc. of WESS’02, Oct. 2nd, Montreal, Canada, pp. 41 – 47.
[Smith & Ramil 2003] Smith N. & Ramil J.F., Qualitative Abstraction and Simulation in the Study of
Software Evolution, ProSim 2003, Portland, Oregon, May 3-4, 2003.
[Smith et al 2004] Smith N., Capiluppi A., Ramil J.F., Qualitative Analysis and Simulation of Open
Source Software Evolution, ProSim 2004, May 24-25, 2004.
[Smith et al 2005] Smith N., Capiluppi A., Ramil J.F., 2005, A Study of Open Source Software
Evolution Data using Qualitative Simulation, accepted for publication in the "Special issue on
software process modeling and simulation", published by The Software Process Improvement and
Practice journal (SPIP).
[Sommerville 2001] Sommerville I, 2001, Software Engineering, 6th Ed., Addison-Wesley,
Wokingham, UK, 2001, 720 pp.
[Suarez et al 2002] Suarez AJ, Abad PJ, Gasca RM, Ortega JA, (2002) "Qualitative Simulation of
Human Resources Subsystem in Software Development Projects", Proc. 16th Intl. Workshop on
Qualitative Reasoning (QR2002), June 10-12, Sitges, Spain, pp. 169 - 176
[Tesoriero and Zelkowitz 1998] Tesoriero R and Zelkowitz MA (1998); “Model of Noisy Software
Engineering Data”, Proc. ICSE 20, Kyoto, Japan, 19 – 25 April, pp. 461 – 476.

184

1

Teaching by Modeling instead of by Models
Thomas Birkhoelzer, Emily Oh Navarro, André van der Hoek


Abstract— Teaching and training is one of the important
applications of software engineering process simulation. Up
until this point, however, it has only been used in the context of
students running simulations of process models that were built
by someone else.
In this paper, we suggest a different approach: to use the
modeling activity for teaching as well, rather than the
simulation activity only. In particular, we pro-pose to assign
students the task of building a new soft-ware process
simulation model using an existing educational software
process simulation environment, SimSE.
First experiences from a feasibility project are reported.
Index Terms—Process modeling, software
simulation, teaching of software engineering.

project

I. INTRODUCTION

T

RAINING and teaching is an important application of
software process simulation [1]. Various models and
environments have been developed targeting this context,
e.g. [3], [4], [5], [6], [7] [8]. These all share the purpose of
giving students virtual experiences of realistic software
processes that would otherwise be infeasible to practice in
an academic environment.
So far, the reported usage of simulation and modeling in
this context is always structurally similar: An existing model
is used by the trainee for virtual experiences, i.e. simulation
is leveraged for teaching, and modeling is done outside the
learning situation by an instructor or some other expert
beforehand.
From other technical fields, however, it is well known
that the modeling by itself provides valuable learning
insights. For example, many classes in engineering
disciplines include the development of a simulation model
in the respective field of application as a final assignment.
In this paper, it is suggested to use software process
modeling and simulation in a similar way: the active
development of the model as a task for students, not just
their passive usage of a pre-existing simulation.
Experiences from a first project are reported and needs
for further work and developments are discussed.
Manuscript received February 6, 2005. The SimSE project is partially
funded by the National Science Foundation under grant numbers DUE0341280, CCR-0093489 and IIS-0205724.
T. Birkhoelzer is with the Department of Electrical Engineering and Information Technology, University of Applied Science, Konstanz, Germany
(phone: +49 7531 206239; e-mail: birkhoelzer@fh-konstanz.de).
E. Navarro is with the Department of Informatics, Donald Bren School
of Information and Computer Science, University of California, Irvine
(e-mail: emilyo@ics.uci.edu).
A. v. d. Hoek is with the Department of Informatics, Donald Bren
School of Information and Computer Science, University of California,
Irvine (e-mail: andre@ics.uci.edu).

II. DIDACTIC GOALS OF MODELING
The development of the model by students has three
unique didactic advantages:
x Modeling requires articulateness and explicitness.
For a simulation model, all assumed relations and
mechanisms of projects or processes must be made
explicit and precise in order to be executable. For
example, it is one thing to just state that a tool would
“improve the process”. A simulation model, on the other
hand, requires one to articulate this assumption explicitly:
which attributes (e.g. error rate or productivity) are
influenced and how?
x Enactment of a simulation provides immediate feedback.
Enacting a simulation usually provides immediate and
obvious feedback about the consequences of relations and
mechanisms stated, much better than any instructor
critiques, especially with regard to errors or neglected
side-effects. This is not to say that the simulation can
replace an instructor. The simulation just provides the
mechanical feedback, such that instructors can
concentrate on translating this into lessons learned.
x Creative task as motivation.
Technical students – engineers as well as computer
scientists – usually love to create things, not just to use
them. In this sense, model creation can provide a much
higher motivation than just model usage.
III. MODELING A SYSTEM ENGINEERING PROJECT
A. Background
As a feasibility study of developing a simulation model
by students as part of normal project management course
work, three undergraduate students of “project engineering”
at the University of Applied Science, Konstanz developed a
simulation model as a project assignment. They were in the
third year of their studies with a background in electrical
engineering and project management, not in software
engineering or computer science.
SimSE was chosen as the modeling tool and simulation
environment, mainly because of its integrated model builder
tool and graphical simulation environment.
Together with the SimSE tools, an existing SimSE model
of a software project following a waterfall process model
was available, which was used as example and template.
B. SimSE Environment
SimSE is a game-based, graphical, interactive software
process modeling and simulation environment designed
specifically for generating educational simulations. Its
purpose is to allow students to practice quasi-realistic, largescale software processes in a fun (and hence, more

185

2
educationally effective [2]) setting; and to allow instructors
to build the simulation models for their students to “play.”
One of the most significant features of SimSE is its model
builder tool, which was designed to make model building
simpler by obliterating the need to learn and program in a
process modeling language. The model builder tool
completely hides the underlying textual modeling language
from the modeler by providing a graphical user interface.
This interface allows one to build a model using only
buttons, drop-down lists, menus, and text boxes – no
programming is required. Once a modeler specifies all of
the object types, start state objects, actions, rules, and
graphics for a model, the environment then generates a
simulation game based on that model.
The graphical, high-level nature of the model input
requires less initial learning overhead compared to a special
modeling or programming language, which was an
important issue considering the students’ non-computer
science background.
Moreover, the model builder, as well as the generated
simulation game, are Java applications. Therefore, only a
Java development kit is required as a prerequisite, which
eases the usage by students on their private hardware.
More information about SimSE and its waterfall model
that was extended for this project can be found at [9].
C. Modeling task
As modeling task, a system engineering project was
chosen, i.e. a project combining hardware and software
parts into a system. Because SimSE is a game-based
simulation environment, it requires that a scenario or “story”
accompany each model. In this case, the simulation’s goal
was the development of a control component for a test
robot. The idea of this story stemmed from work done by
one of the students during an internship. Thus, this
immediately connected the theoretical model to practical
experiences.
The students built the model as an extended waterfall
model that incorporates hardware aspects as well as
software, using the following basic flow: Starting from
system requirements, hardware, software, and supply
components are pursued in separate paths (with the
associated artifacts) and finally integrated into the product.
Therefore, the modeling task was similar to the existing
waterfall simulation model. Theoretically, the artifacts,
activities, and relations from this existing model just needed
to be cloned and extended. At the same time however, to
avoid an overly complex model, the existing mechanisms
needed to be simplified. Both, appropriate extension and
simplification, required a thorough understanding of the
model and, more importantly, of the intentions behind the
model.
D. Modeling Workflow
The SimSE model builder tool supports a modeling
workflow, which closely resembles a didactic
decomposition of project management issues.
1. Definition of the project constituents (artifacts,
deliverables, participants, tools).
This is the first and basic step of any project
management. In the SimSE model builder, this

corresponds to defining the object types (templates for
the simulation objects) and start state (instantiated objects
that the simulation begins with). For the system
engineering project, business, software and hardware
artifacts were defined reflecting the basic steps of a
development process in each category (e.g. specification,
design implementation, test). Participants are employees
with different experience in these three fields (software,
hardware, business).
2. Definition of the actions a manager (player) can take.
These are the inputs into the simulation. There are
typically two classes of such actions: task assignments
(create, review, correct) and management actions (give
bonus, purchase tool, motivate by free coffee, fire).
Whereas the first ones are direct consequences of the list
of artifacts, the second class enables and enforces the
students to define their management style. Each
manager/player action corresponds directly to a SimSE
action – the specific ways that a player can manage,
control, and drive the simulation.
3. Definition of the actions that occur autonomously.
In addition to the actions triggered by the manager, there
are events beyond the control of a manager. In SimSE,
this is modeled by actions that occur automatically (e.g.,
employees take breaks) or randomly (e.g., the customer
introduces new requirements, employees get sick). The
definition of such events teaches basic risk management.
4. Definition of the effects the actions should have.
In SimSE rules are attached to each action. These rules
specify how that action affects the rest of the simulation.
For example, a creation rule affects the completion
percentage of an artifact depending on the productivity of
the participant.
The rules and actions can be prioritized according to
which ones should be evaluated first, based on their
dependencies. For instance, an action that is triggered
based on an employee’s energy level (e.g., take a break)
should be evaluated after another rule that modifies that
employee’s energy level is fired.
Whereas the definition of such rules seems to be
straightforward on a first glance, actually conducting it
reveals two basic challenges: The large amount of such
effects and the difficulty to quantitatively describe it by
mathematical formulas. For example, every project
manger would immediately agree that the productivity of
an employee depends on the experience, the mood, and
the number of parallel tasks, but how to model this by a
mathematical formula, e.g. as a sum or a product?
5. Definition of the dependencies.
Dependencies between artifacts and activities are
modeled in SimSE by effects of rules as well. For
example, a creation rule can have the effect of reducing
the completion percentage of a dependent artifact (or the
effect of increasing the number errors in this artifact).
Whereas this provides a realistic management situation (a
dependent artifact can be worked on before finishing its
precursor), it adds another level of complexity. The
student project stopped at this point.

186

3
6. Definition of the graphics (in parallel to the previous
steps).
To each constituent of the project, an image needs to be
assigned for the simulation. In addition, a pictorial layout
of the simulated office must be defined. Whereas this
provides no direct insight into project management, it
adds a lot to the impression of ownership.
IV. LESSONS AND RECOMMENDATIONS
During the project, the following lessons were learned:
x Modeling is difficult but possible
The modeling task provides a challenge to students. The
translation of project management knowledge into
mathematical formulas and mechanisms is unusual and
unfamiliar. Nevertheless, the student group finally
succeeded: despite some odds, they were able to form a
first simulation model. Of course this model is not yet
complete (specifically, further effects could be modeled
and many dependencies are missing).
x Creative aspects provide high motivation
The opportunity to create their own processes and
mechanisms served as an important motivation for the
students. In this context, also less scientific aspects like
graphics and playful aspects should not be neglected. It
seems to be more fun to design a game-like simulation
than just chart outputs.
x Examples are helpful and necessary
For the system engineering project, the most successful
modus operandi was the study of the artifacts and
mechanisms of the existing model and their appropriate
modification. Designing everything from scratch would
have been much more difficult. Based on that experience,
it is recommended to document and use such examples as
kind of templates or patterns.
x Tools need improvements
Within the traditional usage of simulation models in a
teaching context, the students use only the simulation
environment, not the modeling tool. Therefore, the
simulation environments are designed for non-expert use,
whereas the usability of the modeling tool got less
attention. Instead, the modeling tools were designed to
allow maximal expressiveness and flexibility.
Modeling by students requires modeling tools for nonexpert users as well. Usability with minimal training
might be more important than complicated functionality.
x Implementation-independent notational support missing
One of the most difficult tasks was the specification of
the intended relations and mechanism on paper. For
example, the modeling tool provides interactive menus to
specify the modeling rules. However, to design, discuss,
and document these rules outside the program on paper,
an appropriate notation (not a programming language) is
necessary. For process modeling, such a notation is not
readily available or broadly established. It is difficult for
students to develop such a notation by themselves.
Therefore, early support by the instructors on formal
model design (not just implementation) is necessary. It
might even be helpful, if the modeling tools would be
accompanied by notational tutorials and examples as
well.

V. CONCLUSION
Modeling by themselves forces the students to be precise
and explicit about their assumptions regarding projects and
processes, as the enactment by the simulation provides
immediate feedback. Moreover, the creative nature of the
task seems to meet the motivation of many students in this
area.
This is not to say that such modeling should replace the
use of preexisting models in a learning context. Both have
their distinct advantages: Own modeling is restricted to
small problems with a limited level of detail and
sophistication and might even remain incomplete. It is not
expected that such models can be used to gain really new
insights from the model itself. To gain such insights requires
using and studying more elaborate and detailed models
developed by experts. Using the later one as templates for
the own development as described in Chapter 4 can benefit
both approaches by improving the own results as well as the
appreciation of the “expert-models”.
Therefore, it is expected that the development of
simulation models by students themselves can be a valuable
component of process and project teaching and training
complementing the use of expert-models. The overall
positive outcome of the system engineering project
modeling supports this expectation.
To alleviate this, however, further work is necessary
especially towards developing easy-to-use modeling tools,
appropriate templates and modeling tasks, and unified and
simplified notations.
ACKNOWLEDGMENT
The system engineering project has been developed by
Patrick Schnell, Markus Schulz, and Stefan Weidele. We
would like to thank them for their intellectual curiosity to
try such a task and for their dedication to carrying it
through.
REFERENCES
[1]

[2]

[3]

[4]

[5]

[6]

[7]

187

M. L. Kellner, R. J. Madachy, and D. M. Raffo, “Software Process
Modeling and Simulation: Why? What? How”? Journal of Systems
and Software 46, Elsevier, New York, 1999, pp. 91-105.
M. Ferrari, R. Taylor, and K. VanLehn, “Adapting Work Simulations
for Schools,” The Journal of Educational Computing Research, 21(1),
1999, pp. 25-53.
A. Drappa and J. Ludewig, “Simulation in Software Engineering
Training,” Proceedings of the 22nd International Conference on
Software Engineering, ICSE, Limerick, Ireland, June 2000,
pp. 199-208.
J.S. Collofello, “University/Industry Collaboration in Developing a
Simulation Based Project Management Training Course,”
Proceedings of the Thirteenth Conference on Software Engineering
Education and Training, S. Mengel and P.J. Knoke, Eds.: IEEE
Computer Society, 2000, pp. 161-168.
H. Sharp and P. Hall, “An Interactive Multimedia Software House
Simulation for Postgraduate Software Engineers,” Proceedings of the
22nd International Conference on Software Engineering, ACM, 2000,
pp. 688-691.
D. Pfahl, M. Klemm, and G. Ruhe, “A CBT Module with Integrated
Simulation Component for Software Project Management Education
and Training,” Journal of Systems and Software 59, Elsevier, New
York, 2001, pp. 283 298.
E. Oh Navarro and A. van der Hoek, “Software Process Modeling for
an Interactive, Graphical, Educational Software Engineering
Simulation Game,” Proceedings of the 5th International Workshop on
Software Process Simulation and Modelling (ProSim 2004),
Edinburgh, May 2004, S. 171-176.

4
[8]

[9]

Th. Birkhölzer, L. Dantas, C. Dickmann, J. Vaupel, „Interactive
Simulation of Software Producing Organization's Operations based on
Concepts of CMMI and Balanced Scorecards,“ Proceedings of the 5th
International Workshop on Software Process Simulation and
Modelling (ProSim 2004), Edinburgh, May 2004, S. 123-132.
E.O. Navarro and A. van der Hoek, “Design and Evaluation of an
Educational Software Process Simulation Environment and
Associated Model,” Proceedings of the Eighteenth Conference on
Software Engineering Education and Training, Ottawa, Canada,
IEEE, 2005 (to appear).

188

1

A Simulation Model for Global Software
Development Project
David Raffo and Siri-on Setamanit


Abstract—Global software development (GSD) is becoming
a dominant paradigm in the software industry. Conducting
development projects in multiple countries offers many
potential benefits including significantly reduced cost and
better response times. However, it also poses some difficulties
and challenges in managing this kind of project. Software
Process Simulation Modeling (SPSM) can be used to support,
enrich and evaluate GSD theories, to facilitate understanding,
and to support corporate decisions.
The discrete-event
paradigm and system dynamics paradigm compliment each
other and together enable the construction of models that
capture both the dynamic nature of project variables and the
complex sequences of discrete activities that take place. In this
paper, we outline the objectives and the requirements of a GSD
model, evaluate different types of simulation paradigms,
identify important GSD factors and review relevant
quantitative studies and models that could potentially be
included in the GSD model. We also present a high-level
description of a prototype GSD model we are actively
developing.
Index Terms—Global Software Development, Distributed
Development, Software Process Simulation Modeling, Hybrid
Simulation Model

successfulness of global software development.
Current GSD research may be characterized as
exploratory with case studies and other empirical work.
Unfortunately, they only address limited aspects of GSD
projects. The magnitude and the interaction between
important factors that may affect the performance of global
software development projects are not well captured or
quantified. In short, the overall impact of GSD is still
unknown.
This indicates the need to develop a methodology that can
integrate and synthesize relevant factors, research, theories,
and models in the GSD context, which will enable us to
investigate and examine the overall impacts of GSD. We
believe that using SPSMs for representing GSD projects is a
highly desirable approach for this work. In this paper, we
identify GSD model requirements and important GSD
factors.
We review relevant quantitative studies and
models that could potentially be included in the GSD model.
Finally, we propose a prototype GSD model that draws
upon a number of published GSD empirical studies and
incorporates many important GSD factors.

II. RESEARCH OBJECTIVE

I. GLOBAL SOFTWARE DEVELOPMENT (GSD)

I

N recent years, global software development (GSD) has
become a dominant paradigm in the software industry.
Seeing the benefit of low cost and diverse development
skill, many software companies have begun to develop
software at remote sites or through outsourcing. Other
potential benefits include reduction in time-to-market,
reduction in development costs, better use of scare
resources, and business advantages from proximity to
customers [1-4]. Nevertheless, GSD also poses some
difficulties and challenges to the development team. Due to
geographical dispersion and time-zone differences between
development sites, communication and coordination become
much more difficult. Cultural and language differences also
make it even harder to coordinate and manage collaborative
project work. It has been reported that, for work of equal
size and complexity, multi-site software development takes
much longer than single-site development [5, 6].
Researchers have been attempting to identify and
understand the factors that enable and hinder the

The goal of our work is to develop a GSD model which
will incorporate diverse research and theories regarding
GSD projects (and related fields). It will include a set of
relevant empirical factors, development methods, process
and product parameters, and project environment factors.
The proposed GSD model will fulfill two basic needs:
1.
2.

To support, enrich and evaluate GSD theories.
To support corporate decisions.

At the practical level, simulation can be used to support
and improve organizational decision-making. To support
corporate decisions in global software development,
software process simulation models should have the
capability to address the following questions:

David Raffo is with College of Engineering and Computer Science and
the School of Business Administration, Portland State University, Portland,
OR 97207 USA (e-mail: davidr@sba.pdx.edu).
Siri-on Setamanit is with the School of Business Administration,
Portland State University, Portland, OR 97207 USA (e-mail:
sirion@pdx.edu).

189

General GSD project:
o Should work be distributed across multiple sites or
should it be centralized at single site?
o Under what circumstances do dispersed teams
perform better than co-located teams? When should
a global software development approach be chosen?
o What are the most important factors in GSD projects?

2
Specific GSD project:
o Which development sites should be included in the
project?
o How should work be divided up across sites? What
task allocation strategy should be used for a
particular project?
o What is the forecasted project performance in terms
of cost, quality, and schedule?
o What is the impact of process changes in a GSD
context? Should we add process A? Can we
minimize or skip a portion of process B?

III. SOFTWARE PROCESS SIMULATION MODELING (SPSM)
Software Process Simulation Modeling (SPSM) has been
used to address a variety of issues ranging from strategic
management, project management planning and control,
process improvement, technology adoption, to training and
understanding [7]. In addition to the ability of software
process simulation modeling to support software project
management, it can also be used to support software
engineering theory formulation and testing. For example,
Abdel-Hamid and Madnick used their system dynamics
model to experiment with the applicability of Brooks’ Law
[8].
Software Process Simulation Models can be used as a
platform to combine and synthesize previously developed
theories and models, and incorporate a wide range of
relevant factors. Thus, simulation models may potentially
be used to support real experiments and enrich empirical
studies, which will be beneficial in supporting, enriching,
and evaluating theories [9, 10].

IV. COMPARISON OF SOFTWARE PROCESS SIMULATION
MODELS
Several simulation approaches and languages have been
applied to the software development process including
knowledge-based simulation, agent-based simulation,
system dynamics (or continuous simulation), and discreteevent simulation. Each simulation paradigm has its own
advantages and disadvantages. No single simulation
approach is the best among all situations. The objective of
this section is to identify the most appropriate approach that
would be best suited the objective of our study.
A. Knowledge-based Simulation (KBS) Models
Knowledge-based simulation is defined as the application
of knowledge-based methods within artificial intelligence
(AI) to the field of computer simulation [11]. KBS models
view the process from the perspective of the person in the
loop or in the system. It is used to describe the personal
activities of the people participating in the process. One can
see that the level of details provided by KBS would be too
complex for the GSD model since the GSD model focuses
more on the project level performance.
B. Agent-based Simulation (ABS) Models
Agent-based simulation (ABS) models differ from other
simulation models in that they focus on decentralized

computation. An agent-based model focuses on individual
developers, their characteristics, and how they interact with
one another, thus allows one to observe the characteristics
of the interactions between the developers and their effects
[12].
Like KBS models, ABS models focus on a low level of
detail, and seems to be too detailed for the GSD model.
However, ABS models could be used as a sub-model to
simulate certain social aspects in a software development
process such as the interaction or communication between
developers. Then, the output from the agent-based model
could be used as the input to the GSD model.
C. System Dynamics (SD) Models
SD models describe a system in terms of continuous
functions that are solved via numerical integration. The
values of the model variables are updated continuously as
the simulation proceeds. The system dynamics paradigm is
appropriate for exploring the behavior of the systems or
variables of interest over time. Its strengths lie in its ability
to accommodate the rich interrelationships between
variables, particularly the effects of feedback. However, it
is difficult to describe process steps or complex sequences
of activities using SD Models.
D. Discrete-event Simulation (DES) Models
Discrete-event simulation (DES) models can simulate a
step-by-step emulation of the flow of entities through a
system. The main advantage of DES models is the ability to
capture actual process level details and the ability to
represent each work product of the development process as
being unique through the use of attributes attached to each
work product such as size and complexity. However, the
primary limitation of DES models is that they cannot easily
represent the dynamic nature of the project variables such as
human resources, productivity, defect injection and
detection rates, and etc.
E. Hybrid Simulation Models
A hybrid model, as the name suggests, is the model that
combines different modeling paradigms to better represent a
systems or a software development process. By combining
discrete-event and system dynamic simulations, hybrid
models can represent entities with attributes being acted
upon by activities that are influenced by continuously
changing factors [13].
A system dynamics paradigm is suitable for modeling
continuous factors and their interactions that may impact
GSD projects such as communication, coordination, cultural
issues, learning curve, changing staff levels, and
dynamically varying productivity. On the other hand, the
discrete-event paradigm provides the ability to explicitly
represent the process structure and mechanisms used to
transfer work products and to coordinate activities.
These two paradigms complement each other and thus
provide the capability to capture important issues and
factors in a GSD project [14]. Thus a hybrid simulation
model combining discrete-event and system dynamics
paradigms is the most appropriate approach for developing a
GSD model.

190

3
V. PROPOSED GSD MODEL
This section can be divided into 3 parts. The first part
(Section A) identifies the factors that need to be included in
the GSD model. Some of these factors were cited in our
previous paper [14]. The list shown below is more
comprehensive and it includes factors that we are including
in our envisioned full-scale GSD model. In section B, we
review several quantitative models and discuss how they
can be integrated into the GSD model. Finally, we discuss
the major components of our prototype model at a highlevel (Section C).
A. Important Factors
In order to effectively represent GSD projects, factors
that can affect the performance and the productivity of GSD
projects are needed to be included in a GSD model. Factors
that are important were identified and placed into three
categories, which are (1) the fundamental factors, (2) the
strategic factors, and (3) the organizational factors.
1) The fundamental factors:
The fundamental factors are the impact from the
characteristics of GSD projects include distance, time-zone
differences, and cultural and language differences. A
project manager has little or no control over these factors
since they are caused by the characteristics of the project
itself. However, he can mitigate/alleviate the negative
impacts of these factors by using the right strategy and tool
support. There are five factors in this category including
communication problems, coordination and control
problems, cultural differences, language differences, and
time-zone differences.
1.1) Communication problems
There are two major communication problems in typical
GSD projects: (1) inadequate informal communication and
(2) loss of communication richness.
Inadequate informal communication
Informal and unplanned communication is extremely
important in supporting collaboration in software
development processes [15-17].
However, distance
profoundly reduces the amount of informal communication
[16-19]. Reduction in the frequency of communication can
lead to difficulty in collaborative work, which may lead to
longer development cycle times.
Loss of communication richness
Generally, tasks that need coordination and cooperation
such as software development tasks require rich
communication. The richer the media is, the better the
communication. However, distance between development
sites often prohibits the use of rich media (e.g. face to face
communication, video conferencing, etc.) due to cost,
quality, and implementation issues.
The loss of
communication richness can result in poorer communication
between programmers in different sites, which can lead to
poorer coordination and cooperation.
1.2) Coordination and control problems
Distance and time-zone differences have negative impacts
on coordination and control effectiveness. It is no longer
possible to coordinate by a quick phone call or by walking
around the office. It is also not possible to control the team

members activities by visiting their offices. This can
contribute to lower productivity rates and lower software
quality, which negatively affect the development cycle time.
1.3) Cultural differences
Many researchers agree that national culture differences
is one of the important challenges in global software
development [1, 3, 20-23]. Cultural differences can affect
a GSD project in different ways, including communication
and coordination effectiveness, group decision-making, and
team performance. Although there is no general theory on
cultural differences, we will distinguish culture based on
Geert Hofstede [24] and Edward T. Hall [25] cultural
dimensions.
1.4) Language differences
Although programming languages transcend national
boundaries, developers still have to communicate in order to
clarify things or solve problems.
When people
communicate in a language other than their own, it is
unlikely that they would be able to communicate as
effectively as using their own languages. In addition, nonnative English speaking people usually prefer asynchronous
communication such as email over synchronous
communication since it allows them to read and write at
their own pace [26, 27]. Unfortunately, asynchronous
communication often adds to delays or complicates problem
resolution since it can take days to get back-and-forth
discussions over email [21].
1.5) Time-zone differences
Time-zone differences can have both positive and
negative impact on GSD performance. On the positive side,
theoretically, development cycle time could be reduced as
much as 50% by using a follow-the-sun development
strategy [28]. However, the difference in working hours
creates temporal distance between development sites [21],
thus inhibits the use of synchronous communication.
2) The strategic factors:
Global software development introduces additional
strategic issues (such as development site location, product
architecture, and development strategy) that managers have
to address in addition to what they are exposed to in
managing co-located projects. Their decisions on these
issues will have some impact on the performance of GSD
projects. Consequently, it is important that these factors be
captured in the GSD model. There are five factors in this
category, which are development site, product architecture,
development strategy, distribution overhead, and
distribution effort loss.
2.1) Development site
Different development sites have different technological
and communication infrastructures, which can impact
communication efficiency. In addition, human resources
from different countries will have different characteristics in
terms of the level of education, skill, and experience. This
can impact productivity rate and quality of the software.
2.2) Product architecture
The product architecture determines whether dispersed
sites can work simultaneously and harmoniously. It will
affect the selection of the development strategy.
2.3) Development strategy

191

4
Development strategy (or task allocation strategy) is
concerned with how tasks or work products are allocated
across sites. Development strategy will have a direct impact
on software development operations since it impacts
working hours per day, distribution overhead, and
distribution effort loss. There are three fundamental
development strategies which are module-based, phasebased, and follow-the-sun strategies [1, 14]. Each strategy
will have different impact on project performance [14].
2.4) Distribution overhead
There are two types of distribution overhead:
management overhead and transfer overhead. On GSD
projects, management also needs extra time to decide which
tasks are to be distributed across sites and to monitor the
development task since more work will be done at different
places in a shorter time [29, 30].
Transfer overhead occurs when the work product is
handed over from one site to another [31]. The higher the
transfer frequency is, the higher the transfer overhead. This
additional overhead impacts effort and duration of the GSD
project.
2.5) Distribution effort loss
Distribution effort loss is the additional effort lost due to
unfinished tasks [2]. This effort loss can occur when the
development includes dependent tasks. For example,
coding and testing, if the team from one site cannot finish
coding on time, the developers in other development site
cannot start testing.

focus on main GSD quantitative models including works
from Herbsleb et al. [5, 6, 36], Taweel and Brereton [31],
and Espinosa and Carmel [37, 38]. We found that these
models capture some of the factors mentioned in pervious
section. In this section, we will briefly describe these three
models and discuss how they can be incorporated into the
GSD model.

3) Organizational factors:
GSD introduces new forms of development teams -the
global team or virtual team. The performance of the team
has a strong impact on the performance of the project. The
factors in this category are concerned with the impacts from
virtual teams, which include team formulation and team
dynamics.
3.1) Team formulation
Building relationships and trust between team members
are the prerequisite for team formulation. Without trust, it is
unlikely that a team will work together effectively since the
team members are unwilling to communicate openly across
sites. Several studies [1, 20, 23, 32-34] have emphasized
the importance of having enough time for members to get to
know one another or to have a kick-off meeting at the
beginning of the project. Thus, the virtual team that
establishes trust between members at the beginning of the
project is likely to have better performance.
3.2) Team dynamics
Although trust may have already been established among
distributed teams at the beginning of a project, without
personal interaction or face-to-face communication, the
level of trust will decrease over time [35]. The decrease in
the level of trust will negatively affect the team
performance. Therefore, it is important that trust be
reestablished regularly and continually to maintain the
efficiency and effectiveness of a virtual team [1].

2) Taweel and Brereton
Taweel and Brereton [31] develop a mathematical model
that represents the relationships between development time
(using follow-the-sun strategy) and several factors including
development site, time-zone, project resource, and
distribution overheads.
Contribution to GSD Model: This model identifies the
important factors and their relationship to cycle time that
need to be incorporated into the GSD model. The most
important contribution is the study of different types of
distribution overhead and how they affect the cycle time.

B. GSD Quantitative Models
There are several studies and quantitative researches on
GSD and related fields. For the initial GSD model, we will

1) Herbsleb et al.
Herbsleb et al. [5] found that distributed work items
appear to take about two and a half times longer to complete
than similar items where the work is co-located.
Regression and graphical models were developed to identify
the mechanisms that contribute to the delay [6]. They found
that distance reduces communication frequency, which
results in the ineffectiveness of social network and lack of
“teamness”. This leads to higher number of people involved
in distributed work. The number of people involved is a
reasonable indicator of coordination issues. Higher number
of people suggests less effective communication and
coordination, which can lower the productivity of the
developer [36].
Contribution to GSD Model: The mechanism of delay
helps define the relationship between the communication
problem and the coordination problem factors, and how they
affect the productivity and performance of the project. This
knowledge provides a foundation in modeling the impact of
GSD.

3) Espinosa and Carmel
Espinosa and Carmel developed a mathematical model
[37, 38], extended from a coordination model developed by
Malone [39, 40], to represent coordination costs due to time
separation. The results of the model illustrate the impact of
time-zone differences on project cycle time in terms of
coordination costs (cost due to delays) and vulnerability
costs (costs due to unclear messages).
Contribution to GSD Model: This model emphasizes the
importance of time-zone differences.
The unique
contribution is on how time-zone differences affect
coordination through the use of different communication
mediums, which can further lead to delays and rework.
One can see that each of these three models capture
several important factors mentioned in the previous section,
but not all. This indicates the need to have a model that can
integrate these models and other important factors together
in order to better represent a GSD project.

192

5
C. GSD Model Components
As mentioned before, we believe that the ideal SPSM for
representing GSD projects would have to effectively
support both system dynamics equations and discrete event
logic. Thus, our proposed model consists of two major
components including continuous model and discrete-event
model. The discrete-event (DES) model will represent the
actual software development process steps.
The DES section can be divided into several sub-models
depending on the number of development sites. In other
words, each development site has its own DES sub-model.
Each development site may have different process steps
depending on how tasks are allocated and the activities
needed to be performed. This allows us to capture the
impact of development strategy on performance. Difference
in time-zones can also be modeled by having different site
operates at different working hours. An artifact or work
product will actually be passed between sites at the end of
working day in order to capture the effect of distribution
overhead and distribution effort loss.
The continuous portion of the model will include one
global continuous sub-model as well as a site-specific submodel for each development site. The global sub-model
captures the overall project environment which includes
planning and controlling activities. The information about
the project progress will come from the DES model. The
global continuous sub-model compares the project progress
to the plan and adjusts the plan as necessary.
On the other hand, each development site will have its
own site-specific continuous sub-model. The site-specific
sub-model represents aspects that may be different between
development sites including human resource, productivity,
and error generation and detection rates. The productivity
and the number of developers available will be sent to the
corresponding DES sub-model to drive the development
activities.
The factors associated with communication problems,
coordination problems, cultural and language differences
will be applied to the productivity rate before sending this
rate to the DES sub-model in order to capture the impact of
GSD. The picture of the prototype model that we currently
develop is shown in figure 1 on the next page.
The discrete-event paradigm and continuous paradigm
compliment each other and together enable the construction
of models that capture both the dynamic nature of project
variables as well as the complex sequences of discrete
activities that take place. This combined model will allow a
series of discrete process steps to be executed in a
continuously varying project environment, which allow us
to better represent the GSD project.

SPSMs can then be used to support and improve corporate
decisions by addressing practical questions about GSD in a
broad or project-specific manner. We believe that the
hybrid simulation model combining continuous and
discrete-event paradigms will be an ideal platform for
constructing a GSD model. In this paper, we identify the
factors that need to be included in the model, review the
quantitative model that can be integrated to the GSD model,
and provide an overview of the components we will be
including in our prototype GSD model.

VI. CONCLUSION
In conclusion, software process simulation models can be
used to support empirical studies and experiments in a GSD
context. Using SPSMs will speed the development of
knowledge about GSD. This will help in evaluating,
supporting, and enriching theories about GSD. With the
knowledge and understanding at the theoretical level,

193

6

Figure 1: GSD Prototype Model (high-level view)

194

7
REFERENCES
[1]
[2]

[3]
[4]

[5]

[6]

[7]

[8]
[9]

[10]

[11]

[12]

[13]

[14]

[15]

[16]

[17]

[18]

[19]
[20]

[21]

[22]
[23]

E. Carmel, Global Software Teams. Upper Saddle River, NJ: Prentice
Hall PTR, 1999.
I. Gorton and S. Motwani, "Issues in Co-operative Software
Engineering Using Globally Distributed Teams," Information and
SoftwareTechnology, vol. 38, pp. 647-655, 1996.
J. D. Herbsleb and D. Moitra, "Global Software Development," IEEE
Software, pp. 16-20, 2001.
J. Norbjerg, E. C. Havn, and J. P. Bansler, "Global Production: The
Case of Offshore Programming," presented at Wirtschaftsinformatik
‘97, Physica-Verlag, Berlin, 1997.
J. D. Herbsleb, R. E. Grinter, and T. A. Finholt, "An Empirical Study
of Global Software Development: Distance and Speed," presented at
ICSE 2001, Toronto, Canada, 2001.
J. D. Herbsleb and A. Mockus, "An Empirical Study of Speed and
Communication in Globally Distributed Software Development,"
IEEE Transactions on Software Engineering, vol. 29, pp. 481-494,
2003.
M. I. Kellner, R. J. Madachy, and D. M. Raffo, "Software Process
Simulation Modeling: Why? What? How?," Journal of Systems and
Software, vol. 46, 1999.
T. Abdel-Hamid and S. Madnick, Software Project Dynamics: An
Integrated Approach: Prentice-Hall, 1991.
J. Munch, D. Rombach, and I. Rus, "Creating an Advanced Software
Engineering Laboratory by Combining Empirical Studies with
Process Simulation," presented at The International Workshop on
Software Process Simulation and Modeling, Portland, OR, USA,
2003.
I. Rus, S. Biffl, and M. Halling, "Systematically Combining Process
Simulation and Empirical Data in Support of Decision Analysis in
Software Development," presented at The fourteenth International
Conference on Software Engineering and Knowledge Engineering
(SEKE'02), Ischia, Italy, 2002.
P. A. Fishwick and R. B. Modjeski, Knowledge-Based Simulation:
Methodology and Application, vol. 4. New York: Springer-Verlag
New York, Inc., 1991.
T. Wickenberg and P. Davidsson, "On Multi Agent Based Simulation
of Software Development Processes," presented at Multi-Agent-Based
Simulation, Third International Workshop, MABS 2002, Bologna,
Italy, 2002.
W. Wakeland, R. Martin, and D. M. Raffo, "Using Design of
Experiments, Sensitivity Analysis, and Hybrid Simulation to Evaluate
Changes to a Software Development Process: A Case Study,"
presented at The International Workshop on Software Process
Simulation and Modeling, Portland, OR, USA, 2003.
D. M. Raffo, S. Setamanit, and W. Wakeland, "Towards a Software
Process Simulation Model of Globally Distributed Software
Development Projects," presented at The International Workshop on
Software Process Simulation and Modeling, Portland, OR, USA,
2003.
B. Curtis, H. Krasner, and N. Iscoe, "A Field Study of the Software
Design Process for Large Systems," Communications of the ACM, vol.
31, pp. 1268-1287, 1988.
R. E. Kraut and L. A. Streeter, "Coordination in Software
Development," Communications of the ACM, vol. 38, pp. 69-81,
1995.
J. D. Herbsleb and R. E. Grinter, "Splitting the Organization and
Integrating the Code: Conway's Law Revisited," presented at
International Conference on Software Engineering (ICSE'99), Los
Angeles, CA, 1999.
J. D. Herbsleb and R. E. Grinter, "Conceptual Simplicity Meets
Organizational Complexity: Case Study of a Corporate Metrics
Program," presented at International Conference on Software
Engineering, Kyoto, Japan, 1998.
T. J. Allen, Managing the Flow of Technology. Cambridge, MA: MIT
press, 1977.
D. W. Karolak, Global Software Development: Managing Virtual
Teams and Environments. Los Alamitos, CA: IEEE Computer
Society, 1998.
E. Carmel and R. Agarwal, "Tactical Approached for Alleviating
Distance in Global Software Development," IEEE Software, pp. 2229, 2001.
R. D. Battin, et al., "Leveraging Resources in Global Software
Development," IEEE Software, pp. 70-77, 2001.
D. E. Damian and D. Zowghi, "The Impact of Stakeholders'
Geographical Distribution on Managing Requirements in a Multi-site

[24]

[25]
[26]

[27]

[28]
[29]
[30]
[31]

[32]
[33]

[34]
[35]
[36]

[37]

[38]

[39]
[40]

195

Organization," presented at IEEE Joint International Conference on
Requirements Engineering, Essen, Germany, 2002.
G. Hofstede, Culture's Consequences: International Differences in
Work-Related Values. Newbury Park, CA: Sage Publications, Inc.,
1984.
E. T. Hall, Beyond Culture. New York, NY: Doubleday Books, 1976.
L. Keil and P. Eng., "Experiences in Distributed Development: A
Case Study," presented at The International Workshop on Global
Software Development, Portland, OR USA, 2003.
H. Ishii, "Cross-Cultural Communication and CSCW," in Global
Networks: Computers and International Communication, L. M.
Harasim, Ed. Cambridge, MA: MIT Press, 1993, pp. 143-151.
E. Carmel, "The explosion of global software teams," in
Computerworld, 1997.
G. H. Anthes, "Software Development Goes Global," in
Computerworld, 2000.
R. Fournier, "Teamwork is the Key to Remote Development," in
Inforworld, 2001.
A. Taweel and O. P. Brereton, "Developing Software Across Time
Zones: An Exploratory Empirical Study," presented at Informatica,
Ljubljana, Slovenia, 2002.
D. E. Perry, G. S. Gil, and L. G. Votta, "A Case Study of Successful
Geographically Separated Teamwork."
E. Harding, "US Companies Finding that CASE Travels Well in India
-- Surplus of Skilled Software Professional Makes Outsourceing, Joint
Projects Attractive," in Software Magazine, vol. 11, 1991, pp. 24-28.
B. Geber, "Virtual Teams," Training, vol. 34, pp. 36-40, 1995.
D. Meyer, "A. Tech talk: how managers are stimulating global R&D
communication," Sloan Management Review, 1991.
J. D. Herbsleb and A. Mockus, "Formulation and Preliminary Test of
an Empirical Theory of Coordination in Software Engineering,"
presented at the ACM SIGSOFT Symposium on the Foundations of
Software Engineering (FSE), Helsinki, Finland, 2003.
J. A. Espinosa and E. Carmel, "Modeling Coordination Costs Due to
TIme separation in Global Software Teams," presented at The
International Workshop on Global Software Development, Portland,
OR USA, 2003.
J. A. Espinosa and E. Carmel, "Modeling the Effect of Time
Separation on Coordination Costs in Global Software Teams,"
presented at The 37th Hawaii International Conference on System
Sciences, Hawaii, USA, 2003.
T. W. Malone, "Modeling Coordination in Organization and
Markets," Management Science, vol. 33, pp. 1317-1332, 1987.
T. W. Malone and S. A. Smith, "Modeling the Performance of
Organization Structures," Operations Research, vol. 36, pp. 421-436,
1988.

196

Author Index

Acuña, S.T. ............................................... 26
Angkasaputra, N. ..................................... 83
Barros, M.O. ...................................... 9, 110
Birkhoelzer, T. ............................... 169, 185
Capiluppi, A. .......................................... 179
Cau, A. ..................................................... 63
Collofello, J.S. ....................................... 102
Concas, G. ................................................ 63
Dantas, A.R. ........................................... 110
Dickmann, C. ......................................... 169
Doo-Hwan, B. .......................................... 73
Franczyk, B. ............................................. 48
Gannod, G.C. ......................................... 102
Gómez, M. ................................................ 26
Gonzalez-Barahona, J.M. ........................ 16
Hoek, A. van der .................................... 185
Houston, D. ............................................ 144
Hurtado, N. ............................................ 117
Jensen, C. ................................................. 39
Juristo, N. ................................................. 26
Keungsik, C. ............................................ 73
Kiebusch, S. ............................................. 48
Kirk, D. .................................................. 155
Madachy, R. ..................................... 95, 160
Melis, M. .................................................. 63
Menzies, T. .............................................. 57
Merelo, J.J. ............................................... 16
Münch, J. ............................................... 164
Murphy, B. ................................................. 3

Navarro, E.O. ......................................... 185
Nayak, U. ............................................... 139
Noujeim, C. ............................................ 126
Osterweil, L.J. ........................................ 175
Pfahl, D. ................................................... 83
Raffo, D. .................................. 57, 139, 189
Ramil, J.C.F. .......................................... 179
Raunak, M.S. ......................................... 175
Robles, G. ................................................ 16
Ruiz, M. ................................................. 117
Sandrock, J. ............................................ 126
Scacchi, W. .............................................. 39
Setamanit, S. .......................................... 189
Shaw, M. .................................................... 3
Smith, N. ................................................ 179
Speck, A. .................................................. 48
Stubenrauch, J. ....................................... 169
TagGon, K. .............................................. 73
Tempero, E. ........................................... 155
Torres, J. ................................................ 117
Turnu, I. ................................................... 63
Vaupel, J. ............................................... 169
Veronese, G.O. .......................................... 9
Wakeland, W. ........................................ 139
Weinhardt, C. ......................................... 126
Weiss, D.M. ............................................... 3
Werner, C.M.L. .................................. 9, 110
Yu, C. ..................................................... 102

197

Evaluating the Effectiveness of Process Improvements on Software
Development Cycle Time via System Dynamics Modeling
John D. Tvedt and James S. Collofello
Arizona State University
Department of Computer Science and Engineering
Tempe, AZ 85287-5406 USA
E-mail: {tvedt, collofello}@asu.edu
Abstract
Reducing software development cycle time without
sacrificing quality is crucial to the continued success of
most software development organizations. Software
companies are investing time and money in reengineering
processes incorporating improvements aimed at reducing
their cycle time. Unfortunately, the impact of process
improvements on the cycle time of complex software
processes is not well understood. The objective of our
research has been to provide decision makers with a
model that will enable the prediction of the impact a set
of process improvements will have on their software
development cycle time. This paper describes our initial
results of developing such a model and applying it to
assess the impact of software inspections. The model
enables decision makers to gain insight and perform
controlled experiments to answer "What if?" type
questions, such as, "What kind of cycle time reduction can
I expect to see if I implement inspections?" or "How
much time should I spend on inspections?"

1: Background
1.1: Importance of reducing cycle time
The rush to reengineer processes taking place in
many organizations is the result of increasing competitive
pressures to shorten development cycle time and increase
quality. Reducing the time to produce software benefits
not only the customer, but also the software organization.
A number of benefits of reduced cycle time are listed by
Collier [4].
• Market life of the product is extended.

• Being the first to market along with the high
cost of switching products can give a product a
great advantage in capturing and obtaining a
large market share.
• Higher profit margins. Being first to market
gives a company greater freedom in setting
profit margins.
• The ability to start later and finish on
schedule. Starting later allows the use of
technological advances that may not have been
available to the competition.
"According to a study released in January 1990 by United
Research Co. of Morristown, N.J., six out of 10 CEOs
listed shorter product development cycles as vital to their
company..." [9].
Most of these cycle time reduction reengineering
efforts attempt to take advantage of process improvement
technology such as that advocated by the Software
Engineering Institute's Capability Maturity Model and
ISO 9000. The implication is that a mature development
process increases the quality of the work done, while
reducing cost and development time [8]. Some software
companies that have begun to mature their process have,
indeed, reported cycle time reductions via process
improvements [5].

1.2: The need for selecting among alternative
process improvements
A common problem faced by organizations
attempting to shorten their cycle time is selecting among
the numerous process improvement technologies to
incorporate into their newly reengineered development
process. Even though there is promising evidence

trickling into the pages of computer publications, most
companies are still skeptical of the investment that must
be made before possible improvements will be
experienced. They recognize that process improvements
do not exist in isolation. The impact an improvement
has may be negated by other factors at work in the
particular development organization. For example,
consider a tool that allows some task to be completed in a
shorter period of time. The newly available time may
just be absorbed as slack time by the worker, resulting in
no change to cycle time. Thus, software organizations
need to know, in advance of committing to the process
improvement technology, the kind of impact they can
expect to see. Without such information, they will have
a hard time convincing themselves to take action. In
particular, a software organization needs to be able to
answer the following types of questions:
• What types of process improvements will have
the greatest impact on cycle time?;
• How much effort should be allocated to the
improvement?;
• What will the overall impact be on the
dynamics of software development?;
• How much will the process improvement
cost?;
• How will the process improvement affect cycle
time?; and
• How will this improvement be offset by
reduced schedule pressure, reduced hiring and
increased firing?
Today, the limited mental models often used to answer
these questions are insufficient. The mental models often
fail to capture the complex interaction inherent in the
system. "It is the rare [manager] whose intuition and
experience, when he is presented with the structure and
parameters of [only] a second order linear feedback system,
will permit him to estimate its dynamic characteristics
correctly," [2].

1.3: Current approaches for evaluating the
impact of process improvements
There does not exist a standard method for
determining the impact of specific process improvements
on cycle time. An ideal way for performing this
assessment is conducting a controlled experiment in

which all factors except the independent variable (the
process improvement) are kept constant. Although much
can be learned through this approach about the general
effectiveness of the process improvement, it is normally
not practical to experiment over the entire development
life cycle of a significant size project. Thus, it is often
impossible to assess the overall impact of the proposed
process improvement on development cycle time.
Another approach typically followed is the utilization
of some sort of "pilot project" to assess the new
technology.
Although considerably weaker than
controlled experimentation, the pilot studies often reveal
the general merit of the proposed technology. It remains
difficult to assess the unique impact of the proposed
improvement technology on cycle time due to the
interaction of other project variables and to generalize the
results to other projects.
A third approach to assessing the impact of process
improvements is the utilization of traditional cost
estimation models such as COCOMO [3] and SLIM [10].
These types of models contain dimensionless parameters
used to indicate the productivity and technology level of
the organization. The parameter values are determined by
calibration of the model to previous projects or by
answering a series of questions on a questionnaire. For
example, the Basic COCOMO model calculates the
number of man-months needed to complete a project as:
MM = C (KDSI)K, where
MM = number of man-months,
C = a constant,
KDSI = thousands of delivered source
instructions and
K = a constant.
The user enters the size of the code along with the two
predefined constants to determine the number of manmonths. C and K are determined by calibrating the model
to previous projects.
An important question for a software organization to
answer is how C and K should be modified to reflect the
impact of process improvements when no historical data
exists. One potential answer is to use the Intermediate or
the Advanced COCOMO models. These models contain
cost drivers that allow more information about the
software project to be input than the Basic model. Cost
drivers are basically multipliers to the estimated effort
produced by the model. Modern Programming Practices
is one such cost driver. If a software project is using
modern programming practices, the cost driver would be
set such that its multiplier effect would reduce the amount
of effort estimated. Two questions arise when using this
cost driver: what is the definition of modern

programming practices and how should the cost driver's
value be altered with the addition of a specific
improvement to the existing programming practices.
These questions have no simple answers because the cost
drivers are at a level of granularity too coarse to reflect the
impact of specific process improvements. Some
researchers also believe that models such as COCOMO
are flawed due to their static nature. Abdel-Hamid states,
"The problem, however, is that most such tools are static
models, and are therefore not suited for supporting the
dynamic and iterative planning and control of [resource
allocation]" [2].

2: Proposed approach for evaluating the
effectiveness of process improvements
2.1: Overview of system dynamics modeling
Due to the weaknesses of the approaches presented in
the previous section, we chose to evaluate the impact of
process improvements on software development cycle
time through system dynamics modeling. System
dynamics modeling was developed in the late 1950's at
M.I.T. It has recently been used to model "high-level"
process improvements corresponding to SEI levels of
maturity [12]. System dynamics models differ from
traditional cost estimation models, such as COCOMO, in
that they are not based upon statistical correlations, but
rather cause-effect relationships that are observable in a
real system. An example of a cause-effect relationship
would be a project behind schedule (cause) leading to
hiring more people (effect). These cause-effect
relationships are constantly interacting while the model is
being executed, thus the dynamic interactions of the
system are being modeled, hence its name. A system
dynamics model can contain relationships between people,
product and process in a software development
organization. The most powerful feature of system
dynamics modeling is realized when multiple cause-effect
relationships are connected forming a circular relationship,
known as a feedback loop. The concept of a feedback
loop reveals that any actor in a system will eventually be
affected by its own action. A simple example of the ideas
of cause-effect relationships and feedback loops affecting
people, product and process can be illustrated by the
following scenario:
Consider the situation in which developers,
perceiving their product is behind schedule
(cause), modify their development process by
performing fewer quality assurance activities
(effect/cause), leading to a product of lower
quality (effect/cause), but giving the temporary

perception that the product is back on schedule
(effect/close feedback loop).
The developers perceived the product to be behind
schedule, took action, and finally perceived the product to
be back on schedule, as a result of their actions.
Secondary effects due to the developers' actions, however,
such as the lower quality of the product, will also
eventually impact the perception the developers have as to
being on schedule and will necessitate further actions
being performed. These cause-effect relationships are
explicitly modeled using system dynamics techniques.
Because system dynamics models incorporate the
ways in which people, product and process react to
various situations, the models must be tuned to the
environment that they are modeling. The above scenario
of developers reacting to a product that is behind schedule
would not be handled the same way in all organizations.
In fact, different organizations will have different levels of
productivity due to the experience level of the people
working for them and the difficulty of the product being
developed. Therefore, it is unrealistic for one model to
accurately reflect all software development organizations,
or even all projects within a single development
organization.
Once a system dynamics model has been created and
tailored to the specific development environment, it can
be used to find ways to better manage the process to
eliminate bottlenecks and reduce cycle time. Currently,
the state-of-the-practice of software development is
immature. Even immature software development
environments, however, can benefit from this technique.
The development of the model forces organizations to
define their process and aids in identifying metrics to be
collected. Furthermore, the metrics and the models that
use them do not have to be exact in order to be useful in
decision-making [13]. The model and its use result in a
better understanding of the cause-effect relationships that
underlie the development of software. The power of
modeling software development using system dynamics
techniques is its ability to take into account a number of
factors that affect cycle time to determine the global
impact of their interactions, which would be quite difficult
to ascertain without a simulation. The model may be
converted to a management flight simulator to allow
decision-makers the ability to perform controlled
experiments of their development environment.

2.2: Modular system dynamics modeling
Due to the large number of process improvements
available for evaluation and the necessity of conducting
each evaluation in the context of a system dynamics

model customized to the organization for which the
evaluation is being performed, a modular system
dynamics modeling approach was adopted. A modular
system dynamics model consists of two parts, the base
software process model and the process improvement
model. Once a base model of the software development
process exists, any number of process improvement
models may be plugged in to determine the effect they
will have on software development cycle time. This
modularity gives the process improvement models the
advantage of being used in more than one base model of a
software development environment. For example, the
process improvement model could be integrated with a
base model of development environment "A" and also a
base model of development environment "B". The only
change to the process improvement model would be the
values of its parameters in order to calibrate it to the
specific development environment. The models of the
development environments may have to be modified
structurally, such that they include all necessary model
elements, e.g., system dynamics modeling rates and
levels, for integration.
A conceptual image of the modular model is shown
in Figure 1. On the left is a box representing an existing
system dynamics model of a software development
process. This model would be tailored to a particular
organization. On the right is a box representing the
model of the process improvement.

Process
Improvement

Existing Process

Interface

System dynamics elements
necessary to integrate the models

Figure 1.

Graphic of existing process with
process improvement.

The two models are integrated through the interface
pictured. Information will flow in both directions
between the models. This interface is dependent on
certain model elements, such as levels and rates, existing
in both models. The ovals in Figure 1 represent a cutaway view showing the underlying structure of the system
dynamics model and the elements needed for the interface.
An existing model of an organization may need to be

modified to take advantage of the proposed model, such
that it contains the necessary elements for the interface.

2.3: Approach for constructing modular system
dynamics models
An organization wishing to utilize system dynamics
modeling for assessing the impact of new technologies on
cycle time must perform the following steps:
1. Construct a base model of the software
development process. Construction of a base
model requires detailed modeling of the
organization's software development process as
well as identification of cause-effect relationships
and feedback loops. Sources of information for
model construction include: observation of the
actual software process, interviews of personnel,
analysis of metrics and literature reviews of
relevant publications. An organization wishing
to adapt an existing generic model for rough
assessments might consider the model developed
by Abdel-Hamid [1].
2. Construct the process improvement models.
For each process improvement being considered,
a modular process improvement model must be
developed utilizing the same approach as that of
the base model.
3. Validate the base and process improvement
models. Both the base and process improvement
models must be validated to their accuracy
requirements in order to build confidence in the
models ability to replicate the real system. The
validation process must consider the suitability
of the model for its intended purpose, the
consistency of the model with the real system,
and the utility and effectiveness of the model [6],
[7], [11].
4. Execute the models. The base model and any
of the process improvement models can be
combined to assess the impact of various process
improvements on cycle time.
Analysis of
outputs may also lead to new ideas and
understanding of the system triggering additional
process reengineering as well as consideration of
other process improvements.

3: Demonstration of approach

Our software inspection model does not incorporate the
following:

3.1: Model development
In order to illustrate the feasibility and usefulness of
system dynamics modeling for process improvement
assessment, we applied our approach to the software
inspection process. Our model has the ability to provide
answers to the types of questions, concerning process
improvements, posed in Section 1.2. For the purpose of
our demonstration, we focus mainly on the question of
cycle time reduction. We initially developed a base model
corresponding to a typical organization's waterfall
software development process. We then constructed a
model of the software inspection process which we
integrated with the base model. The software inspection
model enables manipulation of a number of variables
connected to the inspection process in order to understand
their impact on software development cycle time. Direct
manipulation of the following variables are allowed:
• The time spent on each inspection task per
unit of work to be inspected (e.g., the
preparation rate and the inspection rate);
• The size of the inspection team;
• The percent of errors found during inspection;
• The percent of tasks that undergo reinspection;
and
• The defect prevention attributable to the use of
inspections.
Our software inspection model is based on the following
assumptions:
• Time allocated to software inspections takes
time away from software development;
• A larger inspection team will consume more
man-hours per inspection than a smaller team;
• Software inspections find a high percentage of
errors early in the development life cycle; and
• The use of inspections can lead to defect
prevention, because developers get early
feedback as to the types of mistakes they are
making.

• Software developers achieve higher
productivity due to an increase in product
knowledge acquired through the software
inspection process;
• Software developers achieve improved design
estimates due to attention paid to size
estimates during inspection; and
• Software inspections lead to increased
visibility of the amount of work completed.
The model also excludes the interaction that the
inspection team size and the time spent performing
inspection tasks have on the percent of errors found during
inspection. The inspection team size, the time spent on
inspection tasks and the percent of errors found during
inspection can be set at the beginning of a model
simulation according to historical metrics or altered during
the actual simulation.
In order to judge the impact that software inspections
have on software development cycle time, the software
inspection model must be integrated into a model of the
software development process. Once integrated, the
software inspection model will impact a number of
elements in the software development process model.
Figures 2 and 3 are an incomplete, but representative view
of the integrated model. Figure 2 represents the process
steps and effort involved in inspecting work products.
Figure 2, however, does not reveal how time and
manpower are allocated to perform each step in the
inspection process, in order to keep the diagram and ideas
presented simple. Each rate in Figure 2 requires that
manpower be consumed in order to move work products
from one step to the next. Figure 3 shows an
incomplete, but representative implementation of the
interface between the base model of the software
development process and the process improvement model,
that is shown abstractly in Figure 1. Figure 3 represents
the modeling of errors in the base process model of
software development and illustrates the impact
inspections have on error generation and error detection in
the base process model. The impacts that software
inspections have on software development are: software
inspections consume development man-hours, errors are
less expensive to find during software inspection than
system testing, software inspections promote defect
prevention and software inspections reduce the amount of
error regeneration. Before discussing Figure 2 and Figure

3, a brief discussion of flow diagrams, a notation used to
represent system dynamics models, is in order.

Figure 2. Flow diagram of simplified
inspection process steps.

Figure 3.

Flow diagram of inspection's
impact on errors.

Flow diagrams are composed of levels, material
flows, rates, auxiliaries and information links. Levels,
depicted as rectangles, represent the accumulation of a
material in a system. For example, work products and
errors are materials that reside in levels in Figure 2 and
Figure 3. Material flows, depicted as hollow arrows,
indicated the ability for material to flow from one level to
another. Material flows connected to clouds indicate a
flow to, or from, a portion of the system not pictured.
Rates, depicted as circles attached to material flows,
represent the rate of material flow into and out of a level.
For example, error detection rate in Figure 3 controls the
rate at which potentially detectable errors are detected.
Auxiliaries, depicted as circles, aid in the formulation of
rate equations. In Figure 3, defect prevention is an
auxiliary that affects the error generation rate.
Information links, depicted as arrows, indicate the flow of
information in a system. Information about levels, rates
and auxiliaries are transferred using information links.
Information links, unlike material flows, do not affect the
contents of a level.
The first impact that software inspections have on
software development is in the allocation of man-hours
for development. Software inspections require that time
be set aside for them to be done properly. The time
consumed by software inspections is time that cannot be

used for other activities, such as writing software. The
amount of time that an inspection consumes is based on
the size of the inspection team and the time spent on
inspection tasks, e.g., the preparation rate and the
inspection rate. Figure 2 is a simplified view of the steps
that must be taken to perform inspections. It implicitly
models the effort expended to move work products
through all of the process steps associated with
inspection; effort that takes time away from development
activities.
The second impact that software inspections have on
software development is in the detection of errors. Errors
are found early in the life cycle. Errors are less expensive
to find and correct during development than during testing.
This impact is not explicitly shown in Figure 3, except
that a higher error detection rate will increase the number
of errors found early in the life cycle, rather than later
during testing.
The third impact that software inspections have on
software development is in the prevention of defects.
Successful inspections attack the generation of future
defects. Developers that are involved with inspections
learn about the types of errors that they have been making
and are likely to make during the project. This feedback
about the errors they are making leads to fewer errors
being made during the project. Figure 3 shows defect
prevention, due to inspections, impacting the rate of error
generation.
The fourth impact that software inspections have on
software development is a reduction in the regeneration of
errors. Errors that remain undetected often lead to new
errors being generated. For example, undetected design
errors will lead to coding errors, because they are coding
to a flawed design. Software inspections detect errors
early in the life cycle, thus reducing the amount of error
regeneration. Figure 3 indicates that the errors escaping
detection impact the error generation rate. The number of
errors escaping detection is dependent on the preparation
and inspection rates, as shown in Figure 3.
The four impacts that software inspections have on
software development, mentioned above, are the
foundation for the theory upon which the software
inspection model is based. Many details of the
implementation of the theory, using system dynamics
techniques, are absent from Figure 2 and Figure 3, but are
shown in detail elsewhere [14].

3.2: Example model output
The integrated model has been turned into a simple
management flight simulator, allowing for simple
experimentation. This section describes the user interface
of the simulator and the output generated by its use.

Figure 4 shows the simple interface to the simulator.
Input to the simulator is in the form of sliders. The
simple interface has just five slider inputs: an on/off
switch for inspections, the size of the inspection team,
the percent of errors found during inspection, the percent
of work products that fail inspection and the percent of
incorrect error fixes.

Figure 4.

User interface of the inspection
simulator.

Output from the simulator comes in two forms:
numeric displays and graphs. Numeric displays show the
current value of a simulation variable. Man-Days and
Work Products Completed are two examples of numeric
displays. Graphs, on the other hand, display the value of
simulation variables versus time. Each output curve is
labeled with a number for ease of reading. There may be
multiple units of measure on the vertical axis, each
matched to the number of the curve it is representing.
The unit of measure on the horizontal axis is days. The
five output curves represent: 1) currently perceived job
size in terms of work products, 2)ּcumulative work
products developed, 3) cumulative work products tested, 4)
total size of workforce and 5)ּscheduled completion date.
A demonstration of the use of the system dynamic
models for predicting the cycle time reduction due to a
process improvement is in order. Using the integrated
model of the baseline waterfall development life cycle and
the software inspection process improvement, it will be
shown how this modeling technique can be used for
evaluating the impact that a proposed process
improvement would have on development cycle time.
The following demonstration is a simulation of a
hypothetical software team employing the simple
inspection model presented in this paper. The project

being developed is estimated to be 64,000 lines of code
requiring a total workforce of eight developers at the
height of development. Two scenarios of the project
development are simulated holding all variables fixed,
except for the size of the inspection team and the percent
of errors found during inspection.
Figure 5 is the output generated by executing the
model with an inspection team size of six developers
discovering 40 percent of the errors during inspection.
When interpreting the graphical output, the story of the
project is revealed. From Figure 5, the following story
emerges. Curve 1, the currently perceived job size in
work products, reveals that the project size was initially
underestimated. As development progressed, the true size
of the project was revealed. Curve 5, the scheduled
completion date, was not adjusted even as it became
apparent that the project had grown in size. Instead, curve
4, the total size of workforce, indicates that the workforce
was increased in size. In addition, though not shown on
this graph, the workforce worked longer hours to bring
the project back on schedule. Curve 2, cumulative work
products developed, reveals that the project appeared to be
back on schedule, because there were no visible delays in
development of work products. It was not until system
testing that problems in development were discovered.
Curve 3, cumulative work products tested, reveals that
system testing did not go as smoothly as expected. The
poor performance of the inspection team pushed the
discovery of errors back to system testing. During
system testing it was revealed that there was a good
amount of rework to be done and as a result, the scheduled
completion date, curve 5, was once again pushed back.
1: Job Size in WP
1:

1500.00

2:
3:
4:
5:

20.00
500.00

2: WP Developed

3: WP Tested

4: Total Workforce

5
5

1:
2:
3:
4:
5:

5: Completion Date

5

1

1

2

2

1
4

750.00
4

1
10.00
250.00

4
2

1:
2:
3:
4:
5:

0.00
0.00
0.00
0.00

Main: Page 1

Figure 5.

3

2
3

3
125.00

250.00
Days

375.00
5:02 PM

500.00
1/28/95

Software inspection scenario 1.

Figure 6 is the output generated by executing the
model with an inspection team size of three developers
discovering 90 percent of the errors during inspection.
The story is much the same as that shown in Figure 5.
The big difference between Figures 5 and 6 is shown by
curve 3, cumulative work products tested. Using more
effective software inspections, this project was able to

discover errors early in the life cycle and correct them for
much less cost than if they had been found in system test.
In addition, there were no major surprises in system
testing as to the quality of the product developed.
Therefore, with no major amount of rework to be
performed in system test, the project was able to finish
close to its revised schedule.
1: Job Size in WP
1:
2:
3:

1500.00

4:
5:

20.00
500.00

2: WP Developed

3: WP Tested

4: Total Workforce

5

Approach, Prentice-Hall, Englewood Cliffs, New
Jersey, 1991.
[2]

Tarek K. Abdel-Hamid, "THINKING IN CIRCLES,"
American Programmer, May 1993, pp. 3-9.

[3]

Barry W. Boehm, SOFTWARE ENGINEERING
ECONOMICS, Prentice-Hall, Englewood Cliffs, New
Jersey, 1981.

[4]

Ken W. Collier and James S. Collofello, "Issues in
Software Cycle Time Reduction," International
Phoenix Conference on Computers and
Communications, 1995.

[5]

Raymond Dion, "Process Improvement and the
Corporate Balance Sheet," IEEE Software, July 1993,
pp. 28-35.

[6]

Jay W. Forrester, Industrial Dynamics, The M.I.T.
Press, Cambridge, MA, 1961.

[7]

Jay W. Forrester and Peter M. Senge, "Tests for
Building Confidence in System Dynamics Models,"
System Dynamics. TIMS Studies in Management
Sciences, 14 (1980), pp. 209-228.

[8]

Mark C. Paulk, Bill Curtis, Mary Beth Chrissis and
Charles V. Weber, "Capability Maturity Model,
Version 1.1," IEEE Software, July 1993, pp. 18-27.

[9]

T. S. Perry, "Teamwork plus Technology Cuts
Development Time," IEEE Spectrum, October 1990,
pp. 61-67.

[10]

Lawrence H. Putnam, "General empirical solution to
the macro software sizing and estimating problem,"
IEEE Transactions on Software Engineering, Vol. SE4, No. 4, July 1978, pp. 345-361.

[11]

George P. Richardson and Alexander L. Pugh III,
Introduction to System Dynamics Modeling with
DYNAMO, The M.I.T. Press, Cambridge, MA, 1981.

[12]

Howard A. Rubin, Margaret Johnson and Ed Yourdon,
"With the SEI as My Copilot Using Software Process
'Flight Simulation' to Predict the Impact of
Improvements in Process Maturity," A m e r i c a n
Programmer, September 1994, pp. 50-57.

[13]

George Stark, Robert C. Durst and C. W. Vowell,
"Using Metrics in Management Decision Making,"
IEEE Computer, September 1994, pp. 42-48.

[14]

John D. Tvedt, "A System Dynamics Model of the
Software Inspection Process," Technical Report TR95-007, Computer Science and Engineering
Department, Arizona State University, Tempe,
Arizona, 1995.

5: Completion Date

1
2

1:
2:
3:
4:
5:

5

1
4

750.00

4

1
10.00
250.00

4
3

2
1:
2:
3:
4:
5:

0.00
0.00
0.00
0.00

2

Main: Page 1

Figure 6.

3

3
125.00

250.00

375.00

Days

5:20 PM

500.00
1/28/95

Software inspection scenario 2.

4: Conclusions and future work
Our research grew out of the questions posed in
Section 1.2 concerning the impact of process
improvements on software development cycle time. Our
approach in answering those questions has been to use
system dynamics modeling to model the software
development process, allowing experimentation with the
system. We have tried to demonstrate how this technique
may be used to evaluate the effectiveness of process
improvements. At this point in our work we have
developed a base model of the waterfall development life
cycle and a process improvement model of software
inspections. We plan to continue this effort by
developing a base model of the incremental development
process and creating a library of process improvement
models. Some examples of process improvements that
we plan to add to our library are meetingless inspections,
software reuse and risk management. We then plan to
validate our base and process improvement models in
several software development organizations. Finally,
another area of future research is to generalize the interface
between the base process models of software development
and the models of process improvements. The interface is
analogous to tool interfaces. A well defined, generalized
interface would facilitate integration of base process
models with process improvement models.

References
[1]

Tarek Abdel-Hamid and Stuart E. Madnick,
SOFTWARE PROJECT DYNAMICS An Integrated

Variable Strength Interaction Testing of Components
Myra B. Cohen
Peter B. Gibbons
Warwick B. Mugridge
Dept. of Computer Science
University of Auckland
Private Bag 92019
Auckland, New Zealand
myra,peter-g,rick  @cs.auckland.ac.nz

Abstract
Complete interaction testing of components is too costly
in all but the smallest systems. Yet component interactions
are likely to cause unexpected faults. Recently, design of experiment techniques have been applied to software testing
to guarantee a minimum coverage of all  -way interactions
across components. However,  is always fixed. This paper
examines the need to vary the size of  in an individual test
suite and defines a new object, the variable strength covering array, that has this property. We present some computational methods to find variable strength arrays and provide
initial bounds for a group of these objects.

1. Introduction
In order to shorten development times, reduce costs and
improve quality, many organizations are developing software utilizing existing components. These may be commercial off-the-shelf (COTS) or internally developed for
reuse among products. The utilization of existing components requires new development and verification processes
[2]. In particular the interaction of these new components
with each other as well as with the newly developed components within the application must be tested.
Software is becoming increasingly complex in terms of
components and their interactions. Traditional methods of
testing are useful when searching for errors caused by unmatched requirements. However, component based development creates additional challenges for integration testing. The problem space grows rapidly when searching
for unexpected interactions. Suppose we have five components, each with four possible configurations. We have
	


potential interactions. If we combine 100 com-

Charles J. Colbourn
James S. Collofello
Dept of Computer Science and Engineering
Arizona State University
P.O. Box 875406
Tempe, Arizona 85287
charles.colbourn,collofello  @asu.edu


ponents there are
possible combinations. This makes
testing all combinations of interactions infeasible in all but
the smallest of systems.
Instead one can create test suites that guarantee pairwise
or  -wise coverage. For instance we can cover all pairwise
interactions for ten components, each with four possible
configurations,
only
 

!" using$
#  25 test cases. A covering
$# array,
is an
array such that every

 subarray contains all ordered subsets from symbols of size 
at
 least once. The covering array number

%&
! is the minimum
required to satisfy the parameters 
.
Covering arrays have been used for software interaction
testing by D. Cohen et al. in the Automatic Efficient Test
Generator (AETG) [5]. Williams et al. use these to design
tests for the interactions of nodes in a network [12]. Dalal
et al. present empirical results suggesting that testing of
all pairwise interactions in a software system indeed finds a
large percentage of existing faults [7]. In further work, Burr
et al. provide more empirical results to show that this type
of test coverage is effective [3].
In a software
test, the columns of the covering array rep
resent the components or fields. Each component
'# has

levels or configurations.
The
final
test
suite
is
an
ar
ray where is the number of test cases and each test contains one configuration from each component. By mapping
a software test problem to a covering array of strength  we
can guarantee that we have tested all  -way interactions.
In many situations pairwise coverage is sufficient for
testing. However, we must balance the need for stronger
interaction
testing

 
(
%with
)
" the cost of running tests. For instance a
*+%, 
can
)
!-" be achieved for as little as 16
tests, while a
requires at least 64 tests. In
order to appropriately use our resources we want to focus
our testing where it has the most potential value.
The recognition that all software does not need to be

tested equally is captured in the concept of risk-based testing [1]. Risk-based testing prioritizes testing based on
the probability of a failure occurring and the consequences
should the failure occur. High risk areas of the software are
identified and targeted for more comprehensive testing.
The following scenarios point to the need for a more flexible way of examining interaction coverage.
We completely test a system, and find a number of
components with pairwise interaction faults. We believe this may be caused by a bad interaction at a higher
strength, i.e. some triples or quadruples of a group
of components. We may want to revise our testing
to handle the “observed bad components” at a higher
strength.
We thoroughly test another system but have now revised some parts of it. We want to test the whole system with a focus on the components involved in the
changes. We use higher strength testing on certain
components without ignoring the rest.
We have computed software complexity metrics on
some code, and find that certain components are more
complex. These warrant more comprehensive testing.
We have certain components that come from automatic
code generators and have been more/less thoroughly
tested than the human generated code.
One part of a project has been outsourced and needs
more complete testing.
Some of our components are more expensive to test
or to change between configurations. We still want to
test for interactions, but cannot afford to test more than
pairwise interactions for this group of components.
While the goal of testing is to cover as many component interactions as possible, trade-offs must occur. This
paper examines one method for handling variable interaction strengths while still providing a base level of coverage. We define the variable strength covering array, provide
some initial bounds for these objects and outline a computational method for creating them.

2. Background
Suppose we are testing new integrated RAID controller
software. We have four components, (RAID level, operating system (OS), memory configuration and disk interface).
Each one of these components has three possible configurations. We need 81 tests to test all interactions. Instead
we can test all pairwise interactions of these components
with only nine tests. Perhaps though, we know that there

RAID
Level
RAID 0
RAID 1
RAID 5

Component
Operating
Memory
System
Config
Windows XP
64 MB
Linux
128 MB
Novell Netware 6.x
256 MB

Disk
Interface
Ultra-320 SCSI
Ultra-160 SCSI
Ultra-160 SATA

Table 1. Raid integrated controller system: 4
components, each with 3 configurations

are more likely to be interaction problems between three
components: RAID level, OS and memory. We want to
test these interactions more thoroughly. But it may be too
expensive to run tests involving all three way interactions
among components. In this instance we can use three-way
interaction testing among the first three components while
maintaining two-way coverage for the rest. We still have a
minimal coverage guarantee across the components and we
still do not need to run 81 tests. The test suite shown in Table 2 provides this level of variable strength coverage with
27 tests.

RAID
Level
RAID 0
RAID 0
RAID 5
RAID 1
RAID 0
RAID 0
RAID 1
RAID 0
RAID 5
RAID 0
RAID 5
RAID 5
RAID 1
RAID 5
RAID 0
RAID 1
RAID 0
RAID 1
RAID 1
RAID 5
RAID 1
RAID 1
RAID 1
RAID 5
RAID 5
RAID 5
RAID 0

Component
Operating Memory
System
Config
Linux
128 MB
Novell
128 MB
Linux
64 MB
XP
128 MB
Novell
256 MB
XP
128 MB
Novell
256 MB
Linux
64 MB
XP
256 MB
XP
64 MB
Novell
128 MB
XP
128 MB
Linux
256 MB
XP
64 MB
XP
256 MB
Linux
128 MB
Novell
64 MB
Novell
64 MB
Linux
64 MB
Novell
64 MB
XP
256 MB
Novell
128 MB
XP
64 MB
Linux
256 MB
Linux
128 MB
Novell
256 MB
Linux
256 MB

Disk
Interface
Ultra 160-SCSI
Ultra 160-SCSI
Ultra 160-SCSI
Ultra 160-SCSI
Ultra 320
Ultra 160-SCSI
Ultra 160-SCSI
Ultra 320
Ultra 320
Ultra 160-SATA
Ultra 320
Ultra 160-SATA
Ultra 160-SCSI
Ultra 160-SATA
Ultra 160-SATA
Ultra 320
Ultra 320
Ultra 160-SATA
Ultra 160-SCSI
Ultra 160-SCSI
Ultra 160-SATA
Ultra 160-SCSI
Ultra 160-SATA
Ultra 320
Ultra 320
Ultra 160-SCSI
Ultra 160-SATA

Table 2. Variable strength array for Table 1

Commercial software test generators, like AETG, provide only a fixed level of interaction strength [5]. We might
use this to build two separate test suites and run each independently, but this is a more expensive operation and
does not really satisfy the desired criteria. We could instead
just default to the higher strength coverage with more tests.
However, the ability to tune a test suite for specific levels
of coverage is highly desirable, especially as the number of
components and levels increases. Therefore it is useful to
define and create test suites with flexible strengths of interaction coverage and to examine some methods for building
these.

3. Definitions
 

!"
In a covering array,

,  is called the

strength, the degree and the order. A covering array
is optimal if it contains the minimum possible number of
rows. We
minimum numberthe
 call
  this
  covering
 
(
%)
 , " array


!"
 
number,
. For example,

[4].
A
mixed
covering array, denoted
a

+  
%level
 # as
&
   
! 
 
!"!"


, is an
array on sym

	
   , with the following properties:
bols, where
 
 ("
1. Each column 
contains only elements


.
from a set   with    
 #
2. The rows of each
 sub-array cover all  tuples
of values from the  columns at least once.
We can use a shorthand notation to describe our mixed covering array by combining  ’s that are the same. For exam


 two

we can write this .
ple if we have three
’s 
 of
 size
     "!"
Consider an
can also
  
 
  
!
  
!. "This

"


be written as an
where

  	   
 	      	
 
 
   .
and
The following holds:

  


1. The columns are partitioned into  groups   
 


where group  contains
columns. The first 
 
columns belong to the group   , the next
columns

belong to group  , and so on.
  
2. If column "!#  , then  %$&
.

We can nowuse
this
for a fixed-level covering
*+
 
!notation
"

array as well.
indicates that there are pa
rameters each containing a set of symbols. This makes it
easier to see that the values from different components can
come from different symbol sets.
A variable strength covering array,
 #*) denoted as a
'  
  
!  
 
(" 
  "


, is an
mixed level

covering array, of strength  containing , a multi-set of
disjoint mixed level covering arrays each of strength +  .


We can abbreviate duplicate
’s in the multi-set us
ing a similar notation to that of the covering
arrays.
+ , 
 , Sup"
pose
we
have
two
identical
covering
arrays
in



*+%, 
 , "
. This can be written as
.
Ordering
of
the
' 
columns in the representation of a
 is important since
the columns of the covering arrays in are listed consecu
tively from left to right.
'  
,  

 ,- 
 
.  
&, %, 
 ,  "/0 "
An example of a
can be seen in Table 3. The overall array is a mixed level
array of strength two with nine columns containing three
symbols and two columns containing two. There are three
sub-arrays each with strength three. All three way interactions therefore among columns 0-2, 3-5, 6-8 are included.
All two way interactions among all columns are also covered. This has been
*+achieved
%, 
 ,  " with 27 rows which is the optimal size for a
. A covering array that would
cover all three way interactions for all 11 columns, on the
other hand, might need as many as 52 rows.

4. Construction Methods
Fixed strength covering arrays can be built using algebraic constructions if we have certain parameter combina

tions of 
and [4, 12]. Alternately we may use computational search algorithms [5, 6, 11]. Greedy algorithms
form the basis of the AETG and the IPO generators [5, 11].
It has also been shown that simulated annealing is effective
for building covering arrays of both mixed and fixed levels
[6, 10]. Since there are no known constructions for variable
strength arrays at the current time we have chosen to use
a computational search technique to build these. We have
written a simulated annealing program that has been used
to produce all of the results presented in this paper.

4.1. Simulated Annealing
Simulated annealing is a variant of the state space search
technique for solving combinatorial optimization problems.
The hope is that the algorithm finds close to an optimal solution. Most of the time, however, we do not know when
we have reached an optimal solution for the covering array
problem. Instead such a problem can be specified as a set 1
"
of feasible solutions (or states) together with a cost 2  associated with each feasible solution  . An optimal solution
corresponds to a feasible solution with overall (i.e. global)
minimum cost. For each feasible solution 3!41 , we define
a set 576 of transformations (or transitions), each of which
can be used to change  into another feasible solution 98 .
The set of solutions that can be reached from  by applying
  "

a transformation from 5:6 is called the neighborhood
of  .
To start, we randomly choose an initial feasible solution. The algorithm then generates a set of sequences (or

0
0
1
2
2
2
1
0
2
1
1
0
2
0
1
1
2
0
2
0
1
2
0
1
0
1
2

2
1
2
1
2
0
0
0
0
2
0
1
2
0
2
0
1
2
0
1
1
1
0
1
2
1
2

Table 3.

2
2
2
0
0
0
0
0
2
0
2
0
1
2
1
1
2
1
1
1
2
1
1
1
0
0
2

0
1
2
0
2
0
1
0
2
0
2
0
2
2
2
1
2
1
1
1
0
0
1
1
1
2
0

1
2
1
0
2
2
1
2
0
1
1
0
1
2
0
0
0
2
0
0
1
0
1
1
2
2
2

2
0
0
2
0
2
2
1
0
0
2
0
1
1
2
1
1
1
0
2
1
1
0
1
2
2
0

2
1
2
2
1
2
1
0
2
0
1
1
0
1
0
1
0
2
0
1
0
2
2
2
0
1
0

2
1
1
0
2
2
2
0
0
2
1
1
0
2
1
0
2
2
0
0
1
1
1
0
2
0
1

2
0
0
0
1
1
0
1
1
1
2
1
2
2
1
1
0
0
0
2
0
2
1
2
2
0
2

0
0
0
0
1
1
1
1
0
0
1
1
0
0
0
0
1
1
0
0
0
0
1
1
1
0
1

1
0
0
1
0
0
0
1
1
1
0
1
1
0
0
1
1
1
1
0
0
0
1
0
0
1
1

, - 
  
.  
,  , 
 ,  "  0"
' * 
,  
(
 

 " 
 "
Markov chains) of trials. If 2  8
2  then the transition is accepted. If the transition results in a feasible solution  8 of higher cost, then  8 is accepted with probability
 6
	 
 6
 , where 5 is the controlling temperature of
the simulation. The temperature is lowered in small steps
with the system being allowed to approach “equilibrium” at
each temperature through a sequence of trials at this tem
perature.
Usually this is done by setting 5
5 , where

(the control decrement) is a real number slightly less than

.
The idea of allowing a move to a worse solution helps
avoid being stuck in a bad configuration (a local optimum),
while continuing to make progress. Sometimes we know
the cost of an optimal solution and can stop the algorithm
when this is reached. Otherwise we stop the algorithm when
insufficient progress is being made (as determined by an
appropriately defined stopping condition). In this case the
algorithm is said to be frozen.
Simulated annealing has been used by Nurmela and
Östergård [9], for example, to construct covering designs
which have a structure very similar to covering arrays. It
has also been used by Stardom and Cohen, et al. to generate
minimal fixed strength covering arrays for software testing
[6, 10].
In the simulated annealing algorithm the current feasible

solution is an approximation  to a covering array in which
certain  -subsets are not covered. The cost function is based
on the number of  -subsets that are not currently covered.

A covering array itself will have cost . In the variable

strength array, the cost is when (i) all of the  -subsets are

covered and (ii) for each covering array of strength  8 in ,
all /8 -subsets are covered.
A potential transition is made
)
by selecting one of the -sets) belonging to  and then replacing) a random point in this -set by a random point not
in the -set. We calculate only the change in numbers of
 -sets that will occur with this move. Since the subsets of
are disjoint, anychange
we make can affect the overall


'
coverage of the
and at most one of the subsets of .
This means that the extra work required for finding variable
strength arrays over fixed strength arrays does not grow in
proportion to the number of subsets of higher strength.
We keep the number of blocks (test cases) constant
throughout a simulated annealing run and use the method
described by Stardom to determine a final array size [10].
We start with a large random array and then bisect our array
repeatedly until we find a best solution.

4.2. Program Parameters
Good data structures are required to enable the relative
cost of the new feasible solution to be calculated efficiently,
and the transition (if accepted) to be made quickly. We
build an exponentiation table prior to the start of the program. This allows us to approximate the transition probability value using a table lookup. We use ranking algorithms from [8] to hold the values of our  -sets. This allows
us to generalize our algorithms for different strengths without changing the base data structure. When calculating the
change in cost for each transition, we do not need to recalculate all of the  -sets in a test case, but instead only calculate
the change ( -1 subsets).
A constant is set to determine when our program is
frozen. This is the number of consecutive trials allowed
where no change in the cost of the solution has occurred.
For most of our trials this constant has been set to 1,000.
The cooling schedule is very important in simulated annealing. If we cool too quickly, we freeze too early because the probability of allowing a worse solution drops too
quickly. If we cool too slowly or start at too high a temperature, we allow too many poor moves and fail to make
progress
Therefore, if we start at a low temperature and cool
slowly we can maintain a small probability of a bad move
for a long time allowing us to avoid a frozen state, at the
same time continuing to make progress. We have experimented using fixed strength arrays compared with known
algebraic constructions (see [6]). We have found that a starting temperature of approximately 0.20 and a slow cooling

VCA


	
  


	 !"    


	
 &%')(   

C



Min
N
16
27
27
27
27
33






  


  


 


  


   ,


   ,


 


  


 


 


  


 
#$
 !

 !  

   ,

 !  
#$
 !
#$
 !  "
#$
 !


"
"  



#$
&%
  &%')( 

Max
N
17
27
27
27
27
33

Avg
N
16.1
27
27
27
27
33

33*
34
33*
34
41
50
67
36
64
100
125

35

34.8

35
42
51
69
36
64
104
125

34.9
41.4
50.8
67.6
36
64
101
125

125
171
180
214
100
100
304

125
173
180
216
100
100
318

125
172.5
180
215
100
100
308.5

Table 4. Table of sizes for variable strength
arrays
after 10 runs (We have omitted the parameter
*

in our notation for covering arrays in this table due to
space limitations.)

* The minimum values for these VCA’s were found during a separate set of experiments


factor, , of between 0.9998 and 0.99999 every 2,500 iterations works well. Using these parameters, the annealing
algorithm completes in a “reasonable” computational time
on a PIII 1.3GHz
processor running Linux. For instance,
' 
the first few
’s in Table 4, complete
in seconds, while
' 
the larger problems, such as the last
in Table 4, complete within a few hours. The delta in our cost function is
counted as the change in  -sets from our current solution.
Since we can at any point make changes to both the base
array and one of the higher strength arrays, these changes
are added together.
As there is randomness inherent in this algorithm, we run
the algorithm multiple times for any given problem.

5. Results
Table 4 gives the minimum, maximum and average sizes
obtained after 10 runs of thesimulated
annealing algorithm

'
for each of the associated
’s. Each of the 10 runs uses

a different random seed. A starting temperature of .20 and
a decrement parameter of .9998 is used in all cases. In two
cases a smaller sized array was found during the course of
our overall investigation, but was not found during one of
these runs. The numbers are included in the table as well
and are labeled with an asterisk, since these provide a previously unknown bound for their particular arrays. In each
case we show the number of tests required for the base array of strength two. We then provide some examples with
variations on the contents of . Finally we show the arrays
with all of the columns involved in strength three coverage.
We have only shown examples using strength two and three,
but our methods should generalize for any strength  .
What is interesting in Table 4 is that the higher strength
sub-arrays often drive the size of the
final test suite. Such
' 
is the case in the first and second
groups in this table.
We can use this information to make decisions about how
many components can be tested at higher strengths. Since
we must balance the strength of testing with the final size
of the test suite we can use this information in the design
process.
Of course there are cases where the higher strength subsets do not determine the final test suite size since the number of test cases required is a combination
of the number
' 
of levels and the strength. In the last
group in Table
4 the two components each with 10 levels require a minimum of 100 test cases to cover all pairs. In this case we can
cover all of the triples from the 20 preceding columns with
the same number of tests. In such cases, the quality of the
tests can be improved without increasing the number of test
cases. We can set the strength of the 20 preceding columns
to the highest level that is possible without increasing the
test count.
Both situations are similar in the fact that they allow us
to predict a minimum size test suite based on the fixed level
sub-arrays. Since there are better known bounds for fixed
strength arrays we can use this information to drive our decision making processes in creating test suites that are both
manageable in size while providing the highest possible interaction strengths.

6. Conclusions
We have presented a combinatorial object, the variable
strength covering array, which can be used to define software component interaction tests and have discussed one
computational method to produce them. We have presented
some initial results with sizes for a group of these objects.
These arrays allow one to guarantee a minimum strength
of overall coverage while varying the strength among disjoint subsets of components. Although we present these objects for their usefulness in testing component based software systems they may be of use in other disciplines that

currently employ fixed strength covering arrays.
The constraining factor in the final size of the test suite
may be the higher strength sub-array. We can often get a
second level of coverage for almost no extra cost. We see
the potential to use these when there is a need for higher
strength, but we cannot afford to create an entire array of
higher strength due to cost limitations.
Where the constraining factor is the large number of levels in a set of fields at lower strength, it may be possible to
increase the strength of sub-arrays without additional cost,
improving the overall quality of the tests.
Another method of constructing fixed strength covering
arrays is to combine smaller arrays or related objects and to
fill the uncovered  -sets to complete the desired array [4].
We are currently experimenting with some of these techniques
to build variable strength arrays. Since the size of a
' 
may be dependent on the higher strength arrays, we
believe that building these in isolation followed by annealing or other processes to fill in the missing lower strength
 -sets will provide fast and efficient methods to create optimal variable strength arrays.

Acknowledgments
Research is supported by the Consortium for Embedded and Internetworking Technologies and by ARO grant
DAAD 19-1-01-0406. Thanks to the Consortium for Embedded and Internetworking Technologies for making a
visit to ASU possible.

References
[1] J. Bach. James Bach on risk-based testing In STQE
Magazine, Nov/Dec,1999.
[2] L. Brownsword, T. Oberndorf and C. Sledge. Developing new processes for COTS-based systems.
IEEE Software, 17(4):48–55, 2000.
[3] K. Burr and W. Young. Combinatorial test techniques: Table-based automation, test generation
and code coverage. In Proc. of the Intl. Conf. on
Software Testing Analysis & Review, 1998, San
Diego.

[4] M. Chateauneuf and D. Kreher. On the state of
strength-three covering arrays. Journal of Combinatorial Designs, 10(4):217–238, 2002
[5] D. M. Cohen, S. R. Dalal, M. L. Fredman, and
G. C. Patton. The AETG system: an approach
to testing based on combinatorial design. IEEE
Transactions on Software Engineering, 23(7):437–
44, 1997.
[6] M. B. Cohen, C. J. Colbourn, P. B. Gibbons and
W. B. Mugridge. Constructing test suites for interaction testing. In Proc. of the Intl. Conf. on
Sofware Engineering (ICSE 2003), 2003, pp. 3848 , Portland.
[7] S. R. Dalal, A. J. N. Karunanithi, J. M. L. Leaton,
G. C. P. Patton, and B. M. Horowitz. Model-based
testing in practice. In Proc. of the Intl. Conf. on
Software Engineering,(ICSE ’99), 1999, pp. 28594, New York.
[8] D. L. Kreher and D. R. Stinson. Combinatorial
Algorithms, Generation, Enumeration and Search.
CRC Press, Boca Raton, 1999.
[9] K. Nurmela and P. R. J. Östergård. Constructing
covering designs by simulated annealing. Technical report, Digital Systems Laboratory, Helsinki
Univ. of Technology, 1993.
[10] J. Stardom. Metaheuristics and the search for covering and packing arrays. Master’s thesis, Simon
Fraser University, 2001.
[11] K. C. Tai and L. Yu. A test generation strategy for
pairwise testing. IEEE Transactions on Software
Engineering, 28(1):109-111, 2002.
[12] A. W. Williams. Determination of test configurations for pair-wise interaction coverage In Proc.
Thirteenth Int. Conf. Testing Communication Systems, 2000, pp. 57–74.

The Journal of Systems and Software 46 (1999) 173±182

Software process simulation for reliability management
Ioana Rus
a
b

a,1

, James Collofello

a,2

, Peter Lakey

b,*

Computer Science and Engineering Department, Arizona State University, AZ, USA
Boeing Aircraft and Missiles, Mailcode S0343550, St. Louis, MO, 63166-0516, USA
Received 10 November 1998; accepted 11 November 1998

Abstract
This paper describes the use of a process simulator to support software project planning and management. The modeling approach here focuses on software reliability, but is just as applicable to other software quality factors, as well as to cost and schedule
factors. The process simulator was developed as a part of a decision support system for assisting project managers in planning or
tailoring the software development process, in a quality driven manner. The original simulator was developed using the system
dynamics approach. As the model evolved by applying it to a real software development project, a need arose to incorporate the
concepts of discrete event modeling. The system dynamics model and discrete event models each have unique characteristics that
make them more applicable in speci®c situations. The continuous model can be used for project planning and for predicting the
eect of management and reliability engineering decisions. It can also be used as a training tool for project managers. The discrete
event implementation is more detailed and therefore more applicable to project tracking and control. In this paper the structure of
the system dynamics model is presented. The use of the discrete event model to construct a software reliability prediction model for
an army project, the Crusader, is described in detail. Ó 1999 Elsevier Science Inc. All rights reserved.

1. Overview
The concept of process modeling and simulation was
®rst applied to the software development process by
Abdel-Hamid (Abdel-Hamid et al., 1991). Others (Madachy, 1996; Tvedt, 1996) have produced models as well,
but there is little evidence that they have been successfully applied to real software development projects. The
models described in this paper bring two new contributions to the software process modeling and simulation
work. First, the modeling approach emphasizes software
reliability, as opposed to general characteristics of the
software development process. Second, the discrete event
implementation of the model is being applied to a speci®c software project where useful results are expected.
2. Introduction
As the role of software is expanding rapidly in many
aspects of modern life, quality and customer satisfaction
become the main goal for software developers and an
*

Corresponding author. E-mail: peter.b.lakey@boeing.com
E-mail: ioana.rus@asu.edu
2
Email: collofello@asu.edu
1

important marketing consideration for organizations.
However, quality by itself is not a strategy that will
ensure a competitive advantage. There are other project
drivers like budget and delivery time that must be considered in relation to quality. Achieving the optimal
balance among these three factors is a real challenge for
any project manager (Boehm, 1996). The approach
presented in this paper is intended to help managers and
software process engineers to achieve this balance.
Software reliability engineering consists of a general
set of engineering practices applied to the following
tasks: de®ning the reliability objective of a software
system, supporting the development of a system that
achieves this objective and assessing the reliability of the
product through testing and analysis. A detailed list of
reliability engineering activities and methods, together
with their purpose and description can be found in
(Lakey and Neufelder, 1996). A software reliability
program consists of a subset of the above practices and
is de®ned for each project by combining dierent reliability achievement and assessment activities and
methods, according to the project's characteristics. A
number of methods are available to facilitate software
reliability planning and control.
Use of cost estimation models and their corresponding software tools, like COCOMO (Boehm, 1984),

0164-1212/99/$ ± see front matter Ó 1999 Elsevier Science Inc. All rights reserved.
PII: S 0 1 6 4 - 1 2 1 2 ( 9 9 ) 0 0 0 1 0 - 2

174

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

SLIM (Putnam, 1992) and Checkpoint (Jones, 1986), to
estimate the impact of a practice on time and cost has
the following disadvantages: because model tuning is
needed not all relevant historical data might be available; cost drivers are at too coarse a level of granularity
to re¯ect the speci®c technique whose impact needs to be
evaluated. A major drawback of the models is their use
of data from projects other than those to which they are
being applied.
An alternative method to support strategy selection is
process modeling and simulation, which involves analyzing the organizational software development process,
creating a model of the process, and executing the model
and simulating the real process. Although modeling and
simulation has some limitations as well (the model accuracy depends on the quality of the model and of the
calibration), there are many strengths of modeling that
would advocate its use instead of other approaches
mentioned. Modeling captures expert knowledge about
the process of a speci®c organization; it does not need a
real system to experiment with, so it does not aect the
execution of real process; ®nally, dynamic modeling increases the understanding of a real process.
3. Software process modeling and simulation
Developing a model of the software development
process involves the identi®cation of entities, factors,
variables, interactions and operations that are present in
that process and are relevant to the intended use of the
model. The entities include mainly people (developers,
managers, customers, etc.), but can also include facilities, computer equipment, software tools, documentation and work instructions. Factors relevant to a general
software development process include the application
domain, the size of the project, the expected schedule
and delivery date, the hardware platform, and other
considerations. Variables include the number of software engineers, the skill levels of those individuals, the
level of process maturity of an organization, the level of
communication overhead, etc.
The modeling complexity increases when identifying
operations and interactions. Operations are the tasks that
need to be performed in the software process to transform user needs and requirements into executable software code. The typical operations include requirements
analysis, architecture development, detailed design, implementation (code and unit test), integration, and system test. Each of these takes some input (entity) and
generates some transformed output (another entity).
For example, the design operation uses software requirements as the input and transforms these into a
design of the software system.
The most important, and perhaps most dicult,
modeling task is identifying and correctly representing

the interactions among the factors and variables in the
software process that are relevant to the modeling goal.
For instance, how does the number of developers aect
communication overhead and overall productivity?
How does a defect prevention technique aect the
number of defects injected in software documents or
code? How does the eort allocated to regression testing
aect failure intensity during system testing and the
number of defects remaining at delivery time. These
interactions need to be properly modeled in order to
closely represent reality.
While the model is useful for representing the structure of the process, execution of the model is very
helpful in understanding the behavior of the process. It
gives management a tool that has been lacking for a long
time. The simulation capability allows a manager to
make a decision and have a high degree of con®dence in
what the results of that decision will be. Without the
simulation the decision is purely speculation. The complexity of a decision is too overwhelming to be understandable without the dynamic execution of the software
model.
4. Comparison to other approaches
Simulator presented here can be used for tracking
the quality and reliability of the software throughout
the development process. Reliability prediction and estimation models such as reliability growth models and
prediction models have a similar goal. The dierence is
that the reliability growth models are analytical, address only the system testing phase, and each has unrealistic assumptions that restrict their applicability.
The modeling and simulation approach addresses all
the development phases and is tailored to the real
process that is modeled. Reliability prediction models,
as that developed for Rome Laboratory (SAIC, 1987)
are static.
It is postulated here that running simulations to determine reasonable predicted defect levels at delivery in
consideration of cost, schedule and stang is a better
way of predicting than using a static model. This approach allows the metrics tracked during the project to
be used to make adjustments to the model to re¯ect
what is really happening. That is, process behavior is
explained by the model. With a static model, there is no
way to know why deviations from predictions exist. The
Rome Laboratory model is a regression model. All of
these regression models inherently infer a cause±eect
relationship between the independent variables and reliability, when in fact there is no proven causal relationship; rather the entire process is the cause of
software defects and failures.
Tausworthe and Lyu (1996) developed a simulator of
the software reliability process, but it is at a much higher

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

level of process abstraction and captures a very reduced
number of process parameters.
Other system dynamics simulators of the software
development process have been developed, but their
purpose and scope is dierent. Most of the previous
models, such as Abdel-Hamids model (Abdel-Hamid
et al., 1991), focus on the management aspect of
software development as opposed to the technical aspect; they also do not include the requirements analysis phase. Madachy's model (Madachy, 1996) was
developed to analyze the eect of inspections, and
Tvedt developed a model of the incremental life cycle
(Tvedt, 1996) for analyzing cycle time reduction. The
model presented in the next section has been developed to capture the technical issues of software
quality and reliability and to assess the eect of various reliability practices.
5. System dynamics process model description
The following model was developed to support project
planning for the purpose of evaluating software reliability engineering strategies. It is a generic model intended for use in high level decision making. The model
represents the initial work performed by the authors in
software process modeling and simulation and is part of
a decision support system for software reliability engineering strategy selection and assessment (Rus, 1998).
The main components of the model correspond to
the production phases of the software development

175

process (requirements analysis, design and coding) and
the system testing phase. A brief description of a
production phase is presented here. More model details, as well as a description of the SystemTesting
block, together with more simulation results and
model use examples can be found in (Rus, 1998). The
model was implemented in Extend V4.0, a simulation
software package from Imagine That (Extend, 1995),
by using hierarchical blocks. Each ProductionPhase
block has a Development block and a Management
block, corresponding to technical and managerial activities, respectively. The Development block models
items production, as well as quality assurance activities. Items can be requirements documents, design
documents, and source code. The Development block
has two sub-blocks: Items and Defects that are shown
in Fig. 1. Sub-blocks of Items and Defects are expanded in Fig. 2, at the lowest Extend implementation
level.
The Items sub-block models items production and
veri®cation and validation (V&V) activities. Items
from the previous phase enter the current production
phase (ItemsIn) and items corresponding to the current
phase are produced (for instance if the current phase is
design, requirements documents are the input and design documents are generated). The production rate
depends on many factors, such as the productivity of
the personnel (ProdProductIn), and schedule pressure
(SchPresPrIn). There are other factors aecting this
rate that are not shown in the ®gure: process factors
like methods, techniques, tool support, and metrics

Fig. 1. Development and Management blocks of a ProductionPhase.

176

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

Fig. 2. Items and Defects blocks.

collection eort; and product factors like complexity,
criticality and fault tolerance. The items produced are
then veri®ed (by reviews, walkthroughs, and/or inspections) at the V&VRate. This rate depends on factors like personnel productivity (VVProductIn) and
schedule pressure (SchPresVVIn). After veri®cation
and validation, items are passed to the next phase
(ItemsOut). For each item that is generated, there are
also defects injected.
Defect generation, detection and correction is captured by the Defects sub-block. Defects are generated at
a rate depending on the production rate, schedule
pressure and other factors such as process maturity and
development methodology. Defectsi1In and Defectsi2In
are defects propagated from previous phases. The defect
detection rate depends on the veri®cation and validation
rate (VVRateIn), eciency of V&V activities, schedule
pressure, defect density, and other factors. Detected
defects will be corrected, and possibly new defects will
be generated. Undetected defects propagate to subsequent phases (DefectsOut).
The Management block (Fig. 1) models the human
resources, planning and control aspects of the process.
For each of the production, veri®cation and validation
(V&V), and rework activities there is a corresponding
sub-block in the Management block. Each of these three
sub-blocks models the eort consumed (man-days) in
the tasks of producing, verifying and reworking items.
These modules also capture personnel experience increasing with time (learning) and schedule pressure resulting from the dierence between the estimated

schedule and the actual one. Actual productivity determines the rate at which items are produced.
Figs. 3 and 4 present examples of model execution
(process simulation) outputs. Fig. 3(a) shows the variation of the number of items produced and veri®ed
during a production phase. The number of defects injected, detected, and corrected throughout the phase can
be seen in Fig. 3(b). Fig. 4(a) shows the evolution of the
number of coding faults identi®ed and corrected during
system testing, and the coding faults remaining in the
product. Fig. 4(b) presents an example of sensitivity
analysis: the variation of failures encountered during
system testing, with the number of design faults at the
beginning of system testing. These values and shapes of
these graphs will be unique to a speci®c company that
calibrates the model with metrics and inputs speci®c to
that company.
Software reliability engineering practices are modeled
by considering their in¯uences on dierent factors that
impact defects. For instance, defect prevention techniques (e.g. formal methods) will decrease the value of
defect generation rate. Defect detection techniques will
increase the number of defects detected and reduce the
number of defects propagated to the next phase. Fault
tolerance methods (e.g. N-version programming) reduce
the impact that remaining defects have on the reliability
of the operational system.
There is, of course, some additional up-front eort
and cost associated with these practices, but the overall
development time and eort may be reduced, by longterm reduction in rework time. By reducing the number

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

177

Fig. 3. Variation of the number of items (a) and defects (b) through a production phase.

of remaining defects, and/or masking them, the reliability of the product will increase. Simulation is used to
analyze the eect of these practices in terms of failures,
cost, and stang, allowing trade-os among reliability
strategies.

6. Discussion on system dynamics model
The model described above was executed with assumed parameter values for each of the factors identi®ed. By changing these factors individually the

Fig. 4. (a) Variation of the number of coding faults during system testing. (b) Variation of failures encountered during system testing, with the
number of design faults at the beginning of system testing.

178

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

corresponding impact on the outputs of the model (cost,
schedule and quality) can be evaluated. The model is
implemented with building blocks derived from the generic ProductionPhase block by adding details speci®c to
each development phase while maintaining the common
features and architecture. As a result, the model is
modular, extensible, adaptable and ¯exible. The example that we used in (Rus, 1998) corresponds to a waterfall development life cycle model, but the model can
be adapted to represent for example an incremental
process.
The system dynamics approach, which is a continuos
modeling paradigm, considers that items are identical
and treats them uniformly. This is an assumption that
works at a higher level of modeling. If the use and the
goal of the model require more detail, then dierences
between entities and entities' attributes must be considered. For example, design and code items dier by
structural properties such as size, complexity, and
modularity; usage, and eort allocated for development.
Similarly, defects can have dierent types, consequences,
generation and detection phase, etc. In Extend, when
using discrete event modeling, items can have attributes
attached, with random values (within a speci®ed range),
and with a speci®ed distribution. Activity duration and
item generation and processing times can vary with
other model parameters (variables), according to a
speci®ed equation, and can have a probabilistic distribution around the computed value. Subprocesses such
as detailed design and system testing (reliability growth
testing) appear to be more like event-driven processes
rather than continuous.

Therefore, because discrete event modeling allows a
more detailed modeling capability which was required
by the application described in the next section, the
system dynamics model was transformed into an alternative discrete event model for a production phase, as
illustrated by the preliminary design phase example.
7. Discrete event model application
The discrete event model implementation of the
software process was developed to support a speci®c
project, Crusader. The Crusader is a large army development project. There are two vehicles, called Segments,
in each Crusader System. The system consists of the
Self-propelled Howitzer (SPH) and the Re-supply Vehicle (RSV). The software is being developed using object-oriented analysis and design (OOA/D) and Rational
Rose CASE tool. An incremental software life cycle
approach is being used, with speci®c functionality in ®ve
planned major builds.
A top-level architecture has been constructed for the
Crusader software system. It consists of 40 Computer
Software Con®guration Items (CSCIs). These could also
be called subsystems. Each CSCI is developed and
managed independently of the other CSCIs. The approach taken is to model one of these CSCIs, using it as a
prototype for learning the relevant dynamics associated
with defects, reliability and other process parameters.
Many of the important metrics associated with reliability that are captured in the discrete event model are
shown in Fig. 5. Some of the most important metrics

Fig. 5. Crusader software reliability management metrics.

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

needed to support software reliability management are
related to rework. As it pertains to reliability, rework is
important because it helps determine the amount of
reliability improvement that can be expected for a given
schedule and stang level. Software development activities may be completed on time, but if no time is built
in for rework, then the defects that are found during
testing cannot be ®xed and the reliability cannot improve signi®cantly.
Rework is also important from the standpoint of
process improvement. Reducing rework increases the
total amount of quality software that can be developed
per time unit. The model helps pinpoint areas where
improvements should be made to reduce rework. To
substantially reduce rework a signi®cant investment
needs to be made in process improvement during implementation of the software development schedule.
This will increase costs in the short term, and may
lengthen the schedule. These dynamics are captured in
the model.
8. Prototype discrete event model description
The initial model being developed captures the process for Preliminary Design. The structure of the model,
developed using Extend V4.0, is described below. The
prototype discrete event model represents the Crusader
Software Development Process, and it is being piloted
on a single CSCI, which will be called Module X here.
This discrete event model is actually a hybrid model that
also incorporates system dynamics concepts, using
feedback loops. The main purpose of the model is to
predict, track, and control software defects and failures
over the entire Crusader development, through the year
2005. It also facilitates tracking and control of software
project costs and schedules. The model currently addresses only the Preliminary Design phase. Later on this
year it will be expanded to include Detailed Design, Code
and Unit Test, Subsystem Test, Element Test and Segment Test. There will be two modules. The Defect
Module will predict defects, eort, and rework for each
phase through Code and Unit Test. The Failure Module
will estimate reliability and test eort during testing.
There are three types of inputs to Preliminary Design:
Segment Use Cases/Scenarios, Software Requirements
Speci®cations, and the architecture in the form of Rose
Models. The Crusader project has de®ned 10 Use Cases,
40 CSCIs and approximately 40 Rose Subsystems to
correspond to the 40 CSCIs. Module X is responsible
for approx. 50 Segment Scenarios, has 200 CSCI Scenarios of its own, and will use 200 classes in its design. In
a pure discrete event model the activity of Re®ning the
Scenarios would be represented by a single Operation
block, with a single set of outputs, including schedule
and quality. The problem with that approach (only one

179

output data point) is that no dynamics can be incorporated into the activity during the process. In order to
incorporate feedback the Re®ne Scenarios Activity has
been divided into ®ve discrete steps, allowing the outputs of one step to be fed into the next step. The inputs
each are divided up into ®ve sets to be processed during
each iteration. In other words, 10 Segment Scenarios, 40
CSCI Scenarios and 40 Rose Classes are processed for
each step. This is dierent from actual process implementation, but the approximation should be acceptable
for modeling.
Random data is generated for each of the input
products to the Preliminary Design activity. This data is
stored in a spreadsheet ®le. All of this stored data is
accessed throughout the simulation. The values for
Segment Scenario size are normally distributed around
the average value of 10. The values for Scenario Quality
refers to the number of defects associated with a particular scenario prior to it being used to de®ne CSCI
scenarios. Class size can be characterized through a
number of dierent object-oriented design metrics, such
as number of instance methods in a class, number of
instance variables in a class or number of class methods.
The screen capture in Fig. 6 shows the three types of
input products, Segment Scenarios, CSCI Scenarios and
Classes, being read into an Operation block where that
data is processed and resources are expended to perform
the Re®ne CSCI Scenarios task. The data from the
spreadsheet is used to calculate some project parameters
during the simulation. Fig. 6 represents three main steps
for the activity of Re®ning CSCI Scenarios. First, the
scenarios are developed using the Segment Scenarios
allocated to MCS. Then, a Walkthrough is performed to
evaluate the completeness and correctness of the CSCI
Scenarios and their mapping to Segment Scenarios. Finally, any defects found are documented and those are
corrected through the Rework Activity.
During each of these activities data is generated for
Eort ± in terms of labor hours expended, and Quality ±
in terms of numbers of defects. All of this calculated
data is passed on to a ®nal block in the model, which
plots this data graphically over the course of a run. The
list of outputs from the model includes development
eort, review eort, rework eort, defects generated,
defects found and escaped defects. These parameters are
each dependent on a number of factors that interact
with each other over the course of a software development project. The basis of the model is the set of factors
for each parameter and the equations used to calculate
the factor values during execution of the model. These
factors include both process and product factors. Some
of the process factors included in the model are the
Manpower Factor, Schedule Factor, Communication
Overhead Factor, Work Rate Factor, Process Maturity
Factor and Tool Support Factor. The product factors
include Size, Quality, and Complexity.

180

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

Fig. 6. Re®ne scenarios process for Modules X.

The model factors are dynamic. That is, they change
after each event (iteration) occurs in a simulation. Many
of the factors aect other factors, which in turn are affected by the same factors. This provides feedback
loops. The interaction between factors in the model is
intended to represent actual Crusader software development project dynamics. As work is scheduled and
accomplished, the project team will often ®nd that the
eort required for a task is larger than originally anticipated. This occurs in the middle of a development activity. When the project gets behind schedule decisions
are made that change the expected state. Work is sped
up or postponed. Certain activities may be cancelled,
such as periodic walkthroughs. These decisions impact
overall eort and the number of defects injected and
found. Quality can suer if the schedule is emphasized.
The values used for model factors currently are assumptions ± no historical project data is available from
Crusader to represent the relationships among the factors. These assumptions will be replaced with factors
calculated from data collected on the Crusader project
as development progresses and the relationships are
more clearly understood. The intent is to have a working
model by the end of 2000 useful for management decisions.
The screen capture in Fig. 7 shows the output for a
single simulation run. The Actual (simulated) values are
dierent from the Expected values for each set data
being plotted. There are always be dierences as the

model is currently set up because the Size and Quality
data is randomized for each iteration, making it dierent
from the expected size and quality data. This causes the
Size and Quality Factors to be greater than or less than 1.
Depending on amplitude and direction of the dierences
as compared to expected, the actual values would be
above or below the expected values for each of the plots
above.
This model uses a Monte Carlo simulation, which
means there are random draws for each execution,
making the results dierent. With all inputs being the
same, in a single execution of the Module X Preliminary
Design process, the outcome could be as shown in Fig. 7
or some other outcome.
9. Conclusions and future work
A dynamic model of the software development process allows answer questions such as: ``How much time
and eort will it take to complete the Preliminary Design
phase for Module X?'', ``How many defects are going to
be generated during that phase? How will that aect
product reliability?''. ``How much will the eort increase
if defect prevention practices are used? How will that
shorten system testing time?''. These questions are very
dicult to answer using methods currently available in
industry. There is a great deal of variability in the possible outcomes using static models. The modeling

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

181

Fig. 7. Output screen for Module X Discrete Event Model.

methods described in this paper provide insight into why
and how things occur.
Crusader management believes there is a great potential bene®t for using the discrete event modeling approach
not only for reliability, but for many other aspects of
project management, including cost and schedule.
With good data this model can be continuously updated and improved throughout the current program
phase, which concludes at the end of 2000. Prior to the
next phase, Engineering and Manufacturing Development (EMD), the model should be validated to the point
where it supports eective management decisions. Outputs predicted by simulation will be compared with actual data collected from the project. Discrepancies will be
identi®ed and used to improve the validity and accuracy
of the model. With a real-world model, management can
make good decisions throughout the EMD phase of the
program to meet software reliability requirements.
References
Abdel-Hamid, T., Madnick, S.E., 1991. Software Project Dynamics
An Integrated Approach. Prentice-Hall, Englewood Clis, NJ.
Boehm, B.W., 1984. Software engineering economics. IEEE Transactions on Software Engineering 10 (1), 5±21.
Boehm, B.W., Hoh, 1996. Identifying quality-requirements con¯icts.
IEEE Software 25±35.
Jones, C., 1986. The SPR feature point method. Software Productivity
Research.

Extend ± Performance modeling for decision support, 1995. User's
Manual, Imagine That.
Lakey, P., Neufelder, A.M., 1996. System and Software Reliability
Assurance Notebook, Produced for Rome Laboratory by SoftRel. Contact peter.b.lakey@boeing.com for a copy of the
Notebook.
Madachy, R., 1996. System dynamics modeling of an inspection-based
process. Proceedings of the Eighteenth International Conference
on Software Engineering, Berlin, Germany.
Putnam, L.H., 1992. Measures for Excellence ± Reliable Software on
Time, Within Budget, Yourdon Press Computing Series.
Rus, I., 1998. Modeling the impact on project cost and schedule of
software reliability engineering strategies. Ph.D. Dissertation,
Arizona State University, Tempe, Arizona.
SAIC, 1987. Methodology for software reliability prediction. Rome
Laboratory RADC-TR-87-171.
Tausworthe, R., Lyu, M., 1996. Software reliability simulation. In:
Lyu, M. (Eds.), Handbook of Software Reliability Engineering.
IEEE Computer Society Press, McGraw-Hill, New York.
Tvedt, J.D., 1996. An extensible model for evaluating the impact of
process improvements on software development cycle time. Ph.D.
Dissertation, Arizona State University, Tempe, Arizona.
Ioana Rus is a Ph.D. candidate in the Department of Computer Science
and Engineering at Arizona State University. She received her Master in
Computer Science degree from Arizona State University and Bachelors
of Science from Polytechnical Institute Cluj, Romania. Her research
interests include software process improvement, process modeling,
software quality and reliability, arti®cial intelligence, and neural networks. She is a member of the IEEE Computer Society and ACM.
James S. Collofello is a professor in the Department of Computer
Science and Engineering at Arizona State University. He received his
Doctor of Philosophy degree in Computer Science from Northwestern
University, Master of Science in Mathematics and Computer Science
from Northern Illinois University, and Bachelors of Science in

182

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

Mathematics and Computer Science from Northern Illinois University.
His teaching and research interests are in software quality assurance,
software reliability, safety, and maintainability, software testing,
software error analysis, and software project management. He is a
member of the IEEE Computer Society and ACM.
Peter B. Lakey received his MS in Engineering Management from
University of Missouri-Rolla, MBA from Washington University, and

his BSEE from Northwestern University. He joined McDonnell
Douglas (now Boeing) in 1990 as a Reliability Engineer. As an engineer in the Software Engineering Process Group (SEPG), Mr. Lakey
has been responsible for developing tools and processes that can be
used by internal customers to improve software product quality. His
main responsibility now is for managing a Software Reliability Support subcontract with United Defense in Minneapolis on a large Army
program called the Crusader.

SOFTWARE—PRACTICE AND EXPERIENCE, VOL. 23(10), 1095–1105 (OCTOBER 1993)

An Application of Causal Analysis to the
Software Modification Process
james s. collofello

Computer Science Department, Arizona State University, Tempe, AZ 85287, U.S.A.

and
bakul p. gosalia

AG Communication Systems Corporation, Phoenix, AZ 85027, U.S.A.

SUMMARY
The development of high quality large-scale software systems within schedule and budget constraints is a formidable software engineering challenge. The modification of these systems to
incorporate new and changing capabilities poses an even greater challenge. This modification
activity must be performed without adversely affecting the quality of the existing system. Unfortunately, this objective is rarely met. Software modifications often introduce undesirable side-effects,
leading to reduced quality.
In this paper, the software modification process for a large, evolving real-time system is analysed
using causal analysis. Causal analysis is a process for achieving quality improvements via fault
prevention. The fault prevention stems from a careful analysis of faults in search of their causes.
This paper reports our use of causal analysis on several significant modification activities resulting
in about two hundred defects. Recommendations for improved software modification and quality
assurance processes based on our findings are also presented.
key words: Causal analysis

Software maintenance

BACKGROUND
The traditional approach to developing a high quality software product consists of
applying a development methodology with a heavy emphasis on fault detection.
These fault detection processes consist of walkthroughs, inspection and various levels
of testing. A more effective approach to developing a high quality product is an
emphasis on fault prevention. An effective fault prevention approach which is gaining
popularity is causal analysis.1–4 Causal analysis consists of collecting and analyzing
software fault data in order to identify their causes. Once the causes are identified,
process improvements can be made to prevent future occurrences of the faults.
The paper by Jones3 presents a programming process methodology for using causal
analysis and feedback as a means for achieving quality improvement, and ultimately
fault prevention, at IBM. The methodology emphasizes effective use of the fault
data to prevent the recurrence of faults.
The fault prevention methodology is based on the following three concepts:
0038–0644/93/101095–11$10.50
 1993 by John Wiley & Sons, Ltd.

Received 6 April 1993
Revised 6 April 1993

1096

j. s. collofello and b. p. gosalia

1. Designers should evaluate their own faults.
2. Causal analysis should be part of the software development process.
3. Feedback should be part of the process.
An overview of the causal analysis process proposed by Jones is shown in Figure 1.
The first activity consists of a kickoff meeting with the following purposes:
1.
2.
3.
4.

Review
Review
Review
Identify

input fault data.
the methodology guidelines.
appropriate checklists.
the team goals.

Next, the development of the work products occurs, using the feedback from the
kickoff meeting to prevent the creation of errors.
The work products are then validated and reworked as necessary.
A causal analysis activity is then performed, beginning with an analysis of the
faults performed by the owner of the faults. This involves analyzing each fault
to determine
1.
2.
3.
4.
5.

The
The
The
The
The

fault type or category
phase that the fault was found
phase that the fault was created
cause(s) of the fault
solution(s) to prevent the fault from occurring in the future.

Figure 1. Causal analysis process described by Jones

an application of causal analysis

1097

This information is recorded on a causal analysis form which is used to enter the
data into a database. A causal analysis team meets to analyze the data in the
database. In addition, the group may at times need to consult with other people
(designers, testers, etc.) outside of the team to complete the analysis.
The causal analysis team is responsible for identifying the major problem areas
by looking at the fault data as a whole instead of one particular fault at a time.
The team uses a problem-solving process to analyze the data, determine the problem
area(s) to work on, determine the root causes of problem area(s), and develop
recommendations and implementation plans to prevent the problem type(s) from
occurring in the future.
These recommendations are then submitted to an action team, which has the
following responsibilities:
1. Evaluation and prioritization of recommendations.
2. Implementation of recommendations.
3. Dissemination of feedback.
The action team meets periodically (e.g. once a month) to review any new
implementation plans received from the causal analysis teams and to check the status
of the previous implementation plans and action items. The status of the action
items is kept in the causal analysis database and monitored by the action team.
To date, most of the effort reported in causal analysis activities has been focused
on development processes. This is unfortunate, considering the high percentage of
effort consumed by software modification processes. Software modifications occur in
order to add new capabilities or modify existing capabilities of a system. Modifying
a large, complex system is a time-consuming and error-prone activity. This task
becomes more difficult over time as systems grow and their structure deteriorates.5
Thus, more effort must be expended in performing causal analysis activities for
design maintenance tasks. This paper describes such an effort. In the remainder of
this paper, our approach to causal analysis for design maintenance tasks is described,
as well as the application of this approach to a large project. Our results and
recommendations for preventing faults during modification processes are also
presented.
CAUSAL ANALYSIS APPROACH
Our application of causal analysis to the software modification process is shown in
Figure 2. Each of the causal analysis steps in Figure 2 is described in one of the
following subsections.
Obtaining problem reports
The first activity to be performed is collecting problem reports resulting from the
modification processes undergoing causal analysis scrutiny. These problem reports
may be generated internally via testing or externally by the customer. A window of
time must be chosen during which the problem reports are collected. Ideally, the
window of time for obtaining problem reports should begin after the code is modified
and end some period of time after the code was delivered.

1098

j. s. collofello and b. p. gosalia

Figure 2. Causal analysis approach as applied to the software modification process

Prioritizing problems
The next step in our causal analysis approach is prioritizing the faults. Fault
prioritization enables an analysis of those causes which contribute to high priority
problems. Our prioritization consists of three categories:
1. Critical
2. Major
3. Minor
Critical faults impair functionality and prohibit further testing. Major faults partially
impair functionality. Minor faults do not affect normal system operation.

Categorizing errors
After fault prioritization, faults are categorized. The primary objective of this
categorization is to facilitate the link between faults and their causes. Numerous
error categorization schemes have been proposed over the years.6–9 The existing
schemes are, however, directed towards understanding and improving the development
of new software. They do not address the kinds of errors that are unique to
maintenance activities. Thus, we found it necessary to expand upon existing categorization schemes and develop new error categories directed towards maintenance activities. Our categories were formulated based on our experience of analyzing hundreds
of maintenance errors.
Although categorization is not necessary in performing a causal analysis, we found
the categorization useful in identifying fault causes. The fault categorization also
provides useful information when determining the cost-effectiveness of eliminating
fault causes versus attempting to detect the faults if they occur.
The fault categories are described below:

an application of causal analysis

1099

Design faults
This category reflects software faults caused by improper translation of requirements into design during modification. The design at all levels of the program and
data structure is included. The following are examples of typical design faults:
1. Logic faults. Logic faults include sequence faults (improper order of processing
steps), insufficient branch conditions, incorrect branch conditions, incorrectly
nested loops, infinite loops, incorrectly nested if statements, missing validity
checks etc.
2. Computation faults. Computation faults pertain to inaccuracies and mistakes in
the implementation of addition, subtraction, multiplication, and division operations. Computational faults also include incorrect equations or grouping of
parenthesis, mixing data of different unit values, or performing correct computations on the wrong data.
3. Missing exception handling. These faults include failure to handle unique
conditions or cases even when the conditions or cases were specified in the
requirements documents.
4. Timing faults. This type of fault reflects incorrect design of timing critical
software. For example, in a multitasking system, a designer interpreted the
executive command ‘suspend’ to mean that all processing is halted for a
specified amount of time, when in fact the command suspended all tasks
scheduled to be invoked in the same specified amount of time.
5. Data handling faults. These faults include failure to initialize the data before
use, improper use or designation of fixes, mixing up the names of two or more
data items, and improper control of external file devices.
6. Faults in I/O concepts. These faults include input from or output to the wrong
file or on-line device, defining too much data for a record size, formatting
faults, and incorrect input–output protocols with other communication devices.
7. Data definition faults. This category reflects incorrect design of the data
structures to be used in the module or the entire software (incorrectly defined
global data structures). This category also reflects incorrect scoping in data
structures and incorrect data abstractions.
Incompatible interface
Incompatible interface faults occur when the interfaces between two or more
different types of modified components are not compatible. The following are
examples of incompatible interfaces:
1. Different parameters passed to the procedures/functions than expected.
2. Different operations performed by the called modules, functions, or procedures
than expected by the calling modules, procedures or functions. For example,
assume a module A was called by another module B to perform fault checking.
However, as a design change, the fault checking code was moved to another
software module C.
3. Code and database mismatched. For example, suppose that device handler code
tries to access data which is not defined in the database.
4. Executive routines and other routines mismatched. For example, consider the

1100

j. s. collofello and b. p. gosalia

situation where a task needs to remain in a no interrupt condition for a longer
time than allowed by the operating system routines.
5. Software and hardware mismatched.
6. Software and firmware mismatched. For example, the parameters passed by the
firmware may not match with the modified software.
Incorrect code synchronization from parallel projects
For a large evolving system, the software development cycles of multiple releases
may overlap, as shown in Figure 3.
This category of fault occurs when changes from project A, the previous project,
are incorrectly carried into project B, the current project.
Incorrect object patch carryover
In a large maintenance effort, object patches are sometimes needed. These object
patches may remain for several revisions of the system. Errors committed while
modifying modules with object patches fall into this category.
System resource exhaustion
This fault occurs when the system resources (such as memory and real time)
become insufficient.
Examples of faults in this class include: exhaustion of available stack space, the
indiscriminate use of special registers and the use of non-existent stacks, real-time
clocks, and other architectural features.
Determining fault causes
The most critical step in performing causal analysis is determining the cause of
each fault. This task can best be performed by the programmer who introduced the
fault. To facilitate this identification of fault causes, the software designers who
introduced the faults should be interviewed. The interviews should be informal and
performed very carefully so as to not make designers defensive. The purpose of the
analysis for defect prevention must be made clear prior to the interview. Based on
the information acquired from the designer who introduced the fault, a causal
category is selected for the error. Our approach to causal analysis identifies seven
major categories of causes that lead to software modification faults.
Our causal categories differ from others in the literature5,9 which are directed

Figure 3. Software development of large evolving systems

an application of causal analysis

1101

towards finding the causes of errors made during new software development. These
existing causal categorization schemes do not address the unique causes of errors
made during software maintenance.
Our causal categories are not exclusive and, thus, a fault might have several
causes. The causal categories were formulated in a manner that would support
process improvements. The goal of determining the causal category for each fault
is to collect the data necessary to determine which causal categories are leading to
the most problems. This provides a means for evaluating the cost effectiveness of
implementing recommendations to eliminate or reduce fault causes.
Our causal categories customized to modification activities are described below:
System knowledge/experience
This causal category reflects the lack of software design knowledge about the
product or the modification process. Some examples include:
1. Misunderstanding of the existing design. For example, the designer did not
have sufficient understanding of the existing design of the database.
2. Misunderstanding of the modification process. The designer was not aware of
the process followed in the project. For example, the designer was not aware
of the configuration control process.
3. Inadequate understanding of the programming environment. The designer did
not have adequate understanding of the programming environment. The programming environment includes personnel, machinery, management, development
tools, and any other factors which affect the developer’s ability to modify
the system.
4. Inadequate understanding of the customer’s requirements. The designer did not
thoroughly understand the requirements of the customer. For example, the
designer may not understand a feature specified in the requirements documents.
Communication
This causal category reflects communication problems concerning modifications. It
identifies those causes which are not attributed to a lack of knowledge or experience,
but instead to incorrect or incomplete communication. Communication problems are
also caused due to confusion between team members regarding responsibilities
or decisions.
Some examples of communication problems are:
1. Failure of a design group to communicate a last-minute modification.
2. Failure to write a fault-handling routine for a function because each team
member thought someone else was supposed to do it.
Software impacts
This category reflects failure of a software designer to consider all possible impacts
of a software modification.
An example is omission of fault recovery code after the addition of a new piece
of hardware.

1102

j. s. collofello and b. p. gosalia

Methods/standards
This category reflects methods and/or standards violations. It also includes limitations to existing methods or standards which contributed to the faults.
An example may be a missed code review prior to testing.
Feature deployment
This category reflects problems caused by the inability of software, hardware,
firmware or database components to integrate at the same time. It is characterized
by a lack of contingency plans to prevent problems caused by missing components.
Supporting tools
This category reflects problems with supporting tools which introduce faults. An
example is an incorrect compiler which introduces faults in object code.
Human error
This causal category reflects human errors made during the modification process
which are not attributable to other sources. The designer knew what to do and
understood the item thoroughly but simply made a mistake.
An example may be that a designer forgets to add code he intended for a
software load.
Developing recommendations
The last step in our causal analysis process is developing a set of recommendations
to prevent faults from reoccurring. These recommendations must be based on the
data collected during the causal analysis study and customized to the development
environment.
APPLICATION OF THE CAUSAL ANALYSIS APPROACH
In an effort to learn more about the faults generated during software modifications,
we applied our causal analysis process to the creation of a new version of a large
telephony system. This new version contained new features as well as fixes to
problems in the old release. The size of the system was approximately 2·9 million
lines of code. The version analyzed was developed from the previous version by
gradual modifications to the software, firmware, and hardware. About 1 million lines
of code were modified as follows: 85 per cent of the code was added, 10 per cent
of the code was changed, and 5 per cent of the code was deleted. Each of these
gradual modifications corresponded to a software increment. The increments were
added to the system one at a time and tested as shown in Figure 4. The new version
we analyzed consisted of 11 increments. The experience levels of the designers
involved in modifying the software varied widely from two years to as high as
30 years.
The documentation used and created in the process of generating the new version
was typical of that in most software development organizations. Both high level and

an application of causal analysis

1103

Figure 4. Gradual building of a software version release

detailed design documents were created for the new features being added. The
contents of these documents included details about the impacts of the new feature
on the existing modules, hardware, database, firmware, etc. These documents were
then reviewed by designers representing the affected areas. Documentation was also
generated and reviewed for fixes added to the new version.
Although it would have been ideal to collect and analyze problem reports for the
entire modification process, our study was limited to analyzing defects detected
during the first phase of integration testing for each of the incremental loads. This
phase verifies the basic stability and functionality of the system prior to testing the
operation of new features, functions, and fixes. It is, thus, the most likely phase to
detect faults introduced by the modification process. Prior to our data collection,
both code reviews and module testing were generally applied. Thus, faults detected
by these processes were not reflected in our results. In addition, faults detected by
later phases of testing, such as system testing and stress testing were also not
reflected in our results. Our results, therefore, are limited to the kinds of faults
normally detected during integration testing. In our study, this amounted to more
than 200 faults. All 200 faults were analyzed and categorized by this paper’s authors
to ensure reliable results.
RESULTS OF THE CAUSAL ANALYSIS APPLICATION
Figure 5 presents an overall distribution of fault categories across all the problems.
The large number of design and interface faults demonstrates that the modification

Figure 5. Fault category distribution of all the problems

1104

j. s. collofello and b. p. gosalia

Figure 6. Causal categories distribution of all the problems

of an existing design in order to satisfy new requirements is a very error-prone process.
Figure 6 presents the distribution of causal categories for all the problems.
The causal analysis data suggests that almost 80 per cent of all faults are caused
by insufficient knowledge/experience, communication problems or a failure to consider all software impacts. A further analysis of the faults caused by insufficient
knowledge/experience indicated that the majority of those faults were caused by
designers with less than five years of experience.
These results clearly suggest that one significant approach to improving the quality
of maintenance activities is to increase the experience level of those performing
design maintenance tasks. This can be accomplished in many ways. One obvious
approach is motivating designers to continue in their work assignments. Another
approach is to improve design maintenance training and mentoring programs.
The results obtained from the causal analysis also suggest that the design maintenance process can be greatly improved via better communication. This might be
accomplished with more disciplined maintenance methodologies, more thorough
documentation and diligent reviews. Extreme care must be taken to ensure that
information regarding potential software impacts as a result of modification activities
are communicated.
The results from our causal analysis activity were integrated with those of other
ongoing continuous process improvement approaches within our organization and
applied to the development of a subsequent version of the software. The improvements
mainly focused upon an enhanced review process, better training and better communication. The fault data from the subsequent version of the software release showed
a substantial reduction in the number of faults.
CONCLUSIONS AND FUTURE RESEARCH
This paper has presented our results from applying causal analysis to several large
modification activities. Our data suggests that almost 80 per cent of all faults
created during modification activities are caused by insufficient knowledge/experience,
communication problems or a failure to consider all software impacts.
The application of causal analysis to the software modification process has contrib-

an application of causal analysis

1105

uted to a substantial reduction of faults in our organization. Much additional research
needs to be performed to assess whether the results of this study are representative
of that experienced in other large evolving systems. Other research needs to explore
the attributes of an ‘experienced’ designer. Specific design maintenance methodology
improvements must also be formulated and evaluated in terms of their potential
return on investment.
REFERENCES
1. J. L. Gale, J. R. Triso and C. A. Burchfield, ‘Implementing the defect prevention process in the MVS
interactive programming organization’, IBM Systems Journal, 30, (1), 33–43 (1990).
2. R. G. Mays, C. L. Jones, G. J. Holloway and D. P. Studinski, ‘Experiences with defect prevention’,
IBM Systems Journal, 30, (1), 4–32 (1990).
3. C. L. Jones, ‘A process-integrated approach to defect prevention’, IBM Systems Journal, 24, (2), 50–
164 (1985).
4. R. T. Philips, ‘An approach to software causal analysis and defect extinction’, IEEE Globecom 1986, 1,
(12), 412–416 (1986).
5. J. Collofello and J. Buck, ‘Software quality assurance for maintenance’, IEEE Software, No. 5, September
1987, pp. 46–51.
6. A. Endres, ‘An analysis of errors and their causes in system programs’, IEEE Trans. Software Eng., SE1, (2), 140–149 (1975).
7. B. Beizer, Software Testing Techniques, Van Nostrand Reinhold, 1983.
8. J. Collofello and L. Blumer, ‘A proposed software error classification scheme’, Proceedings of the
National Computer Conference, 1985, pp. 537–545.
9. T. Nakajo and H. Kume, ‘A case history of software error cause–effect relationships’, IEEE Trans.
Software Eng., 17, 830–837 (1990).

