The Journal of Systems and Software 46 (1999) 173±182

Software process simulation for reliability management
Ioana Rus
a b

a,1

, James Collofello

a,2

, Peter Lakey

b,*

Computer Science and Engineering Department, Arizona State University, AZ, USA Boeing Aircraft and Missiles, Mailcode S0343550, St. Louis, MO, 63166-0516, USA Received 10 November 1998; accepted 11 November 1998

Abstract This paper describes the use of a process simulator to support software project planning and management. The modeling approach here focuses on software reliability, but is just as applicable to other software quality factors, as well as to cost and schedule factors. The process simulator was developed as a part of a decision support system for assisting project managers in planning or tailoring the software development process, in a quality driven manner. The original simulator was developed using the system dynamics approach. As the model evolved by applying it to a real software development project, a need arose to incorporate the concepts of discrete event modeling. The system dynamics model and discrete event models each have unique characteristics that make them more applicable in speci®c situations. The continuous model can be used for project planning and for predicting the eect of management and reliability engineering decisions. It can also be used as a training tool for project managers. The discrete event implementation is more detailed and therefore more applicable to project tracking and control. In this paper the structure of the system dynamics model is presented. The use of the discrete event model to construct a software reliability prediction model for an army project, the Crusader, is described in detail. Ó 1999 Elsevier Science Inc. All rights reserved.

1. Overview The concept of process modeling and simulation was ®rst applied to the software development process by Abdel-Hamid (Abdel-Hamid et al., 1991). Others (Madachy, 1996; Tvedt, 1996) have produced models as well, but there is little evidence that they have been successfully applied to real software development projects. The models described in this paper bring two new contributions to the software process modeling and simulation work. First, the modeling approach emphasizes software reliability, as opposed to general characteristics of the software development process. Second, the discrete event implementation of the model is being applied to a speci®c software project where useful results are expected. 2. Introduction As the role of software is expanding rapidly in many aspects of modern life, quality and customer satisfaction become the main goal for software developers and an
* 1

Corresponding author. E-mail: peter.b.lakey@boeing.com E-mail: ioana.rus@asu.edu 2 Email: collofello@asu.edu

important marketing consideration for organizations. However, quality by itself is not a strategy that will ensure a competitive advantage. There are other project drivers like budget and delivery time that must be considered in relation to quality. Achieving the optimal balance among these three factors is a real challenge for any project manager (Boehm, 1996). The approach presented in this paper is intended to help managers and software process engineers to achieve this balance. Software reliability engineering consists of a general set of engineering practices applied to the following tasks: de®ning the reliability objective of a software system, supporting the development of a system that achieves this objective and assessing the reliability of the product through testing and analysis. A detailed list of reliability engineering activities and methods, together with their purpose and description can be found in (Lakey and Neufelder, 1996). A software reliability program consists of a subset of the above practices and is de®ned for each project by combining dierent reliability achievement and assessment activities and methods, according to the project's characteristics. A number of methods are available to facilitate software reliability planning and control. Use of cost estimation models and their corresponding software tools, like COCOMO (Boehm, 1984),

0164-1212/99/$ ± see front matter Ó 1999 Elsevier Science Inc. All rights reserved. PII: S 0 1 6 4 - 1 2 1 2 ( 9 9 ) 0 0 0 1 0 - 2

174

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

SLIM (Putnam, 1992) and Checkpoint (Jones, 1986), to estimate the impact of a practice on time and cost has the following disadvantages: because model tuning is needed not all relevant historical data might be available; cost drivers are at too coarse a level of granularity to re¯ect the speci®c technique whose impact needs to be evaluated. A major drawback of the models is their use of data from projects other than those to which they are being applied. An alternative method to support strategy selection is process modeling and simulation, which involves analyzing the organizational software development process, creating a model of the process, and executing the model and simulating the real process. Although modeling and simulation has some limitations as well (the model accuracy depends on the quality of the model and of the calibration), there are many strengths of modeling that would advocate its use instead of other approaches mentioned. Modeling captures expert knowledge about the process of a speci®c organization; it does not need a real system to experiment with, so it does not aect the execution of real process; ®nally, dynamic modeling increases the understanding of a real process. 3. Software process modeling and simulation Developing a model of the software development process involves the identi®cation of entities, factors, variables, interactions and operations that are present in that process and are relevant to the intended use of the model. The entities include mainly people (developers, managers, customers, etc.), but can also include facilities, computer equipment, software tools, documentation and work instructions. Factors relevant to a general software development process include the application domain, the size of the project, the expected schedule and delivery date, the hardware platform, and other considerations. Variables include the number of software engineers, the skill levels of those individuals, the level of process maturity of an organization, the level of communication overhead, etc. The modeling complexity increases when identifying operations and interactions. Operations are the tasks that need to be performed in the software process to transform user needs and requirements into executable software code. The typical operations include requirements analysis, architecture development, detailed design, implementation (code and unit test), integration, and system test. Each of these takes some input (entity) and generates some transformed output (another entity). For example, the design operation uses software requirements as the input and transforms these into a design of the software system. The most important, and perhaps most dicult, modeling task is identifying and correctly representing

the interactions among the factors and variables in the software process that are relevant to the modeling goal. For instance, how does the number of developers aect communication overhead and overall productivity? How does a defect prevention technique aect the number of defects injected in software documents or code? How does the eort allocated to regression testing aect failure intensity during system testing and the number of defects remaining at delivery time. These interactions need to be properly modeled in order to closely represent reality. While the model is useful for representing the structure of the process, execution of the model is very helpful in understanding the behavior of the process. It gives management a tool that has been lacking for a long time. The simulation capability allows a manager to make a decision and have a high degree of con®dence in what the results of that decision will be. Without the simulation the decision is purely speculation. The complexity of a decision is too overwhelming to be understandable without the dynamic execution of the software model. 4. Comparison to other approaches Simulator presented here can be used for tracking the quality and reliability of the software throughout the development process. Reliability prediction and estimation models such as reliability growth models and prediction models have a similar goal. The dierence is that the reliability growth models are analytical, address only the system testing phase, and each has unrealistic assumptions that restrict their applicability. The modeling and simulation approach addresses all the development phases and is tailored to the real process that is modeled. Reliability prediction models, as that developed for Rome Laboratory (SAIC, 1987) are static. It is postulated here that running simulations to determine reasonable predicted defect levels at delivery in consideration of cost, schedule and stang is a better way of predicting than using a static model. This approach allows the metrics tracked during the project to be used to make adjustments to the model to re¯ect what is really happening. That is, process behavior is explained by the model. With a static model, there is no way to know why deviations from predictions exist. The Rome Laboratory model is a regression model. All of these regression models inherently infer a cause±eect relationship between the independent variables and reliability, when in fact there is no proven causal relationship; rather the entire process is the cause of software defects and failures. Tausworthe and Lyu (1996) developed a simulator of the software reliability process, but it is at a much higher

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

175

level of process abstraction and captures a very reduced number of process parameters. Other system dynamics simulators of the software development process have been developed, but their purpose and scope is dierent. Most of the previous models, such as Abdel-Hamids model (Abdel-Hamid et al., 1991), focus on the management aspect of software development as opposed to the technical aspect; they also do not include the requirements analysis phase. Madachy's model (Madachy, 1996) was developed to analyze the eect of inspections, and Tvedt developed a model of the incremental life cycle (Tvedt, 1996) for analyzing cycle time reduction. The model presented in the next section has been developed to capture the technical issues of software quality and reliability and to assess the eect of various reliability practices. 5. System dynamics process model description The following model was developed to support project planning for the purpose of evaluating software reliability engineering strategies. It is a generic model intended for use in high level decision making. The model represents the initial work performed by the authors in software process modeling and simulation and is part of a decision support system for software reliability engineering strategy selection and assessment (Rus, 1998). The main components of the model correspond to the production phases of the software development

process (requirements analysis, design and coding) and the system testing phase. A brief description of a production phase is presented here. More model details, as well as a description of the SystemTesting block, together with more simulation results and model use examples can be found in (Rus, 1998). The model was implemented in Extend V4.0, a simulation software package from Imagine That (Extend, 1995), by using hierarchical blocks. Each ProductionPhase block has a Development block and a Management block, corresponding to technical and managerial activities, respectively. The Development block models items production, as well as quality assurance activities. Items can be requirements documents, design documents, and source code. The Development block has two sub-blocks: Items and Defects that are shown in Fig. 1. Sub-blocks of Items and Defects are expanded in Fig. 2, at the lowest Extend implementation level. The Items sub-block models items production and veri®cation and validation (V&V) activities. Items from the previous phase enter the current production phase (ItemsIn) and items corresponding to the current phase are produced (for instance if the current phase is design, requirements documents are the input and design documents are generated). The production rate depends on many factors, such as the productivity of the personnel (ProdProductIn), and schedule pressure (SchPresPrIn). There are other factors aecting this rate that are not shown in the ®gure: process factors like methods, techniques, tool support, and metrics

Fig. 1. Development and Management blocks of a ProductionPhase.

176

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

Fig. 2. Items and Defects blocks.

collection eort; and product factors like complexity, criticality and fault tolerance. The items produced are then veri®ed (by reviews, walkthroughs, and/or inspections) at the V8VRate. This rate depends on factors like personnel productivity (VVProductIn) and schedule pressure (SchPresVVIn). After veri®cation and validation, items are passed to the next phase (ItemsOut). For each item that is generated, there are also defects injected. Defect generation, detection and correction is captured by the Defects sub-block. Defects are generated at a rate depending on the production rate, schedule pressure and other factors such as process maturity and development methodology. Defectsi1In and Defectsi2In are defects propagated from previous phases. The defect detection rate depends on the veri®cation and validation rate (VVRateIn), eciency of V&V activities, schedule pressure, defect density, and other factors. Detected defects will be corrected, and possibly new defects will be generated. Undetected defects propagate to subsequent phases (DefectsOut). The Management block (Fig. 1) models the human resources, planning and control aspects of the process. For each of the production, veri®cation and validation (V&V), and rework activities there is a corresponding sub-block in the Management block. Each of these three sub-blocks models the eort consumed (man-days) in the tasks of producing, verifying and reworking items. These modules also capture personnel experience increasing with time (learning) and schedule pressure resulting from the dierence between the estimated

schedule and the actual one. Actual productivity determines the rate at which items are produced. Figs. 3 and 4 present examples of model execution (process simulation) outputs. Fig. 3(a) shows the variation of the number of items produced and veri®ed during a production phase. The number of defects injected, detected, and corrected throughout the phase can be seen in Fig. 3(b). Fig. 4(a) shows the evolution of the number of coding faults identi®ed and corrected during system testing, and the coding faults remaining in the product. Fig. 4(b) presents an example of sensitivity analysis: the variation of failures encountered during system testing, with the number of design faults at the beginning of system testing. These values and shapes of these graphs will be unique to a speci®c company that calibrates the model with metrics and inputs speci®c to that company. Software reliability engineering practices are modeled by considering their in¯uences on dierent factors that impact defects. For instance, defect prevention techniques (e.g. formal methods) will decrease the value of defect generation rate. Defect detection techniques will increase the number of defects detected and reduce the number of defects propagated to the next phase. Fault tolerance methods (e.g. N-version programming) reduce the impact that remaining defects have on the reliability of the operational system. There is, of course, some additional up-front eort and cost associated with these practices, but the overall development time and eort may be reduced, by longterm reduction in rework time. By reducing the number

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

177

Fig. 3. Variation of the number of items (a) and defects (b) through a production phase.

of remaining defects, and/or masking them, the reliability of the product will increase. Simulation is used to analyze the eect of these practices in terms of failures, cost, and stang, allowing trade-os among reliability strategies.

6. Discussion on system dynamics model The model described above was executed with assumed parameter values for each of the factors identi®ed. By changing these factors individually the

Fig. 4. (a) Variation of the number of coding faults during system testing. (b) Variation of failures encountered during system testing, with the number of design faults at the beginning of system testing.

178

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

corresponding impact on the outputs of the model (cost, schedule and quality) can be evaluated. The model is implemented with building blocks derived from the generic ProductionPhase block by adding details speci®c to each development phase while maintaining the common features and architecture. As a result, the model is modular, extensible, adaptable and ¯exible. The example that we used in (Rus, 1998) corresponds to a waterfall development life cycle model, but the model can be adapted to represent for example an incremental process. The system dynamics approach, which is a continuos modeling paradigm, considers that items are identical and treats them uniformly. This is an assumption that works at a higher level of modeling. If the use and the goal of the model require more detail, then dierences between entities and entities' attributes must be considered. For example, design and code items dier by structural properties such as size, complexity, and modularity; usage, and eort allocated for development. Similarly, defects can have dierent types, consequences, generation and detection phase, etc. In Extend, when using discrete event modeling, items can have attributes attached, with random values (within a speci®ed range), and with a speci®ed distribution. Activity duration and item generation and processing times can vary with other model parameters (variables), according to a speci®ed equation, and can have a probabilistic distribution around the computed value. Subprocesses such as detailed design and system testing (reliability growth testing) appear to be more like event-driven processes rather than continuous.

Therefore, because discrete event modeling allows a more detailed modeling capability which was required by the application described in the next section, the system dynamics model was transformed into an alternative discrete event model for a production phase, as illustrated by the preliminary design phase example. 7. Discrete event model application The discrete event model implementation of the software process was developed to support a speci®c project, Crusader. The Crusader is a large army development project. There are two vehicles, called Segments, in each Crusader System. The system consists of the Self-propelled Howitzer (SPH) and the Re-supply Vehicle (RSV). The software is being developed using object-oriented analysis and design (OOA/D) and Rational Rose CASE tool. An incremental software life cycle approach is being used, with speci®c functionality in ®ve planned major builds. A top-level architecture has been constructed for the Crusader software system. It consists of 40 Computer Software Con®guration Items (CSCIs). These could also be called subsystems. Each CSCI is developed and managed independently of the other CSCIs. The approach taken is to model one of these CSCIs, using it as a prototype for learning the relevant dynamics associated with defects, reliability and other process parameters. Many of the important metrics associated with reliability that are captured in the discrete event model are shown in Fig. 5. Some of the most important metrics

Fig. 5. Crusader software reliability management metrics.

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

179

needed to support software reliability management are related to rework. As it pertains to reliability, rework is important because it helps determine the amount of reliability improvement that can be expected for a given schedule and stang level. Software development activities may be completed on time, but if no time is built in for rework, then the defects that are found during testing cannot be ®xed and the reliability cannot improve signi®cantly. Rework is also important from the standpoint of process improvement. Reducing rework increases the total amount of quality software that can be developed per time unit. The model helps pinpoint areas where improvements should be made to reduce rework. To substantially reduce rework a signi®cant investment needs to be made in process improvement during implementation of the software development schedule. This will increase costs in the short term, and may lengthen the schedule. These dynamics are captured in the model. 8. Prototype discrete event model description The initial model being developed captures the process for Preliminary Design. The structure of the model, developed using Extend V4.0, is described below. The prototype discrete event model represents the Crusader Software Development Process, and it is being piloted on a single CSCI, which will be called Module X here. This discrete event model is actually a hybrid model that also incorporates system dynamics concepts, using feedback loops. The main purpose of the model is to predict, track, and control software defects and failures over the entire Crusader development, through the year 2005. It also facilitates tracking and control of software project costs and schedules. The model currently addresses only the Preliminary Design phase. Later on this year it will be expanded to include Detailed Design, Code and Unit Test, Subsystem Test, Element Test and Segment Test. There will be two modules. The Defect Module will predict defects, eort, and rework for each phase through Code and Unit Test. The Failure Module will estimate reliability and test eort during testing. There are three types of inputs to Preliminary Design: Segment Use Cases/Scenarios, Software Requirements Speci®cations, and the architecture in the form of Rose Models. The Crusader project has de®ned 10 Use Cases, 40 CSCIs and approximately 40 Rose Subsystems to correspond to the 40 CSCIs. Module X is responsible for approx. 50 Segment Scenarios, has 200 CSCI Scenarios of its own, and will use 200 classes in its design. In a pure discrete event model the activity of Re®ning the Scenarios would be represented by a single Operation block, with a single set of outputs, including schedule and quality. The problem with that approach (only one

output data point) is that no dynamics can be incorporated into the activity during the process. In order to incorporate feedback the Re®ne Scenarios Activity has been divided into ®ve discrete steps, allowing the outputs of one step to be fed into the next step. The inputs each are divided up into ®ve sets to be processed during each iteration. In other words, 10 Segment Scenarios, 40 CSCI Scenarios and 40 Rose Classes are processed for each step. This is dierent from actual process implementation, but the approximation should be acceptable for modeling. Random data is generated for each of the input products to the Preliminary Design activity. This data is stored in a spreadsheet ®le. All of this stored data is accessed throughout the simulation. The values for Segment Scenario size are normally distributed around the average value of 10. The values for Scenario Quality refers to the number of defects associated with a particular scenario prior to it being used to de®ne CSCI scenarios. Class size can be characterized through a number of dierent object-oriented design metrics, such as number of instance methods in a class, number of instance variables in a class or number of class methods. The screen capture in Fig. 6 shows the three types of input products, Segment Scenarios, CSCI Scenarios and Classes, being read into an Operation block where that data is processed and resources are expended to perform the Re®ne CSCI Scenarios task. The data from the spreadsheet is used to calculate some project parameters during the simulation. Fig. 6 represents three main steps for the activity of Re®ning CSCI Scenarios. First, the scenarios are developed using the Segment Scenarios allocated to MCS. Then, a Walkthrough is performed to evaluate the completeness and correctness of the CSCI Scenarios and their mapping to Segment Scenarios. Finally, any defects found are documented and those are corrected through the Rework Activity. During each of these activities data is generated for Eort ± in terms of labor hours expended, and Quality ± in terms of numbers of defects. All of this calculated data is passed on to a ®nal block in the model, which plots this data graphically over the course of a run. The list of outputs from the model includes development eort, review eort, rework eort, defects generated, defects found and escaped defects. These parameters are each dependent on a number of factors that interact with each other over the course of a software development project. The basis of the model is the set of factors for each parameter and the equations used to calculate the factor values during execution of the model. These factors include both process and product factors. Some of the process factors included in the model are the Manpower Factor, Schedule Factor, Communication Overhead Factor, Work Rate Factor, Process Maturity Factor and Tool Support Factor. The product factors include Size, Quality, and Complexity.

180

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

Fig. 6. Re®ne scenarios process for Modules X.

The model factors are dynamic. That is, they change after each event (iteration) occurs in a simulation. Many of the factors aect other factors, which in turn are affected by the same factors. This provides feedback loops. The interaction between factors in the model is intended to represent actual Crusader software development project dynamics. As work is scheduled and accomplished, the project team will often ®nd that the eort required for a task is larger than originally anticipated. This occurs in the middle of a development activity. When the project gets behind schedule decisions are made that change the expected state. Work is sped up or postponed. Certain activities may be cancelled, such as periodic walkthroughs. These decisions impact overall eort and the number of defects injected and found. Quality can suer if the schedule is emphasized. The values used for model factors currently are assumptions ± no historical project data is available from Crusader to represent the relationships among the factors. These assumptions will be replaced with factors calculated from data collected on the Crusader project as development progresses and the relationships are more clearly understood. The intent is to have a working model by the end of 2000 useful for management decisions. The screen capture in Fig. 7 shows the output for a single simulation run. The Actual (simulated) values are dierent from the Expected values for each set data being plotted. There are always be dierences as the

model is currently set up because the Size and Quality data is randomized for each iteration, making it dierent from the expected size and quality data. This causes the Size and Quality Factors to be greater than or less than 1. Depending on amplitude and direction of the dierences as compared to expected, the actual values would be above or below the expected values for each of the plots above. This model uses a Monte Carlo simulation, which means there are random draws for each execution, making the results dierent. With all inputs being the same, in a single execution of the Module X Preliminary Design process, the outcome could be as shown in Fig. 7 or some other outcome. 9. Conclusions and future work A dynamic model of the software development process allows answer questions such as: ``How much time and eort will it take to complete the Preliminary Design phase for Module X?'', ``How many defects are going to be generated during that phase? How will that aect product reliability?''. ``How much will the eort increase if defect prevention practices are used? How will that shorten system testing time?''. These questions are very dicult to answer using methods currently available in industry. There is a great deal of variability in the possible outcomes using static models. The modeling

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

181

Fig. 7. Output screen for Module X Discrete Event Model.

methods described in this paper provide insight into why and how things occur. Crusader management believes there is a great potential bene®t for using the discrete event modeling approach not only for reliability, but for many other aspects of project management, including cost and schedule. With good data this model can be continuously updated and improved throughout the current program phase, which concludes at the end of 2000. Prior to the next phase, Engineering and Manufacturing Development (EMD), the model should be validated to the point where it supports eective management decisions. Outputs predicted by simulation will be compared with actual data collected from the project. Discrepancies will be identi®ed and used to improve the validity and accuracy of the model. With a real-world model, management can make good decisions throughout the EMD phase of the program to meet software reliability requirements. References
Abdel-Hamid, T., Madnick, S.E., 1991. Software Project Dynamics An Integrated Approach. Prentice-Hall, Englewood Clis, NJ. Boehm, B.W., 1984. Software engineering economics. IEEE Transactions on Software Engineering 10 (1), 5±21. Boehm, B.W., Hoh, 1996. Identifying quality-requirements con¯icts. IEEE Software 25±35. Jones, C., 1986. The SPR feature point method. Software Productivity Research.

Extend ± Performance modeling for decision support, 1995. User's Manual, Imagine That. Lakey, P., Neufelder, A.M., 1996. System and Software Reliability Assurance Notebook, Produced for Rome Laboratory by SoftRel. Contact peter.b.lakey@boeing.com for a copy of the Notebook. Madachy, R., 1996. System dynamics modeling of an inspection-based process. Proceedings of the Eighteenth International Conference on Software Engineering, Berlin, Germany. Putnam, L.H., 1992. Measures for Excellence ± Reliable Software on Time, Within Budget, Yourdon Press Computing Series. Rus, I., 1998. Modeling the impact on project cost and schedule of software reliability engineering strategies. Ph.D. Dissertation, Arizona State University, Tempe, Arizona. SAIC, 1987. Methodology for software reliability prediction. Rome Laboratory RADC-TR-87-171. Tausworthe, R., Lyu, M., 1996. Software reliability simulation. In: Lyu, M. (Eds.), Handbook of Software Reliability Engineering. IEEE Computer Society Press, McGraw-Hill, New York. Tvedt, J.D., 1996. An extensible model for evaluating the impact of process improvements on software development cycle time. Ph.D. Dissertation, Arizona State University, Tempe, Arizona. Ioana Rus is a Ph.D. candidate in the Department of Computer Science and Engineering at Arizona State University. She received her Master in Computer Science degree from Arizona State University and Bachelors of Science from Polytechnical Institute Cluj, Romania. Her research interests include software process improvement, process modeling, software quality and reliability, arti®cial intelligence, and neural networks. She is a member of the IEEE Computer Society and ACM. James S. Collofello is a professor in the Department of Computer Science and Engineering at Arizona State University. He received his Doctor of Philosophy degree in Computer Science from Northwestern University, Master of Science in Mathematics and Computer Science from Northern Illinois University, and Bachelors of Science in

182

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182 his BSEE from Northwestern University. He joined McDonnell Douglas (now Boeing) in 1990 as a Reliability Engineer. As an engineer in the Software Engineering Process Group (SEPG), Mr. Lakey has been responsible for developing tools and processes that can be used by internal customers to improve software product quality. His main responsibility now is for managing a Software Reliability Support subcontract with United Defense in Minneapolis on a large Army program called the Crusader.

Mathematics and Computer Science from Northern Illinois University. His teaching and research interests are in software quality assurance, software reliability, safety, and maintainability, software testing, software error analysis, and software project management. He is a member of the IEEE Computer Society and ACM. Peter B. Lakey received his MS in Engineering Management from University of Missouri-Rolla, MBA from Washington University, and

A System Dynamics Software Process Simulator for Staffing Policies Decision Support
Dr. James Collofello Dept. of Computer Science and Engineering Arizona State University Tempe, Arizona 85287-5406 (602) 965-3190 collofello@asu.edu Ioana Rus, Anamika Chauhan Dept. of Computer Science and Engineering Arizona State University Dan Houston Honeywell, Inc. and Dept. of Industrial and Management Systems Engineering Arizona State University Douglas Sycamore Motorola Communication Systems Divsions Scottsdale, Arizona Dr. Dwight Smith-Daniels Department of Management Arizona State University Abstract
Staff attrition is a problem often faced by software development organizations. How can a manager plan for the risk of losses due to attrition? Can policies for this purpose be formulated to address his/her specific organization and project? Proposed was to use a software development process simulator tuned to the specific organization, for running "what-if" scenarios for assessing the effects of managerial staffing decisions on project's budget, schedule and quality. We developed a system dynamics simulator of an incremental software development process and used it for analyzing the effect of the following policies: to replace engineers who leave the project, to overstaff in the beginning of the project or to do nothing, hoping that the project will still be completed in time and within budget. This paper presents the simulator, the experiments that we ran, the results that we obtained and our analysis and conclusions. Introduction We used process modeling and simulation for estimating the effect on the project cost, schedule and rework of different staffing policies. Assessment of such managerial decisions could be also done by other methods like formal experiments, pilot projects, case studies, and experts opinions and surveys. Modeling and simulation is preferred because it does not have the disadvantages of the other methods (like possible irrelevance in the case of formal experiments or pilot projects, interfering with the real process and taking a long time for case studies or subjectivity for experts opinions and surveys). System Dynamics Modeling A software process model represents the process components (activities, products, and roles), together with their attributes and relationships, in order to satisfy the modeling objective. Some modeling objectives are to facilitate a better human understanding and communication, and to support process improvement, process management, automated guidance in performing the process, and automated process execution [6]. In addition, we demonstrate how process modeling experimentation can be used to investigate alternatives for organizational policy formulation. There are many modeling techniques developed and used so far, according to the modeling goal and perspective. The paradigm chosen for this research was system dynamics modeling. Richmond considers system dynamics as a subset of systems thinking. Systems thinking is "the art and science of making reliable inferences about behavior by developing an increasingly deep understanding of underlying structure" [11]. Systems thinking is, according to Richmond, a paradigm and a learning method. System dynamics is defined as "the application of feedback control systems principles and techniques to modeling, analyzing, and understanding the dynamic behavior of complex systems" [2]. System dynamics modeling (SDM) was developed in the late 1950's at M.I.T. SDM is based on cause-effect relationships that are observable in a real system. These cause-effect relationships constantly interact while the computer model is being executed, thus the dynamic

interactions of the system are being modeled, hence its name. A system dynamics model can contain relationships between people, product, and process in a software development organization. The most powerful feature of system dynamics modeling is realized when multiple cause-effect relationships are connected forming a circular relationship, known as a feedback loop. The concept of a feedback loop reveals that any actor in a system will eventually be affected by its own action. Because system dynamics models incorporate the ways in which people, product, and process react to various situations, the models must be tuned to the organizational environment that they are modeling. SDM is a structural approach, as opposed to other estimation models like COCOMO and SLIM, that are based on correlation between metrics from a large number of projects. The automated support for developing and executing SDM simulators enables handling the large complexity of a software development process which can not be handled by a human mental model. Building and using the model results in a better understanding of the cause-effect relationships that underlie the development of software. A simulator to be used for estimating, predicting, and tracking a project requires a quantitative modeling technique. SDM utilizes continuous simulation through evaluation of difference and differential equations. These equations implement both the feedback loops that model the project structures as flows, as well as the rates that model the dynamics of these flows. Thus, application of an SDM model requires an organization to define its software development processes and to identify and collect metrics that characterize these processes. Simulation enables experimenting with the model of the software process, without impacting on the real process. The effects of one factor in isolation can be examined, which cannot be done in a real project. The outcome of the SD simulation has two aspects: an intellectual one, consisting of a better understanding of the process and a practical one, consisting of prediction, tracking, and training. SDM was applied to the software development process for the first time by Tarek Abdel-Hamid and Stuart Madnick [2]. Their model captures the managerial aspects of a waterfall software life cycle. It was the starting point for many subsequent models of the entire process [15], [14] or parts of it [5] and [9] that have been successfully used for resource management [8], [13], process reengineering [4], project planning, and training [12]. Abdel-Hamid [1] studied the impact of turnover, acquisition, and assimilation rates on software project cost and schedule. He found that these two response variables can be significantly affected by an

employment time of less than 1000 days, by a hiring delay less than or greater than 40 days, or by an assimilation delay greater than 20 days. We take this discussion a step further into the practical realm by focusing on the turnover issue and demonstrate how SDM can be used in decision support for strategies that address staff turnover. Description of System Dynamics Model Our system dynamics modeling tool was developed using the ithink simulation software [7]. The model incorporates four basic feedback loops comprised of non-linear system dynamic equations. The four feedback loops were chosen because they encompass the factors that are typically the most influential in software projects [16]. All of these feedback loops begin and end at the object labeled, Schedule and Effort, which is the nucleus of the system. This object represents a project schedule and the effort in person hours to complete a schedule plan. See Figure 1. The first feedback loop represents the staffing profile of a project (refer to the loop "Schedule and Effort", "Staffing Profile", "Experience Level", and "Productivity" on Figure 1). The staffing profile affects productivity based on the number of engineers working on a project, the domain expertise of the engineers, and amount of time an engineer participates on a project. The second feedback loop models the communication overhead (refer to the loop "Schedule and Effort", "Staffing Profile", "Communication Overhead", and "Productivity" on Figure 1). The more people on a project will result in an increase in communication overhead among the team members and thus, decreases the productivity efficiency [14] [15]. The third feedback loop takes into consideration the amount of defects generated by the engineers during the design and coding phases of an increment, which translates into rework hours (refer to the loop "Schedule and Effort", "Staffing Profile", "Experience Level", "Defect Generated, and "Rework Hours" on Figure 1). The tool also models the impact of domain expertise on defect generation. An engineer with less domain expertise generates more defects than an engineer with a higher degree of domain expertise. The fourth feedback loop models the schedule pressure associated with the percentage of work complete per the schedule (refer to the loop "Schedule and Effort", "Schedule Pressure", and "Productivity" on Figure 1). The farther behind schedule the greater the schedule pressure. As schedule pressure increases, engineers will work more efficiently and additional hours, increasing productivity toward completing the work. However, if schedule pressure remains high and

engineers are working many hours of overtime, they begin to generate more defects and eventually an exhaustion limit is reached. Once the exhaustion

limit is reached, productivity will decrease until a time period such that the engineers can recoup and begin working more productively again.

Schedule Pressure

Rework Hours

Schedule and Effort Productivity

Defects Generated

Staffing Profile Communication Overhead Experience Level

Figure 1. Basic Feedback Loops of the Model The model inputs are summarized in Table 2 familiar. For a summary of the experts' credentials, The model outputs are provided in the form of refer to Table 1. graphs that plot the work remaining in each Each expert (or evaluator) was asked to review the increment, current staff loading, total cost, percent tool from a Project Leader position, a Software complete, and quality costs. Figure 2 is an example Developer position, and to assess the tool's technical of the output that shows the graph for Staff Loading characteristics. During the simulations, the experts and Total Cost plots. were to observe the output for valid trend data and for The model was validated by two methods, expert a level of comfort with the accuracy of the results. opinion and reproduction of actual project results. Six The demo given to each expert was identical. After experts from Motorola and very experienced software completing the demo, the experts were given an professionals, examined the simulator model and opportunity to run different scenarios with the expressed their confidence in its ability to model the supplied data or scenarios with their own project data. general behavior of a software project. In addition, Finally, they were given an opportunity to compare one of the experts used the model to accurately the results to any project manage tools they use. reproduce the results of a project with which he was

Table 1. Background Summary of Each Expert Expert #1 Title: Principal Software Engineer Degrees: M.S. and Ph.D. in Computer Science Years of Experience: 13 Years Responsibilities: Project Leader, Systems Engineer, Software Engineer, and Proposal Development. Other Pertinent Information: Former Assistant Professor of Computer Science at Arizona State University, Tempe, AZ. Title: Principal Software Engineer Degrees: Ph.D. in Computer Science Years of Experience: 25+ Years Responsibilities: Project Leader Other Pertinent Information: Former Assistant Professor of Computer Science at Arizona State University, Tempe, AZ. Title: Software Engineer Degrees: B.S in Computer Science Years of Experience: 11 years Responsibilities: Software Development - Currently developing a ManMachine Interface. Special Awards: Special Achievement Award, 1987 - Naval Avionics Center Exceptional Performance, 1991 - Motorola GSTG, CSO Engineering Award, 1993 - Motorola GSTG Title: Software Engineer Degrees: M.S. in Computer Science; M.B.A. in Management Years of Experience: 33 Years Responsibilities: Developing software for a medium size aerospace project. Other Pertinent Information: A retired Air Force Lieutenant Colonel. Title: Chief Software Engineer Degrees: B.S. in Electrical Engineering Years of Experience: 39 Years Responsibilities: Software Process and Quality Special Awards: Dan Nobel Fellow Award (Motorola); IEEE contributor Other Pertinent Information: Author: Motorola GED's Software Process Manual. Instructor: Motorola GED Project Leader Training. Chair: Motorola GED Software Metrics Working Group. Chair: Motorola GED Software Training Program Working Group. Member: Motorola GED Software Defect Prevention Working Group. Three Patents. Developing software fault occurrence and prediction model. Title: Engineering Metrics Manager Degrees: B.S. in Electrical Engineering Years of Experience: 35 Years Responsibilities: Coordinate collection and analysis of Engineering metrics. The first highlight was seeing the results from simulating expert #3's Man-Machine Interface project, which was 80% completed. Before simulating the project, some actual historical data along with some remaining estimated data to complete the ManMachine Interface project was entered into the simulator. After simulating the project with actual

Expert #2

Expert #3

Expert #4

Expert #5

Expert #6

Overall, the tool fared extremely well against the evaluation process and the experts were quite pleased with the results. There were two particularly exciting highlights that occurred during the reviews with expert #3 and expert #5 that are worth noting.

and estimated data, Currently the simulator tracked there is an updated Number of increments 3 within a couple of version of the Productivity percentage points of simulator tool that Engineer 1 (domain inexperienced) .8 expert #3's actual and has been Engineer 2 (domain experienced) 1.0 predicted results. incorporated into a Engineer 3 (very domain 1.2 The second corporate training experienced) highlight was program. The Defect generation rates simulating three sets simulator is being Engineer 1 (domain inexperienced) .0300/hr of project data used to Engineer 2 (domain experienced) .0250/hr supplied by expert #5. demonstrate an Engineer 3 (very domain .0225/hr He generated these incremental experienced) three scenarios with development Defect detection the aid of the approach over a % of defects found in peer reviews 80% COCOMO tool for waterfall % of defects found in integration test 20% the three engineering development Percent of schedule allocated to rework 10% activities: Detail approach, peer Defect removal costs Design, Code and review Found in peer reviews 2 hr/defect Unit Test, and effectiveness, Found in integration testing 10 hr/defect Integration. After schedule simulating the three compression scenarios, expert #5 concurred that the simulator concept, mythical man-month perception, and the produced believable results for all three scenarios [14]. 90% completion syndrome. Description of the experiment Attrition is clearly detrimental to a software development project. When an experienced developer leaves a project prior to its completion, then a project manager faces the question of whether to replace the departing individual or to forego the expense of replacement and make other adjustments, such dropping functionality, slipping the schedule, and rearranging assignments. The answer to the question of replacement may turn on factors such as the experience of the developer, the percent completion of the project, the number of engineers on the project, the time required for hiring or transfer, and the attrition rate of the organization. Replacement is costly, but may be required to keep a project on schedule. It leads one to wonder, can the dilemma presented by attrition be resolved by yet another alternative, that of staffing a project with more than the necessary number of development engineers? Are there project situations in which it is economically feasible, or even desirable, to mitigate the risk of attrition by overstaffing? The experiment described here was motivated by these questions and the results offer an indication of the desirability of staffing policies which include the overstaffing option.

Table 2. Model Inputs

Number of 1: engineers Total Staff Load Time of attrition 1: 50.00 Attrition2:rate 2932.12

Low Value 10 2: Total Cost At the end of increment 1 10%

High Value 20 At the end of increment 2 30%
2

1: 2:

25.00 1466.06

1

2 1

1
1: 2: 0.00 0.00 0.00 Budget: Page 1

1 2
500.00

2

1000.00 Hours

1500.00

2000.00

Figure 2. Sample Output for Staff Loading and Total Cost were used with corresponding productivity factors The experiment was conducted for an incremental (Table 2). The departing engineers were assumed to project, consisting of three equal and non-overlapping be very experienced, while the replacements were increments, with a total effort estimate of 9000 personassumed to be inexperienced. The learning rate is hours. This is roughly equivalent to ten engineers such that inexperienced and experienced engineers completing an increment every eight weeks. Three advance one experience level with each increment levels of application domain experience completed, until they become very experienced. (inexperienced, experienced, and very experienced)  Time in the project at which attrition occurs Three strategies were considered:  Attrition rate  No replacement after attrition The model's response variables recorded for this  Replace engineers as they leave experiment were:  Overstaff at the beginning of the project at the  Project duration relative to the estimated schedule same level as the attrition  Project cost These strategies were considered for combinations  Rework cost (hrs) of three factors: Each strategy was evaluated for two values of each  Number of engineers on the project of the three factors (Table 3).

Table 3. Factors for Studying Staffing Strategies for Attrition

Time of Attrition (End of Number of Run Engineers Increment) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 10 10 10 10 10 10 10 10 10 20 20 20 20 20 20 1 1 1 1 1 1 2 2 2 1 1 1 2 2 2

Attrition Rate 10 10 10 30 30 30 30 30 30 10 10 10 10 10 10

Action: Replace (Y/N) or Overstaff N Y O N Y O N Y O N Y O N Y O

Duration Relative to Estimated Schedule 1.00 .97 .96 1.09 .98 .95 1.04 .98 .91 1.10 1.08 1.08 1.09 1.09 1.08

Project Cost ($1000) 675.10 683.68 701.10 558.30 646.32 692.17 641.10 670.57 738.58 682.70 705.32 741.60 713.90 716.11 772.40

Rework Cost (hr) 771 789 822 724 816 902 778 789 918 833 859 924 851 851 940

Experimental results The data in Table 4 reveals the trends found in the simulation results. Table 4. Results from Simulating Staffing Strategies for Attrition When the number of engineers working on a project is small and attrition rate is low, the effect of attrition is not very visible. For example, consider Runs 1, 2, and 3 in Table 4. In this case, the number of engineers is at the low level (10) and the attrition rate is also at the low level (10%). The results of Run 1 indicate that the project is completed on time without replacement or overstaffing. The fact that the project completes early or on time in all three of these runs suggests that attrition does not have a significant effect if the number of engineers is low and the attrition rate is low. As both the number of engineers working on a project and the attrition rate increase, the effects of attrition becomes more visible. Consider Runs 4, 5, and 6 in Table 4. In Run 4, the estimated schedule is overrun by 9%, but Runs 5 and 6 show early completion. Also, the effects of attrition are more visible when attrition occurs at the end of increment 1 than when it occurs at the end of increment 2. In Run 4 when attrition occurs at the end of increment 1, the project overruns the schedule by 9%, whereas in Run 7 when it occurs at the end of increment 2, the schedule overrun is only 4%. Also, in Run 6, the schedule underrun is 5%, whereas in Run 9, it is 9%. These runs also indicate that attrition costs more when it occurs later in the project. The results in Table 4 may also be viewed as an evaluation of three staffing strategies for attrition in five sets of the three factors. Each set is comprised of (number of engineers, increment number ending at time of attrition, and attrition rate). Because we are considering three factors in this experiment, the orthogonal relationships of these five sets of factors are illustrated in the cube of Figure 3. When the staffing strategy for attrition is compared within each set of factors, desirable strategies can be identified based on cost and project duration against the estimated schedule. These results are summarized in Figure 4, which adds the desirable strategies to the cube of Figure 3. In Figure 4, N represents the strategy of no replacement and no overstaffing, R represents the replacement strategy, and O represents the overstaffing strategy. In some situations either of two strategies is desirable, depending on whether one wishes to minimize cost or complete the project on

schedule (or in the N for cost (20,2,10) case, (10,2,30) minimize the R for schedule Increment no. schedule overrun). For e x a m p l e , completed point (10,1,10) in at time of Figure 4 represents a attrition N for cost (20,2,10) project in which the O for schedule ratio of estimated effort to the number of Attrition rate engineers is high (say finish about 3% (10,1,30) early but will add N for cost about 1% to the cost R for schedule of the project. Overstaffing to anticipate this (10,1,10) attrition will allow (20,1,10) R N No. of engineers the project to complete about 4% early and will increase the project cost about 4%. The

900 hours) and one person leaves at the end of Increment 1. In this scenario, the project can finish on-time without replacing the person. Replacing the person will allow the project to most desirable strategy for this set of factors, then, is no action (N) because it allows the project to finish on time at the lowest cost.

As another example, in the case of 20 engineers on the project and two people leave at the end of increment 2 (point (20,2,10) in Figure 4), other schedule regardless of the chosen course of action, but overstaffing (O) will limit the overrun. Neither

strategies are indicated. The project will overrun the estimated replacing nor overstaffing (N) remains the best option for minimizing cost.

Figure 3. Relationships of Factor Sets in Table 2.

Figure 4. Desirable Staffing Strategies for Attrition under Each Set of Factors most influential factors for modeling software dynamics and, more importantly, their degrees of Conclusions and Future Research influence on project outcomes. Our research grew out of the concern for having strategies to address attrition in software development organizations. We also wished to examine the ability of a more abstract simulator, composed of four process feedback loops, to provide realistic data for supporting the formulation of staffing policies. This experiment suggests several implications for software project staffing in response to attrition. In general, no action for attrition is the least expensive choice and overstaffing is the most expensive choice. The choice of no action is indicated when schedule pressure is high and cost containment is a priority. The replacement strategy has the advantage of alleviating the exhaustion rate and concomitant increases in attrition. Even though overstaffing is the more expensive option, it can have the very desirable effect of minimizing project duration. Thus, this strategy should be considered for projects in which completion date has been identified as the highest priority. Such a circumstance might occur if delivery of a commercial application is required during a market "window of opportunity," or if the contract for a custom application has a penalty attached to late delivery. The cost of overstaffing must be weighed with the other factors in the project and process simulation can provide data for this decision. Regarding process modeling objectives and the use of SDM, this experiment has demonstrated the use of a software development process simulator, one which models the major dynamic influences in a project. The software practitioners who examined the results produced by this experiment found them to be both acceptable and realistic. This model is capable of supporting designed experimentation and providing data for supporting staffing decisions. This group continues to research in software development process simulation. In addition to the questions of attrition, the group continues investigating into questions related to project management training, project risk assessment, and product quality. In terms of process modeling issues, further research continues to identify and validate the

References
[1] Abdel-Hamid, Tarek, A Study of Staff Turnover, "Acquisition, and Assimilation and Their Impact on Software Development Cost and Schedule", Journal of Manager Information Systems, Summer 1989, vol. 6, no. 1, pp. 21-40. [2] Abdel-Hamid, Tarek and Stuart E. Madnick, Software Project Dynamics An Integrated Approach, PrenticeHall, Englewood Cliffs, New Jersey, 1991. [3] Abdel-Hamid, Tarek, "Thinking in Circles", American Programmer , May 1993, pp. 3-9. [4] Bevilaqua, Richard J. and D.E. Thornhill, "Process Modeling", American Programmer, May 1992, pp. 3-9. [5] Collofello, James S., J. Tvedt, Z. Yang, D. Merrill, and I. Rus, "Modeling Software Testing Processes", Proceedings of Computer Software and Applications Conference (CompSAC'95), 1995. [6] Curtis, Bill, M. I. Kellner and J. Over, "Process Modeling", Communications of the ACM, 35(9), Sept. 1992, pp. 75-90. [7] ithink Manual , High Performance Systems, Inc., Hanover, NH, 1994. [8] Lin, Chi Y., "Walking on Battlefields: Tools for Strategic Software Management", American Programmer, May, 1993, pp. 34-39. [9] Madachy Raymond, "System Dynamics Modeling of an Inspection-based Process", Proceedings of the Eighteenth International Conference on Software Engineering, Berlin, Germany, March 1996.

[10] Richardson, George P. and Alexander L. Pugh III, Introduction to System Dynamics Modeling with DYNAMO, The M.I.T. Press, Cambridge, MA, 1981. [11] Richmond, Barry, "System Dynamics/Systems Thinking: Let's Just Get On With It", International System Dynamics Conference, Sterling, Scotland, 1994. [12] Rubin, Howard A., M. Johnson and Ed Yourdon, "With the SEI as My Copilot Using Software Process "Flight Simulation" to Predict the Impact of Improvements in Process Maturity", American Programmer, September 1994, pp. 50-57. [13] Smith, Bradley J, N. Nguyen and R. F. Vidale, "Death of a Software Manager: How to avoid Career Suicide

through Dynamic Software Process Modeling", American Programmer , May 1993, pp. 11-17. [14] Sycamore, Douglas M., Improving Software Project Management Through System Dynamics Modeling, Master of Science Thesis, Arizona State University, 1996. [15] Tvedt, John D., An Extensible Model for Evaluating the Impact of Process Improvements on Software Development Cycle Time, Ph.D. Dissertation, ASU, 1996. [16] Beagley, S.M., "Staying the Course with the Project Control Panel." American Programmer (March 1994) 29-34.

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996

A Design-based Model for the Reduction of Software Cycle Time
Ken W . Collier, Ph.D. Assistant Professor Computer Scienceand Engineering Northern Arizona University, Flagstaff, AZ Ken.Collier@nau.edu Abstract
This paper presents a design-based soj?ware cycle time reduction model that can be easily implemented without replacement of existing development paradigms or design methodologies, The research results suggest that there are many cycle-time factors that are influenced by design decisions. If manipulated carefully it appears that an organization can reduce cycle-time and improve quality simultaneously. The preliminary results look promising and it is expected that further experimentation will support the use of this model. This paper will analyze the basis for the model proposed here, describe the model' s details, and summarize the preliminary results of the model.

JamesS. Collofello, Ph.D. Professor Computer Science Arizona StateUniversity, Tempe, AZ James.Collofello@Asu.Edu
advantage of emerging technologies [ 11. Although much has been written about cost estimation, effort estimation and productivity and process improvement, little research has been directly aimed at reducing software cycle time. The proposed model is based on a systematic and rigorous analysis of the software process and identification of product factors that affect cycle-time, as well as the impact of design decisions on those factors. This design-based model attempts to reduce software development time by iteratively and strategically restructuring the design of a software system. In order to be practical, such a model must provide significant benefits at a minimaI cost. It must be able to be implemented simply without requiring an excess of special training. It must be applicable to as many different types of software systemsas possible, and it must provide measurablebenefits. It should also work in concert with, instead of as a replacement for, existing models and methodologies. This paper is divided into three major sections. The first section provides motivation for the design-based model presented in the paper. It does so by demonstrating the impact of software design decisions on cycle-time, software quality, and scheduling options. The second section details the model and provides theoretical justification for an algorithmic approach to schedule refinement. The model also provides some guidance for restructuring the design to further shorten cycle time. Finally, the results of an evaluation of the model are provided. Although these results are inconclusive, they suggest that the model has promise and meets the requirementspreviously established.

Introduction
Many organizations have relatively mature, effective software-development processes in place and have employed talented software engineers and managers to implement these processes. In such organizations, it is appropriate to question whether each softwaredevelopment effort is performed at peak efficiency and effectiveness. One of the primary goals of any organization is to release high quality software in as little time as possible. Therefore, an increase in software-development efficiency may be manifest as a reduction in development time. However, simply reducing development time is not good enough. It is important that these reduction efforts maintain high levels of both process and product quality. This research involves the examination and control of factors that have an impact on software development time. More specifically, this paper presents a model for reducing the development cycle time by restructuring software design. Reduction of development time, together with increasing quality and productivity, is the goal of many software development organizations. The benefits are numerous including: extended market life; increased market share; higher profit margins; and the ability to take 733 1060-3425/96$5.00 0 1996 IEEE

Software Cycle Time Factors
Design DecisionsAffect Cycle Time and Software Quality
Early in this research project, a systematic analysis of cycle-time factors and issues was conducted. The details of this analysis are outlined in [2]. However, an overview of

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996

the results of that study will serve to motivate the designbasedmodel proposed in this paper. First, the software development cycle was examined to establish a comprehensive set of factors impacting cycletie. These factors were divided into process factors and product factors. Process factors include: software reuse, requirements change, risk management, productivity, personnel availability, software tools, equipment resources, method maturity, verification and validation techniques, quality assurance techniques, integration strategies, and integration schedule. Product factors include: subsystem dependencies, module reuse, subsystem complexity, subsystem independence, subsystem risk, subsystem size, and subsystemgeneralization. Notably, most of these factors are impacted by design decisions. This recognition led to a set of cycle-time improving design goals including: maximize reuse; design independent subsystems:design simple subsystems;ensure completeness; localize risk: design to minimize risk occurrence; ensure correct design: design for low coupling; design for high cohesion; design to maximize independence; design unique subsystems; and design for flexible scheduling. A fortunate, and unexpected side-effect of this analysis is that these are the same design goals that improve design quality. This analysis showed that design decisions impact both design quality and cycle-time, and that these goals are not mutually exclusive. Moreover, the analysis revealed that improving cycle-time through design decisions is likely to improve quality and vice versa

scheduling decisions that increase the chance that a good schedule will be found. The software design and scheduling process can be viewed as a process of making decisions to maximize the chance of finding the best accessibleschedule,given all scheduling constraints [2]. Currently, this attempt at finding a best accessible schedule is a function of intelligent decision-making, intuition, and the application of design and scheduling heuristics. An approach that is highly subject to the abilities and expertise of the software engineers and project managers. An ideal scheduling methodology should be a prescriptive process that encourages the result of a good scheduleregardlessof expertise and ability.

An Overviewof' Project Scheduling
In the late 1950s two computer-based scheduling systems were developed to aid in the scheduling of large engineering projects. Both are based on task dependency networks. The critical-path method (CPM) was developed by Du Pont and Remington Rand. The CPM method is a deterministic scheduling strategy that is based on the best estimate of task-completion time. The program evaluation and review technique (PERT) is a similar method and was developed for the US Navy. The PERT method used probabilistic time estimates [3]. In the CPM approach, a task-dependencynetwork is a directed acyclic graph (DAG), which is developed from two basic elements: activities (tasks) and events (completion of tasks). An activity cannOt be started until its tail event has been reached due to the completion of previous activities. An event has not been reached until all activities leading to it have been completed. Associated with each activity is an estimated time to completion. The critical path is the path through the DAG that represents the longest time to project completion. To help calculate critical path and identify critical activities a set of parameters is established for each activity including: duration, earliest start time, latest start time, earliest ftish time, latest finish time, and float. Float refers to the slack time between earliest start and latest start. All activities on the critical path have a total float of 0. This reflects the idea that critical-path activities must be completed on time in order to keep the project on schedule [3]. PERT scheduling uses many of the same ideas as CPM. However, instead of task completion being a "most likely" estimate, it is a probabilistic estimate. The estimator predicts an optimistic estimate, o, and a pessimistic estimate, p, of task completion. The most likely time, m, falls somewhere between those values. The time estimates are assumed to follow a beta distribution. The expected time is given as te = (o + 4m + p)/6. The expected times are calculated for each activity in the task-dependency network, and the critical path is then determined as in CPM [31.
732

Design DecisionsImpact Development Schedule
Scheduling decisions play a critical role in reducing cycle-time. Therefore, it is appropriate to analyze the factors that constrain scheduling options. The architectural design of a system dictates subsystem dependencies, thereby dictating the possible scheduling strategies. Prior to software design, implementation scheduling possibilities are relatively unbounded by product decisions, i.e., all schedulesare accessible. Selecting a design alternative can be viewed as a reduction in the set of accessible schedules [2]. There are many decisions that further constrain the set of accessible schedules.Staffmg, resource allocation, risk analysis, and identifying high-priority functionality, all constrain the set of accessible schedules. Therefore, making design decisions that home in on the optimal schedule is a cycletime reduction goal. It is unrealistic to suggest that one can always arrive at the best possible design or schedule. Moreover, there is no way of knowing when the best design or schedulehas been developed. A more practical goal is to make design and

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International

Conference on System Sciences - 1996

Shortening the Critical Path Software development time is measured by the critical path through the project' s task dependency network. Hence, all cycle-time reduction efforts can be viewed as efforts to shorten the critical path. There are basically two ways to shorten the critical path: Shorten the completion time of tasks on the critical path, or, remove activities from the critical path The first option can be achieved either by simplifying the task or by increasing the productivity of the work team assigned to the task. Task simplification can be accomplished by either dividing the task into simpler concurrent tasks or by eliminating unnecessarywork from the task. Productivity can be increased by improving: management techniques, resources, or development techniques; or by allocating additional resources [4,5,6]. Although it is not a primary topic of this paper, the maximization of productivity is fundamental to cycle-time reduction. Violating Task Dependencies The second approach to eliminating critical-path activities implies the violation of task dependencies.This goal requires an understanding of the types of relationships between task dependencies.Tasks can be divided into two general subcategories: programming tasks and nonprogramming tasks (e.g., training, technical reviews, etc.). Programming tasks correspond to design components in a software system, and may be low-level modules, or the integrations of multiple modules into subsystems. Therefore, dependenciesbetween programming tasks are connected to dependencies between the components in a design. There are three types of dependencies between tasks: Data Dependency - If module A imports information that module B exports, then module A is data dependentupon module B . Functional Dependency - If module A requires functionality provided by module B to complete its own function, then A is functionally dependentupon B. Resource Dependency - If the completion of module A requires resources that are currently allocated to module B , then A is resource dependentupon B . There is some cost involved in dependency violation. Otherwise, the ideal scheduling approach would be to complete all tasks in parallel and then integrate all at one time. Of course, as Fred Brooks observed, project scheduling and management is not this simple [7]. For programming tasks, the dependency violation cost occurs in the form of scaffolding (i.e., test drivers and code stubs) to simulate me parts of one module upon which another depends. As data and functional dependenciesare violated, the amount of required scaffold development

increases to simulate the dependency, thereby increasing the task completion time. This in turn increases the likelihood of defects being introduced into the code. As defects increase, the debugging time increases. When resource dependencies are violated, the cost dependsupon the type of resource. If two tasks require the use of some limited-access hardware device, then dependency violation might require the purchase of a secondsuch device. The cost is monetary. If two tasks both require some very specialized expertise that few team members possess, then dependency violation means training other programmers. The cost in this case is in terms of tune spent learning and retraining. Many factors contribute to the completion time of programming tasks. Each programming task represents a cycle of subactivities that includes detailed design, coding, unit testing, integration testing, and system testing; all of which are time-consuming. Additionally, there are process-management and control activities involved in each task. One must decide whether the benefits of violating a dependency outweigh the costs. Furthermore, it is unreasonable to expect that all dependencies share the same violation cost. The stronger or more complex the dependency,the greater the cost of violation. Dependency strength may be caused by the degree of coupling between modules or simply by the amount of accessof the module' s componentsby other modules. Consider two tasks, A and B, in which B is dependent upon the completion of A. Dependency-violation cost can be viewed as the addition of some percentage of the completion time of A to B' s completion tune. This percentagereflects the amount of A that must be simulated in order to complete task B before A is actually complete. A violation cost approaching 100% reflects the idea that task A is being simulated in its entirety, which defeats the purpose of violating the dependency. Conversely, an estimated violation cost approaching 0% reflects the notion that B' s dependency on A is artificial and both could be performed in parallel with little consequence. A dependencyclassification scheme is proposed to help determine dependencies that are good candidates for violation. If a dependency violation cost is estimated to range between 0% and 25%, then the dependency is classified as a weak dependency.If the cost is in the range 26%-50%, then the dependencyis moderate. If the cost is in the range 5 l%-75%, then the dependency is strong; while 76%-100% violation cost would be considered very
strong.

Accurately estimating the cost of violating dependenciesis a topic for future research. However, for programming tasks that are functionally or data dependent upon other tasks, the cost is primarily in the creation of code scaffolding. In this case, the estimated cost can be
733

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996

derived from the software cost models used to estimate the initial task durations. Resource dependencies and nonprogramming tasks are not likely to be so simple. From these ideas on critical path shortening, a set of scheduling goals for the reduction of cycle-time form the basis for the model presentedin this paper: 1. Violate low cost dependencies to increase concurrent development whenever cost effective. 2. Transform high cost dependencies into low cost dependencies. 3. Reduce task-completion time by simplifying the task. 4. Reduce task-completion time by dividing it into concurrent subtasks. by increasing 5. Reduce task-completion time productivity.

simple.
l

Component complexity in design determines task stafing - Highly complex components may require

additional personnel resources thereby limiting the degree of concurrency in the schedule.
l

Development learning curves affect productivity and productivity affects task completion time - Design

The Relationship Between Scheduleand Design
The previous section was devoted to project scheduling and identifying the general strategies for shortening the critical path in a schedule. This section examines the impact of design decisions on scheduling outcomes. Moreover, identifying connections between design and scheduling provides a set of techniques for achieving the critical-path-shortening goals previously identified. In general a software design and its development schedule are closely connected due to the fact that design components dictate the work tasks in the development schedule. It is useful to identify other, more subtle, design-scheduleconnections.
l

Design dependencies determine schedule dependencies

- Any degree of coupling between two modules in the design translates into a dependency between the corresponding tasks in the schedule.
l

Design independence determines task concurrency

-

Modules in a design that are uncoupled can be developed in parallel, given no other constraints and assuming that they are not resource dependent.
* Component complexity in design determines task-completion time - Modules that are complex will

components that require programmers to learn special skills will take longer to implement than those components for which programmers are already trained. l So&are reuse afSects task-completion time - Code reuse is almost certainly faster than designing, developing, and testing code from scratch. In general, a design with complex components that are highly dependent upon one another will result in a highly serial schedulewith long task-completion times and excess staffmg needs. Conversely, a design with simple, independent components will result in a highly concurrent schedule with short task completion times and lower required staffing. The schedule-improvement goals previously listed are impacted by design decisions. Task dependencystrength is determined by intermadular dependencies (coupling). Task completion time is affected by maduEe complexity. Task concurrency is affected by intramadular dependencies (cohesion). Clearly, the coupling and cohesion heuristics play a role in determining the critical path length in a schedule. Additionally, information hiding, data abstraction, data localization, and fan-in/fan-out will affect the de pendencies in a development schedule. This is further evidence that existing quality metrics are coincident with the goal of cycle-time reduction. Furthermore, these existing heuristics can provide the necessarymechanisms for improving the development cycle time by a judicious refinement of the design.

A Cycle Time Reduction Model
In [21 a link is made between the factors that affect

take longer to implement and test than modules that are

Architectural Design

Figure 1 - Current Design Cycle

734

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International

Conference on System Sciences - 1996

development cycle time and software design decisions. This lii combined with the link between design and scheduling provide sufficient motivation for a design-based cycle-time reduction model. The model proposed is based on the convergence of the ideas presented in previous sections. This model aims to shorten cycle time by shortening the implementation phase through design and schedule refinement.

Model Description
Current state of the art in software design dictates an iterative process of design and refmement to converge on a design of the highest quality. Figure 1 reflects this notion. Under this model the development schedule is deferred until design is complete. There are some drawbacks to this design model. First, widely used design-based metrics do not provide a mechanism for determining when it is cost-effective to continue design refmement and when to stop refming and move to the next phase. Second, the current design model is not prescriptive. It is difficult to determine which parts of the design deserve refmement at any iteration of the cycle. Finally, although this iterative design process is the state of the art, it is not necessarily the state of the practice. It remains the tendency of many designers to adopt the first design that is generated. This may be becausethe benefits of iteratively improving design are difficult to quantify. The model proposed in this paper is also iterative in nature. However, development scheduling becomes an integral part of the design cycle. In this model the designers develop an initial design and then iteratively work to develop the best schedule (i.e., shortest critical path) that can be implemented using that design. Then the design is refined according to standard practices. Following refinement, a new "best schedule" is developed for the improved design and so on until the schedule does not continue to improve. This model is graphically representedin figure 2. The ultimate goal of this cycle-time reduction model is a design that improves development time without

sacrificing product quality. This model addressessome of the problems with current iterative design models. Critical path length is the driving metric for determining when it is cost-effective to continue design refmement or when to move on to development. Furthermore, because critical path is used to reflect the improvement of each iterative design refinement, the benefits of the iterative process are measurable and can be compared directly to the cost of refinement. For example, if it takes five programmer days to conduct a single design iteration and the critical path is shortened by only three programmer days, then it is clearly not cost-effective to continue refining the design. In this case, the method helps designers determine when to stop designing. The approach taken by this model is to
selectively apply design-improving techniques to key components in the design. The aim of this approach is to

minimize the effort and maximize the benefit.

How The DesignModel Works
This design cycle follows five basic phases: 1. Initial Design - During this phase the system is designed using existing methods and techniques. At this point in the process, the model does not differ from current design models. In fact, this cycle-time reduction design model may be thought of as a design metamodel, since it does not replace popular design techniques and paradigms but is symbiotic with existing methods. 2. Initial Schedule - The initial scheduling phase in this model employs current state-of-the-art scheduling and effort estimation techniques. 3. Schedule Refinement - The schedule-refmement phase of this design model is the point at which the schedule is iteratively analyzed and refined in an effort to shorten the critical path without altering the design. 4. Problem Identification - Now that the schedule has been refined sufficiently, this phase serves the purpose of identifying those system components that, if improved, represent a significant cycle-time improvement.

Architectural Design

Figure 2 - Cycle Time Design Model

735

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

~~~~~~~~ ~~

~_~~~ -~~

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - I996 5. Design Refinement - During this phase efforts are made

to improve those tasks that were identified as problematic by the previous phase. This cycle is iterated using the critical path length as a cycle-time improvement metric. For each iteration of the design cycle, the critical path will reflect the benefit of the refinement efforts. Ideally, the architectural design cycle should halt as soon as the critical path becomes stable or when the benefits at each iteration do not justify the effort We do not propose to replace existing state-of-the-art design and scheduling methods in this model. Instead, the remainder of this section will focus on the development of methods for accomplishing steps 3,4 and 5. Schedule Refinement The third phase in this model deserves closer inspection. Ideally, this phase will reveal the optimal schedule for the current version of the design. It has been observed that the task dependenciesalong the critical path in a schedule can, potentially, be violated. A brief foray into graph theory will motivate an algorithmic approach to the problem of schedule optimization. The scheduling problem can be temporarily simplified by stating it as a graph-manipulation problem restricted by a set of rules. Given a directed acyclic graph (DAG) in which each vertex v has a weight that is represented by wt(v), the weight of the entire graph W is:
/ Vi

W = X wt(vi) ;3cVvi {c is a critical path, vi is a vertex is on C} The burden of a vertex v, b(v) is the sum of the

weights of all immediate predecessorsof v, which are on a critical path. The prehistory of v, ph(v) is the sum of the weights of all predecessorsof v, which are on the longest serial path from the initial vertices to v. These are likely to be the predecessorsof v that are on a critical path. The object is to reposition vertices in the graph in order to achieve the smallest weight (i.e., shortest critical path). However, there are rules for moving vertices: 1. Never disconnect the goal vertex from its predecessors. This is equivalent to eliminating subsystemsfrom the f& integration. 2. Vertices are repositioned by disconnecting them from their critical-path predecessors.This reflects the notion of violating task dependencies. 3. Whenever a vertex v is disconnected from its critical path predecessors,these predecessorsare reconnected to all of v' s immediate successors. This is to prevent the graph from becoming disconnected. 4. Whenever a vertex v is disconnected from its critical-path predecessors,its weight is adjusted by the expression: wt(v) = wt(v) + (m x b(v)). In this expression m representsthe percentageof v' s dependent tasks that must be simulated in order to develop and

test v (i.e., scaffolding). This reflects the cost of violating task dependencies. Some observations can be made from this view of schedule optimization. Fist, it is not likely to be beneficial to reposition slack path vertices. Second, a path that is critical at one point in the optimization process may cease to be critical at some future time. Third, there may be multiple critical paths in the graph at any given time. These conditions and others that are not so obvious must be addressedby any strategy that is to be used to solve this problem. An algorithmic strategy might implement a cycle of vertex repositioning followed by recalculating critical-path length. Such an algorithm must be able to determine which vertex to reposition at each iteration and when to stop. The selection of a vertex for repositioning must be conducted in light of a cost-benefit analysis. The benefit is measured as a reduction in W. The cost is represented by the increase in wt(v) due to dependency violation. The vertex that produces the greatest benefit and least cost is the prime candidate. Some additional observations can be made about the structure of critical paths in this problem space. It is possible that the critical path in a DAG of significant size will not be a single linear sequenceof vertices and edges. Multiple parallel critical paths may require repositioning multiple vertices before seeing a reduction in W. Furthermore, if the critical path diverges at one task and then reconverges later (i.e., becomes temporarily parallel), vertices prior to this divergence or following the reconvergenceshould be manipulated if possible in order to realize immediate benefits. The trouble with each of these scenarios is that they obscure the benefits of repositioning certain candidate vertices. The value of W may remain steady over several iterations before it begins to decrease.Rather than halting when no further decreasein critical path length is seen, it may be more beneficial to halt when an increase is detected. These observationsmotivate a simple greedy algorithm that iteratively repositions critical path vertices based on a cost-benefit analysis and recalculates the value of W, halting when the value of W increases. This algorithm always identifies the vertex that appears to promise the greatest immediate reduction in W. Greedy algorithms are often used in optimizing problems such as this. However, most greedy algorithms accept a "good" solution rather than guaranteeing an optimal solution. This algorithm is no exception. The problem of finding an optimal solution lies in the probability of reaching a local minimum It is possible that the repositioning of a vertex will temporarily increase W in order to realize a greater decreaseon future iterations. Unfortunately, guaranteeing an optimal solution to graph-shortening for a graph of any complexity is a hard

736

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International

Conference on System Sciences -

1996

problem which cannot be solved in polynomial time and is NP-complete [8,9, lo]. Calculating Dependency Violation Cost In determining the net benefit of dependency violation,
m represents the percentage of simulation of the tasks on

conditions and its use may result in a loss of accuracy. The remainder of the discussion of this model will use .25 as a value for m. However, until an empirically established upper bound value is established one may prefer to use actual violation cost estimates. An Algorithm for Schedule Refinement The graph theory concepts of the previous section form the basis for a schedule refmement algorithm that attempts to reposition critical path tasks. Both CPM and PERT represent a project schedule as a DAG in which vertices represent development activities and milestones, while edges represent dependenciesbetween tasks. The duration of each task corresponds to the weight of a vertex, wt(v), and the critical path duration corresponds to W. The prehistory of a task t, ph(t), is the sum of the durations of all tasks preceding t along the critical path. The burden of a task t, b(t), is the sum of the durations of all tasks that immediately precede t. The process of refining the schedule is, in essence, a process of selectively violating task dependencies in a cost-effective manner. It is assumed that the schedule follows a typical CPM or PERT format, in which the information for each task includes: earliest start date (ES), latest start date (LS), earliest finish date (El?), latest ftish date (LF), and duration (D). Furthermore, it is assumed that the dependencies between tasks in a schedule have been categorized using a scheme similar to the one previously described. The following strategy takes the conservative approach of violating only weak dependencies and assumes the worst case within that range (i.e., m = .25). Given more information, the procedure can be modified as is necessary. The algorithm is as follows: 1. Build a set D of weak dependencieson the critical path. These are the primary candidates for violation. 2. For each dependency i-+j E D, where task i is dependent on task j, calculate ph(i) and b(i). Remove from D any dependencies whose cost outweighs i' s prehistory, wt(i) + ph(i) 5 wt(i) + .25b(i). 3. For each dependency i+j E D, calculate the net benefit, n(i) = wt(i) - .25b(i), of removing the dependent task from the critical path and placing it in a slack path. Remove any dependenciesfrom D that have no net benefit, n(i) < 0. D now contains the fmal set of candidates for elimination. 4. Select the dependency i+j E D, for which n(i) is the greatest, remove the dependent task from the critical path, and replace it on a slack path using the following rules: l If i has no successorsand j becomes disconnected from all successors, then do not violate this dependency since j will never become integrated

which another task relies. This requires that up to 100% of the fast task must be simulated in order to complete the second task. Therefore, the cost of violating a dependency lies between 0% and 100% of the duration of the first task. Hence, for a single dependency the value of m is in the range [O.O,l.O]. The value of m increases as the number of tasks on which the target task is dependent increases and may exceed 1.O. Furthermore, whenever a task dependency is eliminated there are likely to be hidden costs due to added communication requirements, inaccuracies in estimating, etc. As m approaches 1 .O the likelihood increases that these hidden costs will cause the dependency violation cost to exceed the benefit. Therefore, it is useful to establish a maximum value for m above which a dependency should not be violated. Empirically defining an upper bound value for m is extremely difficult if not impossible. However, during the development of the model presented here, fifty different schedules and scheduling scenarios were examined. The purpose of these examinations was to identify the behaviors and consequences of task dependency violation. These efforts led to the observation that .25 appears to be a conservatively reasonableupper boundary value for m [ 11. It was consistently observed that whenever the estimated cost of violating a dependency was below 50%, the violation resulted in a decreasein critical path length. In fact, many critical path decreaseswere observed for cost estimates as high as 80%. However, providing for hidden costs, it is deemed prudent to take a highly conservative approach to selecting an upper bound value for m. It was observed during these exercises that 100% of the cases in which the value for m was below 30% resulted in a shortening of the critical path. Compensating for hidden costs, this model uses an upper bound of 25% for m. This choice has the added benefit of allowing the cycle-time reduction model to focus only on weak dependencies as candidates for violation. Assuming inaccuracies in the estimates of dependency violation cost, .25 represents a worst-case value within that range. Fixing m at .25 simplifies the algorithm since categorizing dependency strength is easier than accurately estimating the cost of dependencyviolation. This use of .25 as a fixed value for m in the cycle-time reduction model can easily be replaced by a more empirical value or by the actual violation cost estimate. The selection of a fixed value serves primarily to simplify the use of the model. This value was selected under ultraconservative
737

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International

Conference on System Sciences - 1996

into the system without an additional integration step. In this case, i is the final system integration. This is not likely to be cost-effective. l Connect j to all immediate successorsof i and update i' s ES, LS, EF, and LF values. This reflects the change in the integration strategy for i andi (i can now be developed concurrently with /I. * Update i' s duration by a factor of .25, D = .2.50 l All additional constraints that affect the development schedule should be maintained (e.g., resource allocation, risk prioritization, etc.). It is impossible to define rules for maintaining these constraints. Therefore, it is the responsibility of the scheduler to ensure that they are not violated. 5. Becausea new critical path may emerge as a result of task repositioning, steps l-4 are repeated until the schedule reaches a stable state (i.e., no changes can be made to the schedule) or until the critical path begins to increase. At best, the result of this algorithm is an optimal schedule for the given design under the given constraints. At the least, the resulting schedule will not be any worse than the original schedule. There are additional scheduling-improvement techniques that have not been addressed here, such as resource-leveling, which may help to shorten the critical path [3]. Used in conjunction with such techniques, the model describedhere offers a simple yet powerful means of reducing development time. It is based on observation, intuition, and the mathematical manipulation of task networks and CPM components. A major benefit to this scheduling method is that it is highly automatable once tasks have been identified and a preliminary schedule is developed. Problem Identification Once the development schedule has been refined there are two ways to further shorten the critical path: either remove tasks from the critical path, or shorten the completion time of tasks on the critical path. The only remaining critical tasks are those with high dependency-violation costs or weak dependencies for which there is no net benefit in removing them from the critical path. This phase in the model attempts to shorten the duration of a target task; or to prescribe design refinements that will result in weaker dependencies. The approach taken is to identify the design component that, if refined will result in the greatestcycle-time benefit. This approach is heuristic rather than algorithmic. General guidelines are as follows: 1. Identify tasks in the schedule that have the greatest impact on the critical path.

2. Identify dependenciesthat, if violated, would produce the greatestreduction in the critical-path length. 3. For the identified tasks, examine the potential for either simplifying or decomposing their corresponding design components and estimate the effort required to do so. Retain those that have the greatest benefit for the least effort4. For the identified dependencies, evaluate the effort required to weaken the dependency and evaluate the potential for success.Retain those that have the greatest benefit for the least effort. 5. From the retained tasks and dependencies, select the one that has the greatest overall net benefit in terms of critical-path shortening. 6. Ideally, one task or dependencywill he refined on each design-cycle iteration. However, it may be that by performing simple refinements to several tasks and/or dependencies,a major shortening of the critical path will be experienced on one iteration. These decisions must be determined based on individual project scenarios. Problem identification relies upon the talents of software engineers to make good decisions. These guidelines may easily be supplemented with additional, project-specific information to increase their benefit. Future work in validating cost and benefit estimating techniques wilI further strengthen this phase in the model. Design Refinement After identifying the most beneficial task or dependency for refinement, it is necessary to quickly identify the cause of the problem and resolve it. If the target of refinement is a dependency, then the aim is to weaken that dependency to make its dependent task a candidate for repositioning. Assuming the dependency is not resource dependent, it may be that efforts should be made to decreasecoupling between corresponding design components using existing design heuristics and principles. If the target of the refmement effort is task duration, reducing the complexity of corresponding components might be accomplished through simplification or subdivision. Subdivision implies that the module has poor cohesion. Ideally, system modules should be functionally cohesive. Therefore, the designer should work to reduce cohesion and divide the system component into multiple, independentsubcomponents.

Preliminary Results
This model cannot guarantee a reduction in actual cycle-time since many things can happen between software design and product release. However, preliminary results suggest that the model has promise in achieving the
738

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International

Conference on System Sciences - 1996

1 Actual Size Actual Effort DPS ......................... .......................................................................... ..................................................................... ............................................................ j 2 Actual Size Actual Effort Educated : ......................... .......................................................................... .................................................................... ............................................................. ; 3 Actual Size COCOMO DPS 4 Actual Size COCOMO Educated I I I 5 Actual Size Putnam DPS ......................... .......................................................................... ............................................................................................................................ i 6 Actual Size Putnam Educated ......................... .......................................................................... ............................................................................................................................ ; 7 Expected Size COCOMO DPS 8 Expected Size COCOMO Educated j I f 9 Expected Size ......................... ............................ ............................................ ...................................................................................... ; 10 Expected Size I......................... I........................... ............................................. I....................................................................
Table 1: Demonstration Strategies

following results: 1. Significantly reduce the estimated development time of a software product at the design phase. For the purposes of this effort, any reduction of estimated development time of 5% or more will be considered significant. 2. Maintain or improve the quality of the initial design (i.e., the model will not reduce design quality). 3. Help to focus design-refinement efforts on beneficial design components. Although empirical validation of this model is virtually impossible, demonstration of the model on a variety of software designs under a variety of conditions yields encouraging results. The model was applied to live software systems ranging in size from 736 source lines of code (SLOC) to 6,309 SLOC, and ranging in quality. Three factors were identified as affecting the initial schedule of a particular software design: size estimation technique; effort estimation technique; and scheduling technique. For size estimation in this analysis actual size and expected size were used. For effort estimation, actual efforts were used in addition to the COCOMO and Putnam models [ll, 121. In developing initial schedules, two approaches were used: dependency preserving scheduling (DPS) and educated scheduling. DPS refers to preserving all intermodular dependencies from the design on the corresponding development tasks. Educated scheduling refers to more common scheduling approaches in which task dependencies are determined by functionality, risk, resources,etc. Table 1 shows the variety of combinations of size estimation, effort estimation, and scheduling

approach. By combining each of these factors with each of the five software designs, forty-two different design-scheduling scenarios were used to observe the effects of the model (in some cases,certain combinations were infeasible). Table 2 shows the statistical results of these 42 demonstrations. The data is fairly scattered. However, in all but one of the demonstration cases, a significant (i.e., 5% or greater) reduction in estimated development time was observed. It is notable that the design refinements in each of the demonstration scenarios contributed as much or more to improving quality as to improving estimated cycle-time. The quality of each of the poor quality designs improved, while the high quality designs did not experience any loss in quality. The details of this demonstration of effectiveness can be found in [ 11. It should be noted that the demonstration scenarios are all small programs, and it remains to be seen how well this model scales to largescale software systems.

Summary and Conclusions
Current design cycles do not provide mechanisms for determining on which parts of the system to focus refmement efforts, or for determining when to stop iterating. This model provides a solution to both problems. Through a schedule analysis, the model guides the designer to focus refinement efforts on the most beneficial system components. By using the critical path as a metric of cycle-time improvement, this model helps determine when to halt the design cycle. Additionally, this model

Table 2: Results of 42 Model Demonstrations

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings

of the 29th Annual Hawaii

International

Conference on System Sciences - 1996

provides a mechanism for the designer to determine which techniques to apply in order to resolve targeted problems. In this manner, the cycle-time reduction model helps the designer selectively apply design-improving techniques to key components in the system in order to cost-effectively improve the development cycle time. Applying Laws of Pareto to cycle time, 20% of the software design will account for 80% of the cycle time. The goal of this model is to help software engineers focus their energy on the key 20% for improvement by identifying system components that most significantly impact the critical path length. There are a number of other benefits to this cycle time reduction model: l It is not a replacement methodology. This design approach allows software engineers to continue using state-of-the-art design and scheduling methods.
e This method provides design-improvement feedback.

Bibliography
111 Collier, K.W. "A Design-based Model for the Reduction of
Software Cycle Time", University, 1993. Ph.D. Dissertation, Arizona State

121 Collier, K.W. and J.S. Collofello, "Issues in Software Cycle
Time Reduction", Proceedings: International Phoenix Conference on Computers and Communications, March 2831,1995, pp. 302-309.

[31 Dieter, G.E. Engineering Design: A Materials and Process
Annroach, McGraw-Hill New York 1983. [41 Bisant, D.B. and J.R. Lyle, "A Two-Person Inspection Method to Improve Programming Productivity", IEEE Transactions on Sofhyare Engineering vol. 15 no. 10, 1989, pp. 1294-1304. Boehm, B., M.H. Penedo, D.E. Stuckle, R.D. Williams and A.B Pyster, "A Software Development Environment for Improving Productivity", Computer, vol. 17 no. 6, 1984, pp. 3042. Dart, S.A., R.J. Ellison and P.H. Feiler, "Software Development Environments", Computer, vol. 20 no. 11, 1987, pp. 18-28. I71 Brooks, F.P. The Mvthical Man-Month: Essavs on Software Enaineering, Addison-Wesley, Massachusetts, 1982.

l

Using the critical path as a metric, the impact of design refinements is clear. This method combines established practices. Popular scheduling methods, and design heuristics, cost-estimation techniques are supported in this model.
This method provides motivation for iterative design refinement. Currently, it is unclear how much benefit is

0

gained by placing energy into design iteration. This method provides this information in the form of units of time saved.
l

@I Boctor, F.F. "Some Efficient Multi-heuristic

The method provides a metric for measuring cycle-time reduction. Critical path length offers a quantitative

Procedures for Resource Constrained Project Scheduling", European Journal of Operational Research, vol. 49, 1990, pp. 3-13. Project Scheduling With a Limited Resource", International Journal of Production Research, vol. 29 no. 1, 1991, pp. 185-198.

measure of the effects of the model on estimated cycle-time reduction.
l

PI Khattab, M.M. and F. Choobineh, "A New Approach for

The method allows for fixed

resource

allocations.

Resource constraints that limit scheduling flexibility are accommodated by this model. Furthermore, if resources are not fmed, this model provides useful information for determining resource requirements.
l

[lo] Khattab, M.M. and F. Choobineh, "A New Heuristic for Project Scheduling With a Single Resource Constraint", Computers and Industrial Engineering, vol. 20 no. 3, 1991, pp. 381-387. [ll] Boehm, B. "Software Engineering Economics", IEEE Transactions on Sofiare Engineering vol. SE-10 no. 1, 1984, pp. 4-21.

Unrealistic schedules are identified early by this model. As the designs are refined for shorter cycle

times, the feasibility of initial scheduling estimates becomes apparent allowing for contingency plming and deadline renegotiation early in the product life cycle. Due to the small size of the software designs that were used to evaluate the model, the usefulness of the model on medium or large software designs is unclear. Further empirical evidence is required to make any conclusive statements about the model' s effectiveness. However, initial results are encouraging. This design-based cycletime reduction model can only improve as cost/effort estimation models become more accurate. Furthermore, this model is easily adaptable to organizational idiosyncrasies and to changing technologies,

[12] Putnam, L.H. and W. Myers, Measures for Excellence: Reliable Software on Time, Within Budget, Yourdon Press, New Jersey, 1992. [13] Yourdon, E. Mana&c!
Hall, New Jersey, 1989.

the Structured Techniques, Prentice-

740

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Evaluating the Effectiveness of Process Improvements on Software Development Cycle Time via System Dynamics Modeling
John D. Tvedt and James S. Collofello Arizona State University Department of Computer Science and Engineering Tempe, AZ 85287-5406 USA E-mail: {tvedt, collofello}@asu.edu
Abstract
Reducing software development cycle time without sacrificing quality is crucial to the continued success of most software development organizations. Software companies are investing time and money in reengineering processes incorporating improvements aimed at reducing their cycle time. Unfortunately, the impact of process improvements on the cycle time of complex software processes is not well understood. The objective of our research has been to provide decision makers with a model that will enable the prediction of the impact a set of process improvements will have on their software development cycle time. This paper describes our initial results of developing such a model and applying it to assess the impact of software inspections. The model enables decision makers to gain insight and perform controlled experiments to answer "What if?" type questions, such as, "What kind of cycle time reduction can I expect to see if I implement inspections?" or "How much time should I spend on inspections?" · Being the first to market along with the high cost of switching products can give a product a great advantage in capturing and obtaining a large market share. · Higher profit margins. Being first to market gives a company greater freedom in setting profit margins. · The ability to start later and finish on schedule. Starting later allows the use of technological advances that may not have been available to the competition. "According to a study released in January 1990 by United Research Co. of Morristown, N.J., six out of 10 CEOs listed shorter product development cycles as vital to their company..." [9]. Most of these cycle time reduction reengineering efforts attempt to take advantage of process improvement technology such as that advocated by the Software Engineering Institute's Capability Maturity Model and ISO 9000. The implication is that a mature development process increases the quality of the work done, while reducing cost and development time [8]. Some software companies that have begun to mature their process have, indeed, reported cycle time reductions via process improvements [5].

1: Background
1.1: Importance of reducing cycle time
The rush to reengineer processes taking place in many organizations is the result of increasing competitive pressures to shorten development cycle time and increase quality. Reducing the time to produce software benefits not only the customer, but also the software organization. A number of benefits of reduced cycle time are listed by Collier [4]. · Market life of the product is extended.

1.2: The need for selecting among alternative process improvements
A common problem faced by organizations attempting to shorten their cycle time is selecting among the numerous process improvement technologies to incorporate into their newly reengineered development process. Even though there is promising evidence

trickling into the pages of computer publications, most companies are still skeptical of the investment that must be made before possible improvements will be experienced. They recognize that process improvements do not exist in isolation. The impact an improvement has may be negated by other factors at work in the particular development organization. For example, consider a tool that allows some task to be completed in a shorter period of time. The newly available time may just be absorbed as slack time by the worker, resulting in no change to cycle time. Thus, software organizations need to know, in advance of committing to the process improvement technology, the kind of impact they can expect to see. Without such information, they will have a hard time convincing themselves to take action. In particular, a software organization needs to be able to answer the following types of questions: · What types of process improvements will have the greatest impact on cycle time?; · How much effort should be allocated to the improvement?; · What will the overall impact be on the dynamics of software development?; · How much will the process improvement cost?; · How will the process improvement affect cycle time?; and · How will this improvement be offset by reduced schedule pressure, reduced hiring and increased firing? Today, the limited mental models often used to answer these questions are insufficient. The mental models often fail to capture the complex interaction inherent in the system. "It is the rare [manager] whose intuition and experience, when he is presented with the structure and parameters of [only] a second order linear feedback system, will permit him to estimate its dynamic characteristics correctly," [2].

which all factors except the independent variable (the process improvement) are kept constant. Although much can be learned through this approach about the general effectiveness of the process improvement, it is normally not practical to experiment over the entire development life cycle of a significant size project. Thus, it is often impossible to assess the overall impact of the proposed process improvement on development cycle time. Another approach typically followed is the utilization of some sort of "pilot project" to assess the new technology. Although considerably weaker than controlled experimentation, the pilot studies often reveal the general merit of the proposed technology. It remains difficult to assess the unique impact of the proposed improvement technology on cycle time due to the interaction of other project variables and to generalize the results to other projects. A third approach to assessing the impact of process improvements is the utilization of traditional cost estimation models such as COCOMO [3] and SLIM [10]. These types of models contain dimensionless parameters used to indicate the productivity and technology level of the organization. The parameter values are determined by calibration of the model to previous projects or by answering a series of questions on a questionnaire. For example, the Basic COCOMO model calculates the number of man-months needed to complete a project as: MM = C (KDSI)K, where MM = number of man-months, C = a constant, KDSI = thousands of delivered source instructions and K = a constant. The user enters the size of the code along with the two predefined constants to determine the number of manmonths. C and K are determined by calibrating the model to previous projects. An important question for a software organization to answer is how C and K should be modified to reflect the impact of process improvements when no historical data exists. One potential answer is to use the Intermediate or the Advanced COCOMO models. These models contain cost drivers that allow more information about the software project to be input than the Basic model. Cost drivers are basically multipliers to the estimated effort produced by the model. Modern Programming Practices is one such cost driver. If a software project is using modern programming practices, the cost driver would be set such that its multiplier effect would reduce the amount of effort estimated. Two questions arise when using this cost driver: what is the definition of modern

1.3: Current approaches for evaluating the impact of process improvements
There does not exist a standard method for determining the impact of specific process improvements on cycle time. An ideal way for performing this assessment is conducting a controlled experiment in

programming practices and how should the cost driver's value be altered with the addition of a specific improvement to the existing programming practices. These questions have no simple answers because the cost drivers are at a level of granularity too coarse to reflect the impact of specific process improvements. Some researchers also believe that models such as COCOMO are flawed due to their static nature. Abdel-Hamid states, "The problem, however, is that most such tools are static models, and are therefore not suited for supporting the dynamic and iterative planning and control of [resource allocation]" [2].

perception that the product is back on schedule (effect/close feedback loop). The developers perceived the product to be behind schedule, took action, and finally perceived the product to be back on schedule, as a result of their actions. Secondary effects due to the developers' actions, however, such as the lower quality of the product, will also eventually impact the perception the developers have as to being on schedule and will necessitate further actions being performed. These cause-effect relationships are explicitly modeled using system dynamics techniques. Because system dynamics models incorporate the ways in which people, product and process react to various situations, the models must be tuned to the environment that they are modeling. The above scenario of developers reacting to a product that is behind schedule would not be handled the same way in all organizations. In fact, different organizations will have different levels of productivity due to the experience level of the people working for them and the difficulty of the product being developed. Therefore, it is unrealistic for one model to accurately reflect all software development organizations, or even all projects within a single development organization. Once a system dynamics model has been created and tailored to the specific development environment, it can be used to find ways to better manage the process to eliminate bottlenecks and reduce cycle time. Currently, the state-of-the-practice of software development is immature. Even immature software development environments, however, can benefit from this technique. The development of the model forces organizations to define their process and aids in identifying metrics to be collected. Furthermore, the metrics and the models that use them do not have to be exact in order to be useful in decision-making [13]. The model and its use result in a better understanding of the cause-effect relationships that underlie the development of software. The power of modeling software development using system dynamics techniques is its ability to take into account a number of factors that affect cycle time to determine the global impact of their interactions, which would be quite difficult to ascertain without a simulation. The model may be converted to a management flight simulator to allow decision-makers the ability to perform controlled experiments of their development environment.

2: Proposed approach for evaluating the effectiveness of process improvements
2.1: Overview of system dynamics modeling
Due to the weaknesses of the approaches presented in the previous section, we chose to evaluate the impact of process improvements on software development cycle time through system dynamics modeling. System dynamics modeling was developed in the late 1950's at M.I.T. It has recently been used to model "high-level" process improvements corresponding to SEI levels of maturity [12]. System dynamics models differ from traditional cost estimation models, such as COCOMO, in that they are not based upon statistical correlations, but rather cause-effect relationships that are observable in a real system. An example of a cause-effect relationship would be a project behind schedule (cause) leading to hiring more people (effect). These cause-effect relationships are constantly interacting while the model is being executed, thus the dynamic interactions of the system are being modeled, hence its name. A system dynamics model can contain relationships between people, product and process in a software development organization. The most powerful feature of system dynamics modeling is realized when multiple cause-effect relationships are connected forming a circular relationship, known as a feedback loop. The concept of a feedback loop reveals that any actor in a system will eventually be affected by its own action. A simple example of the ideas of cause-effect relationships and feedback loops affecting people, product and process can be illustrated by the following scenario: Consider the situation in which developers, perceiving their product is behind schedule (cause), modify their development process by performing fewer quality assurance activities (effect/cause), leading to a product of lower quality (effect/cause), but giving the temporary

2.2: Modular system dynamics modeling
Due to the large number of process improvements available for evaluation and the necessity of conducting each evaluation in the context of a system dynamics

model customized to the organization for which the evaluation is being performed, a modular system dynamics modeling approach was adopted. A modular system dynamics model consists of two parts, the base software process model and the process improvement model. Once a base model of the software development process exists, any number of process improvement models may be plugged in to determine the effect they will have on software development cycle time. This modularity gives the process improvement models the advantage of being used in more than one base model of a software development environment. For example, the process improvement model could be integrated with a base model of development environment "A" and also a base model of development environment "B". The only change to the process improvement model would be the values of its parameters in order to calibrate it to the specific development environment. The models of the development environments may have to be modified structurally, such that they include all necessary model elements, e.g., system dynamics modeling rates and levels, for integration. A conceptual image of the modular model is shown in Figure 1. On the left is a box representing an existing system dynamics model of a software development process. This model would be tailored to a particular organization. On the right is a box representing the model of the process improvement.

modified to take advantage of the proposed model, such that it contains the necessary elements for the interface.

2.3: Approach for constructing modular system dynamics models
An organization wishing to utilize system dynamics modeling for assessing the impact of new technologies on cycle time must perform the following steps: 1. Construct a base model of the software development process. Construction of a base model requires detailed modeling of the organization's software development process as well as identification of cause-effect relationships and feedback loops. Sources of information for model construction include: observation of the actual software process, interviews of personnel, analysis of metrics and literature reviews of relevant publications. An organization wishing to adapt an existing generic model for rough assessments might consider the model developed by Abdel-Hamid [1]. 2. Construct the process improvement models. For each process improvement being considered, a modular process improvement model must be developed utilizing the same approach as that of the base model. 3. Validate the base and process improvement models. Both the base and process improvement models must be validated to their accuracy requirements in order to build confidence in the models ability to replicate the real system. The validation process must consider the suitability of the model for its intended purpose, the consistency of the model with the real system, and the utility and effectiveness of the model [6], [7], [11]. 4. Execute the models. The base model and any of the process improvement models can be combined to assess the impact of various process improvements on cycle time. Analysis of outputs may also lead to new ideas and understanding of the system triggering additional process reengineering as well as consideration of other process improvements.

Existing Process

Process Improvement

Interface

System dynamics elements necessary to integrate the models

Figure 1.

Graphic of existing process with process improvement.

The two models are integrated through the interface pictured. Information will flow in both directions between the models. This interface is dependent on certain model elements, such as levels and rates, existing in both models. The ovals in Figure 1 represent a cutaway view showing the underlying structure of the system dynamics model and the elements needed for the interface. An existing model of an organization may need to be

3: Demonstration of approach
3.1: Model development
In order to illustrate the feasibility and usefulness of system dynamics modeling for process improvement assessment, we applied our approach to the software inspection process. Our model has the ability to provide answers to the types of questions, concerning process improvements, posed in Section 1.2. For the purpose of our demonstration, we focus mainly on the question of cycle time reduction. We initially developed a base model corresponding to a typical organization's waterfall software development process. We then constructed a model of the software inspection process which we integrated with the base model. The software inspection model enables manipulation of a number of variables connected to the inspection process in order to understand their impact on software development cycle time. Direct manipulation of the following variables are allowed: · The time spent on each inspection task per unit of work to be inspected (e.g., the preparation rate and the inspection rate); · The size of the inspection team; · The percent of errors found during inspection; · The percent of tasks that undergo reinspection; and · The defect prevention attributable to the use of inspections. Our software inspection model is based on the following assumptions: · Time allocated to software inspections takes time away from software development; · A larger inspection team will consume more man-hours per inspection than a smaller team; · Software inspections find a high percentage of errors early in the development life cycle; and · The use of inspections can lead to defect prevention, because developers get early feedback as to the types of mistakes they are making.

Our software inspection model does not incorporate the following: · Software developers achieve higher productivity due to an increase in product knowledge acquired through the software inspection process; · Software developers achieve improved design estimates due to attention paid to size estimates during inspection; and · Software inspections lead to increased visibility of the amount of work completed. The model also excludes the interaction that the inspection team size and the time spent performing inspection tasks have on the percent of errors found during inspection. The inspection team size, the time spent on inspection tasks and the percent of errors found during inspection can be set at the beginning of a model simulation according to historical metrics or altered during the actual simulation. In order to judge the impact that software inspections have on software development cycle time, the software inspection model must be integrated into a model of the software development process. Once integrated, the software inspection model will impact a number of elements in the software development process model. Figures 2 and 3 are an incomplete, but representative view of the integrated model. Figure 2 represents the process steps and effort involved in inspecting work products. Figure 2, however, does not reveal how time and manpower are allocated to perform each step in the inspection process, in order to keep the diagram and ideas presented simple. Each rate in Figure 2 requires that manpower be consumed in order to move work products from one step to the next. Figure 3 shows an incomplete, but representative implementation of the interface between the base model of the software development process and the process improvement model, that is shown abstractly in Figure 1. Figure 3 represents the modeling of errors in the base process model of software development and illustrates the impact inspections have on error generation and error detection in the base process model. The impacts that software inspections have on software development are: software inspections consume development man-hours, errors are less expensive to find during software inspection than system testing, software inspections promote defect prevention and software inspections reduce the amount of error regeneration. Before discussing Figure 2 and Figure

3, a brief discussion of flow diagrams, a notation used to represent system dynamics models, is in order.

Figure 2. Flow diagram of simplified inspection process steps.

Figure 3.

Flow diagram of inspection's impact on errors.

Flow diagrams are composed of levels, material flows, rates, auxiliaries and information links. Levels, depicted as rectangles, represent the accumulation of a material in a system. For example, work products and errors are materials that reside in levels in Figure 2 and Figure 3. Material flows, depicted as hollow arrows, indicated the ability for material to flow from one level to another. Material flows connected to clouds indicate a flow to, or from, a portion of the system not pictured. Rates, depicted as circles attached to material flows, represent the rate of material flow into and out of a level. For example, error detection rate in Figure 3 controls the rate at which potentially detectable errors are detected. Auxiliaries, depicted as circles, aid in the formulation of rate equations. In Figure 3, defect prevention is an auxiliary that affects the error generation rate. Information links, depicted as arrows, indicate the flow of information in a system. Information about levels, rates and auxiliaries are transferred using information links. Information links, unlike material flows, do not affect the contents of a level. The first impact that software inspections have on software development is in the allocation of man-hours for development. Software inspections require that time be set aside for them to be done properly. The time consumed by software inspections is time that cannot be

used for other activities, such as writing software. The amount of time that an inspection consumes is based on the size of the inspection team and the time spent on inspection tasks, e.g., the preparation rate and the inspection rate. Figure 2 is a simplified view of the steps that must be taken to perform inspections. It implicitly models the effort expended to move work products through all of the process steps associated with inspection; effort that takes time away from development activities. The second impact that software inspections have on software development is in the detection of errors. Errors are found early in the life cycle. Errors are less expensive to find and correct during development than during testing. This impact is not explicitly shown in Figure 3, except that a higher error detection rate will increase the number of errors found early in the life cycle, rather than later during testing. The third impact that software inspections have on software development is in the prevention of defects. Successful inspections attack the generation of future defects. Developers that are involved with inspections learn about the types of errors that they have been making and are likely to make during the project. This feedback about the errors they are making leads to fewer errors being made during the project. Figure 3 shows defect prevention, due to inspections, impacting the rate of error generation. The fourth impact that software inspections have on software development is a reduction in the regeneration of errors. Errors that remain undetected often lead to new errors being generated. For example, undetected design errors will lead to coding errors, because they are coding to a flawed design. Software inspections detect errors early in the life cycle, thus reducing the amount of error regeneration. Figure 3 indicates that the errors escaping detection impact the error generation rate. The number of errors escaping detection is dependent on the preparation and inspection rates, as shown in Figure 3. The four impacts that software inspections have on software development, mentioned above, are the foundation for the theory upon which the software inspection model is based. Many details of the implementation of the theory, using system dynamics techniques, are absent from Figure 2 and Figure 3, but are shown in detail elsewhere [14].

3.2: Example model output
The integrated model has been turned into a simple management flight simulator, allowing for simple experimentation. This section describes the user interface of the simulator and the output generated by its use.

Figure 4 shows the simple interface to the simulator. Input to the simulator is in the form of sliders. The simple interface has just five slider inputs: an on/off switch for inspections, the size of the inspection team, the percent of errors found during inspection, the percent of work products that fail inspection and the percent of incorrect error fixes.

Figure 4.

User interface of the inspection simulator.

Output from the simulator comes in two forms: numeric displays and graphs. Numeric displays show the current value of a simulation variable. Man-Days and Work Products Completed are two examples of numeric displays. Graphs, on the other hand, display the value of simulation variables versus time. Each output curve is labeled with a number for ease of reading. There may be multiple units of measure on the vertical axis, each matched to the number of the curve it is representing. The unit of measure on the horizontal axis is days. The five output curves represent: 1) currently perceived job size in terms of work products, 2)cumulative work products developed, 3) cumulative work products tested, 4) total size of workforce and 5)scheduled completion date. A demonstration of the use of the system dynamic models for predicting the cycle time reduction due to a process improvement is in order. Using the integrated model of the baseline waterfall development life cycle and the software inspection process improvement, it will be shown how this modeling technique can be used for evaluating the impact that a proposed process improvement would have on development cycle time. The following demonstration is a simulation of a hypothetical software team employing the simple inspection model presented in this paper. The project

being developed is estimated to be 64,000 lines of code requiring a total workforce of eight developers at the height of development. Two scenarios of the project development are simulated holding all variables fixed, except for the size of the inspection team and the percent of errors found during inspection. Figure 5 is the output generated by executing the model with an inspection team size of six developers discovering 40 percent of the errors during inspection. When interpreting the graphical output, the story of the project is revealed. From Figure 5, the following story emerges. Curve 1, the currently perceived job size in work products, reveals that the project size was initially underestimated. As development progressed, the true size of the project was revealed. Curve 5, the scheduled completion date, was not adjusted even as it became apparent that the project had grown in size. Instead, curve 4, the total size of workforce, indicates that the workforce was increased in size. In addition, though not shown on this graph, the workforce worked longer hours to bring the project back on schedule. Curve 2, cumulative work products developed, reveals that the project appeared to be back on schedule, because there were no visible delays in development of work products. It was not until system testing that problems in development were discovered. Curve 3, cumulative work products tested, reveals that system testing did not go as smoothly as expected. The poor performance of the inspection team pushed the discovery of errors back to system testing. During system testing it was revealed that there was a good amount of rework to be done and as a result, the scheduled completion date, curve 5, was once again pushed back.
1: Job Size in WP 1: 2: 3: 4: 5: 1500.00 2: WP Developed 3: WP Tested 4: Total Workforce 5: Completion Date

20.00 500.00 5 1 2 4 4 4 2

5 1 2

1: 2: 3: 4: 5:

5 750.00 1 10.00 250.00

1

1: 2: 3: 4: 5:

0.00 0.00 0.00 0.00

2 3 125.00 3 250.00 Days

3 375.00 5:02 PM 500.00 1/28/95

Main: Page 1

Figure 5.

Software inspection scenario 1.

Figure 6 is the output generated by executing the model with an inspection team size of three developers discovering 90 percent of the errors during inspection. The story is much the same as that shown in Figure 5. The big difference between Figures 5 and 6 is shown by curve 3, cumulative work products tested. Using more effective software inspections, this project was able to

discover errors early in the life cycle and correct them for much less cost than if they had been found in system test. In addition, there were no major surprises in system testing as to the quality of the product developed. Therefore, with no major amount of rework to be performed in system test, the project was able to finish close to its revised schedule.
1: Job Size in WP 1: 2: 3: 4: 5: 1500.00 2: WP Developed 3: WP Tested 4: Total Workforce 5: Completion Date

Approach, Prentice-Hall, Englewood Cliffs, New Jersey, 1991. [2] Tarek K. Abdel-Hamid, "THINKING IN CIRCLES," American Programmer, May 1993, pp. 3-9. Barry W. Boehm, SOFTWARE ENGINEERING ECONOMICS, Prentice-Hall, Englewood Cliffs, New Jersey, 1981. Ken W. Collier and James S. Collofello, "Issues in Software Cycle Time Reduction," International Phoenix Conference on Computers and Communications, 1995. Raymond Dion, "Process Improvement and the Corporate Balance Sheet," IEEE Software, July 1993, pp. 28-35. Jay W. Forrester, Industrial Dynamics, The M.I.T. Press, Cambridge, MA, 1961. Jay W. Forrester and Peter M. Senge, "Tests for Building Confidence in System Dynamics Models," System Dynamics. TIMS Studies in Management Sciences, 14 (1980), pp. 209-228. Mark C. Paulk, Bill Curtis, Mary Beth Chrissis and Charles V. Weber, "Capability Maturity Model, Version 1.1," IEEE Software, July 1993, pp. 18-27. T. S. Perry, "Teamwork plus Technology Cuts Development Time," IEEE Spectrum, October 1990, pp. 61-67. Lawrence H. Putnam, "General empirical solution to the macro software sizing and estimating problem," IEEE Transactions on Software Engineering, Vol. SE4, No. 4, July 1978, pp. 345-361. George P. Richardson and Alexander L. Pugh III, Introduction to System Dynamics Modeling with DYNAMO, The M.I.T. Press, Cambridge, MA, 1981. Howard A. Rubin, Margaret Johnson and Ed Yourdon, "With the SEI as My Copilot Using Software Process 'Flight Simulation' to Predict the Impact of Improvements in Process Maturity," A m e r i c a n Programmer, September 1994, pp. 50-57. George Stark, Robert C. Durst and C. W. Vowell, "Using Metrics in Management Decision Making," IEEE Computer, September 1994, pp. 42-48. John D. Tvedt, "A System Dynamics Model of the Software Inspection Process," Technical Report TR95-007, Computer Science and Engineering Department, Arizona State University, Tempe, Arizona, 1995.

[3]

[4]
1 2

20.00 500.00 5

1: 2: 3: 4: 5:

5 750.00 1 10.00 250.00 4

1 4 4

[5]

2 1: 2: 3: 4: 5:

3

0.00 0.00 0.00 0.00 2 3 125.00 3 250.00 Days 375.00 5:20 PM 500.00 1/28/95

[6]

Main: Page 1

Figure 6.

Software inspection scenario 2.

[7]

4: Conclusions and future work
Our research grew out of the questions posed in Section 1.2 concerning the impact of process improvements on software development cycle time. Our approach in answering those questions has been to use system dynamics modeling to model the software development process, allowing experimentation with the system. We have tried to demonstrate how this technique may be used to evaluate the effectiveness of process improvements. At this point in our work we have developed a base model of the waterfall development life cycle and a process improvement model of software inspections. We plan to continue this effort by developing a base model of the incremental development process and creating a library of process improvement models. Some examples of process improvements that we plan to add to our library are meetingless inspections, software reuse and risk management. We then plan to validate our base and process improvement models in several software development organizations. Finally, another area of future research is to generalize the interface between the base process models of software development and the models of process improvements. The interface is analogous to tool interfaces. A well defined, generalized interface would facilitate integration of base process models with process improvement models.
[8]

[9]

[10]

[11]

[12]

[13]

[14]

References
[1] Tarek Abdel-Hamid and Stuart E. Madnick, SOFTWARE PROJECT DYNAMICS An Integrated

SOFTWARE--PRACTICE AND EXPERIENCE, VOL. 23(10), 1095­1105 (OCTOBER 1993)

An Application of Causal Analysis to the Software Modification Process
Computer Science Department, Arizona State University, Tempe, AZ 85287, U.S.A.

james s. collofello and

AG Communication Systems Corporation, Phoenix, AZ 85027, U.S.A.

bakul p. gosalia

SUMMARY The development of high quality large-scale software systems within schedule and budget constraints is a formidable software engineering challenge. The modification of these systems to incorporate new and changing capabilities poses an even greater challenge. This modification activity must be performed without adversely affecting the quality of the existing system. Unfortunately, this objective is rarely met. Software modifications often introduce undesirable side-effects, leading to reduced quality. In this paper, the software modification process for a large, evolving real-time system is analysed using causal analysis. Causal analysis is a process for achieving quality improvements via fault prevention. The fault prevention stems from a careful analysis of faults in search of their causes. This paper reports our use of causal analysis on several significant modification activities resulting in about two hundred defects. Recommendations for improved software modification and quality assurance processes based on our findings are also presented.
key words: Causal analysis Software maintenance

BACKGROUND The traditional approach to developing a high quality software product consists of applying a development methodology with a heavy emphasis on fault detection. These fault detection processes consist of walkthroughs, inspection and various levels of testing. A more effective approach to developing a high quality product is an emphasis on fault prevention. An effective fault prevention approach which is gaining popularity is causal analysis.1­4 Causal analysis consists of collecting and analyzing software fault data in order to identify their causes. Once the causes are identified, process improvements can be made to prevent future occurrences of the faults. The paper by Jones3 presents a programming process methodology for using causal analysis and feedback as a means for achieving quality improvement, and ultimately fault prevention, at IBM. The methodology emphasizes effective use of the fault data to prevent the recurrence of faults. The fault prevention methodology is based on the following three concepts: 0038­0644/93/101095­11$10.50 © 1993 by John Wiley & Sons, Ltd. Received 6 April 1993 Revised 6 April 1993

1096

j. s. collofello and b. p. gosalia

1. Designers should evaluate their own faults. 2. Causal analysis should be part of the software development process. 3. Feedback should be part of the process. An overview of the causal analysis process proposed by Jones is shown in Figure 1. The first activity consists of a kickoff meeting with the following purposes: 1. 2. 3. 4. Review Review Review Identify input fault data. the methodology guidelines. appropriate checklists. the team goals.

Next, the development of the work products occurs, using the feedback from the kickoff meeting to prevent the creation of errors. The work products are then validated and reworked as necessary. A causal analysis activity is then performed, beginning with an analysis of the faults performed by the owner of the faults. This involves analyzing each fault to determine 1. 2. 3. 4. 5. The The The The The fault type or category phase that the fault was found phase that the fault was created cause(s) of the fault solution(s) to prevent the fault from occurring in the future.

Figure 1. Causal analysis process described by Jones

an application of causal analysis

1097

This information is recorded on a causal analysis form which is used to enter the data into a database. A causal analysis team meets to analyze the data in the database. In addition, the group may at times need to consult with other people (designers, testers, etc.) outside of the team to complete the analysis. The causal analysis team is responsible for identifying the major problem areas by looking at the fault data as a whole instead of one particular fault at a time. The team uses a problem-solving process to analyze the data, determine the problem area(s) to work on, determine the root causes of problem area(s), and develop recommendations and implementation plans to prevent the problem type(s) from occurring in the future. These recommendations are then submitted to an action team, which has the following responsibilities: 1. Evaluation and prioritization of recommendations. 2. Implementation of recommendations. 3. Dissemination of feedback. The action team meets periodically (e.g. once a month) to review any new implementation plans received from the causal analysis teams and to check the status of the previous implementation plans and action items. The status of the action items is kept in the causal analysis database and monitored by the action team. To date, most of the effort reported in causal analysis activities has been focused on development processes. This is unfortunate, considering the high percentage of effort consumed by software modification processes. Software modifications occur in order to add new capabilities or modify existing capabilities of a system. Modifying a large, complex system is a time-consuming and error-prone activity. This task becomes more difficult over time as systems grow and their structure deteriorates.5 Thus, more effort must be expended in performing causal analysis activities for design maintenance tasks. This paper describes such an effort. In the remainder of this paper, our approach to causal analysis for design maintenance tasks is described, as well as the application of this approach to a large project. Our results and recommendations for preventing faults during modification processes are also presented. CAUSAL ANALYSIS APPROACH Our application of causal analysis to the software modification process is shown in Figure 2. Each of the causal analysis steps in Figure 2 is described in one of the following subsections. Obtaining problem reports The first activity to be performed is collecting problem reports resulting from the modification processes undergoing causal analysis scrutiny. These problem reports may be generated internally via testing or externally by the customer. A window of time must be chosen during which the problem reports are collected. Ideally, the window of time for obtaining problem reports should begin after the code is modified and end some period of time after the code was delivered.

1098

j. s. collofello and b. p. gosalia

Figure 2. Causal analysis approach as applied to the software modification process

Prioritizing problems The next step in our causal analysis approach is prioritizing the faults. Fault prioritization enables an analysis of those causes which contribute to high priority problems. Our prioritization consists of three categories: 1. Critical 2. Major 3. Minor Critical faults impair functionality and prohibit further testing. Major faults partially impair functionality. Minor faults do not affect normal system operation.

Categorizing errors After fault prioritization, faults are categorized. The primary objective of this categorization is to facilitate the link between faults and their causes. Numerous error categorization schemes have been proposed over the years.6­9 The existing schemes are, however, directed towards understanding and improving the development of new software. They do not address the kinds of errors that are unique to maintenance activities. Thus, we found it necessary to expand upon existing categorization schemes and develop new error categories directed towards maintenance activities. Our categories were formulated based on our experience of analyzing hundreds of maintenance errors. Although categorization is not necessary in performing a causal analysis, we found the categorization useful in identifying fault causes. The fault categorization also provides useful information when determining the cost-effectiveness of eliminating fault causes versus attempting to detect the faults if they occur. The fault categories are described below:

an application of causal analysis Design faults

1099

This category reflects software faults caused by improper translation of requirements into design during modification. The design at all levels of the program and data structure is included. The following are examples of typical design faults: 1. Logic faults. Logic faults include sequence faults (improper order of processing steps), insufficient branch conditions, incorrect branch conditions, incorrectly nested loops, infinite loops, incorrectly nested if statements, missing validity checks etc. 2. Computation faults. Computation faults pertain to inaccuracies and mistakes in the implementation of addition, subtraction, multiplication, and division operations. Computational faults also include incorrect equations or grouping of parenthesis, mixing data of different unit values, or performing correct computations on the wrong data. 3. Missing exception handling. These faults include failure to handle unique conditions or cases even when the conditions or cases were specified in the requirements documents. 4. Timing faults. This type of fault reflects incorrect design of timing critical software. For example, in a multitasking system, a designer interpreted the executive command `suspend' to mean that all processing is halted for a specified amount of time, when in fact the command suspended all tasks scheduled to be invoked in the same specified amount of time. 5. Data handling faults. These faults include failure to initialize the data before use, improper use or designation of fixes, mixing up the names of two or more data items, and improper control of external file devices. 6. Faults in I/O concepts. These faults include input from or output to the wrong file or on-line device, defining too much data for a record size, formatting faults, and incorrect input­output protocols with other communication devices. 7. Data definition faults. This category reflects incorrect design of the data structures to be used in the module or the entire software (incorrectly defined global data structures). This category also reflects incorrect scoping in data structures and incorrect data abstractions. Incompatible interface Incompatible interface faults occur when the interfaces between two or more different types of modified components are not compatible. The following are examples of incompatible interfaces: 1. Different parameters passed to the procedures/functions than expected. 2. Different operations performed by the called modules, functions, or procedures than expected by the calling modules, procedures or functions. For example, assume a module A was called by another module B to perform fault checking. However, as a design change, the fault checking code was moved to another software module C. 3. Code and database mismatched. For example, suppose that device handler code tries to access data which is not defined in the database. 4. Executive routines and other routines mismatched. For example, consider the

1100

j. s. collofello and b. p. gosalia

situation where a task needs to remain in a no interrupt condition for a longer time than allowed by the operating system routines. 5. Software and hardware mismatched. 6. Software and firmware mismatched. For example, the parameters passed by the firmware may not match with the modified software. Incorrect code synchronization from parallel projects For a large evolving system, the software development cycles of multiple releases may overlap, as shown in Figure 3. This category of fault occurs when changes from project A, the previous project, are incorrectly carried into project B, the current project. Incorrect object patch carryover In a large maintenance effort, object patches are sometimes needed. These object patches may remain for several revisions of the system. Errors committed while modifying modules with object patches fall into this category. System resource exhaustion This fault occurs when the system resources (such as memory and real time) become insufficient. Examples of faults in this class include: exhaustion of available stack space, the indiscriminate use of special registers and the use of non-existent stacks, real-time clocks, and other architectural features. Determining fault causes The most critical step in performing causal analysis is determining the cause of each fault. This task can best be performed by the programmer who introduced the fault. To facilitate this identification of fault causes, the software designers who introduced the faults should be interviewed. The interviews should be informal and performed very carefully so as to not make designers defensive. The purpose of the analysis for defect prevention must be made clear prior to the interview. Based on the information acquired from the designer who introduced the fault, a causal category is selected for the error. Our approach to causal analysis identifies seven major categories of causes that lead to software modification faults. Our causal categories differ from others in the literature5,9 which are directed

Figure 3. Software development of large evolving systems

an application of causal analysis

1101

towards finding the causes of errors made during new software development. These existing causal categorization schemes do not address the unique causes of errors made during software maintenance. Our causal categories are not exclusive and, thus, a fault might have several causes. The causal categories were formulated in a manner that would support process improvements. The goal of determining the causal category for each fault is to collect the data necessary to determine which causal categories are leading to the most problems. This provides a means for evaluating the cost effectiveness of implementing recommendations to eliminate or reduce fault causes. Our causal categories customized to modification activities are described below: System knowledge/experience This causal category reflects the lack of software design knowledge about the product or the modification process. Some examples include: 1. Misunderstanding of the existing design. For example, the designer did not have sufficient understanding of the existing design of the database. 2. Misunderstanding of the modification process. The designer was not aware of the process followed in the project. For example, the designer was not aware of the configuration control process. 3. Inadequate understanding of the programming environment. The designer did not have adequate understanding of the programming environment. The programming environment includes personnel, machinery, management, development tools, and any other factors which affect the developer's ability to modify the system. 4. Inadequate understanding of the customer's requirements. The designer did not thoroughly understand the requirements of the customer. For example, the designer may not understand a feature specified in the requirements documents. Communication This causal category reflects communication problems concerning modifications. It identifies those causes which are not attributed to a lack of knowledge or experience, but instead to incorrect or incomplete communication. Communication problems are also caused due to confusion between team members regarding responsibilities or decisions. Some examples of communication problems are: 1. Failure of a design group to communicate a last-minute modification. 2. Failure to write a fault-handling routine for a function because each team member thought someone else was supposed to do it. Software impacts This category reflects failure of a software designer to consider all possible impacts of a software modification. An example is omission of fault recovery code after the addition of a new piece of hardware.

1102 Methods/standards

j. s. collofello and b. p. gosalia

This category reflects methods and/or standards violations. It also includes limitations to existing methods or standards which contributed to the faults. An example may be a missed code review prior to testing. Feature deployment This category reflects problems caused by the inability of software, hardware, firmware or database components to integrate at the same time. It is characterized by a lack of contingency plans to prevent problems caused by missing components. Supporting tools This category reflects problems with supporting tools which introduce faults. An example is an incorrect compiler which introduces faults in object code. Human error This causal category reflects human errors made during the modification process which are not attributable to other sources. The designer knew what to do and understood the item thoroughly but simply made a mistake. An example may be that a designer forgets to add code he intended for a software load. Developing recommendations The last step in our causal analysis process is developing a set of recommendations to prevent faults from reoccurring. These recommendations must be based on the data collected during the causal analysis study and customized to the development environment. APPLICATION OF THE CAUSAL ANALYSIS APPROACH In an effort to learn more about the faults generated during software modifications, we applied our causal analysis process to the creation of a new version of a large telephony system. This new version contained new features as well as fixes to problems in the old release. The size of the system was approximately 2·9 million lines of code. The version analyzed was developed from the previous version by gradual modifications to the software, firmware, and hardware. About 1 million lines of code were modified as follows: 85 per cent of the code was added, 10 per cent of the code was changed, and 5 per cent of the code was deleted. Each of these gradual modifications corresponded to a software increment. The increments were added to the system one at a time and tested as shown in Figure 4. The new version we analyzed consisted of 11 increments. The experience levels of the designers involved in modifying the software varied widely from two years to as high as 30 years. The documentation used and created in the process of generating the new version was typical of that in most software development organizations. Both high level and

an application of causal analysis

1103

Figure 4. Gradual building of a software version release

detailed design documents were created for the new features being added. The contents of these documents included details about the impacts of the new feature on the existing modules, hardware, database, firmware, etc. These documents were then reviewed by designers representing the affected areas. Documentation was also generated and reviewed for fixes added to the new version. Although it would have been ideal to collect and analyze problem reports for the entire modification process, our study was limited to analyzing defects detected during the first phase of integration testing for each of the incremental loads. This phase verifies the basic stability and functionality of the system prior to testing the operation of new features, functions, and fixes. It is, thus, the most likely phase to detect faults introduced by the modification process. Prior to our data collection, both code reviews and module testing were generally applied. Thus, faults detected by these processes were not reflected in our results. In addition, faults detected by later phases of testing, such as system testing and stress testing were also not reflected in our results. Our results, therefore, are limited to the kinds of faults normally detected during integration testing. In our study, this amounted to more than 200 faults. All 200 faults were analyzed and categorized by this paper's authors to ensure reliable results. RESULTS OF THE CAUSAL ANALYSIS APPLICATION Figure 5 presents an overall distribution of fault categories across all the problems. The large number of design and interface faults demonstrates that the modification

Figure 5. Fault category distribution of all the problems

1104

j. s. collofello and b. p. gosalia

Figure 6. Causal categories distribution of all the problems

of an existing design in order to satisfy new requirements is a very error-prone process. Figure 6 presents the distribution of causal categories for all the problems. The causal analysis data suggests that almost 80 per cent of all faults are caused by insufficient knowledge/experience, communication problems or a failure to consider all software impacts. A further analysis of the faults caused by insufficient knowledge/experience indicated that the majority of those faults were caused by designers with less than five years of experience. These results clearly suggest that one significant approach to improving the quality of maintenance activities is to increase the experience level of those performing design maintenance tasks. This can be accomplished in many ways. One obvious approach is motivating designers to continue in their work assignments. Another approach is to improve design maintenance training and mentoring programs. The results obtained from the causal analysis also suggest that the design maintenance process can be greatly improved via better communication. This might be accomplished with more disciplined maintenance methodologies, more thorough documentation and diligent reviews. Extreme care must be taken to ensure that information regarding potential software impacts as a result of modification activities are communicated. The results from our causal analysis activity were integrated with those of other ongoing continuous process improvement approaches within our organization and applied to the development of a subsequent version of the software. The improvements mainly focused upon an enhanced review process, better training and better communication. The fault data from the subsequent version of the software release showed a substantial reduction in the number of faults. CONCLUSIONS AND FUTURE RESEARCH This paper has presented our results from applying causal analysis to several large modification activities. Our data suggests that almost 80 per cent of all faults created during modification activities are caused by insufficient knowledge/experience, communication problems or a failure to consider all software impacts. The application of causal analysis to the software modification process has contrib-

an application of causal analysis

1105

uted to a substantial reduction of faults in our organization. Much additional research needs to be performed to assess whether the results of this study are representative of that experienced in other large evolving systems. Other research needs to explore the attributes of an `experienced' designer. Specific design maintenance methodology improvements must also be formulated and evaluated in terms of their potential return on investment.
REFERENCES 1. J. L. Gale, J. R. Triso and C. A. Burchfield, `Implementing the defect prevention process in the MVS interactive programming organization', IBM Systems Journal, 30, (1), 33­43 (1990). 2. R. G. Mays, C. L. Jones, G. J. Holloway and D. P. Studinski, `Experiences with defect prevention', IBM Systems Journal, 30, (1), 4­32 (1990). 3. C. L. Jones, `A process-integrated approach to defect prevention', IBM Systems Journal, 24, (2), 50­ 164 (1985). 4. R. T. Philips, `An approach to software causal analysis and defect extinction', IEEE Globecom 1986, 1, (12), 412­416 (1986). 5. J. Collofello and J. Buck, `Software quality assurance for maintenance', IEEE Software, No. 5, September 1987, pp. 46­51. 6. A. Endres, `An analysis of errors and their causes in system programs', IEEE Trans. Software Eng., SE1, (2), 140­149 (1975). 7. B. Beizer, Software Testing Techniques, Van Nostrand Reinhold, 1983. 8. J. Collofello and L. Blumer, `A proposed software error classification scheme', Proceedings of the National Computer Conference, 1985, pp. 537­545. 9. T. Nakajo and H. Kume, `A case history of software error cause­effect relationships', IEEE Trans. Software Eng., 17, 830­837 (1990).

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 1

An Agent-Based Model of FLOSS Projects
Nicholas P. Radtke, Arizona State University, USA Marco A. Janssen, Arizona State University, USA James S. Collofello, Arizona State University, USA

What Makes Free/Libre Open Source Software (FLOSS) Projects Successful?

Abstract
The last few years have seen a rapid increase in the number of Free/Libre Open Source Software (FLOSS) projects. Some of these projects, such as Linux and the Apache web server, have become phenomenally successful. However, for every successful FLOSS project there are dozens of FLOSS projects which never succeed. These projects fail to attract developers and/or consumers and, as a result, never get off the ground. The aim of this research is to better understand why some FLOSS projects flourish while others wither and die. This article presents a simple agent-based model that is calibrated on key patterns of data from SourceForge, the largest online site hosting open source projects. The calibrated model provides insight into the conditions necessary for FLOSS success and might be used for scenario analysis of future developments of FLOSS. [Article copies are available for purchase from InfoSci-on-Demand.com] Keywords: Agent-Based Model; Emergent Properties; FLOSS; Open Source; Prediction Success; Simulation

Although the concept of Free/Libre Open Source Software (FLOSS) has been around for many years, it has recently increased in popularity as well as received media attention, not without good reason. Certain characteristics of FLOSS are highly desirable: some FLOSS projects have been shown to be of very high quality (Analysis of the Linux Kernel, 2004; Linux Kernel Software, 2004) and to have low

defect counts (Chelf, 2006); FLOSS is able to exploit parallelism in the software engineering process, resulting in rapid development (Kogut & Metiu, 2001); FLOSS sometimes violates Brooks' law (Rossi, 2004), which states that "adding manpower to a late software product makes it later" (Brooks, 1975); and FLOSS development thrives on an increasing user- and developer-base (Rossi, 2004).

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

2 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009

As open source has become a prominent player in the software market, more people and companies are faced with the possibility of using open source products, which often are seen as free or low-cost solutions to software needs. However, choosing to use open source software is risky business, partly because it is unclear which FLOSS will succeed. To choose an open source project, only to find it stagnates or fails in the near future, could be disastrous, and is cited as a concern by IT managers (T. Smith, 2002). Accurate prediction of a project's likelihood to succeed/fail would therefore benefit those who choose to use FLOSS, allowing more informed selection of open source projects. This article presents an initial step towards the development of an agent-based model that simulates the development of open source projects. Findings from a diverse set of empirical studies of FLOSS projects have been used to formulate the model, which is then calibrated on empirical data from SourceForge, the largest online site hosting open source projects. Such a model can be used for scenario and sensitivity analysis to explore the conditions necessary for the success of FLOSS projects.

BACKGROUND
There have been a limited number of attempts to simulate various parts of the open source development process (Dalle & David, 2004). For example, Dalle and David (2004) use agentbased modeling to create SimCode, a simulator that attempts to model where developers will focus their contributions within a single project. However, in order to predict the success/failure of a single FLOSS project, other existing FLOSS projects, which are vying for a limited pool of developers and users, may need to be considered. This is especially true when multiple FLOSS projects are competing for a limited market share (e.g., two driver projects for the same piece of hardware or rival desktop environments such as GNOME and the KDE). Wagstrom, Herbsleb, and Carley (2005) created OSSim, an agent-based model containing us-

ers, developers, and projects that is driven by social networks. While this model allows for multiple competing projects, the published experiments include a maximum of only four projects (Wagstrom et al., 2005). Preliminary work on modeling competition among projects is currently being explored by Katsamakas and Georgantzas (2007) using a system dynamics framework. By using a population of projects, it is possible to consider factors between the projects, e.g., the relative popularity of a project with respect to other projects as a factor that attracts developers and users to a particular project. Therefore, our model pioneers new territory by attempting to simulate across a large landscape of FLOSS with agent-based modeling. Gao, Madey, and Freeh (2005) approach modeling and simulating the FLOSS community via social network theory, focusing on the relationships between FLOSS developers. While they also use empirical data from the online FLOSS repository SourceForge to calibrate their model, they are mostly interested in replicating the network structure and use network metrics for validation purposes (e.g. network diameter and degree). Our model attempts to replicate other emergent properties of FLOSS development without including the complexities of social networking. However, both teams consider some similar indicators, such as the number of developers working on a project, when evaluating the performance of the models. In addition, there have been attempts to identify factors that influence FLOSS. These have ranged from pure speculation (Raymond's (2000) gift giving culture postulates) to surveys of developers (Rossi, 2004) to case studies using data mined from SourceForge (Michlmayr, 2005). Wang (2007) demonstrates specific factors can be used for predicting the success of FLOSS projects via K-Means clustering. However, this form of machine learning offers no insight into the actual underlying process that causes projects to succeed. Therefore, the research presented here approaches simulating

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 3

the FLOSS development process using agentbased modeling instead of machine learning. To encourage more simulation of the FLOSS development process, Antoniades, Samoladas, Stamelos, Angelis, and Bleris (2005) created a general framework for FLOSS models. The model presented here follows some of the recommendations and best practices suggested in this framework. In addition, Antoniades et al. (2005) developed an initial dynamical simulation model of FLOSS. Although the model presented here is agent-based, many of the techniques, including calibration, validation, and addressing the stochastic nature of the modeling process, are similar between the two models. One difference is the empirical data used for validation: Antoniades et al.'s (2005) model uses mostly code-level metrics from specific projects while the model presented here uses higher project-level statistics gathered across many projects.

·

Active developer count change trends (Wang, 2007)

IDENTIFYING AND SELECTING INFLUENTIAL FACTORS
Factors which are most likely to influence the success/failure of FLOSS must first be identified and then incorporated into the model. Many papers have been published in regards to this, but most of the literature simply speculates on what factors might affect the success and offers reasons why. Note that measuring the success of a FLOSS project is still an open problem: some metrics have been proposed and used but unlike for commercial software, no standards have been established. Some possible success indicators are: · · · · · · Completion of the project (Crowston, Howison, & Annabi, 2006) Progression through maturity stages (Crowston & Scozzi, 2002) Number of developers Level of activity (i.e., bug fixes, new feature implementations, mailing list) Time between releases Project outdegree (Wang, 2007)

English and Schweik (2007) asked eight developers how they defined success and failure of an open source project. Answers varied for success, but all agreed that a project with a lack of users was a failure. Thus having a sufficient user-base may be another metric for success. Papers that consider factors influencing success fall into two categories: those that look at factors that directly affect a project's success (Michlmayr, 2005; Stewart, Ammeter, & Maruping, 2006; S. C. Smith & Sidorova, 2003) and those that look for factors that attract developers to a project (and thus indirectly affect the success of a project) (Bitzer & Schröder, 2005; Rossi, 2004; Raymond, 2000; Lerner & Tirole, 2005). A few go a step further and perform statistical analyses to discover if there is a correlation between certain factors and a project's success/ failure (Lerner & Tirole, 2005; Michlmayr, 2005), and Kowalczykiewicz (2005) uses trends for prediction purposes. Wang (2007) demonstrates that certain factors can be used for accurate prediction using machine learning techniques. Koch (2008) considers factors affecting efficiency after first using data envelopment analysis to show that successful projects tend to have higher efficiencies. In general, factors affecting FLOSS projects fall into two categories: technical factors and social factors. Technical factors are aspects that relate directly to a project and its development and are typically both objective and easy to measure. Examples of technical factors include lines of code and number of developers. The second category is social factors. Social factors pertain to aspects that personally motivate individuals to engage in open source development/use. Examples of social factors include reputation from working on a project, matching interests between the project and the developer/user, popularity of the project with other developers/users, and perceived importance of the code being written (e.g., core versus fringe development (Dalle & David, 2004)). Most of the social factors are subjective and

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

4 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009

rather difficult, if not impossible, to measure. Despite this, it is hard to deny that these might influence the success/failure of a project and therefore social factors are considered in the model. Fortunately, the social factors being considered fall under the domain of public goods, for which there is already a large body of work published (e.g., Ostrom, Gardner, & Walker, 1994; Jerdee & Rosen, 1974; Tajfel, 1981; Axelrod, 1984; Fox & Guyer, 1977). Most of this work is not specific to FLOSS, but in general it explores why people volunteer to contribute to public goods and what contextual factors increase these contributions. The findings of this literature are applied when designing the model, as are findings from publications investigating how FLOSS works, extensive surveys of developers asking why they participate in FLOSS (e.g., Ghosh, Krieger, Glott, & Robles, 2002), and comments and opinions of FLOSS users (e.g., T. Smith, 2002).

INITIAL MODEL
The model universe consists of agents and FLOSS projects. Agents may choose to contribute to or not contribute to, and to consume (i.e. download) or not consume FLOSS projects. At time zero, FLOSS projects are seeded in

the model universe. These initial projects vary randomly in the amount of resources that will be required to complete them. At any time, agents may belong to zero, one, or more than one of the FLOSS projects. The simulation is run with a time step (t) equal to one (40 hour) workweek. Table 1 contains the properties of agents. Table 2 contains the properties of projects. At each time step, agents choose to produce or consume based on their producer and consumer numbers, values between 0.0 and 1.0 that represent probabilities that an agent will produce or consume. Producer and consumer numbers are statically assigned when agents are created and are drawn from a normal distribution. If producing or consuming, an agent calculates a utility score for each project in its memory, which contains a subset of all available projects. The utility function is shown in Box 1. Each term in the utility function represents a weighted factor that attracts agents to a project, where w1 through w5 are weights that control the importance of each factor, with 0.0  w1,w2,w3,w4,w5  1.0 and 5 . Factors i=1 wi = 1.0 were selected based on both FLOSS literature and our own understanding of the FLOSS development process. Keeping it simple, a linear utility equation is used for this version of the model. The first term represents the similarity between the interests of an agent and the direc-

Table 1. Agent properties
Property Consumer number Producer number Needs vector Resources number Description Propensity of an agent to consume (use) FLOSS. Propensity of an agent to contribute to (develop) FLOSS. A vector representing the interests of the agent. A value representing the amount of work an agent can put into FLOSS projects on a weekly basis. A value of 1.0 represents 40 hours. A list of projects the agent knows exist. Type/Range Real [0.0, 1.0] Real [0.0, 1.0] Each scalar in vector is real [0.0, 1.0] Real [0.0, 1.5]

Memory

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 5

Table 2. Project properties
Property Current resources Cumulative resources Resources for completion Download count Maturity Description The amount of resources or work being contributed to the project during the current time interval. The sum, over time increments, of all resources contributed to the project. The total number of resources required to complete the project. The number of times the project has been downloaded. Six ordered stages a project progresses through from creation to completion. An evolving vector representing the interests of the developers involved in the project. Type/Range Real Real Real Integer {planning, pre-alpha, alpha, beta, stable, mature} Each scalar in vector is real [0.0, 1.0]

Needs vector

Box 1.
utility = w1  similarity (agentNeeds, projectNeeds ) + w2  currentResourcesnorm + w3  cumulativeResourcesnorm + w4  downloadsnorm + w5  f (maturity ) (1)

tion of a project; it is currently calculated using cosine similarity between the agent's and project's needs vectors. The second term captures the current popularity of the project and the third term the size of the project implemented so far. The fourth term captures the popularity of a project with consumers based on the cumulative number of downloads a project has received. The fifth term captures the maturity stage of the project. Values with the subscript "norm" have been normalized (e.g., downloadsnorm is a project's download count divided by the maximum number of downloads that any project has received). The discreet function f maps each of the six maturity stages into a value between 0.0 and 1.0, corresponding to the importance of each maturity stage in attracting developers. Since all terms are normalized, the

utility score is always a value between 0.0 and 1.0. Both consumers and producers use the same utility function. This is logical, as most FLOSS developers are also users of FLOSS. For consumers that are not producers, arguably the terms represented in the utility function are still of interest when selecting a project. There is relatively little research published on users compared to developers of FLOSS, so it is unclear if selection criteria are different between the two groups. It is possible that some of the terms included in the utility function are redundant or irrelevant. Part of the model exploration is to determine which of these factors are relevant. See the Calibrating the Model and Results sections below.

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

6 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009

Agents use utility scores in combination with a multinominal logit equation to probabilistically select projects. The multinominal logit allows for imperfect choice, i.e., not always selecting the projects with the highest utility. There is no explicit formulation of communication between agents included in the model; implicitly it is assumed that agents share information about other projects and thus agents know characteristics of projects they are not currently consuming/producing. At each time step, agents update their memory. With a certain probability an agent will be informed of a project and add it to its memory, simulating discovering new projects. Likewise, with a certain probability an agent will remove a project from its memory, simulating forgetting about or losing interest in old projects. Thus, over time an agent's memory may expand and contract. Projects update their needs vector at each iteration using a decaying equation, where the new vector is partially based on the project's previous vector and partially on the needs vectors of the agents currently contributing to the project. An agent's influence on the project's vector is directly proportional to the amount of work the agent is contributing to the project with respect to other agents working on the same project. This represents the direction of a project being influenced by the developers working on it. Finally, project maturity stages are computed based on percent complete threshold values.

in each stage similar to that measured by Weiss. In addition, two other emergent properties were chosen to validate the initial model: · · · Number of developers per FLOSS project. Number of FLOSS projects per developer. By creating a model that mimics a number of key patterns of the data, confidence is derived about the model.

CALIBRATING THE MODEL
The model has a number of parameters that must be assigned values. A small subset of these can be set to likely values based on statistics gathered from surveys or mined from FLOSS repository databases. For the remaining parameters, a search of the parameter space must be performed to find the combination that allows the model to most closely match the empirical data. Since an exhaustive search is not practical, the use of genetic algorithms from evolutionary computation is used to explore the parameter space (Kicinger, Arciszewski, & De Jong, 2005). This is done as follows: an initial population of model parameter sets is created randomly. The model is run with each of the parameter sets and a fitness score is calculated based on the similarity of the generated versus empirical data. The parameter values from these sets are then mutated or crossed-over with other parameter sets to create a new generation of model parameter sets, with a bias for selecting parameters sets that resulted in a high fitness; then the new generation of parameter sets are evaluated and the process repeated. In this case, a genetic algorithm is being used for a stochastic optimization problem for which it is not known when a global optimum is found. Genetic algorithms are appropriate for finding well-performing solutions in a reasonably brief amount of time. Reviewing the values of the best performing parameters will help identify which factors are important/influential in the open source software development process.

VALIDATION METHOD
Creating a model that successfully predicts the success or failure of FLOSS projects is a complicated matter. To aid in the iterative development process, the model is first calibrated to reproduce a set of known, emergent properties from real world FLOSS data. For example, Weiss (2005) surveyed the distribution of projects at SourceForge in each of six development categories: planning, pre-alpha, alpha, beta, stable, and mature. Therefore, the model will need to produce a distribution of projects

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 7

The fitness function chosen for the genetic algorithm is based on the sum of the square of errors between the simulated and empirical data, as shown in Box 2. Since there are three fitness values calculated, one per empirical data set, the three fitness values are averaged to provide a single value for comparison purposes.

RESULTS
Since the model includes stochastic components, multiple runs with a given parameter set were performed and the results averaged. In this case, four runs were performed for each parameter set after initial experimentation showed very low standard deviations even with small numbers of runs. The averaged model results were then compared to the empirical data. As empirical investigations of FLOSS evolution note, it takes approximately four years for a project of medium size to reach

a mature stage (Krishnamurthy, 2002). Thus, the model's performance was evaluated by running the model for 250 time steps, with a time step of one week, for a total simulated time equivalent of a little over five years. All metrics were gathered immediately following the 250th time step. The averaged data (over 4 runs) from the simulator's best parameter set, along with the empirical data, is shown in Figs. 1, 2, and 3. Figure 1 shows the generated percentage of projects in each maturity stage is a similar shape to the empirical data, with the main difference being the highs are too high and the lows are too low in the simulated data. This disparity may be a result of initial model startup conditions. At time 0, the model starts with all projects in the planning stage. This is obviously different than SourceForge, where the projects were gradually added over time, not all at once in the beginning. While the model does add new projects each time step, with a growth rate based on the rate of increase of projects at SourceForge, it may take

Box 2.

fitness = 1 -

sum of square of errors maximum possible sum of square of errors

(2)

Figure 1. Percentage of FLOSS projects in maturity stages. Empirical data from (Weiss, 2005)

Maturity Stages
45 40 35 30 25 20 15 10 5 0 Plan ning Sim Average Emp Value

Percent

Prealpha

Alpha

Beta

Stable Mature

Maturity Stage

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

8 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009

more than 250 time steps for maturity stages to stabilize after the differing initial condition. At the end of the simulation run, just short of 60% of the projects were created sometime during the simulation while the remaining 40% were created at time 0. As shown in Figure 2, the number of developers per projects follows a near-exponential distribution and the simulated data is similar, especially for projects with fewer than seven developers. Note that the data in Figure 2 uses a logarithmic scale to help with a visual comparison between the two data sets. Beyond seven developers, the values match less closely, although this difference is visually amplified as a result of the logarithmic scale and is actually not as large as it might initially appear. Since there are few projects with large numbers of developers in the empirical data, the higher values may be in the noise anyhow and thus focus should be on the similarity of the lower numbers. Figure 3 shows the number of projects per developer is a relatively good match between the simulated and empirical data, with the main difference being the number of developers working on one project. It is likely that this could

be corrected via additional experimentation with parameters. Table 3 contains the average fitness scores for each of the emergent properties for the top performing parameter set. These values provide a quantitative mechanism for confirming the visual comparisons made above: the maturity stage fitness score is indeed lower than the other two properties. The combined fitness is simply the mean of the three fitness scores, although this value could be calculated with uneven weights if, say, matching each property was prioritized. Doing so would affect how the genetic algorithm explored the parameter space. It may be the case that certain properties are easy to reproduce in the model and work over a wide range of parameter sets, in which case these properties may be weighted less than properties that are more difficult to match. Properties which are always matched should be discarded from the model for evolution purposes as they do not discriminate against different parameter sets. Finally, examining the evolved utility weights of the top 10 performing parameter sets provides insight into what factors are important in the model for reproducing the three properties examined. Table 4 contains the averages and standard deviations for each

Figure 2. Percentage of projects with N developers. Empirical data from (Weiss, 2005)

Developers per Project
6 4

ln(percent)

2 0 -2 -4 -6 -8 0 5 10 15 20 25 30 35 40

Sim Average Emp Value

Developers per Project

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 9

Figure 3. Percentage of developers with N projects. Empirical data from (Ghosh et al., 2002)

Projects per Developer
40 35 30 25 20 15 10 5 0 0 1 2 3 4-5 6-7 8-1 11- 16- >20 0 15 20 Sim Average Emp Value

Percent

Projects per Developer

Table 3. Averaged fitness scores for the best
Emergent Property Maturity stage Devs per project Projects per dev Combined Fitness Score 0.9679 0.9837 0.9938 0.9818

Table 4. Utility function weights from the best 10 param
Weight w1 (similarity) w2 (current resources) w3 (cumulative resources) w4 (downloads) w5 (maturity) Mean 0.1849 0.3964 0.0003 0.0022 0.4163 Std. Dev. 0.1137 0.1058 0.0003 0.0039 0.1534

of the weights. It appears that the cumulative number of resources and download counts are not important in reproducing the examined properties in the model. This conclusion is reached by observing these weight's small values (low mean and small variance) in comparison to the other weights (high means and larger variance).

Unfortunately, the high variance of the remaining three weights makes it difficult to rank them in order of importance. Rather, the conclusion is that similarity, current resources, and maturity are all important in the model. Another interesting set of values evolved by the system are the parameters for the producer and consumer numbers. While the producer and consumer numbers are drawn from normal distributions bounded by 0.0 and 1.0 inclusive, neither the mean nor standard deviations of these distributions are known. Therefore, these values are evolved to find the best performing values. Table 5 contains the evolved mean and standard deviation for the producer and consumer numbers averaged from the top 10 parameter sets. Notice that the mean producer number is very high at 0.9801 and very stable across the top 10 parameter sets, with a standard deviation of 0.0079. Likewise, the standard deviation is relatively low at 0.1104 and also stable with a standard deviation of 0.0101. This indicates that the top performing model runs had agents with high propensities to develop. In other words having most agents produce frequently (i.e., most agents be developers) produces better matching of the empirical data. This is in alignment with the notion that FLOSS is a developer-driven

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

10 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009

Table 5. Evolved producer/consumer number distributions parameters
Parameter statistics from top 10 parameter sets Mean Producer number Consumer number Mean Std. Dev. Mean Std. Dev. 0.9801 0.1104 0.6368 0.3475 Std. Dev. 0.0079 0.0101 0.1979 0.3737

Producer/Consumer Number

process. The evolved consumer number mean is much lower and standard deviation is much higher compared to the producer number. Neither one of these parameters is particularly stable, i.e., both have large standard deviations over the top 10 parameter sets. This indicates that the consumer number distribution has little effect on matching the empirical data for the top 10 parameter sets. Note that this is in alignment with the evolved weight for downloads approaching 0.0 in the utility functions. Consumers are not the driving force in matching the empirical data in the model.

DISCUSSION
Once developers join a project, it is likely that they will continue to work on the same project in the future. This is especially evident in the case of core developers, who typically work on a project for an extended period of time. Currently, the model attempts to reproduce this characteristic by giving a boost (taking the square root) of the utility function for projects worked on in the previous time step. In effect, this increases the probability of an agent selecting the same projects to work on in the subsequent time step. Improvements to the model might include adding a switching cost term to the utility function, representing the extra effort required to become familiar with another project. Gao et al. (2005) address this

issue by using probabilities based off data from SourceForge to determine when developers continue with or leave a project they are currently involved with in their FLOSS model. The model's needs vectors serve as an abstraction for representing the interests and corresponding functionalities of the agents and projects respectively. Therefore, the needs vector is at the crux of handling the matching of developers' interests with appropriate projects. For simplicity, initial needs vector values are assigned via a uniform distribution, but exploration of the effects of other distributions may be interesting. For example, if a normal distribution is used, projects with vector components near the mean will have an easy time attracting agents with similar interests. Projects with vector components several standard deviations from the mean may fail to attract any agents. A drawback of a normal distribution is that it makes most projects similar; in reality, projects are spread over a wide spectrum (e.g., from operating systems and drivers to business applications and games), although the actual distribution is unknown and difficult to measure. Currently, needs vectors for projects and agents are generated independently. This has the problem of creating projects which have no interest to any agents. An improvement would be to have agents create projects; when created, a project would clone its associated agent's needs vector (which would then evolve as other agents joined and contributed to the project). This behavior would more closely match SourceForge, where a developer initially registers his/her project. By definition, the project matches the developer's interest at time of registration. For simplicity's sake, currently the model uses a single utility function for both producers and consumers. It is possible that these two groups may attach different weights to factors in the utility function or may even have two completely different utility functions. However, analysis of the model shows that developers are the driving force to reproduce the empirical data. Exploration of a simplified model without

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 11

consumers may show that concerns about using multiple utility functions are irrelevant. One final complication with the model is its internal representations versus reality. For example, a suggested strategy for success in open source projects is to release early and release often (Raymond, 2000). Using this method to determine successful projects within the model is problematic because the model includes no concept of releasing versions of software. Augmenting the model to include a reasonable representation of software releases is non-trivial, if possible at all. Likewise, it is difficult to compare findings of other work on conditions leading to success that map into this model. For example, Lerner and Tirole (2005) consider licensing impacts while Michlmayr (2005) consider version control systems, mailing lists, documentation, portability, and systematic testing policy differences between successful and unsuccessful projects. Unfortunately, none of these aspects easily map into the model for comparison or validation purposes.

emergent properties for validation purposes, the model could move into the realm of prediction. In this case, it would be possible to feed real-life conditions into the model and then observe a given project as it progresses (or lack of progresses) in the FLOSS environment.

REFERENCES
Analysis of the linux kernel. (2004). Research report. (Coverity Incorporated) Antoniades, I., Samoladas, I., Stamelos, I., Angelis, L., & Bleris, G. L. (2005). Dynamical simulation models of the open source development process. In S. Koch (Ed.), Free/open source software development (pp. 174­202). Hershey, PA: Idea Group, Incorporated. Axelrod, R. (1984). The evolution of cooperation. New York: Basic Books. Bitzer, J., & Schröder, P. J. (2005, July). Bug-fixing and code-writing: The private provision of open source software. Information Economics and Policy, 17(3), 389-406. Brooks, F. P. (1975). The mythical man-month: Essays on software engineering. Reading, MA: Addison-Wesley. Chelf, B. (2006). Measuring software quality: a study of open source software. Research report. (Coverity Incorporated) Crowston, K., Howison, J., & Annabi, H. (2006, March/April). Information systems success in free and open source software development: Theory and measures. Software Process: Improvement and Practice, 11(2), 123­148. Crowston, K., & Scozzi, B. (2002). Open source software projects as virtual organizations: competency rallying for software development. In IEE proceedings software, 49, 3­17). Dalle, J.-M., & David, P. A. (2004, November 1). SimCode: Agent-based simulation modelling of open-source software development (Industrial Organization). EconWPA. English, R., & Schweik, C. M. (2007). Identifying success and tragedy of FLOSS commons: A preliminary classification of Sourceforge.net projects.

CONCLUSION
A better understanding of conditions that contribute to the success of FLOSS projects might be a valuable contribution to the future of software engineering. The model is formulated from empirical studies and calibrated using SourceForge data. The calibrated version produces reasonable results for the three emergent properties examined. From the calibrated data, it is concluded that the similarity between a developer and a project, the current resources going towards a project, and the maturity stage of a project are important factors. However, the cumulative resources and number of downloads a project has received are not important in reproducing the emergent properties. The model presented here aids in gaining a better understanding of the conditions necessary for open source projects to succeed. With further iterations of development, including supplementing the model with better data-based values for parameters and adding additional

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

12 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 In FLOSS '07: Proceedings of the first international workshop on emerging trends in FLOSS research and development (p. 11). Washington, DC, USA: IEEE Computer Society. Fox, J., & Guyer, M. (1977, June). Group size and others' strategy in an n-person game. Journal of Conflict Resolution, 21(2), 323­338. Gao, Y., Madey, G., & Freeh, V. (2005, April). Modeling and simulation of the open source software community. In Agent-Directed Simulation Conference (pp. 113­122). San Diego, CA. Ghosh, R. A., Krieger, B., Glott, R., & Robles, G. (2002, June). Part 4: Survey of developers. In Free/ libre and open source software: Survey and study. Maastricht, The Netherlands: University of Maastricht, The Netherlands. Jerdee, T. H., & Rosen, B. (1974). Effects of opportunity to communicate and visibility of individual decisions on behavior in the common interest. Journal of Applied Psychology, 59(6), 712­716. Katsamakas, E., & Georgantzas, N. (2007). Why most open source development projects do not succeed? In FLOSS '07: Proceedings of the first international workshop on emerging trends in FLOSS research and development (p. 3). Washington, DC, USA: IEEE Computer Society. Kicinger, R., Arciszewski, T., & De Jong, K. A. (2005). Evolutionary computation and structural design: A survey of the state of the art. Computers and Structures, 83(23-24), 1943-1978. Koch, S. (2008). Exploring the effects of SourceForge.net coordination and communication tools on the efficiency of open source projects using data envelopment analysis. In S. Morasca (Ed.), Empirical Software Engineering: Springer. Kogut, B., & Metiu, A. (2001, Summer). Opensource software development and distributed innovation. Oxford Review of Economic Policy, 17(2), 248-264. Kowalczykiewicz, K. (2005). Libre projects lifetime profiles analysis. In Free and open source software developers' European meeting 2005. Brussels, Belgium. Krishnamurthy, S. (2002, June). Cave or community?: An empirical examination of 100 mature open source projects. First Monday, 7(6). Lerner, J., & Tirole, J. (2005, April). The scope of open source licensing. Journal of Law, Economics, and Organization, 21(1), 20­56. Linux kernel software quality and security better than most proprietary enterprise software, 4-year Coverity analysis finds. (2004). Press release. (Coverity Incorporated) Michlmayr, M. (2005). Software process maturity and the success of free software projects. In K. Zielinski & T. Szmuc (Eds.), Software engineering: Evolution and emerging technologies (p. 3-14). Krakow, Poland: IOS Press. Ostrom, E., Gardner, R., & Walker, J. (1994). Rules, games and common pool resources. Ann Arbor, MI: University of Michigan Press. Raymond, E. S. (2000, September 11). The cathedral and the bazaar (Tech. Rep. No. 3.0). Thyrsus Enterprises. Rossi, M. A. (2004, April). Decoding the "Free/ Open Source(F/OSS) Software puzzle" a survey of theoretical and empirical contributions (Quaderni No. 424). Dipartimento di Economia Politica, Università degli Studi di Siena. Smith, S. C., & Sidorova, A. (2003). Survival of opensource projects: A population ecology perspective. In ICIS 2003. Proceedings of international conference on information systems 2003. Seattle, WA. Smith, T. (2002, October 1). Open source: Enterprise ready ­ with qualifiers. theOpenEnterprise. (http://www.theopenenterprise.com/story/ TOE20020926S0002) Stewart, K. J., Ammeter, A. P., & Maruping, L. M. (2006, June). Impacts of license choice and organizational sponsorship on user interest and development activity in open source software projects. Information Systems Research, 17(2), 126­144. Tajfel, H. (1981). Human groups and social categories: Studies in social psychology. Cambridge, UK: Cambridge University Press. Wagstrom, P., Herbsleb, J., & Carley, K. (2005). A social network approach to free/open source software simulation. In First international conference on open source systems (pp. 16­23). Wang, Y. (2007). Prediction of success in open source software development. Master of science dissertation, University of California, Davis, Davis, CA.

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 13

Weiss, D. (2005). Quantitative analysis of open source projects on SourceForge. In M. Scotto & G. Succi (Eds.), Proceedings of the first international

conference on open source systems (OSS 2005) (pp. 140­147). Genova, Italy.

Nicholas P. Radtke is a PhD candidate in computer science at Arizona State University. His research focuses on understanding and modeling free/libre open source software engineering processes. Marco A. Janssen is assistant professor on formal modeling of social and social-ecological systems within the School of Human Evolution and Social Change at Arizona State University. He is also the associate director of the Center for the Study of Institutional Diversity. His formal training is within the area of operations research and applied mathematics. His current research focuses on the fit between behavioral, institutional and ecological processes. In his research he combines agent-based models with laboratory experiments and case study analysis. Janssen also performs research on diffusion processes of knowledge and information, with applications in marketing and digital media. James S. Collofello is currently computer science and engineering professor and associate dean for the Engineering School at Arizona State University. He received his PhD in computer science from Northwestern University. His teaching and research interests lie in the software engineering area with an emphasis on software quality assurance, software project management and software process modeling and simulation.

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

The Journal of Systems and Software 82 (2009) 1568­1577

Contents lists available at ScienceDirect

The Journal of Systems and Software
journal homepage: www.elsevier.com/locate/jss

Understanding the effects of requirements volatility in software engineering by using analytical modeling and software process simulation
Susan Ferreira a,*, James Collofello b, Dan Shunk c, Gerald Mackulak c
a

Industrial and Manufacturing Systems Engineering Department, The University of Texas at Arlington, Arlington, TX 76019, USA Computer Science and Engineering Department, Arizona State University, Tempe, AZ 76019, USA c Industrial Engineering Department, Arizona State University, Tempe, AZ 76019, USA
b

a r t i c l e

i n f o

a b s t r a c t
This paper introduces an executable system dynamics simulation model developed to help project managers comprehend the complex impacts related to requirements volatility on a software development project. The simulator extends previous research and adds research results from an empirical survey, including over 50 new parameters derived from the associated survey data, to a base model. The paper discusses detailed results from two cases that show significant cost, schedule, and quality impacts as a result of requirements volatility. The simulator can be used as an effective tool to demonstrate the complex set of factor relationships and effects related to requirements volatility. Ó 2009 Elsevier Inc. All rights reserved.

Article history: Available online 19 March 2009 Keywords: Requirements volatility Software process modeling Requirements engineering risk

1. Requirements volatility introduction Requirements volatility refers to growth or changes in requirements during a project's development lifecycle. There are multiple aliases commonly associated with or related to the phenomenon of requirements volatility. These terms include requirements change, requirements creep, scope creep, requirements instability, and requirements churn among others. Costello (1994) provides a relatively detailed set of metrics for requirements volatility. Other simple metrics for requirements volatility define it as the number of additions, deletions, and modifications made to the requirements set per time unit of interest (per week, month, phase, etc.). Requirements volatility, in its various forms, surfaces as a frequent and high impact risk in numerous empirical studies performed to identify risk factors or to understand variables leading to a project's success or failure (examples include Boehm, 1991; Curtis et al., 1988; Houston, 2000; Jones, 1994; Känsälä, 1997; Moynihan, 1997; Ropponen, 1999; Ropponen and Lyytinen, 2000; Schmidt et al., 2001; The Standish Group, 1995; Tirwana and Keil, 2006). Changes to a set of requirements can occur at multiple points during the development process (Kotonya and Sommerville, 1998). These changes can take place ``while the requirements are being elicited, analyzed and validated and after the system has gone into service". Past philosophy dictated that requirements had to be firm by the completion of the requirements phase and
* Corresponding author. Tel.: +1 817 272 1332; fax: +1 817 272 3406. E-mail addresses: ferreira@uta.edu (S. Ferreira), collofello@asu.edu (J. Collofello), dan.shunk@asu.edu (D. Shunk), mackulak@asu.edu (G. Mackulak). 0164-1212/$ - see front matter Ó 2009 Elsevier Inc. All rights reserved. doi:10.1016/j.jss.2009.03.014

that requirements should not change after this time. This view is now understood to be unrealistic (Reifer, 2000). Kotonya and Sommerville (1998) discuss that requirements change is unavoidable. They also indicate that requirements changes do not necessarily imply that poor requirements engineering practice was utilized as requirements changes could be the result of a combination of factors. The term ``requirements engineering" refers to the processes required to generate and maintain the software requirements throughout the duration of the project. Concern for the effects of requirements volatility is not usually associated with the front end of the process, for example, during requirements definition. Volatility during the requirements definition phase is expected because this is when requirements are being created. However, once the design process begins, the impact of requirements change is progressively greater due to the additional investment in time and effort as the project continues to generate artifacts and complete required tasks. Additions or modifications may need to be made to previously generated or in process project artifacts and additional time investment or scrapped effort can result. Due to the additional unplanned effort, severe consequences can potentially occur, including significant cost and schedule overruns, and at times, cancelled projects. The impact of changing requirements during later phases of a project and approaches for assessing the impacts of these changes has been well documented (Yau et al., 1978, 1986, 1988; Yau and Kishimoto, 1987; Yau and Liu, 1988). In an agile software development environment, changes are welcomed throughout the development process (Beck et al., 2001). The target of this article is not agile type projects but more traditional development type projects.

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

1569

2. The need for a requirements volatility assessment tool The effects of requirements volatility have been discussed in the literature for some time. However, little empirical research has been carried out on the topic of requirements volatility that considered the factors involved and the integrated quantitative effects of requirements volatility on factors related to key project management indicators (cost, schedule, and quality). A relatively small number of studies consider requirements volatility and its associated effects, especially in a manner integrated with other software project management factors. These existing studies primarily fall into a few major research method categories: survey or software assessment based research (Jones, 1998, 1994; Lane, 1998; Nidumolu, 1996; Zowghi et al., 2000; Zowghi and Nurmuliani, 2002), interviews and case studies (Javed et al., 2004; Loconsole and Börstler, 2005, 2007; Nurmuliani et al., 2004; Zowghi and Nurmuliani, 1998), regression analysis (Stark et al., 1999), reliability growth model (Malaiya and Denton, 1999), analytic hierarchy process analysis (Finnie et al., 1993), and simulation models (Houston, 2000; Lin and Levary, 1989; Lin et al., 1997; Madachy et al., 2007; Madachy and Tarbet, 2000; Pfahl and Lebsanft, 2000; Smith et al., 1993). The existing simulation models discussed in the literature were developed and tailored for one organization, have a limited view of requirements volatility or requirements engineering, or do not include requirements engineering processes considered in concert with the rest of the lifecycle or other critical project factors. A paucity of the literature exists on process modeling and simulation work performed in requirements engineering, an area now receiving more focused attention because of the impact that it has on the rest of the systems and software engineering lifecycle. The limited research and relative importance of requirements volatility as a risk and the relatively sparse level of requirements engineering process modeling led the researchers to more analysis and examination of these areas. A system dynamics process model

simulator, the Software Project Management Simulator (SPMS) that includes data which is stochastically based on industry survey data distributions, was then developed as part of a doctoral dissertation (Ferreira, 2002). SPMS illustrates a software business model that considers the effects of requirements volatility on a software project's key management parameters: cost, schedule, and quality. SPMS presents a more comprehensive and detailed view of the researched areas than previous models. The development of the SPMS simulator and associated results developed in this paper are discussed in this journal article. 3. Requirements volatility tool development process This section of the paper briefly discusses the research method used to develop the simulation model. Key research questions addressed in the initial research study include: (1) Which software factors are affected by requirements volatility? (2) How can these factors and the uncertainty associated to these factors be modeled? and (3) What is the project management impact of requirements volatility? Fig. 1 provides a summary view of the processes used during the research effort. Starting with the figure's top left and top right sides and flowing down, the figure illustrates that two efforts (one per figure side) were initiated concurrently and these efforts flowed into the development of the software process simulator discussed in this paper. A rigorous review of the requirements engineering and requirements volatility related literature was performed. Various process and information models were created to represent and assist in analysis and synthesis of the knowledge gained during the literature review. Requirements engineering process models and workflows, an information model, and a causal model were developed prior to the simulator development. Relevant factors and associated relationships were identified based on analysis of the literature review material and discussions with software engineering experts. Further analysis of the captured information led to the

Fig. 1. Research method.

1570

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

development of a causal model (Fig. 2). The causal model is important because it illustrates the cause and effect relationships between software development factors related to requirements volatility and includes hypothesized relationships between factors. The causal model was iteratively developed based on fundamental factor relationships (for example, job size and overall project effort), researcher industry and academic experience, and hypothesized relationships. The figure highlights (blue shading) factors and associated relationships (blue lines) that were further explored during the research effort. This causal model is expected to evolve over time as more relationships are explored and understood. One of the proposed solutions to address the identified research questions was to develop a software process simulator. A simulator was selected because it provides a tool for software project managers and researchers to perform ``what if" analyses, and enables users to examine the risk of various levels of requirements volatility and determine project outcomes. A simulator can represent the complexity of relationships between large quantities of interrelated factors and effectively illustrates the effects and impact of requirements volatility. A simulator with a graphical icon format was chosen to represent project factors and relationships. The previously developed causal model was used to develop the simulator. A subset of the previously generated causal model relationships and factors were selected and modeled. A subset of the causal model relationships and factors was chosen because follow-on work needed to be limited in length to allow key factor data to be collected or these areas were already modeled in the pre-existing simulator that was extended to create the new simulator, SPMS. Concepts and constructs from all of the generated process and information models contributed to the creation of the simulator's workflows and other model sectors. Joint research was performed with Daniel Houston to characterize four deterministic software process simulators using statistical design of experiments (Houston et al., 2001). Results of this experimentation work fed into Dan Houston's development of the Software Process Actualized Risk Simulator (SPARS) (Houston, 2000). The SPMS research simulation model evolved from Houston's SPARS model. The SPARS model also represents an adaptation, as it reuses or modifies large portions from Abdel-Hamid and Madnick's model (1991) and Tvedt's model (1996) and then extends the consolidated model to incorporate effects of a number of risk factors. The SPARS model was selected because of its comprehensive software project management scope and updates for more modern development practices. Reusing components from SPARS facilitated the development of the SPMS model in that common constructs did not need to be recreated and the model used previously validated simulator components. As part of the research effort, the SPARS model was modified to eliminate some unnecessary factors and extended to create the research model, SPMS. As the initial simulator sector designs were generated, walkthroughs were conducted with an initial set of individuals who were familiar with the research. Once an initial version of the model containing the key constructs was completed, a secondary walkthrough of the model was held with four reviewers outside of the research group. These reviewers included representatives from industry and academia that were currently performing research in requirements engineering and/or software process modeling and simulation. Following the model walkthroughs, the simulator was modified to incorporate reviewer comments and suggestions. The model was then ready to include quantitative data. Many of the model variables required data that was not available in the literature. In order to populate these model parameters, a survey was developed and administered to collect the needed data. The Project Management Institute's Information Systems

Specific Interest Group (PMI-ISSIG) sponsored the survey by providing the host site for the web-based survey and sending notifications about the survey to its members. Although PMI-ISSIG was the primary target population for the survey, one mailing was sent to individual Software Engineering Institute (SEI) Software Process Improvement Network (SPIN) group contacts within the United States and to individual professional contacts. Three hundred twelve software project managers and other software development personnel submitted responses for the survey. Survey results indicated that 78% of the respondents experienced some level of requirements volatility on their project. The survey findings highlight that requirements volatility can increase the job size dramatically, extend the project duration, cause major rework, and affect other project variables such as morale, schedule pressure, productivity, and requirements error generation. Fig. 3 shows an example of requirements volatility related data from the survey. The histogram in the figure depicts how requirements change affected the job size as a percent of the original job size. In the vast majority of cases, requirements volatility increased the job size. However, one can see that there were also cases where there was no change or a net decrease in the job size. Survey respondents had an average of 32.4% requirements volatility related job size increases. Other captured volatility effects included significant increases in project rework and reduced team morale. The survey also captured effects from schedule pressure. As the resource effort increases due to requirements volatility (to address job size additions and rework), schedule pressure also increases. The survey data showed increases in requirements error generation as the schedule pressure increases. These effects cause consequences leading to impacts on key project management indicators such as cost, schedule, and quality. More details on the survey findings showing primary and secondary effects of requirements volatility are addressed in Ferreira (2002). Statistical analysis of the survey responses allowed the generation of stochastic distributions for many of the simulation model's requirements volatility and requirements engineering factors and relationships. The model's stochastic inputs are primarily generated using either empirical discrete distributions derived from analyzing histograms of the data or are generated using the inverse transform method (Abramowitz and Stegun, 1964). The selection of the type of distribution depended on the survey analysis results. Using these stochastic inputs, the simulation model's random variates use a random number as an input to generate the desired distribution. Based on the sampling frequency, the distributions are sampled once per run or are sampled continuously throughout a run to be drawn as necessary. Once the derived stochastic factor distributions and relationships were added to the simulator, verification and validation (V&V) exercises were performed. Model validation determines whether a model is a ``useful or reasonable representation of the system" (Pritsker et al., 1997). Verification checks that the simulation model runs as intended. Matko et al. (1992) indicate that modeling work is not an exact science since the real system is never completely known. Given this situation, the validation and verification effort primarily focused on building confidence in the model as a reasonable representation of the system and in its usefulness in the provision of results. The overall approach for verification and validation included tests of the structure and behavior of the model. This strategy follows a framework of guidelines presented by Richardson and Pugh (1981) that builds confidence in the model and its results. The tests focus on suitability and consistency checks. The model verification and validation activities were performed by the model developer, and various software process and project experts. Additional verification work was performed in order to understand differences between the SPMS and SPARS model when the

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

1571

Fig. 2. Causal model.

risk factors are not actualized. Differences between the models in this case were relatively small. More information about this verification work is discussed in Ferreira et al. (2003). 4. Assessment tool capability and use SPMS illustrates researched effects of requirements volatility and includes requirements engineering extensions to the SPARS

software project management model. The SPARS model was modified to eliminate some unnecessary factors and extended to create the research model. Major additions to the base model include the researched results for the effects of requirements volatility and significant extensions to add and support the requirements engineering portions of the software development lifecycle. SPMS encompasses the requirements engineering through test phases of the software development lifecycle. Requirements volatility

1572

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

Fig. 3. Distribution of requirements volatility related percent change in job size.

starts and is stochastically simulated during the project's development and test phases. The model workflows also cover the entry of change requests, change request analysis and review activities, and their disposition (accepted, rejected/deferred).

Table 1 Model classification. Purpose Scope Model approach Planning, understanding, process improvement Medium to large size project, short to long duration, one product/ project team Continuous, mixed mode (stochastic and deterministic variables), iThinkTM simulation tool

SPMS demonstrates causal model effects of requirements volatility. Survey findings showing the impact of requirements volatility on increasing software project job size (this occurs a majority of the time), increased rework, and lowered staff morale are represented in the model using stochastic relationships derived during the survey data analysis. Over 50 new parameters that used distributions derived from the survey data were added to the model. In addition to these survey drawn distributions, a significant number of distributions were reused from other sources, or parameters were modeled using single point inputs that could be changed by a user. The effects of lowered morale on requirements engineering productivity and requirements error generation are represented in the model. Schedule pressure effects on requirements error generation were also studied as part of the survey data analysis and were added to pre-existing schedule pressure effects in the model. Among other survey data used in the simulator, the model includes requirements defect detection effectiveness for various software development activities or milestones and relative work rates of requirements volatility related activities compared to their normal work rate. Other model contributions include the addition of a requirements engineering staff type and requirements engineering support activities which were added to pre-existing simulator development and test personnel and activities. Table 1 presents a view of the model's classification, according to the characterization framework from Kellner et al. (1999). The typical model audience is expected to be software development project managers or researchers seeking to gain an understanding of requirements engineering and requirements volatility and its effects integrated with other software project risks. The model is relatively complex, given its purpose, and assumes a sophisticated

Table 2 Model sector descriptions. Sector name Change request work flow Requirements work flow Development and test work flow Description Stochastic change request entry over 10 intervals. Incoming change request (CR) analysis, change request control board (CCB) review, and disposition Requirements generation and review process including requirements error and defect rework. Stochastic entry of requirements volatility related project scope changes and rework over 10 intervals Work product flow through the development lifecycle, from design and code through testing and rework of design and code errors. Work is pulled from development and test activities for requirements volatility related rework and reduction and for rework of requirements defects A support sector that calculates the amount of product to add to the product cycle for requirements volatility additions based on survey data A support sector that calculates the amount of product to pull from the development and test work for requirements volatility related rework based on survey data A support sector that calculates the amount of product to pull from development and testing work flow activities for requirements related reductions. Includes policy choice that defines where to remove product Entry and summation of requirements engineering, developer, and tester planned staffing profile information Allocation of requirements engineer effort to requirements engineering activities based on activity priority Allocation of developer and tester effort to project activities based on activity priority Generation and detection of requirements errors and defects Generation and detection of design and code errors and defects Entry and exit of staff based on planned staffing profiles, assimilation of new staff, attrition, and replacements. Entry of contract personnel and organizationally experienced personnel handled separately for the different staff groups Attrition (including attrition due to low morale) and replacement calculations for requirements engineers and the grouped set of developers and testers Calculation of requirements engineer and the grouped developer and tester work force levels needed based on willingness to hire Calculation of effort perceived still needed to complete project, effort shortages, schedule pressure, and assumed requirements engineer, developer, tester productivity. Calculations to determine start of development and testing Adjustment of job effort based on requirements volatility related additions, rework, and reductions as well as from underestimation Productivity inputs and calculation of project activity productivity based on productivity multipliers Work rate modification due to effort remaining, effort required, and staff exhaustion. Generation of two staff type productivity multipliers (including learning, communication overhead, staff experience, and morale) Calculation of requirements engineering and development and testing progress. Modification of currently perceived job size based on requirements volatility and discovered work due to underestimation Adjustment of staffing and schedule multipliers based on senior management commitment. This is the only sector that was not modified from the original SPARS model

Requirements change additions Requirements change rework Requirements change reductions Planned staffing Requirements engineer effort allocation Developer and tester effort allocation Requirements quality management Development and test quality management Actual staffing Attrition and replacement Planning Control Adjustment of job effort Productivity inputs Productivity Progress measure Senior management commitment

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

1573

user that is educated on the use of simulation models and software development project management. The model is practically based, relying on a significant and proven foundation of software project management and simulation research. The model is segmented into 20 sectors. The sectors are organized into convenient and logical groupings of related factors. Table 2 provides an overview of the model sectors with a brief description of each in order to give the reader an introduction to the model's scope. An example of one sector excerpted from the model is shown in Fig. 4. The view illustrates the requirements engineering work flow sector. The connections on the right of the sector flow into or out of the development and test work flow sector (sector not shown). The requirements work flow sector encompasses a normal product work flow. Requirements are generated and reviewed during the normal product work flow. The normal requirements work

flow begins at the initiation of the requirements engineering phase, at the To_Be_Worked stock. The To_Be_Worked stock is initially populated with the estimated starting job size. The work then flows through the Generated_Reqts, Reqts_Awaiting_Review, Reviewed_Reqts stocks, and then into the development process as an inflow. The requirements generation activities are aggregated, encompassing requirements elicitation, analysis, negotiation, and initial requirements management. Once reviewed, the requirements product is dispositioned and defective product containing requirements errors is removed from the normal work flow and becomes part of the requirements error rework work flow, to be reworked before flowing into the development and test activities. Requirements defects caught post the requirements phase come back into the requirements defect rework work flow to be reworked. The reworked product then flows back into the development and test activities. Additions to job size due to volatility

Fig. 4. SPMS simulator requirements engineering work flow sector.

1574

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

and underestimation flow into the normal work flow through the course of the project. The lower half of the sector includes the requirements change rework work flow. Rework due to requirements volatility is drawn from the development and test work flows and is worked though the requirements change rework work flow. This work flow contains separate requirements error related activities. The model allows the user to enter detailed inputs related to their project. Other data is automatically extracted from the stochastic distributions derived from the survey data. Table 3 provides a listing of a subset of the new inputs added to the model to provide a flavor for the types of data required from the user. These inputs can be input per run or a set of runs. Table 4 identifies the new

survey distributions added to the model. Data from the survey is extracted from stochastic distributions with timing as defined in Table 4. ``1 Selection per run" means that a value is pulled from the stochastic distribution one time per run and used throughout the run. While the data listed in Tables 3 and 4 do not include all the new or modified factors in the model it does provide a perspective of the type of data that the user can enter and use. 5. Assessment tool results The simulator was used to run two cases for the purpose of comparing them. Each case was setup for 100 runs apiece. The set of 100 runs for each case was selected for convenience as the modeling tool allows additional runs, if desired. The two cases are as follows: (1) A baseline case without the requirements volatility actualized [baseline] and (2) a case with the requirements volatility related factors actualized [reqts volatility]. The data in the square brackets corresponds to the case identifier in later figures. Actualizing the requirements volatility risk for the second case allows the model to represent the stochastic researched effects of requirement volatility. These include survey-based effects related to requirements additions, changes, and modifications as well as entering change requests. Also included, among the other effects, are morale, and related schedule pressure effects on morale. The initial job size was estimated to be 400 function points. The original schedule estimate (planned duration) was defined to be 408 days. Figs. 5­8 depict the differences, at the completion of the projects, between the baseline runs (no requirements volatility considered) and runs with the other case where the requirements volatility risk is actualized for various summary outputs. The model allows additional detailed outputs, if desired. Box plots were used to represent the data because the simulation results were positively skewed given the tendency for higher project size, cost, duration, and released defects (among other outputs) with the actualization of the requirements volatility risk. The box plots provide a graphical display of the center and the variation of the data, allowing one to see the symmetry of the data set (Ott, 1988). With the exception of the final project size (Fig. 5), the baseline results show a small level of variability because some of the model parameters (e.g. requirements engineering process factors) were modeled stochastically. Therefore, even when the requirements volatility risk is not actualized, some variability in results will appear.

Table 3 New factor user inputs (subset only). Factor Quantity of experienced requirements engineers (REs), per interval Quantity of new requirements engineers, per interval Quantity of experienced personnel allocated to training new reqts engineers (%) RE transfer in day (days) Experienced organization RE transfer quantity Time delay to transfer REs off project (days) Max quantity of new RE hires per experienced staff (staff/staff) Time for organization experienced REs (but not project experienced) to be productive (days) Project day that person in org is scheduled to come onto project (day) Quantity of RE staff experienced in the organization who are to be transferred on project (staff) Requirements review adjustment policy ­ adjusts review effort (boolean switch) Reworked requirements review adjustment policy (boolean switch) Requirement error bad fix fraction (%) Requirements defect bad fix fraction (%) Nominal change request analysis productivity (function points/person-day) Nominal requirements generation productivity (function points/person-day) Nominal requirements review productivity (function points/person-day) Requirements volatility (boolean switch) Input quantity 10 (1 for each of 10 intervals) 10 (1 for each of 10 intervals) 1 1 1 1 1 1 1 1 1 Selection 1 Selection 1 1 1 1 1 1 Selection

Table 4 New survey data distributions. Factor Quantity of requirements change requests, per interval (intervals 1­10) Change request time span with requirements volatility (ratio of duration) Determination of change request acceptance/deferral due to schedule pressure (ratio) Relative requirements defect rework productivity (ratio) Relative requirements generation rework productivity (ratio) Relative requirements change error rework productivity (ratio) Relative requirements change requirement review productivity (ratio) Relative design and code requirements defect rework productivity (ratio) Percentage of perceived job size increased due to requirements volatility, per interval (%) Percentage of perceived job size reworked due to requirements volatility, per interval (%) Percentage of perceived job size reduced due to requirements volatility, per interval (%) Requirements review effectiveness (%) Design and code requirement defect detection effectiveness (%) Design and code review requirement defect detection effectiveness (%) Test requirement defect detection effectiveness (%) Reworked requirements review effectiveness (%) Reworked requirements design and code defect detect effectiveness (%) Requirements error multiplier for schedule pressure (ratio) Requirements error multiplier for morale (ratio) Selection frequency 10 Selections per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 10 Selections per run 10 Selections per run 10 Selections per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run (1 selection for each interval)

(1 selection for each interval) (1 selection for each interval) (1 selection for each interval)

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

1575

Fig. 5. Project size box plots.

Fig. 7. Project duration box plots.

Fig. 8. Project released defect density box plots. Fig. 6. Project cost box plots. Table 5 Case study simulation results. Output/Case Average Median 400 485.7 4119.0 5968.9 452.0 608.5 Std. Dev. 0 82.0 35.9 1398.1 4.3 132.8 Minimum 400 382.0 3969.5 3988.5 442.0 444.0 0.726 0.747 Maximum 400 788.3 4155.5 11196.1 461.0 1101.0 0.792 1.170

Fig. 5 illustrates the difference in project size between the baseline case and the requirements volatility case. The baseline size is 400 function points. The average size for the requirements volatility case is 499.9 function points with a standard deviation of 82.0, and a range from 382.0 to 788.3 function points over the 100 runs. The runs that have values below the baseline 400 function points (e.g. 382.0) can be explained by the fact that requirements changes do not always add additional project scope, but can be made to remove scope or unnecessary requirements. However, 93 of the 100 runs had a final project size greater than the base case of 400 functions points with relatively large scope added in the course of the project. This scope addition has a significant negative ripple effect on the project cost (effort in person-days) and project duration. Table 5 contains the detailed statistics for each case including average, median, standard deviation calculations and the minimum and maximum run values. Fig. 6 presents the results for cost. Cost is considered to be human effort consumed during the project in person-days. This cost is equivalent to effort and does not include other project costs. The effort or cost presented in the figure encompasses the cumulative effort for the requirements engineering through test activities.

Size (function points) Baseline 400 Reqts volatility 499.9 Cost (effort in person days) Baseline 4107.6 Reqts volatility 6208.2 Duration (days) Baseline Reqts volatility 452.4 634.0

Released defect density (defects per function point) Baseline 0.753 0.753 0.013 Reqts volatility 0.894 0.886 0.075

The detailed statistics are presented in Table 5. As mentioned earlier, the baseline results do show a small level of variability because some of the model parameters are represented stochastically. The average for the requirements volatility case is over 2000 person-days more than the average for the baseline case.

1576

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

Considering that the baseline cost is an average of 4107.6 person days, this is a very large number for a program manager to justify as it represents more than a 50% increase in resource costs. The cost range for the requirements volatility case is very large as well. The requirements volatility case has a maximum of 11,196.1 person-days. This represents a very significant difference from the baseline maximum. Fig. 7 displays the results for project duration in days. The duration encompasses the requirements engineering through test activities for the project. The detailed statistics for the two cases are shown in Table 5. It takes an average of 452.4 days to complete the project for the baseline case. The average for the requirements volatility case is significantly higher at an average of 634.0 days. As in the case of the cost, the range for the requirements volatility case has a wide span. Fig. 8 presents the project released defect density box plots. The data represents the defects released post the test process and is in units of defects per function point. The quantity of released defects is also significantly higher for the requirements volatility case. The baseline case has an average of 0.753 defects per function point. The average for the requirements volatility case is 0.894. The range span for this case is also relatively large. For the case that include the risk of requirements volatility, the box plots for each parameter have a wide span. The wide span in outcomes indicates a large potential for unpredictable results as well as considerable impact on project results. A comparison of the model results for projects with no requirements volatility and those with requirements volatility show significant differences in the box plots. One can clearly see the potential impact of requirements volatility on key project management parameters such as cost (effort), schedule, and quality. Differences between the baseline and requirements volatility case results can be attributed to the addition (or reduction) in project scope caused by requirements volatility, cause and effect relationships between factors, and the stochastic representation of a number of the factors.

In addition to simulating the effects of requirements volatility, SPMS offers the ability to simulate various project scenario combinations starting with the requirements engineering portion of the lifecycle. The model offers a robust set of parameters that capture various facets of the project including requirements errors and defects, requirements defect density, defect containment effectiveness for various milestones, and other parameters integrated with a very comprehensive development and test related set of factors and relationships. Each of the model distributions can be easily calibrated and tailored to a specific organization's environment. All the other factors are also setup to be readily modifiable to reflect an organization's historical data. Survey data used in the simulator captures information on factors and relationships not previously available from a wide population in the software industry and allows modeling variables stochastically given the large quantity of survey responses. Many of the model variables were modeled stochastically to allow the user to assess project outcomes probabilistically. The model results quantitatively illustrate the potential for significant cost, schedule, and quality impacts related to requirements volatility. Software project managers can use this tool to better understand the effects of requirements volatility and this risk in concert with other common risks. The simulator can be used to assess potential courses of action to address the risk of requirements volatility.

7. Future research Additional research in the area of requirements engineering, as it continues to evolve, is expected to provide a continuous stream of new ideas, perspectives, and information that can allow for the development of richer models and that can represent different facets of understanding in this under-represented yet critical research area. More work needs to be done to model the impact of requirements engineering related processes and policies so that project managers and software development personnel are more aware of the impact of the decisions they make during this phase and how the rest of the lifecycle may be affected by their choices. This work can lead to the further identification of best practices and generate insights valuable to managing software development projects in the future. As this and other simulators that incorporate requirements engineering processes and relationships continue to evolve, additional experimentation with the models may prove valuable in identifying common factors and relationships. The research model presented in this paper is relatively large and complex. Sensitivity analysis can assist in the identification of influential factors in this and other models. The results of further experimentation may be used to ``prune" insignificant factors from the model so that a more efficient model can result (Houston et al., 2001). One of the benefits of a simpler model includes a shortened training and learning ramp-up time. Core concepts that make the most difference in results can be emphasized. Since the model is less complex it becomes easier to understand and may perhaps be more popular given the reduced time to understand and then populate the model with organizational and project specific data. Maintenance time may also be reduced because the model is smaller and it is easier to find and fix problems. Agile processes which welcome requirements changes present another valuable area to study. As these processes continue to mature and more quantitative results are available, simulating different types of agile processes, (e.g. XP, Scrum, etc.) would be of interest to determine the project management ramifications related to cost, schedule, and quality as compared to more traditional approaches.

6. Summary and conclusions Key research questions were addressed as part of the research. These questions include: (1) Which software factors are affected by requirements volatility? (2) How can these factors and the uncertainty associated to these factors be modeled? and (3) What is the project management impact of requirements volatility? A causal model was developed and used to evaluate which software factors are affected by requirements volatility. A survey was administered to collect information for a subset of the factors identified in the causal model. This allowed the researchers to verify the relationships and quantify the level of the relationships. A simulator was chosen as the means to model how the factors relate to other project factors and to represent the uncertainty associated to the factors using stochastic distributions derived from survey data. The SPMS model assists in understanding and evaluating the program management impact of requirements volatility. This simulator can help an interested user to better understand the requirements engineering process and the impact of requirements volatility via its process-oriented workflows and comprehensive scope. The factor relationships represented in the model represent a significant contribution. This work expands our understanding of the requirements engineering process and the effects of requirements volatility. The rigorous research to both understand and leverage the previous foundation of knowledge in requirements engineering and requirements volatility and the thorough nature of the survey and analysis of this valuable data to assess factor relationships and populate the simulator with empirical data is a significant contribution.

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

1577

References
Abdel-Hamid, T., Madnick, S., 1991. Software Project Dynamics: An Integrated Approach. Prentice-Hall, Englewood Cliffs, NJ. Abramowitz, M., Stegun, I.A. (Eds.), 1964. Handbook of Mathematical Functions, Applied Mathematics Series 55. National Bureau of Standards, Washington, DC. Beck, K., Beedle, M., van Bennekum, A., Cockburn, A., Cunningham, W., Fowler, M., Grenning, J., Highsmith, J., Hunt, A., Jeffries, R., Kern, J., Marick, B., Martin, R., Mellor, S., Schwaber, K., Sutherland, J., Thomas, D., 2001. Principles Behind the Agile Manifesto. Retrieved 11.6.2008 from: <http://www.agilemanifesto.org/ principles.html>. Boehm, B.W., 1991. Software risk management: principles and practices. IEEE Software 8 (1), 32­41. Costello, R.J., 1994. Metrics for Requirements Engineering. Master of Science Thesis, California State University, Long Beach. Curtis, B., Krasner, H., Iscoe, N., 1988. A field study of the software design process for large systems. Communications of the ACM 31 (11), 1268­1287. Ferreira, S., 2002. Measuring the Effects of Requirements Volatility on Software Development Projects, Ph.D. Dissertation, Arizona State University. Ferreira, S., Collofello, J., Shunk, D., Mackulak, G., Wolfe, P., 2003. Utilization of Process Modeling and Simulation in Understanding the Effects of Requirements Volatility in Software Development. In: Proceedings of the 2003 Process Simulation Workshop (ProSim 2003). Finnie, G.R., Witting, G.E., Petkov, D.I., 1993. Prioritizing software development productivity factors using the analytic hierarchy process. Journal of Systems and Software 22 (2), 129­139. Houston, D.X., 2000. A Software Project Simulation Model for Risk Management, Ph.D. Dissertation, Arizona State University. Houston, D.X., Ferreira, S., Collofello, J.S., Montgomery, D.C., Mackulak, G.T., Shunk, D.L., 2001. Behavioral characterization: finding and using the influential factors in software process simulation models. Journal of Systems and Software 59 (3), 259­270. Javed, T., Maqsood, M., Durrani, Q.S., 2004. A study to investigate the impact of requirements instability on software defects. ACM SIGSOFT Software Engineering Notes 29 (3), 7. Jones, C., 1994. Assessment and Control of Software Risks. PTR Prentice-Hall, Inc., Englewood Cliffs, NJ. Jones, C., 1998. Estimating Software Costs. McGraw-Hill, New York. Känsälä, K., 1997. Integrating risk assessment with cost estimation. IEEE Software 14 (3), 61­67. Kellner, M.I., Madachy, R., Raffo, D.M., 1999. Software process modeling: why? what? how? Journal of Systems and Software 46 (2­3), 91­105. Kotonya, G., Sommerville, I., 1998. Requirements Engineering: Processes and Techniques. John Wiley and Sons, Ltd.. Lane, M.S., 1998. Enhancing software development productivity in Australian firms. In: Proceedings of the Ninth Australasian Conference on Information Systems (ACIS '98), vol. 1, pp. 337­349. Lin, C.Y., Abdel-Hamid, T., Sherif, J.S., 1997. Software-engineering process simulation model (SEPS). Journal of Systems and Software 38 (3), 263­277. Lin, C.Y., Levary, R.R., 1989. Computer aided software development process design. IEEE Transactions on Software Engineering 15 (9), 1025­1037. Loconsole, A., Börstler, J., 2005. An industrial case study on requirements volatility measures. In: Proceedings of the 12th Asia-Pacific Software Engineering Conference (APSEC '05), 8 p. Loconsole, A., Börstler, J., 2007. Are size measures better than expert judgment? An industrial case study on requirements volatility. In: Proceedings of the 14th Asia-Pacific Software Engineering Conference (APSEC '07), pp. 238­245. Madachy, R., Tarbet, D., 2000. Initial experiences in software process modeling. Software Quality Professional 2 (3), 1­13. Madachy, R., Boehm, B., Lane, J., 2007. Assessing hybrid incremental processes for SISOS development. Software Process: Improvement and Practice 12 (5), 461­ 473. Malaiya, Y.K., Denton, J., 1999. Requirements volatility and defect density. In: Proceedings of the 10th International Symposium on Software Reliability Engineering, pp. 285­294. Matko, D., Zupancic, B., Karba, R., 1992. Simulation and Modeling of Continuous Systems: A Case Study Approach. Prentice-Hall International Ltd., Great Britain. Moynihan, T., 1997. How experienced project managers assess risk. IEEE Software 14 (3), 35­41. Nidumolu, S.R., 1996. Standardization, requirements uncertainty and software project performance. Information and Management 31 (3), 135­150. Nurmuliani, N., Zowghi, D., Fowell, S., 2004. Analysis of requirements volatility during software development life cycle. In: Proceedings of the 2004 Australian Software Engineering Conference (ASWEC '04), pp. 28­37. Ott, L., 1988. An Introduction to Statistical Methods and Data Analysis. PWS-KENT Publishing Company. Pfahl, D., Lebsanft, K., 2000. Using simulation to analyze the impact of software requirements volatility on project performance. Information and Software Technology 42 (14), 1001­1008. Pritsker, A. Alan B., O'Reilly, Jean J., LaVal, David K., 1997. Simulation with Visual SLAM and AweSim. System Publishing Company, West Lafayette, IN. Reifer, D.J., 2000. Requirements management: the search for Nirvana. IEEE Software 17 (3), 45­47. Richardson, George, P., Alexander, L., Pugh III, 1981. Introduction to System Dynamics Modeling with DYNAMO. The MIT Press, Cambridge, MA.

Ropponen, J., 1999. Risk assessment and management practices in software development. Chapter 8 in Beyond the IT Productivity Paradox. John Wiley and Sons. pp. 247­266. Ropponen, J., Lyytinen, K., 2000. Components of software development risk: how to address them? A project manager survey. IEEE Transactions on Software Engineering 26 (2), 98­112. Schmidt, R., Lyytinen, K., Keil, M., Culle, P., 2001. Identifying software project risks: an international Delphi study. Journal of Management Information Systems 17 (4), 5­36. Smith, B.J., Nguyen, N., Vidale, R.F., 1993. Death of a software manager: how to avoid career suicide through dynamic software process modeling. American Programmer 6 (5), 10­17. The Standish Group, 1995. The Chaos Report. Obtained from <http://www. standishgroup.com/chaos.html>. Stark, G.E., Oman, P., Skillicorn, A., Ameele, A., 1999. An examination of the effects of requirements changes on software maintenance releases. Journal of Software Maintenance: Research and Practice 11 (5), 293­309. Tirwana, A., Keil, M., 2006. Functionality risk in information systems development: an empirical investigation. IEEE Transactions on Engineering Management 53 (3), 412­425. Tvedt, J.D., 1996. An Extensible Model for Evaluating the Impact of Process Improvements on Software Development Cycle Time. Ph.D. Dissertation, Arizona State University. Yau, S.S., Collofello, J.S., MacGregor, T., 1978. Ripple effect analysis of software maintenance. In: Proceedings of the IEEE Computer Society's Second International Computer Software and Applications Conference (COMPSAC '78), pp. 60­65. Yau, S.S., Kishimoto, Z., 1987. A method for revalidating modified programs in the maintenance phase. In: Proceedings of 11th IEEE International Computer Software and Applications Conference (COMPSAC `87), pp. 272­277. Yau, S.S., Liu, C.S., 1988. An approach to software requirement specifications. In: Proceedings of 12th International Computer Software and Applications Conference (COMPSAC `88), pp. 83­88. Yau, S.S., Nicholl, R.A., Tsai, J.P., 1986. An evolution model for software maintenance. In: Proceedings of 10th IEEE International Computer Software and Applications Conference (COMPSAC `86), pp. 440­446. Yau, S.S., Nicholl, R.A., Tsai, J.P., Liu, S.S., 1988. An integrated life-cycle model for software maintenance. IEEE Transactions on Software Engineering 14 (8), 1128­1144. Zowghi, D., Nurmuliani, 1998. Investigating requirements volatility during software development: research in progress. In: Proceeding of the Third Australian Conference on Requirements Engineering (ACRE98), pp. 38­48. Zowghi, D., Nurmuliani, 2002. A study of the impact of requirements volatility on software project performance. In: Proceedings of the Ninth Asia-Pacific Software Engineering Conference (ASPEC '02), pp. 3­11. Zowghi, D., Offen, R., Nurmuliani, 2000. The impact of requirements volatility on the software development lifecycle. In: Proceedings of the International Conference on Software Theory and Practice (IFIP World Computer Congress), pp. 19­27.

Susan Ferreira is an Assistant Professor and the founding Director of the Systems Engineering Research Center (SERC) at The University of Texas, Arlington (UTA). Before joining UTA, Dr. Ferreira worked as a systems engineer in the Defense industry on complex software intensive systems. Her industry background includes work for Lockheed Martin, General Dynamics/Motorola, and Northrop Corporation. Her teaching and research interests are related to systems engineering (SE) and include requirements engineering, SE process modeling and simulation, lean SE, SE return on investment, SE cost estimation, and system of systems engineering. James Collofello is currently Associate Dean for the Engineering School and Professor of Computer Science and Engineering at Arizona State University. He received his Ph.D. in Computer Science from Northwestern University. His teaching and research interests lie in the software engineering area with an emphasis on software project management, software quality assurance and software process modeling. In addition to his academic activities, he has also been involved in applied research projects, training and consulting with many large corporations over the last 25 years. Dan Shunk is the Avnet Professor of Supply Network Integration in Industrial Engineering at Arizona State University. He is currently pursuing research into collaborative commerce, global new product development, model-based enterprises and global supply network integration. He won a Fulbright Award in 2002-2003, the 1996 SME International Award for Education, the 1991 and 1999 I&MSE Faculty of the Year award, the 1989 SME Region VII Educator of the Year award, chaired AutoFact in 1985, and won the 1982 SME Outstanding Young Engineer award. Dr. Shunk studied at Purdue where he received his Ph.D. in Industrial Engineering in 1976. Gerald Mackulak is an Associate Professor of Engineering in the Department of Industrial, Systems and Operations Engineering at Arizona State University. He is a graduate of Purdue University receiving his B.Sc., M.Sc., and Ph.D. degrees in the area of Industrial Engineering. His primary area of research is simulation methodology with a focus on model abstraction, execution speed and output analysis. He has authored over 75 articles, served as an associate editor for two simulation journals and continues to guide research in simulation efficiency.

Using Simulation to Facilitate the Study of Software Product Line Evolution1
Yu Chen, Gerald C. Gannod2, James S. Collofello, and Hessam S. Sarjoughian   Dept. of Computer Science and Engineering Division of Computing Studies Arizona State University ­ Tempe Campus Arizona State University ­ Polytechnic Campus Tempe AZ 85287, USA Mesa AZ 85212, USA {yu_chen, gannod, collofello, sarjoughian}@asu.edu Abstract
A product line approach is a disciplined methodology for strategic reuse of source code, requirement specifications, software architectures, design models, components, test cases, and the processes for using the aforementioned artifacts. Software process simulation modeling is a valuable tool for enabling decision making for a wide variety of purposes, ranging from adoption and strategic management to process improvement and planning. In this paper, discrete event simulation is used to provide a framework for the simulation of software product line engineering. We have created an environment that facilitates strategic management and long-term forecasting with respect to software product line development and evolution. simulator that facilitates software product line decision making at an early stage by providing time and cost estimates under various situations. In this paper, discrete event simulation theory and Constructive Product Line Investment Model (COPLIMO) [2] are used to create an environment that facilitates strategic management and long-term forecasting with respect to software product line development and evolution. Specifically, the simulator facilitates the study of the effect of a number of process decisions, including choice of evolution approach, upon factors such as effort and time-to-market. The simulator not only gives statistical results at the end of the simulation, but also visually presents how major product line engineering activities progress and interact over time. The simulator is built upon DEVSJAVA [1], a general-purpose Java-based discrete event simulation framework. The tool is extensible and allows other simulation frameworks and cost models to be used. The remainder of this paper is organized as follows. Section 2 presents background information. Section 3 describes the simulation model and the simulator. Results are discussed in Section 4. Section 5 contains related work and Section 6 draws conclusions and suggests future investigations.

1. Introduction
A software product line is defined as a set of softwareintensive systems sharing a common, managed set of features that satisfy the specific needs of a particular market segment or mission and are developed from a common set of core assets in a prescribed way [8]. A product line approach is a disciplined methodology for strategic reuse of source code, requirement specifications, software architectures, design models, components, test cases, and the processes for using the aforementioned artifacts. Software product line engineering promises large-scale productivity gains, shorter time-to-market, higher product quality, increased customer satisfaction, decreased development and maintenance cost [8]. However, those benefits are not guaranteed under all situations, and they are affected by many factors such as the initiation situation, the adoption and evolution approaches, the market demands, and the available resources. The goal of this research is to develop a
1 2

2. Background
This section describes background information on Software Product Lines, software process simulation, DEVSJAVA [1], and COPLIMO [2].

2.1. Software product lines
Software product line development involves three essential activities: core asset development, product development, and management [8]. Core asset

This material is based upon work supported by the National Science Foundation under grant No. CCR-0133956. Contact author.

development (domain engineering) involves the creation of common assets and the evolution of the assets in response to product feedback, new market needs, etc. Product development (application engineering) creates individual products by reusing the common assets, gives feedback to core asset development, and evolves the products. Management includes technical and organizational management, where technical management is responsible for requirement control and the coordination between core asset and product development activities. There are two main software product line adoption approaches: big bang and incremental [10]. With the big bang approach, core assets are developed for a whole range of products prior to the creation of any individual product. With the incremental approach, core assets are incrementally developed to support the next few upcoming products. In general, the big bang approach has a higher return on investment but involves more risks, while the incremental approach has lower entry costs but higher total costs. The four common software product line adoption situations are: independent, project-integration, reengineering-driven, and leveraged [10]. Under the independent situation, a product line is created without any pre-existing products. Under the project-integration situation, a product line is created to support both existing and future products. Under a reengineering-driven scenario, a product line is created by reengineering existing legacy systems. And the leveraged situation is where a new product line is created based on some existing product lines. Some common product line evolution strategies are: infrastructure-based, branch-and-unite, and bulkintegration [10]. The infrastructure-based strategy does not allow deviation between the core assets and the individual products, and requires that new common features be first implemented into the core assets and then built into products. Both the branch-and-unite and the bulk-integration strategies allow temporal deviation between the core assets and the individual products. The branch-and-unite strategy requires that the new common features be reintegrated into the core assets immediately after the release of the new product, while the bulkintegration strategy allows the new common features to be reintegrated after the release of a group of products.

expensive and risky, so software process simulation modeling is often used to reduce the uncertainty and predict the impact. Software process simulation modeling can be used for various purposes and scopes, and have been supported by many technologies [3]. The software product line process simulator described in this paper is for long-term organization strategic management, and is implemented in DEVSJAVA [1], a Java implementation of the Discrete Event System Specification (DEVS) modeling formalism [1]. The external view of a DEVSJAVA model is a black box with input and output ports. A model receives messages through its input ports and sends out messages via its output ports. Ports and messages are the means and the only means by which a model can communicate with the external world. A DEVSJAVA model is either "atomic" or "coupled". An atomic model is undividable and generally used to build coupled models. A coupled model consists of input and output ports, a finite number of (atomic or coupled) models, and couplings. The couplings link model ports together and are essentially message channels. They also provide a simple way to construct hierarchical models. To execute atomic and coupled models, DEVSJAVA uses distinct atomic and coupled simulators that support incremental simulation model development. These simulators can execute in alternative settings (i.e., sequential, parallel, or distributed). An important feature of the DEVS framework is the ability for models to seamlessly execute either in logical or (near) real-time. Furthermore, due to its availability of its source code and object-oriented design, DEVSJAVA can be extended to incorporate domain-specific (e.g., Software Product Line) logic and semantics.

2.3. COPLIMO
In the simulator, COMPLIMO [2] is used as the cost model to provide cost estimates. COPLIMO is a COCOMO II [9] based model for software product line cost estimation, and has a basic life cycle model and an extended life cycle model. The basic life cycle model has two sub-models: a development model for product line creation and a post-development model for product line evolution. Although the basic life cycle model has many simplification assumptions, it is thought to be good enough for early stage product line trade-off considerations [2]. The basic model also can be easily extended to the extended life cycle model, which allows products have different parameter values instead of the same values. In the implementation, the cost model is designed as a plug-in model, thus other cost models can be plugged in to meet other needs.

2.2. Simulation
A software process is a set of activities, methods, practices, and transformations that people use to develop and maintain software and associated products, such as project plans, design documentations, code, test cases, and user manuals [4]. Adopting a new software process is

3. Approach
A simulation framework and a software cost model were used to develop the simulator. Although DEVSJAVA [1] and COMPLIMO [2] are currently used, they can be replaced by other suitable simulation frameworks and cost models. This section presents the abstract software product line engineering model, the specifics of the simulation models, the assumptions, and the simulation tool.

products are developed by reusing the existing core assets, and existing products are updated after the change of the core assets. Figure 3.1 depicts the process flow. Costs associated with this approach include core asset development costs, new product development costs, and existing product maintenance costs. Compared to the big bang approach, the incremental approach has higher product development costs because of the incompleteness of the core assets, and extra product maintenance costs as the result of a short-term planning penalty.

3.1. Abstract product line model

Figure 3.2. SPL evolution approaches Figure 3.1. SPL adoption approaches Software product line engineering typically involves a creation phase and an evolution phase [10]. Currently the simulator provides two options (big bang and incremental) for the creation stage and two options (infrastructure-based and branch-and-unite) for the evolution stage. In the following, we will discuss the costs associated with those cases in detail. With the big bang adoption approach, core assets are first developed to meet the requirements for a whole range of products. Products are then developed by reusing the core assets [10]. Figure 3.1 illustrates the process flow. Costs associated with this approach include core asset development costs and new product development costs. With the incremental adoption approach, the core assets are incrementally developed to meet the requirements of the next few upcoming products, new With the infrastructure-based product line evolution strategy, the process for building a new product is the following: core assets are updated by incorporating new common requirements, and the new product is developed and existing products are updated. Figure 3.2 shows the process flow. The COPLIMO [2] basic life cycle model assumes that a change to a product causes the same percentage of change on reused code, adapted code, and product unique code. So if the change rate caused by new product requirements is , then the costs for one product development iteration include the costs of maintaining the core assets with a change rate of , the costs of developing the new product, and the costs of maintaining existing products with a change rate of . With the branch-and-unite product line evolution strategy, the process for building a new product is the following: the new product is developed, core assets are updated to incorporate new common features, and existing products are updated (including the newly

created product). Figure 3.2 illustrates the process flow. If  is the change rate caused by new product requirements, then the costs for one product development iteration include the costs of developing a new product with (1-) percentage of the reuse rate, the costs of maintaining the core assets with a change rate of , and the costs of maintaining existing products (including the newly created one) with a change rate of . Product maintenance costs are higher in this case because it has one more product to update, the newly created one. The new product is first created with new features that are not supported by the core assets, then after the core assets are updated to incorporate the new features the new product needs to be updated to keep consistent with the core assets. The new product development costs are also higher with this approach, because of the lower reuse rate.

project cannot be started until the requested resources are granted from the Employee Pool. If the number of employees in the employee pool is not less than the requested number of employees, the requested amount of employees will be granted. Otherwise, if the number of available employees meets the minimum employee level (a model parameter, between 5/8 and 1), then the number of available employees will be granted. In that case, a job can be started with fewer resources but longer development time. In other cases, the employee pool will not grant any resources until enough resources are returned.

3.2. Model development
Twelve DEVSJAVA [1] models were developed to model software product line engineering activities. Figure 3.3 shows a UML diagram depicting the hierarchical structure of the model. Some time constraints are imposed in the simulator: for each atomic model, jobs are processed one by one in FIFO order (or in combination with some priority). The PLPEF (Product Line Process Experimental Frame) is the top level coupled model and contains a Product Line Process instance and an Experimental Frame instance. The Product Line Process models software product line engineering activities. It contains an instance of Technical Management, Core Asset Development, and Employee Pool, and a finite number of Product Development instances. The number of Product Development models to be included depends on the number of projected products in the product line. The Product Line Process receives market demands and dispatches them to Technical Management. It sends out requirements (generated by Technical Management and Maintenance Requirement Generator) and development reports (generated by Core Asset Development and Product Development), which can be used for process monitoring. The Employee Pool models human resource management. It receives resource request and resource return messages, and sends out reply messages to grant resources. Currently, Employee Pool manages the resource requests in either a pure FIFO manner or a FIFO manner where new development jobs are given higher priority. Before starting any development activity, a resource request must be sent to Employee Pool. A Figure 3.3. Model hierarchical structure The Product Development models the application engineering activity. It has a Development instance for product creation and inter-product synchronization (development instance), a Development instance for inner-product maintenance (maintenance instance), and a Maintenance Requirement Generator instance. When the Product Development gets the first requirement, the development instance starts product creation, once that is done, the Maintenance Requirement Generator sends out a requirement to maintain the product for N years (the number of years in the product life cycle), which starts the maintenance instance. After N years, the Maintenance Requirement Generator sends out a stop message, which stops the maintenance activity and the acceptance of new development requirements. The Development models a general software engineering activity. When a new requirement is received, Development sends a resource request to the Employee Pool, waits for the reply from the Employee Pool, starts developing activity when the resources are granted, then returns resources to the Employee Pool and sends a report to Technical Management upon completion. The Development model will stop accepting new requirements when it receives a stop message on its stop port, which means the product reaches the end of its life cycle and needs to be phased out. The Maintenance Requirement Generator models product maintenance requirement generation. Once a new product is released, it sends out a requirement to maintain the product for N years (the number of years in

the product life cycle), and sends a stop message when the product reaches the end of the product life cycle. The Core Asset Development models domainengineering processes. Currently, it is modeled in the same way as the Development model. The domain engineering is not modeled as the same as the application engineering, because in practice technical management often collects the core asset feedback from product development and issues the core asset requirements in the context of product development.
Stage Creation

The Market Demand models the demands for new products from the market. It sends out a new product request after a certain interval, which can be set through the model parameter, "interval". The Transducer observes product line engineering activities for a certain amount of time. During the observation period, it receives development requirements and reports, and tracks the generating and finishing time of each requirement. At the end of a simulation run it will output some statistical information to a file.

Table 3.1. Behavior of technical management
Approach Big bang Activities 1. Create core assets if they do not exist 2. Create new product by fully reusing core assets 1. Increase core assets if necessary 2. Create new product by fully reusing core assets 3. Update existing products 1. Update core assets 2. Create new product by fully reusing core assets and updating existing products (excluding the newly created product) 1. Create new product by partially reusing core assets 2. Update core assets 3. Update existing products (including newly created product)

3.3. Assumptions
In the simulator, we made a number of assumptions as follows. 1. All the employees have the same capability and can work on any project. 2. If task B needs to make use of the results from task A, task B cannot start until task A is finished. 3. Product maintenance starts right after the release of the product and the maintenance activity holds the staff until the product is phased out. The assumptions made by the COPLIMO [2] basic life cycle model are: 4. All products in the product line have the same size, the same fractions of reused (black-box reuse) code, adapted (white-box reuse) code, and product unique code, and the same values for cost drivers and effort modifiers. 5. For each product in the product line, the size, the values of cost drivers, and the values of effort modifiers remain constant over time. For each product, the fractions of reused code, the fractions of adapted code, the fractions of product unique code remain constant during the time when core assets stay the same. 6. A change to a product will cause the same percentage of change to reused code, adapted code, and product unique code. Assumption 2 states that concurrency between interdependent tasks is not supported in the current version, which will be supported to some extent in the future. Assumption 4 can be relaxed by using COPLIMO [2] extended life cycle model, which allows products to have different parameter values. Assumption 5 can be relaxed by allowing users to specify the change trend. Assumption 6 can also be relaxed by allowing users to specify the change rate on different portions of the products. Because COPLIMO is currently used as the underlying cost model, its assumptions are adopted in the simulator. If another cost model is used, these assumptions would be replaced by those made by the new cost model.

Incremental

Evolution

Infrastructure -Based

Branch-andUnite

The Technical Management models requirement generation and control as well as the coordination between core asset and product development. It receives market demands (which are processed in FIFO order), generates requirements for core asset or product development, and accepts development reports. Which requirements will be generated, when will they be generated, and where they will be sent depend on the selected product line adoption and evolution approaches. Technical Management coordinates core asset development and product development activities through keeping the requirement generation in a certain order. Table 3.1 summaries the behavior of Technical Management according to the given strategies. The Experimental Frame consists of a Market Demand instance and a Transducer instance. It feeds Product Line Process with inputs and receives Product Line Process' outputs.

Figure 3.4 Simulation tool in execution

3.4. Simulation tool
The simulation tool was developed in Java and runs in the DEVSJAVA [1] environment. Figure 3.4 shows the user interface. The upper part of the interface shows the current running model and its package, which are "PLPEF" and "productLineProcess", respectively. The middle part shows the models and their hierarchical relationships. The bottom part contains execution control components. The "step" button allows running the simulator step by step, the "run" button allows executing the simulator to the end, and the "restart" button allows starting a new simulation run without quitting the system. The "clock" label displays the current simulation time in the unit of months, and selecting the "always show couplings" checkbox will allow couplings between models to be displayed. The simulation speed can be manipulated at run time to allow execution in near real-time or logical time (slower/faster than real-time).

Figure 3.4 shows that at time 27.783, Core Asset Development is idle, Products 1 is under initial development, Product 2 and 3 are waiting for resources, and Products 4 and 5 are in planning. The messages tell that Product 2 just received requested resources and Product 3 just sent out a resource request. Because of the lack of resources, the Employee Pool cannot grant the requested resources and is waiting for more resources to be returned, which in turn puts Product 3 in wait. Technical Management is idle, Market Demand generates new product requirement in every 12 months. Finally, and Transducer is observing the simulation. This case shows a situation where limited resources cause development activity delay. At the end of each simulation run, a result table is generated similar to Table 3.2. The table has two sections that are divided by a blank line. The top section lists the created products, their first release time (FR), time-to-market (TTM), initial development effort (DPM), initial development time (DT), accumulated development and maintenance effort (APM), accumulated

development and maintenance time (AT), and the number of finished requirements (FR). The bottom section summarizes the total product line evolution effort (TPM), the time when all the requirements are finished (FT), the average annual development effort (APM), the number of requirements generated (TR), and the average time-to-market (ATM). The unit of effort is personmonths and the unit of time is months.
FR 27.8 48.1 48.1 68.4 79.8 100.1

product unique code, unfamiliarity with adapted code, and average change rate caused by new market demands. To study the effect of resources, adoption approaches, and evolution approaches on software product line engineering, we ran the simulator seven times using the same basic parameter values. Accordingly, we varied the number of resources, the type of adoption approach, and type of evolution approach. Table 4.1. Scenarios
Market Demand Interval Infrastructure-based Single product only

Table 3.2. Simulation Result
TTM 27.8 48.1 36.1 44.4 43.8 52.1 DPM 582.2 217.7 217.7 217.7 217.7 217.7 DT 27.8 20.3 20.3 20.3 20.3 20.3 APM 651.6 443.0 443.0 443.0 424.2 405.4

TPM 2810.1 FT 220.1 APM 153.2 TR 20 ATTM 44.9

4. Results
In this section some simulation cases are presented to illustrate the use of the simulator and to demonstrate the analytical capability of the simulator.

1 2 3 4 5 6 7

12 12 12 12 12 12 12

50 40 30 50 50 50 50

4.2. Effect of resources
The inputs to Scenarios 1, 2 and 3 only differ in the values of number of employees (50, 40, and 30, respectively). The results differ in time-to-market, as shown in Figure 4.1. At the beginning, there is little difference, as time progresses, the gap of time-to-market between resource-constrained and non-resourceconstrained scenarios increases. The reason is that as more products are developed, more resources for product maintenance are required, thus less resources are left for new product development, which may increase the resource waiting time and in turn result in a longer timeto-market. The effort associated with Scenario 3 is smaller than the other two cases. That is because in Scenario 3, when Product 10 is released at 202.69, Product 1 and 2 are already phased out (at the time 168.1). Accordingly, no effort is needed to update those two products due to the change of the core.

4.1. Overview
The inputs to the simulator include general parameters and product (core asset) parameters. The general parameters are used to describe software product line process attributes and organization characteristics. These parameters include the maximum number of products that will be supported by the product line, the number of products that will be created during the creation stage, the product line adoption and evolution approaches, the number of employees in an organization, and the market demand intervals. The product (core asset) parameters are primarily determined by the employed cost model (COPLIMO, in this case). These parameters include the size of the product (core assets) in source lines of code, fraction of product unique code, fraction of reused code, fraction of adapted code, percentage of design modified, percentage of code modified, and percent of integration required for modified software. In addition, a number of parameters related to reuse and maintenance are included, such as software understanding of product unique code, software understanding of adapted code, unfamiliarity with

4.3. Effect of adoption approach
The inputs to Scenarios 1 and 4 only differ in product line adoption approach (big bang and incremental, respectively). The results differ in time-to-market, as shown in Figure 4.2. As specified by the inputs, the core

Branch-and-unite

Big bang

Incremental

Scenario

Resources

core p01 p02 p03 p04 p05

AT FR 50.5 3 159.0 4 159.0 4 159.0 4 149.6 3 140.3 2

100 Time-to-Market 80 60 40 20 0 1 2 3 4 5 6 7 8 9 10

Products numEmp=40

numEmp=30

numEmp=50

Figure 4.1. Effect of resources
60 50 40 30 20 10 0 Time-to-Market

1

2

3

4

Big Bang

Products

5

6

7

8

9

10

Incremental

Figure 4.2. Effect of adoption approach
100 Time-to-Market 80 60 40 20 0 1 2 3 4 5 6 7 8 9 10

Branch-and-unite

Products

Infrastructure-based

Figure 4.3. Effect of evolution approach
120 Time-to-Market 100 80 60 40 20 0

1

2 Big_Inf

3

4 Big_Bra

Products Inc_Inf

5

6

7

8 Inc_Bra

9

10 Tra

Figure 4.4. Effect of combined adoption and evolution approach

assets are developed in two steps with the incremental approach. The first increment happens right before the development of Product 1, and implements half of the core assets. The second increment happens right before the development of Product 4, and implements the rest of the core assets. For the first three products, the incremental approach appears to have shorter time-tomarket, mainly because fewer core assets means less time is required for asset development. As request for Product 4 comes, with the incremental approach, the development of the new product can not be started until the rest of the core assets have been implemented. So, we see a big jump in time-to-market from Product 3 to 4. The incremental approach results in higher total effort (6173.36) than the big band approach (5338.47). That is the nature of its process flow.

4.4. Effect of evolution approach
The inputs to Scenarios 1 and 5 differ in product line evolution approach (infrastructure-based and branch-andunite, respectively). Figure 4.3 shows the comparison of the results in time-to-market. As specified by the inputs, the evolution stage starts from Product 7. For Product 7, the branch-and-unite approach has smaller time-tomarket because the product gets developed earlier and does not have to wait for core asset updates. For the later products, the branch-and-unite approach results in longer time-to-market because it requires extra effort to rework new products and imposes more task dependencies, thus reducing concurrency. The total effort of the branchand-unite approach (5340.37) is only a slightly higher than the infrastructure-based approach (5338.47). That is because when Product 9 and 10 are released, some early products have already been phased out, so the costs for updating existing products are reduced.

the third product, then its time-to-market stays between the incremental and the big bang approaches, afterwards its time-to-market starts climbing dramatically but still stays in between the branch-and-unite and infrastructurebased approaches. In our experiment, the reuse rates are not very high (30% for both black-box and white-box reuse) and the product is relatively small (100KSLOC), so the traditional product development time is only slightly longer (about 5 months) than product line engineering approaches. In the case of branch-and-unite evolution approach, the dependencies imposed by that approach overweighs the benefits of reusing the core assets. The total effort of Scenario 1 and 4-7 are 5338.47, 5340.37, 6173.36, 6194.03, and 8760.55, respectively. As we have expected, the traditional approach requires considerably more effort. By largescale reuse, product line approaches generally result in smaller code size to development and maintain. Thus, the total effort on creating and evolving the products in a product line is smaller.

4.6. Validation of model and results
Several steps have been taken to verify and validate the model. First, the results of the simulator have been compared with the results of COCOMO II [9] to make sure the mathematic calculations are correct, and the results are the same (ignoring rounding errors). Second, the results of the simulator have been compared with the common knowledge about the product line, and we feel the results confirm to the common knowledge. Third, a initial small set of experts have reviewed the simulation results, and they feel that the results are consistent with what have been observed in the real world and the abstract model reflects the real process flow at a high level. In future investigations, we plan to continue soliciting expert feedback and compare simulation results with real product line data.

4.5. Effect of adoption and evolution approaches
A situation an organization might face is the need to determine which software development and evolution approaches best fit its goals. Scenarios 1 and 4 ­ 7 show the alternatives the organization might have. Scenario 7 is the case where a traditional software development approach (single product only) is taken, where products are created and evolved independently. Figure 4.4 shows the comparison of the results in time-to-market. As we can see, the big bang with infrastructure-based approach has the shortest average time-to-market, and the incremental with branch-andunite approach has the longest average time-to-market. The traditional approach has the shortest time-to-market for the first two products, the longest time-to-market on

5. Related work
Cohen [7] presents an approach for making a software product line investment determination. The approach uses three factors to justify software product line investment: applications, benefits, and costs. Applications include the number of projected products in the product line, the time they will be developed, and their annual change traffic; benefits consists of the tangible and intangible goals the organization wishes to achieve through a product line approach; costs are the life cycle costs associated with core assets and individual products. Costs are affected by some factors, such as

costs of reuse, degree of reuse, and core assets change rate. Our cost estimation method is consistent with the Cohen approach but provides more capabilities. Regnell et al. use a simulator to study a specific market-driven requirement management process [5]. The goal of simulator is to help in exploring bottleneck and overload situations in the requirement engineering process, investigating which resources are needed to handle a certain frequency of new requirements, and analyzing process improvement proposals. The specific process is modeled using queuing network and discrete event simulation [6]. Our simulator also uses discrete event simulation, but its purpose is to study life cycle issues for a product family instead of a portion of a software engineering process for a single product. Riva and Delrosso recently discussed issues related to software product family evolution [11]. They state that a product family typically evolves from a copy-and-paste approach to a mature software platform. They point out some issues that harm the family evolution, such as organization bureaucracy, dependencies among tasks, slower process of change, and the new requirements that can break the architectural integrity. Their notion of product family used in that paper is different from the definition of a product line [8]. Creating a product family by copy-and-paste is not a product line approach, because the product line approach emphasizes a disciplined strategic reuse, not opportunistic reuse. A product line is actually a product family that has already evolved to a mature software platform. Our simulation results also show that in some cases dependencies imposed by product line approaches result in slower market response than the traditional software engineering approach.

incremental product line adoption approaches and infrastructure-based or branch-and-unite product line evolution strategies. Our future investigations include providing estimates for other software product line initiation situations and approaches, allowing concurrency between inter-dependent tasks to some extent, providing probabilistic demand intervals, incorporating other cost models, and removing a number of the simplification assumptions. Furthermore, we plan to validate the model by comparing the results with real product line data and getting more expert feedback. Also, we want to combine the simulator with an optimization model, so users can specify their end-goal criteria and then allow the simulator to search for the best results.

7. References
[1] B.P. Zeigler and H.S. Sarjoughian, "Introduction to DEVS Modeling & Simulation with JAVA(TM): Developing Component-based Simulation Models", 2003, http://www.acims.arizona.edu/SOFTWARE/software.shtml. [2] B. Boehm, A.W. Brown, R. Madachy, and Y. Yang, "A Software Product Line Life Cycle Cost Estimation Model", USC, June 2003 [3] M. I. Kellner, R. J. Madachy, and D. M. Raffo, "Software Process Modeling and Simulation: Why, What, How", The Journal of Systems and Software, April 1999, pp. 91-105. [4] M. Paulk, et al., "Key Practices of the Capability Maturity Model", Version 1.1, Tech. Rept. CMU/SEI-93-TR-25, Software Engineering Institute, Feb 1993. [5] M. Höst, B. Regnell, et al, "Exploring Bottlenecks in Market-Driven Requirements Management Processes with Discrete Event Simulation", The Journal of Systems and Software, Dec 2001, pp. 323-332. [6] J. Banks, J.S. Carson, and B.L. Nelson, Discrete-Event System Simulation, 2nd Ed., Prentice Hall, Aug 2000. [7] S. Cohen, "Predicting When Product Line Investment Pays", Proceedings of the Second International Workshop on Software Product Lines: Economics, Architectures, and Implications, Toronto Canada, 2001, pp. 15--18. [8] P. Clements and L.M. Northrop, Software Product Lines -Practices and Patterns, Addison-Wesley, Aug 2001 [9] B. Boehm, B. Clark, E. Horowitz, C. Westland, R. Madachy, and R. Selby, "Cost Models for Future Software Life Cycle Processes: COCOMO 2.0," Annals of Software Engineering Special Volume on Software Process and Product Measurement, Science Publishers, Amsterdam, The Netherlands, 1995, pp. 45 - 60. [10] K. Schmidt and M. Verlage, "The Economic Impact of Product Line Adoption and Evolution", IEEE Software, Jul/Aug 2002, pp. 50-57. [11] C. Riva and C.D. Rosso, "Experiences with Software Product Family Evolution", Proceedings of International Workshop on Principles of Software Evolution, Helsinki Finland, Sep 2003, pp. 161-169.

6. Conclusions and future investigations
Software product line engineering promises of reduced cost while still supporting differentiation makes adoption and continued use of the associated approaches attractive. However, in order to make appropriate planning, decision tools are necessary. In this paper, we described a simulator that is intended to support early stage decision-making. The simulator provides both static and dynamic information for the selected software product line engineering process. The statistical result generated at the end of the simulation can be used for trade-off analysis. Stepping through the simulator helps analyzing product line processes, uncovering problems, and improving the understanding of software product line evolution. Currently the simulation tool supports the study of independent product line initiation using big bang or

Variable Strength Interaction Testing of Components
Myra B. Cohen Peter B. Gibbons Warwick B. Mugridge Dept. of Computer Science University of Auckland Private Bag 92019 Auckland, New Zealand   myra,peter-g,rick ¡ @cs.auckland.ac.nz Charles J. Colbourn James S. Collofello Dept of Computer Science and Engineering Arizona State University P.O. Box 875406 Tempe, Arizona 85287   charles.colbourn,collofello ¡ @asu.edu

Abstract
Complete interaction testing of components is too costly in all but the smallest systems. Yet component interactions are likely to cause unexpected faults. Recently, design of experiment techniques have been applied to software testing to guarantee a minimum coverage of all ¢ -way interactions across components. However, ¢ is always fixed. This paper examines the need to vary the size of ¢ in an individual test suite and defines a new object, the variable strength covering array, that has this property. We present some computational methods to find variable strength arrays and provide initial bounds for a group of these objects.

1. Introduction
In order to shorten development times, reduce costs and improve quality, many organizations are developing software utilizing existing components. These may be commercial off-the-shelf (COTS) or internally developed for reuse among products. The utilization of existing components requires new development and verification processes [2]. In particular the interaction of these new components with each other as well as with the newly developed components within the application must be tested. Software is becoming increasingly complex in terms of components and their interactions. Traditional methods of testing are useful when searching for errors caused by unmatched requirements. However, component based development creates additional challenges for integration testing. The problem space grows rapidly when searching for unexpected interactions. Suppose we have five components, each with four possible configurations. We have £¥¤§¦©¨¥£ potential interactions. If we combine 100 com-

£ ponents there are possible combinations. This makes testing all combinations of interactions infeasible in all but the smallest of systems. Instead one can create test suites that guarantee pairwise or ¢ -wise coverage. For instance we can cover all pairwise interactions for ten components, each with four possible configurations, only "!$#&%(' 0)1324 using%6 5 ) 25 test cases. A covering %65 array, is an array such that every ¢ ¢ sub2 array contains all ordered subsets from symbols of size ¢ at % least once. The covering array number 7)832 is the minimum required to satisfy the parameters ¢ . Covering arrays have been used for software interaction testing by D. Cohen et al. in the Automatic Efficient Test Generator (AETG) [5]. Williams et al. use these to design tests for the interactions of nodes in a network [12]. Dalal et al. present empirical results suggesting that testing of all pairwise interactions in a software system indeed finds a large percentage of existing faults [7]. In further work, Burr et al. provide more empirical results to show that this type of test coverage is effective [3]. In a software test, the columns of the covering array rep) 2 resent the components or fields. Each component %95 has ) levels or configurations. The final test suite is an ar% ray where is the number of test cases and each test contains one configuration from each component. By mapping a software test problem to a covering array of strength ¢ we can guarantee that we have tested all ¢ -way interactions. In many situations pairwise coverage is sufficient for testing. However, we must balance the need for stronger interaction testing "!$ #&%(' @7with A£4 the cost of running tests. For instance a "!B#C%('7D 0can A3£E4 be achieved for as little as 16 tests, while a requires at least 64 tests. In order to appropriately use our resources we want to focus our testing where it has the most potential value. The recognition that all software does not need to be

tested equally is captured in the concept of risk-based testing [1]. Risk-based testing prioritizes testing based on the probability of a failure occurring and the consequences should the failure occur. High risk areas of the software are identified and targeted for more comprehensive testing. The following scenarios point to the need for a more flexible way of examining interaction coverage.
 

RAID Level RAID 0 RAID 1 RAID 5

Component Operating Memory System Config Windows XP 64 MB Linux 128 MB Novell Netware 6.x 256 MB

Disk Interface Ultra-320 SCSI Ultra-160 SCSI Ultra-160 SATA

We completely test a system, and find a number of components with pairwise interaction faults. We believe this may be caused by a bad interaction at a higher strength, i.e. some triples or quadruples of a group of components. We may want to revise our testing to handle the "observed bad components" at a higher strength.
 

Table 1. Raid integrated controller system: 4 components, each with 3 configurations

We thoroughly test another system but have now revised some parts of it. We want to test the whole system with a focus on the components involved in the changes. We use higher strength testing on certain components without ignoring the rest.
 

We have computed software complexity metrics on some code, and find that certain components are more complex. These warrant more comprehensive testing.
 

are more likely to be interaction problems between three components: RAID level, OS and memory. We want to test these interactions more thoroughly. But it may be too expensive to run tests involving all three way interactions among components. In this instance we can use three-way interaction testing among the first three components while maintaining two-way coverage for the rest. We still have a minimal coverage guarantee across the components and we still do not need to run 81 tests. The test suite shown in Table 2 provides this level of variable strength coverage with 27 tests.
Component Operating Memory System Config Linux 128 MB Novell 128 MB Linux 64 MB XP 128 MB Novell 256 MB XP 128 MB Novell 256 MB Linux 64 MB XP 256 MB XP 64 MB Novell 128 MB XP 128 MB Linux 256 MB XP 64 MB XP 256 MB Linux 128 MB Novell 64 MB Novell 64 MB Linux 64 MB Novell 64 MB XP 256 MB Novell 128 MB XP 64 MB Linux 256 MB Linux 128 MB Novell 256 MB Linux 256 MB

We have certain components that come from automatic code generators and have been more/less thoroughly tested than the human generated code.
 

One part of a project has been outsourced and needs more complete testing.
 

Some of our components are more expensive to test or to change between configurations. We still want to test for interactions, but cannot afford to test more than pairwise interactions for this group of components.

While the goal of testing is to cover as many component interactions as possible, trade-offs must occur. This paper examines one method for handling variable interaction strengths while still providing a base level of coverage. We define the variable strength covering array, provide some initial bounds for these objects and outline a computational method for creating them.

2. Background
Suppose we are testing new integrated RAID controller software. We have four components, (RAID level, operating system (OS), memory configuration and disk interface). Each one of these components has three possible configurations. We need 81 tests to test all interactions. Instead we can test all pairwise interactions of these components with only nine tests. Perhaps though, we know that there

RAID Level RAID 0 RAID 0 RAID 5 RAID 1 RAID 0 RAID 0 RAID 1 RAID 0 RAID 5 RAID 0 RAID 5 RAID 5 RAID 1 RAID 5 RAID 0 RAID 1 RAID 0 RAID 1 RAID 1 RAID 5 RAID 1 RAID 1 RAID 1 RAID 5 RAID 5 RAID 5 RAID 0

Disk Interface Ultra 160-SCSI Ultra 160-SCSI Ultra 160-SCSI Ultra 160-SCSI Ultra 320 Ultra 160-SCSI Ultra 160-SCSI Ultra 320 Ultra 320 Ultra 160-SATA Ultra 320 Ultra 160-SATA Ultra 160-SCSI Ultra 160-SATA Ultra 160-SATA Ultra 320 Ultra 320 Ultra 160-SATA Ultra 160-SCSI Ultra 160-SCSI Ultra 160-SATA Ultra 160-SCSI Ultra 160-SATA Ultra 320 Ultra 320 Ultra 160-SCSI Ultra 160-SATA

Table 2. Variable strength array for Table 1

Commercial software test generators, like AETG, provide only a fixed level of interaction strength [5]. We might use this to build two separate test suites and run each independently, but this is a more expensive operation and does not really satisfy the desired criteria. We could instead just default to the higher strength coverage with more tests. However, the ability to tune a test suite for specific levels of coverage is highly desirable, especially as the number of components and levels increases. Therefore it is useful to define and create test suites with flexible strengths of interaction coverage and to examine some methods for building these.

3. Definitions
"!$#&%(' 0)1324 In a covering array, ¢ , ¢ is called the ) 2 strength, the degree and the order. A covering array is optimal if it contains the minimum possible number of rows. We minimum number"! the% covering "! call % # this # @7A D 4 array 0)1324 ¦ ¨¥¨ number, . For example, ¢ [4]. A mixed level covering array, denoted a % 5 as   "!$#C% ' 7)8 # 2 32¢¡ ¤£¥£¦£ 32¨§¥434 ) 2  § , is an array on sym¢ 2 ¦© 2 ¦   , with the following properties: bols, where # ¨  )@4 1. Each column  contains only elements  ¦ 2  from a set   with     . % 5 2. The rows of each ¢ sub-array cover all ¢ tuples of values from the ¢ columns at least once.
We can use a shorthand notation to describe our mixed cov2 ering array by combining  's that are the same. For exam2  ! §%$ §' & 's of size two we can write this . ple if we have three " ¡ £¥£¦£ " §( 434   "!$#&%('  ##"  ) Consider an . This can also ¢   "!$#&%(' 0)1 # 2 32¢¡¥¤£¥£¦£ 32¨§ 44  § be written as an where ¢ ) ¦ © ) )  2 ¦ © ) ) "  ¦ © 2 ¥  ¥  ¥   . and The following holds:  ¡ ¤£¥£¥£ ) 1. The columns are partitioned into 0 groups 1  1 )  )1  where group 1 contains columns. The first  ) ¡ columns belong to the group 1  , the next columns ¡ belong to group 1 , and so on. ¦ "  2. If column 24351  , then  768 .
§ We can now"! use this notation for a fixed-level covering B#C%( ' 3 2 4 ) array as well. indicates that there are pa¢ 2 rameters each containing a set of symbols. This makes it easier to see that the values from different components can come from different symbol sets. A variable strength covering array , A denoted as a % 5B 9 "!$#&%('  # 2 32 ¡ ¤£¥£ 2¢@"4  4  ¢ , is an mixed level covering array, of strength ¢ containing , a multi-set of disjoint mixed level covering arrays each of strength C ¢ .

  "! We can abbreviate duplicate 's in the multi-set us¡ ing a similar notation to that of the covering arrays. "!$ #C% 'D  D Sup4 pose we have two identical "! covering arrays in ¡ ¡ B#C%('7D  D 4 . This can be written as . Ordering of the 9 "! columns in the representation of a is important since the columns of the covering arrays in are listed consecu¡ tively from left to right. 9 "!$# !D '  D!E  %F "!$# 8D '7D  D 4G¨H 4 An example of a can be seen in Table 3. The overall array is a mixed level array of strength two with nine columns containing three symbols and two columns containing two. There are three sub-arrays each with strength three. All three way interactions therefore among columns 0-2, 3-5, 6-8 are included. All two way interactions among all columns are also covered. This has been "!B#Cachieved %('7D  D 4 with 27 rows which is the optimal size for a . A covering array that would cover all three way interactions for all 11 columns, on the other hand, might need as many as 52 rows.

4. Construction Methods
Fixed strength covering arrays can be built using algebraic constructions if we have certain parameter combina2 0) tions of ¢ and [4, 12]. Alternately we may use computational search algorithms [5, 6, 11]. Greedy algorithms form the basis of the AETG and the IPO generators [5, 11]. It has also been shown that simulated annealing is effective for building covering arrays of both mixed and fixed levels [6, 10]. Since there are no known constructions for variable strength arrays at the current time we have chosen to use a computational search technique to build these. We have written a simulated annealing program that has been used to produce all of the results presented in this paper.

4.1. Simulated Annealing
Simulated annealing is a variant of the state space search technique for solving combinatorial optimization problems. The hope is that the algorithm finds close to an optimal solution. Most of the time, however, we do not know when we have reached an optimal solution for the covering array problem. Instead such a problem can be specified as #a set I 4 of feasible solutions (or states) together with a cost P  associated with each feasible solution  . An optimal solution corresponds to a feasible solution with overall (i.e. global) minimum cost. For each feasible solution Q3RI , we define a set SUT of transformations (or transitions), each of which can be used to change  into another feasible solution WV . The set of solutions that can be reached from  by applying % # 4  a transformation from SXT is called the neighborhood of  . To start, we randomly choose an initial feasible solution. The algorithm then generates a set of sequences (or

0 0 1 2 2 2 1 0 2 1 1 0 2 0 1 1 2 0 2 0 1 2 0 1 0 1 2

2 1 2 1 2 0 0 0 0 2 0 1 2 0 2 0 1 2 0 1 1 1 0 1 2 1 2

Table 3.

D E  ¡ ¤F "!$# !D  ' D  D 4 H4 9 "!B# !D ' @ ¨

2 2 2 0 0 0 0 0 2 0 2 0 1 2 1 1 2 1 1 1 2 1 1 1 0 0 2

0 1 2 0 2 0 1 0 2 0 2 0 2 2 2 1 2 1 1 1 0 0 1 1 1 2 0

1 2 1 0 2 2 1 2 0 1 1 0 1 2 0 0 0 2 0 0 1 0 1 1 2 2 2

2 0 0 2 0 2 2 1 0 0 2 0 1 1 2 1 1 1 0 2 1 1 0 1 2 2 0

2 1 2 2 1 2 1 0 2 0 1 1 0 1 0 1 0 2 0 1 0 2 2 2 0 1 0

2 1 1 0 2 2 2 0 0 2 1 1 0 2 1 0 2 2 0 0 1 1 1 0 2 0 1

2 0 0 0 1 1 0 1 1 1 2 1 2 2 1 1 0 0 0 2 0 2 1 2 2 0 2

0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 1

1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1

solution is an approximation  to a covering array in which certain ¢ -subsets are not covered. The cost function is based on the number of ¢ -subsets that are not currently covered.  A covering array itself will have cost . In the variable  strength array, the cost is when (i) all of the ¢ -subsets are covered and (ii) for each covering array of strength ¢ V in , all ¢GV -subsets are covered. A potential transition is made A by selecting one of the -setsA belonging to  and then replacing A a random point in this -set by a random point not in the -set. We calculate only the change in numbers of ¢ -sets that will occur with this move. Since the subsets of are disjoint, any change we make can affect the overall 9 "! coverage of the and at most one of the subsets of . This means that the extra work required for finding variable strength arrays over fixed strength arrays does not grow in proportion to the number of subsets of higher strength. We keep the number of blocks (test cases) constant throughout a simulated annealing run and use the method described by Stardom to determine a final array size [10]. We start with a large random array and then bisect our array repeatedly until we find a best solution.

4.2. Program Parameters
Good data structures are required to enable the relative cost of the new feasible solution to be calculated efficiently, and the transition (if accepted) to be made quickly. We build an exponentiation table prior to the start of the program. This allows us to approximate the transition probability value using a table lookup. We use ranking algorithms from [8] to hold the values of our ¢ -sets. This allows us to generalize our algorithms for different strengths without changing the base data structure. When calculating the change in cost for each transition, we do not need to recalculate all of the ¢ -sets in a test case, but instead only calculate the change (¢ -1 subsets). A constant is set to determine when our program is frozen. This is the number of consecutive trials allowed where no change in the cost of the solution has occurred. For most of our trials this constant has been set to 1,000. The cooling schedule is very important in simulated annealing. If we cool too quickly, we freeze too early because the probability of allowing a worse solution drops too quickly. If we cool too slowly or start at too high a temperature, we allow too many poor moves and fail to make progress Therefore, if we start at a low temperature and cool slowly we can maintain a small probability of a bad move for a long time allowing us to avoid a frozen state, at the same time continuing to make progress. We have experimented using fixed strength arrays compared with known algebraic constructions (see [6]). We have found that a starting temperature of approximately 0.20 and a slow cooling

# 4  # 4 Markov chains) of trials. If P  V P  then the transition is accepted. If the transition results in a feasible solution  V of higher cost, then  V is accepted with probability ¡£¢¥¤§¦¨¤ T© ¢¦¨¤ T , where S is the controlling temperature of the simulation. The temperature is lowered in small steps with the system being allowed to approach "equilibrium" at each temperature through a sequence of trials at this tem¦"! perature. Usually this is done by setting S  S , where ! (the control decrement) is a real number slightly less than ¨ . The idea of allowing a move to a worse solution helps avoid being stuck in a bad configuration (a local optimum), while continuing to make progress. Sometimes we know the cost of an optimal solution and can stop the algorithm when this is reached. Otherwise we stop the algorithm when insufficient progress is being made (as determined by an appropriately defined stopping condition). In this case the algorithm is said to be frozen. Simulated annealing has been used by Nurmela and ¨ Osterg° ard [9], for example, to construct covering designs which have a structure very similar to covering arrays. It has also been used by Stardom and Cohen, et al. to generate minimal fixed strength covering arrays for software testing [6, 10]. In the simulated annealing algorithm the current feasible

VCA

¢¤£¦¥¨§©   £ ¡ 

  C¡ !£¦¥"§#%$ £¦¥"§# $ & £¦¥"§#%$  $ £¦¥"§#%'  £¦¥"§#   £¦¥"§# '  , £¦¥"§#   , £¦¥"§#%(  £¦¥"§# (  £¦¥"§#0)  £¦¥"§#%1  £¦¥"§#  

Min N 16 27 27 27 27 33

Max N 17 27 27 27 27 33

Avg N 16.1 27 27 27 27 33

¢¤£¦¥¨§©2 $%3%$%4 &    £ ¡ 

¢¤£¦¥¨§© &879A@0&    £ ¡ 

!£¦¥"§#2 $ 56£¦¥"§#2 $ 3 £¦¥"§# 3 $  £¦¥"§#2 $  , £¦¥"§# 3 $  56£¦¥"§#20$ 3 56£¦¥"§# 3  4 56£¦¥"§#20$ 3

& $4 $&%4 & 

33* 34 33* 34 41 50 67 36 64 100 125 125 171 180 214 100 100 304

35 35 42 51 69 36 64 104 125 125 173 180 216 100 100 318

34.8 34.9 41.4 50.8 67.6 36 64 101 125 125 172.5 180 215 100 100 308.5

!£¦¥"§# 56£¦¥"§#&870   &879A@0& 

in our notation for covering arrays in this table due to space limitations.)

Table 4. Table of sizes for variable strength arrays after 10 runs (We have omitted the parameter B

* The minimum values for these VCA's were found during a separate set of experiments

! factor, , of between 0.9998 and 0.99999 every 2,500 iterations works well. Using these parameters, the annealing algorithm completes in a "reasonable" computational time on a PIII 1.3GHz processor running Linux. For instance, 9 "! the first few 's in Table 4, complete in seconds, while 9 "! the larger problems, such as the last in Table 4, complete within a few hours. The delta in our cost function is counted as the change in ¢ -sets from our current solution. Since we can at any point make changes to both the base array and one of the higher strength arrays, these changes are added together. As there is randomness inherent in this algorithm, we run the algorithm multiple times for any given problem.

a different random seed. A starting temperature of .20 and a decrement parameter of .9998 is used in all cases. In two cases a smaller sized array was found during the course of our overall investigation, but was not found during one of these runs. The numbers are included in the table as well and are labeled with an asterisk, since these provide a previously unknown bound for their particular arrays. In each case we show the number of tests required for the base array of strength two. We then provide some examples with variations on the contents of . Finally we show the arrays with all of the columns involved in strength three coverage. We have only shown examples using strength two and three, but our methods should generalize for any strength ¢ . What is interesting in Table 4 is that the higher strength sub-arrays often drive the size of the final test suite. Such 9 "! is the case in the first and second groups in this table. We can use this information to make decisions about how many components can be tested at higher strengths. Since we must balance the strength of testing with the final size of the test suite we can use this information in the design process. Of course there are cases where the higher strength subsets do not determine the final test suite size since the number of test cases required is a combination of the number 9 "! of levels and the strength. In the last group in Table 4 the two components each with 10 levels require a minimum of 100 test cases to cover all pairs. In this case we can cover all of the triples from the 20 preceding columns with the same number of tests. In such cases, the quality of the tests can be improved without increasing the number of test cases. We can set the strength of the 20 preceding columns to the highest level that is possible without increasing the test count. Both situations are similar in the fact that they allow us to predict a minimum size test suite based on the fixed level sub-arrays. Since there are better known bounds for fixed strength arrays we can use this information to drive our decision making processes in creating test suites that are both manageable in size while providing the highest possible interaction strengths.

6. Conclusions
We have presented a combinatorial object, the variable strength covering array, which can be used to define software component interaction tests and have discussed one computational method to produce them. We have presented some initial results with sizes for a group of these objects. These arrays allow one to guarantee a minimum strength of overall coverage while varying the strength among disjoint subsets of components. Although we present these objects for their usefulness in testing component based software systems they may be of use in other disciplines that

5. Results
Table 4 gives the minimum, maximum and average sizes obtained after 10 runs of the"! simulated annealing algorithm 9 for each of the associated 's. Each of the 10 runs uses

currently employ fixed strength covering arrays. The constraining factor in the final size of the test suite may be the higher strength sub-array. We can often get a second level of coverage for almost no extra cost. We see the potential to use these when there is a need for higher strength, but we cannot afford to create an entire array of higher strength due to cost limitations. Where the constraining factor is the large number of levels in a set of fields at lower strength, it may be possible to increase the strength of sub-arrays without additional cost, improving the overall quality of the tests. Another method of constructing fixed strength covering arrays is to combine smaller arrays or related objects and to fill the uncovered ¢ -sets to complete the desired array [4]. We are currently experimenting with some of these techniques to build variable strength arrays. Since the size of a 9 "! may be dependent on the higher strength arrays, we believe that building these in isolation followed by annealing or other processes to fill in the missing lower strength ¢ -sets will provide fast and efficient methods to create optimal variable strength arrays.

[4] M. Chateauneuf and D. Kreher. On the state of strength-three covering arrays. Journal of Combinatorial Designs, 10(4):217­238, 2002 [5] D. M. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG system: an approach to testing based on combinatorial design. IEEE Transactions on Software Engineering, 23(7):437­ 44, 1997. [6] M. B. Cohen, C. J. Colbourn, P. B. Gibbons and W. B. Mugridge. Constructing test suites for interaction testing. In Proc. of the Intl. Conf. on Sofware Engineering (ICSE 2003), 2003, pp. 3848 , Portland. [7] S. R. Dalal, A. J. N. Karunanithi, J. M. L. Leaton, G. C. P. Patton, and B. M. Horowitz. Model-based testing in practice. In Proc. of the Intl. Conf. on Software Engineering,(ICSE '99), 1999, pp. 28594, New York. [8] D. L. Kreher and D. R. Stinson. Combinatorial Algorithms, Generation, Enumeration and Search. CRC Press, Boca Raton, 1999. ¨ [9] K. Nurmela and P. R. J. Osterg° ard. Constructing covering designs by simulated annealing. Technical report, Digital Systems Laboratory, Helsinki Univ. of Technology, 1993. [10] J. Stardom. Metaheuristics and the search for covering and packing arrays. Master's thesis, Simon Fraser University, 2001. [11] K. C. Tai and L. Yu. A test generation strategy for pairwise testing. IEEE Transactions on Software Engineering, 28(1):109-111, 2002. [12] A. W. Williams. Determination of test configurations for pair-wise interaction coverage In Proc. Thirteenth Int. Conf. Testing Communication Systems, 2000, pp. 57­74.

Acknowledgments
Research is supported by the Consortium for Embedded and Internetworking Technologies and by ARO grant DAAD 19-1-01-0406. Thanks to the Consortium for Embedded and Internetworking Technologies for making a visit to ASU possible.

References
[1] J. Bach. James Bach on risk-based testing In STQE Magazine, Nov/Dec,1999. [2] L. Brownsword, T. Oberndorf and C. Sledge. Developing new processes for COTS-based systems. IEEE Software, 17(4):48­55, 2000. [3] K. Burr and W. Young. Combinatorial test techniques: Table-based automation, test generation and code coverage. In Proc. of the Intl. Conf. on Software Testing Analysis & Review, 1998, San Diego.

The Journal of Systems and Software 46 (1999) 173±182

Software process simulation for reliability management
Ioana Rus
a b

a,1

, James Collofello

a,2

, Peter Lakey

b,*

Computer Science and Engineering Department, Arizona State University, AZ, USA Boeing Aircraft and Missiles, Mailcode S0343550, St. Louis, MO, 63166-0516, USA Received 10 November 1998; accepted 11 November 1998

Abstract This paper describes the use of a process simulator to support software project planning and management. The modeling approach here focuses on software reliability, but is just as applicable to other software quality factors, as well as to cost and schedule factors. The process simulator was developed as a part of a decision support system for assisting project managers in planning or tailoring the software development process, in a quality driven manner. The original simulator was developed using the system dynamics approach. As the model evolved by applying it to a real software development project, a need arose to incorporate the concepts of discrete event modeling. The system dynamics model and discrete event models each have unique characteristics that make them more applicable in speci®c situations. The continuous model can be used for project planning and for predicting the eect of management and reliability engineering decisions. It can also be used as a training tool for project managers. The discrete event implementation is more detailed and therefore more applicable to project tracking and control. In this paper the structure of the system dynamics model is presented. The use of the discrete event model to construct a software reliability prediction model for an army project, the Crusader, is described in detail. Ó 1999 Elsevier Science Inc. All rights reserved.

1. Overview The concept of process modeling and simulation was ®rst applied to the software development process by Abdel-Hamid (Abdel-Hamid et al., 1991). Others (Madachy, 1996; Tvedt, 1996) have produced models as well, but there is little evidence that they have been successfully applied to real software development projects. The models described in this paper bring two new contributions to the software process modeling and simulation work. First, the modeling approach emphasizes software reliability, as opposed to general characteristics of the software development process. Second, the discrete event implementation of the model is being applied to a speci®c software project where useful results are expected. 2. Introduction As the role of software is expanding rapidly in many aspects of modern life, quality and customer satisfaction become the main goal for software developers and an
* 1

Corresponding author. E-mail: peter.b.lakey@boeing.com E-mail: ioana.rus@asu.edu 2 Email: collofello@asu.edu

important marketing consideration for organizations. However, quality by itself is not a strategy that will ensure a competitive advantage. There are other project drivers like budget and delivery time that must be considered in relation to quality. Achieving the optimal balance among these three factors is a real challenge for any project manager (Boehm, 1996). The approach presented in this paper is intended to help managers and software process engineers to achieve this balance. Software reliability engineering consists of a general set of engineering practices applied to the following tasks: de®ning the reliability objective of a software system, supporting the development of a system that achieves this objective and assessing the reliability of the product through testing and analysis. A detailed list of reliability engineering activities and methods, together with their purpose and description can be found in (Lakey and Neufelder, 1996). A software reliability program consists of a subset of the above practices and is de®ned for each project by combining dierent reliability achievement and assessment activities and methods, according to the project's characteristics. A number of methods are available to facilitate software reliability planning and control. Use of cost estimation models and their corresponding software tools, like COCOMO (Boehm, 1984),

0164-1212/99/$ ± see front matter Ó 1999 Elsevier Science Inc. All rights reserved. PII: S 0 1 6 4 - 1 2 1 2 ( 9 9 ) 0 0 0 1 0 - 2

174

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

SLIM (Putnam, 1992) and Checkpoint (Jones, 1986), to estimate the impact of a practice on time and cost has the following disadvantages: because model tuning is needed not all relevant historical data might be available; cost drivers are at too coarse a level of granularity to re¯ect the speci®c technique whose impact needs to be evaluated. A major drawback of the models is their use of data from projects other than those to which they are being applied. An alternative method to support strategy selection is process modeling and simulation, which involves analyzing the organizational software development process, creating a model of the process, and executing the model and simulating the real process. Although modeling and simulation has some limitations as well (the model accuracy depends on the quality of the model and of the calibration), there are many strengths of modeling that would advocate its use instead of other approaches mentioned. Modeling captures expert knowledge about the process of a speci®c organization; it does not need a real system to experiment with, so it does not aect the execution of real process; ®nally, dynamic modeling increases the understanding of a real process. 3. Software process modeling and simulation Developing a model of the software development process involves the identi®cation of entities, factors, variables, interactions and operations that are present in that process and are relevant to the intended use of the model. The entities include mainly people (developers, managers, customers, etc.), but can also include facilities, computer equipment, software tools, documentation and work instructions. Factors relevant to a general software development process include the application domain, the size of the project, the expected schedule and delivery date, the hardware platform, and other considerations. Variables include the number of software engineers, the skill levels of those individuals, the level of process maturity of an organization, the level of communication overhead, etc. The modeling complexity increases when identifying operations and interactions. Operations are the tasks that need to be performed in the software process to transform user needs and requirements into executable software code. The typical operations include requirements analysis, architecture development, detailed design, implementation (code and unit test), integration, and system test. Each of these takes some input (entity) and generates some transformed output (another entity). For example, the design operation uses software requirements as the input and transforms these into a design of the software system. The most important, and perhaps most dicult, modeling task is identifying and correctly representing

the interactions among the factors and variables in the software process that are relevant to the modeling goal. For instance, how does the number of developers aect communication overhead and overall productivity? How does a defect prevention technique aect the number of defects injected in software documents or code? How does the eort allocated to regression testing aect failure intensity during system testing and the number of defects remaining at delivery time. These interactions need to be properly modeled in order to closely represent reality. While the model is useful for representing the structure of the process, execution of the model is very helpful in understanding the behavior of the process. It gives management a tool that has been lacking for a long time. The simulation capability allows a manager to make a decision and have a high degree of con®dence in what the results of that decision will be. Without the simulation the decision is purely speculation. The complexity of a decision is too overwhelming to be understandable without the dynamic execution of the software model. 4. Comparison to other approaches Simulator presented here can be used for tracking the quality and reliability of the software throughout the development process. Reliability prediction and estimation models such as reliability growth models and prediction models have a similar goal. The dierence is that the reliability growth models are analytical, address only the system testing phase, and each has unrealistic assumptions that restrict their applicability. The modeling and simulation approach addresses all the development phases and is tailored to the real process that is modeled. Reliability prediction models, as that developed for Rome Laboratory (SAIC, 1987) are static. It is postulated here that running simulations to determine reasonable predicted defect levels at delivery in consideration of cost, schedule and stang is a better way of predicting than using a static model. This approach allows the metrics tracked during the project to be used to make adjustments to the model to re¯ect what is really happening. That is, process behavior is explained by the model. With a static model, there is no way to know why deviations from predictions exist. The Rome Laboratory model is a regression model. All of these regression models inherently infer a cause±eect relationship between the independent variables and reliability, when in fact there is no proven causal relationship; rather the entire process is the cause of software defects and failures. Tausworthe and Lyu (1996) developed a simulator of the software reliability process, but it is at a much higher

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

175

level of process abstraction and captures a very reduced number of process parameters. Other system dynamics simulators of the software development process have been developed, but their purpose and scope is dierent. Most of the previous models, such as Abdel-Hamids model (Abdel-Hamid et al., 1991), focus on the management aspect of software development as opposed to the technical aspect; they also do not include the requirements analysis phase. Madachy's model (Madachy, 1996) was developed to analyze the eect of inspections, and Tvedt developed a model of the incremental life cycle (Tvedt, 1996) for analyzing cycle time reduction. The model presented in the next section has been developed to capture the technical issues of software quality and reliability and to assess the eect of various reliability practices. 5. System dynamics process model description The following model was developed to support project planning for the purpose of evaluating software reliability engineering strategies. It is a generic model intended for use in high level decision making. The model represents the initial work performed by the authors in software process modeling and simulation and is part of a decision support system for software reliability engineering strategy selection and assessment (Rus, 1998). The main components of the model correspond to the production phases of the software development

process (requirements analysis, design and coding) and the system testing phase. A brief description of a production phase is presented here. More model details, as well as a description of the SystemTesting block, together with more simulation results and model use examples can be found in (Rus, 1998). The model was implemented in Extend V4.0, a simulation software package from Imagine That (Extend, 1995), by using hierarchical blocks. Each ProductionPhase block has a Development block and a Management block, corresponding to technical and managerial activities, respectively. The Development block models items production, as well as quality assurance activities. Items can be requirements documents, design documents, and source code. The Development block has two sub-blocks: Items and Defects that are shown in Fig. 1. Sub-blocks of Items and Defects are expanded in Fig. 2, at the lowest Extend implementation level. The Items sub-block models items production and veri®cation and validation (V&V) activities. Items from the previous phase enter the current production phase (ItemsIn) and items corresponding to the current phase are produced (for instance if the current phase is design, requirements documents are the input and design documents are generated). The production rate depends on many factors, such as the productivity of the personnel (ProdProductIn), and schedule pressure (SchPresPrIn). There are other factors aecting this rate that are not shown in the ®gure: process factors like methods, techniques, tool support, and metrics

Fig. 1. Development and Management blocks of a ProductionPhase.

176

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

Fig. 2. Items and Defects blocks.

collection eort; and product factors like complexity, criticality and fault tolerance. The items produced are then veri®ed (by reviews, walkthroughs, and/or inspections) at the V8VRate. This rate depends on factors like personnel productivity (VVProductIn) and schedule pressure (SchPresVVIn). After veri®cation and validation, items are passed to the next phase (ItemsOut). For each item that is generated, there are also defects injected. Defect generation, detection and correction is captured by the Defects sub-block. Defects are generated at a rate depending on the production rate, schedule pressure and other factors such as process maturity and development methodology. Defectsi1In and Defectsi2In are defects propagated from previous phases. The defect detection rate depends on the veri®cation and validation rate (VVRateIn), eciency of V&V activities, schedule pressure, defect density, and other factors. Detected defects will be corrected, and possibly new defects will be generated. Undetected defects propagate to subsequent phases (DefectsOut). The Management block (Fig. 1) models the human resources, planning and control aspects of the process. For each of the production, veri®cation and validation (V&V), and rework activities there is a corresponding sub-block in the Management block. Each of these three sub-blocks models the eort consumed (man-days) in the tasks of producing, verifying and reworking items. These modules also capture personnel experience increasing with time (learning) and schedule pressure resulting from the dierence between the estimated

schedule and the actual one. Actual productivity determines the rate at which items are produced. Figs. 3 and 4 present examples of model execution (process simulation) outputs. Fig. 3(a) shows the variation of the number of items produced and veri®ed during a production phase. The number of defects injected, detected, and corrected throughout the phase can be seen in Fig. 3(b). Fig. 4(a) shows the evolution of the number of coding faults identi®ed and corrected during system testing, and the coding faults remaining in the product. Fig. 4(b) presents an example of sensitivity analysis: the variation of failures encountered during system testing, with the number of design faults at the beginning of system testing. These values and shapes of these graphs will be unique to a speci®c company that calibrates the model with metrics and inputs speci®c to that company. Software reliability engineering practices are modeled by considering their in¯uences on dierent factors that impact defects. For instance, defect prevention techniques (e.g. formal methods) will decrease the value of defect generation rate. Defect detection techniques will increase the number of defects detected and reduce the number of defects propagated to the next phase. Fault tolerance methods (e.g. N-version programming) reduce the impact that remaining defects have on the reliability of the operational system. There is, of course, some additional up-front eort and cost associated with these practices, but the overall development time and eort may be reduced, by longterm reduction in rework time. By reducing the number

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

177

Fig. 3. Variation of the number of items (a) and defects (b) through a production phase.

of remaining defects, and/or masking them, the reliability of the product will increase. Simulation is used to analyze the eect of these practices in terms of failures, cost, and stang, allowing trade-os among reliability strategies.

6. Discussion on system dynamics model The model described above was executed with assumed parameter values for each of the factors identi®ed. By changing these factors individually the

Fig. 4. (a) Variation of the number of coding faults during system testing. (b) Variation of failures encountered during system testing, with the number of design faults at the beginning of system testing.

178

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

corresponding impact on the outputs of the model (cost, schedule and quality) can be evaluated. The model is implemented with building blocks derived from the generic ProductionPhase block by adding details speci®c to each development phase while maintaining the common features and architecture. As a result, the model is modular, extensible, adaptable and ¯exible. The example that we used in (Rus, 1998) corresponds to a waterfall development life cycle model, but the model can be adapted to represent for example an incremental process. The system dynamics approach, which is a continuos modeling paradigm, considers that items are identical and treats them uniformly. This is an assumption that works at a higher level of modeling. If the use and the goal of the model require more detail, then dierences between entities and entities' attributes must be considered. For example, design and code items dier by structural properties such as size, complexity, and modularity; usage, and eort allocated for development. Similarly, defects can have dierent types, consequences, generation and detection phase, etc. In Extend, when using discrete event modeling, items can have attributes attached, with random values (within a speci®ed range), and with a speci®ed distribution. Activity duration and item generation and processing times can vary with other model parameters (variables), according to a speci®ed equation, and can have a probabilistic distribution around the computed value. Subprocesses such as detailed design and system testing (reliability growth testing) appear to be more like event-driven processes rather than continuous.

Therefore, because discrete event modeling allows a more detailed modeling capability which was required by the application described in the next section, the system dynamics model was transformed into an alternative discrete event model for a production phase, as illustrated by the preliminary design phase example. 7. Discrete event model application The discrete event model implementation of the software process was developed to support a speci®c project, Crusader. The Crusader is a large army development project. There are two vehicles, called Segments, in each Crusader System. The system consists of the Self-propelled Howitzer (SPH) and the Re-supply Vehicle (RSV). The software is being developed using object-oriented analysis and design (OOA/D) and Rational Rose CASE tool. An incremental software life cycle approach is being used, with speci®c functionality in ®ve planned major builds. A top-level architecture has been constructed for the Crusader software system. It consists of 40 Computer Software Con®guration Items (CSCIs). These could also be called subsystems. Each CSCI is developed and managed independently of the other CSCIs. The approach taken is to model one of these CSCIs, using it as a prototype for learning the relevant dynamics associated with defects, reliability and other process parameters. Many of the important metrics associated with reliability that are captured in the discrete event model are shown in Fig. 5. Some of the most important metrics

Fig. 5. Crusader software reliability management metrics.

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

179

needed to support software reliability management are related to rework. As it pertains to reliability, rework is important because it helps determine the amount of reliability improvement that can be expected for a given schedule and stang level. Software development activities may be completed on time, but if no time is built in for rework, then the defects that are found during testing cannot be ®xed and the reliability cannot improve signi®cantly. Rework is also important from the standpoint of process improvement. Reducing rework increases the total amount of quality software that can be developed per time unit. The model helps pinpoint areas where improvements should be made to reduce rework. To substantially reduce rework a signi®cant investment needs to be made in process improvement during implementation of the software development schedule. This will increase costs in the short term, and may lengthen the schedule. These dynamics are captured in the model. 8. Prototype discrete event model description The initial model being developed captures the process for Preliminary Design. The structure of the model, developed using Extend V4.0, is described below. The prototype discrete event model represents the Crusader Software Development Process, and it is being piloted on a single CSCI, which will be called Module X here. This discrete event model is actually a hybrid model that also incorporates system dynamics concepts, using feedback loops. The main purpose of the model is to predict, track, and control software defects and failures over the entire Crusader development, through the year 2005. It also facilitates tracking and control of software project costs and schedules. The model currently addresses only the Preliminary Design phase. Later on this year it will be expanded to include Detailed Design, Code and Unit Test, Subsystem Test, Element Test and Segment Test. There will be two modules. The Defect Module will predict defects, eort, and rework for each phase through Code and Unit Test. The Failure Module will estimate reliability and test eort during testing. There are three types of inputs to Preliminary Design: Segment Use Cases/Scenarios, Software Requirements Speci®cations, and the architecture in the form of Rose Models. The Crusader project has de®ned 10 Use Cases, 40 CSCIs and approximately 40 Rose Subsystems to correspond to the 40 CSCIs. Module X is responsible for approx. 50 Segment Scenarios, has 200 CSCI Scenarios of its own, and will use 200 classes in its design. In a pure discrete event model the activity of Re®ning the Scenarios would be represented by a single Operation block, with a single set of outputs, including schedule and quality. The problem with that approach (only one

output data point) is that no dynamics can be incorporated into the activity during the process. In order to incorporate feedback the Re®ne Scenarios Activity has been divided into ®ve discrete steps, allowing the outputs of one step to be fed into the next step. The inputs each are divided up into ®ve sets to be processed during each iteration. In other words, 10 Segment Scenarios, 40 CSCI Scenarios and 40 Rose Classes are processed for each step. This is dierent from actual process implementation, but the approximation should be acceptable for modeling. Random data is generated for each of the input products to the Preliminary Design activity. This data is stored in a spreadsheet ®le. All of this stored data is accessed throughout the simulation. The values for Segment Scenario size are normally distributed around the average value of 10. The values for Scenario Quality refers to the number of defects associated with a particular scenario prior to it being used to de®ne CSCI scenarios. Class size can be characterized through a number of dierent object-oriented design metrics, such as number of instance methods in a class, number of instance variables in a class or number of class methods. The screen capture in Fig. 6 shows the three types of input products, Segment Scenarios, CSCI Scenarios and Classes, being read into an Operation block where that data is processed and resources are expended to perform the Re®ne CSCI Scenarios task. The data from the spreadsheet is used to calculate some project parameters during the simulation. Fig. 6 represents three main steps for the activity of Re®ning CSCI Scenarios. First, the scenarios are developed using the Segment Scenarios allocated to MCS. Then, a Walkthrough is performed to evaluate the completeness and correctness of the CSCI Scenarios and their mapping to Segment Scenarios. Finally, any defects found are documented and those are corrected through the Rework Activity. During each of these activities data is generated for Eort ± in terms of labor hours expended, and Quality ± in terms of numbers of defects. All of this calculated data is passed on to a ®nal block in the model, which plots this data graphically over the course of a run. The list of outputs from the model includes development eort, review eort, rework eort, defects generated, defects found and escaped defects. These parameters are each dependent on a number of factors that interact with each other over the course of a software development project. The basis of the model is the set of factors for each parameter and the equations used to calculate the factor values during execution of the model. These factors include both process and product factors. Some of the process factors included in the model are the Manpower Factor, Schedule Factor, Communication Overhead Factor, Work Rate Factor, Process Maturity Factor and Tool Support Factor. The product factors include Size, Quality, and Complexity.

180

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

Fig. 6. Re®ne scenarios process for Modules X.

The model factors are dynamic. That is, they change after each event (iteration) occurs in a simulation. Many of the factors aect other factors, which in turn are affected by the same factors. This provides feedback loops. The interaction between factors in the model is intended to represent actual Crusader software development project dynamics. As work is scheduled and accomplished, the project team will often ®nd that the eort required for a task is larger than originally anticipated. This occurs in the middle of a development activity. When the project gets behind schedule decisions are made that change the expected state. Work is sped up or postponed. Certain activities may be cancelled, such as periodic walkthroughs. These decisions impact overall eort and the number of defects injected and found. Quality can suer if the schedule is emphasized. The values used for model factors currently are assumptions ± no historical project data is available from Crusader to represent the relationships among the factors. These assumptions will be replaced with factors calculated from data collected on the Crusader project as development progresses and the relationships are more clearly understood. The intent is to have a working model by the end of 2000 useful for management decisions. The screen capture in Fig. 7 shows the output for a single simulation run. The Actual (simulated) values are dierent from the Expected values for each set data being plotted. There are always be dierences as the

model is currently set up because the Size and Quality data is randomized for each iteration, making it dierent from the expected size and quality data. This causes the Size and Quality Factors to be greater than or less than 1. Depending on amplitude and direction of the dierences as compared to expected, the actual values would be above or below the expected values for each of the plots above. This model uses a Monte Carlo simulation, which means there are random draws for each execution, making the results dierent. With all inputs being the same, in a single execution of the Module X Preliminary Design process, the outcome could be as shown in Fig. 7 or some other outcome. 9. Conclusions and future work A dynamic model of the software development process allows answer questions such as: ``How much time and eort will it take to complete the Preliminary Design phase for Module X?'', ``How many defects are going to be generated during that phase? How will that aect product reliability?''. ``How much will the eort increase if defect prevention practices are used? How will that shorten system testing time?''. These questions are very dicult to answer using methods currently available in industry. There is a great deal of variability in the possible outcomes using static models. The modeling

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182

181

Fig. 7. Output screen for Module X Discrete Event Model.

methods described in this paper provide insight into why and how things occur. Crusader management believes there is a great potential bene®t for using the discrete event modeling approach not only for reliability, but for many other aspects of project management, including cost and schedule. With good data this model can be continuously updated and improved throughout the current program phase, which concludes at the end of 2000. Prior to the next phase, Engineering and Manufacturing Development (EMD), the model should be validated to the point where it supports eective management decisions. Outputs predicted by simulation will be compared with actual data collected from the project. Discrepancies will be identi®ed and used to improve the validity and accuracy of the model. With a real-world model, management can make good decisions throughout the EMD phase of the program to meet software reliability requirements. References
Abdel-Hamid, T., Madnick, S.E., 1991. Software Project Dynamics An Integrated Approach. Prentice-Hall, Englewood Clis, NJ. Boehm, B.W., 1984. Software engineering economics. IEEE Transactions on Software Engineering 10 (1), 5±21. Boehm, B.W., Hoh, 1996. Identifying quality-requirements con¯icts. IEEE Software 25±35. Jones, C., 1986. The SPR feature point method. Software Productivity Research.

Extend ± Performance modeling for decision support, 1995. User's Manual, Imagine That. Lakey, P., Neufelder, A.M., 1996. System and Software Reliability Assurance Notebook, Produced for Rome Laboratory by SoftRel. Contact peter.b.lakey@boeing.com for a copy of the Notebook. Madachy, R., 1996. System dynamics modeling of an inspection-based process. Proceedings of the Eighteenth International Conference on Software Engineering, Berlin, Germany. Putnam, L.H., 1992. Measures for Excellence ± Reliable Software on Time, Within Budget, Yourdon Press Computing Series. Rus, I., 1998. Modeling the impact on project cost and schedule of software reliability engineering strategies. Ph.D. Dissertation, Arizona State University, Tempe, Arizona. SAIC, 1987. Methodology for software reliability prediction. Rome Laboratory RADC-TR-87-171. Tausworthe, R., Lyu, M., 1996. Software reliability simulation. In: Lyu, M. (Eds.), Handbook of Software Reliability Engineering. IEEE Computer Society Press, McGraw-Hill, New York. Tvedt, J.D., 1996. An extensible model for evaluating the impact of process improvements on software development cycle time. Ph.D. Dissertation, Arizona State University, Tempe, Arizona. Ioana Rus is a Ph.D. candidate in the Department of Computer Science and Engineering at Arizona State University. She received her Master in Computer Science degree from Arizona State University and Bachelors of Science from Polytechnical Institute Cluj, Romania. Her research interests include software process improvement, process modeling, software quality and reliability, arti®cial intelligence, and neural networks. She is a member of the IEEE Computer Society and ACM. James S. Collofello is a professor in the Department of Computer Science and Engineering at Arizona State University. He received his Doctor of Philosophy degree in Computer Science from Northwestern University, Master of Science in Mathematics and Computer Science from Northern Illinois University, and Bachelors of Science in

182

I. Rus et al. / The Journal of Systems and Software 46 (1999) 173±182 his BSEE from Northwestern University. He joined McDonnell Douglas (now Boeing) in 1990 as a Reliability Engineer. As an engineer in the Software Engineering Process Group (SEPG), Mr. Lakey has been responsible for developing tools and processes that can be used by internal customers to improve software product quality. His main responsibility now is for managing a Software Reliability Support subcontract with United Defense in Minneapolis on a large Army program called the Crusader.

Mathematics and Computer Science from Northern Illinois University. His teaching and research interests are in software quality assurance, software reliability, safety, and maintainability, software testing, software error analysis, and software project management. He is a member of the IEEE Computer Society and ACM. Peter B. Lakey received his MS in Engineering Management from University of Missouri-Rolla, MBA from Washington University, and

A System Dynamics Software Process Simulator for Staffing Policies Decision Support
Dr. James Collofello Dept. of Computer Science and Engineering Arizona State University Tempe, Arizona 85287-5406 (602) 965-3190 collofello@asu.edu Ioana Rus, Anamika Chauhan Dept. of Computer Science and Engineering Arizona State University Dan Houston Honeywell, Inc. and Dept. of Industrial and Management Systems Engineering Arizona State University Douglas Sycamore Motorola Communication Systems Divsions Scottsdale, Arizona Dr. Dwight Smith-Daniels Department of Management Arizona State University Abstract
Staff attrition is a problem often faced by software development organizations. How can a manager plan for the risk of losses due to attrition? Can policies for this purpose be formulated to address his/her specific organization and project? Proposed was to use a software development process simulator tuned to the specific organization, for running "what-if" scenarios for assessing the effects of managerial staffing decisions on project's budget, schedule and quality. We developed a system dynamics simulator of an incremental software development process and used it for analyzing the effect of the following policies: to replace engineers who leave the project, to overstaff in the beginning of the project or to do nothing, hoping that the project will still be completed in time and within budget. This paper presents the simulator, the experiments that we ran, the results that we obtained and our analysis and conclusions. Introduction We used process modeling and simulation for estimating the effect on the project cost, schedule and rework of different staffing policies. Assessment of such managerial decisions could be also done by other methods like formal experiments, pilot projects, case studies, and experts opinions and surveys. Modeling and simulation is preferred because it does not have the disadvantages of the other methods (like possible irrelevance in the case of formal experiments or pilot projects, interfering with the real process and taking a long time for case studies or subjectivity for experts opinions and surveys). System Dynamics Modeling A software process model represents the process components (activities, products, and roles), together with their attributes and relationships, in order to satisfy the modeling objective. Some modeling objectives are to facilitate a better human understanding and communication, and to support process improvement, process management, automated guidance in performing the process, and automated process execution [6]. In addition, we demonstrate how process modeling experimentation can be used to investigate alternatives for organizational policy formulation. There are many modeling techniques developed and used so far, according to the modeling goal and perspective. The paradigm chosen for this research was system dynamics modeling. Richmond considers system dynamics as a subset of systems thinking. Systems thinking is "the art and science of making reliable inferences about behavior by developing an increasingly deep understanding of underlying structure" [11]. Systems thinking is, according to Richmond, a paradigm and a learning method. System dynamics is defined as "the application of feedback control systems principles and techniques to modeling, analyzing, and understanding the dynamic behavior of complex systems" [2]. System dynamics modeling (SDM) was developed in the late 1950's at M.I.T. SDM is based on cause-effect relationships that are observable in a real system. These cause-effect relationships constantly interact while the computer model is being executed, thus the dynamic

interactions of the system are being modeled, hence its name. A system dynamics model can contain relationships between people, product, and process in a software development organization. The most powerful feature of system dynamics modeling is realized when multiple cause-effect relationships are connected forming a circular relationship, known as a feedback loop. The concept of a feedback loop reveals that any actor in a system will eventually be affected by its own action. Because system dynamics models incorporate the ways in which people, product, and process react to various situations, the models must be tuned to the organizational environment that they are modeling. SDM is a structural approach, as opposed to other estimation models like COCOMO and SLIM, that are based on correlation between metrics from a large number of projects. The automated support for developing and executing SDM simulators enables handling the large complexity of a software development process which can not be handled by a human mental model. Building and using the model results in a better understanding of the cause-effect relationships that underlie the development of software. A simulator to be used for estimating, predicting, and tracking a project requires a quantitative modeling technique. SDM utilizes continuous simulation through evaluation of difference and differential equations. These equations implement both the feedback loops that model the project structures as flows, as well as the rates that model the dynamics of these flows. Thus, application of an SDM model requires an organization to define its software development processes and to identify and collect metrics that characterize these processes. Simulation enables experimenting with the model of the software process, without impacting on the real process. The effects of one factor in isolation can be examined, which cannot be done in a real project. The outcome of the SD simulation has two aspects: an intellectual one, consisting of a better understanding of the process and a practical one, consisting of prediction, tracking, and training. SDM was applied to the software development process for the first time by Tarek Abdel-Hamid and Stuart Madnick [2]. Their model captures the managerial aspects of a waterfall software life cycle. It was the starting point for many subsequent models of the entire process [15], [14] or parts of it [5] and [9] that have been successfully used for resource management [8], [13], process reengineering [4], project planning, and training [12]. Abdel-Hamid [1] studied the impact of turnover, acquisition, and assimilation rates on software project cost and schedule. He found that these two response variables can be significantly affected by an

employment time of less than 1000 days, by a hiring delay less than or greater than 40 days, or by an assimilation delay greater than 20 days. We take this discussion a step further into the practical realm by focusing on the turnover issue and demonstrate how SDM can be used in decision support for strategies that address staff turnover. Description of System Dynamics Model Our system dynamics modeling tool was developed using the ithink simulation software [7]. The model incorporates four basic feedback loops comprised of non-linear system dynamic equations. The four feedback loops were chosen because they encompass the factors that are typically the most influential in software projects [16]. All of these feedback loops begin and end at the object labeled, Schedule and Effort, which is the nucleus of the system. This object represents a project schedule and the effort in person hours to complete a schedule plan. See Figure 1. The first feedback loop represents the staffing profile of a project (refer to the loop "Schedule and Effort", "Staffing Profile", "Experience Level", and "Productivity" on Figure 1). The staffing profile affects productivity based on the number of engineers working on a project, the domain expertise of the engineers, and amount of time an engineer participates on a project. The second feedback loop models the communication overhead (refer to the loop "Schedule and Effort", "Staffing Profile", "Communication Overhead", and "Productivity" on Figure 1). The more people on a project will result in an increase in communication overhead among the team members and thus, decreases the productivity efficiency [14] [15]. The third feedback loop takes into consideration the amount of defects generated by the engineers during the design and coding phases of an increment, which translates into rework hours (refer to the loop "Schedule and Effort", "Staffing Profile", "Experience Level", "Defect Generated, and "Rework Hours" on Figure 1). The tool also models the impact of domain expertise on defect generation. An engineer with less domain expertise generates more defects than an engineer with a higher degree of domain expertise. The fourth feedback loop models the schedule pressure associated with the percentage of work complete per the schedule (refer to the loop "Schedule and Effort", "Schedule Pressure", and "Productivity" on Figure 1). The farther behind schedule the greater the schedule pressure. As schedule pressure increases, engineers will work more efficiently and additional hours, increasing productivity toward completing the work. However, if schedule pressure remains high and

engineers are working many hours of overtime, they begin to generate more defects and eventually an exhaustion limit is reached. Once the exhaustion

limit is reached, productivity will decrease until a time period such that the engineers can recoup and begin working more productively again.

Schedule Pressure

Rework Hours

Schedule and Effort Productivity

Defects Generated

Staffing Profile Communication Overhead Experience Level

Figure 1. Basic Feedback Loops of the Model The model inputs are summarized in Table 2 familiar. For a summary of the experts' credentials, The model outputs are provided in the form of refer to Table 1. graphs that plot the work remaining in each Each expert (or evaluator) was asked to review the increment, current staff loading, total cost, percent tool from a Project Leader position, a Software complete, and quality costs. Figure 2 is an example Developer position, and to assess the tool's technical of the output that shows the graph for Staff Loading characteristics. During the simulations, the experts and Total Cost plots. were to observe the output for valid trend data and for The model was validated by two methods, expert a level of comfort with the accuracy of the results. opinion and reproduction of actual project results. Six The demo given to each expert was identical. After experts from Motorola and very experienced software completing the demo, the experts were given an professionals, examined the simulator model and opportunity to run different scenarios with the expressed their confidence in its ability to model the supplied data or scenarios with their own project data. general behavior of a software project. In addition, Finally, they were given an opportunity to compare one of the experts used the model to accurately the results to any project manage tools they use. reproduce the results of a project with which he was

Table 1. Background Summary of Each Expert Expert #1 Title: Principal Software Engineer Degrees: M.S. and Ph.D. in Computer Science Years of Experience: 13 Years Responsibilities: Project Leader, Systems Engineer, Software Engineer, and Proposal Development. Other Pertinent Information: Former Assistant Professor of Computer Science at Arizona State University, Tempe, AZ. Title: Principal Software Engineer Degrees: Ph.D. in Computer Science Years of Experience: 25+ Years Responsibilities: Project Leader Other Pertinent Information: Former Assistant Professor of Computer Science at Arizona State University, Tempe, AZ. Title: Software Engineer Degrees: B.S in Computer Science Years of Experience: 11 years Responsibilities: Software Development - Currently developing a ManMachine Interface. Special Awards: Special Achievement Award, 1987 - Naval Avionics Center Exceptional Performance, 1991 - Motorola GSTG, CSO Engineering Award, 1993 - Motorola GSTG Title: Software Engineer Degrees: M.S. in Computer Science; M.B.A. in Management Years of Experience: 33 Years Responsibilities: Developing software for a medium size aerospace project. Other Pertinent Information: A retired Air Force Lieutenant Colonel. Title: Chief Software Engineer Degrees: B.S. in Electrical Engineering Years of Experience: 39 Years Responsibilities: Software Process and Quality Special Awards: Dan Nobel Fellow Award (Motorola); IEEE contributor Other Pertinent Information: Author: Motorola GED's Software Process Manual. Instructor: Motorola GED Project Leader Training. Chair: Motorola GED Software Metrics Working Group. Chair: Motorola GED Software Training Program Working Group. Member: Motorola GED Software Defect Prevention Working Group. Three Patents. Developing software fault occurrence and prediction model. Title: Engineering Metrics Manager Degrees: B.S. in Electrical Engineering Years of Experience: 35 Years Responsibilities: Coordinate collection and analysis of Engineering metrics. The first highlight was seeing the results from simulating expert #3's Man-Machine Interface project, which was 80% completed. Before simulating the project, some actual historical data along with some remaining estimated data to complete the ManMachine Interface project was entered into the simulator. After simulating the project with actual

Expert #2

Expert #3

Expert #4

Expert #5

Expert #6

Overall, the tool fared extremely well against the evaluation process and the experts were quite pleased with the results. There were two particularly exciting highlights that occurred during the reviews with expert #3 and expert #5 that are worth noting.

and estimated data, Currently the simulator tracked there is an updated Number of increments 3 within a couple of version of the Productivity percentage points of simulator tool that Engineer 1 (domain inexperienced) .8 expert #3's actual and has been Engineer 2 (domain experienced) 1.0 predicted results. incorporated into a Engineer 3 (very domain 1.2 The second corporate training experienced) highlight was program. The Defect generation rates simulating three sets simulator is being Engineer 1 (domain inexperienced) .0300/hr of project data used to Engineer 2 (domain experienced) .0250/hr supplied by expert #5. demonstrate an Engineer 3 (very domain .0225/hr He generated these incremental experienced) three scenarios with development Defect detection the aid of the approach over a % of defects found in peer reviews 80% COCOMO tool for waterfall % of defects found in integration test 20% the three engineering development Percent of schedule allocated to rework 10% activities: Detail approach, peer Defect removal costs Design, Code and review Found in peer reviews 2 hr/defect Unit Test, and effectiveness, Found in integration testing 10 hr/defect Integration. After schedule simulating the three compression scenarios, expert #5 concurred that the simulator concept, mythical man-month perception, and the produced believable results for all three scenarios [14]. 90% completion syndrome. Description of the experiment Attrition is clearly detrimental to a software development project. When an experienced developer leaves a project prior to its completion, then a project manager faces the question of whether to replace the departing individual or to forego the expense of replacement and make other adjustments, such dropping functionality, slipping the schedule, and rearranging assignments. The answer to the question of replacement may turn on factors such as the experience of the developer, the percent completion of the project, the number of engineers on the project, the time required for hiring or transfer, and the attrition rate of the organization. Replacement is costly, but may be required to keep a project on schedule. It leads one to wonder, can the dilemma presented by attrition be resolved by yet another alternative, that of staffing a project with more than the necessary number of development engineers? Are there project situations in which it is economically feasible, or even desirable, to mitigate the risk of attrition by overstaffing? The experiment described here was motivated by these questions and the results offer an indication of the desirability of staffing policies which include the overstaffing option.

Table 2. Model Inputs

Number of 1: engineers Total Staff Load Time of attrition 1: 50.00 Attrition2:rate 2932.12

Low Value 10 2: Total Cost At the end of increment 1 10%

High Value 20 At the end of increment 2 30%
2

1: 2:

25.00 1466.06

1

2 1

1
1: 2: 0.00 0.00 0.00 Budget: Page 1

1 2
500.00

2

1000.00 Hours

1500.00

2000.00

Figure 2. Sample Output for Staff Loading and Total Cost were used with corresponding productivity factors The experiment was conducted for an incremental (Table 2). The departing engineers were assumed to project, consisting of three equal and non-overlapping be very experienced, while the replacements were increments, with a total effort estimate of 9000 personassumed to be inexperienced. The learning rate is hours. This is roughly equivalent to ten engineers such that inexperienced and experienced engineers completing an increment every eight weeks. Three advance one experience level with each increment levels of application domain experience completed, until they become very experienced. (inexperienced, experienced, and very experienced)  Time in the project at which attrition occurs Three strategies were considered:  Attrition rate  No replacement after attrition The model's response variables recorded for this  Replace engineers as they leave experiment were:  Overstaff at the beginning of the project at the  Project duration relative to the estimated schedule same level as the attrition  Project cost These strategies were considered for combinations  Rework cost (hrs) of three factors: Each strategy was evaluated for two values of each  Number of engineers on the project of the three factors (Table 3).

Table 3. Factors for Studying Staffing Strategies for Attrition

Time of Attrition (End of Number of Run Engineers Increment) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 10 10 10 10 10 10 10 10 10 20 20 20 20 20 20 1 1 1 1 1 1 2 2 2 1 1 1 2 2 2

Attrition Rate 10 10 10 30 30 30 30 30 30 10 10 10 10 10 10

Action: Replace (Y/N) or Overstaff N Y O N Y O N Y O N Y O N Y O

Duration Relative to Estimated Schedule 1.00 .97 .96 1.09 .98 .95 1.04 .98 .91 1.10 1.08 1.08 1.09 1.09 1.08

Project Cost ($1000) 675.10 683.68 701.10 558.30 646.32 692.17 641.10 670.57 738.58 682.70 705.32 741.60 713.90 716.11 772.40

Rework Cost (hr) 771 789 822 724 816 902 778 789 918 833 859 924 851 851 940

Experimental results The data in Table 4 reveals the trends found in the simulation results. Table 4. Results from Simulating Staffing Strategies for Attrition When the number of engineers working on a project is small and attrition rate is low, the effect of attrition is not very visible. For example, consider Runs 1, 2, and 3 in Table 4. In this case, the number of engineers is at the low level (10) and the attrition rate is also at the low level (10%). The results of Run 1 indicate that the project is completed on time without replacement or overstaffing. The fact that the project completes early or on time in all three of these runs suggests that attrition does not have a significant effect if the number of engineers is low and the attrition rate is low. As both the number of engineers working on a project and the attrition rate increase, the effects of attrition becomes more visible. Consider Runs 4, 5, and 6 in Table 4. In Run 4, the estimated schedule is overrun by 9%, but Runs 5 and 6 show early completion. Also, the effects of attrition are more visible when attrition occurs at the end of increment 1 than when it occurs at the end of increment 2. In Run 4 when attrition occurs at the end of increment 1, the project overruns the schedule by 9%, whereas in Run 7 when it occurs at the end of increment 2, the schedule overrun is only 4%. Also, in Run 6, the schedule underrun is 5%, whereas in Run 9, it is 9%. These runs also indicate that attrition costs more when it occurs later in the project. The results in Table 4 may also be viewed as an evaluation of three staffing strategies for attrition in five sets of the three factors. Each set is comprised of (number of engineers, increment number ending at time of attrition, and attrition rate). Because we are considering three factors in this experiment, the orthogonal relationships of these five sets of factors are illustrated in the cube of Figure 3. When the staffing strategy for attrition is compared within each set of factors, desirable strategies can be identified based on cost and project duration against the estimated schedule. These results are summarized in Figure 4, which adds the desirable strategies to the cube of Figure 3. In Figure 4, N represents the strategy of no replacement and no overstaffing, R represents the replacement strategy, and O represents the overstaffing strategy. In some situations either of two strategies is desirable, depending on whether one wishes to minimize cost or complete the project on

schedule (or in the N for cost (20,2,10) case, (10,2,30) minimize the R for schedule Increment no. schedule overrun). For e x a m p l e , completed point (10,1,10) in at time of Figure 4 represents a attrition N for cost (20,2,10) project in which the O for schedule ratio of estimated effort to the number of Attrition rate engineers is high (say finish about 3% (10,1,30) early but will add N for cost about 1% to the cost R for schedule of the project. Overstaffing to anticipate this (10,1,10) attrition will allow (20,1,10) R N No. of engineers the project to complete about 4% early and will increase the project cost about 4%. The

900 hours) and one person leaves at the end of Increment 1. In this scenario, the project can finish on-time without replacing the person. Replacing the person will allow the project to most desirable strategy for this set of factors, then, is no action (N) because it allows the project to finish on time at the lowest cost.

As another example, in the case of 20 engineers on the project and two people leave at the end of increment 2 (point (20,2,10) in Figure 4), other schedule regardless of the chosen course of action, but overstaffing (O) will limit the overrun. Neither

strategies are indicated. The project will overrun the estimated replacing nor overstaffing (N) remains the best option for minimizing cost.

Figure 3. Relationships of Factor Sets in Table 2.

Figure 4. Desirable Staffing Strategies for Attrition under Each Set of Factors most influential factors for modeling software dynamics and, more importantly, their degrees of Conclusions and Future Research influence on project outcomes. Our research grew out of the concern for having strategies to address attrition in software development organizations. We also wished to examine the ability of a more abstract simulator, composed of four process feedback loops, to provide realistic data for supporting the formulation of staffing policies. This experiment suggests several implications for software project staffing in response to attrition. In general, no action for attrition is the least expensive choice and overstaffing is the most expensive choice. The choice of no action is indicated when schedule pressure is high and cost containment is a priority. The replacement strategy has the advantage of alleviating the exhaustion rate and concomitant increases in attrition. Even though overstaffing is the more expensive option, it can have the very desirable effect of minimizing project duration. Thus, this strategy should be considered for projects in which completion date has been identified as the highest priority. Such a circumstance might occur if delivery of a commercial application is required during a market "window of opportunity," or if the contract for a custom application has a penalty attached to late delivery. The cost of overstaffing must be weighed with the other factors in the project and process simulation can provide data for this decision. Regarding process modeling objectives and the use of SDM, this experiment has demonstrated the use of a software development process simulator, one which models the major dynamic influences in a project. The software practitioners who examined the results produced by this experiment found them to be both acceptable and realistic. This model is capable of supporting designed experimentation and providing data for supporting staffing decisions. This group continues to research in software development process simulation. In addition to the questions of attrition, the group continues investigating into questions related to project management training, project risk assessment, and product quality. In terms of process modeling issues, further research continues to identify and validate the

References
[1] Abdel-Hamid, Tarek, A Study of Staff Turnover, "Acquisition, and Assimilation and Their Impact on Software Development Cost and Schedule", Journal of Manager Information Systems, Summer 1989, vol. 6, no. 1, pp. 21-40. [2] Abdel-Hamid, Tarek and Stuart E. Madnick, Software Project Dynamics An Integrated Approach, PrenticeHall, Englewood Cliffs, New Jersey, 1991. [3] Abdel-Hamid, Tarek, "Thinking in Circles", American Programmer , May 1993, pp. 3-9. [4] Bevilaqua, Richard J. and D.E. Thornhill, "Process Modeling", American Programmer, May 1992, pp. 3-9. [5] Collofello, James S., J. Tvedt, Z. Yang, D. Merrill, and I. Rus, "Modeling Software Testing Processes", Proceedings of Computer Software and Applications Conference (CompSAC'95), 1995. [6] Curtis, Bill, M. I. Kellner and J. Over, "Process Modeling", Communications of the ACM, 35(9), Sept. 1992, pp. 75-90. [7] ithink Manual , High Performance Systems, Inc., Hanover, NH, 1994. [8] Lin, Chi Y., "Walking on Battlefields: Tools for Strategic Software Management", American Programmer, May, 1993, pp. 34-39. [9] Madachy Raymond, "System Dynamics Modeling of an Inspection-based Process", Proceedings of the Eighteenth International Conference on Software Engineering, Berlin, Germany, March 1996.

[10] Richardson, George P. and Alexander L. Pugh III, Introduction to System Dynamics Modeling with DYNAMO, The M.I.T. Press, Cambridge, MA, 1981. [11] Richmond, Barry, "System Dynamics/Systems Thinking: Let's Just Get On With It", International System Dynamics Conference, Sterling, Scotland, 1994. [12] Rubin, Howard A., M. Johnson and Ed Yourdon, "With the SEI as My Copilot Using Software Process "Flight Simulation" to Predict the Impact of Improvements in Process Maturity", American Programmer, September 1994, pp. 50-57. [13] Smith, Bradley J, N. Nguyen and R. F. Vidale, "Death of a Software Manager: How to avoid Career Suicide

through Dynamic Software Process Modeling", American Programmer , May 1993, pp. 11-17. [14] Sycamore, Douglas M., Improving Software Project Management Through System Dynamics Modeling, Master of Science Thesis, Arizona State University, 1996. [15] Tvedt, John D., An Extensible Model for Evaluating the Impact of Process Improvements on Software Development Cycle Time, Ph.D. Dissertation, ASU, 1996. [16] Beagley, S.M., "Staying the Course with the Project Control Panel." American Programmer (March 1994) 29-34.

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996

A Design-based Model for the Reduction of Software Cycle Time
Ken W . Collier, Ph.D. Assistant Professor Computer Scienceand Engineering Northern Arizona University, Flagstaff, AZ Ken.Collier@nau.edu Abstract
This paper presents a design-based soj?ware cycle time reduction model that can be easily implemented without replacement of existing development paradigms or design methodologies, The research results suggest that there are many cycle-time factors that are influenced by design decisions. If manipulated carefully it appears that an organization can reduce cycle-time and improve quality simultaneously. The preliminary results look promising and it is expected that further experimentation will support the use of this model. This paper will analyze the basis for the model proposed here, describe the model' s details, and summarize the preliminary results of the model.

JamesS. Collofello, Ph.D. Professor Computer Science Arizona StateUniversity, Tempe, AZ James.Collofello@Asu.Edu
advantage of emerging technologies [ 11. Although much has been written about cost estimation, effort estimation and productivity and process improvement, little research has been directly aimed at reducing software cycle time. The proposed model is based on a systematic and rigorous analysis of the software process and identification of product factors that affect cycle-time, as well as the impact of design decisions on those factors. This design-based model attempts to reduce software development time by iteratively and strategically restructuring the design of a software system. In order to be practical, such a model must provide significant benefits at a minimaI cost. It must be able to be implemented simply without requiring an excess of special training. It must be applicable to as many different types of software systemsas possible, and it must provide measurablebenefits. It should also work in concert with, instead of as a replacement for, existing models and methodologies. This paper is divided into three major sections. The first section provides motivation for the design-based model presented in the paper. It does so by demonstrating the impact of software design decisions on cycle-time, software quality, and scheduling options. The second section details the model and provides theoretical justification for an algorithmic approach to schedule refinement. The model also provides some guidance for restructuring the design to further shorten cycle time. Finally, the results of an evaluation of the model are provided. Although these results are inconclusive, they suggest that the model has promise and meets the requirementspreviously established.

Introduction
Many organizations have relatively mature, effective software-development processes in place and have employed talented software engineers and managers to implement these processes. In such organizations, it is appropriate to question whether each softwaredevelopment effort is performed at peak efficiency and effectiveness. One of the primary goals of any organization is to release high quality software in as little time as possible. Therefore, an increase in software-development efficiency may be manifest as a reduction in development time. However, simply reducing development time is not good enough. It is important that these reduction efforts maintain high levels of both process and product quality. This research involves the examination and control of factors that have an impact on software development time. More specifically, this paper presents a model for reducing the development cycle time by restructuring software design. Reduction of development time, together with increasing quality and productivity, is the goal of many software development organizations. The benefits are numerous including: extended market life; increased market share; higher profit margins; and the ability to take 733 1060-3425/96$5.00 0 1996 IEEE

Software Cycle Time Factors
Design DecisionsAffect Cycle Time and Software Quality
Early in this research project, a systematic analysis of cycle-time factors and issues was conducted. The details of this analysis are outlined in [2]. However, an overview of

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996

the results of that study will serve to motivate the designbasedmodel proposed in this paper. First, the software development cycle was examined to establish a comprehensive set of factors impacting cycletie. These factors were divided into process factors and product factors. Process factors include: software reuse, requirements change, risk management, productivity, personnel availability, software tools, equipment resources, method maturity, verification and validation techniques, quality assurance techniques, integration strategies, and integration schedule. Product factors include: subsystem dependencies, module reuse, subsystem complexity, subsystem independence, subsystem risk, subsystem size, and subsystemgeneralization. Notably, most of these factors are impacted by design decisions. This recognition led to a set of cycle-time improving design goals including: maximize reuse; design independent subsystems:design simple subsystems;ensure completeness; localize risk: design to minimize risk occurrence; ensure correct design: design for low coupling; design for high cohesion; design to maximize independence; design unique subsystems; and design for flexible scheduling. A fortunate, and unexpected side-effect of this analysis is that these are the same design goals that improve design quality. This analysis showed that design decisions impact both design quality and cycle-time, and that these goals are not mutually exclusive. Moreover, the analysis revealed that improving cycle-time through design decisions is likely to improve quality and vice versa

scheduling decisions that increase the chance that a good schedule will be found. The software design and scheduling process can be viewed as a process of making decisions to maximize the chance of finding the best accessibleschedule,given all scheduling constraints [2]. Currently, this attempt at finding a best accessible schedule is a function of intelligent decision-making, intuition, and the application of design and scheduling heuristics. An approach that is highly subject to the abilities and expertise of the software engineers and project managers. An ideal scheduling methodology should be a prescriptive process that encourages the result of a good scheduleregardlessof expertise and ability.

An Overviewof' Project Scheduling
In the late 1950s two computer-based scheduling systems were developed to aid in the scheduling of large engineering projects. Both are based on task dependency networks. The critical-path method (CPM) was developed by Du Pont and Remington Rand. The CPM method is a deterministic scheduling strategy that is based on the best estimate of task-completion time. The program evaluation and review technique (PERT) is a similar method and was developed for the US Navy. The PERT method used probabilistic time estimates [3]. In the CPM approach, a task-dependencynetwork is a directed acyclic graph (DAG), which is developed from two basic elements: activities (tasks) and events (completion of tasks). An activity cannOt be started until its tail event has been reached due to the completion of previous activities. An event has not been reached until all activities leading to it have been completed. Associated with each activity is an estimated time to completion. The critical path is the path through the DAG that represents the longest time to project completion. To help calculate critical path and identify critical activities a set of parameters is established for each activity including: duration, earliest start time, latest start time, earliest ftish time, latest finish time, and float. Float refers to the slack time between earliest start and latest start. All activities on the critical path have a total float of 0. This reflects the idea that critical-path activities must be completed on time in order to keep the project on schedule [3]. PERT scheduling uses many of the same ideas as CPM. However, instead of task completion being a "most likely" estimate, it is a probabilistic estimate. The estimator predicts an optimistic estimate, o, and a pessimistic estimate, p, of task completion. The most likely time, m, falls somewhere between those values. The time estimates are assumed to follow a beta distribution. The expected time is given as te = (o + 4m + p)/6. The expected times are calculated for each activity in the task-dependency network, and the critical path is then determined as in CPM [31.
732

Design DecisionsImpact Development Schedule
Scheduling decisions play a critical role in reducing cycle-time. Therefore, it is appropriate to analyze the factors that constrain scheduling options. The architectural design of a system dictates subsystem dependencies, thereby dictating the possible scheduling strategies. Prior to software design, implementation scheduling possibilities are relatively unbounded by product decisions, i.e., all schedulesare accessible. Selecting a design alternative can be viewed as a reduction in the set of accessible schedules [2]. There are many decisions that further constrain the set of accessible schedules.Staffmg, resource allocation, risk analysis, and identifying high-priority functionality, all constrain the set of accessible schedules. Therefore, making design decisions that home in on the optimal schedule is a cycletime reduction goal. It is unrealistic to suggest that one can always arrive at the best possible design or schedule. Moreover, there is no way of knowing when the best design or schedulehas been developed. A more practical goal is to make design and

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International

Conference on System Sciences - 1996

Shortening the Critical Path Software development time is measured by the critical path through the project' s task dependency network. Hence, all cycle-time reduction efforts can be viewed as efforts to shorten the critical path. There are basically two ways to shorten the critical path: Shorten the completion time of tasks on the critical path, or, remove activities from the critical path The first option can be achieved either by simplifying the task or by increasing the productivity of the work team assigned to the task. Task simplification can be accomplished by either dividing the task into simpler concurrent tasks or by eliminating unnecessarywork from the task. Productivity can be increased by improving: management techniques, resources, or development techniques; or by allocating additional resources [4,5,6]. Although it is not a primary topic of this paper, the maximization of productivity is fundamental to cycle-time reduction. Violating Task Dependencies The second approach to eliminating critical-path activities implies the violation of task dependencies.This goal requires an understanding of the types of relationships between task dependencies.Tasks can be divided into two general subcategories: programming tasks and nonprogramming tasks (e.g., training, technical reviews, etc.). Programming tasks correspond to design components in a software system, and may be low-level modules, or the integrations of multiple modules into subsystems. Therefore, dependenciesbetween programming tasks are connected to dependencies between the components in a design. There are three types of dependencies between tasks: Data Dependency - If module A imports information that module B exports, then module A is data dependentupon module B . Functional Dependency - If module A requires functionality provided by module B to complete its own function, then A is functionally dependentupon B. Resource Dependency - If the completion of module A requires resources that are currently allocated to module B , then A is resource dependentupon B . There is some cost involved in dependency violation. Otherwise, the ideal scheduling approach would be to complete all tasks in parallel and then integrate all at one time. Of course, as Fred Brooks observed, project scheduling and management is not this simple [7]. For programming tasks, the dependency violation cost occurs in the form of scaffolding (i.e., test drivers and code stubs) to simulate me parts of one module upon which another depends. As data and functional dependenciesare violated, the amount of required scaffold development

increases to simulate the dependency, thereby increasing the task completion time. This in turn increases the likelihood of defects being introduced into the code. As defects increase, the debugging time increases. When resource dependencies are violated, the cost dependsupon the type of resource. If two tasks require the use of some limited-access hardware device, then dependency violation might require the purchase of a secondsuch device. The cost is monetary. If two tasks both require some very specialized expertise that few team members possess, then dependency violation means training other programmers. The cost in this case is in terms of tune spent learning and retraining. Many factors contribute to the completion time of programming tasks. Each programming task represents a cycle of subactivities that includes detailed design, coding, unit testing, integration testing, and system testing; all of which are time-consuming. Additionally, there are process-management and control activities involved in each task. One must decide whether the benefits of violating a dependency outweigh the costs. Furthermore, it is unreasonable to expect that all dependencies share the same violation cost. The stronger or more complex the dependency,the greater the cost of violation. Dependency strength may be caused by the degree of coupling between modules or simply by the amount of accessof the module' s componentsby other modules. Consider two tasks, A and B, in which B is dependent upon the completion of A. Dependency-violation cost can be viewed as the addition of some percentage of the completion time of A to B' s completion tune. This percentagereflects the amount of A that must be simulated in order to complete task B before A is actually complete. A violation cost approaching 100% reflects the idea that task A is being simulated in its entirety, which defeats the purpose of violating the dependency. Conversely, an estimated violation cost approaching 0% reflects the notion that B' s dependency on A is artificial and both could be performed in parallel with little consequence. A dependencyclassification scheme is proposed to help determine dependencies that are good candidates for violation. If a dependency violation cost is estimated to range between 0% and 25%, then the dependency is classified as a weak dependency.If the cost is in the range 26%-50%, then the dependencyis moderate. If the cost is in the range 5 l%-75%, then the dependency is strong; while 76%-100% violation cost would be considered very
strong.

Accurately estimating the cost of violating dependenciesis a topic for future research. However, for programming tasks that are functionally or data dependent upon other tasks, the cost is primarily in the creation of code scaffolding. In this case, the estimated cost can be
733

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - 1996

derived from the software cost models used to estimate the initial task durations. Resource dependencies and nonprogramming tasks are not likely to be so simple. From these ideas on critical path shortening, a set of scheduling goals for the reduction of cycle-time form the basis for the model presentedin this paper: 1. Violate low cost dependencies to increase concurrent development whenever cost effective. 2. Transform high cost dependencies into low cost dependencies. 3. Reduce task-completion time by simplifying the task. 4. Reduce task-completion time by dividing it into concurrent subtasks. by increasing 5. Reduce task-completion time productivity.

simple.
l

Component complexity in design determines task stafing - Highly complex components may require

additional personnel resources thereby limiting the degree of concurrency in the schedule.
l

Development learning curves affect productivity and productivity affects task completion time - Design

The Relationship Between Scheduleand Design
The previous section was devoted to project scheduling and identifying the general strategies for shortening the critical path in a schedule. This section examines the impact of design decisions on scheduling outcomes. Moreover, identifying connections between design and scheduling provides a set of techniques for achieving the critical-path-shortening goals previously identified. In general a software design and its development schedule are closely connected due to the fact that design components dictate the work tasks in the development schedule. It is useful to identify other, more subtle, design-scheduleconnections.
l

Design dependencies determine schedule dependencies

- Any degree of coupling between two modules in the design translates into a dependency between the corresponding tasks in the schedule.
l

Design independence determines task concurrency

-

Modules in a design that are uncoupled can be developed in parallel, given no other constraints and assuming that they are not resource dependent.
* Component complexity in design determines task-completion time - Modules that are complex will

components that require programmers to learn special skills will take longer to implement than those components for which programmers are already trained. l So&are reuse afSects task-completion time - Code reuse is almost certainly faster than designing, developing, and testing code from scratch. In general, a design with complex components that are highly dependent upon one another will result in a highly serial schedulewith long task-completion times and excess staffmg needs. Conversely, a design with simple, independent components will result in a highly concurrent schedule with short task completion times and lower required staffing. The schedule-improvement goals previously listed are impacted by design decisions. Task dependencystrength is determined by intermadular dependencies (coupling). Task completion time is affected by maduEe complexity. Task concurrency is affected by intramadular dependencies (cohesion). Clearly, the coupling and cohesion heuristics play a role in determining the critical path length in a schedule. Additionally, information hiding, data abstraction, data localization, and fan-in/fan-out will affect the de pendencies in a development schedule. This is further evidence that existing quality metrics are coincident with the goal of cycle-time reduction. Furthermore, these existing heuristics can provide the necessarymechanisms for improving the development cycle time by a judicious refinement of the design.

A Cycle Time Reduction Model
In [21 a link is made between the factors that affect

take longer to implement and test than modules that are

Architectural Design

Figure 1 - Current Design Cycle

734

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International

Conference on System Sciences - 1996

development cycle time and software design decisions. This lii combined with the link between design and scheduling provide sufficient motivation for a design-based cycle-time reduction model. The model proposed is based on the convergence of the ideas presented in previous sections. This model aims to shorten cycle time by shortening the implementation phase through design and schedule refinement.

Model Description
Current state of the art in software design dictates an iterative process of design and refmement to converge on a design of the highest quality. Figure 1 reflects this notion. Under this model the development schedule is deferred until design is complete. There are some drawbacks to this design model. First, widely used design-based metrics do not provide a mechanism for determining when it is cost-effective to continue design refmement and when to stop refming and move to the next phase. Second, the current design model is not prescriptive. It is difficult to determine which parts of the design deserve refmement at any iteration of the cycle. Finally, although this iterative design process is the state of the art, it is not necessarily the state of the practice. It remains the tendency of many designers to adopt the first design that is generated. This may be becausethe benefits of iteratively improving design are difficult to quantify. The model proposed in this paper is also iterative in nature. However, development scheduling becomes an integral part of the design cycle. In this model the designers develop an initial design and then iteratively work to develop the best schedule (i.e., shortest critical path) that can be implemented using that design. Then the design is refined according to standard practices. Following refinement, a new "best schedule" is developed for the improved design and so on until the schedule does not continue to improve. This model is graphically representedin figure 2. The ultimate goal of this cycle-time reduction model is a design that improves development time without

sacrificing product quality. This model addressessome of the problems with current iterative design models. Critical path length is the driving metric for determining when it is cost-effective to continue design refmement or when to move on to development. Furthermore, because critical path is used to reflect the improvement of each iterative design refinement, the benefits of the iterative process are measurable and can be compared directly to the cost of refinement. For example, if it takes five programmer days to conduct a single design iteration and the critical path is shortened by only three programmer days, then it is clearly not cost-effective to continue refining the design. In this case, the method helps designers determine when to stop designing. The approach taken by this model is to
selectively apply design-improving techniques to key components in the design. The aim of this approach is to

minimize the effort and maximize the benefit.

How The DesignModel Works
This design cycle follows five basic phases: 1. Initial Design - During this phase the system is designed using existing methods and techniques. At this point in the process, the model does not differ from current design models. In fact, this cycle-time reduction design model may be thought of as a design metamodel, since it does not replace popular design techniques and paradigms but is symbiotic with existing methods. 2. Initial Schedule - The initial scheduling phase in this model employs current state-of-the-art scheduling and effort estimation techniques. 3. Schedule Refinement - The schedule-refmement phase of this design model is the point at which the schedule is iteratively analyzed and refined in an effort to shorten the critical path without altering the design. 4. Problem Identification - Now that the schedule has been refined sufficiently, this phase serves the purpose of identifying those system components that, if improved, represent a significant cycle-time improvement.

Architectural Design

Figure 2 - Cycle Time Design Model

735

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

~~~~~~~~ ~~

~_~~~ -~~

Proceedings of the 29th Annual Hawaii International Conference on System Sciences - I996 5. Design Refinement - During this phase efforts are made

to improve those tasks that were identified as problematic by the previous phase. This cycle is iterated using the critical path length as a cycle-time improvement metric. For each iteration of the design cycle, the critical path will reflect the benefit of the refinement efforts. Ideally, the architectural design cycle should halt as soon as the critical path becomes stable or when the benefits at each iteration do not justify the effort We do not propose to replace existing state-of-the-art design and scheduling methods in this model. Instead, the remainder of this section will focus on the development of methods for accomplishing steps 3,4 and 5. Schedule Refinement The third phase in this model deserves closer inspection. Ideally, this phase will reveal the optimal schedule for the current version of the design. It has been observed that the task dependenciesalong the critical path in a schedule can, potentially, be violated. A brief foray into graph theory will motivate an algorithmic approach to the problem of schedule optimization. The scheduling problem can be temporarily simplified by stating it as a graph-manipulation problem restricted by a set of rules. Given a directed acyclic graph (DAG) in which each vertex v has a weight that is represented by wt(v), the weight of the entire graph W is:
/ Vi

W = X wt(vi) ;3cVvi {c is a critical path, vi is a vertex is on C} The burden of a vertex v, b(v) is the sum of the

weights of all immediate predecessorsof v, which are on a critical path. The prehistory of v, ph(v) is the sum of the weights of all predecessorsof v, which are on the longest serial path from the initial vertices to v. These are likely to be the predecessorsof v that are on a critical path. The object is to reposition vertices in the graph in order to achieve the smallest weight (i.e., shortest critical path). However, there are rules for moving vertices: 1. Never disconnect the goal vertex from its predecessors. This is equivalent to eliminating subsystemsfrom the f& integration. 2. Vertices are repositioned by disconnecting them from their critical-path predecessors.This reflects the notion of violating task dependencies. 3. Whenever a vertex v is disconnected from its critical path predecessors,these predecessorsare reconnected to all of v' s immediate successors. This is to prevent the graph from becoming disconnected. 4. Whenever a vertex v is disconnected from its critical-path predecessors,its weight is adjusted by the expression: wt(v) = wt(v) + (m x b(v)). In this expression m representsthe percentageof v' s dependent tasks that must be simulated in order to develop and

test v (i.e., scaffolding). This reflects the cost of violating task dependencies. Some observations can be made from this view of schedule optimization. Fist, it is not likely to be beneficial to reposition slack path vertices. Second, a path that is critical at one point in the optimization process may cease to be critical at some future time. Third, there may be multiple critical paths in the graph at any given time. These conditions and others that are not so obvious must be addressedby any strategy that is to be used to solve this problem. An algorithmic strategy might implement a cycle of vertex repositioning followed by recalculating critical-path length. Such an algorithm must be able to determine which vertex to reposition at each iteration and when to stop. The selection of a vertex for repositioning must be conducted in light of a cost-benefit analysis. The benefit is measured as a reduction in W. The cost is represented by the increase in wt(v) due to dependency violation. The vertex that produces the greatest benefit and least cost is the prime candidate. Some additional observations can be made about the structure of critical paths in this problem space. It is possible that the critical path in a DAG of significant size will not be a single linear sequenceof vertices and edges. Multiple parallel critical paths may require repositioning multiple vertices before seeing a reduction in W. Furthermore, if the critical path diverges at one task and then reconverges later (i.e., becomes temporarily parallel), vertices prior to this divergence or following the reconvergenceshould be manipulated if possible in order to realize immediate benefits. The trouble with each of these scenarios is that they obscure the benefits of repositioning certain candidate vertices. The value of W may remain steady over several iterations before it begins to decrease.Rather than halting when no further decreasein critical path length is seen, it may be more beneficial to halt when an increase is detected. These observationsmotivate a simple greedy algorithm that iteratively repositions critical path vertices based on a cost-benefit analysis and recalculates the value of W, halting when the value of W increases. This algorithm always identifies the vertex that appears to promise the greatest immediate reduction in W. Greedy algorithms are often used in optimizing problems such as this. However, most greedy algorithms accept a "good" solution rather than guaranteeing an optimal solution. This algorithm is no exception. The problem of finding an optimal solution lies in the probability of reaching a local minimum It is possible that the repositioning of a vertex will temporarily increase W in order to realize a greater decreaseon future iterations. Unfortunately, guaranteeing an optimal solution to graph-shortening for a graph of any complexity is a hard

736

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International

Conference on System Sciences -

1996

problem which cannot be solved in polynomial time and is NP-complete [8,9, lo]. Calculating Dependency Violation Cost In determining the net benefit of dependency violation,
m represents the percentage of simulation of the tasks on

conditions and its use may result in a loss of accuracy. The remainder of the discussion of this model will use .25 as a value for m. However, until an empirically established upper bound value is established one may prefer to use actual violation cost estimates. An Algorithm for Schedule Refinement The graph theory concepts of the previous section form the basis for a schedule refmement algorithm that attempts to reposition critical path tasks. Both CPM and PERT represent a project schedule as a DAG in which vertices represent development activities and milestones, while edges represent dependenciesbetween tasks. The duration of each task corresponds to the weight of a vertex, wt(v), and the critical path duration corresponds to W. The prehistory of a task t, ph(t), is the sum of the durations of all tasks preceding t along the critical path. The burden of a task t, b(t), is the sum of the durations of all tasks that immediately precede t. The process of refining the schedule is, in essence, a process of selectively violating task dependencies in a cost-effective manner. It is assumed that the schedule follows a typical CPM or PERT format, in which the information for each task includes: earliest start date (ES), latest start date (LS), earliest finish date (El?), latest ftish date (LF), and duration (D). Furthermore, it is assumed that the dependencies between tasks in a schedule have been categorized using a scheme similar to the one previously described. The following strategy takes the conservative approach of violating only weak dependencies and assumes the worst case within that range (i.e., m = .25). Given more information, the procedure can be modified as is necessary. The algorithm is as follows: 1. Build a set D of weak dependencieson the critical path. These are the primary candidates for violation. 2. For each dependency i-+j E D, where task i is dependent on task j, calculate ph(i) and b(i). Remove from D any dependencies whose cost outweighs i' s prehistory, wt(i) + ph(i) 5 wt(i) + .25b(i). 3. For each dependency i+j E D, calculate the net benefit, n(i) = wt(i) - .25b(i), of removing the dependent task from the critical path and placing it in a slack path. Remove any dependenciesfrom D that have no net benefit, n(i) < 0. D now contains the fmal set of candidates for elimination. 4. Select the dependency i+j E D, for which n(i) is the greatest, remove the dependent task from the critical path, and replace it on a slack path using the following rules: l If i has no successorsand j becomes disconnected from all successors, then do not violate this dependency since j will never become integrated

which another task relies. This requires that up to 100% of the fast task must be simulated in order to complete the second task. Therefore, the cost of violating a dependency lies between 0% and 100% of the duration of the first task. Hence, for a single dependency the value of m is in the range [O.O,l.O]. The value of m increases as the number of tasks on which the target task is dependent increases and may exceed 1.O. Furthermore, whenever a task dependency is eliminated there are likely to be hidden costs due to added communication requirements, inaccuracies in estimating, etc. As m approaches 1 .O the likelihood increases that these hidden costs will cause the dependency violation cost to exceed the benefit. Therefore, it is useful to establish a maximum value for m above which a dependency should not be violated. Empirically defining an upper bound value for m is extremely difficult if not impossible. However, during the development of the model presented here, fifty different schedules and scheduling scenarios were examined. The purpose of these examinations was to identify the behaviors and consequences of task dependency violation. These efforts led to the observation that .25 appears to be a conservatively reasonableupper boundary value for m [ 11. It was consistently observed that whenever the estimated cost of violating a dependency was below 50%, the violation resulted in a decreasein critical path length. In fact, many critical path decreaseswere observed for cost estimates as high as 80%. However, providing for hidden costs, it is deemed prudent to take a highly conservative approach to selecting an upper bound value for m. It was observed during these exercises that 100% of the cases in which the value for m was below 30% resulted in a shortening of the critical path. Compensating for hidden costs, this model uses an upper bound of 25% for m. This choice has the added benefit of allowing the cycle-time reduction model to focus only on weak dependencies as candidates for violation. Assuming inaccuracies in the estimates of dependency violation cost, .25 represents a worst-case value within that range. Fixing m at .25 simplifies the algorithm since categorizing dependency strength is easier than accurately estimating the cost of dependencyviolation. This use of .25 as a fixed value for m in the cycle-time reduction model can easily be replaced by a more empirical value or by the actual violation cost estimate. The selection of a fixed value serves primarily to simplify the use of the model. This value was selected under ultraconservative
737

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International

Conference on System Sciences - 1996

into the system without an additional integration step. In this case, i is the final system integration. This is not likely to be cost-effective. l Connect j to all immediate successorsof i and update i' s ES, LS, EF, and LF values. This reflects the change in the integration strategy for i andi (i can now be developed concurrently with /I. * Update i' s duration by a factor of .25, D = .2.50 l All additional constraints that affect the development schedule should be maintained (e.g., resource allocation, risk prioritization, etc.). It is impossible to define rules for maintaining these constraints. Therefore, it is the responsibility of the scheduler to ensure that they are not violated. 5. Becausea new critical path may emerge as a result of task repositioning, steps l-4 are repeated until the schedule reaches a stable state (i.e., no changes can be made to the schedule) or until the critical path begins to increase. At best, the result of this algorithm is an optimal schedule for the given design under the given constraints. At the least, the resulting schedule will not be any worse than the original schedule. There are additional scheduling-improvement techniques that have not been addressed here, such as resource-leveling, which may help to shorten the critical path [3]. Used in conjunction with such techniques, the model describedhere offers a simple yet powerful means of reducing development time. It is based on observation, intuition, and the mathematical manipulation of task networks and CPM components. A major benefit to this scheduling method is that it is highly automatable once tasks have been identified and a preliminary schedule is developed. Problem Identification Once the development schedule has been refined there are two ways to further shorten the critical path: either remove tasks from the critical path, or shorten the completion time of tasks on the critical path. The only remaining critical tasks are those with high dependency-violation costs or weak dependencies for which there is no net benefit in removing them from the critical path. This phase in the model attempts to shorten the duration of a target task; or to prescribe design refinements that will result in weaker dependencies. The approach taken is to identify the design component that, if refined will result in the greatestcycle-time benefit. This approach is heuristic rather than algorithmic. General guidelines are as follows: 1. Identify tasks in the schedule that have the greatest impact on the critical path.

2. Identify dependenciesthat, if violated, would produce the greatestreduction in the critical-path length. 3. For the identified tasks, examine the potential for either simplifying or decomposing their corresponding design components and estimate the effort required to do so. Retain those that have the greatest benefit for the least effort4. For the identified dependencies, evaluate the effort required to weaken the dependency and evaluate the potential for success.Retain those that have the greatest benefit for the least effort. 5. From the retained tasks and dependencies, select the one that has the greatest overall net benefit in terms of critical-path shortening. 6. Ideally, one task or dependencywill he refined on each design-cycle iteration. However, it may be that by performing simple refinements to several tasks and/or dependencies,a major shortening of the critical path will be experienced on one iteration. These decisions must be determined based on individual project scenarios. Problem identification relies upon the talents of software engineers to make good decisions. These guidelines may easily be supplemented with additional, project-specific information to increase their benefit. Future work in validating cost and benefit estimating techniques wilI further strengthen this phase in the model. Design Refinement After identifying the most beneficial task or dependency for refinement, it is necessary to quickly identify the cause of the problem and resolve it. If the target of refinement is a dependency, then the aim is to weaken that dependency to make its dependent task a candidate for repositioning. Assuming the dependency is not resource dependent, it may be that efforts should be made to decreasecoupling between corresponding design components using existing design heuristics and principles. If the target of the refmement effort is task duration, reducing the complexity of corresponding components might be accomplished through simplification or subdivision. Subdivision implies that the module has poor cohesion. Ideally, system modules should be functionally cohesive. Therefore, the designer should work to reduce cohesion and divide the system component into multiple, independentsubcomponents.

Preliminary Results
This model cannot guarantee a reduction in actual cycle-time since many things can happen between software design and product release. However, preliminary results suggest that the model has promise in achieving the
738

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings of the 29th Annual Hawaii International

Conference on System Sciences - 1996

1 Actual Size Actual Effort DPS ......................... .......................................................................... ..................................................................... ............................................................ j 2 Actual Size Actual Effort Educated : ......................... .......................................................................... .................................................................... ............................................................. ; 3 Actual Size COCOMO DPS 4 Actual Size COCOMO Educated I I I 5 Actual Size Putnam DPS ......................... .......................................................................... ............................................................................................................................ i 6 Actual Size Putnam Educated ......................... .......................................................................... ............................................................................................................................ ; 7 Expected Size COCOMO DPS 8 Expected Size COCOMO Educated j I f 9 Expected Size ......................... ............................ ............................................ ...................................................................................... ; 10 Expected Size I......................... I........................... ............................................. I....................................................................
Table 1: Demonstration Strategies

following results: 1. Significantly reduce the estimated development time of a software product at the design phase. For the purposes of this effort, any reduction of estimated development time of 5% or more will be considered significant. 2. Maintain or improve the quality of the initial design (i.e., the model will not reduce design quality). 3. Help to focus design-refinement efforts on beneficial design components. Although empirical validation of this model is virtually impossible, demonstration of the model on a variety of software designs under a variety of conditions yields encouraging results. The model was applied to live software systems ranging in size from 736 source lines of code (SLOC) to 6,309 SLOC, and ranging in quality. Three factors were identified as affecting the initial schedule of a particular software design: size estimation technique; effort estimation technique; and scheduling technique. For size estimation in this analysis actual size and expected size were used. For effort estimation, actual efforts were used in addition to the COCOMO and Putnam models [ll, 121. In developing initial schedules, two approaches were used: dependency preserving scheduling (DPS) and educated scheduling. DPS refers to preserving all intermodular dependencies from the design on the corresponding development tasks. Educated scheduling refers to more common scheduling approaches in which task dependencies are determined by functionality, risk, resources,etc. Table 1 shows the variety of combinations of size estimation, effort estimation, and scheduling

approach. By combining each of these factors with each of the five software designs, forty-two different design-scheduling scenarios were used to observe the effects of the model (in some cases,certain combinations were infeasible). Table 2 shows the statistical results of these 42 demonstrations. The data is fairly scattered. However, in all but one of the demonstration cases, a significant (i.e., 5% or greater) reduction in estimated development time was observed. It is notable that the design refinements in each of the demonstration scenarios contributed as much or more to improving quality as to improving estimated cycle-time. The quality of each of the poor quality designs improved, while the high quality designs did not experience any loss in quality. The details of this demonstration of effectiveness can be found in [ 11. It should be noted that the demonstration scenarios are all small programs, and it remains to be seen how well this model scales to largescale software systems.

Summary and Conclusions
Current design cycles do not provide mechanisms for determining on which parts of the system to focus refmement efforts, or for determining when to stop iterating. This model provides a solution to both problems. Through a schedule analysis, the model guides the designer to focus refinement efforts on the most beneficial system components. By using the critical path as a metric of cycle-time improvement, this model helps determine when to halt the design cycle. Additionally, this model

Table 2: Results of 42 Model Demonstrations

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Proceedings

of the 29th Annual Hawaii

International

Conference on System Sciences - 1996

provides a mechanism for the designer to determine which techniques to apply in order to resolve targeted problems. In this manner, the cycle-time reduction model helps the designer selectively apply design-improving techniques to key components in the system in order to cost-effectively improve the development cycle time. Applying Laws of Pareto to cycle time, 20% of the software design will account for 80% of the cycle time. The goal of this model is to help software engineers focus their energy on the key 20% for improvement by identifying system components that most significantly impact the critical path length. There are a number of other benefits to this cycle time reduction model: l It is not a replacement methodology. This design approach allows software engineers to continue using state-of-the-art design and scheduling methods.
e This method provides design-improvement feedback.

Bibliography
111 Collier, K.W. "A Design-based Model for the Reduction of
Software Cycle Time", University, 1993. Ph.D. Dissertation, Arizona State

121 Collier, K.W. and J.S. Collofello, "Issues in Software Cycle
Time Reduction", Proceedings: International Phoenix Conference on Computers and Communications, March 2831,1995, pp. 302-309.

[31 Dieter, G.E. Engineering Design: A Materials and Process
Annroach, McGraw-Hill New York 1983. [41 Bisant, D.B. and J.R. Lyle, "A Two-Person Inspection Method to Improve Programming Productivity", IEEE Transactions on Sofhyare Engineering vol. 15 no. 10, 1989, pp. 1294-1304. Boehm, B., M.H. Penedo, D.E. Stuckle, R.D. Williams and A.B Pyster, "A Software Development Environment for Improving Productivity", Computer, vol. 17 no. 6, 1984, pp. 3042. Dart, S.A., R.J. Ellison and P.H. Feiler, "Software Development Environments", Computer, vol. 20 no. 11, 1987, pp. 18-28. I71 Brooks, F.P. The Mvthical Man-Month: Essavs on Software Enaineering, Addison-Wesley, Massachusetts, 1982.

l

Using the critical path as a metric, the impact of design refinements is clear. This method combines established practices. Popular scheduling methods, and design heuristics, cost-estimation techniques are supported in this model.
This method provides motivation for iterative design refinement. Currently, it is unclear how much benefit is

0

gained by placing energy into design iteration. This method provides this information in the form of units of time saved.
l

@I Boctor, F.F. "Some Efficient Multi-heuristic

The method provides a metric for measuring cycle-time reduction. Critical path length offers a quantitative

Procedures for Resource Constrained Project Scheduling", European Journal of Operational Research, vol. 49, 1990, pp. 3-13. Project Scheduling With a Limited Resource", International Journal of Production Research, vol. 29 no. 1, 1991, pp. 185-198.

measure of the effects of the model on estimated cycle-time reduction.
l

PI Khattab, M.M. and F. Choobineh, "A New Approach for

The method allows for fixed

resource

allocations.

Resource constraints that limit scheduling flexibility are accommodated by this model. Furthermore, if resources are not fmed, this model provides useful information for determining resource requirements.
l

[lo] Khattab, M.M. and F. Choobineh, "A New Heuristic for Project Scheduling With a Single Resource Constraint", Computers and Industrial Engineering, vol. 20 no. 3, 1991, pp. 381-387. [ll] Boehm, B. "Software Engineering Economics", IEEE Transactions on Sofiare Engineering vol. SE-10 no. 1, 1984, pp. 4-21.

Unrealistic schedules are identified early by this model. As the designs are refined for shorter cycle

times, the feasibility of initial scheduling estimates becomes apparent allowing for contingency plming and deadline renegotiation early in the product life cycle. Due to the small size of the software designs that were used to evaluate the model, the usefulness of the model on medium or large software designs is unclear. Further empirical evidence is required to make any conclusive statements about the model' s effectiveness. However, initial results are encouraging. This design-based cycletime reduction model can only improve as cost/effort estimation models become more accurate. Furthermore, this model is easily adaptable to organizational idiosyncrasies and to changing technologies,

[12] Putnam, L.H. and W. Myers, Measures for Excellence: Reliable Software on Time, Within Budget, Yourdon Press, New Jersey, 1992. [13] Yourdon, E. Mana&c!
Hall, New Jersey, 1989.

the Structured Techniques, Prentice-

740

Proceedings of the 1996 Hawaii International Conference on System Sciences (HICSS-29) 1060-3425/96 $10.00 © 1996 IEEE

Evaluating the Effectiveness of Process Improvements on Software Development Cycle Time via System Dynamics Modeling
John D. Tvedt and James S. Collofello Arizona State University Department of Computer Science and Engineering Tempe, AZ 85287-5406 USA E-mail: {tvedt, collofello}@asu.edu
Abstract
Reducing software development cycle time without sacrificing quality is crucial to the continued success of most software development organizations. Software companies are investing time and money in reengineering processes incorporating improvements aimed at reducing their cycle time. Unfortunately, the impact of process improvements on the cycle time of complex software processes is not well understood. The objective of our research has been to provide decision makers with a model that will enable the prediction of the impact a set of process improvements will have on their software development cycle time. This paper describes our initial results of developing such a model and applying it to assess the impact of software inspections. The model enables decision makers to gain insight and perform controlled experiments to answer "What if?" type questions, such as, "What kind of cycle time reduction can I expect to see if I implement inspections?" or "How much time should I spend on inspections?" · Being the first to market along with the high cost of switching products can give a product a great advantage in capturing and obtaining a large market share. · Higher profit margins. Being first to market gives a company greater freedom in setting profit margins. · The ability to start later and finish on schedule. Starting later allows the use of technological advances that may not have been available to the competition. "According to a study released in January 1990 by United Research Co. of Morristown, N.J., six out of 10 CEOs listed shorter product development cycles as vital to their company..." [9]. Most of these cycle time reduction reengineering efforts attempt to take advantage of process improvement technology such as that advocated by the Software Engineering Institute's Capability Maturity Model and ISO 9000. The implication is that a mature development process increases the quality of the work done, while reducing cost and development time [8]. Some software companies that have begun to mature their process have, indeed, reported cycle time reductions via process improvements [5].

1: Background
1.1: Importance of reducing cycle time
The rush to reengineer processes taking place in many organizations is the result of increasing competitive pressures to shorten development cycle time and increase quality. Reducing the time to produce software benefits not only the customer, but also the software organization. A number of benefits of reduced cycle time are listed by Collier [4]. · Market life of the product is extended.

1.2: The need for selecting among alternative process improvements
A common problem faced by organizations attempting to shorten their cycle time is selecting among the numerous process improvement technologies to incorporate into their newly reengineered development process. Even though there is promising evidence

trickling into the pages of computer publications, most companies are still skeptical of the investment that must be made before possible improvements will be experienced. They recognize that process improvements do not exist in isolation. The impact an improvement has may be negated by other factors at work in the particular development organization. For example, consider a tool that allows some task to be completed in a shorter period of time. The newly available time may just be absorbed as slack time by the worker, resulting in no change to cycle time. Thus, software organizations need to know, in advance of committing to the process improvement technology, the kind of impact they can expect to see. Without such information, they will have a hard time convincing themselves to take action. In particular, a software organization needs to be able to answer the following types of questions: · What types of process improvements will have the greatest impact on cycle time?; · How much effort should be allocated to the improvement?; · What will the overall impact be on the dynamics of software development?; · How much will the process improvement cost?; · How will the process improvement affect cycle time?; and · How will this improvement be offset by reduced schedule pressure, reduced hiring and increased firing? Today, the limited mental models often used to answer these questions are insufficient. The mental models often fail to capture the complex interaction inherent in the system. "It is the rare [manager] whose intuition and experience, when he is presented with the structure and parameters of [only] a second order linear feedback system, will permit him to estimate its dynamic characteristics correctly," [2].

which all factors except the independent variable (the process improvement) are kept constant. Although much can be learned through this approach about the general effectiveness of the process improvement, it is normally not practical to experiment over the entire development life cycle of a significant size project. Thus, it is often impossible to assess the overall impact of the proposed process improvement on development cycle time. Another approach typically followed is the utilization of some sort of "pilot project" to assess the new technology. Although considerably weaker than controlled experimentation, the pilot studies often reveal the general merit of the proposed technology. It remains difficult to assess the unique impact of the proposed improvement technology on cycle time due to the interaction of other project variables and to generalize the results to other projects. A third approach to assessing the impact of process improvements is the utilization of traditional cost estimation models such as COCOMO [3] and SLIM [10]. These types of models contain dimensionless parameters used to indicate the productivity and technology level of the organization. The parameter values are determined by calibration of the model to previous projects or by answering a series of questions on a questionnaire. For example, the Basic COCOMO model calculates the number of man-months needed to complete a project as: MM = C (KDSI)K, where MM = number of man-months, C = a constant, KDSI = thousands of delivered source instructions and K = a constant. The user enters the size of the code along with the two predefined constants to determine the number of manmonths. C and K are determined by calibrating the model to previous projects. An important question for a software organization to answer is how C and K should be modified to reflect the impact of process improvements when no historical data exists. One potential answer is to use the Intermediate or the Advanced COCOMO models. These models contain cost drivers that allow more information about the software project to be input than the Basic model. Cost drivers are basically multipliers to the estimated effort produced by the model. Modern Programming Practices is one such cost driver. If a software project is using modern programming practices, the cost driver would be set such that its multiplier effect would reduce the amount of effort estimated. Two questions arise when using this cost driver: what is the definition of modern

1.3: Current approaches for evaluating the impact of process improvements
There does not exist a standard method for determining the impact of specific process improvements on cycle time. An ideal way for performing this assessment is conducting a controlled experiment in

programming practices and how should the cost driver's value be altered with the addition of a specific improvement to the existing programming practices. These questions have no simple answers because the cost drivers are at a level of granularity too coarse to reflect the impact of specific process improvements. Some researchers also believe that models such as COCOMO are flawed due to their static nature. Abdel-Hamid states, "The problem, however, is that most such tools are static models, and are therefore not suited for supporting the dynamic and iterative planning and control of [resource allocation]" [2].

perception that the product is back on schedule (effect/close feedback loop). The developers perceived the product to be behind schedule, took action, and finally perceived the product to be back on schedule, as a result of their actions. Secondary effects due to the developers' actions, however, such as the lower quality of the product, will also eventually impact the perception the developers have as to being on schedule and will necessitate further actions being performed. These cause-effect relationships are explicitly modeled using system dynamics techniques. Because system dynamics models incorporate the ways in which people, product and process react to various situations, the models must be tuned to the environment that they are modeling. The above scenario of developers reacting to a product that is behind schedule would not be handled the same way in all organizations. In fact, different organizations will have different levels of productivity due to the experience level of the people working for them and the difficulty of the product being developed. Therefore, it is unrealistic for one model to accurately reflect all software development organizations, or even all projects within a single development organization. Once a system dynamics model has been created and tailored to the specific development environment, it can be used to find ways to better manage the process to eliminate bottlenecks and reduce cycle time. Currently, the state-of-the-practice of software development is immature. Even immature software development environments, however, can benefit from this technique. The development of the model forces organizations to define their process and aids in identifying metrics to be collected. Furthermore, the metrics and the models that use them do not have to be exact in order to be useful in decision-making [13]. The model and its use result in a better understanding of the cause-effect relationships that underlie the development of software. The power of modeling software development using system dynamics techniques is its ability to take into account a number of factors that affect cycle time to determine the global impact of their interactions, which would be quite difficult to ascertain without a simulation. The model may be converted to a management flight simulator to allow decision-makers the ability to perform controlled experiments of their development environment.

2: Proposed approach for evaluating the effectiveness of process improvements
2.1: Overview of system dynamics modeling
Due to the weaknesses of the approaches presented in the previous section, we chose to evaluate the impact of process improvements on software development cycle time through system dynamics modeling. System dynamics modeling was developed in the late 1950's at M.I.T. It has recently been used to model "high-level" process improvements corresponding to SEI levels of maturity [12]. System dynamics models differ from traditional cost estimation models, such as COCOMO, in that they are not based upon statistical correlations, but rather cause-effect relationships that are observable in a real system. An example of a cause-effect relationship would be a project behind schedule (cause) leading to hiring more people (effect). These cause-effect relationships are constantly interacting while the model is being executed, thus the dynamic interactions of the system are being modeled, hence its name. A system dynamics model can contain relationships between people, product and process in a software development organization. The most powerful feature of system dynamics modeling is realized when multiple cause-effect relationships are connected forming a circular relationship, known as a feedback loop. The concept of a feedback loop reveals that any actor in a system will eventually be affected by its own action. A simple example of the ideas of cause-effect relationships and feedback loops affecting people, product and process can be illustrated by the following scenario: Consider the situation in which developers, perceiving their product is behind schedule (cause), modify their development process by performing fewer quality assurance activities (effect/cause), leading to a product of lower quality (effect/cause), but giving the temporary

2.2: Modular system dynamics modeling
Due to the large number of process improvements available for evaluation and the necessity of conducting each evaluation in the context of a system dynamics

model customized to the organization for which the evaluation is being performed, a modular system dynamics modeling approach was adopted. A modular system dynamics model consists of two parts, the base software process model and the process improvement model. Once a base model of the software development process exists, any number of process improvement models may be plugged in to determine the effect they will have on software development cycle time. This modularity gives the process improvement models the advantage of being used in more than one base model of a software development environment. For example, the process improvement model could be integrated with a base model of development environment "A" and also a base model of development environment "B". The only change to the process improvement model would be the values of its parameters in order to calibrate it to the specific development environment. The models of the development environments may have to be modified structurally, such that they include all necessary model elements, e.g., system dynamics modeling rates and levels, for integration. A conceptual image of the modular model is shown in Figure 1. On the left is a box representing an existing system dynamics model of a software development process. This model would be tailored to a particular organization. On the right is a box representing the model of the process improvement.

modified to take advantage of the proposed model, such that it contains the necessary elements for the interface.

2.3: Approach for constructing modular system dynamics models
An organization wishing to utilize system dynamics modeling for assessing the impact of new technologies on cycle time must perform the following steps: 1. Construct a base model of the software development process. Construction of a base model requires detailed modeling of the organization's software development process as well as identification of cause-effect relationships and feedback loops. Sources of information for model construction include: observation of the actual software process, interviews of personnel, analysis of metrics and literature reviews of relevant publications. An organization wishing to adapt an existing generic model for rough assessments might consider the model developed by Abdel-Hamid [1]. 2. Construct the process improvement models. For each process improvement being considered, a modular process improvement model must be developed utilizing the same approach as that of the base model. 3. Validate the base and process improvement models. Both the base and process improvement models must be validated to their accuracy requirements in order to build confidence in the models ability to replicate the real system. The validation process must consider the suitability of the model for its intended purpose, the consistency of the model with the real system, and the utility and effectiveness of the model [6], [7], [11]. 4. Execute the models. The base model and any of the process improvement models can be combined to assess the impact of various process improvements on cycle time. Analysis of outputs may also lead to new ideas and understanding of the system triggering additional process reengineering as well as consideration of other process improvements.

Existing Process

Process Improvement

Interface

System dynamics elements necessary to integrate the models

Figure 1.

Graphic of existing process with process improvement.

The two models are integrated through the interface pictured. Information will flow in both directions between the models. This interface is dependent on certain model elements, such as levels and rates, existing in both models. The ovals in Figure 1 represent a cutaway view showing the underlying structure of the system dynamics model and the elements needed for the interface. An existing model of an organization may need to be

3: Demonstration of approach
3.1: Model development
In order to illustrate the feasibility and usefulness of system dynamics modeling for process improvement assessment, we applied our approach to the software inspection process. Our model has the ability to provide answers to the types of questions, concerning process improvements, posed in Section 1.2. For the purpose of our demonstration, we focus mainly on the question of cycle time reduction. We initially developed a base model corresponding to a typical organization's waterfall software development process. We then constructed a model of the software inspection process which we integrated with the base model. The software inspection model enables manipulation of a number of variables connected to the inspection process in order to understand their impact on software development cycle time. Direct manipulation of the following variables are allowed: · The time spent on each inspection task per unit of work to be inspected (e.g., the preparation rate and the inspection rate); · The size of the inspection team; · The percent of errors found during inspection; · The percent of tasks that undergo reinspection; and · The defect prevention attributable to the use of inspections. Our software inspection model is based on the following assumptions: · Time allocated to software inspections takes time away from software development; · A larger inspection team will consume more man-hours per inspection than a smaller team; · Software inspections find a high percentage of errors early in the development life cycle; and · The use of inspections can lead to defect prevention, because developers get early feedback as to the types of mistakes they are making.

Our software inspection model does not incorporate the following: · Software developers achieve higher productivity due to an increase in product knowledge acquired through the software inspection process; · Software developers achieve improved design estimates due to attention paid to size estimates during inspection; and · Software inspections lead to increased visibility of the amount of work completed. The model also excludes the interaction that the inspection team size and the time spent performing inspection tasks have on the percent of errors found during inspection. The inspection team size, the time spent on inspection tasks and the percent of errors found during inspection can be set at the beginning of a model simulation according to historical metrics or altered during the actual simulation. In order to judge the impact that software inspections have on software development cycle time, the software inspection model must be integrated into a model of the software development process. Once integrated, the software inspection model will impact a number of elements in the software development process model. Figures 2 and 3 are an incomplete, but representative view of the integrated model. Figure 2 represents the process steps and effort involved in inspecting work products. Figure 2, however, does not reveal how time and manpower are allocated to perform each step in the inspection process, in order to keep the diagram and ideas presented simple. Each rate in Figure 2 requires that manpower be consumed in order to move work products from one step to the next. Figure 3 shows an incomplete, but representative implementation of the interface between the base model of the software development process and the process improvement model, that is shown abstractly in Figure 1. Figure 3 represents the modeling of errors in the base process model of software development and illustrates the impact inspections have on error generation and error detection in the base process model. The impacts that software inspections have on software development are: software inspections consume development man-hours, errors are less expensive to find during software inspection than system testing, software inspections promote defect prevention and software inspections reduce the amount of error regeneration. Before discussing Figure 2 and Figure

3, a brief discussion of flow diagrams, a notation used to represent system dynamics models, is in order.

Figure 2. Flow diagram of simplified inspection process steps.

Figure 3.

Flow diagram of inspection's impact on errors.

Flow diagrams are composed of levels, material flows, rates, auxiliaries and information links. Levels, depicted as rectangles, represent the accumulation of a material in a system. For example, work products and errors are materials that reside in levels in Figure 2 and Figure 3. Material flows, depicted as hollow arrows, indicated the ability for material to flow from one level to another. Material flows connected to clouds indicate a flow to, or from, a portion of the system not pictured. Rates, depicted as circles attached to material flows, represent the rate of material flow into and out of a level. For example, error detection rate in Figure 3 controls the rate at which potentially detectable errors are detected. Auxiliaries, depicted as circles, aid in the formulation of rate equations. In Figure 3, defect prevention is an auxiliary that affects the error generation rate. Information links, depicted as arrows, indicate the flow of information in a system. Information about levels, rates and auxiliaries are transferred using information links. Information links, unlike material flows, do not affect the contents of a level. The first impact that software inspections have on software development is in the allocation of man-hours for development. Software inspections require that time be set aside for them to be done properly. The time consumed by software inspections is time that cannot be

used for other activities, such as writing software. The amount of time that an inspection consumes is based on the size of the inspection team and the time spent on inspection tasks, e.g., the preparation rate and the inspection rate. Figure 2 is a simplified view of the steps that must be taken to perform inspections. It implicitly models the effort expended to move work products through all of the process steps associated with inspection; effort that takes time away from development activities. The second impact that software inspections have on software development is in the detection of errors. Errors are found early in the life cycle. Errors are less expensive to find and correct during development than during testing. This impact is not explicitly shown in Figure 3, except that a higher error detection rate will increase the number of errors found early in the life cycle, rather than later during testing. The third impact that software inspections have on software development is in the prevention of defects. Successful inspections attack the generation of future defects. Developers that are involved with inspections learn about the types of errors that they have been making and are likely to make during the project. This feedback about the errors they are making leads to fewer errors being made during the project. Figure 3 shows defect prevention, due to inspections, impacting the rate of error generation. The fourth impact that software inspections have on software development is a reduction in the regeneration of errors. Errors that remain undetected often lead to new errors being generated. For example, undetected design errors will lead to coding errors, because they are coding to a flawed design. Software inspections detect errors early in the life cycle, thus reducing the amount of error regeneration. Figure 3 indicates that the errors escaping detection impact the error generation rate. The number of errors escaping detection is dependent on the preparation and inspection rates, as shown in Figure 3. The four impacts that software inspections have on software development, mentioned above, are the foundation for the theory upon which the software inspection model is based. Many details of the implementation of the theory, using system dynamics techniques, are absent from Figure 2 and Figure 3, but are shown in detail elsewhere [14].

3.2: Example model output
The integrated model has been turned into a simple management flight simulator, allowing for simple experimentation. This section describes the user interface of the simulator and the output generated by its use.

Figure 4 shows the simple interface to the simulator. Input to the simulator is in the form of sliders. The simple interface has just five slider inputs: an on/off switch for inspections, the size of the inspection team, the percent of errors found during inspection, the percent of work products that fail inspection and the percent of incorrect error fixes.

Figure 4.

User interface of the inspection simulator.

Output from the simulator comes in two forms: numeric displays and graphs. Numeric displays show the current value of a simulation variable. Man-Days and Work Products Completed are two examples of numeric displays. Graphs, on the other hand, display the value of simulation variables versus time. Each output curve is labeled with a number for ease of reading. There may be multiple units of measure on the vertical axis, each matched to the number of the curve it is representing. The unit of measure on the horizontal axis is days. The five output curves represent: 1) currently perceived job size in terms of work products, 2)cumulative work products developed, 3) cumulative work products tested, 4) total size of workforce and 5)scheduled completion date. A demonstration of the use of the system dynamic models for predicting the cycle time reduction due to a process improvement is in order. Using the integrated model of the baseline waterfall development life cycle and the software inspection process improvement, it will be shown how this modeling technique can be used for evaluating the impact that a proposed process improvement would have on development cycle time. The following demonstration is a simulation of a hypothetical software team employing the simple inspection model presented in this paper. The project

being developed is estimated to be 64,000 lines of code requiring a total workforce of eight developers at the height of development. Two scenarios of the project development are simulated holding all variables fixed, except for the size of the inspection team and the percent of errors found during inspection. Figure 5 is the output generated by executing the model with an inspection team size of six developers discovering 40 percent of the errors during inspection. When interpreting the graphical output, the story of the project is revealed. From Figure 5, the following story emerges. Curve 1, the currently perceived job size in work products, reveals that the project size was initially underestimated. As development progressed, the true size of the project was revealed. Curve 5, the scheduled completion date, was not adjusted even as it became apparent that the project had grown in size. Instead, curve 4, the total size of workforce, indicates that the workforce was increased in size. In addition, though not shown on this graph, the workforce worked longer hours to bring the project back on schedule. Curve 2, cumulative work products developed, reveals that the project appeared to be back on schedule, because there were no visible delays in development of work products. It was not until system testing that problems in development were discovered. Curve 3, cumulative work products tested, reveals that system testing did not go as smoothly as expected. The poor performance of the inspection team pushed the discovery of errors back to system testing. During system testing it was revealed that there was a good amount of rework to be done and as a result, the scheduled completion date, curve 5, was once again pushed back.
1: Job Size in WP 1: 2: 3: 4: 5: 1500.00 2: WP Developed 3: WP Tested 4: Total Workforce 5: Completion Date

20.00 500.00 5 1 2 4 4 4 2

5 1 2

1: 2: 3: 4: 5:

5 750.00 1 10.00 250.00

1

1: 2: 3: 4: 5:

0.00 0.00 0.00 0.00

2 3 125.00 3 250.00 Days

3 375.00 5:02 PM 500.00 1/28/95

Main: Page 1

Figure 5.

Software inspection scenario 1.

Figure 6 is the output generated by executing the model with an inspection team size of three developers discovering 90 percent of the errors during inspection. The story is much the same as that shown in Figure 5. The big difference between Figures 5 and 6 is shown by curve 3, cumulative work products tested. Using more effective software inspections, this project was able to

discover errors early in the life cycle and correct them for much less cost than if they had been found in system test. In addition, there were no major surprises in system testing as to the quality of the product developed. Therefore, with no major amount of rework to be performed in system test, the project was able to finish close to its revised schedule.
1: Job Size in WP 1: 2: 3: 4: 5: 1500.00 2: WP Developed 3: WP Tested 4: Total Workforce 5: Completion Date

Approach, Prentice-Hall, Englewood Cliffs, New Jersey, 1991. [2] Tarek K. Abdel-Hamid, "THINKING IN CIRCLES," American Programmer, May 1993, pp. 3-9. Barry W. Boehm, SOFTWARE ENGINEERING ECONOMICS, Prentice-Hall, Englewood Cliffs, New Jersey, 1981. Ken W. Collier and James S. Collofello, "Issues in Software Cycle Time Reduction," International Phoenix Conference on Computers and Communications, 1995. Raymond Dion, "Process Improvement and the Corporate Balance Sheet," IEEE Software, July 1993, pp. 28-35. Jay W. Forrester, Industrial Dynamics, The M.I.T. Press, Cambridge, MA, 1961. Jay W. Forrester and Peter M. Senge, "Tests for Building Confidence in System Dynamics Models," System Dynamics. TIMS Studies in Management Sciences, 14 (1980), pp. 209-228. Mark C. Paulk, Bill Curtis, Mary Beth Chrissis and Charles V. Weber, "Capability Maturity Model, Version 1.1," IEEE Software, July 1993, pp. 18-27. T. S. Perry, "Teamwork plus Technology Cuts Development Time," IEEE Spectrum, October 1990, pp. 61-67. Lawrence H. Putnam, "General empirical solution to the macro software sizing and estimating problem," IEEE Transactions on Software Engineering, Vol. SE4, No. 4, July 1978, pp. 345-361. George P. Richardson and Alexander L. Pugh III, Introduction to System Dynamics Modeling with DYNAMO, The M.I.T. Press, Cambridge, MA, 1981. Howard A. Rubin, Margaret Johnson and Ed Yourdon, "With the SEI as My Copilot Using Software Process 'Flight Simulation' to Predict the Impact of Improvements in Process Maturity," A m e r i c a n Programmer, September 1994, pp. 50-57. George Stark, Robert C. Durst and C. W. Vowell, "Using Metrics in Management Decision Making," IEEE Computer, September 1994, pp. 42-48. John D. Tvedt, "A System Dynamics Model of the Software Inspection Process," Technical Report TR95-007, Computer Science and Engineering Department, Arizona State University, Tempe, Arizona, 1995.

[3]

[4]
1 2

20.00 500.00 5

1: 2: 3: 4: 5:

5 750.00 1 10.00 250.00 4

1 4 4

[5]

2 1: 2: 3: 4: 5:

3

0.00 0.00 0.00 0.00 2 3 125.00 3 250.00 Days 375.00 5:20 PM 500.00 1/28/95

[6]

Main: Page 1

Figure 6.

Software inspection scenario 2.

[7]

4: Conclusions and future work
Our research grew out of the questions posed in Section 1.2 concerning the impact of process improvements on software development cycle time. Our approach in answering those questions has been to use system dynamics modeling to model the software development process, allowing experimentation with the system. We have tried to demonstrate how this technique may be used to evaluate the effectiveness of process improvements. At this point in our work we have developed a base model of the waterfall development life cycle and a process improvement model of software inspections. We plan to continue this effort by developing a base model of the incremental development process and creating a library of process improvement models. Some examples of process improvements that we plan to add to our library are meetingless inspections, software reuse and risk management. We then plan to validate our base and process improvement models in several software development organizations. Finally, another area of future research is to generalize the interface between the base process models of software development and the models of process improvements. The interface is analogous to tool interfaces. A well defined, generalized interface would facilitate integration of base process models with process improvement models.
[8]

[9]

[10]

[11]

[12]

[13]

[14]

References
[1] Tarek Abdel-Hamid and Stuart E. Madnick, SOFTWARE PROJECT DYNAMICS An Integrated

SOFTWARE--PRACTICE AND EXPERIENCE, VOL. 23(10), 1095­1105 (OCTOBER 1993)

An Application of Causal Analysis to the Software Modification Process
Computer Science Department, Arizona State University, Tempe, AZ 85287, U.S.A.

james s. collofello and

AG Communication Systems Corporation, Phoenix, AZ 85027, U.S.A.

bakul p. gosalia

SUMMARY The development of high quality large-scale software systems within schedule and budget constraints is a formidable software engineering challenge. The modification of these systems to incorporate new and changing capabilities poses an even greater challenge. This modification activity must be performed without adversely affecting the quality of the existing system. Unfortunately, this objective is rarely met. Software modifications often introduce undesirable side-effects, leading to reduced quality. In this paper, the software modification process for a large, evolving real-time system is analysed using causal analysis. Causal analysis is a process for achieving quality improvements via fault prevention. The fault prevention stems from a careful analysis of faults in search of their causes. This paper reports our use of causal analysis on several significant modification activities resulting in about two hundred defects. Recommendations for improved software modification and quality assurance processes based on our findings are also presented.
key words: Causal analysis Software maintenance

BACKGROUND The traditional approach to developing a high quality software product consists of applying a development methodology with a heavy emphasis on fault detection. These fault detection processes consist of walkthroughs, inspection and various levels of testing. A more effective approach to developing a high quality product is an emphasis on fault prevention. An effective fault prevention approach which is gaining popularity is causal analysis.1­4 Causal analysis consists of collecting and analyzing software fault data in order to identify their causes. Once the causes are identified, process improvements can be made to prevent future occurrences of the faults. The paper by Jones3 presents a programming process methodology for using causal analysis and feedback as a means for achieving quality improvement, and ultimately fault prevention, at IBM. The methodology emphasizes effective use of the fault data to prevent the recurrence of faults. The fault prevention methodology is based on the following three concepts: 0038­0644/93/101095­11$10.50 © 1993 by John Wiley & Sons, Ltd. Received 6 April 1993 Revised 6 April 1993

1096

j. s. collofello and b. p. gosalia

1. Designers should evaluate their own faults. 2. Causal analysis should be part of the software development process. 3. Feedback should be part of the process. An overview of the causal analysis process proposed by Jones is shown in Figure 1. The first activity consists of a kickoff meeting with the following purposes: 1. 2. 3. 4. Review Review Review Identify input fault data. the methodology guidelines. appropriate checklists. the team goals.

Next, the development of the work products occurs, using the feedback from the kickoff meeting to prevent the creation of errors. The work products are then validated and reworked as necessary. A causal analysis activity is then performed, beginning with an analysis of the faults performed by the owner of the faults. This involves analyzing each fault to determine 1. 2. 3. 4. 5. The The The The The fault type or category phase that the fault was found phase that the fault was created cause(s) of the fault solution(s) to prevent the fault from occurring in the future.

Figure 1. Causal analysis process described by Jones

an application of causal analysis

1097

This information is recorded on a causal analysis form which is used to enter the data into a database. A causal analysis team meets to analyze the data in the database. In addition, the group may at times need to consult with other people (designers, testers, etc.) outside of the team to complete the analysis. The causal analysis team is responsible for identifying the major problem areas by looking at the fault data as a whole instead of one particular fault at a time. The team uses a problem-solving process to analyze the data, determine the problem area(s) to work on, determine the root causes of problem area(s), and develop recommendations and implementation plans to prevent the problem type(s) from occurring in the future. These recommendations are then submitted to an action team, which has the following responsibilities: 1. Evaluation and prioritization of recommendations. 2. Implementation of recommendations. 3. Dissemination of feedback. The action team meets periodically (e.g. once a month) to review any new implementation plans received from the causal analysis teams and to check the status of the previous implementation plans and action items. The status of the action items is kept in the causal analysis database and monitored by the action team. To date, most of the effort reported in causal analysis activities has been focused on development processes. This is unfortunate, considering the high percentage of effort consumed by software modification processes. Software modifications occur in order to add new capabilities or modify existing capabilities of a system. Modifying a large, complex system is a time-consuming and error-prone activity. This task becomes more difficult over time as systems grow and their structure deteriorates.5 Thus, more effort must be expended in performing causal analysis activities for design maintenance tasks. This paper describes such an effort. In the remainder of this paper, our approach to causal analysis for design maintenance tasks is described, as well as the application of this approach to a large project. Our results and recommendations for preventing faults during modification processes are also presented. CAUSAL ANALYSIS APPROACH Our application of causal analysis to the software modification process is shown in Figure 2. Each of the causal analysis steps in Figure 2 is described in one of the following subsections. Obtaining problem reports The first activity to be performed is collecting problem reports resulting from the modification processes undergoing causal analysis scrutiny. These problem reports may be generated internally via testing or externally by the customer. A window of time must be chosen during which the problem reports are collected. Ideally, the window of time for obtaining problem reports should begin after the code is modified and end some period of time after the code was delivered.

1098

j. s. collofello and b. p. gosalia

Figure 2. Causal analysis approach as applied to the software modification process

Prioritizing problems The next step in our causal analysis approach is prioritizing the faults. Fault prioritization enables an analysis of those causes which contribute to high priority problems. Our prioritization consists of three categories: 1. Critical 2. Major 3. Minor Critical faults impair functionality and prohibit further testing. Major faults partially impair functionality. Minor faults do not affect normal system operation.

Categorizing errors After fault prioritization, faults are categorized. The primary objective of this categorization is to facilitate the link between faults and their causes. Numerous error categorization schemes have been proposed over the years.6­9 The existing schemes are, however, directed towards understanding and improving the development of new software. They do not address the kinds of errors that are unique to maintenance activities. Thus, we found it necessary to expand upon existing categorization schemes and develop new error categories directed towards maintenance activities. Our categories were formulated based on our experience of analyzing hundreds of maintenance errors. Although categorization is not necessary in performing a causal analysis, we found the categorization useful in identifying fault causes. The fault categorization also provides useful information when determining the cost-effectiveness of eliminating fault causes versus attempting to detect the faults if they occur. The fault categories are described below:

an application of causal analysis Design faults

1099

This category reflects software faults caused by improper translation of requirements into design during modification. The design at all levels of the program and data structure is included. The following are examples of typical design faults: 1. Logic faults. Logic faults include sequence faults (improper order of processing steps), insufficient branch conditions, incorrect branch conditions, incorrectly nested loops, infinite loops, incorrectly nested if statements, missing validity checks etc. 2. Computation faults. Computation faults pertain to inaccuracies and mistakes in the implementation of addition, subtraction, multiplication, and division operations. Computational faults also include incorrect equations or grouping of parenthesis, mixing data of different unit values, or performing correct computations on the wrong data. 3. Missing exception handling. These faults include failure to handle unique conditions or cases even when the conditions or cases were specified in the requirements documents. 4. Timing faults. This type of fault reflects incorrect design of timing critical software. For example, in a multitasking system, a designer interpreted the executive command `suspend' to mean that all processing is halted for a specified amount of time, when in fact the command suspended all tasks scheduled to be invoked in the same specified amount of time. 5. Data handling faults. These faults include failure to initialize the data before use, improper use or designation of fixes, mixing up the names of two or more data items, and improper control of external file devices. 6. Faults in I/O concepts. These faults include input from or output to the wrong file or on-line device, defining too much data for a record size, formatting faults, and incorrect input­output protocols with other communication devices. 7. Data definition faults. This category reflects incorrect design of the data structures to be used in the module or the entire software (incorrectly defined global data structures). This category also reflects incorrect scoping in data structures and incorrect data abstractions. Incompatible interface Incompatible interface faults occur when the interfaces between two or more different types of modified components are not compatible. The following are examples of incompatible interfaces: 1. Different parameters passed to the procedures/functions than expected. 2. Different operations performed by the called modules, functions, or procedures than expected by the calling modules, procedures or functions. For example, assume a module A was called by another module B to perform fault checking. However, as a design change, the fault checking code was moved to another software module C. 3. Code and database mismatched. For example, suppose that device handler code tries to access data which is not defined in the database. 4. Executive routines and other routines mismatched. For example, consider the

1100

j. s. collofello and b. p. gosalia

situation where a task needs to remain in a no interrupt condition for a longer time than allowed by the operating system routines. 5. Software and hardware mismatched. 6. Software and firmware mismatched. For example, the parameters passed by the firmware may not match with the modified software. Incorrect code synchronization from parallel projects For a large evolving system, the software development cycles of multiple releases may overlap, as shown in Figure 3. This category of fault occurs when changes from project A, the previous project, are incorrectly carried into project B, the current project. Incorrect object patch carryover In a large maintenance effort, object patches are sometimes needed. These object patches may remain for several revisions of the system. Errors committed while modifying modules with object patches fall into this category. System resource exhaustion This fault occurs when the system resources (such as memory and real time) become insufficient. Examples of faults in this class include: exhaustion of available stack space, the indiscriminate use of special registers and the use of non-existent stacks, real-time clocks, and other architectural features. Determining fault causes The most critical step in performing causal analysis is determining the cause of each fault. This task can best be performed by the programmer who introduced the fault. To facilitate this identification of fault causes, the software designers who introduced the faults should be interviewed. The interviews should be informal and performed very carefully so as to not make designers defensive. The purpose of the analysis for defect prevention must be made clear prior to the interview. Based on the information acquired from the designer who introduced the fault, a causal category is selected for the error. Our approach to causal analysis identifies seven major categories of causes that lead to software modification faults. Our causal categories differ from others in the literature5,9 which are directed

Figure 3. Software development of large evolving systems

an application of causal analysis

1101

towards finding the causes of errors made during new software development. These existing causal categorization schemes do not address the unique causes of errors made during software maintenance. Our causal categories are not exclusive and, thus, a fault might have several causes. The causal categories were formulated in a manner that would support process improvements. The goal of determining the causal category for each fault is to collect the data necessary to determine which causal categories are leading to the most problems. This provides a means for evaluating the cost effectiveness of implementing recommendations to eliminate or reduce fault causes. Our causal categories customized to modification activities are described below: System knowledge/experience This causal category reflects the lack of software design knowledge about the product or the modification process. Some examples include: 1. Misunderstanding of the existing design. For example, the designer did not have sufficient understanding of the existing design of the database. 2. Misunderstanding of the modification process. The designer was not aware of the process followed in the project. For example, the designer was not aware of the configuration control process. 3. Inadequate understanding of the programming environment. The designer did not have adequate understanding of the programming environment. The programming environment includes personnel, machinery, management, development tools, and any other factors which affect the developer's ability to modify the system. 4. Inadequate understanding of the customer's requirements. The designer did not thoroughly understand the requirements of the customer. For example, the designer may not understand a feature specified in the requirements documents. Communication This causal category reflects communication problems concerning modifications. It identifies those causes which are not attributed to a lack of knowledge or experience, but instead to incorrect or incomplete communication. Communication problems are also caused due to confusion between team members regarding responsibilities or decisions. Some examples of communication problems are: 1. Failure of a design group to communicate a last-minute modification. 2. Failure to write a fault-handling routine for a function because each team member thought someone else was supposed to do it. Software impacts This category reflects failure of a software designer to consider all possible impacts of a software modification. An example is omission of fault recovery code after the addition of a new piece of hardware.

1102 Methods/standards

j. s. collofello and b. p. gosalia

This category reflects methods and/or standards violations. It also includes limitations to existing methods or standards which contributed to the faults. An example may be a missed code review prior to testing. Feature deployment This category reflects problems caused by the inability of software, hardware, firmware or database components to integrate at the same time. It is characterized by a lack of contingency plans to prevent problems caused by missing components. Supporting tools This category reflects problems with supporting tools which introduce faults. An example is an incorrect compiler which introduces faults in object code. Human error This causal category reflects human errors made during the modification process which are not attributable to other sources. The designer knew what to do and understood the item thoroughly but simply made a mistake. An example may be that a designer forgets to add code he intended for a software load. Developing recommendations The last step in our causal analysis process is developing a set of recommendations to prevent faults from reoccurring. These recommendations must be based on the data collected during the causal analysis study and customized to the development environment. APPLICATION OF THE CAUSAL ANALYSIS APPROACH In an effort to learn more about the faults generated during software modifications, we applied our causal analysis process to the creation of a new version of a large telephony system. This new version contained new features as well as fixes to problems in the old release. The size of the system was approximately 2·9 million lines of code. The version analyzed was developed from the previous version by gradual modifications to the software, firmware, and hardware. About 1 million lines of code were modified as follows: 85 per cent of the code was added, 10 per cent of the code was changed, and 5 per cent of the code was deleted. Each of these gradual modifications corresponded to a software increment. The increments were added to the system one at a time and tested as shown in Figure 4. The new version we analyzed consisted of 11 increments. The experience levels of the designers involved in modifying the software varied widely from two years to as high as 30 years. The documentation used and created in the process of generating the new version was typical of that in most software development organizations. Both high level and

an application of causal analysis

1103

Figure 4. Gradual building of a software version release

detailed design documents were created for the new features being added. The contents of these documents included details about the impacts of the new feature on the existing modules, hardware, database, firmware, etc. These documents were then reviewed by designers representing the affected areas. Documentation was also generated and reviewed for fixes added to the new version. Although it would have been ideal to collect and analyze problem reports for the entire modification process, our study was limited to analyzing defects detected during the first phase of integration testing for each of the incremental loads. This phase verifies the basic stability and functionality of the system prior to testing the operation of new features, functions, and fixes. It is, thus, the most likely phase to detect faults introduced by the modification process. Prior to our data collection, both code reviews and module testing were generally applied. Thus, faults detected by these processes were not reflected in our results. In addition, faults detected by later phases of testing, such as system testing and stress testing were also not reflected in our results. Our results, therefore, are limited to the kinds of faults normally detected during integration testing. In our study, this amounted to more than 200 faults. All 200 faults were analyzed and categorized by this paper's authors to ensure reliable results. RESULTS OF THE CAUSAL ANALYSIS APPLICATION Figure 5 presents an overall distribution of fault categories across all the problems. The large number of design and interface faults demonstrates that the modification

Figure 5. Fault category distribution of all the problems

1104

j. s. collofello and b. p. gosalia

Figure 6. Causal categories distribution of all the problems

of an existing design in order to satisfy new requirements is a very error-prone process. Figure 6 presents the distribution of causal categories for all the problems. The causal analysis data suggests that almost 80 per cent of all faults are caused by insufficient knowledge/experience, communication problems or a failure to consider all software impacts. A further analysis of the faults caused by insufficient knowledge/experience indicated that the majority of those faults were caused by designers with less than five years of experience. These results clearly suggest that one significant approach to improving the quality of maintenance activities is to increase the experience level of those performing design maintenance tasks. This can be accomplished in many ways. One obvious approach is motivating designers to continue in their work assignments. Another approach is to improve design maintenance training and mentoring programs. The results obtained from the causal analysis also suggest that the design maintenance process can be greatly improved via better communication. This might be accomplished with more disciplined maintenance methodologies, more thorough documentation and diligent reviews. Extreme care must be taken to ensure that information regarding potential software impacts as a result of modification activities are communicated. The results from our causal analysis activity were integrated with those of other ongoing continuous process improvement approaches within our organization and applied to the development of a subsequent version of the software. The improvements mainly focused upon an enhanced review process, better training and better communication. The fault data from the subsequent version of the software release showed a substantial reduction in the number of faults. CONCLUSIONS AND FUTURE RESEARCH This paper has presented our results from applying causal analysis to several large modification activities. Our data suggests that almost 80 per cent of all faults created during modification activities are caused by insufficient knowledge/experience, communication problems or a failure to consider all software impacts. The application of causal analysis to the software modification process has contrib-

an application of causal analysis

1105

uted to a substantial reduction of faults in our organization. Much additional research needs to be performed to assess whether the results of this study are representative of that experienced in other large evolving systems. Other research needs to explore the attributes of an `experienced' designer. Specific design maintenance methodology improvements must also be formulated and evaluated in terms of their potential return on investment.
REFERENCES 1. J. L. Gale, J. R. Triso and C. A. Burchfield, `Implementing the defect prevention process in the MVS interactive programming organization', IBM Systems Journal, 30, (1), 33­43 (1990). 2. R. G. Mays, C. L. Jones, G. J. Holloway and D. P. Studinski, `Experiences with defect prevention', IBM Systems Journal, 30, (1), 4­32 (1990). 3. C. L. Jones, `A process-integrated approach to defect prevention', IBM Systems Journal, 24, (2), 50­ 164 (1985). 4. R. T. Philips, `An approach to software causal analysis and defect extinction', IEEE Globecom 1986, 1, (12), 412­416 (1986). 5. J. Collofello and J. Buck, `Software quality assurance for maintenance', IEEE Software, No. 5, September 1987, pp. 46­51. 6. A. Endres, `An analysis of errors and their causes in system programs', IEEE Trans. Software Eng., SE1, (2), 140­149 (1975). 7. B. Beizer, Software Testing Techniques, Van Nostrand Reinhold, 1983. 8. J. Collofello and L. Blumer, `A proposed software error classification scheme', Proceedings of the National Computer Conference, 1985, pp. 537­545. 9. T. Nakajo and H. Kume, `A case history of software error cause­effect relationships', IEEE Trans. Software Eng., 17, 830­837 (1990).

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 1

An Agent-Based Model of FLOSS Projects
Nicholas P. Radtke, Arizona State University, USA Marco A. Janssen, Arizona State University, USA James S. Collofello, Arizona State University, USA

What Makes Free/Libre Open Source Software (FLOSS) Projects Successful?

Abstract
The last few years have seen a rapid increase in the number of Free/Libre Open Source Software (FLOSS) projects. Some of these projects, such as Linux and the Apache web server, have become phenomenally successful. However, for every successful FLOSS project there are dozens of FLOSS projects which never succeed. These projects fail to attract developers and/or consumers and, as a result, never get off the ground. The aim of this research is to better understand why some FLOSS projects flourish while others wither and die. This article presents a simple agent-based model that is calibrated on key patterns of data from SourceForge, the largest online site hosting open source projects. The calibrated model provides insight into the conditions necessary for FLOSS success and might be used for scenario analysis of future developments of FLOSS. [Article copies are available for purchase from InfoSci-on-Demand.com] Keywords: Agent-Based Model; Emergent Properties; FLOSS; Open Source; Prediction Success; Simulation

Although the concept of Free/Libre Open Source Software (FLOSS) has been around for many years, it has recently increased in popularity as well as received media attention, not without good reason. Certain characteristics of FLOSS are highly desirable: some FLOSS projects have been shown to be of very high quality (Analysis of the Linux Kernel, 2004; Linux Kernel Software, 2004) and to have low

defect counts (Chelf, 2006); FLOSS is able to exploit parallelism in the software engineering process, resulting in rapid development (Kogut & Metiu, 2001); FLOSS sometimes violates Brooks' law (Rossi, 2004), which states that "adding manpower to a late software product makes it later" (Brooks, 1975); and FLOSS development thrives on an increasing user- and developer-base (Rossi, 2004).

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

2 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009

As open source has become a prominent player in the software market, more people and companies are faced with the possibility of using open source products, which often are seen as free or low-cost solutions to software needs. However, choosing to use open source software is risky business, partly because it is unclear which FLOSS will succeed. To choose an open source project, only to find it stagnates or fails in the near future, could be disastrous, and is cited as a concern by IT managers (T. Smith, 2002). Accurate prediction of a project's likelihood to succeed/fail would therefore benefit those who choose to use FLOSS, allowing more informed selection of open source projects. This article presents an initial step towards the development of an agent-based model that simulates the development of open source projects. Findings from a diverse set of empirical studies of FLOSS projects have been used to formulate the model, which is then calibrated on empirical data from SourceForge, the largest online site hosting open source projects. Such a model can be used for scenario and sensitivity analysis to explore the conditions necessary for the success of FLOSS projects.

BACKGROUND
There have been a limited number of attempts to simulate various parts of the open source development process (Dalle & David, 2004). For example, Dalle and David (2004) use agentbased modeling to create SimCode, a simulator that attempts to model where developers will focus their contributions within a single project. However, in order to predict the success/failure of a single FLOSS project, other existing FLOSS projects, which are vying for a limited pool of developers and users, may need to be considered. This is especially true when multiple FLOSS projects are competing for a limited market share (e.g., two driver projects for the same piece of hardware or rival desktop environments such as GNOME and the KDE). Wagstrom, Herbsleb, and Carley (2005) created OSSim, an agent-based model containing us-

ers, developers, and projects that is driven by social networks. While this model allows for multiple competing projects, the published experiments include a maximum of only four projects (Wagstrom et al., 2005). Preliminary work on modeling competition among projects is currently being explored by Katsamakas and Georgantzas (2007) using a system dynamics framework. By using a population of projects, it is possible to consider factors between the projects, e.g., the relative popularity of a project with respect to other projects as a factor that attracts developers and users to a particular project. Therefore, our model pioneers new territory by attempting to simulate across a large landscape of FLOSS with agent-based modeling. Gao, Madey, and Freeh (2005) approach modeling and simulating the FLOSS community via social network theory, focusing on the relationships between FLOSS developers. While they also use empirical data from the online FLOSS repository SourceForge to calibrate their model, they are mostly interested in replicating the network structure and use network metrics for validation purposes (e.g. network diameter and degree). Our model attempts to replicate other emergent properties of FLOSS development without including the complexities of social networking. However, both teams consider some similar indicators, such as the number of developers working on a project, when evaluating the performance of the models. In addition, there have been attempts to identify factors that influence FLOSS. These have ranged from pure speculation (Raymond's (2000) gift giving culture postulates) to surveys of developers (Rossi, 2004) to case studies using data mined from SourceForge (Michlmayr, 2005). Wang (2007) demonstrates specific factors can be used for predicting the success of FLOSS projects via K-Means clustering. However, this form of machine learning offers no insight into the actual underlying process that causes projects to succeed. Therefore, the research presented here approaches simulating

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 3

the FLOSS development process using agentbased modeling instead of machine learning. To encourage more simulation of the FLOSS development process, Antoniades, Samoladas, Stamelos, Angelis, and Bleris (2005) created a general framework for FLOSS models. The model presented here follows some of the recommendations and best practices suggested in this framework. In addition, Antoniades et al. (2005) developed an initial dynamical simulation model of FLOSS. Although the model presented here is agent-based, many of the techniques, including calibration, validation, and addressing the stochastic nature of the modeling process, are similar between the two models. One difference is the empirical data used for validation: Antoniades et al.'s (2005) model uses mostly code-level metrics from specific projects while the model presented here uses higher project-level statistics gathered across many projects.

·

Active developer count change trends (Wang, 2007)

IDENTIFYING AND SELECTING INFLUENTIAL FACTORS
Factors which are most likely to influence the success/failure of FLOSS must first be identified and then incorporated into the model. Many papers have been published in regards to this, but most of the literature simply speculates on what factors might affect the success and offers reasons why. Note that measuring the success of a FLOSS project is still an open problem: some metrics have been proposed and used but unlike for commercial software, no standards have been established. Some possible success indicators are: · · · · · · Completion of the project (Crowston, Howison, & Annabi, 2006) Progression through maturity stages (Crowston & Scozzi, 2002) Number of developers Level of activity (i.e., bug fixes, new feature implementations, mailing list) Time between releases Project outdegree (Wang, 2007)

English and Schweik (2007) asked eight developers how they defined success and failure of an open source project. Answers varied for success, but all agreed that a project with a lack of users was a failure. Thus having a sufficient user-base may be another metric for success. Papers that consider factors influencing success fall into two categories: those that look at factors that directly affect a project's success (Michlmayr, 2005; Stewart, Ammeter, & Maruping, 2006; S. C. Smith & Sidorova, 2003) and those that look for factors that attract developers to a project (and thus indirectly affect the success of a project) (Bitzer & Schröder, 2005; Rossi, 2004; Raymond, 2000; Lerner & Tirole, 2005). A few go a step further and perform statistical analyses to discover if there is a correlation between certain factors and a project's success/ failure (Lerner & Tirole, 2005; Michlmayr, 2005), and Kowalczykiewicz (2005) uses trends for prediction purposes. Wang (2007) demonstrates that certain factors can be used for accurate prediction using machine learning techniques. Koch (2008) considers factors affecting efficiency after first using data envelopment analysis to show that successful projects tend to have higher efficiencies. In general, factors affecting FLOSS projects fall into two categories: technical factors and social factors. Technical factors are aspects that relate directly to a project and its development and are typically both objective and easy to measure. Examples of technical factors include lines of code and number of developers. The second category is social factors. Social factors pertain to aspects that personally motivate individuals to engage in open source development/use. Examples of social factors include reputation from working on a project, matching interests between the project and the developer/user, popularity of the project with other developers/users, and perceived importance of the code being written (e.g., core versus fringe development (Dalle & David, 2004)). Most of the social factors are subjective and

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

4 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009

rather difficult, if not impossible, to measure. Despite this, it is hard to deny that these might influence the success/failure of a project and therefore social factors are considered in the model. Fortunately, the social factors being considered fall under the domain of public goods, for which there is already a large body of work published (e.g., Ostrom, Gardner, & Walker, 1994; Jerdee & Rosen, 1974; Tajfel, 1981; Axelrod, 1984; Fox & Guyer, 1977). Most of this work is not specific to FLOSS, but in general it explores why people volunteer to contribute to public goods and what contextual factors increase these contributions. The findings of this literature are applied when designing the model, as are findings from publications investigating how FLOSS works, extensive surveys of developers asking why they participate in FLOSS (e.g., Ghosh, Krieger, Glott, & Robles, 2002), and comments and opinions of FLOSS users (e.g., T. Smith, 2002).

INITIAL MODEL
The model universe consists of agents and FLOSS projects. Agents may choose to contribute to or not contribute to, and to consume (i.e. download) or not consume FLOSS projects. At time zero, FLOSS projects are seeded in

the model universe. These initial projects vary randomly in the amount of resources that will be required to complete them. At any time, agents may belong to zero, one, or more than one of the FLOSS projects. The simulation is run with a time step (t) equal to one (40 hour) workweek. Table 1 contains the properties of agents. Table 2 contains the properties of projects. At each time step, agents choose to produce or consume based on their producer and consumer numbers, values between 0.0 and 1.0 that represent probabilities that an agent will produce or consume. Producer and consumer numbers are statically assigned when agents are created and are drawn from a normal distribution. If producing or consuming, an agent calculates a utility score for each project in its memory, which contains a subset of all available projects. The utility function is shown in Box 1. Each term in the utility function represents a weighted factor that attracts agents to a project, where w1 through w5 are weights that control the importance of each factor, with 0.0  w1,w2,w3,w4,w5  1.0 and 5 . Factors i=1 wi = 1.0 were selected based on both FLOSS literature and our own understanding of the FLOSS development process. Keeping it simple, a linear utility equation is used for this version of the model. The first term represents the similarity between the interests of an agent and the direc-

Table 1. Agent properties
Property Consumer number Producer number Needs vector Resources number Description Propensity of an agent to consume (use) FLOSS. Propensity of an agent to contribute to (develop) FLOSS. A vector representing the interests of the agent. A value representing the amount of work an agent can put into FLOSS projects on a weekly basis. A value of 1.0 represents 40 hours. A list of projects the agent knows exist. Type/Range Real [0.0, 1.0] Real [0.0, 1.0] Each scalar in vector is real [0.0, 1.0] Real [0.0, 1.5]

Memory

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 5

Table 2. Project properties
Property Current resources Cumulative resources Resources for completion Download count Maturity Description The amount of resources or work being contributed to the project during the current time interval. The sum, over time increments, of all resources contributed to the project. The total number of resources required to complete the project. The number of times the project has been downloaded. Six ordered stages a project progresses through from creation to completion. An evolving vector representing the interests of the developers involved in the project. Type/Range Real Real Real Integer {planning, pre-alpha, alpha, beta, stable, mature} Each scalar in vector is real [0.0, 1.0]

Needs vector

Box 1.
utility = w1  similarity (agentNeeds, projectNeeds ) + w2  currentResourcesnorm + w3  cumulativeResourcesnorm + w4  downloadsnorm + w5  f (maturity ) (1)

tion of a project; it is currently calculated using cosine similarity between the agent's and project's needs vectors. The second term captures the current popularity of the project and the third term the size of the project implemented so far. The fourth term captures the popularity of a project with consumers based on the cumulative number of downloads a project has received. The fifth term captures the maturity stage of the project. Values with the subscript "norm" have been normalized (e.g., downloadsnorm is a project's download count divided by the maximum number of downloads that any project has received). The discreet function f maps each of the six maturity stages into a value between 0.0 and 1.0, corresponding to the importance of each maturity stage in attracting developers. Since all terms are normalized, the

utility score is always a value between 0.0 and 1.0. Both consumers and producers use the same utility function. This is logical, as most FLOSS developers are also users of FLOSS. For consumers that are not producers, arguably the terms represented in the utility function are still of interest when selecting a project. There is relatively little research published on users compared to developers of FLOSS, so it is unclear if selection criteria are different between the two groups. It is possible that some of the terms included in the utility function are redundant or irrelevant. Part of the model exploration is to determine which of these factors are relevant. See the Calibrating the Model and Results sections below.

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

6 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009

Agents use utility scores in combination with a multinominal logit equation to probabilistically select projects. The multinominal logit allows for imperfect choice, i.e., not always selecting the projects with the highest utility. There is no explicit formulation of communication between agents included in the model; implicitly it is assumed that agents share information about other projects and thus agents know characteristics of projects they are not currently consuming/producing. At each time step, agents update their memory. With a certain probability an agent will be informed of a project and add it to its memory, simulating discovering new projects. Likewise, with a certain probability an agent will remove a project from its memory, simulating forgetting about or losing interest in old projects. Thus, over time an agent's memory may expand and contract. Projects update their needs vector at each iteration using a decaying equation, where the new vector is partially based on the project's previous vector and partially on the needs vectors of the agents currently contributing to the project. An agent's influence on the project's vector is directly proportional to the amount of work the agent is contributing to the project with respect to other agents working on the same project. This represents the direction of a project being influenced by the developers working on it. Finally, project maturity stages are computed based on percent complete threshold values.

in each stage similar to that measured by Weiss. In addition, two other emergent properties were chosen to validate the initial model: · · · Number of developers per FLOSS project. Number of FLOSS projects per developer. By creating a model that mimics a number of key patterns of the data, confidence is derived about the model.

CALIBRATING THE MODEL
The model has a number of parameters that must be assigned values. A small subset of these can be set to likely values based on statistics gathered from surveys or mined from FLOSS repository databases. For the remaining parameters, a search of the parameter space must be performed to find the combination that allows the model to most closely match the empirical data. Since an exhaustive search is not practical, the use of genetic algorithms from evolutionary computation is used to explore the parameter space (Kicinger, Arciszewski, & De Jong, 2005). This is done as follows: an initial population of model parameter sets is created randomly. The model is run with each of the parameter sets and a fitness score is calculated based on the similarity of the generated versus empirical data. The parameter values from these sets are then mutated or crossed-over with other parameter sets to create a new generation of model parameter sets, with a bias for selecting parameters sets that resulted in a high fitness; then the new generation of parameter sets are evaluated and the process repeated. In this case, a genetic algorithm is being used for a stochastic optimization problem for which it is not known when a global optimum is found. Genetic algorithms are appropriate for finding well-performing solutions in a reasonably brief amount of time. Reviewing the values of the best performing parameters will help identify which factors are important/influential in the open source software development process.

VALIDATION METHOD
Creating a model that successfully predicts the success or failure of FLOSS projects is a complicated matter. To aid in the iterative development process, the model is first calibrated to reproduce a set of known, emergent properties from real world FLOSS data. For example, Weiss (2005) surveyed the distribution of projects at SourceForge in each of six development categories: planning, pre-alpha, alpha, beta, stable, and mature. Therefore, the model will need to produce a distribution of projects

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 7

The fitness function chosen for the genetic algorithm is based on the sum of the square of errors between the simulated and empirical data, as shown in Box 2. Since there are three fitness values calculated, one per empirical data set, the three fitness values are averaged to provide a single value for comparison purposes.

RESULTS
Since the model includes stochastic components, multiple runs with a given parameter set were performed and the results averaged. In this case, four runs were performed for each parameter set after initial experimentation showed very low standard deviations even with small numbers of runs. The averaged model results were then compared to the empirical data. As empirical investigations of FLOSS evolution note, it takes approximately four years for a project of medium size to reach

a mature stage (Krishnamurthy, 2002). Thus, the model's performance was evaluated by running the model for 250 time steps, with a time step of one week, for a total simulated time equivalent of a little over five years. All metrics were gathered immediately following the 250th time step. The averaged data (over 4 runs) from the simulator's best parameter set, along with the empirical data, is shown in Figs. 1, 2, and 3. Figure 1 shows the generated percentage of projects in each maturity stage is a similar shape to the empirical data, with the main difference being the highs are too high and the lows are too low in the simulated data. This disparity may be a result of initial model startup conditions. At time 0, the model starts with all projects in the planning stage. This is obviously different than SourceForge, where the projects were gradually added over time, not all at once in the beginning. While the model does add new projects each time step, with a growth rate based on the rate of increase of projects at SourceForge, it may take

Box 2.

fitness = 1 -

sum of square of errors maximum possible sum of square of errors

(2)

Figure 1. Percentage of FLOSS projects in maturity stages. Empirical data from (Weiss, 2005)

Maturity Stages
45 40 35 30 25 20 15 10 5 0 Plan ning Sim Average Emp Value

Percent

Prealpha

Alpha

Beta

Stable Mature

Maturity Stage

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

8 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009

more than 250 time steps for maturity stages to stabilize after the differing initial condition. At the end of the simulation run, just short of 60% of the projects were created sometime during the simulation while the remaining 40% were created at time 0. As shown in Figure 2, the number of developers per projects follows a near-exponential distribution and the simulated data is similar, especially for projects with fewer than seven developers. Note that the data in Figure 2 uses a logarithmic scale to help with a visual comparison between the two data sets. Beyond seven developers, the values match less closely, although this difference is visually amplified as a result of the logarithmic scale and is actually not as large as it might initially appear. Since there are few projects with large numbers of developers in the empirical data, the higher values may be in the noise anyhow and thus focus should be on the similarity of the lower numbers. Figure 3 shows the number of projects per developer is a relatively good match between the simulated and empirical data, with the main difference being the number of developers working on one project. It is likely that this could

be corrected via additional experimentation with parameters. Table 3 contains the average fitness scores for each of the emergent properties for the top performing parameter set. These values provide a quantitative mechanism for confirming the visual comparisons made above: the maturity stage fitness score is indeed lower than the other two properties. The combined fitness is simply the mean of the three fitness scores, although this value could be calculated with uneven weights if, say, matching each property was prioritized. Doing so would affect how the genetic algorithm explored the parameter space. It may be the case that certain properties are easy to reproduce in the model and work over a wide range of parameter sets, in which case these properties may be weighted less than properties that are more difficult to match. Properties which are always matched should be discarded from the model for evolution purposes as they do not discriminate against different parameter sets. Finally, examining the evolved utility weights of the top 10 performing parameter sets provides insight into what factors are important in the model for reproducing the three properties examined. Table 4 contains the averages and standard deviations for each

Figure 2. Percentage of projects with N developers. Empirical data from (Weiss, 2005)

Developers per Project
6 4

ln(percent)

2 0 -2 -4 -6 -8 0 5 10 15 20 25 30 35 40

Sim Average Emp Value

Developers per Project

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 9

Figure 3. Percentage of developers with N projects. Empirical data from (Ghosh et al., 2002)

Projects per Developer
40 35 30 25 20 15 10 5 0 0 1 2 3 4-5 6-7 8-1 11- 16- >20 0 15 20 Sim Average Emp Value

Percent

Projects per Developer

Table 3. Averaged fitness scores for the best
Emergent Property Maturity stage Devs per project Projects per dev Combined Fitness Score 0.9679 0.9837 0.9938 0.9818

Table 4. Utility function weights from the best 10 param
Weight w1 (similarity) w2 (current resources) w3 (cumulative resources) w4 (downloads) w5 (maturity) Mean 0.1849 0.3964 0.0003 0.0022 0.4163 Std. Dev. 0.1137 0.1058 0.0003 0.0039 0.1534

of the weights. It appears that the cumulative number of resources and download counts are not important in reproducing the examined properties in the model. This conclusion is reached by observing these weight's small values (low mean and small variance) in comparison to the other weights (high means and larger variance).

Unfortunately, the high variance of the remaining three weights makes it difficult to rank them in order of importance. Rather, the conclusion is that similarity, current resources, and maturity are all important in the model. Another interesting set of values evolved by the system are the parameters for the producer and consumer numbers. While the producer and consumer numbers are drawn from normal distributions bounded by 0.0 and 1.0 inclusive, neither the mean nor standard deviations of these distributions are known. Therefore, these values are evolved to find the best performing values. Table 5 contains the evolved mean and standard deviation for the producer and consumer numbers averaged from the top 10 parameter sets. Notice that the mean producer number is very high at 0.9801 and very stable across the top 10 parameter sets, with a standard deviation of 0.0079. Likewise, the standard deviation is relatively low at 0.1104 and also stable with a standard deviation of 0.0101. This indicates that the top performing model runs had agents with high propensities to develop. In other words having most agents produce frequently (i.e., most agents be developers) produces better matching of the empirical data. This is in alignment with the notion that FLOSS is a developer-driven

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

10 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009

Table 5. Evolved producer/consumer number distributions parameters
Parameter statistics from top 10 parameter sets Mean Producer number Consumer number Mean Std. Dev. Mean Std. Dev. 0.9801 0.1104 0.6368 0.3475 Std. Dev. 0.0079 0.0101 0.1979 0.3737

Producer/Consumer Number

process. The evolved consumer number mean is much lower and standard deviation is much higher compared to the producer number. Neither one of these parameters is particularly stable, i.e., both have large standard deviations over the top 10 parameter sets. This indicates that the consumer number distribution has little effect on matching the empirical data for the top 10 parameter sets. Note that this is in alignment with the evolved weight for downloads approaching 0.0 in the utility functions. Consumers are not the driving force in matching the empirical data in the model.

DISCUSSION
Once developers join a project, it is likely that they will continue to work on the same project in the future. This is especially evident in the case of core developers, who typically work on a project for an extended period of time. Currently, the model attempts to reproduce this characteristic by giving a boost (taking the square root) of the utility function for projects worked on in the previous time step. In effect, this increases the probability of an agent selecting the same projects to work on in the subsequent time step. Improvements to the model might include adding a switching cost term to the utility function, representing the extra effort required to become familiar with another project. Gao et al. (2005) address this

issue by using probabilities based off data from SourceForge to determine when developers continue with or leave a project they are currently involved with in their FLOSS model. The model's needs vectors serve as an abstraction for representing the interests and corresponding functionalities of the agents and projects respectively. Therefore, the needs vector is at the crux of handling the matching of developers' interests with appropriate projects. For simplicity, initial needs vector values are assigned via a uniform distribution, but exploration of the effects of other distributions may be interesting. For example, if a normal distribution is used, projects with vector components near the mean will have an easy time attracting agents with similar interests. Projects with vector components several standard deviations from the mean may fail to attract any agents. A drawback of a normal distribution is that it makes most projects similar; in reality, projects are spread over a wide spectrum (e.g., from operating systems and drivers to business applications and games), although the actual distribution is unknown and difficult to measure. Currently, needs vectors for projects and agents are generated independently. This has the problem of creating projects which have no interest to any agents. An improvement would be to have agents create projects; when created, a project would clone its associated agent's needs vector (which would then evolve as other agents joined and contributed to the project). This behavior would more closely match SourceForge, where a developer initially registers his/her project. By definition, the project matches the developer's interest at time of registration. For simplicity's sake, currently the model uses a single utility function for both producers and consumers. It is possible that these two groups may attach different weights to factors in the utility function or may even have two completely different utility functions. However, analysis of the model shows that developers are the driving force to reproduce the empirical data. Exploration of a simplified model without

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 11

consumers may show that concerns about using multiple utility functions are irrelevant. One final complication with the model is its internal representations versus reality. For example, a suggested strategy for success in open source projects is to release early and release often (Raymond, 2000). Using this method to determine successful projects within the model is problematic because the model includes no concept of releasing versions of software. Augmenting the model to include a reasonable representation of software releases is non-trivial, if possible at all. Likewise, it is difficult to compare findings of other work on conditions leading to success that map into this model. For example, Lerner and Tirole (2005) consider licensing impacts while Michlmayr (2005) consider version control systems, mailing lists, documentation, portability, and systematic testing policy differences between successful and unsuccessful projects. Unfortunately, none of these aspects easily map into the model for comparison or validation purposes.

emergent properties for validation purposes, the model could move into the realm of prediction. In this case, it would be possible to feed real-life conditions into the model and then observe a given project as it progresses (or lack of progresses) in the FLOSS environment.

REFERENCES
Analysis of the linux kernel. (2004). Research report. (Coverity Incorporated) Antoniades, I., Samoladas, I., Stamelos, I., Angelis, L., & Bleris, G. L. (2005). Dynamical simulation models of the open source development process. In S. Koch (Ed.), Free/open source software development (pp. 174­202). Hershey, PA: Idea Group, Incorporated. Axelrod, R. (1984). The evolution of cooperation. New York: Basic Books. Bitzer, J., & Schröder, P. J. (2005, July). Bug-fixing and code-writing: The private provision of open source software. Information Economics and Policy, 17(3), 389-406. Brooks, F. P. (1975). The mythical man-month: Essays on software engineering. Reading, MA: Addison-Wesley. Chelf, B. (2006). Measuring software quality: a study of open source software. Research report. (Coverity Incorporated) Crowston, K., Howison, J., & Annabi, H. (2006, March/April). Information systems success in free and open source software development: Theory and measures. Software Process: Improvement and Practice, 11(2), 123­148. Crowston, K., & Scozzi, B. (2002). Open source software projects as virtual organizations: competency rallying for software development. In IEE proceedings software, 49, 3­17). Dalle, J.-M., & David, P. A. (2004, November 1). SimCode: Agent-based simulation modelling of open-source software development (Industrial Organization). EconWPA. English, R., & Schweik, C. M. (2007). Identifying success and tragedy of FLOSS commons: A preliminary classification of Sourceforge.net projects.

CONCLUSION
A better understanding of conditions that contribute to the success of FLOSS projects might be a valuable contribution to the future of software engineering. The model is formulated from empirical studies and calibrated using SourceForge data. The calibrated version produces reasonable results for the three emergent properties examined. From the calibrated data, it is concluded that the similarity between a developer and a project, the current resources going towards a project, and the maturity stage of a project are important factors. However, the cumulative resources and number of downloads a project has received are not important in reproducing the emergent properties. The model presented here aids in gaining a better understanding of the conditions necessary for open source projects to succeed. With further iterations of development, including supplementing the model with better data-based values for parameters and adding additional

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

12 International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 In FLOSS '07: Proceedings of the first international workshop on emerging trends in FLOSS research and development (p. 11). Washington, DC, USA: IEEE Computer Society. Fox, J., & Guyer, M. (1977, June). Group size and others' strategy in an n-person game. Journal of Conflict Resolution, 21(2), 323­338. Gao, Y., Madey, G., & Freeh, V. (2005, April). Modeling and simulation of the open source software community. In Agent-Directed Simulation Conference (pp. 113­122). San Diego, CA. Ghosh, R. A., Krieger, B., Glott, R., & Robles, G. (2002, June). Part 4: Survey of developers. In Free/ libre and open source software: Survey and study. Maastricht, The Netherlands: University of Maastricht, The Netherlands. Jerdee, T. H., & Rosen, B. (1974). Effects of opportunity to communicate and visibility of individual decisions on behavior in the common interest. Journal of Applied Psychology, 59(6), 712­716. Katsamakas, E., & Georgantzas, N. (2007). Why most open source development projects do not succeed? In FLOSS '07: Proceedings of the first international workshop on emerging trends in FLOSS research and development (p. 3). Washington, DC, USA: IEEE Computer Society. Kicinger, R., Arciszewski, T., & De Jong, K. A. (2005). Evolutionary computation and structural design: A survey of the state of the art. Computers and Structures, 83(23-24), 1943-1978. Koch, S. (2008). Exploring the effects of SourceForge.net coordination and communication tools on the efficiency of open source projects using data envelopment analysis. In S. Morasca (Ed.), Empirical Software Engineering: Springer. Kogut, B., & Metiu, A. (2001, Summer). Opensource software development and distributed innovation. Oxford Review of Economic Policy, 17(2), 248-264. Kowalczykiewicz, K. (2005). Libre projects lifetime profiles analysis. In Free and open source software developers' European meeting 2005. Brussels, Belgium. Krishnamurthy, S. (2002, June). Cave or community?: An empirical examination of 100 mature open source projects. First Monday, 7(6). Lerner, J., & Tirole, J. (2005, April). The scope of open source licensing. Journal of Law, Economics, and Organization, 21(1), 20­56. Linux kernel software quality and security better than most proprietary enterprise software, 4-year Coverity analysis finds. (2004). Press release. (Coverity Incorporated) Michlmayr, M. (2005). Software process maturity and the success of free software projects. In K. Zielinski & T. Szmuc (Eds.), Software engineering: Evolution and emerging technologies (p. 3-14). Krakow, Poland: IOS Press. Ostrom, E., Gardner, R., & Walker, J. (1994). Rules, games and common pool resources. Ann Arbor, MI: University of Michigan Press. Raymond, E. S. (2000, September 11). The cathedral and the bazaar (Tech. Rep. No. 3.0). Thyrsus Enterprises. Rossi, M. A. (2004, April). Decoding the "Free/ Open Source(F/OSS) Software puzzle" a survey of theoretical and empirical contributions (Quaderni No. 424). Dipartimento di Economia Politica, Università degli Studi di Siena. Smith, S. C., & Sidorova, A. (2003). Survival of opensource projects: A population ecology perspective. In ICIS 2003. Proceedings of international conference on information systems 2003. Seattle, WA. Smith, T. (2002, October 1). Open source: Enterprise ready ­ with qualifiers. theOpenEnterprise. (http://www.theopenenterprise.com/story/ TOE20020926S0002) Stewart, K. J., Ammeter, A. P., & Maruping, L. M. (2006, June). Impacts of license choice and organizational sponsorship on user interest and development activity in open source software projects. Information Systems Research, 17(2), 126­144. Tajfel, H. (1981). Human groups and social categories: Studies in social psychology. Cambridge, UK: Cambridge University Press. Wagstrom, P., Herbsleb, J., & Carley, K. (2005). A social network approach to free/open source software simulation. In First international conference on open source systems (pp. 16­23). Wang, Y. (2007). Prediction of success in open source software development. Master of science dissertation, University of California, Davis, Davis, CA.

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

International Journal of Open Source Software & Processes, 1(2), 1-13, April-June 2009 13

Weiss, D. (2005). Quantitative analysis of open source projects on SourceForge. In M. Scotto & G. Succi (Eds.), Proceedings of the first international

conference on open source systems (OSS 2005) (pp. 140­147). Genova, Italy.

Nicholas P. Radtke is a PhD candidate in computer science at Arizona State University. His research focuses on understanding and modeling free/libre open source software engineering processes. Marco A. Janssen is assistant professor on formal modeling of social and social-ecological systems within the School of Human Evolution and Social Change at Arizona State University. He is also the associate director of the Center for the Study of Institutional Diversity. His formal training is within the area of operations research and applied mathematics. His current research focuses on the fit between behavioral, institutional and ecological processes. In his research he combines agent-based models with laboratory experiments and case study analysis. Janssen also performs research on diffusion processes of knowledge and information, with applications in marketing and digital media. James S. Collofello is currently computer science and engineering professor and associate dean for the Engineering School at Arizona State University. He received his PhD in computer science from Northwestern University. His teaching and research interests lie in the software engineering area with an emphasis on software quality assurance, software project management and software process modeling and simulation.

Copyright © 2009, IGI Global. Copying or distributing in print or electronic forms without written permission of IGI Global is prohibited.

The Journal of Systems and Software 82 (2009) 1568­1577

Contents lists available at ScienceDirect

The Journal of Systems and Software
journal homepage: www.elsevier.com/locate/jss

Understanding the effects of requirements volatility in software engineering by using analytical modeling and software process simulation
Susan Ferreira a,*, James Collofello b, Dan Shunk c, Gerald Mackulak c
a

Industrial and Manufacturing Systems Engineering Department, The University of Texas at Arlington, Arlington, TX 76019, USA Computer Science and Engineering Department, Arizona State University, Tempe, AZ 76019, USA c Industrial Engineering Department, Arizona State University, Tempe, AZ 76019, USA
b

a r t i c l e

i n f o

a b s t r a c t
This paper introduces an executable system dynamics simulation model developed to help project managers comprehend the complex impacts related to requirements volatility on a software development project. The simulator extends previous research and adds research results from an empirical survey, including over 50 new parameters derived from the associated survey data, to a base model. The paper discusses detailed results from two cases that show significant cost, schedule, and quality impacts as a result of requirements volatility. The simulator can be used as an effective tool to demonstrate the complex set of factor relationships and effects related to requirements volatility. Ó 2009 Elsevier Inc. All rights reserved.

Article history: Available online 19 March 2009 Keywords: Requirements volatility Software process modeling Requirements engineering risk

1. Requirements volatility introduction Requirements volatility refers to growth or changes in requirements during a project's development lifecycle. There are multiple aliases commonly associated with or related to the phenomenon of requirements volatility. These terms include requirements change, requirements creep, scope creep, requirements instability, and requirements churn among others. Costello (1994) provides a relatively detailed set of metrics for requirements volatility. Other simple metrics for requirements volatility define it as the number of additions, deletions, and modifications made to the requirements set per time unit of interest (per week, month, phase, etc.). Requirements volatility, in its various forms, surfaces as a frequent and high impact risk in numerous empirical studies performed to identify risk factors or to understand variables leading to a project's success or failure (examples include Boehm, 1991; Curtis et al., 1988; Houston, 2000; Jones, 1994; Känsälä, 1997; Moynihan, 1997; Ropponen, 1999; Ropponen and Lyytinen, 2000; Schmidt et al., 2001; The Standish Group, 1995; Tirwana and Keil, 2006). Changes to a set of requirements can occur at multiple points during the development process (Kotonya and Sommerville, 1998). These changes can take place ``while the requirements are being elicited, analyzed and validated and after the system has gone into service". Past philosophy dictated that requirements had to be firm by the completion of the requirements phase and
* Corresponding author. Tel.: +1 817 272 1332; fax: +1 817 272 3406. E-mail addresses: ferreira@uta.edu (S. Ferreira), collofello@asu.edu (J. Collofello), dan.shunk@asu.edu (D. Shunk), mackulak@asu.edu (G. Mackulak). 0164-1212/$ - see front matter Ó 2009 Elsevier Inc. All rights reserved. doi:10.1016/j.jss.2009.03.014

that requirements should not change after this time. This view is now understood to be unrealistic (Reifer, 2000). Kotonya and Sommerville (1998) discuss that requirements change is unavoidable. They also indicate that requirements changes do not necessarily imply that poor requirements engineering practice was utilized as requirements changes could be the result of a combination of factors. The term ``requirements engineering" refers to the processes required to generate and maintain the software requirements throughout the duration of the project. Concern for the effects of requirements volatility is not usually associated with the front end of the process, for example, during requirements definition. Volatility during the requirements definition phase is expected because this is when requirements are being created. However, once the design process begins, the impact of requirements change is progressively greater due to the additional investment in time and effort as the project continues to generate artifacts and complete required tasks. Additions or modifications may need to be made to previously generated or in process project artifacts and additional time investment or scrapped effort can result. Due to the additional unplanned effort, severe consequences can potentially occur, including significant cost and schedule overruns, and at times, cancelled projects. The impact of changing requirements during later phases of a project and approaches for assessing the impacts of these changes has been well documented (Yau et al., 1978, 1986, 1988; Yau and Kishimoto, 1987; Yau and Liu, 1988). In an agile software development environment, changes are welcomed throughout the development process (Beck et al., 2001). The target of this article is not agile type projects but more traditional development type projects.

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

1569

2. The need for a requirements volatility assessment tool The effects of requirements volatility have been discussed in the literature for some time. However, little empirical research has been carried out on the topic of requirements volatility that considered the factors involved and the integrated quantitative effects of requirements volatility on factors related to key project management indicators (cost, schedule, and quality). A relatively small number of studies consider requirements volatility and its associated effects, especially in a manner integrated with other software project management factors. These existing studies primarily fall into a few major research method categories: survey or software assessment based research (Jones, 1998, 1994; Lane, 1998; Nidumolu, 1996; Zowghi et al., 2000; Zowghi and Nurmuliani, 2002), interviews and case studies (Javed et al., 2004; Loconsole and Börstler, 2005, 2007; Nurmuliani et al., 2004; Zowghi and Nurmuliani, 1998), regression analysis (Stark et al., 1999), reliability growth model (Malaiya and Denton, 1999), analytic hierarchy process analysis (Finnie et al., 1993), and simulation models (Houston, 2000; Lin and Levary, 1989; Lin et al., 1997; Madachy et al., 2007; Madachy and Tarbet, 2000; Pfahl and Lebsanft, 2000; Smith et al., 1993). The existing simulation models discussed in the literature were developed and tailored for one organization, have a limited view of requirements volatility or requirements engineering, or do not include requirements engineering processes considered in concert with the rest of the lifecycle or other critical project factors. A paucity of the literature exists on process modeling and simulation work performed in requirements engineering, an area now receiving more focused attention because of the impact that it has on the rest of the systems and software engineering lifecycle. The limited research and relative importance of requirements volatility as a risk and the relatively sparse level of requirements engineering process modeling led the researchers to more analysis and examination of these areas. A system dynamics process model

simulator, the Software Project Management Simulator (SPMS) that includes data which is stochastically based on industry survey data distributions, was then developed as part of a doctoral dissertation (Ferreira, 2002). SPMS illustrates a software business model that considers the effects of requirements volatility on a software project's key management parameters: cost, schedule, and quality. SPMS presents a more comprehensive and detailed view of the researched areas than previous models. The development of the SPMS simulator and associated results developed in this paper are discussed in this journal article. 3. Requirements volatility tool development process This section of the paper briefly discusses the research method used to develop the simulation model. Key research questions addressed in the initial research study include: (1) Which software factors are affected by requirements volatility? (2) How can these factors and the uncertainty associated to these factors be modeled? and (3) What is the project management impact of requirements volatility? Fig. 1 provides a summary view of the processes used during the research effort. Starting with the figure's top left and top right sides and flowing down, the figure illustrates that two efforts (one per figure side) were initiated concurrently and these efforts flowed into the development of the software process simulator discussed in this paper. A rigorous review of the requirements engineering and requirements volatility related literature was performed. Various process and information models were created to represent and assist in analysis and synthesis of the knowledge gained during the literature review. Requirements engineering process models and workflows, an information model, and a causal model were developed prior to the simulator development. Relevant factors and associated relationships were identified based on analysis of the literature review material and discussions with software engineering experts. Further analysis of the captured information led to the

Fig. 1. Research method.

1570

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

development of a causal model (Fig. 2). The causal model is important because it illustrates the cause and effect relationships between software development factors related to requirements volatility and includes hypothesized relationships between factors. The causal model was iteratively developed based on fundamental factor relationships (for example, job size and overall project effort), researcher industry and academic experience, and hypothesized relationships. The figure highlights (blue shading) factors and associated relationships (blue lines) that were further explored during the research effort. This causal model is expected to evolve over time as more relationships are explored and understood. One of the proposed solutions to address the identified research questions was to develop a software process simulator. A simulator was selected because it provides a tool for software project managers and researchers to perform ``what if" analyses, and enables users to examine the risk of various levels of requirements volatility and determine project outcomes. A simulator can represent the complexity of relationships between large quantities of interrelated factors and effectively illustrates the effects and impact of requirements volatility. A simulator with a graphical icon format was chosen to represent project factors and relationships. The previously developed causal model was used to develop the simulator. A subset of the previously generated causal model relationships and factors were selected and modeled. A subset of the causal model relationships and factors was chosen because follow-on work needed to be limited in length to allow key factor data to be collected or these areas were already modeled in the pre-existing simulator that was extended to create the new simulator, SPMS. Concepts and constructs from all of the generated process and information models contributed to the creation of the simulator's workflows and other model sectors. Joint research was performed with Daniel Houston to characterize four deterministic software process simulators using statistical design of experiments (Houston et al., 2001). Results of this experimentation work fed into Dan Houston's development of the Software Process Actualized Risk Simulator (SPARS) (Houston, 2000). The SPMS research simulation model evolved from Houston's SPARS model. The SPARS model also represents an adaptation, as it reuses or modifies large portions from Abdel-Hamid and Madnick's model (1991) and Tvedt's model (1996) and then extends the consolidated model to incorporate effects of a number of risk factors. The SPARS model was selected because of its comprehensive software project management scope and updates for more modern development practices. Reusing components from SPARS facilitated the development of the SPMS model in that common constructs did not need to be recreated and the model used previously validated simulator components. As part of the research effort, the SPARS model was modified to eliminate some unnecessary factors and extended to create the research model, SPMS. As the initial simulator sector designs were generated, walkthroughs were conducted with an initial set of individuals who were familiar with the research. Once an initial version of the model containing the key constructs was completed, a secondary walkthrough of the model was held with four reviewers outside of the research group. These reviewers included representatives from industry and academia that were currently performing research in requirements engineering and/or software process modeling and simulation. Following the model walkthroughs, the simulator was modified to incorporate reviewer comments and suggestions. The model was then ready to include quantitative data. Many of the model variables required data that was not available in the literature. In order to populate these model parameters, a survey was developed and administered to collect the needed data. The Project Management Institute's Information Systems

Specific Interest Group (PMI-ISSIG) sponsored the survey by providing the host site for the web-based survey and sending notifications about the survey to its members. Although PMI-ISSIG was the primary target population for the survey, one mailing was sent to individual Software Engineering Institute (SEI) Software Process Improvement Network (SPIN) group contacts within the United States and to individual professional contacts. Three hundred twelve software project managers and other software development personnel submitted responses for the survey. Survey results indicated that 78% of the respondents experienced some level of requirements volatility on their project. The survey findings highlight that requirements volatility can increase the job size dramatically, extend the project duration, cause major rework, and affect other project variables such as morale, schedule pressure, productivity, and requirements error generation. Fig. 3 shows an example of requirements volatility related data from the survey. The histogram in the figure depicts how requirements change affected the job size as a percent of the original job size. In the vast majority of cases, requirements volatility increased the job size. However, one can see that there were also cases where there was no change or a net decrease in the job size. Survey respondents had an average of 32.4% requirements volatility related job size increases. Other captured volatility effects included significant increases in project rework and reduced team morale. The survey also captured effects from schedule pressure. As the resource effort increases due to requirements volatility (to address job size additions and rework), schedule pressure also increases. The survey data showed increases in requirements error generation as the schedule pressure increases. These effects cause consequences leading to impacts on key project management indicators such as cost, schedule, and quality. More details on the survey findings showing primary and secondary effects of requirements volatility are addressed in Ferreira (2002). Statistical analysis of the survey responses allowed the generation of stochastic distributions for many of the simulation model's requirements volatility and requirements engineering factors and relationships. The model's stochastic inputs are primarily generated using either empirical discrete distributions derived from analyzing histograms of the data or are generated using the inverse transform method (Abramowitz and Stegun, 1964). The selection of the type of distribution depended on the survey analysis results. Using these stochastic inputs, the simulation model's random variates use a random number as an input to generate the desired distribution. Based on the sampling frequency, the distributions are sampled once per run or are sampled continuously throughout a run to be drawn as necessary. Once the derived stochastic factor distributions and relationships were added to the simulator, verification and validation (V&V) exercises were performed. Model validation determines whether a model is a ``useful or reasonable representation of the system" (Pritsker et al., 1997). Verification checks that the simulation model runs as intended. Matko et al. (1992) indicate that modeling work is not an exact science since the real system is never completely known. Given this situation, the validation and verification effort primarily focused on building confidence in the model as a reasonable representation of the system and in its usefulness in the provision of results. The overall approach for verification and validation included tests of the structure and behavior of the model. This strategy follows a framework of guidelines presented by Richardson and Pugh (1981) that builds confidence in the model and its results. The tests focus on suitability and consistency checks. The model verification and validation activities were performed by the model developer, and various software process and project experts. Additional verification work was performed in order to understand differences between the SPMS and SPARS model when the

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

1571

Fig. 2. Causal model.

risk factors are not actualized. Differences between the models in this case were relatively small. More information about this verification work is discussed in Ferreira et al. (2003). 4. Assessment tool capability and use SPMS illustrates researched effects of requirements volatility and includes requirements engineering extensions to the SPARS

software project management model. The SPARS model was modified to eliminate some unnecessary factors and extended to create the research model. Major additions to the base model include the researched results for the effects of requirements volatility and significant extensions to add and support the requirements engineering portions of the software development lifecycle. SPMS encompasses the requirements engineering through test phases of the software development lifecycle. Requirements volatility

1572

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

Fig. 3. Distribution of requirements volatility related percent change in job size.

starts and is stochastically simulated during the project's development and test phases. The model workflows also cover the entry of change requests, change request analysis and review activities, and their disposition (accepted, rejected/deferred).

Table 1 Model classification. Purpose Scope Model approach Planning, understanding, process improvement Medium to large size project, short to long duration, one product/ project team Continuous, mixed mode (stochastic and deterministic variables), iThinkTM simulation tool

SPMS demonstrates causal model effects of requirements volatility. Survey findings showing the impact of requirements volatility on increasing software project job size (this occurs a majority of the time), increased rework, and lowered staff morale are represented in the model using stochastic relationships derived during the survey data analysis. Over 50 new parameters that used distributions derived from the survey data were added to the model. In addition to these survey drawn distributions, a significant number of distributions were reused from other sources, or parameters were modeled using single point inputs that could be changed by a user. The effects of lowered morale on requirements engineering productivity and requirements error generation are represented in the model. Schedule pressure effects on requirements error generation were also studied as part of the survey data analysis and were added to pre-existing schedule pressure effects in the model. Among other survey data used in the simulator, the model includes requirements defect detection effectiveness for various software development activities or milestones and relative work rates of requirements volatility related activities compared to their normal work rate. Other model contributions include the addition of a requirements engineering staff type and requirements engineering support activities which were added to pre-existing simulator development and test personnel and activities. Table 1 presents a view of the model's classification, according to the characterization framework from Kellner et al. (1999). The typical model audience is expected to be software development project managers or researchers seeking to gain an understanding of requirements engineering and requirements volatility and its effects integrated with other software project risks. The model is relatively complex, given its purpose, and assumes a sophisticated

Table 2 Model sector descriptions. Sector name Change request work flow Requirements work flow Development and test work flow Description Stochastic change request entry over 10 intervals. Incoming change request (CR) analysis, change request control board (CCB) review, and disposition Requirements generation and review process including requirements error and defect rework. Stochastic entry of requirements volatility related project scope changes and rework over 10 intervals Work product flow through the development lifecycle, from design and code through testing and rework of design and code errors. Work is pulled from development and test activities for requirements volatility related rework and reduction and for rework of requirements defects A support sector that calculates the amount of product to add to the product cycle for requirements volatility additions based on survey data A support sector that calculates the amount of product to pull from the development and test work for requirements volatility related rework based on survey data A support sector that calculates the amount of product to pull from development and testing work flow activities for requirements related reductions. Includes policy choice that defines where to remove product Entry and summation of requirements engineering, developer, and tester planned staffing profile information Allocation of requirements engineer effort to requirements engineering activities based on activity priority Allocation of developer and tester effort to project activities based on activity priority Generation and detection of requirements errors and defects Generation and detection of design and code errors and defects Entry and exit of staff based on planned staffing profiles, assimilation of new staff, attrition, and replacements. Entry of contract personnel and organizationally experienced personnel handled separately for the different staff groups Attrition (including attrition due to low morale) and replacement calculations for requirements engineers and the grouped set of developers and testers Calculation of requirements engineer and the grouped developer and tester work force levels needed based on willingness to hire Calculation of effort perceived still needed to complete project, effort shortages, schedule pressure, and assumed requirements engineer, developer, tester productivity. Calculations to determine start of development and testing Adjustment of job effort based on requirements volatility related additions, rework, and reductions as well as from underestimation Productivity inputs and calculation of project activity productivity based on productivity multipliers Work rate modification due to effort remaining, effort required, and staff exhaustion. Generation of two staff type productivity multipliers (including learning, communication overhead, staff experience, and morale) Calculation of requirements engineering and development and testing progress. Modification of currently perceived job size based on requirements volatility and discovered work due to underestimation Adjustment of staffing and schedule multipliers based on senior management commitment. This is the only sector that was not modified from the original SPARS model

Requirements change additions Requirements change rework Requirements change reductions Planned staffing Requirements engineer effort allocation Developer and tester effort allocation Requirements quality management Development and test quality management Actual staffing Attrition and replacement Planning Control Adjustment of job effort Productivity inputs Productivity Progress measure Senior management commitment

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

1573

user that is educated on the use of simulation models and software development project management. The model is practically based, relying on a significant and proven foundation of software project management and simulation research. The model is segmented into 20 sectors. The sectors are organized into convenient and logical groupings of related factors. Table 2 provides an overview of the model sectors with a brief description of each in order to give the reader an introduction to the model's scope. An example of one sector excerpted from the model is shown in Fig. 4. The view illustrates the requirements engineering work flow sector. The connections on the right of the sector flow into or out of the development and test work flow sector (sector not shown). The requirements work flow sector encompasses a normal product work flow. Requirements are generated and reviewed during the normal product work flow. The normal requirements work

flow begins at the initiation of the requirements engineering phase, at the To_Be_Worked stock. The To_Be_Worked stock is initially populated with the estimated starting job size. The work then flows through the Generated_Reqts, Reqts_Awaiting_Review, Reviewed_Reqts stocks, and then into the development process as an inflow. The requirements generation activities are aggregated, encompassing requirements elicitation, analysis, negotiation, and initial requirements management. Once reviewed, the requirements product is dispositioned and defective product containing requirements errors is removed from the normal work flow and becomes part of the requirements error rework work flow, to be reworked before flowing into the development and test activities. Requirements defects caught post the requirements phase come back into the requirements defect rework work flow to be reworked. The reworked product then flows back into the development and test activities. Additions to job size due to volatility

Fig. 4. SPMS simulator requirements engineering work flow sector.

1574

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

and underestimation flow into the normal work flow through the course of the project. The lower half of the sector includes the requirements change rework work flow. Rework due to requirements volatility is drawn from the development and test work flows and is worked though the requirements change rework work flow. This work flow contains separate requirements error related activities. The model allows the user to enter detailed inputs related to their project. Other data is automatically extracted from the stochastic distributions derived from the survey data. Table 3 provides a listing of a subset of the new inputs added to the model to provide a flavor for the types of data required from the user. These inputs can be input per run or a set of runs. Table 4 identifies the new

survey distributions added to the model. Data from the survey is extracted from stochastic distributions with timing as defined in Table 4. ``1 Selection per run" means that a value is pulled from the stochastic distribution one time per run and used throughout the run. While the data listed in Tables 3 and 4 do not include all the new or modified factors in the model it does provide a perspective of the type of data that the user can enter and use. 5. Assessment tool results The simulator was used to run two cases for the purpose of comparing them. Each case was setup for 100 runs apiece. The set of 100 runs for each case was selected for convenience as the modeling tool allows additional runs, if desired. The two cases are as follows: (1) A baseline case without the requirements volatility actualized [baseline] and (2) a case with the requirements volatility related factors actualized [reqts volatility]. The data in the square brackets corresponds to the case identifier in later figures. Actualizing the requirements volatility risk for the second case allows the model to represent the stochastic researched effects of requirement volatility. These include survey-based effects related to requirements additions, changes, and modifications as well as entering change requests. Also included, among the other effects, are morale, and related schedule pressure effects on morale. The initial job size was estimated to be 400 function points. The original schedule estimate (planned duration) was defined to be 408 days. Figs. 5­8 depict the differences, at the completion of the projects, between the baseline runs (no requirements volatility considered) and runs with the other case where the requirements volatility risk is actualized for various summary outputs. The model allows additional detailed outputs, if desired. Box plots were used to represent the data because the simulation results were positively skewed given the tendency for higher project size, cost, duration, and released defects (among other outputs) with the actualization of the requirements volatility risk. The box plots provide a graphical display of the center and the variation of the data, allowing one to see the symmetry of the data set (Ott, 1988). With the exception of the final project size (Fig. 5), the baseline results show a small level of variability because some of the model parameters (e.g. requirements engineering process factors) were modeled stochastically. Therefore, even when the requirements volatility risk is not actualized, some variability in results will appear.

Table 3 New factor user inputs (subset only). Factor Quantity of experienced requirements engineers (REs), per interval Quantity of new requirements engineers, per interval Quantity of experienced personnel allocated to training new reqts engineers (%) RE transfer in day (days) Experienced organization RE transfer quantity Time delay to transfer REs off project (days) Max quantity of new RE hires per experienced staff (staff/staff) Time for organization experienced REs (but not project experienced) to be productive (days) Project day that person in org is scheduled to come onto project (day) Quantity of RE staff experienced in the organization who are to be transferred on project (staff) Requirements review adjustment policy ­ adjusts review effort (boolean switch) Reworked requirements review adjustment policy (boolean switch) Requirement error bad fix fraction (%) Requirements defect bad fix fraction (%) Nominal change request analysis productivity (function points/person-day) Nominal requirements generation productivity (function points/person-day) Nominal requirements review productivity (function points/person-day) Requirements volatility (boolean switch) Input quantity 10 (1 for each of 10 intervals) 10 (1 for each of 10 intervals) 1 1 1 1 1 1 1 1 1 Selection 1 Selection 1 1 1 1 1 1 Selection

Table 4 New survey data distributions. Factor Quantity of requirements change requests, per interval (intervals 1­10) Change request time span with requirements volatility (ratio of duration) Determination of change request acceptance/deferral due to schedule pressure (ratio) Relative requirements defect rework productivity (ratio) Relative requirements generation rework productivity (ratio) Relative requirements change error rework productivity (ratio) Relative requirements change requirement review productivity (ratio) Relative design and code requirements defect rework productivity (ratio) Percentage of perceived job size increased due to requirements volatility, per interval (%) Percentage of perceived job size reworked due to requirements volatility, per interval (%) Percentage of perceived job size reduced due to requirements volatility, per interval (%) Requirements review effectiveness (%) Design and code requirement defect detection effectiveness (%) Design and code review requirement defect detection effectiveness (%) Test requirement defect detection effectiveness (%) Reworked requirements review effectiveness (%) Reworked requirements design and code defect detect effectiveness (%) Requirements error multiplier for schedule pressure (ratio) Requirements error multiplier for morale (ratio) Selection frequency 10 Selections per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 10 Selections per run 10 Selections per run 10 Selections per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run 1 Selection per run (1 selection for each interval)

(1 selection for each interval) (1 selection for each interval) (1 selection for each interval)

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

1575

Fig. 5. Project size box plots.

Fig. 7. Project duration box plots.

Fig. 8. Project released defect density box plots. Fig. 6. Project cost box plots. Table 5 Case study simulation results. Output/Case Average Median 400 485.7 4119.0 5968.9 452.0 608.5 Std. Dev. 0 82.0 35.9 1398.1 4.3 132.8 Minimum 400 382.0 3969.5 3988.5 442.0 444.0 0.726 0.747 Maximum 400 788.3 4155.5 11196.1 461.0 1101.0 0.792 1.170

Fig. 5 illustrates the difference in project size between the baseline case and the requirements volatility case. The baseline size is 400 function points. The average size for the requirements volatility case is 499.9 function points with a standard deviation of 82.0, and a range from 382.0 to 788.3 function points over the 100 runs. The runs that have values below the baseline 400 function points (e.g. 382.0) can be explained by the fact that requirements changes do not always add additional project scope, but can be made to remove scope or unnecessary requirements. However, 93 of the 100 runs had a final project size greater than the base case of 400 functions points with relatively large scope added in the course of the project. This scope addition has a significant negative ripple effect on the project cost (effort in person-days) and project duration. Table 5 contains the detailed statistics for each case including average, median, standard deviation calculations and the minimum and maximum run values. Fig. 6 presents the results for cost. Cost is considered to be human effort consumed during the project in person-days. This cost is equivalent to effort and does not include other project costs. The effort or cost presented in the figure encompasses the cumulative effort for the requirements engineering through test activities.

Size (function points) Baseline 400 Reqts volatility 499.9 Cost (effort in person days) Baseline 4107.6 Reqts volatility 6208.2 Duration (days) Baseline Reqts volatility 452.4 634.0

Released defect density (defects per function point) Baseline 0.753 0.753 0.013 Reqts volatility 0.894 0.886 0.075

The detailed statistics are presented in Table 5. As mentioned earlier, the baseline results do show a small level of variability because some of the model parameters are represented stochastically. The average for the requirements volatility case is over 2000 person-days more than the average for the baseline case.

1576

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

Considering that the baseline cost is an average of 4107.6 person days, this is a very large number for a program manager to justify as it represents more than a 50% increase in resource costs. The cost range for the requirements volatility case is very large as well. The requirements volatility case has a maximum of 11,196.1 person-days. This represents a very significant difference from the baseline maximum. Fig. 7 displays the results for project duration in days. The duration encompasses the requirements engineering through test activities for the project. The detailed statistics for the two cases are shown in Table 5. It takes an average of 452.4 days to complete the project for the baseline case. The average for the requirements volatility case is significantly higher at an average of 634.0 days. As in the case of the cost, the range for the requirements volatility case has a wide span. Fig. 8 presents the project released defect density box plots. The data represents the defects released post the test process and is in units of defects per function point. The quantity of released defects is also significantly higher for the requirements volatility case. The baseline case has an average of 0.753 defects per function point. The average for the requirements volatility case is 0.894. The range span for this case is also relatively large. For the case that include the risk of requirements volatility, the box plots for each parameter have a wide span. The wide span in outcomes indicates a large potential for unpredictable results as well as considerable impact on project results. A comparison of the model results for projects with no requirements volatility and those with requirements volatility show significant differences in the box plots. One can clearly see the potential impact of requirements volatility on key project management parameters such as cost (effort), schedule, and quality. Differences between the baseline and requirements volatility case results can be attributed to the addition (or reduction) in project scope caused by requirements volatility, cause and effect relationships between factors, and the stochastic representation of a number of the factors.

In addition to simulating the effects of requirements volatility, SPMS offers the ability to simulate various project scenario combinations starting with the requirements engineering portion of the lifecycle. The model offers a robust set of parameters that capture various facets of the project including requirements errors and defects, requirements defect density, defect containment effectiveness for various milestones, and other parameters integrated with a very comprehensive development and test related set of factors and relationships. Each of the model distributions can be easily calibrated and tailored to a specific organization's environment. All the other factors are also setup to be readily modifiable to reflect an organization's historical data. Survey data used in the simulator captures information on factors and relationships not previously available from a wide population in the software industry and allows modeling variables stochastically given the large quantity of survey responses. Many of the model variables were modeled stochastically to allow the user to assess project outcomes probabilistically. The model results quantitatively illustrate the potential for significant cost, schedule, and quality impacts related to requirements volatility. Software project managers can use this tool to better understand the effects of requirements volatility and this risk in concert with other common risks. The simulator can be used to assess potential courses of action to address the risk of requirements volatility.

7. Future research Additional research in the area of requirements engineering, as it continues to evolve, is expected to provide a continuous stream of new ideas, perspectives, and information that can allow for the development of richer models and that can represent different facets of understanding in this under-represented yet critical research area. More work needs to be done to model the impact of requirements engineering related processes and policies so that project managers and software development personnel are more aware of the impact of the decisions they make during this phase and how the rest of the lifecycle may be affected by their choices. This work can lead to the further identification of best practices and generate insights valuable to managing software development projects in the future. As this and other simulators that incorporate requirements engineering processes and relationships continue to evolve, additional experimentation with the models may prove valuable in identifying common factors and relationships. The research model presented in this paper is relatively large and complex. Sensitivity analysis can assist in the identification of influential factors in this and other models. The results of further experimentation may be used to ``prune" insignificant factors from the model so that a more efficient model can result (Houston et al., 2001). One of the benefits of a simpler model includes a shortened training and learning ramp-up time. Core concepts that make the most difference in results can be emphasized. Since the model is less complex it becomes easier to understand and may perhaps be more popular given the reduced time to understand and then populate the model with organizational and project specific data. Maintenance time may also be reduced because the model is smaller and it is easier to find and fix problems. Agile processes which welcome requirements changes present another valuable area to study. As these processes continue to mature and more quantitative results are available, simulating different types of agile processes, (e.g. XP, Scrum, etc.) would be of interest to determine the project management ramifications related to cost, schedule, and quality as compared to more traditional approaches.

6. Summary and conclusions Key research questions were addressed as part of the research. These questions include: (1) Which software factors are affected by requirements volatility? (2) How can these factors and the uncertainty associated to these factors be modeled? and (3) What is the project management impact of requirements volatility? A causal model was developed and used to evaluate which software factors are affected by requirements volatility. A survey was administered to collect information for a subset of the factors identified in the causal model. This allowed the researchers to verify the relationships and quantify the level of the relationships. A simulator was chosen as the means to model how the factors relate to other project factors and to represent the uncertainty associated to the factors using stochastic distributions derived from survey data. The SPMS model assists in understanding and evaluating the program management impact of requirements volatility. This simulator can help an interested user to better understand the requirements engineering process and the impact of requirements volatility via its process-oriented workflows and comprehensive scope. The factor relationships represented in the model represent a significant contribution. This work expands our understanding of the requirements engineering process and the effects of requirements volatility. The rigorous research to both understand and leverage the previous foundation of knowledge in requirements engineering and requirements volatility and the thorough nature of the survey and analysis of this valuable data to assess factor relationships and populate the simulator with empirical data is a significant contribution.

S. Ferreira et al. / The Journal of Systems and Software 82 (2009) 1568­1577

1577

References
Abdel-Hamid, T., Madnick, S., 1991. Software Project Dynamics: An Integrated Approach. Prentice-Hall, Englewood Cliffs, NJ. Abramowitz, M., Stegun, I.A. (Eds.), 1964. Handbook of Mathematical Functions, Applied Mathematics Series 55. National Bureau of Standards, Washington, DC. Beck, K., Beedle, M., van Bennekum, A., Cockburn, A., Cunningham, W., Fowler, M., Grenning, J., Highsmith, J., Hunt, A., Jeffries, R., Kern, J., Marick, B., Martin, R., Mellor, S., Schwaber, K., Sutherland, J., Thomas, D., 2001. Principles Behind the Agile Manifesto. Retrieved 11.6.2008 from: <http://www.agilemanifesto.org/ principles.html>. Boehm, B.W., 1991. Software risk management: principles and practices. IEEE Software 8 (1), 32­41. Costello, R.J., 1994. Metrics for Requirements Engineering. Master of Science Thesis, California State University, Long Beach. Curtis, B., Krasner, H., Iscoe, N., 1988. A field study of the software design process for large systems. Communications of the ACM 31 (11), 1268­1287. Ferreira, S., 2002. Measuring the Effects of Requirements Volatility on Software Development Projects, Ph.D. Dissertation, Arizona State University. Ferreira, S., Collofello, J., Shunk, D., Mackulak, G., Wolfe, P., 2003. Utilization of Process Modeling and Simulation in Understanding the Effects of Requirements Volatility in Software Development. In: Proceedings of the 2003 Process Simulation Workshop (ProSim 2003). Finnie, G.R., Witting, G.E., Petkov, D.I., 1993. Prioritizing software development productivity factors using the analytic hierarchy process. Journal of Systems and Software 22 (2), 129­139. Houston, D.X., 2000. A Software Project Simulation Model for Risk Management, Ph.D. Dissertation, Arizona State University. Houston, D.X., Ferreira, S., Collofello, J.S., Montgomery, D.C., Mackulak, G.T., Shunk, D.L., 2001. Behavioral characterization: finding and using the influential factors in software process simulation models. Journal of Systems and Software 59 (3), 259­270. Javed, T., Maqsood, M., Durrani, Q.S., 2004. A study to investigate the impact of requirements instability on software defects. ACM SIGSOFT Software Engineering Notes 29 (3), 7. Jones, C., 1994. Assessment and Control of Software Risks. PTR Prentice-Hall, Inc., Englewood Cliffs, NJ. Jones, C., 1998. Estimating Software Costs. McGraw-Hill, New York. Känsälä, K., 1997. Integrating risk assessment with cost estimation. IEEE Software 14 (3), 61­67. Kellner, M.I., Madachy, R., Raffo, D.M., 1999. Software process modeling: why? what? how? Journal of Systems and Software 46 (2­3), 91­105. Kotonya, G., Sommerville, I., 1998. Requirements Engineering: Processes and Techniques. John Wiley and Sons, Ltd.. Lane, M.S., 1998. Enhancing software development productivity in Australian firms. In: Proceedings of the Ninth Australasian Conference on Information Systems (ACIS '98), vol. 1, pp. 337­349. Lin, C.Y., Abdel-Hamid, T., Sherif, J.S., 1997. Software-engineering process simulation model (SEPS). Journal of Systems and Software 38 (3), 263­277. Lin, C.Y., Levary, R.R., 1989. Computer aided software development process design. IEEE Transactions on Software Engineering 15 (9), 1025­1037. Loconsole, A., Börstler, J., 2005. An industrial case study on requirements volatility measures. In: Proceedings of the 12th Asia-Pacific Software Engineering Conference (APSEC '05), 8 p. Loconsole, A., Börstler, J., 2007. Are size measures better than expert judgment? An industrial case study on requirements volatility. In: Proceedings of the 14th Asia-Pacific Software Engineering Conference (APSEC '07), pp. 238­245. Madachy, R., Tarbet, D., 2000. Initial experiences in software process modeling. Software Quality Professional 2 (3), 1­13. Madachy, R., Boehm, B., Lane, J., 2007. Assessing hybrid incremental processes for SISOS development. Software Process: Improvement and Practice 12 (5), 461­ 473. Malaiya, Y.K., Denton, J., 1999. Requirements volatility and defect density. In: Proceedings of the 10th International Symposium on Software Reliability Engineering, pp. 285­294. Matko, D., Zupancic, B., Karba, R., 1992. Simulation and Modeling of Continuous Systems: A Case Study Approach. Prentice-Hall International Ltd., Great Britain. Moynihan, T., 1997. How experienced project managers assess risk. IEEE Software 14 (3), 35­41. Nidumolu, S.R., 1996. Standardization, requirements uncertainty and software project performance. Information and Management 31 (3), 135­150. Nurmuliani, N., Zowghi, D., Fowell, S., 2004. Analysis of requirements volatility during software development life cycle. In: Proceedings of the 2004 Australian Software Engineering Conference (ASWEC '04), pp. 28­37. Ott, L., 1988. An Introduction to Statistical Methods and Data Analysis. PWS-KENT Publishing Company. Pfahl, D., Lebsanft, K., 2000. Using simulation to analyze the impact of software requirements volatility on project performance. Information and Software Technology 42 (14), 1001­1008. Pritsker, A. Alan B., O'Reilly, Jean J., LaVal, David K., 1997. Simulation with Visual SLAM and AweSim. System Publishing Company, West Lafayette, IN. Reifer, D.J., 2000. Requirements management: the search for Nirvana. IEEE Software 17 (3), 45­47. Richardson, George, P., Alexander, L., Pugh III, 1981. Introduction to System Dynamics Modeling with DYNAMO. The MIT Press, Cambridge, MA.

Ropponen, J., 1999. Risk assessment and management practices in software development. Chapter 8 in Beyond the IT Productivity Paradox. John Wiley and Sons. pp. 247­266. Ropponen, J., Lyytinen, K., 2000. Components of software development risk: how to address them? A project manager survey. IEEE Transactions on Software Engineering 26 (2), 98­112. Schmidt, R., Lyytinen, K., Keil, M., Culle, P., 2001. Identifying software project risks: an international Delphi study. Journal of Management Information Systems 17 (4), 5­36. Smith, B.J., Nguyen, N., Vidale, R.F., 1993. Death of a software manager: how to avoid career suicide through dynamic software process modeling. American Programmer 6 (5), 10­17. The Standish Group, 1995. The Chaos Report. Obtained from <http://www. standishgroup.com/chaos.html>. Stark, G.E., Oman, P., Skillicorn, A., Ameele, A., 1999. An examination of the effects of requirements changes on software maintenance releases. Journal of Software Maintenance: Research and Practice 11 (5), 293­309. Tirwana, A., Keil, M., 2006. Functionality risk in information systems development: an empirical investigation. IEEE Transactions on Engineering Management 53 (3), 412­425. Tvedt, J.D., 1996. An Extensible Model for Evaluating the Impact of Process Improvements on Software Development Cycle Time. Ph.D. Dissertation, Arizona State University. Yau, S.S., Collofello, J.S., MacGregor, T., 1978. Ripple effect analysis of software maintenance. In: Proceedings of the IEEE Computer Society's Second International Computer Software and Applications Conference (COMPSAC '78), pp. 60­65. Yau, S.S., Kishimoto, Z., 1987. A method for revalidating modified programs in the maintenance phase. In: Proceedings of 11th IEEE International Computer Software and Applications Conference (COMPSAC `87), pp. 272­277. Yau, S.S., Liu, C.S., 1988. An approach to software requirement specifications. In: Proceedings of 12th International Computer Software and Applications Conference (COMPSAC `88), pp. 83­88. Yau, S.S., Nicholl, R.A., Tsai, J.P., 1986. An evolution model for software maintenance. In: Proceedings of 10th IEEE International Computer Software and Applications Conference (COMPSAC `86), pp. 440­446. Yau, S.S., Nicholl, R.A., Tsai, J.P., Liu, S.S., 1988. An integrated life-cycle model for software maintenance. IEEE Transactions on Software Engineering 14 (8), 1128­1144. Zowghi, D., Nurmuliani, 1998. Investigating requirements volatility during software development: research in progress. In: Proceeding of the Third Australian Conference on Requirements Engineering (ACRE98), pp. 38­48. Zowghi, D., Nurmuliani, 2002. A study of the impact of requirements volatility on software project performance. In: Proceedings of the Ninth Asia-Pacific Software Engineering Conference (ASPEC '02), pp. 3­11. Zowghi, D., Offen, R., Nurmuliani, 2000. The impact of requirements volatility on the software development lifecycle. In: Proceedings of the International Conference on Software Theory and Practice (IFIP World Computer Congress), pp. 19­27.

Susan Ferreira is an Assistant Professor and the founding Director of the Systems Engineering Research Center (SERC) at The University of Texas, Arlington (UTA). Before joining UTA, Dr. Ferreira worked as a systems engineer in the Defense industry on complex software intensive systems. Her industry background includes work for Lockheed Martin, General Dynamics/Motorola, and Northrop Corporation. Her teaching and research interests are related to systems engineering (SE) and include requirements engineering, SE process modeling and simulation, lean SE, SE return on investment, SE cost estimation, and system of systems engineering. James Collofello is currently Associate Dean for the Engineering School and Professor of Computer Science and Engineering at Arizona State University. He received his Ph.D. in Computer Science from Northwestern University. His teaching and research interests lie in the software engineering area with an emphasis on software project management, software quality assurance and software process modeling. In addition to his academic activities, he has also been involved in applied research projects, training and consulting with many large corporations over the last 25 years. Dan Shunk is the Avnet Professor of Supply Network Integration in Industrial Engineering at Arizona State University. He is currently pursuing research into collaborative commerce, global new product development, model-based enterprises and global supply network integration. He won a Fulbright Award in 2002-2003, the 1996 SME International Award for Education, the 1991 and 1999 I&MSE Faculty of the Year award, the 1989 SME Region VII Educator of the Year award, chaired AutoFact in 1985, and won the 1982 SME Outstanding Young Engineer award. Dr. Shunk studied at Purdue where he received his Ph.D. in Industrial Engineering in 1976. Gerald Mackulak is an Associate Professor of Engineering in the Department of Industrial, Systems and Operations Engineering at Arizona State University. He is a graduate of Purdue University receiving his B.Sc., M.Sc., and Ph.D. degrees in the area of Industrial Engineering. His primary area of research is simulation methodology with a focus on model abstraction, execution speed and output analysis. He has authored over 75 articles, served as an associate editor for two simulation journals and continues to guide research in simulation efficiency.

Using Simulation to Facilitate the Study of Software Product Line Evolution1
Yu Chen, Gerald C. Gannod2, James S. Collofello, and Hessam S. Sarjoughian   Dept. of Computer Science and Engineering Division of Computing Studies Arizona State University ­ Tempe Campus Arizona State University ­ Polytechnic Campus Tempe AZ 85287, USA Mesa AZ 85212, USA {yu_chen, gannod, collofello, sarjoughian}@asu.edu Abstract
A product line approach is a disciplined methodology for strategic reuse of source code, requirement specifications, software architectures, design models, components, test cases, and the processes for using the aforementioned artifacts. Software process simulation modeling is a valuable tool for enabling decision making for a wide variety of purposes, ranging from adoption and strategic management to process improvement and planning. In this paper, discrete event simulation is used to provide a framework for the simulation of software product line engineering. We have created an environment that facilitates strategic management and long-term forecasting with respect to software product line development and evolution. simulator that facilitates software product line decision making at an early stage by providing time and cost estimates under various situations. In this paper, discrete event simulation theory and Constructive Product Line Investment Model (COPLIMO) [2] are used to create an environment that facilitates strategic management and long-term forecasting with respect to software product line development and evolution. Specifically, the simulator facilitates the study of the effect of a number of process decisions, including choice of evolution approach, upon factors such as effort and time-to-market. The simulator not only gives statistical results at the end of the simulation, but also visually presents how major product line engineering activities progress and interact over time. The simulator is built upon DEVSJAVA [1], a general-purpose Java-based discrete event simulation framework. The tool is extensible and allows other simulation frameworks and cost models to be used. The remainder of this paper is organized as follows. Section 2 presents background information. Section 3 describes the simulation model and the simulator. Results are discussed in Section 4. Section 5 contains related work and Section 6 draws conclusions and suggests future investigations.

1. Introduction
A software product line is defined as a set of softwareintensive systems sharing a common, managed set of features that satisfy the specific needs of a particular market segment or mission and are developed from a common set of core assets in a prescribed way [8]. A product line approach is a disciplined methodology for strategic reuse of source code, requirement specifications, software architectures, design models, components, test cases, and the processes for using the aforementioned artifacts. Software product line engineering promises large-scale productivity gains, shorter time-to-market, higher product quality, increased customer satisfaction, decreased development and maintenance cost [8]. However, those benefits are not guaranteed under all situations, and they are affected by many factors such as the initiation situation, the adoption and evolution approaches, the market demands, and the available resources. The goal of this research is to develop a
1 2

2. Background
This section describes background information on Software Product Lines, software process simulation, DEVSJAVA [1], and COPLIMO [2].

2.1. Software product lines
Software product line development involves three essential activities: core asset development, product development, and management [8]. Core asset

This material is based upon work supported by the National Science Foundation under grant No. CCR-0133956. Contact author.

development (domain engineering) involves the creation of common assets and the evolution of the assets in response to product feedback, new market needs, etc. Product development (application engineering) creates individual products by reusing the common assets, gives feedback to core asset development, and evolves the products. Management includes technical and organizational management, where technical management is responsible for requirement control and the coordination between core asset and product development activities. There are two main software product line adoption approaches: big bang and incremental [10]. With the big bang approach, core assets are developed for a whole range of products prior to the creation of any individual product. With the incremental approach, core assets are incrementally developed to support the next few upcoming products. In general, the big bang approach has a higher return on investment but involves more risks, while the incremental approach has lower entry costs but higher total costs. The four common software product line adoption situations are: independent, project-integration, reengineering-driven, and leveraged [10]. Under the independent situation, a product line is created without any pre-existing products. Under the project-integration situation, a product line is created to support both existing and future products. Under a reengineering-driven scenario, a product line is created by reengineering existing legacy systems. And the leveraged situation is where a new product line is created based on some existing product lines. Some common product line evolution strategies are: infrastructure-based, branch-and-unite, and bulkintegration [10]. The infrastructure-based strategy does not allow deviation between the core assets and the individual products, and requires that new common features be first implemented into the core assets and then built into products. Both the branch-and-unite and the bulk-integration strategies allow temporal deviation between the core assets and the individual products. The branch-and-unite strategy requires that the new common features be reintegrated into the core assets immediately after the release of the new product, while the bulkintegration strategy allows the new common features to be reintegrated after the release of a group of products.

expensive and risky, so software process simulation modeling is often used to reduce the uncertainty and predict the impact. Software process simulation modeling can be used for various purposes and scopes, and have been supported by many technologies [3]. The software product line process simulator described in this paper is for long-term organization strategic management, and is implemented in DEVSJAVA [1], a Java implementation of the Discrete Event System Specification (DEVS) modeling formalism [1]. The external view of a DEVSJAVA model is a black box with input and output ports. A model receives messages through its input ports and sends out messages via its output ports. Ports and messages are the means and the only means by which a model can communicate with the external world. A DEVSJAVA model is either "atomic" or "coupled". An atomic model is undividable and generally used to build coupled models. A coupled model consists of input and output ports, a finite number of (atomic or coupled) models, and couplings. The couplings link model ports together and are essentially message channels. They also provide a simple way to construct hierarchical models. To execute atomic and coupled models, DEVSJAVA uses distinct atomic and coupled simulators that support incremental simulation model development. These simulators can execute in alternative settings (i.e., sequential, parallel, or distributed). An important feature of the DEVS framework is the ability for models to seamlessly execute either in logical or (near) real-time. Furthermore, due to its availability of its source code and object-oriented design, DEVSJAVA can be extended to incorporate domain-specific (e.g., Software Product Line) logic and semantics.

2.3. COPLIMO
In the simulator, COMPLIMO [2] is used as the cost model to provide cost estimates. COPLIMO is a COCOMO II [9] based model for software product line cost estimation, and has a basic life cycle model and an extended life cycle model. The basic life cycle model has two sub-models: a development model for product line creation and a post-development model for product line evolution. Although the basic life cycle model has many simplification assumptions, it is thought to be good enough for early stage product line trade-off considerations [2]. The basic model also can be easily extended to the extended life cycle model, which allows products have different parameter values instead of the same values. In the implementation, the cost model is designed as a plug-in model, thus other cost models can be plugged in to meet other needs.

2.2. Simulation
A software process is a set of activities, methods, practices, and transformations that people use to develop and maintain software and associated products, such as project plans, design documentations, code, test cases, and user manuals [4]. Adopting a new software process is

3. Approach
A simulation framework and a software cost model were used to develop the simulator. Although DEVSJAVA [1] and COMPLIMO [2] are currently used, they can be replaced by other suitable simulation frameworks and cost models. This section presents the abstract software product line engineering model, the specifics of the simulation models, the assumptions, and the simulation tool.

products are developed by reusing the existing core assets, and existing products are updated after the change of the core assets. Figure 3.1 depicts the process flow. Costs associated with this approach include core asset development costs, new product development costs, and existing product maintenance costs. Compared to the big bang approach, the incremental approach has higher product development costs because of the incompleteness of the core assets, and extra product maintenance costs as the result of a short-term planning penalty.

3.1. Abstract product line model

Figure 3.2. SPL evolution approaches Figure 3.1. SPL adoption approaches Software product line engineering typically involves a creation phase and an evolution phase [10]. Currently the simulator provides two options (big bang and incremental) for the creation stage and two options (infrastructure-based and branch-and-unite) for the evolution stage. In the following, we will discuss the costs associated with those cases in detail. With the big bang adoption approach, core assets are first developed to meet the requirements for a whole range of products. Products are then developed by reusing the core assets [10]. Figure 3.1 illustrates the process flow. Costs associated with this approach include core asset development costs and new product development costs. With the incremental adoption approach, the core assets are incrementally developed to meet the requirements of the next few upcoming products, new With the infrastructure-based product line evolution strategy, the process for building a new product is the following: core assets are updated by incorporating new common requirements, and the new product is developed and existing products are updated. Figure 3.2 shows the process flow. The COPLIMO [2] basic life cycle model assumes that a change to a product causes the same percentage of change on reused code, adapted code, and product unique code. So if the change rate caused by new product requirements is , then the costs for one product development iteration include the costs of maintaining the core assets with a change rate of , the costs of developing the new product, and the costs of maintaining existing products with a change rate of . With the branch-and-unite product line evolution strategy, the process for building a new product is the following: the new product is developed, core assets are updated to incorporate new common features, and existing products are updated (including the newly

created product). Figure 3.2 illustrates the process flow. If  is the change rate caused by new product requirements, then the costs for one product development iteration include the costs of developing a new product with (1-) percentage of the reuse rate, the costs of maintaining the core assets with a change rate of , and the costs of maintaining existing products (including the newly created one) with a change rate of . Product maintenance costs are higher in this case because it has one more product to update, the newly created one. The new product is first created with new features that are not supported by the core assets, then after the core assets are updated to incorporate the new features the new product needs to be updated to keep consistent with the core assets. The new product development costs are also higher with this approach, because of the lower reuse rate.

project cannot be started until the requested resources are granted from the Employee Pool. If the number of employees in the employee pool is not less than the requested number of employees, the requested amount of employees will be granted. Otherwise, if the number of available employees meets the minimum employee level (a model parameter, between 5/8 and 1), then the number of available employees will be granted. In that case, a job can be started with fewer resources but longer development time. In other cases, the employee pool will not grant any resources until enough resources are returned.

3.2. Model development
Twelve DEVSJAVA [1] models were developed to model software product line engineering activities. Figure 3.3 shows a UML diagram depicting the hierarchical structure of the model. Some time constraints are imposed in the simulator: for each atomic model, jobs are processed one by one in FIFO order (or in combination with some priority). The PLPEF (Product Line Process Experimental Frame) is the top level coupled model and contains a Product Line Process instance and an Experimental Frame instance. The Product Line Process models software product line engineering activities. It contains an instance of Technical Management, Core Asset Development, and Employee Pool, and a finite number of Product Development instances. The number of Product Development models to be included depends on the number of projected products in the product line. The Product Line Process receives market demands and dispatches them to Technical Management. It sends out requirements (generated by Technical Management and Maintenance Requirement Generator) and development reports (generated by Core Asset Development and Product Development), which can be used for process monitoring. The Employee Pool models human resource management. It receives resource request and resource return messages, and sends out reply messages to grant resources. Currently, Employee Pool manages the resource requests in either a pure FIFO manner or a FIFO manner where new development jobs are given higher priority. Before starting any development activity, a resource request must be sent to Employee Pool. A Figure 3.3. Model hierarchical structure The Product Development models the application engineering activity. It has a Development instance for product creation and inter-product synchronization (development instance), a Development instance for inner-product maintenance (maintenance instance), and a Maintenance Requirement Generator instance. When the Product Development gets the first requirement, the development instance starts product creation, once that is done, the Maintenance Requirement Generator sends out a requirement to maintain the product for N years (the number of years in the product life cycle), which starts the maintenance instance. After N years, the Maintenance Requirement Generator sends out a stop message, which stops the maintenance activity and the acceptance of new development requirements. The Development models a general software engineering activity. When a new requirement is received, Development sends a resource request to the Employee Pool, waits for the reply from the Employee Pool, starts developing activity when the resources are granted, then returns resources to the Employee Pool and sends a report to Technical Management upon completion. The Development model will stop accepting new requirements when it receives a stop message on its stop port, which means the product reaches the end of its life cycle and needs to be phased out. The Maintenance Requirement Generator models product maintenance requirement generation. Once a new product is released, it sends out a requirement to maintain the product for N years (the number of years in

the product life cycle), and sends a stop message when the product reaches the end of the product life cycle. The Core Asset Development models domainengineering processes. Currently, it is modeled in the same way as the Development model. The domain engineering is not modeled as the same as the application engineering, because in practice technical management often collects the core asset feedback from product development and issues the core asset requirements in the context of product development.
Stage Creation

The Market Demand models the demands for new products from the market. It sends out a new product request after a certain interval, which can be set through the model parameter, "interval". The Transducer observes product line engineering activities for a certain amount of time. During the observation period, it receives development requirements and reports, and tracks the generating and finishing time of each requirement. At the end of a simulation run it will output some statistical information to a file.

Table 3.1. Behavior of technical management
Approach Big bang Activities 1. Create core assets if they do not exist 2. Create new product by fully reusing core assets 1. Increase core assets if necessary 2. Create new product by fully reusing core assets 3. Update existing products 1. Update core assets 2. Create new product by fully reusing core assets and updating existing products (excluding the newly created product) 1. Create new product by partially reusing core assets 2. Update core assets 3. Update existing products (including newly created product)

3.3. Assumptions
In the simulator, we made a number of assumptions as follows. 1. All the employees have the same capability and can work on any project. 2. If task B needs to make use of the results from task A, task B cannot start until task A is finished. 3. Product maintenance starts right after the release of the product and the maintenance activity holds the staff until the product is phased out. The assumptions made by the COPLIMO [2] basic life cycle model are: 4. All products in the product line have the same size, the same fractions of reused (black-box reuse) code, adapted (white-box reuse) code, and product unique code, and the same values for cost drivers and effort modifiers. 5. For each product in the product line, the size, the values of cost drivers, and the values of effort modifiers remain constant over time. For each product, the fractions of reused code, the fractions of adapted code, the fractions of product unique code remain constant during the time when core assets stay the same. 6. A change to a product will cause the same percentage of change to reused code, adapted code, and product unique code. Assumption 2 states that concurrency between interdependent tasks is not supported in the current version, which will be supported to some extent in the future. Assumption 4 can be relaxed by using COPLIMO [2] extended life cycle model, which allows products to have different parameter values. Assumption 5 can be relaxed by allowing users to specify the change trend. Assumption 6 can also be relaxed by allowing users to specify the change rate on different portions of the products. Because COPLIMO is currently used as the underlying cost model, its assumptions are adopted in the simulator. If another cost model is used, these assumptions would be replaced by those made by the new cost model.

Incremental

Evolution

Infrastructure -Based

Branch-andUnite

The Technical Management models requirement generation and control as well as the coordination between core asset and product development. It receives market demands (which are processed in FIFO order), generates requirements for core asset or product development, and accepts development reports. Which requirements will be generated, when will they be generated, and where they will be sent depend on the selected product line adoption and evolution approaches. Technical Management coordinates core asset development and product development activities through keeping the requirement generation in a certain order. Table 3.1 summaries the behavior of Technical Management according to the given strategies. The Experimental Frame consists of a Market Demand instance and a Transducer instance. It feeds Product Line Process with inputs and receives Product Line Process' outputs.

Figure 3.4 Simulation tool in execution

3.4. Simulation tool
The simulation tool was developed in Java and runs in the DEVSJAVA [1] environment. Figure 3.4 shows the user interface. The upper part of the interface shows the current running model and its package, which are "PLPEF" and "productLineProcess", respectively. The middle part shows the models and their hierarchical relationships. The bottom part contains execution control components. The "step" button allows running the simulator step by step, the "run" button allows executing the simulator to the end, and the "restart" button allows starting a new simulation run without quitting the system. The "clock" label displays the current simulation time in the unit of months, and selecting the "always show couplings" checkbox will allow couplings between models to be displayed. The simulation speed can be manipulated at run time to allow execution in near real-time or logical time (slower/faster than real-time).

Figure 3.4 shows that at time 27.783, Core Asset Development is idle, Products 1 is under initial development, Product 2 and 3 are waiting for resources, and Products 4 and 5 are in planning. The messages tell that Product 2 just received requested resources and Product 3 just sent out a resource request. Because of the lack of resources, the Employee Pool cannot grant the requested resources and is waiting for more resources to be returned, which in turn puts Product 3 in wait. Technical Management is idle, Market Demand generates new product requirement in every 12 months. Finally, and Transducer is observing the simulation. This case shows a situation where limited resources cause development activity delay. At the end of each simulation run, a result table is generated similar to Table 3.2. The table has two sections that are divided by a blank line. The top section lists the created products, their first release time (FR), time-to-market (TTM), initial development effort (DPM), initial development time (DT), accumulated development and maintenance effort (APM), accumulated

development and maintenance time (AT), and the number of finished requirements (FR). The bottom section summarizes the total product line evolution effort (TPM), the time when all the requirements are finished (FT), the average annual development effort (APM), the number of requirements generated (TR), and the average time-to-market (ATM). The unit of effort is personmonths and the unit of time is months.
FR 27.8 48.1 48.1 68.4 79.8 100.1

product unique code, unfamiliarity with adapted code, and average change rate caused by new market demands. To study the effect of resources, adoption approaches, and evolution approaches on software product line engineering, we ran the simulator seven times using the same basic parameter values. Accordingly, we varied the number of resources, the type of adoption approach, and type of evolution approach. Table 4.1. Scenarios
Market Demand Interval Infrastructure-based Single product only

Table 3.2. Simulation Result
TTM 27.8 48.1 36.1 44.4 43.8 52.1 DPM 582.2 217.7 217.7 217.7 217.7 217.7 DT 27.8 20.3 20.3 20.3 20.3 20.3 APM 651.6 443.0 443.0 443.0 424.2 405.4

TPM 2810.1 FT 220.1 APM 153.2 TR 20 ATTM 44.9

4. Results
In this section some simulation cases are presented to illustrate the use of the simulator and to demonstrate the analytical capability of the simulator.

1 2 3 4 5 6 7

12 12 12 12 12 12 12

50 40 30 50 50 50 50

4.2. Effect of resources
The inputs to Scenarios 1, 2 and 3 only differ in the values of number of employees (50, 40, and 30, respectively). The results differ in time-to-market, as shown in Figure 4.1. At the beginning, there is little difference, as time progresses, the gap of time-to-market between resource-constrained and non-resourceconstrained scenarios increases. The reason is that as more products are developed, more resources for product maintenance are required, thus less resources are left for new product development, which may increase the resource waiting time and in turn result in a longer timeto-market. The effort associated with Scenario 3 is smaller than the other two cases. That is because in Scenario 3, when Product 10 is released at 202.69, Product 1 and 2 are already phased out (at the time 168.1). Accordingly, no effort is needed to update those two products due to the change of the core.

4.1. Overview
The inputs to the simulator include general parameters and product (core asset) parameters. The general parameters are used to describe software product line process attributes and organization characteristics. These parameters include the maximum number of products that will be supported by the product line, the number of products that will be created during the creation stage, the product line adoption and evolution approaches, the number of employees in an organization, and the market demand intervals. The product (core asset) parameters are primarily determined by the employed cost model (COPLIMO, in this case). These parameters include the size of the product (core assets) in source lines of code, fraction of product unique code, fraction of reused code, fraction of adapted code, percentage of design modified, percentage of code modified, and percent of integration required for modified software. In addition, a number of parameters related to reuse and maintenance are included, such as software understanding of product unique code, software understanding of adapted code, unfamiliarity with

4.3. Effect of adoption approach
The inputs to Scenarios 1 and 4 only differ in product line adoption approach (big bang and incremental, respectively). The results differ in time-to-market, as shown in Figure 4.2. As specified by the inputs, the core

Branch-and-unite

Big bang

Incremental

Scenario

Resources

core p01 p02 p03 p04 p05

AT FR 50.5 3 159.0 4 159.0 4 159.0 4 149.6 3 140.3 2

100 Time-to-Market 80 60 40 20 0 1 2 3 4 5 6 7 8 9 10

Products numEmp=40

numEmp=30

numEmp=50

Figure 4.1. Effect of resources
60 50 40 30 20 10 0 Time-to-Market

1

2

3

4

Big Bang

Products

5

6

7

8

9

10

Incremental

Figure 4.2. Effect of adoption approach
100 Time-to-Market 80 60 40 20 0 1 2 3 4 5 6 7 8 9 10

Branch-and-unite

Products

Infrastructure-based

Figure 4.3. Effect of evolution approach
120 Time-to-Market 100 80 60 40 20 0

1

2 Big_Inf

3

4 Big_Bra

Products Inc_Inf

5

6

7

8 Inc_Bra

9

10 Tra

Figure 4.4. Effect of combined adoption and evolution approach

assets are developed in two steps with the incremental approach. The first increment happens right before the development of Product 1, and implements half of the core assets. The second increment happens right before the development of Product 4, and implements the rest of the core assets. For the first three products, the incremental approach appears to have shorter time-tomarket, mainly because fewer core assets means less time is required for asset development. As request for Product 4 comes, with the incremental approach, the development of the new product can not be started until the rest of the core assets have been implemented. So, we see a big jump in time-to-market from Product 3 to 4. The incremental approach results in higher total effort (6173.36) than the big band approach (5338.47). That is the nature of its process flow.

4.4. Effect of evolution approach
The inputs to Scenarios 1 and 5 differ in product line evolution approach (infrastructure-based and branch-andunite, respectively). Figure 4.3 shows the comparison of the results in time-to-market. As specified by the inputs, the evolution stage starts from Product 7. For Product 7, the branch-and-unite approach has smaller time-tomarket because the product gets developed earlier and does not have to wait for core asset updates. For the later products, the branch-and-unite approach results in longer time-to-market because it requires extra effort to rework new products and imposes more task dependencies, thus reducing concurrency. The total effort of the branchand-unite approach (5340.37) is only a slightly higher than the infrastructure-based approach (5338.47). That is because when Product 9 and 10 are released, some early products have already been phased out, so the costs for updating existing products are reduced.

the third product, then its time-to-market stays between the incremental and the big bang approaches, afterwards its time-to-market starts climbing dramatically but still stays in between the branch-and-unite and infrastructurebased approaches. In our experiment, the reuse rates are not very high (30% for both black-box and white-box reuse) and the product is relatively small (100KSLOC), so the traditional product development time is only slightly longer (about 5 months) than product line engineering approaches. In the case of branch-and-unite evolution approach, the dependencies imposed by that approach overweighs the benefits of reusing the core assets. The total effort of Scenario 1 and 4-7 are 5338.47, 5340.37, 6173.36, 6194.03, and 8760.55, respectively. As we have expected, the traditional approach requires considerably more effort. By largescale reuse, product line approaches generally result in smaller code size to development and maintain. Thus, the total effort on creating and evolving the products in a product line is smaller.

4.6. Validation of model and results
Several steps have been taken to verify and validate the model. First, the results of the simulator have been compared with the results of COCOMO II [9] to make sure the mathematic calculations are correct, and the results are the same (ignoring rounding errors). Second, the results of the simulator have been compared with the common knowledge about the product line, and we feel the results confirm to the common knowledge. Third, a initial small set of experts have reviewed the simulation results, and they feel that the results are consistent with what have been observed in the real world and the abstract model reflects the real process flow at a high level. In future investigations, we plan to continue soliciting expert feedback and compare simulation results with real product line data.

4.5. Effect of adoption and evolution approaches
A situation an organization might face is the need to determine which software development and evolution approaches best fit its goals. Scenarios 1 and 4 ­ 7 show the alternatives the organization might have. Scenario 7 is the case where a traditional software development approach (single product only) is taken, where products are created and evolved independently. Figure 4.4 shows the comparison of the results in time-to-market. As we can see, the big bang with infrastructure-based approach has the shortest average time-to-market, and the incremental with branch-andunite approach has the longest average time-to-market. The traditional approach has the shortest time-to-market for the first two products, the longest time-to-market on

5. Related work
Cohen [7] presents an approach for making a software product line investment determination. The approach uses three factors to justify software product line investment: applications, benefits, and costs. Applications include the number of projected products in the product line, the time they will be developed, and their annual change traffic; benefits consists of the tangible and intangible goals the organization wishes to achieve through a product line approach; costs are the life cycle costs associated with core assets and individual products. Costs are affected by some factors, such as

costs of reuse, degree of reuse, and core assets change rate. Our cost estimation method is consistent with the Cohen approach but provides more capabilities. Regnell et al. use a simulator to study a specific market-driven requirement management process [5]. The goal of simulator is to help in exploring bottleneck and overload situations in the requirement engineering process, investigating which resources are needed to handle a certain frequency of new requirements, and analyzing process improvement proposals. The specific process is modeled using queuing network and discrete event simulation [6]. Our simulator also uses discrete event simulation, but its purpose is to study life cycle issues for a product family instead of a portion of a software engineering process for a single product. Riva and Delrosso recently discussed issues related to software product family evolution [11]. They state that a product family typically evolves from a copy-and-paste approach to a mature software platform. They point out some issues that harm the family evolution, such as organization bureaucracy, dependencies among tasks, slower process of change, and the new requirements that can break the architectural integrity. Their notion of product family used in that paper is different from the definition of a product line [8]. Creating a product family by copy-and-paste is not a product line approach, because the product line approach emphasizes a disciplined strategic reuse, not opportunistic reuse. A product line is actually a product family that has already evolved to a mature software platform. Our simulation results also show that in some cases dependencies imposed by product line approaches result in slower market response than the traditional software engineering approach.

incremental product line adoption approaches and infrastructure-based or branch-and-unite product line evolution strategies. Our future investigations include providing estimates for other software product line initiation situations and approaches, allowing concurrency between inter-dependent tasks to some extent, providing probabilistic demand intervals, incorporating other cost models, and removing a number of the simplification assumptions. Furthermore, we plan to validate the model by comparing the results with real product line data and getting more expert feedback. Also, we want to combine the simulator with an optimization model, so users can specify their end-goal criteria and then allow the simulator to search for the best results.

7. References
[1] B.P. Zeigler and H.S. Sarjoughian, "Introduction to DEVS Modeling & Simulation with JAVA(TM): Developing Component-based Simulation Models", 2003, http://www.acims.arizona.edu/SOFTWARE/software.shtml. [2] B. Boehm, A.W. Brown, R. Madachy, and Y. Yang, "A Software Product Line Life Cycle Cost Estimation Model", USC, June 2003 [3] M. I. Kellner, R. J. Madachy, and D. M. Raffo, "Software Process Modeling and Simulation: Why, What, How", The Journal of Systems and Software, April 1999, pp. 91-105. [4] M. Paulk, et al., "Key Practices of the Capability Maturity Model", Version 1.1, Tech. Rept. CMU/SEI-93-TR-25, Software Engineering Institute, Feb 1993. [5] M. Höst, B. Regnell, et al, "Exploring Bottlenecks in Market-Driven Requirements Management Processes with Discrete Event Simulation", The Journal of Systems and Software, Dec 2001, pp. 323-332. [6] J. Banks, J.S. Carson, and B.L. Nelson, Discrete-Event System Simulation, 2nd Ed., Prentice Hall, Aug 2000. [7] S. Cohen, "Predicting When Product Line Investment Pays", Proceedings of the Second International Workshop on Software Product Lines: Economics, Architectures, and Implications, Toronto Canada, 2001, pp. 15--18. [8] P. Clements and L.M. Northrop, Software Product Lines -Practices and Patterns, Addison-Wesley, Aug 2001 [9] B. Boehm, B. Clark, E. Horowitz, C. Westland, R. Madachy, and R. Selby, "Cost Models for Future Software Life Cycle Processes: COCOMO 2.0," Annals of Software Engineering Special Volume on Software Process and Product Measurement, Science Publishers, Amsterdam, The Netherlands, 1995, pp. 45 - 60. [10] K. Schmidt and M. Verlage, "The Economic Impact of Product Line Adoption and Evolution", IEEE Software, Jul/Aug 2002, pp. 50-57. [11] C. Riva and C.D. Rosso, "Experiences with Software Product Family Evolution", Proceedings of International Workshop on Principles of Software Evolution, Helsinki Finland, Sep 2003, pp. 161-169.

6. Conclusions and future investigations
Software product line engineering promises of reduced cost while still supporting differentiation makes adoption and continued use of the associated approaches attractive. However, in order to make appropriate planning, decision tools are necessary. In this paper, we described a simulator that is intended to support early stage decision-making. The simulator provides both static and dynamic information for the selected software product line engineering process. The statistical result generated at the end of the simulation can be used for trade-off analysis. Stepping through the simulator helps analyzing product line processes, uncovering problems, and improving the understanding of software product line evolution. Currently the simulation tool supports the study of independent product line initiation using big bang or

Variable Strength Interaction Testing of Components
Myra B. Cohen Peter B. Gibbons Warwick B. Mugridge Dept. of Computer Science University of Auckland Private Bag 92019 Auckland, New Zealand   myra,peter-g,rick ¡ @cs.auckland.ac.nz Charles J. Colbourn James S. Collofello Dept of Computer Science and Engineering Arizona State University P.O. Box 875406 Tempe, Arizona 85287   charles.colbourn,collofello ¡ @asu.edu

Abstract
Complete interaction testing of components is too costly in all but the smallest systems. Yet component interactions are likely to cause unexpected faults. Recently, design of experiment techniques have been applied to software testing to guarantee a minimum coverage of all ¢ -way interactions across components. However, ¢ is always fixed. This paper examines the need to vary the size of ¢ in an individual test suite and defines a new object, the variable strength covering array, that has this property. We present some computational methods to find variable strength arrays and provide initial bounds for a group of these objects.

1. Introduction
In order to shorten development times, reduce costs and improve quality, many organizations are developing software utilizing existing components. These may be commercial off-the-shelf (COTS) or internally developed for reuse among products. The utilization of existing components requires new development and verification processes [2]. In particular the interaction of these new components with each other as well as with the newly developed components within the application must be tested. Software is becoming increasingly complex in terms of components and their interactions. Traditional methods of testing are useful when searching for errors caused by unmatched requirements. However, component based development creates additional challenges for integration testing. The problem space grows rapidly when searching for unexpected interactions. Suppose we have five components, each with four possible configurations. We have £¥¤§¦©¨¥£ potential interactions. If we combine 100 com-

£ ponents there are possible combinations. This makes testing all combinations of interactions infeasible in all but the smallest of systems. Instead one can create test suites that guarantee pairwise or ¢ -wise coverage. For instance we can cover all pairwise interactions for ten components, each with four possible configurations, only "!$#&%(' 0)1324 using%6 5 ) 25 test cases. A covering %65 array, is an array such that every ¢ ¢ sub2 array contains all ordered subsets from symbols of size ¢ at % least once. The covering array number 7)832 is the minimum required to satisfy the parameters ¢ . Covering arrays have been used for software interaction testing by D. Cohen et al. in the Automatic Efficient Test Generator (AETG) [5]. Williams et al. use these to design tests for the interactions of nodes in a network [12]. Dalal et al. present empirical results suggesting that testing of all pairwise interactions in a software system indeed finds a large percentage of existing faults [7]. In further work, Burr et al. provide more empirical results to show that this type of test coverage is effective [3]. In a software test, the columns of the covering array rep) 2 resent the components or fields. Each component %95 has ) levels or configurations. The final test suite is an ar% ray where is the number of test cases and each test contains one configuration from each component. By mapping a software test problem to a covering array of strength ¢ we can guarantee that we have tested all ¢ -way interactions. In many situations pairwise coverage is sufficient for testing. However, we must balance the need for stronger interaction testing "!$ #&%(' @7with A£4 the cost of running tests. For instance a "!B#C%('7D 0can A3£E4 be achieved for as little as 16 tests, while a requires at least 64 tests. In order to appropriately use our resources we want to focus our testing where it has the most potential value. The recognition that all software does not need to be

tested equally is captured in the concept of risk-based testing [1]. Risk-based testing prioritizes testing based on the probability of a failure occurring and the consequences should the failure occur. High risk areas of the software are identified and targeted for more comprehensive testing. The following scenarios point to the need for a more flexible way of examining interaction coverage.
 

RAID Level RAID 0 RAID 1 RAID 5

Component Operating Memory System Config Windows XP 64 MB Linux 128 MB Novell Netware 6.x 256 MB

Disk Interface Ultra-320 SCSI Ultra-160 SCSI Ultra-160 SATA

We completely test a system, and find a number of components with pairwise interaction faults. We believe this may be caused by a bad interaction at a higher strength, i.e. some triples or quadruples of a group of components. We may want to revise our testing to handle the "observed bad components" at a higher strength.
 

Table 1. Raid integrated controller system: 4 components, each with 3 configurations

We thoroughly test another system but have now revised some parts of it. We want to test the whole system with a focus on the components involved in the changes. We use higher strength testing on certain components without ignoring the rest.
 

We have computed software complexity metrics on some code, and find that certain components are more complex. These warrant more comprehensive testing.
 

are more likely to be interaction problems between three components: RAID level, OS and memory. We want to test these interactions more thoroughly. But it may be too expensive to run tests involving all three way interactions among components. In this instance we can use three-way interaction testing among the first three components while maintaining two-way coverage for the rest. We still have a minimal coverage guarantee across the components and we still do not need to run 81 tests. The test suite shown in Table 2 provides this level of variable strength coverage with 27 tests.
Component Operating Memory System Config Linux 128 MB Novell 128 MB Linux 64 MB XP 128 MB Novell 256 MB XP 128 MB Novell 256 MB Linux 64 MB XP 256 MB XP 64 MB Novell 128 MB XP 128 MB Linux 256 MB XP 64 MB XP 256 MB Linux 128 MB Novell 64 MB Novell 64 MB Linux 64 MB Novell 64 MB XP 256 MB Novell 128 MB XP 64 MB Linux 256 MB Linux 128 MB Novell 256 MB Linux 256 MB

We have certain components that come from automatic code generators and have been more/less thoroughly tested than the human generated code.
 

One part of a project has been outsourced and needs more complete testing.
 

Some of our components are more expensive to test or to change between configurations. We still want to test for interactions, but cannot afford to test more than pairwise interactions for this group of components.

While the goal of testing is to cover as many component interactions as possible, trade-offs must occur. This paper examines one method for handling variable interaction strengths while still providing a base level of coverage. We define the variable strength covering array, provide some initial bounds for these objects and outline a computational method for creating them.

2. Background
Suppose we are testing new integrated RAID controller software. We have four components, (RAID level, operating system (OS), memory configuration and disk interface). Each one of these components has three possible configurations. We need 81 tests to test all interactions. Instead we can test all pairwise interactions of these components with only nine tests. Perhaps though, we know that there

RAID Level RAID 0 RAID 0 RAID 5 RAID 1 RAID 0 RAID 0 RAID 1 RAID 0 RAID 5 RAID 0 RAID 5 RAID 5 RAID 1 RAID 5 RAID 0 RAID 1 RAID 0 RAID 1 RAID 1 RAID 5 RAID 1 RAID 1 RAID 1 RAID 5 RAID 5 RAID 5 RAID 0

Disk Interface Ultra 160-SCSI Ultra 160-SCSI Ultra 160-SCSI Ultra 160-SCSI Ultra 320 Ultra 160-SCSI Ultra 160-SCSI Ultra 320 Ultra 320 Ultra 160-SATA Ultra 320 Ultra 160-SATA Ultra 160-SCSI Ultra 160-SATA Ultra 160-SATA Ultra 320 Ultra 320 Ultra 160-SATA Ultra 160-SCSI Ultra 160-SCSI Ultra 160-SATA Ultra 160-SCSI Ultra 160-SATA Ultra 320 Ultra 320 Ultra 160-SCSI Ultra 160-SATA

Table 2. Variable strength array for Table 1

Commercial software test generators, like AETG, provide only a fixed level of interaction strength [5]. We might use this to build two separate test suites and run each independently, but this is a more expensive operation and does not really satisfy the desired criteria. We could instead just default to the higher strength coverage with more tests. However, the ability to tune a test suite for specific levels of coverage is highly desirable, especially as the number of components and levels increases. Therefore it is useful to define and create test suites with flexible strengths of interaction coverage and to examine some methods for building these.

3. Definitions
"!$#&%(' 0)1324 In a covering array, ¢ , ¢ is called the ) 2 strength, the degree and the order. A covering array is optimal if it contains the minimum possible number of rows. We minimum number"! the% covering "! call % # this # @7A D 4 array 0)1324 ¦ ¨¥¨ number, . For example, ¢ [4]. A mixed level covering array, denoted a % 5 as   "!$#C% ' 7)8 # 2 32¢¡ ¤£¥£¦£ 32¨§¥434 ) 2  § , is an array on sym¢ 2 ¦© 2 ¦   , with the following properties: bols, where # ¨  )@4 1. Each column  contains only elements  ¦ 2  from a set   with     . % 5 2. The rows of each ¢ sub-array cover all ¢ tuples of values from the ¢ columns at least once.
We can use a shorthand notation to describe our mixed cov2 ering array by combining  's that are the same. For exam2  ! §%$ §' & 's of size two we can write this . ple if we have three " ¡ £¥£¦£ " §( 434   "!$#&%('  ##"  ) Consider an . This can also ¢   "!$#&%(' 0)1 # 2 32¢¡¥¤£¥£¦£ 32¨§ 44  § be written as an where ¢ ) ¦ © ) )  2 ¦ © ) ) "  ¦ © 2 ¥  ¥  ¥   . and The following holds:  ¡ ¤£¥£¥£ ) 1. The columns are partitioned into 0 groups 1  1 )  )1  where group 1 contains columns. The first  ) ¡ columns belong to the group 1  , the next columns ¡ belong to group 1 , and so on. ¦ "  2. If column 24351  , then  768 .
§ We can now"! use this notation for a fixed-level covering B#C%( ' 3 2 4 ) array as well. indicates that there are pa¢ 2 rameters each containing a set of symbols. This makes it easier to see that the values from different components can come from different symbol sets. A variable strength covering array , A denoted as a % 5B 9 "!$#&%('  # 2 32 ¡ ¤£¥£ 2¢@"4  4  ¢ , is an mixed level covering array, of strength ¢ containing , a multi-set of disjoint mixed level covering arrays each of strength C ¢ .

  "! We can abbreviate duplicate 's in the multi-set us¡ ing a similar notation to that of the covering arrays. "!$ #C% 'D  D Sup4 pose we have two identical "! covering arrays in ¡ ¡ B#C%('7D  D 4 . This can be written as . Ordering of the 9 "! columns in the representation of a is important since the columns of the covering arrays in are listed consecu¡ tively from left to right. 9 "!$# !D '  D!E  %F "!$# 8D '7D  D 4G¨H 4 An example of a can be seen in Table 3. The overall array is a mixed level array of strength two with nine columns containing three symbols and two columns containing two. There are three sub-arrays each with strength three. All three way interactions therefore among columns 0-2, 3-5, 6-8 are included. All two way interactions among all columns are also covered. This has been "!B#Cachieved %('7D  D 4 with 27 rows which is the optimal size for a . A covering array that would cover all three way interactions for all 11 columns, on the other hand, might need as many as 52 rows.

4. Construction Methods
Fixed strength covering arrays can be built using algebraic constructions if we have certain parameter combina2 0) tions of ¢ and [4, 12]. Alternately we may use computational search algorithms [5, 6, 11]. Greedy algorithms form the basis of the AETG and the IPO generators [5, 11]. It has also been shown that simulated annealing is effective for building covering arrays of both mixed and fixed levels [6, 10]. Since there are no known constructions for variable strength arrays at the current time we have chosen to use a computational search technique to build these. We have written a simulated annealing program that has been used to produce all of the results presented in this paper.

4.1. Simulated Annealing
Simulated annealing is a variant of the state space search technique for solving combinatorial optimization problems. The hope is that the algorithm finds close to an optimal solution. Most of the time, however, we do not know when we have reached an optimal solution for the covering array problem. Instead such a problem can be specified as #a set I 4 of feasible solutions (or states) together with a cost P  associated with each feasible solution  . An optimal solution corresponds to a feasible solution with overall (i.e. global) minimum cost. For each feasible solution Q3RI , we define a set SUT of transformations (or transitions), each of which can be used to change  into another feasible solution WV . The set of solutions that can be reached from  by applying % # 4  a transformation from SXT is called the neighborhood of  . To start, we randomly choose an initial feasible solution. The algorithm then generates a set of sequences (or

0 0 1 2 2 2 1 0 2 1 1 0 2 0 1 1 2 0 2 0 1 2 0 1 0 1 2

2 1 2 1 2 0 0 0 0 2 0 1 2 0 2 0 1 2 0 1 1 1 0 1 2 1 2

Table 3.

D E  ¡ ¤F "!$# !D  ' D  D 4 H4 9 "!B# !D ' @ ¨

2 2 2 0 0 0 0 0 2 0 2 0 1 2 1 1 2 1 1 1 2 1 1 1 0 0 2

0 1 2 0 2 0 1 0 2 0 2 0 2 2 2 1 2 1 1 1 0 0 1 1 1 2 0

1 2 1 0 2 2 1 2 0 1 1 0 1 2 0 0 0 2 0 0 1 0 1 1 2 2 2

2 0 0 2 0 2 2 1 0 0 2 0 1 1 2 1 1 1 0 2 1 1 0 1 2 2 0

2 1 2 2 1 2 1 0 2 0 1 1 0 1 0 1 0 2 0 1 0 2 2 2 0 1 0

2 1 1 0 2 2 2 0 0 2 1 1 0 2 1 0 2 2 0 0 1 1 1 0 2 0 1

2 0 0 0 1 1 0 1 1 1 2 1 2 2 1 1 0 0 0 2 0 2 1 2 2 0 2

0 0 0 0 1 1 1 1 0 0 1 1 0 0 0 0 1 1 0 0 0 0 1 1 1 0 1

1 0 0 1 0 0 0 1 1 1 0 1 1 0 0 1 1 1 1 0 0 0 1 0 0 1 1

solution is an approximation  to a covering array in which certain ¢ -subsets are not covered. The cost function is based on the number of ¢ -subsets that are not currently covered.  A covering array itself will have cost . In the variable  strength array, the cost is when (i) all of the ¢ -subsets are covered and (ii) for each covering array of strength ¢ V in , all ¢GV -subsets are covered. A potential transition is made A by selecting one of the -setsA belonging to  and then replacing A a random point in this -set by a random point not in the -set. We calculate only the change in numbers of ¢ -sets that will occur with this move. Since the subsets of are disjoint, any change we make can affect the overall 9 "! coverage of the and at most one of the subsets of . This means that the extra work required for finding variable strength arrays over fixed strength arrays does not grow in proportion to the number of subsets of higher strength. We keep the number of blocks (test cases) constant throughout a simulated annealing run and use the method described by Stardom to determine a final array size [10]. We start with a large random array and then bisect our array repeatedly until we find a best solution.

4.2. Program Parameters
Good data structures are required to enable the relative cost of the new feasible solution to be calculated efficiently, and the transition (if accepted) to be made quickly. We build an exponentiation table prior to the start of the program. This allows us to approximate the transition probability value using a table lookup. We use ranking algorithms from [8] to hold the values of our ¢ -sets. This allows us to generalize our algorithms for different strengths without changing the base data structure. When calculating the change in cost for each transition, we do not need to recalculate all of the ¢ -sets in a test case, but instead only calculate the change (¢ -1 subsets). A constant is set to determine when our program is frozen. This is the number of consecutive trials allowed where no change in the cost of the solution has occurred. For most of our trials this constant has been set to 1,000. The cooling schedule is very important in simulated annealing. If we cool too quickly, we freeze too early because the probability of allowing a worse solution drops too quickly. If we cool too slowly or start at too high a temperature, we allow too many poor moves and fail to make progress Therefore, if we start at a low temperature and cool slowly we can maintain a small probability of a bad move for a long time allowing us to avoid a frozen state, at the same time continuing to make progress. We have experimented using fixed strength arrays compared with known algebraic constructions (see [6]). We have found that a starting temperature of approximately 0.20 and a slow cooling

# 4  # 4 Markov chains) of trials. If P  V P  then the transition is accepted. If the transition results in a feasible solution  V of higher cost, then  V is accepted with probability ¡£¢¥¤§¦¨¤ T© ¢¦¨¤ T , where S is the controlling temperature of the simulation. The temperature is lowered in small steps with the system being allowed to approach "equilibrium" at each temperature through a sequence of trials at this tem¦"! perature. Usually this is done by setting S  S , where ! (the control decrement) is a real number slightly less than ¨ . The idea of allowing a move to a worse solution helps avoid being stuck in a bad configuration (a local optimum), while continuing to make progress. Sometimes we know the cost of an optimal solution and can stop the algorithm when this is reached. Otherwise we stop the algorithm when insufficient progress is being made (as determined by an appropriately defined stopping condition). In this case the algorithm is said to be frozen. Simulated annealing has been used by Nurmela and ¨ Osterg° ard [9], for example, to construct covering designs which have a structure very similar to covering arrays. It has also been used by Stardom and Cohen, et al. to generate minimal fixed strength covering arrays for software testing [6, 10]. In the simulated annealing algorithm the current feasible

VCA

¢¤£¦¥¨§©   £ ¡ 

  C¡ !£¦¥"§#%$ £¦¥"§# $ & £¦¥"§#%$  $ £¦¥"§#%'  £¦¥"§#   £¦¥"§# '  , £¦¥"§#   , £¦¥"§#%(  £¦¥"§# (  £¦¥"§#0)  £¦¥"§#%1  £¦¥"§#  

Min N 16 27 27 27 27 33

Max N 17 27 27 27 27 33

Avg N 16.1 27 27 27 27 33

¢¤£¦¥¨§©2 $%3%$%4 &    £ ¡ 

¢¤£¦¥¨§© &879A@0&    £ ¡ 

!£¦¥"§#2 $ 56£¦¥"§#2 $ 3 £¦¥"§# 3 $  £¦¥"§#2 $  , £¦¥"§# 3 $  56£¦¥"§#20$ 3 56£¦¥"§# 3  4 56£¦¥"§#20$ 3

& $4 $&%4 & 

33* 34 33* 34 41 50 67 36 64 100 125 125 171 180 214 100 100 304

35 35 42 51 69 36 64 104 125 125 173 180 216 100 100 318

34.8 34.9 41.4 50.8 67.6 36 64 101 125 125 172.5 180 215 100 100 308.5

!£¦¥"§# 56£¦¥"§#&870   &879A@0& 

in our notation for covering arrays in this table due to space limitations.)

Table 4. Table of sizes for variable strength arrays after 10 runs (We have omitted the parameter B

* The minimum values for these VCA's were found during a separate set of experiments

! factor, , of between 0.9998 and 0.99999 every 2,500 iterations works well. Using these parameters, the annealing algorithm completes in a "reasonable" computational time on a PIII 1.3GHz processor running Linux. For instance, 9 "! the first few 's in Table 4, complete in seconds, while 9 "! the larger problems, such as the last in Table 4, complete within a few hours. The delta in our cost function is counted as the change in ¢ -sets from our current solution. Since we can at any point make changes to both the base array and one of the higher strength arrays, these changes are added together. As there is randomness inherent in this algorithm, we run the algorithm multiple times for any given problem.

a different random seed. A starting temperature of .20 and a decrement parameter of .9998 is used in all cases. In two cases a smaller sized array was found during the course of our overall investigation, but was not found during one of these runs. The numbers are included in the table as well and are labeled with an asterisk, since these provide a previously unknown bound for their particular arrays. In each case we show the number of tests required for the base array of strength two. We then provide some examples with variations on the contents of . Finally we show the arrays with all of the columns involved in strength three coverage. We have only shown examples using strength two and three, but our methods should generalize for any strength ¢ . What is interesting in Table 4 is that the higher strength sub-arrays often drive the size of the final test suite. Such 9 "! is the case in the first and second groups in this table. We can use this information to make decisions about how many components can be tested at higher strengths. Since we must balance the strength of testing with the final size of the test suite we can use this information in the design process. Of course there are cases where the higher strength subsets do not determine the final test suite size since the number of test cases required is a combination of the number 9 "! of levels and the strength. In the last group in Table 4 the two components each with 10 levels require a minimum of 100 test cases to cover all pairs. In this case we can cover all of the triples from the 20 preceding columns with the same number of tests. In such cases, the quality of the tests can be improved without increasing the number of test cases. We can set the strength of the 20 preceding columns to the highest level that is possible without increasing the test count. Both situations are similar in the fact that they allow us to predict a minimum size test suite based on the fixed level sub-arrays. Since there are better known bounds for fixed strength arrays we can use this information to drive our decision making processes in creating test suites that are both manageable in size while providing the highest possible interaction strengths.

6. Conclusions
We have presented a combinatorial object, the variable strength covering array, which can be used to define software component interaction tests and have discussed one computational method to produce them. We have presented some initial results with sizes for a group of these objects. These arrays allow one to guarantee a minimum strength of overall coverage while varying the strength among disjoint subsets of components. Although we present these objects for their usefulness in testing component based software systems they may be of use in other disciplines that

5. Results
Table 4 gives the minimum, maximum and average sizes obtained after 10 runs of the"! simulated annealing algorithm 9 for each of the associated 's. Each of the 10 runs uses

currently employ fixed strength covering arrays. The constraining factor in the final size of the test suite may be the higher strength sub-array. We can often get a second level of coverage for almost no extra cost. We see the potential to use these when there is a need for higher strength, but we cannot afford to create an entire array of higher strength due to cost limitations. Where the constraining factor is the large number of levels in a set of fields at lower strength, it may be possible to increase the strength of sub-arrays without additional cost, improving the overall quality of the tests. Another method of constructing fixed strength covering arrays is to combine smaller arrays or related objects and to fill the uncovered ¢ -sets to complete the desired array [4]. We are currently experimenting with some of these techniques to build variable strength arrays. Since the size of a 9 "! may be dependent on the higher strength arrays, we believe that building these in isolation followed by annealing or other processes to fill in the missing lower strength ¢ -sets will provide fast and efficient methods to create optimal variable strength arrays.

[4] M. Chateauneuf and D. Kreher. On the state of strength-three covering arrays. Journal of Combinatorial Designs, 10(4):217­238, 2002 [5] D. M. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG system: an approach to testing based on combinatorial design. IEEE Transactions on Software Engineering, 23(7):437­ 44, 1997. [6] M. B. Cohen, C. J. Colbourn, P. B. Gibbons and W. B. Mugridge. Constructing test suites for interaction testing. In Proc. of the Intl. Conf. on Sofware Engineering (ICSE 2003), 2003, pp. 3848 , Portland. [7] S. R. Dalal, A. J. N. Karunanithi, J. M. L. Leaton, G. C. P. Patton, and B. M. Horowitz. Model-based testing in practice. In Proc. of the Intl. Conf. on Software Engineering,(ICSE '99), 1999, pp. 28594, New York. [8] D. L. Kreher and D. R. Stinson. Combinatorial Algorithms, Generation, Enumeration and Search. CRC Press, Boca Raton, 1999. ¨ [9] K. Nurmela and P. R. J. Osterg° ard. Constructing covering designs by simulated annealing. Technical report, Digital Systems Laboratory, Helsinki Univ. of Technology, 1993. [10] J. Stardom. Metaheuristics and the search for covering and packing arrays. Master's thesis, Simon Fraser University, 2001. [11] K. C. Tai and L. Yu. A test generation strategy for pairwise testing. IEEE Transactions on Software Engineering, 28(1):109-111, 2002. [12] A. W. Williams. Determination of test configurations for pair-wise interaction coverage In Proc. Thirteenth Int. Conf. Testing Communication Systems, 2000, pp. 57­74.

Acknowledgments
Research is supported by the Consortium for Embedded and Internetworking Technologies and by ARO grant DAAD 19-1-01-0406. Thanks to the Consortium for Embedded and Internetworking Technologies for making a visit to ASU possible.

References
[1] J. Bach. James Bach on risk-based testing In STQE Magazine, Nov/Dec,1999. [2] L. Brownsword, T. Oberndorf and C. Sledge. Developing new processes for COTS-based systems. IEEE Software, 17(4):48­55, 2000. [3] K. Burr and W. Young. Combinatorial test techniques: Table-based automation, test generation and code coverage. In Proc. of the Intl. Conf. on Software Testing Analysis & Review, 1998, San Diego.

