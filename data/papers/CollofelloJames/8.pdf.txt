Variable Strength Interaction Testing of Components
Myra B. Cohen
Peter B. Gibbons
Warwick B. Mugridge
Dept. of Computer Science
University of Auckland
Private Bag 92019
Auckland, New Zealand
myra,peter-g,rick  @cs.auckland.ac.nz

Abstract
Complete interaction testing of components is too costly
in all but the smallest systems. Yet component interactions
are likely to cause unexpected faults. Recently, design of experiment techniques have been applied to software testing
to guarantee a minimum coverage of all  -way interactions
across components. However,  is always fixed. This paper
examines the need to vary the size of  in an individual test
suite and defines a new object, the variable strength covering array, that has this property. We present some computational methods to find variable strength arrays and provide
initial bounds for a group of these objects.

1. Introduction
In order to shorten development times, reduce costs and
improve quality, many organizations are developing software utilizing existing components. These may be commercial off-the-shelf (COTS) or internally developed for
reuse among products. The utilization of existing components requires new development and verification processes
[2]. In particular the interaction of these new components
with each other as well as with the newly developed components within the application must be tested.
Software is becoming increasingly complex in terms of
components and their interactions. Traditional methods of
testing are useful when searching for errors caused by unmatched requirements. However, component based development creates additional challenges for integration testing. The problem space grows rapidly when searching
for unexpected interactions. Suppose we have five components, each with four possible configurations. We have
	

potential interactions. If we combine 100 com-

Charles J. Colbourn
James S. Collofello
Dept of Computer Science and Engineering
Arizona State University
P.O. Box 875406
Tempe, Arizona 85287
charles.colbourn,collofello  @asu.edu


ponents there are
possible combinations. This makes
testing all combinations of interactions infeasible in all but
the smallest of systems.
Instead one can create test suites that guarantee pairwise
or  -wise coverage. For instance we can cover all pairwise
interactions for ten components, each with four possible
configurations,
only
 

!" using$
#  25 test cases. A covering
$# array,
is an
array such that every

 subarray contains all ordered subsets from symbols of size 
at
 least once. The covering array number

%&
! is the minimum
required to satisfy the parameters 
.
Covering arrays have been used for software interaction
testing by D. Cohen et al. in the Automatic Efficient Test
Generator (AETG) [5]. Williams et al. use these to design
tests for the interactions of nodes in a network [12]. Dalal
et al. present empirical results suggesting that testing of
all pairwise interactions in a software system indeed finds a
large percentage of existing faults [7]. In further work, Burr
et al. provide more empirical results to show that this type
of test coverage is effective [3].
In a software
test, the columns of the covering array rep
resent the components or fields. Each component
'# has

levels or configurations.
The
final
test
suite
is
an
ar
ray where is the number of test cases and each test contains one configuration from each component. By mapping
a software test problem to a covering array of strength  we
can guarantee that we have tested all  -way interactions.
In many situations pairwise coverage is sufficient for
testing. However, we must balance the need for stronger
interaction
testing

 (
%with
)
" the cost of running tests. For instance a
*+%, 
can
)
!-" be achieved for as little as 16
tests, while a
requires at least 64 tests. In
order to appropriately use our resources we want to focus
our testing where it has the most potential value.
The recognition that all software does not need to be

tested equally is captured in the concept of risk-based testing [1]. Risk-based testing prioritizes testing based on
the probability of a failure occurring and the consequences
should the failure occur. High risk areas of the software are
identified and targeted for more comprehensive testing.
The following scenarios point to the need for a more flexible way of examining interaction coverage.
We completely test a system, and find a number of
components with pairwise interaction faults. We believe this may be caused by a bad interaction at a higher
strength, i.e. some triples or quadruples of a group
of components. We may want to revise our testing
to handle the ‚Äúobserved bad components‚Äù at a higher
strength.
We thoroughly test another system but have now revised some parts of it. We want to test the whole system with a focus on the components involved in the
changes. We use higher strength testing on certain
components without ignoring the rest.
We have computed software complexity metrics on
some code, and find that certain components are more
complex. These warrant more comprehensive testing.
We have certain components that come from automatic
code generators and have been more/less thoroughly
tested than the human generated code.
One part of a project has been outsourced and needs
more complete testing.
Some of our components are more expensive to test
or to change between configurations. We still want to
test for interactions, but cannot afford to test more than
pairwise interactions for this group of components.
While the goal of testing is to cover as many component interactions as possible, trade-offs must occur. This
paper examines one method for handling variable interaction strengths while still providing a base level of coverage. We define the variable strength covering array, provide
some initial bounds for these objects and outline a computational method for creating them.

2. Background
Suppose we are testing new integrated RAID controller
software. We have four components, (RAID level, operating system (OS), memory configuration and disk interface).
Each one of these components has three possible configurations. We need 81 tests to test all interactions. Instead
we can test all pairwise interactions of these components
with only nine tests. Perhaps though, we know that there

RAID
Level
RAID 0
RAID 1
RAID 5

Component
Operating
Memory
System
Config
Windows XP
64 MB
Linux
128 MB
Novell Netware 6.x
256 MB

Disk
Interface
Ultra-320 SCSI
Ultra-160 SCSI
Ultra-160 SATA

Table 1. Raid integrated controller system: 4
components, each with 3 configurations

are more likely to be interaction problems between three
components: RAID level, OS and memory. We want to
test these interactions more thoroughly. But it may be too
expensive to run tests involving all three way interactions
among components. In this instance we can use three-way
interaction testing among the first three components while
maintaining two-way coverage for the rest. We still have a
minimal coverage guarantee across the components and we
still do not need to run 81 tests. The test suite shown in Table 2 provides this level of variable strength coverage with
27 tests.

RAID
Level
RAID 0
RAID 0
RAID 5
RAID 1
RAID 0
RAID 0
RAID 1
RAID 0
RAID 5
RAID 0
RAID 5
RAID 5
RAID 1
RAID 5
RAID 0
RAID 1
RAID 0
RAID 1
RAID 1
RAID 5
RAID 1
RAID 1
RAID 1
RAID 5
RAID 5
RAID 5
RAID 0

Component
Operating Memory
System
Config
Linux
128 MB
Novell
128 MB
Linux
64 MB
XP
128 MB
Novell
256 MB
XP
128 MB
Novell
256 MB
Linux
64 MB
XP
256 MB
XP
64 MB
Novell
128 MB
XP
128 MB
Linux
256 MB
XP
64 MB
XP
256 MB
Linux
128 MB
Novell
64 MB
Novell
64 MB
Linux
64 MB
Novell
64 MB
XP
256 MB
Novell
128 MB
XP
64 MB
Linux
256 MB
Linux
128 MB
Novell
256 MB
Linux
256 MB

Disk
Interface
Ultra 160-SCSI
Ultra 160-SCSI
Ultra 160-SCSI
Ultra 160-SCSI
Ultra 320
Ultra 160-SCSI
Ultra 160-SCSI
Ultra 320
Ultra 320
Ultra 160-SATA
Ultra 320
Ultra 160-SATA
Ultra 160-SCSI
Ultra 160-SATA
Ultra 160-SATA
Ultra 320
Ultra 320
Ultra 160-SATA
Ultra 160-SCSI
Ultra 160-SCSI
Ultra 160-SATA
Ultra 160-SCSI
Ultra 160-SATA
Ultra 320
Ultra 320
Ultra 160-SCSI
Ultra 160-SATA

Table 2. Variable strength array for Table 1

Commercial software test generators, like AETG, provide only a fixed level of interaction strength [5]. We might
use this to build two separate test suites and run each independently, but this is a more expensive operation and
does not really satisfy the desired criteria. We could instead
just default to the higher strength coverage with more tests.
However, the ability to tune a test suite for specific levels
of coverage is highly desirable, especially as the number of
components and levels increases. Therefore it is useful to
define and create test suites with flexible strengths of interaction coverage and to examine some methods for building
these.

3. Definitions
 

!"
In a covering array,

,  is called the

strength, the degree and the order. A covering array
is optimal if it contains the minimum possible number of
rows. We
minimum numberthe
 call
  this
  covering
 (
%)
 , " array


!"
 
number,
. For example,

[4].
A
mixed
covering array, denoted
a

+  
%level
 # as
&
   
! 
 
!"!"


, is an
array on sym

	
   , with the following properties:
bols, where
 
 ("
1. Each column 
contains only elements


.
from a set   with    
 #
2. The rows of each
 sub-array cover all  tuples
of values from the  columns at least once.
We can use a shorthand notation to describe our mixed covering array by combining  ‚Äôs that are the same. For exam

 two

we can write this .
ple if we have three
‚Äôs 
 of
 size
     "!"
Consider an
can also
  
 
  
!
  
!. "This

"


be written as an
where

  	   
 	      	
 
 
   .
and
The following holds:

  


1. The columns are partitioned into  groups   
 


where group  contains
columns. The first 
 
columns belong to the group   , the next
columns

belong to group  , and so on.
  
2. If column "!#  , then  %$&
.

We can nowuse
this
for a fixed-level covering
*+
 
!notation
"

array as well.
indicates that there are pa
rameters each containing a set of symbols. This makes it
easier to see that the values from different components can
come from different symbol sets.
A variable strength covering array,
 #*) denoted as a
'  
  
!  
 
(" 
  "


, is an
mixed level

covering array, of strength  containing , a multi-set of
disjoint mixed level covering arrays each of strength +  .


We can abbreviate duplicate
‚Äôs in the multi-set us
ing a similar notation to that of the covering
arrays.
+ , 
 , Sup"
pose
we
have
two
identical
covering
arrays
in



*+%, 
 , "
. This can be written as
.
Ordering
of
the
' 
columns in the representation of a
 is important since
the columns of the covering arrays in are listed consecu
tively from left to right.
'  ,  
 ,-  
.  &, %, 
 ,  "/0 "
An example of a
can be seen in Table 3. The overall array is a mixed level
array of strength two with nine columns containing three
symbols and two columns containing two. There are three
sub-arrays each with strength three. All three way interactions therefore among columns 0-2, 3-5, 6-8 are included.
All two way interactions among all columns are also covered. This has been
*+achieved
%, 
 ,  " with 27 rows which is the optimal size for a
. A covering array that would
cover all three way interactions for all 11 columns, on the
other hand, might need as many as 52 rows.

4. Construction Methods
Fixed strength covering arrays can be built using algebraic constructions if we have certain parameter combina

tions of 
and [4, 12]. Alternately we may use computational search algorithms [5, 6, 11]. Greedy algorithms
form the basis of the AETG and the IPO generators [5, 11].
It has also been shown that simulated annealing is effective
for building covering arrays of both mixed and fixed levels
[6, 10]. Since there are no known constructions for variable
strength arrays at the current time we have chosen to use
a computational search technique to build these. We have
written a simulated annealing program that has been used
to produce all of the results presented in this paper.

4.1. Simulated Annealing
Simulated annealing is a variant of the state space search
technique for solving combinatorial optimization problems.
The hope is that the algorithm finds close to an optimal solution. Most of the time, however, we do not know when
we have reached an optimal solution for the covering array
problem. Instead such a problem can be specified as a set 1
"
of feasible solutions (or states) together with a cost 2  associated with each feasible solution  . An optimal solution
corresponds to a feasible solution with overall (i.e. global)
minimum cost. For each feasible solution 3!41 , we define
a set 576 of transformations (or transitions), each of which
can be used to change  into another feasible solution 98 .
The set of solutions that can be reached from  by applying
  "

a transformation from 5:6 is called the neighborhood
of  .
To start, we randomly choose an initial feasible solution. The algorithm then generates a set of sequences (or

0
0
1
2
2
2
1
0
2
1
1
0
2
0
1
1
2
0
2
0
1
2
0
1
0
1
2

2
1
2
1
2
0
0
0
0
2
0
1
2
0
2
0
1
2
0
1
1
1
0
1
2
1
2

Table 3.

2
2
2
0
0
0
0
0
2
0
2
0
1
2
1
1
2
1
1
1
2
1
1
1
0
0
2

0
1
2
0
2
0
1
0
2
0
2
0
2
2
2
1
2
1
1
1
0
0
1
1
1
2
0

1
2
1
0
2
2
1
2
0
1
1
0
1
2
0
0
0
2
0
0
1
0
1
1
2
2
2

2
0
0
2
0
2
2
1
0
0
2
0
1
1
2
1
1
1
0
2
1
1
0
1
2
2
0

2
1
2
2
1
2
1
0
2
0
1
1
0
1
0
1
0
2
0
1
0
2
2
2
0
1
0

2
1
1
0
2
2
2
0
0
2
1
1
0
2
1
0
2
2
0
0
1
1
1
0
2
0
1

2
0
0
0
1
1
0
1
1
1
2
1
2
2
1
1
0
0
0
2
0
2
1
2
2
0
2

0
0
0
0
1
1
1
1
0
0
1
1
0
0
0
0
1
1
0
0
0
0
1
1
1
0
1

1
0
0
1
0
0
0
1
1
1
0
1
1
0
0
1
1
1
1
0
0
0
1
0
0
1
1

, -   
.  ,  , 
 ,  "  0"
' * ,  (
 

 " 
 "
Markov chains) of trials. If 2  8
2  then the transition is accepted. If the transition results in a feasible solution  8 of higher cost, then  8 is accepted with probability
 6
	  6
 , where 5 is the controlling temperature of
the simulation. The temperature is lowered in small steps
with the system being allowed to approach ‚Äúequilibrium‚Äù at
each temperature through a sequence of trials at this tem
perature.
Usually this is done by setting 5
5 , where

(the control decrement) is a real number slightly less than

.
The idea of allowing a move to a worse solution helps
avoid being stuck in a bad configuration (a local optimum),
while continuing to make progress. Sometimes we know
the cost of an optimal solution and can stop the algorithm
when this is reached. Otherwise we stop the algorithm when
insufficient progress is being made (as determined by an
appropriately defined stopping condition). In this case the
algorithm is said to be frozen.
Simulated annealing has been used by Nurmela and
OÃàstergaÃärd [9], for example, to construct covering designs
which have a structure very similar to covering arrays. It
has also been used by Stardom and Cohen, et al. to generate
minimal fixed strength covering arrays for software testing
[6, 10].
In the simulated annealing algorithm the current feasible

solution is an approximation  to a covering array in which
certain  -subsets are not covered. The cost function is based
on the number of  -subsets that are not currently covered.

A covering array itself will have cost . In the variable

strength array, the cost is when (i) all of the  -subsets are

covered and (ii) for each covering array of strength  8 in ,
all /8 -subsets are covered.
A potential transition is made
)
by selecting one of the -sets) belonging to  and then replacing) a random point in this -set by a random point not
in the -set. We calculate only the change in numbers of
 -sets that will occur with this move. Since the subsets of
are disjoint, anychange
we make can affect the overall


'
coverage of the
and at most one of the subsets of .
This means that the extra work required for finding variable
strength arrays over fixed strength arrays does not grow in
proportion to the number of subsets of higher strength.
We keep the number of blocks (test cases) constant
throughout a simulated annealing run and use the method
described by Stardom to determine a final array size [10].
We start with a large random array and then bisect our array
repeatedly until we find a best solution.

4.2. Program Parameters
Good data structures are required to enable the relative
cost of the new feasible solution to be calculated efficiently,
and the transition (if accepted) to be made quickly. We
build an exponentiation table prior to the start of the program. This allows us to approximate the transition probability value using a table lookup. We use ranking algorithms from [8] to hold the values of our  -sets. This allows
us to generalize our algorithms for different strengths without changing the base data structure. When calculating the
change in cost for each transition, we do not need to recalculate all of the  -sets in a test case, but instead only calculate
the change ( -1 subsets).
A constant is set to determine when our program is
frozen. This is the number of consecutive trials allowed
where no change in the cost of the solution has occurred.
For most of our trials this constant has been set to 1,000.
The cooling schedule is very important in simulated annealing. If we cool too quickly, we freeze too early because the probability of allowing a worse solution drops too
quickly. If we cool too slowly or start at too high a temperature, we allow too many poor moves and fail to make
progress
Therefore, if we start at a low temperature and cool
slowly we can maintain a small probability of a bad move
for a long time allowing us to avoid a frozen state, at the
same time continuing to make progress. We have experimented using fixed strength arrays compared with known
algebraic constructions (see [6]). We have found that a starting temperature of approximately 0.20 and a slow cooling

VCA


	  


	 !"    


	 &%')(   

C



Min
N
16
27
27
27
27
33


  
  
 
  
   ,
   ,
 
  
 
 
  

 
#$ !
 !  
   ,
 !  
#$ !
#$ !  "
#$ !


"
"  


#$&%  &%')( 

Max
N
17
27
27
27
27
33

Avg
N
16.1
27
27
27
27
33

33*
34
33*
34
41
50
67
36
64
100
125

35

34.8

35
42
51
69
36
64
104
125

34.9
41.4
50.8
67.6
36
64
101
125

125
171
180
214
100
100
304

125
173
180
216
100
100
318

125
172.5
180
215
100
100
308.5

Table 4. Table of sizes for variable strength
arrays
after 10 runs (We have omitted the parameter
*

in our notation for covering arrays in this table due to
space limitations.)

* The minimum values for these VCA‚Äôs were found during a separate set of experiments


factor, , of between 0.9998 and 0.99999 every 2,500 iterations works well. Using these parameters, the annealing
algorithm completes in a ‚Äúreasonable‚Äù computational time
on a PIII 1.3GHz
processor running Linux. For instance,
' 
the first few
‚Äôs in Table 4, complete
in seconds, while
' 
the larger problems, such as the last
in Table 4, complete within a few hours. The delta in our cost function is
counted as the change in  -sets from our current solution.
Since we can at any point make changes to both the base
array and one of the higher strength arrays, these changes
are added together.
As there is randomness inherent in this algorithm, we run
the algorithm multiple times for any given problem.

5. Results
Table 4 gives the minimum, maximum and average sizes
obtained after 10 runs of thesimulated
annealing algorithm

'
for each of the associated
‚Äôs. Each of the 10 runs uses

a different random seed. A starting temperature of .20 and
a decrement parameter of .9998 is used in all cases. In two
cases a smaller sized array was found during the course of
our overall investigation, but was not found during one of
these runs. The numbers are included in the table as well
and are labeled with an asterisk, since these provide a previously unknown bound for their particular arrays. In each
case we show the number of tests required for the base array of strength two. We then provide some examples with
variations on the contents of . Finally we show the arrays
with all of the columns involved in strength three coverage.
We have only shown examples using strength two and three,
but our methods should generalize for any strength  .
What is interesting in Table 4 is that the higher strength
sub-arrays often drive the size of the
final test suite. Such
' 
is the case in the first and second
groups in this table.
We can use this information to make decisions about how
many components can be tested at higher strengths. Since
we must balance the strength of testing with the final size
of the test suite we can use this information in the design
process.
Of course there are cases where the higher strength subsets do not determine the final test suite size since the number of test cases required is a combination
of the number
' 
of levels and the strength. In the last
group in Table
4 the two components each with 10 levels require a minimum of 100 test cases to cover all pairs. In this case we can
cover all of the triples from the 20 preceding columns with
the same number of tests. In such cases, the quality of the
tests can be improved without increasing the number of test
cases. We can set the strength of the 20 preceding columns
to the highest level that is possible without increasing the
test count.
Both situations are similar in the fact that they allow us
to predict a minimum size test suite based on the fixed level
sub-arrays. Since there are better known bounds for fixed
strength arrays we can use this information to drive our decision making processes in creating test suites that are both
manageable in size while providing the highest possible interaction strengths.

6. Conclusions
We have presented a combinatorial object, the variable
strength covering array, which can be used to define software component interaction tests and have discussed one
computational method to produce them. We have presented
some initial results with sizes for a group of these objects.
These arrays allow one to guarantee a minimum strength
of overall coverage while varying the strength among disjoint subsets of components. Although we present these objects for their usefulness in testing component based software systems they may be of use in other disciplines that

currently employ fixed strength covering arrays.
The constraining factor in the final size of the test suite
may be the higher strength sub-array. We can often get a
second level of coverage for almost no extra cost. We see
the potential to use these when there is a need for higher
strength, but we cannot afford to create an entire array of
higher strength due to cost limitations.
Where the constraining factor is the large number of levels in a set of fields at lower strength, it may be possible to
increase the strength of sub-arrays without additional cost,
improving the overall quality of the tests.
Another method of constructing fixed strength covering
arrays is to combine smaller arrays or related objects and to
fill the uncovered  -sets to complete the desired array [4].
We are currently experimenting with some of these techniques
to build variable strength arrays. Since the size of a
' 
may be dependent on the higher strength arrays, we
believe that building these in isolation followed by annealing or other processes to fill in the missing lower strength
 -sets will provide fast and efficient methods to create optimal variable strength arrays.

Acknowledgments
Research is supported by the Consortium for Embedded and Internetworking Technologies and by ARO grant
DAAD 19-1-01-0406. Thanks to the Consortium for Embedded and Internetworking Technologies for making a
visit to ASU possible.

References
[1] J. Bach. James Bach on risk-based testing In STQE
Magazine, Nov/Dec,1999.
[2] L. Brownsword, T. Oberndorf and C. Sledge. Developing new processes for COTS-based systems.
IEEE Software, 17(4):48‚Äì55, 2000.
[3] K. Burr and W. Young. Combinatorial test techniques: Table-based automation, test generation
and code coverage. In Proc. of the Intl. Conf. on
Software Testing Analysis & Review, 1998, San
Diego.

[4] M. Chateauneuf and D. Kreher. On the state of
strength-three covering arrays. Journal of Combinatorial Designs, 10(4):217‚Äì238, 2002
[5] D. M. Cohen, S. R. Dalal, M. L. Fredman, and
G. C. Patton. The AETG system: an approach
to testing based on combinatorial design. IEEE
Transactions on Software Engineering, 23(7):437‚Äì
44, 1997.
[6] M. B. Cohen, C. J. Colbourn, P. B. Gibbons and
W. B. Mugridge. Constructing test suites for interaction testing. In Proc. of the Intl. Conf. on
Sofware Engineering (ICSE 2003), 2003, pp. 3848 , Portland.
[7] S. R. Dalal, A. J. N. Karunanithi, J. M. L. Leaton,
G. C. P. Patton, and B. M. Horowitz. Model-based
testing in practice. In Proc. of the Intl. Conf. on
Software Engineering,(ICSE ‚Äô99), 1999, pp. 28594, New York.
[8] D. L. Kreher and D. R. Stinson. Combinatorial
Algorithms, Generation, Enumeration and Search.
CRC Press, Boca Raton, 1999.
[9] K. Nurmela and P. R. J. OÃàstergaÃärd. Constructing
covering designs by simulated annealing. Technical report, Digital Systems Laboratory, Helsinki
Univ. of Technology, 1993.
[10] J. Stardom. Metaheuristics and the search for covering and packing arrays. Master‚Äôs thesis, Simon
Fraser University, 2001.
[11] K. C. Tai and L. Yu. A test generation strategy for
pairwise testing. IEEE Transactions on Software
Engineering, 28(1):109-111, 2002.
[12] A. W. Williams. Determination of test configurations for pair-wise interaction coverage In Proc.
Thirteenth Int. Conf. Testing Communication Systems, 2000, pp. 57‚Äì74.

