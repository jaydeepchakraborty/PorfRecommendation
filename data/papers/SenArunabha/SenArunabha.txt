Influence Propagation in Adversarial Setting: How to Defeat Competition with Least Amount of Investment
Shahrzad Shirazipourazad, Brian Bogard, Harsh Vachhani, Arunabha Sen
School of Computing, Informatics and Decision Systems Engineering Arizona State University Tempe, AZ 85287

{sshiraz1, bbogard, hvachhan, asen}@asu.edu Paul Horn
Department of Mathematics Harvard University Cambridge, MA 09322

phorn@math.harvard.edu ABSTRACT
It has been observed that individuals' decisions to adopt a product or innovation are often influenced by the recommendations of their friends and acquaintances. Motivated by this observation, the last few years have seen a number of studies on influence maximization in social networks. The primary goal of these studies is identification of k most influential nodes in a network. A major limitation of these studies is that they focus on a non-adversarial environment, where only one player is engaged in influencing the nodes. However, in a realistic scenario multiple players attempt to influence the nodes in a competitive fashion. The proposed model considers a competitive environment where a node that has not yet adopted an innovation, can adopt only one of the several competing innovations and once it adopts an innovation, it does not switch. The paper studies the scenario where the first player has already chosen a set of k nodes and the second player, with the knowledge of the choice of the first, attempts to identify a smallest set of nodes (excluding the ones already chosen by the first) so that when the influence propagation process ends, the number of nodes influenced by the second player is larger than the number of nodes influenced by the first. The paper studies two propagation models and shows that in both the models, the identification of the smallest set of nodes to defeat the adversary is NP-Hard. It provides an approximation algorithm and proves that the performance bound is tight. It also presents the results of extensive experimentation using the collaboration network data. Experimental results show that the second player can easily defeat the first with this algorithm, if the first utilizes the node degree or closeness centrality based algorithms for the selection of influential nodes. The proposed algorithm also provides better performance if the second player utilizes it instead of the greedy algorithm to maximize its influence.

Categories and Subject Descriptors
F.2.2 [Analysis of Algorithms and Problem Complexity]: [Non-numerical Algorithms and Problems]

General Terms
Algorithms, Experimentation, Performance

Keywords
Social Networks, Influence Maximization, Adversarial Environment

1.

INTRODUCTION

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. CIKM'12, October 29­November 2, 2012, Maui, HI, USA. Copyright 2012 ACM 978-1-4503-1156-4/12/10 ...$15.00.

It has been widely observed in various studies in social sciences and economics that an individuals' decision to adopt a product, behavior or innovation is often influenced by the recommendations of their friends and acquaintances. Motivated by this observation, the last few years have seen a number of studies on influence maximization problem in social networks [2, 3, 4, 6, 7, 11, 13]. One major goal of several of these studies is identification of k most influential nodes in a network. A product manufacturer may want to identify the k most influential nodes in the network, as she may want to incentivize these nodes to buy the new product by providing free samples to them, on the expectation that once these nodes are convinced about the quality of the product, they will recommend it to their friends on the social network and encourage them to buy the product. This set of k nodes, being the most influential on the network, will have the largest impact on convincing the rest of the nodes about the quality of the product. Since the manufacturer has a fixed budget for advertising, she can provide free samples only to a limited number of nodes in the network. The size of the advertising budget determines the value of the parameter k. It may be noted that most of the studies on influence propagation are geared toward a non-adversarial environment, where only one manufacturer (player) is attempting

to influence the nodes of a social network to buy her product. However, in a realistic market scenario, most often there exists multiple players, each attempting to sell their competing products or innovations. For example, just as Coke attempts to convince customers in an emerging market about the quality of their beverage, its main competitor, Pepsi, also does the same. Both the competitors have only a finite advertisement budget and both of them want to derive the greatest benefit out of their advertising campaign. The goal of both the players often is to capture a share of this emerging market that is larger than its competition. The non-adversarial influence propagation models consider scenarios where a user (a node u in a social network graph G = (V, E )) adopts (or does not adopt) an innovation based on how her acquaintances have adopted the innovation. In these models each node u in the social network graph is in one of the following two states: (i) u has adopted innovation A, and (ii) u has not adopted innovation A but u is open to the idea of adoption. One can visualize such a scenario by coloring the nodes of the social network graph with red if they have adopted the innovation A and with white if they have not adopted A yet, but are open to the idea of adopting A in the future. As the diffusion process progresses with time, by observing changing color of the nodes of the graph one can infer if innovation A is being adopted by the members of the social network. Although, this paper focus on influence propagation in social networks, conceptually, the scenario is identical for spread of any contagion through a network - be it spread of diseases through a human contact network or spread of worms through the Internet. The influence (contagion) propagation models can be divided into three distinct classes: · Class I: Non-adversarial · Class II: Adversarial with passive adversary · Class III Adversarial with active adversary The problems in classes I and II can be stated as follows: · Class I: How to identify a set of k initial (seed) nodes, so that once they are influenced/infected, they will infect the largest number of uninfected nodes in the network? · Class II: Given that a subset of the nodes is already influenced/infected, how to identify a set of k uninfected nodes, so that when they are immunized, they will have the largest impact in preventing the uninfected nodes from being infected. In most of the influence propagation models, influence propagates in a step-by-step fashion and as such there is a notion of time step (or propagation step) involved. The expected number of nodes influenced at the end of time step D is at most the expected number of nodes influenced at the end of time step D + 1. In other words, expected number of nodes influenced at the end of time step D is a nondecreasing function of D. The Class I influence propagation problem considered in [11] may be viewed to have three dimensions, (i) the number of seed nodes activated at the beginning (budget or cost of influence), (ii) the expected number of activated nodes at the end of propagation (impact or coverage of initial seed nodes), and (iii) time steps for propagation. The objective of the influence maximization problem considered in [11], is

to maximize the coverage subject to a budget constraint but without any constraint on the number of time steps. The Class I problem considered in [11] can be stated in the following way: "Which k white nodes should be colored red initially, so that the largest number of white nodes turn to red at the end of propagation process?". The Class II problems can be stated in the following way: "Given that some nodes are already colored red, which k white nodes should be colored blue, so that this set of nodes will have the largest impact in preventing the white nodes from turning red. In Class I, there is no notion of an adversary. The red nodes are trying to convert all the white nodes into red nodes and there is no agent that is actively trying to prevent this conversion. The Class II, although it has a notion of an adversary (i.e., the blue nodes) which is trying to slow down (or stop) white-to-red conversion, at best this agent can be viewed as a passive adversary, because its goal is to prevent white-to-red conversion, and it is not engaged in white-to-blue conversion. This gives rise to Class III, a truly adversarial scenario, where the red agent is trying to convert all the white nodes into red, while the blue agent is trying to convert all the white nodes into blue. In this case, the blue agent can be viewed as an active adversary of the red agent. The Class III models the scenario where a node u is being actively encouraged by an adversary not only not to adopt the innovation but also to adopt a competing innovation. In this case, each node u in the social network graph can be in one of following three states: (i) u has adopted innovation A , (ii) u has adopted innovation B , and (iii) u has not adopted any innovation A or B but is open to the idea of adopting either one of them. This adversarial scenario can be viewed as a classic case of a strategic conflict game between the proponent(s) and the opponent(s) of adoption of an innovation and a game is won by the proponent(s) if u decides to adopt the innovation A. This paper studies a Class III scenario where two vendors (players) are trying to sell their competing products by influencing the nodes of a social network. The goal of both the players is to have a market share that is larger than its competition. It considers the scenario where the first player (P1 ) has already chosen the k nodes to have a large influence (coverage) on the social network. The second player is aware of the first player's choice and the goal of the second player (P2 ) is to identify a smallest set of nodes (excluding the ones already chosen by the first player) so that the number of nodes influenced by the second player will be larger than the number of nodes influenced by the first player within D time steps. In other words, the objective of the problem is to minimze the cost subject to the constraint that the coverage of the second player is larger than the coverage of the first player within D time steps. Since the goal of the second player is to win the "game" (i.e., to have a larger coverage or market share), with influencing (incentivizing) as few nodes as possible, the problem under study in this paper is referred to as the "Winning with Minimum Investment" (WMI) problem. In [3], the authors study a similar problem belonging to class III. However, the objective of the problem studied in [3] is different from the one being studied in this paper. The goal of the second player in the problem studied in [3] is not to defeat the first player with least amount of investment, but to maximize its own influence.

Using the same two influence propagation models introduced in [3], the contributions of the paper may be listed as follows: · Introduction of a new influence propagation problem in an adversarial setting where the goal of the second player is to defeat the first within D time steps and least amount of cost (i.e., number of seed nodes) · NP-Hardness proof for the problem under both the influence propagation models · Approximation algorithm for the problem with a tight performance bound. · Experimental evaluation of the Approximation algorithm with collaboration network data Experimental results show that utilizing the proposed algorithm, the second player can easily defeat the first, if the first player utilizes the node degree or closeness centrality based algorithms for the selection of the initial (seed) nodes. The proposed algorithm also provides better performance for the second player if she utilizes it instead of the algorithm to maximize influence proposed in [3], in the sense that it requires selection of a fewer number of seed nodes to defeat the first player. The rest of the paper is organized as follows. The section II summarizes related work on influence propagation. The section III describes the propagation models used in the paper in detail. The sections IV, V and VI discuss the problem statement, computational complexity and approximation algorithm results respectively. The results of experimental evaluation is presented in section VII and section VIII concludes the paper.

2.

BACKGROUND AND RELATED WORK

The studies on identification of influential nodes in a social network were triggered by a paper authored by Domingos and Richardson [6]. They introduced the notion of "network value" of a node in a social network and using a Markov random field model where a joint distribution over all node behavior is specified, computed the network value of the nodes. Kempe, Kleinberg and Tardos followed up the work in [6] by providing new models derived from mathematical sociology and interacting particle systems [11]. They made a number of important contributions by providing approximation algorithms for maximizing the spread of influence in these models by utilizing the submodularity property of the objective functions. In addition to providing algorithms with provable performance guarantee, they also presented experimental results on large collaboration networks. Their experimental results showed that their greedy approximation algorithm significantly out-performed the node selection heuristics based on degree centrality and distance centrality [18]. The approximation algorithm proposed in [11] is computeintensive. Accordingly, several researchers approached the issue of scalability from different directions. Chen et. al. in [4] provided improvement of the original greedy algorithm of [11] and proposed a degree discount heuristic to improve influence spread. Mathioudakis in [13] introduced the notion of sparsification of influence networks and presented an algorithm, SPINE, to compute the "backbone" of the influence network. Utilizing SPINE as a pre-processing step for the

influence maximization problem, they showed that computation on the sparsified model provided significant improvements in terms of speedup without compromising accuracy. Wang et. al. in [17] considered the influential node identification problem in a mobile social network and presented a two step process, where in the first step, communities in the social network are detected and in the second step a subset of communities is selected to identify the influential nodes. Experimental results with data from large real world mobile social network showed that their algorithm performed an order of magnitude faster than the state-of-the-art greedy algorithm for finding the top-k influential nodes. A simulated annealing (SA) based algorithm for finding the top-k influential nodes was presented in [10]. It has been reported in [10], that using data from four real networks, the SA based algorithm performed 2-3 orders of magnitude faster than the state-of-the-art greedy algorithm. In addition to attempts to address the scalability issue of the greedy algorithm in [11], efforts on variations of the original problem formulation and also the computation model is underway in the research community. In [7] two new problem formulations are provided. In the first formulation, the goal is to minimize the cost, subject to the constraint that coverage exceeds a minimum threshold  without any constraint on the number of time steps. The goal of the second formulation is to minimize the number of time steps, subject to a budget constraint k and a coverage constraint  . For the first version of the problem, the authors provide a simple greedy algorithm and show that it provides a bicriteria approximation. For the second version, they show that even bicriteria or tricriteria approximations are hard under several conditions. In [1], the authors argue that a user (a node in the social network) may be influenced by positive recommendations from a group of friends (neighbors in the network) but that does not necessarily imply that she will adopt the product herself. However, she may pass on her positive impression about the product to another group of friends. Clearly, such a model departs from the model considered in [11]. The authors in [1] consider an "adoption maximization" problem instead of "influence maximization" problem and present both analytical and experimental results for the new problem. The authors in [12] argue that a limitation of the traditional influence analysis technique is that they only consider positive relations (agreement, trust) and ignore the negative relations (distrust, disagreement). Moreover, the traditional techniques also ignore conformity of people, i.e., an individual's inclination to be influenced. The paper studies the interplay between influence and conformity of each individual and computes the influence and conformity indices of individuals. The authors in [5] suggest an alternate way of measuring the influencing capability of an individual on her peers, through the individuals reach within the social network for certain actions. All the references discussed in the last three paragraphs pertain to the class I (non-adversarial) problems as defined in the previous section. Results on study of class II problems (adversarial with passive adversary) is presented in [8]. It focuses on identification of blockers, the nodes that are most effective in blocking the spread of a dynamic process through a social network, and reports that simple local measures such as the degree of a node are good indicators of its effectiveness as a blocker. The blocker identification problem has been extensively studied in the public health community, where

the goal is to stop or slow down progress of an infectious disease by immunizing a small set of key individuals in the community. As indicated in the previous section, the WMI problem studied in this paper belongs to Class III (adversarial with active adversary). Unfortunately, there exists only a handful of studies on problems belonging to Class III. Bharathi et. al. were one of the earliest to study a Class III problem [2]. They proposed a mathematical model for diffusion of multiple innovations in a network, an approximation algorithm with a (1 - 1/e) performance guarantee for computing the best response to an opponent's strategy. In addition they prove that the "price of competition" of the game is at most 2. While game theoretic framework was utilized for deriving the results in [2], Carnes et al. used an algorithmic framework to study a Class III problem [3]. Their research primarily extends the problem studied in [11] from the Class I domain to the Class III domain. They study the follower's perspective (i.e., the player who entered the market after the first player) and investigate how a follower can maximize her influence in the network with a limited budget, given that the first player has already entered the market and influenced a certain number of key individuals (nodes in the network). They prove that the influence maximization problem for the second player is NP-complete and provide an approximation algorithm that is guaranteed to produce a solution within 63% of the optimal. Adversarial models in evolutionary game dynamics was studied by Istrate em et al. in [9]. In all the problems discussed in [2, 3] once a node adopts an innovation (i.e., changes its color from white to red or white to blue), it is not allowed to change its color, i.e., the model precludes the possibility of an individual changing her mind. However, the model considered by Nowak et al. in [16] there are only red and blue nodes (no white nodes) and the model allows a node to change its color from red to blue and vice-versa. Although this model was developed to capture a biological phenomenon involving viruses and cells, this model can be equally effective in capturing the phenomenon of the spread of ideas and behaviors in human population. Using evolutionary game theoretic and evolutionary graph theoretic techniques, the authors establish fundamental laws that govern choices of competing players regarding strategies.

spective. Since this paper studies the problem with only two competing players, the models proposed in [3] are more relevant for this study than the one proposed in [2]. Accordingly, the influence propagation models of [3] are used here. Since these models, Distance-based Model (DBM) and Wavepropagation Model (WPM), are generalization of the ICM, the paper first discusses ICM and then DBM and WPM.

3.1

Independent Cascade Model

The social network is modeled as a graph G = (V, E ), where each node represents an individual. Each individual may either be active (i.e., has adopted innovation) or inactive. A node can switch from an inactive state to an active state but cannot switch back in the other direction. The propagation process from the perspective of an inactivate node v  V can be described in the following way: With passage of time, more and more of v 's neighbors become active and this may cause v to become active at some time step. The activation of v in turn may trigger activation of some of v 's inactive neighbors. In the ICM model there exists a set of nodes V  V that are active (seed nodes) initially and the rest of the nodes are inactive. Influence propagation unfolds in discrete steps following a randomized process. When a node v first becomes active in time step d, it has a single chance to activate each of its inactive neighbors w with probability pv,w at time step d + 1. If v succeeds, w become active at d + 1. However, if v fails, it doesn't get another chance to turn w active. The process of conversion of nodes from the inactive to the active state continues, till no further activation is possible. Since v influences w with probability pv,w , the v - w edge is considered active with probability pv,w . The set of active edges is denoted by Ea .

3.2

Generalized ICM for Adversarial Scenario

3.

INFLUENCE PROPAGATION MODELS

A number of influence propagation models for the non-adversarial scenario have been proposed in the literature [11]. Among these, the Linear Threshold Model (LTM) and the Independent Cascade Model (ICM) have drawn most attention in the research community. As indicated earlier, the literature on influence propagation in adversarial scenario with active adversaries is very sparse [2, 3]. Bharati et al. in [2] and Carnes et al. in [3] have studied influence propagation in adversarial scenario with active adversaries, and have proposed two different models for it. Both of these two models are generalizations of the Independent Cascade Model. The model proposed in [2] is suitable for a multiplayer scenario, whereas the model proposed in [3] is for two competing players. Bharati et al. in [2] study the problem from a game-theoretic perspective and focus on finding best response strategies for the players. Carnes et al. on the other hand study the problem from an algorithmic per-

The ICM can be adapted to handle adversarial scenario by allowing the nodes to be in one of the following three states - (i) active by adopting innovation A, (ii) active by adopting innovation B , and (iii) inactive. We use the notation IA and IB to indicate the initial adopters (seed nodes) of technologies A and B respectively. The nodes in the set V - (IA  IB ) are the nodes that are inactive initially. The sets IA and IB are disjoint, i.e., IA  IB = . Just as in ICM, an active node v may influence each one of its inactive neighbors w with probability pv,w . However, in an adversarial scenario, an inactive node w, may be in a situation where one of its active neighbor v attempts to influence w with innovation A, whereas another active neighbor u attempts to influence w with innovation B . In order to deal with this situation, the authors in [3] proposed two new models - (i) Distance-based Model, and (ii) Wave-propagation Model. The models specify the probability with which the node w will be influenced, when its active neighbors attempt to influence w with two competing technologies. The GICM operates on a random subgraph of the social network graph G = (V, E ), where each edge is included independently with probability pv,w . The details of these two models are described in the following two subsections.

3.3

Distance-based Model

Suppose that du (I, Ea ) denotes the shortest path distance from the node u to the node set I where I = IA IB along the active edges in the edge set Ea . If u is not connected to any node of I using only the active edges Ea , then du (I, Ea ) =

. Let u (IA , du (I, Ea )) and u (IB , du (I, Ea )) be the number of nodes in IA and IB respectively, at distance du (I, Ea ) from u along edges in Ea . The probability that node u adopts innovation i  {A, B } when maximum number of propagation steps is D is denoted by Pi (u|IA , IB , Ea , D) and is computed in the following way: if du (I, Ea )  D, u (Ii ,du (I,Ea )) ; Pi (u|IA , IB , Ea , D) = u (IA ,du ( I,Ea ))+u (IB ,du (I,Ea )) otherwise, it is zero. In this model the expected number of nodes which adopt i  {A, B } will be computed in the following way: j (IA , IB , D) = E
uV

L1
...

L2
...

L3
...

Ln
...

e1

e2

e3

...

en xn ... x2 ... a x1

y1 y2

... s1 s2 s3 sm

ynr

Figure 1: Graph G = (V, E ) of WMI instance in set cover reduction

Pi (u|IA , IB , Ea , D)

5.1

Distance-based Model

where j = 1 if i = A; else j = 2 and the expectation is over the set of active edges.

Decision version of WMI: Is there a set IB where |IB |  M and 2 (IA , IB , D) > 1 (IA , IB , D)? Theorem 1. WMI is NP-hard for the distance-based model. Proof: In order to prove that WMI is NP-hard when diffusion is based on distance based model, we reduce the NPcompete Set Cover problem to W M I . The decision version of the Set Cover problem is defined in the following way: A ground set of elements S = {e1 , e2 , . . . , en }, a collection of sets C = {s1 , s2 , . . . , sm } such that si  S and a positive integer K  |C | are given. The question is whether there exists a collection Q  C that covers all the elements in S and |Q|  K . Given an instance of set cover problem we construct an instance of W M I . We compute G = (V, E ) in the following way. For every element ei  S we add a node ei and for every set sj  C we add a node sj to V . We add an edge (ei , sj ) to E for every ei and sj if ei  sj . Also, we add a node a and nodes x1 , . . . , xn to V . Then, for every ei we add edges (a, xi ) and (xi , ei ) to E . Moreover, for every ei we add a set of r nodes, Li = {li,j |1  j  r} to V and we connect them directly to ei . We identify the value of r later in the proof. Finally, we add n × r additional nodes, y1 , . . . , yn×r , to V and edges (yt , a), 1  t  n × r (Fig. 1). We consider that all edges are active; i.e., pu,v = 1 for all edges in E . We assign D = 4 equal to the diameter of the graph G, M = K and IA = {a}. Now, we show that the set cover problem has a solution if and only if there is a set IB  V - IA such that |IB |  M and 2 (IA , IB , D) > 1 (IA , IB , D). First we consider that there is a collection Q  C that covers S and |Q|  K . Then IB includes all nodes sj corresponding to the sets in Q. In this case, all ei will be at distance one from IB and two from IA . So, all ei and the nodes in Li will adopt IB with probability one. Moreover, the nodes sj  / IB are two hops away from IB while 3 hops away from IA . Hence, all nodes sj will adopt IB . Therefore, we have 2 (IA , IB , D) = m + n(1 + r); so, 2 (IA , IB , D) > 1 (IA , IB , D). Next, we show that if there is no collection Q of size K that covers all elements then there is no set IB  V - IA of size M where 2 (IA , IB , D) > 1 (IA , IB , D). Considering that set cover does not have a solution, there should be at least one ei whose distance from IB cannot be one; so, there is an ei and consequently nodes in Li that choose A and the probability that with the probability at least K1 +1 they choose B is at most KK . Also, at most K nodes from +1 x1 , . . . , xn can be at distance less than or equal to 1 from IB . Hence n - K of them will adopt A with probability one. Therefore, we have

3.4

Wave-propagation Model

In this model, in step d < D all nodes that are at distance d - 1 from some node in I have adopted technology A or B and all nodes that are farther than d - 1 from I have not adopted any technology yet(where the distance is measured with respect to active edges). Every node at distance d from I chooses one of its neighbors at distance d - 1 from I independently at random and adopt the same technology as its neighbor. For every node u, S denotes the set of neighbors of u that are closer to I than u; i.e., their distance from I is du (I, Ea ) - 1. In this model Pi (u|IA , IB , Ea , D), the probability that node u adopts innovation i  {A, B } in at most D steps, is computed as follows: If du (I, Ea )  D, P (v |I ,IB ,Ea ,D ) ; Pi (u|IA , IB , Ea , D) = vS i |SA | otherwise, it is zero. In this model the expected number of nodes which adopt i  {A, B } will be computed in the following way: j (IA , IB , D) = E
uV

Pi (u|IA , IB , Ea , D)

where j = 1 if i = A; else j = 2 and the expectation is over the set of active edges.

4.

PROBLEM STATEMENT

The WMI problem can be stated informally as follows: Given a diffusion model and the information that a subset of network nodes IA have already adopted innovation A marketed by player P1 , what is the fewest number of nodes should player P2 (marketing innovation B ) target so that by the end of D time steps, the number of nodes that adopt innovation B will exceed the number of nodes that adopt innovation A? If 1 (IA , IB , D) and 2 (IA , IB , D) denote the expected number of nodes that adopt innovations A and B respectively within D time steps, the objective of the WMI problem is to minimize | IB | subject to 2 (IA , IB , D) > 1 (IA , IB , D)

5.

COMPUTATIONAL COMPLEXITY

In this section, we prove that W M I problem is NP-hard for both propagation models.

2 (IA , IB , D)  m + (n - 1)(1 + r) + KK (r + 1) + K and +1 1 1 (IA , IB , D)  1 + nr + n - K + K +1 (r + 1). We choose r in our instance large enough such that r > Then we have 1 + nr + n - K + K1 (r + 1) > m + (n - 1)(1 + +1 r) + KK ( r + 1) + K ; so  ( I , I , D) < 1 (IA , IB , D). 2 A B +1
(m+2K -2)(K +1)+K -1 . 2

Algorithm 1 GWMI
Input: G = (V, E ), IA , D Output: IB 1: while  (IA , IB , D)  0 do 2: for every node i  V - (IA  IB ) do 3: Compute Fi 4: end for 5: Select node j with maximum Fj 6: IB = IB  {j } 7: end while 8: return IB

5.2

Wave Propagation Model

Theorem 2. WMI is NP-hard for the wave propagation model. Proof: Similar to Theorem 1, we reduce decision version of Set Cover problem to decision version of W M I when wave propagation model is used for diffusion. We construct an instance of W M I in the same way as in Theorem 1. The only change that should be made to this instance is the value of r which will be computed later. We need to show that the set cover problem has a solution if and only if there is a set IB  V - IA such that |IB |  M and 2 (IA , IB , D) > 1 (IA , IB , D). First we consider that there is a collection Q  C that covers S and |Q|  K . Then IB includes all nodes sj corresponding to the sets in Q. Similar to the proof of Theorem 1 we have 2 (IA , IB , D) = m + n(1 + r); so, 2 (IA , IB , D) > 1 (IA , IB , D). Next, we show that if there is no collection Q of size K that covers all elements then there is no set IB  V - IA of size M where 2 (IA , IB , D) > 1 (IA , IB , D). Considering the construction of G and the fact that set cover does not have a solution , there should be at least one ei whose distance from IB cannot be one or smaller. Since the node xi connected to this ei will have probability 1 to accept A and the maximum number of nodes in first hop neighborhood of ei that are at distance one from IA  IB is m + 1, there is an ei and consequently nodes in Li that choose A and the probability that with the probability at least m1 +1 m they choose B is at most m . Also, at most K nodes from +1 x1 , . . . , xn or y1 , . . . , yn×r can be at distance less than or equal to 1 from IB . Hence n(r + 1) - K of them will adopt A with probability one. Therefore, we have m (r + 1) + K and 2 (IA , IB , D)  m + (n - 1)(1 + r) + m +1 1 1 (IA , IB , D)  1+ n(r +1) - K + m+1 (r +1). We choose r in
3 our instance large enough such that r > m + K (m + 1) - 2 . 2 1 Then we have 1 + n(r + 1) - K + m+1 (r + 1) > m + (n - m 1)(1+ r)+ m (r +1)+ K ; so 2 (IA , IB , D) < 1 (IA , IB , D). +1
2

In [11], it is mentioned that computing the exact value of 1 (IA , , D) efficiently is an open question. Similarly, there is no known way to compute 1 (IA , IB , D), 2 (IA , IB , D) in both propagation models efficiently. However, by sampling the active sets we can get a close approximation with high probability. Given IA , IB and a set of active edges Ea , computation of 1 and 2 in both propagation models has O(n3 ) time complexity since it needs computation of single all-pairs shortest paths. Given IA , IB and input graph G, using sampling, we can then approximate 1 and 2 to within (1+  ) for any  > 0 where the running time depends on 1/ [3].

6.1

Upper Bound Computation

Theorem 3. GWMI has a log n approximation ratio. t be the set of B 's initial adopters selected by Proof. Let IB 0 , D) = GW M I at step t. Initially, IB is empty and  (IA , IB -1 (IA , , D). In every iteration t, the nodes in the optiopt t-1  mal set of B 's initial adopters, IB , will make  (IA , IB opt opt IB , D) positive. We denote the size of IB by OP T and the size of the solution of GW M I by H . Therefore, There t-1 will be at least one node in V - {IA  IB } that increases
t-1 , D) at least by  (IA , IB
t-1 | (IA ,IB ,d)| . OP T

Let, vt be the node
t-1 | (IA ,IB ,D )| . OP T

selected by GW M I at iteration t. Then, Fvt  Therefore, for t < H we have
t t-1 , D )| - | (IA , IB , D)|  | (IA , IB

t-1 | (IA , IB , D)| OP T

0  | (IA , IB , D)|(1 -

1 t ) OP T

6.

APPROXIMATION ALGORITHM

Since we proved that finding the optimal solution for W M I is hard, in this section we propose a greedy algorithm called GW M I . In this algorithm either of the two propagation models discussed before can be used as the diffusion process. Let  (IA , IB , D) be (2 (IA , IB , D) - 1 (IA , IB , D)). We define Fi to denote the amount of increase in the value of  when node i is added to IB ; i.e., Fi =  (IA , IB  {i}, D) -  (IA , IB , D). Initially IB is empty. Hence,  (IA , IB , D)  0. The algorithm executes through iterations and in each iteration node i  V - IA with the maximum Fi is selected. The steps of the algorithm GW M I has been shown in Algorithm 1.

0 , D)| = 1 (IA , , D)  n. Hence Also, we know that | (IA , IB we have -t 1 t t | (IA , IB , D)|  n(1 - )  ne OP T . OP T Since adding a node to IB will increase  (IA , IB , D) at least t by one, we need to find the smallest t that | (IA , IB , D)| < 1. Then adding at most one more node will make  (IA , IB , D) positive. Therefore, H  1 + OP T ln n. We note that this proof holds for both propagation models.

6.2

Lower Bound Computation

We now give a construction giving the lower bound for GWMI when distance-based propagation model is used. Let vertices and G(n, 3/4) be the X and Y be disjoint sets of n 2 Erd os-Renyi random graph on X  Y with p = 3/4. We take two new vertices u and v , connect u to all vertices of X and v to all vertices of Y . Now, we add a disjoint star S with n + 2 leaves and connect the center of the star to u and v . This yields our graph G (Fig. 2).

larly v ) is chosen, then increase is at most
G(n, 3/4) X n/2 u Y n/2 v Red set

1+

1 1 |X | + |Y | (1) k+1 (k + 1)(k + 2) 1 n 1 n = (1/4)k + (1/4)k + O(n3/4 ). k+1 2 (k + 1)(k + 2) 2

Figure 2: Construction of G.

We consider that the center of the star is the only initial adopter of A (red node), and pu,v is uniform and it is 1 for all the edges of G and D = 3. An optimal set of initial adopters of B (initial blue nodes) includes u, v and any of the leaves of S . We claim that the greedy algorithm GW M I will select (log n) vertices with high probability, assuming n is large enough. In order to prove this we first state a technical lemma giving a condition that G satisfies with high probability. Let S  X  Y . We say S is fair if 1. |X \  (S )| = (1/4)|S | n + O(n3/4 ) and |Y \  (S )| = 2 |S | n 3/4 (1/4) 2 + O(n ). where  (S ) is the set of one hop neighbors of vertices in S. We claim the following lemma, whose proof we defer: Lemma 4. With probability 1 - o(1) every set S  X  Y 1 ln(n) is fair. Furthermore, the induced graph with |S | < 100 on X  Y has diameter 2, every vertex in Y is at distance at most 2 from u and every vertex in X is at distance at most 2 from v . Assuming Lemma 4 we prove the lower bound. In particular we prove the following: The greedy algorithm selects at 1 least 100 ln n vertices from X  Y . We proceed by induction. At the first step, the greedy algorithm has to choose between a vertex in X  Y , one of u or v , or one of the vertices in the star. Selecting a vertex in the star will cause the number of blue vertices to increase by one and red vertices to decrease, a net change of two. Selecting u (or resp. v ) will increase blue (and decrease red) by a total of 1 + n +n ; since every 2 4 vertex in X will be at distance 1 from a blue vertex and every vertex in Y will be at distance 2 from both u and the red vertex if u is selected. On the other hand, by fairness, if a vertex x in X  Y is selected; the increase in blue is at least n 3n +n + O(n3/4 ); since 34 + O(n3/4 ) vertices are at distance 4 8 n 1 from x and the other 4 + O(n3/4 ) are at distance 2 from both x and the red vertex. Therefore the greedy algorithm will select from X  Y at the first time. Now suppose that the greedy algorithm has selected from 1 X Y a total of k < 100 ln n times. Let B denote the selected set, and X = X \  (B ) and Y = Y \  (B ). Every vertex in X  Y is at distance two from all k blue vertices, and hence k they are currently blue with probability k+1 . Furthermore by fairness X and Y are both of size (1/4)k n + O(n3/4 ). 2 Again, the greedy algorithm must choose: If u (or simi-

On the other hand, if a vertex x in X  Y is chosen, the increase is at least 1 1 | (x)  X | + | (x)  Y | (2) k+1 k+1 1 |X  Y \  (X )| + (k + 1)(k + 2) 1 3 n 1 n =2· · (1/4)k + (1/4)k+1 + O(n3/4 ); k+1 4 2 (k + 1)(k + 2) 2 therefore, (2) - (1) is positive and hence the vertex in X  Y will be chosen as desired. We note that this construction is for sufficiently large n and (1/4)k n >> n3/4 .
1 Proof of Lemma 4. Let S  X Y , with |S | < 100 ln n. Then n E[|X \ S |] = (1/4)|S | (|X | - |X  S |) = (1/4)|S | + O(ln n). 2

Let XS = |X \ S |. Chernoff bounds imply that P(|XS -E[XS ]| > n3/4 )  exp(-( n3/2 ))  exp(-(n1/2 )). E[XS ]

Bounds for |Y \ S | follow similarly. On the other hand there are at most
1 100

ln n

i=1

n i



1 ln(n) · nln n , 100

sets S . Thus union bounds imply every set is fair with probability 1 - exp(-(n1/2 )). Note that the expected number of common neighbors ben tween x and y in X  Y is 9 , and Chernoff bounds plus 16 union bounds imply every pair x and y is of distance 2 (and n in fact has (1 - o(1)) 9 common neighbors). Likewise, u 16 n expected neighbors and Cherand a vertex in Y have 38 n noff bounds imply that every pair has (1 + o(1)) 38 common neighbors. Likewise, for v and vertices in X . A union bound over all events completes the proof.

7.

SIMULATION

In this section we evaluate the performance of our approximation algorithm, GW M I , on a real network data set. It has been suggested in [15] that the co-authorship graphs are representative of typical social networks. As such, we use the real collaboration network data set of the scientists posting preprints on the high-energy theory archive at www.arxiv.org, 1995-1999 [14]. This network has 8361 nodes (authors) and 15751 edges. The largest connected component has 5835 number of nodes (authors) and maximum distance between the nodes in a connected component is 19. Our experiments were conducted on a high performance computer which is a 5K processor Dell Linux Cluster. The program is parallelized with OpenMP, optimized with Intel compiler and was executed on an 8 core compute node. The cores in the node have equal access to a common pool of shared memory. Each node is comprised of 2.66/2.83 GHz

processors, 8MB cache, 16GB memory and 8 cores. Since our experiments required execution of the algorithm on a large number of instantiation of a social network (the graphs were different as their set of active edges were different), we used OpenMP for parallelization of the graph instances for the simulation with one data set. In the first set of experiments we evaluate the performance of GWMI algorithm against the results obtained from the heuristics based on node degree and closeness centrality. These heuristics are most often used in social networks to identify most influential nodes [11]. We also compare performance of GWMI with the greedy algorithm proposed in [3] for selection of seed nodes for the second player P2 . In our model the first player P1 is trying to market product A and the second player P2 is trying to market product B . Since WMI problem is NP-hard and the input data set is large, computation of the optimal solution within a reasonable amount of time is unlikely. It may be noted that there is no known way of computing the exact value of 1 (IA , IB , D) and 2 (IA , IB , D) efficiently [11]. Accordingly, we use sampling of the active edge sets to obtain close approximation of 1 (IA , IB , D), 2 (IA , IB , D) with high probability. As in the experiments reported in papers [11, 3], we assign the edge probabilities to be 0.1. In all the experiments we use WPM as the diffusion model. The node degree based heuristic selects the nodes in the decreasing order of their degrees and the closeness centrality based heuristic selects the nodes in the increasing order of their average distance to other nodes. The distance between two nodes that are not in the same connected component is taken to be n, where n is number of nodes in the network. In the greedy algorithm proposed in [3], in every iteration the node that increases 2 (IA , IB , D) the most is selected. We refer to this algorithm as Second Player Influence Maximization (SPIM) algorithm. In these experiments, maximum number of propagation steps is taken to be 10, i.e., D = 10. In the experiments, the player P1 used node degree based heuristic to select its k initial adopters. In our experiments, the size of initial adopters of A is varied from 20 to 100. The results of this set of experiments using the WPM is shown in Fig. 3. The Fig. 3 shows that all five sizes of the initial adopters of A (20, 40, 60, 80, 100), the GWMI algorithm required the fewest number of initial adopters of B necessary to defeat A's influence at the end of time step 10. The legend Degree-Degree in Fig. 3 denotes that both the players are using the node degree based heuristics to select the initial adopters. Similarly,the legend Degree-GWMI denotes that while P1 is using the node degree based heuristics to select the initial adopters, P2 is using the GWMI algorithm to do the same. The Figs. 4 and 5 show the coverage (i.e., the number of nodes influenced at the end of 10 time steps) for players P1 and P2 respectively. Although the GWMI algorithm does not make an effort to minimize the coverage of P1 , it may be observed from the Fig. 4, the coverage of P1 is less if P2 uses GWMI instead of SPIM. Thus P2 is better off using GWMI instead of SPIM, if in addition to be able to defeat P1 with least investment (i.e., initial adopters), P2 wants to have a smaller market share for P1 . The Fig. 5 shows the coverage of P2 at the end of ten time steps. It may be observed from the Fig. 5, that at all five data points the coverage for P2 is highest when she uses the SPIM algorithm. This is not surprising as the stated goal of SPIM is to maxi-

200 Number of Initial Adopters of B 160

120
80 40

Degree-Closeness Degree-Degree Degree-SPIM Degree-GWMI
0 20 40 60 80 100 Number of Initial Adopters of A 120

0

Figure 3: Number of initial adopters of B for different values of |IA |

350 300 Coverage of A 250 200 150 100 20 40 60 Number of Initial Adopters of A 80 Degree-Degree Degree-Closeness Degree-GWMI Degree-SPIM

Figure 4: Expected number of nodes adopting A after 10 propagation steps

mize P2 's coverage (influence). However, this figure may be somewhat misleading because it does not provide the information pertaining to the number of initial adopters required by the SPIM algorithm to achieve the higher coverage. By its stated objective, the number of initial adopters required by GWMI to defeat P1 cannot be higher than the the number of initial adopters required by SPIM. Once this is factored in, and we compute the coverage per initial adopter, we find that the coverage per initial adopter of the SPIM algorithm is very close to that of the GWMI algorithm. This is shown in Fig. 6. From Fig. 3 it is clear that the node degree and centrality based heuristics and the SPIM algorithm require a larger number of initial adopters of B to beat A than is needed by the GW M I algorithm. While this is a negative aspect of SPIM (cost), it also has a positive aspect in the sense that at the end of ten time steps, it also secures a larger coverage for B (benefit). We compute the additional benefit provided by the additional initial adopters. Let IB (X ) be the smallest set of initial adopters of B that is required by algorithm X to defeat A and 2(X ) be the expected number of nodes that adopt B after D propagation steps. Here X can be node-degree or centrality based heuristic or the SPIM algorithm. In the case, (2(X ) - 2(GW M I ) ) indicates the additional benefit and (|IB (X ) | - |IB (GW M I ) |) indicates the additional cost. In this case, (2(X ) - 2(GW M I ) )/(|IB (X ) | - |IB (GW M I ) |) indicates the average market share gain of B with each additional initial adopter when using algorithm

350 300 Coverage of B 250
Extended Benefit of B per Additional Initial Adopter

0.4

Degree-Degree Degree-Closeness Degree-GWMI Degree-SPIM

0.3 0.2

Degree-Degree Degree-Closeness Degree-SPIM

200
150 100 20 40 60 Number of Initial Adopters of A 80

0.1 0 20 40 60 Number of Initial Adopters of A 80

Figure 5: Expected number of nodes adopting B after 10 propagation steps
9 8 7 6 5 4 3 Degree-Degree Degree-Closeness Degree-GWMI Degree-SPIM

Figure 8: Extended benefit that B can capture per additional initial adopter with respect to GW M I but also (1(GW M I ) - 1(X ) ). It introduces a notion of extended benefit by combining these two factors in the following way: (2(X ) - 2(GW M I ) ) - (1(GW M I ) - 1(X ) ). With this notion of extended benefit, ((2(X ) - 2(GW M I ) ) + (1(GW M I ) - 1(X ) )) |IB (X ) | - |IB (GW M I ) | indicates the average market share gain of B with each additional initial adopter when using algorithm X . The Fig. 8 depicts the results for the heuristics and SPIM. It may be observed from Fig. 7 that when extended benefit is considered, the average market share gain of B with each additional initial adopter diminishes even more drastically with increase of the number of initial adopters of A, when it uses the SPIM algorithm. Moreover, the gain of each additional initial adopter is smaller than 1 and implies that the additional adopter is not worth its cost. In the second set of experiments we investigate different strategies for selection of initial adopters of A when P2 uses GW M I . The strategies that we consider for selection of initial adopters of A includes the greedy algorithm proposed in [11] and heuristics based on node degree and closeness centrality. In these experiments WPM is used as diffusion model and D = 10. Fig. 9 depicts the results of these experiments. We observe that the closeness-centrality based heuristic performs poorly in comparison to other two algorithms. This is true because the number of initial adopters of B that it needs to defeat A's overall influence (coverage) is much smaller than the size of initial adopters of A. More specifically, for closeness-centrality based heuristic, for |IA | values greater than 60, the number of initial adopters of B is less than 50% of |IA |. This set of results show that if the influence maximization algorithm (IM) proposed in [11] is used for the selection of IA , it forces P2 to select a large set for IB in order to be able to defeat P1 within D time steps.

Coverage of B per Initial Adopter of B

2 1 0
20 40 60 Number of Initial Adopters of A 80

Figure 6: Expected number of nodes adopting B per initial adopter of B after 10 propagation steps

X . The Fig. 7 depicts the results for the heuristics and SPIM. The negative gains are not shown. It may be observed from Fig. 7 that the average market share gain of B with each additional initial adopter diminishes with increase of the number of initial adopters of A, when it uses the SPIM algorithm. While the stated objective of P2 is to have a larger market share than P1 with the fewest number of initial adopters, it may also have two other unstated objectives - (i) to have a large 2(X ) and (ii) a small 1(X ) for all X (1(X ) be the expected number of nodes that adopt A after D time steps). Therefore while considering the benefit of the additional initial adopters, we can consider not only (2(X ) - 2(GW M I ) )

Average Increase in Market Share of B per additional initial adopter

12
Degree-Degree 10 8 Degree-Closeness Degree-SPIM

6
4

8.

CONCLUSION

2
0 20 40 60 Number of Initial Adopters of A 80

Figure 7: Average market share increase that innovation B can capture per additional initial adopter with respect to GW M I

In this paper we have introduced a new influence propagation problem in an adversarial setting where the goal of the second player is to defeat the first within D time steps and least cost, measured in terms of the number of seed nodes. Considering two different influence propagation models, we provided the NP-Hardness proof for the problem and an approximation algorithm with a tight performance bound. In addition, we evaluated the performance of the approximation algorithm with collaboration network data.

120
Number of Initial Adopters of B 100 80 60 40 20 0 0

Degree-GWMI IM-GWMI Closeness-GWMI

20

40 60 80 Number of Initial adopters of A

100

120

Figure 9: Size of initial adopters of B for different values of |IA | We can envisage at least two new directions of research with this problem. In the first direction, P2 is not aware of P1 's choice. In the second direction, back and forth transition of the nodes between two competing products is allowed.

9.

ACKNOWLEDGMENTS

The research was supported in part by a grant to the Center for the Study of Religion and Conflict at Arizona State University (N00014-09-1-0815). The award was funded through the Office of the Secretary of Defense Minerva program, and managed out of the Office of Naval Research. The content is solely the responsibility of the authors and does not necessarily represent the views of the Office of Naval Research. In addition, it was also supported in part by the DTRA grant HDTRA1-09-1-0032 and the AFOSR grant FA955009-1-0120.

10.

REFERENCES

[1] S. Bhagat, A. Goyal, and L. V. Lakshmanan. Maximizing product adoption in social networks. In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM '12, pages 603­612, 2012. [2] S. Bharathi, D. Kempe, and M. Salek. Competitive influence maximization in social networks. In Proceedings of the 3rd international conference on Internet and network economics, WINE'07, pages 306­311, 2007. [3] T. Carnes, C. Nagarajan, S. M. Wild, and A. van Zuylen. Maximizing influence in a competitive social network: a follower's perspective. In Proceedings of the ninth international conference on Electronic commerce, ICEC '07, pages 351­360, 2007. [4] W. Chen, Y. Wang, and S. Yang. Efficient influence maximization in social networks. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '09, pages 199­208, 2009. [5] K. Dave, R. Bhatt, and V. Varma. Modelling action cascades in social networks. 2011.

[6] P. Domingos and M. Richardson. Mining the network value of customers. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, KDD '01, pages 57­66, 2001. [7] A. Goyal, F. Bonchi, L. V. S. Lakshmanan, and S. Venkatasubramanian. Approximation analysis of influence spread in social networks. arXiv:1008.2005v4, 2011. [8] H. Habiba, Y. Yu, T. Y. Berger-Wolf, and J. Saia. Finding spread blockers in dynamic networks. In Proceedings of the Second international conference on Advances in social network mining and analysis, SNAKDD'08, pages 55­76, 2010. [9] G. Istrate, M. V. Marathe, and S. S. Ravi. Adversarial models in evolutionary game dynamics. In Proceedings of the twelfth annual ACM-SIAM symposium on Discrete algorithms, SODA '01, pages 719­720, 2001. [10] Q. Jiang, G. Song, C. Gao, Y. Wang, W. Si, and K. Xie. Simulated annealing based influence maximization in social networks. 2011. [11] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of influence through a social network. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD '03, pages 137­146, 2003. [12] H. Li, S. S. Bhowmick, and A. Sun. Casino: towards conformity-aware social influence analysis in online social networks. In Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM '11, pages 1007­1012, 2011. [13] M. Mathioudakis, F. Bonchi, C. Castillo, A. Gionis, and A. Ukkonen. Sparsification of influence networks. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD '11, pages 529­537, 2011. [14] M. Newman. http://networkdata.ics.uci.edu/data/hep-th/. [15] M. E. J. Newman. The structure of scientific collaboration networks. Proceedings of the National Academy of Sciences of the United States of America, 98(2):404­409, 2001. [16] M. A. Nowak, C. E. Tarnita, and T. Antal. Evolutionary dynamics in structured populations. Philosophical Transactions of the Royal Society B: Biological Sciences, 365(1537):19­30, 2010. [17] Y. Wang, G. Cong, G. Song, and K. Xie. Community-based greedy algorithm for mining top-k influential nodes in mobile social networks. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD '10, pages 1039­1048, 2010. [18] S. Wasserman and K. Faust. Social Network Analysis: Methods and Applications. Number 8 in Structural analysis in the social sciences. Cambridge University Press, 1 edition, 1994.

On the Entity Hardening Problem in Multi-layered Interdependent Networks
Joydeep Banerjee, Arun Das, Chenyang Zhou, Anisha Mazumder and Arunabha Sen
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering Arizona State University Tempe, Arizona 85287 Email: {joydeep.banerjee, arun.das, czhou24, anisha.mazumder, asen}@asu.edu Abstract--The power grid and the communication network are highly interdependent on each other for their well being. In recent times the research community has shown significant interest in modeling such interdependent networks and studying the impact of failures on these networks. Although a number of models have been proposed, many of them are simplistic in nature and fail to capture the complex interdependencies that exist between the entities of these networks. To overcome the limitations, recently an Implicative Interdependency Model that utilizes Boolean Logic, was proposed and a number of problems were studied. In this paper we study the "entity hardening" problem, where by "entity hardening" we imply the ability of the network operator to ensure that an adversary (be it Nature or human) cannot take a network entity from operative to inoperative state. Given that the network operator with a limited budget can only harden k entities, the goal of the entity hardening problem is to identify the set of k entities whose hardening will ensure maximum benefit for the operator, i.e. maximally reduce the ability of the adversary to degrade the network. We show that the problem is solvable in polynomial time for some cases, whereas for others it is NP-complete. We provide the optimal solution using ILP, and propose a heuristic approach to solve the problem. We evaluate the efficacy of our heuristic using power and communication network data of Maricopa County, Arizona. The experiments show that our heuristic almost always produces near optimal results.

arXiv:1412.6686v1 [cs.NI] 20 Dec 2014

that may exist between network entities, such as when entity ai is operational, if entities (i) bj and bk and bl are operational, or (ii) bm and bn are operational, or (iii) bp is operational. Graph based interdependency models proposed in the literature such as [3], [4], [5], [10], [6], [7] including [1], [2] cannot capture such complex interdependency involving both conjunctive and disjunctive terms between entities of multi-layer networks. To overcome these limitations, an Implicative Interdependency Model that utilizes Boolean Logic, was recently proposed in [9], and a number of problems including computation of K most vulnerable nodes [9], root cause of failure analysis [11], and progressive recovery from failures [12], were studied using this model. In this paper we study the "entity hardening" problem in the interdependent power-communication network using the Implicative Interdependency Model (IIM). By "entity hardening", we imply the ability of the network operator to ensure that an adversary (be it Nature or human), cannot take a network entity from an operative (operational) to an inoperative (failed) state. We assume that the adversary is clever and is capable of identifying the most vulnerable entities in the network that causes maximum damage to the interdependent system. However, the adversary does not have an unlimited budget and has the resources to destroy at most K entities of the interdependent network. The network operator is also aware of adversary's target entities for destruction. Since we assume that once an entity is "hardened" by the network operator it cannot be destroyed by the adversary, if all K targets of the adversary are hardened by the network operator, then the adversary cannot induce any failure in the network. However, if due to resource limitations the network operator is able to strengthen only k entities, where k < K, these k entities have to be carefully chosen. The goal of the entity hardening problem is to identify the set of k entities whose hardening will ensure maximum benefit for the operator, i.e. maximally reduce the ability of the adversary to degrade the network. We classify the entity hardening problem into four different cases depending on the nature of the interdependency relationships. We show that the first case can be solved in polynomial time, and all other cases are shown to be NP-complete. We provide an inapproximability result for the second case, an approximation algorithm for the third case, and a heuristic for the fourth (general) case. We evaluate the efficacy of our heuristic using power and communication network data of Maricopa County, Arizona. The experiments show that our

I.

I NTRODUCTION

The critical infrastructures of a nation form a complex symbiotic ecosystem where individual infrastructures are heavily interdependent on each other for being fully functional. Two such critical systems that rely heavily on each other for their well being are the power and communication network infrastructures. For instance, power grid entities such as SCADA systems, that are used to remotely operate power generation units, receive their control commands over the communication network infrastructure, while communication network entities such as routers and base stations are inoperable without electric power. Thus, failure introduced in the system either by Nature (hurricanes), or man (terrorist attacks), can trigger further failures in the system due to interdependencies between the entities of the two infrastructures. Although a number of models have been proposed for modeling and analysis of interdependent multi-layered networks [1], [2], [3], [4], [5], [6], [7], [8], many of these models are simplistic in nature and fail to capture the complex interdependencies that exists between the entities of these networks. As noted in [9], these models fail to model complex interdependencies

2

heuristic almost always produces near optimal results. The paper is organized as follows, the IIM model is presented in Section II, in Sections III and IV we formally state the entity hardening problem and analyze its computational complexity, Section V outlines the optimal and heuristic solutions to the problem, Section VI shows the experimental results, and finally Section VII concludes this paper. II. I NTERDEPENDENCY M ODEL

III.

P ROBLEM F ORMULATION

We now present an overview of the underlying IIM interdependency model [9]. IIM uses Boolean Logic to model the interdependencies between network entities, these interdependent relationships are termed as Implicative Interdependency Relations (IDRs). We represent this interdependent network setting as I (A, B, F (A, B )), where sets A and B are the power and communication network entities respectively, and F (A, B ) is the set of dependency relations, or IDRs. Table I represents a sample interdependent network I (A, B, F (A, B )), where A = {a1 , a2 , a3 , a4 }, B = {b1 , b2 , b3 } and F (A, B ) is the set of IDRs (dependency relations) between the entities of A and B . In this example, the IDR b1  a1 a3 + a2 implies that entity b1 is operational when both the entities a1 and a3 are operational, or entity a2 is operational. The conjunction of entities, such as a1 a3 , is also referred to as a minterm.
Power Network a1  b1 b2 a2  b1 + b2 a3  b1 + b2 + b3 a4  b1 + b3 Comm. Network b1  a1 a3 + a2 b2  a1 a2 a3 b3  a1 + a2 + a3 --

Before we make a formal statement of the entity hardening problem in the IIM setting, we explain it with the help of an example. Consider an interdependent system as outlined in the IDR set shown in Table I. It may be easily checked that when the adversary budget is K= 2, the most vulnerable entities of this system are {a2 , b3 }. If the network operator doesn't harden any one of the entities a2 or b3 , then in this example all the network entities eventually fail, as seen from the fault propagation in Table II. When the network operator chooses to harden both a2 and b3 then none of the entities in the network fail if the adversary restricts the attack only to the two most vulnerable entities of the network, which in this example happens to be {a2 , b3 }. If the network operator has resources to harden only one entity and the operator chooses to harden a2 , the destruction of b3 by the adversary will eventually lead to the failure of no other entities of the network, as shown in Table III(a). If on the other hand, the network operator chooses to harden b3 , destruction by the adversary of a2 will eventually lead to the failure of the entities {a2 , b2 , a1 , b1 } as shown in Table III(b). Clearly in this scenario the operator should harden a2 instead of b3 . Definition: Kill Set of a set of Entities(S ): The kill set of a set of entities S , is the set of all entities that will eventually fail due to failure of S and the interdependencies between the entities of the network as given by the set of IDR's. The kill set of a set of entities S is denoted by KillSet(S ). It may be noted that the search for k entities to be hardened is restricted to the KillSet(S ), where S is the set of K most vulnerable entities in the network, because hardening any entity not in KillSet(S ) does not provide any benefit to the network operator. In this study we also assume that the set of K most vulnerable entities in the network is unique.
Entities 0 a1 a2 a3 a4 b1 b2 b3 0  0 0 0 0 1 Time Steps (t) 1 2 3 0  0 0 0 0 1 0  0 0 0 0 1 0  0 0 0 0 1 Entities 4 0  0 0 0 0 0 a1 a2 a3 a4 b1 b2 b3 0 0 1 0 0 0 0  Time Steps (t) 1 2 3 0 1 0 0 0 1  1 1 0 0 0 1  1 1 0 0 1 1  4 1 1 0 0 1 1 

TABLE I: Implicative Interdependency Relations of a sample network

Given a set of inoperable (failed) entities, a time stepped failure cascade can be derived from the dependency relationships outlined in the IDR set. For example, for the interdependent network outlined in Table I, Table II shows the failure propagation when entities {a2 , b3 } fail at the initial time step (t = 0). It may be noted that the model assumes that dependent entities fail immediately in the next time step, for example, when {a2 , b3 } fail at t = 0, b2 fails at t = 1 as b2 is dependent on a2 for its survival. The system reaches a steady state when the failure propagation process stops. In this example, when {a2 , b3 } fail at t = 0, the steady state is reached at time step t = 4.
Entities 0 a1 a2 a3 a4 b1 b2 b3 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 Time Steps (t) 2 3 4 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 5 1 1 1 1 1 1 1 6 1 1 1 1 1 1 1

(a) Entity a2 is hardened

(b) Entity b3 is hardened

TABLE III: Failure cascade propagation with entity hardening. Entities {a2 , b3 } are attacked at time step t = 0. A value of 1 denotes entity failure, 0 otherwise.  denotes a hardened entity.

TABLE II: Failure cascade propagation when entities {a2 , b3 } fail at time step t = 0. A value of 1 denotes entity failure, and 0 otherwise

We now proceed to formulate the entity hardening problem formally. Given an interdependent network system I (A, B, F (A, B )), and the set of K most vulnerable entities of the system A  B  , where A  A and B   B : The Entity Hardening (ENH) problem INSTANCE: Given: (i) An interdependent network system I (A, B, F (A, B )), where the sets A and B represent the entities of the two networks, and F (A, B ) is the set of IDRs. (ii) The set of K most vulnerable entities of the system A  B  , where A  A and B   B (iii) Two positive integers k, k < K and EF .

A primary consideration for using this model is the accurate formulation of the IDRs that is representative of the underlying physical power and communication network infrastructures. This can either be done by careful analysis as done in [8], or by consultation with experts of these infrastructures. We utilize IIM to model the interdependency between the two networks and analyze the entity hardening problem in this setting.

3

QUESTION:Is there a set of entities H = A  B  , A  A, B   B, |H|  k , such that hardening H entities results in no more than EF entities to fail after entities A  B  fail at time step t = 0. We note some of the assumptions for the ENH problem: First, we assume that once an entity is hardened, it is always operational and does not fail at any time step of the observation, even when the entity is part of the K most vulnerable entities. Second, we assume that k < K, as otherwise the selection of K entities for hardening ensures that no entities fail at all. Finally, as noted earlier, we assume that the set of K most vulnerable entities in the network is unique. We now proceed to analyze the computational complexity of the ENH problem. IV. C OMPUTATIONAL C OMPLEXITY A NALYSIS

[9]. So with two entities {xi , xj }  A  B  and Cxi  Cxj = Cxj i.e, Cxj  Cxi , if xi is hardened it prevents the failure of Cxi - Cxj entities (provided that none of the entities in Cxi - Cxj - {xi } are in A  B  ). With this assertion, for an entity xi  A  B  , steps 4-7 of Algorithm 1 finds the actual entities for which failure is prevented by hardening xi . The set D = {Dx1 , Dx2 , ..., DxK } comprises of these set of entities for each hardened entity xi . To prove that Algorithm 1 finds the optimal solution we make the following two assertions: First, consider any two sets Dxi and Dxj . It is implied from step 6 of Algorithm 1 that / A  B  is Dxi  Dxj = . Second, consider an entity xp  hardened. If xp fails when entities in A  B  fails initially then it would belong to some set Dxi . Thus hardening xp results in preventing the failure of entities that is a proper subset of Dxi . Hence the entities to be hardened must belong to A  B  only. Owing to the two assertions it directly follows that with a given budget k , hardening k highest cardinality sets from the set D ensures prevention of failure for the maximum number of entities. B. Case II: Problem Instance with One Minterm of Arbitrary Size The IDRs of Case II have a single minterm of arbitrary p size. This can be represented as xi  j =1 yj , where xi and yj are entities of network A(B ) and B (A) respectively and the size of the minterm is p. The Entity Hardening problem with respect to Case II is NP-complete and is proved in Theorem 2. An inapproximability proof for this case of the problem is given in Theorem 3 Theorem 2. The Entity Hardening problem for Case II is NP Complete Proof: The Entity Hardening problem for case II is proved to be NP complete by giving a reduction from the Densest pSubhypergraph problem [13], a known NP-complete problem. An instance of the Densest p-Subhypergraph problem includes a hypergraph G = (V, E ), a parameter p and a parameter M . The problem asks the question whether there exists a set of vertices |V  |  V and |V  |  p such that the subgraph induced with this set of vertices has at least M hyperedges. From an instance of the Densest p-Subhypergraph problem we create an instance of the ENH problem in the following way. For each vertex vi and each hyperedge ej an entity bi and aj are added to the set B and A respectively. For each hyperedge ej with ej = {vm , vn , vq } (say) an IDR of form aj  bm bn bq is created. It is assumed that the value of K is set of |V |. The values of k and EF are set to p and |V | + |E | - p - M (where |A| = |V | and |B | = |E |) respectively. In the constructed instance only entities of set A are dependent on entities of set B . Additionally the dependency for an entity ai consists of conjunction of entities in set B . Hence for an entity ai  A to fail, either it itself has to fail initially or all entities to which ai is dependent on has to fail. It is to be noted that the entities in set B has no induced failure i.e., there is no cascade. Following from this assertion, with K = p, the solution A =  and B  = B would fail all entities in set A  B . Moreover this is the single unique solution to the problem instance. This is because by including one entity

For an interdependent network I (A, B, F (A, B )) the IDRs can be represented in four different forms. We analyze the computational complexity of the ENH problem for each of these cases separately. A. Case I: Problem Instance with One Minterm of Size One The IDRs of Case I have a single minterm of size 1. This can be represented as xi  yj , where xi and yj are entities of network A(B ) and B (A) respectively. We show that the ENH problem for Case I can be solved optimally in polynomial time. Algorithm 1: Entity Hardening Algorithm for systems with Case I type interdependencies
Data: An interdependent network I (A, B, F (A, B )), set of K most vulnerable entities A  B  , A  A, B   B , hardening budget k and a set H = . Result: Set of hardened entities H. begin For each entity xi  (A  B  ) compute the set of kill sets C = {Cx1 , Cx2 , ..., CxK }, where Cxi = KillSet(xi ) ; Create a copy D = {Dx1 , Dx2 , ..., DxK } of set C ; for (i=1; i  K; i++) do for (j=1, j = i; j  K; j++) do if Cxj  Cxi then Dxi  Dxi \ Dxj ; Choose the top k sets from D with highest cardinality ; For each of the Dxi  D sets chosen in Step 8, H  H  xi ; return H

1 2 3 4 5 6 7

8 9 10

Theorem 1. Algorithm 1 solves the Entity Hardening problem for Case I optimally in polynomial time. Proof: It is shown in [9] that the kill set for all entities in the interdependent network can be computed in O(n3 ) where n = |A| + |B |, thus computing the kill sets for K entities takes O(Kn2 ). Step 4-7 of the algorithm runs in O(K2 ). Choosing the k highest cardinality sets can be found using any standard sorting algorithm in O(Klog (K)). Hence Algorithm 1 runs in O(Kn2 ). For two kill sets Cxi and Cxj it can be shown that either Cxi  Cxj =  or Cxi  Cxj = Cxi or Cxi  Cxj = Cxj

4

ai in the initial failure set would result in not failing at least one entity bj for a given budget K = p. Hence it won't fail the entire set of entities in A  B . If an entity in set A is hardened then it would have no effect in failure prevention of any other entities. Whereas hardening an entity bm  B might result in failure prevention of an entity ai  A with IDR aj  bm bn bq provided that entities bn , bq are also defended. With k = p (and K  |V | = |B |) it can be ensured that entities to be defended are from set B  . To prove the theorem consider that there is a solution to the Densest p-Subhypergraph problem. Then there exist p vertices which induces a subgraph which has at least M hyperedges. Hardening the entities bi  B  for each vertex vi in the solution of the Densest p-Subhypergraph problem would then ensure that at least M entities in set A are protected from failure. This is because the entities in set A for which the failure is prevented corresponds to the hyperedges in the induced subgraph. Thus the number of entities that fail after hardening p entities is at most |V | + |E | - p - M , solving the ENH problem. Now consider that there is a solution to the ENH problem. As previously stated, the entities to be hardened will always be from set B  . So defending p entities from set B  would result in failure prevention of at least M entities in set A such that EF  |V | + |E | - p - M . Hence, the vertex induced subgraph would have at least M hyperedges when vertices corresponding to the entities hardened are included in the solution of the Densest p-Subhypergraph problem, thus solving it. Theorem 3. For an interdependent network I (A, B, F (A, B )) with n = |A  B | and F (A, B ) having IDRs of form Case II, it is hard to approximate the ENH problem within a factor of 1 for some  > 0. log(n)
2

set S we add an entity bi in set B . For all subsets in S , say Sp , Sm , Sn , which has the element xi there is an IDR of form ai  bm + bn + bl . The values of positive integers k and EF are set to M and m - M respectively. It is assumed that the value of K = m. With similar reasoning as that of Case II it can be shown that for K = m the maximum number of node failures (i.e. failure of all entities in A  B ) would occur if A =  and B  = B . This is also the single unique solution to the problem instance. The constructed instance also ensures that the entities to be hardened are from set B  (A not considered as it is equal to ). This is because protecting an entity ai  A would only result in prevention of its own failure whereas protecting an entity bj  B would result in failure prevention of its own and all other entities in set A for which it appears in its IDR. To begin with the proof, consider that there is a solution to the Set Cover problem. Then there exist M subsets (or elements in set S ) whose union results in the set S . Hardening the entities in set B corresponding to the subsets selected would ensure that all entities in set A are prevented from failure. This is because for the dependency of each entity ai  A there exist at least one entity (in set B ) that is hardened. Hence the number of entities that fails after hardening is m-M which is equal to EF , thus solving the ENH problem. Now, consider that there is a solution to the ENH problem. As discussed above the entities to be hardened should be from set B  . To achieve EF = m - M with k = M , no entities in the set A must fail. Hence for each entity ai  A at least one entity in set B that appears in its IDR has to be hardened. Thus, it directly follows that the union of subsets in set S corresponding to the entities hardened is equal to the set S , solving the Set Cover Problem. 1) Approximation Scheme for Case 3: In this subsection we provide an approximation algorithm for Case 3 of the problem. For an interdependent network I (A, B, F (A, B )) with the initial failed set of entities as A  B  we define Protection Set of each entity as follows. Definition: For an entity xi  A  B the Protection Set is defined as the entities that would be prevented from failure by hardening the entity xi when all entities in A  B  fails initially. This is represented as P (xi |A  B  ). The Protection Set of each entity can be computed in O((n + m)2 ) where n and m are the number of entities and number of minterms respectively in an interdependent network I (A, B, F (A, B )) . Theorem 5. For two entities xi , xj  A  B , P (xi |A  B  )  P (xj |A  B  ) = P (xi , xj |A  B  ) when IDRs are of form Case III. Proof: Assume that defending two entities xi and xj would result in preventing failure of P (xi , xj |A  B  ) entities with |P (xi |A  B  )  P (xj |A  B  )| < |P (xi , xj |A  B  )|. Then there exist at least one entity xp  / P (xi |A  B  )  P (xj |A  B  ) such that it's failure is prevented only if xi and xj is protected together. So two entities xm and xn (with xm  P (xi |A B  ) and xn  P (xj |A B  ) or vice versa) have to be

Proof: From Theorem 2, Densest p-Subhypergraph problem has been shown to be a special case of the ENH problem with IDRs of form Case II. Densest p-Subhypergraph problem is proved to be inapproximable within a factor of log1 2 (n) ( > 0) in [13]. Hence the theorem follows. C. Case III: Problem Instance with an Arbitrary Number of Minterm of Size One The IDRs of Case III have arbitrary number of minterm of size 1. This can be represented as xi  p q=1 yq , where xi and yq are entities of network A(B ) and B (A) respectively and the number of minterms are p. The ENH problem with respect to Case III is NP-complete and is proved in Theorem 4. Theorem 4. The ENH problem for Case III is NP Complete Proof: The ENH problem for case III is proved to be NP complete by giving a reduction from the Set Cover Problem, a well known NP-complete problem. An instance of the Set Cover problem includes a set S = {x1 , x2 , ..., xn }, a set S = {S1 , S2 , ..., Sm } where Si  S and a positive integer M . The problem asks the question whether there exists at most M subsets from set S whose union would result in the set S . From an instance of the set cover problem we create an instance of the ENH problem in the following way. For each element xi in set S we add an entity ai in set A. For each subset Si in

5

present in the IDR of xp . As the IDRs are of form Case III so if any one of xm or xn is protected then xp is protected, hence a contradiction. On the other way round P (xi , xj |A  B  ) contains all entities which would be prevented from failure if xi or xj is defended alone. So it directly follows that |P (xi |A  B  )  P (xj |A  B  )| > |P (xi , xj |A  B  )| is not possible. Hence the theorem holds. Theorem 6. There exists an 1 - 1 e approximation algorithm that approximates the ENH problem for Case III. Proof: The approximation algorithm is constructed by modeling the problem as Maximum Coverage problem. An instance of the maximum coverage problem consists of a set S = {x1 , x2 , ..., xn }, a set S = {S1 , S2 , ..., Sm } where Si  S and a positive integer M . The objective of the problem is to find a set S   S and |S  |  M such that Si S Si is maximized. For a given initial failure set A  B  with |A | + |B  |  K, let P (xi |A  B  ) denote the protection set for each entity xi  A  B . We construct a set S = A  B and for each entity xi a set Sxi  S such that Sxi = P (xi |A  B  ). Each set Sxi is added as an element of a set S . The conversion of the problem to Maximum Coverage problem can be done in polynomial time. By Theorem 5 defending a set of entities X  S would result in failure prevention of xi X Sxi entities. Hence, with the constructed sets S and S and a positive integer M (with M = k ) finding the Maximum Coverage would ensure the failure protection of maximum number of entities in A  B . This is same as the ENH problem of Case III. As there exists an 1 - 1 e approximation algorithm for the Maximum Coverage problem hence the theorem holds. D. Case IV: Problem Instance with an Arbitrary Number of Minterms of Arbitrary Size The IDRs of Case IV have arbitrary number of minterm of arbitrary size. This can be represented as xi  qj1 p j2 =1 yj2 , where xi and yj2 are entities of network j1 =1 A(B ) and B (A) respectively and there are p minterms each of size qj1 . Theorem 7. The Entity Hardening problem for Case IV is NP Complete Proof: Case II and Case III are special cases of Case IV. Hence following from Theorem 2 and Theorem 4 the computational complexity of the Entity Hardening problem is NP-complete in Case IV. V. S OLUTIONS TO
THE

It is to be noted that the maximum number cascading steps is upper bounded by |A| + |B | - 1 = m + n - 1. The objective function can now be formulated as follows:
m n

min
i=1

xi(m+n-1) +
j =1

yj (m+n-1)

(1)

The objective in (1) minimizes the number of entities failed after the cascading failure with the respective constraints for the Entity Hardening problem as follows:
n m

Constraint Set 1:
i=1

qxi +
j =1

qyj = k , with qxi , qyj  [0, 1].

If an entity xi (yj ) is defended then qxi = 1 (qyj = 1) and 0 otherwise. Constraint Set 2: xi0  gi - qxi and yi0  hi - qyi . This constraint implies that only if an entity is not defended and gi (hi ) is 1 then the entity will fail at the initial time step. Constraint Set 3: xid  xi(d-1) , d, 1  d  m + n - 1, and yid  yi(d-1) , d, 1  d  m + n - 1, in order to ensure that for an entity which fails in a particular time step would remain in failed state at all subsequent time steps. Constraint Set 4: Modeling of the constraint to capture the cascade propagation for IIM is similar to the constraints established in [9]. A brief presentation of this constraint is provided here. Consider an IDR ai  bj bp bl + bm bn + bq of type Case IV. The following steps are enumerated to depict the cascade propagation: Step 1: Replace all minterms of size greater than one with a variable. In the example provided we have the transformed minterm as ai  c1 + c2 + bq with c1  bj bp bl and c2  bm bn (c1 , c2  {0, 1}) as the new IDRs. Note that after transformation, the original IDR is in the form of Case III and the introduced IDRs are in the form of Case II. Step 2: For each variable c, a constraints is added to capture the cascade propagation. Let N be the number of entities in the minterm on which c is dependent. In the example for the variable c1 with IDR c1  bj bp bl , constraints y +y d-1) +yl(d-1) c1d  j(d-1) p(N and c1d  yj (d-1) + yp(d-1) + yl(d-1) d, 1  d  m + n - 1 are introduced (with N = 3 in this case). If IDR of an entity is already in form of Case II, i.e.,ai  bj bp bl then constraints xid  yj(d-1) +yp(d-1) +yl(d-1) - qxi and xid  yj (d-1) + yp(d-1) + N yl(d-1) d, 1  d  m + n - 1 are introduced (with N = 3). These constraints satisfies that if the entity xi is hardened initially then it is not dead at any time step. Step 3: Let M be the number of minterms in the transformed IDR as described in Step 1. In the given example with IDR ai  c1 + c2 + bq constraints of form xid  c1(d-1) + c2(d-1) + yq(d-1) - (M - 1) - qxi and xid  c1(d-1) +c2(d-1) +yq(d-1) d, 1  d  m + n - 1 are introduced. M These constraints ensures that even if all the minterms of xi has at least one entity in dead state then it will be alive if the entity is hardened initially. For all IDRs of type Case I and Case III, the constraint discussed in this step is used.

E NTITY H ARDENING P ROBLEM

A. Optimal Solution using Integer Linear Programming We propose an Integer Linear Program (ILP) that solves the Entity Hardening problem optimally. Let [G, H ] with G = {g1 , g2 , ..., gn } and H = {h1 , h2 , ..., hm } denote the entities in set A and B respectively with hi = 0 (gj = 0) if entity ai (bj ) is alive and hi = 1 (gj = 1) otherwise. Given an integer k let [G, H ] be the solution (with value of 1 corresponding to entities failed initially) that cause maximum number of entity failure. Two variables xid and yjd are used in the ILP with xid = 1 (yjd = 1), when entity ai  A (bj  B ) is in a failed state at time step d, and 0 otherwise. The number of entities to be defended is considered to be k .

6

B. Heuristic In this subsection we provide a greedy heuristic solution to the Entity Hardening problem. For an interdependent network I (A, B, F (A, B )) with the initial failed set of entities as A  B  , Protection Set of each entity has been defined in the approximation scheme of Case III. To design the heuristic we define Minterm Coverage Number of each entity in A  B as follows: Definition: For an entity xi  A  B the Minterm Coverage Number is defined as the number of minterms that can be removed from F (A, B ) without affecting the cascading process by hardening the entity xi when all entities in A  B  fails initially. This is represented as M (xi |A  B  ). Similar to the computation of Protection Set the Minterm Coverage Number of each entity can be computed in O((n + m)2 ). With these definitions the heuristic is given in Algorithm 2. The algorithm takes in as input an interdependent network I (A, B, F (A, B )) with S = A  B . Step 4-5 is done to reduce the search space as it directly follows that the set of entities in Q wouldn't effect the hardening process. In each iteration of the while loop an entity xd is greedily selected which when hardened would prevent failure of maximum number of entities. This ensures that at each step the number of entities failed is minimized. In case of a tie, among all entities involved in the tie, the entity having the highest Minterm Coverage Number is included in the solution. This gives a higher priority to the entity which when hardened, has more impact on failure minimization in subsequent iterations of the while loop. The interdependent network I (A, B, F (A, B )) is updated in steps 13-16 of the algorithm. This takes into account the effect of hardening an entity in the current iteration on entities hardened in the following iterations. Run Time Analysis of Algorithm 2: For this analysis we consider n to be the total number of entities and m to be the total number of minterms. Updates in step 4 can be done in O(m) and step 5 in O(n). The while loop iterates for k times. In each iteration of the while loop step 7 and step 8 takes at most O((n + m)2 ) and O(nlog (n)) time respectively. On branching in step 9, step 10 and step 11 takes O((n + m)2 ) and O(nlog (n)) time respectively. Updates in step 13 takes O(n) time and in step 14 takes O(n + m) time. Step 12, 16 and 17 runs in constant time. Hence Algorithm 2 runs in O(k (n + m)2 ) time. VI. E XPERIMENTAL R ESULTS

Algorithm 2: Heuristic Solution to the ENH Problem
Data: An interdependent network I (A, B, F (A, B )) (with S = A  B ), set of entities A  B  failed initially causing maximum failure in the interdependent network with |A | + |B  | = K and hardening budget k. Result: Set of hardened entities H. begin Initialize S   A  B  ; Initialize H = ; Update F (A, B ) as follows -- (a) let Q be the set of entities that does not fail on failing K entities, (b) remove IDRs corresponding to entities in set Q, (c) remove from minterm of entities not in set Q all entities which are in set Q ; Update S = S \ Q ; while (k entities are not hardened) do For each entity xi  S compute the Protection Set P (xi |S  ); Choose the entity xd with highest cardinality of the set |P (xd |S  )|; if (more than one entity has the same highest cardinality value) then For each such entity xj compute the Minterm Coverage Number M (xj |S  ) ; Choose the entity xd with highest Minterm Coverage Number. ; In case of a tie choose arbitrarily; Update S  S - P (xd |S  ); Update F (A, B ) by removing (i) IDRs corresponding to all entities in P (xd |S  ), and (ii) occurrence of these entities in IDRs of entities not in P (xd |S  ); if (xd  S  ) then Update S   S  - {xd }; Update H = H  xd ; return H ;

1 2 3 4

5 6 7 8 9 10 11 12 13 14

15 16 17 18

located within the geographic region formed the set A and B respectively. Each region was represented by an interdependent network I (A, B, F (A, B )). We use the IDR construction rules as defined in [9] to generate F (A, B ).

In this section we present the experimental results of the Entity Hardening problem by comparing the optimal solution computed using an ILP, and the proposed heuristic algorithm. The experiments were conducted on real world power grid data obtained from Platts (www.platts.com), and communication network data obtained from GeoTel (www.geo-tel.com) of Maricopa County, Arizona. The data consisted of 70 power plants and 470 transmission lines in the power network, and 2, 690 cell towers, 7, 100 fiber-lit buildings and 42, 723 fiber links in the communication network. We identified five nonintersecting geographical regions from the data set and labeled them from regions 1 through 5. For each of the regions, the entities of the power and communication network that were

In all of our simulations IBM CPLEX Optimizer 12.5 to solve ILPs and Python 3 for heuristic is used. To analyze the Entity Hardening problem the value of K was set to 8. The ILP in [9] was used to compute the K most vulnerable nodes in the network, and the set of failed entities due to the failure of the K entities was also computed. For the five regions, when the K = 8 most vulnerable nodes failed, the total number of failed entities in the network were 28, 23, 28, 28 and 27 respectively. With the K most vulnerable nodes and final set of failed nodes as input, the ILP and heuristic of the Entity Hardening problem are compared with k = 1, 3, 5, 7. The results of these simulations are shown in Figure 1. It is observed that the heuristic solution differs more from optimal at higher values of k (factor of 0.5 and 0.67 for Regions 1 and 3 respectively with k = 7). This is primarily because of the greedy nature of Algorithm 2. However on an average the heuristic solution differs by a factor of 0.13 from the optimal.

7

14 Number of entities failed 12 10 8 6 4 2 0 1 12

13

Number of entities failed

12 10 8 6 4 2 0 1 3 7 7

Number of entities failed

ILP solution Heuristic

14 13

13

14 ILP solution Heuristic 12 10 8 6 4 2 0 1 12

13

ILP solution Heuristic

8 6 5 3 1 3 5 7 Number of entities hardened 3

7 6 4 3 2 1 3 5 7 Number of entities hardened

3 1 1

3 5 7 Number of entities hardened

(a) Region 1
12 Number of entities failed 10 8 6 4 2 0 1 6 5 4 3 2 1 3 5 7 Number of entities hardened 11 11

(b) Region 2
8 Number of entities failed ILP solution Heuristic 7 6 5 4 3 2 1 0 1 3 5 7 Number of entities hardened 1 3 5 7

(c) Region 3
ILP solution Heuristic

(d) Region 4

(e) Region 5

Fig. 1: Comparison chart of the optimal solution (ILP) with the heuristic by varying number of entities hardened for each identified region

VII.

C ONCLUSION

In this paper we studied the entity hardening problem in multi-layer networks. We modeled the interdependencies shared between the networks using IIM, and formulated the the Entity Hardening problem in this setting. We showed that the problem is solvable in polynomial time for some cases, whereas for others it is NP-complete. We evaluated the efficacy of our heuristic using power and communication network data of Maricopa County, Arizona. Our experiments showed that our heuristic almost always produces near optimal results. R EFERENCES
[1] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin, "Catastrophic cascade of failures in interdependent networks," Nature, vol. 464, no. 7291, pp. 1025­1028, 2010. J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, "Networks formed from interdependent networks," Nature Physics, vol. 8, no. 1, pp. 40­48, 2011. J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, "Cascade of failures in coupled network systems with multiple support-dependence relations," Physical Review E, vol. 83, no. 3, p. 036116, 2011. V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and R. Setola, "Modelling interdependent infrastructures using interacting dynamical models," International Journal of Critical Infrastructures, vol. 4, no. 1, pp. 63­79, 2008. P. Zhang, S. Peeta, and T. Friesz, "Dynamic game theoretic model of multi-layer infrastructure networks," Networks and Spatial Economics, vol. 5, no. 2, pp. 147­178, 2005. M. Parandehgheibi and E. Modiano, "Robustness of interdependent networks: The case of communication networks and the power grid," arXiv preprint arXiv:1304.0356, 2013. D. T. Nguyen, Y. Shen, and M. T. Thai, "Detecting critical nodes in interdependent power networks for vulnerability assessment," 2013.

[2]

[3]

[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman, "Power grid vulnerability to geographically correlated failures-analysis and control implications," arXiv preprint arXiv:1206.1099, 2012. [9] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, "Identification of k most vulnerable nodes in multi-layered network using a new model of interdependency," in Computer Communications Workshops (INFOCOM WKSHPS), 2014 IEEE Conference on. IEEE, 2014, pp. 831­836. [10] J.-F. Castet and J. H. Saleh, "Interdependent multi-layer networks: Modeling and survivability analysis with applications to space-based networks," PloS one, vol. 8, no. 4, p. e60402, 2013. [11] A. Das, J. Banerjee, and A. Sen, "Root cause analysis of failures in interdependent power-communication networks," in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910­915. [12] A. Mazumder, C. Zhou, A. Das, and A. Sen, "Progressive recovery from failure in multi-layered interdependent network using a new model of interdependency," in Conference on Critical Information Infrastructures Security (CRITIS), 2014. Springer, 2014. [13] M. Hajiaghayi, K. Jain, K. Konwar, L. Lau, I. Mandoiu, A. Russell, A. Shvartsman, and V. Vazirani, "The minimum k-colored subgraph problem in haplotyping and dna primer selection," in Proceedings of the International Workshop on Bioinformatics Research and Applications (IWBRA). Citeseer, 2006.

[4]

[5]

[6]

[7]

2014 IEEE 15th International Conference on High Performance Switching and Routing (HPSR)

On Shortest Single/Multiple Path Computation
Problems in Fiber-Wireless (FiWi) Access Networks
Chenyang Zhouâ , Anisha Mazumderâ , Arunabha Senâ , Martin Reissleinâ  and Andrea Richaâ
â School of Computing, Informatics and Decision Systems Engineering
â  School of Electrical, Computer, and Energy Engineering

Arizona State University
Email: {czhou24, anisha.mazumder, asen, reisslein, aricha}@asu.edu

AbstractâFiber-Wireless (FiWi) networks have received considerable attention in the research community in the last few
years as they offer an attractive way of integrating optical and
wireless technology. As in every other type of networks, routing
plays a major role in FiWi networks. Accordingly, a number of
routing algorithms for FiWi networks have been proposed. Most
of the routing algorithms attempt to ï¬nd the âshortest pathâ
from the source to the destination. A recent paper proposed
a novel path length metric, where the contribution of a link
towards path length computation depends not only on that link
but also every other link that constitutes the path from the
source to the destination. In this paper we address the problem
of computing the shortest path using this path length metric.
Moreover, we consider a variation of the metric and also provide
an algorithm to compute the shortest path using this variation.
As multipath routing provides a number of advantages over
single path routing, we consider disjoint path routing with the
new path length metric. We show that while the single path
computation problem can be solved in polynomial time in both
the cases, the disjoint path computation problem is NP-complete.
We provide optimal solution for the NP-complete problem using
integer linear programming and also provide two approximation
algorithms with a performance bound of 4 and 2 respectively.
The experimental evaluation of the approximation algorithms
produced a near optimal solution in a fraction of a second.

I. I NTRODUCTION
Path computation problems are arguably one of the most
well studied family of problems in communication networks.
In most of these problems, one or more weight is associated
with a link representing, among other things, the cost, delay or
the reliability of that link. The objective most often is to ï¬nd
a least weighted path (or âshortest pathâ) between a speciï¬ed
source-destination node pair. In most of these problems, if a
link l is a part of a path P , then the contribution of the link
l on the âlengthâ of the path P depends only on the weight
w(l) of the link l, and is oblivious of the weights of the links
traversed before or after traversing the link l on the path P .
However, in a recent paper on optical-wireless FiWi network
[5], the authors have proposed a path length metric, where the
contribution of the link l on the âlengthâ of the path P depends
not only on its own weight w(l), but also on the weights
of all the links of the path P . As the authors of [5] do not
present any algorithm for computing the shortest path between
the source-destination node pair using this new metric, we
present a polynomial time algorithm for this problem in this
paper. This result is interesting because of the nature of new

978-1-4799-1633-7/14/$31.00 Â©2014 IEEE

metric proposed in [5], one key property on which the shortest
path algorithm due to Dijkstra is based, that is, subpath of a
shortest path is shortest, is no longer valid. We show that even
without this key property, not only it is possible to compute
the shortest path in polynomial time using the new metric, it
is also possible to compute the shortest path in polynomial
time, with a variation of the metric proposed in [5].
The rest of the paper is organized as follows. In section
III, we present the path length metric proposed for the FiWi
network in [5] and a variation of it. In section IV we provide
algorithms for computing the shortest path using these two
metrics. As multi-path routing offers signiï¬cant advantage
over single path routing [6], [7], [8], [9], we also consider
the problem of computation of a pair of node disjoint paths
between a source-destination node pair using the metric proposed in [5]. We show that while the single path computation
problem can be solved in polynomial time in all these cases,
the disjoint path computation problem is NP-complete. The
contributions of the paper are as follows;
â¢ Polynomial time algorithm for single path routing (metric
1) in FiWi networks
â¢ Polynomial time algorithm for single path routing (metric
2) in FiWi networks
â¢ NP-completeness proof of disjoint path routing (metric
1) in FiWi networks
â¢ Optimal solution for disjoint path routing (metric 1) in
FiWi networks using Integer Linear Programming
â¢ One approximation algorithm for disjoint path routing in
FiWi networks with an approximation bound of 4 and
computation complexity O((n + m)log n)
â¢ One approximation algorithm for disjoint path routing in
FiWi networks with an approximation bound of 2 and
computation complexity O(m(n + m)log n)
â¢ Experimental evaluation results of the approximation
algorithm for disjoint path routing in FiWi networks
II. R ELATED W ORK
Fiber-Wirelss (FiWi) networks is a hybrid access network
resulting from the convergence of optical access networks
such as Passive Optical Networks (PONs) and wireless access
networks such as Wireless Mesh Networks (WMNs) capable of providing low cost, high bandwidth last mile access.

131

Because it provides an attractive way of integrating optical
and wireless technology, Fiber-Wireless (FiWi) networks have
received considerable attention in the research community in
the last few years [1], [2], [3], [4], [5], [8], [9]. The minimum
interference routing algorithm for the FiWi environment was
ï¬rst proposed in [4]. In this algorithm the path length was
measured in terms of the number of hops in the wireless
part of the FiWi network. The rationale for this choice was
that the maximum throughput of the wireless part is typically
much smaller than the throughput of the optical part, and
hence minimization of the wireless hop count should lead to
maximizing the throughout of the FiWi network. However,
the authors of [5] noted that minimization of the wireless
hop count does not always lead to throughput maximization.
Accordingly, the path length metric proposed by them in
[5] pays considerable importance to the trafï¬c intensity at a
generic FiWi network node. The results presented in this paper
are motivated by the path length metric proposed in [5].
III. P ROBLEM F ORMULATION
In the classical path problem, each edge e â E of the graph
G = (V, E), has a weight w(e) associated with it and if there
is a path P from the node v0 to vk in the graph G = (V, E)
w

w

w

w

v0 â1 v1 â2 v2 â3 v3 . . . âk vk
then the path length or the distance between the nodes v0 and
vk is given by
w(Pv0 ,vk ) = w1 + w2 + Â· Â· Â· + wk
However, in the path length metric proposed in [5] for
optical-wireless FiWi networks [1], [2], [3], the contribution
of ei to the path length computation depends not only on
the weight wi , but also on the weights of the other edges
that constitute the path. In the following section, we discuss
this metric and a variation of it. We also also formulate the
multipath computation problem using this metric.
The Optimized FiWi Routing Algorithm (OFRA) proposed
in [5] computes the âlengthâ (or weight) of a path P from v0
to vk using the following metric




(wu ) + max (wu )
w (Pv0 ,vk ) = min
P

âuâP

âuâP

where wu represents the trafï¬c intensity at a generic FiWi
network node u, which may be an optical node in the ï¬ber
backhaul or a wireless node in wireless mesh front-end. In
order to compute shortest path using this metric, in our
formulation, instead of associating a trafï¬c intensity âweightâ
(wu ) with nodes, we associate them with edges. This can easily
be achieved by replacing the node u with weight wu with two
nodes u1 and u2 , connecting them with an edge (u1 , u2 ) and
assigning the weight wu on this edge. In this scenario, if there
is a path P from the node v0 to vk in the graph G = (V, E)
w

w

w

w

v0 â1 v1 â2 v2 â3 . . . âk vk
then the path length between the nodes v0 and vk is given by

w+ (Pv0 ,vk )

=
=

w1 + w2 + . . . + wk + max(w1 , w2 , . . . wk )
k

wi + maxki=1 wi
i=1

In the second metric, the length a path Pv0 ,vk :
v0 âv1 âv2 â . . . âvk , between the nodes v0 and vk is given
by

wÌ(Pv0 ,vk )

=
=

k

i=1
k


wi + CN T (Pv0 ,vk ) â max(w1 , w2 , . . . wk )
wi + CN T (Pv0 ,vk ) â maxki=1 wi

i=1

where CN T (Pv0 ,vk ) is the count of the number of times
max (w1 , w2 , . . . wk ) appears on the path Pv0 ,vk . We study the
shortest path computation problems in FiWi networks using
the above metrics and provide polynomial time algorithms for
solution in subsections IV-A and IV-B.
If wmax = max(w1 , w2 , . . . wk ), we refer to the corresponding edge (link) as emax . If there are multiple edges having
the weight of wmax , we arbitrarily choose any one of them as
emax . It may be noted that both the metrics have an interesting
property in that in both cases, the contribution of an edge e
on the path length computation depends not only on the edge
e but also on every other edge on the path. This is so, because
if the edge e happens to be emax , contribution of this edge
in computation of w+ (Pv0 ,vk ) and wÌ(Pv0 ,vk ) will be 2 â w(e)
and CN T (Pv0 ,vk ) â w(e) respectively. If e is not emax , then
its contribution will be w(e) for both the metrics.
As multipath routing provides an opportunity for higher
throughput, lower delay, and better load balancing and resilience, its use have been proposed in ï¬ber networks [6],
wireless networks [7] and recently in integrated ï¬ber-wireless
networks [8], [9]. Accordingly, we study the problem of
computing a pair of edge disjoint paths between a sourcedestination node pair s and d, such that the length of the
longer path (path length computation using the ï¬rst metric)
is shortest among all edge disjoint path pairs between the
nodes s and d. In subsection IV-C we prove that this problem
is NP-complete, in subsection IV-D, we provide an optimal
solution for the problem using integer linear programming,
in subsections IV-E and IV-F we provide two approximation
algorithms for the problem with a performance bound of 4
and 2 respectively, and in subsection IV-F we provide results
of experimental evaluation of the approximation algorithms.
IV. PATH P ROBLEMS IN F I W I N ETWORKS
In this section, we present (i) two different algorithms for
shortest path computation using two different metrics, (ii)
NP-completeness proof for the disjoint path problem, (iii)
two approximation algorithms for the disjoint path problem,
and (iv) experimental evaluation results of the approximation
algorithms.

132

It may be noted that, in both metrics w+ (Pv0 ,vk ) and
wÌ(Pv0 ,vk ), we call an edge e â Pv0 ,vk crucial, if w(e) =
maxki=1 w(e ), âe â Pv0 ,vk .
A. Shortest Path Computation using Metric 1
It may be recalled that the path length
k metric used in this
case is the following: w+ (Pv0 ,vk ) = i=1 wi + maxki=1 wi . If
k
the path length metric was given as w(Pv0 ,vk ) = i=1 wi ,
algorithms due to Dijkstra and Bellman-Ford could have been
used to compute the shortest path between a source-destination
node pair. One important property of the path length metric
that is exploited by Dijkstraâs algorithm is that âsubpath of a
shortest
path is shortestâ. However, the new path length metric
k
k
i=1 wi +maxi=1 wi does not have this property. We illustrate
this with the example below.
Consider two paths P1 and P2 from the node v0 to v3 in the
w
w
w
graph G = (V, E), where P1 : v0 â1 v1 â2 v2 â3 v3 and P2 :
w4
w5
w3
v0 â v4 â v2 â v3 . If w1 = 0.25, w2 = 5, w3 = 4.75, w4 =
2, w5 = 4, the length of the path P1 , w+ (P1 ) = w1 +w2 +w3 +
max(w1 , w2 , w3 ) = 0.25 + 5 + 4.75 + max(0.25, 5, 4.75) = 15
and the length of the path P2 , w+ (P2 ) = w4 + w5 + w6 +
max(w4 , w5 .w6 ) = 2 + 4 + 4.75 + max(2, 4, 4.75) = 15.5.
Although P1 is shortest path in this scenario, the length of
w
w
its subpath v0 â1 v1 â2 v2 is 0.25 + 5 + max (0.25, 5) =
10.25, which is greater than the length of a subpath of P2
w
w
v0 â4 v4 â5 v2 2 + 4 + max (2, 4) = 10, demonstrating that
the assertion that âsubpath of a shortest path is shortestâ no
longer holds in this path length metric.
As the assertion âsubpath of a shortest path is shortestâ no
longer holds in this path length metric, we cannot use the
standard shortest path algorithm due to Dijkstra in this case.
However, we show that we can still compute the shortest path
between a source-destination node pair in polynomial time by
repeated application of the Dijkstraâs algorithm. The algorithm
is described next.
For a given graph G = (V, E), w.l.o.g, we assume |V | = n
and |E| = m. Deï¬ne Ge as subgraph of G by deleting edges
whose weight is greater than w(e).
Also, as Dijkstraâs algorithm does, we need to maintain
distance vector. We deï¬ne distv be distance (length of shortest
path) from s to v, Î v be predecessor of v and maxedgev be
weight of the crucial edge from s to v via the shortest path,
ansv be optimal solution (length) from s to v.
Different Ge can be treated as different layers of the
G. For any path P , we deï¬ne the function eâ (P ) as the
crucial edge along P . It is easy to observe that if
Pd is the
optimal path from s to node d then w(Pd ) =
eâP w(e)
and w+ (Pd ) = w(Pd ) + w(eâ (Pd )). It may be noted that
henceforth, we shorten Ps,d to Pd , because we consider that
the source is ï¬xed while the destination d is variable.
Lemma 1. w(Pd ) is minimum in Geâ (Pd ) .
Proof: It is obvious that Pd still exists in Geâ (Pd ) , since
edges on Pd are not abandoned. Suppose Pd is not shortest,
then there must be another path Pd s.t. w(Pd ) < w(Pd ).
Noting that the crucial edge on Pd , namely e , is no longer than

Algorithm 1 Modiï¬ed Dijkstraâs Algorithm
1: Initialize ansv = â for for all v â V
2: sort all edges according to w(e) in ascending order
3: for i = 1 to m do
4:
Initialize distv = â, Î v = nil, maxedgev = 0 for
all v â V
5:
dists = 0
6:
Q = the set of all nodes in graph
7:
while Q is not empty do
8:
u = Extract-Min(Q)
9:
for each neighbor v of u do
10:
if eu,v â E(Gei ) then
11:
t = MAX {maxedgeu , w(eu,v )}
12:
if distu + w(eu,v ) < distv then
13:
distv = distu + w(eu,v )
14:
maxedgev = t
15:
Î v = u
16:
else if distu + w(eu,v ) == distv then
17:
if maxedgev > t then
18:
maxedgev = t
19:
Î v = u
20:
end if
21:
end if
22:
end if
23:
end for
24:
end while
25:
for each node v do
26:
ansv = min{ansv , distv + maxedgev }
27:
end for
28: end for
eâ (Pd ) since they both belong to Geâ (Pd ) . Hence w+ (Pd ) =
w(Pd )+w(e ) < w(Pd )+w(eâ (Pd )) = w+ (Pd ), contradicting
Pd is optimal.
Lemma 2. Modiï¬ed Dijkstraâs Algorithm (MDA) computes
shortest path while keeping the crucial edge as short as
possible in every iteration.
Proof: Line 4 to 24 works similar to the standard Dijkstraâs algorithm does. Besides, when updating distance, MDA
also updates the crucial edge to guarantee that it lies on the
path and when there is a tie, MDA will choose the edge with
the smaller weight.
Theorem 1. Modiï¬ed Dijkstraâs Algorithm computes optimal
solution for every node v in O(m(n + m)logn) time.
Proof: Lemma 1 indicates for any node v â V , optimal
solution can be obtained by enumerating all possible crucial
edges eâ (Pv ) and computing shortest path on Geâ (Pv ) . By sorting all edges in nondecreasing order, every subgraph Geâ (Pv )
is considered and it is shown in lemma 2, MDA correctly
computes shortest path for every node v in every Geâ (Pv ) .
Then optimal solution is obtained by examining all shortest
path using the w() metric plus the corresponding crucial edge.
Dijkstraâs algorithm runs O((n + m)logn) time when using

133

binary heap, hence MDA runs in O(m(n+m)logn) time when
considering all layers.
B. Shortest Path Computation using Metric 2
Given a path P , let eâ (P ) be the crucial edge along the
P and CN T (P ) be the number of occurrence of such edge.
Now
a path Q, such that wÌ(P ) =
 our objective becomes to ï¬nd
â
w(e)
+
CN
T
(Q)
â
w(e
(Q))
is minimum.
eâQ
The layering technique can also be used in this problem.
However, shortest path under a ceratin layer may not become
a valid candidate for optimal solution. Here, we introduce a
dynamic programming algorithm that can solve the problem
optimally in O(n2 m2 ) time.
Input is a weighted graph G = (V, E), |V | = n, |E| = m
with a speciï¬ed source node s. In this paper, we only
consider nonnegative edge weight. As shown before, we use
Ge to represent the residue graph by deleting edges longer
than e in G. Different from MDA1, in order to consider
the number of crucial edges, distv is replaced by an array
dist0v , dist1v , ....distnv . One can think distcv be the shortest
distance from s to v by going through exactly c crucial edges
and possibly some shorter edges. Similarly, we replace Î v by
Î cv , 0 â¤ c â¤ n. Each Î cv records predecessor of v for the
path corresponding to distcv . Lastly, ansv is used as optimal
solution from s to v.
Lemma 3. If Pv is the best path from s to v, i.e., wÌ(Pv ) is
minimum among all s-v path, then Pv is computed in Geâ (Pv )
CN T (Pv )
and distv
= w(Pv ).
Proof: By deï¬nition, Pv exists in Geâ (Pv ) and
CN T (Pv ) â¥ 1 since any path should go through at least one
crucial edge. Noting wÌ(Pv ) = w(Pv )+CN T (Pv )âw(eâ (Pv )),
on one hand if we treat CN T (Pv ) as a ï¬xed number, then we
need to keep w(Pv ) as small as possible. Inspired by idea
of bellman-ford algorithm, we can achieve it by enumerating
|Pv |, i.e., number of edges on Pv . On the other hand, we need
to keep tracking number of crucial edges as well. Hence, distcv
is adopted to maintain such information, superscript c reï¬ects
exact number of crucial edges. From line 12 to line 25, distcv
is updated either when it comes from a neighbor who has
already witnessed c crucial edges or it comes from a neighbor
with c â 1 crucial edges and the edge between is crucial. In
either case, node v gets a path, say P  , with exact c crucial
edges on it and w(P ) is minimum. At last, Pv can be selected
by enumerating number of crucial edges and that is what line
30 to 32 does.
Lemma 4. Maxedge Shortest Path Algorithm(MSPA) runs in
O(n2 m) time for each Ge .
Proof: We can apply similar analysis of bellman-ford
algorithm. However, we need to update distcv array, it takes
extra O(n) time for every node v in every iteration when
enumerating |Pv |. Hence, total running time is O(n2 m).
Theorem 2. MSPA computes optimal path for every v â V
in O(n2 m2 ) time.

Algorithm 2 Maxedge Shortest Path Algorithm
1: Initialize ansv = â for for all v â V
2: sort all edges according to w(e) in ascending order, say
e1 , e2 , ..., em after sorting
3: for i = 1 to m do
4:
Initialize distcv = â, Î cv = nil for all v â V and all
0â¤câ¤n
5:
dist0s = 0
6:
for j = 1 to n â 1 do
7:
for k = 0 to j do
8:
for every node v â V do
9:
if distkv = â then
10:
continue
11:
end if
12:
for every neighbor u of v do
13:
if w(eu,v ) > w(ei ) then
14:
continue
15:
else if w(eu,v ) == w(eâ ) then
16:
if distkv + w(eu,v )
<
then
distk+1
u
17:
distk+1
= distkv +
u
w(eu,v )
18:
Î k+1
=v
u
19:
end if
20:
else
21:
if distkv + w(eu,v ) < distku
then
22:
distku = distkv +
w(eu,v )
23:
Î ku = v
24:
end if
25:
end if
26:
end for
27:
end for
28:
end for
29:
end for
30:
for i = 1 to n â 1 do
31:
ansv = min{ansv , distiv + i â w(ei )}
32:
end for
33: end for

Proof: By Lemma 3, if Pv is obtained when computing
Geâ Pv . Then, by considering all possible Geâ , we could get
Pv in one of these layering. It takes O(m) to generate all Geâ ,
by Lemma 4, MSPA runs in v â V in O(n2 m2 ) time.
C. Computational Complexity of Disjoint Path Problem
In this section, we study edge disjoint path in optical
wireless network. By reduction from well known Min-Max
2-Path Problem, i.e., min-max 2 edge disjoint path problem
under normal length measurement, we show it is also
NP-complete if we try to minimize the longer path when w+
length is applied. Then we give an ILP formulation to solve
this problem optimally. At last, we provide two approximation
algorithm, one with approximation ratio 4, running time

134

O((m + n)logn), the other one with approximation ratio 2
while running time is O(m(m + n)logn).

D. Optimal Solution for the Disjoint Path Problem
Here, we give an ILP formulation for MinMax2OWFN.
ILP for MinMax2OWFN

Min-Max 2 Disjoint Path Problem (MinMax2PP)
Instance: An undirected graph G = (V, E) with a positive
weight w( e) associated with each edge e â E, a source node
s â V , a destination node t â V , and a positive number X.
Question: Does there exist a pair of edge disjoint paths P1
and P2 from s to d in G such that w(P1 ) â¤ w(P2 ) â¤ X?

min
s.t.

MP



The MinMax2PP problem is shown to be NP-complete in
[10]. With a small modiï¬cation, we show NP-completeness
still holds if w+ length measurement is adopted.

fi,j,1 â

fj,i,1 =

(i,j)âE

(j,i)âE





fi,j,2 â

(i,j)âE

Min-Max 2 Disjoint Path Problem in Optical Wireless
Networks (MinMax2OWFN)
Instance: An undirected graph G = (V, E) with a positive
weight w( e) associated with each edge e â E, a source node
s â V , a destination node t â V , and a positive number X.
Question: Does there exist a pair of edge disjoint paths P1
and P2 from s to t in G such that w+ (P1 ) â¤ w+ (P2 ) â¤ X  ?



fj,i,2 =

(j,i)âE

â§
âª
â¨
âª
â©
â§
âª
â¨
âª
â©

1
â1
0

i=s
i=t
otherwise

1
â1
0

i=s
i=t
otherwise

fi,j,1 + fi,j,2 â¤ 1
w1 â¥ fi,j,1 â w(i, j)

â(i, j) â E

w2 â¥ fi,j,2 â w(i, j)

fi,j,1 â w(i, j)
M P â¥ w1 +

â(i, j) â E

â(i, j) â E

(i,j)âE



M P â¥ w2 +

Theorem 3. The MinMax2OWFN is NP-complete

fi,j,2 â w(i, j)

(i,j)âE

Proof: Evidently, MinMax2OWFN is in NP class, given
two edge joint path P1 and P2 , we can check if w+ (P1 ) â¤
w+ (P2 ) â¤ X  in polynomial time.
We then transfer from MinMax2PP to MinMax2OWFN.
Let graph G = (V, E) with source node s , destination t and
an integer X be an instance of MinMax2PP, we construct an
instance Gâ of MinMax2OWFN in following way.
1) Create an identical graph G with same nodes and edges
in G.
2) Add one node s0 to G .
3) Create two parallel edges e01 , e02 between s0 and s,
w(e01 ) = w(e02 ) = maxeâG(E) w(e)
4) Choose s0 to the source node in G and t to be the
destination.
5) Set X  = X + 2w(e01 )
It is easy to see, the construction takes polynomial time.
Now we need to show a instance of MinMax2OWFN have
two edge disjoint paths from s0 to t with length at most X  if
and only if the corresponding instance have two edge disjoint
paths from s to t with length at most X.
Suppose there are two edge disjoint paths P1 and P2 from
s0 to t in G , such that w+ (P1 ) â¤ w+ (P2 ) â¤ X  . By the
way we construct G , P1 and P2 must go through e01 and
e02 . W.l.o.g. we say e01 â P1 and e02 â P2 . Since w(e01 ) =
w(e02 ) = maxeâE(G ) {w(e)}, therefore e01 and e02 are the
crucial edge on P1 and P2 respectively. Hence, P1 â e01 and
P2 âe02 are two edge disjoint path in G, with length no greater
than X  â 2w(e01 ) = X.
Conversely, now suppose P1 and P2 are two edge joint paths
in G satisfying w(P1 ) â¤ w(P2 ) â¤ X. We follow the same
argument above, P1 + e01 and P2 + e02 are two desired paths,
with length not exceeding X + 2w(e01 ) = X  .

fi,j,1 = {0, 1},

fi,j,2 = {0, 1}

â(i, j) â E

The following is a brief description of this ILP formulation.
The ï¬rst two equation represent ï¬ow constraint as normal
shortest path problem does. fi,j,1 = 1 indicates path P1 goes
through edge (i, j), and 0 otherwise. So it is with fi,j,2 and
path P2 . Constraint 3 ensures two edges are disjoint, since
fi,j,1 and fi,j,2 cannot both be 1 at the same time. w1 , w2 act
as the weights of the crucial edges on P1 and P2 respectively.
Finally, we deï¬ne M P to be the maximum of w+ (P1 ) and
w+ (P2 ) and therefore try to minimize it.
E. Approximation Algorithm for the Disjoint Path Problem,
with approximation factor 4
Next we propose a 4-approximation algorithm which runs
in O((n + m)logn) time.
Given G = (V, E) with source s and destination t, the idea
of approximation algorithm is to ï¬nd two disjoint P1 and P2
such that w(P1 ) + w(P2 ) is minimized. Such P1 and P2 can
be found either using min cost max ï¬ow algorithm or the
algorithm due to Suurballe presented in [11]. And we need to
show both w+ (P1 ) and w+ (P2 ) are at most four times of the
optimal solution.
Algorithm 3 MinMax2OWFN Approximation Algorithm 1 (MAA1)
1: Run Suurballeâs algorithm on G, denote P1 , P2 be two
resulting path.
2: Compute w + (P1 ) and w + (P2 ).
3: Output max{w + (P1 ), w + (P2 )}.

135

Lemma 5. For any path P , w+ (P ) â¤ 2w(P ).
Proof: By deï¬nition, w+ (P ) = w(P ) + w(eâ (P )). Since
w(e (P )) â¤ w(P ), then w+ (P ) â¤ 2 â w(P ).
â

Lemma 6. If P1 and P2 are two edge joint path from s to
t such that w(P1 ) + w(P2 ) is minimum, then w+ (P1 ) and
w+ (P2 ) are at most four times of the optimal solution.
Proof: Say opt is the optimal value of a
M inM ax2OW F N instance and Q1 ,Q2 are two s â t
edge disjoint path in one optimal solution. W.l.o.g, we may
suppose w+ (P1 ) â¥ w+ (P2 ) and w+ (Q1 ) â¥ w+ (Q2 ). Let
w(P1 ) + w(P2 ) = p and w(Q1 ) + w(Q2 ) = q, by assumption,
p â¤ q. Also, we have w+ (P1 ) = w(P1 ) + eâ (P1 ) â¤ 2p,
opt = w+ (Q1 ) = w(Q1 ) + eâ (Q1 ) > 2q . Hence,
+
w+ (P2 )
(P1 )
2p
â¤ w opt
< q/2
â¤4
opt
Theorem 4. MAA1 is a 4-approximation algorithm running
in O((n + m)logn) time and 4 is a tight bound.
Proof: By Lemma 5 and 6, MAA1 has approximation
ratio at most 4.Then we show MAA1 has approximation at
least 4 for certain cases. Consider the following graph.

Algorithm 4 MinMax2OWFN Approximation Algorithm
2(MAA2)
1: set ans = â
2: for every Ge of G do
3:
Run Suurballeâs algorithm on Ge , denote P1 , P2 be
two resulting path.
4:
Compute w+ (P1 ) and w+ (P2 ).
5:
ans = min{ans, max{w+ (P1 ), w+ (P2 )}}.
6: end for
7: Output ans.
w(P1 )+w(e )
max{w+ (Q1 ), w+ (Q2 )} < 2. We
w(P1 )+w(e )
Suppose max{w
+ (Q ), w + (Q )} â¥ 2,
1
2

w(e ). It sufï¬ces to show

prove

it by contradiction.

then

w(P1 ) + w(e ) â¥ w+ (Q1 ) + w+ (Q2 )
Which follows,
w(P1 ) + w(e ) â¥ w(Q1 ) + w(eâ (Q1 )) + w(Q2 ) + w(eâ (Q2 ))
By deï¬nition, e is one of eâ (Q1 ), eâ (Q2 ). Hence,
w(P1 ) > w(Q1 ) + w(Q2 )
It is impossible since w(P1 ) + w(P2 ) is minimum in layer
G e .
Theorem 5. MAA2 is a 2-approximation algorithm running
in O(m(n + m)logn) time and 2 is a tight bound.

It is easy to check, P1 = {s â t}, P2 = {s â r â t} are
two edge disjoint path with minimum length 2k+2, w+ (P1 ) =
4k > w+ (P2 ) = 3. However, let Q1 = {s â u1 â u2 â
... â ukâ1 â uk â r â t}, Q2 = {s â r â v1 â v2 â
... â vkâ1 â vk â t}, then w(Q1 ) + w(Q2 ) = 2k + 4 while
+
4k
1)
w+ (Q1 ) = w+ (Q2 ) = k + 3. ww+(P
Q1 = k+3 â 4 when k is
sufï¬ciently large. Hence, 4 is a tight bound for MAA1.
We need O((n + m)logn) time running Suurballeâs algorithm and O(n) time computing w+ (P1 ) and w+ (P2 ).
Therefore total running time is O((n + m)logn).
F. Approximation Algorithm for the Disjoint Path Problem,
with approximation factor 2
In MAA1, layering technique is not used and we only
consider the original graph. However, by taking all Ge of G
into account, we can have a better approximation ratio.
Say Q1 , Q2 are two disjoint paths in one optimal solution.
Let e = max{eâ (Q1 ), eâ (Q2 )} and P1 , P2 be the resulting
paths when computing layer Ge ; w.l.o.g, we may assume
w(P1 ) > w(P2 ). Also, let anse = max{w+ (P1 ), w+ (P2 )}.
Lemma 7. anse < 2 max{w+ (Q1 ), w+ (Q2 )}.
Proof: Noting that w(eâ (P1 )) â¤ w(e ) and w(eâ (P2 )) â¤
w(e ) since they both belong to Ge . Then anse â¤ w(P1 ) +


Proof: By Lemma 7, in one of the layer, we guarantee
to have a 2-approximation solution. Since we take minimum
outcome among all layers, the ï¬nal result is no worse than
twice of the optimal solution. Now we need to show there
exists certain case, such that MAA2 is no good than twice of
the optimal solution. Consider the following graph

There is only one layer, and P1 = {s â x1 â x2 â
... â x2kâ1 â x2 k â t}, P2 = {s â r â t} are two
edge disjoint path with minimum length 2k + 3, w+ (P1 ) =
2k + 2 > w+ (P2 ) = 3. Again, set Q1 = {s â u1 â u2 â
... â ukâ1 â uk â r â t}, Q2 = {s â r â v1 â v2 â
... â vkâ1 â vk â t}, then w(Q1 ) + w(Q2 ) = 2k + 4 while
+
2k+2
1)
w+ (Q1 ) = w+ (Q2 ) = k + 3. ww+(P
Q1 = k+3 â 2 when k is
sufï¬ciently large. Hence, 2 is a tight bound for MAA2.
Finally, it is easy to see that the running time is O(m(n +
m)logn).

136

S
node
14
18
1
18
20
10
1
14
20
10
18
1
20
14
10
20
5

D
node
2
8
6
4
3
3
11
6
7
5
12
20
13
19
17
16
11

Opt
Sol
47
46
28
50
40
27
35
50
38
36
22
46
26
29
36
29
40

Approx
Sol 1
55
46
28
58
40
27
35
52
38
38
22
52
26
29
36
29
48

Approx
ratio 1
1.17
1
1
1.16
1
1
1
1.04
1
1.05
1
1.13
1
1
1
1
1.2

Approx
Sol 2
55
46
28
57
40
27
35
52
38
38
22
52
26
29
36
29
48

Approx
ratio 2
1.17
1
1
1.14
1
1
1
1.04
1
1.05
1
1.13
1
1
1
1
1.2

Fig. 1.

TABLE I
C OMPARISON OF THE A PPROXIMATE SOLUTIONS WITH THE O PTIMAL
SOLUTION FOR THE ARPANET GRAPH

The ARPANET graph with 20 nodes and 32 links

approximation algorithms. Both the approximation algorithms
have a constant factor approximation bound. However, there is
a trade-off between the quality of the solution (approximation
bound) and the execution time. Finally, we show that both the
approximation algorithms obtain near optimal results through
simulation using the ARPANET topology.

G. Experimental Results for the Disjoint Path Problem
In this section, we present the results of simulations for
comparing the performance of our approximation algorithms
with the optimal solution when w+ () metric is applied.
The simulation experiments have been carried out on the
ARPANET topology (as shown in Fig 1 with nodes and links
shown in black) which has twenty nodes and thirty two links.
The weights of the links have been randomly generated and
lie in the range of two and eleven (as shown in red in Fig
1) and we consider the graph to be undirected. The results of
the comparison is presented in Table I. We have compared the
lengths of the longer of the two edge disjoint paths computed
by the optimal and the approximate solutions for seventeen
different source-destination pairs. It may be noted that for
almost 65% of the cases, the approximate algorithms obtain
the optimal solution. In the remaining cases, the approximate
solutions lie within a factor of 1.2 of the optimal solution
Thus, even though the approximation ratio in the worst case
are proven to be 4 and 2, in practical cases, it is within 1.2.
From these experimental results, we can conclude that the
approximation algorithms produce optimal or near optimal
solutions in majority of the cases. It may be noted that the
two approximation algorithms perform in a similar fashion
in the ARPANET graph, however, as proven theoretically,
the two approximation algorithms differ in their worst case
approximation ratio.
V. C ONCLUSION

R EFERENCES
[1] N. Ghazisaidi, M. Maier, and C. M. Assi, âFiber-Wireless (FiWi) Access
Networks: A Surveyâ, IEEE Communications Magazine, vol. 47, no. 2,
pp 160-167, Feb. 2009.
[2] N. Ghazisaidi, and M. Maier, âFiber-Wireless (FiWi) Access Networks:
Challenges and Opportunitiesâ, IEEE Network, vol. 25, no. 1, pp 36-42,
Feb. 2011.
[3] Z. Zheng, J. Wang, X. Wang, âONU placement in ï¬ber-wireless (FiWi)
networks considering peer-to-peer communicationsâ, IEEE Globecom, 2009.
[4] Z. Zheng, J. Wang, X. Wang, âA study of network throughput gain
in optical-wireless (FiWi) networks subject to peer-to-peer commuincationsâ, IEEE ICC, 2009.
[5] F. Aurzada, M. Levesque, M. Maier, M. Reisslein, âFiWi Access
Networks Based on Next-Generation PON and Gigabit-Class WLAN
Technologies: A Capacity and Delay Analysisâ, IEEE/ACM Transactions
on Networking, to appear.
[6] A. Sen, B.Hao . B. Shen , L.Zhou and S. Ganguly, âOn maximum
available bandwidth through disjoint pathsâ, Proc. of IEEE Conf. on
High Performance Switching and Routing, 2005.
[7] M. Mosko, J.J. Garcia-Luna-Aceves, âMultipath routing in wireless
mesh networksâ, Proc. of IEEE Workshop on Wireless Mesh Networks, 2005.
[8] J. Wang, K. Wu, S. Li and C. Qiao ,âPerformance Modeling and Analysis
of Multi-Path Routing in Integrated Fiber-Wireless (FiWi) Networksâ,
IEEE Infocom mini conference, 2010.
[9] S. Li, J. Wang, C. Qiao, Y. Xu ,âMitigating Packet Reordering in
FiWi Networksâ, IEEE/OSA Journal of Optical Communications and
Networking, vol. 3, pp.134-144, 2011.
[10] C. Li, S.T. McCormick and D.Simchi-Levi, âComplexity of Finding Two
Disjoint Paths with Min- Max Objectiveâ, Discrete Applied Mathematics, vol. 26, pp. 105-115, 1990.
[11] J. W. Suurballe, âDisjoint paths in a networkâ, Networks, vol. 4, pp. 125145, 1974.

In this paper, we study the shortest path problem in FiWi
networks. Based on the path length metrics proposed in [3],
[5], we present polynomial time algorithms for the single
path scenario. In the disjoint path scenario, we prove that the
problem of ï¬nding a pair of disjoint paths, where the length
of the longer path is shortest, is NP-complete. We provide an
ILP solution for the disjoint path problem and propose two

137

On the Entity Hardening Problem in Multi-layered Interdependent Networks
Joydeep Banerjee, Arun Das, Chenyang Zhou, Anisha Mazumder and Arunabha Sen
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering Arizona State University Tempe, Arizona 85287 Email: {joydeep.banerjee, arun.das, czhou24, anisha.mazumder, asen}@asu.edu Abstract--The power grid and the communication network are highly interdependent on each other for their well being. In recent times the research community has shown significant interest in modeling such interdependent networks and studying the impact of failures on these networks. Although a number of models have been proposed, many of them are simplistic in nature and fail to capture the complex interdependencies that exist between the entities of these networks. To overcome the limitations, recently an Implicative Interdependency Model that utilizes Boolean Logic, was proposed and a number of problems were studied. In this paper we study the "entity hardening" problem, where by "entity hardening" we imply the ability of the network operator to ensure that an adversary (be it Nature or human) cannot take a network entity from operative to inoperative state. Given that the network operator with a limited budget can only harden k entities, the goal of the entity hardening problem is to identify the set of k entities whose hardening will ensure maximum benefit for the operator, i.e. maximally reduce the ability of the adversary to degrade the network. We show that the problem is solvable in polynomial time for some cases, whereas for others it is NP-complete. We provide the optimal solution using ILP, and propose a heuristic approach to solve the problem. We evaluate the efficacy of our heuristic using power and communication network data of Maricopa County, Arizona. The experiments show that our heuristic almost always produces near optimal results.

arXiv:1412.6686v1 [cs.NI] 20 Dec 2014

that may exist between network entities, such as when entity ai is operational, if entities (i) bj and bk and bl are operational, or (ii) bm and bn are operational, or (iii) bp is operational. Graph based interdependency models proposed in the literature such as [3], [4], [5], [10], [6], [7] including [1], [2] cannot capture such complex interdependency involving both conjunctive and disjunctive terms between entities of multi-layer networks. To overcome these limitations, an Implicative Interdependency Model that utilizes Boolean Logic, was recently proposed in [9], and a number of problems including computation of K most vulnerable nodes [9], root cause of failure analysis [11], and progressive recovery from failures [12], were studied using this model. In this paper we study the "entity hardening" problem in the interdependent power-communication network using the Implicative Interdependency Model (IIM). By "entity hardening", we imply the ability of the network operator to ensure that an adversary (be it Nature or human), cannot take a network entity from an operative (operational) to an inoperative (failed) state. We assume that the adversary is clever and is capable of identifying the most vulnerable entities in the network that causes maximum damage to the interdependent system. However, the adversary does not have an unlimited budget and has the resources to destroy at most K entities of the interdependent network. The network operator is also aware of adversary's target entities for destruction. Since we assume that once an entity is "hardened" by the network operator it cannot be destroyed by the adversary, if all K targets of the adversary are hardened by the network operator, then the adversary cannot induce any failure in the network. However, if due to resource limitations the network operator is able to strengthen only k entities, where k < K, these k entities have to be carefully chosen. The goal of the entity hardening problem is to identify the set of k entities whose hardening will ensure maximum benefit for the operator, i.e. maximally reduce the ability of the adversary to degrade the network. We classify the entity hardening problem into four different cases depending on the nature of the interdependency relationships. We show that the first case can be solved in polynomial time, and all other cases are shown to be NP-complete. We provide an inapproximability result for the second case, an approximation algorithm for the third case, and a heuristic for the fourth (general) case. We evaluate the efficacy of our heuristic using power and communication network data of Maricopa County, Arizona. The experiments show that our

I.

I NTRODUCTION

The critical infrastructures of a nation form a complex symbiotic ecosystem where individual infrastructures are heavily interdependent on each other for being fully functional. Two such critical systems that rely heavily on each other for their well being are the power and communication network infrastructures. For instance, power grid entities such as SCADA systems, that are used to remotely operate power generation units, receive their control commands over the communication network infrastructure, while communication network entities such as routers and base stations are inoperable without electric power. Thus, failure introduced in the system either by Nature (hurricanes), or man (terrorist attacks), can trigger further failures in the system due to interdependencies between the entities of the two infrastructures. Although a number of models have been proposed for modeling and analysis of interdependent multi-layered networks [1], [2], [3], [4], [5], [6], [7], [8], many of these models are simplistic in nature and fail to capture the complex interdependencies that exists between the entities of these networks. As noted in [9], these models fail to model complex interdependencies

2

heuristic almost always produces near optimal results. The paper is organized as follows, the IIM model is presented in Section II, in Sections III and IV we formally state the entity hardening problem and analyze its computational complexity, Section V outlines the optimal and heuristic solutions to the problem, Section VI shows the experimental results, and finally Section VII concludes this paper. II. I NTERDEPENDENCY M ODEL

III.

P ROBLEM F ORMULATION

We now present an overview of the underlying IIM interdependency model [9]. IIM uses Boolean Logic to model the interdependencies between network entities, these interdependent relationships are termed as Implicative Interdependency Relations (IDRs). We represent this interdependent network setting as I (A, B, F (A, B )), where sets A and B are the power and communication network entities respectively, and F (A, B ) is the set of dependency relations, or IDRs. Table I represents a sample interdependent network I (A, B, F (A, B )), where A = {a1 , a2 , a3 , a4 }, B = {b1 , b2 , b3 } and F (A, B ) is the set of IDRs (dependency relations) between the entities of A and B . In this example, the IDR b1  a1 a3 + a2 implies that entity b1 is operational when both the entities a1 and a3 are operational, or entity a2 is operational. The conjunction of entities, such as a1 a3 , is also referred to as a minterm.
Power Network a1  b1 b2 a2  b1 + b2 a3  b1 + b2 + b3 a4  b1 + b3 Comm. Network b1  a1 a3 + a2 b2  a1 a2 a3 b3  a1 + a2 + a3 --

Before we make a formal statement of the entity hardening problem in the IIM setting, we explain it with the help of an example. Consider an interdependent system as outlined in the IDR set shown in Table I. It may be easily checked that when the adversary budget is K= 2, the most vulnerable entities of this system are {a2 , b3 }. If the network operator doesn't harden any one of the entities a2 or b3 , then in this example all the network entities eventually fail, as seen from the fault propagation in Table II. When the network operator chooses to harden both a2 and b3 then none of the entities in the network fail if the adversary restricts the attack only to the two most vulnerable entities of the network, which in this example happens to be {a2 , b3 }. If the network operator has resources to harden only one entity and the operator chooses to harden a2 , the destruction of b3 by the adversary will eventually lead to the failure of no other entities of the network, as shown in Table III(a). If on the other hand, the network operator chooses to harden b3 , destruction by the adversary of a2 will eventually lead to the failure of the entities {a2 , b2 , a1 , b1 } as shown in Table III(b). Clearly in this scenario the operator should harden a2 instead of b3 . Definition: Kill Set of a set of Entities(S ): The kill set of a set of entities S , is the set of all entities that will eventually fail due to failure of S and the interdependencies between the entities of the network as given by the set of IDR's. The kill set of a set of entities S is denoted by KillSet(S ). It may be noted that the search for k entities to be hardened is restricted to the KillSet(S ), where S is the set of K most vulnerable entities in the network, because hardening any entity not in KillSet(S ) does not provide any benefit to the network operator. In this study we also assume that the set of K most vulnerable entities in the network is unique.
Entities 0 a1 a2 a3 a4 b1 b2 b3 0  0 0 0 0 1 Time Steps (t) 1 2 3 0  0 0 0 0 1 0  0 0 0 0 1 0  0 0 0 0 1 Entities 4 0  0 0 0 0 0 a1 a2 a3 a4 b1 b2 b3 0 0 1 0 0 0 0  Time Steps (t) 1 2 3 0 1 0 0 0 1  1 1 0 0 0 1  1 1 0 0 1 1  4 1 1 0 0 1 1 

TABLE I: Implicative Interdependency Relations of a sample network

Given a set of inoperable (failed) entities, a time stepped failure cascade can be derived from the dependency relationships outlined in the IDR set. For example, for the interdependent network outlined in Table I, Table II shows the failure propagation when entities {a2 , b3 } fail at the initial time step (t = 0). It may be noted that the model assumes that dependent entities fail immediately in the next time step, for example, when {a2 , b3 } fail at t = 0, b2 fails at t = 1 as b2 is dependent on a2 for its survival. The system reaches a steady state when the failure propagation process stops. In this example, when {a2 , b3 } fail at t = 0, the steady state is reached at time step t = 4.
Entities 0 a1 a2 a3 a4 b1 b2 b3 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 Time Steps (t) 2 3 4 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 5 1 1 1 1 1 1 1 6 1 1 1 1 1 1 1

(a) Entity a2 is hardened

(b) Entity b3 is hardened

TABLE III: Failure cascade propagation with entity hardening. Entities {a2 , b3 } are attacked at time step t = 0. A value of 1 denotes entity failure, 0 otherwise.  denotes a hardened entity.

TABLE II: Failure cascade propagation when entities {a2 , b3 } fail at time step t = 0. A value of 1 denotes entity failure, and 0 otherwise

We now proceed to formulate the entity hardening problem formally. Given an interdependent network system I (A, B, F (A, B )), and the set of K most vulnerable entities of the system A  B  , where A  A and B   B : The Entity Hardening (ENH) problem INSTANCE: Given: (i) An interdependent network system I (A, B, F (A, B )), where the sets A and B represent the entities of the two networks, and F (A, B ) is the set of IDRs. (ii) The set of K most vulnerable entities of the system A  B  , where A  A and B   B (iii) Two positive integers k, k < K and EF .

A primary consideration for using this model is the accurate formulation of the IDRs that is representative of the underlying physical power and communication network infrastructures. This can either be done by careful analysis as done in [8], or by consultation with experts of these infrastructures. We utilize IIM to model the interdependency between the two networks and analyze the entity hardening problem in this setting.

3

QUESTION:Is there a set of entities H = A  B  , A  A, B   B, |H|  k , such that hardening H entities results in no more than EF entities to fail after entities A  B  fail at time step t = 0. We note some of the assumptions for the ENH problem: First, we assume that once an entity is hardened, it is always operational and does not fail at any time step of the observation, even when the entity is part of the K most vulnerable entities. Second, we assume that k < K, as otherwise the selection of K entities for hardening ensures that no entities fail at all. Finally, as noted earlier, we assume that the set of K most vulnerable entities in the network is unique. We now proceed to analyze the computational complexity of the ENH problem. IV. C OMPUTATIONAL C OMPLEXITY A NALYSIS

[9]. So with two entities {xi , xj }  A  B  and Cxi  Cxj = Cxj i.e, Cxj  Cxi , if xi is hardened it prevents the failure of Cxi - Cxj entities (provided that none of the entities in Cxi - Cxj - {xi } are in A  B  ). With this assertion, for an entity xi  A  B  , steps 4-7 of Algorithm 1 finds the actual entities for which failure is prevented by hardening xi . The set D = {Dx1 , Dx2 , ..., DxK } comprises of these set of entities for each hardened entity xi . To prove that Algorithm 1 finds the optimal solution we make the following two assertions: First, consider any two sets Dxi and Dxj . It is implied from step 6 of Algorithm 1 that / A  B  is Dxi  Dxj = . Second, consider an entity xp  hardened. If xp fails when entities in A  B  fails initially then it would belong to some set Dxi . Thus hardening xp results in preventing the failure of entities that is a proper subset of Dxi . Hence the entities to be hardened must belong to A  B  only. Owing to the two assertions it directly follows that with a given budget k , hardening k highest cardinality sets from the set D ensures prevention of failure for the maximum number of entities. B. Case II: Problem Instance with One Minterm of Arbitrary Size The IDRs of Case II have a single minterm of arbitrary p size. This can be represented as xi  j =1 yj , where xi and yj are entities of network A(B ) and B (A) respectively and the size of the minterm is p. The Entity Hardening problem with respect to Case II is NP-complete and is proved in Theorem 2. An inapproximability proof for this case of the problem is given in Theorem 3 Theorem 2. The Entity Hardening problem for Case II is NP Complete Proof: The Entity Hardening problem for case II is proved to be NP complete by giving a reduction from the Densest pSubhypergraph problem [13], a known NP-complete problem. An instance of the Densest p-Subhypergraph problem includes a hypergraph G = (V, E ), a parameter p and a parameter M . The problem asks the question whether there exists a set of vertices |V  |  V and |V  |  p such that the subgraph induced with this set of vertices has at least M hyperedges. From an instance of the Densest p-Subhypergraph problem we create an instance of the ENH problem in the following way. For each vertex vi and each hyperedge ej an entity bi and aj are added to the set B and A respectively. For each hyperedge ej with ej = {vm , vn , vq } (say) an IDR of form aj  bm bn bq is created. It is assumed that the value of K is set of |V |. The values of k and EF are set to p and |V | + |E | - p - M (where |A| = |V | and |B | = |E |) respectively. In the constructed instance only entities of set A are dependent on entities of set B . Additionally the dependency for an entity ai consists of conjunction of entities in set B . Hence for an entity ai  A to fail, either it itself has to fail initially or all entities to which ai is dependent on has to fail. It is to be noted that the entities in set B has no induced failure i.e., there is no cascade. Following from this assertion, with K = p, the solution A =  and B  = B would fail all entities in set A  B . Moreover this is the single unique solution to the problem instance. This is because by including one entity

For an interdependent network I (A, B, F (A, B )) the IDRs can be represented in four different forms. We analyze the computational complexity of the ENH problem for each of these cases separately. A. Case I: Problem Instance with One Minterm of Size One The IDRs of Case I have a single minterm of size 1. This can be represented as xi  yj , where xi and yj are entities of network A(B ) and B (A) respectively. We show that the ENH problem for Case I can be solved optimally in polynomial time. Algorithm 1: Entity Hardening Algorithm for systems with Case I type interdependencies
Data: An interdependent network I (A, B, F (A, B )), set of K most vulnerable entities A  B  , A  A, B   B , hardening budget k and a set H = . Result: Set of hardened entities H. begin For each entity xi  (A  B  ) compute the set of kill sets C = {Cx1 , Cx2 , ..., CxK }, where Cxi = KillSet(xi ) ; Create a copy D = {Dx1 , Dx2 , ..., DxK } of set C ; for (i=1; i  K; i++) do for (j=1, j = i; j  K; j++) do if Cxj  Cxi then Dxi  Dxi \ Dxj ; Choose the top k sets from D with highest cardinality ; For each of the Dxi  D sets chosen in Step 8, H  H  xi ; return H

1 2 3 4 5 6 7

8 9 10

Theorem 1. Algorithm 1 solves the Entity Hardening problem for Case I optimally in polynomial time. Proof: It is shown in [9] that the kill set for all entities in the interdependent network can be computed in O(n3 ) where n = |A| + |B |, thus computing the kill sets for K entities takes O(Kn2 ). Step 4-7 of the algorithm runs in O(K2 ). Choosing the k highest cardinality sets can be found using any standard sorting algorithm in O(Klog (K)). Hence Algorithm 1 runs in O(Kn2 ). For two kill sets Cxi and Cxj it can be shown that either Cxi  Cxj =  or Cxi  Cxj = Cxi or Cxi  Cxj = Cxj

4

ai in the initial failure set would result in not failing at least one entity bj for a given budget K = p. Hence it won't fail the entire set of entities in A  B . If an entity in set A is hardened then it would have no effect in failure prevention of any other entities. Whereas hardening an entity bm  B might result in failure prevention of an entity ai  A with IDR aj  bm bn bq provided that entities bn , bq are also defended. With k = p (and K  |V | = |B |) it can be ensured that entities to be defended are from set B  . To prove the theorem consider that there is a solution to the Densest p-Subhypergraph problem. Then there exist p vertices which induces a subgraph which has at least M hyperedges. Hardening the entities bi  B  for each vertex vi in the solution of the Densest p-Subhypergraph problem would then ensure that at least M entities in set A are protected from failure. This is because the entities in set A for which the failure is prevented corresponds to the hyperedges in the induced subgraph. Thus the number of entities that fail after hardening p entities is at most |V | + |E | - p - M , solving the ENH problem. Now consider that there is a solution to the ENH problem. As previously stated, the entities to be hardened will always be from set B  . So defending p entities from set B  would result in failure prevention of at least M entities in set A such that EF  |V | + |E | - p - M . Hence, the vertex induced subgraph would have at least M hyperedges when vertices corresponding to the entities hardened are included in the solution of the Densest p-Subhypergraph problem, thus solving it. Theorem 3. For an interdependent network I (A, B, F (A, B )) with n = |A  B | and F (A, B ) having IDRs of form Case II, it is hard to approximate the ENH problem within a factor of 1 for some  > 0. log(n)
2

set S we add an entity bi in set B . For all subsets in S , say Sp , Sm , Sn , which has the element xi there is an IDR of form ai  bm + bn + bl . The values of positive integers k and EF are set to M and m - M respectively. It is assumed that the value of K = m. With similar reasoning as that of Case II it can be shown that for K = m the maximum number of node failures (i.e. failure of all entities in A  B ) would occur if A =  and B  = B . This is also the single unique solution to the problem instance. The constructed instance also ensures that the entities to be hardened are from set B  (A not considered as it is equal to ). This is because protecting an entity ai  A would only result in prevention of its own failure whereas protecting an entity bj  B would result in failure prevention of its own and all other entities in set A for which it appears in its IDR. To begin with the proof, consider that there is a solution to the Set Cover problem. Then there exist M subsets (or elements in set S ) whose union results in the set S . Hardening the entities in set B corresponding to the subsets selected would ensure that all entities in set A are prevented from failure. This is because for the dependency of each entity ai  A there exist at least one entity (in set B ) that is hardened. Hence the number of entities that fails after hardening is m-M which is equal to EF , thus solving the ENH problem. Now, consider that there is a solution to the ENH problem. As discussed above the entities to be hardened should be from set B  . To achieve EF = m - M with k = M , no entities in the set A must fail. Hence for each entity ai  A at least one entity in set B that appears in its IDR has to be hardened. Thus, it directly follows that the union of subsets in set S corresponding to the entities hardened is equal to the set S , solving the Set Cover Problem. 1) Approximation Scheme for Case 3: In this subsection we provide an approximation algorithm for Case 3 of the problem. For an interdependent network I (A, B, F (A, B )) with the initial failed set of entities as A  B  we define Protection Set of each entity as follows. Definition: For an entity xi  A  B the Protection Set is defined as the entities that would be prevented from failure by hardening the entity xi when all entities in A  B  fails initially. This is represented as P (xi |A  B  ). The Protection Set of each entity can be computed in O((n + m)2 ) where n and m are the number of entities and number of minterms respectively in an interdependent network I (A, B, F (A, B )) . Theorem 5. For two entities xi , xj  A  B , P (xi |A  B  )  P (xj |A  B  ) = P (xi , xj |A  B  ) when IDRs are of form Case III. Proof: Assume that defending two entities xi and xj would result in preventing failure of P (xi , xj |A  B  ) entities with |P (xi |A  B  )  P (xj |A  B  )| < |P (xi , xj |A  B  )|. Then there exist at least one entity xp  / P (xi |A  B  )  P (xj |A  B  ) such that it's failure is prevented only if xi and xj is protected together. So two entities xm and xn (with xm  P (xi |A B  ) and xn  P (xj |A B  ) or vice versa) have to be

Proof: From Theorem 2, Densest p-Subhypergraph problem has been shown to be a special case of the ENH problem with IDRs of form Case II. Densest p-Subhypergraph problem is proved to be inapproximable within a factor of log1 2 (n) ( > 0) in [13]. Hence the theorem follows. C. Case III: Problem Instance with an Arbitrary Number of Minterm of Size One The IDRs of Case III have arbitrary number of minterm of size 1. This can be represented as xi  p q=1 yq , where xi and yq are entities of network A(B ) and B (A) respectively and the number of minterms are p. The ENH problem with respect to Case III is NP-complete and is proved in Theorem 4. Theorem 4. The ENH problem for Case III is NP Complete Proof: The ENH problem for case III is proved to be NP complete by giving a reduction from the Set Cover Problem, a well known NP-complete problem. An instance of the Set Cover problem includes a set S = {x1 , x2 , ..., xn }, a set S = {S1 , S2 , ..., Sm } where Si  S and a positive integer M . The problem asks the question whether there exists at most M subsets from set S whose union would result in the set S . From an instance of the set cover problem we create an instance of the ENH problem in the following way. For each element xi in set S we add an entity ai in set A. For each subset Si in

5

present in the IDR of xp . As the IDRs are of form Case III so if any one of xm or xn is protected then xp is protected, hence a contradiction. On the other way round P (xi , xj |A  B  ) contains all entities which would be prevented from failure if xi or xj is defended alone. So it directly follows that |P (xi |A  B  )  P (xj |A  B  )| > |P (xi , xj |A  B  )| is not possible. Hence the theorem holds. Theorem 6. There exists an 1 - 1 e approximation algorithm that approximates the ENH problem for Case III. Proof: The approximation algorithm is constructed by modeling the problem as Maximum Coverage problem. An instance of the maximum coverage problem consists of a set S = {x1 , x2 , ..., xn }, a set S = {S1 , S2 , ..., Sm } where Si  S and a positive integer M . The objective of the problem is to find a set S   S and |S  |  M such that Si S Si is maximized. For a given initial failure set A  B  with |A | + |B  |  K, let P (xi |A  B  ) denote the protection set for each entity xi  A  B . We construct a set S = A  B and for each entity xi a set Sxi  S such that Sxi = P (xi |A  B  ). Each set Sxi is added as an element of a set S . The conversion of the problem to Maximum Coverage problem can be done in polynomial time. By Theorem 5 defending a set of entities X  S would result in failure prevention of xi X Sxi entities. Hence, with the constructed sets S and S and a positive integer M (with M = k ) finding the Maximum Coverage would ensure the failure protection of maximum number of entities in A  B . This is same as the ENH problem of Case III. As there exists an 1 - 1 e approximation algorithm for the Maximum Coverage problem hence the theorem holds. D. Case IV: Problem Instance with an Arbitrary Number of Minterms of Arbitrary Size The IDRs of Case IV have arbitrary number of minterm of arbitrary size. This can be represented as xi  qj1 p j2 =1 yj2 , where xi and yj2 are entities of network j1 =1 A(B ) and B (A) respectively and there are p minterms each of size qj1 . Theorem 7. The Entity Hardening problem for Case IV is NP Complete Proof: Case II and Case III are special cases of Case IV. Hence following from Theorem 2 and Theorem 4 the computational complexity of the Entity Hardening problem is NP-complete in Case IV. V. S OLUTIONS TO
THE

It is to be noted that the maximum number cascading steps is upper bounded by |A| + |B | - 1 = m + n - 1. The objective function can now be formulated as follows:
m n

min
i=1

xi(m+n-1) +
j =1

yj (m+n-1)

(1)

The objective in (1) minimizes the number of entities failed after the cascading failure with the respective constraints for the Entity Hardening problem as follows:
n m

Constraint Set 1:
i=1

qxi +
j =1

qyj = k , with qxi , qyj  [0, 1].

If an entity xi (yj ) is defended then qxi = 1 (qyj = 1) and 0 otherwise. Constraint Set 2: xi0  gi - qxi and yi0  hi - qyi . This constraint implies that only if an entity is not defended and gi (hi ) is 1 then the entity will fail at the initial time step. Constraint Set 3: xid  xi(d-1) , d, 1  d  m + n - 1, and yid  yi(d-1) , d, 1  d  m + n - 1, in order to ensure that for an entity which fails in a particular time step would remain in failed state at all subsequent time steps. Constraint Set 4: Modeling of the constraint to capture the cascade propagation for IIM is similar to the constraints established in [9]. A brief presentation of this constraint is provided here. Consider an IDR ai  bj bp bl + bm bn + bq of type Case IV. The following steps are enumerated to depict the cascade propagation: Step 1: Replace all minterms of size greater than one with a variable. In the example provided we have the transformed minterm as ai  c1 + c2 + bq with c1  bj bp bl and c2  bm bn (c1 , c2  {0, 1}) as the new IDRs. Note that after transformation, the original IDR is in the form of Case III and the introduced IDRs are in the form of Case II. Step 2: For each variable c, a constraints is added to capture the cascade propagation. Let N be the number of entities in the minterm on which c is dependent. In the example for the variable c1 with IDR c1  bj bp bl , constraints y +y d-1) +yl(d-1) c1d  j(d-1) p(N and c1d  yj (d-1) + yp(d-1) + yl(d-1) d, 1  d  m + n - 1 are introduced (with N = 3 in this case). If IDR of an entity is already in form of Case II, i.e.,ai  bj bp bl then constraints xid  yj(d-1) +yp(d-1) +yl(d-1) - qxi and xid  yj (d-1) + yp(d-1) + N yl(d-1) d, 1  d  m + n - 1 are introduced (with N = 3). These constraints satisfies that if the entity xi is hardened initially then it is not dead at any time step. Step 3: Let M be the number of minterms in the transformed IDR as described in Step 1. In the given example with IDR ai  c1 + c2 + bq constraints of form xid  c1(d-1) + c2(d-1) + yq(d-1) - (M - 1) - qxi and xid  c1(d-1) +c2(d-1) +yq(d-1) d, 1  d  m + n - 1 are introduced. M These constraints ensures that even if all the minterms of xi has at least one entity in dead state then it will be alive if the entity is hardened initially. For all IDRs of type Case I and Case III, the constraint discussed in this step is used.

E NTITY H ARDENING P ROBLEM

A. Optimal Solution using Integer Linear Programming We propose an Integer Linear Program (ILP) that solves the Entity Hardening problem optimally. Let [G, H ] with G = {g1 , g2 , ..., gn } and H = {h1 , h2 , ..., hm } denote the entities in set A and B respectively with hi = 0 (gj = 0) if entity ai (bj ) is alive and hi = 1 (gj = 1) otherwise. Given an integer k let [G, H ] be the solution (with value of 1 corresponding to entities failed initially) that cause maximum number of entity failure. Two variables xid and yjd are used in the ILP with xid = 1 (yjd = 1), when entity ai  A (bj  B ) is in a failed state at time step d, and 0 otherwise. The number of entities to be defended is considered to be k .

6

B. Heuristic In this subsection we provide a greedy heuristic solution to the Entity Hardening problem. For an interdependent network I (A, B, F (A, B )) with the initial failed set of entities as A  B  , Protection Set of each entity has been defined in the approximation scheme of Case III. To design the heuristic we define Minterm Coverage Number of each entity in A  B as follows: Definition: For an entity xi  A  B the Minterm Coverage Number is defined as the number of minterms that can be removed from F (A, B ) without affecting the cascading process by hardening the entity xi when all entities in A  B  fails initially. This is represented as M (xi |A  B  ). Similar to the computation of Protection Set the Minterm Coverage Number of each entity can be computed in O((n + m)2 ). With these definitions the heuristic is given in Algorithm 2. The algorithm takes in as input an interdependent network I (A, B, F (A, B )) with S = A  B . Step 4-5 is done to reduce the search space as it directly follows that the set of entities in Q wouldn't effect the hardening process. In each iteration of the while loop an entity xd is greedily selected which when hardened would prevent failure of maximum number of entities. This ensures that at each step the number of entities failed is minimized. In case of a tie, among all entities involved in the tie, the entity having the highest Minterm Coverage Number is included in the solution. This gives a higher priority to the entity which when hardened, has more impact on failure minimization in subsequent iterations of the while loop. The interdependent network I (A, B, F (A, B )) is updated in steps 13-16 of the algorithm. This takes into account the effect of hardening an entity in the current iteration on entities hardened in the following iterations. Run Time Analysis of Algorithm 2: For this analysis we consider n to be the total number of entities and m to be the total number of minterms. Updates in step 4 can be done in O(m) and step 5 in O(n). The while loop iterates for k times. In each iteration of the while loop step 7 and step 8 takes at most O((n + m)2 ) and O(nlog (n)) time respectively. On branching in step 9, step 10 and step 11 takes O((n + m)2 ) and O(nlog (n)) time respectively. Updates in step 13 takes O(n) time and in step 14 takes O(n + m) time. Step 12, 16 and 17 runs in constant time. Hence Algorithm 2 runs in O(k (n + m)2 ) time. VI. E XPERIMENTAL R ESULTS

Algorithm 2: Heuristic Solution to the ENH Problem
Data: An interdependent network I (A, B, F (A, B )) (with S = A  B ), set of entities A  B  failed initially causing maximum failure in the interdependent network with |A | + |B  | = K and hardening budget k. Result: Set of hardened entities H. begin Initialize S   A  B  ; Initialize H = ; Update F (A, B ) as follows -- (a) let Q be the set of entities that does not fail on failing K entities, (b) remove IDRs corresponding to entities in set Q, (c) remove from minterm of entities not in set Q all entities which are in set Q ; Update S = S \ Q ; while (k entities are not hardened) do For each entity xi  S compute the Protection Set P (xi |S  ); Choose the entity xd with highest cardinality of the set |P (xd |S  )|; if (more than one entity has the same highest cardinality value) then For each such entity xj compute the Minterm Coverage Number M (xj |S  ) ; Choose the entity xd with highest Minterm Coverage Number. ; In case of a tie choose arbitrarily; Update S  S - P (xd |S  ); Update F (A, B ) by removing (i) IDRs corresponding to all entities in P (xd |S  ), and (ii) occurrence of these entities in IDRs of entities not in P (xd |S  ); if (xd  S  ) then Update S   S  - {xd }; Update H = H  xd ; return H ;

1 2 3 4

5 6 7 8 9 10 11 12 13 14

15 16 17 18

located within the geographic region formed the set A and B respectively. Each region was represented by an interdependent network I (A, B, F (A, B )). We use the IDR construction rules as defined in [9] to generate F (A, B ).

In this section we present the experimental results of the Entity Hardening problem by comparing the optimal solution computed using an ILP, and the proposed heuristic algorithm. The experiments were conducted on real world power grid data obtained from Platts (www.platts.com), and communication network data obtained from GeoTel (www.geo-tel.com) of Maricopa County, Arizona. The data consisted of 70 power plants and 470 transmission lines in the power network, and 2, 690 cell towers, 7, 100 fiber-lit buildings and 42, 723 fiber links in the communication network. We identified five nonintersecting geographical regions from the data set and labeled them from regions 1 through 5. For each of the regions, the entities of the power and communication network that were

In all of our simulations IBM CPLEX Optimizer 12.5 to solve ILPs and Python 3 for heuristic is used. To analyze the Entity Hardening problem the value of K was set to 8. The ILP in [9] was used to compute the K most vulnerable nodes in the network, and the set of failed entities due to the failure of the K entities was also computed. For the five regions, when the K = 8 most vulnerable nodes failed, the total number of failed entities in the network were 28, 23, 28, 28 and 27 respectively. With the K most vulnerable nodes and final set of failed nodes as input, the ILP and heuristic of the Entity Hardening problem are compared with k = 1, 3, 5, 7. The results of these simulations are shown in Figure 1. It is observed that the heuristic solution differs more from optimal at higher values of k (factor of 0.5 and 0.67 for Regions 1 and 3 respectively with k = 7). This is primarily because of the greedy nature of Algorithm 2. However on an average the heuristic solution differs by a factor of 0.13 from the optimal.

7

14 Number of entities failed 12 10 8 6 4 2 0 1 12

13

Number of entities failed

12 10 8 6 4 2 0 1 3 7 7

Number of entities failed

ILP solution Heuristic

14 13

13

14 ILP solution Heuristic 12 10 8 6 4 2 0 1 12

13

ILP solution Heuristic

8 6 5 3 1 3 5 7 Number of entities hardened 3

7 6 4 3 2 1 3 5 7 Number of entities hardened

3 1 1

3 5 7 Number of entities hardened

(a) Region 1
12 Number of entities failed 10 8 6 4 2 0 1 6 5 4 3 2 1 3 5 7 Number of entities hardened 11 11

(b) Region 2
8 Number of entities failed ILP solution Heuristic 7 6 5 4 3 2 1 0 1 3 5 7 Number of entities hardened 1 3 5 7

(c) Region 3
ILP solution Heuristic

(d) Region 4

(e) Region 5

Fig. 1: Comparison chart of the optimal solution (ILP) with the heuristic by varying number of entities hardened for each identified region

VII.

C ONCLUSION

In this paper we studied the entity hardening problem in multi-layer networks. We modeled the interdependencies shared between the networks using IIM, and formulated the the Entity Hardening problem in this setting. We showed that the problem is solvable in polynomial time for some cases, whereas for others it is NP-complete. We evaluated the efficacy of our heuristic using power and communication network data of Maricopa County, Arizona. Our experiments showed that our heuristic almost always produces near optimal results. R EFERENCES
[1] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin, "Catastrophic cascade of failures in interdependent networks," Nature, vol. 464, no. 7291, pp. 1025­1028, 2010. J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, "Networks formed from interdependent networks," Nature Physics, vol. 8, no. 1, pp. 40­48, 2011. J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, "Cascade of failures in coupled network systems with multiple support-dependence relations," Physical Review E, vol. 83, no. 3, p. 036116, 2011. V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and R. Setola, "Modelling interdependent infrastructures using interacting dynamical models," International Journal of Critical Infrastructures, vol. 4, no. 1, pp. 63­79, 2008. P. Zhang, S. Peeta, and T. Friesz, "Dynamic game theoretic model of multi-layer infrastructure networks," Networks and Spatial Economics, vol. 5, no. 2, pp. 147­178, 2005. M. Parandehgheibi and E. Modiano, "Robustness of interdependent networks: The case of communication networks and the power grid," arXiv preprint arXiv:1304.0356, 2013. D. T. Nguyen, Y. Shen, and M. T. Thai, "Detecting critical nodes in interdependent power networks for vulnerability assessment," 2013.

[2]

[3]

[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman, "Power grid vulnerability to geographically correlated failures-analysis and control implications," arXiv preprint arXiv:1206.1099, 2012. [9] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, "Identification of k most vulnerable nodes in multi-layered network using a new model of interdependency," in Computer Communications Workshops (INFOCOM WKSHPS), 2014 IEEE Conference on. IEEE, 2014, pp. 831­836. [10] J.-F. Castet and J. H. Saleh, "Interdependent multi-layer networks: Modeling and survivability analysis with applications to space-based networks," PloS one, vol. 8, no. 4, p. e60402, 2013. [11] A. Das, J. Banerjee, and A. Sen, "Root cause analysis of failures in interdependent power-communication networks," in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910­915. [12] A. Mazumder, C. Zhou, A. Das, and A. Sen, "Progressive recovery from failure in multi-layered interdependent network using a new model of interdependency," in Conference on Critical Information Infrastructures Security (CRITIS), 2014. Springer, 2014. [13] M. Hajiaghayi, K. Jain, K. Konwar, L. Lau, I. Mandoiu, A. Russell, A. Shvartsman, and V. Vazirani, "The minimum k-colored subgraph problem in haplotyping and dna primer selection," in Proceedings of the International Workshop on Bioinformatics Research and Applications (IWBRA). Citeseer, 2006.

[4]

[5]

[6]

[7]

On the Entity Hardening Problem in Multi-layered
Interdependent Networks
Joydeep Banerjee, Arun Das, Chenyang Zhou, Anisha Mazumder and Arunabha Sen

arXiv:1412.6686v1 [cs.NI] 20 Dec 2014

Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {joydeep.banerjee, arun.das, czhou24, anisha.mazumder, asen}@asu.edu
AbstractâThe power grid and the communication network
are highly interdependent on each other for their well being.
In recent times the research community has shown significant
interest in modeling such interdependent networks and studying
the impact of failures on these networks. Although a number
of models have been proposed, many of them are simplistic in
nature and fail to capture the complex interdependencies that
exist between the entities of these networks. To overcome the
limitations, recently an Implicative Interdependency Model that
utilizes Boolean Logic, was proposed and a number of problems
were studied. In this paper we study the âentity hardeningâ
problem, where by âentity hardeningâ we imply the ability of the
network operator to ensure that an adversary (be it Nature or
human) cannot take a network entity from operative to inoperative
state. Given that the network operator with a limited budget
can only harden k entities, the goal of the entity hardening
problem is to identify the set of k entities whose hardening will
ensure maximum benefit for the operator, i.e. maximally reduce
the ability of the adversary to degrade the network. We show
that the problem is solvable in polynomial time for some cases,
whereas for others it is NP-complete. We provide the optimal
solution using ILP, and propose a heuristic approach to solve the
problem. We evaluate the efficacy of our heuristic using power
and communication network data of Maricopa County, Arizona.
The experiments show that our heuristic almost always produces
near optimal results.

I.

I NTRODUCTION

The critical infrastructures of a nation form a complex symbiotic ecosystem where individual infrastructures are heavily
interdependent on each other for being fully functional. Two
such critical systems that rely heavily on each other for their
well being are the power and communication network infrastructures. For instance, power grid entities such as SCADA
systems, that are used to remotely operate power generation
units, receive their control commands over the communication
network infrastructure, while communication network entities
such as routers and base stations are inoperable without electric
power. Thus, failure introduced in the system either by Nature
(hurricanes), or man (terrorist attacks), can trigger further
failures in the system due to interdependencies between the
entities of the two infrastructures.
Although a number of models have been proposed for modeling and analysis of interdependent multi-layered networks
[1], [2], [3], [4], [5], [6], [7], [8], many of these models are simplistic in nature and fail to capture the complex interdependencies that exists between the entities of these networks. As noted
in [9], these models fail to model complex interdependencies

that may exist between network entities, such as when entity ai
is operational, if entities (i) bj and bk and bl are operational, or
(ii) bm and bn are operational, or (iii) bp is operational. Graph
based interdependency models proposed in the literature such
as [3], [4], [5], [10], [6], [7] including [1], [2] cannot capture
such complex interdependency involving both conjunctive and
disjunctive terms between entities of multi-layer networks. To
overcome these limitations, an Implicative Interdependency
Model that utilizes Boolean Logic, was recently proposed in
[9], and a number of problems including computation of K
most vulnerable nodes [9], root cause of failure analysis [11],
and progressive recovery from failures [12], were studied using
this model.
In this paper we study the âentity hardeningâ problem in
the interdependent power-communication network using the
Implicative Interdependency Model (IIM). By âentity hardeningâ, we imply the ability of the network operator to ensure
that an adversary (be it Nature or human), cannot take a network entity from an operative (operational) to an inoperative
(failed) state. We assume that the adversary is clever and
is capable of identifying the most vulnerable entities in the
network that causes maximum damage to the interdependent
system. However, the adversary does not have an unlimited
budget and has the resources to destroy at most K entities
of the interdependent network. The network operator is also
aware of adversaryâs target entities for destruction. Since we
assume that once an entity is âhardenedâ by the network
operator it cannot be destroyed by the adversary, if all K
targets of the adversary are hardened by the network operator,
then the adversary cannot induce any failure in the network.
However, if due to resource limitations the network operator
is able to strengthen only k entities, where k < K, these k
entities have to be carefully chosen. The goal of the entity
hardening problem is to identify the set of k entities whose
hardening will ensure maximum benefit for the operator, i.e.
maximally reduce the ability of the adversary to degrade the
network.
We classify the entity hardening problem into four different
cases depending on the nature of the interdependency relationships. We show that the first case can be solved in polynomial
time, and all other cases are shown to be NP-complete. We
provide an inapproximability result for the second case, an
approximation algorithm for the third case, and a heuristic
for the fourth (general) case. We evaluate the efficacy of our
heuristic using power and communication network data of
Maricopa County, Arizona. The experiments show that our

2

heuristic almost always produces near optimal results.
The paper is organized as follows, the IIM model is
presented in Section II, in Sections III and IV we formally
state the entity hardening problem and analyze its computational complexity, Section V outlines the optimal and heuristic
solutions to the problem, Section VI shows the experimental
results, and finally Section VII concludes this paper.
II.

I NTERDEPENDENCY M ODEL

We now present an overview of the underlying IIM interdependency model [9]. IIM uses Boolean Logic to model
the interdependencies between network entities, these interdependent relationships are termed as Implicative Interdependency Relations (IDRs). We represent this interdependent network setting as I(A, B, F (A, B)), where sets A
and B are the power and communication network entities
respectively, and F (A, B) is the set of dependency relations,
or IDRs. Table I represents a sample interdependent network I(A, B, F (A, B)), where A = {a1 , a2 , a3 , a4 }, B =
{b1 , b2 , b3 } and F (A, B) is the set of IDRs (dependency
relations) between the entities of A and B. In this example,
the IDR b1 â a1 a3 + a2 implies that entity b1 is operational
when both the entities a1 and a3 are operational, or entity a2
is operational. The conjunction of entities, such as a1 a3 , is
also referred to as a minterm.
Power Network
a1 â b1 b2
a2 â b1 + b2
a3 â b1 + b2 + b3
a4 â b1 + b3

Comm. Network
b1 â a1 a3 + a2
b2 â a1 a2 a3
b3 â a1 + a2 + a3
ââ

TABLE I: Implicative Interdependency Relations of a sample network

Given a set of inoperable (failed) entities, a time stepped
failure cascade can be derived from the dependency relationships outlined in the IDR set. For example, for the interdependent network outlined in Table I, Table II shows the failure
propagation when entities {a2 , b3 } fail at the initial time step
(t = 0). It may be noted that the model assumes that dependent
entities fail immediately in the next time step, for example,
when {a2 , b3 } fail at t = 0, b2 fails at t = 1 as b2 is dependent
on a2 for its survival. The system reaches a steady state when
the failure propagation process stops. In this example, when
{a2 , b3 } fail at t = 0, the steady state is reached at time step
t = 4.
Entities
a1
a2
a3
a4
b1
b2
b3

0

1

0
1
0
0
0
0
1

0
1
0
0
0
1
1

Time Steps (t)
2
3
4
1
1
0
0
0
1
1

1
1
0
0
1
1
1

1
1
1
1
1
1
1

5

6

1
1
1
1
1
1
1

1
1
1
1
1
1
1

TABLE II: Failure cascade propagation when entities {a2 , b3 } fail at
time step t = 0. A value of 1 denotes entity failure, and 0 otherwise

A primary consideration for using this model is the accurate
formulation of the IDRs that is representative of the underlying
physical power and communication network infrastructures.
This can either be done by careful analysis as done in [8], or
by consultation with experts of these infrastructures. We utilize
IIM to model the interdependency between the two networks
and analyze the entity hardening problem in this setting.

III.

P ROBLEM F ORMULATION

Before we make a formal statement of the entity hardening
problem in the IIM setting, we explain it with the help of an
example. Consider an interdependent system as outlined in the
IDR set shown in Table I. It may be easily checked that when
the adversary budget is K= 2, the most vulnerable entities
of this system are {a2 , b3 }. If the network operator doesnât
harden any one of the entities a2 or b3 , then in this example
all the network entities eventually fail, as seen from the fault
propagation in Table II. When the network operator chooses
to harden both a2 and b3 then none of the entities in the
network fail if the adversary restricts the attack only to the two
most vulnerable entities of the network, which in this example
happens to be {a2 , b3 }. If the network operator has resources
to harden only one entity and the operator chooses to harden
a2 , the destruction of b3 by the adversary will eventually lead
to the failure of no other entities of the network, as shown in
Table III(a). If on the other hand, the network operator chooses
to harden b3 , destruction by the adversary of a2 will eventually
lead to the failure of the entities {a2 , b2 , a1 , b1 } as shown in
Table III(b). Clearly in this scenario the operator should harden
a2 instead of b3 .
Definition: Kill Set of a set of Entities(S): The kill set of a
set of entities S, is the set of all entities that will eventually
fail due to failure of S and the interdependencies between the
entities of the network as given by the set of IDRâs. The kill
set of a set of entities S is denoted by KillSet(S).
It may be noted that the search for k entities to be hardened
is restricted to the KillSet(S), where S is the set of K
most vulnerable entities in the network, because hardening any
entity not in KillSet(S) does not provide any benefit to the
network operator. In this study we also assume that the set of
K most vulnerable entities in the network is unique.
Entities
0
a1
a2
a3
a4
b1
b2
b3

0
â
0
0
0
0
1

Time Steps (t)
1
2
3
0
â
0
0
0
0
1

0
â
0
0
0
0
1

0
â
0
0
0
0
1

(a) Entity a2 is hardened

Entities
4
0
â
0
0
0
0
0

0
a1
a2
a3
a4
b1
b2
b3

0
1
0
0
0
0
â

Time Steps (t)
1
2
3
0
1
0
0
0
1
â

1
1
0
0
0
1
â

1
1
0
0
1
1
â

4
1
1
0
0
1
1
â

(b) Entity b3 is hardened

TABLE III: Failure cascade propagation with entity hardening. Entities {a2 , b3 } are attacked at time step t = 0. A value of 1 denotes
entity failure, 0 otherwise. â denotes a hardened entity.

We now proceed to formulate the entity hardening
problem formally. Given an interdependent network system
I(A, B, F (A, B)), and the set of K most vulnerable entities
of the system Aâ² âª B â² , where Aâ² â A and B â² â B:
The Entity Hardening (ENH) problem
INSTANCE: Given:
(i) An interdependent network system I(A, B, F (A, B)),
where the sets A and B represent the entities of the two
networks, and F (A, B) is the set of IDRs.
(ii) The set of K most vulnerable entities of the system
Aâ² âª B â² , where Aâ² â A and B â² â B
(iii) Two positive integers k, k < K and EF .

3

QUESTION:Is there a set of entities H = Aâ²â² âª B â²â² , Aâ²â² â
A, B â²â² â B, |H| â¤ k, such that hardening H entities results
in no more than EF entities to fail after entities Aâ² âª B â² fail
at time step t = 0.
We note some of the assumptions for the ENH problem:
First, we assume that once an entity is hardened, it is always
operational and does not fail at any time step of the observation, even when the entity is part of the K most vulnerable
entities. Second, we assume that k < K, as otherwise the
selection of K entities for hardening ensures that no entities
fail at all. Finally, as noted earlier, we assume that the set of
K most vulnerable entities in the network is unique. We now
proceed to analyze the computational complexity of the ENH
problem.
IV.

C OMPUTATIONAL C OMPLEXITY A NALYSIS

For an interdependent network I(A, B, F (A, B)) the IDRs
can be represented in four different forms. We analyze the
computational complexity of the ENH problem for each of
these cases separately.
A. Case I: Problem Instance with One Minterm of Size One
The IDRs of Case I have a single minterm of size 1. This
can be represented as xi â yj , where xi and yj are entities of
network A(B) and B(A) respectively. We show that the ENH
problem for Case I can be solved optimally in polynomial time.
Algorithm 1: Entity Hardening Algorithm for systems
with Case I type interdependencies

1
2
3
4
5
6
7

8
9
10

Data: An interdependent network I(A, B, F(A, B)), set of
K most vulnerable entities Aâ² âª B â² , Aâ² â A, B â² â B,
hardening budget k and a set H = â.
Result: Set of hardened entities H.
begin
For each entity xi â (Aâ² âª B â² ) compute the set of kill sets
C = {Cx1 , Cx2 , ..., CxK }, where Cxi = KillSet(xi ) ;
Create a copy D = {Dx1 , Dx2 , ..., DxK } of set C ;
for (i=1; i â¤ K; i++) do
for (j=1, j 6= i; j â¤ K; j++) do
if Cxj â Cxi then
Dxi â Dxi \ Dxj ;
Choose the top k sets from D with highest cardinality ;
For each of the Dxi â D sets chosen in Step 8,
H â H âª xi ;
return H

Theorem 1. Algorithm 1 solves the Entity Hardening problem
for Case I optimally in polynomial time.
Proof: It is shown in [9] that the kill set for all entities in
the interdependent network can be computed in O(n3 ) where
n = |A| + |B|, thus computing the kill sets for K entities takes
O(Kn2 ). Step 4-7 of the algorithm runs in O(K2 ). Choosing
the k highest cardinality sets can be found using any standard
sorting algorithm in O(Klog(K)). Hence Algorithm 1 runs in
O(Kn2 ).
For two kill sets Cxi and Cxj it can be shown that either
Cxi â© Cxj = â or Cxi â© Cxj = Cxi or Cxi â© Cxj = Cxj

[9]. So with two entities {xi , xj } â Aâ² âª B â² and Cxi â© Cxj =
Cxj i.e, Cxj â Cxi , if xi is hardened it prevents the failure
of Cxi â Cxj entities (provided that none of the entities in
Cxi â Cxj â {xi } are in Aâ² âª B â² ). With this assertion, for
an entity xi â Aâ² â© B â² , steps 4-7 of Algorithm 1 finds the
actual entities for which failure is prevented by hardening xi .
The set D = {Dx1 , Dx2 , ..., DxK } comprises of these set of
entities for each hardened entity xi .
To prove that Algorithm 1 finds the optimal solution we
make the following two assertions: First, consider any two sets
Dxi and Dxj . It is implied from step 6 of Algorithm 1 that
/ Aâ² âª B â² is
Dxi â© Dxj = â. Second, consider an entity xp â
hardened. If xp fails when entities in Aâ² âªB â² fails initially then
it would belong to some set Dxi . Thus hardening xp results
in preventing the failure of entities that is a proper subset of
Dxi . Hence the entities to be hardened must belong to Aâ² âª B â²
only. Owing to the two assertions it directly follows that with
a given budget k, hardening k highest cardinality sets from the
set D ensures prevention of failure for the maximum number
of entities.
B. Case II: Problem Instance with One Minterm of Arbitrary
Size
The IDRs of Case II have a single
Qpminterm of arbitrary
size. This can be represented as xi â j=1 yj , where xi and
yj are entities of network A(B) and B(A) respectively and the
size of the minterm is p. The Entity Hardening problem with
respect to Case II is NP-complete and is proved in Theorem
2. An inapproximability proof for this case of the problem is
given in Theorem 3
Theorem 2. The Entity Hardening problem for Case II is NP
Complete
Proof: The Entity Hardening problem for case II is proved
to be NP complete by giving a reduction from the Densest pSubhypergraph problem [13], a known NP-complete problem.
An instance of the Densest p-Subhypergraph problem includes
a hypergraph G = (V, E), a parameter p and a parameter M .
The problem asks the question whether there exists a set of
vertices |V â² | â V and |V â² | â¤ p such that the subgraph induced
with this set of vertices has at least M hyperedges. From an
instance of the Densest p-Subhypergraph problem we create
an instance of the ENH problem in the following way. For
each vertex vi and each hyperedge ej an entity bi and aj are
added to the set B and A respectively. For each hyperedge ej
with ej = {vm , vn , vq } (say) an IDR of form aj â bm bn bq is
created. It is assumed that the value of K is set of |V |. The
values of k and EF are set to p and |V | + |E| â p â M (where
|A| = |V | and |B| = |E|) respectively.
In the constructed instance only entities of set A are
dependent on entities of set B. Additionally the dependency
for an entity ai consists of conjunction of entities in set B.
Hence for an entity ai â A to fail, either it itself has to fail
initially or all entities to which ai is dependent on has to fail.
It is to be noted that the entities in set B has no induced failure
i.e., there is no cascade. Following from this assertion, with
K = p, the solution Aâ² = â and B â² = B would fail all entities
in set A âª B. Moreover this is the single unique solution to
the problem instance. This is because by including one entity

4

ai in the initial failure set would result in not failing at least
one entity bj for a given budget K = p. Hence it wonât fail
the entire set of entities in A âª B.
If an entity in set A is hardened then it would have no effect
in failure prevention of any other entities. Whereas hardening
an entity bm â B might result in failure prevention of an entity
ai â A with IDR aj â bm bn bq provided that entities bn , bq
are also defended. With k = p (and K â¤ |V | = |B|) it can be
ensured that entities to be defended are from set B â² .
To prove the theorem consider that there is a solution to the
Densest p-Subhypergraph problem. Then there exist p vertices
which induces a subgraph which has at least M hyperedges.
Hardening the entities bi â B â² for each vertex vi in the solution
of the Densest p-Subhypergraph problem would then ensure
that at least M entities in set A are protected from failure.
This is because the entities in set A for which the failure
is prevented corresponds to the hyperedges in the induced
subgraph. Thus the number of entities that fail after hardening
p entities is at most |V | + |E| â p â M , solving the ENH
problem. Now consider that there is a solution to the ENH
problem. As previously stated, the entities to be hardened will
always be from set B â² . So defending p entities from set B â²
would result in failure prevention of at least M entities in set
A such that EF â¤ |V | + |E| â p â M . Hence, the vertex
induced subgraph would have at least M hyperedges when
vertices corresponding to the entities hardened are included
in the solution of the Densest p-Subhypergraph problem, thus
solving it.
Theorem 3. For an interdependent network I(A, B, F (A, B))
with n = |A âª B| and F (A, B) having IDRs of form Case II,
it is hard to approximate the ENH problem within a factor of
1
for some Î» > 0.
log(n)Î»
2

Proof: From Theorem 2, Densest p-Subhypergraph problem has been shown to be a special case of the ENH problem
with IDRs of form Case II. Densest p-Subhypergraph problem
1
is proved to be inapproximable within a factor of log(n)
Î»
2
(Î» > 0) in [13]. Hence the theorem follows.
C. Case III: Problem Instance with an Arbitrary Number of
Minterm of Size One
The IDRs of Case III have arbitrary number
P of minterm of
size 1. This can be represented as xi â pq=1 yq , where xi
and yq are entities of network A(B) and B(A) respectively
and the number of minterms are p. The ENH problem with
respect to Case III is NP-complete and is proved in Theorem
4.
Theorem 4. The ENH problem for Case III is NP Complete
Proof: The ENH problem for case III is proved to be NP
complete by giving a reduction from the Set Cover Problem,
a well known NP-complete problem. An instance of the Set
Cover problem includes a set S = {x1 , x2 , ..., xn }, a set S =
{S1 , S2 , ..., Sm } where Si â S and a positive integer M . The
problem asks the question whether there exists at most M
subsets from set S whose union would result in the set S. From
an instance of the set cover problem we create an instance of
the ENH problem in the following way. For each element xi
in set S we add an entity ai in set A. For each subset Si in

set S we add an entity bi in set B. For all subsets in S, say
Sp , Sm , Sn , which has the element xi there is an IDR of form
ai â bm + bn + bl . The values of positive integers k and EF
are set to M and m â M respectively. It is assumed that the
value of K = m.
With similar reasoning as that of Case II it can be shown
that for K = m the maximum number of node failures (i.e.
failure of all entities in A âª B) would occur if Aâ² = â and
B â² = B. This is also the single unique solution to the problem
instance.
The constructed instance also ensures that the entities to
be hardened are from set B â² (Aâ² not considered as it is equal
to â). This is because protecting an entity ai â A would only
result in prevention of its own failure whereas protecting an
entity bj â B would result in failure prevention of its own and
all other entities in set A for which it appears in its IDR.
To begin with the proof, consider that there is a solution
to the Set Cover problem. Then there exist M subsets (or
elements in set S) whose union results in the set S. Hardening
the entities in set B corresponding to the subsets selected
would ensure that all entities in set A are prevented from
failure. This is because for the dependency of each entity
ai â A there exist at least one entity (in set B) that is hardened.
Hence the number of entities that fails after hardening is mâM
which is equal to EF , thus solving the ENH problem. Now,
consider that there is a solution to the ENH problem. As
discussed above the entities to be hardened should be from
set B â² . To achieve EF = m â M with k = M , no entities
in the set A must fail. Hence for each entity ai â A at least
one entity in set B that appears in its IDR has to be hardened.
Thus, it directly follows that the union of subsets in set S
corresponding to the entities hardened is equal to the set S,
solving the Set Cover Problem.
1) Approximation Scheme for Case 3: In this subsection we
provide an approximation algorithm for Case 3 of the problem.
For an interdependent network I(A, B, F (A, B)) with the
initial failed set of entities as Aâ² âª B â² we define Protection
Set of each entity as follows.
Definition: For an entity xi â A âª B the Protection Set is
defined as the entities that would be prevented from failure
by hardening the entity xi when all entities in Aâ² âª B â² fails
initially. This is represented as P (xi |Aâ² âª B â² ).
The Protection Set of each entity can be computed in
O((n + m)2 ) where n and m are the number of entities and
number of minterms respectively in an interdependent network
I(A, B, F (A, B)) .
Theorem 5. For two entities xi , xj â A âª B, P (xi |Aâ² âª B â² ) âª
P (xj |Aâ² âª B â² ) = P (xi , xj |Aâ² âª B â² ) when IDRs are of form
Case III.
Proof: Assume that defending two entities xi and xj
would result in preventing failure of P (xi , xj |Aâ² âª B â² ) entities
with |P (xi |Aâ² âª B â² ) âª P (xj |Aâ² âª B â² )| < |P (xi , xj |Aâ² âª B â² )|.
Then there exist at least one entity xp â
/ P (xi |Aâ² âª B â² ) âª
P (xj |Aâ² âª B â² ) such that itâs failure is prevented only if xi and
xj is protected together. So two entities xm and xn (with xm â
P (xi |Aâ² âªB â² ) and xn â P (xj |Aâ² âªB â² ) or vice versa) have to be

5

present in the IDR of xp . As the IDRs are of form Case III so if
any one of xm or xn is protected then xp is protected, hence
a contradiction. On the other way round P (xi , xj |Aâ² âª B â² )
contains all entities which would be prevented from failure
if xi or xj is defended alone. So it directly follows that
|P (xi |Aâ² âª B â² ) âª P (xj |Aâ² âª B â² )| > |P (xi , xj |Aâ² âª B â² )| is
not possible. Hence the theorem holds.
Theorem 6. There exists an 1 â 1e approximation algorithm
that approximates the ENH problem for Case III.
Proof: The approximation algorithm is constructed by
modeling the problem as Maximum Coverage problem. An
instance of the maximum coverage problem consists of a
set S = {x1 , x2 , ..., xn }, a set S = {S1 , S2 , ..., Sm } where
Si â S and a positive integer M . The objective of the problem
is to find a set S â² â S and |S â² | â¤ M such that âªSi âS Si
is maximized. For a given initial failure set Aâ² âª B â² with
|Aâ² |+|B â² | â¤ K, let P (xi |Aâ² âªB â² ) denote the protection set for
each entity xi â A âª B. We construct a set S = A âª B and for
each entity xi a set Sxi â S such that Sxi = P (xi |Aâ² âª B â² ).
Each set Sxi is added as an element of a set S. The conversion
of the problem to Maximum Coverage problem can be done
in polynomial time. By Theorem 5 defending a set of entities
X â S would result in failure prevention of âªxi âX Sxi entities.
Hence, with the constructed sets S and S and a positive integer
M (with M = k) finding the Maximum Coverage would
ensure the failure protection of maximum number of entities in
A âª B. This is same as the ENH problem of Case III. As there
exists an 1 â 1e approximation algorithm for the Maximum
Coverage problem hence the theorem holds.
D. Case IV: Problem Instance with an Arbitrary Number of
Minterms of Arbitrary Size
The IDRs of Case IV have arbitrary number of minterm
of
Pp arbitrary
Qqj1 size. This can be represented as xi â
j2 =1 yj2 , where xi and yj2 are entities of network
j1 =1
A(B) and B(A) respectively and there are p minterms each
of size qj1 .
Theorem 7. The Entity Hardening problem for Case IV is NP
Complete
Proof: Case II and Case III are special cases of Case
IV. Hence following from Theorem 2 and Theorem 4 the
computational complexity of the Entity Hardening problem is
NP-complete in Case IV.
V.

S OLUTIONS TO

THE

E NTITY H ARDENING P ROBLEM

A. Optimal Solution using Integer Linear Programming
We propose an Integer Linear Program (ILP) that solves
the Entity Hardening problem optimally. Let [G, H] with
G = {g1 , g2 , ..., gn } and H = {h1 , h2 , ..., hm } denote the
entities in set A and B respectively with hi = 0 (gj = 0)
if entity ai (bj ) is alive and hi = 1 (gj = 1) otherwise.
Given an integer k let [G, H] be the solution (with value of 1
corresponding to entities failed initially) that cause maximum
number of entity failure. Two variables xid and yjd are used
in the ILP with xid = 1 (yjd = 1), when entity ai â A
(bj â B) is in a failed state at time step d, and 0 otherwise.
The number of entities to be defended is considered to be k.

It is to be noted that the maximum number cascading steps is
upper bounded by |A| + |B| â 1 = m + n â 1. The objective
function can now be formulated as follows:
min

n
m

X
X
yj(m+nâ1)
xi(m+nâ1) +

(1)

j=1

i=1

The objective in (1) minimizes the number of entities failed
after the cascading failure with the respective constraints for
the Entity Hardening problem as follows:
Constraint Set 1:

n
P

i=1

qxi +

m
P

qyj = k , with qxi , qyj â [0, 1].

j=1

If an entity xi (yj ) is defended then qxi = 1 (qyj = 1) and 0
otherwise.
Constraint Set 2: xi0 â¥ gi â qxi and yi0 â¥ hi â qyi .
This constraint implies that only if an entity is not defended
and gi (hi ) is 1 then the entity will fail at the initial time step.
Constraint Set 3: xid â¥ xi(dâ1) , âd, 1 â¤ d â¤ m + n â 1, and
yid â¥ yi(dâ1) , âd, 1 â¤ d â¤ m + n â 1, in order to ensure
that for an entity which fails in a particular time step would
remain in failed state at all subsequent time steps.
Constraint Set 4: Modeling of the constraint to capture
the cascade propagation for IIM is similar to the constraints
established in [9]. A brief presentation of this constraint is
provided here. Consider an IDR ai â bj bp bl + bm bn + bq of
type Case IV. The following steps are enumerated to depict
the cascade propagation:
Step 1: Replace all minterms of size greater than one with a
variable. In the example provided we have the transformed
minterm as ai â c1 + c2 + bq with c1 â bj bp bl and
c2 â bm bn (c1 , c2 â {0, 1}) as the new IDRs. Note that after
transformation, the original IDR is in the form of Case III and
the introduced IDRs are in the form of Case II.
Step 2: For each variable c, a constraints is added to capture
the cascade propagation. Let N be the number of entities
in the minterm on which c is dependent. In the example
for the variable c1 with IDR c1 â bj bp bl , constraints
y
+y
+yl(dâ1)
c1d â¥ j(dâ1) p(dâ1)
and c1d â¤ yj(dâ1) + yp(dâ1) +
N
yl(dâ1) âd, 1 â¤ d â¤ m + n â 1 are introduced (with
N = 3 in this case). If IDR of an entity is already in
form of Case II, i.e.,ai â bj bp bl then constraints xid â¥
yj(dâ1) +yp(dâ1) +yl(dâ1)
â qxi and xid â¤ yj(dâ1) + yp(dâ1) +
N
yl(dâ1) âd, 1 â¤ d â¤ m + n â 1 are introduced (with N = 3).
These constraints satisfies that if the entity xi is hardened
initially then it is not dead at any time step.
Step 3: Let M be the number of minterms in the transformed IDR as described in Step 1. In the given example
with IDR ai â c1 + c2 + bq constraints of form xid â¥
c1(dâ1) + c2(dâ1) + yq(dâ1) â (M â 1) â qxi and xid â¤
c1(dâ1) +c2(dâ1) +yq(dâ1)
âd, 1 â¤ d â¤ m + n â 1 are introduced.
M
These constraints ensures that even if all the minterms of xi
has at least one entity in dead state then it will be alive if the
entity is hardened initially. For all IDRs of type Case I and
Case III, the constraint discussed in this step is used.

6

B. Heuristic

Algorithm 2: Heuristic Solution to the ENH Problem

In this subsection we provide a greedy heuristic solution to
the Entity Hardening problem. For an interdependent network
I(A, B, F (A, B)) with the initial failed set of entities as
Aâ² âª B â² , Protection Set of each entity has been defined in
the approximation scheme of Case III. To design the heuristic
we define Minterm Coverage Number of each entity in A âª B
as follows:

1
2
3
4

Definition: For an entity xi â A âª B the Minterm Coverage
Number is defined as the number of minterms that can be
removed from F (A, B) without affecting the cascading process
by hardening the entity xi when all entities in Aâ² âª B â² fails
initially. This is represented as M (xi |Aâ² âª B â² ).
Similar to the computation of Protection Set the Minterm
Coverage Number of each entity can be computed in O((n +
m)2 ). With these definitions the heuristic is given in Algorithm
2. The algorithm takes in as input an interdependent network
I(A, B, F (A, B)) with S = AâªB. Step 4-5 is done to reduce
the search space as it directly follows that the set of entities
in Q wouldnât effect the hardening process. In each iteration
of the while loop an entity xd is greedily selected which
when hardened would prevent failure of maximum number of
entities. This ensures that at each step the number of entities
failed is minimized. In case of a tie, among all entities involved
in the tie, the entity having the highest Minterm Coverage
Number is included in the solution. This gives a higher priority
to the entity which when hardened, has more impact on failure
minimization in subsequent iterations of the while loop. The
interdependent network I(A, B, F (A, B)) is updated in steps
13-16 of the algorithm. This takes into account the effect of
hardening an entity in the current iteration on entities hardened
in the following iterations.
Run Time Analysis of Algorithm 2: For this analysis we
consider n to be the total number of entities and m to be
the total number of minterms. Updates in step 4 can be done
in O(m) and step 5 in O(n). The while loop iterates for k
times. In each iteration of the while loop step 7 and step 8 takes
at most O((n + m)2 ) and O(nlog(n)) time respectively. On
branching in step 9, step 10 and step 11 takes O((n + m)2 )
and O(nlog(n)) time respectively. Updates in step 13 takes
O(n) time and in step 14 takes O(n + m) time. Step 12,
16 and 17 runs in constant time. Hence Algorithm 2 runs in
O(k(n + m)2 ) time.
VI.

E XPERIMENTAL R ESULTS

In this section we present the experimental results of the
Entity Hardening problem by comparing the optimal solution
computed using an ILP, and the proposed heuristic algorithm.
The experiments were conducted on real world power grid data
obtained from Platts (www.platts.com), and communication
network data obtained from GeoTel (www.geo-tel.com) of
Maricopa County, Arizona. The data consisted of 70 power
plants and 470 transmission lines in the power network, and
2, 690 cell towers, 7, 100 fiber-lit buildings and 42, 723 fiber
links in the communication network. We identified five nonintersecting geographical regions from the data set and labeled
them from regions 1 through 5. For each of the regions, the
entities of the power and communication network that were

5
6
7
8
9
10
11
12

Data: An interdependent network I(A, B, F(A, B)) (with
S = A âª B), set of entities Aâ² âª B â² failed initially
causing maximum failure in the interdependent network
with |Aâ² | + |B â² | = K and hardening budget k.
Result: Set of hardened entities H.
begin
Initialize S â² â Aâ² âª B â² ;
Initialize H = â;
Update F(A, B) as follows â (a) let Q be the set of
entities that does not fail on failing K entities, (b) remove
IDRs corresponding to entities in set Q, (c) remove from
minterm of entities not in set Q all entities which are in
set Q ;
Update S = S \ Q ;
while (k entities are not hardened) do
For each entity xi â S compute the Protection Set
P (xi |S â² );
Choose the entity xd with highest cardinality of the
set |P (xd |S â² )|;
if (more than one entity has the same highest
cardinality value) then
For each such entity xj compute the Minterm
Coverage Number M (xj |S â² ) ;
Choose the entity xd with highest Minterm
Coverage Number. ;
In case of a tie choose arbitrarily;

16

Update S â S â P (xd |S â² );
Update F(A, B) by removing (i) IDRs corresponding
to all entities in P (xd |S â² ), and (ii) occurrence of
these entities in IDRs of entities not in P (xd |S â² );
if (xd â S â² ) then
Update S â² â S â² â {xd };

17

Update H = H âª xd ;

13
14

15

18

return H ;

located within the geographic region formed the set A and B
respectively. Each region was represented by an interdependent
network I(A, B, F (A, B)). We use the IDR construction rules
as defined in [9] to generate F (A, B).

In all of our simulations IBM CPLEX Optimizer 12.5 to
solve ILPs and Python 3 for heuristic is used. To analyze
the Entity Hardening problem the value of K was set to 8.
The ILP in [9] was used to compute the K most vulnerable
nodes in the network, and the set of failed entities due to
the failure of the K entities was also computed. For the five
regions, when the K = 8 most vulnerable nodes failed, the
total number of failed entities in the network were 28, 23, 28,
28 and 27 respectively. With the K most vulnerable nodes and
final set of failed nodes as input, the ILP and heuristic of the
Entity Hardening problem are compared with k = 1, 3, 5, 7.
The results of these simulations are shown in Figure 1. It is
observed that the heuristic solution differs more from optimal
at higher values of k (factor of 0.5 and 0.67 for Regions 1
and 3 respectively with k = 7). This is primarily because of
the greedy nature of Algorithm 2. However on an average the
heuristic solution differs by a factor of 0.13 from the optimal.

7

14 13

ILP solution
Heuristic

10
8

7
6

6

4

4

3
2

2
0

1
1

14

13

ILP solution
Heuristic

12
10
8

7

4

3

Number of entities failed

3

2

1
1

1

13

ILP solution
Heuristic

12

12
10

8

8
6

6

5

4

3

2
1

3
5
7
Number of entities hardened

(b) Region 2
11

ILP solution
Heuristic

10
8
6
5

4

4

3
2

2

1
1

(c) Region 3

8

11

6

3
1

0

3
5
7
Number of entities hardened

(a) Region 1

0

7

6

0

3
5
7
Number of entities hardened

12

Number of entities failed

13
12

3
5
7
Number of entities hardened

(d) Region 4

Number of entities failed

12

Number of entities failed

Number of entities failed

14

7

ILP solution
Heuristic

7

6
5

5

4
3

3

2
1
0

1

1

3
5
7
Number of entities hardened

(e) Region 5

Fig. 1: Comparison chart of the optimal solution (ILP) with the heuristic by varying number of entities hardened for each identified region

VII.

C ONCLUSION

In this paper we studied the entity hardening problem
in multi-layer networks. We modeled the interdependencies
shared between the networks using IIM, and formulated the
the Entity Hardening problem in this setting. We showed that
the problem is solvable in polynomial time for some cases,
whereas for others it is NP-complete. We evaluated the efficacy
of our heuristic using power and communication network data
of Maricopa County, Arizona. Our experiments showed that
our heuristic almost always produces near optimal results.
R EFERENCES
[1]

[2]

[3]

[4]

[5]

[6]

[7]

S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
âCatastrophic cascade of failures in interdependent networks,â Nature,
vol. 464, no. 7291, pp. 1025â1028, 2010.
J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, âNetworks formed
from interdependent networks,â Nature Physics, vol. 8, no. 1, pp. 40â48,
2011.
J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, âCascade of
failures in coupled network systems with multiple support-dependence
relations,â Physical Review E, vol. 83, no. 3, p. 036116, 2011.
V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and
R. Setola, âModelling interdependent infrastructures using interacting
dynamical models,â International Journal of Critical Infrastructures,
vol. 4, no. 1, pp. 63â79, 2008.
P. Zhang, S. Peeta, and T. Friesz, âDynamic game theoretic model of
multi-layer infrastructure networks,â Networks and Spatial Economics,
vol. 5, no. 2, pp. 147â178, 2005.
M. Parandehgheibi and E. Modiano, âRobustness of interdependent
networks: The case of communication networks and the power grid,â
arXiv preprint arXiv:1304.0356, 2013.
D. T. Nguyen, Y. Shen, and M. T. Thai, âDetecting critical nodes in
interdependent power networks for vulnerability assessment,â 2013.

[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman,
âPower grid vulnerability to geographically correlated failures-analysis
and control implications,â arXiv preprint arXiv:1206.1099, 2012.
[9] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, âIdentification of k most vulnerable nodes in multi-layered network using a new
model of interdependency,â in Computer Communications Workshops
(INFOCOM WKSHPS), 2014 IEEE Conference on. IEEE, 2014, pp.
831â836.
[10] J.-F. Castet and J. H. Saleh, âInterdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,â PloS one, vol. 8, no. 4, p. e60402, 2013.
[11] A. Das, J. Banerjee, and A. Sen, âRoot cause analysis of failures in
interdependent power-communication networks,â in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910â915.
[12] A. Mazumder, C. Zhou, A. Das, and A. Sen, âProgressive recovery from
failure in multi-layered interdependent network using a new model of
interdependency,â in Conference on Critical Information Infrastructures
Security (CRITIS), 2014. Springer, 2014.
[13] M. Hajiaghayi, K. Jain, K. Konwar, L. Lau, I. Mandoiu, A. Russell,
A. Shvartsman, and V. Vazirani, âThe minimum k-colored subgraph
problem in haplotyping and dna primer selection,â in Proceedings of the
International Workshop on Bioinformatics Research and Applications
(IWBRA). Citeseer, 2006.

Identification of K Most Vulnerable Nodes in Multi-layered Network Using a New Model of Interdependency
Arunabha Sen, Anisha Mazumder, Joydeep Banerjee, Arun Das and Randy Compton
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering Arizona State University Tempe, Arizona 85287 Email: {asen, amazumde, Joydeep.Banerjee, adas22, Randy.Compton}@asu.edu

arXiv:1401.1783v1 [cs.NI] 8 Jan 2014

Abstract--The critical infrastructures of the nation including the power grid and the communication network are highly interdependent. Recognizing the need for a deeper understanding of the interdependency in a multi-layered network, significant efforts have been made by the research community in the last few years to achieve this goal. Accordingly a number of models have been proposed and analyzed. Unfortunately, most of the models are over simplified and, as such, they fail to capture the complex interdependency that exists between entities of the power grid and the communication networks involving a combination of conjunctive and disjunctive relations. To overcome the limitations of existing models, we propose a new model that is able to capture such complex interdependency relations. Utilizing this model, we provide techniques to identify the K most vulnerable nodes of an interdependent network. We show that the problem can be solved in polynomial time in some special cases, whereas for some others, the problem is NP-complete. We establish that this problem is equivalent to computation of a fixed point of a multilayered network system and we provide a technique for its computation utilizing Integer Linear Programming. Finally, we evaluate the efficacy of our technique using real data collected from the power grid and the communication network that span the Maricopa County of Arizona.

I. I NTRODUCTION In the last few years there has been an increasing awareness in the research community that the critical infrastructures of the nation are closely coupled in the sense that the well being of one infrastructure depends heavily on the well being of another. A case in point is the interdependency between the electric power grid and the communication network. The power grid entities, such as the SCADA systems that control power stations and sub-stations, receive their commands through communication networks, while the entities of communication network, such as routers and base stations, cannot operate without electric power. Cascading failures in the power grid, are even more complex now because of the coupling between power grid and communication network. Due to this coupling, not only entities in power networks, such as generators and transmission lines, can trigger power failure, communication network entities, such as routers and optical fiber lines, can also trigger failure in power grid. Thus it is essential that the interdependency between different types of networks be understood well, so that preventive measures can be taken to

avoid cascading catastrophic failures in multi-layered network environments. Recognizing the need for a deeper understanding of the interdependency in a multi-layered network, significant efforts have been made in the research community in the last few years to achieve this goal [1], [2], [3], [4], [5], [6], [7], [8]. Accordingly a number of models have been proposed and analyzed. Unfortunately, many of the proposed models are overly simplistic in nature and as such they fail to capture the complex interdependency that exists between power grid and communication networks. In a highly cited paper [1], the authors assume that every node in one network depends on one and only one node of the other network. However, in a follow up paper [2], the same authors argue that this assumption may not be valid in the real world and a single node in one network may depend on more than one node in the other network. A node in one network may be functional ("alive") as long as one supporting node on the other network is functional. Although this generalization can account for disjunctive dependency of a node in the A network (say ai ) on more than one node in the B network (say, bj and bk ), implying that ai may be "alive" as long as either bi or bj is alive, it cannot account for conjunctive dependency of the form when both bj and bk has to be alive in order for ai to be alive. In a real network the dependency is likely to be even more complex involving both disjunctive and conjunctive components. For example, ai may be alive if (i) bj and bk and bl are alive, or (ii) bm and bn are alive, or (iii) bp is alive. The graph based interdependency models proposed in the literature [3], [4], [5], [9], [6], [7] including [1], [2] cannot capture such complex interdependency between entities of multilayer networks. In order to capture such complex interdependency, we propose a new model using Boolean logic. Utilizing this comprehensive model, we provide techniques to identify the K most vulnerable nodes of an interdependent multilayered network system. We show that the this problem can be solved in polynomial time for some special cases, whereas for some others, the problem is NP-complete. We also show that this problem is equivalent to computation of a fixed point [10] and we provide a technique utilizing Integer Linear Programming

2

to compute that fixed point. Finally, we evaluate the efficacy of our technique using real data collected from power grid and communication networks that span Maricopa County of Arizona.

Entities a1 a2 a3 a4 b1 b2 b3 t0 1 0 0 0 0 0 0 t1 1 0 0 0 0 0 1

Time Steps t2 t3 t4 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1

t5 1 1 1 1 1 1 1

t6 1 1 1 1 1 1 1

II. I NTERDEPENDENCY M ODEL We describe the model for an interdependent network with two layers. However, the concept can easily be generalized to deal with networks with more layers. Suppose that the network entities in layer 1 are referred to as the A type entities, A = {a1 , . . . , an } and entities in layer 2 are referred to as the B type entities, B = {b1 , . . . , bm }. If the layer 1 entity ai is operational if (i) the layer 2 entities bj , bk , bl are operational, or (ii) bm , bn are operational, or (iii) bp is operational, we express it in terms of live equations of the form ai  bj bk bl + bm bn + bp . The live equation for a B type entity br can be expressed in a similar fashion in terms of A type entities. If br is operational if (i) the layer 1 entities as , at , au , av are operational, or (ii) aw , az are operational, we express it in terms of live equations of the form br  as at au av + aw az . It may be noted that the live equations only provide a necessary condition for entities such as ai or br to be operational. In other words, ai or br may fail independently and may be not operational even when the conditions given by the corresponding live equations are satisfied. A live equation in general will have the following tj Ti form: xi  j =1 k=1 yj,k where xi and yj,k are elements of the set A (B ) and B (A) respectively, Ti represents the number of min-terms in the live equation and tj refers to the size of the j -th min-term (the size of a min-term is equal to the number of A or B elements in that min-term). In the example ai  bj bk bl + bm bn + bp , Ti = 3, t1 = 3, t2 = 2, t3 = 1, xi = ai , y2,1 = bm , y2,2 = bp . We refer to the live equations of the form ai  bj bk bl + bm bn + bp also as First Order Dependency Relations, because these relations express direct dependency of the A type entities on B type entities and vice-versa. It may be noted however that as A type entities are dependent on B type entities, which in turn depends on A type entities, the failure of some A type entities can trigger the failure of other A type entities, though indirectly, through some B type entities. Such interdependency creates a cascade of failures in multilayered networks when only a few entities of either A type or B type (or a combination) fails. We illustrate this with the help of an example. The live equations for this example is shown in table I.
Power Network a1  b1 + b2 a2  b1 b3 + b2 a3  b1 b2 b3 a4  b1 + b2 + b3 Communication Network b1  a1 + a2 a3 b2  a1 + a3 b3  a1 a2 --

TABLE II: Time Stepped Cascade Effect for a Multilayer Network

Fig. 1: Cascading failures reach steady state after p time steps

As shown in table II, the failure of only one entity a1 at time step t0 triggered a chain of failures that resulted in the failure of all the entities of the network after by timestep t4 . A table entry of 1 indicates that the entity is "dead". In this example, the failure of a1 at t0 triggered the failure of b3 at t1 , which in turn triggered the failure of a3 at t2 . The failure of b3 at t1 was due to the dependency relation b3  a1 a2 and the failure of a3 at t2 was due to the dependency relation a3  b1 b2 b3 . The cascading failure process initiated by failure (or death) of a subset of A type entities at timestep t0 , A0 d and 0 till it reaches its final steady a subset of B type entities Bd state is shown diagrammatically in figure 1. Accordingly, a multilayered network can be viewed as a "closed loop" control system. Finding the steady state after an initial failure in this case is equivalent of computing the fixed point of a function p p p F (.) such that F (Ap d  Bd ) = Ad  Bd , where p represents the number of steps when the system reaches the steady state. We define a set of K entities in a multi-layered network as most vulnerable, if failure of these K entities triggers the failure of the largest number of other entities. The goal of the K most vulnerable nodes problem is to identify this set of 0 nodes. This is equivalent to identifying A0 d  A, Bd  B , that p p 0 maximizes |Ad  Bd |, subject to the constraint that |A0 d  Bd |  K. The dependency relations (live equations) can be formed either after careful analysis of the multilayer network along the lines carried out in [8], or after consultation with the engineers of the local utility and internet service providers. III. C OMPUTATIONAL C OMPLEXITY AND A LGORITHMS Based on the number and the size of the min-terms in the dependency relations, we divide them into four different cases as shown in Table III. The algorithms for finding the K most vulnerable nodes in the multilayer networks and computation complexity for each of the cases are discussed in the following four subsections.
Case Case I Case II Case III Case IV No. of Min-terms 1 1 Arbitrary Arbitrary Size of Min-terms 1 Arbitrary 1 Arbitrary

TABLE I: Live equations for a Multilayer Network

TABLE III: Equation Types for Dependency Relations

3

A. Case I: Problem Instance with One Min-term of Size One In this case, a live equation in general will have the following form: xi  yj where xi and yj are elements of the set A (B ) and B (A) respectively. In the example ai  bj , xi = ai , y1 = bj . It may be noted that a conjunctive implication of the form ai  bj bk can also be written as two separate implications ai  bj and ai  bk . However, such cases are considered in Case II and is excluded from consideration in Case I. The exclusion of such implications implies that the entities that appear on the LHS of an implication in Case I are unique. This property enables us to develop a polynomial time algorithm for the solution of the K most vulnerable node problem for this case. We present the algorithm next. Algorithm 1 Input: (i) A set S of implications of the form of y  x, where x, y  A  B , (ii) An integer K. Output: A set V where |V | = K and V  A  B such that failure of entities in V at time step t0 results in failure of the largest number of entities in A  B when the steady state is reached. Step 1. We construct a directed graph G = (V, E ), where V = A  B . For each implication y  x in S , where x, y  A  B , we introduce a directed edge (x, y )  E . Step 2. For each node xi  V , we construct a transitive closure set Cxi as follows: If there is a path from xi to some node yi  V in G, then we include yi in Cxi . It may be recalled that |A| + |B | = n + m. So, we get n + m transitive closure sets Cxi , 1  i  (n + m). We call each xi to be the seed entity for the transitive closure set Cxi . Step 3. We remove all the transitive closure sets which are proper subsets of some other transitive closure set. Step 4. Sort the remaining transitive closure sets Cxi , where the rank of the closure sets is determined by the cardinality of the sets. The sets with a larger number of entities are ranked higher than the sets with a fewer number of entities. Step 5. Construct the set V by selecting the seed entities of the top K transitive closure sets. If the number of remaining transitive closure sets is less than K (say, K ), arbitrarily select the remaining entities. Time complexity of Algorithm 1: Step 1 takes O(n + m + |S |) time. Step 2 can be executed in O((n + m)3 ) time. Step 3 takes at most O((n + m)2 ) time. Step 4 sorts at most |S | entries, a standard sorting algorithm takes O(|S | log |S |) time. Selecting K entities in step 5 takes O(K) time. Since |S |  n + m, hence the overall time complexity is O((n + m)3 ) Theorem 1. For each pair of transitive closure sets Cxi and Cxj produced in step 2 of algorithm 1, either Cxi  Cxj =  or Cxi  Cxj = Cxi or Cxi  Cxj = Cxj , where xi = xj . Proof: Consider, if possible, that there is a pair of transitive closure sets Cxi and Cxj produced in step 2 of algorithm 1, such that Cxi  Cxj =  and Cxi  Cxj = Cxi and Cxi  Cxj =

Cxj . Let xk  Cxi  Cxj . This implies that there is a path from xi to xk (path1 ) as well as there is a path from xj to xk , (path2 ). Since, xi = xj and Cxi  Cxj = Cxi and Cxi  Cxj = Cxj , there is some xl in the path1 such that xl also belongs to path2 . W.l.o.g, let us consider that xl be the first node in path1 such that xl also belongs to path2 . This implies that xl has in-degree greater than 1. This in turn implies that there are two implications in the set of implications S such that xl appears in the L.H.S of both. This is a contradiction because this violates a characteristic of the implications in Case I. Hence, our initial assumption was wrong and the theorem is proven. Theorem 2. Algorithm 1 gives an optimal solution for the problem of selecting K most vulnerable entities in a multilayer network for case I dependencies. Proof: Consider that the set V returned by the algorithm is not optimal and the optimal solution is VOP T . Let us consider there is a entity xi  A  B such that xi  VOP T \ V . Evidently, (i) Cxi was either deleted in step 3 or (ii) |Cxi | is less than the cardinalities of all the transitive closure sets with seed entities xj  V , because our algorithm did not select xi . Hence, in both cases, replacing any entity xj  V by xi reduces the total number of entities killed. Thus, the number of dead entities by the failure of entities in VOP T is lesser than that caused by the failure of the entities in V , contradicting the optimality of VOP T . Hence, the algorithm does in fact return the optimal solution. B. Case II: Problem Instance with One Min-term of Arbitrary Size In this case, a live equation in general will have the q following form: xi  k=1 yj where xi and yj are elements of the set A (B ) and B (A) respectively, q represents the size of min-term. In the example ai  bj bk bl , q = 3, xi = ai , y1 = bj , y2 = bk , y3 = bk . 1) Computational Complexity: We show that computation of K most vulnerable nodes (K-MVN) in a multilayer network is NP-complete in Case II. We formally state the problem next. Instance: Given a set of dependency relations between A and B type entities in the form of live equations xi  q k=1 yj , integers K and L. Question: Is there a subset of A and B type entities of size at most K whose "death" (failure) at time t0 , triggers a cascade of failures resulting in failures of at least L entities, when the steady state is reached? Theorem 3. The K-MVN problem is NP-complete. Proof: We prove that the K-MVN problem is NP-complete by giving a transformation for the vertex cover (VC) problem. An instance of the vertex cover problem is specified by an undirected graph G = (V, E ) and an integer R. We want to know if there is a subset of nodes S  V of size at most R, so that every edge has at least one end point in S . From an instance of the VC problem, we create an instance of the

4

K-MVN problem in the following way. First, from the graph G = (V, E ), we create a directed graph G = (V, E ) by replacing each edge e  E by two oppositely directed edges e1 and e2 in E (the end vertices of e1 and e2 are same as the end vertices of e). Corresponding to a node vi in G that has incoming edges from other nodes (say) vj , vk and vl , we create a dependency relation (live equation) vi  vj vk vl . We set K = R and L = |V |. The corresponding death equation is of the form v ¯i  v ¯ ¯ ¯l (obtained by taking negation j +v k +v of the live equation). We set K = R and L = |V |. It can now easily be verified that if the graph G = (V, E ) has a vertex cover of size R iff in the created instance of K-MVN problem death (failure) of at most K entities at time t0 , will trigger a cascade of failures resulting in failures of at least L entities, when the steady state is reached. 2) Optimal Solution with Integer Linear Programming: In this case, we can find and optimal solution to the KMVN problem using Integer Linear Programming (ILP). We associate binary indicator variables xi (yi ) to capture the state of the entities ai (bi ). xi (yi ) is 1 when ai (bi ) is dead and 0 otherwise. Since we want find the set of K entities whose failure at time step t0 triggers cascading failure resulting in the failure of the largest number of entities, the objective of the n m ILP can be written as follows maximize i=1 xi + i=1 yi It may be noted that the variables in the objective function do not have any notion of time. However, cascading failure takes place in time steps, ai triggers failure of bj at time step t1 , which in turn triggers failure of ak in time step t2 and so on. Accordingly, in order to capture the cascading failure process, we need to introduce the notion of time into the variables of the ILP. If the numbers of A and B type entities are n and m respectively, the steady state must be reached by time step n + m - 1 (cascading process starts at time step 0, t0 ). Accordingly, we introduce n + m versions of the variables xi and yi , i.e., xi [0], . . . , xi [n + m - 1] and yi [0], . . . , yi [n + m - 1]. To indicate the state of entities ai and bi at times t0 , . . . , tn+m-1 . The objective of the ILP is now changed to
n m

The optimal solution to K-MVN problem for Case II can be found by solving the above ILP. C. Case III: Problem Instance with an Arbitrary Number of Min-terms of Size One A live equation in this special case will have the following q form: xi  j =1 yj where xi and yj are elements of the set A (B ) and B (A) respectively, q represents the number of minterms in the live equation. In the example ai  bj + bk + bl , q = 3, xi = ai , y1 = bj , y2 = bk , y3 = bl . 1) Computational Complexity: We show that a special case of the problem instances with an arbitrary number of min-terms of size one is same as the Subset Cover problem (defined below), which is proven to be NPcomplete. We define Implication Set(A) to be the Ti set of all implications of the form ai  j =1 bj and Implication Set(B ) to be the set of all implications of the Ti form bi  j =1 aj . Now consider a subset of the set of problem instances with an arbitrary number of min-terms of size one where either Implication Set(A) =  or Implication Set(B ) = . Let A = {ai |ai is the element on the LHS of an implication} in the Implication Set(A). The set B is defined accordingly. If Implication Set(B ) =  then B = . In this case, failure of any ai , 1  i  n type entities will not cause failure of any bj , 1  j  m type entities. Since an adversary can cause failure of only K entities, the adversary would like to choose only those K entities that will cause failure of the largest number of entities. In this scenario, there is no reason for the adversary to attack any ai , 1  i  n type entities as they will not cause failure of any bj , 1  j  m type entities. On the other hand, if the adversary attacks K bj type entities, not only those K bj type entities will be destroyed, some ai type entities will also be destroyed due to the implications in the Implication Set(A). As such the goal of the adversary will be to carefully choose K bj , 1  j  m type entities that will destroy the largest number of ai type entities. In its abstract form, the problem can be viewed as the Subset Cover problem. Subset Cover Problem Instance: A set S = {s1 , . . . , sm }, a set S of m subsets of S , i.e., S = {S1 , . . . , Sr }, where Si  S, i, 1  i  r, integers p and q . Question: Is there a p element subset S of S (p < n) that completely covers at least q elements of the set S ? (A set S is said to be completely covering an element Si , i, 1  i  m of the set S , if S  Si = Si , i, 1  i  m.) The set S in the subset cover problem corresponds to the set B = {b1 , . . . , bm }, and each set Si , 1  i  r corresponds to an implication in the ImplicationS et(A) and comprises of the bj 's that appear on the RHS of the implication. The goal of the problem is to select a subset B of B that maximizes the number of Si 's completely covered by B .

maximize
i=1

xi [n + m - 1] +
i=1

yi [n + m - 1]

Subject to the constraint that no more than K entities can fail at time t0 . n m Constraint 1: i=1 yi [0]  K In order i=1 xi [0] + to ensure that the cascading failure process conforms to the dependency relations between type A and B entities, additional constraints must be imposed. Constraint 2: If an entity fails at time fails at time step p, (i.e., tp ) it should continue to be in the failed state at all time steps t > p. That is xi (t)  xi (t - 1), t, 1  t  n + m - 1. Same constraint applies to yi (t). Constraint 3: The dependency relation (death equation) ¯ ¯ ¯ a ¯i  b j + bk + bl can be translated into a linear constraint in the following way xi (t)  yj (t - 1)+ yk (t - 1)+ yl (t - 1), t, 1  t  n + m - 1.

5

Theorem 4. The Subset Cover problem is NP-complete. Proof: We prove that the Subset Cover problem is NPcomplete by giving a transformation from the well known Clique problem. It may be recalled that an instance of the Clique problem is specified by a graph G = (V, E ) and an integer K . The decision question is whether or not a clique of size at least K exists in the graph G = (V, E ). We show that a clique of size K exists in graph G = (V, E ) iff the Subset Cover problem instance has a p element subset S of S that completely covers at least q elements of the set S . From an instance of the Clique problem, we create an instance of the Subset Cover problem in the following way. Corresponding to every vertex vi , 1  i  n of the graph G = (V, E ) (V = {v1 , . . . , vn }), we create an element in the set S = {s1 , . . . , sn }. Corresponding to every edge ei , 1  i  m, we create m subsets of S , i.e., S = {S1 , . . . , Sm }, where Si corresponds to a two element subset of nodes, corresponding to the end vertices of the edge ei . We set the parameters p = K and q = K (K - 1)/2. Next we show that in the instance of the subset cover problem created by the above construction process, a p element subset S of S exists that completely covers at least q elements of the set S , iff the graph G = (V, E ) has a clique of size at least K . Suppose that the graph G = (V, E ) has a clique of size K . It is clear that in the created instance of the subset cover problem, we will have K (K - 1)/2 elements in the set S , that will be completely covered by a K element subset of the set S . The K element subset of S corresponds to the set of K nodes that make up the clique in G = (V, E ) and the K (K - 1)/2 elements in the set S corresponds to the edges of the graph G = (V, E ) that corresponds to the edges of the clique. Conversely, suppose that the instance of the Subset Cover problem has K element subset of S that completely covers K (K - 1)/2 elements of the set S . Since the elements of S corresponds to the edges in G, in order to completely cover K (K - 1)/2 edges, at least K nodes (elements of the set S ) will be necessary. As such, this set of K nodes will constitute a clique in the graph G = (V, E ). 2) Optimal Solution with Integer Linear Programming: If q the live equation is in the form xi  k=1 yj then the "death equation" (obtained by taking negation of the live equation) q will be in the product form x ¯i  j =1 y ¯j . If the live equation is given as ai  bj + bk , then the death equation will be given ¯ ¯ as a ¯i  b j bk . By associating binary indicator variables xi and yi to capture the state of the entities ai and bi , we can follow almost identical procedure as in Case II, with only one exception. It may be recalled that in Case II, the death equations such ¯ ¯ as a ¯i  b j + bk was translated into a linear constraint xi (t)  yj (t - 1) + yk (t - 1), t, 1  t  n + m - 1. However a similar translation in Case III, with death equations such as ¯ ¯ a ¯i  b j bk , will result in a non-linear constraint of the form xi (t)  yj (t - 1)yk (t - 1), t, 1  t  n + m - 1. Fortunately, a non-linear constraint of this form can be replaced a linear constraint such as 2xi (t)  yj (t - 1) + yk (t - 1), t, 1 

t  n + m - 1. After this transformation, we can compute the optimal solution using integer linear programming. D. Case IV: Problem Instance with an Arbitrary Number of Min-terms of Arbitrary Size 1) Computational Complexity: Since both Case II and Case III are special cases of Case IV, the computational complexity of finding the K most vulnerable nodes in the multilayer network in NP-complete in Case IV also. 2) Optimal Solution with Integer Linear Programming: The optimal solution to this version of the problem can be computed by combining the techniques developed for the solution of the versions of the problems considered in Cases II and III. IV. E XPERIMENTAL RESULTS We applied our model to study multilayer vulnerability issues in Maricopa County, the most densely populated county of Arizona with approximately 60% of Arizonas population residing in it. Specifically, we wanted to find out if some regions of Maricopa County were more vulnerable to failure than some other regions. The data for our multi-layered network were obtained from different sources. We obtained the data for the power network (network A) from Platts (http://www.platts.com/). Our power network dataset consists of 70 power plants and 470 transmission lines. Our communication network (network B) data were obtained from GeoTel (http://www.geo-tel.com/). Our communication network data consists of 2, 690 cell towers and 7, 100 fiber-lit buildings as well as 42, 723 fiber links. Snapshots of our power network data and communication network data are shown in figure 2. In the power network snapshot of sub-figure(a), the orange markers show locations of powerplants while the yellow continuous lines represent the transmission lines. In the communication network snapshot of sub-figure (b) the pink markers show the location of fiber-lit buildings, the orange markers show the location of cell towers and the green continuous lines represent the fiber links. In our dataset, `load' in the Power Network is divided into Cell towers and Fiber-lit buildings. Although there exists various other physical entities which also draw electric power and hence can be viewed as load to the power network, as they are not relevant to our study on interdependency between power and communication networks, we ignore such entities. Thus in network A, we have the three types of Power Network Entities (PNE's) - Generators, Load (consisting of Cell towers and Fiber-lit buildings) and Transmission lines (denoted by a1 , a2 , a3 respectively). For the Communication Network, we have the following Communication Network Entities (CNE's) - Cell Towers, Fiber-lit buildings and Fiber links (denoted by b1 , b2 , b3 respectively). We consider the Fiber-lit buildings as a communication network entities as they house routers which definitely are communication network entities. From the raw data we construct Implication Set(A) and Implication Set(B), by following the rules stated below: Rules: We consider that a PNE is dependent on a set of CNEs for being in the active state (`alive') or being in the

6

(a) Snapshot of Power Network in Maricopa County

(b) Snapshot of Communication Network in Maricopa County

Fig. 2: Snapshots of power network and communication network in Maricopa County)

inactive state (`dead'). Similarly, a CNE is dependent on a set of PNEs for being active or inactive state. For simplicity we consider the live equations with at most two minterms. For the same reason we consider the size of each minterm is at most two. Generators (a1,i , 1  i  p, where p is the total number of generators): We consider that each generator (a1.i ) is dependent on the nearest Cell Tower (b1,j ) or the nearest Fiber-lit building (b2,k ) and the corresponding Fiber link (b3,l ) connecting b2,k and a1,i . Thus, we have a1,i  b1,j + b2,k × b3,l Load (a2,i , 1  i  q , where q is the total number of loads): We consider that the loads in the power network do not depend on any CNE. Transmission Lines (a3,i , 1  i  r, where r is the total number of transmission lines): We consider that the transmission lines do not depend on any CNE. Cell Towers (b1,i , 1  i  s, where s is the total number of cell towers): We consider the cell towers depend on the nearest pair of generators and the corresponding transmission line connecting the generator to the cell tower. Thus, we have b1,i  a1,j × a3,k + a1,j × a3,k Fiber-lit Buildings (b2,i , 1  i  t, where t is the total number of fiber-lit buildings): We consider that the fiber-lit buildings depend on the nearest pair of generators and the corresponding transmission lines connecting the generators to the cell tower. Thus, we have b2,i  a1,j × a3,k + a1,j × a3,k Fiber Links (b3,i , 1  i  u, where u is the total number of fiber links)): We consider that the fiber links do not depend on any PNE. Because of experimental resource limitation, we have considered 5 regions of Maricopa County for our experiments. We used IBM CPLEX Optimizer 12.5 to run the formulated ILP's on the experimental dataset. We show our results in the figure 3. We observe that in each of the regions there is a specific budget threshold beyond which each additional increment in budget results in the death of only one entity. The reason for this behavior is our assumption that entities such as the transmission lines and the fiberlinks are not dependent on any other entities. We notice that all the entities of the two networks can be destroyed with a budget of about 60%

of the number of entities of the two networks A and B . Most importantly, we find that the degree of vulnerability of all the five regions considered in our study are close and no one region stands out as being extremely vulnerable.

Fig. 3: Experimental results of failure vulnerability across five regions of Maricopa county

R EFERENCES
[1] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin, "Catastrophic cascade of failures in interdependent networks," Nature, vol. 464, no. 7291, pp. 1025­1028, 2010. [2] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, "Networks formed from interdependent networks," Nature Physics, vol. 8, no. 1, pp. 40­48, 2011. [3] J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, "Cascade of failures in coupled network systems with multiple support-dependence relations," Physical Review E, vol. 83, no. 3, p. 036116, 2011. [4] V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and R. Setola, "Modelling interdependent infrastructures using interacting dynamical models," International Journal of Critical Infrastructures, vol. 4, no. 1, pp. 63­79, 2008. [5] P. Zhang, S. Peeta, and T. Friesz, "Dynamic game theoretic model of multi-layer infrastructure networks," Networks and Spatial Economics, vol. 5, no. 2, pp. 147­178, 2005. [6] M. Parandehgheibi and E. Modiano, "Robustness of interdependent networks: The case of communication networks and the power grid," arXiv preprint arXiv:1304.0356, 2013. [7] D. T. Nguyen, Y. Shen, and M. T. Thai, "Detecting critical nodes in interdependent power networks for vulnerability assessment," 2013. [8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman, "Power grid vulnerability to geographically correlated failures-analysis and control implications," arXiv preprint arXiv:1206.1099, 2012. [9] J.-F. Castet and J. H. Saleh, "Interdependent multi-layer networks: Modeling and survivability analysis with applications to space-based networks," PloS one, vol. 8, no. 4, p. e60402, 2013. [10] A. Fudenberg and J. Tirole, Game Theory. Ane Books, 2010.

Soc. Netw. Anal. Min. DOI 10.1007/s13278-012-0072-x

ORIGINAL ARTICLE

A system for ranking organizations using social scale analysis
Sukru Tikves · Sujogya Banerjee · Hamy Temkit · Sedat Gokalp · Hasan Davulcu · Arunaba Sen · Steven Corman · Mark Woodward Shreejay Nair · Inayah Rohmaniyah · Ali Amin
·

Received: 20 December 2011 / Revised: 4 April 2012 / Accepted: 20 April 2012 Ó Springer-Verlag 2012

Abstract In this paper, we utilize feature extraction and model-fitting techniques to process the rhetoric found in the web sites of 23 Indonesian Islamic religious organizations to profile their ideology and activity patterns along a hypothesized radical/counter-radical scale, and present an end-to-end system that is able to help researchers to visualize the data in an interactive fashion on a timeline. The subject data of this study is 37,000 articles downloaded from the web sites of these organizations dating from 2001 to 2011. We develop algorithms to rank these organizations by assigning them to probable positions on the scale. We show that the developed Rasch model fits the data using Andersen's LR-test. We create a gold standard of the ranking of these organizations through an expertise elicitation tool. We compute expert-to-expert agreements, and we present experimental results comparing the performance of three baseline methods to show that the Rasch model not only outperforms the baseline methods, but it is also the only system that performs at expert-level accuracy.
S. Tikves (&) Á S. Banerjee Á H. Temkit Á S. Gokalp Á H. Davulcu Á A. Sen Á S. Corman Á M. Woodward Á S. Nair Arizona State University, P.O. Box 87-8809, Tempe, AZ 85281, USA e-mail: sukru@asu.edu; stikves@asu.edu S. Banerjee e-mail: sujogya@asu.edu H. Temkit e-mail: mtemkit@asu.edu S. Gokalp e-mail: sgokalp@asu.edu H. Davulcu e-mail: hdavulcu@asu.edu A. Sen e-mail: asen@asu.edu

1 Introduction Being able to asses information on radical and moderate actors in a geographic area is an important research topic for national security. Radicalism is the ideological conviction that it is acceptable, and in some cases, obligatory to use violence to effect profound political, cultural and religious transformations and change the existing social order fundamentally. Muslim radical movements have complex origins and depend on diverse factors that enable translation of their radical ideology into social, political and religious movements. Crelinste (2002), in his work, states that ``both violence and terrorism possess a logic and grammar that must be understood if we are to prevent or control them''. Therefore, analysis of Muslim radical and counter-radical movements requires attention to the global, national and local social, economic and political contexts in which they are located. Similarly, in the Islamic context, counter-radical discourse takes various different forms; discursive and narrative refutations of extremist claims, symbolic action such as ritual and other religious and
S. Corman e-mail: scorman@asu.edu M. Woodward e-mail: mataram@asu.edu S. Nair e-mail: snair8@asu.edu I. Rohmaniyah Center for Religious and Cross Cultural Studies, Gadjah Mada University, Yogyakarta, Indonesia e-mail: rochmaniyah@yahoo.com A. Amin State College of Islamic Studies (STAIN), Manado, Indonesia e-mail: aleejtr77@yahoo.com

123

S. Tikves et al.

cultural practices, and Islamic arguments for pluralism, peaceful relations with non-Muslims, democracy, etc. The most effective counter-radicals are likely to be religiously conservative Muslims. Effective containment and defeat of radicalism depends on our ability to recognize various levels of radicalization, and detection of counter-radical voices. In our previous work (Davulcu et al. 2010), we attempted a clustering approach to obtain ``natural groupings'' of a number of local non-government religious social movements and organizations in Indonesia. Social scientists on our team observed that clustering results were not fully able to separate all counter-radical or radical organizations into pure clusters. Pure radical clusters were easily identified due to high similarity among their support for violent practices. Pure counterradical clusters were identified due to their strong reactionary opposition to violent practices through protests and rhetoric. But the rest of the groupings were mixed. We realized that binary labeling as counter-radical or radical does not capture the overlap, movement and interactivity among these organizations. In this paper, we hypothesize that both counterradical and radical movements in Muslim societies exhibit distinct combinations of discrete states comprising various social, political, and religious beliefs, attitudes and practices, that can be mapped to a latent linear continuum or a scale. Using such a scale, an analyst can determine where exactly along the spectrum any particular group lies, and also potentially where it is heading with its rhetoric and activity. Given the complex nature of the task, such as regional differences in local cultures, beliefs and practices, and in the absence of readily available high-accuracy parsers, highly structured religio-social ontologies, and information extraction systems; we decided to devise a multi-lingual nonlinguistic text processing pipeline that relies on only statistical modeling of keyword frequency and co-occurrence information. However, we designed the system to be able to incorporate additional information extracted from the text, if available. For example, named entity recognition (NER), machine translation, and GIS-based location lookup information are part of the user-interface presentation. We (Tikves et al. 2011) worked with social scientists on our team to come up with an orthogonal model comprising of two primary dimensions. Both dimensions, (1) radical/ counter-radical and (2) violent/non-violent, are characterized as latent, partial orders of discrete beliefs and practices based on a generalization of item order in Guttman scaling (Guttman 1950) using a Rasch model (Andric 1988). A true Guttman scale is a deterministic process, i.e., if a social movement subscribes to a certain belief or practice, then it must also agree with all lower-order practices and beliefs on the scale. Of course, such perfect order is rare in the social world. The Rasch model provides a probabilistic framework for Guttman scales to accommodate for incomplete observations and measurement errors.

We have designed a web-based system to visualize this orthogonal model. The web tools provided by the system allows drilling down on specific data, and plotting the trends and trajectories of organizations on a timeline. It consists of several modules: an off-line web mining, and data-processing pipeline, two web services for application logic, and an AJAX-based presentation layer. The web-based interface built for this study can be accessed through the web site at http://www.demo.minerva-project.org. In this paper, we present several scenarios with this tool in Sect. 5. In this paper, we present feature extraction, feature selection, and model-fitting techniques to process the rhetoric found in the web sites of 23 religious Indonesian organizations--comprising a total of 37,000 articles dating from 2001 to 2011. We aim to identify their ideology and activity patterns along a hypothesized radical/counter-radical scale, and rank them to probable positions on this scale (McPhee 1995). The automated ordering of organizations is formed by ranking the organizations according to their estimated positions on the latent scale. We used the eRm1 package to fit the Rasch model on this data set, and identify organizations' positions based on maximum likelihood estimation (Le Cam 1990). We show that the model fits the data using the Andersen's likelihood ratio test (LR-test) (Hessen 2010). We also created a gold standard of the ranking of these organizations through an expert-opinion elicitation tool, and through the opinions of three ethnographers on our team who collectively possess 35 years of scholarly expertise on Indonesia and Islam. We computed expert-to-gold standard agreements, as well as compared the performance of three different baseline computational methods to show that the Rasch model presented here not only performs the best among the baseline methods but that it is also the only method that performs at an expert level of accuracy. 1.1 Organization of the paper Next section provides an introduction to the theory of Guttman scaling and Rash models. Section 3 defines the problem, presents the system architecture, and the methods used to solve the problem. Section 4 describes the Indonesian corpus, expert-opinion elicitation tool, baseline computational methods, and experimental evaluations. Section 5 discusses the user-interface designed for navigating our findings. Section 6 concludes the paper.

2 Introduction of Guttman scaling and Rasch model In social science, scaling is a process of measuring and ordering entities called subjects, based on their qualitative
1

http://www.r-forge.r-project.org/projects/erm/.

123

A system for ranking organizations using social scale analysis

attributes called items. In general, subjects are requested to respond to surveys conducted by means of structured interviews or questionnaires. Items are presented to the subjects in form of questions. Statistical analysis of the response of the subjects on the questions about items are used in scaling the subjects. Some of the widely followed scaling procedure in social science surveys are Likert scale (Likert 1932), Thurnstone scale (Thurnstone 1928), and Guttman scale (McIver 1981). In Likert scale, subjects indicate their magnitude of agreement or disagreement about an item (from strongly agree to strongly disagree) on a five- to ten-point scale. On the other hand, Thurnstone scale is a formal method of ordering the attitudes of the subjects toward the items. Guttman scaling procedure orders both the subjects and the items simultaneously with respect to some underlying cumulative continuum. In this paper, we follow the Guttman scaling process to rank the organizations based on their response on the radical and counter-radical keywords. 2.1 Guttman scaling A Guttman scale (Guttman 1950) presents a number of items to which each subject is requested to provide a dichotomous response, e.g., agree/disagree, yes/no, or 1/0. This scaling procedure is based on the premise that the items have strict orders (i.e., the items are presented to the subjects ranked according to the level of the item's difficulty). An item ``A'' is said to be ``more difficult'' than an item ``B'', if any subject answering ``yes'' on item ``A'' implies that the subject will also answer ``yes'' on item ``B''. A subject who responds to an item positively is expected to respond positively to all the items of lesser difficulty. For example, to find out how extreme a subject's view is on Guttman scale, the subject is presented with the following series of items in question form. (1) Are you willing to permit immigrants to live in your country? (2) Are you willing to permit immigrants to live in your community? (3) Are you willing to permit immigrants to live in your neighborhood? (4) Are you willing to permit immigrants to live to your next door? (5) Are you willing to permit your child to marry an immigrant? If the items form a Guttman scale, any subject agreeing with any item in this series, will also agree with other items of lower rank-order in this series. Guttman scale is a deterministic process and the score of a subject depends on the number of affirmative responses he has made on the items. So, a score of 2 for a subject in the above Guttman scale not only means he has given affirmative response to two of the questions or items but also indicates that he agrees with two particular questions, namely the first and second. Scores in Guttman scale can also be interpreted as the ``ability'' of a subject in answering questions sorted in increasing order of ``difficulty''. These scores when presented on an underlying scale,

give us an ordering of the subjects based on their ``ability'' also. The objective of our paper is to order the Indonesian Islamic organizations based on their views on religio-social keywords which have an inherent ordering. For example, two such keywords are ``Quran'' and ``Sharia''. An organization supporting ``Sharia'' will also likely to ``believe in Quran''. So it makes sense to use Guttman scaling procedure to rank the organizations and their beliefs and practices. One drawback of Guttman scale is that it is deterministic and assumes a strict ordering of the items. In real world, it is difficult to order all the items in such a strict level of increasing difficulty, therefore, perfect scales are not often observed in practice. Furthermore, many times, the order of the items are not known since they are not straightforwardly comparable. In addition, measurement errors might lead to responses that do not strictly fit the ordering. As a result, we can no longer conclude deterministically that if a subject answers a question affirmative, whether she will be able to give affirmative answers to other questions of lower order in the same questionnaire. We use Rasch model to overcome this drawback by taking into account measurement error. 2.2 Rasch model Rasch model (Andric 1988) provides a probabilistic framework for Guttman scales. In Rasch model, the probability of a specified binary response (e.g., a subject agreeing or disagreeing to an item) is modeled as a function of subject's and item's parameters. Specifically in the simple Rasch model, the probability of a positive response (yes) is modeled as a logistic function of the difference between the subject and item's parameters. Item parameters pertain to the difficulty of items while subject parameters pertain to the ability of subjects who are assessed. A subject of higher ability, related to the difficulty of an item, has higher probability to respond to a question affirmatively. In this paper, Rasch models are used to assess the organizations degree of being radical or counter-radical based on the religio-social keywords (items) appearing in their rhetoric. Rasch model also maps the responses of the subjects to the items in binary or dichotomous format, i.e., 1 or 0. Let Bernoulli variable Xvi denotes the response of a subject v to the item i, variable hv denotes the parameter of ``ability'' of the subject v and bi denotes the parameter of ``difficulty'' of an item i. According to the simple Rasch model, the probability that the subject v responds 1 for item i is given by: PðXvi ¼ 1jhv ; bi Þ ¼ expðhv À bi Þ : 1 þ expðhv À bi Þ

Rasch model assumes that the data under analysis have the following properties.

123

S. Tikves et al.

Unidimensionality P(xvi = 1|hv, bi, a) = P(xvi = 1|hv, bi), i.e., the response probability does not depend on other variable 2. Sufficiency sum of responses contains all information on ability of a subject, regardless which item it has responded 3. Conditional independence for a fixed subject, there is no correlation between any two items 4. Monotonicity response probability increases with higher values of h, i.e., subject's ability. P Items with si = n v xvi value of 0 or n, and subjects with Pk rv = i xvi value of 0 or k are removed prior to estimation, where n is the total number of subjects and k is the total number of items. Running Rasch model on the data gives us an item parameter estimate or a score for each item. In general, the estimation of bi or score for an item i is calculated through conditional maximum likelihood (CML) estimation (Pawitan 2001). The conditional likelihood function for measuring item parameter estimate is defined as: Y expðÀbi si Þ Lc ¼ Pðxvi jrv Þ ¼ Q P r xjr expðÀbi xvi Þ v 1. where r represents the sum over all combinations of r items. Similarly, the maximum likelihood is used to calculate subject parameter estimation hv or score for each subject. Expectation-maximization algorithms (Hunter 2004) are used in implementing CML estimation in Rasch model. We can also assess whether the data fit the model by looking at goodness of fit indices, such as the Andersen's LR-test. To evaluate the quality of these measurements, we run Anderson LR-test (Hessen 2010) on the set of data. The test gives us a goodness of fit of the data in Rasch model, i.e., it tells us whether the data follows the assumptions of Rasch model. A p value, returned by the test, indicates the goodness of fit and a p value2 higher than 0.05 indicates no presence of lack of fit. 2.3 Implementing Rasch model in the text mining domain In this paper, we use Guttman scaling and Rasch model to find a ranking of 23 religious organizations based on extremity of their views are on radicalism and counterradicalism. In our application, Rasch-model subjects correspond to a group of religious organizations, and items correspond to a set of keywords for socio-cultural, political, religious radical and counter-radical beliefs, and practices. An organization responding ``yes'' to a feature means the organization exhibits that feature in its narrative,
2

while an organization responding ``no'' to a feature indicates that the organization does not exhibit such a feature. Difficulty of an item translates to strength of the corresponding attitude in defining radical or counter-radical ideology of any organization. Similarly ability of a subject in this case means the degree of radicalism or counterradicalism exhibited by an organization's rhetoric. Other works in text-mining domain, such as sentiment analysis, have used Rasch model in their analysis (Drehmer et al. 2000). Details of keyword extraction and selection are presented in Sect. 3.3.

3 Methods 3.1 Problem definition The primary goal of this study is to build a semi-automated method to rank religious organizations from a certain geographical region on a scale of radicalism versus counterradicalism using their web sites. The efficacy of the generated model is evaluated by comparing it against baseline methods and expert-level performance. In addition to accomplishing these goals, we also present an end-to-end system architecture, and a graphical user-interface design to facilitate faceted search and browsing of this corpus. 3.2 System architecture A summary of the system architecture can be seen in Fig. 1. The system is a composition of four components: a data-gathering component, which does web crawling, and text extraction; a scale generation component, performing scaling algorithms; application services component, which consists of several web services, and finally, a web userinterface component, presenting the data to the end user. 3.2.1 Data gathering Initially, social scientists are invited to use their domain and area expertise to identify a set of organizations, and hypothesize any number of unipolar or bipolar scales that could explain the variance among their beliefs and practices. Next, a set of web crawling scripts are created for extraction of articles from those organizations' web sites. For each organization's corpus, we extract their top-k n-grams, and a union of all these phrases are presented to experts for feature selection. Downloaded articles are then converted into XML structures, containing their original text, their set of keywords, and extracted information such as person, location and organization names using a NER tool for Indonesian language, and their machine translations into English.

http://www.en.wikipedia.org/wiki/P-value.

123

A system for ranking organizations using social scale analysis

economic, and religious} keywords corresponding to beliefs, goals and practices. During this process, our team of experts screened a total of 790 candidate keywords and they selected 29 keywords for inclusion in the radical scale, and 26 keywords for inclusion in the counter-radical scale. 3.4 Debates and perspective analysis Upon inspecting the keywords selected by our team of experts, we observed that some of these keywords correspond to differing perspectives on a set of topics that are debated within these web sites. Definition of debate is ``a formal discussion on a particular topic in a public meeting or legislative assembly, in which opposing arguments are put forward''.3 During a debate on a particular topic, like education, both radical and counter-radical organizations discuss different perspectives such as ``secular multi-cultural education'' versus ``Sharia based religious education''. To design an automated perspective detection algorithm, we made the following simplifying assumptions. 1. 3.3 Keyword extraction and selection 2. To identify candidate keywords, one option was to translate the documents into English and apply readily available keyword-extraction methods (Michael 2010). However, it was preferable to preserve the original expression of the phrases in the original language. Hence, we utilized a nonlinguistic technique that relies only on statistical occurrence, and frequency information. Within each document, the words were separated by whitespace or punctuation marks. We considered each keyword to be an n-gram of one to three words. We treated each organization as one document and calculated the term frequency-inverse document frequency (TF-IDF) (Salton 1988) values for every single n-gram mentioned by these organizations. Top 100 n-grams with the highest TF-IDF values from each organization were used to generate a candidate list of topics that these organizations discuss most frequently. Next, we asked our team of experts to screen and manually select identify {social, political, Organizations will mostly discuss their own perspective in a debate. Organizations will occasionally mention others perspectives, however, then relate them back to their own perspective.

Fig. 1 An overview of the system architecture

An example document snippet is shown in Fig. 2. Here the original input (content, source), and a sample of the automatically extracted information corresponding to DATE, PERSON, and LOCATION can be seen. The corresponding XML versions for each input document are then stored in a document database for processing.

In the following sections, we present a mathematical formulation of the perspective keyword-generation problem for a given topic, provide an NP-completeness proof, and design an exact solution through an integer linear programming (ILP)-based solver. Our future work involves finding an efficient approximation algorithm for this problem. 3.4.1 Perspective keywords-generation problem Perspective keywords-generation problem (PKGP) is defined as follows. Given a topic (a keyword) T, and two sets of documents TR and TCR where TR contains n documents TR ¼ fDR;1 ; DR;2 ; . . .; DR;n g and TCR contains m documents TCR ¼ fDCR;1 ; DCR;2 ; . . .; DCR;m g: From each document DR;i 2 TR ðDCR;j 2 TCR Þ, we collect a set of words WR,i, V1 B i B n (WCR,j, V1 B j B m) which appear two words before and two words after each occurrence of the topic T in that document. Let us define W as the union of all the WR,i, V1 B i B n and WCR,j, V1 B j B m. If the cardinality of W is p, then W can be given as W ¼ fw1 ; w2 ; . . .; wp g ¼ fWR;1 [ WR;2 [ Á Á Á [ WR;n [ WCR;1 [ WCR;2 [ Á Á Á [ WCR;m g Let the frequency of word wk in document DR,i is given as fR,i(wk) and the frequency of word wk in document DCR,j as fCR,j(wk).
3

Fig. 2 A portion of a document represented in the system

Oxford online dictionary.

123

S. Tikves et al.

Question: Are there two non-empty disjoint subsets of W, named W 0 and W 00 and W 0 \ W 00 ¼ ;; such that for every DR,i V1 B i B n, X X fR;i ðwk Þ ! fR;i ðwl Þ ð 1Þ
wk 2 W 0 wl 2W 00

Since WPP is known to be NP-complete, PKGP is also NPcomplete. 3.4.3 Integer linear programming formulation for PKGP We formulate an ILP to solve the PKGP optimally. For each word wi 2 W , we use two variables xi and yi. xi is 1 if and only if the word wi is in W1 and yi is 1 if and only if the word wi is in W2. Then constraint (3) means sets W1 and W2 disjoint. Constraint (4) ensures that these sets (W1 and W2) are also non-empty. Constraints (5) and (6) ensure the constraints 1 and 2 in problem statement. The objective minimizes the summation of cardinality of W1 and W2. Variables: For each word wi, & 1; if word wi is assigned to set W1 xi ¼ 0; otherwise. & 1; if word wi is assigned to set W2 yi ¼ 0; otherwise. Pp min i ¼1 x i þ y i s: t : xi þ y i
n X i¼1 p X wk 2 W p X wk 2 W

and for every DCR,j V1 B j B m, X X fCR;j ðwk Þ fCR;j ðwl Þ
wk 2 W 0 wl 2W 00

ð 2Þ

and jW 0 j þ jW 00 j K ? In optimization version of the problem, we will try to minimize jW 0 j þ jW 00 j: 3.4.2 Computational complexity of PKGP Definition 1 [Weak Partition problem (WPP)] Instance A finite set A ¼ fa1 ; . . .; an g and a size sðai Þ 2 Z þ ; 8i; 1 i n: Question Does the set A contain two nonempty sub-sets A1 and A2 that (1) A1 \ A2 ¼ ;; (2) A1 [ P P A2  A and (3) ai 2A1 sðai Þ ¼ aj 2A2 sðaj Þ? WPP has been shown to be NP-complete in (van Emde Boa 1981). Theorem 1 PKGP is NP-complete.

1;
n X i ¼1

8i ¼ 1; . . .; p y i ! 1; 8i ¼ 1; . . .; p

ð 3Þ ð 4Þ

xi ! 1 and

Proof It is easy to see that PKGP is in NP since a nondeterministic algorithm needs only to guess a partition of the word set W into W 0 and W 00 and check in polynomial time if all the constraints hold for this partition and also if jW 0 j þ jW 00 j K : WPP is a restricted version of PKGP. First we create a restricted instance of PKGP as follows: let TR and TCR contains one documents each, i.e, TR = {DR,1} and TCR = {DCR,1}. Frequency of a word wi 2 W ; 81 i n in document DR,1 and DCR,1 is taken to be equal, i.e., fR,1(wi) = fCR,1(wi) = s(ai). The parameter K is taken to be equal to |W|. This instance of PKGP is similar to an instance of WP in the following way: the set A contains element ai for every word wi 2 W : So, |W| = |A|. In addition, s(ai) = fR,1(wi) = fCR,1(wi), V1 B i B n. If we find a weak partition of A, as sets A1 and A2 such that P P ai 2A1 sðai Þ ¼ aj 2A2 sðaj Þ, then we can find subsets of W, as sets W1 and W2, such that wi 2 W1 if ai 2 A1 and P wj 2 W2 if aj 2 A2 , respectively. In addition, ai 2A1 sðai Þ ¼ P P 0 aj 2A2 sðaj Þ; implies that both the constraints P P P wi 2 W fR;1 ðwi Þ ! wj 2W 00 fR;1 ðwj Þ and wi 2W 0 fCR;1 ðwi Þ wj 2W 00 fCR;1 ðwj Þ are true, because s(ai) = fR,1(wi) = fCR,1(wi), V1 B i B n. Since K = |W|, the constraint jW 0 j þ jW 00 j K will trivially hold. So, WPP is a restricted version of PKGP.

fR;i ðwk Þðxk À yk Þ ! 0;

8i ¼ 1; . . .; n

ð 5Þ

fR;i ðwk Þðxk À yk Þ

0;

8i ¼ 1; . . .; m 8i ¼ 1; . . .; p

ð 6Þ ð 7Þ

xi 2 f0; 1g; yi 2 f0; 1g;

3.4.4 Social scale generation Social scale generation is done by building response tables; a pair of tables for a bipolar scale, such as radical/counterradical (R/CR), or a single table for a unipolar scale, by thresholding the occurrence frequencies of the selected keywords in the organizations' web corpus. The scale-generation architecture is shown in Fig. 3. Here, the flow of the processes and data can be seen as interactions between experts and automated modules. The system works as follows. ­ Initially, area experts to identify a set of organizations, and hypothesize any number of unipolar or bipolar scales that could explain the variance among the beliefs and practices of the organizations. Next, we crawl and download the web sites of the organizations, and the system automatically extracts the top-k candidate keywords for consideration in the

­

123

A system for ranking organizations using social scale analysis

­

Two types of other information are collected for evaluation purposes. First, expert rankings of the organizations, using a graphical drag-and-drop expertopinion elicitation tool shown in Fig. 11. Expert rankings are merged into a consensus gold standard of rankings. Next, two other computational baseline methods; one based on simple sorting, and another based on principal component analysis (Jolliffe 2002), are used to generate alternative computational rankings shown in Fig. 12.

In addition, the data for the violence/non-violence are gathered using a separately developed tool, by collecting the opinion of the experts. A future work will also include automated generation of this dimension, as well. 3.5 Feature extraction After identifying the keywords for the analysis, we needed to search the web site corpus of the organizations for the matching items. This yielded a term-document matrix. This task was performed in a simple three-step procedure; initially, the occurrence frequencies of particular keywords were counted within each organization's corpus, then, a threshold matrix was calculated from the initial values, and finally, a binary response matrix was generated by applying these thresholds to the initial values. The frequency metric is shown in formula 8, where k is the keyword, o is the organization, and Do is the document set pertaining to that particular organization. fo;k ¼ jfdjk 2 d; d 2 Do gj jDo j ð 8Þ

Fig. 3 A model of the system architecture

­

­

hypothesized scale. Social scientists screen the list of extracted keywords, and select the relevant ones for inclusion in further analysis. The system builds response tables; a pair of tables for a bipolar scale (such as radical/counter-radical R/CR), or a single table for a unipolar scale, by thresholding the occurrence frequencies of the selected keywords in the organizations' web corpus. See Figs. 4 and 5 for the response tables for the R/CR scale. The response tables are fed as input to the Rasch Model building algorithm. The algorithm produces a metric to validate the fitness of the model, and rankings of the organizations and keywords. Figures 6 and 7 show the relative positions of the organizations and keywords on the latent scales. The algorithm also produces a metric to validate the fitness of the model.

A threshold value for each keyword is calculated by taking the median of the values in the related column. Median was preferred over mean as a threshold, since the distribution of the values did not fit Gaussian distribution, yet median empirically proved to be a better measure.

Fig. 4 Radical subset of organizations and keywords, sorted according to aggregate row values

123

S. Tikves et al. Fig. 5 Counter-radical subset of organizations and keywords, sorted according to aggregate row values

Fig. 6 Radical subset of organizations and keywords

Finally, each element was converted into a binary value by comparing it to the column's threshold. English translations of the keywords are presented for clarity in Figs. 4 and 5. 3.6 Model fitting We fit the Rasch model on two datasets: (1) radical organizations with radical keywords and (2) counter-radical organizations with counter-radical keywords. We used the eRm package in R, an open source statistical software package,4 to fit a Rasch model to the dataset, and obtain the organizations' scores on the latent scale, which are the the subject parameter estimates (hv) discussed in the previous section. The eRm package5 fits Rasch models and provide subjects or organizations parameter estimates based on maximum likelihood estimation.
4 5

The automated scale of the organizations is formed by ranking the organizations according to their estimates on the latent scale. Not only we can provide the organization estimates but we can also assess whether the model fits the data by looking at several goodness of fit indices, such as the Andersen's LR-test. 3.7 Application services We use two backend services in the application layer to present the data to the user interface. First, all the extracted textual information are stored in Apache Solr,6 providing facilities like full-text search and faceting (Tunkelang 2009), using an AJAX interface. In addition, a WCF-based scaling service is used to infer scales in real time. This particular service loads the response table, and the previously generated scale data, and estimates the R/CR scale
6

http://www.cran.r-project.org/. http://www.r-forge.r-project.org/projects/erm/.

http://www.lucene.apache.org/solr/.

123

A system for ranking organizations using social scale analysis

Fig. 7 Counter-radical subset of organizations and keywords

for a subset of the input. Number of positive responses are interpolated on the scale to generate the scale, and the expert opinion is used for a static violent/non-violent (V/NV) scale. While the interpolation is based on a sufficient statistics, future work on speeding up Rasch model generation for real-time use would be beneficial. 3.8 User interface The user interface is responsible for representing our input data, and the findings to the experts in an interactive fashion. Users should be in control of the selection of the data displayed, and filtering with organization names, or a specific date range, or using other parameters such as arbitrary keywords, or geographic locations. While performing these tasks, it should provide results to the user with a minimum of delay, allowing quick drilling down to interactively model the scenarios that users have in mind. The user interface is implemented as an interactive AJAX-based application, using ajaxsolr7 framework. In addition to the search and navigation capabilities provided with ajaxsolr, it also adds functional widgets for visualizing the organizations on a scale, mapping the intensity of the locations, displaying demographics trends, and so on. A more detailed discussion of the user interface is provided in Sect. 5. The presentation of the scale, however, brings the following challenges. ­ It would be preferable to plot the locations on the same range as the input collection. However, the Rasch scale is on a latent range (Figs. 8, 9).

­

Since this will be an interactive application, users would prefer to see almost instantaneous results. Yet, the eRm model generation is computationally expensive.

We resolve the first issue by uniformly scaling the ranges into [-10, 10], making it consistent with the inputs. The second issue requires a more specific solution. We make use of the fact that the raw person scores pertaining to number of positive responses is a sufficient statistics for the Rasch model (G 1961) to estimate scale values on the fly. Since we know the date range, and the selected organizations currently visible in the user interface, it is possible to quickly generate a response matrix for this subset of the data, and merge it with the previously known scale information to generate interpolated scale values.

Radical Scale

Person Parameters (Theta)

-4 0

-2

0

2

5

10

15

20

25

30

Person Raw Scores
7

http://www.evolvingweb.github.com/ajax-solr/.

Fig. 8 Radical scale

123

S. Tikves et al.

Here we have opted to include all the organizations in threshold calculations. This is because, the radical or counter-radical activity intensities are always measured relative to the other organizations participating in the same time period. However, while the scale is based on all the organizations, only the ones specifically asked will be presented to the user.

4 Experimental evaluation 4.1 Indonesian corpus The corpus domain is the online articles published by the web sites of the 23 religious organizations identified in Indonesia, in the Indonesian language. These sources are the web sites or blogs of the identified think tanks and organizations. As discussed in the Sect. 1, each source was classified as either radical or counter-radical by the area experts. We downloaded a total of 37,000 Indonesian articles published in these 23 web sites, dating from 2001 to 2011. For each web site, a specific REGEX filter was used to strip off the headers, footers, advertising sections and to extract the plain text from the HTML code. The psuedo-code for the subset scale-generation procedure is presented in Algorithm 1. The process starts with identifying the subset of documents in the (start, end) date range (lines 2­5). Then the keyword frequencies, and thresholds are calculated for the entire set of organizations on this document subset (lines 6­14). Finally, response tables for the subset of organizations is generated (lines 15­17), and then the sums need to be interpolated (lines 18­23), to be able to generate a scale on the [-10,10] range (line 24).
Counter-Radical Scale

4.2 The quadrants model Our project leverages the results of our previous work, which relied on social theory including Durkheim's (2004) research on collective representations, Simmel's (2008) work on conflict and social differentiation, Wallace's (1956) writings on revitalization movements, and Tilly and Bayat's studies on contemporary social movement theory (Tilly 2004; Bayat 2007). Our team has also developed, and is currently testing a theoretically based class model comprised of continuous latent scales. The first pair of scales focus on distinctions between the goals and methods of counter-radical and radical discourse, and capture the degree to which individuals, groups, and behaviors aim to influence the social order (change orientation) and the methods by which they attempt to do so (change strategies). Quadrants model (see Fig. 10) captures multiple social trends in four quadrants (A, B, C, and D), and it makes the significant distinction between violent and not-violent dimensions of both radicalisms and counter radicalisms. Using the quadrants model, a researcher can locate organizations, individuals, and discourses in broader categories while still considering subtle differences between groups within categories. A researcher can document movement and trends from category to category, and identify points where movement is likely to happen.

Person Parameters (Theta)

-4 0

-2

0

2

5

10

15

20

25

Person Raw Scores

Fig. 9 Counter-radical scale

123

A system for ranking organizations using social scale analysis

rankings. The individual scores for each organization were combined and averaged to obtain the consensus gold standard rankings along the hypothesized R/CR scale. A work is in progress for building a publicly accessible expert opinion collection toolkit. The preliminary version can be accessed at: http://www.minerva-project.org/ DataCollector. 4.4 Computationally generated scale The ranking discovered by the Rasch model fitting the corpus has been evaluated against the gold standard rankings of the organizations provided by the experts. The difference between two separate rankings have been calculated using the following misplacement error measure in Eq. (9). P jGðoÞÀRðoÞj error ðG; RÞ ¼
o 2O jOj

Fig. 10 The quadrants model

4.3 Expert opinion and gold standard of rankings We collaborated with three area experts, who collectively possess 35 years of scholarly expertise on Indonesia and Islam. To build a gold standard of orderings of the organizations, we built a graphical drag-and-drop user-interface tool to collect the opinions of each of the area experts. A screenshot of the tool is shown in Fig. 11. Each expert, separately evaluated and ranked the organizations in the dataset according to a two dimensional scale of radical/counter-radical (R/CR) and violent/nonviolent (V/NV) axis. The consensus among the experts was high; since per item standard deviations among the experts' scores along the R/CR axis over a range of [-10, 10], across all organizations were 2.75. In addition, 90 % of the items have less than 22.6 % difference in their

jO j

ð 9Þ

Here, O is the set of organizations, G and R are one to one mapping functions of rankings from set O to range [1,|O|]. For two exactly matching rankings, the error(G, R) will be zero, whereas for two inversely sorted rankings it is expected to be 0.5 (when the size of O is even). In addition, a random ranking is expected to have a error of 0.375. 4.5 Expert-to-gold standard error We calculated the error between each expert's ranking and their consensus gold standard of rankings. The first expert's error measure is 0.06, and the second and third expert's errors are 0.12 and 0.14 correspondingly as shown in the last row of the table in Fig. 12. The average error of our experts against their gold standard ranking is 0.11. 4.6 Baseline: sorting with aggregate score The first baseline we used was constructed by sorting the organizations according to the number of different keywords observed in their corpus. While this provided a pattern similar to a Guttman scale, and orderings of the organizations matched to a certain degree with the gold standard as shown in Fig. 12, the error for this baseline was 0.19, which is higher than the average expert's performance. 4.7 Baseline: principal component analysis A stronger baseline was built by employing principal component analysis (Jolliffe 2002), and sorting the organizations according to their projections in the first principal component of the term­document matrix. Since experts selected the R/CR scale relevant keywords only, it was expected that the first principal component would reflect the corresponding scale. PCA proved to be performing

Fig. 11 The visual interface of the expert-opinion collector for manually placing the organizations on the two dimensional scale

123

S. Tikves et al. Fig. 12 Computational and expert rankings

better than the aggregate score sorting, with an error measure of 0.18. However, this error rate is still higher than the error rate of each expert. 4.8 Performance of the Rasch model ranking system The p values from the Anderson LR goodness of fit test from model (1) and model (2) (mentioned in Sect. 3.6) are 0.85 and 0.669, respectively, suggesting no evidence of lack of fit. The Rasch models allow us to get a natural order of the organizations, according to their ``abilities'', i.e., radicalism and counter-radicalism in this case. This system had an error measure of 0.10, which actually provided a higher ranking performance than the average performance of our experts'-- performing better than the majority of our area experts. 4.9 Evaluations Our experiments showed that the hypothesized compatibility of the R/CR scale for the Indonesian corpus is valid. Not only the Rasch model was statistically fitting the response matrix but also the generated ranking performance was better than the average expert performance. Among our computational baseline methods, the Rasch Model was the only method producing expert-level performance as shown in Fig. 12. This preliminary analysis with the R/CR scale shows that when experts assist the system with keyword selection, the web corpus of organizations provides rich-enough

information and patterns to enable a computational method to rank them accurately.

5 Web application overview A sample snapshot of the web application can be seen in Fig. 13. It is composed of four main widgets for visualization and navigation. The top-left section which contains the Search and Navigation widget (1) that allows filtering of the document subset using parametric search queries and keyword based search criteria. The top-right section is the Quadrant widget (2) which displays the organizations active in the currently selected time frame on a twodimensional axis, using violence and radicalism scales. The bottom-left section consists of two Treemap widgets (3) which displays the demographics and the top keywords (markers) of the current selection. The bottom-right section has a Timeline widget (4) which provides a visualization of the keywords (markers) trends on a time line. The navigation in the user interface starts with the Navigation widget (top-left) of the web application. Here the user is able to filter down the corpus utilizing full-text search queries, or faceting using keywords, locations, demographics, or choosing a subset of organizations. The Quadrant widget (top-right) provides a plot of the currently selected organizations on the two dimensional scale. The radical/counter-radical (R/CR) axis is

123

A system for ranking organizations using social scale analysis

Fig. 13 A sample snapshot of the web application (color figure online)

dynamically calculated in real time, using the subset of organizations, and the time range of the current selection. The location change on the time range for each organization is shown as a color-coded path, with three markers, a light circle corresponding to the position at the beginning of the period, a dark circle corresponding to the end of the period, and a dark-small circle for the middle. A red line between the circle denotes the rise of radical activities in the organization's behavior. A blue line denotes the opposite. The smaller circle is useful to see the overall movement of an organization. For example, between the range Aug 2005 and Aug 2007, EraMuslim's activities were radical (center of A quadrant), then became almost counter radical (the smaller circle denotes this mid point in the movement), and then jumped up again. The V/NV axis is retrieved from expert opinion in the current version, and dynamic calculation of this axis is left for a future version. The Timeline widget (bottom-right) displays the trends of the most frequent markers on a time line. Initially the subset of markers presented defaults to all available, however it is possible to restrict the selection of markers to a more limited set among radical/counter-radical, economical, political, religious, or social domains. Timeline widget can also be used for selecting a date range of interest. The Treemap widgets (bottom-left) are used to display the relative frequencies of demographics and keywords

(markers). The displayed marker category selection for this widget is synchronized with the Timeline widget. In the following sections, we present some scenarios and findings to illustrate the capabilities of the web interface. 5.1 Scenario 1: radical organizations' trends In this scenario, we analyze both violent and non-violent radical organizations. Our web application shows the ideologies that these organizations are propagating. We can see8 the most prominent markers associated with these radical organizations. Markers such as ``infidel'', ``Sharia'', and ``violence'' show an increasing trend between 2001 and 2011. A very strict interpretation of ``Sharia'' is used by radical organizations to justify their actions (Widhiarto 2010; Hasan 2009). ``Sharia'' peaks during this period as shown in Fig. 14. 5.2 Scenario 2: C-quadrant organizations' trends We now analyze Front Pembela Islam (FPI), an Islamic organization in Indonesia established in 1998. FPI is well known for its violent acts (Frost et al. 2010; Rondonuwu
8

Select the filter ``Radical'' from the search options and then in the Markers Menu select [Religious ! Radical Markers].

123

S. Tikves et al.

Fig. 14 Trend of radical markers

2010) justified by a strict interpretation of Sharia (for the Study of Terrorism 2011). Our documents for FPI ranges between 2000 and 2010. Using our web application's plots of the movement of FPI in the C Quadrant, we found that FPI consistently rised higher on the radical scale as shown in Fig. 15. We selected the following time ranges, 2000­2003, 2002­2006, 2006­2010 and analyzed the trends of various markers associated with FPI. There was a substantial increase in the intensity of various radical markers such as ``infidel'', ``Mujahedin'', ``pornography''.9 Since 2006, we also saw a steep increase in the frequency of marker ``Ahmadiyya'', as shown in Fig. 16, which indicates FPI's increased opposition to this heretical sect (Rahmat and Sihaloho 2011). 5.3 Scenario 3: A-quadrant organizations' trends We analyze Hizb ut-Tahrir also known as Hizb ut-Tahrir Indonesia (HTI), a radical organization widely believed to be non-violent (Ward 2009), which has been active in Indonesia since 1982 (Osman 2011). Between 2007 and 2009, our web application shows various radical and nonradical markers associated with this organization.

Fig. 15 Consistent rise of FPI on the radical scale

Fig. 16 ``Ahmadiyya'' peaking during the period 2006­2010

Radical ``Sharia'', ``Infidel'', ``Caliph'', ``Violence''

Non-Radical ``Politics'', ``Indonesian Islam'', ``Election'', ``Liberal'', ``Democracy''

During the same period, we see a steady increase in the frequency of the radical marker ``Sharia''. This is consistent with one of HTI's goals of implementing Sharia in Indonesia (Hasan 2009). Hizb ut-Tahrir openly propagates
9

Fig. 17 ``Khilafah'' ideology of Hizb ut-Tahrir

Select ``Radical'' and ``FPI'' from the filters, then select the time range 2002­2006 or 2006­2010, then select ``radical'' markers under ``R/CR'' menu.

the ideology of Khilafah, which believes in unification of all Muslim countries as a single Islamic State (Zakaria 2011; Mohamed Osman 2010). Figure 17 shows

123

A system for ranking organizations using social scale analysis

Searching for the text ``suicide bombing'', we see that one of the related markers is ``ideology''. Adding the keyword ``ideology'' to the search filter reveals a new set of markers including the ``sin'' keyword. Adding ``sin'' to our search, we obtain a set of matching documents. One of the top matches, is titled ``Mengapa Saya Berubah?'' (english translation: ``Why I changed?'')13. This article is by a reformed terrorist, debunking the misinterpretation of the jihad-related verses used by violent groups.

6 Conclusions and future work In our experiments, not only did the data show fitness with the Rasch Model for the R/CR scale but also the Rasch rankings of the organizations are better than the output of the other baseline computational methods, and they are at expert-level performance when compared with the consensus gold standard rankings. Rasch model also provided us with another output, namely the ranking of selected keywords (items) on the R/CR scale. Although preliminary observations indicates that this can be a valuable asset by itself, we plan to further investigate the quality and utility of this ranking as future work. While the model has been demonstrated to fit on the R/CR scale, two major expansion points can be investigated in the future work, namely the violent/non-violent scale, and enhancement of feature selection. Although our experts have identified a second dimension, evaluating its correlation to R/CR axis, or existence of other significant ones could be beneficial. In addition, the features can be enhanced by experimenting with the significance of the radical keywords in the counter-radical organization corpora, and vice-versa. A practical method to increase the automation of keyword generation has been discussed in Sect. 3.4. Future work will involve finding an efficient approximation algorithm for this model, for decreasing the necessity of expert interaction for this particular step. Other interesting work includes making our expert opinion elicitation tool available online to a wider and more geographically distributed audience to crowdsource (Snow et al. 2008) the needed expertise for making lists of local organizations, identifying their web sources, and overcome the complex task of construction and validation of significant and fitting scales (work is currently underway to build this tool). Another interesting dimension is to look at synthesis and analysis of scales that do have a strict hierarchy of keywords, but adhere to more flexible partial order models (James and John 2002).
13

Fig. 18 Decline of the HTI in the radical scale

``Khilafah'' as the most prominent marker10 in Hizb utTahrir's discourse. By looking at the Quadrants widget (in Fig. 18), we can infer that HTI has been moderating its narrative. 5.4 Scenario 4: B-quadrant organizations' trends In this scenario, we discuss the trends of counter radical organizations like NU and DaarulUluum. We also show an interesting scenario on the topic of ``Suicide Bombing'' using the keyword based Navigation widget. The ``counter radical'' markers11 associated with these organizations are: ``politics'', ``election'', ``Indonesian Islam'', ``liberal'', ``human rights''. These organizations support democracy and elections, which is shown by the high frequency of the markers ``politics'' and ``election''. Their narrative has local interpretation of Islam at its core, which is shown by the marker ``Indonesian Islam''. On analyzing the occurrences of radical markers12 in B-Quadrant, we find that counter radical organizations are very vocal against all of radical markers. One of the interesting radical markers is ``Suicide Bombing''. Most of the counter radical organizations are against suicide bombings.(Malang 2006). We will now demonstrate how combination of parametric and keyword search, and various widgets in the web application can help reveal opposition to ``Suicide Bombing'' by counter-radical organizations.
10

Select ``Hizb ut-Tahrir'' and ``radical'' from filters. Select the time range 2007­2009. The markers can be seen by selecting the options of Markers Menu [Religious ! Religious Markers]. 11 Select CounterRadical filter in the search option, then from the Markers Menu select [R/CR ! Counter Radical]. 12 In the Markers Menu select [R/CR ! Radical].

http://www.islamlib.com/id/artikel/mengapa-saya-berubah/.

123

S. Tikves et al. Acknowledgments This research was supported by US DoDs Minerva Research Initiative Grant N00014-09-1-0815, Project leader: Prof. Mark Woodward, Arizona State University, and the project title is ``Finding Allies for the War of Words: Mapping the Diffusion and Influence of Counter-Radical Muslim Discourse''. doi:10.1080/09546553.2010.496317. http://www.tandfonline. com/doi/abs/10.1080/09546553.2010.496317 Osman MNM (2011) Preparing for the caliphate. Asian Stud Assoc Aust E-Bull 80:14­16. ISSN:1449-4418 Pawitan Y (2001) In all likelihood: statistical modelling and inference using likelihood. Oxford University Press, USA Rahmat Sihaloho M (2011) FPI vows to disband ahmadiyah 'whatever it takes'. http://www.thejakartaglobe.com/home/fpivows-to-disband-ahmadiyah-whatever-it-takes/423477 [Online accessed 21 Nov 2011] Rasch G (1961) On general laws and the meaning of measurement in psychology. In: Proceedings of the fourth Berkeley symposium on mathematical statistics and psychology, 4, p 332 Rondonuwu O, Creagh S (2010) Opposition grows to indonesia's hardline fpi islamists. http://www.in.reuters.com/article/ 2010/06/30/idINIndia-49777620100630 [Online accessed 21 Nov 2011] Salton G, Buckley C (1988) Term-weighting approaches in automatic text retrieval. In: Information Processing and Management, vol 25, pp 513­523 Simmel G (2008) Sociological theory. McGraw-Hill, New York Snow R, O'Connor B, Jurafsky D, Ng AY (2008) Cheap and fast-- but is it good?: evaluating non-expert annotations for natural language tasks. In: Proceedings of the conference on empirical methods in natural language processing, EMNLP '08, pp 254­263. Association for Computational Linguistics, Stroudsburg, PA, USA. http://www.portal.acm.org/citation.cfm? id=1613715.1613751 for the Study of Terrorism NC, to Terrorism R (2011) Terrorist organization profile: front for defenders of Islam. http://www. start.umd.edu/start/data_collections/tops/terrorist_organization_ profile.asp?id=4026 [Online accessed 21 Nov 2011] Thurstone LL (1928) Attitudes can be measured. Am J Sociol 33:529­554 Tikves S, Banerjee S, Temkit H, Gokalp S, Davulcu H, Sen A, Corman S, Woodward M, Rochmaniyah I, Amin A (2011) A system for ranking organizations using social scale analysis. In: EISIC. IEEE, pp 308­313. http://www.ieeexplore.ieee.org/xpl/ mostRecentIssue.jsp?punumber=6059524 Tilly C (2004) Social Movements. Paradigm Publishers, USA Tunkelang D (2009) Faceted Search: synthesis lectures on information concepts, retrieval, and services. Morgan & Claypool Publishers, UK. doi:10.2200/S00190ED1V01Y200904ICR005 van Emde Boas P (1981) Another NP-complete partition problem and the complexity of computing short vectors in a lattice: Tech. Rep. 81-04. Mathematisch Instituut, Amsterdam Wallace A (1956) Revitalization movements. Am Anthropol 58:264­281 Ward K (2009) Non-violent extremists? Hizbut Tahrir Indonesia. Aust J Intl Affairs 63(2):149­164 doi:10.1080/1035771090 2895103.http://www.tandfonline.com/doi/abs/10.1080/10357710 902895103 Widhiarto H (2010) Radical groups urge Bekasi administration to implement Sharia law. http://www.thejakartapost.com/news/2010/ 06/27/radical-groups-urge-bekasi-administration-implementsharia-law.html [Online accessed 21 Nov 2011] Zakaria Y (2011) A Global Caliphate: reality or fantasy? http://www. usa.mediamonitors.net/content/view/full/91207 [Online accessed 21 Nov 2011]

References
Andrich D (1988) Rasch models for measurement. Sage, USA Bayat A (2007) Making Islam Democratic: social movements and the post-Islamist turn. Stanford University Press, USA Crelinsten R (2002) Analysing terrorism and counter-terrorism: a communication model. Terror Political Violence 14:77­122 Davulcu H, Ahmed ST, Gokalp S, Temkit MH, Taylor T, Woodward M, Amin A (2010) Analyzing sentiment markers describing radical and counter-radical elements in online news. In: Proceedings of the 2010 IEEE second international conference on social computing, IEEE Computer Society, SOCIALCOM'10, pp 335­340 Drehmer D, Belohlav J, Coye R (2000) An exploration of employee participation using a scaling approach. Group Org Manage 25(4):397 Durkheim E (2004) The cultural logic of collective representations: social theory the multicultural and classic readings. Wesleyan University: Westview Press Frost F, Rann A, Chin A (2010) Terrorism in southeast asia. http://www.aph.gov.au/library/intguide/FAD/sea.html [Online accessed 21 Nov 2011] Guttman L (1950) The basis for scalogram analysis. Meas Predict 4:60­90 Hasan N (2009) Islamic militancy, Sharia, and democratic consolidation in post-Suharto Indonesia. RSIS Working Papers. 143/07 Hessen D (2010) Likelihood ratio tests for special Rasch models. J Edu Behav Stat 35(6):611 Hunter D, Lange K (2004) A tutorial on mm algorithms. Am Stat 58(1):30­37 James AW, John LM (2002) Algebraic representations of beliefs and attitudes: partial order models for item responses. Sociol Methodol 29:113­146 Jolliffe I (2002) Principal component analysis: Springer series in statistics. Springer, Germany Le Cam L (1990) Maximum likelihood an introduction. ISI Rev 58(2):153­171 Likert R (1932) A technique for the measurement of attitudes. Arch Psychol 140:1­55 Malang (2006) NU chairman deplores suicide bombing attempt. http://www.nu.or.id/page/en/dinamic_detil/15/28282/News/NU_ chairman_deplores_suicide_bombing_attempt.html [Online accessed 22 Nov 2011] McIver J, Carmines E (1981) Unidimensional scaling, vol 24. Sage Publications Inc, USA McPhee RD, Corman S (1995) An activity-based theory of communication networks in organizations, applied to the case of a local church. Commun Monogr 62:1­20 Michael WB, Kogan J (2010) Text mining: applications and theory. Wiley, London Mohamed Osman MN (2010) Reviving the Caliphate in the Nusantara: Hizbut Tahrir Indonesia's mobilization strategy and its impact in Indonesia. Terror Political Violence 22(4):601­622.

123

Spatio-Temporal Signal Recovery from Political Tweets in Indonesia
Anisha Mazumder, Arun Das, Nyunsu Kim, Sedat Gokalp, Arunabha Sen, Hasan Davulcu School of Computing, Informatics and Decision Systems Engineering Arizona State University Tempe, Arizona - 85287 Email: {Anisha.Mazumder, adas22, nkim30, Sedat.Gokalp, asen, hdavulcu}@asu.edu
Abstract--Online social network community now provides an enormous volume of data for analyzing human sentiment about people, places, events and political activities. It is increasingly clear that analysis of such data can provide great insights on the social, political and cultural aspect of the participants of these networks. As part of the Minerva project, currently underway at Arizona State University, we have analyzed a large volume of Twitter data to understand radical political activity in the provinces of Indonesia. Based on analysis of radical/counter radical sentiments expressed in tweets by Twitter users, we create a Heat Map of Indonesia which visually demonstrates the degree of radical activities in various provinces of Indonesia. We create the Heat Map of Indonesia by computing (i) the Radicalization Index and (ii) the Location Index of each Twitter user from Indonesia, who has expressed some radical sentiment in her tweets. The conclusions derived from our analysis matches significantly with the analysis of Wahid Institute, a leading political think tank of Indonesia, thus validating our results. Index Terms--radical, tweet, Radicalization Index, Location Index, Heat Map

I. I NTRODUCTION The sheer popularity of online social media nowadays is reflected by the immense amount of data being fed every second by people from all over the world. It is becoming increasingly evident that analysis of this huge online dataset can provide great insights on the social, political and cultural aspect of the Twitter users and possibly the non-Twitter users as well. In [2], the authors have developed Socioscope, a tool for extracting signal from noisy social media data. Utilizing a Socioscope like mechanism, we have developed a tool for recovering spatio-temporal signals from tweets generated in Indonesia. Our interest in analyzing tweets from Indonesia developed in the context of the Minerva1 project, currently underway at Arizona State University. The goal of this project is to increase the understanding of movements within Muslim communities towards radicalism or counter radicalism. Based on the support and opposition of certain beliefs and practices of an individual (as expressed in her tweet), we can assign a Radicalization Index to that individual. In addition, from the self declared home location of a Twitter user and the locations of her tweets, we can compute a distribution of Location Index for that user. The map of Indonesia is divided up into a set of regions and the Location Index of a user provides the
1A

probability of the user to be in a specific region at a specific time. For this analysis a region corresponds to a province of Indonesia. Finally, from the Radicalization Index and Location Index of individuals, Heat Index of a region , which is a composite measure of the number of radical tweeters of that region and their `degree of radicalism', is computed. In our model we have a set of tweeters (or users), U = {U1 , U2 , . . . , Un }. Each user Ui , 1  i  n creates a set of tweets Ti = {Ti,1 , Ti,2 , . . . , Ti,ti }. The set of all tweets by n all users is denoted by T = i=1 Ti . The geographic area from where the tweets originate is divided into a set of regions R = {R1 , R2 , . . . , Rm }. In our study m is equal to thirty four, the number of provinces and special administrative regions of Indonesia. Each user Ui , 1  i  n has a home location HLi , 1  i  n associated with her, which may or may not be declared. Each tweet Ti,k , 1  i  n, 1  k  ti has a geo-location GLi,k , 1  i  n, 1  k  ti associated with it. However, GLi,k for some tweets Ti,k may not be known as the user Ui might turn her GPS off. Accordingly, we can divide the set of users in four different classes: (i) Class 1: user Ui whose home location is declared and geo-location of at least one tweet is known, (ii) Class 2: Ui whose home location is not declared and geo-location of at least one tweet is known, (iii) Class 3: Ui whose home location is declared and geolocation of none of the tweets are known, and (iv) Class 4 : Ui whose home location is not declared and geo-location of none of the tweets are known. From the input data set (U, T, R ), we compute, (i) Location Index, Li of each user Ui , 1  i  n, (ii) Radicalization Index, RDi of each user Ui , 1  i  n, and finally, combining Li and RDi , we compute (iii) Heat Index, Hj of each region Rj , 1  j  m. It may be noted that whereas RDi , 1  i  n is a scalar value, Li is a vector of size m, (Li,1 , . . . , Li,m ), where Li,j indicates the probability of user Ui being located in region Rj i.e. Li,j indicates the probability of the Actual home location of Ui being Rj . Finally, the Heat Index Hj of n region Rj , 1  j  m is computed as Hj = i=1 RDi × Li,j , j, 1  j  m. We thus provide a generic technique for generating time-varying political Heat Maps of a geographical region based on the Twitter data analysis. Throughout this paper we have used `region ' and `location' interchangeably

project sponsored by the U.S. Department of Defense

to mean an `Indonesian Province'. It is to be noted that for our calculations, we have considered all Indonesian provinces including special administrative regions such as Yogyakarta and special capital region such as Jakarta. II. R ELATED W ORK Computation of Heat Map of Indonesia requires the computation of the following: First, we compute the Radicalization Index of a user Ui by analyzing the content of her tweets. Second, Location Index of the user Ui is computed from her geo-location containing tweets (if any) and also from her home location declared as a part of her Twitter profile (if at all provided). It is to be noted that we do not consider users who have neither of these two sources of location information present. Identification of the location of users using Twitter data has been quite a focus of recent research. Inferring location from tweets have been pursued by [14], [15], [16]. Studies conducted in [4], [5], [6], [7] combine location information and text from social-network data history to infer various questions such as user preferences and provide recommendations. However, we do not rely on any `checking in' information for our computations and providing recommendations is not our goal. We do employ the notion of regions - the thirty four provinces of Indonesia are the regions of interest for our problem. Thus, `geo-coding' (the use of gazetteers) is applicable to our problem. However, just as in [3], we too argue that location estimates are multi-modal probability distributions, rather than particular points or regions. However, it may be noted that in contrast to [3], we are interested only in Indonesia and in Indonesian provinces - thus our estimate of the location of the user must be the probability of each Indonesian province as the Actual home location of the user under consideration, rather than the probability of the user being located in each and every point on the surface of the earth. This implies that our world comprises of Indonesia only and individual geo-co-ordinates are bunched into the corresponding province of Indonesia. As a result, we apply the combination of `geocoding' and the modification of the techniques in [3]. Thus, we use gazetteers for the Declared Home Location of the Twitter Users to map those to a specific province of Indonesia (This is explained in further details in Section VII ). This combined with the geo-coordinate information about the user (obtained from her tweets containing geo-location) gives us the probability distribution of the user across the thirty four provinces of Indonesia. We thus obtain a simple yet effective means of computing the geo-location of the user as compared to other more complex methodologies such as Topic Detection Techniques [20], [21], [22]. Human mobility is modeled as a stochastic process in [8]. Following the studies of [8], in [1], the authors study the manner in which the movements of human beings are related to time of the day, geography as well as social ties. They intend to predict the exact location of a person based on various factors which the authors have identified, including impact of

social network. Similar problems have been studied by [9], [10], [11]. However, in our problem, there is no notion of prediction of location of users involved. Besides, we consider categorical distribution. However, we do use the concept of mixture of distributions in the lines of [1]. Another line of research which focuses on location estimation by content-analysis of the tweets of a user has been studied by [12], [13]. They use the techniques of feature selection from tweets of users, following it up with training and classification. However, we do not apply content based analysis in this current work, but rely on the geo-location containing tweets of users in our dataset and also the user declared home location to obtain the location distribution of the users. In [17], the authors analyze tweets generated during the United Kingdom 2010 General Election to measure political sentiments as well. They have identified the specific features of the political parties and their ultimate goal is to infer the political affiliation of a user based on her tweets. We also study a similar problem, however our goal is not to identify the political affiliations of users, rather we compute the `degree of radicalism' of the user. Besides, our technique is completely different from theirs. Unlike them, we apply a very simple yet effective term-frequency analysis of tweets and leverage heavily on our team of domain experts. We validated our classification of users into radicals and counter radicals by classifying some well-known counter radical leaders of Indonesia (Our validation process is discussed in further details in Section IX). The work in [35] which is followed by [18] is very relevant to our technique of Radicalization Index assignment to users. These works too deal with the recovery of radical signals from the online posts of social media users and thereby identify individuals as potential `lone wolf terrorists'. They specifically focus on presenting a framework for combining entity matching techniques for detecting extremist behavior on discussion boards. These `lone wolf terrorists' might leave weak signals of radicalism through their comments or posts on discussion boards, signals made further weaker by the use of aliases. Identification and analysis of such weak signals of radicalism by the use of topic-filtered web harvesting as well as application of natural language processing techniques, thereby fusing aliases for identifying the person form the basis of the works of [35]. Their work is fundamentally different from ours because we deal specifically with the users' publicly available tweets only - this eliminates the availability of the vital background information such as characteristic ( `radical internet forum', `capability internet forum' ) annotation of particular discussion boards that is leveraged in [35] . However, we also have used the technique of application of keyword analysis and crawling of web-sites of well-known radical/ counter radical organizations of Indonesia as discussed in later sections. Furthermore, [35] and [18] do not deal with location profiling of users which is one of the two major goals of our work.

Fig. 1: The flow diagram of our Heat Map computation technique. The Web data mentioned here refers to the documents generated by crawling the web pages of radical and counter radical organizations of Indonesia.

III. M OTIVATION AND D ISTINGUISHING F EATURES OF
OUR WORK

The motivation for our work is to provide a visualization of the spatio-temporal distribution of the radical population of Indonesia by recovering political signals from Twitter data. A pictorial description of our methodology is provided in Figure 1. Similar retrieval of signals using Twitter data is the motivation of the work of [2]. In [2], they find the location distribution of tweets mentioning roadkills using human beings as sensors. Similarity of our work with [2] is that we too use human beings as sensors to the extent that we use tweets of people of Indonesia to infer radicalism Heat Indices of the provinces of Indonesia. However, our work is significantly different from theirs. First, unlike [2], we intend to find the distribution of (radical) individuals, so we should not factor in any `human population bias' i.e variation of densities of people across the different provinces of Indonesia. Second, our problem is much more complex because we not only need to know from which location have the radical tweets come in greater number, but also the `degree of radicalism' of the tweets - so we need to comprehend the sentiment of the tweets. The major difference here is that in our case, we need a finer grained distinction among the radical tweets specifying which tweets are more radical and which tweets are less. So, questions of interest for us are(Qs1) the `degree of radicalism' of tweet tw (Qs2) the originating location of tweet tw Thus, Heat Index of a region factors in both the count of the radical tweets from the region as well as the `degree of radicalism' of the tweets. However, there are certain challenges in answering these questions. As for Qs1, a tweet can at most be 140 characters long. This is indeed too little information to ascertain the `degree of radicalism' of tweets on individual basis. Thus, we go one level up the hierarchy and consider individual users instead of individual tweets and try to answer the two questions in the context of individual users. We collect all the tweets from individual users and assign the `degree of radicalism' to the user based on her tweets. Now, Qs2 would have been easy to answer with respect to individual tweets if all the tweets had geo-co-ordinate information because Twitter API2 provides geo-location information of tweets if the user
2 https://dev.twitter.com/docs/streaming-apis and ter.com/docs/platform-objects/tweets have been used

https://dev.twit-

had chosen to reveal her location at the time of tweeting. However, there are certain problems with this approach - first, the percentage of tweets containing geo-location information is very scarce (such tweets constitute less than 1% of our dataset). Second, when we consider individual users, it is unjustified to assume that all her tweets containing geo-location information will point to a single region, even if all her tweets contained geo-location information. Thus, the best estimate of the location of the user is the probability distribution of the user's location over the Indonesian provinces. We consider categorical distribution of the users into the thirty four provinces of Indonesia. The motivation behind employing categorical distribution instead of say Gaussian distribution over the entire landscape of Indonesia is that we want to obtain a political Heat Map of Indonesia with the granularity level of a province. Another possibility, that is feasible however not pursued by us in this current work, is dividing Indonesia in the form of grids with varied granularity. Finer granularity poses the problem of insufficient data from every grid, because, as mentioned previously, most tweets do not contain geo-location information. So, most grids will have no geo-location containing tweet. Our technique of Location Index computation is discussed in further details in the following section. Sentiment Analysis using social media data has been attempted by works such as [31] which tries to exploit patterns in online social media communication and also by [32] which uses background lexical information and refining of the same for specific domains by supervised learning techniques. However, we have computed Radicalization Indices using simpler text regression techniques similar to [33] and [34]. Our technique of Radicalization Index computation, which is verified to be quite accurate is discussed in further details in Section V. In summary, individual Twitter users are our chosen level of granularity - we obtain all the necessary information pertaining to each user. Next we characterize the user based on those information, not only on the radicalization scale but we also obtain a location distribution of the user over the regions of Indonesia. Hence, there is no prediction of the location of the user involved as in [1]. It is to be noted that we consider only the users classified as radical by our Radicalization Index computation method. Hence, our major contribution is the

development of a robust technique to obtain the political Heat Map of any geographic area. IV. L OCATION I NDEX C OMPUTATION As discussed earlier, each user Ui , 1  i  n has a home location HLi , 1  i  n associated with her, which may or may not be declared. Each tweet Ti,k , 1  i  n, 1  k  ti has a geo-location GLi,k , 1  i  n, 1  k  ti associated with it. However, GLi,k for some tweets Ti,k may not be known as the user Ui might turn her GPS off. Even when user Ui has a Declared Home Location DHLi , it may not be accurate. User Ui might intentionally or inadvertently misstate her location. Accordingly, we do not accept the DHLi at its face value as the Actual home location of Ui . Instead, we compute a matrix, which we term as the general Computed Home Location matrix gCHL, from the entire dataset barring the timespan (month in our case) for which the Heat Map is being generated. The created matrix gCHL is an m × m matrix where gCHLa,b , 1  a  m, 1  b  m, is the conditional probability of the Actual home location of a user being region Rb , when her Declared Home Location is region Ra , as learnt from the dataset. The gCHL matrix is computed using the following three steps provided in Algorithm 1. Thus, gCHLa,b is given by: gCHLa,b = X Y where, X = The number of tweets in T such that the author of the tweet has Declared Home Location as Ra and geo-location of the tweet is Rb Y = The number of tweets in T such that the author of the tweet has Declared Home Location as Ra Let the ath row of the gCHL matrix be denoted by gCHLa . Now Computed Home Location vector for the user Ui denoted by CHLi is assigned the value of gCHLa if the Declared Home Location of Ui is region Ra . It is to be noted that the gCHL matrix is general (and not user specific) and is computed using the entire Twitter data set comprising all users. From those tweets Ti,k , 1  k  ti of user Ui , that contain the geo-location information GLi,k (i.e., when the GPS is not turned off at the time of the tweet), we compute the Computed Geo Location vector CGLi of length m, where CGLi,j , 1  j  m, is the probability of the Actual home location of user Ui being region Rj , as learnt from the tweets of Ui . The CGLi,j is computed in the following way:
A CGLi,j = B

CHLi and CGLi , where CGLi is completely user-specific. However, CHLi is partially user-specific - it does depend on the user because CHLi is based on her Declared Home Location, but it also depends on the general distribution which depends on the entire population mass. It is evident that both CHLi and CGLi are categorical distribution over the thirty four Indonesian provinces. Now, we know that a mixture of discrete distributions over any finite number of categories is just another distribution over those categories. In order to combine CGLi and CHLi we obtain a convex combination of the two to obtain Li,j in the following way: Li,j = (1 - i )  CHLi,j + i  CGLi,j (1)

Now, the mixture weights i for the user Ui is learnt from the data itself and is calculated as i = |Ti |/|Ti |. Li,j essentially is given by Li,j = |Ti |  CHLi,j + |Ti |  CGLi,j (2)

which gives equation (1) when normalized by |Ti | = |Ti | + |Ti | i.e the total number of tweets posted by the user Ui where, · Ti = set of tweets produced by user Ui · Ti = subset of Ti and represents the set of tweets by Ui that contains geo-location information · Ti = subset of Ti and represents the set of tweets by Ui that do not contain geo-location information The motivation behind this definition of the mixture weight is that for the Ti tweets which contain geo-location information, we consider the user-specific location distribution information inferred from the particular user's geo-location containing tweets. However, for the tweets of Ti , we have no location information except for the general information that given a Declared Home Location for any user Uv in our dataset as Ra , the location distribution for Uv is CHLv = gCHLa . Thus, if the Declared Home Location of Ui is given to be Ra , we consider CHLi = gCHLa . Evidently, we depend on this semi-user-specific location distribution information for the tweets Ti of Ui . This simple formulation of Li,j also captures the fact that we rely more on CGLi than on CHLi when the number of tweets with geo-location information, generated by Ui is high, however if that count is low ( or even absent), instead of discarding the particular user's information, we obtain the location distribution of Ui from her Declared Home Location. We experimented by using only geo-location containing tweets and we saw that the results are far more accurate if we included users of Type 3 - This is intuitively correct because the geo-location containing tweets form less than 1% of the entire dataset. As noted earlier, the set of users can be divided into four different classes. We do not try to compute Li,j values for the users belonging to Class 4. For users belonging to the other three classes, we compute Li,j using equation (1). For the users belonging to Class 3, we obtain i to be zero, as we do not have any geo-location data from the tweets to compute i .

where, A = The number of tweets in Ti whose geo-location is Rj and B = The number of tweets in Ti whose geo-location is known We thus obtain two pieces of information about the Actual home location of the user Ui in the form of two distributions:

Algorithm 1 Counting Algorithm for computation of the general Computed Home Location gCHL
· · ·

Step 1: Initialize gCHLa,b = 0, 1  a  m, 1  b  m Step 2: For each tweet tw in T , increment gCHLa,b if Declared home location of the author of tw and the geo-location of tw are Ra and Rb respectively Step 3: Make each row gCHLa of gCHL matrix row stochastic, 1  a  m

V. R ADICALIZATION I NDEX C OMPUTATION We intend to assign a Radicalization Index RDi to a Twitter user Ui based on the content of her tweets. Each tweet can contain up to 140 characters. Thus the content of a single tweet does not provide adequate information regarding the user's ideology. We collect tweets from users over a period of time (in our case a month) and for each user Ui we create a document Di that contains all the tweets Ti of that user, during that period of time. As there exists a one-to-one correspondence between Ui and Di , by assigning a Radicalization Index to Di , we essentially assign a Radicalization Index RDi to Ui . Classical predictive model Multiple Linear regression [23], [24], [25] fits our application, since it is a dichotomous classification problem with multiple predictor variables, where the predictor variables are the terms of our "vocabulary". Classical classification methods such as Logistic Regression which has applications in a wide variety of domains can also be used for document classification [26]. Thus, Logistic Regression can also be applied for our problem. However, Linear Regression was selected instead of Logistic Regression because it out-performed the Logistic one through 10-fold cross validation. This well-known technique divides the given dataset into 10 segments and then uses 90% of the data ( i.e nine segments) as training data and 10% of the data ( i.e. one segment ) as the test data. Linear Regression showed around 98% of accuracy, but Logistic Regression showed 83-85% of accuracy. The implementation of our approach proceeds in the following way: First, we identify a set of Indonesian political organizations. Next, social scientists in our Minerva team, who are domain experts for Indonesia, hypothesize a classification to label each organization as radical or counter radical based on these organizations beliefs and practices. Using web crawling tools, we download a large number of documents from the web sites of these organizations. We use the term "vocabulary" to mean the set of all unique terms that appear in all documents from all organizations. All the documents of an organization are assigned the same Radicalization Index as the Radicalization Index assigned to the organization by the domain experts in our team. This set of documents together with their Radicalization Indices form the training dataset for our model. After that we use the model to assign a Radicalization Index to the document Di created from the tweets of user Ui . This Radicalization Index of document Di is taken to be the Radicalization Index of user Ui . A. Problem Formulation: We formulate the problem in a general sparse learning framework and solve the following optimization problem (3)

using the techniques from [27] . This is indeed a sparse learning problem because the vocabulary is very large compared to the number of words used in a document. 1  2 2 Ax - y 2 + x 2+ x 2 2 where A  Rs×p , y  Rs×1 , and x  Rp×1 min
x 1

(3)

In our application, we have · A is Document × Term matrix which is constructed as follows: The set of terms (t1 , . . . tp ) includes all the terms from all the documents by all the organizations, barring the stop words. The size of the vocabulary in this case is p. If data is collected by crawling web sites of different organization (O1 , . . . , Oq ) and documents (di,1 , . . . , d1,ri ) are collected from the web site of organization Oi , 1  i  q , the total number of rows of the matrix A is s = q i=1 ri , and it has the following structure. Document/T erm d1 d2 .... ds
· ·

t1 .... .... .... ....

t2 .... .... .... ....

.... .... .... .... ....

tp .... .... .... ....

·

Aij = term f requency of the j th term in the ith document such that Aij  0, 1  i  s, 1  j  p. yi  {+1, -1} is the class of each document Di , 1  i  s. As indicated earlier, the Radicalization Index of a document is the same the Radicalization Index of the organization that created that document. Thus, when an organization is labeled as radical (or counter radical) by the domain experts, all the documents pertaining to that organization is marked as +1 (or -1). Thus yi = +1 if Di , 1  i  s belongs to an organization marked as radical by the experts, or yi = -1 if Di , 1  i  s belongs to an organization marked as counter radical by the experts. xj is the weight for each term tj , 1  j  p. This is the parameter estimated by optimizing the objective function (3). The xj 's thus form the predictor variables of the model.

Let us further clarify the three terms involved in the convex optimization problem:
·

Ax - y 2 - this first term is related to the sum of the squared errors to fit a straight line to a set of data

1 2

2

points. The objective function (3) thus is the optimization problem of minimizing this sum of squared-errors. 2  · 2 x 2 - this term deals with the ridge regression, which is an extra level of shrinkage. We set  = 0 as we were mainly driven by sparsity. ·  x 1 - this term involving the L1 norm deals with the sparsity of the solution vector x. For different values of  we obtain a solution vector x which represents the weights associated with each term tj , 1  j  p ( the same terms which are considered in the A matrix). Some of these weights are positive, some negative (values can be very close to 0). The terms with positive (or negative) weights are the radical (or counter radical) words. The top (ones with weights having high magnitude) radical and counter radical words are presented to the experts for validation. We experiment with several  values resulting in x vectors of various sparsity until the list of top radical and counter radical words are approved by the field experts. We use the Matlab implementation of the SLEP package [28] that utilizes gradient descent approach to solve the optimization problem (3). This package can handle matrices of 20M entries within a couple of seconds on a machine with standard configuration. The input to the SLEP package are the values of A, , and y . The SLEP model outputs the weight vector x. B. Assignment of Radicalization Index: For each time period (in our case one month), each user Ui will be assigned Radicalization Index RDi based on their tweets within that period. This is done as follows: · As mentioned earlier, each tweet which can only contain a maximum of 140 characters is too insufficient for inferring the radicalism of the user. Hence, from the tweets of each user Ui we form a User Document Di which is the conglomeration of all her tweets over a period of one month. It is to be noted here that many users choose to tweet quite infrequently, hence even if we collect tweets for one month, a user might have tweeted only once or twice during the entire one month which defeats the purpose of collecting tweets for a month. Hence, we further apply the constraint that we consider only those users who have tweeted at least seven times in a month. The value of this threshold has been arrived at empirically after experimentation with various values of the threshold. · With the help of the model that has been fitted using the organization documents, we classify the User Documents. Let each User Document Di which is a termf requency row be denoted by the row vector tc of count of terms from our "vocabulary". · Each user Ui receives a `score' which we refer to as Radicalization Index RDi of user Ui . RDi is given by
p

where p is the size of our vocabulary. This provides us a time-series of RDi values for the users, which will make it possible to analyze the transition dynamics for each user. It is evident that a high positive RDi indicates that Ui is highly radical whereas a high negative RDi indicates that Ui is highly counter radical. VI. H EAT I NDEX C OMPUTATION Once we have obtained the Location Indices Li , 1  i  n and Radicalization Indices RDi , 1  i  n, for all the users Ui , 1  i  n , the Heat Index Hj of region Rj , 1  j  m is computed as Hj =
n i=1

RDi × Li,j , j, 1  j  m.

The Heat Index Hj for a region Rj indicates the degree of prevalence of radical ideologies among the people of Rj by taking into account both the number of radical tweeters living in Rj and also their `degree of radicalism'. In Table I we present a time-varying Heat Map of Indonesia by computing the map in three different time intervals of October 10 November 10, November 11 - December 10 and December 11 - January 10. We found a drastic change in the heat indices during the interval of November 10 ­ December 10. But we could not discern any particular event which could have triggered the same. VII. DATA C OLLECTION Since our model requires the computation of both the Radicalization Index RDi as well as the Location Index Li for each user Ui , we followed a two step data collection procedure described as follows: · For the purpose collecting the training data set for computing the Radicalization Index, we crawled the websites of 36 well-known Indonesian organizations which are classified as radical or counter radical by our field experts. A few of the organizations are mentioned in Table II. We crawled the websites of all these different organizations and collected a total of 78,135 documents which after pre-processing and filtering resulted into 49,250 documents. The reason for the reduction from the number of crawled documents to the number of useful documents is that many of the crawled documents did not have any relevant information (for example documents having only advertisements) and hence were discarded during pre-processing. Each of the documents on a average contained 280 words i.e on an average 2880 characters. All documents pertaining to an organization were labeled as radical or counter radical depending on the outlook professed by the organization itself. These were then used for fitting our Radicalization Index computation model. · For our study on recovery of political signals pertaining to trend of radical activities in Indonesia, we chose Twitter as the data collection platform as Indonesia features as one of the top five global market segments of Twitter by reach, and accounts for 19.0% to 20.8% of Twitter's

RDi = tc .x =
j =1

tcj xj

TABLE I: The table provides the top 5 province or special region names based on their computed Heat Index values (also mentioned alongwith) for October 10 - November 10, November 11 - December 10, December 11- January 10
Province Name Jakarta East Java West Java Yogyakarta Central Java Heat Index 5.48 2.95 2.68 1.74 1.68 Province Name Jakarta East Java Yogyakarta Central Java West Java Heat Index 16.16 12.33 4.53 3.7 3.39 Province Name Jakarta Yogyakarta West Java East Java Central Java Heat Index 4.71 1.82 1.25 1.20 0.69

TABLE II: Table showing some of the well-known radical and counter radical organizations of Indonesia
Radical Organizations AdianHusaini PKS Arrahmah AbuJibriel EraMuslim HizbutTahrir MillahIbrahim Counter radical Organizations DaarulUluum Interfidei IslamLiberal NU PPIM Paramadina LKIS

TABLE III: Keyword markers used for filtering Twitter Stream API
Keyword "penegakan syariah" "jihad majelis" "mati syahid" "ajaran islam" "pendidikan agama di sekolah" "asasi manusia" "demokrasi yang" "kebebasan beragama" "sekularisme" "di negeri negeri islam" Interpretation enforcement of Sharia jihad assemblies martyrdom the teaching of Islam religious education in schools human rights democracy religious freedom secularism Islamic state in the country

total reach by country (Dec 2010)3 . No other publicly available portal offers access to opinions posted online by the Indonesian populace on a similar scale as does Twitter. For gathering tweets, we use Twitter's Stream API to access Twitter's global stream of publicly available tweet data. Since our goal is to recover "political signals", we setup a keyword filter on the Stream API to gather tweets that relate to radical and counter radical ideologies. The keywords used for this filtration have been identified by the social scientists in our Minerva project team and are considered to be significant markers of radical and counter radical ideologies in the Indonesian context. The keyword list includes radical markers and a few such markers are listed in Table III. We collected tweet data for a three-month interval and gathered a total of 12,152,874 tweets from October 10, 2012 to January 10, 2013 ( Figure 2) that matched the keyword filtration criteria ( Table III ). We used the three months of data to calculate the Radicalization Indices of the users. In this research, we are interested in the probability distribution
3 http://www.billhartzer.com/pages/comscore-twitter-latin-america-usage/

Location Index Li of user Ui over the thirty four provinces of Indonesia, thus we focus only on users from Indonesia. The keywords used are in Indonesian language and narrows down the tweets we obtained from the Twitter API. Thus, the geocode in majority of cases indicated a location in Indonesia. However, not all geo-codes are from Indonesia. We ignore those tweets in the current work. Thus, out of these 12 million tweets, 110,063 tweets contained geo-locations that mapped to regions within Indonesia. To apply this reverse geo-coding, we used the OpenStreetMap API4 . A user repository was constructed by including only those users whose Declared Home Locations matched with an identifiable Indonesian city or province. Now the user declared home location which the user mentions as a part of her profile could consist of any text according to the user's whim. We found texts such as "Dark side of the moon" or "somewhere in this big world" or "Here" or "infront of my laptop" and hence, there is a need for pre-processing of the text. Also, the users provided location information to varied degrees of granularity ranging from continents to towns, however we are interested in the fixed granularity level of Indonesian provinces and the special regions such as Jakarta and Yogyakarta. Hence we manually created a database of towns and cities of all of the Indonesian provinces. Each of the provinces were annotated with 42 cities/ towns on an average with Papua being the highest which was annotated with 70 cities/towns. Using this database we then assigned a legitimate Declared Home Location to as many users as possible. The final user repository consisted of 959,911 unique users.

Fig. 2: Figure showing the number of tweets collected over our observation period

http://www.comscoredatamine.com/2011/02/the-netherlands-leads-globalmarkets-in-twitter-reach/

4 The relevant information about the API could be found at http://wiki.openstreetmap.org/wiki/Nominatim

VIII. E XPERIMENTAL R ESULTS We created Heat Maps of Indonesia on a monthly basis. We computed the RDi of each user U i for each month from October 10 to January 10, as long as Ui sent at least 7 tweets in that month Again, for each user Ui we computed the Location Index Li by considering all her tweets over the period of the month.. For that we computed the general Computed Home Location gCHL matrix. The(i, j )th entry gives the probability of a user with a Declared Home Location of Ri being located in Rj , 1  i  m, 1  j  m.
·

IX. VALIDATION For the purpose of validation of the Radicalization Index, we computed the Radicalization Indices of some well-known counter radical leaders of Indonesia for the months that they had tweeted for more than 7 times which we consider as our threshold. Our classifier gave perfect accuracy. By accuracy of the classification we mean the percentage of time the leaders who are thus known to be counter radical were classified as counter radical by our classifier. We did not validate the Location Index computation technique because of the lack of the ground truth of the Actual home location of users. However, our results of Heat Index are validated by the findings of the Indonesia-based Wahid Institute5 (named after Abdurrahman Wahid, an Indonesian Muslim religious and political leader who served as the President of Indonesia from 1999 to 2001). Wahid Institute promotes a moderate version of Islam through dialogue events, publications, and public advocacies. The institute also releases an annual religious freedom report on religious life in Indonesia. According to the Wahid Institute's Annual Report of 20126 , the top four provinces of Indonesia where radical activities are most observable are West Java, Aceh, East Java, and Central Java. It may be noted here, that three out of the four most radical provinces identified by the Wahid Institute, also appear at the very top of our list. Also, our field experts have confirmed Jakarta to be a center of radical activities. It may be mentioned here that field studies7 in January 2012 by Setara Institute8 , a well-known NGO based in Indonesia, showed that the strong radicalism of the young muslim population in Yogyakarta and Central Java are making them hot targets to be recruited as Jihadists. In May 2012, a mob attack by Indonesian Mujahidin Council on a book launch of a well-known Canadian author, an advocate of LGBT, took place in Yogyakarta. In September 2012, there has been arrests of potential terrorists from Yogyakarta9 . Because, Wahid Institute has mentioned about Indonesian provinces only, it might be expected that Jakarta and Yogyakarta, being special administrative regions, are missing from their list however, we do not have access to their full report. The high radicalism of the Java provinces are also corroborated by reports of the Setara Institute. The only radically active province that shows up in the Wahid Institute report but does not appear at the top of our list is Aceh, located at the north west corner of Indonesia. It is worth mentioning here that Aceh was completely devastated by the 2004 Indian Ocean Tsunami and is still recovering from its effects. Aceh is also one of the least economically developed provinces of Indonesia. We believe that due to the lack of economic advancement in Aceh, the level of Internet penetration in Aceh is fairly small and not many people from Aceh are active tweeters. This may explain
5 http://berkleycenter.georgetown.edu/resources/organizations/wahidinstitute 6 Released on December 28, 2012 7 http://www.setara-institute.org/en/content/study-shows-how-youngradical-indonesian-muslims-become-terrorists 8 http://www.setara-institute.org/ 9 http://www.washingtontimes.com/multimedia/image/indonesia-terrorjpg/

gCHL matrix : The gCHL matrix provides interesting insights on the Indonesian population. We computed the gCHL - matrix on all possible doublets among the three months of observation period. i.e for each month for calculating the Location Indices Li of users, we have generated the gCHL matrix using the other two months of data. Thus, in each case, we had training data of two months and test data of one month. Among the three gCHL - matrices generated, we saw that the Declared Home Location of users give us a good insight on the Actual home location of the user. Thus, instead of merely depending on the geo-co-ordinates of users, we should consider the home location from the user's profile and home location declarations are much more abundant than geo-location containing tweets. However, depending solely on Declared Home Locations can be deceptive. We also observed that people with Declared Home Locations in various different provinces from all around Indonesia such as Bangka Belitung, Banten, Maluku, West Nusa Tenggara, East Nusa Tenggara and Papua have a very strong tendency to have high probability of having Actual home location in Jakarta (as observed from our results over three months). This is very intuitive because Jakarta being the Capital Region must have attracted people from different parts of Indonesia for prospective settlement. We further made an interesting observation that people with Declared Home Location of East Kalimantan have considerable geo-location containing tweets from Central Kalimantan.

The Heat Indices values for the thirty four Indonesian provinces are computed using our approach for three months of our observation period - namely October 10 - November 10, November 11 - December 10, December 11 -January 10. Among all Indonesian provinces the top five provinces and special regions along with their Heat Index values are presented in Table I for the three months. Color maps of Indonesia with Heat Indices is shown in Figure 3, where darker colors indicate a higher level of radical tweeting, and lighter colors indicate a lower level of radical tweeting. It may be seen from Figure 3 that the area around Jakarta and the Java provinces are highly active in radical tweet creation. According to our Twitter data analysis, the provinces Jakarta, East Java, Yogyakarta and Central Java, along with West Java are the top provinces that generate a high level of radical activities.

(a) Heat Map for October 10 to November 10

(b) Heat Map for November 11 to December 10

(c) Heat Map for December 11 to January 10

(d) Radicalism Scale

Index

Fig. 3: Heat Maps of Indonesia

the reason for Aceh not showing up among our list of top radically active provinces. X. C ONCLUSION We have developed a generic robust technique for recovering signals pertaining to a geographical area such as a country using Twitter Data. We have applied our technique to our Indonesian dataset and have observed high accuracy. The goal of our work is the generation of a political Heat Map of Indonesia which will clearly indicate the provinces of Indonesia where radical narrative is prominent. The granularity of the Radicalization Index Assignment used is a Twitter user, while the granularity of regions used is an Indonesian Province. Thus, we have analyzed tweets made by a user Ui in a month to assign a Radicalization Index RDi to Ui - Thus RDi indicates how radical is Ui in her political outlook . Also, by mining the tweets in our database we assign a Location Index Li to Ui . For computation of the Location Index, the sources of location information used are not only the geolocation tagging as provided by Twitter API for the tweets which contain that information (such tweets constitute less than 1% of our dataset), but we also use the user declared home location information from her profile (after considerable amount of pre-processing and cleansing). We have combined these two sources of information for inferring the probability distribution of the location of the users among the provinces of Indonesia. Thus, by considering the RDi 's for all the users over a period of one month, in conjunction with the Location Indices Li 's we generate the political Heat Maps of the likes

of Figure 3. We have got a time series of Heat Maps, which can help us in mapping trends by regions in radical discourse. Such Heat Maps can prove to be very useful in studying the spatio-temporal dynamics of the people of Indonesia so far as their political outlook is concerned R EFERENCES
[1] Cho, E., Myers, S. A., & Leskovec, J. (2011, August). Friendship and mobility: user movement in location-based social networks. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1082-1090). ACM. [2] Xu, J. M., Bhargava, A., Nowak, R., & Zhu, X. (2012). Socioscope: spatio-temporal signal recovery from social media. In Machine Learning and Knowledge Discovery in Databases (pp. 644-659). Springer Berlin Heidelberg. [3] Priedhorsky, R., Culotta, A., & Del Valle, S. Y. (2013). Inferring the Origin Locations of Tweets with Quantitative Confidence. arXiv preprint arXiv:1305.3932. [4] Yang, D., Zhang, D., Yu, Z., & Wang, Z. (2013, May). A sentimentenhanced personalized location recommendation system. In Proceedings of the 24th ACM Conference on Hypertext and Social Media (pp. 119-128). ACM. [5] Rahimi, S. M., & Wang, X. (2013). Location Recommendation Based on Periodicity of Human Activities and Location Categories. In Advances in Knowledge Discovery and Data Mining (pp. 377-389). Springer Berlin Heidelberg. [6] Li, Y., Steiner, M., Wang, L., Zhang, Z. L., & Bao, J. Exploring Venue Popularity in Foursquare. [7] Li, Y., Steiner, M., Wang, L., Zhang, Z. L., & Bao, J. (2012, December). Dissecting foursquare venue popularity via random region sampling. In Proceedings of the 2012 ACM conference on CoNEXT student workshop (pp. 21-22). ACM. [8] Gonzalez, M. C., Hidalgo, C. A., & Barabasi, A. L. (2008). Understanding individual human mobility patterns. Nature, 453(7196), 779-782.

[9] Sadilek, A., Kautz, H., & Bigham, J. P. (2012, February). Finding your friends and following them to where you are. In Proceedings of the fifth ACM international conference on Web search and data mining (pp. 723-732). ACM. [10] Noulas, A., Scellato, S., Lambiotte, R., Pontil, M., & Mascolo, C. (2012). A tale of many cities: universal patterns in human urban mobility. PloS one, 7(5), e37027. [11] Lu, X., Bengtsson, L., & Holme, P. (2012). Predictability of population displacement after the 2010 Haiti earthquake. Proceedings of the National Academy of Sciences, 109(29), 11576-11581. [12] Chang, H. W., Lee, D., Eltaher, M., & Lee, J. (2012, August). @ Phillies Tweeting from Philly? Predicting Twitter User Locations with Spatial Word Usage. In Advances in Social Networks Analysis and Mining (ASONAM), 2012 IEEE/ACM International Conference on (pp. 111-118). IEEE. [13] Mahmud, J., Nichols, J., & Drews, C. (2012). Where is this tweet from? inferring home locations of twitter users. Proc AAAI ICWSM, 12. [14] Eisenstein, J., O'Connor, B., Smith, N. A., & Xing, E. P. (2010, October). A latent variable model for geographic lexical variation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (pp. 1277-1287). Association for Computational Linguistics. [15] Cheng, Z., Caverlee, J., & Lee, K. (2010, October). You are where you tweet: a content-based approach to geo-locating twitter users. In Proceedings of the 19th ACM international conference on Information and knowledge management (pp. 759-768). ACM. [16] Hecht, B., Hong, L., Suh, B., & Chi, E. H. (2011, May). Tweets from Justin Bieber's heart: the dynamics of the location field in user profiles. In Proceedings of the 2011 annual conference on Human factors in computing systems (pp. 237-246). ACM. [17] Boutet, A., Kim, H., & Yoneki, E. (2012, August). What's in Twitter: I Know What Parties are Popular and Who You are Supporting Now!. In Advances in Social Networks Analysis and Mining (ASONAM), 2012 IEEE/ACM International Conference on (pp. 132-139). IEEE. [18] Dahlin, J., Johansson, F., Kaati, L., Martenson, C., & Svenson, P. (2012, August). Combining Entity Matching Techniques for Detecting Extremist Behavior on Discussion Boards. In Advances in Social Networks Analysis and Mining (ASONAM), 2012 IEEE/ACM International Conference on (pp. 850-857). IEEE. [19] Eisenstein, J., Ahmed, A., & Xing, E. P. (2011, June). Sparse additive generative models of text. In International Conference on Machine Learning (ICML). [20] Hong, L., Ahmed, A., Gurumurthy, S., Smola, A. J., & Tsioutsiouliklis, K. (2012, April). Discovering geographical topics in the twitter stream. In Proceedings of the 21st international conference on World Wide Web (pp. 769-778). ACM. [21] O'Connor, B., Eisenstein, J., Xing, E. P., & Smith, N. A. (2010). Discovering demographic language variation. In Workshop on Machine Learning for Social Computing at NIPS. [22] Yin, Z., Cao, L., Han, J., Zhai, C., & Huang, T. (2011, March). Geographical topic discovery and comparison. In Proceedings of the 20th international conference on World wide web (pp. 247-256). ACM. [23] Zhang, T. (2009). Some sharp performance bounds for least squares regression with L1 regularization. The Annals of Statistics, 37(5A), 2109-2144. [24] S.Kim, K.Koh, M.Lustig, S.Boyd and D. Gorinevsky, An Interior-Point Method for Large-Scale l1 - Regularized Least Squares Journal of seleccted topics in Signal Processing, Vol. 1, No. 4, pages 606 - 617, Dec 2007 [25] Kolter, J. Z., & Ng, A. Y. (2009, June). Regularization and feature selection in least-squares temporal difference learning. In Proceedings of the 26th Annual International Conference on Machine Learning (pp. 521-528). ACM. [26] Brzezinski, J. R., & Knafl, G. J. (1999). Logistic regression modeling for context-based classification. In Database and Expert Systems Applications, 1999. Proceedings. Tenth International Workshop on (pp. 755-759). IEEE. [27] Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent. Journal of statistical software, 33(1), 1. [28] J. Liu, S. Ji and Jieping Ye. SLEP: Sparse Learning with Efficient Projections, Arizona State University (2009) [29] Lee, S. I., Lee, H., Abbeel, P., & Ng, A. Y. (2006, July). Efficient L~ 1 Regularized Logistic Regression. In Proceedings of the National

[30] [31]

[32]

[33]

[34]

[35]

Conference on Artificial Intelligence (Vol. 21, No. 1, p. 401). Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999. T. P. Minka, A comparison of numerical optimizaers for logistic regression, Technical report, 2007 Thelwall, M., Buckley, K., Paltoglou, G., Skowron, M., Garcia, D., Gobron, S., ... & Holyst, J. A. (2013). Damping Sentiment Analysis in Online Communication: Discussions, Monologs and Dialogs. In Computational Linguistics and Intelligent Text Processing (pp. 1-12). Springer Berlin Heidelberg. Melville, P., Gryc, W., & Lawrence, R. D. (2009, June). Sentiment analysis of blogs by combining lexical knowledge with text classification. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1275-1284). ACM. Joshi, M., Das, D., Gimpel, K., & Smith, N. A. (2010, June). Movie reviews and revenues: An experiment in text regression. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (pp. 293-296). Association for Computational Linguistics. Kogan, S., Levin, D., Routledge, B. R., Sagi, J. S., & Smith, N. A. (2009, May). Predicting risk from financial reports with regression. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (pp. 272-280). Association for Computational Linguistics Brynielsson, J., Horndahl, A., Johansson, F., Kaati, L., Martenson, C., & Svenson, P. (2012, August). Analysis of weak signals for detecting lone wolf terrorists. In Intelligence and Security Informatics Conference (EISIC), 2012 European (pp. 197-204). IEEE.

Spatio-Temporal Signal Recovery from Political
Tweets in Indonesia
Anisha Mazumder, Arun Das, Nyunsu Kim, Sedat Gokalp, Arunabha Sen, Hasan Davulcu
School of Computing, Informatics and Decision Systems Engineering
Arizona State University
Tempe, Arizona - 85287
Email: {Anisha.Mazumder, adas22, nkim30, Sedat.Gokalp, asen, hdavulcu}@asu.edu
AbstractâOnline social network community now provides an
enormous volume of data for analyzing human sentiment about
people, places, events and political activities. It is increasingly
clear that analysis of such data can provide great insights on the
social, political and cultural aspect of the participants of these
networks. As part of the Minerva project, currently underway
at Arizona State University, we have analyzed a large volume
of Twitter data to understand radical political activity in the
provinces of Indonesia. Based on analysis of radical/counter
radical sentiments expressed in tweets by Twitter users, we
create a Heat Map of Indonesia which visually demonstrates the
degree of radical activities in various provinces of Indonesia.
We create the Heat Map of Indonesia by computing (i) the
Radicalization Index and (ii) the Location Index of each Twitter
user from Indonesia, who has expressed some radical sentiment
in her tweets. The conclusions derived from our analysis matches
significantly with the analysis of Wahid Institute, a leading
political think tank of Indonesia, thus validating our results.
Index Termsâradical, tweet, Radicalization Index, Location
Index, Heat Map

I. I NTRODUCTION
The sheer popularity of online social media nowadays is
reflected by the immense amount of data being fed every
second by people from all over the world. It is becoming
increasingly evident that analysis of this huge online dataset
can provide great insights on the social, political and cultural
aspect of the Twitter users and possibly the non-Twitter users
as well. In [2], the authors have developed Socioscope, a tool
for extracting signal from noisy social media data. Utilizing
a Socioscope like mechanism, we have developed a tool for
recovering spatio-temporal signals from tweets generated in
Indonesia. Our interest in analyzing tweets from Indonesia
developed in the context of the Minerva1 project, currently
underway at Arizona State University. The goal of this project
is to increase the understanding of movements within Muslim
communities towards radicalism or counter radicalism. Based
on the support and opposition of certain beliefs and practices
of an individual (as expressed in her tweet), we can assign a
Radicalization Index to that individual. In addition, from the
self declared home location of a Twitter user and the locations
of her tweets, we can compute a distribution of Location
Index for that user. The map of Indonesia is divided up into
a set of regions and the Location Index of a user provides the
1A

project sponsored by the U.S. Department of Defense

probability of the user to be in a specific region at a specific
time. For this analysis a region corresponds to a province of
Indonesia. Finally, from the Radicalization Index and Location
Index of individuals, Heat Index of a region , which is a
composite measure of the number of radical tweeters of that
region and their âdegree of radicalismâ, is computed.
In our model we have a set of tweeters (or users), U =
{U1 , U2 , . . . , Un }. Each user Ui , 1 â¤ i â¤ n creates a set of
tweets Ti = {Ti,1 , Ti,2 , . . . , Ti,t
Sin}. The set of all tweets by
all users is denoted by T = i=1 Ti . The geographic area
from where the tweets originate is divided into a set of regions
R = {R1 , R2 , . . . , Rm }. In our study m is equal to thirty four,
the number of provinces and special administrative regions of
Indonesia. Each user Ui , 1 â¤ i â¤ n has a home location
HLi , 1 â¤ i â¤ n associated with her, which may or may not
be declared. Each tweet Ti,k , 1 â¤ i â¤ n, 1 â¤ k â¤ ti has a
geo-location GLi,k , 1 â¤ i â¤ n, 1 â¤ k â¤ ti associated with
it. However, GLi,k for some tweets Ti,k may not be known
as the user Ui might turn her GPS off. Accordingly, we can
divide the set of users in four different classes:
(i) Class 1: user Ui whose home location is declared and
geo-location of at least one tweet is known,
(ii) Class 2: Ui whose home location is not declared and
geo-location of at least one tweet is known,
(iii) Class 3: Ui whose home location is declared and geolocation of none of the tweets are known, and
(iv) Class 4 : Ui whose home location is not declared and
geo-location of none of the tweets are known.
From the input data set (U, T, R ), we compute, (i) Location
Index, Li of each user Ui , 1 â¤ i â¤ n, (ii) Radicalization Index,
RDi of each user Ui , 1 â¤ i â¤ n, and finally, combining Li
and RDi , we compute (iii) Heat Index, Hj of each region
Rj , 1 â¤ j â¤ m. It may be noted that whereas RDi , 1 â¤ i â¤ n
is a scalar value, Li is a vector of size m, (Li,1 , . . . , Li,m ),
where Li,j indicates the probability of user Ui being located
in region Rj i.e. Li,j indicates the probability of the Actual
home location of Ui being Rj . Finally, the HeatPIndex Hj of
n
region Rj , 1 â¤ j â¤ m is computed as Hj = i=1 RDi Ã
Li,j , âj, 1 â¤ j â¤ m. We thus provide a generic technique for
generating time-varying political Heat Maps of a geographical
region based on the Twitter data analysis. Throughout this
paper we have used âregion â and âlocationâ interchangeably

to mean an âIndonesian Provinceâ. It is to be noted that for
our calculations, we have considered all Indonesian provinces
including special administrative regions such as Yogyakarta
and special capital region such as Jakarta.
II. R ELATED W ORK
Computation of Heat Map of Indonesia requires the computation of the following: First, we compute the Radicalization
Index of a user Ui by analyzing the content of her tweets.
Second, Location Index of the user Ui is computed from her
geo-location containing tweets (if any) and also from her home
location declared as a part of her Twitter profile (if at all
provided). It is to be noted that we do not consider users
who have neither of these two sources of location information
present.
Identification of the location of users using Twitter data
has been quite a focus of recent research. Inferring location
from tweets have been pursued by [14], [15], [16]. Studies
conducted in [4], [5], [6], [7] combine location information
and text from social-network data history to infer various questions such as user preferences and provide recommendations.
However, we do not rely on any âchecking inâ information for
our computations and providing recommendations is not our
goal.
We do employ the notion of regions - the thirty four
provinces of Indonesia are the regions of interest for our problem. Thus, âgeo-codingâ (the use of gazetteers) is applicable to
our problem. However, just as in [3], we too argue that location
estimates are multi-modal probability distributions, rather than
particular points or regions. However, it may be noted that in
contrast to [3], we are interested only in Indonesia and in
Indonesian provinces - thus our estimate of the location of the
user must be the probability of each Indonesian province as the
Actual home location of the user under consideration, rather
than the probability of the user being located in each and every
point on the surface of the earth. This implies that our world
comprises of Indonesia only and individual geo-co-ordinates
are bunched into the corresponding province of Indonesia. As
a result, we apply the combination of âgeocodingâ and the
modification of the techniques in [3]. Thus, we use gazetteers
for the Declared Home Location of the Twitter Users to map
those to a specific province of Indonesia (This is explained in
further details in Section VII ). This combined with the geo-coordinate information about the user (obtained from her tweets
containing geo-location) gives us the probability distribution
of the user across the thirty four provinces of Indonesia. We
thus obtain a simple yet effective means of computing the
geo-location of the user as compared to other more complex
methodologies such as Topic Detection Techniques [20], [21],
[22].
Human mobility is modeled as a stochastic process in [8].
Following the studies of [8], in [1], the authors study the
manner in which the movements of human beings are related
to time of the day, geography as well as social ties. They intend
to predict the exact location of a person based on various
factors which the authors have identified, including impact of

social network. Similar problems have been studied by [9],
[10], [11]. However, in our problem, there is no notion of
prediction of location of users involved. Besides, we consider
categorical distribution. However, we do use the concept of
mixture of distributions in the lines of [1].
Another line of research which focuses on location estimation by content-analysis of the tweets of a user has been
studied by [12], [13]. They use the techniques of feature
selection from tweets of users, following it up with training
and classification. However, we do not apply content based
analysis in this current work, but rely on the geo-location
containing tweets of users in our dataset and also the user
declared home location to obtain the location distribution of
the users.
In [17], the authors analyze tweets generated during the
United Kingdom 2010 General Election to measure political
sentiments as well. They have identified the specific features
of the political parties and their ultimate goal is to infer
the political affiliation of a user based on her tweets. We
also study a similar problem, however our goal is not to
identify the political affiliations of users, rather we compute
the âdegree of radicalismâ of the user. Besides, our technique
is completely different from theirs. Unlike them, we apply a
very simple yet effective term-frequency analysis of tweets and
leverage heavily on our team of domain experts. We validated
our classification of users into radicals and counter radicals
by classifying some well-known counter radical leaders of
Indonesia (Our validation process is discussed in further details
in Section IX).
The work in [35] which is followed by [18] is very relevant
to our technique of Radicalization Index assignment to users.
These works too deal with the recovery of radical signals
from the online posts of social media users and thereby
identify individuals as potential âlone wolf terroristsâ. They
specifically focus on presenting a framework for combining
entity matching techniques for detecting extremist behavior
on discussion boards. These âlone wolf terroristsâ might leave
weak signals of radicalism through their comments or posts
on discussion boards, signals made further weaker by the use
of aliases. Identification and analysis of such weak signals
of radicalism by the use of topic-filtered web harvesting as
well as application of natural language processing techniques,
thereby fusing aliases for identifying the person form the basis
of the works of [35]. Their work is fundamentally different
from ours because we deal specifically with the usersâ publicly
available tweets only - this eliminates the availability of the
vital background information such as characteristic ( âradical
internet forumâ, âcapability internet forumâ ) annotation of particular discussion boards that is leveraged in [35] . However,
we also have used the technique of application of keyword
analysis and crawling of web-sites of well-known radical/
counter radical organizations of Indonesia as discussed in later
sections. Furthermore, [35] and [18] do not deal with location
profiling of users which is one of the two major goals of our
work.

Fig. 1: The flow diagram of our Heat Map computation technique. The Web data mentioned here refers to the documents
generated by crawling the web pages of radical and counter radical organizations of Indonesia.

III. M OTIVATION AND D ISTINGUISHING F EATURES OF
OUR WORK

The motivation for our work is to provide a visualization
of the spatio-temporal distribution of the radical population of
Indonesia by recovering political signals from Twitter data.
A pictorial description of our methodology is provided in
Figure 1. Similar retrieval of signals using Twitter data is the
motivation of the work of [2]. In [2], they find the location
distribution of tweets mentioning roadkills using human beings
as sensors. Similarity of our work with [2] is that we too use
human beings as sensors to the extent that we use tweets of
people of Indonesia to infer radicalism Heat Indices of the
provinces of Indonesia. However, our work is significantly
different from theirs. First, unlike [2], we intend to find the
distribution of (radical) individuals, so we should not factor
in any âhuman population biasâ i.e variation of densities of
people across the different provinces of Indonesia. Second,
our problem is much more complex because we not only
need to know from which location have the radical tweets
come in greater number, but also the âdegree of radicalismâ of
the tweets - so we need to comprehend the sentiment of the
tweets. The major difference here is that in our case, we need
a finer grained distinction among the radical tweets specifying
which tweets are more radical and which tweets are less. So,
questions of interest for us are(Qs1) the âdegree of radicalismâ of tweet tw
(Qs2) the originating location of tweet tw
Thus, Heat Index of a region factors in both the count of
the radical tweets from the region as well as the âdegree of
radicalismâ of the tweets. However, there are certain challenges
in answering these questions. As for Qs1, a tweet can at most
be 140 characters long. This is indeed too little information
to ascertain the âdegree of radicalismâ of tweets on individual
basis. Thus, we go one level up the hierarchy and consider
individual users instead of individual tweets and try to answer
the two questions in the context of individual users. We collect
all the tweets from individual users and assign the âdegree of
radicalismâ to the user based on her tweets. Now, Qs2 would
have been easy to answer with respect to individual tweets if
all the tweets had geo-co-ordinate information because Twitter
API2 provides geo-location information of tweets if the user
2 https://dev.twitter.com/docs/streaming-apis
and
ter.com/docs/platform-objects/tweets have been used

https://dev.twit-

had chosen to reveal her location at the time of tweeting.
However, there are certain problems with this approach - first,
the percentage of tweets containing geo-location information
is very scarce (such tweets constitute less than 1% of our
dataset). Second, when we consider individual users, it is unjustified to assume that all her tweets containing geo-location
information will point to a single region, even if all her tweets
contained geo-location information. Thus, the best estimate of
the location of the user is the probability distribution of the
userâs location over the Indonesian provinces.
We consider categorical distribution of the users into the
thirty four provinces of Indonesia. The motivation behind
employing categorical distribution instead of say Gaussian
distribution over the entire landscape of Indonesia is that we
want to obtain a political Heat Map of Indonesia with the
granularity level of a province. Another possibility, that is
feasible however not pursued by us in this current work, is
dividing Indonesia in the form of grids with varied granularity.
Finer granularity poses the problem of insufficient data from
every grid, because, as mentioned previously, most tweets
do not contain geo-location information. So, most grids will
have no geo-location containing tweet. Our technique of
Location Index computation is discussed in further details in
the following section.
Sentiment Analysis using social media data has been attempted by works such as [31] which tries to exploit patterns in online social media communication and also by [32]
which uses background lexical information and refining of the
same for specific domains by supervised learning techniques.
However, we have computed Radicalization Indices using
simpler text regression techniques similar to [33] and [34].
Our technique of Radicalization Index computation, which is
verified to be quite accurate is discussed in further details in
Section V.
In summary, individual Twitter users are our chosen level of
granularity - we obtain all the necessary information pertaining
to each user. Next we characterize the user based on those
information, not only on the radicalization scale but we also
obtain a location distribution of the user over the regions of
Indonesia. Hence, there is no prediction of the location of
the user involved as in [1]. It is to be noted that we consider
only the users classified as radical by our Radicalization Index
computation method. Hence, our major contribution is the

development of a robust technique to obtain the political Heat
Map of any geographic area.
IV. L OCATION I NDEX C OMPUTATION
As discussed earlier, each user Ui , 1 â¤ i â¤ n has a home
location HLi , 1 â¤ i â¤ n associated with her, which may or
may not be declared. Each tweet Ti,k , 1 â¤ i â¤ n, 1 â¤ k â¤ ti
has a geo-location GLi,k , 1 â¤ i â¤ n, 1 â¤ k â¤ ti associated
with it. However, GLi,k for some tweets Ti,k may not be
known as the user Ui might turn her GPS off. Even when
user Ui has a Declared Home Location DHLi , it may not be
accurate. User Ui might intentionally or inadvertently misstate
her location. Accordingly, we do not accept the DHLi at its
face value as the Actual home location of Ui . Instead, we
compute a matrix, which we term as the general Computed
Home Location matrix gCHL, from the entire dataset barring
the timespan (month in our case) for which the Heat Map
is being generated. The created matrix gCHL is an m Ã m
matrix where gCHLa,b , 1 â¤ a â¤ m, 1 â¤ b â¤ m, is the
conditional probability of the Actual home location of a user
being region Rb , when her Declared Home Location is region
Ra , as learnt from the dataset. The gCHL matrix is computed
using the following three steps provided in Algorithm 1. Thus,
gCHLa,b is given by:
gCHLa,b = X
Y
where,
X = The number of tweets in T such that the author of the
tweet has Declared Home Location as Ra and geo-location of
the tweet is Rb
Y = The number of tweets in T such that the author of the
tweet has Declared Home Location as Ra
Let the ath row of the gCHL matrix be denoted by gCHLa .
Now Computed Home Location vector for the user Ui denoted
by CHLi is assigned the value of gCHLa if the Declared
Home Location of Ui is region Ra . It is to be noted that
the gCHL matrix is general (and not user specific) and is
computed using the entire Twitter data set comprising all users.
From those tweets Ti,k , 1 â¤ k â¤ ti of user Ui , that contain
the geo-location information GLi,k (i.e., when the GPS is not
turned off at the time of the tweet), we compute the Computed
Geo Location vector CGLi of length m, where CGLi,j , 1 â¤
j â¤ m, is the probability of the Actual home location of
user Ui being region Rj , as learnt from the tweets of Ui . The
CGLi,j is computed in the following way:
A
CGLi,j = B

where,
A = The number of tweets in Ti whose geo-location is
Rj and
B = The number of tweets in Ti whose geo-location is
known
We thus obtain two pieces of information about the Actual
home location of the user Ui in the form of two distributions:

CHLi and CGLi , where CGLi is completely user-specific.
However, CHLi is partially user-specific - it does depend
on the user because CHLi is based on her Declared Home
Location, but it also depends on the general distribution which
depends on the entire population mass. It is evident that both
CHLi and CGLi are categorical distribution over the thirty
four Indonesian provinces. Now, we know that a mixture of
discrete distributions over any finite number of categories is
just another distribution over those categories. In order to
combine CGLi and CHLi we obtain a convex combination
of the two to obtain Li,j in the following way:
Li,j = (1 â Ïi ) â CHLi,j + Ïi â CGLi,j

(1)

Now, the mixture weights Ïi for the user Ui is learnt from
the data itself and is calculated as Ïi = |Ti0 |/|Ti |.
Li,j essentially is given by
00

0

Li,j = |Ti | â CHLi,j + |Ti | â CGLi,j

(2)
0

which gives equation (1) when normalized by |Ti | = |Ti | +
00
|Ti | i.e the total number of tweets posted by the user Ui
where,
â¢ Ti = set of tweets produced by user Ui
0
â¢ Ti = subset of Ti and represents the set of tweets by Ui
that contains geo-location information
00
â¢ Ti = subset of Ti and represents the set of tweets by Ui
that do not contain geo-location information
The motivation behind this definition of the mixture weight is
0
that for the Ti tweets which contain geo-location information,
we consider the user-specific location distribution information
inferred from the particular userâs geo-location containing
00
tweets. However, for the tweets of Ti , we have no location
information except for the general information that given a
Declared Home Location for any user Uv in our dataset as
Ra , the location distribution for Uv is CHLv = gCHLa .
Thus, if the Declared Home Location of Ui is given to be
Ra , we consider CHLi = gCHLa . Evidently, we depend
on this semi-user-specific location distribution information for
00
the tweets Ti of Ui . This simple formulation of Li,j also
captures the fact that we rely more on CGLi than on CHLi
when the number of tweets with geo-location information,
generated by Ui is high, however if that count is low ( or even
absent), instead of discarding the particular userâs information,
we obtain the location distribution of Ui from her Declared
Home Location. We experimented by using only geo-location
containing tweets and we saw that the results are far more
accurate if we included users of Type 3 - This is intuitively
correct because the geo-location containing tweets form less
than 1% of the entire dataset.
As noted earlier, the set of users can be divided into four
different classes. We do not try to compute Li,j values for the
users belonging to Class 4. For users belonging to the other
three classes, we compute Li,j using equation (1). For the
users belonging to Class 3, we obtain Ïi to be zero, as we
do not have any geo-location data from the tweets to compute
Ïi .

Algorithm 1 Counting Algorithm for computation of the general Computed Home Location gCHL
â¢
â¢
â¢

Step 1: Initialize gCHLa,b = 0, 1 â¤ a â¤ m, 1 â¤ b â¤ m
Step 2: For each tweet tw in T , increment gCHLa,b if Declared home location of the author of tw and the geo-location
of tw are Ra and Rb respectively
Step 3: Make each row gCHLa of gCHL matrix row stochastic, 1 â¤ a â¤ m

V. R ADICALIZATION I NDEX C OMPUTATION
We intend to assign a Radicalization Index RDi to a Twitter
user Ui based on the content of her tweets. Each tweet can
contain up to 140 characters. Thus the content of a single
tweet does not provide adequate information regarding the
userâs ideology. We collect tweets from users over a period
of time (in our case a month) and for each user Ui we create
a document Di that contains all the tweets Ti of that user,
during that period of time. As there exists a one-to-one correspondence between Ui and Di , by assigning a Radicalization
Index to Di , we essentially assign a Radicalization Index RDi
to Ui . Classical predictive model Multiple Linear regression
[23], [24], [25] fits our application, since it is a dichotomous
classification problem with multiple predictor variables, where
the predictor variables are the terms of our âvocabularyâ.
Classical classification methods such as Logistic Regression
which has applications in a wide variety of domains can
also be used for document classification [26]. Thus, Logistic
Regression can also be applied for our problem. However,
Linear Regression was selected instead of Logistic Regression
because it out-performed the Logistic one through 10-fold
cross validation. This well-known technique divides the given
dataset into 10 segments and then uses 90% of the data ( i.e
nine segments) as training data and 10% of the data ( i.e. one
segment ) as the test data. Linear Regression showed around
98% of accuracy, but Logistic Regression showed 83-85%
of accuracy. The implementation of our approach proceeds
in the following way: First, we identify a set of Indonesian
political organizations. Next, social scientists in our Minerva
team, who are domain experts for Indonesia, hypothesize a
classification to label each organization as radical or counter
radical based on these organizations beliefs and practices.
Using web crawling tools, we download a large number of
documents from the web sites of these organizations. We
use the term âvocabularyâ to mean the set of all unique
terms that appear in all documents from all organizations.
All the documents of an organization are assigned the same
Radicalization Index as the Radicalization Index assigned to
the organization by the domain experts in our team. This set of
documents together with their Radicalization Indices form the
training dataset for our model. After that we use the model to
assign a Radicalization Index to the document Di created from
the tweets of user Ui . This Radicalization Index of document
Di is taken to be the Radicalization Index of user Ui .
A. Problem Formulation:
We formulate the problem in a general sparse learning
framework and solve the following optimization problem (3)

using the techniques from [27] . This is indeed a sparse learning problem because the vocabulary is very large compared to
the number of words used in a document.
1
Ï
2
2
min kAx â yk2 + kxk2 + Î» kxk1
x 2
2
where A â RsÃp , y â RsÃ1 , and x â RpÃ1

(3)

In our application, we have
â¢ A is Document Ã Term matrix which is constructed as
follows: The set of terms (t1 , . . . tp ) includes all the terms
from all the documents by all the organizations, barring
the stop words. The size of the vocabulary in this case is
p.
If data is collected by crawling web sites of different organization (O1 , . . . , Oq ) and documents (di,1 , . . . , d1,ri ) are
collected from the web site of organization Oi , 1 â¤ i â¤ q,
the total number of rows of the matrix A is
s = Î£qi=1 ri ,
and it has the following structure.
Document/T erm
d1
d2
....
ds
â¢
â¢

â¢

t1
....
....
....
....

t2
....
....
....
....

....
....
....
....
....

tp
....
....
....
....

Aij = term f requency of the j th term in the ith
document such that Aij â¥ 0, 1 â¤ i â¤ s, 1 â¤ j â¤ p.
yi â {+1, â1} is the class of each document Di , 1 â¤
i â¤ s. As indicated earlier, the Radicalization Index of
a document is the same the Radicalization Index of the
organization that created that document. Thus, when an
organization is labeled as radical (or counter radical) by
the domain experts, all the documents pertaining to that
organization is marked as +1 (or â1). Thus yi = +1
if Di , 1 â¤ i â¤ s belongs to an organization marked as
radical by the experts, or yi = â1 if Di , 1 â¤ i â¤ s
belongs to an organization marked as counter radical by
the experts.
xj is the weight for each term tj , 1 â¤ j â¤ p. This is the
parameter estimated by optimizing the objective function
(3). The xj âs thus form the predictor variables of the
model.

Let us further clarify the three terms involved in the convex
optimization problem:
â¢

1
2

2

kAx â yk2 - this first term is related to the sum of
the squared errors to fit a straight line to a set of data

points. The objective function (3) thus is the optimization
problem of minimizing this sum of squared-errors.
2
Ï
â¢ 2 kxk2 - this term deals with the ridge regression, which
is an extra level of shrinkage. We set Ï = 0 as we were
mainly driven by sparsity.
â¢ Î» kxk1 - this term involving the L1 norm deals with the
sparsity of the solution vector x. For different values of
Î» we obtain a solution vector x which represents the
weights associated with each term tj , 1 â¤ j â¤ p ( the
same terms which are considered in the A matrix). Some
of these weights are positive, some negative (values can
be very close to 0). The terms with positive (or negative)
weights are the radical (or counter radical) words. The
top (ones with weights having high magnitude) radical
and counter radical words are presented to the experts for
validation. We experiment with several Î» values resulting
in x vectors of various sparsity until the list of top
radical and counter radical words are approved by the
field experts.
We use the Matlab implementation of the SLEP package [28]
that utilizes gradient descent approach to solve the optimization problem (3). This package can handle matrices of 20M
entries within a couple of seconds on a machine with standard
configuration. The input to the SLEP package are the values
of A, Î», and y. The SLEP model outputs the weight vector x.

For each time period (in our case one month), each user
Ui will be assigned Radicalization Index RDi based on their
tweets within that period. This is done as follows:
â¢ As mentioned earlier, each tweet which can only contain
a maximum of 140 characters is too insufficient for
inferring the radicalism of the user. Hence, from the
tweets of each user Ui we form a User Document Di
which is the conglomeration of all her tweets over a
period of one month. It is to be noted here that many
users choose to tweet quite infrequently, hence even if we
collect tweets for one month, a user might have tweeted
only once or twice during the entire one month which
defeats the purpose of collecting tweets for a month.
Hence, we further apply the constraint that we consider
only those users who have tweeted at least seven times
in a month. The value of this threshold has been arrived
at empirically after experimentation with various values
of the threshold.
â¢ With the help of the model that has been fitted using the
organization documents, we classify the User Documents.
Let each User Document Di which is a termf requency
row be denoted by the row vector tc of count of terms
from our âvocabularyâ.
â¢ Each user Ui receives a âscoreâ which we refer to as
Radicalization Index RDi of user Ui . RDi is given by
p
X
j=1

VI. H EAT I NDEX C OMPUTATION
Once we have obtained the Location Indices Li , 1 â¤ i â¤ n
and Radicalization Indices RDi , 1 â¤ i â¤ n, for all the users
Ui , 1 â¤ i â¤ n , the Heat Index Hj of region Rj , 1 â¤ j â¤ m
is computed as
Pn
Hj = i=1 RDi Ã Li,j , âj, 1 â¤ j â¤ m.
The Heat Index Hj for a region Rj indicates the degree of
prevalence of radical ideologies among the people of Rj by
taking into account both the number of radical tweeters living
in Rj and also their âdegree of radicalismâ. In Table I we
present a time-varying Heat Map of Indonesia by computing
the map in three different time intervals of October 10 November 10, November 11 - December 10 and December
11 - January 10. We found a drastic change in the heat
indices during the interval of November 10 â December 10.
But we could not discern any particular event which could
have triggered the same.
VII. DATA C OLLECTION

B. Assignment of Radicalization Index:

RDi = tc .x =

where p is the size of our vocabulary.
This provides us a time-series of RDi values for the users,
which will make it possible to analyze the transition dynamics
for each user. It is evident that a high positive RDi indicates
that Ui is highly radical whereas a high negative RDi indicates
that Ui is highly counter radical.

tcj xj

Since our model requires the computation of both the Radicalization Index RDi as well as the Location Index Li for each
user Ui , we followed a two step data collection procedure
described as follows:
â¢ For the purpose collecting the training data set for computing the Radicalization Index, we crawled the websites
of 36 well-known Indonesian organizations which are
classified as radical or counter radical by our field experts.
A few of the organizations are mentioned in Table II. We
crawled the websites of all these different organizations
and collected a total of 78,135 documents which after
pre-processing and filtering resulted into 49,250 documents. The reason for the reduction from the number of
crawled documents to the number of useful documents
is that many of the crawled documents did not have
any relevant information (for example documents having
only advertisements) and hence were discarded during
pre-processing. Each of the documents on a average
contained 280 words i.e on an average 2880 characters.
All documents pertaining to an organization were labeled
as radical or counter radical depending on the outlook
professed by the organization itself. These were then used
for fitting our Radicalization Index computation model.
â¢ For our study on recovery of political signals pertaining to
trend of radical activities in Indonesia, we chose Twitter
as the data collection platform as Indonesia features as
one of the top five global market segments of Twitter
by reach, and accounts for 19.0% to 20.8% of Twitterâs

TABLE I: The table provides the top 5 province or special region names based on their computed Heat Index values (also
mentioned alongwith) for October 10 - November 10, November 11 - December 10, December 11- January 10
Province Name
Jakarta
East Java
West Java
Yogyakarta
Central Java

Heat Index
5.48
2.95
2.68
1.74
1.68

Province Name
Jakarta
East Java
Yogyakarta
Central Java
West Java

TABLE II: Table showing some of the well-known radical and
counter radical organizations of Indonesia
Radical Organizations
AdianHusaini
PKS
Arrahmah
AbuJibriel
EraMuslim
HizbutTahrir
MillahIbrahim

Counter radical Organizations
DaarulUluum
Interfidei
IslamLiberal
NU
PPIM
Paramadina
LKIS

TABLE III: Keyword markers used for filtering Twitter Stream
API
Keyword
âpenegakan syariahâ
âjihad majelisâ
âmati syahidâ
âajaran islamâ
âpendidikan agama di sekolahâ
âasasi manusiaâ
âdemokrasi yangâ
âkebebasan beragamaâ
âsekularismeâ
âdi negeri negeri islamâ

Interpretation
enforcement of Sharia
jihad assemblies
martyrdom
the teaching of Islam
religious education in schools
human rights
democracy
religious freedom
secularism
Islamic state in the country

total reach by country (Dec 2010)3 . No other publicly
available portal offers access to opinions posted online
by the Indonesian populace on a similar scale as does
Twitter. For gathering tweets, we use Twitterâs Stream
API to access Twitterâs global stream of publicly available
tweet data. Since our goal is to recover âpolitical signalsâ,
we setup a keyword filter on the Stream API to gather
tweets that relate to radical and counter radical ideologies.
The keywords used for this filtration have been identified
by the social scientists in our Minerva project team and
are considered to be significant markers of radical and
counter radical ideologies in the Indonesian context. The
keyword list includes radical markers and a few such
markers are listed in Table III.

Heat Index
16.16
12.33
4.53
3.7
3.39

Province Name
Jakarta
Yogyakarta
West Java
East Java
Central Java

Heat Index
4.71
1.82
1.25
1.20
0.69

Location Index Li of user Ui over the thirty four provinces of
Indonesia, thus we focus only on users from Indonesia. The
keywords used are in Indonesian language and narrows down
the tweets we obtained from the Twitter API. Thus, the geocode in majority of cases indicated a location in Indonesia.
However, not all geo-codes are from Indonesia. We ignore
those tweets in the current work. Thus, out of these 12 million
tweets, 110,063 tweets contained geo-locations that mapped to
regions within Indonesia. To apply this reverse geo-coding,
we used the OpenStreetMap API4 . A user repository was
constructed by including only those users whose Declared
Home Locations matched with an identifiable Indonesian city
or province. Now the user declared home location which the
user mentions as a part of her profile could consist of any
text according to the userâs whim. We found texts such as
âDark side of the moonâ or âsomewhere in this big worldâ
or âHereâ or âinfront of my laptopâ and hence, there is a
need for pre-processing of the text. Also, the users provided
location information to varied degrees of granularity ranging
from continents to towns, however we are interested in the
fixed granularity level of Indonesian provinces and the special
regions such as Jakarta and Yogyakarta. Hence we manually
created a database of towns and cities of all of the Indonesian
provinces. Each of the provinces were annotated with 42 cities/
towns on an average with Papua being the highest which
was annotated with 70 cities/towns. Using this database we
then assigned a legitimate Declared Home Location to as
many users as possible. The final user repository consisted
of 959,911 unique users.

Fig. 2: Figure showing the number of tweets collected over
our observation period

We collected tweet data for a three-month interval and gathered a total of 12,152,874 tweets from October 10, 2012
to January 10, 2013 ( Figure 2) that matched the keyword
filtration criteria ( Table III ). We used the three months of
data to calculate the Radicalization Indices of the users. In
this research, we are interested in the probability distribution
3 http://www.billhartzer.com/pages/comscore-twitter-latin-america-usage/

http://www.comscoredatamine.com/2011/02/the-netherlands-leads-globalmarkets-in-twitter-reach/

4 The relevant information about the API could be found at http://wiki.openstreetmap.org/wiki/Nominatim

VIII. E XPERIMENTAL R ESULTS
We created Heat Maps of Indonesia on a monthly basis. We
computed the RDi of each user U i for each month from
October 10 to January 10, as long as Ui sent at least 7 tweets in
that month Again, for each user Ui we computed the Location
Index Li by considering all her tweets over the period of the
month.. For that we computed the general Computed Home
Location gCHL matrix. The(i, j)th entry gives the probability
of a user with a Declared Home Location of Ri being located
in Rj , 1 â¤ i â¤ m, 1 â¤ j â¤ m.
â¢

gCHL matrix : The gCHL matrix provides interesting
insights on the Indonesian population. We computed the
gCHL - matrix on all possible doublets among the
three months of observation period. i.e for each month
for calculating the Location Indices Li of users, we
have generated the gCHL matrix using the other two
months of data. Thus, in each case, we had training
data of two months and test data of one month. Among
the three gCHL - matrices generated, we saw that the
Declared Home Location of users give us a good insight
on the Actual home location of the user. Thus, instead of
merely depending on the geo-co-ordinates of users, we
should consider the home location from the userâs profile
and home location declarations are much more abundant
than geo-location containing tweets. However, depending
solely on Declared Home Locations can be deceptive. We
also observed that people with Declared Home Locations
in various different provinces from all around Indonesia
such as Bangka Belitung, Banten, Maluku, West Nusa
Tenggara, East Nusa Tenggara and Papua have a very
strong tendency to have high probability of having Actual
home location in Jakarta (as observed from our results
over three months). This is very intuitive because Jakarta
being the Capital Region must have attracted people from
different parts of Indonesia for prospective settlement.
We further made an interesting observation that people
with Declared Home Location of East Kalimantan have
considerable geo-location containing tweets from Central
Kalimantan.

The Heat Indices values for the thirty four Indonesian
provinces are computed using our approach for three months
of our observation period - namely October 10 - November
10, November 11 - December 10, December 11 -January
10. Among all Indonesian provinces the top five provinces
and special regions along with their Heat Index values are
presented in Table I for the three months. Color maps of
Indonesia with Heat Indices is shown in Figure 3, where darker
colors indicate a higher level of radical tweeting, and lighter
colors indicate a lower level of radical tweeting. It may be
seen from Figure 3 that the area around Jakarta and the Java
provinces are highly active in radical tweet creation. According
to our Twitter data analysis, the provinces Jakarta, East Java,
Yogyakarta and Central Java, along with West Java are the top
provinces that generate a high level of radical activities.

IX. VALIDATION
For the purpose of validation of the Radicalization Index,
we computed the Radicalization Indices of some well-known
counter radical leaders of Indonesia for the months that they
had tweeted for more than 7 times which we consider as our
threshold. Our classifier gave perfect accuracy. By accuracy of
the classification we mean the percentage of time the leaders
who are thus known to be counter radical were classified as
counter radical by our classifier. We did not validate the Location Index computation technique because of the lack of the
ground truth of the Actual home location of users. However,
our results of Heat Index are validated by the findings of the
Indonesia-based Wahid Institute5 (named after Abdurrahman
Wahid, an Indonesian Muslim religious and political leader
who served as the President of Indonesia from 1999 to 2001).
Wahid Institute promotes a moderate version of Islam through
dialogue events, publications, and public advocacies. The
institute also releases an annual religious freedom report on
religious life in Indonesia. According to the Wahid Instituteâs
Annual Report of 20126 , the top four provinces of Indonesia
where radical activities are most observable are West Java,
Aceh, East Java, and Central Java. It may be noted here,
that three out of the four most radical provinces identified
by the Wahid Institute, also appear at the very top of our
list. Also, our field experts have confirmed Jakarta to be a
center of radical activities. It may be mentioned here that field
studies7 in January 2012 by Setara Institute8 , a well-known
NGO based in Indonesia, showed that the strong radicalism
of the young muslim population in Yogyakarta and Central
Java are making them hot targets to be recruited as Jihadists. In
May 2012, a mob attack by Indonesian Mujahidin Council on a
book launch of a well-known Canadian author, an advocate of
LGBT, took place in Yogyakarta. In September 2012, there has
been arrests of potential terrorists from Yogyakarta9 . Because,
Wahid Institute has mentioned about Indonesian provinces
only, it might be expected that Jakarta and Yogyakarta, being
special administrative regions, are missing from their list however, we do not have access to their full report. The
high radicalism of the Java provinces are also corroborated
by reports of the Setara Institute. The only radically active
province that shows up in the Wahid Institute report but does
not appear at the top of our list is Aceh, located at the north
west corner of Indonesia. It is worth mentioning here that Aceh
was completely devastated by the 2004 Indian Ocean Tsunami
and is still recovering from its effects. Aceh is also one of
the least economically developed provinces of Indonesia. We
believe that due to the lack of economic advancement in Aceh,
the level of Internet penetration in Aceh is fairly small and not
many people from Aceh are active tweeters. This may explain
5 http://berkleycenter.georgetown.edu/resources/organizations/wahidinstitute
6 Released on December 28, 2012
7 http://www.setara-institute.org/en/content/study-shows-how-youngradical-indonesian-muslims-become-terrorists
8 http://www.setara-institute.org/
9 http://www.washingtontimes.com/multimedia/image/indonesia-terrorjpg/

(a) Heat Map for October 10 to November 10

(b) Heat Map for November 11 to December 10

(c) Heat Map for December 11 to January 10

(d) Radicalism
Scale

Index

Fig. 3: Heat Maps of Indonesia

the reason for Aceh not showing up among our list of top
radically active provinces.
X. C ONCLUSION
We have developed a generic robust technique for recovering signals pertaining to a geographical area such as a
country using Twitter Data. We have applied our technique
to our Indonesian dataset and have observed high accuracy.
The goal of our work is the generation of a political Heat
Map of Indonesia which will clearly indicate the provinces of
Indonesia where radical narrative is prominent. The granularity
of the Radicalization Index Assignment used is a Twitter
user, while the granularity of regions used is an Indonesian
Province. Thus, we have analyzed tweets made by a user Ui
in a month to assign a Radicalization Index RDi to Ui - Thus
RDi indicates how radical is Ui in her political outlook . Also,
by mining the tweets in our database we assign a Location
Index Li to Ui . For computation of the Location Index, the
sources of location information used are not only the geolocation tagging as provided by Twitter API for the tweets
which contain that information (such tweets constitute less
than 1% of our dataset), but we also use the user declared
home location information from her profile (after considerable
amount of pre-processing and cleansing). We have combined
these two sources of information for inferring the probability
distribution of the location of the users among the provinces
of Indonesia. Thus, by considering the RDi âs for all the users
over a period of one month, in conjunction with the Location
Indices Li âs we generate the political Heat Maps of the likes

of Figure 3. We have got a time series of Heat Maps, which
can help us in mapping trends by regions in radical discourse.
Such Heat Maps can prove to be very useful in studying the
spatio-temporal dynamics of the people of Indonesia so far as
their political outlook is concerned
R EFERENCES
[1] Cho, E., Myers, S. A., & Leskovec, J. (2011, August). Friendship
and mobility: user movement in location-based social networks. In
Proceedings of the 17th ACM SIGKDD international conference on
Knowledge discovery and data mining (pp. 1082-1090). ACM.
[2] Xu, J. M., Bhargava, A., Nowak, R., & Zhu, X. (2012). Socioscope:
spatio-temporal signal recovery from social media. In Machine Learning
and Knowledge Discovery in Databases (pp. 644-659). Springer Berlin
Heidelberg.
[3] Priedhorsky, R., Culotta, A., & Del Valle, S. Y. (2013). Inferring the
Origin Locations of Tweets with Quantitative Confidence. arXiv preprint
arXiv:1305.3932.
[4] Yang, D., Zhang, D., Yu, Z., & Wang, Z. (2013, May). A sentimentenhanced personalized location recommendation system. In Proceedings
of the 24th ACM Conference on Hypertext and Social Media (pp.
119-128). ACM.
[5] Rahimi, S. M., & Wang, X. (2013). Location Recommendation Based on
Periodicity of Human Activities and Location Categories. In Advances in
Knowledge Discovery and Data Mining (pp. 377-389). Springer Berlin
Heidelberg.
[6] Li, Y., Steiner, M., Wang, L., Zhang, Z. L., & Bao, J. Exploring Venue
Popularity in Foursquare.
[7] Li, Y., Steiner, M., Wang, L., Zhang, Z. L., & Bao, J. (2012, December).
Dissecting foursquare venue popularity via random region sampling.
In Proceedings of the 2012 ACM conference on CoNEXT student
workshop (pp. 21-22). ACM.
[8] Gonzalez, M. C., Hidalgo, C. A., & Barabasi, A. L. (2008). Understanding individual human mobility patterns. Nature, 453(7196), 779-782.

[9] Sadilek, A., Kautz, H., & Bigham, J. P. (2012, February). Finding your
friends and following them to where you are. In Proceedings of the
fifth ACM international conference on Web search and data mining (pp.
723-732). ACM.
[10] Noulas, A., Scellato, S., Lambiotte, R., Pontil, M., & Mascolo, C.
(2012). A tale of many cities: universal patterns in human urban mobility.
PloS one, 7(5), e37027.
[11] Lu, X., Bengtsson, L., & Holme, P. (2012). Predictability of population displacement after the 2010 Haiti earthquake. Proceedings of the
National Academy of Sciences, 109(29), 11576-11581.
[12] Chang, H. W., Lee, D., Eltaher, M., & Lee, J. (2012, August). @
Phillies Tweeting from Philly? Predicting Twitter User Locations with
Spatial Word Usage. In Advances in Social Networks Analysis and
Mining (ASONAM), 2012 IEEE/ACM International Conference on (pp.
111-118). IEEE.
[13] Mahmud, J., Nichols, J., & Drews, C. (2012). Where is this tweet from?
inferring home locations of twitter users. Proc AAAI ICWSM, 12.
[14] Eisenstein, J., OâConnor, B., Smith, N. A., & Xing, E. P. (2010,
October). A latent variable model for geographic lexical variation. In
Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing (pp. 1277-1287). Association for Computational
Linguistics.
[15] Cheng, Z., Caverlee, J., & Lee, K. (2010, October). You are where
you tweet: a content-based approach to geo-locating twitter users. In
Proceedings of the 19th ACM international conference on Information
and knowledge management (pp. 759-768). ACM.
[16] Hecht, B., Hong, L., Suh, B., & Chi, E. H. (2011, May). Tweets from
Justin Bieberâs heart: the dynamics of the location field in user profiles.
In Proceedings of the 2011 annual conference on Human factors in
computing systems (pp. 237-246). ACM.
[17] Boutet, A., Kim, H., & Yoneki, E. (2012, August). Whatâs in Twitter: I
Know What Parties are Popular and Who You are Supporting Now!. In
Advances in Social Networks Analysis and Mining (ASONAM), 2012
IEEE/ACM International Conference on (pp. 132-139). IEEE.
[18] Dahlin, J., Johansson, F., Kaati, L., Martenson, C., & Svenson, P.
(2012, August). Combining Entity Matching Techniques for Detecting
Extremist Behavior on Discussion Boards. In Advances in Social Networks Analysis and Mining (ASONAM), 2012 IEEE/ACM International
Conference on (pp. 850-857). IEEE.
[19] Eisenstein, J., Ahmed, A., & Xing, E. P. (2011, June). Sparse additive
generative models of text. In International Conference on Machine
Learning (ICML).
[20] Hong, L., Ahmed, A., Gurumurthy, S., Smola, A. J., & Tsioutsiouliklis,
K. (2012, April). Discovering geographical topics in the twitter stream.
In Proceedings of the 21st international conference on World Wide Web
(pp. 769-778). ACM.
[21] OâConnor, B., Eisenstein, J., Xing, E. P., & Smith, N. A. (2010).
Discovering demographic language variation. In Workshop on Machine
Learning for Social Computing at NIPS.
[22] Yin, Z., Cao, L., Han, J., Zhai, C., & Huang, T. (2011, March).
Geographical topic discovery and comparison. In Proceedings of the
20th international conference on World wide web (pp. 247-256). ACM.
[23] Zhang, T. (2009). Some sharp performance bounds for least squares
regression with L1 regularization. The Annals of Statistics, 37(5A),
2109-2144.
[24] S.Kim, K.Koh, M.Lustig, S.Boyd and D. Gorinevsky, An Interior-Point
Method for Large-Scale l1 - Regularized Least Squares Journal of
seleccted topics in Signal Processing, Vol. 1, No. 4, pages 606 - 617,
Dec 2007
[25] Kolter, J. Z., & Ng, A. Y. (2009, June). Regularization and feature
selection in least-squares temporal difference learning. In Proceedings
of the 26th Annual International Conference on Machine Learning (pp.
521-528). ACM.
[26] Brzezinski, J. R., & Knafl, G. J. (1999). Logistic regression modeling for context-based classification. In Database and Expert Systems
Applications, 1999. Proceedings. Tenth International Workshop on (pp.
755-759). IEEE.
[27] Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths for
generalized linear models via coordinate descent. Journal of statistical
software, 33(1), 1.
[28] J. Liu, S. Ji and Jieping Ye. SLEP: Sparse Learning with Efficient
Projections, Arizona State University (2009)
[29] Lee, S. I., Lee, H., Abbeel, P., & Ng, A. Y. (2006, July). Efficient
LË 1 Regularized Logistic Regression. In Proceedings of the National

[30]
[31]

[32]

[33]

[34]

[35]

Conference on Artificial Intelligence (Vol. 21, No. 1, p. 401). Menlo
Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.
T. P. Minka, A comparison of numerical optimizaers for logistic regression, Technical report, 2007
Thelwall, M., Buckley, K., Paltoglou, G., Skowron, M., Garcia, D.,
Gobron, S., ... & Holyst, J. A. (2013). Damping Sentiment Analysis
in Online Communication: Discussions, Monologs and Dialogs. In
Computational Linguistics and Intelligent Text Processing (pp. 1-12).
Springer Berlin Heidelberg.
Melville, P., Gryc, W., & Lawrence, R. D. (2009, June). Sentiment analysis of blogs by combining lexical knowledge with text classification.
In Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining (pp. 1275-1284). ACM.
Joshi, M., Das, D., Gimpel, K., & Smith, N. A. (2010, June). Movie
reviews and revenues: An experiment in text regression. In Human
Language Technologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computational Linguistics (pp.
293-296). Association for Computational Linguistics.
Kogan, S., Levin, D., Routledge, B. R., Sagi, J. S., & Smith, N. A.
(2009, May). Predicting risk from financial reports with regression.
In Proceedings of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the Association for
Computational Linguistics (pp. 272-280). Association for Computational
Linguistics
Brynielsson, J., Horndahl, A., Johansson, F., Kaati, L., Martenson, C.,
& Svenson, P. (2012, August). Analysis of weak signals for detecting
lone wolf terrorists. In Intelligence and Security Informatics Conference
(EISIC), 2012 European (pp. 197-204). IEEE.

A Network Planning and Management Tool for
Mitigating the Impact of Spatially Correlated Failures in
Infrastructure Networks
Arun Das, Arunabha Sen, Chunming Qiao, Nasir Ghani, Nathalie Mitton

To cite this version:
Arun Das, Arunabha Sen, Chunming Qiao, Nasir Ghani, Nathalie Mitton. A Network Planning
and Management Tool for Mitigating the Impact of Spatially Correlated Failures in Infrastructure Networks. International Conference on Design of Reliable Communication Networks
(DRCN), Mar 2016, Paris, France. <hal-01254982>

HAL Id: hal-01254982
https://hal.inria.fr/hal-01254982
Submitted on 21 Mar 2016

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

Lâarchive ouverte pluridisciplinaire HAL, est
destineÌe au deÌpoÌt et aÌ la diffusion de documents
scientifiques de niveau recherche, publieÌs ou non,
eÌmanant des eÌtablissements dâenseignement et de
recherche francÌ§ais ou eÌtrangers, des laboratoires
publics ou priveÌs.

A Network Planning and Management Tool for
Mitigating the Impact of Spatially Correlated
Failures in Infrastructure Networks
Arun Dasâ , Arunabha Senâ , Chunming Qiaoâ  , Nasir Ghaniâ¡ , Nathalie MittonÂ§

â School of Computing, Informatics and Decision System Engineering, Arizona State University, Tempe, Arizona 85287, USA
â  Department of Computer Science and Engineering, SUNY at Buffalo, Buffalo, NY 14201, USA
â¡ Department of Electrical Engineering, University of South Florida, Tampa, FL 33620, USA
Â§ Inria, 40 Avenue Halley, 59650 Villeneuve DâASCQ, France

Email: arun.das@asu.edu, asen@asu.edu, qiao@computer.org, nghani@usf.edu, nathalie.mitton@inria.fr
AbstractâCurrent practices of fault-tolerant network design
ignore the fact that most network infrastructure faults are
localized or spatially correlated (i.e., confined to regions). Network
operators require new tools to mitigate the impact of such
region based faults on their infrastructures. Utilizing the support
from the U.S. Department of Defense, and by consolidating a
wide range of theories and solutions developed in the last few
years, the authors of this paper have developed an advanced
Network Planning and Management Tool (NPMT) that facilitates
the design and provisioning of robust and resilient networks.
The tool provides multi-faceted network design, evaluation and
simulation capabilities for network planners. Future extensions
of the tool currently being worked upon not only expand the tools
capabilities, but also extend these capabilities to heterogeneous
interdependent networks such as communication, power, water
and satellite networks.

I.

I NTRODUCTION AND M OTIVATION

It is extremely important that planners for large wide area
networks have the right tools to design robust and resilient
networks that can effectively withstand large scale geographically correlated failures in their networks. Such failures can
be triggered by nature (hurricane or earthquake) or humans
(nuclear attack or conventional weapon attack over a large
geographical area). With research support from the U.S. Defense Threat Reduction Agency, an agency whose mission is to
protect the U.S. against Weapons of Mass Destruction (WMD),
such as nuclear, biological or chemical attacks, the authors
of this paper, over the last six years have developed a wide
ranging set of concepts and techniques for enhancing network
robustness against spatially correlated or region based faults.
We have recently incorporated these concepts and techniques
into a Network Planning and Management Tool (NPMT) [1]
for the benefit of network designers, planners and operators.
In this paper, we first describe the novel concepts developed
to design networks that are robust against region based faults,
and then describe how these concepts have been incorporated
into the NPMT. The goal of this paper is to bring to the
attention of the networking research community, and the
audience of the workshop on DRCN in particular, the existence
of NPMT as a tool that consolidates a large body of work on
spatially correlated failures, and as a tool that can be used by
This work was supported in part by the NSF grant 1441214, and by grant
HDTRA1-09-1-0032 from the U.S. Defense Threat Reduction Agency

the community to meet the needs for robust network design
against spatially correlated failures. In essence, this paperâs
contribution should not be measured in terms of new analytical
findings, but in terms of service to the networking community.
We use the term WMD attack to imply a large scale
geographically correlated failure such as failures caused by an
earthquake, hurricane or nuclear attack. The characteristic of a
WMD attack is massive but localized faults. The connectivity
of a network [2] is generally accepted as a metric for evaluating
the fault-tolerance capability of a network [3]. If a networkâs
connectivity is k + 1, then the network can tolerate up to k
faults, implying that the surviving network remains connected
even after k failures. The connectivity metric, however, has no
way of capturing locality, i.e., the faulty nodes/edges may be
close or far away from each other. Thus, the connectivity metric cannot distinguish between faults that are geographically
correlated (a WMD fault characteristic), and faults that are not.
Connectivity as a metric also fails to capture other important
structural properties of the network such as the number or
size of the connected components [2] into which a network
disintegrates when the number of failed nodes/edges exceeds
the node/edge connectivity of the network.
Recognizing the limitations of connectivity as a metric
for capturing the special characteristics of geographically correlated failures, the authors of [4] introduced the notion of
region-based connectivity. A region may be defined either with
reference to the network graph or to the network geometry (i.e.,
layout of the network in a two or three dimensional space). For
example, a region may be defined as a subgraph with diameter
d (where the diameter of a graph is defined as the maximum
of the shortest path distance between a pair of nodes, taken
over all source-destination node pairs). Or, a region may also
be defined as a collection of nodes and edges in the network
graph layout that is covered by a circular area in that layout.
Figure 1(a) shows an example of a circular region-based fault.
The NPMT described in this paper is intended to support
design and analysis of single layered and multi-layered interdependent heterogeneous networks. In essence, the NPMT
is particularly suitable for planning and design of critical
infrastructures. For example, from the single network layer
perspective, the NPMT enables backbone communication network providers, such as, AT&T, Sprint, Qwest and Level 3
Communications, to (i) identify the most vulnerable parts of

(a)

(b)

(c)

Fig. 1: (a) Network with circular fault region, (b) Optical fiber network of a major U.S. provider, (c) Optical Fiber network of a major European
provider disrupted by a WMD attack

their network against a WMD attack, and (ii) reinforce the
network with least cost to eliminate or significantly reduce
the threat of network disruption due to a WMD attack. Figure
1(b) shows the backbone network of a major U.S. provider
and Figure 1(c) shows how the backbone network of a major
European provider can potentially be disrupted by a WMD
attack. From a multi-layer perspective the NPMT can be used
for design and analysis of smart cities, where heterogeneous
networks ranging from disparate telecom networks (such as
2G, 3G, WiFi, Bluetooth, etc.) to water, electricity and gas
distribution networks, form a complex interdependent ecosystem. Subsequently, failures in one network, for example a leak
in the water distribution network, may deteriorate other nearby
(spatially correlated) infrastructures such as gas or electricity
whose pipes and cables may get affected due to the leak. In this
context, a tool like NPMT can be an asset for utility companies
and smart city planners to quickly perform (i) root cause
analysis of failure, and (ii) forecast fault evolution, to direct
repairs and maintenance towards specific network components
and restrict fault propagation. To the best of our knowledge no
such tool is available today that supports features for planning
and designing of single layer and multi-layer interdependent
networks in the presence of spatially correlated faults.
Several studies in the network research community have
focused on different aspects of spatially correlated or regionbased faults in networks [5-11], however, to the best of our
knowledge there does not exist an executable platform that
consolidates the findings and techniques of these studies into
a readily usable tool. The NPMT is intended to fill that gap
and be such a platform that can incorporate the outcomes
developed in [5-11] into executable modules to be integrated
into the NPMT. This will allow network designers, planners
and operators to use the results of these studies in their real
world operational networks.
The rest of the paper is organized as follows: In Section
II we present an overview of the underlying concepts and
theoretical results that the NPMT operates on. In Section III
we outline the capabilities of the NPMT and finally Section
IV concludes this paper.
II.

C ONCEPTS , M ETRICS AND S OLUTION T ECHNIQUES

In this section we give a brief overview of the underlying
concepts, metrics and solution techniques that the Network

Planning and Management Tool (NPMT) utilizes to carry out
its functional operations. The NPMT is built as a modular
execution engine that can execute smaller reusable modules
to perform desired operations on a network topology. In this
respect, the current version of the NPMT comprises of different
modules that deal with both static and dynamic aspects of
robust and resilient network design. The modular approach
allows design, development and testing of these modules to
be done independently and defers the integration into the
NPMT until a module meets itâs functional requirements. In
the following sub-sections we give a brief overview of the
analytical foundations of these modules. It may be noted that,
as of writing this paper not all modules have been implemented
and integrated into the NPMT. Accordingly, we highlight our
ongoing work in the discussion below.
A. Region-Based Fault Metrics Computation Module
As outlined in Section I, connectivity as a metric fails to
capture several characteristics of the network in presence of
spatially correlated failures. For instance, the number or size of
the connected components into which a network disintegrates
in the presence of a spatially correlated fault is not captured by
the traditional connectivity metric. In order to overcome these
gaps and capture such network state characteristics, several
metrics and their computation techniques have been proposed
by the research community. For a given network topology, the
NPMT can analyze the network and compute metrics pertinent
to network state in the presence of spatially correlated faults.
The following metrics are supported by the NPMT:
Region-based Connectivity Metric Computation
Region based connectivity can be considered under two fault
models â (i) Single Region Fault Model (sRFM) where faults
are confined to a single region [4], and (ii) Multiple Region
Fault Model (mRFM) where faults are confined to k regions
for some specified k [12].
Formally, in sRFM, the single-region-based (node) connectivity of graph G with a specified definition of region R,
sÎºR (G), is defined as follows: Suppose that {R1 , . . . , Rk } is
the set of all possible regions of the graph G. Consider a
k-dimensional vector T whose i-th entry, T [i], indicates the
number of nodes in region Ri whose failure will disconnect
the graph G. If the graph G remains connected even after the

failure of all nodes of the region Ri then T [i] is set equal to
â. The region-based connectivity of a graph G with region
R, is then computed as follows:
sÎºR (G) = min T [i]
1â¤iâ¤k

In mRFM, the multi-region-based (node) connectivity of graph
G with a specified definition of region R, mÎºR (G), is defined
as the minimum number of regions whose removal (i.e.,
removal of all nodes in the regions and edges incident on them)
will disconnect the graph.
Polynomial time algorithms to compute region-based connectivity in sRFM was presented in [4]. The NPMT contains an
implementation of this algorithm that can be used to compute
the Region-based Connectivity for a given network topology.
Region-based
Component
Decomposition
(RBCDN) Metric Computation

Number

Proposed by the authors of [13], the Region-Based Component Decomposition Number, or RBCDN of graph G = (V, E)
with a specified definition of region R is defined the following
way: Suppose that {R1 , . . . , Rk } is the set of all possible
regions of the graph G. Consider a k-dimensional vector C
whose i-th entry, C[i], indicates the number of connected
components in which G decomposes when all entities in Ri
fails. RBCDN of a graph G with region R is defined as follows:
Î´R (G) = max C[i]
1â¤iâ¤k

RBCDN as a metric provides a insight into the worst case
scenario on how fragmented a network can become in the
presence of a spatially correlated fault. In [13] the authors
propose techniques to compute the RBCDN and the NPMT
provides an implementation of this algorithm that can be used
on user selected network topologies.
Region-based Smallest/Largest Component Size Metric
Computation
The Region-Based Smallest (Largest) Component Size, or
RBSCS/RBLCS was proposed in [14], and is defined for
a graph G = (V, E) with a specified definition of region
R, as follows: Suppose that {R1 , . . . , Rk } is the set of all
possible regions of the graph G. Consider a k-dimensional
vector CS (CL ) whose i-th entry, CS [i] (CL [i]), indicates the
size of the smallest (largest) connected component in which G
decomposes when all nodes in Ri fails. The RBSCS Î±R (G)
and RBLCS Î²R (G) of graph G with region R is defined as:
Î±R (G) = min CS [i] and Î²R (G) = min CL [i]
1â¤iâ¤k

1â¤iâ¤k

The RBLCS and RBSCS metrics provide insights on how well
a networkâs performance degrades in the presence of regionbased faults. Depending on the needs of graceful performance
degradation, networks designers may choose to design networks that have a small value of RBCDN (Î´R (G)) and a high
value of either RBLCS (Î±R (G)) or RBSCS (Î²R (G)). The
NPMT allows the user to compute the RBLCS and RBSCS
metrics for a chosen network topology.

is a need for techniques to compute the set of regions, given
a network and some fault specification. In [14], given a graph
Gâs layout on a two-dimensional plane and a fault radius r,
the authors provide a polynomial time algorithm to compute all
distinguishable or distinct circular regions with radius r. Two
fault regions are considered indistinguishable if they contain
the same set of links and nodes. The authors considered both
wired networks, where nodes and edges can be part of a failure
region, and wireless networks, where only nodes can be part
of a failure region. It was shown in [14] that the number of
distinct regions in wireless and wired networks are O(n2 )
and O(n4 ) respectively, and that all distinct regions can be
computed in O(n6 ) time, where n is the number of nodes.
The NPMT is bundled with an implementation of the
technique outlined in [14]. Given a network topology and a
fault radius, the NPMT can compute all distinct regions of
the network which can then be used by other modules of
the NPMT, such as the Metric Computation Module and the
Region-disjoint Path Computation Module (discussed next).
C. Region-disjoint Paths Computation Module
For a graph G = (V, E), a set of region-disjoint paths
P between a source node s and destination node d with a
specified definition of region R, is defined as follows: Suppose
that {R1 , . . . , Rk } is the set of all possible regions of graph
G and path Pu â P contains a set of nodes and edges from
G such that Pu forms a path from s to d, {s, d} â V . Then,
for every pair of paths {Pu , Pv } â P, u 6= v, Pu and Pv are
region-disjoint, i.e. there is no region in R that both the paths
traverse. Formally, region-disjoint paths are defined as follows,
for all i = 1, . . . , k:
|(Pu â© Ri ) â© (Pv â© Ri )| = 0, â{Pu , Pv } â P, u 6= v
Although region-disjoint path computation has been addressed
in [8], the authors consider a model where faults do not cause
edges to fail unless a failed edge is associated with a failed
node. In this model an edge cannot fail on itâs own and can
only fail when one of the nodes incident on the edge fails. This
assumption is considerably restrictive and possibly unusable
for designers of larger networks where spatially correlated
faults can affect nodes and edges independently. In order to
overcome this limitation the NPMT supports computation of
region-disjoint paths in the presence of circular faults using an
Integer Linear Program (ILP) that doesnât presuppose any such
restrictions. The NPMT is capable of computing two regiondisjoint paths from given source and destination nodes such
that the sum of lengths of the two paths is minimum. Also, as
the source (destination) node is part of a region that is traversed
by both paths (as both paths have the same staring and ending
points), no region disjoint path may exist. To accommodate this
situation the NPMT accommodates the use of no-fault zones
â a circular area around the source and destination nodes that
is immune to faults. Future extensions of this module include
computing more than two paths, and including other selection
criteria such as minimizing the maximum path length.

B. Distinct Regions Computation Module

D. Region-based Fault Tolerant Distributed File Storage Module

It may be noted that all the previously defined metrics
operate on a given graph and a set of regions. Thus, there

In the preceding discussions the importance of a node
in keeping the network connected is emphasized, however,

individual nodes can also act as data stores of the network
and the removal of a node from a network (due to a regionbased fault), may not only cause connectivity losses, but also
data losses. To address such data loss risks, distributed storage
techniques are often employed that enhances data survivability
in the presence of faults. One such technique is redundancy,
such as by (i) storing multiple copies of the entire file, or
(ii) storing different fragments of the same file at different
nodes in the network. In the popular (N, K), N â¥ K file
distribution scheme, from a file F of size |F |, N segments
of size |F |/K are created in such a way that it is possible to
reconstruct the entire file by accessing any K segments. For
such a reconstruction scheme to work, it is essential that the
K segments of the file are stored in nodes that are connected
to each other in the network. However, in the event of failures,
the network may become disconnected (i.e., split into several
connected components) and K segments may not be accessible
in the residual network to reconstruct the file F .
From the context of data survivability in the presence of
spatially correlated faults in networks, the NPMT supports
a âRegion-based Distributed File Storage Moduleâ that implements an algorithm proposed in [11] that ensures that:
(i) even when the network is fractured into disconnected
components due to a region-based fault, at least one of the
largest components will have access to at least K distinct file
segments with which to reconstruct the entire file, and (ii)
the total storage requirement is minimized. As of writing this
paper, this module is currently under development and will be
part of the NPMT upon its completion.
E. Robust Multi-layer Interdependent Network Design Module
In todayâs world, a multitude of heterogeneous interconnected networks form a symbiotic ecosystem that supports all
of the economic, political and social aspects of human life.
For example, the critical infrastructures of the nation such
as the power grid and the communication network are highly
interdependent on each other, and any adverse effects on one
network can affect the other network. Thus, isolated network
analysis is no longer sufficient to design and operate such
interconnected and interdependent network systems.
Recognizing this need for a deeper understanding of the
interdependency in such multi-layered network systems, significant efforts have been made by the research community in the
last few years, and accordingly, a number of analytical models
have been proposed to analyze such interdependencies [15-17].
However, most of these models are simplistic and fail to
capture the complex interdependencies that may exist between
entities of the power grid and communication networks. To
overcome the limitations of existing models, the authors of
[18] have proposed an Implicative Interdependency Model that
is able to capture such complex interdependency. Utilizing
this model, several problems on multi-layer interdependent
networks have been studied, such as (i) identification of the
K most vulnerable nodes [18], (ii) root cause analysis of
failures [19], (iii) the entity hardening problem [20], (iv) the
smallest pseudo-target set identification problem [21], and (v)
the robustness analysis problem [22].
This module will support multi-layer network interdependency modeling using the Implicative Interdependency Model,

and analysis of multi-layer networks using the techniques proposed in [18-22]. The module is currently under development
and will be part of the NPMT upon its completion.
F. Module for Progressive Recovery from Region-based Failures
With this module, the NPMT addresses post-fault recovery
techniques in the aftermath of region-based faults on multilayer interdependent networks. To restore an interdependent
network system from a post-fault scenario to its pre-failure
state, all the faulty network entities (nodes/edges) have to be
repaired or replaced. However, resource limitations may prevent simultaneous restoration of all failed units of the network.
Accordingly, the failed units have to be restored in a sequenced
manner. As each network entity in its operational state adds
some utility value to the interdependent network system, when
a unit recovers from a failed state to an operational state,
the unit starts providing some âbenefitâ to the system. Since
different units have different utility values to the system, the
sequence in which the failed units are restored is important as
the recovery sequence determines the cumulative system utility
during the recovery process.
As discussed in Section II-E, the Implicative Interdependency Model provides a powerful technique for modeling
dependencies in multi-layer interdependent networks. Using
this model the authors of [23] studied the progressive recovery
problem in interdependent networks with the objective of
maximizing system utility during the system recovery process.
This module implements the progressive recovery algorithm of
[23], and can be used to sequence recovery of network entities
from a post-fault to a pre-fault network state that maximizes
system utility during the recovery process. The module is
currently under development and will be part of the NPMT
upon its completion.
III.

A RCHITECTURE AND S YSTEM C APABILITIES

In this section we first outline the system architecture, and
then discuss the different capabilities of the NPMT.
A. System Architecture
View

Service

Fault Analyzer

Path Analyzer

Topology Manager

Profile Manager

Traffic and Fault Simulator

Core Modules:
Network Topology Manager
Region-Based Fault Analysis

Controller

Disjoint Path Analysis

Visualization Engine

Simulation Engine

Execution Engine

Common Modules:

Repository

N/W Fault Impact Analyzer

Path Planning Algorithms

Model
Simulation Data

User/Roles

Path Archive

Fault Generation Engine

N/W Topologies

Fault Archive

Library Faults

Request Generation Engine

Fig. 2: The NPMT High-Level Architecture

The NPMT is implemented as a web-application that
allows the user to remotely connect and operate the tool from
a browser. The web-application follows the standard three-tier
architecture and has a client tier, application tier, and database

(a)
(b)
Fig. 3: (a) Topology Manager â create, edit and manage network topologies, (b) Fault Analyzer â generic fault analysis, metric computations

tier. The tool has been developed following the Model-ViewController (MVC) design pattern. Figure 2 outlines the high
level architecture and some of the components of the tool.

map tiles are rendered from OpenStreetMap [24]. The NPMT
uses the OpenLayers API to support an user interactive map
interface.

The tool is currently accessible from Arizona State Universityâs WAN, and runs from our testbed server. The toolâs webapplication is deployed on an Apache Tomcat 7 instance, and
the repository used is MySQL. The application tier business
logic for operations on network topologies, such as RegionBased Fault Analysis and Region Disjoint Path Analysis, are
implemented in Java. Additional packages and libraries, such
as IBM ILOG CPLEX Optimization Studio libraries (required
for solving Integer Linear Programs), are setup and made
available on the testbed server. Our testbed server is a 64bit Intel Core 2 Quad Core (2.66 GHz) system with 8 GB of
RAM running an Ubuntu 14.04 instance.

To create the topology and place nodes and edges on the
map, the user can either point-and-click on the map itself,
or can type in specific latitude and longitude coordinates and
then proceed to add the network entity. Capacities for each
edge (in Gigabits per second), can also be specified during the
edge creation process. Once a network topology is created, the
topology must be saved to be used for Network Analysis and
Network Simulation. The topologies are saved on the NPMT
server and can be loaded back into the Topology Manager to
edit any entity or attribute of the network.

B. System Capabilities
The NPMT is designed to be used by following a three
step workflow comprising of (i) Network Creation, (ii) Network Analysis, and (iii) Network Simulation. Accordingly, the
individual features and the executable modules of the NPMT
are bundled around these three workflow steps. The following
list enumerates the current high-level features of the tool and
the corresponding workflow step that each feature emulates:
1)
2)
3)
4)

Topology Management (Network Creation)
Fault Analysis (Network Analysis)
Path Analysis (Network Analysis)
Traffic and Fault Impact Simulation (Network Simulation)

Each of the above features are accessible from a tabbed
interface of the tool and can be navigated to from any part of
the web-application. In the following subsections we discuss
each of the features and provide a brief functional overview.
Topology Management
Network Creation is the first step of the NPMT workflow and
the Topology Manager interface allows the user to create, edit,
save and delete network topologies. The Topology Manager
presents the user with a geographical map interface that she
can interact with to manage network topologies. The displayed

Figure 3(a) shows a screen grab of the Topology Manager.
As seen in the figure, the map interface is on the right and the
user interact-able menu is on the left. The user can click on the
map to to add nodes and edges, or can alternatively type in the
latitude and longitude coordinates in the input fields available
on the menu. The menu also lists the nodes and edges that are
part of the topology. Selecting an edge or node from these lists
highlights the network entity on the map (in yellow), and the
user can then proceed to remove the entity from the network if
necessary. The displayed map overlays can be toggled from a
dropdown menu available on the map (in blue in Figure 3(a)).
Finally, as seen in Figure 3(a), options for saving, loading, and
deleting topologies are available to the user directly below the
displayed mapâs dimensions.
Fault Analysis
Once network topologies are created from the Topology Manager, the Fault Analyzer can be used to analyze the created
networks for their resilience in the presence of spatially correlated faults. In the context of the NPMT, network resiliency
is measured by how well the network performs when benchmarked against the metrics outlined in Section II-A. It may
be noted that the metrics of Section II-A emphasize resilience
from the aspect of connectivity in the presence of a spatially
correlated fault. For example, the more number of disconnected
components a network has due to a fault, the worse is the
networkâs resilience (as captured by the metric RBCDN). It

(a)
(b)
Fig. 4: Fault Analyzer - Specified Fault Analysis. (a) User specified fault coordinates, (b) Fault impact of the user defined fault and an imported
library fault (coordinates for the state of California, USA)

may be noted that, for the purpose of this analysis the NPMT
assumes that any network entity (nodes/edges), that fall within
the fault area are all rendered inoperable, i.e. the fault model
is deterministic and if a network entity falls within the fault
region, it necessarily fails. To carry out this analysis, the user
first selects a network topology and can then choose to either
perform a generic fault analysis, or a specified fault analysis.
These analyses are described below.
Generic Fault Analysis: In the generic fault analysis, for a
selected network topology, the user specifies a fault feature and
the tool computes the values of the individual metrics listed
in Section II-A. The NPMT can generically analyzes circular
faults, and the supported fault feature is the fault radius r.
As shown in Figure 3(b), the user can specify the fault
radius r from the left menu. The tool then performs the
generic fault analysis by (i) computing all the distinct regions with radius r using the techniques implemented in
the module âDistinct Regions Computation Moduleâ (Section
II-B), and (ii) computes the individual metrics using the
techniques implemented in the module âRegion-Based Fault
Metrics Computation Moduleâ (Section II-A). The results
are subsequently reported back to the user. For the network
selected in Figure 3(b) and radius r = 500 km., the computed
Region-based Component Decomposition Number (RBCDN)
is 2, the Region-based Largest Component Size (RBLCS) is 9
and the Region-based Smallest Component Size (RBSCS) is
1. Finally, the number of distinct regions computed is 112.
As shown in Figure 3(b), the user is also presented with
sample worst case fault scenarios where a distinct fault causes
the network to fragment into the same number of components
as the RBCDN. Selecting one of the listed faults updates
the displayed network with the faultâs impact. In Figure 3(b)
the fault centered at 36.249â¦ N , â85.696â¦ E is selected. The
nodes and edges rendered inoperable by the fault are grayed
out, while the surviving nodes and edges are shown in green
and black respectively. The connected components in the
fragmented network are highlighted by a light-green region. In
this example, the loss of the two grayed out nodes causes the
network to fragment into two disconnected components: one

with 9 components, and the other with 1 component. Options
for saving the analysis results are available from the menu.
Specified Fault Analysis: In the specified fault analysis, the
user can provide the exact coordinates of one or more faults
and visualize the impact of these faults on the selected network.
The user has the option to save and load faults to visualize the
impact of a fault on different networks. The NPMT also comes
bundled with a set of library faults that the user can choose
from to simulate fault impact on a network. The current set
of library faults consist of the coordinates of the 50 states
of the USA. The inclusion of a fault library in the NPMT is
to provide the user with pre-defined fault scenarios based on
known fault patterns, faults centered at a target of interest, or
recorded faults. For example, fault impact zones of Level 4
hurricanes such as hurricane Katrina or hurricane Sandy.
As shown in Figure 4(a), to specify the exact coordinates of
the fault region the user can either type in the exact coordinates
of the fault region coordinates, or can click on the map to add
such coordinates. The user also has the option for importing
library faults. Once all the fault regions are defined, the NPMT
can simulate the impact of the fault on the selected network.
In Figure 4(b), apart from the user specified fault region,
the boundary of the state of California has been imported
from the fault library and the selected network has been
analyzed for these two fault regions. The updated map shows
the impacted nodes and edges in gray, while the operable
nodes and edges are shown in green and black respectively.
The connected components are shown with a green region. As
seen in Figure 4(b) the menu displays impact statistics such
as, the number of surviving nodes/edges and the number of
connected components. The user is provided with the option
to save the analysis results for later reference, and also the
option to save the defined fault regions for later use.
Path Analyzer
The Path Analyzer allows the user to analyze the network by
computing paths between source and destination nodes that
provide protection against spatially correlated faults. As in the
Fault Analyzer, the Path Analyzer allows the user to specify
a fault feature, and the tool then proceeds to compute paths

(a)
(b)
Fig. 5: Path Analyzer - Region disjoint paths between a source and destination nodes for given fault radius (r) and no-fault zone radius (nfr )
(a) r = 100 km., nfr = 300 km. (b) r = 120 km., nfr = 300 km.

between a given source and destination node pair such that (i)
at least one of the paths survive in the presence of one or more
spatially correlated faults, and (ii) satisfy some other network
resource constraint.
In the current version of the tool the faults considered
are circular faults and the supported fault feature that can
be specified by the user is the fault radius r. The number
of spatially correlated faults considered for path analysis is
one, and the number of paths computed is two, i.e. the NPMT
computes two paths such that if a single circular fault with
radius r occurs anywhere in the network, at least one of the two
paths computed will not be affected by the fault. The network
resource constraint supported is that the sum of lengths of the
two paths computed must be minimum.
It may be noted that a single fault can also render inoperable either the source node, or the destination node, or both,
and thus there always exists a fault region that affects all paths
computed and no region disjoint paths can exist such that at
least one path remains immune to the fault. To accommodate
this case when the source and/or destination nodes themselves
are part of the fault region, the NPMT supports a âNo-Fault
Zoneâ parameter. The user can specify a no-fault zone radius
nfr for both the source and destination nodes that reserves
two circular areas with radius nfr centered at the source and
destination nodes such that network entities, or parts of a
network entity (such as an edge segment), that falls within
this no-fault zone are immune to faults.
Figures 5(a) and 5(b) show screen grabs of the path
analyzer computation for different input values of fault radius
(r). The no-fault zone set to a radius of nfr = 300 km. and
is shown as a white circular region centered at the source and
destination nodes. The computed paths are shown in orange
and blue, and the lengths of each of these to paths are reported
in the left menu. The effect of the path selection criteria, i.e.
the sum of the lengths of the two paths must be minimum,
is also visible in Figures 5(a) and 5(b). In Figure 5(a) when
r = 100 km., the sum of lengths of the two paths is 5793.24
km., however in Figure 5(b) increasing r to 120 km. the
previously computed paths are no longer feasible as a region
fault exists that can impact both these paths. Hence, new paths
are computed and the sum of the new lengths is 5921.69 km.

Traffic and Fault Impact Simulation
For a selected network, the Traffic and Impact Simulator allows
users to generate traffic and faults to analyze the impact of
faults on a load bearing network. To perform this analysis,
a simulation schedule consisting of bandwidth requests and
faults is generated by the NPMT using user provided simulation parameters. Parameters such as total number of time steps
in the schedule, total number of requests in the schedule, minimum/maximum request bandwidth and minimum/maximum
request hold times can be specified by the user. The source and
destination nodes for each request can be generated randomly,
or can be user specified. For introducing faults in the schedule,
the user can specify the number of faults to introduce and can
either specify the exact fault coordinates, or introduce random
circular faults from the set of all possible distinct circular faults
for a specified fault radius. Time intervals of the faults can be
user specified, or can be randomly generated by the NPMT.
Using the request and fault settings, the NPMT then generates
a time stepped simulation schedule of requests and faults. Once
the schedule is finalized, the user can specify the algorithm to
be used in the simulation to route requests from source and
destination nodes, and then proceed to run the simulation.
As shown in the screen grabs of Figures 6(a) and 6(b), the
left menu of the Traffic and Impact Simulator contains the fault
and simulation parameter fields that can be used to generate
the schedule and run the simulation. The tables below the
mapâs dimensions allow the user fine grained control over the
requests and faults that will be simulated. Once the simulation
is complete, for each time interval the network state can be
visualized from the âEvent Simulation Resultsâ table. The user
can click on a row of this table to visualize the network state
on the map for that specific time interval. The user can also
âplayâ the simulation results and the NPMT will iterate over
all the time steps and update the map with the network state
at each step. In Figures 6(a) and 6(b) the impact of a fault
and the corresponding response of the network is shown. In
Figure 6(a) the network is fault free, but in Figure 6(b) a fault
is introduced and an edge is rendered inoperable. It can be
seen that the red and yellow flows of Figure 6(a) are impacted
by the fault, however, as bandwidth is available, in Figure 6(b)
the flows are rerouted in response to this fault.

(a)
(b)
Fig. 6: Traffic and Fault Impact Simulator (a) Pre-Fault network state, (b) Post-Fault network state â rerouted red and yellow flows)

IV.

C ONCLUSION

In this paper we presented a summary of the work done
towards developing a Network Planning and Management Tool
(NPMT), intended to support design and analysis of single
layer and multi-layer networks in the presence of spatially
correlated faults. We highlighted that the NPMT is particularly
suitable for planning and design of critical infrastructures.
We described the underlying novel concepts that have been
developed to enhance robustness of networks in presence of
region based faults, and then described how those concepts
have been incorporated into the NPMT. The goal of this
paper was to bring to the attention of the networking research
community, and to the audience of the workshop on DRCN
in particular, about the existence of NPMT as a tool that
consolidates a large body of work on spatially correlated faults.
To the best of our knowledge no such tool is available today
that supports planning and designing of single layer and multilayer networks in the presence of spatially correlated faults.
R EFERENCES
[1] NEXT Lab, Arizona State University. The Network Planning and
Management Tool. [Online]. Available: http://netsci.asu.edu/networktool/
[2] R. Diestel, Graph Theory. Springer, 2005.
[3] E. Ganesan and D. K. Pradhan, âThe Hyper-deBruijn Networks: Scalable
Versatile Architecture,â IEEE Transactions on Parrallel and Distributed
Systems, vol. 4, no. 9, September 1993.
[4] A. Sen, B. H. Shen, L. Zhou, and B. Hao, âFault-tolerance in Sensor
Networks: A New Evaluation Metric,â in Proceedings of IEEE Infocom,
Barcelona, Spain, April 2006, pp. 1â12.
[5] S. Neumayer and E. Modiano, âNetwork reliability with geographically
correlated failures,â in INFOCOM, 2010 Proceedings IEEE. IEEE,
2010, pp. 1â9.
[6] Y. Cheng, M. T. Gardner, J. Li, R. May, D. Medhi, and J. P. Sterbenz,
âOptimised heuristics for a geodiverse routing protocol,â in 10th International Conference on the Design of Reliable Communication Networks
(DRCN), 2014, 2014, pp. 1â9.
[7] P. Agarwal, A. Efrat, S. Ganjugunte, D. Hay, S. Sankararaman, and
G. Zussman, âThe resilience of wdm networks to probabilistic geographical failures,â in Proceedings of IEEE INFOCOM, 2011.
[8] S. Trajanovski, F. Kuipers, A. Ilic, J. Crowcroft, and P. Van Mieghem,
âFinding critical regions and region-disjoint paths in a network,â
IEEE/ACM Transactions on Networking, vol. 23, no. 3, pp. 908â921,
2015.
[9] S. Banerjee, A. Das, A. Mazumder, Z. Derakhshandeh, and A. Sen, âOn
the impact of coding parameters on storage requirement of region-based
fault tolerant distributed file system design,â in Computing, Networking
and Communications (ICNC), International Conference on. IEEE, 2014,
pp. 78â82.

[10] A. Mazumder, A. Das, C. Zhou, and A. Sen, âRegion based fault-tolerant
distributed file storage system design under budget constraint,â in Reliable Networks Design and Modeling (RNDM), 2014 6th International
Workshop on. IEEE, 2014, pp. 61â68.
[11] A. Sen, A. Mazumder, S. Banerjee, A. Das, C. Zhou, and S. Shirazipourazad, âRegion-based fault-tolerant distributed file storage system
design in networks,â Networks, Wiley Online Library, 2015.
[12] A. Sen, S. Murthy, and S. Banerjee, âRegion-based connectivity-a new
paradigm for design of fault-tolerant networks,â in High Performance
Switching and Routing, 2009. HPSR 2009. International Conference on.
IEEE, 2009, pp. 1â7.
[13] S. Banerjee, S. Shirazipourazad, P. Ghosh, and A. Sen, âBeyond
connectivity-new metrics to evaluate robustness of networks,â in High
Performance Switching and Routing (HPSR), 2011 IEEE 12th International Conference on. IEEE, 2011, pp. 171â177.
[14] S. Banerjee, S. Shirazipourazad, and A. Sen, âDesign and analysis of
networks with large components in presence of region-based faults,â in
International Conference on Communications (ICC). IEEE, 2011.
[15] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
âCatastrophic cascade of failures in interdependent networks,â Nature,
vol. 464, no. 7291, pp. 1025â1028, 2010.
[16] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, âNetworks formed
from interdependent networks,â Nature Physics, vol. 8, no. 1, pp. 40â48,
2011.
[17] J.-F. Castet and J. H. Saleh, âInterdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,â PloS one, vol. 8, no. 4, p. e60402, 2013.
[18] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, âIdentification of k most vulnerable nodes in multi-layered network using a
new model of interdependency,â in NetSciCom Workshop (INFOCOM
WKSHPS), Conference on Computer Communications. IEEE, 2014,
pp. 831â836.
[19] A. Das, J. Banerjee, and A. Sen, âRoot cause analysis of failures in
interdependent power-communication networks,â in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910â915.
[20] J. Banerjee, A. Das, C. Zhou, A. Mazumder, and A. Sen, âOn the entity
hardening problem in multi-layered interdependent networks,â in WIDN
Workshop (INFOCOM WKSHPS), 2015 IEEE Conference on Computer
Communications. IEEE, 2015, pp. 648â653.
[21] A. Das, C. Zhou, J. Banerjee, and A. Sen, âOn the smallest pseudo
target set identification problem for targeted attack on interdependent
power-communication networks,â in Military Communications Conference (MILCOM), IEEE (To appear). IEEE, 2015.
[22] J. Banerjee, C. Zhou, A. Das, and A. Sen, âOn robustness in multilayer interdependent networks,â in Conference on Critical Information
Infrastructures Security (CRITIS) (To appear). Springer, 2015.
[23] A. Mazumder, C. Zhou, A. Das, and A. Sen, âProgressive recovery from
failure in multi-layered interdependent network using a new model of
interdependency,â in Conference on Critical Information Infrastructures
Security (CRITIS). Springer, 2014.
[24] OpenStreetMap Contributors. OpenStreetMap. [Online]. Available:
www.openstreetmap.org

Analysis of On-line Routing and Spectrum Allocation in Spectrum-sliced Optical Networks
Shahrzad Shirazipourazad, Zahra Derakhshandeh and Arunabha Sen
School of Computing, Informatics and Decision System Engineering Arizona State University Tempe, Arizona 85287 Email: {sshiraz1, zderakhs, asen}@asu.edu

Abstract--The orthogonal frequency division multiplexing (OFDM) technology provides an opportunity for efficient resource utilization in optical networks. It allows allocation of multiple sub-carriers to meet traffic demands of varying size. Utilizing OFDM technology, a spectrum efficient and scalable optical transport network called SLICE was proposed recently. The SLICE architecture enables sub-wavelength, super-wavelength resource allocation and multiple rate data traffic that results in efficient use of spectrum. However, the benefit is accompanied by additional complexities in resource allocation. In SLICE architecture, in order to minimize utilized spectrum, one has to solve the routing and spectrum allocation (RSA) problem, a generalization of the routing and wavelength allocation (RWA) problem. In this paper, we focus our attention to the on-line version of RSA problem and provide an algorithm for the ring network with a competitive ratio of min{O(log(dmax )), O(log(k))} where k is the total number of requests and dmax is the maximum demand in terms of the number of sub-carriers. Moreover, we provide a heuristic for the network with arbitrary topology and measure the effectiveness of the heuristic with extensive simulation.

I. I NTRODUCTION It is being increasingly recognized by the optical network designers that in order to meet the challenges posed by the explosive growth of the network traffic, the networks must be operated in the most innovative and efficient manner. The traditional WDM network operates at the granularity of a wavelength, which may lead to inefficient use of resources as some connection requests may not have enough traffic to utilize the full capacity of a wavelength. However such wastage of networking resources can be avoided if the optical network can be made to operate at a finer grain (i.e., subwavelength level) instead of the current practice of course grain operation (i.e., wavelength level). Recent introduction of Orthogonal Frequency Division Multiplexing (OFDM) technology in optical networks [1] offers an opportunity for operating optical networks at a much finer grain than what is currently possible. The advantages offered by the OFDM in terms of flexibility and scalability originate from the unique multicarrier nature of this technology [1]. Utilizing the OFDM technology, a spectrum efficient and scalable optical transport network called spectrum-sliced elastic optical path network (SLICE) was proposed recently [2]. Just as the ability to operate at a granularity finer than a wavelength (i.e., a sub-wavelength) will enable the network operator to manage resources more efficiently, the same is

true if the operator is provided with capability to operate at super-wavelength granularity. Such a capability will be useful for the network operator to meet large traffic demand. The goal of SLICE architecture is to allocate variable sized optical bandwidths that matches with the user traffic demands. It achieves that goal by slicing off spectral resources of a route and allocating only the requested amount to establish an endto-end optical path. Although the sub-wavelength (sub-carrier) level allocation capability of SLICE leads to more effective resource utilization, it also leads to additional complexities in network control and management. First, if a call requests for d sub-carriers, the network controller must allocate d consecutive sub-carriers to this request. Second, if the paths corresponding to two requests R1 and R2 share a fiber link, not only the set of carriers allocated to R1 and R2 must be disjoint, in order to avoid interference, they must be separated from each other in the spectrum domain by a few carriers, known as guard carriers or guard bands. The first and the second constraints are known as the sub-carrier consecutiveness constraint and the guardcarrier constraint respectively [3]. The introduction of the subcarrier consecutiveness constraint significantly increases the complexity of the Routing and Spectrum Assignment (RSA) problem that needs to be solved in SLICE. The RSA problem may be informally defined as follows: Given a network topology and a set of call requests with varying demands (in terms of the number of sub-carriers) find a route for each request and allocate a number of sub-carriers to each request (equal to their requested demand), so that the utilized part of the spectrum span is minimized. It may be noted that if the demand of each request is one sub-carrier, then the RSA problem reduces to the Routing and Wavelength Assignment (RWA) problem, which has been studied extensively. One can conceive of two different versions of the RSA problem - offline and on-line. In the off-line version all the requests are known ahead of time before path and spectrum allocation for any request is carried out. In the on-line version, the requests come in a sequence and path and spectrum allocation for a request has to carried out at the time of arrival of that request. Because the off-line version has the luxury of knowing all the requests, it can carry out better optimization of utilized spectrum span than its on-line counterpart. In this paper we study the on-line version of RSA problem. Previous studies on the on-line version of the RSA problem,

[4]­[9] primarily focus on the development of efficient heuristics for the problem. The effectiveness of these heuristics are primarily evaluated through simulation. To the best of our knowledge, very little analytical results are available in the literature regarding the performance of these heuristics. In this paper we present analytical results relating to the on-line version of the RSA problem when the network topology is a ring. The performance of an on-line algorithm is measured in terms of the metric competitive ratio. In this metric, the performance of an on-line algorithm is compared with the performance of an optimal off-line algorithm that knows the sequence of requests in advance. The maximum ratio between their respective performances, taken over all sequences, is known as the competitive ratio of the algorithm [10]. In this paper, we provide an algorithm for the on-line version of the RSA problem for the ring network with a competitive ratio of min{O(log(dmax )), O(log(k ))} where k is the total number of requests, dmax = max1ik di , and di is the demand in terms of the number of sub-carriers associated with request Ri . Moreover, we provide a heuristic for the network with arbitrary topology and measure the effectiveness of the heuristic with extensive simulation. The rest of the paper is organized as follows. We discuss related works in section II. In section III we introduce definitions and notations. We present problem statement for the on-line RSA problem in section IV. Analytical results for on-line RSA in rings is presented in section V. A heuristic and experimental results for the arbitrary network topology is presented in section VI. Section VII concludes the paper. II. R ELATED W ORK Utilizing the optical OFDM technology, the SLICE architecture proposes a novel scheme for slicing off the spectral resources of a route, resulting in more efficient utilization [2]. The fact that the sub-carriers in the SLICE architecture have to be assigned in a contiguous manner, led to the formulation of the RSA problem. To the best of our knowledge, the RSA problem was originally introduced in [4], [11], [12]. Since then a few other papers, [3], [13] have also studied the RSA problem. In most of these studies [3], [12]­[14], the authors propose an integer linear program based solution and a heuristic solution for the off-line RSA problem. Based on the experimental results, the authors claim effectiveness of their heuristics. The on-line version of RSA problem has been studied in [4]­[9]. In all of these papers, the objective of the on-line RSA problem is to maximize the number of requests that can be satisfied and minimize the blocking probability. In this version of on-line RSA problem, the number of available spectrum sub-carriers is limited. The authors of these papers proposed heuristic solutions mainly by modifying the Dijkstra shortest path algorithm or using K-shortest path algorithm accompanying with the First-Fit algorithm. To the best of our knowledge none of these papers consider the objective of minimizing the utilized spectrum while satisfying all the requests. It may be the case that all the requests should be satisfied while the utilized spectrum is minimized. In this paper

we propose a new heuristic for arbitrary network graphs. We also modify the K-shortest path approach for this version of the on-line RSA and through simulations we evaluate their performance. Most of the studies both on on-line and off-line RSA do not present any analytical results for the RSA problem, even for the simplest optical network topologies such as rings. The ring topology is of particular importance in the optical domain because of its application in metro networks and in some long haul networks. A major thrust of our effort is to present analytical results for the on-line RSA for optical networks with ring structure. III. D EFINITIONS AND N OTATIONS Spectrum Slice/Interval: A number of consecutive sub-carriers from ai to bi denoted by [ai , bi ], that is allocated to a specific request Ri to establish a connection between (si , ti ) with di sub-carriers. The length of this slice is bi - ai + 1 = di . Spectrum Span/Spread: The total amount of spectrum used for allocating a slice to all the requests; If Ri , 1  i  k is allocated the spectrum interval [ai , bi ] then the spectrum span is [ min ai , max bi ].
1ik 1ik

Chromatic Number: The Chromatic Number, (G), of a graph G = (V, E ) is the fewest number of colors necessary to color the nodes of the graph, such that no two adjacent nodes have the same color. Interval Chromatic Number (ICN): Consider a weighted graph G = (V, E, w) with a strictly positive integer weight w(v ) associated with each node v  V . An interval t-coloring of G = (V, E, w) is a function c from V to {1,2, . . . , t} such that c(x) + w(x) - 1  t and if both c(x)  c(y ) and (x, y )  E then c(x) + w(x) - 1 < c(y ). We can view an interval coloring c of G as assigning an interval [c(v ), . . . , c(v ) + w(v ) - 1] of w(v ) consecutive colors to each vertex v so that the intervals of colors assigned to two adjacent vertices (i.e., the pair of nodes that has an edge between them) do not overlap. If interval t-coloring is feasible for a graph G then G is said to be interval t-colorable. The interval chromatic number of G , denoted by int (G ) is the least t such that G has a interval t-coloring [15]. Interval Graph: Let F be a family of non-empty sets. The intersection graph of F is obtained by representing each set in F by a node and connecting the two nodes with an edge, if and only if the corresponding sets intersect. The intersection graph of a family of intervals on a linearly ordered set (such as the real line) is called Interval Graph. Path Intersection Graph: Consider a graph G = (V, E ) and a set of paths P = {P1 , . . . , Pk }, where each Pi is a path between a node pair (si , ti ), i, 1  i  k . A graph G = (V , E ) is a Path Intersection Graph corresponding to P , if each vertex pi  V corresponds to a path Pi  P and two nodes pi and pj in V have an edge between them, if the corresponding paths Pi and Pj in P have at least one common edge in E . IV. P ROBLEM F ORMULATION In this section we provide a formal statement of the on-line routing and spectrum allocation problem.

On-line Routing and Spectrum Allocation (RSA) Problem: A graph G = (V, E ) representing the network topology is given. The connection requests arrive in a sequence one by one where k is the total number of requests. The ith connection request is denoted by a triple Ri = (si , ti , di ), 1  i  k , where si represents a source node, ti represents a destination node, and di represents the demand between si and ti in terms of sub-carriers. Once a request Ri arrives without knowledge of the future requests, assign a path Pi from si to ti and assign a spectrum interval Ii = [ai , bi ] of length di to Pi , such that for every pair of requests i and j, j  i the intervals Ii and Ij do not overlap if the corresponding paths Pi and Pj share an edge between them in G = (V, E ). Moreover, if the paths Pi and Pj overlap, not only the corresponding intervals Ii and Ij must be non-overlapping, these two intervals must be separated by a fixed number of sub-carriers, known as the guard band. The objective is to minimize spectrum span, I = [min1ik ai , max1ik bi ]. Without loss of generality, we number the first available sub-carrier one and the rest are numbered accordingly. We note that guard-band constraint can be satisfied by increasing the demand values by guard-band value g . In other words, in an instance of RSA problem, RSA1 with guard-band g1 > 0 and requests {Ri = (si , ti , di )|1  i  k }, we can increase the demand values in each request by g1 and consider another instance of RSA, RSA2 where guard-band g2 = 0 and for every request Ri in RSA1 , request Ri = (si , ti , di + g1 ) is added to RSA2 . Then the optimal solution of RSA2 can be used to create the optimal solution of RSA1 by removing the last g1 sub-carriers from each spectrum slice assigned to each request (for the proof, reader is referred to the proof of the Observation 4 in [16]). As a result, from this point onward we assume that guard-band is zero. The RSA problem has two distinct components - the routing component and the spectrum allocation component. When routing is given and the paths for the requests are known then interval chromatic number (ICN) of the intersection graph of request paths finds the solution of the SA problem. Let G = (V , E , w) be the weighted path intersection graph of paths of all requests where V = {p1 , p2 , . . . , pk } and each node pi corresponds to the path of request Ri and the weight of pi is di ; i.e., w(pi ) = di . Let int (G ) be the ICN of graph G . In computation of int (G ), each node pi  V is assigned an interval [ai , bi ] of colors with length w(pi ) = di where the intervals of two adjacent vertices do not intersect and total number of distinct colors used is minimum. Therefore, interval [ai , bi ] can be allocated to the path Pi in G and no two paths with common edge intersect in their spectrum intervals. Hence, the spectrum span of int (G ) is sufficient for the spectrum allocation of requests in G with predefined set of paths P . Moreover, int (G ) is the minimum spectrum span needed in the SA problem; otherwise, it contradicts with int (G ) being the minimum interval chromatic number of G . It is known that computation of ICN of interval graphs is NP-complete (Problem SR2 in [17]). Fig. 1 shows an example of SA instance where the network graph is a ring with 8 nodes and requests are {R1 =

1 8 7 2 3

p1 p3 p4

6 5

4

p5

p2

(a)

(b)

Fig. 1. (a) An example of SA instance where the network graph is a ring (b) Path intersection graph G of SA instance in (a)

(1, 3, 15), R2 = (1, 6, 6), R3 = (2, 5, 6), R4 = (2, 8, 6), R5 = (4, 7, 12)}. Dashed lines show the paths for the requests. Fig. 1(b) depicts G , the path intersection graph of these paths where w(p1 ) = 15, w(p2 ) = 6, w(p3 ) = 6, w(p4 ) = 6 and w(p5 ) = 12. In this example, int (G ) is 24 where the requests R1 to R5 are assigned intervals [1, 15], [13, 18], [16, 21], [19, 24] and [1, 12] respectively. V. O N - LINE ROUTING AND S PECTRUM A LLOCATION P ROBLEM IN R INGS Theorem 1: RSA problem (the off-line case) is NPComplete when the optical network topology is a Ring. Proof: If the demands of the requests in the off-line RSA instance are all equal to one, then RSA problem becomes RWA problem. In [18], it is proven that the RWA problem for optical networks with a ring topology is NP-complete. Since RWA problem is a special case of the RSA problem, it follows that the RSA problem for optical networks with a ring topology is also NP-complete. Next, we propose an on-line algorithm for RSA problem when network topology is a ring. In this algorithm, first we use cut-one-link approach and after removing one link the induced graph is a chain. In the chain for every request there exists just one path. Therefore routing is trivial. For the spectrum assignment, we use First-Fit technique that finds the first free spectrum interval fit the demand of the current request. The steps of the algorithms are explained in Algorithm 1. Algorithm 1 On-line RSA in Ring 1: Remove an edge e  E randomly; Let Gp be the induced chain; 2: while A new request arrives do 3: Find the path for the request in graph Gp ; 4: Compute the first free spectrum interval fit the demand of the current request 5: end while Theorem 2: Algorithm 1 has competitive ratio of min{O(log(dmax )), O(log(k ))} where k is total number of requests and dmax = max1ik di . Proof: In order to compute the competitive ratio we need to compare the spectrum span of Algorithm 1 with the optimal spectrum span of off-line RSA where the sequence of requests is known in advance. After removing one edge randomly from G = (V, E ) in Algorithm 1, the induced graph Gp is a chain. Let OP T and OP Tp be the optimal spectrum

span in RSA problem when network graph is G and Gp , respectively, and I be the size of the spectrum computed by Algorithm 1. Clearly, the intersection graph of paths of the requests in Gp is an interval graph (a path from node i to node j in Gp can be interpreted as an interval from i to j ). Let Gp be the path intersection graph. Therefore, minimum spectrum needed to satisfy requests in Gp is equivalent to the int (Gp ). Based on the paper [10], First-Fit algorithm will have competitive ratio of min{O(log(dmax )), O(log(Gp ))} for on-line interval coloring in Gp . Also it is obvious Gp  k (i.e., chromatic number of Gp is at most as large as the number of nodes in Gp that is number of requests). Hence, we have (1) I  min{O(log(dmax )), O(log(k ))} · OP Tp . We denote the set of paths in the optimal solution of RSA (off-line) when network graph is G by POP T . The paths in POP T can be 1 2 1 partitioned into two subsets, Pe and Pe such that Pe is the set 2 of paths that include edge e and the paths in Pe do not include 1 2 edge e. Let OP Te and OP Te be the ICN of the intersection 1 2 graph of paths in Pe and Pe respectively. Then we have (2) 1 2 1 OP T  max(OP Te , OP Te ). Since all the paths in Pe have intersection in edge e, their intervals do not intersect. Clearly, 1 2 (3) OP Tp  OP Te + OP Te . The reason is that in the worst case, all requests that were routed through edge e in POP T are routed the other way in Gp and now they at most need 1 OP Te spectrum span not intersecting the spectrum allocated 2 to the paths in Pe . Therefore, using relations in (2) and (3) we have OP Tp  2OP T . Also, based on relation (1) we can conclude I  min{O(log(dmax )), O(log(k ))} · OP T . VI. A H EURISTIC AND R ESULTS FOR G ENERAL G RAPHS In this section first we present our heuristic for on-line RSA problem in general graphs. Then we present the results of our extensive simulation that demonstrate the efficacy of our heuristic for the on-line RSA problem by comparing it against (i) the optimal solution and (ii) the solution obtained by executing the heuristic based on K -shortest path and FirstFit technique. Minimum Sub-Carrier Path Heuristic (MSCP): The main idea in our heuristic is that it tries to find disjoint paths for routing the requests to increase the reuse of sub-carriers in spectrum allocation. Of this concern, we define a new weight function on the edges (fibers) of the network where weight of an edge e  E , w(e) will be largest sub-carrier number that is used in that edge. We also define the weight of a path, P from node s to node t to be maxeP {w(e)}. For each new request, M SCP selects the path with minimum weight. The minimum weight path can be computed by modifying the distance function in Dijkstra algorithm so that it considers the new weight function as the distance. After finding the path, M SCP uses First-Fit algorithm to find the first available spectrum slice with the length of the request demand in all the edges of the path. Then, M SCP updates the weight of every edge in the path to the largest sub-carrier so far used in that edge. For each request, time complexity of minimumweight path computation is O(|V |2 ) and First-Fit algorithm takes O(k |V |2 ) where k is the number of requests. Hence, time complexity of M SCP is O(k 2 |V |2 ).

K -Shortest Path Heuristic (KSP): In this heuristic, initially K shortest paths are computed between every pair of nodes in the network using [19] algorithm with O(k |V |3 ). When a request Ri arrives, for every path in the K shortest paths between si and ti we compute First-Fit algorithm to find the first available spectrum slice [ai , bi ] with the length of di . Then we select the path whose first available spectrum slice [ai , bi ] has the smallest bi . This algorithm takes O(Kk 2 |V |2 ) for satisfying all the k requests. We perform our experiments on the NSFnet (Fig. 2(a)) and the fiber network of Level-3 that spans Europe (Fig. 3(a)) [20]. We view the NSFnet and Level-3 networks as examples of a small and a large network respectively. In Fig. 2(b), we present the results obtained from ILP , M SCP and KSP when executed on the NSFnet. We find the optimal solution of the RSA problem (off-line) by solving an ILP using the software package CPLEX. Since computing the optimal solution by ILP takes considerable amount of time, we need to do this set of experiments for small number of requests. In this set of experiments, the number of requests, k , is varied from 2 to 6 with step of one. For each value of k , we generate 10 instances. In each instance we generate k requests randomly and consider them one at a time. For this set of experiments all the demand values are at most 5, (i.e., dmax  5). The average spectrum span computed by each of the three methods is shown in Fig. 2(b). It may be observed that the average spectrum span of M SCP is closest to the ILP almost in all cases. The ratio of the average spectrum span of M SCP to ILP is at most 1.28 demonstrating the closeness of the M SCP to the optimal. The results in these experiments also show that M SCP works better than KSP algorithm in almost all the cases even when number of paths in KSP is K = 3. We repeat similar experiments for larger value of k , where k = 10 and we change the value of dmax from 5 to 25. The result of these experiments is depicted in Fig. 2(c). It can be observed that spectrum span in M SCP is at least 12% smaller than the span in KSP where K = 1 and it is even smaller than the one in KSP where K = 2. When K = 3 in KSP , KSP needs smaller spectrum span than M SCP but its time complexity is at least 3 times M SCP . We perform our next set of experiments on the Level-3 network shown in Fig. 3(a). In these experiments, first, we vary k from 10 to 60 with step of 10. For a specific value of k we generate 10 instances. In all these instances, the maximum demand is limited to 10 (i.e., dmax  10). The average utilized spectrum span is shown in Fig. 3(b). These results show that M SCP efficacy with respect to utilized spectrum span is almost the same as KSP when K = 2. We also conduct experiments for the case that values of dmax is varied from 5 to 25 with step of 5, while keeping the number of requests k constant at 20. We compute the average spectrum span over 10 random instances for each value of dmax . The results are shown in Fig. 3(c). According to these results, M SCP 's performance is better than KSP when K = 2 especially for larger values of dmax . According to the last experiments we may conclude that M SCP outperforms KSP when K = 2 for larger values of dmax .

8
Average Spectrum Span
MI WA

50

Average Spectrum Span

NY

7 6 5 4

dmax  5

40

k = 10

CA1

UT NE CO

IL

NJ PA

3
2 1 2 3 4 5 Number of requests (k) 6

MD CA2 TX GA

KSP (K=1) KSP (K=2) KSP (K=3) MSCP ILP 7

30
20 10 0 0 5 10 15 20 25 Maximum demand (dmax) 30 KSP (K=1) KSP (K=2) KSP (K=3) MSCP

(a)

(b)

(c)

Fig. 2. (a) The 14-node NSF Network, (b) The average spectrum span in NSF Network for different values of k where dmax  5, (c) different values of dmax where k = 10
160 140 Average Spectrum Span 180

dmax  10
Average Spectrum Span

160 140 120 100 KSP (K=1) KSP (K=2) MSCP k = 20

120
100 80 60 40

KSP (K=1)
KSP (K=2) MSCP

80
60 40 20

20
0

0

20

40 60 Number of requests (k)

80

0 0 10 20 30 Maximum demand (dmax) 40

(a) Fig. 3.

(b)

(c)

(a) Level-3 network over Europe, (b) The average spectrum span in Level-3 network for dmax  10, (c) k = 20

VII. C ONCLUSION In this paper we study on-line version of Routing and Spectrum Allocation problem in OFDM-based optical networks. We propose an algorithm for the ring network with a competitive ratio of min{O(log(dmax )), O(log(k ))} where k is the total number of requests and dmax is the maximum demand. In addition, we provide a heuristic for networks with arbitrary topology and measure its effectiveness with extensive simulation. In future, we plan to develop efficient algorithms for on-line RSA in networks with tree and grid topologies. We also would like to extend our results to the case that different modulation models can be used. Acknowledgement The research was supported by the DTRA grant HDTRA1-09-1-0032 and the AFOSR grant FA9550-091-0120. R EFERENCES
[1] W. Shieh, "Ofdm for flexible high-speed optical networks," Journal of Lightwave Technology, vol. 29, no. 10, pp. 1560 ­1577, 2011. [2] M. Jinno, H. Takara, B. Kozicki, Y. Tsukishima, Y. Sone, and S. Matsuoka, "Spectrum-efficient and scalable elastic optical path network: architecture, benefits, and enabling technologies," IEEE Communications Magazine, vol. 47, no. 11, pp. 66 ­73, 2009. [3] Y. Wang, X. Cao, and Y. Pan, "A study of the routing and spectrum allocation in spectrum-sliced elastic optical path networks," in INFOCOM, 2011, pp. 1503­1511. [4] M. Jinno, B. Kozicki, H. Takara, A. Watanabe, Y. Sone, T. Tanaka, and A. Hirano, "Distance-adaptive spectrum resource allocation in spectrumsliced elastic optical path network [topics in optical communications]," IEEE Communications Magazine, vol. 48, no. 8, pp. 138 ­145, 2010. [5] K. Christodoulopoulos, I. Tomkos, and E. Varvarigos, "Dynamic bandwidth allocation in flexible ofdm-based networks," in OFC/NFOEC, 2011. [6] X. Wan, N. Hua, and X. Zheng, "Dynamic routing and spectrum assignment in spectrum-flexible transparent optical networks," IEEE/OSA Journal of Optical Communications and Networking, vol. 4, no. 8, pp. 603 ­613, 2012.

[7] T. Takagi, H. Hasegawa, K. Sato, Y. Sone, B. Kozicki, A. Hirano, and M. Jinno, "Dynamic routing and frequency slot assignment for elastic optical path networks that adopt distance adaptive modulation," in OFC/NFOEC, 2011. [8] A. Castro, L. Velasco, M. Ruiz, M. Klinkowski, J. P. Fern´ andez-Palacios, and D. Careglio, "Dynamic routing and spectrum (re)allocation in future flexgrid optical networks," Compututer Networks, vol. 56, no. 12, pp. 2869­2883, 2012. [9] G. Shen and Q. Yang, "From coarse grid to mini-grid to gridless: How much can gridless help contentionless?" in OFC/NFOEC, 2011. [10] M. G. Luby, "Tight bounds for dynamic storage allocation," SIAM Journal on Discrete Mathematics, vol. 9, no. 1, pp. 155­166, Feb. 1996. [11] A. N. Patel, P. N. Ji, J. P. Jue, and T. Wang, "Routing, wavelength assignment, and spectrum allocation in transparent flexible optical wdm (fwdm) networks," in Photonics in Switching, 2010. [12] K. Christodoulopoulos, I. Tomkos, and E. A. Varvarigos, "Routing and spectrum allocation in ofdm-based optical networks with elastic bandwidth allocation," in GLOBECOM, 2010, pp. 1­6. [13] M. Klinkowski and K. Walkowiak, "Routing and spectrum assignment inspectrum sliced elastic optical path network," IEEE Communications Letters, vol. 15, no. 8, pp. 884­886, 2011. [14] K. Christodoulopoulos, I. Tomkos, and E. Varvarigos, "Elastic bandwidth allocation in flexible ofdm-based optical networks," Journal of Lightwave Technology, vol. 29, no. 9, pp. 1354 ­1366, 2011. [15] H. A. Kierstead, "A polynomial time approximation algorithm for dynamic storage allocation," Discrete Mathematics, vol. 87, no. 2-3, pp. 231­237, 1991. [16] S. Shirazipourazad, C. Zhou, Z. Derakhshandeh, and A. Sen. On routing and spectrum allocation in spectrum-sliced optical networks. [Online]. Available: http://www.public.asu.edu/sshiraz1/RSA.pdf [17] M. R. Garey and D. S. Johnson, Computers and Intractability; A Guide to the Theory of NP-Completeness. W. H. Freeman & Co., 1990. [18] T. Erlebach and K. Jansen, "The complexity of path coloring and call scheduling," Theoretical Computer Science, vol. 255, no. 1-2, pp. 33­50, 2001. [19] J. Y. Yen, "Finding the k shortest loopless paths in a network," Management Science, vol. 17, no. 11, pp. 712­716, 1971. [20] Level 3 Communications, Network Map. [Online]. Available: http: //www.level3.com/en/resource-library/maps/level-3-network-map/

Identification of K Most Vulnerable Nodes in
Multi-layered Network Using a New Model of
Interdependency
Arunabha Sen, Anisha Mazumder, Joydeep Banerjee, Arun Das and Randy Compton
Computer Science and Engineering Program

arXiv:1401.1783v1 [cs.NI] 8 Jan 2014

School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {asen, amazumde, Joydeep.Banerjee, adas22, Randy.Compton}@asu.edu

AbstractâThe critical infrastructures of the nation including
the power grid and the communication network are highly
interdependent. Recognizing the need for a deeper understanding
of the interdependency in a multi-layered network, significant
efforts have been made by the research community in the last
few years to achieve this goal. Accordingly a number of models
have been proposed and analyzed. Unfortunately, most of the
models are over simplified and, as such, they fail to capture the
complex interdependency that exists between entities of the power
grid and the communication networks involving a combination of
conjunctive and disjunctive relations. To overcome the limitations
of existing models, we propose a new model that is able to
capture such complex interdependency relations. Utilizing this
model, we provide techniques to identify the K most vulnerable
nodes of an interdependent network. We show that the problem
can be solved in polynomial time in some special cases, whereas
for some others, the problem is NP-complete. We establish that
this problem is equivalent to computation of a fixed point of a
multilayered network system and we provide a technique for its
computation utilizing Integer Linear Programming. Finally, we
evaluate the efficacy of our technique using real data collected
from the power grid and the communication network that span
the Maricopa County of Arizona.

I. I NTRODUCTION
In the last few years there has been an increasing awareness
in the research community that the critical infrastructures of
the nation are closely coupled in the sense that the well being
of one infrastructure depends heavily on the well being of another. A case in point is the interdependency between the electric power grid and the communication network. The power
grid entities, such as the SCADA systems that control power
stations and sub-stations, receive their commands through
communication networks, while the entities of communication
network, such as routers and base stations, cannot operate
without electric power. Cascading failures in the power grid,
are even more complex now because of the coupling between
power grid and communication network. Due to this coupling,
not only entities in power networks, such as generators and
transmission lines, can trigger power failure, communication
network entities, such as routers and optical fiber lines, can
also trigger failure in power grid. Thus it is essential that
the interdependency between different types of networks be
understood well, so that preventive measures can be taken to

avoid cascading catastrophic failures in multi-layered network
environments.
Recognizing the need for a deeper understanding of the
interdependency in a multi-layered network, significant efforts
have been made in the research community in the last few
years to achieve this goal [1], [2], [3], [4], [5], [6], [7], [8].
Accordingly a number of models have been proposed and
analyzed. Unfortunately, many of the proposed models are
overly simplistic in nature and as such they fail to capture
the complex interdependency that exists between power grid
and communication networks. In a highly cited paper [1], the
authors assume that every node in one network depends on one
and only one node of the other network. However, in a follow
up paper [2], the same authors argue that this assumption may
not be valid in the real world and a single node in one network
may depend on more than one node in the other network. A
node in one network may be functional (âaliveâ) as long as
one supporting node on the other network is functional.
Although this generalization can account for disjunctive
dependency of a node in the A network (say ai ) on more
than one node in the B network (say, bj and bk ), implying
that ai may be âaliveâ as long as either bi or bj is alive,
it cannot account for conjunctive dependency of the form
when both bj and bk has to be alive in order for ai to
be alive. In a real network the dependency is likely to be
even more complex involving both disjunctive and conjunctive
components. For example, ai may be alive if (i) bj and bk and
bl are alive, or (ii) bm and bn are alive, or (iii) bp is alive. The
graph based interdependency models proposed in the literature
[3], [4], [5], [9], [6], [7] including [1], [2] cannot capture
such complex interdependency between entities of multilayer
networks. In order to capture such complex interdependency,
we propose a new model using Boolean logic. Utilizing this
comprehensive model, we provide techniques to identify the
K most vulnerable nodes of an interdependent multilayered
network system. We show that the this problem can be solved
in polynomial time for some special cases, whereas for some
others, the problem is NP-complete. We also show that this
problem is equivalent to computation of a fixed point [10] and
we provide a technique utilizing Integer Linear Programming

2

to compute that fixed point. Finally, we evaluate the efficacy
of our technique using real data collected from power grid
and communication networks that span Maricopa County of
Arizona.

II. I NTERDEPENDENCY M ODEL
We describe the model for an interdependent network with
two layers. However, the concept can easily be generalized
to deal with networks with more layers. Suppose that the
network entities in layer 1 are referred to as the A type
entities, A = {a1 , . . . , an } and entities in layer 2 are referred
to as the B type entities, B = {b1 , . . . , bm }. If the layer 1
entity ai is operational if (i) the layer 2 entities bj , bk , bl
are operational, or (ii) bm , bn are operational, or (iii) bp
is operational, we express it in terms of live equations of
the form ai â bj bk bl + bm bn + bp . The live equation for
a B type entity br can be expressed in a similar fashion
in terms of A type entities. If br is operational if (i) the
layer 1 entities as , at , au , av are operational, or (ii) aw , az
are operational, we express it in terms of live equations of
the form br â as at au av + aw az . It may be noted that the
live equations only provide a necessary condition for entities
such as ai or br to be operational. In other words, ai or br
may fail independently and may be not operational even when
the conditions given by the corresponding live equations are
satisfied. A P
live equation
in general will have the following
Ti Qtj
form: xi â j=1
y
k=1 j,k where xi and yj,k are elements
of the set A (B) and B (A) respectively, Ti represents the
number of min-terms in the live equation and tj refers to the
size of the j-th min-term (the size of a min-term is equal to the
number of A or B elements in that min-term). In the example
ai â bj bk bl + bm bn + bp , Ti = 3, t1 = 3, t2 = 2, t3 = 1,
xi = ai , y2,1 = bm , y2,2 = bp .
We refer to the live equations of the form ai â bj bk bl +
bm bn + bp also as First Order Dependency Relations, because
these relations express direct dependency of the A type entities
on B type entities and vice-versa. It may be noted however
that as A type entities are dependent on B type entities,
which in turn depends on A type entities, the failure of
some A type entities can trigger the failure of other A type
entities, though indirectly, through some B type entities. Such
interdependency creates a cascade of failures in multilayered
networks when only a few entities of either A type or B type
(or a combination) fails. We illustrate this with the help of
an example. The live equations for this example is shown in
table I.
Power Network
a1 â b1 + b2
a2 â b1 b3 + b2
a3 â b1 b2 b3
a4 â b1 + b2 + b3

Communication Network
b1 â a1 + a2 a3
b2 â a1 + a3
b3 â a1 a2
ââ

TABLE I: Live equations for a Multilayer Network

Entities
a1
a2
a3
a4
b1
b2
b3

t0
1
0
0
0
0
0
0

t1
1
0
0
0
0
0
1

Time Steps
t2
t3
t4
1
1
1
0
0
1
1
1
1
0
0
1
0
1
1
0
1
1
1
1
1

t5
1
1
1
1
1
1
1

t6
1
1
1
1
1
1
1

TABLE II: Time Stepped Cascade Effect for a Multilayer Network

Fig. 1: Cascading failures reach steady state after p time steps

As shown in table II, the failure of only one entity a1 at
time step t0 triggered a chain of failures that resulted in the
failure of all the entities of the network after by timestep t4 .
A table entry of 1 indicates that the entity is âdeadâ. In this
example, the failure of a1 at t0 triggered the failure of b3 at
t1 , which in turn triggered the failure of a3 at t2 . The failure
of b3 at t1 was due to the dependency relation b3 â a1 a2
and the failure of a3 at t2 was due to the dependency relation
a3 â b1 b2 b3 . The cascading failure process initiated by failure
(or death) of a subset of A type entities at timestep t0 , A0d and
a subset of B type entities Bd0 till it reaches its final steady
state is shown diagrammatically in figure 1. Accordingly, a
multilayered network can be viewed as a âclosed loopâ control
system. Finding the steady state after an initial failure in this
case is equivalent of computing the fixed point of a function
F(.) such that F(Apd âª Bdp ) = Apd âª Bdp , where p represents
the number of steps when the system reaches the steady state.
We define a set of K entities in a multi-layered network
as most vulnerable, if failure of these K entities triggers the
failure of the largest number of other entities. The goal of
the K most vulnerable nodes problem is to identify this set of
nodes. This is equivalent to identifying A0d â A, Bd0 â B, that
maximizes |Apd âªBdp |, subject to the constraint that |A0d âªBd0 | â¤
K.
The dependency relations (live equations) can be formed
either after careful analysis of the multilayer network along the
lines carried out in [8], or after consultation with the engineers
of the local utility and internet service providers.
III. C OMPUTATIONAL C OMPLEXITY AND A LGORITHMS
Based on the number and the size of the min-terms in the
dependency relations, we divide them into four different cases
as shown in Table III. The algorithms for finding the K most
vulnerable nodes in the multilayer networks and computation
complexity for each of the cases are discussed in the following
four subsections.
Case
Case I
Case II
Case III
Case IV

No. of Min-terms
1
1
Arbitrary
Arbitrary

Size of Min-terms
1
Arbitrary
1
Arbitrary

TABLE III: Equation Types for Dependency Relations

3

A. Case I: Problem Instance with One Min-term of Size One
In this case, a live equation in general will have the following form: xi â yj where xi and yj are elements of the set A
(B) and B (A) respectively. In the example ai â bj , xi = ai ,
y1 = bj . It may be noted that a conjunctive implication of
the form ai â bj bk can also be written as two separate
implications ai â bj and ai â bk . However, such cases are
considered in Case II and is excluded from consideration in
Case I. The exclusion of such implications implies that the
entities that appear on the LHS of an implication in Case I
are unique. This property enables us to develop a polynomial
time algorithm for the solution of the K most vulnerable node
problem for this case. We present the algorithm next.
Algorithm 1
Input: (i) A set S of implications of the form of y â x,
where x, y â A âª B, (ii) An integer K.
Output: A set V 0 where |V 0 | = K and V 0 â A âª B such
that failure of entities in V 0 at time step t0 results in failure
of the largest number of entities in A âª B when the steady
state is reached.
Step 1. We construct a directed graph G = (V, E), where
V = A âª B. For each implication y â x in S, where x, y â
A âª B, we introduce a directed edge (x, y) â E.
Step 2. For each node xi â V , we construct a transitive
closure set Cxi as follows: If there is a path from xi to some
node yi â V in G, then we include yi in Cxi . It may be
recalled that |A| + |B| = n + m. So, we get n + m transitive
closure sets Cxi , 1 â¤ i â¤ (n + m). We call each xi to be the
seed entity for the transitive closure set Cxi .
Step 3. We remove all the transitive closure sets which are
proper subsets of some other transitive closure set.
Step 4. Sort the remaining transitive closure sets Cxi ,
where the rank of the closure sets is determined by the
cardinality of the sets. The sets with a larger number of entities
are ranked higher than the sets with a fewer number of entities.
Step 5. Construct the set V 0 by selecting the seed entities
of the top K transitive closure sets. If the number of remaining
transitive closure sets is less than K (say, K0 ), arbitrarily select
the remaining entities.
Time complexity of Algorithm 1: Step 1 takes O(n + m + |S|)
time. Step 2 can be executed in O((n+m)3 ) time. Step 3 takes
at most O((n + m)2 ) time. Step 4 sorts at most |S| entries, a
standard sorting algorithm takes O(|S| log |S|) time. Selecting
K entities in step 5 takes O(K) time. Since |S| â¤ n+m, hence
the overall time complexity is O((n + m)3 )
Theorem 1. For each pair of transitive closure sets Cxi and
Cxj produced in step 2 of algorithm 1, either Cxi â© Cxj = â
or Cxi â© Cxj = Cxi or Cxi â© Cxj = Cxj , where xi 6= xj .
Proof: Consider, if possible, that there is a pair of transitive
closure sets Cxi and Cxj produced in step 2 of algorithm 1,
such that Cxi â©Cxj 6= â and Cxi â©Cxj 6= Cxi and Cxi â©Cxj 6=

Cxj . Let xk â Cxi â© Cxj . This implies that there is a path
from xi to xk (path1 ) as well as there is a path from xj to xk ,
(path2 ). Since, xi 6= xj and Cxi â©Cxj 6= Cxi and Cxi â©Cxj =
Cxj , there is some xl in the path1 such that xl also belongs to
path2 . W.l.o.g, let us consider that xl be the first node in path1
such that xl also belongs to path2 . This implies that xl has
in-degree greater than 1. This in turn implies that there are two
implications in the set of implications S such that xl appears in
the L.H.S of both. This is a contradiction because this violates
a characteristic of the implications in Case I. Hence, our initial
assumption was wrong and the theorem is proven.
Theorem 2. Algorithm 1 gives an optimal solution for the
problem of selecting K most vulnerable entities in a multilayer network for case I dependencies.
Proof: Consider that the set V 0 returned by the algorithm is
not optimal and the optimal solution is VOP T . Let us consider
there is a entity xi â A âª B such that xi â VOP T \ V 0 .
Evidently, (i) Cxi was either deleted in step 3 or (ii) |Cxi | is
less than the cardinalities of all the transitive closure sets with
seed entities xj â V 0 , because our algorithm did not select
xi . Hence, in both cases, replacing any entity xj â V 0 by xi
reduces the total number of entities killed. Thus, the number
of dead entities by the failure of entities in VOP T is lesser than
that caused by the failure of the entities in V 0 , contradicting
the optimality of VOP T . Hence, the algorithm does in fact
return the optimal solution.
B. Case II: Problem Instance with One Min-term of Arbitrary
Size
In this case, a liveQ equation in general will have the
q
following form: xi â k=1 yj where xi and yj are elements
of the set A (B) and B (A) respectively, q represents the size
of min-term. In the example ai â bj bk bl , q = 3, xi = ai ,
y1 = bj , y2 = bk , y3 = bk .
1) Computational Complexity: We show that computation
of K most vulnerable nodes (K-MVN) in a multilayer network
is NP-complete in Case II. We formally state the problem next.
Instance: Given a set of dependency relations between
A
Qqand B type entities in the form of live equations xi â
k=1 yj , integers K and L.
Question: Is there a subset of A and B type entities of
size at most K whose âdeathâ (failure) at time t0 , triggers a
cascade of failures resulting in failures of at least L entities,
when the steady state is reached?
Theorem 3. The K-MVN problem is NP-complete.
Proof: We prove that the K-MVN problem is NP-complete
by giving a transformation for the vertex cover (VC) problem.
An instance of the vertex cover problem is specified by an
undirected graph G = (V, E) and an integer R. We want to
know if there is a subset of nodes S â V of size at most
R, so that every edge has at least one end point in S. From
an instance of the VC problem, we create an instance of the

4

K-MVN problem in the following way. First, from the graph
G = (V, E), we create a directed graph G0 = (V, E 0 ) by
replacing each edge e â E by two oppositely directed edges
e1 and e2 in E 0 (the end vertices of e1 and e2 are same as
the end vertices of e). Corresponding to a node vi in G0 that
has incoming edges from other nodes (say) vj , vk and vl , we
create a dependency relation (live equation) vi â vj vk vl . We
set K = R and L = |V |. The corresponding death equation is
of the form vÂ¯i â vÂ¯j + vÂ¯k + vÂ¯l (obtained by taking negation
of the live equation). We set K = R and L = |V |. It can now
easily be verified that if the graph G = (V, E) has a vertex
cover of size R iff in the created instance of K-MVN problem
death (failure) of at most K entities at time t0 , will trigger a
cascade of failures resulting in failures of at least L entities,
when the steady state is reached.
2) Optimal Solution with Integer Linear Programming:
In this case, we can find and optimal solution to the KMVN problem using Integer Linear Programming (ILP). We
associate binary indicator variables xi (yi ) to capture the state
of the entities ai (bi ). xi (yi ) is 1 when ai (bi ) is dead and
0 otherwise. Since we want find the set of K entities whose
failure at time step t0 triggers cascading failure resulting in the
failure of the largest number of entities, the
the
Pnobjective
Pof
m
ILP can be written as follows maximize
x
+
i
i=1
i=1 yi
It may be noted that the variables in the objective function
do not have any notion of time. However, cascading failure
takes place in time steps, ai triggers failure of bj at time
step t1 , which in turn triggers failure of ak in time step t2
and so on. Accordingly, in order to capture the cascading
failure process, we need to introduce the notion of time into
the variables of the ILP. If the numbers of A and B type
entities are n and m respectively, the steady state must be
reached by time step n + m â 1 (cascading process starts at
time step 0, t0 ). Accordingly, we introduce n + m versions
of the variables xi and yi , i.e., xi [0], . . . , xi [n + m â 1] and
yi [0], . . . , yi [n+mâ1]. To indicate the state of entities ai and
bi at times t0 , . . . , tn+mâ1 . The objective of the ILP is now
changed to
maximize

n
X
i=1

xi [n + m â 1] +

m
X

yi [n + m â 1]

i=1

Subject to the constraint that no more than K entities can
fail at time t0 .
Pn
Pm
Constraint 1:
i=1 yi [0] â¤ K In order
i=1 xi [0] +
to ensure that the cascading failure process conforms to
the dependency relations between type A and B entities,
additional constraints must be imposed.
Constraint 2: If an entity fails at time fails at time step p,
(i.e., tp ) it should continue to be in the failed state at all time
steps t > p. That is xi (t) â¥ xi (t â 1), ât, 1 â¤ t â¤ n + m â 1.
Same constraint applies to yi (t).
Constraint 3: The dependency relation (death equation)
aÂ¯i â bÂ¯j +bÂ¯k +bÂ¯l can be translated into a linear constraint in the
following way xi (t) â¤ yj (tâ1)+yk (tâ1)+yl (tâ1), ât, 1 â¤
t â¤ n + m â 1.

The optimal solution to K-MVN problem for Case II can be
found by solving the above ILP.
C. Case III: Problem Instance with an Arbitrary Number of
Min-terms of Size One
A live equation
Pq in this special case will have the following
form: xi â j=1 yj where xi and yj are elements of the set
A (B) and B (A) respectively, q represents the number of minterms in the live equation. In the example ai â bj + bk + bl ,
q = 3, xi = ai , y1 = bj , y2 = bk , y3 = bl .
1) Computational Complexity: We show that a special
case of the problem instances with an arbitrary number
of min-terms of size one is same as the Subset Cover
problem (defined below), which is proven to be NPcomplete. We define Implication Set(A) Pto be the
Ti
set of all implications of the form ai â
j=1 bj and
ImplicationPSet(B) to be the set of all implications of the
Ti
form bi â
j=1 aj . Now consider a subset of the set of
problem instances with an arbitrary number of min-terms
of size one where either Implication Set(A) = â
or Implication Set(B)
=
â. Let A0
=
{ai |ai is the element on the LHS of an implication}
in the Implication Set(A). The set B 0 is defined
accordingly. If Implication Set(B) = â then B 0 = â. In
this case, failure of any ai , 1 â¤ i â¤ n type entities will not
cause failure of any bj , 1 â¤ j â¤ m type entities. Since an
adversary can cause failure of only K entities, the adversary
would like to choose only those K entities that will cause
failure of the largest number of entities. In this scenario, there
is no reason for the adversary to attack any ai , 1 â¤ i â¤ n type
entities as they will not cause failure of any bj , 1 â¤ j â¤ m
type entities. On the other hand, if the adversary attacks
K bj type entities, not only those K bj type entities will
be destroyed, some ai type entities will also be destroyed
due to the implications in the Implication Set(A). As
such the goal of the adversary will be to carefully choose
K bj , 1 â¤ j â¤ m type entities that will destroy the largest
number of ai type entities. In its abstract form, the problem
can be viewed as the Subset Cover problem.
Subset Cover Problem
Instance: A set S = {s1 , . . . , sm }, a set S of m subsets of S,
i.e., S = {S1 , . . . , Sr }, where Si â S, âi, 1 â¤ i â¤ r, integers
p and q.
Question: Is there a p element subset S 0 of S (p < n) that
completely covers at least q elements of the set S? (A set S 0 is
said to be completely covering an element Si , âi, 1 â¤ i â¤ m
of the set S, if S 0 â© Si = Si , âi, 1 â¤ i â¤ m.)
The set S in the subset cover problem corresponds to the
set B = {b1 , . . . , bm }, and each set Si , 1 â¤ i â¤ r corresponds
to an implication in the ImplicationS et(A) and comprises of
the bj âs that appear on the RHS of the implication. The goal
of the problem is to select a subset B 00 of B that maximizes
the number of Si âs completely covered by B 00 .

5

Theorem 4. The Subset Cover problem is NP-complete.
Proof: We prove that the Subset Cover problem is NPcomplete by giving a transformation from the well known
Clique problem. It may be recalled that an instance of the
Clique problem is specified by a graph G = (V, E) and an
integer K. The decision question is whether or not a clique of
size at least K exists in the graph G = (V, E). We show that
a clique of size K exists in graph G = (V, E) iff the Subset
Cover problem instance has a p element subset S 0 of S that
completely covers at least q elements of the set S.
From an instance of the Clique problem, we create an
instance of the Subset Cover problem in the following way.
Corresponding to every vertex vi , 1 â¤ i â¤ n of the graph
G = (V, E) (V = {v1 , . . . , vn }), we create an element
in the set S = {s1 , . . . , sn }. Corresponding to every edge
ei , 1 â¤ i â¤ m, we create m subsets of S, i.e., S =
{S1 , . . . , Sm }, where Si corresponds to a two element subset
of nodes, corresponding to the end vertices of the edge ei . We
set the parameters p = K and q = K(K â 1)/2. Next we
show that in the instance of the subset cover problem created
by the above construction process, a p element subset S 0 of
S exists that completely covers at least q elements of the set
S, iff the graph G = (V, E) has a clique of size at least K.
Suppose that the graph G = (V, E) has a clique of size
K. It is clear that in the created instance of the subset cover
problem, we will have K(K â 1)/2 elements in the set S,
that will be completely covered by a K element subset of
the set S. The K element subset of S corresponds to the set
of K nodes that make up the clique in G = (V, E) and the
K(K â 1)/2 elements in the set S corresponds to the edges
of the graph G = (V, E) that corresponds to the edges of
the clique. Conversely, suppose that the instance of the Subset
Cover problem has K element subset of S that completely
covers K(K â 1)/2 elements of the set S. Since the elements
of S corresponds to the edges in G, in order to completely
cover K(K â 1)/2 edges, at least K nodes (elements of the
set S) will be necessary. As such, this set of K nodes will
constitute a clique in the graph G = (V, E).
2) Optimal Solution with Integer Linear
Programming: If
Pq
the live equation is in the form xi â k=1 yj then the âdeath
equationâ (obtained by taking negation
of the live equation)
Qq
will be in the product form xÌi â j=1 yÌj . If the live equation
is given as ai â bj + bk , then the death equation will be given
as aÂ¯i â bÂ¯j bÂ¯k .
By associating binary indicator variables xi and yi to
capture the state of the entities ai and bi , we can follow almost
identical procedure as in Case II, with only one exception.
It may be recalled that in Case II, the death equations such
as aÂ¯i â bÂ¯j + bÂ¯k was translated into a linear constraint
xi (t) â¤ yj (t â 1) + yk (t â 1), ât, 1 â¤ t â¤ n + m â 1. However
a similar translation in Case III, with death equations such as
aÂ¯i â bÂ¯j bÂ¯k , will result in a non-linear constraint of the form
xi (t) â¤ yj (t â 1)yk (t â 1), ât, 1 â¤ t â¤ n + m â 1. Fortunately,
a non-linear constraint of this form can be replaced a linear
constraint such as 2xi (t) â¤ yj (t â 1) + yk (t â 1), ât, 1 â¤

t â¤ n + m â 1. After this transformation, we can compute the
optimal solution using integer linear programming.
D. Case IV: Problem Instance with an Arbitrary Number of
Min-terms of Arbitrary Size
1) Computational Complexity: Since both Case II and Case
III are special cases of Case IV, the computational complexity
of finding the K most vulnerable nodes in the multilayer
network in NP-complete in Case IV also.
2) Optimal Solution with Integer Linear Programming:
The optimal solution to this version of the problem can be
computed by combining the techniques developed for the
solution of the versions of the problems considered in Cases
II and III.
IV. E XPERIMENTAL RESULTS
We applied our model to study multilayer vulnerability
issues in Maricopa County, the most densely populated county
of Arizona with approximately 60% of Arizonas population
residing in it. Specifically, we wanted to find out if some
regions of Maricopa County were more vulnerable to failure
than some other regions. The data for our multi-layered
network were obtained from different sources. We obtained
the data for the power network (network A) from Platts
(http://www.platts.com/). Our power network dataset consists
of 70 power plants and 470 transmission lines. Our communication network (network B) data were obtained from GeoTel
(http://www.geo-tel.com/). Our communication network data
consists of 2, 690 cell towers and 7, 100 fiber-lit buildings as
well as 42, 723 fiber links. Snapshots of our power network
data and communication network data are shown in figure 2. In
the power network snapshot of sub-figure(a), the orange markers show locations of powerplants while the yellow continuous
lines represent the transmission lines. In the communication
network snapshot of sub-figure (b) the pink markers show the
location of fiber-lit buildings, the orange markers show the
location of cell towers and the green continuous lines represent
the fiber links. In our dataset, âloadâ in the Power Network is
divided into Cell towers and Fiber-lit buildings. Although there
exists various other physical entities which also draw electric
power and hence can be viewed as load to the power network,
as they are not relevant to our study on interdependency
between power and communication networks, we ignore such
entities. Thus in network A, we have the three types of Power
Network Entities (PNEâs) - Generators, Load (consisting of
Cell towers and Fiber-lit buildings) and Transmission lines
(denoted by a1 , a2 , a3 respectively). For the Communication
Network, we have the following Communication Network
Entities (CNEâs) - Cell Towers, Fiber-lit buildings and Fiber
links (denoted by b1 , b2 , b3 respectively). We consider the
Fiber-lit buildings as a communication network entities as they
house routers which definitely are communication network
entities. From the raw data we construct Implication Set(A)
and Implication Set(B), by following the rules stated below:
Rules: We consider that a PNE is dependent on a set of
CNEs for being in the active state (âaliveâ) or being in the

6

(a) Snapshot of Power Network in Maricopa County

(b) Snapshot of Communication Network in Maricopa County

Fig. 2: Snapshots of power network and communication network in Maricopa County)

inactive state (âdeadâ). Similarly, a CNE is dependent on a set
of PNEs for being active or inactive state. For simplicity we
consider the live equations with at most two minterms. For
the same reason we consider the size of each minterm is at
most two.

of the number of entities of the two networks A and B. Most
importantly, we find that the degree of vulnerability of all
the five regions considered in our study are close and no one
region stands out as being extremely vulnerable.

Generators (a1,i , 1 â¤ i â¤ p, where p is the total number
of generators): We consider that each generator (a1.i ) is
dependent on the nearest Cell Tower (b1,j ) or the nearest
Fiber-lit building (b2,k ) and the corresponding Fiber link (b3,l )
connecting b2,k and a1,i . Thus, we have
a1,i â b1,j + b2,k Ã b3,l
Load (a2,i , 1 â¤ i â¤ q, where q is the total number of loads):
We consider that the loads in the power network do not depend
on any CNE.
Transmission Lines (a3,i , 1 â¤ i â¤ r, where r is the total number of transmission lines): We consider that the transmission
lines do not depend on any CNE.
Cell Towers (b1,i , 1 â¤ i â¤ s, where s is the total number
of cell towers): We consider the cell towers depend on the
nearest pair of generators and the corresponding transmission
line connecting the generator to the cell tower. Thus, we have
b1,i â a1,j Ã a3,k + a1,j 0 Ã a3,k0
Fiber-lit Buildings (b2,i , 1 â¤ i â¤ t, where t is the total number
of fiber-lit buildings): We consider that the fiber-lit buildings
depend on the nearest pair of generators and the corresponding
transmission lines connecting the generators to the cell tower.
Thus, we have b2,i â a1,j Ã a3,k + a1,j 0 Ã a3,k0
Fiber Links (b3,i , 1 â¤ i â¤ u, where u is the total number of
fiber links)): We consider that the fiber links do not depend
on any PNE.
Because of experimental resource limitation, we have considered 5 regions of Maricopa County for our experiments.
We used IBM CPLEX Optimizer 12.5 to run the formulated
ILPâs on the experimental dataset. We show our results in
the figure 3. We observe that in each of the regions there
is a specific budget threshold beyond which each additional
increment in budget results in the death of only one entity. The
reason for this behavior is our assumption that entities such
as the transmission lines and the fiberlinks are not dependent
on any other entities. We notice that all the entities of the
two networks can be destroyed with a budget of about 60%

Fig. 3: Experimental results of failure vulnerability across five regions
of Maricopa county

R EFERENCES
[1] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
âCatastrophic cascade of failures in interdependent networks,â Nature,
vol. 464, no. 7291, pp. 1025â1028, 2010.
[2] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, âNetworks formed
from interdependent networks,â Nature Physics, vol. 8, no. 1, pp. 40â48,
2011.
[3] J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, âCascade of
failures in coupled network systems with multiple support-dependence
relations,â Physical Review E, vol. 83, no. 3, p. 036116, 2011.
[4] V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and
R. Setola, âModelling interdependent infrastructures using interacting
dynamical models,â International Journal of Critical Infrastructures,
vol. 4, no. 1, pp. 63â79, 2008.
[5] P. Zhang, S. Peeta, and T. Friesz, âDynamic game theoretic model of
multi-layer infrastructure networks,â Networks and Spatial Economics,
vol. 5, no. 2, pp. 147â178, 2005.
[6] M. Parandehgheibi and E. Modiano, âRobustness of interdependent
networks: The case of communication networks and the power grid,â
arXiv preprint arXiv:1304.0356, 2013.
[7] D. T. Nguyen, Y. Shen, and M. T. Thai, âDetecting critical nodes in
interdependent power networks for vulnerability assessment,â 2013.
[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman,
âPower grid vulnerability to geographically correlated failures-analysis
and control implications,â arXiv preprint arXiv:1206.1099, 2012.
[9] J.-F. Castet and J. H. Saleh, âInterdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,â PloS one, vol. 8, no. 4, p. e60402, 2013.
[10] A. Fudenberg and J. Tirole, Game Theory. Ane Books, 2010.

On the Entity Hardening Problem in Multi-layered
Interdependent Networks
Joydeep Banerjee, Arun Das, Chenyang Zhou, Anisha Mazumder and Arunabha Sen

arXiv:1412.6686v1 [cs.NI] 20 Dec 2014

Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {joydeep.banerjee, arun.das, czhou24, anisha.mazumder, asen}@asu.edu
AbstractâThe power grid and the communication network
are highly interdependent on each other for their well being.
In recent times the research community has shown significant
interest in modeling such interdependent networks and studying
the impact of failures on these networks. Although a number
of models have been proposed, many of them are simplistic in
nature and fail to capture the complex interdependencies that
exist between the entities of these networks. To overcome the
limitations, recently an Implicative Interdependency Model that
utilizes Boolean Logic, was proposed and a number of problems
were studied. In this paper we study the âentity hardeningâ
problem, where by âentity hardeningâ we imply the ability of the
network operator to ensure that an adversary (be it Nature or
human) cannot take a network entity from operative to inoperative
state. Given that the network operator with a limited budget
can only harden k entities, the goal of the entity hardening
problem is to identify the set of k entities whose hardening will
ensure maximum benefit for the operator, i.e. maximally reduce
the ability of the adversary to degrade the network. We show
that the problem is solvable in polynomial time for some cases,
whereas for others it is NP-complete. We provide the optimal
solution using ILP, and propose a heuristic approach to solve the
problem. We evaluate the efficacy of our heuristic using power
and communication network data of Maricopa County, Arizona.
The experiments show that our heuristic almost always produces
near optimal results.

I.

I NTRODUCTION

The critical infrastructures of a nation form a complex symbiotic ecosystem where individual infrastructures are heavily
interdependent on each other for being fully functional. Two
such critical systems that rely heavily on each other for their
well being are the power and communication network infrastructures. For instance, power grid entities such as SCADA
systems, that are used to remotely operate power generation
units, receive their control commands over the communication
network infrastructure, while communication network entities
such as routers and base stations are inoperable without electric
power. Thus, failure introduced in the system either by Nature
(hurricanes), or man (terrorist attacks), can trigger further
failures in the system due to interdependencies between the
entities of the two infrastructures.
Although a number of models have been proposed for modeling and analysis of interdependent multi-layered networks
[1], [2], [3], [4], [5], [6], [7], [8], many of these models are simplistic in nature and fail to capture the complex interdependencies that exists between the entities of these networks. As noted
in [9], these models fail to model complex interdependencies

that may exist between network entities, such as when entity ai
is operational, if entities (i) bj and bk and bl are operational, or
(ii) bm and bn are operational, or (iii) bp is operational. Graph
based interdependency models proposed in the literature such
as [3], [4], [5], [10], [6], [7] including [1], [2] cannot capture
such complex interdependency involving both conjunctive and
disjunctive terms between entities of multi-layer networks. To
overcome these limitations, an Implicative Interdependency
Model that utilizes Boolean Logic, was recently proposed in
[9], and a number of problems including computation of K
most vulnerable nodes [9], root cause of failure analysis [11],
and progressive recovery from failures [12], were studied using
this model.
In this paper we study the âentity hardeningâ problem in
the interdependent power-communication network using the
Implicative Interdependency Model (IIM). By âentity hardeningâ, we imply the ability of the network operator to ensure
that an adversary (be it Nature or human), cannot take a network entity from an operative (operational) to an inoperative
(failed) state. We assume that the adversary is clever and
is capable of identifying the most vulnerable entities in the
network that causes maximum damage to the interdependent
system. However, the adversary does not have an unlimited
budget and has the resources to destroy at most K entities
of the interdependent network. The network operator is also
aware of adversaryâs target entities for destruction. Since we
assume that once an entity is âhardenedâ by the network
operator it cannot be destroyed by the adversary, if all K
targets of the adversary are hardened by the network operator,
then the adversary cannot induce any failure in the network.
However, if due to resource limitations the network operator
is able to strengthen only k entities, where k < K, these k
entities have to be carefully chosen. The goal of the entity
hardening problem is to identify the set of k entities whose
hardening will ensure maximum benefit for the operator, i.e.
maximally reduce the ability of the adversary to degrade the
network.
We classify the entity hardening problem into four different
cases depending on the nature of the interdependency relationships. We show that the first case can be solved in polynomial
time, and all other cases are shown to be NP-complete. We
provide an inapproximability result for the second case, an
approximation algorithm for the third case, and a heuristic
for the fourth (general) case. We evaluate the efficacy of our
heuristic using power and communication network data of
Maricopa County, Arizona. The experiments show that our

2

heuristic almost always produces near optimal results.
The paper is organized as follows, the IIM model is
presented in Section II, in Sections III and IV we formally
state the entity hardening problem and analyze its computational complexity, Section V outlines the optimal and heuristic
solutions to the problem, Section VI shows the experimental
results, and finally Section VII concludes this paper.
II.

I NTERDEPENDENCY M ODEL

We now present an overview of the underlying IIM interdependency model [9]. IIM uses Boolean Logic to model
the interdependencies between network entities, these interdependent relationships are termed as Implicative Interdependency Relations (IDRs). We represent this interdependent network setting as I(A, B, F (A, B)), where sets A
and B are the power and communication network entities
respectively, and F (A, B) is the set of dependency relations,
or IDRs. Table I represents a sample interdependent network I(A, B, F (A, B)), where A = {a1 , a2 , a3 , a4 }, B =
{b1 , b2 , b3 } and F (A, B) is the set of IDRs (dependency
relations) between the entities of A and B. In this example,
the IDR b1 â a1 a3 + a2 implies that entity b1 is operational
when both the entities a1 and a3 are operational, or entity a2
is operational. The conjunction of entities, such as a1 a3 , is
also referred to as a minterm.
Power Network
a1 â b1 b2
a2 â b1 + b2
a3 â b1 + b2 + b3
a4 â b1 + b3

Comm. Network
b1 â a1 a3 + a2
b2 â a1 a2 a3
b3 â a1 + a2 + a3
ââ

TABLE I: Implicative Interdependency Relations of a sample network

Given a set of inoperable (failed) entities, a time stepped
failure cascade can be derived from the dependency relationships outlined in the IDR set. For example, for the interdependent network outlined in Table I, Table II shows the failure
propagation when entities {a2 , b3 } fail at the initial time step
(t = 0). It may be noted that the model assumes that dependent
entities fail immediately in the next time step, for example,
when {a2 , b3 } fail at t = 0, b2 fails at t = 1 as b2 is dependent
on a2 for its survival. The system reaches a steady state when
the failure propagation process stops. In this example, when
{a2 , b3 } fail at t = 0, the steady state is reached at time step
t = 4.
Entities
a1
a2
a3
a4
b1
b2
b3

0

1

0
1
0
0
0
0
1

0
1
0
0
0
1
1

Time Steps (t)
2
3
4
1
1
0
0
0
1
1

1
1
0
0
1
1
1

1
1
1
1
1
1
1

5

6

1
1
1
1
1
1
1

1
1
1
1
1
1
1

TABLE II: Failure cascade propagation when entities {a2 , b3 } fail at
time step t = 0. A value of 1 denotes entity failure, and 0 otherwise

A primary consideration for using this model is the accurate
formulation of the IDRs that is representative of the underlying
physical power and communication network infrastructures.
This can either be done by careful analysis as done in [8], or
by consultation with experts of these infrastructures. We utilize
IIM to model the interdependency between the two networks
and analyze the entity hardening problem in this setting.

III.

P ROBLEM F ORMULATION

Before we make a formal statement of the entity hardening
problem in the IIM setting, we explain it with the help of an
example. Consider an interdependent system as outlined in the
IDR set shown in Table I. It may be easily checked that when
the adversary budget is K= 2, the most vulnerable entities
of this system are {a2 , b3 }. If the network operator doesnât
harden any one of the entities a2 or b3 , then in this example
all the network entities eventually fail, as seen from the fault
propagation in Table II. When the network operator chooses
to harden both a2 and b3 then none of the entities in the
network fail if the adversary restricts the attack only to the two
most vulnerable entities of the network, which in this example
happens to be {a2 , b3 }. If the network operator has resources
to harden only one entity and the operator chooses to harden
a2 , the destruction of b3 by the adversary will eventually lead
to the failure of no other entities of the network, as shown in
Table III(a). If on the other hand, the network operator chooses
to harden b3 , destruction by the adversary of a2 will eventually
lead to the failure of the entities {a2 , b2 , a1 , b1 } as shown in
Table III(b). Clearly in this scenario the operator should harden
a2 instead of b3 .
Definition: Kill Set of a set of Entities(S): The kill set of a
set of entities S, is the set of all entities that will eventually
fail due to failure of S and the interdependencies between the
entities of the network as given by the set of IDRâs. The kill
set of a set of entities S is denoted by KillSet(S).
It may be noted that the search for k entities to be hardened
is restricted to the KillSet(S), where S is the set of K
most vulnerable entities in the network, because hardening any
entity not in KillSet(S) does not provide any benefit to the
network operator. In this study we also assume that the set of
K most vulnerable entities in the network is unique.
Entities
0
a1
a2
a3
a4
b1
b2
b3

0
â
0
0
0
0
1

Time Steps (t)
1
2
3
0
â
0
0
0
0
1

0
â
0
0
0
0
1

0
â
0
0
0
0
1

(a) Entity a2 is hardened

Entities
4
0
â
0
0
0
0
0

0
a1
a2
a3
a4
b1
b2
b3

0
1
0
0
0
0
â

Time Steps (t)
1
2
3
0
1
0
0
0
1
â

1
1
0
0
0
1
â

1
1
0
0
1
1
â

4
1
1
0
0
1
1
â

(b) Entity b3 is hardened

TABLE III: Failure cascade propagation with entity hardening. Entities {a2 , b3 } are attacked at time step t = 0. A value of 1 denotes
entity failure, 0 otherwise. â denotes a hardened entity.

We now proceed to formulate the entity hardening
problem formally. Given an interdependent network system
I(A, B, F (A, B)), and the set of K most vulnerable entities
of the system Aâ² âª B â² , where Aâ² â A and B â² â B:
The Entity Hardening (ENH) problem
INSTANCE: Given:
(i) An interdependent network system I(A, B, F (A, B)),
where the sets A and B represent the entities of the two
networks, and F (A, B) is the set of IDRs.
(ii) The set of K most vulnerable entities of the system
Aâ² âª B â² , where Aâ² â A and B â² â B
(iii) Two positive integers k, k < K and EF .

3

QUESTION:Is there a set of entities H = Aâ²â² âª B â²â² , Aâ²â² â
A, B â²â² â B, |H| â¤ k, such that hardening H entities results
in no more than EF entities to fail after entities Aâ² âª B â² fail
at time step t = 0.
We note some of the assumptions for the ENH problem:
First, we assume that once an entity is hardened, it is always
operational and does not fail at any time step of the observation, even when the entity is part of the K most vulnerable
entities. Second, we assume that k < K, as otherwise the
selection of K entities for hardening ensures that no entities
fail at all. Finally, as noted earlier, we assume that the set of
K most vulnerable entities in the network is unique. We now
proceed to analyze the computational complexity of the ENH
problem.
IV.

C OMPUTATIONAL C OMPLEXITY A NALYSIS

For an interdependent network I(A, B, F (A, B)) the IDRs
can be represented in four different forms. We analyze the
computational complexity of the ENH problem for each of
these cases separately.
A. Case I: Problem Instance with One Minterm of Size One
The IDRs of Case I have a single minterm of size 1. This
can be represented as xi â yj , where xi and yj are entities of
network A(B) and B(A) respectively. We show that the ENH
problem for Case I can be solved optimally in polynomial time.
Algorithm 1: Entity Hardening Algorithm for systems
with Case I type interdependencies

1
2
3
4
5
6
7

8
9
10

Data: An interdependent network I(A, B, F(A, B)), set of
K most vulnerable entities Aâ² âª B â² , Aâ² â A, B â² â B,
hardening budget k and a set H = â.
Result: Set of hardened entities H.
begin
For each entity xi â (Aâ² âª B â² ) compute the set of kill sets
C = {Cx1 , Cx2 , ..., CxK }, where Cxi = KillSet(xi ) ;
Create a copy D = {Dx1 , Dx2 , ..., DxK } of set C ;
for (i=1; i â¤ K; i++) do
for (j=1, j 6= i; j â¤ K; j++) do
if Cxj â Cxi then
Dxi â Dxi \ Dxj ;
Choose the top k sets from D with highest cardinality ;
For each of the Dxi â D sets chosen in Step 8,
H â H âª xi ;
return H

Theorem 1. Algorithm 1 solves the Entity Hardening problem
for Case I optimally in polynomial time.
Proof: It is shown in [9] that the kill set for all entities in
the interdependent network can be computed in O(n3 ) where
n = |A| + |B|, thus computing the kill sets for K entities takes
O(Kn2 ). Step 4-7 of the algorithm runs in O(K2 ). Choosing
the k highest cardinality sets can be found using any standard
sorting algorithm in O(Klog(K)). Hence Algorithm 1 runs in
O(Kn2 ).
For two kill sets Cxi and Cxj it can be shown that either
Cxi â© Cxj = â or Cxi â© Cxj = Cxi or Cxi â© Cxj = Cxj

[9]. So with two entities {xi , xj } â Aâ² âª B â² and Cxi â© Cxj =
Cxj i.e, Cxj â Cxi , if xi is hardened it prevents the failure
of Cxi â Cxj entities (provided that none of the entities in
Cxi â Cxj â {xi } are in Aâ² âª B â² ). With this assertion, for
an entity xi â Aâ² â© B â² , steps 4-7 of Algorithm 1 finds the
actual entities for which failure is prevented by hardening xi .
The set D = {Dx1 , Dx2 , ..., DxK } comprises of these set of
entities for each hardened entity xi .
To prove that Algorithm 1 finds the optimal solution we
make the following two assertions: First, consider any two sets
Dxi and Dxj . It is implied from step 6 of Algorithm 1 that
/ Aâ² âª B â² is
Dxi â© Dxj = â. Second, consider an entity xp â
hardened. If xp fails when entities in Aâ² âªB â² fails initially then
it would belong to some set Dxi . Thus hardening xp results
in preventing the failure of entities that is a proper subset of
Dxi . Hence the entities to be hardened must belong to Aâ² âª B â²
only. Owing to the two assertions it directly follows that with
a given budget k, hardening k highest cardinality sets from the
set D ensures prevention of failure for the maximum number
of entities.
B. Case II: Problem Instance with One Minterm of Arbitrary
Size
The IDRs of Case II have a single
Qpminterm of arbitrary
size. This can be represented as xi â j=1 yj , where xi and
yj are entities of network A(B) and B(A) respectively and the
size of the minterm is p. The Entity Hardening problem with
respect to Case II is NP-complete and is proved in Theorem
2. An inapproximability proof for this case of the problem is
given in Theorem 3
Theorem 2. The Entity Hardening problem for Case II is NP
Complete
Proof: The Entity Hardening problem for case II is proved
to be NP complete by giving a reduction from the Densest pSubhypergraph problem [13], a known NP-complete problem.
An instance of the Densest p-Subhypergraph problem includes
a hypergraph G = (V, E), a parameter p and a parameter M .
The problem asks the question whether there exists a set of
vertices |V â² | â V and |V â² | â¤ p such that the subgraph induced
with this set of vertices has at least M hyperedges. From an
instance of the Densest p-Subhypergraph problem we create
an instance of the ENH problem in the following way. For
each vertex vi and each hyperedge ej an entity bi and aj are
added to the set B and A respectively. For each hyperedge ej
with ej = {vm , vn , vq } (say) an IDR of form aj â bm bn bq is
created. It is assumed that the value of K is set of |V |. The
values of k and EF are set to p and |V | + |E| â p â M (where
|A| = |V | and |B| = |E|) respectively.
In the constructed instance only entities of set A are
dependent on entities of set B. Additionally the dependency
for an entity ai consists of conjunction of entities in set B.
Hence for an entity ai â A to fail, either it itself has to fail
initially or all entities to which ai is dependent on has to fail.
It is to be noted that the entities in set B has no induced failure
i.e., there is no cascade. Following from this assertion, with
K = p, the solution Aâ² = â and B â² = B would fail all entities
in set A âª B. Moreover this is the single unique solution to
the problem instance. This is because by including one entity

4

ai in the initial failure set would result in not failing at least
one entity bj for a given budget K = p. Hence it wonât fail
the entire set of entities in A âª B.
If an entity in set A is hardened then it would have no effect
in failure prevention of any other entities. Whereas hardening
an entity bm â B might result in failure prevention of an entity
ai â A with IDR aj â bm bn bq provided that entities bn , bq
are also defended. With k = p (and K â¤ |V | = |B|) it can be
ensured that entities to be defended are from set B â² .
To prove the theorem consider that there is a solution to the
Densest p-Subhypergraph problem. Then there exist p vertices
which induces a subgraph which has at least M hyperedges.
Hardening the entities bi â B â² for each vertex vi in the solution
of the Densest p-Subhypergraph problem would then ensure
that at least M entities in set A are protected from failure.
This is because the entities in set A for which the failure
is prevented corresponds to the hyperedges in the induced
subgraph. Thus the number of entities that fail after hardening
p entities is at most |V | + |E| â p â M , solving the ENH
problem. Now consider that there is a solution to the ENH
problem. As previously stated, the entities to be hardened will
always be from set B â² . So defending p entities from set B â²
would result in failure prevention of at least M entities in set
A such that EF â¤ |V | + |E| â p â M . Hence, the vertex
induced subgraph would have at least M hyperedges when
vertices corresponding to the entities hardened are included
in the solution of the Densest p-Subhypergraph problem, thus
solving it.
Theorem 3. For an interdependent network I(A, B, F (A, B))
with n = |A âª B| and F (A, B) having IDRs of form Case II,
it is hard to approximate the ENH problem within a factor of
1
for some Î» > 0.
log(n)Î»
2

Proof: From Theorem 2, Densest p-Subhypergraph problem has been shown to be a special case of the ENH problem
with IDRs of form Case II. Densest p-Subhypergraph problem
1
is proved to be inapproximable within a factor of log(n)
Î»
2
(Î» > 0) in [13]. Hence the theorem follows.
C. Case III: Problem Instance with an Arbitrary Number of
Minterm of Size One
The IDRs of Case III have arbitrary number
P of minterm of
size 1. This can be represented as xi â pq=1 yq , where xi
and yq are entities of network A(B) and B(A) respectively
and the number of minterms are p. The ENH problem with
respect to Case III is NP-complete and is proved in Theorem
4.
Theorem 4. The ENH problem for Case III is NP Complete
Proof: The ENH problem for case III is proved to be NP
complete by giving a reduction from the Set Cover Problem,
a well known NP-complete problem. An instance of the Set
Cover problem includes a set S = {x1 , x2 , ..., xn }, a set S =
{S1 , S2 , ..., Sm } where Si â S and a positive integer M . The
problem asks the question whether there exists at most M
subsets from set S whose union would result in the set S. From
an instance of the set cover problem we create an instance of
the ENH problem in the following way. For each element xi
in set S we add an entity ai in set A. For each subset Si in

set S we add an entity bi in set B. For all subsets in S, say
Sp , Sm , Sn , which has the element xi there is an IDR of form
ai â bm + bn + bl . The values of positive integers k and EF
are set to M and m â M respectively. It is assumed that the
value of K = m.
With similar reasoning as that of Case II it can be shown
that for K = m the maximum number of node failures (i.e.
failure of all entities in A âª B) would occur if Aâ² = â and
B â² = B. This is also the single unique solution to the problem
instance.
The constructed instance also ensures that the entities to
be hardened are from set B â² (Aâ² not considered as it is equal
to â). This is because protecting an entity ai â A would only
result in prevention of its own failure whereas protecting an
entity bj â B would result in failure prevention of its own and
all other entities in set A for which it appears in its IDR.
To begin with the proof, consider that there is a solution
to the Set Cover problem. Then there exist M subsets (or
elements in set S) whose union results in the set S. Hardening
the entities in set B corresponding to the subsets selected
would ensure that all entities in set A are prevented from
failure. This is because for the dependency of each entity
ai â A there exist at least one entity (in set B) that is hardened.
Hence the number of entities that fails after hardening is mâM
which is equal to EF , thus solving the ENH problem. Now,
consider that there is a solution to the ENH problem. As
discussed above the entities to be hardened should be from
set B â² . To achieve EF = m â M with k = M , no entities
in the set A must fail. Hence for each entity ai â A at least
one entity in set B that appears in its IDR has to be hardened.
Thus, it directly follows that the union of subsets in set S
corresponding to the entities hardened is equal to the set S,
solving the Set Cover Problem.
1) Approximation Scheme for Case 3: In this subsection we
provide an approximation algorithm for Case 3 of the problem.
For an interdependent network I(A, B, F (A, B)) with the
initial failed set of entities as Aâ² âª B â² we define Protection
Set of each entity as follows.
Definition: For an entity xi â A âª B the Protection Set is
defined as the entities that would be prevented from failure
by hardening the entity xi when all entities in Aâ² âª B â² fails
initially. This is represented as P (xi |Aâ² âª B â² ).
The Protection Set of each entity can be computed in
O((n + m)2 ) where n and m are the number of entities and
number of minterms respectively in an interdependent network
I(A, B, F (A, B)) .
Theorem 5. For two entities xi , xj â A âª B, P (xi |Aâ² âª B â² ) âª
P (xj |Aâ² âª B â² ) = P (xi , xj |Aâ² âª B â² ) when IDRs are of form
Case III.
Proof: Assume that defending two entities xi and xj
would result in preventing failure of P (xi , xj |Aâ² âª B â² ) entities
with |P (xi |Aâ² âª B â² ) âª P (xj |Aâ² âª B â² )| < |P (xi , xj |Aâ² âª B â² )|.
Then there exist at least one entity xp â
/ P (xi |Aâ² âª B â² ) âª
P (xj |Aâ² âª B â² ) such that itâs failure is prevented only if xi and
xj is protected together. So two entities xm and xn (with xm â
P (xi |Aâ² âªB â² ) and xn â P (xj |Aâ² âªB â² ) or vice versa) have to be

5

present in the IDR of xp . As the IDRs are of form Case III so if
any one of xm or xn is protected then xp is protected, hence
a contradiction. On the other way round P (xi , xj |Aâ² âª B â² )
contains all entities which would be prevented from failure
if xi or xj is defended alone. So it directly follows that
|P (xi |Aâ² âª B â² ) âª P (xj |Aâ² âª B â² )| > |P (xi , xj |Aâ² âª B â² )| is
not possible. Hence the theorem holds.
Theorem 6. There exists an 1 â 1e approximation algorithm
that approximates the ENH problem for Case III.
Proof: The approximation algorithm is constructed by
modeling the problem as Maximum Coverage problem. An
instance of the maximum coverage problem consists of a
set S = {x1 , x2 , ..., xn }, a set S = {S1 , S2 , ..., Sm } where
Si â S and a positive integer M . The objective of the problem
is to find a set S â² â S and |S â² | â¤ M such that âªSi âS Si
is maximized. For a given initial failure set Aâ² âª B â² with
|Aâ² |+|B â² | â¤ K, let P (xi |Aâ² âªB â² ) denote the protection set for
each entity xi â A âª B. We construct a set S = A âª B and for
each entity xi a set Sxi â S such that Sxi = P (xi |Aâ² âª B â² ).
Each set Sxi is added as an element of a set S. The conversion
of the problem to Maximum Coverage problem can be done
in polynomial time. By Theorem 5 defending a set of entities
X â S would result in failure prevention of âªxi âX Sxi entities.
Hence, with the constructed sets S and S and a positive integer
M (with M = k) finding the Maximum Coverage would
ensure the failure protection of maximum number of entities in
A âª B. This is same as the ENH problem of Case III. As there
exists an 1 â 1e approximation algorithm for the Maximum
Coverage problem hence the theorem holds.
D. Case IV: Problem Instance with an Arbitrary Number of
Minterms of Arbitrary Size
The IDRs of Case IV have arbitrary number of minterm
of
Pp arbitrary
Qqj1 size. This can be represented as xi â
j2 =1 yj2 , where xi and yj2 are entities of network
j1 =1
A(B) and B(A) respectively and there are p minterms each
of size qj1 .
Theorem 7. The Entity Hardening problem for Case IV is NP
Complete
Proof: Case II and Case III are special cases of Case
IV. Hence following from Theorem 2 and Theorem 4 the
computational complexity of the Entity Hardening problem is
NP-complete in Case IV.
V.

S OLUTIONS TO

THE

E NTITY H ARDENING P ROBLEM

A. Optimal Solution using Integer Linear Programming
We propose an Integer Linear Program (ILP) that solves
the Entity Hardening problem optimally. Let [G, H] with
G = {g1 , g2 , ..., gn } and H = {h1 , h2 , ..., hm } denote the
entities in set A and B respectively with hi = 0 (gj = 0)
if entity ai (bj ) is alive and hi = 1 (gj = 1) otherwise.
Given an integer k let [G, H] be the solution (with value of 1
corresponding to entities failed initially) that cause maximum
number of entity failure. Two variables xid and yjd are used
in the ILP with xid = 1 (yjd = 1), when entity ai â A
(bj â B) is in a failed state at time step d, and 0 otherwise.
The number of entities to be defended is considered to be k.

It is to be noted that the maximum number cascading steps is
upper bounded by |A| + |B| â 1 = m + n â 1. The objective
function can now be formulated as follows:
min

n
m

X
X
yj(m+nâ1)
xi(m+nâ1) +

(1)

j=1

i=1

The objective in (1) minimizes the number of entities failed
after the cascading failure with the respective constraints for
the Entity Hardening problem as follows:
Constraint Set 1:

n
P

i=1

qxi +

m
P

qyj = k , with qxi , qyj â [0, 1].

j=1

If an entity xi (yj ) is defended then qxi = 1 (qyj = 1) and 0
otherwise.
Constraint Set 2: xi0 â¥ gi â qxi and yi0 â¥ hi â qyi .
This constraint implies that only if an entity is not defended
and gi (hi ) is 1 then the entity will fail at the initial time step.
Constraint Set 3: xid â¥ xi(dâ1) , âd, 1 â¤ d â¤ m + n â 1, and
yid â¥ yi(dâ1) , âd, 1 â¤ d â¤ m + n â 1, in order to ensure
that for an entity which fails in a particular time step would
remain in failed state at all subsequent time steps.
Constraint Set 4: Modeling of the constraint to capture
the cascade propagation for IIM is similar to the constraints
established in [9]. A brief presentation of this constraint is
provided here. Consider an IDR ai â bj bp bl + bm bn + bq of
type Case IV. The following steps are enumerated to depict
the cascade propagation:
Step 1: Replace all minterms of size greater than one with a
variable. In the example provided we have the transformed
minterm as ai â c1 + c2 + bq with c1 â bj bp bl and
c2 â bm bn (c1 , c2 â {0, 1}) as the new IDRs. Note that after
transformation, the original IDR is in the form of Case III and
the introduced IDRs are in the form of Case II.
Step 2: For each variable c, a constraints is added to capture
the cascade propagation. Let N be the number of entities
in the minterm on which c is dependent. In the example
for the variable c1 with IDR c1 â bj bp bl , constraints
y
+y
+yl(dâ1)
c1d â¥ j(dâ1) p(dâ1)
and c1d â¤ yj(dâ1) + yp(dâ1) +
N
yl(dâ1) âd, 1 â¤ d â¤ m + n â 1 are introduced (with
N = 3 in this case). If IDR of an entity is already in
form of Case II, i.e.,ai â bj bp bl then constraints xid â¥
yj(dâ1) +yp(dâ1) +yl(dâ1)
â qxi and xid â¤ yj(dâ1) + yp(dâ1) +
N
yl(dâ1) âd, 1 â¤ d â¤ m + n â 1 are introduced (with N = 3).
These constraints satisfies that if the entity xi is hardened
initially then it is not dead at any time step.
Step 3: Let M be the number of minterms in the transformed IDR as described in Step 1. In the given example
with IDR ai â c1 + c2 + bq constraints of form xid â¥
c1(dâ1) + c2(dâ1) + yq(dâ1) â (M â 1) â qxi and xid â¤
c1(dâ1) +c2(dâ1) +yq(dâ1)
âd, 1 â¤ d â¤ m + n â 1 are introduced.
M
These constraints ensures that even if all the minterms of xi
has at least one entity in dead state then it will be alive if the
entity is hardened initially. For all IDRs of type Case I and
Case III, the constraint discussed in this step is used.

6

B. Heuristic

Algorithm 2: Heuristic Solution to the ENH Problem

In this subsection we provide a greedy heuristic solution to
the Entity Hardening problem. For an interdependent network
I(A, B, F (A, B)) with the initial failed set of entities as
Aâ² âª B â² , Protection Set of each entity has been defined in
the approximation scheme of Case III. To design the heuristic
we define Minterm Coverage Number of each entity in A âª B
as follows:

1
2
3
4

Definition: For an entity xi â A âª B the Minterm Coverage
Number is defined as the number of minterms that can be
removed from F (A, B) without affecting the cascading process
by hardening the entity xi when all entities in Aâ² âª B â² fails
initially. This is represented as M (xi |Aâ² âª B â² ).
Similar to the computation of Protection Set the Minterm
Coverage Number of each entity can be computed in O((n +
m)2 ). With these definitions the heuristic is given in Algorithm
2. The algorithm takes in as input an interdependent network
I(A, B, F (A, B)) with S = AâªB. Step 4-5 is done to reduce
the search space as it directly follows that the set of entities
in Q wouldnât effect the hardening process. In each iteration
of the while loop an entity xd is greedily selected which
when hardened would prevent failure of maximum number of
entities. This ensures that at each step the number of entities
failed is minimized. In case of a tie, among all entities involved
in the tie, the entity having the highest Minterm Coverage
Number is included in the solution. This gives a higher priority
to the entity which when hardened, has more impact on failure
minimization in subsequent iterations of the while loop. The
interdependent network I(A, B, F (A, B)) is updated in steps
13-16 of the algorithm. This takes into account the effect of
hardening an entity in the current iteration on entities hardened
in the following iterations.
Run Time Analysis of Algorithm 2: For this analysis we
consider n to be the total number of entities and m to be
the total number of minterms. Updates in step 4 can be done
in O(m) and step 5 in O(n). The while loop iterates for k
times. In each iteration of the while loop step 7 and step 8 takes
at most O((n + m)2 ) and O(nlog(n)) time respectively. On
branching in step 9, step 10 and step 11 takes O((n + m)2 )
and O(nlog(n)) time respectively. Updates in step 13 takes
O(n) time and in step 14 takes O(n + m) time. Step 12,
16 and 17 runs in constant time. Hence Algorithm 2 runs in
O(k(n + m)2 ) time.
VI.

E XPERIMENTAL R ESULTS

In this section we present the experimental results of the
Entity Hardening problem by comparing the optimal solution
computed using an ILP, and the proposed heuristic algorithm.
The experiments were conducted on real world power grid data
obtained from Platts (www.platts.com), and communication
network data obtained from GeoTel (www.geo-tel.com) of
Maricopa County, Arizona. The data consisted of 70 power
plants and 470 transmission lines in the power network, and
2, 690 cell towers, 7, 100 fiber-lit buildings and 42, 723 fiber
links in the communication network. We identified five nonintersecting geographical regions from the data set and labeled
them from regions 1 through 5. For each of the regions, the
entities of the power and communication network that were

5
6
7
8
9
10
11
12

Data: An interdependent network I(A, B, F(A, B)) (with
S = A âª B), set of entities Aâ² âª B â² failed initially
causing maximum failure in the interdependent network
with |Aâ² | + |B â² | = K and hardening budget k.
Result: Set of hardened entities H.
begin
Initialize S â² â Aâ² âª B â² ;
Initialize H = â;
Update F(A, B) as follows â (a) let Q be the set of
entities that does not fail on failing K entities, (b) remove
IDRs corresponding to entities in set Q, (c) remove from
minterm of entities not in set Q all entities which are in
set Q ;
Update S = S \ Q ;
while (k entities are not hardened) do
For each entity xi â S compute the Protection Set
P (xi |S â² );
Choose the entity xd with highest cardinality of the
set |P (xd |S â² )|;
if (more than one entity has the same highest
cardinality value) then
For each such entity xj compute the Minterm
Coverage Number M (xj |S â² ) ;
Choose the entity xd with highest Minterm
Coverage Number. ;
In case of a tie choose arbitrarily;

16

Update S â S â P (xd |S â² );
Update F(A, B) by removing (i) IDRs corresponding
to all entities in P (xd |S â² ), and (ii) occurrence of
these entities in IDRs of entities not in P (xd |S â² );
if (xd â S â² ) then
Update S â² â S â² â {xd };

17

Update H = H âª xd ;

13
14

15

18

return H ;

located within the geographic region formed the set A and B
respectively. Each region was represented by an interdependent
network I(A, B, F (A, B)). We use the IDR construction rules
as defined in [9] to generate F (A, B).

In all of our simulations IBM CPLEX Optimizer 12.5 to
solve ILPs and Python 3 for heuristic is used. To analyze
the Entity Hardening problem the value of K was set to 8.
The ILP in [9] was used to compute the K most vulnerable
nodes in the network, and the set of failed entities due to
the failure of the K entities was also computed. For the five
regions, when the K = 8 most vulnerable nodes failed, the
total number of failed entities in the network were 28, 23, 28,
28 and 27 respectively. With the K most vulnerable nodes and
final set of failed nodes as input, the ILP and heuristic of the
Entity Hardening problem are compared with k = 1, 3, 5, 7.
The results of these simulations are shown in Figure 1. It is
observed that the heuristic solution differs more from optimal
at higher values of k (factor of 0.5 and 0.67 for Regions 1
and 3 respectively with k = 7). This is primarily because of
the greedy nature of Algorithm 2. However on an average the
heuristic solution differs by a factor of 0.13 from the optimal.

7

14 13

ILP solution
Heuristic

10
8

7
6

6

4

4

3
2

2
0

1
1

14

13

ILP solution
Heuristic

12
10
8

7

4

3

Number of entities failed

3

2

1
1

1

13

ILP solution
Heuristic

12

12
10

8

8
6

6

5

4

3

2
1

3
5
7
Number of entities hardened

(b) Region 2
11

ILP solution
Heuristic

10
8
6
5

4

4

3
2

2

1
1

(c) Region 3

8

11

6

3
1

0

3
5
7
Number of entities hardened

(a) Region 1

0

7

6

0

3
5
7
Number of entities hardened

12

Number of entities failed

13
12

3
5
7
Number of entities hardened

(d) Region 4

Number of entities failed

12

Number of entities failed

Number of entities failed

14

7

ILP solution
Heuristic

7

6
5

5

4
3

3

2
1
0

1

1

3
5
7
Number of entities hardened

(e) Region 5

Fig. 1: Comparison chart of the optimal solution (ILP) with the heuristic by varying number of entities hardened for each identified region

VII.

C ONCLUSION

In this paper we studied the entity hardening problem
in multi-layer networks. We modeled the interdependencies
shared between the networks using IIM, and formulated the
the Entity Hardening problem in this setting. We showed that
the problem is solvable in polynomial time for some cases,
whereas for others it is NP-complete. We evaluated the efficacy
of our heuristic using power and communication network data
of Maricopa County, Arizona. Our experiments showed that
our heuristic almost always produces near optimal results.
R EFERENCES
[1]

[2]

[3]

[4]

[5]

[6]

[7]

S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
âCatastrophic cascade of failures in interdependent networks,â Nature,
vol. 464, no. 7291, pp. 1025â1028, 2010.
J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, âNetworks formed
from interdependent networks,â Nature Physics, vol. 8, no. 1, pp. 40â48,
2011.
J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, âCascade of
failures in coupled network systems with multiple support-dependence
relations,â Physical Review E, vol. 83, no. 3, p. 036116, 2011.
V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and
R. Setola, âModelling interdependent infrastructures using interacting
dynamical models,â International Journal of Critical Infrastructures,
vol. 4, no. 1, pp. 63â79, 2008.
P. Zhang, S. Peeta, and T. Friesz, âDynamic game theoretic model of
multi-layer infrastructure networks,â Networks and Spatial Economics,
vol. 5, no. 2, pp. 147â178, 2005.
M. Parandehgheibi and E. Modiano, âRobustness of interdependent
networks: The case of communication networks and the power grid,â
arXiv preprint arXiv:1304.0356, 2013.
D. T. Nguyen, Y. Shen, and M. T. Thai, âDetecting critical nodes in
interdependent power networks for vulnerability assessment,â 2013.

[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman,
âPower grid vulnerability to geographically correlated failures-analysis
and control implications,â arXiv preprint arXiv:1206.1099, 2012.
[9] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, âIdentification of k most vulnerable nodes in multi-layered network using a new
model of interdependency,â in Computer Communications Workshops
(INFOCOM WKSHPS), 2014 IEEE Conference on. IEEE, 2014, pp.
831â836.
[10] J.-F. Castet and J. H. Saleh, âInterdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,â PloS one, vol. 8, no. 4, p. e60402, 2013.
[11] A. Das, J. Banerjee, and A. Sen, âRoot cause analysis of failures in
interdependent power-communication networks,â in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910â915.
[12] A. Mazumder, C. Zhou, A. Das, and A. Sen, âProgressive recovery from
failure in multi-layered interdependent network using a new model of
interdependency,â in Conference on Critical Information Infrastructures
Security (CRITIS), 2014. Springer, 2014.
[13] M. Hajiaghayi, K. Jain, K. Konwar, L. Lau, I. Mandoiu, A. Russell,
A. Shvartsman, and V. Vazirani, âThe minimum k-colored subgraph
problem in haplotyping and dna primer selection,â in Proceedings of the
International Workshop on Bioinformatics Research and Applications
(IWBRA). Citeseer, 2006.

Identification of K Most Vulnerable Nodes in
Multi-layered Network Using a New Model of
Interdependency
Arunabha Sen, Anisha Mazumder, Joydeep Banerjee, Arun Das and Randy Compton
Computer Science and Engineering Program

arXiv:1401.1783v1 [cs.NI] 8 Jan 2014

School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {asen, amazumde, Joydeep.Banerjee, adas22, Randy.Compton}@asu.edu

AbstractâThe critical infrastructures of the nation including
the power grid and the communication network are highly
interdependent. Recognizing the need for a deeper understanding
of the interdependency in a multi-layered network, significant
efforts have been made by the research community in the last
few years to achieve this goal. Accordingly a number of models
have been proposed and analyzed. Unfortunately, most of the
models are over simplified and, as such, they fail to capture the
complex interdependency that exists between entities of the power
grid and the communication networks involving a combination of
conjunctive and disjunctive relations. To overcome the limitations
of existing models, we propose a new model that is able to
capture such complex interdependency relations. Utilizing this
model, we provide techniques to identify the K most vulnerable
nodes of an interdependent network. We show that the problem
can be solved in polynomial time in some special cases, whereas
for some others, the problem is NP-complete. We establish that
this problem is equivalent to computation of a fixed point of a
multilayered network system and we provide a technique for its
computation utilizing Integer Linear Programming. Finally, we
evaluate the efficacy of our technique using real data collected
from the power grid and the communication network that span
the Maricopa County of Arizona.

I. I NTRODUCTION
In the last few years there has been an increasing awareness
in the research community that the critical infrastructures of
the nation are closely coupled in the sense that the well being
of one infrastructure depends heavily on the well being of another. A case in point is the interdependency between the electric power grid and the communication network. The power
grid entities, such as the SCADA systems that control power
stations and sub-stations, receive their commands through
communication networks, while the entities of communication
network, such as routers and base stations, cannot operate
without electric power. Cascading failures in the power grid,
are even more complex now because of the coupling between
power grid and communication network. Due to this coupling,
not only entities in power networks, such as generators and
transmission lines, can trigger power failure, communication
network entities, such as routers and optical fiber lines, can
also trigger failure in power grid. Thus it is essential that
the interdependency between different types of networks be
understood well, so that preventive measures can be taken to

avoid cascading catastrophic failures in multi-layered network
environments.
Recognizing the need for a deeper understanding of the
interdependency in a multi-layered network, significant efforts
have been made in the research community in the last few
years to achieve this goal [1], [2], [3], [4], [5], [6], [7], [8].
Accordingly a number of models have been proposed and
analyzed. Unfortunately, many of the proposed models are
overly simplistic in nature and as such they fail to capture
the complex interdependency that exists between power grid
and communication networks. In a highly cited paper [1], the
authors assume that every node in one network depends on one
and only one node of the other network. However, in a follow
up paper [2], the same authors argue that this assumption may
not be valid in the real world and a single node in one network
may depend on more than one node in the other network. A
node in one network may be functional (âaliveâ) as long as
one supporting node on the other network is functional.
Although this generalization can account for disjunctive
dependency of a node in the A network (say ai ) on more
than one node in the B network (say, bj and bk ), implying
that ai may be âaliveâ as long as either bi or bj is alive,
it cannot account for conjunctive dependency of the form
when both bj and bk has to be alive in order for ai to
be alive. In a real network the dependency is likely to be
even more complex involving both disjunctive and conjunctive
components. For example, ai may be alive if (i) bj and bk and
bl are alive, or (ii) bm and bn are alive, or (iii) bp is alive. The
graph based interdependency models proposed in the literature
[3], [4], [5], [9], [6], [7] including [1], [2] cannot capture
such complex interdependency between entities of multilayer
networks. In order to capture such complex interdependency,
we propose a new model using Boolean logic. Utilizing this
comprehensive model, we provide techniques to identify the
K most vulnerable nodes of an interdependent multilayered
network system. We show that the this problem can be solved
in polynomial time for some special cases, whereas for some
others, the problem is NP-complete. We also show that this
problem is equivalent to computation of a fixed point [10] and
we provide a technique utilizing Integer Linear Programming

2

to compute that fixed point. Finally, we evaluate the efficacy
of our technique using real data collected from power grid
and communication networks that span Maricopa County of
Arizona.

II. I NTERDEPENDENCY M ODEL
We describe the model for an interdependent network with
two layers. However, the concept can easily be generalized
to deal with networks with more layers. Suppose that the
network entities in layer 1 are referred to as the A type
entities, A = {a1 , . . . , an } and entities in layer 2 are referred
to as the B type entities, B = {b1 , . . . , bm }. If the layer 1
entity ai is operational if (i) the layer 2 entities bj , bk , bl
are operational, or (ii) bm , bn are operational, or (iii) bp
is operational, we express it in terms of live equations of
the form ai â bj bk bl + bm bn + bp . The live equation for
a B type entity br can be expressed in a similar fashion
in terms of A type entities. If br is operational if (i) the
layer 1 entities as , at , au , av are operational, or (ii) aw , az
are operational, we express it in terms of live equations of
the form br â as at au av + aw az . It may be noted that the
live equations only provide a necessary condition for entities
such as ai or br to be operational. In other words, ai or br
may fail independently and may be not operational even when
the conditions given by the corresponding live equations are
satisfied. A P
live equation
in general will have the following
Ti Qtj
form: xi â j=1
y
k=1 j,k where xi and yj,k are elements
of the set A (B) and B (A) respectively, Ti represents the
number of min-terms in the live equation and tj refers to the
size of the j-th min-term (the size of a min-term is equal to the
number of A or B elements in that min-term). In the example
ai â bj bk bl + bm bn + bp , Ti = 3, t1 = 3, t2 = 2, t3 = 1,
xi = ai , y2,1 = bm , y2,2 = bp .
We refer to the live equations of the form ai â bj bk bl +
bm bn + bp also as First Order Dependency Relations, because
these relations express direct dependency of the A type entities
on B type entities and vice-versa. It may be noted however
that as A type entities are dependent on B type entities,
which in turn depends on A type entities, the failure of
some A type entities can trigger the failure of other A type
entities, though indirectly, through some B type entities. Such
interdependency creates a cascade of failures in multilayered
networks when only a few entities of either A type or B type
(or a combination) fails. We illustrate this with the help of
an example. The live equations for this example is shown in
table I.
Power Network
a1 â b1 + b2
a2 â b1 b3 + b2
a3 â b1 b2 b3
a4 â b1 + b2 + b3

Communication Network
b1 â a1 + a2 a3
b2 â a1 + a3
b3 â a1 a2
ââ

TABLE I: Live equations for a Multilayer Network

Entities
a1
a2
a3
a4
b1
b2
b3

t0
1
0
0
0
0
0
0

t1
1
0
0
0
0
0
1

Time Steps
t2
t3
t4
1
1
1
0
0
1
1
1
1
0
0
1
0
1
1
0
1
1
1
1
1

t5
1
1
1
1
1
1
1

t6
1
1
1
1
1
1
1

TABLE II: Time Stepped Cascade Effect for a Multilayer Network

Fig. 1: Cascading failures reach steady state after p time steps

As shown in table II, the failure of only one entity a1 at
time step t0 triggered a chain of failures that resulted in the
failure of all the entities of the network after by timestep t4 .
A table entry of 1 indicates that the entity is âdeadâ. In this
example, the failure of a1 at t0 triggered the failure of b3 at
t1 , which in turn triggered the failure of a3 at t2 . The failure
of b3 at t1 was due to the dependency relation b3 â a1 a2
and the failure of a3 at t2 was due to the dependency relation
a3 â b1 b2 b3 . The cascading failure process initiated by failure
(or death) of a subset of A type entities at timestep t0 , A0d and
a subset of B type entities Bd0 till it reaches its final steady
state is shown diagrammatically in figure 1. Accordingly, a
multilayered network can be viewed as a âclosed loopâ control
system. Finding the steady state after an initial failure in this
case is equivalent of computing the fixed point of a function
F(.) such that F(Apd âª Bdp ) = Apd âª Bdp , where p represents
the number of steps when the system reaches the steady state.
We define a set of K entities in a multi-layered network
as most vulnerable, if failure of these K entities triggers the
failure of the largest number of other entities. The goal of
the K most vulnerable nodes problem is to identify this set of
nodes. This is equivalent to identifying A0d â A, Bd0 â B, that
maximizes |Apd âªBdp |, subject to the constraint that |A0d âªBd0 | â¤
K.
The dependency relations (live equations) can be formed
either after careful analysis of the multilayer network along the
lines carried out in [8], or after consultation with the engineers
of the local utility and internet service providers.
III. C OMPUTATIONAL C OMPLEXITY AND A LGORITHMS
Based on the number and the size of the min-terms in the
dependency relations, we divide them into four different cases
as shown in Table III. The algorithms for finding the K most
vulnerable nodes in the multilayer networks and computation
complexity for each of the cases are discussed in the following
four subsections.
Case
Case I
Case II
Case III
Case IV

No. of Min-terms
1
1
Arbitrary
Arbitrary

Size of Min-terms
1
Arbitrary
1
Arbitrary

TABLE III: Equation Types for Dependency Relations

3

A. Case I: Problem Instance with One Min-term of Size One
In this case, a live equation in general will have the following form: xi â yj where xi and yj are elements of the set A
(B) and B (A) respectively. In the example ai â bj , xi = ai ,
y1 = bj . It may be noted that a conjunctive implication of
the form ai â bj bk can also be written as two separate
implications ai â bj and ai â bk . However, such cases are
considered in Case II and is excluded from consideration in
Case I. The exclusion of such implications implies that the
entities that appear on the LHS of an implication in Case I
are unique. This property enables us to develop a polynomial
time algorithm for the solution of the K most vulnerable node
problem for this case. We present the algorithm next.
Algorithm 1
Input: (i) A set S of implications of the form of y â x,
where x, y â A âª B, (ii) An integer K.
Output: A set V 0 where |V 0 | = K and V 0 â A âª B such
that failure of entities in V 0 at time step t0 results in failure
of the largest number of entities in A âª B when the steady
state is reached.
Step 1. We construct a directed graph G = (V, E), where
V = A âª B. For each implication y â x in S, where x, y â
A âª B, we introduce a directed edge (x, y) â E.
Step 2. For each node xi â V , we construct a transitive
closure set Cxi as follows: If there is a path from xi to some
node yi â V in G, then we include yi in Cxi . It may be
recalled that |A| + |B| = n + m. So, we get n + m transitive
closure sets Cxi , 1 â¤ i â¤ (n + m). We call each xi to be the
seed entity for the transitive closure set Cxi .
Step 3. We remove all the transitive closure sets which are
proper subsets of some other transitive closure set.
Step 4. Sort the remaining transitive closure sets Cxi ,
where the rank of the closure sets is determined by the
cardinality of the sets. The sets with a larger number of entities
are ranked higher than the sets with a fewer number of entities.
Step 5. Construct the set V 0 by selecting the seed entities
of the top K transitive closure sets. If the number of remaining
transitive closure sets is less than K (say, K0 ), arbitrarily select
the remaining entities.
Time complexity of Algorithm 1: Step 1 takes O(n + m + |S|)
time. Step 2 can be executed in O((n+m)3 ) time. Step 3 takes
at most O((n + m)2 ) time. Step 4 sorts at most |S| entries, a
standard sorting algorithm takes O(|S| log |S|) time. Selecting
K entities in step 5 takes O(K) time. Since |S| â¤ n+m, hence
the overall time complexity is O((n + m)3 )
Theorem 1. For each pair of transitive closure sets Cxi and
Cxj produced in step 2 of algorithm 1, either Cxi â© Cxj = â
or Cxi â© Cxj = Cxi or Cxi â© Cxj = Cxj , where xi 6= xj .
Proof: Consider, if possible, that there is a pair of transitive
closure sets Cxi and Cxj produced in step 2 of algorithm 1,
such that Cxi â©Cxj 6= â and Cxi â©Cxj 6= Cxi and Cxi â©Cxj 6=

Cxj . Let xk â Cxi â© Cxj . This implies that there is a path
from xi to xk (path1 ) as well as there is a path from xj to xk ,
(path2 ). Since, xi 6= xj and Cxi â©Cxj 6= Cxi and Cxi â©Cxj =
Cxj , there is some xl in the path1 such that xl also belongs to
path2 . W.l.o.g, let us consider that xl be the first node in path1
such that xl also belongs to path2 . This implies that xl has
in-degree greater than 1. This in turn implies that there are two
implications in the set of implications S such that xl appears in
the L.H.S of both. This is a contradiction because this violates
a characteristic of the implications in Case I. Hence, our initial
assumption was wrong and the theorem is proven.
Theorem 2. Algorithm 1 gives an optimal solution for the
problem of selecting K most vulnerable entities in a multilayer network for case I dependencies.
Proof: Consider that the set V 0 returned by the algorithm is
not optimal and the optimal solution is VOP T . Let us consider
there is a entity xi â A âª B such that xi â VOP T \ V 0 .
Evidently, (i) Cxi was either deleted in step 3 or (ii) |Cxi | is
less than the cardinalities of all the transitive closure sets with
seed entities xj â V 0 , because our algorithm did not select
xi . Hence, in both cases, replacing any entity xj â V 0 by xi
reduces the total number of entities killed. Thus, the number
of dead entities by the failure of entities in VOP T is lesser than
that caused by the failure of the entities in V 0 , contradicting
the optimality of VOP T . Hence, the algorithm does in fact
return the optimal solution.
B. Case II: Problem Instance with One Min-term of Arbitrary
Size
In this case, a liveQ equation in general will have the
q
following form: xi â k=1 yj where xi and yj are elements
of the set A (B) and B (A) respectively, q represents the size
of min-term. In the example ai â bj bk bl , q = 3, xi = ai ,
y1 = bj , y2 = bk , y3 = bk .
1) Computational Complexity: We show that computation
of K most vulnerable nodes (K-MVN) in a multilayer network
is NP-complete in Case II. We formally state the problem next.
Instance: Given a set of dependency relations between
A
Qqand B type entities in the form of live equations xi â
k=1 yj , integers K and L.
Question: Is there a subset of A and B type entities of
size at most K whose âdeathâ (failure) at time t0 , triggers a
cascade of failures resulting in failures of at least L entities,
when the steady state is reached?
Theorem 3. The K-MVN problem is NP-complete.
Proof: We prove that the K-MVN problem is NP-complete
by giving a transformation for the vertex cover (VC) problem.
An instance of the vertex cover problem is specified by an
undirected graph G = (V, E) and an integer R. We want to
know if there is a subset of nodes S â V of size at most
R, so that every edge has at least one end point in S. From
an instance of the VC problem, we create an instance of the

4

K-MVN problem in the following way. First, from the graph
G = (V, E), we create a directed graph G0 = (V, E 0 ) by
replacing each edge e â E by two oppositely directed edges
e1 and e2 in E 0 (the end vertices of e1 and e2 are same as
the end vertices of e). Corresponding to a node vi in G0 that
has incoming edges from other nodes (say) vj , vk and vl , we
create a dependency relation (live equation) vi â vj vk vl . We
set K = R and L = |V |. The corresponding death equation is
of the form vÂ¯i â vÂ¯j + vÂ¯k + vÂ¯l (obtained by taking negation
of the live equation). We set K = R and L = |V |. It can now
easily be verified that if the graph G = (V, E) has a vertex
cover of size R iff in the created instance of K-MVN problem
death (failure) of at most K entities at time t0 , will trigger a
cascade of failures resulting in failures of at least L entities,
when the steady state is reached.
2) Optimal Solution with Integer Linear Programming:
In this case, we can find and optimal solution to the KMVN problem using Integer Linear Programming (ILP). We
associate binary indicator variables xi (yi ) to capture the state
of the entities ai (bi ). xi (yi ) is 1 when ai (bi ) is dead and
0 otherwise. Since we want find the set of K entities whose
failure at time step t0 triggers cascading failure resulting in the
failure of the largest number of entities, the
the
Pnobjective
Pof
m
ILP can be written as follows maximize
x
+
i
i=1
i=1 yi
It may be noted that the variables in the objective function
do not have any notion of time. However, cascading failure
takes place in time steps, ai triggers failure of bj at time
step t1 , which in turn triggers failure of ak in time step t2
and so on. Accordingly, in order to capture the cascading
failure process, we need to introduce the notion of time into
the variables of the ILP. If the numbers of A and B type
entities are n and m respectively, the steady state must be
reached by time step n + m â 1 (cascading process starts at
time step 0, t0 ). Accordingly, we introduce n + m versions
of the variables xi and yi , i.e., xi [0], . . . , xi [n + m â 1] and
yi [0], . . . , yi [n+mâ1]. To indicate the state of entities ai and
bi at times t0 , . . . , tn+mâ1 . The objective of the ILP is now
changed to
maximize

n
X
i=1

xi [n + m â 1] +

m
X

yi [n + m â 1]

i=1

Subject to the constraint that no more than K entities can
fail at time t0 .
Pn
Pm
Constraint 1:
i=1 yi [0] â¤ K In order
i=1 xi [0] +
to ensure that the cascading failure process conforms to
the dependency relations between type A and B entities,
additional constraints must be imposed.
Constraint 2: If an entity fails at time fails at time step p,
(i.e., tp ) it should continue to be in the failed state at all time
steps t > p. That is xi (t) â¥ xi (t â 1), ât, 1 â¤ t â¤ n + m â 1.
Same constraint applies to yi (t).
Constraint 3: The dependency relation (death equation)
aÂ¯i â bÂ¯j +bÂ¯k +bÂ¯l can be translated into a linear constraint in the
following way xi (t) â¤ yj (tâ1)+yk (tâ1)+yl (tâ1), ât, 1 â¤
t â¤ n + m â 1.

The optimal solution to K-MVN problem for Case II can be
found by solving the above ILP.
C. Case III: Problem Instance with an Arbitrary Number of
Min-terms of Size One
A live equation
Pq in this special case will have the following
form: xi â j=1 yj where xi and yj are elements of the set
A (B) and B (A) respectively, q represents the number of minterms in the live equation. In the example ai â bj + bk + bl ,
q = 3, xi = ai , y1 = bj , y2 = bk , y3 = bl .
1) Computational Complexity: We show that a special
case of the problem instances with an arbitrary number
of min-terms of size one is same as the Subset Cover
problem (defined below), which is proven to be NPcomplete. We define Implication Set(A) Pto be the
Ti
set of all implications of the form ai â
j=1 bj and
ImplicationPSet(B) to be the set of all implications of the
Ti
form bi â
j=1 aj . Now consider a subset of the set of
problem instances with an arbitrary number of min-terms
of size one where either Implication Set(A) = â
or Implication Set(B)
=
â. Let A0
=
{ai |ai is the element on the LHS of an implication}
in the Implication Set(A). The set B 0 is defined
accordingly. If Implication Set(B) = â then B 0 = â. In
this case, failure of any ai , 1 â¤ i â¤ n type entities will not
cause failure of any bj , 1 â¤ j â¤ m type entities. Since an
adversary can cause failure of only K entities, the adversary
would like to choose only those K entities that will cause
failure of the largest number of entities. In this scenario, there
is no reason for the adversary to attack any ai , 1 â¤ i â¤ n type
entities as they will not cause failure of any bj , 1 â¤ j â¤ m
type entities. On the other hand, if the adversary attacks
K bj type entities, not only those K bj type entities will
be destroyed, some ai type entities will also be destroyed
due to the implications in the Implication Set(A). As
such the goal of the adversary will be to carefully choose
K bj , 1 â¤ j â¤ m type entities that will destroy the largest
number of ai type entities. In its abstract form, the problem
can be viewed as the Subset Cover problem.
Subset Cover Problem
Instance: A set S = {s1 , . . . , sm }, a set S of m subsets of S,
i.e., S = {S1 , . . . , Sr }, where Si â S, âi, 1 â¤ i â¤ r, integers
p and q.
Question: Is there a p element subset S 0 of S (p < n) that
completely covers at least q elements of the set S? (A set S 0 is
said to be completely covering an element Si , âi, 1 â¤ i â¤ m
of the set S, if S 0 â© Si = Si , âi, 1 â¤ i â¤ m.)
The set S in the subset cover problem corresponds to the
set B = {b1 , . . . , bm }, and each set Si , 1 â¤ i â¤ r corresponds
to an implication in the ImplicationS et(A) and comprises of
the bj âs that appear on the RHS of the implication. The goal
of the problem is to select a subset B 00 of B that maximizes
the number of Si âs completely covered by B 00 .

5

Theorem 4. The Subset Cover problem is NP-complete.
Proof: We prove that the Subset Cover problem is NPcomplete by giving a transformation from the well known
Clique problem. It may be recalled that an instance of the
Clique problem is specified by a graph G = (V, E) and an
integer K. The decision question is whether or not a clique of
size at least K exists in the graph G = (V, E). We show that
a clique of size K exists in graph G = (V, E) iff the Subset
Cover problem instance has a p element subset S 0 of S that
completely covers at least q elements of the set S.
From an instance of the Clique problem, we create an
instance of the Subset Cover problem in the following way.
Corresponding to every vertex vi , 1 â¤ i â¤ n of the graph
G = (V, E) (V = {v1 , . . . , vn }), we create an element
in the set S = {s1 , . . . , sn }. Corresponding to every edge
ei , 1 â¤ i â¤ m, we create m subsets of S, i.e., S =
{S1 , . . . , Sm }, where Si corresponds to a two element subset
of nodes, corresponding to the end vertices of the edge ei . We
set the parameters p = K and q = K(K â 1)/2. Next we
show that in the instance of the subset cover problem created
by the above construction process, a p element subset S 0 of
S exists that completely covers at least q elements of the set
S, iff the graph G = (V, E) has a clique of size at least K.
Suppose that the graph G = (V, E) has a clique of size
K. It is clear that in the created instance of the subset cover
problem, we will have K(K â 1)/2 elements in the set S,
that will be completely covered by a K element subset of
the set S. The K element subset of S corresponds to the set
of K nodes that make up the clique in G = (V, E) and the
K(K â 1)/2 elements in the set S corresponds to the edges
of the graph G = (V, E) that corresponds to the edges of
the clique. Conversely, suppose that the instance of the Subset
Cover problem has K element subset of S that completely
covers K(K â 1)/2 elements of the set S. Since the elements
of S corresponds to the edges in G, in order to completely
cover K(K â 1)/2 edges, at least K nodes (elements of the
set S) will be necessary. As such, this set of K nodes will
constitute a clique in the graph G = (V, E).
2) Optimal Solution with Integer Linear
Programming: If
Pq
the live equation is in the form xi â k=1 yj then the âdeath
equationâ (obtained by taking negation
of the live equation)
Qq
will be in the product form xÌi â j=1 yÌj . If the live equation
is given as ai â bj + bk , then the death equation will be given
as aÂ¯i â bÂ¯j bÂ¯k .
By associating binary indicator variables xi and yi to
capture the state of the entities ai and bi , we can follow almost
identical procedure as in Case II, with only one exception.
It may be recalled that in Case II, the death equations such
as aÂ¯i â bÂ¯j + bÂ¯k was translated into a linear constraint
xi (t) â¤ yj (t â 1) + yk (t â 1), ât, 1 â¤ t â¤ n + m â 1. However
a similar translation in Case III, with death equations such as
aÂ¯i â bÂ¯j bÂ¯k , will result in a non-linear constraint of the form
xi (t) â¤ yj (t â 1)yk (t â 1), ât, 1 â¤ t â¤ n + m â 1. Fortunately,
a non-linear constraint of this form can be replaced a linear
constraint such as 2xi (t) â¤ yj (t â 1) + yk (t â 1), ât, 1 â¤

t â¤ n + m â 1. After this transformation, we can compute the
optimal solution using integer linear programming.
D. Case IV: Problem Instance with an Arbitrary Number of
Min-terms of Arbitrary Size
1) Computational Complexity: Since both Case II and Case
III are special cases of Case IV, the computational complexity
of finding the K most vulnerable nodes in the multilayer
network in NP-complete in Case IV also.
2) Optimal Solution with Integer Linear Programming:
The optimal solution to this version of the problem can be
computed by combining the techniques developed for the
solution of the versions of the problems considered in Cases
II and III.
IV. E XPERIMENTAL RESULTS
We applied our model to study multilayer vulnerability
issues in Maricopa County, the most densely populated county
of Arizona with approximately 60% of Arizonas population
residing in it. Specifically, we wanted to find out if some
regions of Maricopa County were more vulnerable to failure
than some other regions. The data for our multi-layered
network were obtained from different sources. We obtained
the data for the power network (network A) from Platts
(http://www.platts.com/). Our power network dataset consists
of 70 power plants and 470 transmission lines. Our communication network (network B) data were obtained from GeoTel
(http://www.geo-tel.com/). Our communication network data
consists of 2, 690 cell towers and 7, 100 fiber-lit buildings as
well as 42, 723 fiber links. Snapshots of our power network
data and communication network data are shown in figure 2. In
the power network snapshot of sub-figure(a), the orange markers show locations of powerplants while the yellow continuous
lines represent the transmission lines. In the communication
network snapshot of sub-figure (b) the pink markers show the
location of fiber-lit buildings, the orange markers show the
location of cell towers and the green continuous lines represent
the fiber links. In our dataset, âloadâ in the Power Network is
divided into Cell towers and Fiber-lit buildings. Although there
exists various other physical entities which also draw electric
power and hence can be viewed as load to the power network,
as they are not relevant to our study on interdependency
between power and communication networks, we ignore such
entities. Thus in network A, we have the three types of Power
Network Entities (PNEâs) - Generators, Load (consisting of
Cell towers and Fiber-lit buildings) and Transmission lines
(denoted by a1 , a2 , a3 respectively). For the Communication
Network, we have the following Communication Network
Entities (CNEâs) - Cell Towers, Fiber-lit buildings and Fiber
links (denoted by b1 , b2 , b3 respectively). We consider the
Fiber-lit buildings as a communication network entities as they
house routers which definitely are communication network
entities. From the raw data we construct Implication Set(A)
and Implication Set(B), by following the rules stated below:
Rules: We consider that a PNE is dependent on a set of
CNEs for being in the active state (âaliveâ) or being in the

6

(a) Snapshot of Power Network in Maricopa County

(b) Snapshot of Communication Network in Maricopa County

Fig. 2: Snapshots of power network and communication network in Maricopa County)

inactive state (âdeadâ). Similarly, a CNE is dependent on a set
of PNEs for being active or inactive state. For simplicity we
consider the live equations with at most two minterms. For
the same reason we consider the size of each minterm is at
most two.

of the number of entities of the two networks A and B. Most
importantly, we find that the degree of vulnerability of all
the five regions considered in our study are close and no one
region stands out as being extremely vulnerable.

Generators (a1,i , 1 â¤ i â¤ p, where p is the total number
of generators): We consider that each generator (a1.i ) is
dependent on the nearest Cell Tower (b1,j ) or the nearest
Fiber-lit building (b2,k ) and the corresponding Fiber link (b3,l )
connecting b2,k and a1,i . Thus, we have
a1,i â b1,j + b2,k Ã b3,l
Load (a2,i , 1 â¤ i â¤ q, where q is the total number of loads):
We consider that the loads in the power network do not depend
on any CNE.
Transmission Lines (a3,i , 1 â¤ i â¤ r, where r is the total number of transmission lines): We consider that the transmission
lines do not depend on any CNE.
Cell Towers (b1,i , 1 â¤ i â¤ s, where s is the total number
of cell towers): We consider the cell towers depend on the
nearest pair of generators and the corresponding transmission
line connecting the generator to the cell tower. Thus, we have
b1,i â a1,j Ã a3,k + a1,j 0 Ã a3,k0
Fiber-lit Buildings (b2,i , 1 â¤ i â¤ t, where t is the total number
of fiber-lit buildings): We consider that the fiber-lit buildings
depend on the nearest pair of generators and the corresponding
transmission lines connecting the generators to the cell tower.
Thus, we have b2,i â a1,j Ã a3,k + a1,j 0 Ã a3,k0
Fiber Links (b3,i , 1 â¤ i â¤ u, where u is the total number of
fiber links)): We consider that the fiber links do not depend
on any PNE.
Because of experimental resource limitation, we have considered 5 regions of Maricopa County for our experiments.
We used IBM CPLEX Optimizer 12.5 to run the formulated
ILPâs on the experimental dataset. We show our results in
the figure 3. We observe that in each of the regions there
is a specific budget threshold beyond which each additional
increment in budget results in the death of only one entity. The
reason for this behavior is our assumption that entities such
as the transmission lines and the fiberlinks are not dependent
on any other entities. We notice that all the entities of the
two networks can be destroyed with a budget of about 60%

Fig. 3: Experimental results of failure vulnerability across five regions
of Maricopa county

R EFERENCES
[1] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
âCatastrophic cascade of failures in interdependent networks,â Nature,
vol. 464, no. 7291, pp. 1025â1028, 2010.
[2] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, âNetworks formed
from interdependent networks,â Nature Physics, vol. 8, no. 1, pp. 40â48,
2011.
[3] J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, âCascade of
failures in coupled network systems with multiple support-dependence
relations,â Physical Review E, vol. 83, no. 3, p. 036116, 2011.
[4] V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and
R. Setola, âModelling interdependent infrastructures using interacting
dynamical models,â International Journal of Critical Infrastructures,
vol. 4, no. 1, pp. 63â79, 2008.
[5] P. Zhang, S. Peeta, and T. Friesz, âDynamic game theoretic model of
multi-layer infrastructure networks,â Networks and Spatial Economics,
vol. 5, no. 2, pp. 147â178, 2005.
[6] M. Parandehgheibi and E. Modiano, âRobustness of interdependent
networks: The case of communication networks and the power grid,â
arXiv preprint arXiv:1304.0356, 2013.
[7] D. T. Nguyen, Y. Shen, and M. T. Thai, âDetecting critical nodes in
interdependent power networks for vulnerability assessment,â 2013.
[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman,
âPower grid vulnerability to geographically correlated failures-analysis
and control implications,â arXiv preprint arXiv:1206.1099, 2012.
[9] J.-F. Castet and J. H. Saleh, âInterdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,â PloS one, vol. 8, no. 4, p. e60402, 2013.
[10] A. Fudenberg and J. Tirole, Game Theory. Ane Books, 2010.

2014 IEEE 15th International Conference on High Performance Switching and Routing (HPSR)

On Shortest Single/Multiple Path Computation
Problems in Fiber-Wireless (FiWi) Access Networks
Chenyang Zhouâ , Anisha Mazumderâ , Arunabha Senâ , Martin Reissleinâ  and Andrea Richaâ
â School of Computing, Informatics and Decision Systems Engineering
â  School of Electrical, Computer, and Energy Engineering

Arizona State University
Email: {czhou24, anisha.mazumder, asen, reisslein, aricha}@asu.edu

AbstractâFiber-Wireless (FiWi) networks have received considerable attention in the research community in the last few
years as they offer an attractive way of integrating optical and
wireless technology. As in every other type of networks, routing
plays a major role in FiWi networks. Accordingly, a number of
routing algorithms for FiWi networks have been proposed. Most
of the routing algorithms attempt to ï¬nd the âshortest pathâ
from the source to the destination. A recent paper proposed
a novel path length metric, where the contribution of a link
towards path length computation depends not only on that link
but also every other link that constitutes the path from the
source to the destination. In this paper we address the problem
of computing the shortest path using this path length metric.
Moreover, we consider a variation of the metric and also provide
an algorithm to compute the shortest path using this variation.
As multipath routing provides a number of advantages over
single path routing, we consider disjoint path routing with the
new path length metric. We show that while the single path
computation problem can be solved in polynomial time in both
the cases, the disjoint path computation problem is NP-complete.
We provide optimal solution for the NP-complete problem using
integer linear programming and also provide two approximation
algorithms with a performance bound of 4 and 2 respectively.
The experimental evaluation of the approximation algorithms
produced a near optimal solution in a fraction of a second.

I. I NTRODUCTION
Path computation problems are arguably one of the most
well studied family of problems in communication networks.
In most of these problems, one or more weight is associated
with a link representing, among other things, the cost, delay or
the reliability of that link. The objective most often is to ï¬nd
a least weighted path (or âshortest pathâ) between a speciï¬ed
source-destination node pair. In most of these problems, if a
link l is a part of a path P , then the contribution of the link
l on the âlengthâ of the path P depends only on the weight
w(l) of the link l, and is oblivious of the weights of the links
traversed before or after traversing the link l on the path P .
However, in a recent paper on optical-wireless FiWi network
[5], the authors have proposed a path length metric, where the
contribution of the link l on the âlengthâ of the path P depends
not only on its own weight w(l), but also on the weights
of all the links of the path P . As the authors of [5] do not
present any algorithm for computing the shortest path between
the source-destination node pair using this new metric, we
present a polynomial time algorithm for this problem in this
paper. This result is interesting because of the nature of new

978-1-4799-1633-7/14/$31.00 Â©2014 IEEE

metric proposed in [5], one key property on which the shortest
path algorithm due to Dijkstra is based, that is, subpath of a
shortest path is shortest, is no longer valid. We show that even
without this key property, not only it is possible to compute
the shortest path in polynomial time using the new metric, it
is also possible to compute the shortest path in polynomial
time, with a variation of the metric proposed in [5].
The rest of the paper is organized as follows. In section
III, we present the path length metric proposed for the FiWi
network in [5] and a variation of it. In section IV we provide
algorithms for computing the shortest path using these two
metrics. As multi-path routing offers signiï¬cant advantage
over single path routing [6], [7], [8], [9], we also consider
the problem of computation of a pair of node disjoint paths
between a source-destination node pair using the metric proposed in [5]. We show that while the single path computation
problem can be solved in polynomial time in all these cases,
the disjoint path computation problem is NP-complete. The
contributions of the paper are as follows;
â¢ Polynomial time algorithm for single path routing (metric
1) in FiWi networks
â¢ Polynomial time algorithm for single path routing (metric
2) in FiWi networks
â¢ NP-completeness proof of disjoint path routing (metric
1) in FiWi networks
â¢ Optimal solution for disjoint path routing (metric 1) in
FiWi networks using Integer Linear Programming
â¢ One approximation algorithm for disjoint path routing in
FiWi networks with an approximation bound of 4 and
computation complexity O((n + m)log n)
â¢ One approximation algorithm for disjoint path routing in
FiWi networks with an approximation bound of 2 and
computation complexity O(m(n + m)log n)
â¢ Experimental evaluation results of the approximation
algorithm for disjoint path routing in FiWi networks
II. R ELATED W ORK
Fiber-Wirelss (FiWi) networks is a hybrid access network
resulting from the convergence of optical access networks
such as Passive Optical Networks (PONs) and wireless access
networks such as Wireless Mesh Networks (WMNs) capable of providing low cost, high bandwidth last mile access.

131

Because it provides an attractive way of integrating optical
and wireless technology, Fiber-Wireless (FiWi) networks have
received considerable attention in the research community in
the last few years [1], [2], [3], [4], [5], [8], [9]. The minimum
interference routing algorithm for the FiWi environment was
ï¬rst proposed in [4]. In this algorithm the path length was
measured in terms of the number of hops in the wireless
part of the FiWi network. The rationale for this choice was
that the maximum throughput of the wireless part is typically
much smaller than the throughput of the optical part, and
hence minimization of the wireless hop count should lead to
maximizing the throughout of the FiWi network. However,
the authors of [5] noted that minimization of the wireless
hop count does not always lead to throughput maximization.
Accordingly, the path length metric proposed by them in
[5] pays considerable importance to the trafï¬c intensity at a
generic FiWi network node. The results presented in this paper
are motivated by the path length metric proposed in [5].
III. P ROBLEM F ORMULATION
In the classical path problem, each edge e â E of the graph
G = (V, E), has a weight w(e) associated with it and if there
is a path P from the node v0 to vk in the graph G = (V, E)
w

w

w

w

v0 â1 v1 â2 v2 â3 v3 . . . âk vk
then the path length or the distance between the nodes v0 and
vk is given by
w(Pv0 ,vk ) = w1 + w2 + Â· Â· Â· + wk
However, in the path length metric proposed in [5] for
optical-wireless FiWi networks [1], [2], [3], the contribution
of ei to the path length computation depends not only on
the weight wi , but also on the weights of the other edges
that constitute the path. In the following section, we discuss
this metric and a variation of it. We also also formulate the
multipath computation problem using this metric.
The Optimized FiWi Routing Algorithm (OFRA) proposed
in [5] computes the âlengthâ (or weight) of a path P from v0
to vk using the following metric




(wu ) + max (wu )
w (Pv0 ,vk ) = min
P

âuâP

âuâP

where wu represents the trafï¬c intensity at a generic FiWi
network node u, which may be an optical node in the ï¬ber
backhaul or a wireless node in wireless mesh front-end. In
order to compute shortest path using this metric, in our
formulation, instead of associating a trafï¬c intensity âweightâ
(wu ) with nodes, we associate them with edges. This can easily
be achieved by replacing the node u with weight wu with two
nodes u1 and u2 , connecting them with an edge (u1 , u2 ) and
assigning the weight wu on this edge. In this scenario, if there
is a path P from the node v0 to vk in the graph G = (V, E)
w

w

w

w

v0 â1 v1 â2 v2 â3 . . . âk vk
then the path length between the nodes v0 and vk is given by

w+ (Pv0 ,vk )

=
=

w1 + w2 + . . . + wk + max(w1 , w2 , . . . wk )
k

wi + maxki=1 wi
i=1

In the second metric, the length a path Pv0 ,vk :
v0 âv1 âv2 â . . . âvk , between the nodes v0 and vk is given
by

wÌ(Pv0 ,vk )

=
=

k

i=1
k


wi + CN T (Pv0 ,vk ) â max(w1 , w2 , . . . wk )
wi + CN T (Pv0 ,vk ) â maxki=1 wi

i=1

where CN T (Pv0 ,vk ) is the count of the number of times
max (w1 , w2 , . . . wk ) appears on the path Pv0 ,vk . We study the
shortest path computation problems in FiWi networks using
the above metrics and provide polynomial time algorithms for
solution in subsections IV-A and IV-B.
If wmax = max(w1 , w2 , . . . wk ), we refer to the corresponding edge (link) as emax . If there are multiple edges having
the weight of wmax , we arbitrarily choose any one of them as
emax . It may be noted that both the metrics have an interesting
property in that in both cases, the contribution of an edge e
on the path length computation depends not only on the edge
e but also on every other edge on the path. This is so, because
if the edge e happens to be emax , contribution of this edge
in computation of w+ (Pv0 ,vk ) and wÌ(Pv0 ,vk ) will be 2 â w(e)
and CN T (Pv0 ,vk ) â w(e) respectively. If e is not emax , then
its contribution will be w(e) for both the metrics.
As multipath routing provides an opportunity for higher
throughput, lower delay, and better load balancing and resilience, its use have been proposed in ï¬ber networks [6],
wireless networks [7] and recently in integrated ï¬ber-wireless
networks [8], [9]. Accordingly, we study the problem of
computing a pair of edge disjoint paths between a sourcedestination node pair s and d, such that the length of the
longer path (path length computation using the ï¬rst metric)
is shortest among all edge disjoint path pairs between the
nodes s and d. In subsection IV-C we prove that this problem
is NP-complete, in subsection IV-D, we provide an optimal
solution for the problem using integer linear programming,
in subsections IV-E and IV-F we provide two approximation
algorithms for the problem with a performance bound of 4
and 2 respectively, and in subsection IV-F we provide results
of experimental evaluation of the approximation algorithms.
IV. PATH P ROBLEMS IN F I W I N ETWORKS
In this section, we present (i) two different algorithms for
shortest path computation using two different metrics, (ii)
NP-completeness proof for the disjoint path problem, (iii)
two approximation algorithms for the disjoint path problem,
and (iv) experimental evaluation results of the approximation
algorithms.

132

It may be noted that, in both metrics w+ (Pv0 ,vk ) and
wÌ(Pv0 ,vk ), we call an edge e â Pv0 ,vk crucial, if w(e) =
maxki=1 w(e ), âe â Pv0 ,vk .
A. Shortest Path Computation using Metric 1
It may be recalled that the path length
k metric used in this
case is the following: w+ (Pv0 ,vk ) = i=1 wi + maxki=1 wi . If
k
the path length metric was given as w(Pv0 ,vk ) = i=1 wi ,
algorithms due to Dijkstra and Bellman-Ford could have been
used to compute the shortest path between a source-destination
node pair. One important property of the path length metric
that is exploited by Dijkstraâs algorithm is that âsubpath of a
shortest
path is shortestâ. However, the new path length metric
k
k
i=1 wi +maxi=1 wi does not have this property. We illustrate
this with the example below.
Consider two paths P1 and P2 from the node v0 to v3 in the
w
w
w
graph G = (V, E), where P1 : v0 â1 v1 â2 v2 â3 v3 and P2 :
w4
w5
w3
v0 â v4 â v2 â v3 . If w1 = 0.25, w2 = 5, w3 = 4.75, w4 =
2, w5 = 4, the length of the path P1 , w+ (P1 ) = w1 +w2 +w3 +
max(w1 , w2 , w3 ) = 0.25 + 5 + 4.75 + max(0.25, 5, 4.75) = 15
and the length of the path P2 , w+ (P2 ) = w4 + w5 + w6 +
max(w4 , w5 .w6 ) = 2 + 4 + 4.75 + max(2, 4, 4.75) = 15.5.
Although P1 is shortest path in this scenario, the length of
w
w
its subpath v0 â1 v1 â2 v2 is 0.25 + 5 + max (0.25, 5) =
10.25, which is greater than the length of a subpath of P2
w
w
v0 â4 v4 â5 v2 2 + 4 + max (2, 4) = 10, demonstrating that
the assertion that âsubpath of a shortest path is shortestâ no
longer holds in this path length metric.
As the assertion âsubpath of a shortest path is shortestâ no
longer holds in this path length metric, we cannot use the
standard shortest path algorithm due to Dijkstra in this case.
However, we show that we can still compute the shortest path
between a source-destination node pair in polynomial time by
repeated application of the Dijkstraâs algorithm. The algorithm
is described next.
For a given graph G = (V, E), w.l.o.g, we assume |V | = n
and |E| = m. Deï¬ne Ge as subgraph of G by deleting edges
whose weight is greater than w(e).
Also, as Dijkstraâs algorithm does, we need to maintain
distance vector. We deï¬ne distv be distance (length of shortest
path) from s to v, Î v be predecessor of v and maxedgev be
weight of the crucial edge from s to v via the shortest path,
ansv be optimal solution (length) from s to v.
Different Ge can be treated as different layers of the
G. For any path P , we deï¬ne the function eâ (P ) as the
crucial edge along P . It is easy to observe that if
Pd is the
optimal path from s to node d then w(Pd ) =
eâP w(e)
and w+ (Pd ) = w(Pd ) + w(eâ (Pd )). It may be noted that
henceforth, we shorten Ps,d to Pd , because we consider that
the source is ï¬xed while the destination d is variable.
Lemma 1. w(Pd ) is minimum in Geâ (Pd ) .
Proof: It is obvious that Pd still exists in Geâ (Pd ) , since
edges on Pd are not abandoned. Suppose Pd is not shortest,
then there must be another path Pd s.t. w(Pd ) < w(Pd ).
Noting that the crucial edge on Pd , namely e , is no longer than

Algorithm 1 Modiï¬ed Dijkstraâs Algorithm
1: Initialize ansv = â for for all v â V
2: sort all edges according to w(e) in ascending order
3: for i = 1 to m do
4:
Initialize distv = â, Î v = nil, maxedgev = 0 for
all v â V
5:
dists = 0
6:
Q = the set of all nodes in graph
7:
while Q is not empty do
8:
u = Extract-Min(Q)
9:
for each neighbor v of u do
10:
if eu,v â E(Gei ) then
11:
t = MAX {maxedgeu , w(eu,v )}
12:
if distu + w(eu,v ) < distv then
13:
distv = distu + w(eu,v )
14:
maxedgev = t
15:
Î v = u
16:
else if distu + w(eu,v ) == distv then
17:
if maxedgev > t then
18:
maxedgev = t
19:
Î v = u
20:
end if
21:
end if
22:
end if
23:
end for
24:
end while
25:
for each node v do
26:
ansv = min{ansv , distv + maxedgev }
27:
end for
28: end for
eâ (Pd ) since they both belong to Geâ (Pd ) . Hence w+ (Pd ) =
w(Pd )+w(e ) < w(Pd )+w(eâ (Pd )) = w+ (Pd ), contradicting
Pd is optimal.
Lemma 2. Modiï¬ed Dijkstraâs Algorithm (MDA) computes
shortest path while keeping the crucial edge as short as
possible in every iteration.
Proof: Line 4 to 24 works similar to the standard Dijkstraâs algorithm does. Besides, when updating distance, MDA
also updates the crucial edge to guarantee that it lies on the
path and when there is a tie, MDA will choose the edge with
the smaller weight.
Theorem 1. Modiï¬ed Dijkstraâs Algorithm computes optimal
solution for every node v in O(m(n + m)logn) time.
Proof: Lemma 1 indicates for any node v â V , optimal
solution can be obtained by enumerating all possible crucial
edges eâ (Pv ) and computing shortest path on Geâ (Pv ) . By sorting all edges in nondecreasing order, every subgraph Geâ (Pv )
is considered and it is shown in lemma 2, MDA correctly
computes shortest path for every node v in every Geâ (Pv ) .
Then optimal solution is obtained by examining all shortest
path using the w() metric plus the corresponding crucial edge.
Dijkstraâs algorithm runs O((n + m)logn) time when using

133

binary heap, hence MDA runs in O(m(n+m)logn) time when
considering all layers.
B. Shortest Path Computation using Metric 2
Given a path P , let eâ (P ) be the crucial edge along the
P and CN T (P ) be the number of occurrence of such edge.
Now
a path Q, such that wÌ(P ) =
 our objective becomes to ï¬nd
â
w(e)
+
CN
T
(Q)
â
w(e
(Q))
is minimum.
eâQ
The layering technique can also be used in this problem.
However, shortest path under a ceratin layer may not become
a valid candidate for optimal solution. Here, we introduce a
dynamic programming algorithm that can solve the problem
optimally in O(n2 m2 ) time.
Input is a weighted graph G = (V, E), |V | = n, |E| = m
with a speciï¬ed source node s. In this paper, we only
consider nonnegative edge weight. As shown before, we use
Ge to represent the residue graph by deleting edges longer
than e in G. Different from MDA1, in order to consider
the number of crucial edges, distv is replaced by an array
dist0v , dist1v , ....distnv . One can think distcv be the shortest
distance from s to v by going through exactly c crucial edges
and possibly some shorter edges. Similarly, we replace Î v by
Î cv , 0 â¤ c â¤ n. Each Î cv records predecessor of v for the
path corresponding to distcv . Lastly, ansv is used as optimal
solution from s to v.
Lemma 3. If Pv is the best path from s to v, i.e., wÌ(Pv ) is
minimum among all s-v path, then Pv is computed in Geâ (Pv )
CN T (Pv )
and distv
= w(Pv ).
Proof: By deï¬nition, Pv exists in Geâ (Pv ) and
CN T (Pv ) â¥ 1 since any path should go through at least one
crucial edge. Noting wÌ(Pv ) = w(Pv )+CN T (Pv )âw(eâ (Pv )),
on one hand if we treat CN T (Pv ) as a ï¬xed number, then we
need to keep w(Pv ) as small as possible. Inspired by idea
of bellman-ford algorithm, we can achieve it by enumerating
|Pv |, i.e., number of edges on Pv . On the other hand, we need
to keep tracking number of crucial edges as well. Hence, distcv
is adopted to maintain such information, superscript c reï¬ects
exact number of crucial edges. From line 12 to line 25, distcv
is updated either when it comes from a neighbor who has
already witnessed c crucial edges or it comes from a neighbor
with c â 1 crucial edges and the edge between is crucial. In
either case, node v gets a path, say P  , with exact c crucial
edges on it and w(P ) is minimum. At last, Pv can be selected
by enumerating number of crucial edges and that is what line
30 to 32 does.
Lemma 4. Maxedge Shortest Path Algorithm(MSPA) runs in
O(n2 m) time for each Ge .
Proof: We can apply similar analysis of bellman-ford
algorithm. However, we need to update distcv array, it takes
extra O(n) time for every node v in every iteration when
enumerating |Pv |. Hence, total running time is O(n2 m).
Theorem 2. MSPA computes optimal path for every v â V
in O(n2 m2 ) time.

Algorithm 2 Maxedge Shortest Path Algorithm
1: Initialize ansv = â for for all v â V
2: sort all edges according to w(e) in ascending order, say
e1 , e2 , ..., em after sorting
3: for i = 1 to m do
4:
Initialize distcv = â, Î cv = nil for all v â V and all
0â¤câ¤n
5:
dist0s = 0
6:
for j = 1 to n â 1 do
7:
for k = 0 to j do
8:
for every node v â V do
9:
if distkv = â then
10:
continue
11:
end if
12:
for every neighbor u of v do
13:
if w(eu,v ) > w(ei ) then
14:
continue
15:
else if w(eu,v ) == w(eâ ) then
16:
if distkv + w(eu,v )
<
then
distk+1
u
17:
distk+1
= distkv +
u
w(eu,v )
18:
Î k+1
=v
u
19:
end if
20:
else
21:
if distkv + w(eu,v ) < distku
then
22:
distku = distkv +
w(eu,v )
23:
Î ku = v
24:
end if
25:
end if
26:
end for
27:
end for
28:
end for
29:
end for
30:
for i = 1 to n â 1 do
31:
ansv = min{ansv , distiv + i â w(ei )}
32:
end for
33: end for

Proof: By Lemma 3, if Pv is obtained when computing
Geâ Pv . Then, by considering all possible Geâ , we could get
Pv in one of these layering. It takes O(m) to generate all Geâ ,
by Lemma 4, MSPA runs in v â V in O(n2 m2 ) time.
C. Computational Complexity of Disjoint Path Problem
In this section, we study edge disjoint path in optical
wireless network. By reduction from well known Min-Max
2-Path Problem, i.e., min-max 2 edge disjoint path problem
under normal length measurement, we show it is also
NP-complete if we try to minimize the longer path when w+
length is applied. Then we give an ILP formulation to solve
this problem optimally. At last, we provide two approximation
algorithm, one with approximation ratio 4, running time

134

O((m + n)logn), the other one with approximation ratio 2
while running time is O(m(m + n)logn).

D. Optimal Solution for the Disjoint Path Problem
Here, we give an ILP formulation for MinMax2OWFN.
ILP for MinMax2OWFN

Min-Max 2 Disjoint Path Problem (MinMax2PP)
Instance: An undirected graph G = (V, E) with a positive
weight w( e) associated with each edge e â E, a source node
s â V , a destination node t â V , and a positive number X.
Question: Does there exist a pair of edge disjoint paths P1
and P2 from s to d in G such that w(P1 ) â¤ w(P2 ) â¤ X?

min
s.t.

MP



The MinMax2PP problem is shown to be NP-complete in
[10]. With a small modiï¬cation, we show NP-completeness
still holds if w+ length measurement is adopted.

fi,j,1 â

fj,i,1 =

(i,j)âE

(j,i)âE





fi,j,2 â

(i,j)âE

Min-Max 2 Disjoint Path Problem in Optical Wireless
Networks (MinMax2OWFN)
Instance: An undirected graph G = (V, E) with a positive
weight w( e) associated with each edge e â E, a source node
s â V , a destination node t â V , and a positive number X.
Question: Does there exist a pair of edge disjoint paths P1
and P2 from s to t in G such that w+ (P1 ) â¤ w+ (P2 ) â¤ X  ?



fj,i,2 =

(j,i)âE

â§
âª
â¨
âª
â©
â§
âª
â¨
âª
â©

1
â1
0

i=s
i=t
otherwise

1
â1
0

i=s
i=t
otherwise

fi,j,1 + fi,j,2 â¤ 1
w1 â¥ fi,j,1 â w(i, j)

â(i, j) â E

w2 â¥ fi,j,2 â w(i, j)

fi,j,1 â w(i, j)
M P â¥ w1 +

â(i, j) â E

â(i, j) â E

(i,j)âE



M P â¥ w2 +

Theorem 3. The MinMax2OWFN is NP-complete

fi,j,2 â w(i, j)

(i,j)âE

Proof: Evidently, MinMax2OWFN is in NP class, given
two edge joint path P1 and P2 , we can check if w+ (P1 ) â¤
w+ (P2 ) â¤ X  in polynomial time.
We then transfer from MinMax2PP to MinMax2OWFN.
Let graph G = (V, E) with source node s , destination t and
an integer X be an instance of MinMax2PP, we construct an
instance Gâ of MinMax2OWFN in following way.
1) Create an identical graph G with same nodes and edges
in G.
2) Add one node s0 to G .
3) Create two parallel edges e01 , e02 between s0 and s,
w(e01 ) = w(e02 ) = maxeâG(E) w(e)
4) Choose s0 to the source node in G and t to be the
destination.
5) Set X  = X + 2w(e01 )
It is easy to see, the construction takes polynomial time.
Now we need to show a instance of MinMax2OWFN have
two edge disjoint paths from s0 to t with length at most X  if
and only if the corresponding instance have two edge disjoint
paths from s to t with length at most X.
Suppose there are two edge disjoint paths P1 and P2 from
s0 to t in G , such that w+ (P1 ) â¤ w+ (P2 ) â¤ X  . By the
way we construct G , P1 and P2 must go through e01 and
e02 . W.l.o.g. we say e01 â P1 and e02 â P2 . Since w(e01 ) =
w(e02 ) = maxeâE(G ) {w(e)}, therefore e01 and e02 are the
crucial edge on P1 and P2 respectively. Hence, P1 â e01 and
P2 âe02 are two edge disjoint path in G, with length no greater
than X  â 2w(e01 ) = X.
Conversely, now suppose P1 and P2 are two edge joint paths
in G satisfying w(P1 ) â¤ w(P2 ) â¤ X. We follow the same
argument above, P1 + e01 and P2 + e02 are two desired paths,
with length not exceeding X + 2w(e01 ) = X  .

fi,j,1 = {0, 1},

fi,j,2 = {0, 1}

â(i, j) â E

The following is a brief description of this ILP formulation.
The ï¬rst two equation represent ï¬ow constraint as normal
shortest path problem does. fi,j,1 = 1 indicates path P1 goes
through edge (i, j), and 0 otherwise. So it is with fi,j,2 and
path P2 . Constraint 3 ensures two edges are disjoint, since
fi,j,1 and fi,j,2 cannot both be 1 at the same time. w1 , w2 act
as the weights of the crucial edges on P1 and P2 respectively.
Finally, we deï¬ne M P to be the maximum of w+ (P1 ) and
w+ (P2 ) and therefore try to minimize it.
E. Approximation Algorithm for the Disjoint Path Problem,
with approximation factor 4
Next we propose a 4-approximation algorithm which runs
in O((n + m)logn) time.
Given G = (V, E) with source s and destination t, the idea
of approximation algorithm is to ï¬nd two disjoint P1 and P2
such that w(P1 ) + w(P2 ) is minimized. Such P1 and P2 can
be found either using min cost max ï¬ow algorithm or the
algorithm due to Suurballe presented in [11]. And we need to
show both w+ (P1 ) and w+ (P2 ) are at most four times of the
optimal solution.
Algorithm 3 MinMax2OWFN Approximation Algorithm 1 (MAA1)
1: Run Suurballeâs algorithm on G, denote P1 , P2 be two
resulting path.
2: Compute w + (P1 ) and w + (P2 ).
3: Output max{w + (P1 ), w + (P2 )}.

135

Lemma 5. For any path P , w+ (P ) â¤ 2w(P ).
Proof: By deï¬nition, w+ (P ) = w(P ) + w(eâ (P )). Since
w(e (P )) â¤ w(P ), then w+ (P ) â¤ 2 â w(P ).
â

Lemma 6. If P1 and P2 are two edge joint path from s to
t such that w(P1 ) + w(P2 ) is minimum, then w+ (P1 ) and
w+ (P2 ) are at most four times of the optimal solution.
Proof: Say opt is the optimal value of a
M inM ax2OW F N instance and Q1 ,Q2 are two s â t
edge disjoint path in one optimal solution. W.l.o.g, we may
suppose w+ (P1 ) â¥ w+ (P2 ) and w+ (Q1 ) â¥ w+ (Q2 ). Let
w(P1 ) + w(P2 ) = p and w(Q1 ) + w(Q2 ) = q, by assumption,
p â¤ q. Also, we have w+ (P1 ) = w(P1 ) + eâ (P1 ) â¤ 2p,
opt = w+ (Q1 ) = w(Q1 ) + eâ (Q1 ) > 2q . Hence,
+
w+ (P2 )
(P1 )
2p
â¤ w opt
< q/2
â¤4
opt
Theorem 4. MAA1 is a 4-approximation algorithm running
in O((n + m)logn) time and 4 is a tight bound.
Proof: By Lemma 5 and 6, MAA1 has approximation
ratio at most 4.Then we show MAA1 has approximation at
least 4 for certain cases. Consider the following graph.

Algorithm 4 MinMax2OWFN Approximation Algorithm
2(MAA2)
1: set ans = â
2: for every Ge of G do
3:
Run Suurballeâs algorithm on Ge , denote P1 , P2 be
two resulting path.
4:
Compute w+ (P1 ) and w+ (P2 ).
5:
ans = min{ans, max{w+ (P1 ), w+ (P2 )}}.
6: end for
7: Output ans.
w(P1 )+w(e )
max{w+ (Q1 ), w+ (Q2 )} < 2. We
w(P1 )+w(e )
Suppose max{w
+ (Q ), w + (Q )} â¥ 2,
1
2

w(e ). It sufï¬ces to show

prove

it by contradiction.

then

w(P1 ) + w(e ) â¥ w+ (Q1 ) + w+ (Q2 )
Which follows,
w(P1 ) + w(e ) â¥ w(Q1 ) + w(eâ (Q1 )) + w(Q2 ) + w(eâ (Q2 ))
By deï¬nition, e is one of eâ (Q1 ), eâ (Q2 ). Hence,
w(P1 ) > w(Q1 ) + w(Q2 )
It is impossible since w(P1 ) + w(P2 ) is minimum in layer
G e .
Theorem 5. MAA2 is a 2-approximation algorithm running
in O(m(n + m)logn) time and 2 is a tight bound.

It is easy to check, P1 = {s â t}, P2 = {s â r â t} are
two edge disjoint path with minimum length 2k+2, w+ (P1 ) =
4k > w+ (P2 ) = 3. However, let Q1 = {s â u1 â u2 â
... â ukâ1 â uk â r â t}, Q2 = {s â r â v1 â v2 â
... â vkâ1 â vk â t}, then w(Q1 ) + w(Q2 ) = 2k + 4 while
+
4k
1)
w+ (Q1 ) = w+ (Q2 ) = k + 3. ww+(P
Q1 = k+3 â 4 when k is
sufï¬ciently large. Hence, 4 is a tight bound for MAA1.
We need O((n + m)logn) time running Suurballeâs algorithm and O(n) time computing w+ (P1 ) and w+ (P2 ).
Therefore total running time is O((n + m)logn).
F. Approximation Algorithm for the Disjoint Path Problem,
with approximation factor 2
In MAA1, layering technique is not used and we only
consider the original graph. However, by taking all Ge of G
into account, we can have a better approximation ratio.
Say Q1 , Q2 are two disjoint paths in one optimal solution.
Let e = max{eâ (Q1 ), eâ (Q2 )} and P1 , P2 be the resulting
paths when computing layer Ge ; w.l.o.g, we may assume
w(P1 ) > w(P2 ). Also, let anse = max{w+ (P1 ), w+ (P2 )}.
Lemma 7. anse < 2 max{w+ (Q1 ), w+ (Q2 )}.
Proof: Noting that w(eâ (P1 )) â¤ w(e ) and w(eâ (P2 )) â¤
w(e ) since they both belong to Ge . Then anse â¤ w(P1 ) +


Proof: By Lemma 7, in one of the layer, we guarantee
to have a 2-approximation solution. Since we take minimum
outcome among all layers, the ï¬nal result is no worse than
twice of the optimal solution. Now we need to show there
exists certain case, such that MAA2 is no good than twice of
the optimal solution. Consider the following graph

There is only one layer, and P1 = {s â x1 â x2 â
... â x2kâ1 â x2 k â t}, P2 = {s â r â t} are two
edge disjoint path with minimum length 2k + 3, w+ (P1 ) =
2k + 2 > w+ (P2 ) = 3. Again, set Q1 = {s â u1 â u2 â
... â ukâ1 â uk â r â t}, Q2 = {s â r â v1 â v2 â
... â vkâ1 â vk â t}, then w(Q1 ) + w(Q2 ) = 2k + 4 while
+
2k+2
1)
w+ (Q1 ) = w+ (Q2 ) = k + 3. ww+(P
Q1 = k+3 â 2 when k is
sufï¬ciently large. Hence, 2 is a tight bound for MAA2.
Finally, it is easy to see that the running time is O(m(n +
m)logn).

136

S
node
14
18
1
18
20
10
1
14
20
10
18
1
20
14
10
20
5

D
node
2
8
6
4
3
3
11
6
7
5
12
20
13
19
17
16
11

Opt
Sol
47
46
28
50
40
27
35
50
38
36
22
46
26
29
36
29
40

Approx
Sol 1
55
46
28
58
40
27
35
52
38
38
22
52
26
29
36
29
48

Approx
ratio 1
1.17
1
1
1.16
1
1
1
1.04
1
1.05
1
1.13
1
1
1
1
1.2

Approx
Sol 2
55
46
28
57
40
27
35
52
38
38
22
52
26
29
36
29
48

Approx
ratio 2
1.17
1
1
1.14
1
1
1
1.04
1
1.05
1
1.13
1
1
1
1
1.2

Fig. 1.

TABLE I
C OMPARISON OF THE A PPROXIMATE SOLUTIONS WITH THE O PTIMAL
SOLUTION FOR THE ARPANET GRAPH

The ARPANET graph with 20 nodes and 32 links

approximation algorithms. Both the approximation algorithms
have a constant factor approximation bound. However, there is
a trade-off between the quality of the solution (approximation
bound) and the execution time. Finally, we show that both the
approximation algorithms obtain near optimal results through
simulation using the ARPANET topology.

G. Experimental Results for the Disjoint Path Problem
In this section, we present the results of simulations for
comparing the performance of our approximation algorithms
with the optimal solution when w+ () metric is applied.
The simulation experiments have been carried out on the
ARPANET topology (as shown in Fig 1 with nodes and links
shown in black) which has twenty nodes and thirty two links.
The weights of the links have been randomly generated and
lie in the range of two and eleven (as shown in red in Fig
1) and we consider the graph to be undirected. The results of
the comparison is presented in Table I. We have compared the
lengths of the longer of the two edge disjoint paths computed
by the optimal and the approximate solutions for seventeen
different source-destination pairs. It may be noted that for
almost 65% of the cases, the approximate algorithms obtain
the optimal solution. In the remaining cases, the approximate
solutions lie within a factor of 1.2 of the optimal solution
Thus, even though the approximation ratio in the worst case
are proven to be 4 and 2, in practical cases, it is within 1.2.
From these experimental results, we can conclude that the
approximation algorithms produce optimal or near optimal
solutions in majority of the cases. It may be noted that the
two approximation algorithms perform in a similar fashion
in the ARPANET graph, however, as proven theoretically,
the two approximation algorithms differ in their worst case
approximation ratio.
V. C ONCLUSION

R EFERENCES
[1] N. Ghazisaidi, M. Maier, and C. M. Assi, âFiber-Wireless (FiWi) Access
Networks: A Surveyâ, IEEE Communications Magazine, vol. 47, no. 2,
pp 160-167, Feb. 2009.
[2] N. Ghazisaidi, and M. Maier, âFiber-Wireless (FiWi) Access Networks:
Challenges and Opportunitiesâ, IEEE Network, vol. 25, no. 1, pp 36-42,
Feb. 2011.
[3] Z. Zheng, J. Wang, X. Wang, âONU placement in ï¬ber-wireless (FiWi)
networks considering peer-to-peer communicationsâ, IEEE Globecom, 2009.
[4] Z. Zheng, J. Wang, X. Wang, âA study of network throughput gain
in optical-wireless (FiWi) networks subject to peer-to-peer commuincationsâ, IEEE ICC, 2009.
[5] F. Aurzada, M. Levesque, M. Maier, M. Reisslein, âFiWi Access
Networks Based on Next-Generation PON and Gigabit-Class WLAN
Technologies: A Capacity and Delay Analysisâ, IEEE/ACM Transactions
on Networking, to appear.
[6] A. Sen, B.Hao . B. Shen , L.Zhou and S. Ganguly, âOn maximum
available bandwidth through disjoint pathsâ, Proc. of IEEE Conf. on
High Performance Switching and Routing, 2005.
[7] M. Mosko, J.J. Garcia-Luna-Aceves, âMultipath routing in wireless
mesh networksâ, Proc. of IEEE Workshop on Wireless Mesh Networks, 2005.
[8] J. Wang, K. Wu, S. Li and C. Qiao ,âPerformance Modeling and Analysis
of Multi-Path Routing in Integrated Fiber-Wireless (FiWi) Networksâ,
IEEE Infocom mini conference, 2010.
[9] S. Li, J. Wang, C. Qiao, Y. Xu ,âMitigating Packet Reordering in
FiWi Networksâ, IEEE/OSA Journal of Optical Communications and
Networking, vol. 3, pp.134-144, 2011.
[10] C. Li, S.T. McCormick and D.Simchi-Levi, âComplexity of Finding Two
Disjoint Paths with Min- Max Objectiveâ, Discrete Applied Mathematics, vol. 26, pp. 105-115, 1990.
[11] J. W. Suurballe, âDisjoint paths in a networkâ, Networks, vol. 4, pp. 125145, 1974.

In this paper, we study the shortest path problem in FiWi
networks. Based on the path length metrics proposed in [3],
[5], we present polynomial time algorithms for the single
path scenario. In the disjoint path scenario, we prove that the
problem of ï¬nding a pair of disjoint paths, where the length
of the longer path is shortest, is NP-complete. We provide an
ILP solution for the disjoint path problem and propose two

137

Partitioning Signed Bipartite Graphs for
Classification of Individuals and Organizations
Sujogya Banerjee, Kaushik Sarkar, Sedat Gokalp,
Arunabha Sen, and Hasan Davulcu
Arizona State University
P.O. Box 87-8809, Tempe, AZ, 85281 USA
{sujogya,kaushik.sarkar,sedat.gokalp,asen,hdavulcu}@asu.edu

Abstract. In this paper, we use signed bipartite graphs to model opinions expressed by one type of entities (e.g., individuals, organizations)
about another (e.g., political issues, religious beliefs), and based on the
strength of that opinion, partition both types of entities into two clusters. The clustering is done in such a way that support for the second
type of entity by the first within a cluster is high and across the cluster
is low. We develop an automated partitioning tool that can be used to
classify individuals and/or organizations into two disjoint groups based
on their beliefs, practices and expressed opinions.

1

Introduction

The goal of the Minerva1 project, currently underway at Arizona State University is to increase understanding of movements within Muslim communities
actively working to counter violent extremism. As a part of this study, we have
collected over 800,000 documents from web sites various organizations in Indonesia. Based on the support and opposition of certain beliefs and practices, we can
partition the set of organizations O into two groups O1 and O2 and the set of
beliefs and practices B into two groups, B1 and B2 , such that organizations in O1
support B1 and oppose B2 , while the organizations O2 support B2 and oppose
B1 . With the domain knowledge of the social scientists in our team regarding
the beliefs and practices of Indonesian community, we can then label one group
as being radical and other as counter-radical.
Although the motivation for our work was driven by Minerva, the the problem
that is being addressed in this paper is much broader in nature. In the mathematical sociology community, the problem is known as the Signed two-mode network
partitioning problem [1]. In its mathematical abstraction, the problem is specified by a bipartite graph G = (U âª V, E) and label function Ï : E â {P, N }.
The node sets U and V may be representing the set of organizations O and the
set of beliefs B respectively. If the label of an edge from oi â O to bj â B is
P , it implies oi supports (or has positive opinion) about bj . If the label of an
edge is N , it implies oi opposes (or has negative opinion) about bj . The goal of
1

A project sponsored by the U.S. Department of Defense

(a)

(b)

Fig. 1: Partitioning of the node set U and V with the desired goal
the partitioning problem is to divide the node sets U and V into two subsets
(U1 , U2 ) and (V1 , V2 ) respectively, such that
1. number of P edges (positive opinion or support) between nodes within block
1 (P11 between U1 and V1 ) and block 2 (P22 between U2 and V2 ) is high,
2. number of P edges between nodes across block 1 and block 2 (edges P12
between U1 and V2 and P21 between U2 and V1 ) is low,
3. number of N edges (negative opinion or opposition) between nodes within
block 1 (N11 between U1 and V1 ) and block 2 (N22 between U2 and V2 ) is
low and
4. number N edges between nodes across block 1 and block 2 (edges N12 between U1 and V2 and N21 between U2 and V1 ) is high.
The goal of partitioning is depicted in Fig. 1, where the green edges indicate
support (i.e, P edges) and the red edges indicate opposition (i.e, N edges). We
can realize these goals by maximizing [(P 11 + P 22 + N 12 + N 21) â (P 12 + P 21 +
N 11 + N 22)].
Signed two-mode network partitioning problem can be applied in a multitude
of domains, where the node sets U and V can represent different entities. For
example, (i) U and V may represent the members of the U.S. Senate/House of
Representatives and the bills before the senate/house of representatives where
they cast their votes, either supporting or opposing the bill; (ii) U and V may
represent the political blogs/bloggers and various issues confronting the nation,
where they express their opinions either supporting or opposing issues. Clearly,
availability of an automated tool that will co-cluster the entities represented by
U and V , will be valuable to individuals and organizations that need a coarse
grain (two-modal) partitioning of the data set represented by the node set U
and V . This tool can help classify individuals or organizations as radicals vs.
counter-radicals, or liberals vs. conservatives or violent vs. non-violent, etc.
The main contribution of this effort is the development of a fast automated
tool (and associated algorithms) for co-clustering the entities represented by the
node sets U and V . We first compute an optimal solution of the partitioning
problem using an integer linear program to be used as a benchmark for our
heuristic solution. We then develop a heuristic solution and compare its performance using three real data sets. The real data sets include voting records
of the Republican and Democratic members of the 111th US Congress and the
opinions expressed in top twenty two liberal and conservative blogs. In all these

data sets our partitioning tool produces high quality solution (i.e., with low misclassification) at a low cost (in terms of computation time). To the best of our
knowledge, our Minerva research group is the first to present an efficient computational technique for partitioning of signed bipartite graph and apply it to
some real data sets.

2

Related works

As the literature on clustering, classification and partitioning is really vast, due
to page limitations, we only refer to the ones that are most relevant to this
paper [1â4, 8, 7]. The two key features of the partitioning problem addressed in
this paper are (i) the graph is bipartite and (ii) the weights on the edges are
signed (i.e., the weights are both positive and negative). Simultaneous clustering
of two sets of entities (represented by two sets of nodes in the bipartite graph)
was considered in the context of document clustering in [4, 8]. In these studies one
set of entities are the documents and the other set is terms or words. Although
these efforts study the bipartite graph partition problem, they are distinctly
different from our study in one respect. In our study, the edge weights are signed,
whereas the edges weights considered in [4, 8] are unsigned. Graph partitioning
problem with signed edge weights was studied in [2, 3]. However, these studies are
also distinctly different from our study in that, while they focus on partitioning
general (i.e., arbitrary) graphs, we focus our attention to partitioning bipartite
graphs. The study that comes closest to our study is [1, 7], where attention is
focused on partitioning of a signed bipartite graphs. However, neither [1] nor
[7] present any efficient algorithm to solve the partitioning problem in signed
bipartite graph.

3

Problem Formulation

In this section we formally define the partitioning problem.
Signed Bipartite Graph Partition Problem (SBGPP): An edge labeled weighted
bipartite graph G = (U âª V, E) where U = {u1 , u2 , . . . , un } represents entities
of type I and V = {v1 , v2 , . . . , vm } represents entities of type II. Each edge
(u, v) â E has two functions associated with it: (i) label function Ï : E â {P, N },
which indicates the type of opinion (positive or negative), and (ii) weight function
w : E â Z, which indicates the strength of that opinion. AN = [wn (u, v)] and
AP = [wp (u, v)] are the weighted adjacency matrix for edges with label N and P
respectively. If the node set U is partitioned into U1 and U2 and V is partitioned
into V1 and V2 , the strength of the positive and negative opinions of the entities
of type I regarding the entities of type II are defined as follows:
For all edges (u, v) â E,
X X
X X
X X
P11 =
wp (u, v), P12 =
wp (u, v), P22 =
wp (u, v)
P21 =

uâU1 vâV1

uâU1 vâV2

X X

X X

uâU2 vâV1

wp (u, v), N11 =

uâU1 vâV1

uâU2 vâV2

wn (u, v), N12 =

X X

uâU1 vâV2

wn (u, v)

N22 =

X X

wn (u, v), N21 =

uâU2 vâV2

X X

wn (u, v)

uâU2 vâV1

Problem: Find a partition of the node set U into U1 and U2 and V into V1 and V2
such that [(P 11 + P 22 + N 12 + N 21) â (P 12 + P 21 + N 11 + N 22)] is maximized.

4

Computational Techniques

In this section we give a mathematical programming technique to find the optimal solution for the SBGPP. Since computational time for finding optimal solution for large graphs is unacceptably high, we present a heuristic in subsequent
section to solve the SBGPP.
4.1

Optimal Solution for SBGPP

The goal of the SBGPP is to partition U into two disjoint sets U1 and U2
(similarly V into V1 and V2 ). For each node in u â U and each partition Ui , i =
1, 2, we use a variable bui . bui is 1 iff in u is in Ui . Similarly we define variable
pvi for all v â V . We will refer B1 = U1 âª V1 and B2 = U2 âª V2 as blocks 1 and
2 respectively.
V ariables: For each node u â U, v â V and each partition Ui , Vi , i = 1, 2
(
(
1, if node u is in partition Ui
1, if node v is in partition Vi
bui =
pvi =
0, otherwise.
0, otherwise.
The mathematical programming formulation is given as follows:
max

L=

2 X X
X

(wp (u, v) â wn (u, v))bui pvi

i=1 uâUi vâVi

+

2
X
X X

(wn (u, v) â wp (u, v))bui pvj

i,j=1 uâUi vâVj
i6=j

s.t

bu1 + bu2 = 1,

âu â U

(1)

pu1 + pu2 = 1,

âp â V

(2)

The objective function computes the objective value given by the expression L.
We want to maximize L. It may be noted that the above quadratic objective
function can easily be changed into a linear function by simple variable transformation [6]. Constraint 1 and 2 ensures that each node in U and V belongs to
one particular block.
4.2

Move-based Heuristics

We present a move-based heuristic to find an approximate solution of SBGPP.
The move-based heuristic is a variant of well known FM algorithm [5] for partitioning graphs. The algorithm starts with a random initial partition and iteratively moves nodes from one block to another such that the value of the objective

function is improved. The âgainâ of a node is defined as the value by which the
objective function increases if the node is moved from one block to the other.
In each iteration the node with the highest gain is moved from one block to
the other. In case of a tie a node is chosen arbitrarily. After a node is moved,
it is locked and is not moved until the next pass. The heuristic is presented in
Algorithm 1. It should be noted that original FM algorithm will not work for our
problem as SBGPP relates to signed bipartite graphs with a completely different
objective function and doesnât have any size constraints. As a result the node
gain computation routine Algorithm 2 is considerably different from the original
FM algorithm. Algorithm 1 runs for r different initial random partition of the
nodes to avoid the possibility of being stuck at a local maxima. In practice the
heuristic converges very fast, mostly in 2 to 3 passes.
Algorithm 1: Move-based Heuristic (MBH)

13

Input : A weighted signed bipartite graph H = (U âª V, E)
Output: A partition of the nodes U1 , U2 and V1 , V2 such that objective value L
is maximum
L ââ 0;
for i ââ 1 to r do
Generate a random partitioning of the nodes in U into U1 and U2 and nodes
in V into V1 and V2 ;
repeat
Compute gains of all nodes using Algorithm 2 ;
repeat
Among all the unlocked nodes select the node of highest gain. Move
the node to the other block and call it base node. Lock the base
node;
Update the node gains of all the free neighbors of the base node;
until Until all the nodes are locked ;
Change the current partition into a new partition that has the largest
value of the objective function in this pass ;
Unlock all the nodes;
until If the objective value Lâ² improves during the last pass;
if Lâ² â¥ L then Lâ² â L and save the current partition

14

return L and the final partition of nodes

1
2
3
4
5
6
7

8
9
10
11
12

5

Experimental Results and Discussions

To validate the effectiveness of our heuristic and benchmark its performance we
tested the heuristic both on synthetic and real world data. The real world data
consists of US Congress (SENATE, REP) and political blogosphere (BLOG) data
sets.
5.1

US Congress Data [SENATE, REP]

The US Congress has been collecting data since the very first congress of the US
history. This data has been encoded as XML files and publicly shared through

Algorithm 2: Node Gain Computation
Input : A weighted signed bipartite graph G = (U âª V, E)
Output: Gains of all nodes
foreach node u â U âª V do
gain(u) ââ 0;
// FBlock = "from block" of node u, ToBlock = "to block" of node
u, w(e) = weight of edge e and # = number
foreach edge e â E with l(e) = N of node u do
if # nodes of e in ToBlock is 0 then gain(u) â gain(u) + 2 â w(e);
if # nodes of e in FBlock is 1 then gain(u) â gain(u) â 2 â w(e);

1
2

3
4
5

foreach edge e â E with l(e) = P of node u do
if # nodes of e in ToBlock is 0 then gain(u) â gain(u) â 2 â w(e);
if # nodes of e in FBlock is 1 then gain(u) â gain(u) + 2 â w(e);

6
7
8

the govtrack.us project2 . From various types of data available at the project
site, we collected the roll call votes for the 111th US Congress which includes
The Senate and The House of Representatives and covers the years 2009-2010.
The 111th Senate data contains information about 108 senators and their votes
on 696 bills3 . The 111th Congress has 451 representatives and the data contains
their vote on 1655 bills.
We extracted the SENATE and REP data in adjacency matrices A|U|Ã|V | ,
with U vertices representing the congressmen, and the V vertices representing
the bills. The edge (ui , vj ), ui â U, vj â V has weight 1 if the congressman
ui votes âYeaâ for the bill vj , â1 if the congressman votes âNayâ, and 0 if he
did not attend the session. We have the original classification vector for both
the congressmen and the bills in terms of which party they represent (or which
party sponsored the bill). The first two columns of Table 1 provide information
about this data as well as the partitioning accuracies of the algorithms. Figure
2 depicts the partitioned vote matrices of the 111th US Congress data, where
rows representing the congressmen and the columns representing the bills. Also,
the light green color represents âYeaâ votes, and dark red represents âNayâ votes.
5.2

Blog Data [BLOG]

As Web 2.0 platforms gained popularity, it became easy for web users to be a
part of the web and express their opinions, mostly through blogs. Most blogs
are maintained by individuals, whereas there are also professional blogs with a
group of authors. In this study, we focus on a set of popular political liberal or
conservative blogs that have a clearly declared positions. These blogs contain
discussions about social, political, economic issues and related key individuals.
2
3

http://www.govtrack.us/data
Normally, each congress has 100 senators (2 from each state), however in many of
the congresses, there are unexpected changes on the seats caused by displacements
or deaths.

(a) 111th US House

(b) 111th US Senate

Fig. 2: Vote matrix of US Congress after partitioning

Table 1: Descriptive summaries of the graphs for each dataset with the Heuristic
accuracy

Vertices in V

111th US Senate
64 Democrat
42 Republican
Senator
696 Bills

111th US House
268 Democrat
183 Republican
Representatives
1655 Bills

Graph Density
Heuristic accuracy

88.36 %
100.00%

91.23 %
99.56%

Vertices in U

Political Blogosphere
13 Liberal
9 Conservative
Blogs
20 Liberal
14 Conservative People
39.04 %
98.21%

They express positive sentiment towards individuals whom they share ideologies
with, and negative sentiment towards the others. In these blogs, it is also common
to see criticism of people within the same camp, and also support for people from
the other camp.
In this experiment, we collected a list of 22 most popular liberal and conservative blogs from the Technorati4 rankings. For each blog, we fetched the posts
for the period of 6 months before the 2008 US presidential elections (May - October, 2008). We expected to have high intensity of the debates and discussions
and resulting in a bipolar clustering in the data. Table 2 shows the partial list
of blogs with their URLs, political camps and the number of posts for the given
period.
We use AlchemyAPI5 to run a named entity tagger to extract the people
names mentioned in the posts, and an entity-level sentiment analysis which provided us with weighted and signed sentiment (positive values indicating support,
and negative indicating opposition) for each person. This information was used
to synthesize a signed bipartite graph (the BLOG data), where the blogs and
people correspond to the two sets of vertices U and V . The aij values of the adjacency matrix A are the cumulative sum of sentiment values for each mention
of the person vj by the blog ui .
4
5

http://technorati.com
http://www.alchemyapi.com

To get a gold standard list of the most influential liberal and conservative
people, we used The Telegraph List6 for 2007. The third column of Table 1
provides information about this data as well as the partitioning accuracies of
the algorithm.
Table 2: Political Blogs
Blog name
URL
Huffington Post
http://www.huffingtonpost.com/
Daily Kos
http://www.dailykos.com/
Boing Boing
http://www.boingboing.net/
Crooks and Liars
http://www.crooksandliars.com/
Firedoglake
http://www.firedoglake.com/
Hot Air
http://hotair.com/
Reason - Hit and Run http://reason.com/blog
Little green footballs http://littlegreenfootballs.com/
Atlas shrugs
http://atlasshrugs2000.typepad.com/
Stop the ACLU
http://www.stoptheaclu.com/
Wizbangblog
http://wizbangblog.com/

6

Political view
Liberal
Liberal
Liberal
Liberal
Liberal
Conservative
Conservative
Conservative
Conservative
Conservative
Conservative

Size
3959
1957
1576
1497
1354
1579
1563
787
773
741
621

Conclusion

In this paper we study the problem of partitioning signed bipartite graph with
relevant application in political, religious and social domains. We provided a fast
heuristic to find the solution for this problem. We tested the high accuracy of
our heuristic on three sets of real data collected from political domain.

References
1. Andrej, M., Doreian, P.: Partitioning signed two-mode networks. Journal of Mathematical Sociology 33, 196â221 (2009)
2. Bansal, N., Blum, A., Chawla, S.: Correlation clustering. In: MACHINE LEARNING. pp. 238â247 (2002)
3. Charikar, M., Guruswami, V., Wirth, A.: Clustering with qualitative information.
In: Proceedings of the 44th Annual IEEE FOCS (2003)
4. Dhillon, I.S.: Co-clustering documents and word using bipartite spectral graph partitioning. In: Proceedings of the KDD. IEEE (2001)
5. Fiduccia, C., Mattheyses, R.: A linear-time heuristic for improving network partitions. In: Papers on Twenty-five years of electronic design automation. pp. 241â247.
ACM (1988)
6. Sen, A., Deng, H., Guha, S.: On a graph partition problem with application to vlsi
layout. Inf. Process. Lett. 43(2), 87â94 (1992)
7. Zaslavsky, T.: Frustration vs. clusterability in two-mode signed networks (signed
bipartite graphs) (2010)
8. Zha, H., He, X., Ding, C., Simon, H., Gu, M.: Bipartite graph partitioning and data
clustering. In: Proceedings of the 10th International Conference on Information and
Knowledge Management. pp. 25â32. ACM (2001)
6

The-top-US-conservatives-and-liberals.html

INL/EXT-06-11464

Critical Infrastructure Interdependency Modeling: A Survey of U.S. and International Research
P. Pederson D. Dudenhoeffer S. Hartley M. Permann August 2006

The INL is a U.S. Department of Energy National Laboratory operated by Battelle Energy Alliance

INL/EXT-06-11464

Critical Infrastructure Interdependency Modeling: A Survey of U.S. and International Research

P. Pedersona D. Dudenhoefferb S. Hartleyb M. Permannb
a

b

Technical Support Working Group, Washington D.C. Idaho National Laboratory

August 2006

Idaho National Laboratory Idaho Falls, Idaho 83415

Prepared for the Technical Support Working Group Under Work for Others Agreement 05734 Under DOE Idaho Operations Office Contract DE-AC07-05ID14517

ABSTRACT
"The Nation's health, wealth, and security rely on the production and distribution of certain goods and services. The array of physical assets, processes, and organizations across which these goods and services move are called critical infrastructures."1 This statement is as true in the U.S. as in any country in the world. Recent world events such as the 9-11 terrorist attacks, London bombings, and gulf coast hurricanes have highlighted the importance of stable electric, gas and oil, water, transportation, banking and finance, and control and communication infrastructure systems. Be it through direct connectivity, policies and procedures, or geospatial proximity, most critical infrastructure systems interact. These interactions often create complex relationships, dependencies, and interdependencies that cross infrastructure boundaries. The modeling and analysis of interdependencies between critical infrastructure elements is a relatively new and very important field of study. The U.S. Technical Support Working Group (TSWG) has sponsored this survey to identify and describe this current area of research including the current activities in this field being conducted both in the U.S. and internationally. The main objective of this study is to develop a single source reference of critical infrastructure interdependency modeling tools (CIIMT) that could be applied to allow users to objectively assess the capabilities of CIIMT. This information will provide guidance for directing research and development to address the gaps in development. The results will inform researchers of the TSWG Infrastructure Protection Subgroup of research and development efforts and allow a more focused approach to addressing the needs of CIIMT end-user needs. This report first presents the field of infrastructure interdependency analysis, describes the survey methodology, and presents the leading research efforts in both a cumulative table and through individual datasheets. Data was collected from open source material and when possible through direct contact with the individuals leading the research.

iii

iv

Table EX-1. Summary of areas surveyed.
Area Model by Infrastructure Sector Simulation Type System Model Hardware Platform Requirements PC HPC Windows Software Requirements User and Maturity Levels

Simulation Name X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X Maturity: RS DV MI MC Research Development Mature Internal Mature Commercial X X X X X X X X X X X X X A A A A A X X X X X X X X X X X X X X X X X X X A X X X X X X X X X X X X X X X X X X X X X X X X X I X A X X X X I X X X X A X X A X X X X X X X X X X X X X X X X X X X X X X X X X X X X A X X X X X X X X X X X X X X X X X X X X X X X X A X A X X X

Developer

Electric Natural Drinking Power Gas Water Sewage Water SCADA Telecom Continuous Discrete Integrated Coupled Storm Water Human Activity Financial Networks Computer Networks Oil Pipeline Rail System Highway System Waterway System

Police/ Regulatory Constraints

Linux

Solaris

Users IA X X EA B B X X B X X B

Maturity Level RS MI MC MI DV MI MI

1

AIMS

UNB

2

Athena

On Target Technologies, Inc.

3

CARVER2

National Infrastructure Institute

4

CI3

ANL

5

CIMS

INL

6

CIP/DSS

LANL, SNL, ANL

7

CIPMA

Australia

8

CISIA

University Roma Tre

9

COMM-ASPEN

SNL

IA B IA X X X IA IA B X X IA IA IA

DV MC MI DV DV MC MI RS RS

10

DEW

EDD

11

EMCAS

ANL

12

FAIT

SNL

13

FINSIM

LANL

14

Fort Future

USACE

15

IEISS

LANL

16

IIM

UV

17

Knowledge Management and Visualization

CMU

18

MIN

Purdue

19

MUNICIPAL

RPI

20

N-ABLE

SNL

IA

MI DV IA B B RS MC MI RS

21

NEMO

SPARTA

22

Net-Centric GIS

York University

23

NEXUS Fusion Framework

IntePoint, LLC.

24

NGtools

ANL

25

NSRAM

JMU

26

PFNAM

ANL

27

TRAGIS

ORNL

MI X X X B IA IA MC MI DV

28

TRANSIMS

LANL

29

UIS

LANL

30

WISE

LANL

Simulation Type: I Input-Output Model A Agent-based

Users: IA EA B

Internal Analyst External Analyst Both

See Notes.

v

NOTES:

1

AIMS (Agent-Based Infrastructure Modeling and Simulation) is an agent-based system to simulate and model the (national and cross-border) interdependencies and survivability of Canada's critical infrastructures. Point of Contact (POC): Dr. Stephen Marsh, Adjunct Professor, University of New Brunswick, Canada, Stephen.Marsh@nrc-cnrc.gc.ca

2

Athena is an analysis and modeling tool that is designed to analyze a network of nodes (actors, concepts, and physical) as a "system of systems" by merging various political, military, economic, social, information, and infrastructure (PMESII) models and their associated crossdependencies. Athena incorporates several reasoning algorithms that allow sophisticated inter- and intra-dependency analysis between and through nodes. POC: Dr. Brian Drabble, On Target Technologies, brain@ontgttech.com

3

CARVER2 is a simple software program that provides a quick and easy way to prioritizes potential terrorist targets. It compares and rates the critical infrastructure and key assets in jurisdictions by producing a mathematical score for each potential target. It is the first step for conducting more in-depth vulnerability assessments. CARVER2 helps users make "apples vs. oranges" comparisons such as a water system vs. an energy grid vs. a bridge. POC: Ronald Peimer, National Infrastructure Institute Center for Infrastructure Expertise, rpeimer@ni2.org

4

CI3 (Critical Infrastructures Interdependencies Integrator) is a software tool for emulating (Monte Carlo simulation) the amount of time or cost (or both) needed for activities that must be completed to restore a given infrastructure component, a specific infrastructure system, or an interdependent set of infrastructures to an operational state. The software tool provides a framework for recognizing interdependencies and incorporating uncertainty into the analysis of critical infrastructures. POC: Dr. James Peerenboom, Argonne National Laboratory, jpeerenboom@anl.com

5

CIMS (Critical Infrastructure Modeling System) is a high level M&S tool that allows visualization in a 3D environment the cascading consequence of infrastructure perturbations. Events can be scripted or assets directly manipulated within the environment during a simulation run to illustrate consequence. POC: Don Dudenhoeffer, Idaho National Laboratory, Donald.Dudenhoeffer@inl.gov

6

CIP/DSS (Critical Infrastructure Protection Decision Support System) simulates the dynamics of individual infrastructures and couples separate infrastructures to each other according to their interdependencies. CIP/DSS models asset information at the aggregate level. For example, a focus area can estimate the number of hospital beds affected by an event, but it cannot directly retrieve information relative to a particular hospital. It utilizes the commercial simulation software Vensim. POC: Randy Michelsen, Los Alamos National Laboratory, rem@lanl.gov POC: Theresa Brown, Sandia National Laboratories, tjbrown@sandia.gov

7

CIPMA (Critical Infrastructure Protection Modeling and Analysis) is a computer based tool to support business and government decision making for critical infrastructure (CI) protection, counter-terrorism, and emergency management, especially with regard to prevention, preparedness, and planning and recovery. POC: Australian Government ­ Attorney General's Department (AGD), Michael Jerks ­ Director, Major Projects, Michael.Jerks@ag.gov.au

8

CISIA (Critical Infrastructure Simulation by Interdependent Agents) is described by the authors as a hybrid of the two modeling approaches; interdependency analysis and system analysis. It is a bottom-up complex adaptive systems (CAS) model using interactive agents. The CISIA simulator is designed to analyze short term effects of failures in terms of fault propagation and performance degradation (Panzieri, 2004). POC: Stefano Panzieri Universita Roma Tre, Italy, panzieri@uniroma3.it

9

DEW (Distribution Engineering Workstation) provides over 30 applications for analysis, design, and control of electrical and other physical network systems. DEW allows all of its components (data sets and algorithms) to be reused by a new application, allowing new solutions to build on top of existing work. This provides for cross collaborations among different groups and the emergence of solutions to complex problems. DEW is being used to identify and analyze interdependencies in large scale electrical power systems and fluid systems of aircraft carriers. DEW is open architecture, non-proprietary. POC: Electrical Distribution Design, Inc., Dr. Robert Broadwater, dew@vt.edu

10

EMCAS (Electricity Market Complex Adaptive System) combines engineering techniques with quantitative market analysis: DC load flow models allow you to simulate the actual operation of the physical system configuration as well as regulatory rules imposed on market operations. POC: Guenter Conzelmann, Argonne National Laboratory, guenter@anl.gov

11

FAIT (Fast Analysis Infrastructure Tool) is primarily an economic analysis tool utilizing REMI to conduct economic impact assessment across multiple sectors. It does promote interdependency discovery for first order relationships. The program resides on an SNL server and supports web access. POC: Theresa Brown, Sandia National Laboratories, tjbrown@sandia.gov

12

FINSIM is an agent-based model of cash and barter transactions that are dependant on contractual relationships and a network at the federal reserve level. Agent based models create transactions which rely on telecommunications and electric power. POC: Randy Michelsen, Los Alamos National Laboratory, rem@lanl.gov

13

Fort Future is a collaborative, web-based planning system that uses simulation to test plans for Department of Defense (DoD) installations. It uses an open, service-oriented architecture to allow multiple simulations to be run simultaneously from the same set of alternative, organized into a study. The web-based workbench provides geographic information system (GIS)-based plan editors, controls simulations, and organizes results into a decision matrix. Fort Future assesses the impact of critical infrastructure on mission using a "Virtual Installation" simulation that contains models for transportation, electrical power, water systems, including waterborne chemical/biological/radiological (CBR) agents, airborne CBR plume, facilities, mission tasks and processes, agents, and dynamic plans. POC: U.S. Army Corps of Engineers, Engineer Research and Development Center, Construction Engineering Research Laboratory (CERL), Dr. Michael P. Case, Michael.P.Case@erdc.usace.army.mil

14

IEISS (Interdependent Energy Infrastructure Simulation System) is an actor-based infrastructure modeling, simulation, and analysis tool designed to assist individuals in analyzing and understanding interdependent energy infrastructures. The actor-based infrastructure components were developed in IEISS to realistically simulate the dynamic interactions within each of the infrastructures, as well as, the interconnections between the infrastructures. POC: Randy Michelsen, Los Alamos National Laboratory, rem@lanl.gov

15

IIM (Inoperability input-output model) based on Leontief's input-output model, characterizes interdependencies among sectors in the economy and analyzes initial disruptions to a set of sectors and the resulting ripple effects. POC: Yacov Y. Haimes, University of Virginia, haimes@virginia.edu

16

Knowledge Management and Visualization is a research project to analyze vulnerabilities associated with delivery of fuel. It is designed to help ensure availability of supply and to visualize the impacts for decision support. The project has focused on coal deliveries to power plants because, while vulnerabilities at the power plant level (production) are easier to identify, vulnerabilities and impacts associated with delivery of fuel are more uncertain. Also, data on coal shipments is readily available. POC: Carnegie Mellon University, H. Scott Matthews, hsm@cmu.edu

17

MIN (multilayer infrastructure network) is a preliminary network flow equilibrium model of dynamic multilayer infrastructure networks in the form of a differential game involving two essential time scales. In particular, three coupled network layers--automobiles, urban freight, and data--are modeled as being comprised of Cournot-Nash dynamic agents. An agent-based simulation solution structure is introduced to solve the flow equilibrium and optimal budget allocation problem for these three layers under the assumption of a super authority that oversees investments in the infrastructure of all three technologies and thereby creates a dynamic Stackelberg leader-follower game. POC: Purdue School of Civil Engineering, Dr. Srinivas Peeta, peeta@purdue.edu

vi

18

MUNICIPAL (Multi-Network Interdependent Critical Infrastructure Program for Analysis of Lifelines) is a GIS user interface, built on a formal, mathematical representation of a set of civil infrastructure systems that explicitly incorporates the interdependencies among them. The mathematical foundation or decision support system is called the interdependent layered network (ILN) model. ILN is a mixed-integer, network-flow based model implemented in software drawing on a database containing infrastructure attributes. MUNICIPAL provides the capability to understand how a disruptive event affects the interdependent set of civil infrastructures. POC: Rensselaer Polytechnic Institute (RPI), Earl E. Lee II, Leee7@rpi.edu

19

N-ABLE (Next-generation agent-based economic laboratory) simulates the economy using an agent-based discrete-event model. Agents make economic decisions including purchasing products, hiring workers, selling bonds, collecting welfare payments, conducting open market operations, and others. N-ABLE has been used to evaluate electric power and rail transportation disruptions on commodity production. POC: Theresa Brown, Sandia National Laboratories, tjbrown@sandia.gov

20

NEMO (Net-Centric Effects-based operations MOdel) relies on the following domain specific legacy simulations: CitiLabs' Voyager simulation provides road and rail network analysis. Advantica provides the solver tools for electrical power networks, water and gas pipelines. Users define relationships between components. POC: Brent L. Goodwin, SPARTA Corporation, brent.goodwin@sparta.com

21

Net-Centric GIS is a framework for using GIS interoperability for supporting emergency management decision makers by providing effective data sharing and timely access to infrastructure interdependency information. POC: York University, Toronto, Ontario, Canada, Rifaat Abdalla, abdalla@yorku.ca

22

NEXUS Fusion FrameworkTM is a planning and response tool that visualizes intended and unintended effects and consequences of an event across multiple infrastructure, social, and population behavior models. It is a single framework that incorporates geospatial, graph based (social, economic), and population behavior models in the same simulation space for cross-infrastructure relationship analysis. The framework takes a holistic system-of-systems view to support cross system analyses of cascading events within and between complex networks. POC: IntePoint, LLC, Mark Armstrong, Mark.Armstrong@IntePoint.com

23

NGtools (natural gas infrastructure toolset). NGtools was developed to provide an analyst with a quick method to access, review, and display components of the natural gas network; perform varying levels of component and systems analysis, and display analysis results. POC: Argonne National Laboratory, Infrastructure Assurance Center (IAC), Dr. James Peerenboom, jpeerenboom@anl.gov

24

NSRAM (Network Security Risk Assessment Modeling) is centered on the analysis of large interconnected multi-infrastructure models. POC: Jim McManus, James Madison University, McManuJP@jmu.edu

25

PFNAM (Petroleum Fuels Network Analysis Model) was developed to perform hydraulic calculations of pipeline transport of crude oil and petroleum products. A network consists of links (pipe segments), nodes (pipe junctions), pump stations, valves, and pressure-reducing stations. The model tracks the flow of oil in each pipe and the pressure at each node. "Point-and-click" motions allow the analyst to create a representative model of the liquids pipeline network in order to set up and run a simulation. Graphical and tabular results provided for each simulation enable analysts to quantify the impact of infrastructure disruptions on the pipeline segment or system. This software tool provides a framework for introducing pipeline component dependencies into critical infrastructure analyses. POC: Argonne National Laboratory, Infrastructure Assurance Center, Steve Folga, sfolga@anl.gov

26

TRAGIS (Transportation Routing Analysis Geographic Information System), available via a client server architecture from a web server residing at ORNL. Calculates transportation rouge information based on regulatory guidance for shipping hazardous materials. POC: Paul E. Johnson, Oak Ridge National Laboratory, johnsonpe@ornl.gov

27

TRANSIM is an agent-based system capable of simulating a synthetic populations second-by-second movements of every person and vehicle through the transportation network of a large metropolitan area. TRANSIMS provides planners with a synthetic population's daily activity patterns (such as travel to work, shopping and recreation, etc.), simulates the movements of individual vehicles on a regional transportation network, and estimates the air pollution emissions generated by vehicle movements. POC: Randy Michelsen, Los Alamos National Laboratory, rem@lanl.gov

28

WISE (Water Infrastructure Simulation Environment) is an analytic framework supporting the evaluation of water infrastructure in terms of both infrastructure specific and interdependency issues. WISE involves the integration of geographic information systems with a wide range of infrastructure analysis tools including industry standard hydraulic simulation engines (e.g., EPANET and SWMM) as well as Los Alamos National Laboratory interdependency simulation systems such as the Interdependent Energy Infrastructure Simulation System (IEISS). POC: Randy Michelsen, Los Alamos National Laboratory, rem@lanl.gov

vii

viii

CONTENTS
ABSTRACT.................................................................................................................................................iii INTRODUCTION ........................................................................................................................................ 1 Technical Support Working Group .................................................................................................... 1 Background......................................................................................................................................... 1 INFRASTRUCTURE INTERDEPENDENCIES......................................................................................... 3 Interdependency Formalization .......................................................................................................... 5 Interdependency Types....................................................................................................................... 6 Problem Space .................................................................................................................................... 8 SURVEY METHODOLOGY ...................................................................................................................... 9 Infrastructures..................................................................................................................................... 9 Modeling and Simulation Technique ............................................................................................... 10 Integrated vs. Coupled Models......................................................................................................... 10 Hardware/Software Requirements.................................................................................................... 10 Intended user .................................................................................................................................... 10 Maturity level ................................................................................................................................... 10 STATE-OF-THE-ART REPORT ............................................................................................................... 11 POLITICAL, MILITARY, ECONOMIC, SOCIAL, INFORMATION, AND INFRASTRUCTURE MODELING ACTIVITIES.............................................................................................................. 12 DATA SOURCES ...................................................................................................................................... 13 U.S. RESEARCH AND SPONSORING ORGANIZATIONS .................................................................. 14 CHALLENGES AND RESEARCH NEEDS ............................................................................................. 16 CONCLUSIONS......................................................................................................................................... 19 ACKNOWLEDGEMENT .......................................................................................................................... 20 REFERENCES ......................................................................................................................................... 115

ix

FIGURES
1. 2. Infrastructure interdependencies......................................................................................................... 3 Thick, black smoke billows out of the railroad tunnel near Oriole Park at Camden Yards. Interstate 395 and the baseball park were closed, along with the Inner Harbor (see Reference 9). ............................................................................................................................... 4 An official surveys the gaping hole and broken 40-in. water main at Howard and Lombard streets (see Reference 10). .................................................................................................................. 4 Sample dependency matrix................................................................................................................. 5 Cascading consequence example (see Reference 13)......................................................................... 7 PMESII node and effects relation (see Reference 19)...................................................................... 12

3. 4. 5. 6.

TABLES
EX-1. Summary of areas surveyed................................................................................................................ v 1. 2. 3. Multiscale time hierarchy of power systems. ................................................................................... 16 Strengths and weaknesses of HLA. .................................................................................................. 17 Strengths and weaknesses of DIS..................................................................................................... 17

x

Critical Infrastructure Interdependency Modeling: A Survey of U.S. and International Research
INTRODUCTION
"The Nation's health, wealth, and security rely on the production and distribution of certain goods and services. The array of physical assets, processes, and organizations across which these goods and services move are called critical infrastructures."2 This statement is as true in the U.S. as in any country in the world. Recent world events such as the 9-11 terrorist attacks, London bombings, and gulf coast hurricanes have highlighted the importance of stable electric, gas and oil, water, transportation, banking and finance, and control and communication infrastructure systems. Be it through direct connectivity, policies and procedures, or geospatial proximity, most critical infrastructure systems interact. These interactions often create complex relationships, dependencies, and interdependencies that cross infrastructure boundaries. The modeling and analysis of interdependencies between critical infrastructure elements is a relatively new and very important field of study. Much effort is currently being spent to develop models that accurately simulate critical infrastructure behavior and identify interdependencies and vulnerabilities. The results of these simulations are used by private companies, government agencies, military, and communities to plan for expansion, reduce costs, enhance redundancy, improve traffic flow, and to prepare for and respond to emergencies. Modelers have developed various innovative modeling approaches including agent based modeling, effects-based operations (EBO) models, input-output models, models based on game theory, mathematical models, and models based on risk. These have been applied to infrastructure of shipboard systems, University campuses, large power grids, and waterways to name a few. Modeling is complicated by the quality and availability of data, intricacy of systems, complexity of interactions between infrastructure sectors, and implications and sensitivity of results. This survey identifies and catalogs much of the state-of-the-art research being conducted in the area of infrastructure interdependency modeling and analysis.

Technical Support Working Group
The U.S. Technical Support Working Group (TSWG) is the sponsor for this effort.3 TSWG is a national forum to identify, prioritize, and coordinate interagency and international research and development (R&D) requirements for combating terrorism. The aim of TSWG is to support rapidly developed technologies and product development to provide tools for combating terrorism. It supports multiple U.S. government agencies as well as major allies. The main objective of this study is to develop a single source reference of critical infrastructure interdependency modeling tools (CIIMT) that could be applied to allow users to objectively assess the capabilities of CIIMT. This information will provide guidance for directing R&D to address the gaps in development. The results will inform the R&D efforts of the TSWG Infrastructure Protection Subgroup of R&D efforts and allow a more focused approach to addressing the needs of CIIMT end-user needs.

Background
The study and analysis of infrastructure interdependencies is relatively new. The interdependencies between critical infrastructures received little attention in the early 1990s. However, in the mid 1990s events such as the Oklahoma City bombing in 1995 and the report from the Defense Science Board Task Force on Information Warfare in 1996, and the increased reliance on information and computerized control systems brought the increasing importance of

1

infrastructure interdependencies into focus. Also in 1996, President Clinton established the President's Commission on Critical Infrastructure Protection (PCCIP).4 The PCCIP report was released in 1997 and though it identified no immediate critical threats to national infrastructures, it did highlight the importance of interdependencies including those between power, transportation, emergency response, vital human services, banking and finance, and telecommunications, especially through digital means. A general recommendation of the commission was that since the lion's share (approximately 85%) of the nation's critical infrastructure is in private hands, there needs to be good cooperation and information sharing between government and private sector. In May of 1998, Presidential Decision Directive (PDD) no. 63 was released. That directive set a national goal to protect the nation's critical infrastructure from deliberate attacks by 2003. PDD-63 was followed by executive orders (E.O.s) by both Presidents Clinton (E.O. 131305 in July 1999) and Bush (E.O. 132316 in 2001) establishing Information Sharing and Analysis Centers that were largely private-sector run and a National Infrastructure Advisory Council (NIAC). While there were some changes in the wording of

the E.O.s, the functions of NIAC remained largely the same. We have since seen the establishment of the U.S. Department of Homeland Security (DHS) in November of 2002 and the National Infrastructure Simulation and Analysis Center (NISAC) in fall of 2001. NISAC is a partnership between Sandia National Laboratory (SNL) and Los Alamos National Laboratory (LANL) established to develop advanced infrastructure modeling and simulation techniques that identify vulnerabilities and interdependencies. This increased attention has been followed by increases in funding to universities, national laboratories, and private companies involved in modeling and simulation of critical interdependencies. Funding has come from national organizations, private investments, the Department of Defense (DoD), U.S. government agencies (DHS, U.S. Department of Energy [DOE], Department of Commerce, and others), and other governments and agencies. The increased funding and level of efforts has led to much innovative work in this area. Thus, while efforts focusing on modeling of critical infrastructure interdependencies have only begun recently, much valuable work has already been done.

2

INFRASTRUCTURE INTERDEPENDENCIES
"One of the most frequently identified shortfalls in knowledge related to enhancing critical infrastructure protection capabilities is the incomplete understanding of interdependencies between infrastructures. Because these interdependencies are complex, modeling efforts are commonly seen as a first step to answering persistent questions about the "real" vulnerability of infrastructures."7 The importance of "What are infrastructure inter-dependencies, and how are they modeled?" is addressed in this section. References to interdependent relationships in this paper are actually referring to as dependent relationships or influences between infrastructures. Figure 1 illustrates common representations of infrastructure based on the scenario of a flooding event and the subsequent response. Parallels to this scenario with the events in New Orleans during Hurricane Katrina can easily be drawn. Within the

figure, individual infrastructure networks are represented on a single plane. The parallel lines represent individual sectors or subsets within that particular infrastructure. The spheres or nodes represent key infrastructure components within that sector from the events in New Orleans The energy sector infrastructure, for example, during Hurricane Katrina contains the sectors of electrical generation and distribution, natural gas production and distribution, etc. Ties and dependencies exist within each infrastructure and between the different sectors. The solid lines in Figure 1, crossing sectors and connecting nodes, represent internal dependencies, while the dashed lines represent dependencies that also exist between different infrastructures (infrastructure interdependencies). The example in Figure 1 is a simple attempt to portray the complexity of dependencies that may exist between components. In chaotic environments such as emergency response to catastrophic events, decision makers should

Figure 1. Infrastructure interdependencies.

3

understand the dynamics underlying the infrastructures. Failure to understand those dynamics will result in ineffective response and poor coordination between decision makers and agencies responsible for rescue, recovery, and restoration. It could also cause the mismanagement of resources, including supplies, rescue personnel, and security teams. At best, emergency responders will lose public trust, at worst, human life. This interrelationship among infrastructures and its potential for cascading effects was never more evident than on July 19, 2001 when a 62-car freight train carrying hazardous chemicals derailed in Baltimore's Howard Street Tunnel, Figure 2. This disaster, in addition to its expected effect on rail system traffic, automobile traffic, and emergency services, caused a cascading degradation of infrastructure components not previously anticipated. For example, the tunnel fire caused a water main to break above the tunnel shooting geysers 20 ft into the air, Figure 3. The break caused localized flooding which exceeded a depth of three feet in some areas. Additionally, the flooding knocked out electricity to about 1,200 downtown Baltimore residences.8 Fiber optical cables running through the tunnel were also destroyed. This resulted in major disruptions to phone and cell phone service, email service, web services, and data services to major corporations including WorldCom Inc., Verizon Communications Inc., the Hearst Corp. in New York City, Nextel Communications Inc., and the Baltimore Sun newspaper.9 Disruption to rail services and its effects on the Middle Atlantic states were significant also.10 These effects included delays in coal delivery and also limestone delivery for steel. Figure 3. An official surveys the gaping hole and broken 40-in. water main at Howard and Lombard streets (see Reference 10). Figure 2. Thick, black smoke billows out of the railroad tunnel near Oriole Park at Camden Yards. Interstate 395 and the baseball park were closed, along with the Inner Harbor (see Reference 9).

4

A dependency matrix is another way to represent interdependencies between infrastructure networks and their relative impact. The Critical Infrastructure Protection Task Force of Canada used a dependency matrix (see Figure 4) to relate the interdependency among six sectors identified as crucial: Government, Energy and Utilities, Services, Transportation, Safety, and Communications.11 The matrix is their attempt to better understand the level of dependency and the potential impact among sectors. Infrastructure owners historically concerned with the operation of their own, often well defined domains must now contend with unbounded networks brought about by greater information technology connectivity. There is a growing need to analyze and better understand the chains of influence that cross multiple sectors that can induce potentially unforeseen secondary effects. This survey addresses a growing concern dealing

with the influence or impact, that one infrastructure can have, either directly or indirectly, upon another. The cross infrastructure effects continue to grow as information technology pushes interconnectivity between all aspects of business. Infrastructure interdependencies therefore refer to relationships or influences that an element in one infrastructure imparts upon another infrastructure.

Interdependency Formalization
Precisely how is an infrastructure interdependency relationship defined? Dudenhoeffer, Permann and Manic12 model the levels of infrastructure as a large graph in which nodes represent infrastructure components, and edges the relations between nodes.

Figure 4. Sample dependency matrix.

5

A formal model of this infrastructure and the interrelationships is presented in the following definitions: 1. An infrastructure network, I, is a set of nodes related to each other by a common function. The network may be connected or disjoint. It may be directional, bi-directional or have elements of both. Internal relationships/dependencies within the infrastructure I are represented by edge (a, b) with a, b  I. 2. Given Ii and Ij are infrastructure networks, i z j, a  Ii and b  Ij, an interdependency is defined as a relationship between infrastructures and represented as the edge (a,b) which implies that node b is dependent upon node a. Depending on the nature or type of the relationship, this relationship may be reflexive in that (a,b)  (b,a).

thunderstorm resulting in a loss of power to an office building and all the computers inside. x Informational Interdependency. An informational or control requirement between components. For example: a supervisory control and data acquisition (SCADA) system that monitors and controls elements on the electrical power grid. A loss of the SCADA system will not by itself shut down the grid, but the ability to remotely monitor and operate the breakers is lost. Likewise, this relationship may represent a piece of information or intelligence flowing from a node that supports a decision process elsewhere. An example is the dispatch of emergency services. While the responders may be fully capable of responding, an informational requirement exists as to answering where, what, and when to initiate response. x Geospatial Interdependency. A relationship that exists entirely because of the proximity of components. For example: flooding or a fire may affect all the assets located in one building or area. x Policy/Procedural Interdependency. An interdependency that exists due to policy or procedure that relates a state or event change in one infrastructure sector component to a subsequent effect on another component. Note that the impact of this event may still exist given the recovery of an asset. For example: after aircraft were flown into the World Trade Towers "all U.S. air transportation was halted for more than 24 hours, and commercial flights did not resume for three to four days."14 x Societal Interdependency. The interdependencies or influences that an infrastructure component event may have on societal factors such as public opinion, public confidence, fear, and cultural issues. Even if no physical linkage or relationship exists, consequences from events in one infrastructure may impact other infrastructures. This influence may also be time sensitive and decay over time from the original event grows. For example: air traffic following the 9-11 attack dropped significantly while the public evaluated the safety of travel. This resulted in layoffs within 6

Interdependency Types
Interdependencies can be of different types. Several taxonomies have been presented3 to categorize the types of interdependencies. Rinaldi, Peerenboom, and Kelly13 describe dependencies in terms of four general categories: x Physical ­ a physical reliance on material flow from one infrastructure to another x Cyber ­ a reliance on information transfer between infrastructure x Geographic ­ a local environmental event affects components across multiple infrastructures due to physical proximity x Logical ­ a dependency that exists between infrastructures that does not fall into one of the above categories. This study used a slightly expanded taxonomy developed by Dudenhoeffer and Permann.4 The categorization classifies the following types of relationships: x Physical. A requirement, often engineering reliance between components. For example: a tree falls on a power line during a

the airline industry and bankruptcy filings by some of the smaller airlines (see Reference 12). Again, while the dependencies within an individual infrastructure network are often well understood, the region of interest in interdependency and effects modeling is the influence or impact that one infrastructure can impart upon another. Therefore, the key effects to model and gain understanding of are the chains of influence that cross multiple sectors and induce potentially unforeseen n-ary effects. These chains, potentially composed of multiple interdependency types, compose the paths or arcs between infrastructure components or nodes denoted as {(a,b), (b,c), (c,d), ...(y,z)}. This particular path represents the cascading consequence of an event or the derived dependency of node z on node a,

further denoted (aDz). Likewise the genesis of the chain may not be singular in that the end effect is the influence of multiple nodes, denoted by (abc..Dz). These paths may not be unique in terms of effect, they may change over time, and their behavior may be cumulative in nature, i.e., the end effect may be the culmination of multiple predicated events. The intertwining of networks in this fashion represents a complex system where emergent behaviors are rarely fully understood. Rinaldi, Peerenboom and Kelly (see Reference 13) provide a nice visual representation of this intertwining and the potential cascading effects. This is shown in Figure 5.

Figure 5. Cascading consequence example (see Reference 13).

7

Problem Space
Thus given the realm of interdependency analysis, what are the goals for modeling and simulation efforts? In the analysis of infrastructure interdependencies and the subsequent emergent system behaviors, some of the major problem areas being examined include: 1. Given a set of initiating events {E(a), E(b), ...} what is the cascading impact on a subset of nodes {x, y, z , ...}? 2. Given a set of nodes {x, y, z,...} and a desired end state, what is a set of events {E(a), E(b), ...} that would cause this effect?

3. Given a set of events {E(a), E(b), ...} and a set of observed outcomes of on nodes {x, y, z,....}, is it possible to determine the derived interdependence (abDxyz)? 4. Given a set of infrastructure networks and a critical function, what is the subset of critical nodes {x, y, z , ...} across all networks that will adversely impact a specific mission functionality due to direct or derived dependency?

8

SURVEY METHODOLOGY
The areas included in this survey were selected because they focus on modeling and simulation across multiple infrastructure layers. Systems such as geographical information systems (GIS), which may provide geospatial relationships, are not included unless they possess additional analytical capabilities. Each model examined in the survey offers unique capabilities and provides specific insights into various aspects of the problem domain. The modeling approaches and the objectives of the efforts varied greatly. Specific parameters in the survey were of interest for comparison. One of the goals of the survey was to identify potential resources for a wide range of customers and domains. Six major categories were considered in the survey: x Infrastructures x Modeling and simulation technique x Integrated vs. coupled models x Hardware/software requirements x Intended user x Maturity level. Each of these categories is briefly discussed below.

government as well as the infrastructure relied upon for the defense and national security of the U.S. x Private business, government, and the national security apparatus increasingly depend on an interdependent network of critical physical and information infrastructures, including telecommunications, energy, financial services, water, and transportation sectors. x A continuous national effort is required to ensure the reliable provision of cyber and physical infrastructure services critical to maintaining the national defense, continuity of government, economic prosperity, and quality of life in the U.S.. x This national effort requires extensive modeling and analytic capabilities for purposes of evaluating appropriate mechanisms to ensure the stability of these complex and interdependent systems, and to underpin policy recommendations, so as to achieve the continuous viability and adequate protection of the critical infrastructure of the Nation.16 Although countries tend to have slightly different lists detailing their "critical sectors," most contain elements of the following: x Agriculture and food x Water x Public health and safety x Emergency services x Government x Defense industrial base x Information and telecommunications x Energy x Transportation x Banking and finance x Industry/manufacturing x Postal and shipping. These sectors in turn contain individual infrastructures such as highways, rail systems, electric power generation and distribution, etc.

Infrastructures
The U.S. Patriot Act defines critical infrastructure as "systems and assets, whether physical or virtual, so vital to the U.S. that the incapacity or destruction of such systems and assets would have a debilitating impact on security, national economic security, national public health or safety, or any combination of those matters."15 Further, congress set forth the following findings in Section 1016 of the U.S. Patriot Act: x The information revolution has transformed the conduct of business and the operations of

9

Some of these systems are managed by government agencies, but the majority resides with industry. This survey attempts to capture and describe the infrastructures/infrastructure sectors each program models. This report seeks to reflect only those infrastructures that have been actually modeled and not those presumed to be capable of being modeled.

integrated models tend to model at a much higher level than coupled models.

Hardware/Software Requirements
In an effort to identify possible tool sets, the survey captures the portability and exportability of programs and data.

Intended user
The survey categorizes products as internal analytical tools intended for internal use only or external analytical tools available for use outside the developing organization. This decision relates to the level of expertise required to use the product, the application requirements, and the analytical output of the product. The requirement is sometimes driven by the size, complexity, or proprietary nature underlying the data

Modeling and Simulation Technique
This category attempts to capture the modeling and simulation method used for the infrastructure and interdependencies. It has multiple dimensions that include those of time (continuous vs. discrete time step) and modeling technique (Markov chains, Petri Nets, dynamic simulation, agent-based, physics based, ordinary differential equations, input-output model, etc.).

Maturity level
The following four categories were used to identify the product's level of maturity: x Research ­ the product is still highly conceptual without vetted application in real-world domains. x Development ­ the product has been applied and validated against real-world infrastructure. Beyond conceptual, the product has been used by internal or external customers, but is still undergoing substantial development. x Mature analytic ­ the product has reached a high level of code stability and is part of a vested internal analytical process. The results of analysis may be an external report, but the tool usage is strictly internal to the organization. x Mature commercial ­ the tool is a commercially licensed product.

Integrated vs. Coupled Models
During the course of the survey it became apparent that two different approaches were often used to conduct cross infrastructure analysis. One approach was to create an integrated system model that attempted to model multiple infrastructures and their interdependencies within one framework. The other approach coupled a series of individual infrastructure simulations together, which then illustrated the cascading influence between them. An example of this approach would be an electric grid simulation that determines an outage area for a specific event. The electrical outage area is then fed to a telecommunication model used to determine the subsequent impact on message routing. This impact is fed to a financial simulation that determines the loss of telecommunication impact on commerce and financial transactions. As one might expect,

10

STATE-OF-THE-ART REPORT
Appendix A contains data on U.S. and international efforts and interdependency modeling tools. The information is presented at a high level with POC information for those desiring greater detail.

11

POLITICAL, MILITARY, ECONOMIC, SOCIAL, INFORMATION, AND INFRASTRUCTURE MODELING ACTIVITIES
A modeling area that closely follows infrastructure interdependency modeling is EBO modeling and analysis. War and conflict are rarely confined to only the battlefield and force-on-force engagement. Potential U.S. adversaries comprise a complex and interdependent system of systems, all of which contribute, to some degree, toward their societal coherence, will, and capability to pursue a course of action contrary to U.S. interests.17 Conflict, war, and reconstruction represent a complex set of influences, competing goals, and resources. The battle environment, and thus the means of victory, are often shaped by the intricate interactions between them. Many point to the emergence of a new generation of warfare termed fourth generation warfare (4GW). Retired Colonel Thomas Hammes, U.S. Military Complex, describes this concept: "4GW uses all available networks--political, economic, social, and military--to convince the enemy's political decision makers that their strategic goals are either unachievable or too costly for the perceived benefit. It is an evolved form of insurgency. Still rooted in the fundamental precept that superior political will, when properly employed, can defeat greater economic and military power, 4GW makes use of a society's networks to carry on its fight. Unlike previous

generations of warfare, it does not attempt to win by defeating the enemy's military forces. Instead, via the networks, it directly attacks the minds of enemy decision makers to destroy the enemy's political will."18 Operational Net Assessment (ONA) is the integration of people, processes, and tools that use multiple information sources and collaborative analysis to build shared knowledge of the adversary, the environment, and ourselves in understanding and effectively employing EBO. ONA analytical products are based on a system-of-systems analysis and the understanding of key relationships, dependencies, strengths, and vulnerabilities within and between the adversary's political, military, economic, social, information, and infrastructure (PMESII) elements. These products identify leverage points, key nodes, and links that we can act upon to decisively influence the adversary's behavior, capabilities, perceptions, and decisions.19 Within this operating environment, EBOs are actions that change the state of a system to achieve directed policy aims using the integrated application of the diplomatic, informational, military, and economic instruments of national power. In order to achieve EBO, however, it is imperative to understand the relationships and influences of the PMESII dimensions that shape the actions of the adversary, of allies, and of your organization. Figure 6 illustrates this concept showing a representation of the connectivity and interdependencies between these dimensions as both a strength and potential weakness.

Figure 6. PMESII node and effects relation (see Reference 19).

12

DATA SOURCES
The paradigm of modeling and simulation is "garbage in, garbage out." Having credible and traceable data available to use is key to infrastructure and interdependency modeling. Gathering information on a particular infrastructure is possibly the most significant challenge. Interdependency modeling also requires that gathered information (assets) be linked across multiple infrastructures. Supporting data for these analyses often spread across multiple data sets. The fact that most infrastructures data is held by private industry and, to a large extent, considered proprietary in nature complicates the situation further. The data is often accompanied by the analytical requirement for a certain level of domain expertise in identifying and validating cross infrastructure influences. The scale of the model also determines the possible sources of information. Consider, for example, the electrical power grid. If the goal is to model assets on a national scale, data equivalent to transmission level information may suffice with broad asset effects drawn from course outage area determination. If the goal is to evaluate a particular city, compound, or facility, distribution level information is required reflecting a far greater level of granularity. Commercial geospatial data sets such as those provided by ESRI, Platts, etc., provide coarse level data that may suffice for initial model development, but they lack the detail needed to construct a more precise model. Public census provides a good data source for an initial data set. Recall however, that the census data reflects nighttime residential demographics in terms of grid-wise statistics, which may not be adequate in terms of population mobility and granularity. To mitigate the shortcomings of data, several efforts have been made to compose and validate detail infrastructure and demographic data sources. Two of the data sets used by those surveyed are LandScan and National Asset Database:

x LandScan ­ The LandScan series of data sets have been developed and are maintained by Oak Ridge National Laboratory. They are a population distribution model, database, and tool developed from census data that incorporates other spatial information for greater accuracy and granularity. The LandScan series consist of LandScan Global representing data in 30 arc second grid cells for ~1 km resolution, LandScan Interim, which has a 15-arc (~450 m) second resolution, and LandScan USA with 3-arc-second resolution for ~90 m resolution with both day and night time population distributions and demographic and socioeconomic characteristics data.20 x National Asset Database ­ In July 2004, the Office of Infrastructure Protection (DHS/IP) initiated a data call to states and territories requesting a listing of assets deemed of national or local importance. The collection, named the National Asset Database, contains basic asset and facility information, including data associated with location, POC, and risk attributes. In addition to these specialized data sets, several DOE national laboratories maintain system expertise that includes detailed infrastructure data. These information sets are, to a large degree, the result of industry nondisclosure agreements and therefore are not generally releasable for public use. x LANL ­ National electrical generation and transmission data x Argonne National Laboratory (ANL) ­ Natural gas and oil pipeline data x Oak Ridge National Laboratory (ORNL) ­ National transportation sector information including rail systems, highway, and waterway data and models x Idaho National Laboratory (INL) ­ National electrical power SCADA system information.

13

U.S. RESEARCH AND SPONSORING ORGANIZATIONS
The modeling and simulation of infrastructure interdependencies is a substantial effort in terms of development resources such as infrastructure expertise, modeling and simulation, data accessibility, and so on. For this reason, U.S. government agencies are currently doing most of the research in this area. In order to understand the current focus on ongoing research, it is important to understand the thrust of these organizations. A brief description of the more prominent supporting agencies and their programs are described as follows: x Department of Homeland Security (DHS) ­ The NISAC provides advanced modeling and simulation capabilities for the analysis of critical infrastructures, their interdependencies, vulnerabilities, and complexities. These capabilities help improve the robustness of our nation's critical infrastructures by aiding decision makers in the areas of policy analysis, investment and mitigation planning, education and training, and near real-time assistance to crisis response organizations. The NISAC program is sponsored by the DHS Information Analysis and Infrastructure Protection Directorate. NISAC is a core partnership of Los Alamos and Sandia National Laboratories. NISAC integrates the modeling and simulation expertise of both laboratories to address the nation's potential vulnerabilities and the consequence of disruption among our critical infrastructures.21 x Department of Energy (DOE) ­ The Visualization and Modeling Working Group (VMWG) sponsored by DOE's Office of Electricity Delivery and Energy Reliability activates in response to national energy emergencies to provide data, analyses, and visualization tools as was done for Hurricanes Katrina and Rita. The VMWG was formed in September 2003 to improve the ability of DOE to perform quick turn-around analyses during energy emergencies. It is comprised of energy experts from several DOE offices and energy infrastructure and modeling experts from various DOE national laboratories. Their technical expertise is combined with modeling, GIS, data libraries on past energy disruptions, 14

and other tools to conduct in-depth analysis. DOE national laboratories provide the bulk of this modeling and analysis.22 x Technical Support Working Group (TSWG) ­ TSWG is an inter-agency organization tasked with providing technologies to a variety of government organizations. Their development and product deployment goals focus on identifying and answering specific programmatic needs versus sponsoring national infrastructure modeling and simulation initiatives. This study attempts to identify available and developing resources that may be utilized to address those needs.23 x Defense Advanced Research Projects Agency (DARPA) ­ DARPA is a central research and development organization for DoD. It manages and directs selected basic and applied research and development projects for DoD, and pursues research and technology where risk and payoff are both very high and where success may provide dramatic advances for traditional military roles and missions. DARPA also has a research program in the area of crossdimensional infrastructure influence modeling. By focusing on PMESII dimension interactions, DARPA is leading the Integrated Battle Command. The objective of this program is the development of decision aids to support the commander in conducting a future, complex, multidimensional, coalition, and effects-based campaign. The decision aids will assist the commander and staff in generating, assessing, and visualizing the consequences of employing diplomatic, military, information operations and economic actions, singularly or in combinations, to achieve effects against the adversary's PMESII systems. The decision aids will also assist the commander and staff in constructing, visualizing, and evaluating campaign plans that exploit the impact of multidimensional effects and the interaction among effects. http://www.darpa.mil/ato/solicit/IBC/index.htm. x Department of the Air Force, Air Force Materiel Command, (AFRL) ­ Similar to DARPA, AFRL is leading multiple research efforts in developing PMSEII analytical models. One effort is the Commander's Predictive Environment program, whose objective is to provide a decision support environment that

enables the joint force commander to anticipate and shape the future battlespace. Similar in view to the DARPA effort, the battlespace is seen as a complex and interrelated system of PMESII dimensions. A full understanding of the battlespace requires comprehension of how these interrelated factors affect not only the adversary, but also friendly forces. The focus of

this research program is to (1) model and analyze adversaries, self, and neutrals as a complex adaptive system; (2) understand key relationships, dependencies, and vulnerabilities of adversary/self/neutrals; and (3) identify leverage points that represent opportunities to influence capabilities, perceptions, decision making, and behavior.24

15

CHALLENGES AND RESEARCH NEEDS
Critical infrastructure interdependency modeling has many of the same challenges that one can expect with any modeling and simulation domain: data accessibility, model development, and model validation. Interdependency modeling is further complicated by the extremely large and disparate cross sector analysis required. Many extremely detailed single sector models have been developed. One driving research question asks: "How do we leverage these existing models into a common operating picture?" Such a question is further exasperated by the granularity and the time factors associated with the models. For example, Table 1 illustrates the multiple time scales that exist within the electrical power sector. While currently no standards exists that directly address infrastructure and specifically cross sector modeling, standards do exists for exchanging information between distributed simulations. The two most common methods are the High Level Architecture (HLA) and the Distributed Interactive Simulation (DIS) frameworks. HLA, developed under the leadership of the Defense Modeling and Simulation Office is a general purpose high-level simulation architecture/framework to facilitate the interoperability of multiple types of models and simulations. The purpose of its development is to support reuse and interoperability across the large numbers of different types of simulations developed and maintained by DoD. Within HLA, simulation objects exist as federates in a larger simulation federation. HLA was approved as an open standard through the Institute of Electrical and Electronic Engineers (IEEE) -- IEEE Standard 1516 -- in September 2000.

Table 1. Multiscale time hierarchy of power systems.25
Action/Operation Time frame

Wave effects (fast dynamics, lightning caused over voltages) Switching over voltages Fault protection

Microseconds to milliseconds Milliseconds

100 milliseconds or a few cycles Electromagnetic effects in machine Milliseconds to windings seconds Stability 60 cycles or 1 second Stability augmentation Seconds Electromechanical effects of Milliseconds to oscillations in motors & generators minutes Tie line load frequency control 1 to 10 seconds; ongoing Economic load dispatch 10 seconds to 1 hour; ongoing Seconds to Thermodynamic changes from hours boiler control action (slow dynamics) System structure monitoring (what Steady state; is energized & what is not) ongoing System state measurement and Steady state; estimation ongoing System security monitoring Steady state; ongoing Load management, load 1 hour to 1 day forecasting, generation scheduling. or longer; ongoing Maintenance scheduling Months to 1 year; ongoing. Expansion planning Years; ongoing Power plant site selection, design, construction, environmental impact, etc. 10 years or longer

16

Table 2 provides a listing of HLA strengths and weaknesses as detailed by Schmitz and Neubecker.26 Additional information on HLA can be found by contacting hla@dmso.mil or via the website https://www.dmso.mil/public/transition/hla/. Table 2. Strengths and weaknesses of HLA.
HLA Strengths HLA Weaknesses

program. Table 3 provides an assessment of the strengths and weakens of DIS by the IAPG. Further information on DIS can be found at http://www.sei.cmu.edu/architecture/Architectures_ for_DIS.html#291. Table 3. Strengths and weaknesses of DIS.
DIS Strengths DIS Weaknesses

HLA is an open standard that will be supported beyond 2006 (ref. IEEE 1516). The architecture can be implemented across different computing environments. Provides a documented process for developing distributed simulation systems, e.g., the federation development execution process. More "bandwidth" friendly.

HLA developments may be subject to significant changes in order to meet future needs. Changes to future HLA standards may have significant impact on local implementations. U.S. will continue to lead HLA development and thus there may be dependence on U.S. support for software implementations.

DIS is an open standard (ref: IEEE 1278.x). The architecture can be implemented across different computing environments.

Scalability ­ difficult to scale up to very large exercises, e.g., >500 simulation entities.

The resources and time required to implement an HLA federation can be Supports real-time, faster significant -- up to than real-time, and eventdouble that required for driven time domains. noncompliant Availability of implementations. commercial off the shelf HLA does not ensure (COTS) software support plug-and-play tools, e.g., data interoperability, it capture/replay, simulation facilitates (federation) exercise communication. management (reduces the requirements for bespoke HLA compliance cannot be established in developments). abstract, but only by reference to a defined federation. DIS is another framework for linking real-time and potentially distributed simulations. Defined under IEEE Standard 1278, the chief objective of DIS was to create real-time, synthetic, virtual representations of the warfare environment. This environment is created by interconnecting separate, distributed computers/simulators, called component simulator nodes. These nodes typically represent entities on the order of a military unit. DIS has its roots in the DARPA simulation networking 17

Efficiency ­ rigid structure of data protocols (PDUs) leads Provides a set of well to inefficiency of defined data protocols to network resources, e.g., support the interaction of wide area network real-time simulation (WAN) bandwidth. systems. IEEE standards will not Availability of COTS be developed to meet software support tools future simulation (e.g., DIS Stealth requirements. Viewers, DIS Data DIS only supports realLoggers) reduces the requirements for bespoke time simulations, it does not support event driven, developments. faster than real-time DIS is a stable "product." applications. Limited number of PDUs. HLA and DIS are examples of frameworks that integrate "real-time" simulation models. Information is passed actively between models and timing between models is synchronized. This method may support some aspects of infrastructure model integration. The issue may arise however when the computational time for processing a model makes this type of integration unrealistic, i.e., the computational requirements greatly exceed real-time. One potential method to address this issue and also to provide a more rapid response capability is to develop scenario libraries consisting of preprocessed scenarios with run profiles available for immediate access. Los Alamos National Laboratory utilizes this approach with their Scenario Library Visualizer. Another method of model integration consists of devising a common architecture to distribute

information between models. This method is currently used by Los Alamos National Laboratory and NISAC to relate impacts across different infrastructure models. In a broad sense, a damage profile based on expected physical damage is constructed first. An example of this is determining power outages based on projected high wind profiles, surge, and flooding models associated with hurricanes. The physical impact of the event is transformed into impact on the power grid in terms of outage areas. This information is then passed to other models (water, financial, transportation, etc.) such that the corresponding impact in the electrical power sector integrates into other sectors. In this way, impact cascades across infrastructure boundaries and presents potential effects via infrastructure interdependencies. This type of model integration works well when the timing between infrastructures precludes a true federation of simulations. Interdependency discovery and validation is another challenging area of research. Although physical interdependencies can be derived by subject matter experts, doing so on a large scale is a

resource challenge. Discovery methods and tools, including automated mapping, are essential for highfidelity models. Fast Analysis Infrastructure Tool (FAIT) by Sandia National Laboratory conducts rough first order interdependency mapping based on simple rule sets. The IEISS model and Los Alamos National Laboratory suite of models use outage areas to identify geospatial and gross order dependencies. The Critical Infrastructure Modeling System (CIMS) developed by Idaho National Laboratory likewise supports geospatial dependencies, but requires manual direct association for other dependencies. Identifying and mapping societal interdependencies is perhaps the most challenging aspect in terms of discovery, mapping, and validation. Identifying a multicultural response and the duration of impacts on a society is challenging. The impact of "like" events can be speculated, but drawing inferences to unforeseen and rare events relative to the other infrastructure sectors is a challenging area of active research. This is one of the main focuses of PMESII research that is underway.

18

CONCLUSIONS
Infrastructure interdependency modeling is a relatively new area of research and analysis, but recent events of both natural disasters and malicious acts have shown that the impact of these cross infrastructure relationships can be measured. Significant research efforts are underway in the U.S. and abroad. One observation resulting from this effort is that no cross program working group or forum is specifically dedicated to this critical area of research. Most research exchange occurs within

specific programs. Consequently, a limited exchange of ideas has occurred across the sponsoring agencies in this area. The strongest collaboration exists between DHS and DOE, mainly due to the fact that the same research teams are sponsored by both organizations. One suggestion from our study is the development, whether formally or informally, of a national or international working group with a central focus of infrastructure interdependency analysis. It is hoped that this state-of-the-art report will serve to not only report on current activities, but will also act as a catalysts for information exchange for such activities.

19

ACKNOWLEDGEMENT
We appreciate the many contributors to this report. Our preferred method has been to directly interact with the project leaders in collecting this information. All that have participated have been extremely supportive. Again, this is an ongoing project and we apologize to those efforts which were not recognized in this first report.

Please forward comments on material contained within this document and also points of contacts for those efforts not covered in this initial document to Donald.Dudenhoeffer@inl.gov. Finally, we would like to express our gratitude to Dr. Steve Fernandez of Los Alamos Laboratory who acted as a constant guide and source of data for this report.

20

Appendix A

21

Table Abbreviations: Infrastructure Sectors EP Electric Power NG Natural Gas DW Drinking Water SW Sewage Water ST Storm Water HA Human Activity FN Financial Networks SCADA Supervisory Control and Data Acquisition TC Telecom CN Computer Networks OL Oil Pipeline RL Rail System HW Highway System WW Waterway System POL Policy/Regulatory constraints Simulation Type I Input-Output Model A Agent-based Intended Users Types IA Internal Analyst EA External Analyst B Both Maturity Level RS Research DV Development MI Mature Internal MC Mature Commercial

22

Model Name Organization POC

Agent-Based Infrastructure Modeling and Simulation (AIMS) University of New Brunswick Infrastructures Dr. Ali Ghorbani, Professor Various ghorbani@unb.ca Dr. Stephen Marsh, Adjunct Professor Stephen.Marsh@nrc-cnrc.gc.ca

Description Overview ­ AIMS is an agent-based system to simulate and model the (national and cross-border) interdependencies and survivability of Canada's Critical Infrastructures. Development goals ­ Goals are to incorporate into complete critical infrastructure (CI) crisis management system and plan to model New Brunswick's Information and Communication Technology (ICT), power, water, etc. Intended users ­ Users will include CI managers, users, planners, and emergency services personnel. System output ­ Visualization for training monitoring and planning. It's a possibility to add mapping systems. Maturity ­ The system is in development. Areas modeled ­ New Brunswick Critical Infrastructures. Customers/sponsors ­ National Research Council Canada (CNRCC). Model Framework Underlying model ­ Agent-based modeling uses universal mark-up language (UML) and service oriented architectures. Plans are to use a multi-agent development kit in the future such as Agent Oriented Software Group (AOS) JACKTM, Java Agent DEvelopment (JADE) framework, or other agent software. Simulation ­ Simulation scenarios have included the Moncton area forest fire, the Saint John Port disaster, and a Border incident. Data format ­ The data and text are in UML and Environmental Systems Research Institute, Inc. (ESRI)'s ArcGIS formats. Sensor data ­ Not specified. Coupling with other models ­ This model is designed to couple with other models, but that capability has not been tested to date. Human activity modeling ­ Included in model. System Requirements Not specified. Hardware Not specified. Software Other Notes

References Stephen Marsh, Critical Infrastructure Interdependencies, http://iit-iti.nrc-cnrc.gc.ca/colloq/0405/0411-04_e.html, November 4, 2004, Webpage visited July 10, 2006.

23

24

Model Name Athena Organization POC

On Target Technologies, Inc. Dr. Brian Drabble brain@ontgttech.com Dr. Maris "Buster" McCrabb buster@dmmventures.com

Infrastructures All (physical to conceptual)

Description Overview ­ Athena is an analysis and modeling tool that is designed to analyze a network of nodes (actors, concepts and physical) as a "system of systems" by merging various political, military, economic, social, information, and infrastructure (PMESII) models and their associated crossdependencies. Athena incorporates several reasoning algorithms that allow sophisticated inter- and intra-dependency analysis between and through nodes. Model construction is quick and simple point and uses a simple point and click interface. Development goals ­ Automatic Network Extraction (engineering models), Semantic Reasoning across transitive dependencies & Interfacing to different information sources. Intended users ­ Military for in analyzing disruptive military effects, Law Enforcement for analyzing disruptions of criminal gangs and enterprises, Disaster/Network Recovery to determine repair priorities, and Economic for competitive analysis. Output ­ Graphical interface showing nodes and linkages with criticalities and interdependencies indicated. Multiple analytical capabilities. Can be linked to GIS data. Maturity ­ Evolving. Areas modeled ­ Athena is capable of modeling any entity including countries, states, cities, roads, and facilities. Customers/sponsors ­ Air Force Research Laboratory (AFRL) IFSA sponsored the original work. Funding is now provided by DARPA and USSTRATCOM who will deploy the tool in late 2006. Model Framework Underlying model(s) ­ Fusion of Barlow's model of horizontal cross-dependency with weighting, Warden's model of vertical cross-dependency, and the McCrabb-Drabble model of time-phased linkages between models. This is a fractal model that allows the description of a Strategic Entity through Centers of Gravity (COG) to Target Systems to Target sets and where appropriate targets. Simulation ­ System allows full-scale simulations. Data format ­ Accepts data in variety of formats. Sensor data ­ Accepts sensor data/feeds to update model nodes and changing interactions (e.g., strength) between nodes. Human activity ­ This tool models human activity/capability as part of the network (e.g., loss of plant manager may decrease network capability). Nodes may be humans or concepts. Coupling with other models ­ Couples readily with other engineering models, databases, sensor networks, etc. System Requirements Laptop 2 GB processor speed, 60 GB hard drive, 500 MB RAM. Hardware Windows XP or similar, program is written in C. Software Other Notes

25

References Athena: Effects-based Cross-Dependency Modeling for Target Systems Analysis Final Report. (Limited distribution) Final Athena Demonstration (Microsoft PowerPoint presentation).

26

Model Name Organization POC

CARVER2 TM National Infrastructure Institute Center for Infrastructure Expertise Ronald Peimer rpeimer@ni2.org

Infrastructures User defined

Description Overview ­ CARVER2 is a simple software program that provides a quick and easy way to prioritizes potential terrorist targets. It compares and rates the critical infrastructure and key assets in jurisdictions by producing a mathematical score for each potential target. It is the first step for conducting more indepth vulnerability assessments. CARVER2 helps users make "apples vs. oranges" comparisons such as a water system vs. an energy grid vs. a bridge. Development goals ­ None goals have been stated. Intended users ­ Federal, state and local government officials are the intended users for this program. Output ­ The CARVER2 tool outputs various reports with priority scores and background information for different infrastructure elements. Maturity ­ This is a free product by request. Areas modeled ­ Determined by user. Customers/sponsors ­ This tool is Sponsored by the US Department of Commerce, National Institute for Standards and Technology (NIST). Model Framework Underlying model(s) ­ The support is a relational database. Simulation ­ This tool has no simulation capability. Data format ­ Text. Sensor data ­ No sensor data has been incorporated in the tool. Human activity ­ Not modeled. Coupling with other models ­ There is no coupling with other models. System Requirements PC or laptop running Microsoft Windows operating system. Hardware Distributed via CD no other software needed. Software Other Notes

27

References NI2 Center for Infrastructure Expertise Critical Infrastructure Library, http://www.ni2ciel.org/,Webpage visited July 3, 2006. National Infrastructure Institute home page, http://www.ni2.org/default.asp, Webpage visited July 3, 2006 CARVER2 Project Page, http://www.ni2cie.org/CARVER2.asp, Webpage visited July 3, 2006. 28

Model Name Organization POC

COMM-ASPEN Sandia National Laboratory

Infrastructures FN, TEL

Description Overview ­ CommAspen is a new agent-based model for simulating the interdependent effects of market decisions and disruptions in the telecommunications infrastructure on other critical infrastructures in the U.S. economy such as banking and finance, and electric power. CommAspen extends and modifies the capabilities of Aspen-EE, an agent-based model previously developed by Sandia National Laboratories to analyze the interdependencies between the electric power system and other critical infrastructures. CommAspen has been tested on a series of scenarios in which the communications network has been disrupted, due to congestion and outages. Analysis of the scenario results indicates that communications networks simulated by the model behave as their counterparts do in the real world. Results also show that the model could be used to analyze the economic impact of communications congestion and outages. Development goals ­ To analyze interdependent infrastructure systems in a more holistic way, Sandia and other research institutions have developed models of critical infrastructure systems using agentbased approaches. Sandia's first agent-based model of the U.S. economy, developed in the mid-1990s, is called Aspen. This model is a Monte Carlo simulation that uses agents to represent various decisionmaking segments in the economy, such as banks, households, industries, and the Federal Reserve. An agent is a computational entity that receives information and acts on its environment in an autonomous way; that is, an agent's behavior depends at least partially on its own experience. Through the use of evolutionary learning techniques, Aspen allows us to examine the interactive behavior of these agents as they make real-life decisions in an environment where agents communicate with each other and adapt their behaviors to changing economic conditions, all the while learning from their past experience. In 2000, Sandia developed a new model of infrastructure interdependency called AspenEE. This model extended the capabilities of Aspen to include the impact of market structures and power outages in the electric power system, a critical infrastructure, on other infrastructures in the economy. One of the limitations of agent-based models in current development at Sandia and other research institutions is that communication is treated simply as a message passing between agents. Effectively, the telecommunications infrastructure is not specifically represented. None of the models simulates the differences in communication over telephone, computer, wireless, or other networks and therefore cannot model the impact of specific communication failures on the whole system. Nor can current models simulate the impact of other infrastructure failures on telecommunications. To address the communications deficiencies described above, Sandia revised and restructured the Aspen-EE model to include a more realistic representation of the telecommunications infrastructure. This new model of infrastructure interdependency is called CommAspen. In CommAspen, communication is treated as an integrated agent system capable of creating, transforming, sending, receiving, and storing information and messages over time and across distance. With CommAspen, we can model communication networks or medium-specific vulnerabilities to failures and their dependence on supporting infrastructures like power. Intended users ­ Internal analyst. System output ­ Not specified. Maturity ­ Development. Areas modeled ­ Not specified. Customers/sponsors ­ Not specified. 29

Model Framework Underlying model ­ There are several ways that we can implement the notion of infrastructures in CommAspen. One method of representing certain types of infrastructures in CommAspen is through the use of spigots and sinks. Such infrastructures are for commodities that run continuously, like water from a municipality and electricity from a local utility. A sink is where a producer puts product into an infrastructure. For example, a power company may have a natural gas-fired electric generating plant producing power. It would put power on the transmission lines by passing the power into the associated sink. A spigot is where a consumer gets the product, such as turning on the lights in a residence or getting water from a faucet. Simulation ­ Agent Based Model. Data format ­ Not specified. Sensor data ­ None. Coupling with other models ­ No. Human activity modeling ­ Not Known. System Requirements Not specified. Hardware Not specified. Software Other Notes

Images: None.

References Barton, Dianne C., Eric D. Edison, David A. Schoenwald, Roger G. Cox, and Rhonda K. Reinert, "Simulating Economic Effects of Disruptions in the Telecommunications Infrastructure", SAND REPORT, SAND2004-0101, Printed January 2004.

30

Model Name Organization POC

Critical Infrastructures Interdependencies Integrator (CI3) Argonne National Laboratory Infrastructures Dr. James Peerenboom EP,NG,SCADA,TC jpeerenboom@anl.com

Description Overview ­ CI3 is a software tool for emulating (Monte Carlo simulation) the amount of time or cost (or both) needed for activities that must be completed to restore a given infrastructure component, a specific infrastructure system, or an interdependent set of infrastructures to an operational state. The software tool provides a framework for recognizing interdependencies and incorporating uncertainty into the analysis of critical infrastructures. Development goals ­ No goals stated. Intended users ­ Infrastructure owners. System output ­ Graphs and tables of completion time and cost distributions for repairs to quantify the impacts of infrastructure disruptions. Maturity ­ The system is in development. Areas modeled ­ No specific areas are mentioned. Argonne has developed transition diagrams for repair of damages to the following: natural gas transmission pipeline, petroleum, oil, liquids (POL) pumping station, natural gas city gate station, propane air plant, natural gas compressor station, natural gas underground storage facility, supervisory control and data acquisition (SCADA) communications tower, electrical substation, transformer, and an optical telecommunications cable. Customers/sponsors ­ U.S. Department of Energy. Model Framework Underlying model ­ Transition diagrams coupled to Monte Carlo simulator. Simulation ­ Transition diagrams are easy to create via point-and-click techniques to simulate recovery and restoration activities for covered infrastructure. Data format ­ Not specified. Sensor data ­ Model does not accept sensor data. Coupling with other models ­ None. Human activity modeling ­ Human activities (travel, repair, assessment) are included in simulations. System Requirements Not specified. Hardware Not specified. Software Other Notes

31

References

32

Model Name Organization POC

Critical Infrastructure Modeling System (CIMS©) Idaho National Laboratory (INL) Donald Dudenhoeffer Donald.Dudenhoeffer@inl.gov

Infrastructures EP, SCADA, HW, HA, POL, PMESII

Description Overview ­ A modeling and simulation framework that combines geospatial information and a four dimensional (4D) environment (time-based) to support `what if' analysis. Development goals ­ Provide decision makers with a highly adaptable and easily constructed `wargaming' tool to assess infrastructure vulnerabilities including policy and response plans. Operating at a high level of simulation, it supports rapid `point and click' model development to allow the adaptation of models to rapidly changing environments. Intended users ­ Emergency planners and responders. System output ­ Four dimensional geospatial visualization in a VTK framework along with report generation. Maturity ­ Development ­ in the process of commercial licensing. Areas modeled ­ Idaho National Laboratory, New Orleans Louisiana. Customers/sponsors ­ Research has been ongoing for the past 4 years under the INL National Security Divisions. Sponsors have included the INL's internal research program, the Department of Energy, the U.S. Air Force Research Laboratory (AFRL), and negotiations are underway with the State of Louisiana. Model Framework Underlying model ­ The underlying model is a network representation of infrastructure utilizing nodes and edges for assets and relationships. Graphical objects such as aerial images, 3DS images, or VRML models can be tied to the assets. Additionally, information can be embedded within nodes such as documents, web site hyperlinks, web cams, avis, etc Simulation ­ Agent-based discrete event simulation. Data format ­ Flat files are used as direct feeds to the simulations. These files can be fed by a multitude of different databases including Access, GIS, etc Sensor data ­ Agent objects(nodes) can have autonomous behaviors or they can be fed by external sensor input. Coupling with other models ­ Yes. Human activity modeling ­ Human activity can be modeled directly or as the result of policy/procedure enactment. System Requirements Cross platform compatibility ­ Windows, UNIX/LINUX, and Solaris. Internet Hardware connectivity required to access embedded links. No external software to CIMS ~ requires < 5 meg of disk space. Software

33

Other Notes The objective of CIMS was to create a rapid modeling and analysis capability that did not require extensive data collection or proprietary GIS software. As such, CIMS allows the ability to create models and infrastructure simulations on the fly embedding new intelligence as it becomes available. Model development can start with an aerial image or a scanned/sketched chart/map image. All information is georeferenced. Models construction can occur via one of three methods. x Direct manipulation of the network descriptor flat files x Conversion from a database to the flat file format x Point and click network construction via the Model Builder Application. User interactivity with the Model. The models were developed with a wargaming approach to allow maximum user interaction with the simulation. Thus the user has several different ways to interact with the data: x An event script can be created to initiate specific events at a designated time x The user can select and directly manipulate the state of individual nodes and edges, i.e., shutting down an electrical substation or making a bridge impassible x The user can inject events during runtime, i.e., placing and detonating a bomb to observe cascading impacts.

34

New Orleans Model

Damage Profile due to flooding ­ illustrating loss of infrastructure.

3D Stereo Representation of downtown on SGI P i

Images 35

Model showing loss of an Electrical Substation

Rotated Side view showing building profiles at an angle with the electrical infrastructure separated from the buildings to highlight the connectivity. Multiple infrastructures can be displayed to show direct and spatial relationships.

36

References
Dudenhoeffer, D.D, M. R. Permann, and M. Manic, "CIMS: A Framework For Infrastructure Interdependency Modeling And Analysis." Submitted to Proceedings of the 2006 Winter Simulation Conference, ed L. F. Perrone, F. P. Wieland, J. Liu, B. G. Lawson, D. M. Nicol, and R. M. Fujimoto. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, 2006. Dudenhoeffer, D. D., M. R. Permann, and R. L. Boring, 2006. Decision consequence in complex environments: Visualizing decision impact. In Proceeding of Sharing Solutions for Emergencies and Hazardous Environments. American Nuclear Society Joint Topical Meeting: 9th Emergency Preparedness and Response/11th Robotics and Remote Systems for Hazardous Environments. Dudenhoeffer, D. D., M. R. Permann, and E.M. Sussman. 2002. A Parallel Simulation Framework For Infrastructure Modeling And Analysis. In Proceedings of the 2002 Winter Simulation Conference, ed E. Yücesan, C. H. Chen, J. L. Snowdon, and J. M. Charnes, 1971-1977. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers.

Critical Infrastructure Modeling System Fact Sheet <http://www.inl.gov/nationalsecurity/factsheets/docs/cims.pdf>

37

38

Model Name Organization POC

The Critical Infrastructure Protection Decision Support System (CIP/DSS) Los Alamos National Laboratory Infrastructures Randy Michelsen ALL rem@lanl.gov Sandia National Laboratories Theresa Brown tjbrown@sandia.gov

Description

Overview ­ The Critical Infrastructure Protection Decision Support System (CIP/DSS) simulates the dynamics of individual infrastructures and couples separate infrastructures to each other according to their interdependencies. For example, repairing damage to the electric power grid in a city requires transportation to failure sites and delivery of parts, fuel for repair vehicles telecommunications for problem diagnosis and coordination of repairs, and the availability of labor crews. The repair itself involves diagnosis, ordering parts, dispatching crews, and performing work. The electric power grid responds to the initial damage and to the completion of repairs with changes in its operating characteristics. Dynamic processes like these are represented in the CIP/DSS infrastructure sector simulations by differential equations, discrete events, and codified rules of operation. Many of these variables are output metrics estimating the human health, economic, or environmental effects of disturbances to the infrastructures. CIP/DSS will assist decision makers in making informed choices by:
x Functionally representing all 14 critical infrastructures with their interdependencies x Computing human health and safety, economic, public confidence, national security, and environmental impacts x Synthesizing a methodology that is technically sound, defensible, and extendable. Development goals ­ Charter is to model all infrastructures and key assets. Used for quick response on areas Los Alamos National Laboratory (LANL) doesn't have data for. Intended users ­ Internal analyst at LANL. System output ­ Graphs representing the impact on multiple state variables such as hospital beds occupied, etc.

Maturity ­ Development ­ Initiated as a proof-of-concept in August 2003. Completed a prototype model and two case studies in February2004.
Areas modeled ­ Not specified. Customers/sponsors ­ DHS. Model Framework Underlying model ­ The national and metropolitan consequence models are implemented using Vensim, which reads input parameters from and writes output time series to an Oracle relational database of "consequence" metrics, which are abstracted into a much smaller set of "decision" metrics. The decision support software (written in Visual Basic) accesses the decision database to compute utility values for various scenarios and alternatives. Simulation ­ Vensim is used for developing, analyzing, and packaging high quality dynamic feedback models. Models are constructed graphically or in a text editor. Features include dynamic functions, subscripting (arrays), Monte Carlo sensitivity analysis, optimization, data handling, and application interfaces. Data format ­ Vensim Model. Sensor data ­ No ability to input live data feeds. 39

Coupling with other models ­ No. Human activity modeling ­ Human activity can be modeled directly or as the result of policy/procedure enactment. System Requirements The Vensim family of software runs on Windows (95/98/Millennium/NT/2000/XP) Hardware and the Power Macintosh running System 7 or higher (in Classic mode under OSX). Vensim requires 8 MB of memory and 8 MB of disk space for a full installation. A demonstration version of Vensim is available free for either Windows or Macintosh. CIPDSS is a model built within Vensim simulation software by Ventura Software (http://www.vensim.com/brochure.html). Other Notes CIP/DSS (Critical Infrastructure Protection Decision Support System) simulates the dynamics of individual infrastructures and couples separate infrastructures to each other according to their interdependencies. CIP/DSS models asset information at the aggregate level. For example with a focus area, it can estimate the number of hospital beds affected by an event, but it cannot directly retrieve information relative to a particular hospital. It utilizes the commercial simulation software Vensim.

40

References Bush B, L. Dauelsberg, R. LeClaire, D. Powell (LANL), S. DeLand (SNL), and M. Samsa (ANL), Critical Infrastructure Protection Decision Support System (CIP/DSS) Project Overview, LA-UR-051870, July 2005.

41

42

Model Name Organization POC

Critical Infrastructure Protection (CIP) Modeling and Analysis (CIPMA) Program Australian Government ­ Attorney General's Infrastructures Department (AGD) FN, TC, EP, NG, OL Michael Jerks ­ Director, Major Projects Michael.Jerks@ag.gov.au

Description Overview ­ The Critical Infrastructure Protection Modeling and Analysis program (CIPMA) is a computer based tool to support business and government decision making for critical infrastructure (CI) protection, counter-terrorism and emergency management, especially with regard to prevention, preparedness, and planning and recovery. CIPMA is designed to examine the relationships and dependencies within and between critical infrastructure systems, and to demonstrate how a failure in one sector can greatly affect the operations of critical infrastructure in other sectors. CIPMA uses a vast array of data and information from a range of sources to model and simulate the behavior and dependency relationships of critical infrastructure systems. The capability will include a series of impact models to analyze the effects of a disruption to CI services. The CIPMA Program currently focuses on three priority sectors: banking and finance, communications, and energy. The capability was launched by the Attorney-General in February 2006. "Proof of concept" of the capability was successfully demonstrated to key business and government stakeholders in May 2006. Although CIPMA is still in development, results from the capability are already assisting the development and direction of government policy in national security and critical infrastructure protection (CIP), and helping owners and operators to better protect their critical infrastructure. Development goals ­ The current focus is on broadening and deepening CIPMA coverage of the three priority sectors, the Sydney commercial business district (CBD) precinct, and development of impact models for the Decision Support Module. The impact models will assess the flow-on consequences of a CI service disruption, the economic impacts of the disruption, the effects on population, time/duration and area of the disruption, and the behavior of networks and clusters of infrastructure as a result of the service interruption. Work on a fourth sector will commence by July 2007. Intended users ­ Users include CI owners and operators and Australian local governments. System output ­ Output will include geographic information system (GIS) functionality for data capture, management, and visualization. System behavior will determine dependencies and time-based impacts of disruptive events on infrastructure networks. Maturity ­ In development, some tools are complete. Areas modeled ­ Australian critical infrastructure networks and high priority precincts (e.g., capital cities). Customers/sponsors ­ Australian government, state and territory governments, CI owners and operators. Model Framework Underlying model(s) ­ System Dynamic Models. Simulation ­ Telecommunication connectivity matrix and expert systems. Data format ­ The format is geographic information system (GIS) and relational database. Sensor data ­ Not currently equipped for sensor input. Human activity ­ Contains human activity model. Coupling with other models ­ Model couples with earthquake, tsunami inundation, bomb blast, and plume models. System Requirements Not specified. Hardware ArcGIS, ArcSDE, Oracle, Vensim DSS, Dynamic Network System (DNS), CLIPS, Software 43

Java Runtime Environment (JRE). Other Notes CIPMA is a very detailed modeling and analysis initiative which contains sensitive business information about the operation of Australia's critical infrastructure networks, relationships and dependencies. The IP is owned and managed by Attorney-General Department (AGD) on behalf of the Australian Government. The CIPMA Development Team of AGD, Geoscience Australia (GA) and the Commonwealth Scientific and Industrial Research Organization (CSIRO) has been in discussions with the US Department of Homeland Security (DHS) and Argonne, Sandia, and Los Alamos National Laboratories regarding the Critical Infrastructure Decision Support System (CIP-DSS), and the similarities and differences between the two capabilities, since November 2004. AGD is currently preparing a Project Arrangement for ongoing consultation with DHS and the three National labs under the Homeland Security Science and Technology Treaty (HSST). References Fact sheet on CIPMA program http://www.tisn.gov.au/agd/WWW/rwpattach.nsf/VAP/(7A188806B7893EBA0402BC1472412E58)~ Overview+of+CIPMA.PDF/$file/Overview+of+CIPMA.PDF, Webpage visited July 3, 2006. AusGeo News, Protecting the Nation, http://www.ga.gov.au/ausgeonews/ausgeonews200509/cip.jsp, Issue No. 79, September 2005, Webpage visited July 3, 2006.

44

Model Name Organization POC

Critical Infrastructure Simulation by Interdependent Agents (CISIA) Universita Roma Tre Infrastructures Stefano Panzieri EP,SCADA panzieri@uniroma3.it Giovanni Ulivi ulivi@uniroma3.it

Description Overview ­ This model is described by the authors as a hybrid of the two modeling approaches; interdependency analysis and system analysis. It is a bottom-up complex adaptive systems (CAS) model using interactive agents. The critical infrastructure simulation by interdependent agents (CISIA) simulator is designed to analyze short term effects of failures in terms of fault propagation and performance degradation (Panzieri, 2004). The simulator is based on Recursive Porus Agent Simulation Toolkit, Repast, open-source agent-based development software with libraries of classes for creating, running, displaying and collecting data from a agent based simulations. It extends the Java classes of Repast defining a new class for each type of macro component present into any infrastructure: such as, electric power plant, transmission line, telecommunication channel, waste-water system, etc. Development goals ­Work is ongoing to further validate the CISIA approach and to analyze how intelligent reaction, and autonomy capabilities (e.g., decentralized control strategies), might be used to improve the robustness of the system of system's composed by different heterogeneous and interdependent infrastructures. Intended users ­ Infrastructure owners, planners, and emergency responders. System output ­ Graphic models showing the operative level incidence matrix and physical fault incidence matrices (FIMs) between elements in the model. In this case air conditioning, electric power, and SCADA. Maturity ­ The system is in development. Areas modeled ­ An unspecified (for security reasons) University Campus. Customers/sponsors ­ Not indicated. Model Framework Underlying model ­ Agent-based model based on Repast, in order to handle many heterogeneous infrastructures into a single framework. Agent behavior is abstracted to allow use of a small set of common quantities; operative level, requirements (needs), and faults. Agent interactions include; induced faults, input requirements, and input operative level. Outputs include: propagated faults, output requirements, and output operative level. Simulation ­ During simulation agents communicate via messages. An agent sends messages to its neighbors to specify its requirements to communicate its level of service (operative level), and to propagate faults (physical-faults, geographical-faults, and cyber faults). Data format ­ Relational database. Sensor data ­ Model does not accept sensor data. Coupling with other models ­ CISIA implements an easy-linkage/black box philosophy: any model obtains connecting together agents without any modification of their internal structure. Human activity modeling ­ Not incorporated.

45

System Requirements Not specified. Hardware Not specified. Software Other Notes Each agent class defines the behavioral roles of the element and its input/output quantities in term of which resources the agent needed and supply. Moreover, the class defines which type of failure can be propagated to (generated from) the agent. An agent may propagate different types of failure to a different set of neighbors.

References Panzieri, S., R. Setola, G. Ulivi (2004). An agent based simulator for critical interdependent infrastructures. Proc. 2nd International Conference on Critical Infrastructures, October 24-27, 2004. Panzieri, S., R. Setola, G. Ulivi , An Approach to Model Complex Interdependent Infrastructures, International Federation of Automatic Control (IFAC), http://www.dia.uniroma3.it/~panzieri/Articoli/WorldIFAC05CIIP.pdf#search='An%20Approach%20to%20Model%20Complex%20Interdependent%20Infrastructur es', Webpage visited July 10, 2006.

46

Model Name Organization POC

Distributed Engineering Workstation (DEW) Electrical Distribution Design, Inc. Dr. Robert Broadwater dew@vt.edu

Infrastructures EL, SCADA

Description Overview ­ The Distribution Engineering Workstation (DEW) provides over 30 applications for analysis, design, and control of electrical and other physical network systems. DEW allows all of its components (data sets and algorithms) to be reused by a new application, allowing new solutions to build on top of existing work. This provides for cross collaborations among different groups and the emergence of solutions to complex problems. DEW is being used to identify and analyze interdependencies in large scale electrical power systems and fluid systems of aircraft carriers. DEW is open architecture, non-proprietary. Development goals ­ Electrical Distribution Design, Inc. (EDD) continues to develop and support DEW. They aspire to achieve combined analysis of systems with millions of nodes and to develop a seamless approach to asset management. DEW's architecture provides an open platform for development. The DEW system model can be linked to asset management records, daily operational procedures, events, long- and short-term planning, and more. Intended users ­ Users are utilities, analysts, and military. System output ­ The system is used for operation and control of electrical system and analysis of reconfiguration of damaged systems. Maturity ­ Mature product is in broad use. Areas modeled ­ This model has been used in St. Louis, MO, Detroit, MI, Consolidated Edison, NY, Aircraft Carriers. Customers/sponsors ­ Electric Power Research Institute (EPRI) along with Department of Defense and Department of Energy sponsored the original development. Users include Northrop Grumman (naval applications), Detroit Edison (Detroit, MI), Ameren (St. Louis, MO), Orange and Rockland (Pearl River, NY), and Consolidated Edison (New York). Model Framework Underlying model ­ EDD's approach is built around a combination of concepts from graph theory, physical network modeling, and generic programming. The DEW model incorporates power flow, fault, reliability, reconfiguration for restoration, and over 30 other algorithms. Simulation ­ Simulations may be run manually with mouse and keyboard, automatically controlled from user developed applications, or set up to run in batch mode over numerous systems and/or time points. Data format ­ Model data is stored in relational SQL-compliant databases; real-time measurement data comes from common object request broker architecture (CORBA) interface or plant information (PI) time series databases. Sensor data ­ DEW can handle any number of measurements and any types of measurements that are modeled, through its PI or CORBA interface. Coupling with other models ­ DEW can attach to other models, such as geographic information system (GIS) models, via provided interface. System Requirements Laptop/Server/Circuit server. Hardware Win 2000, XP, User interface. Software Other Notes EDD is working with the utility industry, Virginia Tech, and other universities to develop a 47

comprehensive Integrated System Model (ISM) based design, operations and maintenance management system. This concept is being applied to critical infrastructures including naval ships and gas and water utilities. Through work with the utility industry and Department of Energy, EDD has demonstrated it is possible to use the same ISM for analysis, design, operations, and real-time control. EDD has also used ISM based analysis to manage reconfigurable system models with more than 3 million objects and 200 million attached historical measurement values. The ISM provides a complete, seamless view of a physical plant that forms a common context for multi-discipline team collaboration, distributed processing, synergistic research and development, and providing infinite extensibility. Any data or algorithm that can be attached to the ISM is also associated with all other data and algorithms attached to the ISM. The ISM uses linked list type traces to dynamically adapt data management and analysis whenever the system is changed through modification, maintenance or operation. EDD is structuring its current research and development work so that it that can eventually be combined into a generic integration platform for collaborative analysis, design, and operations for energy systems (CADOE). CADOE will directly support and structure low overhead collaboration among electric utilities, gas utilities, regulatory and policy making agencies, suppliers, integrators, aggregators, and customers. CADOE is envisioned to encompass simulation, analysis, alternative design evaluation, training, and real-time operations support.

Model of Ship Critical Infrastructure.

48

Dense Electrical Power System Model References Broadwater, Robert, et al., Power Engineering, http://www.ecpe.vt.edu/news/ar04/power2004.pdf#search='distributed%20engineering %20workstation%20epri', Webpage visited July 3, 2006. SAM Six, Products: Dew, http://www.samsix.com/dew.htm, Webpage visited July 3,2006. Tam, Kwa-Sur and Robert Broadwater, Virginia Tech Presentation, http://www.eng.vt.edu/research/dom_pres/TamBroadwater%20Systems%20Presentation.pdf#search='vt%20dew' Webpage visited July 3, 2006.

49

50

Model Name Organization POC

Electricity Market Complex Adaptive System (EMCAS) Argonne National Laboratory Guenter Conzelmann (ANL) guenter@anl.gov

Infrastructures Power Systems and Markets

Description Overview ­ Electricity Market Complex Adaptive System (EMCAS) uses agent-based modeling to simulate the operation of complex power systems. EMCAS can be used as an "electronic-laboratory" to probe the possible operational and economic impacts on the power system of various external events. Market participants are represented as "agents" with their own set of objectives, decisionmaking rules, and behavioral patterns. Agents are modeled as independent entities that make decisions and take actions using limited and/or uncertain information available to them, similar to how organizations and individuals operate in the real world. EMCAS includes all the entities participating in power markets, including consumers, generation companies (GenCos), Transmission Companies (TransCos), Distribution Companies (DisCos), Demand Companies (DemCos), Independent System Operators (ISO) or Regional Transmission Organizations (RTO), and regulators. Development goals ­ Continue to develop EMCAS as a new approach to model and simulate the operations of restructured electricity markets. Intended users ­ EMCAS was first applied for a regulatory commission in the mid-western United States. At the beginning of 2005, the software became commercially available and current clients include research institutes, power companies, transmission companies, and regulatory offices in South Korea, Portugal, and Spain. The Iberian EMCAS application includes the simulation of hydropower, wind power, and a variety of other renewable resources. System output ­ EMCAS utilizes a graphical user interface to develop market configurations, display model inputs, and analyze simulation results (see screen captures on next page). Results are stored in HDF format and can be exported in text and spreadsheet formats. In addition to the energy spot markets and bilateral financial contract markets, EMCAS also includes a simplified representation of ancillary services markets; Detailed representation of the transmission system, using a Direct Current Optimal Power Flow (DC OPF) algorithm to compute locational marginal prices (LMP) and identify transmission congestion and price impacts of congestion; Chronological simulation of hourly market prices over short or long time periods; Hourly bid-based market clearing, scheduling and dispatch in day-ahead and real-time markets; Representation of different bidding strategies, from production cost bidding to various forms of physical and economic withholding strategies; Ability to change prevailing market rules (regarding congestion management, pricing mechanisms, price caps etc.) provides the opportunity to test the robustness and vulnerability to gaming of different market designs; and Calculation of cost, revenues, and profits for all relevant agents in the system. Maturity ­ Commercial Product distributed by ADICA Consulting, LLC. Areas modeled ­ Illinois electrical market, Iberia, France, South Korea, Poland, Central Europe Customers/sponsors ­ At the beginning of 2005, the software became commercially available and current clients include research institutes, power companies, transmission companies, and regulatory offices in South Korea, Portugal, and Spain.

51

Model Framework Underlying model(s) ­ Agent-based modeling and simulation. Simulation ­ EMCAS simulates the operation of a power system and computes electricity prices for each hour and each location in the transmission network. Electricity prices are driven by demand for electricity, cost of electricity production, the extent of transmission congestion, external random or non-random events, such as unit outages or system disruptions, and company strategies. Model results include the economic impacts on individual companies and consumer groups under various scenarios. Data format ­ The user builds the system configuration either within the EMCAS graphical user interface or by preparing and importing a set of well-defined input files. Sensor data ­ The model also includes bilateral financial contracts. Real-time prices are calculated in a real-time dispatch using a DC optimal power flow model. Human activity ­ Model includes different types of consumers (e.g., residential, industrial, and commercial) with their respective electricity consumption profiles. Coupling with other models ­ Couples with hydropower models (e.g., VALORAGUA) and detailed power flow models (e.g., PowerWorld). System Requirements A network with 10 nodes (buses or locations), 70 aggregated thermal generating units, Hardware 13 generation companies, one transmission company, one ISO, and one regulator takes approximately 60 minutes for a one-year simulation (8760 hours) on a desktop PC with a 2.0 GHz AMD Athlon2000+ processor and 1 GB of RAM. For multi-year simulations, it is recommended to use a brand-new, high-end PC, preferably with dual core processors and 2+ GB of RAM. Commercial optimizer (LINGO), long-term hydro model (e.g., VALORAGUA). Software Other Notes Adaptability to Local Market and System Conditions: The EMCAS model is fully customizable and not hardwired to any particular system. Network configurations can be simple and aggregate consisting of a few to several dozen network nodes and links, or detailed bus-level representations with several thousand network elements. The level of detail largely depends on data availability and particular analysis objectives. References ADICA Consulting, LLC., Innovative Solutions for Analyzing Energy Markets, http://www.adica.com/media/downloads/ADICA_Overview_2006.pdf, Webpage visited July 3, 2006. ADICA Consulting, LLC., Electricity Market Complex Adaptive System (EMCAS) Software, http://www.adica.com/media/downloads/EMCAS_Model_Overview.pdf, Webpage visited July 3, 2006. ADICA Consulting, LLC., Electricity Market Complex Adaptive Systems (EMCAS), http://www.adica.com/media/downloads/EMCAS_Specifications_2006.pdf, Webpage visited July 3, 2006. Argonne National Laboratory, Simulating GenCo Bidding Strategies in Electricity Markets with an Agent-Based Model, http://www.iaee.org/documents/denver/Thimmapuram.pdf#search='emcas', Webpage visited July 3, 2006.

52

53

54

Model Name Fast Analysis Infrastructure Tool (FAIT) Sandia National Laboratory (SNL) Organization Infrastructures Theresa Brown POC EP, NG, POL, TL, Emergency Services tjbrown@sandia.gov Description Overview ­ National Infrastructure Simulation and Analysis Center (NISAC) analysts are regularly tasked by the Directorate for Preparedness in the Department of Homeland Security (DHS) with determining the significance and interdependencies associated with elements of the nation's critical infrastructure. The Fast Analysis Infrastructure Tool (FAIT) has been developed to meet this need. FAIT utilizes system expert-defined object-oriented interdependencies, encoded in a rule-based expert systems software language (JESS), to define relationships between infrastructure assets across different infrastructures. These interdependencies take into account proximity, known service boundaries, ownership, and other unique characteristics of assets found in their associated metadata. In a similar fashion, co-location of assets can be analyzed based exclusively on available spatial data. The association process is dynamic, allowing for the substitution of data sets and the inclusion of new rules reflecting additional infrastructures, as data accuracy is improved and infrastructure analysis requirements expand. FAIT also utilizes established Input/Output (I/O) methods for estimating the economic consequence of the disruption of an asset. Each of these analysis elements (interdependency, co-location, economic analysis) have been extended from their original `asset-level' analysis, to allow for the analysis of a specified region. Here, rules written for individual assets are executed en masse on classes of demand infrastructures, like assets of the emergency services (e.g., fire and police stations) and public health (e.g., hospitals) infrastructures, which lie in a defined analysis area, such as a hurricane damage zone, to identify those elements of supply infrastructures (e.g., electric power and telecommunications) which serve the largest number of particular sets of demand infrastructures. FAIT's regional economic analysis takes as input economic data (from the Bureau of the Census) for the disrupted area (as modeled by other NISAC capabilities). When coupled with other NISAC modeling results (estimates for the duration of the disruption and recovery, and the range of magnitude of disruption for the disrupted region), FAIT creates a regional economic analysis, an understanding of the direct and indirect economic consequences, for each sector of the economy in each county in the analysis area. Development goals ­ The FAIT development team is constantly modifying their development goals to best support the requirements of NISAC analysts, in responding to questions from DHS. Current goals include the following: Expansion of existing FAIT capabilities to cover infrastructures not in the current analysis set; Enhancement of economic analysis capability to more accurately represent the consequences of the loss of infrastructure services on the performance of individual industrial sectors; Incorporation of infrastructure-specific models to define areas of consequence due to the failure of asset(s) in a given infrastructure; and Development of a network `metacrawler' designed to associate sparse metadata (e.g., transportation system commodity throughput) with fragmented system elements (e.g., segments of the national rail network). Intended users ­ Analysts on NISAC's Fast Analysis and Simulation Team. System Output ­ Web-based, printer-friendly description of assets, their interdependencies, economic consequence of disruption, and other information associated with asset by system users. Maturity ­ In development, utilized by NISAC Fast Analysis and Simulation Team to support NISAC analyses for DHS/Preparedness. Areas modeled ­ First-order interdependencies for selected classes of assets in the energy, telecommunications, emergency services, and public health sectors, nationwide (based on data availability). Customers/sponsors ­ DHS/IP ­ NISAC. 55

Model Framework Underlying model ­ Dependency model is an object-oriented expert system model of infrastructure interdependencies. The economic model centers on the economic disruption over an area or region from a discrete event. Economic methodology best employed for disruptions with a timeframe of 1 week to 1 month. Simulation ­ For identification of interdependencies, FAIT utilizes an expert system developed in JESS. Economic analysis within FAIT is performed utilizing Input-Output methodologies. Both elements are coded in Java. Data format ­ FAIT utilizes spatial and tabular data Sensor data ­ None. Ability to couple with other models ­ None; though results of other models (documents, files) can be coupled through the FAIT architecture to particular assets, classes of assets, or infrastructures with which they are associated. Human Activity modeling ­ None. System Requirements None, for the end user. Program resides on a SNL server and supports web access. Hardware Internet Browser Software Other Notes FAIT allows for external information, (e.g. web addresses or files), to be `attached' to specific assets, classes of assets, or infrastructure sectors, such that when those areas are examined in the future, the associated information is accessible to future users.

References National Infrastructure Simulation and Analysis Center, Fait Analysis Infrastructure Tool Fact Sheet,http://www.sandia.gov/mission/homeland/factsheets/nisac/FAIT_factsheet.pdf.

56

Model Name Financial System Infrastructure (FinSim) Los Alamos National Laboratory Organization Infrastructures FIN Sam Flaim POC Description Overview ­ The Financial System Infrastructure (FinSim) is an agent-based model of cash and barter transactions that is dependant on contractual relationships and a network at the federal reserve level. Agent based models create transactions which rely on telecommunications and electric power. Dependencies can cause deadlocks in the situation where one is unable to pay until being paid. The MIITS module asks every transaction whether there is an electronic connection available to make the transaction. The payments and settlement systems (PSS) module makes the validity checks. Development goals ­ Development started in January 2005 to protect the physical infrastructure of payment and trading systems initiated by the events of 9-11. All current models didn't address the transaction system, just the economic impact. Intended users ­ Internal analyst. System output ­ The system output is the number of financial institutions affected. Output is in a textbased format. Maturity ­ Development. Areas modeled ­ National Federal Reserve Banking System -- Financial Customers/sponsors ­ Sponsor is the National Infrastructure Simulation and Analysis Center (NISAC), Department of Homeland Security (DHS). Model Framework Underlying model ­ Agent-based model. Simulation ­ FinSim models financial transactions modeling the 12 FRB, about 9,700 FedWire participants, and almost 28,000 financial institutions registered with FedACH. This includes the electronic PSS--networks with contractual as well as electronic links and nodes PSSs include: FedWire, FedNet, CHIPS, FedACH, Commercial ACHS ~50 Cash & barter (excluded from FinSim) Data format ­ Not specified. Sensor data ­ No direct sensor feeds. Coupling with other models ­ Yes, coupling is done indirectly. Electrical power failure (IEISS output)  Telecom failure (MIITS output)  PSS failures (FINSIM) Human activity modeling ­ None. System Requirements Larger models require a computer cluster. Hardware Java. Software Other Notes

57

References Financial System Infrastructure--FinSim, LAUR-05-9147.

58

Model Name Organization

POC

Fort Future U.S. Army Corps of Engineers, Engineer Research and Development Center, Construction Engineering Research Laboratory (CERL) Dr. Michael P. Case Michael.P.Case@erdc.usace.army.mil

Infrastructures All support infrastructures for a military installation

Description Overview ­ Fort Future is a collaborative, web-based planning system that uses simulation to test plans for Department of Defense (DoD) installations. It uses an open, service-oriented architecture to allow multiple simulations to be run simultaneously from the same set of alternative, organized into a study. The web-based workbench provides geographic information system (GIS)-based plan editors, controls simulations, and organizes results into a decision matrix. Fort Future assesses the impact of critical infrastructure on mission using a "Virtual Installation" simulation that contains models for transportation, electrical power, water systems, including waterborne chemical/biological/radiological (CBR) agents, airborne CBR plume, facilities, mission tasks and processes, agents, and dynamic plans. The Virtual Installation simulation was built using Argonne National Laboratory's Dynamic Information Architecture System (DIAS) framework and will be ported to the Repast agent modeling toolkit by September of 2006. Other models support analysis of encroachment, sustainability, and facility design. Development goals ­ Demonstrate the use of simulation to improve planning for DoD Installations. Incorporate scenario descriptions into Simulation Interoperability Standards Organization (SISO) Military Scenario Definition Language (MSDL). Intended users ­ Users will include installation and regional planners, US Army Corps of Engineers, and researchers. System output ­ Output of the simulations is collected by a web-based collaborative workbench and presented as a decision matrix. The workbench can be customized to present output specific to particular simulations. Maturity ­ This product is in development with some tools complete. The product will be complete by October, 2006. Areas modeled ­ Fort Benning, Fort Shafter, Fort Bragg, and Fort Carson. Customers/sponsors ­ United States Army. Model Framework Underlying model(s) ­ The agent-based Virtual Installation is based on DIAS and Repast. Water modeling uses EPAnet. CBR plume model uses the Defense Threat Reduction Agency (DTRA) Hazardous Prediction and Assessment Capability (HPAC) Tool. Simulation ­ This model supports complex and lengthy scenario simulations Data format ­ GIS ­ Environmental Systems Research Institute, Inc. (ESRI) Geodatabase and SHP files (Tri-service Spatial Data Standards). Scenarios ­ XML. Sensor data ­ Not accepted. Human activity ­ Human activities are modeled, however there are no humans in the simulation loop. Coupling with other models ­ Fort Future is built to collaborate with multiple models using simple object access protocol (SOAP).

59

System Requirements Fort Future is a server-based application, accessed over the internet using a Hardware web-browser. Fort Future has been tested on Windows and Linux servers. The workbench runs as a Software J2EE application on JBoss 3.x. Persistence is provided by MySQL or Oracle relational databases. Geospatial information is provided by ESRI ArcSDE and ArcGIS server. Users access the workbench using a web-browser. Other Notes Users of Fort Future at the installation, regional, or national level will be able to set up planning scenarios, conduct dynamic analyses over time periods of up to 30 years, and compare scenario results. Fort Future will allow decision makers to: · Provide an integrated sustainability planning capability to support mission-essential task list (METL) analysis, master planning, and natural and cultural resource planning. · Simulate the impact of critical infrastructure failure on the installation mission. · Simulate and optimize planning for force projection. Metrics will focus on risk-based evaluation of an installation's ability to project forces over time. · Simulate urban and regional growth around installations as a foundation for analysis of mission sustainability. Factors to be evaluated include encroachment, noise, traffic congestion, habitat, and threatened and endangered species. · Manage facility requirements to rapidly generate, visualize, and analyze facilities for the Objective Force. The analysis will include force protection and sustainability issues.

Electrical Infrastructure (capacity & interruption)

Water Infrastructure(flow & CBR)

CBR Plume Modeling

Collaborative Web-based Decision Support

60

National/Regional Scale

Installation Scale

Facility Scale

SIRRA Sustainable Installation Regional Resource Assessment

LEAM 30 Year Encroachment Simulation

Facility Composer:
Accelerating MILCON Transformation

SPiRiT & LEED
Sustainability Rating

AT Standards
References Fact sheet on Fort Future, www.erdc.usace.army.mil/pls/erdcpub/docs/erdc/images/ERDCFactSheet_ Research_FortFuture.pdf#search='fort%20future', Webpage accessed July 3, 2006. US Army Corps of Engineers, www.erdc.usace.army.mil, Webpage visited July 3, 2006. Discussion of US Army Corps of Engineers ongoing research, http://www.erdc.usace.army.mil/pls/erdcpub/WWW_WELCOME.NAVIGATION_ PAGE?tmp_next_page=61605&tmp_Main_Topic=51585, Webpage visited July 3, 2006.

61

62

Model Name Organization POC

Inoperability Input-Output Model (IIM) University of Virginia Center for Risk Management of Engineering Systems, Director and founder ­ Lawrence R. Quarles Professor of Systems and Information Engineering and Civil Engineering Yacov Y. Haimes Yyh4f@virginia.edu

Infrastructures Financial networks, highway networks

Description Overview ­ Inoperability Input-Output Model (IIM) is a computer-based analytical model capable of analyzing the impacts of an attack on an infrastructure and the cascading effects (in economic and inoperability terms) on all other interconnected and interdependent infrastructures. The model uses U.S. Bureau of Economic Analysis (BEA) data for assessing economic interdependencies. IIM allows systematic prioritization of infrastructure sectors that are economically critical and identifies sectors whose operability is critical during recovery. The model can be used to represent workforce recovery following a terrorist attack and identify essential response personnel. IIM also models recovery rates of different infrastructure sectors following an event. Development goals ­ Not specified. Intended users ­ Analysts and emergency planning and response organizations are the intended users. System output ­ The model outputs various data and metrics in text and graphically. Maturity ­ The model has been used with cooperation of various local and state governments. Areas modeled ­ IIM has been used to model Virginia's transportation systems in various cities (e.g., Hampton City, Norfolk, and Virginia Beach) and support Department of Homeland Security (DHS) security alert levels for the greater New York area and to support a commission on high-altitude electromagnetic pulse (HEMP) attacks. Customers/sponsors ­ Customers and sponsors of IIM include the State of Virginia, U.S. DHS, Defense Threat Reduction Agency (DTRA), the Department of Defense (DoD) and the Commission on High Altitude EMP Attacks on the U.S. Model Framework Underlying model ­ IIS is a mathematical model based on Wassily Leontif's input-output model for the U.S. economy which describes economic interdependencies. Simulation ­ IIM simulates the behaviors of multiple infrastructure sectors during and following perturbations (such as terrorist attacks on modeled infrastructure) using economic and other data to assess the criticality of the effects. Data format ­ Data are retrieved from and stored in relational databases containing information including employment and earnings data, commodity flow data, and geographic location data. Sensor data ­ Not specified. Coupling with other models ­ Not specified. Human activity modeling ­ IIM has been used to model human activity in response to transportation disruptions. System Requirements Not specified. Hardware Not specified. Software Other Notes

63

IIM Calculates Propagating Effects. References Haimes, Y. (2005), Risk-Based Framework for Modeling Infrastructure Interdependencies, University of Southern California Terrorism Risk Analysis Symposium, Los Angeles, California, January 14, 2005, http://www.usc.edu/dept/create/events/2005_01_31/Risk_Based_Framework_for_Modeling_Infrastructure_ Interdependencies.pdf#search='yacov%20Haimes%20interdependencies', Webpage visited July 12, 2006. Haimes, Yacov Y., et al., "Inoperability input-output model for interdependent infrastructure sectors. I: Theory and methodology," Journal of Infrastructure Systems, Vol. 11, No. 2, June 2005, pp. 67-79. Haimes, Y. (2004). Assessment and Management of Transportation Infrastructure Security using the Inoperability Input-Output Model (IIM), October 19, 2004, http://www.virginiadot.org/infoservice/resources/TransConfHaimes%20-%20VDOT-Roanoke-October-192004-2.pdf#search='Yacov%20Y.%20Haimes%20interdependencies', Webpage visited July 12, 2006.

64

Model Name Organization POC

Interdependent Energy Infrastructure Simulation System (IEISS) Los Alamos National Laboratory Infrastructures Joe Holland EP, NG

Description Overview ­ The Interdependent Energy Infrastructure Simulation System (IEISS) is an actor-based infrastructure modeling, simulation, and analysis tool designed to assist individuals in analyzing and understanding interdependent energy infrastructures. The actor-based infrastructure components were developed in IEISS to realistically simulate the dynamic interactions within each of the infrastructures, as well as, the interconnections between the infrastructures. In particular, it has the ability to analyze and simulate the interdependent electric power and natural gas infrastructures. IEISS Water is a water distribution simulation capability for simulating urban scale water infrastructures and their interdependencies. Development goals ­ The ultimate goal for IEISS is a multi-infrastructure modeling framework that can be used to analyze the complex, nonlinear interactions (interdependencies) among interdependent infrastructures including electric power, natural gas, petroleum, water, and other network based infrastructures that is scalable to multiple spatial (e.g., urban to regional) and temporal resolutions Intended users ­ Internal Analyst ­ IEISS used to support the development of an impact report on for specific infrastructure events (such as, hurricanes, terrorist attacks, etc.). System output ­ System output include the identification of outage areas (e.g., electrical outage areas). Output visualization is current in Java OpenMaps and is exportable to ESRI compatible shape files. Maturity ­ Mature Internal. Areas modeled ­ numerous US metropolitan areas. Customers/sponsors ­ Sponsor is NISAC ­ DHS. Model Framework Underlying model ­ IEISS is an actor-based infrastructure modeling, simulation, and analysis tool designed to assist individuals in analyzing and understanding interdependent energy infrastructures. Simulation ­ A continuous time based model with an underling physical engine for system dynamics. Data format ­ Data is input via xml format from a variety of databases. Sensor data ­ no direct sensor feeds. Coupling with other models ­ Yes, coupling is done indirectly. The output of IEISS will serve as the input to other infrastructure models to identify cross infrastructure effects. Human activity modeling ­ None at this time. System Requirements Cross platform compatibility ­ Windows and LINUX compatibility. Hardware Requires the Java Virtual Machine. Software Other Notes IEISS is coupled with other LANL modeling tools. Of particular note is the Scenario Library Visualizer (SLV). SLV is a scenario library of outage simulations, which includes a custom visualization tools to provide map-based view of scenarios that have been evaluated in IEISS. The goal has been to identify potential impacts to critical infrastructures dependent upon electric power. SLV has principally been used during fast-response exercises for analysis of hurricane impacts (restoration of hurricanes Charlie and Ivan in '04; Dennis, Katrina, Ophelia, Rita, Wilma in `05) SLV has also modeled electric power restoration during ice storms and during DOE-sponsored exercises involving low-voltage scenarios.

65

References "NISAC Energy Sector ­ IEISS," NISAC Capabilities Workshop, LA-UR-03-1159, Portland, Oregon, 26-27 March 2003. Los Alamos National Laboratory, "Energy Infrastructure Modeling at LANL," LALP-03-027, LA-UR-03-0658. Los Alamos National Laboratory, "Energy and Environmental Programs Compendium," LA-LP-02-216.

66

Model Name Organization POC

Knowledge Management and Visualization in Support of Vulnerability Assessment of Electricity Production Carnegie Mellon University Infrastructures H. Scott Matthews EP (RL, WW, HW limited) hsm@cmu.edu Department of Energy (DOE) National Energy Technology Laboratory (NETL) Pittsburgh and Morgantown Campuses

Description Overview ­ This is a research project to analyze vulnerabilities associated with delivery of fuel. It is designed to help ensure availability of supply and to visualize the impacts for decision support. The project has focused on coal deliveries to power plants because, while vulnerabilities at the power plant level (production) are easier to identify, vulnerabilities and impacts associated with delivery of fuel are more uncertain. Also, data on coal shipments is readily available. Development goals ­ The first phase of the project focused on the origin (mines) and destination (power plant) layers of the coal model. The middle (transportation) layer will be focused on in the future. Additional work will also be done to improve tools for data mining such as, classification of transportation assets, better prediction of impacts, and improved sequential pattern analysis tools. Intended users ­ Planners are the intended users. System output ­ Output includes maps and chart graphics showing mines, transportation routes, and affected (with degree of vulnerability to disruption) power plants. Maturity ­ This project is currently in the prototype stage with ongoing research. Areas modeled ­ United States with emphasis on coal mines in Wyoming. Customers/sponsors ­ Department of Energy (DOE) National Energy Technology Laboratory (NETL). Model Framework Underlying model(s) ­ Statistical data and analysis tools drawing on data derived from data warehouses. Simulation ­ Mines can be removed from the network and a simulation run to identify plants affected and the degree of the impact on production. Data format ­ Data were used from several databases including; Coaldat (developed by Platts containing ~ 2500 coal transactions per month), Coal Transportation Rate Database developed by the Energy Information Administration supplemented with data from the Department of Transportation (DOT) Surface Transportation Board (STB), National Transportation Atlas Database (NTAD), and PowerMAP, a geographical information system (GIS) developed by Platts containing map layers of power plants and mines. Sensor data ­ Not included. Human activity ­ Not modeled. Coupling with other models ­ Not specified. System Requirements Not specified. Hardware Not specified. Software

67

Other Notes The Transportation Routing Analysis Geographic Information System (TRAGIS) developed by Oak Ridge National Laboratory (ORNL) was evaluated to help with the problem of routing (of coal supplies). TRAGIS is designed to schedule possible routes by selecting the origin and destination with one transportation mode (e.g., highway, rail, and waterway modes) and route type (e.g., commercial [default], quickest, shortest, and others). Currently, it is not able to schedule routes for multimodal transportation as is often used to deliver coal. While the most frequently used mode of transporting coal is railroad, many transactions are shipped multimode, such as by barge then by railroad. Therefore, a multimodal route scheduling solution is necessary for acquiring more accurate transportation analyses.

68

References Information in the preceding section was obtained from draft report and communications with point of contact.

69

70

Model Name Organization POC

Multi-Layer Infrastructure Networks (MIN) Purdue School of Civil Engineering Dr. Srinivas Peeta peeta@purdue.edu George Mason University Dr. Terry Friesz tfriesz@gmu.edu

Infrastructures HW, HA

Description Overview ­ This is a preliminary network flow equilibrium model of dynamic multi-layer infrastructure networks (MIN) in the form of a differential game involving two essential time scales. In particular, three coupled network layers--automobiles, urban freight, and data--are modeled as being comprised of Cournot-Nash dynamic agents. An agent-based simulation solution structure is introduced to solve the flow equilibrium and optimal budget allocation problem for these three layers under the assumption of a super authority that oversees investments in the infrastructure of all three technologies and thereby creates a dynamic Stackelberg leader-follower game. Development goals ­ Continue to develop a generalized framework to address both equilibrium and disequilibrium scenarios. Intended users ­ Community planners and engineers. System output ­ Charts, graphs, behavioral trends. Maturity ­ Research. Areas modeled ­ Urban transportation (e.g., auto, urban freight, and data). Customers/sponsors ­ The National Science Foundation sponsored the work. Model Framework Underlying model ­ Agent based simulation of multi-layer infrastructure networks. The three-layer model consists of an auto, urban freight, and data layer flow sub models. These three sub models are combined and solved using an agent-based simulation approach. Simulation ­ Temporal dynamic flow model involving producers and consumers. Data format ­ Not specified. Sensor data ­ Not incorporated. Coupling with other models ­ Unknown. Human activity modeling ­ Models human activity as consumers. System Requirements Not specified. Hardware Not specified. Software Other Notes

References

71

72

Model Name Organization POC

Multi-Network Interdependent Critical Infrastructure Program for Analysis of Lifelines (MUNICIPAL) Rensselaer Polytechnic Institute (RPI) Infrastructures Earl E. Lee II TC, EP, RL Leee7@rpi.edu William A. Wallace wallaw@rpi.edu John E. Mitchell mitchj@rpi.edu David M. Mendonca mendonca@njit.edu

Description Overview ­ Multi-Network Interdependent Critical Infrastructure Program for Analysis of Lifelines (MUNICIPAL) is a geographic information system (GIS) user interface, built on a formal, mathematical representation of a set of civil infrastructure systems that explicitly incorporates the interdependencies among them. The mathematical foundation or decision support system is called the Interdependent Layered Network (ILN) model. ILN is a mixed-integer, network-flow based model implemented in software drawing on a database containing infrastructure attributes. MUNICIPAL provides the capability to understand how a disruptive event affects the interdependent set of civil infrastructures. This can help communities train for and respond to events that disrupt services required for their health, safety, and economic well being. It can be used to help assess the vulnerability of systems due to their reliance on other systems. The model is generic (applicable to more than one location) and not specific to a particular type of event, such as an earthquake or hurricane. Development goals ­ Once the Los Angeles and Manhattan data sets are complete, mathematical and technical assessments will be conducted. The system will also be evaluated by infrastructure system managers and emergency response organizations. Intended users ­ MUNICIPAL is intended for use by personnel in charge of response and restoration efforts following a disruptive event and as a training tool for personnel who guide response and restoration efforts. System output ­ A GIS interface displays systems and identifies affected areas. An operator can update the conditions of components of the set of systems modeled, add temporary systems during restoration, and display areas affected by inabilities to meet demands. Maturity ­ Prototype system. Areas modeled ­ Manhattan, NY and Los Angeles, CA. Customers/sponsors ­ National Science Foundation. Model Framework Underlying model ­ MUNICIPAL consists of a GIS interface for the user, a database with the attributes of the set of infrastructures, the ILN module, and the vulnerability module. Simulation ­ With identification of paths or components of concern, MUNICIPAL identifies components in the parent system which these paths or components rely on. For example, placing power supply components in a failed condition will identify telecommunications components that rely on these sections of power to fail. By proposing new connections within telecomm, MUNICIPAL can help to determine if a feasible path (or paths) exists and the set of nodes that constitute this path (or set of paths). MUNICIPAL can also be used for the addition of temporary or alternative power sources or any other analyses relating to improving reliability by adding redundancy. Data format ­ ESRI ArcGIS, relational database, text. 73

Sensor data ­ Not currently configured for sensor data. Coupling with other models ­ Not specified. Human activity modeling ­ Not specified. System Requirements Not specified. Hardware Not specified. Software Other Notes

References Lee, Earl E. II, et al., Decision Technologies for Protection of Critical Infrastructures, http://www.rpi.edu/~mitchj/papers/decisiontechnologies.pdf#search='decision%20technologies, Webpage visited July 10, 2006.

74

75

76

Model Name Organization POC

Natural Gas Infrastructure Toolset (NGtools ) Argonne National Laboratory, Infrastructure Assurance Center (IAC) Dr. James Peerenboom jpeerenboom@anl.gov

Infrastructures NG, EL

Description Overview ­ The Infrastructure Assurance Center (IAC) has developed a set of tools to represent the physical components of the natural gas network. The Natural Gas Infrastructure Toolset (NGtools) was developed to provide an analyst with a quick method to access, review, and display components of the natural gas network; perform varying levels of component and systems analysis, and display analysis results. Development goals ­ Not specified. Intended users ­ Natural gas suppliers and users (e.g., electric utilities). System output ­ Geographic (using GIS) or schematic view of pipeline system, charts and graphs showing failure sets (e.g., pumping stations and power stations) and the amount of time to gas depletion. The system allows various analyses on component and system level, and displays results. Maturity ­ The system is in development. Areas modeled ­ Not specified. Customers/sponsors ­ US Department of Energy. Model Framework Underlying model ­ Agent-based. Simulation ­ NGflow simulates steady-state gas network flows and provides gas flow movements under various operating conditions based on gas flow balancing algorithms and available system flow data. Data format ­ Not specified. Sensor data ­ Not specified. Coupling with other models ­ Not specified. Human activity modeling ­ Human activities are not modeled. System Requirements Not specified. Hardware Not specified. Software Other Notes There are four tools in the toolset; NGanalyzer, NGcut, NGflow, and NGdepletion as described in the following: NGanalyzer assists in analyzing gas system characteristics and vulnerabilities. Key considerations include the number of city-gates, available storage, and pipeline capacity and interconnections. The figure below shows an example of the shortest path distance from major gas supply areas to a sample site as calculated by the model. NGcut determines network component failure sets that could isolate a specific location or site from all supply sources. One of the advantages of using this model is that it significantly decreases the time needed to analyze site isolation issues by automating the construction of failure sets. The model also allows analysts to consider a larger number of failures and to broaden an analysis. Failure sets identified by NGcut provide an initial set of components that require closer examination. NGflow identifies critical links and nodes in a network topology. This tool provides an alternative to using very detailed, data-intensive commercial flow simulation models. The model also gives a unique snapshot of the gas transmission infrastructure that supports a certain location or site. NGdepletion addresses outage duration times and determines whether and when a component outage will affect a specific location or site. The model computes the amount of time that line pack can continue supplying gas to a site.

77

References

78

Model Name Organization POC

Net-Centric Effects-based operations MOdel (NEMO) SPARTA Brent L. Goodwin Brent.goodwin@sparta.com Laura Lee Laura.lee@sparta.com

Infrastructures TC, EP, NG, DW

Description Overview ­ Net-Centric Effects-based operations MOdel (NEMO) is an effects-based planning and analysis application for modeling the cascading effects of events across multiple infrastructure networks. It is a Net-Centric compliant application, relying on a service oriented architecture (SOA) approach to access infrastructure models, data repositories, and mapping tools. NEMO models interactions across electrical power, water, gas, and road networks using an on/off interaction behavior between the components of the different networks, and provides a solid foundation for advancement. NEMO provides a first of its kind capability for observing second and higher order effects of operations against opponents' infrastructure networks. Development goals ­ Efforts are underway to integrate social/political networks into the effects-based operation (EBO) process. Future development needs to enhance the program capabilities for integrating additional relationship definitions, multi-agent capabilities, and optimization. Intended users ­ Planners and analysts are the intended users. System output ­ NEMO displays maps overlaid with nodes and linkages between various infrastructures. Disruptions and cascading effects are highlighted during simulations. Maturity ­ This is a prototype system. Areas modeled ­ Not specified. Customers/sponsors ­ NEMO was internally developed by SPARTA. Model Framework Underlying model ­ The graphical user interface (GUI) is backed by an SOA consisting of two web services; one accesses to a geo-spatial database for storage and retrieval of network databases, and the other coordinates interaction with the various infrastructure models used to provide network status feedback. The geo-spatial database web service, Earth Resource Terrain Hierarchical Archive (ERTHA), contains nearly 200GB of network definitions that may be accessed via the NEMO GUI and used to support effects-based analysis. ERTHA is a geographical information system (GIS) database, based on ESRI products, of infrastructure data items (e.g., power lines, road networks) that were developed as an unclassified source. Abstracting access to data through a web service decouples NEMO from a specific database and specific vendors, making it possible to integrate other data sources in the future. Simulation ­ NEMO provides a basic capability for effects-based planning and performing "what if" analysis of actions. Data format ­ Data is in Environmental Systems Research Institute's (ESRI) ERTHA relational database format. Other models utilize a model interface client (MIC) translator and eXtensible Markup Language (XML). Sensor data ­ Not included. Human activity modeling ­ Changes to include human activity modeling are in progress. Coupling with other models ­ NEMO integrates four infrastructure models: lines of communications, electrical power, gas pipelines, and water pipelines. The models used to evaluate these networks are industry best-of-breed simulation tools for their domains. CitiLabs' Voyager simulation provides road and rail network analysis, while Advantica (formerly Stoner Engineering) provides the Solver tools for electrical power networks as well as the water and gas pipelines. System Requirements Windows XP/2000. Hardware Not Specified. Software 79

Other Notes Efforts are ongoing and mostly complete to integrate social/political networks into the EBO process. For the most part, these efforts are complete. We have integrated the Political Science-Identity (PSI) model (from University of Pennsylvania, Dr. Ian Lustick) into our architecture, and have developed operators that alter the contentedness of a population based on associated physical infrastructure. Further information on PS-I is available at http://jasss.soc.surrey.ac.uk/5/3/7.html.

· ERTHA Web Service ­ Interface for all GIS Infrastructure Models » "Get" shape files and associated attributes Web Service · ArcIMS and ArcXML ­ ESRI's interface to ArcSDE and its Data ­ ArcIMS : Internet Map Server ­ ArcXML : Layer Definition and Query Language for ArcIMS ArcXML · ArcSDE ­ Spatial Database Engine ­ Centralized management of geographic information in a DBMS » Vector, raster, table, annotation, relationships, CAD ­ Contains a subset of JIVA's data · DBMS ­ Oracle database ­ Features as objects » Geometry » Attributes » Behavior (rules, methods, relationships) ­ Uses ArcSDE for multi-user access and versioning

NEMO

ERTHA Web Service
Apache Java JRun

ArcXML ArcIMS ArcSDE
JIVA data

ArcIMS

ArcSDE

DBMS
Oracle Windows

80

References JASSS, PS-1: A User-Friendly Agent-Based Modeling Platform for Testing Theories of Political Identity and Political Stability, http://jasss.soc.surrey.ac.uk/5/3/7.html, Webpage visited July 3, 2006. Sparta, Planning and Assessing Effects Based Operations, www.sparta.com/sew/publications/NEMOfor-ICCRTS.pdf, Webpage visited July 3, 2006.

81

82

Model Name Organization POC

Network-Centric GIS York University, Toronto, Ontario, Canada Rifaat Abdalla abdalla@yorku.ca

Infrastructures RL, HW, WW

Description Overview ­ This system is a framework for using geographical information system (GIS) interoperability for supporting emergency management decision makers by providing effective data sharing and timely access to infrastructure interdependency information. Development goals ­ There are no development goals identified at present. Intended users ­ This is intended for emergency planners and responders. System output ­ Output are GeoServNet (York University GeoICT Lab Product) GIS 2 and 3D images. Maturity ­ Proof of principle. Areas modeled ­ This has been modeled in Vancouver, British Columbia (Earthquake scenario) and Toronto, Ontario (Flood scenario). Customers/sponsors ­ Ongoing research began under Canada's Joint Infrastructure Interdependencies Research program (JIIRP), which is jointly funded by the Natural Sciences and Engineering Research Council (NSERC) and the department of Public Safety and Emergency Preparedness Canada (PSEPC). Model Framework Underlying model ­ Underlying models are GIS technologies including ArcGIS 9 (desktop) and GeoServNet (web-based), GSNBuilder, GSNAdministrator, GSNServer, GSNPublisher, GSNViewer, and HEC-RAS (used for hydraulic simulation with ArcView GIS). Simulation ­ The system has GIS-based spatial-temporal simulations. Data format ­ Data formats are GIS data, graphics, and text. Knowledge-base information is stored in a specially designed object-oriented database. The project used Environmental Systems Research Institute's (ESRI) Geodatabase model. Sensor data ­ Hydraulic gauges provide information for water surface levels and there exists a capability for integrating other live sensor information. Coupling with other models ­ None. Human activity modeling ­ Not included. System Requirements Pentium 4 with 512 RAM, broadband connection. Hardware ESRI ArcGIS, GeoServNet. Software Other Notes Model creation process for the flood model: x Preparation of different data layers x Digitize floodplain, banks, stream centerline, and stream cross section using HEC-GeoRAS extension for ArcView x Input flood parameters using channel geometry created in ArcView and model a flood scenario using HEC-RAS, GIS interoperability is utilized for sharing and visualization of the disaster model x Delineate flood layers using HEC-RAS export ASCII data and data layers with help of ArcView and HEC-GeoRAS extension. Populate flood layers produced in GeoServNet using standard processing procedures. The following steps are useful for defining location based infrastructure interdependencies (LBII) for a particular area: 83

x Identify critical infrastructure sectors in the study area x Analyze processes and operations for each sector x Analyze dependencies x Determine Interdependencies x Collect data x Model and visualize (interoperable 3D internet-based). Earthquake scenario modeling is based on using a Geological Survey of Canada Shakemap for the city of Vancouver. Critical infrastructure at risk was identified based on GIS modeling. Building damage density was analyzed based on IKONOS satellite imagery. Population at risk was identified based on census information and the Shakemap. Location based infrastructure interdependency was modeled.

Spatial Model Showing Critical Infrastructures at Risk

84

GeoServNet 3D Damage Assessment Model of Downtown Vancouver References Abdalla, R. Tao, V. and H. Ali., 2005. "Location-Based Infrastructure Interdependency: New Term, New Modeling Approach," Proceedings of Geoinformatics 2005, Toronto, August 17-19, 2005 CD. Abdalla, R., K. Niall, and V. Tao, 2005. "A framework for modeling Critical Infrastructure Interdependency Using GIS," Canadian Risk and Hazard Symposium, Toronto, 19-21 November 2005. Joint Infrastructure Interdependency Research Program, Modeling Infrastructure Interdependency for Emergency Management Using a Network-Centric Spatial Decision Support System Approach, www.geoict.yorku.ca/JIIRP/Jiirp.htm, Webpage visited July 10, 2006.

85

86

Model Name Organization POC

Network Security Risk Assessment Model (NSRAM) Tool for Critical Infrastructure Protection (CIP) Project James Madison University (JMU), Institute for Infrastructures Infrastructure and Information Assurance EP, CN Philip Riley RileyPB@jmu.edu Jim McManus McManuJP@jmu.edu Samuel T. Redwine, Jr. RedwinST@jmu.edu George Baker BakerGH@jmu.edu Taz Daughtrey DaughtHT@jmu.edu

Description Overview ­ The network security risk assessment model (NSRAM) tool is a complex network system simulation modeling tool that emphasizes the analysis (including risk analysis) of large interconnected multi-infrastructure models. It is designed to be portable, and uses portable and expandable database and model structures. The tool also provides a framework to simulate large networks and analyze their behavior under conditions where the network suffers failures or structural breakdowns. In order to accurately portray the severity of network failures, repair variables (time to repair, cost to repair, repair priorities) must be considered. NSRAM's unique repair element set consists of repair entities with specialized functions that allow users to accurately simulate any configuration of fault detection and repair schemes. The intent of these repair element sets is to more accurately model the human response to perceived system damage. The repair element sets identify symptoms, test the system to determine the elements that are damaged, attempt to repair the damage, and then attempt system recovery. If symptoms are still present, the repair elements repeat the above cycle until the system is recovered. Inspection routines will also be accommodated so that preventative maintenance effects are accurately incorporated. The tool is flexible and can be used to model different infrastructure networks, such as computers, electrical systems, and highway systems. Development goals ­ James Madison University (JMU) is continuing development to add strong security features, improve the graphical user interface (GUI) and database efficiency, and to develop an emergency radio system element set. JMU is also developing the concept of sophisticated repair element sets that interact via predefined algorithms to more accurately simulate repair personnel reaction to system insults or malfunctions. These repair element sets are unique in that they interact with the simulation network model in a predetermined manner, but their operating rules can be changed to allow the user to optimize repair strategies. Intended users ­ Analysts are the intended users. System output ­ NSRAM contains a GUI for developing models and scenarios, and interpreting output. The data output is flexible to facilitate post simulation processing. Maturity ­ NSRAM is currently in development as part of the CIP project. Areas modeled ­ Not specified. Customers/sponsors ­ Not specified. Model Framework Underlying model ­ Not specified. Simulation ­ Developed simulation elements for computer and electrical power distribution networks. 87

Data format ­ Not specified. Sensor data ­ Not specified. Human Activity ­ NSRAM models human activities such as responses to system damage. Coupling with other models ­ Not specified. System Requirements Not specified. Hardware Not specified. Software Other Notes

References McManus, Jim, et al., Network Security Risk Assessment Model(NSRAM) Tool for Critical Infrastructure Protection Project, http://www.jmu.edu/iiia/webdocs/ppt_NSRAM_Tool.ppt, July 12, 2006. Redwine, Sam, et al., Network Security Risk Assessment Model Tool, http://www.jmu.edu/cisat/frd/abstracts04/redwine_sam.html, April 3, 2006.

Model Name Organization POC

Next-generation agent-based economic 'laboratory' (N-ABLE) Sandia National Laboratory Infrastructures Theresa Brown FIN, POL tjbrown@sandia.gov 88

Description Overview ­ The NISAC Agent-Based Laboratory for Economics (N-ABLE) is a software system for analyzing the economic factors, feedbacks, and downstream effects of infrastructure interdependencies. N-ABLE is a simulation environment in which hundreds of thousands to millions of individual economic actors simulate real-world manufacturing firms, households, and government agencies. NABLE can specifically address questions such as: 1. Which economic sectors are most vulnerable to infrastructure disruptions and interdependencies? 2. What firms are most affected  who does well, poorly? 3. What are the different qualitative and quantitative ways in which economic sectors use the energy, transportation, financial, and communication sectors? 4. What short-run infrastructure changes affect economic performance (and vice versa)? 5. How do systems of firms and individuals respond and adapt over time and over regions? 6. What economic mechanisms do national, state, and local governments have or need to have to assist firms and other economic sectors in their regions? Development goals ­ Developed to provide decision makers with a firm-level understanding of the interdependencies between infrastructure sectors and the economy. Intended users ­ Economic Analysts. System Output ­ Geographical charts and statistical output. Maturity ­ Mature Internal. Areas modeled ­ Examples: chemical, food, financial, manufacturing sectors. Customers/sponsors ­ Department of Homeland Security Model Framework Underlying model ­ N-ABLE models the economy at the level of the individual firm; each N-ABLE firm is complete with individual buyers, production supervisors, sellers, and strategic planners who collectively navigate through economic disruption and recovery. N-ABLE's simulations of thousands to millions of firms provide the fidelity necessary to understand and implement better infrastructure policies. Simulation ­ Agent Based. N-ABLE microsimulates the economy using an agent-based discrete-event model. This modeling approach is well suited for investigating the behavior of complex, nonlinear stochastic systems like the economy. Agents start each time increment making decisions much like their real-life counterparts. Decisions about what actions to take are based either on probabilities computed from actual microeconomic data or on results of learning models such as genetic algorithms. These decisions include setting sales prices, purchasing products, setting production schedules, hiring workers, buying and selling financial instruments, conducting open market operations, and others. Macroeconomic variables, such as gross domestic product, inflation (CPI), and the unemployment rate are computed as individual-firm and aggregate system measures of the performance of the economy. Data format ­ not specified. Sensor data ­ None. Ability to couple with other models ­ Not known. Human activity modeling ­ Human in the loop activity supported within the simulation. System Requirements Computer cluster Hardware Not specified. Software

89

Figure 1. Geographical Simulation Output

Figure 2. Statistical Simulation Output References NISAC Agent-Based Laboratory for Economics (N-ABLETM) Fact Sheet http://www.sandia.gov/mission/homeland/factsheets/nisac/NISAC_N-ABLE_factsheet.pdf

90

Model Name

Organization POC

NEXUS Fusion FrameworkTM ­ IntePoint, LLC Critical Infrastructure Integration Modeling and Simulation, University of North Carolina at Charlotte (UNCC) IntePoint, LLC ­ Commercial Product Infrastructures Mark Armstrong EP, TC, HW, HA, RL Mark.Armstrong@IntePoint.com University of North Carolina, UNCC Development wjtotone@uncc.edu

Description Overview ­ NEXUS Fusion FrameworkTM is a planning and response tool that visualizes intended and unintended effects and consequences of an event across multiple infrastructure, social, and population behavior models. It is a single framework that incorporates geospatial, graph based (social, economic), and population behavior models in the same simulation space for cross-infrastructure relationship analysis. The framework takes a holistic system-of-systems view to support cross system analyses of cascading events within and between complex networks. Development goals ­ Not specified. Intended users ­ Department of Defense (DoD) Leadership/Analysts. Output ­ Output includes 2, 2.5, and 3-D graphical and geospatial temporal views of modeled infrastructure. Maturity ­ Version 1.1 was delivered to DoD and accreditation is expected in the summer of 2006. Multiple infrastructure models have already been built & tested using DoD data. Version 2.0 is under development with delivery in summer 2006. Areas modeled ­ Model had been used in many areas including New Orleans, Houston, and Federal Emergency Management Agency (FEMA) Region 5. Customers/sponsors ­ The team is working on the sixth project in 3 years with DoD. Model Framework Underlying model(s) ­ Intelligent agent-based system within the context of a Geographic Information System (GIS) environment, open architecture Simulation ­ Simulation playback offers a foundation for heuristics and supports a collaborative, sharable simulation result that can be viewed by analysts and consumers. Visual display of cause/effect allows determination of rules and inferred relationships. The model supports network component validation and verification of data points. Facilitates identification of missing intelligence. Modification of rules and branching supports analysis of multiple scenarios based on initial starting boundaries. Additionally, multiple geographical regions can participate in the same simulation. Data format - Not specified. Sensor data ­ Architecture supports sensor data, not actively incorporated into the model. Human activity ­ Incorporates infrastructure-aware population behavior models. Coupling with other models ­ Flexible, scaleable, and extensible in that it allows "plug and play" of models of the same infrastructure, multiple models of the same infrastructure, and incorporation of other infrastructure models into the simulation. System Requirements Not specified. Hardware Not specified. Software Other Notes Leverages Environmental Systems Research Institute, Inc. (ESRI) ArcGIS capabilities for geospatial 91

display and analysis. Uses ESRI ArcGIS Geodatabase to capture: x Critical attributes x Critical relationships x Predictive analytics x Meta-driven inference engines x System-of-systems causality analysis x Temporal view x Incorporates specialized functionality off-the-shelf as needed.

92

References

93

94

Model Name Organization POC

Petroleum Fuels Network Analysis Model (PFNAM) Argonne National Laboratory, Infrastructure Assurance Center Steve Folga sfolga@anl.gov

Infrastructures NG, OL

Description Overview ­ Petroleum Fuels Network Analysis Model (PFNAM) was developed to perform hydraulic calculations of pipeline transport of crude oil and petroleum products. A network consists of links (pipe segments), nodes (pipe junctions), pump stations, valves, and pressure-reducing stations. The model tracks the flow of oil in each pipe and the pressure at each node. "Point-and-click" motions allow the analyst to create a representative model of the liquids pipeline network in order to set up and run a simulation. Graphical and tabular results provided for each simulation enable analysts to quantify the impact of infrastructure disruptions on the pipeline segment or system. This software tool provides a framework for introducing pipeline component dependencies into critical infrastructure analyses. Development goals ­ Not specified. Intended users ­ Not specified. System output ­ Results include graphs and tables for steady-state flow rate, pressure, and line capacity distributions. The hydraulic gradient along the pipeline is also displayed. After a simulation, the analysis results indicate the potential effect on pipeline operations. The diagram below indicates that the long-term loss of a specific pump station can lead to isolation or curtailment of the deliveries of petroleum fuels. Maturity ­ The system is in development into the DOT. NET framework. Areas modeled ­ Experts at Argonne have applied PFNAM to a number of crude oil and refined petroleum products pipelines. Other potential applications are being explored. Customers/sponsors ­ US Department of Defense. Model Framework Underlying model ­ Mathematical model. Simulation ­ PFNAM allows the analyst to address a wide range of "what if" questions. Two of the main outputs of a PFNAM simulation are pressure and pipeline capacity estimates along the pipeline. This allows the analyst to determine whether an outage of a pipeline component will result in pipeline shutdown or degradation in pipeline throughput. Data format ­ Access database. Sensor data ­ Accepts pipeline pressure and flow. Coupling with other models ­ This model is compatible with the NG Tool set developed at Argonne. Human activity modeling ­ Human activities are not modeled. System Requirements Not specified. Hardware Not specified. Software Other Notes

95

References

96

97

98

Model Name Organization POC

Transportation Routing Analysis Geographic Information System (TRAGIS) Oak Ridge National Laboratory Infrastructures Paul E. Johnson RL, HW, WW, POL johnsonpe@ornl.gov

Description Overview ­ The Transportation Routing Analysis Geographic Information System (TRAGIS) model is used to calculate highway, rail, or waterway routes within the United States. TRAGIS is a client-server application with the user interface and map data files residing on the user's personal computer and the routing engine and network data files on a network server. By default, the model calculates commercial highway routes; but with the change of the route type, the model can determine routes that meet the U.S. Department of Transportation (DOT) regulations for shipments of highway route-controlled quantities (HRCQ) of radioactive material, routes for shipments to the Waste Isolation Pilot Plant (WIPP), the shortest, or the quickest route. Development goals ­ The goal for WebTRAGIS is to have national 1:100,000-scale routing networks. The highway network developed for TRAGIS is a 1:100,000-scale database. The legacy HIGHWAY model used a stick figure network with nodes digitized at 1:250,000-scale. The TRAGIS highway network was developed from the U.S. Geological Survey (USGS) Digital Line Graphs and the U.S. Bureau of Census Topologically Integrated Geographic Encoding and Referencing (TIGER) system. The rail network used in the initial version of TRAGIS was the same database as that used in the INTERLINE model. This network also was a stick figure network with nodes that were digitized from variable scaled maps. A 1:100,000-scale rail network is now incorporated into TRAGIS. The current inland waterway network is based on the USGS 1:2,000,000-U.S. Geodata. Deep-water routes are depicted in WebTRAGIS as straight-line segments. It is planned to incorporate a 1:100,000-scale waterway database into the model at a future time so that all modes will be at a consistent scale. Intended users ­ internal and external transportation route planners. System output ­ Web-based Graphical 2D map display or textual reports. Maturity ­ Mature ­ commercial. Areas modeled ­ United States. Customers/sponsors ­ Funding for the development of TRAGIS has been provided by the National Transportation Program (NTP) of the U.S. Department of Energy (DOE). Model Framework Underlying model ­ TRAGIS is a client-server application where the user interface and map data files reside on the user's personal computer (PC) and the routing engine and its large data files reside on the server. The model uses the World Wide Web (WWW) for communications between the client and the server. There are two user interfaces for TRAGIS: WebTRAGIS, which is the primary client user interface, and BatchTRAGIS, which is a specialized user interface that allows multiple routes to be prepared and then calculated at one time. Simulation ­ The simulation utilizes a network flow model, which determines the optimal routes based upon an optimization of the impedance measures between endpoints. The impedance is a valued function based upon route type and requirements. Transportation between various sectors is modeled (such as, rail to road transfer, barge to rail, etc.). Population demographics is a component of the model to determine routing criteria for HAZMAT. Data format ­ Not specified. Sensor data ­ None. Coupling with other models ­ Not directly. Human activity modeling ­ No. System Requirements 99

Hardware Software Other Notes

PC with Internet Access. Windows.

100

References WebTragis https://tragis.ornl.gov/ Transportation RoutingAnalysis Geographic Information System (TRAGIS) User's Manual, Oak Ridge National Laboratory, ORNL/NTRC-006 Rev 0, June 2003. https://tragis.ornl.gov/TRAGISmanual.pdf.

101

102

Model Name TRANSIM Los Alamos National Laboratory Organization Infrastructures HW, HA Jim Smith POC Description Overview ­ TRANSIMS is an agent-based simulation system capable of simulating the second-by-second movements of every person and every vehicle through the transportation network of a large metropolitan area. It consists of mutually supporting simulations, models, and databases. By employing advanced computational and analytical techniques, it creates an integrated environment for regional transportation system analysis. TRANSIMS is an integrated suite of products containing an easy-to-use graphical user interface for the modeling functions, a GIS-based network editor, 3D data visualization and animation software, and a reporting system. TRANSIMS is designed to give transportation planners more accurate, complete information on: · Traffic impacts · Energy consumption · Traffic congestion · Land use planning. The core code version of TRANSIMS (TRANSIMS-LANL), developed at Los Alamos National Laboratory, is distributed for a nominal fee to universities on this Web site. The commercial version of TRANSIMS (TRANSIMS-DOT) was developed from the core software package especially for the Department of Transportation by IBM, and it has a more elaborate interface and specific features to meet requirements by the DOT. It is not available on this Web site. Development goals ­ Started as laboratory-directed research and development in the late 1980s for the Department of Transportation. Funding is continuing under Department of Homeland Security (DHS) National Infrastructure Simulation and Analysis Center (NISAC). TRANSIMS technology was developed under U.S. Department of Transportation and EPA funding at the Los Alamos National Laboratory (LANL) over the last eight years. It is a result of an effort to develop new transportation and air quality modeling methodologies required by the Clean Air Act, the Transportation Equity Act for the 21st Century (TEA 21), and other regulations. Intended users ­ Internal analyst ­used to support the development of an impact report on for specific infrastructure events (such as, hurricanes, terrorist attacks, etc.), external analysts. System output ­ Visualization of demographics data with a city or region illustrating the human activity such as traffic patterns and work patterns. Maturity ­ Mature internal and commercial. Areas modeled ­ Customers/sponsors ­ NISAC ­ DHS. Model Framework Underlying model ­ Cellular Autonoma. Simulation ­ Discrete event, agent based simulation. Data ­ Multiple data sources including Census data, Household Survey Data, Dunn and Bradstreet Data. Sensor data ­ No direct sensor feeds. Coupling with other models ­ Yes, coupling is done directly (EPISIM). Human activity modeling ­ Yes ­ social network and human mobility model. System Requirements Memory and disk requirements depend upon the scenario that is used, but large Hardware networks require a large Linux cluster. Some scenarios may consist of 10 ­ 100 103

Software

million agents. The TRANSIMS distribution requires that the user install the following software. Linux · X11R6 libraries (Xmu, Xi, X11, Xext, Xt) · OpenGL and the OpenGL Utilities Toolkit libraries (Mesa/Glut) · Linux libraries (stdc++, ld-linux, ICE, SM) · Perl. All of the third-party software used by TRANSIMS is available on Red Hat Linux distribution CDs. The latest versions of the following packages should be installed: kernel, kernel-headers, gcc, glibc, libstdc++, make, perl, XFree86, Mesa, Mesa-devel, Mesa-Glut, Mesa-Glut-devel, MPI, and PVM. Solaris XllR6 libraries in /usr/openwin, OpenGL, OpenGL Utilities Toolkit libraries (glut), and Perl. Metis, PVM, MPI, and SPRNG are supplied with the TRANSIMS distribution.

Other Notes Los Alamos National Laboratory's TRANSIMS software is based on a computationally intensive, agent-based simulation technology requiring significant multiprocessor computing hardware. Programs in the TRANSIMS software suite are distributed applications with components running on different hardware/software platforms. To install and run all of the components of the TRANSIMS suite, the customer must procure and set up the following three types of computer systems: 1. Unix/Linux server(s) for hosting the core TRANSIMS software, Oracle database, and serverside components of the TRANSIMS modeling interface. Customers who wish to execute largesize problems must have procured multiserver Linux computing cluster or an equivalent multiprocessor UNIX-based framework. 2. Windows workstation(s) for running the Network Editor, the client-side modeling interface, and Crystal Reports. 3. Optional Linux workstation(s) for running the Visualizer. Alternatively, the customer may wish to equip the Linux server with a high-end graphics card and use the server as the Visualizer platform. A version of the output Visualizer that operates on the Windows workstation is in development. TRANSIMS was tested in a Linux cluster environment on Red Hat Linux 6.2 and compiled with gcc/g++ 2.95.2. Limited tests in a single-CPU environment were done on Red Hat Linux 7.1 using gcc/g++ 2.96. To run the traffic microsimulator under PVM or MPI, the Linux kernel must be compiled with networking support and must have an assigned IP address and a host name. An actual network card is not required. The following options must be selected in the Linux kernel configuration: · Networking support (CONFIG_NET) · System V IPC (CONFIG_SYSVIPC) · TCP/IP networking (CONFIG_INET) · Dummy-net driver support (CONFIG_DUMMY) · The appropriate network card driver. The default kernel shipped with Red Hat 6.2 and 7.1 is configured with the appropriate options. The following package categories should be selected during Red Hat Linux installation to run the TRANSIMS components: 104

· X Window System · Mesa/GL · Glut. Additional package categories should be selected to compile the TRANSIMS components: · C Development · Development Libraries · C++ Development · X Development.

105

106

References http://www.transims.net/home.html Zoltán Toroczkai "Agent-Based Modeling as a Decision Making Tool: How to Halt a Smallpox Epidemic How to Halt a Smallpox Epidemic", Center for Nonlinear Studies, Theoretical Division, Los Alamos National Laboratory.

107

108

Model Name Organization POC

Water Infrastructure Simulation Environment (WISE) Los Alamos National Laboratory Joe Holland

Infrastructures DW, SW, ST

Description Overview ­ The Water Infrastructure Simulation Environment (WISE) is an analytic framework supporting the evaluation of water infrastructure in terms of both infrastructure specific and interdependency issues. Development goals ­ Not specified. Intended users ­ Internal analyst ­ IEISS used to support the development of an impact report on for specific infrastructure events (such as, hurricanes, terrorist attacks, etc.). System output ­ Key components in the WISE framework are ArcWISE, a GIS based graphical user interface, and IEISS Water, a water infrastructure interdependency simulation capability within IEISS. ArcWISE leverages the existing data management, analysis, and display capabilities within geographic information systems while also extending them to infer, improve, and amend water infrastructure data in support of running hydraulic simulation engines such as EPANET or IEISS Water. ArcWISE also provides tools for defining and simulating infrastructure damage events, such as a fire, and generating water demand/sewage production estimates. IEISS Water is an extension of the IEISS analysis software to water distribution infrastructure simulation. Maturity ­ Development. Areas modeled ­ Numerous U.S. metropolitan areas. Customers/sponsors ­ NISAC ­ DHS. Model Framework Underlying model ­ Flow and Dispersion Model. Simulation ­ A continuous time based model with an underling physical engine for system dynamics. WISE involves the integration of geographic information systems with a wide range of infrastructure analysis tools including industry standard hydraulic simulation engines (e.g., EPANET and SWMM) as well as Los Alamos National Laboratory interdependency simulation systems such the Urban Infrastructure Suite (UIS) and the Interdependent Energy Infrastructure Simulation System (IEISS). Data format ­ Not specified. Sensor data ­ No direct sensor feeds. Coupling with other models ­ Yes, coupling is done indirectly. The output of IEISS will serve as the input to other infrastructure models to identify cross infrastructure effects. Human activity modeling ­ None at this time. System Requirements Not specified. Hardware ArcWise, EPANET and SWMM. Software Other Notes

109

References

110

Model Name Organization

POC

MIT Screening Methodology--A Screening Methodology for the Identification and Ranking of Infrastructure Vulnerabilities Due to Terrorism Massachusetts Institute of Technology (MIT), Infrastructures Engineering Systems Division and Department of Electric power, Nuclear Science and Engineering natural gas, and George E. Apostolakis drinking water apostola@mit.edu Douglas M. Lemon

Description Overview ­ This research proposes a methodology for the identification and prioritization of vulnerabilities in infrastructures. Portions of the Massachusetts Institute of Technology (MIT) campus were assessed using this methodology. Infrastructures are modeled as digraphs and graph theory is employed to identify the candidate vulnerable scenarios. Screening of scenarios is performed to produce a prioritized list of vulnerabilities. Prioritization is based on multiattribute utility theory (MAUT). The value of a lost element is based on a rated impact of losing infrastructure services. Development goals ­ Professor Apostolakas and others are continuing to extend this work as described in the "Other Notes" section, which follows. Intended users ­ The intended users for this methodology include analysts and decision makers for evaluation and risk management. System output ­ The system provides numeric ranking values for infrastructure elements as output. Maturity ­ This is a research and development level method. Areas modeled ­ Portions of MIT campus including electric power, water, and natural gas infrastructures. Customers/sponsors ­ MIT and the U.S. Navy sponsored the work. Model Framework Underlying model ­ This methodology is based on graph theory, MAUT (for identifying and ranking vulnerabilities), and mathematical network analysis (for infrastructure modeling). Simulation ­ The method allows simulations based on perceived terrorist threats. Data format ­ Not specified. Sensor data ­ None. Coupling with other models ­ Not specified. Human activity modeling ­ None. System Requirements Not specified. Hardware Not Specified. Software Other Notes Since the publication of the subject methodology, MIT has continued similar work. A recent paper, "A Methodology for Ranking the Elements of Water-Supply Networks," co-written by David Michaud-- also of MIT--has been accepted for publication in the Journal of Infrastructure Systems in 2006. That work is based on a case study of a mid-sized city and presents a scenario-based methodology for ranking elements of water-supply networks.

111

References Apostolakas, G., and Lemon, D. (2005). "A Screening Methodology for the Identification and Ranking of Infrastructure Vulnerabilities Due to Terrorism," Risk Analysis, Vol. 25, No. 2, pp. 361-376.

112

Model Name Organization POC

The Urban Infrastructure Suite (UIS) Los Alamos National Laboratory Randy Michelsen rem@lanl.gov Infrastructures HW, HA, TC, AST, SW, DW

Description Overview ­ The Urban Infrastructure Suite (UIS) is a set of interoperable modules that employ advanced modeling and simulation methodologies to represent urban infrastructures and populations. These simulation-based modules are linked through a common interface for the flow of information between UIS sector simulations to model urban transportation, telecommunications, public health, energy, financial (commodity markets), and water-distribution infrastructures and their interdependencies. x Urban Population Mobility Simulation Technologies (UPMoST) Module x Epidemiological Simulation Systems (EpiSims) Module x Telecommunications Sector: AdHopNet Module x Transportation Analysis Simulation System (TRANSIMS) Module x Water Infrastructure Simulation Environment (WISE) x Generic Cities Project Development goals ­ The project objective (NISAC) is to understand the infrastructures' performance under unusual conditions, the effects of interdependencies, and the dynamics of their interconnections. To better understand the complexities of the interconnected infrastructures, the team has collaborated with private sector infrastructure experts to develop methodologies and tools for characterizing and simulating their performance. Intended users ­ LANL internal analysts. System Output ­ Graphical overlays and textual based output. Maturity ­ Development. Areas modeled ­ Multiple. Customers/sponsors ­ DHS ­ NISAC. Model Framework Underlying models: x Urban Population Mobility Simulation Technologies (UPMoST) Module x Epidemiological Simulation Systems (EpiSims) Module - a contact-based approach for evaluating the spread of disease among a populace. It looks at infection rates based on the assumed numbers of contacts people in different demographic groups might have with others in their families, workplaces, and communities. Interactions/contacts are based on the TRANSIM's mobility model. x Telecommunications Sector: MIITS Module (formerly AdHopNet) ­ end to end communications system simulation, agent based simulating individual packets, devices, connections, etc., input is TRANSIMS mobility model. x Transportation Analysis Simulation System (TRANSIMS) Module ­ synthetic population model, cellular automata microsimulation. The output is population mobility with demographics x Water Infrastructure Simulation Environment (WISE) ­ is an analytic framework supporting the evaluation of water infrastructure in terms of both infrastructure specific and interdependency issues. x Generic Cities Project ­ module to create representative but not necessarily accurate city representations in terms of demographic data. Simulation ­ TRANSIM mobility/social network model--agent-based, Epidemic model-- differential equation based. Data ­ Multiple sources. Sensor data ­ None. Ability to couple with other models ­ Suite of coupled modules for different infrastructure 113

sectors. Human Activity modeling ­ Yes. Mobility and Social Interaction Model. System Requirements Hardware Software Other Notes Large models require a Linux Cluster. Linux, various.

Images:

References Barrett, Christopher L, Stephen Eubank, V.S. Anil Kumar, and Madhav V. Marathe From The Mathematics of Networks, Understanding Large-Scale Social and Infrastructure Networks: A Simulation-Based Approach, SIAM News, Volume 37, Number 4, May 2004

114

REFERENCES
1. 2. 3. 4. 5. 6. 7. 8. Congressional Research Service Report for Congress. 2002 Critical Infrastructures: Background, Policy and Implementation. Available online at http://www.iwar.org.uk/cip/resources/pdd63/crs-report.pdf. Congressional Research Service Report for Congress. 2002 Critical Infrastructures: Background, Policy and Implementation. Available online at http://www.iwar.org.uk/cip/resources/pdd63/crs-report.pdf. http://www.tswg.gov Executive Order, 13010. Critical Infrastructure Protection. Federal Register. Vol. 61. No. 138. July 17, 1996. pp. 3747-3750. Executive Order, 13130 National Infrastructure Assurance Council, Federal Register, Vol. 64, No. 137, July 19, 1999. pp. 38535-38536. Executive order 13231 Critical Infrastructure Protection in the Information Age. Federal Register. Vol. 66. No. 202. October 18, 2001. pp. 53063­53071. The NIAC is established on page 53069. D. Mussington, "Concepts for Enhancing Critical Infrastructure Protection: Relating Y2K to CIP Research and Development." RAND:Science and Technology Institute, Santa Monica, CA, 2002, p 29. Layton, L. and D. Phillips. 2001. Train Sets Tunnel Afire, Shuts Down Baltimore. Available online via < http://www.washingtonpost.com/ac2/wp-dyn?pagename=article&node=&contentId=A175422001Jul18>, accessed March 28, 2002. Ratner, A. 2001. Train derailment severs communications. Available online via <http://www.baltimoresun.com/news/local/bal-email19.story?coll=bal-home-headlines>, accessed March 28, 2002.

9.

10. Little, R. and P. Adams. 2001, Tunnel fire choking East Coast rail freight. Available online via <http://www.baltimoresun.com/news/local/bal-te.bz.freight20jul20.story?coll= bal-home-headlines>, Accessed March 28, 2002. 11. M. Dunn, and I.Wigert. International CIIP Handbook 2004: An Inventory and Analysis of Protection Policies in Fourteen Countries. Zurich: Swiss Federal Institute of Technology: 2004, p. 243. 12. D. D. Dudenhoeffer, M. R. Permann, and M Manic, "CIMS: A Framework For Infrastructure Interdependency Modeling And Analysis." Submitted to Proceedings of the 2006 Winter Simulation Conference, L. F. Perrone, F. P. Wieland, J. Liu, B. G. Lawson, D. M. Nicol, and R. M. Fujimoto. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, 2006. 13. S. Rinaldi, J. Peerenboom, and T. Kelly. "Identifying, Understanding, and Analyzing Critical Infrastructure Interdependencies," IEEE Control Systems Magazine, IEEE, December 2001, pp. 11-25. 14. U.S. Department Of Energy Office of Critical Infrastructure Protection. 2001. Critical Infrastructure Interdependencies: Impact of the September 11 Terrorist Attacks on the World Trade Center, A Case Study, 2001, p. 10. 15. UNITED STATES CONGRESS, "U.S.A. Patriot Act", 2001, http://www.epic.org/privacy/terrorism/hr3162.html. 16. UNITED STATES CONGRESS, "U.S.A. Patriot" Act, 2001, http://www.epic.org/privacy/terrorism/hr3162.html 115

17. United States Joint Forces Command, Operational Net Assessment (ONA) Concept Of Operations For Millennium Challenge 02 (MC-02), October 2001. 18. Hammes, T. The Sling and the Stone, Zenith Press, St. Paul MN, 2004, p.2. 19. United States Joint Forces Command, The Joint Warfighting Center, Joint Doctrine Series Pamphlet 4, Doctrinal Implications of Operational Net Assessment (ONA), 2004. 20. LandScan, http://www.ornl.gov/sci/landscan/index.html. 21. Los Alamos National Laboratory, http://lanl.gov/orgs/d/nisac/, http://www.sandia.gov/mission/homeland/programs/critical/nisac.html. 22. National Energy Technology Laboratory, http://www.netl.doe.gov/onsite_research/Facilities/energy.html. 23. Technical Support Working Group, http://www.tswg.gov. 24. Federal Business Opportunities, http://www.fbo.gov/spg/USAF/AFMC/ AFRLRRS/Reference%2DNumber%2DBAA%2D06%2D07%2DIFKA/SynopsisP.html. 25. Amin, M. "National Infrastructures as Complex Interactive Networks", Chapter 14 in Automation,Control, and Complexity: New Developments and Directions, Samad & Weyrauch (Eds.), John Wiley and Sons, March 2000, Table 14.1. 26. Schmitz, W. and K. A. Neubecker. 2003. Architecture of an Integrated Model Hierarchy: Work Package 6, Deliverable D6.2, ACIP Technical Report IST-2001-37257: 32. Available via http://www.iabg.de/acip/doc/wp6/D62_architecture.pdf, Accessed April 1, 2006.

116

arXiv:1404.6890v1 [cs.DS] 28 Apr 2014

Analysis of d-Hop Dominating Set Problem for
Directed Graph with Indegree Bounded by One
Joydeep Banerjee, Arun Das and Arunabha Sen
Computer Science and Engineering Program,
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
{Joydeep.Banerjee, adas,asen}@asu.edu
April 29, 2014
Abstract
d-hop dominating set problem was introduced for cluster formation in
wireless ad-hoc networks and is proved to be NP-complete. Dominating
set problem for directed graph with indegree of at most 1 can be solved
polynomially. In this article we perform an analysis of d-hop dominating
set problem for directed graph with indegree 1. We show that the problem
can be solved polynomially by exploiting certain properties of the graph
under consideration.

1

Introduction

d-hop minimum dominating set problem was introduced in [1] for cluster formation in wireless ad-hoc networks. The problem is proved to be NP complete.
The decision version of minimum d-hop dominating set problem is given as follows:
Instance: An undirected graph G = (V, E), two positive integers d and K.
â²
â²
Question: Is there a subset |V | â V with |V | â¤ K such that for every vertex
â²
â²
vâ
/ V â V is at most d hops away from at least one vertex in V .
In a directed graph G = (V, E) a node u dominates a node v if the edge
(u, v) â E. For a directed graph dominating set is proved to NP complete[2].
But for directed graph with indegree of at most 1, the problem can be solved
polynomially [2].
In this article we analyze the problem d-hop minimum dominating set problem
for directed graph with indegree bounded by 1. We show the existence of a poly-

1

nomial time algorithm of the problem by exploiting certain properties of the
graph under consideration.

2

Analysis of the Problem

Certain properties and definitions of a directed graph with indegree bounded
by one (denoted as the graph GD = (VD , ED ) is introduced before analysis of
the problem.
Property 2.1 Each weakly connected subgraph of the directed graph GD consists of at most one cycle with no incoming edge to a node in the cycle from a
node not in the cycle.
Thus a weakly connected component is either a Directed Acyclic Graph (DAG)
or has a cycle with all nodes not in the cycle having a directed path from all
nodes in the cycle to it.
Definition 2.1 A leaf node of a weakly connected component is defined
as the node with an incoming edge and no outgoing edge.
Definition 2.2 The farthest leaf node of a weakly connected component is
defined as the node which is at furthest hops from the root node (if the graph is
a DAG) or from a node in the cycle.
Definition 2.3 An isolated strongly connected component is defined as
the component which is not a subgraph of a weakly connected component of the
graph GD
Property 2.2 Each isolated strongly connected subgraph of the directed graph
GD is composed of exactly one cycle.
Exploiting the properties of the graph GD an algorithm (Algorithm 1) is
framed. The proof of optimality and time complexity analysis of the algorithm
are done in Theorem 1 and Theorem 2 respectively.
Theorem 2.1 Algorithm 1 gives the optimum solution of d-hop dominating set
problem for the graph GD .
Proof For an isolated strongly connected component GS = (VS , ES ) at least
â |VdS | â nodes has to be included in the solution. Algorithm 1 selects â |VdS | â nodes
with at least â |VdS | â â 1 nodes separated by exactly d hops. Thus it includes
the optimum nodes in the solution. For a weakly connected component a leaf
node has to be dominated by a node within d hops away from it. Selecting
the farthest leaf node would ensure that after updating the graph GW with
node removals would not result in segregation of the updated graph with more
than one weakly connected component. Moreover selecting a leaf node which
2

Algorithm 1 Algorithm for finding d-hop dominating set of graph GD
Set D = â;
Compute all weakly connected component and isolated strongly connected
component of graph GD ;
for (Each weakly connected component GW = (VW , EW ) of graph GD ) do
while (Graph GW is not empty) do
if (Graph GW is a isolated strongly connected component) then
For all n nodes in graph GW , include â nd â nodes in set D with at
least â nd â â 1 nodes separated by exactly d hops;
break;
end if
Pick the farthest leaf node v from graph GW ;
Include node u in set D such that the number of hops from u to v is
maximum but is less than d;
Update graph GW by removing all nodes (and their edges) which are
within d hops from u, including node u;
end while
end for
for (Each isolated strongly connected component GS = (VS , ES ) of graph
GD ) do
Include â |VdS | â nodes in set D with at least â |VdS | â â 1 nodes separated by
exactly d hops;
end for
is not farthest might result in excluding the node which d-hop dominates it
along with the farthest leaf node. Thus another node needs to be included in
the solution to d-hop dominate the farthest leaf node and so the solution would
not be optimum. Hence a node that is maximum hops away from the farthest
leaf node (with number of hops less than equal to d) for the weakly connected
graph GW (with or without updating with node removals) has to be included
in the optimum solution of the problem. Additionally consider computing the
d-hop dominating set for a weakly connected component GW with one cycle.
This may result in an isolated strongly connected component of original graph
GW (i.e. the cycle) after subsequent update of graph GW with node removals
(as in Algorithm 1). The d-hop dominating set of the resultant graph then can
be solved similarly to that of strongly connected component, which is optimum.
Hence Algorithm 1 computes the optimum d-hop dominating set problem for
each weakly connected component and strongly connected component of graph
GD polynomially.
Theorem 2.2 Algorithm 1 solves d-hop dominating set problem for the graph
GD polynomially with time complexity of order O
Proof All strongly connected components can be found using Tarjanâs strongly
connected components algorithm [3] in O(|VD | + |ED |) time. The isolated
3

strongly connected component from the computed strongly connected components can be found in another O(|VD |) time. Hence computing the isolated
strongly connected component takes O(|VD |(|VD | + |ED |)) time. Deleting all
nodes in the strongly connected component would result in segregation of the
graph GD into weakly connected components. All weakly connected components can be found by computing existence of pairwise path between two nodes.
It can be done in O(|VD | + |ED |) time for each pair and each weakly connected
component can be computed in O(|VD |(|VD |+ED |)) time. Both the first for loop
and the while loop inside it iterates for at most O(|VD |) times. The operation
inside the if branch can be computed O(|VD |) time. The farthest leaf node can
be found in O(|VD |2 ) time. The d-hop dominating node of the farthest leaf node
can be found in O(|VD |) time. Hence the d-hop dominating set computation
for all weakly connected component takes O(|VD |4 ) time. For computing the
d-hop dominating set for all strongly connected component, the for loop takes
O(|VD |) time. The computation inside the for loop is same as the computation
inside the if branch of the weakly connected component d-hop dominating set
computation and takes O(|VD |) time. So the total time complexity for finding
d-hop dominating set for strongly connected component is O(|VD |2 ). Hence
Algorithm 1 takes in total an O(|VD |4 ) time to compute the optimum solution
of the problem.

3

Conclusion

In this article analysis of d-hop dominating set for directed graph with indegree
bounded by one is performed. It is found that exploiting certain properties of
the graph under consideration an algorithm can solve the problem in polynomial
time, with run time complexity bounded by four times the number of vertices
in the graph.

References
[1] Amis, Alan D., et al. âMax-min d-cluster formation in wireless ad hoc networks.â INFOCOM 2000. Nineteenth Annual Joint Conference of the IEEE
Computer and Communications Societies. Proceedings. IEEE. Vol. 1. IEEE,
2000.
[2] Albers, Susanne, and Tomasz Radzik, eds. AlgorithmsâESA 2004: 12th Annual European Symposium, Bergen, Norway, September 14-17, 2004, Proceedings. Vol. 12. Springer, 2004.
[3] Tarjan, R. E. âDepth-first search and linear graph algorithmsâ, SIAM Journal on Computing 1 (2): 146160, 1972.

4

arXiv:1404.6890v1 [cs.DS] 28 Apr 2014

Analysis of d-Hop Dominating Set Problem for
Directed Graph with Indegree Bounded by One
Joydeep Banerjee, Arun Das and Arunabha Sen
Computer Science and Engineering Program,
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
{Joydeep.Banerjee, adas,asen}@asu.edu
April 29, 2014
Abstract
d-hop dominating set problem was introduced for cluster formation in
wireless ad-hoc networks and is proved to be NP-complete. Dominating
set problem for directed graph with indegree of at most 1 can be solved
polynomially. In this article we perform an analysis of d-hop dominating
set problem for directed graph with indegree 1. We show that the problem
can be solved polynomially by exploiting certain properties of the graph
under consideration.

1

Introduction

d-hop minimum dominating set problem was introduced in [1] for cluster formation in wireless ad-hoc networks. The problem is proved to be NP complete.
The decision version of minimum d-hop dominating set problem is given as follows:
Instance: An undirected graph G = (V, E), two positive integers d and K.
â²
â²
Question: Is there a subset |V | â V with |V | â¤ K such that for every vertex
â²
â²
vâ
/ V â V is at most d hops away from at least one vertex in V .
In a directed graph G = (V, E) a node u dominates a node v if the edge
(u, v) â E. For a directed graph dominating set is proved to NP complete[2].
But for directed graph with indegree of at most 1, the problem can be solved
polynomially [2].
In this article we analyze the problem d-hop minimum dominating set problem
for directed graph with indegree bounded by 1. We show the existence of a poly-

1

nomial time algorithm of the problem by exploiting certain properties of the
graph under consideration.

2

Analysis of the Problem

Certain properties and definitions of a directed graph with indegree bounded
by one (denoted as the graph GD = (VD , ED ) is introduced before analysis of
the problem.
Property 2.1 Each weakly connected subgraph of the directed graph GD consists of at most one cycle with no incoming edge to a node in the cycle from a
node not in the cycle.
Thus a weakly connected component is either a Directed Acyclic Graph (DAG)
or has a cycle with all nodes not in the cycle having a directed path from all
nodes in the cycle to it.
Definition 2.1 A leaf node of a weakly connected component is defined
as the node with an incoming edge and no outgoing edge.
Definition 2.2 The farthest leaf node of a weakly connected component is
defined as the node which is at furthest hops from the root node (if the graph is
a DAG) or from a node in the cycle.
Definition 2.3 An isolated strongly connected component is defined as
the component which is not a subgraph of a weakly connected component of the
graph GD
Property 2.2 Each isolated strongly connected subgraph of the directed graph
GD is composed of exactly one cycle.
Exploiting the properties of the graph GD an algorithm (Algorithm 1) is
framed. The proof of optimality and time complexity analysis of the algorithm
are done in Theorem 1 and Theorem 2 respectively.
Theorem 2.1 Algorithm 1 gives the optimum solution of d-hop dominating set
problem for the graph GD .
Proof For an isolated strongly connected component GS = (VS , ES ) at least
â |VdS | â nodes has to be included in the solution. Algorithm 1 selects â |VdS | â nodes
with at least â |VdS | â â 1 nodes separated by exactly d hops. Thus it includes
the optimum nodes in the solution. For a weakly connected component a leaf
node has to be dominated by a node within d hops away from it. Selecting
the farthest leaf node would ensure that after updating the graph GW with
node removals would not result in segregation of the updated graph with more
than one weakly connected component. Moreover selecting a leaf node which
2

Algorithm 1 Algorithm for finding d-hop dominating set of graph GD
Set D = â;
Compute all weakly connected component and isolated strongly connected
component of graph GD ;
for (Each weakly connected component GW = (VW , EW ) of graph GD ) do
while (Graph GW is not empty) do
if (Graph GW is a isolated strongly connected component) then
For all n nodes in graph GW , include â nd â nodes in set D with at
least â nd â â 1 nodes separated by exactly d hops;
break;
end if
Pick the farthest leaf node v from graph GW ;
Include node u in set D such that the number of hops from u to v is
maximum but is less than d;
Update graph GW by removing all nodes (and their edges) which are
within d hops from u, including node u;
end while
end for
for (Each isolated strongly connected component GS = (VS , ES ) of graph
GD ) do
Include â |VdS | â nodes in set D with at least â |VdS | â â 1 nodes separated by
exactly d hops;
end for
is not farthest might result in excluding the node which d-hop dominates it
along with the farthest leaf node. Thus another node needs to be included in
the solution to d-hop dominate the farthest leaf node and so the solution would
not be optimum. Hence a node that is maximum hops away from the farthest
leaf node (with number of hops less than equal to d) for the weakly connected
graph GW (with or without updating with node removals) has to be included
in the optimum solution of the problem. Additionally consider computing the
d-hop dominating set for a weakly connected component GW with one cycle.
This may result in an isolated strongly connected component of original graph
GW (i.e. the cycle) after subsequent update of graph GW with node removals
(as in Algorithm 1). The d-hop dominating set of the resultant graph then can
be solved similarly to that of strongly connected component, which is optimum.
Hence Algorithm 1 computes the optimum d-hop dominating set problem for
each weakly connected component and strongly connected component of graph
GD polynomially.
Theorem 2.2 Algorithm 1 solves d-hop dominating set problem for the graph
GD polynomially with time complexity of order O
Proof All strongly connected components can be found using Tarjanâs strongly
connected components algorithm [3] in O(|VD | + |ED |) time. The isolated
3

strongly connected component from the computed strongly connected components can be found in another O(|VD |) time. Hence computing the isolated
strongly connected component takes O(|VD |(|VD | + |ED |)) time. Deleting all
nodes in the strongly connected component would result in segregation of the
graph GD into weakly connected components. All weakly connected components can be found by computing existence of pairwise path between two nodes.
It can be done in O(|VD | + |ED |) time for each pair and each weakly connected
component can be computed in O(|VD |(|VD |+ED |)) time. Both the first for loop
and the while loop inside it iterates for at most O(|VD |) times. The operation
inside the if branch can be computed O(|VD |) time. The farthest leaf node can
be found in O(|VD |2 ) time. The d-hop dominating node of the farthest leaf node
can be found in O(|VD |) time. Hence the d-hop dominating set computation
for all weakly connected component takes O(|VD |4 ) time. For computing the
d-hop dominating set for all strongly connected component, the for loop takes
O(|VD |) time. The computation inside the for loop is same as the computation
inside the if branch of the weakly connected component d-hop dominating set
computation and takes O(|VD |) time. So the total time complexity for finding
d-hop dominating set for strongly connected component is O(|VD |2 ). Hence
Algorithm 1 takes in total an O(|VD |4 ) time to compute the optimum solution
of the problem.

3

Conclusion

In this article analysis of d-hop dominating set for directed graph with indegree
bounded by one is performed. It is found that exploiting certain properties of
the graph under consideration an algorithm can solve the problem in polynomial
time, with run time complexity bounded by four times the number of vertices
in the graph.

References
[1] Amis, Alan D., et al. âMax-min d-cluster formation in wireless ad hoc networks.â INFOCOM 2000. Nineteenth Annual Joint Conference of the IEEE
Computer and Communications Societies. Proceedings. IEEE. Vol. 1. IEEE,
2000.
[2] Albers, Susanne, and Tomasz Radzik, eds. AlgorithmsâESA 2004: 12th Annual European Symposium, Bergen, Norway, September 14-17, 2004, Proceedings. Vol. 12. Springer, 2004.
[3] Tarjan, R. E. âDepth-first search and linear graph algorithmsâ, SIAM Journal on Computing 1 (2): 146160, 1972.

4

Influence Propagation in Adversarial Setting: How to Defeat Competition with Least Amount of Investment
Shahrzad Shirazipourazad, Brian Bogard, Harsh Vachhani, Arunabha Sen
School of Computing, Informatics and Decision Systems Engineering Arizona State University Tempe, AZ 85287

{sshiraz1, bbogard, hvachhan, asen}@asu.edu Paul Horn
Department of Mathematics Harvard University Cambridge, MA 09322

phorn@math.harvard.edu ABSTRACT
It has been observed that individuals' decisions to adopt a product or innovation are often influenced by the recommendations of their friends and acquaintances. Motivated by this observation, the last few years have seen a number of studies on influence maximization in social networks. The primary goal of these studies is identification of k most influential nodes in a network. A major limitation of these studies is that they focus on a non-adversarial environment, where only one player is engaged in influencing the nodes. However, in a realistic scenario multiple players attempt to influence the nodes in a competitive fashion. The proposed model considers a competitive environment where a node that has not yet adopted an innovation, can adopt only one of the several competing innovations and once it adopts an innovation, it does not switch. The paper studies the scenario where the first player has already chosen a set of k nodes and the second player, with the knowledge of the choice of the first, attempts to identify a smallest set of nodes (excluding the ones already chosen by the first) so that when the influence propagation process ends, the number of nodes influenced by the second player is larger than the number of nodes influenced by the first. The paper studies two propagation models and shows that in both the models, the identification of the smallest set of nodes to defeat the adversary is NP-Hard. It provides an approximation algorithm and proves that the performance bound is tight. It also presents the results of extensive experimentation using the collaboration network data. Experimental results show that the second player can easily defeat the first with this algorithm, if the first utilizes the node degree or closeness centrality based algorithms for the selection of influential nodes. The proposed algorithm also provides better performance if the second player utilizes it instead of the greedy algorithm to maximize its influence.

Categories and Subject Descriptors
F.2.2 [Analysis of Algorithms and Problem Complexity]: [Non-numerical Algorithms and Problems]

General Terms
Algorithms, Experimentation, Performance

Keywords
Social Networks, Influence Maximization, Adversarial Environment

1.

INTRODUCTION

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. CIKM'12, October 29­November 2, 2012, Maui, HI, USA. Copyright 2012 ACM 978-1-4503-1156-4/12/10 ...$15.00.

It has been widely observed in various studies in social sciences and economics that an individuals' decision to adopt a product, behavior or innovation is often influenced by the recommendations of their friends and acquaintances. Motivated by this observation, the last few years have seen a number of studies on influence maximization problem in social networks [2, 3, 4, 6, 7, 11, 13]. One major goal of several of these studies is identification of k most influential nodes in a network. A product manufacturer may want to identify the k most influential nodes in the network, as she may want to incentivize these nodes to buy the new product by providing free samples to them, on the expectation that once these nodes are convinced about the quality of the product, they will recommend it to their friends on the social network and encourage them to buy the product. This set of k nodes, being the most influential on the network, will have the largest impact on convincing the rest of the nodes about the quality of the product. Since the manufacturer has a fixed budget for advertising, she can provide free samples only to a limited number of nodes in the network. The size of the advertising budget determines the value of the parameter k. It may be noted that most of the studies on influence propagation are geared toward a non-adversarial environment, where only one manufacturer (player) is attempting

to influence the nodes of a social network to buy her product. However, in a realistic market scenario, most often there exists multiple players, each attempting to sell their competing products or innovations. For example, just as Coke attempts to convince customers in an emerging market about the quality of their beverage, its main competitor, Pepsi, also does the same. Both the competitors have only a finite advertisement budget and both of them want to derive the greatest benefit out of their advertising campaign. The goal of both the players often is to capture a share of this emerging market that is larger than its competition. The non-adversarial influence propagation models consider scenarios where a user (a node u in a social network graph G = (V, E )) adopts (or does not adopt) an innovation based on how her acquaintances have adopted the innovation. In these models each node u in the social network graph is in one of the following two states: (i) u has adopted innovation A, and (ii) u has not adopted innovation A but u is open to the idea of adoption. One can visualize such a scenario by coloring the nodes of the social network graph with red if they have adopted the innovation A and with white if they have not adopted A yet, but are open to the idea of adopting A in the future. As the diffusion process progresses with time, by observing changing color of the nodes of the graph one can infer if innovation A is being adopted by the members of the social network. Although, this paper focus on influence propagation in social networks, conceptually, the scenario is identical for spread of any contagion through a network - be it spread of diseases through a human contact network or spread of worms through the Internet. The influence (contagion) propagation models can be divided into three distinct classes: · Class I: Non-adversarial · Class II: Adversarial with passive adversary · Class III Adversarial with active adversary The problems in classes I and II can be stated as follows: · Class I: How to identify a set of k initial (seed) nodes, so that once they are influenced/infected, they will infect the largest number of uninfected nodes in the network? · Class II: Given that a subset of the nodes is already influenced/infected, how to identify a set of k uninfected nodes, so that when they are immunized, they will have the largest impact in preventing the uninfected nodes from being infected. In most of the influence propagation models, influence propagates in a step-by-step fashion and as such there is a notion of time step (or propagation step) involved. The expected number of nodes influenced at the end of time step D is at most the expected number of nodes influenced at the end of time step D + 1. In other words, expected number of nodes influenced at the end of time step D is a nondecreasing function of D. The Class I influence propagation problem considered in [11] may be viewed to have three dimensions, (i) the number of seed nodes activated at the beginning (budget or cost of influence), (ii) the expected number of activated nodes at the end of propagation (impact or coverage of initial seed nodes), and (iii) time steps for propagation. The objective of the influence maximization problem considered in [11], is

to maximize the coverage subject to a budget constraint but without any constraint on the number of time steps. The Class I problem considered in [11] can be stated in the following way: "Which k white nodes should be colored red initially, so that the largest number of white nodes turn to red at the end of propagation process?". The Class II problems can be stated in the following way: "Given that some nodes are already colored red, which k white nodes should be colored blue, so that this set of nodes will have the largest impact in preventing the white nodes from turning red. In Class I, there is no notion of an adversary. The red nodes are trying to convert all the white nodes into red nodes and there is no agent that is actively trying to prevent this conversion. The Class II, although it has a notion of an adversary (i.e., the blue nodes) which is trying to slow down (or stop) white-to-red conversion, at best this agent can be viewed as a passive adversary, because its goal is to prevent white-to-red conversion, and it is not engaged in white-to-blue conversion. This gives rise to Class III, a truly adversarial scenario, where the red agent is trying to convert all the white nodes into red, while the blue agent is trying to convert all the white nodes into blue. In this case, the blue agent can be viewed as an active adversary of the red agent. The Class III models the scenario where a node u is being actively encouraged by an adversary not only not to adopt the innovation but also to adopt a competing innovation. In this case, each node u in the social network graph can be in one of following three states: (i) u has adopted innovation A , (ii) u has adopted innovation B , and (iii) u has not adopted any innovation A or B but is open to the idea of adopting either one of them. This adversarial scenario can be viewed as a classic case of a strategic conflict game between the proponent(s) and the opponent(s) of adoption of an innovation and a game is won by the proponent(s) if u decides to adopt the innovation A. This paper studies a Class III scenario where two vendors (players) are trying to sell their competing products by influencing the nodes of a social network. The goal of both the players is to have a market share that is larger than its competition. It considers the scenario where the first player (P1 ) has already chosen the k nodes to have a large influence (coverage) on the social network. The second player is aware of the first player's choice and the goal of the second player (P2 ) is to identify a smallest set of nodes (excluding the ones already chosen by the first player) so that the number of nodes influenced by the second player will be larger than the number of nodes influenced by the first player within D time steps. In other words, the objective of the problem is to minimze the cost subject to the constraint that the coverage of the second player is larger than the coverage of the first player within D time steps. Since the goal of the second player is to win the "game" (i.e., to have a larger coverage or market share), with influencing (incentivizing) as few nodes as possible, the problem under study in this paper is referred to as the "Winning with Minimum Investment" (WMI) problem. In [3], the authors study a similar problem belonging to class III. However, the objective of the problem studied in [3] is different from the one being studied in this paper. The goal of the second player in the problem studied in [3] is not to defeat the first player with least amount of investment, but to maximize its own influence.

Using the same two influence propagation models introduced in [3], the contributions of the paper may be listed as follows: · Introduction of a new influence propagation problem in an adversarial setting where the goal of the second player is to defeat the first within D time steps and least amount of cost (i.e., number of seed nodes) · NP-Hardness proof for the problem under both the influence propagation models · Approximation algorithm for the problem with a tight performance bound. · Experimental evaluation of the Approximation algorithm with collaboration network data Experimental results show that utilizing the proposed algorithm, the second player can easily defeat the first, if the first player utilizes the node degree or closeness centrality based algorithms for the selection of the initial (seed) nodes. The proposed algorithm also provides better performance for the second player if she utilizes it instead of the algorithm to maximize influence proposed in [3], in the sense that it requires selection of a fewer number of seed nodes to defeat the first player. The rest of the paper is organized as follows. The section II summarizes related work on influence propagation. The section III describes the propagation models used in the paper in detail. The sections IV, V and VI discuss the problem statement, computational complexity and approximation algorithm results respectively. The results of experimental evaluation is presented in section VII and section VIII concludes the paper.

2.

BACKGROUND AND RELATED WORK

The studies on identification of influential nodes in a social network were triggered by a paper authored by Domingos and Richardson [6]. They introduced the notion of "network value" of a node in a social network and using a Markov random field model where a joint distribution over all node behavior is specified, computed the network value of the nodes. Kempe, Kleinberg and Tardos followed up the work in [6] by providing new models derived from mathematical sociology and interacting particle systems [11]. They made a number of important contributions by providing approximation algorithms for maximizing the spread of influence in these models by utilizing the submodularity property of the objective functions. In addition to providing algorithms with provable performance guarantee, they also presented experimental results on large collaboration networks. Their experimental results showed that their greedy approximation algorithm significantly out-performed the node selection heuristics based on degree centrality and distance centrality [18]. The approximation algorithm proposed in [11] is computeintensive. Accordingly, several researchers approached the issue of scalability from different directions. Chen et. al. in [4] provided improvement of the original greedy algorithm of [11] and proposed a degree discount heuristic to improve influence spread. Mathioudakis in [13] introduced the notion of sparsification of influence networks and presented an algorithm, SPINE, to compute the "backbone" of the influence network. Utilizing SPINE as a pre-processing step for the

influence maximization problem, they showed that computation on the sparsified model provided significant improvements in terms of speedup without compromising accuracy. Wang et. al. in [17] considered the influential node identification problem in a mobile social network and presented a two step process, where in the first step, communities in the social network are detected and in the second step a subset of communities is selected to identify the influential nodes. Experimental results with data from large real world mobile social network showed that their algorithm performed an order of magnitude faster than the state-of-the-art greedy algorithm for finding the top-k influential nodes. A simulated annealing (SA) based algorithm for finding the top-k influential nodes was presented in [10]. It has been reported in [10], that using data from four real networks, the SA based algorithm performed 2-3 orders of magnitude faster than the state-of-the-art greedy algorithm. In addition to attempts to address the scalability issue of the greedy algorithm in [11], efforts on variations of the original problem formulation and also the computation model is underway in the research community. In [7] two new problem formulations are provided. In the first formulation, the goal is to minimize the cost, subject to the constraint that coverage exceeds a minimum threshold  without any constraint on the number of time steps. The goal of the second formulation is to minimize the number of time steps, subject to a budget constraint k and a coverage constraint  . For the first version of the problem, the authors provide a simple greedy algorithm and show that it provides a bicriteria approximation. For the second version, they show that even bicriteria or tricriteria approximations are hard under several conditions. In [1], the authors argue that a user (a node in the social network) may be influenced by positive recommendations from a group of friends (neighbors in the network) but that does not necessarily imply that she will adopt the product herself. However, she may pass on her positive impression about the product to another group of friends. Clearly, such a model departs from the model considered in [11]. The authors in [1] consider an "adoption maximization" problem instead of "influence maximization" problem and present both analytical and experimental results for the new problem. The authors in [12] argue that a limitation of the traditional influence analysis technique is that they only consider positive relations (agreement, trust) and ignore the negative relations (distrust, disagreement). Moreover, the traditional techniques also ignore conformity of people, i.e., an individual's inclination to be influenced. The paper studies the interplay between influence and conformity of each individual and computes the influence and conformity indices of individuals. The authors in [5] suggest an alternate way of measuring the influencing capability of an individual on her peers, through the individuals reach within the social network for certain actions. All the references discussed in the last three paragraphs pertain to the class I (non-adversarial) problems as defined in the previous section. Results on study of class II problems (adversarial with passive adversary) is presented in [8]. It focuses on identification of blockers, the nodes that are most effective in blocking the spread of a dynamic process through a social network, and reports that simple local measures such as the degree of a node are good indicators of its effectiveness as a blocker. The blocker identification problem has been extensively studied in the public health community, where

the goal is to stop or slow down progress of an infectious disease by immunizing a small set of key individuals in the community. As indicated in the previous section, the WMI problem studied in this paper belongs to Class III (adversarial with active adversary). Unfortunately, there exists only a handful of studies on problems belonging to Class III. Bharathi et. al. were one of the earliest to study a Class III problem [2]. They proposed a mathematical model for diffusion of multiple innovations in a network, an approximation algorithm with a (1 - 1/e) performance guarantee for computing the best response to an opponent's strategy. In addition they prove that the "price of competition" of the game is at most 2. While game theoretic framework was utilized for deriving the results in [2], Carnes et al. used an algorithmic framework to study a Class III problem [3]. Their research primarily extends the problem studied in [11] from the Class I domain to the Class III domain. They study the follower's perspective (i.e., the player who entered the market after the first player) and investigate how a follower can maximize her influence in the network with a limited budget, given that the first player has already entered the market and influenced a certain number of key individuals (nodes in the network). They prove that the influence maximization problem for the second player is NP-complete and provide an approximation algorithm that is guaranteed to produce a solution within 63% of the optimal. Adversarial models in evolutionary game dynamics was studied by Istrate em et al. in [9]. In all the problems discussed in [2, 3] once a node adopts an innovation (i.e., changes its color from white to red or white to blue), it is not allowed to change its color, i.e., the model precludes the possibility of an individual changing her mind. However, the model considered by Nowak et al. in [16] there are only red and blue nodes (no white nodes) and the model allows a node to change its color from red to blue and vice-versa. Although this model was developed to capture a biological phenomenon involving viruses and cells, this model can be equally effective in capturing the phenomenon of the spread of ideas and behaviors in human population. Using evolutionary game theoretic and evolutionary graph theoretic techniques, the authors establish fundamental laws that govern choices of competing players regarding strategies.

spective. Since this paper studies the problem with only two competing players, the models proposed in [3] are more relevant for this study than the one proposed in [2]. Accordingly, the influence propagation models of [3] are used here. Since these models, Distance-based Model (DBM) and Wavepropagation Model (WPM), are generalization of the ICM, the paper first discusses ICM and then DBM and WPM.

3.1

Independent Cascade Model

The social network is modeled as a graph G = (V, E ), where each node represents an individual. Each individual may either be active (i.e., has adopted innovation) or inactive. A node can switch from an inactive state to an active state but cannot switch back in the other direction. The propagation process from the perspective of an inactivate node v  V can be described in the following way: With passage of time, more and more of v 's neighbors become active and this may cause v to become active at some time step. The activation of v in turn may trigger activation of some of v 's inactive neighbors. In the ICM model there exists a set of nodes V  V that are active (seed nodes) initially and the rest of the nodes are inactive. Influence propagation unfolds in discrete steps following a randomized process. When a node v first becomes active in time step d, it has a single chance to activate each of its inactive neighbors w with probability pv,w at time step d + 1. If v succeeds, w become active at d + 1. However, if v fails, it doesn't get another chance to turn w active. The process of conversion of nodes from the inactive to the active state continues, till no further activation is possible. Since v influences w with probability pv,w , the v - w edge is considered active with probability pv,w . The set of active edges is denoted by Ea .

3.2

Generalized ICM for Adversarial Scenario

3.

INFLUENCE PROPAGATION MODELS

A number of influence propagation models for the non-adversarial scenario have been proposed in the literature [11]. Among these, the Linear Threshold Model (LTM) and the Independent Cascade Model (ICM) have drawn most attention in the research community. As indicated earlier, the literature on influence propagation in adversarial scenario with active adversaries is very sparse [2, 3]. Bharati et al. in [2] and Carnes et al. in [3] have studied influence propagation in adversarial scenario with active adversaries, and have proposed two different models for it. Both of these two models are generalizations of the Independent Cascade Model. The model proposed in [2] is suitable for a multiplayer scenario, whereas the model proposed in [3] is for two competing players. Bharati et al. in [2] study the problem from a game-theoretic perspective and focus on finding best response strategies for the players. Carnes et al. on the other hand study the problem from an algorithmic per-

The ICM can be adapted to handle adversarial scenario by allowing the nodes to be in one of the following three states - (i) active by adopting innovation A, (ii) active by adopting innovation B , and (iii) inactive. We use the notation IA and IB to indicate the initial adopters (seed nodes) of technologies A and B respectively. The nodes in the set V - (IA  IB ) are the nodes that are inactive initially. The sets IA and IB are disjoint, i.e., IA  IB = . Just as in ICM, an active node v may influence each one of its inactive neighbors w with probability pv,w . However, in an adversarial scenario, an inactive node w, may be in a situation where one of its active neighbor v attempts to influence w with innovation A, whereas another active neighbor u attempts to influence w with innovation B . In order to deal with this situation, the authors in [3] proposed two new models - (i) Distance-based Model, and (ii) Wave-propagation Model. The models specify the probability with which the node w will be influenced, when its active neighbors attempt to influence w with two competing technologies. The GICM operates on a random subgraph of the social network graph G = (V, E ), where each edge is included independently with probability pv,w . The details of these two models are described in the following two subsections.

3.3

Distance-based Model

Suppose that du (I, Ea ) denotes the shortest path distance from the node u to the node set I where I = IA IB along the active edges in the edge set Ea . If u is not connected to any node of I using only the active edges Ea , then du (I, Ea ) =

. Let u (IA , du (I, Ea )) and u (IB , du (I, Ea )) be the number of nodes in IA and IB respectively, at distance du (I, Ea ) from u along edges in Ea . The probability that node u adopts innovation i  {A, B } when maximum number of propagation steps is D is denoted by Pi (u|IA , IB , Ea , D) and is computed in the following way: if du (I, Ea )  D, u (Ii ,du (I,Ea )) ; Pi (u|IA , IB , Ea , D) = u (IA ,du ( I,Ea ))+u (IB ,du (I,Ea )) otherwise, it is zero. In this model the expected number of nodes which adopt i  {A, B } will be computed in the following way: j (IA , IB , D) = E
uV

L1
...

L2
...

L3
...

Ln
...

e1

e2

e3

...

en xn ... x2 ... a x1

y1 y2

... s1 s2 s3 sm

ynr

Figure 1: Graph G = (V, E ) of WMI instance in set cover reduction

Pi (u|IA , IB , Ea , D)

5.1

Distance-based Model

where j = 1 if i = A; else j = 2 and the expectation is over the set of active edges.

Decision version of WMI: Is there a set IB where |IB |  M and 2 (IA , IB , D) > 1 (IA , IB , D)? Theorem 1. WMI is NP-hard for the distance-based model. Proof: In order to prove that WMI is NP-hard when diffusion is based on distance based model, we reduce the NPcompete Set Cover problem to W M I . The decision version of the Set Cover problem is defined in the following way: A ground set of elements S = {e1 , e2 , . . . , en }, a collection of sets C = {s1 , s2 , . . . , sm } such that si  S and a positive integer K  |C | are given. The question is whether there exists a collection Q  C that covers all the elements in S and |Q|  K . Given an instance of set cover problem we construct an instance of W M I . We compute G = (V, E ) in the following way. For every element ei  S we add a node ei and for every set sj  C we add a node sj to V . We add an edge (ei , sj ) to E for every ei and sj if ei  sj . Also, we add a node a and nodes x1 , . . . , xn to V . Then, for every ei we add edges (a, xi ) and (xi , ei ) to E . Moreover, for every ei we add a set of r nodes, Li = {li,j |1  j  r} to V and we connect them directly to ei . We identify the value of r later in the proof. Finally, we add n × r additional nodes, y1 , . . . , yn×r , to V and edges (yt , a), 1  t  n × r (Fig. 1). We consider that all edges are active; i.e., pu,v = 1 for all edges in E . We assign D = 4 equal to the diameter of the graph G, M = K and IA = {a}. Now, we show that the set cover problem has a solution if and only if there is a set IB  V - IA such that |IB |  M and 2 (IA , IB , D) > 1 (IA , IB , D). First we consider that there is a collection Q  C that covers S and |Q|  K . Then IB includes all nodes sj corresponding to the sets in Q. In this case, all ei will be at distance one from IB and two from IA . So, all ei and the nodes in Li will adopt IB with probability one. Moreover, the nodes sj  / IB are two hops away from IB while 3 hops away from IA . Hence, all nodes sj will adopt IB . Therefore, we have 2 (IA , IB , D) = m + n(1 + r); so, 2 (IA , IB , D) > 1 (IA , IB , D). Next, we show that if there is no collection Q of size K that covers all elements then there is no set IB  V - IA of size M where 2 (IA , IB , D) > 1 (IA , IB , D). Considering that set cover does not have a solution, there should be at least one ei whose distance from IB cannot be one; so, there is an ei and consequently nodes in Li that choose A and the probability that with the probability at least K1 +1 they choose B is at most KK . Also, at most K nodes from +1 x1 , . . . , xn can be at distance less than or equal to 1 from IB . Hence n - K of them will adopt A with probability one. Therefore, we have

3.4

Wave-propagation Model

In this model, in step d < D all nodes that are at distance d - 1 from some node in I have adopted technology A or B and all nodes that are farther than d - 1 from I have not adopted any technology yet(where the distance is measured with respect to active edges). Every node at distance d from I chooses one of its neighbors at distance d - 1 from I independently at random and adopt the same technology as its neighbor. For every node u, S denotes the set of neighbors of u that are closer to I than u; i.e., their distance from I is du (I, Ea ) - 1. In this model Pi (u|IA , IB , Ea , D), the probability that node u adopts innovation i  {A, B } in at most D steps, is computed as follows: If du (I, Ea )  D, P (v |I ,IB ,Ea ,D ) ; Pi (u|IA , IB , Ea , D) = vS i |SA | otherwise, it is zero. In this model the expected number of nodes which adopt i  {A, B } will be computed in the following way: j (IA , IB , D) = E
uV

Pi (u|IA , IB , Ea , D)

where j = 1 if i = A; else j = 2 and the expectation is over the set of active edges.

4.

PROBLEM STATEMENT

The WMI problem can be stated informally as follows: Given a diffusion model and the information that a subset of network nodes IA have already adopted innovation A marketed by player P1 , what is the fewest number of nodes should player P2 (marketing innovation B ) target so that by the end of D time steps, the number of nodes that adopt innovation B will exceed the number of nodes that adopt innovation A? If 1 (IA , IB , D) and 2 (IA , IB , D) denote the expected number of nodes that adopt innovations A and B respectively within D time steps, the objective of the WMI problem is to minimize | IB | subject to 2 (IA , IB , D) > 1 (IA , IB , D)

5.

COMPUTATIONAL COMPLEXITY

In this section, we prove that W M I problem is NP-hard for both propagation models.

2 (IA , IB , D)  m + (n - 1)(1 + r) + KK (r + 1) + K and +1 1 1 (IA , IB , D)  1 + nr + n - K + K +1 (r + 1). We choose r in our instance large enough such that r > Then we have 1 + nr + n - K + K1 (r + 1) > m + (n - 1)(1 + +1 r) + KK ( r + 1) + K ; so  ( I , I , D) < 1 (IA , IB , D). 2 A B +1
(m+2K -2)(K +1)+K -1 . 2

Algorithm 1 GWMI
Input: G = (V, E ), IA , D Output: IB 1: while  (IA , IB , D)  0 do 2: for every node i  V - (IA  IB ) do 3: Compute Fi 4: end for 5: Select node j with maximum Fj 6: IB = IB  {j } 7: end while 8: return IB

5.2

Wave Propagation Model

Theorem 2. WMI is NP-hard for the wave propagation model. Proof: Similar to Theorem 1, we reduce decision version of Set Cover problem to decision version of W M I when wave propagation model is used for diffusion. We construct an instance of W M I in the same way as in Theorem 1. The only change that should be made to this instance is the value of r which will be computed later. We need to show that the set cover problem has a solution if and only if there is a set IB  V - IA such that |IB |  M and 2 (IA , IB , D) > 1 (IA , IB , D). First we consider that there is a collection Q  C that covers S and |Q|  K . Then IB includes all nodes sj corresponding to the sets in Q. Similar to the proof of Theorem 1 we have 2 (IA , IB , D) = m + n(1 + r); so, 2 (IA , IB , D) > 1 (IA , IB , D). Next, we show that if there is no collection Q of size K that covers all elements then there is no set IB  V - IA of size M where 2 (IA , IB , D) > 1 (IA , IB , D). Considering the construction of G and the fact that set cover does not have a solution , there should be at least one ei whose distance from IB cannot be one or smaller. Since the node xi connected to this ei will have probability 1 to accept A and the maximum number of nodes in first hop neighborhood of ei that are at distance one from IA  IB is m + 1, there is an ei and consequently nodes in Li that choose A and the probability that with the probability at least m1 +1 m they choose B is at most m . Also, at most K nodes from +1 x1 , . . . , xn or y1 , . . . , yn×r can be at distance less than or equal to 1 from IB . Hence n(r + 1) - K of them will adopt A with probability one. Therefore, we have m (r + 1) + K and 2 (IA , IB , D)  m + (n - 1)(1 + r) + m +1 1 1 (IA , IB , D)  1+ n(r +1) - K + m+1 (r +1). We choose r in
3 our instance large enough such that r > m + K (m + 1) - 2 . 2 1 Then we have 1 + n(r + 1) - K + m+1 (r + 1) > m + (n - m 1)(1+ r)+ m (r +1)+ K ; so 2 (IA , IB , D) < 1 (IA , IB , D). +1
2

In [11], it is mentioned that computing the exact value of 1 (IA , , D) efficiently is an open question. Similarly, there is no known way to compute 1 (IA , IB , D), 2 (IA , IB , D) in both propagation models efficiently. However, by sampling the active sets we can get a close approximation with high probability. Given IA , IB and a set of active edges Ea , computation of 1 and 2 in both propagation models has O(n3 ) time complexity since it needs computation of single all-pairs shortest paths. Given IA , IB and input graph G, using sampling, we can then approximate 1 and 2 to within (1+  ) for any  > 0 where the running time depends on 1/ [3].

6.1

Upper Bound Computation

Theorem 3. GWMI has a log n approximation ratio. t be the set of B 's initial adopters selected by Proof. Let IB 0 , D) = GW M I at step t. Initially, IB is empty and  (IA , IB -1 (IA , , D). In every iteration t, the nodes in the optiopt t-1  mal set of B 's initial adopters, IB , will make  (IA , IB opt opt IB , D) positive. We denote the size of IB by OP T and the size of the solution of GW M I by H . Therefore, There t-1 will be at least one node in V - {IA  IB } that increases
t-1 , D) at least by  (IA , IB
t-1 | (IA ,IB ,d)| . OP T

Let, vt be the node
t-1 | (IA ,IB ,D )| . OP T

selected by GW M I at iteration t. Then, Fvt  Therefore, for t < H we have
t t-1 , D )| - | (IA , IB , D)|  | (IA , IB

t-1 | (IA , IB , D)| OP T

0  | (IA , IB , D)|(1 -

1 t ) OP T

6.

APPROXIMATION ALGORITHM

Since we proved that finding the optimal solution for W M I is hard, in this section we propose a greedy algorithm called GW M I . In this algorithm either of the two propagation models discussed before can be used as the diffusion process. Let  (IA , IB , D) be (2 (IA , IB , D) - 1 (IA , IB , D)). We define Fi to denote the amount of increase in the value of  when node i is added to IB ; i.e., Fi =  (IA , IB  {i}, D) -  (IA , IB , D). Initially IB is empty. Hence,  (IA , IB , D)  0. The algorithm executes through iterations and in each iteration node i  V - IA with the maximum Fi is selected. The steps of the algorithm GW M I has been shown in Algorithm 1.

0 , D)| = 1 (IA , , D)  n. Hence Also, we know that | (IA , IB we have -t 1 t t | (IA , IB , D)|  n(1 - )  ne OP T . OP T Since adding a node to IB will increase  (IA , IB , D) at least t by one, we need to find the smallest t that | (IA , IB , D)| < 1. Then adding at most one more node will make  (IA , IB , D) positive. Therefore, H  1 + OP T ln n. We note that this proof holds for both propagation models.

6.2

Lower Bound Computation

We now give a construction giving the lower bound for GWMI when distance-based propagation model is used. Let vertices and G(n, 3/4) be the X and Y be disjoint sets of n 2 Erd os-Renyi random graph on X  Y with p = 3/4. We take two new vertices u and v , connect u to all vertices of X and v to all vertices of Y . Now, we add a disjoint star S with n + 2 leaves and connect the center of the star to u and v . This yields our graph G (Fig. 2).

larly v ) is chosen, then increase is at most
G(n, 3/4) X n/2 u Y n/2 v Red set

1+

1 1 |X | + |Y | (1) k+1 (k + 1)(k + 2) 1 n 1 n = (1/4)k + (1/4)k + O(n3/4 ). k+1 2 (k + 1)(k + 2) 2

Figure 2: Construction of G.

We consider that the center of the star is the only initial adopter of A (red node), and pu,v is uniform and it is 1 for all the edges of G and D = 3. An optimal set of initial adopters of B (initial blue nodes) includes u, v and any of the leaves of S . We claim that the greedy algorithm GW M I will select (log n) vertices with high probability, assuming n is large enough. In order to prove this we first state a technical lemma giving a condition that G satisfies with high probability. Let S  X  Y . We say S is fair if 1. |X \  (S )| = (1/4)|S | n + O(n3/4 ) and |Y \  (S )| = 2 |S | n 3/4 (1/4) 2 + O(n ). where  (S ) is the set of one hop neighbors of vertices in S. We claim the following lemma, whose proof we defer: Lemma 4. With probability 1 - o(1) every set S  X  Y 1 ln(n) is fair. Furthermore, the induced graph with |S | < 100 on X  Y has diameter 2, every vertex in Y is at distance at most 2 from u and every vertex in X is at distance at most 2 from v . Assuming Lemma 4 we prove the lower bound. In particular we prove the following: The greedy algorithm selects at 1 least 100 ln n vertices from X  Y . We proceed by induction. At the first step, the greedy algorithm has to choose between a vertex in X  Y , one of u or v , or one of the vertices in the star. Selecting a vertex in the star will cause the number of blue vertices to increase by one and red vertices to decrease, a net change of two. Selecting u (or resp. v ) will increase blue (and decrease red) by a total of 1 + n +n ; since every 2 4 vertex in X will be at distance 1 from a blue vertex and every vertex in Y will be at distance 2 from both u and the red vertex if u is selected. On the other hand, by fairness, if a vertex x in X  Y is selected; the increase in blue is at least n 3n +n + O(n3/4 ); since 34 + O(n3/4 ) vertices are at distance 4 8 n 1 from x and the other 4 + O(n3/4 ) are at distance 2 from both x and the red vertex. Therefore the greedy algorithm will select from X  Y at the first time. Now suppose that the greedy algorithm has selected from 1 X Y a total of k < 100 ln n times. Let B denote the selected set, and X = X \  (B ) and Y = Y \  (B ). Every vertex in X  Y is at distance two from all k blue vertices, and hence k they are currently blue with probability k+1 . Furthermore by fairness X and Y are both of size (1/4)k n + O(n3/4 ). 2 Again, the greedy algorithm must choose: If u (or simi-

On the other hand, if a vertex x in X  Y is chosen, the increase is at least 1 1 | (x)  X | + | (x)  Y | (2) k+1 k+1 1 |X  Y \  (X )| + (k + 1)(k + 2) 1 3 n 1 n =2· · (1/4)k + (1/4)k+1 + O(n3/4 ); k+1 4 2 (k + 1)(k + 2) 2 therefore, (2) - (1) is positive and hence the vertex in X  Y will be chosen as desired. We note that this construction is for sufficiently large n and (1/4)k n >> n3/4 .
1 Proof of Lemma 4. Let S  X Y , with |S | < 100 ln n. Then n E[|X \ S |] = (1/4)|S | (|X | - |X  S |) = (1/4)|S | + O(ln n). 2

Let XS = |X \ S |. Chernoff bounds imply that P(|XS -E[XS ]| > n3/4 )  exp(-( n3/2 ))  exp(-(n1/2 )). E[XS ]

Bounds for |Y \ S | follow similarly. On the other hand there are at most
1 100

ln n

i=1

n i



1 ln(n) · nln n , 100

sets S . Thus union bounds imply every set is fair with probability 1 - exp(-(n1/2 )). Note that the expected number of common neighbors ben tween x and y in X  Y is 9 , and Chernoff bounds plus 16 union bounds imply every pair x and y is of distance 2 (and n in fact has (1 - o(1)) 9 common neighbors). Likewise, u 16 n expected neighbors and Cherand a vertex in Y have 38 n noff bounds imply that every pair has (1 + o(1)) 38 common neighbors. Likewise, for v and vertices in X . A union bound over all events completes the proof.

7.

SIMULATION

In this section we evaluate the performance of our approximation algorithm, GW M I , on a real network data set. It has been suggested in [15] that the co-authorship graphs are representative of typical social networks. As such, we use the real collaboration network data set of the scientists posting preprints on the high-energy theory archive at www.arxiv.org, 1995-1999 [14]. This network has 8361 nodes (authors) and 15751 edges. The largest connected component has 5835 number of nodes (authors) and maximum distance between the nodes in a connected component is 19. Our experiments were conducted on a high performance computer which is a 5K processor Dell Linux Cluster. The program is parallelized with OpenMP, optimized with Intel compiler and was executed on an 8 core compute node. The cores in the node have equal access to a common pool of shared memory. Each node is comprised of 2.66/2.83 GHz

processors, 8MB cache, 16GB memory and 8 cores. Since our experiments required execution of the algorithm on a large number of instantiation of a social network (the graphs were different as their set of active edges were different), we used OpenMP for parallelization of the graph instances for the simulation with one data set. In the first set of experiments we evaluate the performance of GWMI algorithm against the results obtained from the heuristics based on node degree and closeness centrality. These heuristics are most often used in social networks to identify most influential nodes [11]. We also compare performance of GWMI with the greedy algorithm proposed in [3] for selection of seed nodes for the second player P2 . In our model the first player P1 is trying to market product A and the second player P2 is trying to market product B . Since WMI problem is NP-hard and the input data set is large, computation of the optimal solution within a reasonable amount of time is unlikely. It may be noted that there is no known way of computing the exact value of 1 (IA , IB , D) and 2 (IA , IB , D) efficiently [11]. Accordingly, we use sampling of the active edge sets to obtain close approximation of 1 (IA , IB , D), 2 (IA , IB , D) with high probability. As in the experiments reported in papers [11, 3], we assign the edge probabilities to be 0.1. In all the experiments we use WPM as the diffusion model. The node degree based heuristic selects the nodes in the decreasing order of their degrees and the closeness centrality based heuristic selects the nodes in the increasing order of their average distance to other nodes. The distance between two nodes that are not in the same connected component is taken to be n, where n is number of nodes in the network. In the greedy algorithm proposed in [3], in every iteration the node that increases 2 (IA , IB , D) the most is selected. We refer to this algorithm as Second Player Influence Maximization (SPIM) algorithm. In these experiments, maximum number of propagation steps is taken to be 10, i.e., D = 10. In the experiments, the player P1 used node degree based heuristic to select its k initial adopters. In our experiments, the size of initial adopters of A is varied from 20 to 100. The results of this set of experiments using the WPM is shown in Fig. 3. The Fig. 3 shows that all five sizes of the initial adopters of A (20, 40, 60, 80, 100), the GWMI algorithm required the fewest number of initial adopters of B necessary to defeat A's influence at the end of time step 10. The legend Degree-Degree in Fig. 3 denotes that both the players are using the node degree based heuristics to select the initial adopters. Similarly,the legend Degree-GWMI denotes that while P1 is using the node degree based heuristics to select the initial adopters, P2 is using the GWMI algorithm to do the same. The Figs. 4 and 5 show the coverage (i.e., the number of nodes influenced at the end of 10 time steps) for players P1 and P2 respectively. Although the GWMI algorithm does not make an effort to minimize the coverage of P1 , it may be observed from the Fig. 4, the coverage of P1 is less if P2 uses GWMI instead of SPIM. Thus P2 is better off using GWMI instead of SPIM, if in addition to be able to defeat P1 with least investment (i.e., initial adopters), P2 wants to have a smaller market share for P1 . The Fig. 5 shows the coverage of P2 at the end of ten time steps. It may be observed from the Fig. 5, that at all five data points the coverage for P2 is highest when she uses the SPIM algorithm. This is not surprising as the stated goal of SPIM is to maxi-

200 Number of Initial Adopters of B 160

120
80 40

Degree-Closeness Degree-Degree Degree-SPIM Degree-GWMI
0 20 40 60 80 100 Number of Initial Adopters of A 120

0

Figure 3: Number of initial adopters of B for different values of |IA |

350 300 Coverage of A 250 200 150 100 20 40 60 Number of Initial Adopters of A 80 Degree-Degree Degree-Closeness Degree-GWMI Degree-SPIM

Figure 4: Expected number of nodes adopting A after 10 propagation steps

mize P2 's coverage (influence). However, this figure may be somewhat misleading because it does not provide the information pertaining to the number of initial adopters required by the SPIM algorithm to achieve the higher coverage. By its stated objective, the number of initial adopters required by GWMI to defeat P1 cannot be higher than the the number of initial adopters required by SPIM. Once this is factored in, and we compute the coverage per initial adopter, we find that the coverage per initial adopter of the SPIM algorithm is very close to that of the GWMI algorithm. This is shown in Fig. 6. From Fig. 3 it is clear that the node degree and centrality based heuristics and the SPIM algorithm require a larger number of initial adopters of B to beat A than is needed by the GW M I algorithm. While this is a negative aspect of SPIM (cost), it also has a positive aspect in the sense that at the end of ten time steps, it also secures a larger coverage for B (benefit). We compute the additional benefit provided by the additional initial adopters. Let IB (X ) be the smallest set of initial adopters of B that is required by algorithm X to defeat A and 2(X ) be the expected number of nodes that adopt B after D propagation steps. Here X can be node-degree or centrality based heuristic or the SPIM algorithm. In the case, (2(X ) - 2(GW M I ) ) indicates the additional benefit and (|IB (X ) | - |IB (GW M I ) |) indicates the additional cost. In this case, (2(X ) - 2(GW M I ) )/(|IB (X ) | - |IB (GW M I ) |) indicates the average market share gain of B with each additional initial adopter when using algorithm

350 300 Coverage of B 250
Extended Benefit of B per Additional Initial Adopter

0.4

Degree-Degree Degree-Closeness Degree-GWMI Degree-SPIM

0.3 0.2

Degree-Degree Degree-Closeness Degree-SPIM

200
150 100 20 40 60 Number of Initial Adopters of A 80

0.1 0 20 40 60 Number of Initial Adopters of A 80

Figure 5: Expected number of nodes adopting B after 10 propagation steps
9 8 7 6 5 4 3 Degree-Degree Degree-Closeness Degree-GWMI Degree-SPIM

Figure 8: Extended benefit that B can capture per additional initial adopter with respect to GW M I but also (1(GW M I ) - 1(X ) ). It introduces a notion of extended benefit by combining these two factors in the following way: (2(X ) - 2(GW M I ) ) - (1(GW M I ) - 1(X ) ). With this notion of extended benefit, ((2(X ) - 2(GW M I ) ) + (1(GW M I ) - 1(X ) )) |IB (X ) | - |IB (GW M I ) | indicates the average market share gain of B with each additional initial adopter when using algorithm X . The Fig. 8 depicts the results for the heuristics and SPIM. It may be observed from Fig. 7 that when extended benefit is considered, the average market share gain of B with each additional initial adopter diminishes even more drastically with increase of the number of initial adopters of A, when it uses the SPIM algorithm. Moreover, the gain of each additional initial adopter is smaller than 1 and implies that the additional adopter is not worth its cost. In the second set of experiments we investigate different strategies for selection of initial adopters of A when P2 uses GW M I . The strategies that we consider for selection of initial adopters of A includes the greedy algorithm proposed in [11] and heuristics based on node degree and closeness centrality. In these experiments WPM is used as diffusion model and D = 10. Fig. 9 depicts the results of these experiments. We observe that the closeness-centrality based heuristic performs poorly in comparison to other two algorithms. This is true because the number of initial adopters of B that it needs to defeat A's overall influence (coverage) is much smaller than the size of initial adopters of A. More specifically, for closeness-centrality based heuristic, for |IA | values greater than 60, the number of initial adopters of B is less than 50% of |IA |. This set of results show that if the influence maximization algorithm (IM) proposed in [11] is used for the selection of IA , it forces P2 to select a large set for IB in order to be able to defeat P1 within D time steps.

Coverage of B per Initial Adopter of B

2 1 0
20 40 60 Number of Initial Adopters of A 80

Figure 6: Expected number of nodes adopting B per initial adopter of B after 10 propagation steps

X . The Fig. 7 depicts the results for the heuristics and SPIM. The negative gains are not shown. It may be observed from Fig. 7 that the average market share gain of B with each additional initial adopter diminishes with increase of the number of initial adopters of A, when it uses the SPIM algorithm. While the stated objective of P2 is to have a larger market share than P1 with the fewest number of initial adopters, it may also have two other unstated objectives - (i) to have a large 2(X ) and (ii) a small 1(X ) for all X (1(X ) be the expected number of nodes that adopt A after D time steps). Therefore while considering the benefit of the additional initial adopters, we can consider not only (2(X ) - 2(GW M I ) )

Average Increase in Market Share of B per additional initial adopter

12
Degree-Degree 10 8 Degree-Closeness Degree-SPIM

6
4

8.

CONCLUSION

2
0 20 40 60 Number of Initial Adopters of A 80

Figure 7: Average market share increase that innovation B can capture per additional initial adopter with respect to GW M I

In this paper we have introduced a new influence propagation problem in an adversarial setting where the goal of the second player is to defeat the first within D time steps and least cost, measured in terms of the number of seed nodes. Considering two different influence propagation models, we provided the NP-Hardness proof for the problem and an approximation algorithm with a tight performance bound. In addition, we evaluated the performance of the approximation algorithm with collaboration network data.

120
Number of Initial Adopters of B 100 80 60 40 20 0 0

Degree-GWMI IM-GWMI Closeness-GWMI

20

40 60 80 Number of Initial adopters of A

100

120

Figure 9: Size of initial adopters of B for different values of |IA | We can envisage at least two new directions of research with this problem. In the first direction, P2 is not aware of P1 's choice. In the second direction, back and forth transition of the nodes between two competing products is allowed.

9.

ACKNOWLEDGMENTS

The research was supported in part by a grant to the Center for the Study of Religion and Conflict at Arizona State University (N00014-09-1-0815). The award was funded through the Office of the Secretary of Defense Minerva program, and managed out of the Office of Naval Research. The content is solely the responsibility of the authors and does not necessarily represent the views of the Office of Naval Research. In addition, it was also supported in part by the DTRA grant HDTRA1-09-1-0032 and the AFOSR grant FA955009-1-0120.

10.

REFERENCES

[1] S. Bhagat, A. Goyal, and L. V. Lakshmanan. Maximizing product adoption in social networks. In Proceedings of the fifth ACM international conference on Web search and data mining, WSDM '12, pages 603­612, 2012. [2] S. Bharathi, D. Kempe, and M. Salek. Competitive influence maximization in social networks. In Proceedings of the 3rd international conference on Internet and network economics, WINE'07, pages 306­311, 2007. [3] T. Carnes, C. Nagarajan, S. M. Wild, and A. van Zuylen. Maximizing influence in a competitive social network: a follower's perspective. In Proceedings of the ninth international conference on Electronic commerce, ICEC '07, pages 351­360, 2007. [4] W. Chen, Y. Wang, and S. Yang. Efficient influence maximization in social networks. In Proceedings of the 15th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining, KDD '09, pages 199­208, 2009. [5] K. Dave, R. Bhatt, and V. Varma. Modelling action cascades in social networks. 2011.

[6] P. Domingos and M. Richardson. Mining the network value of customers. In Proceedings of the seventh ACM SIGKDD international conference on Knowledge discovery and data mining, KDD '01, pages 57­66, 2001. [7] A. Goyal, F. Bonchi, L. V. S. Lakshmanan, and S. Venkatasubramanian. Approximation analysis of influence spread in social networks. arXiv:1008.2005v4, 2011. [8] H. Habiba, Y. Yu, T. Y. Berger-Wolf, and J. Saia. Finding spread blockers in dynamic networks. In Proceedings of the Second international conference on Advances in social network mining and analysis, SNAKDD'08, pages 55­76, 2010. [9] G. Istrate, M. V. Marathe, and S. S. Ravi. Adversarial models in evolutionary game dynamics. In Proceedings of the twelfth annual ACM-SIAM symposium on Discrete algorithms, SODA '01, pages 719­720, 2001. [10] Q. Jiang, G. Song, C. Gao, Y. Wang, W. Si, and K. Xie. Simulated annealing based influence maximization in social networks. 2011. [11] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing the spread of influence through a social network. In Proceedings of the ninth ACM SIGKDD international conference on Knowledge discovery and data mining, KDD '03, pages 137­146, 2003. [12] H. Li, S. S. Bhowmick, and A. Sun. Casino: towards conformity-aware social influence analysis in online social networks. In Proceedings of the 20th ACM international conference on Information and knowledge management, CIKM '11, pages 1007­1012, 2011. [13] M. Mathioudakis, F. Bonchi, C. Castillo, A. Gionis, and A. Ukkonen. Sparsification of influence networks. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD '11, pages 529­537, 2011. [14] M. Newman. http://networkdata.ics.uci.edu/data/hep-th/. [15] M. E. J. Newman. The structure of scientific collaboration networks. Proceedings of the National Academy of Sciences of the United States of America, 98(2):404­409, 2001. [16] M. A. Nowak, C. E. Tarnita, and T. Antal. Evolutionary dynamics in structured populations. Philosophical Transactions of the Royal Society B: Biological Sciences, 365(1537):19­30, 2010. [17] Y. Wang, G. Cong, G. Song, and K. Xie. Community-based greedy algorithm for mining top-k influential nodes in mobile social networks. In Proceedings of the 16th ACM SIGKDD international conference on Knowledge discovery and data mining, KDD '10, pages 1039­1048, 2010. [18] S. Wasserman and K. Faust. Social Network Analysis: Methods and Applications. Number 8 in Structural analysis in the social sciences. Cambridge University Press, 1 edition, 1994.

On the Entity Hardening Problem in Multi-layered Interdependent Networks
Joydeep Banerjee, Arun Das, Chenyang Zhou, Anisha Mazumder and Arunabha Sen
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering Arizona State University Tempe, Arizona 85287 Email: {joydeep.banerjee, arun.das, czhou24, anisha.mazumder, asen}@asu.edu Abstract--The power grid and the communication network are highly interdependent on each other for their well being. In recent times the research community has shown significant interest in modeling such interdependent networks and studying the impact of failures on these networks. Although a number of models have been proposed, many of them are simplistic in nature and fail to capture the complex interdependencies that exist between the entities of these networks. To overcome the limitations, recently an Implicative Interdependency Model that utilizes Boolean Logic, was proposed and a number of problems were studied. In this paper we study the "entity hardening" problem, where by "entity hardening" we imply the ability of the network operator to ensure that an adversary (be it Nature or human) cannot take a network entity from operative to inoperative state. Given that the network operator with a limited budget can only harden k entities, the goal of the entity hardening problem is to identify the set of k entities whose hardening will ensure maximum benefit for the operator, i.e. maximally reduce the ability of the adversary to degrade the network. We show that the problem is solvable in polynomial time for some cases, whereas for others it is NP-complete. We provide the optimal solution using ILP, and propose a heuristic approach to solve the problem. We evaluate the efficacy of our heuristic using power and communication network data of Maricopa County, Arizona. The experiments show that our heuristic almost always produces near optimal results.

arXiv:1412.6686v1 [cs.NI] 20 Dec 2014

that may exist between network entities, such as when entity ai is operational, if entities (i) bj and bk and bl are operational, or (ii) bm and bn are operational, or (iii) bp is operational. Graph based interdependency models proposed in the literature such as [3], [4], [5], [10], [6], [7] including [1], [2] cannot capture such complex interdependency involving both conjunctive and disjunctive terms between entities of multi-layer networks. To overcome these limitations, an Implicative Interdependency Model that utilizes Boolean Logic, was recently proposed in [9], and a number of problems including computation of K most vulnerable nodes [9], root cause of failure analysis [11], and progressive recovery from failures [12], were studied using this model. In this paper we study the "entity hardening" problem in the interdependent power-communication network using the Implicative Interdependency Model (IIM). By "entity hardening", we imply the ability of the network operator to ensure that an adversary (be it Nature or human), cannot take a network entity from an operative (operational) to an inoperative (failed) state. We assume that the adversary is clever and is capable of identifying the most vulnerable entities in the network that causes maximum damage to the interdependent system. However, the adversary does not have an unlimited budget and has the resources to destroy at most K entities of the interdependent network. The network operator is also aware of adversary's target entities for destruction. Since we assume that once an entity is "hardened" by the network operator it cannot be destroyed by the adversary, if all K targets of the adversary are hardened by the network operator, then the adversary cannot induce any failure in the network. However, if due to resource limitations the network operator is able to strengthen only k entities, where k < K, these k entities have to be carefully chosen. The goal of the entity hardening problem is to identify the set of k entities whose hardening will ensure maximum benefit for the operator, i.e. maximally reduce the ability of the adversary to degrade the network. We classify the entity hardening problem into four different cases depending on the nature of the interdependency relationships. We show that the first case can be solved in polynomial time, and all other cases are shown to be NP-complete. We provide an inapproximability result for the second case, an approximation algorithm for the third case, and a heuristic for the fourth (general) case. We evaluate the efficacy of our heuristic using power and communication network data of Maricopa County, Arizona. The experiments show that our

I.

I NTRODUCTION

The critical infrastructures of a nation form a complex symbiotic ecosystem where individual infrastructures are heavily interdependent on each other for being fully functional. Two such critical systems that rely heavily on each other for their well being are the power and communication network infrastructures. For instance, power grid entities such as SCADA systems, that are used to remotely operate power generation units, receive their control commands over the communication network infrastructure, while communication network entities such as routers and base stations are inoperable without electric power. Thus, failure introduced in the system either by Nature (hurricanes), or man (terrorist attacks), can trigger further failures in the system due to interdependencies between the entities of the two infrastructures. Although a number of models have been proposed for modeling and analysis of interdependent multi-layered networks [1], [2], [3], [4], [5], [6], [7], [8], many of these models are simplistic in nature and fail to capture the complex interdependencies that exists between the entities of these networks. As noted in [9], these models fail to model complex interdependencies

2

heuristic almost always produces near optimal results. The paper is organized as follows, the IIM model is presented in Section II, in Sections III and IV we formally state the entity hardening problem and analyze its computational complexity, Section V outlines the optimal and heuristic solutions to the problem, Section VI shows the experimental results, and finally Section VII concludes this paper. II. I NTERDEPENDENCY M ODEL

III.

P ROBLEM F ORMULATION

We now present an overview of the underlying IIM interdependency model [9]. IIM uses Boolean Logic to model the interdependencies between network entities, these interdependent relationships are termed as Implicative Interdependency Relations (IDRs). We represent this interdependent network setting as I (A, B, F (A, B )), where sets A and B are the power and communication network entities respectively, and F (A, B ) is the set of dependency relations, or IDRs. Table I represents a sample interdependent network I (A, B, F (A, B )), where A = {a1 , a2 , a3 , a4 }, B = {b1 , b2 , b3 } and F (A, B ) is the set of IDRs (dependency relations) between the entities of A and B . In this example, the IDR b1  a1 a3 + a2 implies that entity b1 is operational when both the entities a1 and a3 are operational, or entity a2 is operational. The conjunction of entities, such as a1 a3 , is also referred to as a minterm.
Power Network a1  b1 b2 a2  b1 + b2 a3  b1 + b2 + b3 a4  b1 + b3 Comm. Network b1  a1 a3 + a2 b2  a1 a2 a3 b3  a1 + a2 + a3 --

Before we make a formal statement of the entity hardening problem in the IIM setting, we explain it with the help of an example. Consider an interdependent system as outlined in the IDR set shown in Table I. It may be easily checked that when the adversary budget is K= 2, the most vulnerable entities of this system are {a2 , b3 }. If the network operator doesn't harden any one of the entities a2 or b3 , then in this example all the network entities eventually fail, as seen from the fault propagation in Table II. When the network operator chooses to harden both a2 and b3 then none of the entities in the network fail if the adversary restricts the attack only to the two most vulnerable entities of the network, which in this example happens to be {a2 , b3 }. If the network operator has resources to harden only one entity and the operator chooses to harden a2 , the destruction of b3 by the adversary will eventually lead to the failure of no other entities of the network, as shown in Table III(a). If on the other hand, the network operator chooses to harden b3 , destruction by the adversary of a2 will eventually lead to the failure of the entities {a2 , b2 , a1 , b1 } as shown in Table III(b). Clearly in this scenario the operator should harden a2 instead of b3 . Definition: Kill Set of a set of Entities(S ): The kill set of a set of entities S , is the set of all entities that will eventually fail due to failure of S and the interdependencies between the entities of the network as given by the set of IDR's. The kill set of a set of entities S is denoted by KillSet(S ). It may be noted that the search for k entities to be hardened is restricted to the KillSet(S ), where S is the set of K most vulnerable entities in the network, because hardening any entity not in KillSet(S ) does not provide any benefit to the network operator. In this study we also assume that the set of K most vulnerable entities in the network is unique.
Entities 0 a1 a2 a3 a4 b1 b2 b3 0  0 0 0 0 1 Time Steps (t) 1 2 3 0  0 0 0 0 1 0  0 0 0 0 1 0  0 0 0 0 1 Entities 4 0  0 0 0 0 0 a1 a2 a3 a4 b1 b2 b3 0 0 1 0 0 0 0  Time Steps (t) 1 2 3 0 1 0 0 0 1  1 1 0 0 0 1  1 1 0 0 1 1  4 1 1 0 0 1 1 

TABLE I: Implicative Interdependency Relations of a sample network

Given a set of inoperable (failed) entities, a time stepped failure cascade can be derived from the dependency relationships outlined in the IDR set. For example, for the interdependent network outlined in Table I, Table II shows the failure propagation when entities {a2 , b3 } fail at the initial time step (t = 0). It may be noted that the model assumes that dependent entities fail immediately in the next time step, for example, when {a2 , b3 } fail at t = 0, b2 fails at t = 1 as b2 is dependent on a2 for its survival. The system reaches a steady state when the failure propagation process stops. In this example, when {a2 , b3 } fail at t = 0, the steady state is reached at time step t = 4.
Entities 0 a1 a2 a3 a4 b1 b2 b3 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 Time Steps (t) 2 3 4 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 5 1 1 1 1 1 1 1 6 1 1 1 1 1 1 1

(a) Entity a2 is hardened

(b) Entity b3 is hardened

TABLE III: Failure cascade propagation with entity hardening. Entities {a2 , b3 } are attacked at time step t = 0. A value of 1 denotes entity failure, 0 otherwise.  denotes a hardened entity.

TABLE II: Failure cascade propagation when entities {a2 , b3 } fail at time step t = 0. A value of 1 denotes entity failure, and 0 otherwise

We now proceed to formulate the entity hardening problem formally. Given an interdependent network system I (A, B, F (A, B )), and the set of K most vulnerable entities of the system A  B  , where A  A and B   B : The Entity Hardening (ENH) problem INSTANCE: Given: (i) An interdependent network system I (A, B, F (A, B )), where the sets A and B represent the entities of the two networks, and F (A, B ) is the set of IDRs. (ii) The set of K most vulnerable entities of the system A  B  , where A  A and B   B (iii) Two positive integers k, k < K and EF .

A primary consideration for using this model is the accurate formulation of the IDRs that is representative of the underlying physical power and communication network infrastructures. This can either be done by careful analysis as done in [8], or by consultation with experts of these infrastructures. We utilize IIM to model the interdependency between the two networks and analyze the entity hardening problem in this setting.

3

QUESTION:Is there a set of entities H = A  B  , A  A, B   B, |H|  k , such that hardening H entities results in no more than EF entities to fail after entities A  B  fail at time step t = 0. We note some of the assumptions for the ENH problem: First, we assume that once an entity is hardened, it is always operational and does not fail at any time step of the observation, even when the entity is part of the K most vulnerable entities. Second, we assume that k < K, as otherwise the selection of K entities for hardening ensures that no entities fail at all. Finally, as noted earlier, we assume that the set of K most vulnerable entities in the network is unique. We now proceed to analyze the computational complexity of the ENH problem. IV. C OMPUTATIONAL C OMPLEXITY A NALYSIS

[9]. So with two entities {xi , xj }  A  B  and Cxi  Cxj = Cxj i.e, Cxj  Cxi , if xi is hardened it prevents the failure of Cxi - Cxj entities (provided that none of the entities in Cxi - Cxj - {xi } are in A  B  ). With this assertion, for an entity xi  A  B  , steps 4-7 of Algorithm 1 finds the actual entities for which failure is prevented by hardening xi . The set D = {Dx1 , Dx2 , ..., DxK } comprises of these set of entities for each hardened entity xi . To prove that Algorithm 1 finds the optimal solution we make the following two assertions: First, consider any two sets Dxi and Dxj . It is implied from step 6 of Algorithm 1 that / A  B  is Dxi  Dxj = . Second, consider an entity xp  hardened. If xp fails when entities in A  B  fails initially then it would belong to some set Dxi . Thus hardening xp results in preventing the failure of entities that is a proper subset of Dxi . Hence the entities to be hardened must belong to A  B  only. Owing to the two assertions it directly follows that with a given budget k , hardening k highest cardinality sets from the set D ensures prevention of failure for the maximum number of entities. B. Case II: Problem Instance with One Minterm of Arbitrary Size The IDRs of Case II have a single minterm of arbitrary p size. This can be represented as xi  j =1 yj , where xi and yj are entities of network A(B ) and B (A) respectively and the size of the minterm is p. The Entity Hardening problem with respect to Case II is NP-complete and is proved in Theorem 2. An inapproximability proof for this case of the problem is given in Theorem 3 Theorem 2. The Entity Hardening problem for Case II is NP Complete Proof: The Entity Hardening problem for case II is proved to be NP complete by giving a reduction from the Densest pSubhypergraph problem [13], a known NP-complete problem. An instance of the Densest p-Subhypergraph problem includes a hypergraph G = (V, E ), a parameter p and a parameter M . The problem asks the question whether there exists a set of vertices |V  |  V and |V  |  p such that the subgraph induced with this set of vertices has at least M hyperedges. From an instance of the Densest p-Subhypergraph problem we create an instance of the ENH problem in the following way. For each vertex vi and each hyperedge ej an entity bi and aj are added to the set B and A respectively. For each hyperedge ej with ej = {vm , vn , vq } (say) an IDR of form aj  bm bn bq is created. It is assumed that the value of K is set of |V |. The values of k and EF are set to p and |V | + |E | - p - M (where |A| = |V | and |B | = |E |) respectively. In the constructed instance only entities of set A are dependent on entities of set B . Additionally the dependency for an entity ai consists of conjunction of entities in set B . Hence for an entity ai  A to fail, either it itself has to fail initially or all entities to which ai is dependent on has to fail. It is to be noted that the entities in set B has no induced failure i.e., there is no cascade. Following from this assertion, with K = p, the solution A =  and B  = B would fail all entities in set A  B . Moreover this is the single unique solution to the problem instance. This is because by including one entity

For an interdependent network I (A, B, F (A, B )) the IDRs can be represented in four different forms. We analyze the computational complexity of the ENH problem for each of these cases separately. A. Case I: Problem Instance with One Minterm of Size One The IDRs of Case I have a single minterm of size 1. This can be represented as xi  yj , where xi and yj are entities of network A(B ) and B (A) respectively. We show that the ENH problem for Case I can be solved optimally in polynomial time. Algorithm 1: Entity Hardening Algorithm for systems with Case I type interdependencies
Data: An interdependent network I (A, B, F (A, B )), set of K most vulnerable entities A  B  , A  A, B   B , hardening budget k and a set H = . Result: Set of hardened entities H. begin For each entity xi  (A  B  ) compute the set of kill sets C = {Cx1 , Cx2 , ..., CxK }, where Cxi = KillSet(xi ) ; Create a copy D = {Dx1 , Dx2 , ..., DxK } of set C ; for (i=1; i  K; i++) do for (j=1, j = i; j  K; j++) do if Cxj  Cxi then Dxi  Dxi \ Dxj ; Choose the top k sets from D with highest cardinality ; For each of the Dxi  D sets chosen in Step 8, H  H  xi ; return H

1 2 3 4 5 6 7

8 9 10

Theorem 1. Algorithm 1 solves the Entity Hardening problem for Case I optimally in polynomial time. Proof: It is shown in [9] that the kill set for all entities in the interdependent network can be computed in O(n3 ) where n = |A| + |B |, thus computing the kill sets for K entities takes O(Kn2 ). Step 4-7 of the algorithm runs in O(K2 ). Choosing the k highest cardinality sets can be found using any standard sorting algorithm in O(Klog (K)). Hence Algorithm 1 runs in O(Kn2 ). For two kill sets Cxi and Cxj it can be shown that either Cxi  Cxj =  or Cxi  Cxj = Cxi or Cxi  Cxj = Cxj

4

ai in the initial failure set would result in not failing at least one entity bj for a given budget K = p. Hence it won't fail the entire set of entities in A  B . If an entity in set A is hardened then it would have no effect in failure prevention of any other entities. Whereas hardening an entity bm  B might result in failure prevention of an entity ai  A with IDR aj  bm bn bq provided that entities bn , bq are also defended. With k = p (and K  |V | = |B |) it can be ensured that entities to be defended are from set B  . To prove the theorem consider that there is a solution to the Densest p-Subhypergraph problem. Then there exist p vertices which induces a subgraph which has at least M hyperedges. Hardening the entities bi  B  for each vertex vi in the solution of the Densest p-Subhypergraph problem would then ensure that at least M entities in set A are protected from failure. This is because the entities in set A for which the failure is prevented corresponds to the hyperedges in the induced subgraph. Thus the number of entities that fail after hardening p entities is at most |V | + |E | - p - M , solving the ENH problem. Now consider that there is a solution to the ENH problem. As previously stated, the entities to be hardened will always be from set B  . So defending p entities from set B  would result in failure prevention of at least M entities in set A such that EF  |V | + |E | - p - M . Hence, the vertex induced subgraph would have at least M hyperedges when vertices corresponding to the entities hardened are included in the solution of the Densest p-Subhypergraph problem, thus solving it. Theorem 3. For an interdependent network I (A, B, F (A, B )) with n = |A  B | and F (A, B ) having IDRs of form Case II, it is hard to approximate the ENH problem within a factor of 1 for some  > 0. log(n)
2

set S we add an entity bi in set B . For all subsets in S , say Sp , Sm , Sn , which has the element xi there is an IDR of form ai  bm + bn + bl . The values of positive integers k and EF are set to M and m - M respectively. It is assumed that the value of K = m. With similar reasoning as that of Case II it can be shown that for K = m the maximum number of node failures (i.e. failure of all entities in A  B ) would occur if A =  and B  = B . This is also the single unique solution to the problem instance. The constructed instance also ensures that the entities to be hardened are from set B  (A not considered as it is equal to ). This is because protecting an entity ai  A would only result in prevention of its own failure whereas protecting an entity bj  B would result in failure prevention of its own and all other entities in set A for which it appears in its IDR. To begin with the proof, consider that there is a solution to the Set Cover problem. Then there exist M subsets (or elements in set S ) whose union results in the set S . Hardening the entities in set B corresponding to the subsets selected would ensure that all entities in set A are prevented from failure. This is because for the dependency of each entity ai  A there exist at least one entity (in set B ) that is hardened. Hence the number of entities that fails after hardening is m-M which is equal to EF , thus solving the ENH problem. Now, consider that there is a solution to the ENH problem. As discussed above the entities to be hardened should be from set B  . To achieve EF = m - M with k = M , no entities in the set A must fail. Hence for each entity ai  A at least one entity in set B that appears in its IDR has to be hardened. Thus, it directly follows that the union of subsets in set S corresponding to the entities hardened is equal to the set S , solving the Set Cover Problem. 1) Approximation Scheme for Case 3: In this subsection we provide an approximation algorithm for Case 3 of the problem. For an interdependent network I (A, B, F (A, B )) with the initial failed set of entities as A  B  we define Protection Set of each entity as follows. Definition: For an entity xi  A  B the Protection Set is defined as the entities that would be prevented from failure by hardening the entity xi when all entities in A  B  fails initially. This is represented as P (xi |A  B  ). The Protection Set of each entity can be computed in O((n + m)2 ) where n and m are the number of entities and number of minterms respectively in an interdependent network I (A, B, F (A, B )) . Theorem 5. For two entities xi , xj  A  B , P (xi |A  B  )  P (xj |A  B  ) = P (xi , xj |A  B  ) when IDRs are of form Case III. Proof: Assume that defending two entities xi and xj would result in preventing failure of P (xi , xj |A  B  ) entities with |P (xi |A  B  )  P (xj |A  B  )| < |P (xi , xj |A  B  )|. Then there exist at least one entity xp  / P (xi |A  B  )  P (xj |A  B  ) such that it's failure is prevented only if xi and xj is protected together. So two entities xm and xn (with xm  P (xi |A B  ) and xn  P (xj |A B  ) or vice versa) have to be

Proof: From Theorem 2, Densest p-Subhypergraph problem has been shown to be a special case of the ENH problem with IDRs of form Case II. Densest p-Subhypergraph problem is proved to be inapproximable within a factor of log1 2 (n) ( > 0) in [13]. Hence the theorem follows. C. Case III: Problem Instance with an Arbitrary Number of Minterm of Size One The IDRs of Case III have arbitrary number of minterm of size 1. This can be represented as xi  p q=1 yq , where xi and yq are entities of network A(B ) and B (A) respectively and the number of minterms are p. The ENH problem with respect to Case III is NP-complete and is proved in Theorem 4. Theorem 4. The ENH problem for Case III is NP Complete Proof: The ENH problem for case III is proved to be NP complete by giving a reduction from the Set Cover Problem, a well known NP-complete problem. An instance of the Set Cover problem includes a set S = {x1 , x2 , ..., xn }, a set S = {S1 , S2 , ..., Sm } where Si  S and a positive integer M . The problem asks the question whether there exists at most M subsets from set S whose union would result in the set S . From an instance of the set cover problem we create an instance of the ENH problem in the following way. For each element xi in set S we add an entity ai in set A. For each subset Si in

5

present in the IDR of xp . As the IDRs are of form Case III so if any one of xm or xn is protected then xp is protected, hence a contradiction. On the other way round P (xi , xj |A  B  ) contains all entities which would be prevented from failure if xi or xj is defended alone. So it directly follows that |P (xi |A  B  )  P (xj |A  B  )| > |P (xi , xj |A  B  )| is not possible. Hence the theorem holds. Theorem 6. There exists an 1 - 1 e approximation algorithm that approximates the ENH problem for Case III. Proof: The approximation algorithm is constructed by modeling the problem as Maximum Coverage problem. An instance of the maximum coverage problem consists of a set S = {x1 , x2 , ..., xn }, a set S = {S1 , S2 , ..., Sm } where Si  S and a positive integer M . The objective of the problem is to find a set S   S and |S  |  M such that Si S Si is maximized. For a given initial failure set A  B  with |A | + |B  |  K, let P (xi |A  B  ) denote the protection set for each entity xi  A  B . We construct a set S = A  B and for each entity xi a set Sxi  S such that Sxi = P (xi |A  B  ). Each set Sxi is added as an element of a set S . The conversion of the problem to Maximum Coverage problem can be done in polynomial time. By Theorem 5 defending a set of entities X  S would result in failure prevention of xi X Sxi entities. Hence, with the constructed sets S and S and a positive integer M (with M = k ) finding the Maximum Coverage would ensure the failure protection of maximum number of entities in A  B . This is same as the ENH problem of Case III. As there exists an 1 - 1 e approximation algorithm for the Maximum Coverage problem hence the theorem holds. D. Case IV: Problem Instance with an Arbitrary Number of Minterms of Arbitrary Size The IDRs of Case IV have arbitrary number of minterm of arbitrary size. This can be represented as xi  qj1 p j2 =1 yj2 , where xi and yj2 are entities of network j1 =1 A(B ) and B (A) respectively and there are p minterms each of size qj1 . Theorem 7. The Entity Hardening problem for Case IV is NP Complete Proof: Case II and Case III are special cases of Case IV. Hence following from Theorem 2 and Theorem 4 the computational complexity of the Entity Hardening problem is NP-complete in Case IV. V. S OLUTIONS TO
THE

It is to be noted that the maximum number cascading steps is upper bounded by |A| + |B | - 1 = m + n - 1. The objective function can now be formulated as follows:
m n

min
i=1

xi(m+n-1) +
j =1

yj (m+n-1)

(1)

The objective in (1) minimizes the number of entities failed after the cascading failure with the respective constraints for the Entity Hardening problem as follows:
n m

Constraint Set 1:
i=1

qxi +
j =1

qyj = k , with qxi , qyj  [0, 1].

If an entity xi (yj ) is defended then qxi = 1 (qyj = 1) and 0 otherwise. Constraint Set 2: xi0  gi - qxi and yi0  hi - qyi . This constraint implies that only if an entity is not defended and gi (hi ) is 1 then the entity will fail at the initial time step. Constraint Set 3: xid  xi(d-1) , d, 1  d  m + n - 1, and yid  yi(d-1) , d, 1  d  m + n - 1, in order to ensure that for an entity which fails in a particular time step would remain in failed state at all subsequent time steps. Constraint Set 4: Modeling of the constraint to capture the cascade propagation for IIM is similar to the constraints established in [9]. A brief presentation of this constraint is provided here. Consider an IDR ai  bj bp bl + bm bn + bq of type Case IV. The following steps are enumerated to depict the cascade propagation: Step 1: Replace all minterms of size greater than one with a variable. In the example provided we have the transformed minterm as ai  c1 + c2 + bq with c1  bj bp bl and c2  bm bn (c1 , c2  {0, 1}) as the new IDRs. Note that after transformation, the original IDR is in the form of Case III and the introduced IDRs are in the form of Case II. Step 2: For each variable c, a constraints is added to capture the cascade propagation. Let N be the number of entities in the minterm on which c is dependent. In the example for the variable c1 with IDR c1  bj bp bl , constraints y +y d-1) +yl(d-1) c1d  j(d-1) p(N and c1d  yj (d-1) + yp(d-1) + yl(d-1) d, 1  d  m + n - 1 are introduced (with N = 3 in this case). If IDR of an entity is already in form of Case II, i.e.,ai  bj bp bl then constraints xid  yj(d-1) +yp(d-1) +yl(d-1) - qxi and xid  yj (d-1) + yp(d-1) + N yl(d-1) d, 1  d  m + n - 1 are introduced (with N = 3). These constraints satisfies that if the entity xi is hardened initially then it is not dead at any time step. Step 3: Let M be the number of minterms in the transformed IDR as described in Step 1. In the given example with IDR ai  c1 + c2 + bq constraints of form xid  c1(d-1) + c2(d-1) + yq(d-1) - (M - 1) - qxi and xid  c1(d-1) +c2(d-1) +yq(d-1) d, 1  d  m + n - 1 are introduced. M These constraints ensures that even if all the minterms of xi has at least one entity in dead state then it will be alive if the entity is hardened initially. For all IDRs of type Case I and Case III, the constraint discussed in this step is used.

E NTITY H ARDENING P ROBLEM

A. Optimal Solution using Integer Linear Programming We propose an Integer Linear Program (ILP) that solves the Entity Hardening problem optimally. Let [G, H ] with G = {g1 , g2 , ..., gn } and H = {h1 , h2 , ..., hm } denote the entities in set A and B respectively with hi = 0 (gj = 0) if entity ai (bj ) is alive and hi = 1 (gj = 1) otherwise. Given an integer k let [G, H ] be the solution (with value of 1 corresponding to entities failed initially) that cause maximum number of entity failure. Two variables xid and yjd are used in the ILP with xid = 1 (yjd = 1), when entity ai  A (bj  B ) is in a failed state at time step d, and 0 otherwise. The number of entities to be defended is considered to be k .

6

B. Heuristic In this subsection we provide a greedy heuristic solution to the Entity Hardening problem. For an interdependent network I (A, B, F (A, B )) with the initial failed set of entities as A  B  , Protection Set of each entity has been defined in the approximation scheme of Case III. To design the heuristic we define Minterm Coverage Number of each entity in A  B as follows: Definition: For an entity xi  A  B the Minterm Coverage Number is defined as the number of minterms that can be removed from F (A, B ) without affecting the cascading process by hardening the entity xi when all entities in A  B  fails initially. This is represented as M (xi |A  B  ). Similar to the computation of Protection Set the Minterm Coverage Number of each entity can be computed in O((n + m)2 ). With these definitions the heuristic is given in Algorithm 2. The algorithm takes in as input an interdependent network I (A, B, F (A, B )) with S = A  B . Step 4-5 is done to reduce the search space as it directly follows that the set of entities in Q wouldn't effect the hardening process. In each iteration of the while loop an entity xd is greedily selected which when hardened would prevent failure of maximum number of entities. This ensures that at each step the number of entities failed is minimized. In case of a tie, among all entities involved in the tie, the entity having the highest Minterm Coverage Number is included in the solution. This gives a higher priority to the entity which when hardened, has more impact on failure minimization in subsequent iterations of the while loop. The interdependent network I (A, B, F (A, B )) is updated in steps 13-16 of the algorithm. This takes into account the effect of hardening an entity in the current iteration on entities hardened in the following iterations. Run Time Analysis of Algorithm 2: For this analysis we consider n to be the total number of entities and m to be the total number of minterms. Updates in step 4 can be done in O(m) and step 5 in O(n). The while loop iterates for k times. In each iteration of the while loop step 7 and step 8 takes at most O((n + m)2 ) and O(nlog (n)) time respectively. On branching in step 9, step 10 and step 11 takes O((n + m)2 ) and O(nlog (n)) time respectively. Updates in step 13 takes O(n) time and in step 14 takes O(n + m) time. Step 12, 16 and 17 runs in constant time. Hence Algorithm 2 runs in O(k (n + m)2 ) time. VI. E XPERIMENTAL R ESULTS

Algorithm 2: Heuristic Solution to the ENH Problem
Data: An interdependent network I (A, B, F (A, B )) (with S = A  B ), set of entities A  B  failed initially causing maximum failure in the interdependent network with |A | + |B  | = K and hardening budget k. Result: Set of hardened entities H. begin Initialize S   A  B  ; Initialize H = ; Update F (A, B ) as follows -- (a) let Q be the set of entities that does not fail on failing K entities, (b) remove IDRs corresponding to entities in set Q, (c) remove from minterm of entities not in set Q all entities which are in set Q ; Update S = S \ Q ; while (k entities are not hardened) do For each entity xi  S compute the Protection Set P (xi |S  ); Choose the entity xd with highest cardinality of the set |P (xd |S  )|; if (more than one entity has the same highest cardinality value) then For each such entity xj compute the Minterm Coverage Number M (xj |S  ) ; Choose the entity xd with highest Minterm Coverage Number. ; In case of a tie choose arbitrarily; Update S  S - P (xd |S  ); Update F (A, B ) by removing (i) IDRs corresponding to all entities in P (xd |S  ), and (ii) occurrence of these entities in IDRs of entities not in P (xd |S  ); if (xd  S  ) then Update S   S  - {xd }; Update H = H  xd ; return H ;

1 2 3 4

5 6 7 8 9 10 11 12 13 14

15 16 17 18

located within the geographic region formed the set A and B respectively. Each region was represented by an interdependent network I (A, B, F (A, B )). We use the IDR construction rules as defined in [9] to generate F (A, B ).

In this section we present the experimental results of the Entity Hardening problem by comparing the optimal solution computed using an ILP, and the proposed heuristic algorithm. The experiments were conducted on real world power grid data obtained from Platts (www.platts.com), and communication network data obtained from GeoTel (www.geo-tel.com) of Maricopa County, Arizona. The data consisted of 70 power plants and 470 transmission lines in the power network, and 2, 690 cell towers, 7, 100 fiber-lit buildings and 42, 723 fiber links in the communication network. We identified five nonintersecting geographical regions from the data set and labeled them from regions 1 through 5. For each of the regions, the entities of the power and communication network that were

In all of our simulations IBM CPLEX Optimizer 12.5 to solve ILPs and Python 3 for heuristic is used. To analyze the Entity Hardening problem the value of K was set to 8. The ILP in [9] was used to compute the K most vulnerable nodes in the network, and the set of failed entities due to the failure of the K entities was also computed. For the five regions, when the K = 8 most vulnerable nodes failed, the total number of failed entities in the network were 28, 23, 28, 28 and 27 respectively. With the K most vulnerable nodes and final set of failed nodes as input, the ILP and heuristic of the Entity Hardening problem are compared with k = 1, 3, 5, 7. The results of these simulations are shown in Figure 1. It is observed that the heuristic solution differs more from optimal at higher values of k (factor of 0.5 and 0.67 for Regions 1 and 3 respectively with k = 7). This is primarily because of the greedy nature of Algorithm 2. However on an average the heuristic solution differs by a factor of 0.13 from the optimal.

7

14 Number of entities failed 12 10 8 6 4 2 0 1 12

13

Number of entities failed

12 10 8 6 4 2 0 1 3 7 7

Number of entities failed

ILP solution Heuristic

14 13

13

14 ILP solution Heuristic 12 10 8 6 4 2 0 1 12

13

ILP solution Heuristic

8 6 5 3 1 3 5 7 Number of entities hardened 3

7 6 4 3 2 1 3 5 7 Number of entities hardened

3 1 1

3 5 7 Number of entities hardened

(a) Region 1
12 Number of entities failed 10 8 6 4 2 0 1 6 5 4 3 2 1 3 5 7 Number of entities hardened 11 11

(b) Region 2
8 Number of entities failed ILP solution Heuristic 7 6 5 4 3 2 1 0 1 3 5 7 Number of entities hardened 1 3 5 7

(c) Region 3
ILP solution Heuristic

(d) Region 4

(e) Region 5

Fig. 1: Comparison chart of the optimal solution (ILP) with the heuristic by varying number of entities hardened for each identified region

VII.

C ONCLUSION

In this paper we studied the entity hardening problem in multi-layer networks. We modeled the interdependencies shared between the networks using IIM, and formulated the the Entity Hardening problem in this setting. We showed that the problem is solvable in polynomial time for some cases, whereas for others it is NP-complete. We evaluated the efficacy of our heuristic using power and communication network data of Maricopa County, Arizona. Our experiments showed that our heuristic almost always produces near optimal results. R EFERENCES
[1] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin, "Catastrophic cascade of failures in interdependent networks," Nature, vol. 464, no. 7291, pp. 1025­1028, 2010. J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, "Networks formed from interdependent networks," Nature Physics, vol. 8, no. 1, pp. 40­48, 2011. J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, "Cascade of failures in coupled network systems with multiple support-dependence relations," Physical Review E, vol. 83, no. 3, p. 036116, 2011. V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and R. Setola, "Modelling interdependent infrastructures using interacting dynamical models," International Journal of Critical Infrastructures, vol. 4, no. 1, pp. 63­79, 2008. P. Zhang, S. Peeta, and T. Friesz, "Dynamic game theoretic model of multi-layer infrastructure networks," Networks and Spatial Economics, vol. 5, no. 2, pp. 147­178, 2005. M. Parandehgheibi and E. Modiano, "Robustness of interdependent networks: The case of communication networks and the power grid," arXiv preprint arXiv:1304.0356, 2013. D. T. Nguyen, Y. Shen, and M. T. Thai, "Detecting critical nodes in interdependent power networks for vulnerability assessment," 2013.

[2]

[3]

[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman, "Power grid vulnerability to geographically correlated failures-analysis and control implications," arXiv preprint arXiv:1206.1099, 2012. [9] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, "Identification of k most vulnerable nodes in multi-layered network using a new model of interdependency," in Computer Communications Workshops (INFOCOM WKSHPS), 2014 IEEE Conference on. IEEE, 2014, pp. 831­836. [10] J.-F. Castet and J. H. Saleh, "Interdependent multi-layer networks: Modeling and survivability analysis with applications to space-based networks," PloS one, vol. 8, no. 4, p. e60402, 2013. [11] A. Das, J. Banerjee, and A. Sen, "Root cause analysis of failures in interdependent power-communication networks," in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910­915. [12] A. Mazumder, C. Zhou, A. Das, and A. Sen, "Progressive recovery from failure in multi-layered interdependent network using a new model of interdependency," in Conference on Critical Information Infrastructures Security (CRITIS), 2014. Springer, 2014. [13] M. Hajiaghayi, K. Jain, K. Konwar, L. Lau, I. Mandoiu, A. Russell, A. Shvartsman, and V. Vazirani, "The minimum k-colored subgraph problem in haplotyping and dna primer selection," in Proceedings of the International Workshop on Bioinformatics Research and Applications (IWBRA). Citeseer, 2006.

[4]

[5]

[6]

[7]

2014 IEEE 15th International Conference on High Performance Switching and Routing (HPSR)

On Shortest Single/Multiple Path Computation
Problems in Fiber-Wireless (FiWi) Access Networks
Chenyang Zhouâ , Anisha Mazumderâ , Arunabha Senâ , Martin Reissleinâ  and Andrea Richaâ
â School of Computing, Informatics and Decision Systems Engineering
â  School of Electrical, Computer, and Energy Engineering

Arizona State University
Email: {czhou24, anisha.mazumder, asen, reisslein, aricha}@asu.edu

AbstractâFiber-Wireless (FiWi) networks have received considerable attention in the research community in the last few
years as they offer an attractive way of integrating optical and
wireless technology. As in every other type of networks, routing
plays a major role in FiWi networks. Accordingly, a number of
routing algorithms for FiWi networks have been proposed. Most
of the routing algorithms attempt to ï¬nd the âshortest pathâ
from the source to the destination. A recent paper proposed
a novel path length metric, where the contribution of a link
towards path length computation depends not only on that link
but also every other link that constitutes the path from the
source to the destination. In this paper we address the problem
of computing the shortest path using this path length metric.
Moreover, we consider a variation of the metric and also provide
an algorithm to compute the shortest path using this variation.
As multipath routing provides a number of advantages over
single path routing, we consider disjoint path routing with the
new path length metric. We show that while the single path
computation problem can be solved in polynomial time in both
the cases, the disjoint path computation problem is NP-complete.
We provide optimal solution for the NP-complete problem using
integer linear programming and also provide two approximation
algorithms with a performance bound of 4 and 2 respectively.
The experimental evaluation of the approximation algorithms
produced a near optimal solution in a fraction of a second.

I. I NTRODUCTION
Path computation problems are arguably one of the most
well studied family of problems in communication networks.
In most of these problems, one or more weight is associated
with a link representing, among other things, the cost, delay or
the reliability of that link. The objective most often is to ï¬nd
a least weighted path (or âshortest pathâ) between a speciï¬ed
source-destination node pair. In most of these problems, if a
link l is a part of a path P , then the contribution of the link
l on the âlengthâ of the path P depends only on the weight
w(l) of the link l, and is oblivious of the weights of the links
traversed before or after traversing the link l on the path P .
However, in a recent paper on optical-wireless FiWi network
[5], the authors have proposed a path length metric, where the
contribution of the link l on the âlengthâ of the path P depends
not only on its own weight w(l), but also on the weights
of all the links of the path P . As the authors of [5] do not
present any algorithm for computing the shortest path between
the source-destination node pair using this new metric, we
present a polynomial time algorithm for this problem in this
paper. This result is interesting because of the nature of new

978-1-4799-1633-7/14/$31.00 Â©2014 IEEE

metric proposed in [5], one key property on which the shortest
path algorithm due to Dijkstra is based, that is, subpath of a
shortest path is shortest, is no longer valid. We show that even
without this key property, not only it is possible to compute
the shortest path in polynomial time using the new metric, it
is also possible to compute the shortest path in polynomial
time, with a variation of the metric proposed in [5].
The rest of the paper is organized as follows. In section
III, we present the path length metric proposed for the FiWi
network in [5] and a variation of it. In section IV we provide
algorithms for computing the shortest path using these two
metrics. As multi-path routing offers signiï¬cant advantage
over single path routing [6], [7], [8], [9], we also consider
the problem of computation of a pair of node disjoint paths
between a source-destination node pair using the metric proposed in [5]. We show that while the single path computation
problem can be solved in polynomial time in all these cases,
the disjoint path computation problem is NP-complete. The
contributions of the paper are as follows;
â¢ Polynomial time algorithm for single path routing (metric
1) in FiWi networks
â¢ Polynomial time algorithm for single path routing (metric
2) in FiWi networks
â¢ NP-completeness proof of disjoint path routing (metric
1) in FiWi networks
â¢ Optimal solution for disjoint path routing (metric 1) in
FiWi networks using Integer Linear Programming
â¢ One approximation algorithm for disjoint path routing in
FiWi networks with an approximation bound of 4 and
computation complexity O((n + m)log n)
â¢ One approximation algorithm for disjoint path routing in
FiWi networks with an approximation bound of 2 and
computation complexity O(m(n + m)log n)
â¢ Experimental evaluation results of the approximation
algorithm for disjoint path routing in FiWi networks
II. R ELATED W ORK
Fiber-Wirelss (FiWi) networks is a hybrid access network
resulting from the convergence of optical access networks
such as Passive Optical Networks (PONs) and wireless access
networks such as Wireless Mesh Networks (WMNs) capable of providing low cost, high bandwidth last mile access.

131

Because it provides an attractive way of integrating optical
and wireless technology, Fiber-Wireless (FiWi) networks have
received considerable attention in the research community in
the last few years [1], [2], [3], [4], [5], [8], [9]. The minimum
interference routing algorithm for the FiWi environment was
ï¬rst proposed in [4]. In this algorithm the path length was
measured in terms of the number of hops in the wireless
part of the FiWi network. The rationale for this choice was
that the maximum throughput of the wireless part is typically
much smaller than the throughput of the optical part, and
hence minimization of the wireless hop count should lead to
maximizing the throughout of the FiWi network. However,
the authors of [5] noted that minimization of the wireless
hop count does not always lead to throughput maximization.
Accordingly, the path length metric proposed by them in
[5] pays considerable importance to the trafï¬c intensity at a
generic FiWi network node. The results presented in this paper
are motivated by the path length metric proposed in [5].
III. P ROBLEM F ORMULATION
In the classical path problem, each edge e â E of the graph
G = (V, E), has a weight w(e) associated with it and if there
is a path P from the node v0 to vk in the graph G = (V, E)
w

w

w

w

v0 â1 v1 â2 v2 â3 v3 . . . âk vk
then the path length or the distance between the nodes v0 and
vk is given by
w(Pv0 ,vk ) = w1 + w2 + Â· Â· Â· + wk
However, in the path length metric proposed in [5] for
optical-wireless FiWi networks [1], [2], [3], the contribution
of ei to the path length computation depends not only on
the weight wi , but also on the weights of the other edges
that constitute the path. In the following section, we discuss
this metric and a variation of it. We also also formulate the
multipath computation problem using this metric.
The Optimized FiWi Routing Algorithm (OFRA) proposed
in [5] computes the âlengthâ (or weight) of a path P from v0
to vk using the following metric




(wu ) + max (wu )
w (Pv0 ,vk ) = min
P

âuâP

âuâP

where wu represents the trafï¬c intensity at a generic FiWi
network node u, which may be an optical node in the ï¬ber
backhaul or a wireless node in wireless mesh front-end. In
order to compute shortest path using this metric, in our
formulation, instead of associating a trafï¬c intensity âweightâ
(wu ) with nodes, we associate them with edges. This can easily
be achieved by replacing the node u with weight wu with two
nodes u1 and u2 , connecting them with an edge (u1 , u2 ) and
assigning the weight wu on this edge. In this scenario, if there
is a path P from the node v0 to vk in the graph G = (V, E)
w

w

w

w

v0 â1 v1 â2 v2 â3 . . . âk vk
then the path length between the nodes v0 and vk is given by

w+ (Pv0 ,vk )

=
=

w1 + w2 + . . . + wk + max(w1 , w2 , . . . wk )
k

wi + maxki=1 wi
i=1

In the second metric, the length a path Pv0 ,vk :
v0 âv1 âv2 â . . . âvk , between the nodes v0 and vk is given
by

wÌ(Pv0 ,vk )

=
=

k

i=1
k


wi + CN T (Pv0 ,vk ) â max(w1 , w2 , . . . wk )
wi + CN T (Pv0 ,vk ) â maxki=1 wi

i=1

where CN T (Pv0 ,vk ) is the count of the number of times
max (w1 , w2 , . . . wk ) appears on the path Pv0 ,vk . We study the
shortest path computation problems in FiWi networks using
the above metrics and provide polynomial time algorithms for
solution in subsections IV-A and IV-B.
If wmax = max(w1 , w2 , . . . wk ), we refer to the corresponding edge (link) as emax . If there are multiple edges having
the weight of wmax , we arbitrarily choose any one of them as
emax . It may be noted that both the metrics have an interesting
property in that in both cases, the contribution of an edge e
on the path length computation depends not only on the edge
e but also on every other edge on the path. This is so, because
if the edge e happens to be emax , contribution of this edge
in computation of w+ (Pv0 ,vk ) and wÌ(Pv0 ,vk ) will be 2 â w(e)
and CN T (Pv0 ,vk ) â w(e) respectively. If e is not emax , then
its contribution will be w(e) for both the metrics.
As multipath routing provides an opportunity for higher
throughput, lower delay, and better load balancing and resilience, its use have been proposed in ï¬ber networks [6],
wireless networks [7] and recently in integrated ï¬ber-wireless
networks [8], [9]. Accordingly, we study the problem of
computing a pair of edge disjoint paths between a sourcedestination node pair s and d, such that the length of the
longer path (path length computation using the ï¬rst metric)
is shortest among all edge disjoint path pairs between the
nodes s and d. In subsection IV-C we prove that this problem
is NP-complete, in subsection IV-D, we provide an optimal
solution for the problem using integer linear programming,
in subsections IV-E and IV-F we provide two approximation
algorithms for the problem with a performance bound of 4
and 2 respectively, and in subsection IV-F we provide results
of experimental evaluation of the approximation algorithms.
IV. PATH P ROBLEMS IN F I W I N ETWORKS
In this section, we present (i) two different algorithms for
shortest path computation using two different metrics, (ii)
NP-completeness proof for the disjoint path problem, (iii)
two approximation algorithms for the disjoint path problem,
and (iv) experimental evaluation results of the approximation
algorithms.

132

It may be noted that, in both metrics w+ (Pv0 ,vk ) and
wÌ(Pv0 ,vk ), we call an edge e â Pv0 ,vk crucial, if w(e) =
maxki=1 w(e ), âe â Pv0 ,vk .
A. Shortest Path Computation using Metric 1
It may be recalled that the path length
k metric used in this
case is the following: w+ (Pv0 ,vk ) = i=1 wi + maxki=1 wi . If
k
the path length metric was given as w(Pv0 ,vk ) = i=1 wi ,
algorithms due to Dijkstra and Bellman-Ford could have been
used to compute the shortest path between a source-destination
node pair. One important property of the path length metric
that is exploited by Dijkstraâs algorithm is that âsubpath of a
shortest
path is shortestâ. However, the new path length metric
k
k
i=1 wi +maxi=1 wi does not have this property. We illustrate
this with the example below.
Consider two paths P1 and P2 from the node v0 to v3 in the
w
w
w
graph G = (V, E), where P1 : v0 â1 v1 â2 v2 â3 v3 and P2 :
w4
w5
w3
v0 â v4 â v2 â v3 . If w1 = 0.25, w2 = 5, w3 = 4.75, w4 =
2, w5 = 4, the length of the path P1 , w+ (P1 ) = w1 +w2 +w3 +
max(w1 , w2 , w3 ) = 0.25 + 5 + 4.75 + max(0.25, 5, 4.75) = 15
and the length of the path P2 , w+ (P2 ) = w4 + w5 + w6 +
max(w4 , w5 .w6 ) = 2 + 4 + 4.75 + max(2, 4, 4.75) = 15.5.
Although P1 is shortest path in this scenario, the length of
w
w
its subpath v0 â1 v1 â2 v2 is 0.25 + 5 + max (0.25, 5) =
10.25, which is greater than the length of a subpath of P2
w
w
v0 â4 v4 â5 v2 2 + 4 + max (2, 4) = 10, demonstrating that
the assertion that âsubpath of a shortest path is shortestâ no
longer holds in this path length metric.
As the assertion âsubpath of a shortest path is shortestâ no
longer holds in this path length metric, we cannot use the
standard shortest path algorithm due to Dijkstra in this case.
However, we show that we can still compute the shortest path
between a source-destination node pair in polynomial time by
repeated application of the Dijkstraâs algorithm. The algorithm
is described next.
For a given graph G = (V, E), w.l.o.g, we assume |V | = n
and |E| = m. Deï¬ne Ge as subgraph of G by deleting edges
whose weight is greater than w(e).
Also, as Dijkstraâs algorithm does, we need to maintain
distance vector. We deï¬ne distv be distance (length of shortest
path) from s to v, Î v be predecessor of v and maxedgev be
weight of the crucial edge from s to v via the shortest path,
ansv be optimal solution (length) from s to v.
Different Ge can be treated as different layers of the
G. For any path P , we deï¬ne the function eâ (P ) as the
crucial edge along P . It is easy to observe that if
Pd is the
optimal path from s to node d then w(Pd ) =
eâP w(e)
and w+ (Pd ) = w(Pd ) + w(eâ (Pd )). It may be noted that
henceforth, we shorten Ps,d to Pd , because we consider that
the source is ï¬xed while the destination d is variable.
Lemma 1. w(Pd ) is minimum in Geâ (Pd ) .
Proof: It is obvious that Pd still exists in Geâ (Pd ) , since
edges on Pd are not abandoned. Suppose Pd is not shortest,
then there must be another path Pd s.t. w(Pd ) < w(Pd ).
Noting that the crucial edge on Pd , namely e , is no longer than

Algorithm 1 Modiï¬ed Dijkstraâs Algorithm
1: Initialize ansv = â for for all v â V
2: sort all edges according to w(e) in ascending order
3: for i = 1 to m do
4:
Initialize distv = â, Î v = nil, maxedgev = 0 for
all v â V
5:
dists = 0
6:
Q = the set of all nodes in graph
7:
while Q is not empty do
8:
u = Extract-Min(Q)
9:
for each neighbor v of u do
10:
if eu,v â E(Gei ) then
11:
t = MAX {maxedgeu , w(eu,v )}
12:
if distu + w(eu,v ) < distv then
13:
distv = distu + w(eu,v )
14:
maxedgev = t
15:
Î v = u
16:
else if distu + w(eu,v ) == distv then
17:
if maxedgev > t then
18:
maxedgev = t
19:
Î v = u
20:
end if
21:
end if
22:
end if
23:
end for
24:
end while
25:
for each node v do
26:
ansv = min{ansv , distv + maxedgev }
27:
end for
28: end for
eâ (Pd ) since they both belong to Geâ (Pd ) . Hence w+ (Pd ) =
w(Pd )+w(e ) < w(Pd )+w(eâ (Pd )) = w+ (Pd ), contradicting
Pd is optimal.
Lemma 2. Modiï¬ed Dijkstraâs Algorithm (MDA) computes
shortest path while keeping the crucial edge as short as
possible in every iteration.
Proof: Line 4 to 24 works similar to the standard Dijkstraâs algorithm does. Besides, when updating distance, MDA
also updates the crucial edge to guarantee that it lies on the
path and when there is a tie, MDA will choose the edge with
the smaller weight.
Theorem 1. Modiï¬ed Dijkstraâs Algorithm computes optimal
solution for every node v in O(m(n + m)logn) time.
Proof: Lemma 1 indicates for any node v â V , optimal
solution can be obtained by enumerating all possible crucial
edges eâ (Pv ) and computing shortest path on Geâ (Pv ) . By sorting all edges in nondecreasing order, every subgraph Geâ (Pv )
is considered and it is shown in lemma 2, MDA correctly
computes shortest path for every node v in every Geâ (Pv ) .
Then optimal solution is obtained by examining all shortest
path using the w() metric plus the corresponding crucial edge.
Dijkstraâs algorithm runs O((n + m)logn) time when using

133

binary heap, hence MDA runs in O(m(n+m)logn) time when
considering all layers.
B. Shortest Path Computation using Metric 2
Given a path P , let eâ (P ) be the crucial edge along the
P and CN T (P ) be the number of occurrence of such edge.
Now
a path Q, such that wÌ(P ) =
 our objective becomes to ï¬nd
â
w(e)
+
CN
T
(Q)
â
w(e
(Q))
is minimum.
eâQ
The layering technique can also be used in this problem.
However, shortest path under a ceratin layer may not become
a valid candidate for optimal solution. Here, we introduce a
dynamic programming algorithm that can solve the problem
optimally in O(n2 m2 ) time.
Input is a weighted graph G = (V, E), |V | = n, |E| = m
with a speciï¬ed source node s. In this paper, we only
consider nonnegative edge weight. As shown before, we use
Ge to represent the residue graph by deleting edges longer
than e in G. Different from MDA1, in order to consider
the number of crucial edges, distv is replaced by an array
dist0v , dist1v , ....distnv . One can think distcv be the shortest
distance from s to v by going through exactly c crucial edges
and possibly some shorter edges. Similarly, we replace Î v by
Î cv , 0 â¤ c â¤ n. Each Î cv records predecessor of v for the
path corresponding to distcv . Lastly, ansv is used as optimal
solution from s to v.
Lemma 3. If Pv is the best path from s to v, i.e., wÌ(Pv ) is
minimum among all s-v path, then Pv is computed in Geâ (Pv )
CN T (Pv )
and distv
= w(Pv ).
Proof: By deï¬nition, Pv exists in Geâ (Pv ) and
CN T (Pv ) â¥ 1 since any path should go through at least one
crucial edge. Noting wÌ(Pv ) = w(Pv )+CN T (Pv )âw(eâ (Pv )),
on one hand if we treat CN T (Pv ) as a ï¬xed number, then we
need to keep w(Pv ) as small as possible. Inspired by idea
of bellman-ford algorithm, we can achieve it by enumerating
|Pv |, i.e., number of edges on Pv . On the other hand, we need
to keep tracking number of crucial edges as well. Hence, distcv
is adopted to maintain such information, superscript c reï¬ects
exact number of crucial edges. From line 12 to line 25, distcv
is updated either when it comes from a neighbor who has
already witnessed c crucial edges or it comes from a neighbor
with c â 1 crucial edges and the edge between is crucial. In
either case, node v gets a path, say P  , with exact c crucial
edges on it and w(P ) is minimum. At last, Pv can be selected
by enumerating number of crucial edges and that is what line
30 to 32 does.
Lemma 4. Maxedge Shortest Path Algorithm(MSPA) runs in
O(n2 m) time for each Ge .
Proof: We can apply similar analysis of bellman-ford
algorithm. However, we need to update distcv array, it takes
extra O(n) time for every node v in every iteration when
enumerating |Pv |. Hence, total running time is O(n2 m).
Theorem 2. MSPA computes optimal path for every v â V
in O(n2 m2 ) time.

Algorithm 2 Maxedge Shortest Path Algorithm
1: Initialize ansv = â for for all v â V
2: sort all edges according to w(e) in ascending order, say
e1 , e2 , ..., em after sorting
3: for i = 1 to m do
4:
Initialize distcv = â, Î cv = nil for all v â V and all
0â¤câ¤n
5:
dist0s = 0
6:
for j = 1 to n â 1 do
7:
for k = 0 to j do
8:
for every node v â V do
9:
if distkv = â then
10:
continue
11:
end if
12:
for every neighbor u of v do
13:
if w(eu,v ) > w(ei ) then
14:
continue
15:
else if w(eu,v ) == w(eâ ) then
16:
if distkv + w(eu,v )
<
then
distk+1
u
17:
distk+1
= distkv +
u
w(eu,v )
18:
Î k+1
=v
u
19:
end if
20:
else
21:
if distkv + w(eu,v ) < distku
then
22:
distku = distkv +
w(eu,v )
23:
Î ku = v
24:
end if
25:
end if
26:
end for
27:
end for
28:
end for
29:
end for
30:
for i = 1 to n â 1 do
31:
ansv = min{ansv , distiv + i â w(ei )}
32:
end for
33: end for

Proof: By Lemma 3, if Pv is obtained when computing
Geâ Pv . Then, by considering all possible Geâ , we could get
Pv in one of these layering. It takes O(m) to generate all Geâ ,
by Lemma 4, MSPA runs in v â V in O(n2 m2 ) time.
C. Computational Complexity of Disjoint Path Problem
In this section, we study edge disjoint path in optical
wireless network. By reduction from well known Min-Max
2-Path Problem, i.e., min-max 2 edge disjoint path problem
under normal length measurement, we show it is also
NP-complete if we try to minimize the longer path when w+
length is applied. Then we give an ILP formulation to solve
this problem optimally. At last, we provide two approximation
algorithm, one with approximation ratio 4, running time

134

O((m + n)logn), the other one with approximation ratio 2
while running time is O(m(m + n)logn).

D. Optimal Solution for the Disjoint Path Problem
Here, we give an ILP formulation for MinMax2OWFN.
ILP for MinMax2OWFN

Min-Max 2 Disjoint Path Problem (MinMax2PP)
Instance: An undirected graph G = (V, E) with a positive
weight w( e) associated with each edge e â E, a source node
s â V , a destination node t â V , and a positive number X.
Question: Does there exist a pair of edge disjoint paths P1
and P2 from s to d in G such that w(P1 ) â¤ w(P2 ) â¤ X?

min
s.t.

MP



The MinMax2PP problem is shown to be NP-complete in
[10]. With a small modiï¬cation, we show NP-completeness
still holds if w+ length measurement is adopted.

fi,j,1 â

fj,i,1 =

(i,j)âE

(j,i)âE





fi,j,2 â

(i,j)âE

Min-Max 2 Disjoint Path Problem in Optical Wireless
Networks (MinMax2OWFN)
Instance: An undirected graph G = (V, E) with a positive
weight w( e) associated with each edge e â E, a source node
s â V , a destination node t â V , and a positive number X.
Question: Does there exist a pair of edge disjoint paths P1
and P2 from s to t in G such that w+ (P1 ) â¤ w+ (P2 ) â¤ X  ?



fj,i,2 =

(j,i)âE

â§
âª
â¨
âª
â©
â§
âª
â¨
âª
â©

1
â1
0

i=s
i=t
otherwise

1
â1
0

i=s
i=t
otherwise

fi,j,1 + fi,j,2 â¤ 1
w1 â¥ fi,j,1 â w(i, j)

â(i, j) â E

w2 â¥ fi,j,2 â w(i, j)

fi,j,1 â w(i, j)
M P â¥ w1 +

â(i, j) â E

â(i, j) â E

(i,j)âE



M P â¥ w2 +

Theorem 3. The MinMax2OWFN is NP-complete

fi,j,2 â w(i, j)

(i,j)âE

Proof: Evidently, MinMax2OWFN is in NP class, given
two edge joint path P1 and P2 , we can check if w+ (P1 ) â¤
w+ (P2 ) â¤ X  in polynomial time.
We then transfer from MinMax2PP to MinMax2OWFN.
Let graph G = (V, E) with source node s , destination t and
an integer X be an instance of MinMax2PP, we construct an
instance Gâ of MinMax2OWFN in following way.
1) Create an identical graph G with same nodes and edges
in G.
2) Add one node s0 to G .
3) Create two parallel edges e01 , e02 between s0 and s,
w(e01 ) = w(e02 ) = maxeâG(E) w(e)
4) Choose s0 to the source node in G and t to be the
destination.
5) Set X  = X + 2w(e01 )
It is easy to see, the construction takes polynomial time.
Now we need to show a instance of MinMax2OWFN have
two edge disjoint paths from s0 to t with length at most X  if
and only if the corresponding instance have two edge disjoint
paths from s to t with length at most X.
Suppose there are two edge disjoint paths P1 and P2 from
s0 to t in G , such that w+ (P1 ) â¤ w+ (P2 ) â¤ X  . By the
way we construct G , P1 and P2 must go through e01 and
e02 . W.l.o.g. we say e01 â P1 and e02 â P2 . Since w(e01 ) =
w(e02 ) = maxeâE(G ) {w(e)}, therefore e01 and e02 are the
crucial edge on P1 and P2 respectively. Hence, P1 â e01 and
P2 âe02 are two edge disjoint path in G, with length no greater
than X  â 2w(e01 ) = X.
Conversely, now suppose P1 and P2 are two edge joint paths
in G satisfying w(P1 ) â¤ w(P2 ) â¤ X. We follow the same
argument above, P1 + e01 and P2 + e02 are two desired paths,
with length not exceeding X + 2w(e01 ) = X  .

fi,j,1 = {0, 1},

fi,j,2 = {0, 1}

â(i, j) â E

The following is a brief description of this ILP formulation.
The ï¬rst two equation represent ï¬ow constraint as normal
shortest path problem does. fi,j,1 = 1 indicates path P1 goes
through edge (i, j), and 0 otherwise. So it is with fi,j,2 and
path P2 . Constraint 3 ensures two edges are disjoint, since
fi,j,1 and fi,j,2 cannot both be 1 at the same time. w1 , w2 act
as the weights of the crucial edges on P1 and P2 respectively.
Finally, we deï¬ne M P to be the maximum of w+ (P1 ) and
w+ (P2 ) and therefore try to minimize it.
E. Approximation Algorithm for the Disjoint Path Problem,
with approximation factor 4
Next we propose a 4-approximation algorithm which runs
in O((n + m)logn) time.
Given G = (V, E) with source s and destination t, the idea
of approximation algorithm is to ï¬nd two disjoint P1 and P2
such that w(P1 ) + w(P2 ) is minimized. Such P1 and P2 can
be found either using min cost max ï¬ow algorithm or the
algorithm due to Suurballe presented in [11]. And we need to
show both w+ (P1 ) and w+ (P2 ) are at most four times of the
optimal solution.
Algorithm 3 MinMax2OWFN Approximation Algorithm 1 (MAA1)
1: Run Suurballeâs algorithm on G, denote P1 , P2 be two
resulting path.
2: Compute w + (P1 ) and w + (P2 ).
3: Output max{w + (P1 ), w + (P2 )}.

135

Lemma 5. For any path P , w+ (P ) â¤ 2w(P ).
Proof: By deï¬nition, w+ (P ) = w(P ) + w(eâ (P )). Since
w(e (P )) â¤ w(P ), then w+ (P ) â¤ 2 â w(P ).
â

Lemma 6. If P1 and P2 are two edge joint path from s to
t such that w(P1 ) + w(P2 ) is minimum, then w+ (P1 ) and
w+ (P2 ) are at most four times of the optimal solution.
Proof: Say opt is the optimal value of a
M inM ax2OW F N instance and Q1 ,Q2 are two s â t
edge disjoint path in one optimal solution. W.l.o.g, we may
suppose w+ (P1 ) â¥ w+ (P2 ) and w+ (Q1 ) â¥ w+ (Q2 ). Let
w(P1 ) + w(P2 ) = p and w(Q1 ) + w(Q2 ) = q, by assumption,
p â¤ q. Also, we have w+ (P1 ) = w(P1 ) + eâ (P1 ) â¤ 2p,
opt = w+ (Q1 ) = w(Q1 ) + eâ (Q1 ) > 2q . Hence,
+
w+ (P2 )
(P1 )
2p
â¤ w opt
< q/2
â¤4
opt
Theorem 4. MAA1 is a 4-approximation algorithm running
in O((n + m)logn) time and 4 is a tight bound.
Proof: By Lemma 5 and 6, MAA1 has approximation
ratio at most 4.Then we show MAA1 has approximation at
least 4 for certain cases. Consider the following graph.

Algorithm 4 MinMax2OWFN Approximation Algorithm
2(MAA2)
1: set ans = â
2: for every Ge of G do
3:
Run Suurballeâs algorithm on Ge , denote P1 , P2 be
two resulting path.
4:
Compute w+ (P1 ) and w+ (P2 ).
5:
ans = min{ans, max{w+ (P1 ), w+ (P2 )}}.
6: end for
7: Output ans.
w(P1 )+w(e )
max{w+ (Q1 ), w+ (Q2 )} < 2. We
w(P1 )+w(e )
Suppose max{w
+ (Q ), w + (Q )} â¥ 2,
1
2

w(e ). It sufï¬ces to show

prove

it by contradiction.

then

w(P1 ) + w(e ) â¥ w+ (Q1 ) + w+ (Q2 )
Which follows,
w(P1 ) + w(e ) â¥ w(Q1 ) + w(eâ (Q1 )) + w(Q2 ) + w(eâ (Q2 ))
By deï¬nition, e is one of eâ (Q1 ), eâ (Q2 ). Hence,
w(P1 ) > w(Q1 ) + w(Q2 )
It is impossible since w(P1 ) + w(P2 ) is minimum in layer
G e .
Theorem 5. MAA2 is a 2-approximation algorithm running
in O(m(n + m)logn) time and 2 is a tight bound.

It is easy to check, P1 = {s â t}, P2 = {s â r â t} are
two edge disjoint path with minimum length 2k+2, w+ (P1 ) =
4k > w+ (P2 ) = 3. However, let Q1 = {s â u1 â u2 â
... â ukâ1 â uk â r â t}, Q2 = {s â r â v1 â v2 â
... â vkâ1 â vk â t}, then w(Q1 ) + w(Q2 ) = 2k + 4 while
+
4k
1)
w+ (Q1 ) = w+ (Q2 ) = k + 3. ww+(P
Q1 = k+3 â 4 when k is
sufï¬ciently large. Hence, 4 is a tight bound for MAA1.
We need O((n + m)logn) time running Suurballeâs algorithm and O(n) time computing w+ (P1 ) and w+ (P2 ).
Therefore total running time is O((n + m)logn).
F. Approximation Algorithm for the Disjoint Path Problem,
with approximation factor 2
In MAA1, layering technique is not used and we only
consider the original graph. However, by taking all Ge of G
into account, we can have a better approximation ratio.
Say Q1 , Q2 are two disjoint paths in one optimal solution.
Let e = max{eâ (Q1 ), eâ (Q2 )} and P1 , P2 be the resulting
paths when computing layer Ge ; w.l.o.g, we may assume
w(P1 ) > w(P2 ). Also, let anse = max{w+ (P1 ), w+ (P2 )}.
Lemma 7. anse < 2 max{w+ (Q1 ), w+ (Q2 )}.
Proof: Noting that w(eâ (P1 )) â¤ w(e ) and w(eâ (P2 )) â¤
w(e ) since they both belong to Ge . Then anse â¤ w(P1 ) +


Proof: By Lemma 7, in one of the layer, we guarantee
to have a 2-approximation solution. Since we take minimum
outcome among all layers, the ï¬nal result is no worse than
twice of the optimal solution. Now we need to show there
exists certain case, such that MAA2 is no good than twice of
the optimal solution. Consider the following graph

There is only one layer, and P1 = {s â x1 â x2 â
... â x2kâ1 â x2 k â t}, P2 = {s â r â t} are two
edge disjoint path with minimum length 2k + 3, w+ (P1 ) =
2k + 2 > w+ (P2 ) = 3. Again, set Q1 = {s â u1 â u2 â
... â ukâ1 â uk â r â t}, Q2 = {s â r â v1 â v2 â
... â vkâ1 â vk â t}, then w(Q1 ) + w(Q2 ) = 2k + 4 while
+
2k+2
1)
w+ (Q1 ) = w+ (Q2 ) = k + 3. ww+(P
Q1 = k+3 â 2 when k is
sufï¬ciently large. Hence, 2 is a tight bound for MAA2.
Finally, it is easy to see that the running time is O(m(n +
m)logn).

136

S
node
14
18
1
18
20
10
1
14
20
10
18
1
20
14
10
20
5

D
node
2
8
6
4
3
3
11
6
7
5
12
20
13
19
17
16
11

Opt
Sol
47
46
28
50
40
27
35
50
38
36
22
46
26
29
36
29
40

Approx
Sol 1
55
46
28
58
40
27
35
52
38
38
22
52
26
29
36
29
48

Approx
ratio 1
1.17
1
1
1.16
1
1
1
1.04
1
1.05
1
1.13
1
1
1
1
1.2

Approx
Sol 2
55
46
28
57
40
27
35
52
38
38
22
52
26
29
36
29
48

Approx
ratio 2
1.17
1
1
1.14
1
1
1
1.04
1
1.05
1
1.13
1
1
1
1
1.2

Fig. 1.

TABLE I
C OMPARISON OF THE A PPROXIMATE SOLUTIONS WITH THE O PTIMAL
SOLUTION FOR THE ARPANET GRAPH

The ARPANET graph with 20 nodes and 32 links

approximation algorithms. Both the approximation algorithms
have a constant factor approximation bound. However, there is
a trade-off between the quality of the solution (approximation
bound) and the execution time. Finally, we show that both the
approximation algorithms obtain near optimal results through
simulation using the ARPANET topology.

G. Experimental Results for the Disjoint Path Problem
In this section, we present the results of simulations for
comparing the performance of our approximation algorithms
with the optimal solution when w+ () metric is applied.
The simulation experiments have been carried out on the
ARPANET topology (as shown in Fig 1 with nodes and links
shown in black) which has twenty nodes and thirty two links.
The weights of the links have been randomly generated and
lie in the range of two and eleven (as shown in red in Fig
1) and we consider the graph to be undirected. The results of
the comparison is presented in Table I. We have compared the
lengths of the longer of the two edge disjoint paths computed
by the optimal and the approximate solutions for seventeen
different source-destination pairs. It may be noted that for
almost 65% of the cases, the approximate algorithms obtain
the optimal solution. In the remaining cases, the approximate
solutions lie within a factor of 1.2 of the optimal solution
Thus, even though the approximation ratio in the worst case
are proven to be 4 and 2, in practical cases, it is within 1.2.
From these experimental results, we can conclude that the
approximation algorithms produce optimal or near optimal
solutions in majority of the cases. It may be noted that the
two approximation algorithms perform in a similar fashion
in the ARPANET graph, however, as proven theoretically,
the two approximation algorithms differ in their worst case
approximation ratio.
V. C ONCLUSION

R EFERENCES
[1] N. Ghazisaidi, M. Maier, and C. M. Assi, âFiber-Wireless (FiWi) Access
Networks: A Surveyâ, IEEE Communications Magazine, vol. 47, no. 2,
pp 160-167, Feb. 2009.
[2] N. Ghazisaidi, and M. Maier, âFiber-Wireless (FiWi) Access Networks:
Challenges and Opportunitiesâ, IEEE Network, vol. 25, no. 1, pp 36-42,
Feb. 2011.
[3] Z. Zheng, J. Wang, X. Wang, âONU placement in ï¬ber-wireless (FiWi)
networks considering peer-to-peer communicationsâ, IEEE Globecom, 2009.
[4] Z. Zheng, J. Wang, X. Wang, âA study of network throughput gain
in optical-wireless (FiWi) networks subject to peer-to-peer commuincationsâ, IEEE ICC, 2009.
[5] F. Aurzada, M. Levesque, M. Maier, M. Reisslein, âFiWi Access
Networks Based on Next-Generation PON and Gigabit-Class WLAN
Technologies: A Capacity and Delay Analysisâ, IEEE/ACM Transactions
on Networking, to appear.
[6] A. Sen, B.Hao . B. Shen , L.Zhou and S. Ganguly, âOn maximum
available bandwidth through disjoint pathsâ, Proc. of IEEE Conf. on
High Performance Switching and Routing, 2005.
[7] M. Mosko, J.J. Garcia-Luna-Aceves, âMultipath routing in wireless
mesh networksâ, Proc. of IEEE Workshop on Wireless Mesh Networks, 2005.
[8] J. Wang, K. Wu, S. Li and C. Qiao ,âPerformance Modeling and Analysis
of Multi-Path Routing in Integrated Fiber-Wireless (FiWi) Networksâ,
IEEE Infocom mini conference, 2010.
[9] S. Li, J. Wang, C. Qiao, Y. Xu ,âMitigating Packet Reordering in
FiWi Networksâ, IEEE/OSA Journal of Optical Communications and
Networking, vol. 3, pp.134-144, 2011.
[10] C. Li, S.T. McCormick and D.Simchi-Levi, âComplexity of Finding Two
Disjoint Paths with Min- Max Objectiveâ, Discrete Applied Mathematics, vol. 26, pp. 105-115, 1990.
[11] J. W. Suurballe, âDisjoint paths in a networkâ, Networks, vol. 4, pp. 125145, 1974.

In this paper, we study the shortest path problem in FiWi
networks. Based on the path length metrics proposed in [3],
[5], we present polynomial time algorithms for the single
path scenario. In the disjoint path scenario, we prove that the
problem of ï¬nding a pair of disjoint paths, where the length
of the longer path is shortest, is NP-complete. We provide an
ILP solution for the disjoint path problem and propose two

137

On the Entity Hardening Problem in Multi-layered Interdependent Networks
Joydeep Banerjee, Arun Das, Chenyang Zhou, Anisha Mazumder and Arunabha Sen
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering Arizona State University Tempe, Arizona 85287 Email: {joydeep.banerjee, arun.das, czhou24, anisha.mazumder, asen}@asu.edu Abstract--The power grid and the communication network are highly interdependent on each other for their well being. In recent times the research community has shown significant interest in modeling such interdependent networks and studying the impact of failures on these networks. Although a number of models have been proposed, many of them are simplistic in nature and fail to capture the complex interdependencies that exist between the entities of these networks. To overcome the limitations, recently an Implicative Interdependency Model that utilizes Boolean Logic, was proposed and a number of problems were studied. In this paper we study the "entity hardening" problem, where by "entity hardening" we imply the ability of the network operator to ensure that an adversary (be it Nature or human) cannot take a network entity from operative to inoperative state. Given that the network operator with a limited budget can only harden k entities, the goal of the entity hardening problem is to identify the set of k entities whose hardening will ensure maximum benefit for the operator, i.e. maximally reduce the ability of the adversary to degrade the network. We show that the problem is solvable in polynomial time for some cases, whereas for others it is NP-complete. We provide the optimal solution using ILP, and propose a heuristic approach to solve the problem. We evaluate the efficacy of our heuristic using power and communication network data of Maricopa County, Arizona. The experiments show that our heuristic almost always produces near optimal results.

arXiv:1412.6686v1 [cs.NI] 20 Dec 2014

that may exist between network entities, such as when entity ai is operational, if entities (i) bj and bk and bl are operational, or (ii) bm and bn are operational, or (iii) bp is operational. Graph based interdependency models proposed in the literature such as [3], [4], [5], [10], [6], [7] including [1], [2] cannot capture such complex interdependency involving both conjunctive and disjunctive terms between entities of multi-layer networks. To overcome these limitations, an Implicative Interdependency Model that utilizes Boolean Logic, was recently proposed in [9], and a number of problems including computation of K most vulnerable nodes [9], root cause of failure analysis [11], and progressive recovery from failures [12], were studied using this model. In this paper we study the "entity hardening" problem in the interdependent power-communication network using the Implicative Interdependency Model (IIM). By "entity hardening", we imply the ability of the network operator to ensure that an adversary (be it Nature or human), cannot take a network entity from an operative (operational) to an inoperative (failed) state. We assume that the adversary is clever and is capable of identifying the most vulnerable entities in the network that causes maximum damage to the interdependent system. However, the adversary does not have an unlimited budget and has the resources to destroy at most K entities of the interdependent network. The network operator is also aware of adversary's target entities for destruction. Since we assume that once an entity is "hardened" by the network operator it cannot be destroyed by the adversary, if all K targets of the adversary are hardened by the network operator, then the adversary cannot induce any failure in the network. However, if due to resource limitations the network operator is able to strengthen only k entities, where k < K, these k entities have to be carefully chosen. The goal of the entity hardening problem is to identify the set of k entities whose hardening will ensure maximum benefit for the operator, i.e. maximally reduce the ability of the adversary to degrade the network. We classify the entity hardening problem into four different cases depending on the nature of the interdependency relationships. We show that the first case can be solved in polynomial time, and all other cases are shown to be NP-complete. We provide an inapproximability result for the second case, an approximation algorithm for the third case, and a heuristic for the fourth (general) case. We evaluate the efficacy of our heuristic using power and communication network data of Maricopa County, Arizona. The experiments show that our

I.

I NTRODUCTION

The critical infrastructures of a nation form a complex symbiotic ecosystem where individual infrastructures are heavily interdependent on each other for being fully functional. Two such critical systems that rely heavily on each other for their well being are the power and communication network infrastructures. For instance, power grid entities such as SCADA systems, that are used to remotely operate power generation units, receive their control commands over the communication network infrastructure, while communication network entities such as routers and base stations are inoperable without electric power. Thus, failure introduced in the system either by Nature (hurricanes), or man (terrorist attacks), can trigger further failures in the system due to interdependencies between the entities of the two infrastructures. Although a number of models have been proposed for modeling and analysis of interdependent multi-layered networks [1], [2], [3], [4], [5], [6], [7], [8], many of these models are simplistic in nature and fail to capture the complex interdependencies that exists between the entities of these networks. As noted in [9], these models fail to model complex interdependencies

2

heuristic almost always produces near optimal results. The paper is organized as follows, the IIM model is presented in Section II, in Sections III and IV we formally state the entity hardening problem and analyze its computational complexity, Section V outlines the optimal and heuristic solutions to the problem, Section VI shows the experimental results, and finally Section VII concludes this paper. II. I NTERDEPENDENCY M ODEL

III.

P ROBLEM F ORMULATION

We now present an overview of the underlying IIM interdependency model [9]. IIM uses Boolean Logic to model the interdependencies between network entities, these interdependent relationships are termed as Implicative Interdependency Relations (IDRs). We represent this interdependent network setting as I (A, B, F (A, B )), where sets A and B are the power and communication network entities respectively, and F (A, B ) is the set of dependency relations, or IDRs. Table I represents a sample interdependent network I (A, B, F (A, B )), where A = {a1 , a2 , a3 , a4 }, B = {b1 , b2 , b3 } and F (A, B ) is the set of IDRs (dependency relations) between the entities of A and B . In this example, the IDR b1  a1 a3 + a2 implies that entity b1 is operational when both the entities a1 and a3 are operational, or entity a2 is operational. The conjunction of entities, such as a1 a3 , is also referred to as a minterm.
Power Network a1  b1 b2 a2  b1 + b2 a3  b1 + b2 + b3 a4  b1 + b3 Comm. Network b1  a1 a3 + a2 b2  a1 a2 a3 b3  a1 + a2 + a3 --

Before we make a formal statement of the entity hardening problem in the IIM setting, we explain it with the help of an example. Consider an interdependent system as outlined in the IDR set shown in Table I. It may be easily checked that when the adversary budget is K= 2, the most vulnerable entities of this system are {a2 , b3 }. If the network operator doesn't harden any one of the entities a2 or b3 , then in this example all the network entities eventually fail, as seen from the fault propagation in Table II. When the network operator chooses to harden both a2 and b3 then none of the entities in the network fail if the adversary restricts the attack only to the two most vulnerable entities of the network, which in this example happens to be {a2 , b3 }. If the network operator has resources to harden only one entity and the operator chooses to harden a2 , the destruction of b3 by the adversary will eventually lead to the failure of no other entities of the network, as shown in Table III(a). If on the other hand, the network operator chooses to harden b3 , destruction by the adversary of a2 will eventually lead to the failure of the entities {a2 , b2 , a1 , b1 } as shown in Table III(b). Clearly in this scenario the operator should harden a2 instead of b3 . Definition: Kill Set of a set of Entities(S ): The kill set of a set of entities S , is the set of all entities that will eventually fail due to failure of S and the interdependencies between the entities of the network as given by the set of IDR's. The kill set of a set of entities S is denoted by KillSet(S ). It may be noted that the search for k entities to be hardened is restricted to the KillSet(S ), where S is the set of K most vulnerable entities in the network, because hardening any entity not in KillSet(S ) does not provide any benefit to the network operator. In this study we also assume that the set of K most vulnerable entities in the network is unique.
Entities 0 a1 a2 a3 a4 b1 b2 b3 0  0 0 0 0 1 Time Steps (t) 1 2 3 0  0 0 0 0 1 0  0 0 0 0 1 0  0 0 0 0 1 Entities 4 0  0 0 0 0 0 a1 a2 a3 a4 b1 b2 b3 0 0 1 0 0 0 0  Time Steps (t) 1 2 3 0 1 0 0 0 1  1 1 0 0 0 1  1 1 0 0 1 1  4 1 1 0 0 1 1 

TABLE I: Implicative Interdependency Relations of a sample network

Given a set of inoperable (failed) entities, a time stepped failure cascade can be derived from the dependency relationships outlined in the IDR set. For example, for the interdependent network outlined in Table I, Table II shows the failure propagation when entities {a2 , b3 } fail at the initial time step (t = 0). It may be noted that the model assumes that dependent entities fail immediately in the next time step, for example, when {a2 , b3 } fail at t = 0, b2 fails at t = 1 as b2 is dependent on a2 for its survival. The system reaches a steady state when the failure propagation process stops. In this example, when {a2 , b3 } fail at t = 0, the steady state is reached at time step t = 4.
Entities 0 a1 a2 a3 a4 b1 b2 b3 0 1 0 0 0 0 1 1 0 1 0 0 0 1 1 Time Steps (t) 2 3 4 1 1 0 0 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 5 1 1 1 1 1 1 1 6 1 1 1 1 1 1 1

(a) Entity a2 is hardened

(b) Entity b3 is hardened

TABLE III: Failure cascade propagation with entity hardening. Entities {a2 , b3 } are attacked at time step t = 0. A value of 1 denotes entity failure, 0 otherwise.  denotes a hardened entity.

TABLE II: Failure cascade propagation when entities {a2 , b3 } fail at time step t = 0. A value of 1 denotes entity failure, and 0 otherwise

We now proceed to formulate the entity hardening problem formally. Given an interdependent network system I (A, B, F (A, B )), and the set of K most vulnerable entities of the system A  B  , where A  A and B   B : The Entity Hardening (ENH) problem INSTANCE: Given: (i) An interdependent network system I (A, B, F (A, B )), where the sets A and B represent the entities of the two networks, and F (A, B ) is the set of IDRs. (ii) The set of K most vulnerable entities of the system A  B  , where A  A and B   B (iii) Two positive integers k, k < K and EF .

A primary consideration for using this model is the accurate formulation of the IDRs that is representative of the underlying physical power and communication network infrastructures. This can either be done by careful analysis as done in [8], or by consultation with experts of these infrastructures. We utilize IIM to model the interdependency between the two networks and analyze the entity hardening problem in this setting.

3

QUESTION:Is there a set of entities H = A  B  , A  A, B   B, |H|  k , such that hardening H entities results in no more than EF entities to fail after entities A  B  fail at time step t = 0. We note some of the assumptions for the ENH problem: First, we assume that once an entity is hardened, it is always operational and does not fail at any time step of the observation, even when the entity is part of the K most vulnerable entities. Second, we assume that k < K, as otherwise the selection of K entities for hardening ensures that no entities fail at all. Finally, as noted earlier, we assume that the set of K most vulnerable entities in the network is unique. We now proceed to analyze the computational complexity of the ENH problem. IV. C OMPUTATIONAL C OMPLEXITY A NALYSIS

[9]. So with two entities {xi , xj }  A  B  and Cxi  Cxj = Cxj i.e, Cxj  Cxi , if xi is hardened it prevents the failure of Cxi - Cxj entities (provided that none of the entities in Cxi - Cxj - {xi } are in A  B  ). With this assertion, for an entity xi  A  B  , steps 4-7 of Algorithm 1 finds the actual entities for which failure is prevented by hardening xi . The set D = {Dx1 , Dx2 , ..., DxK } comprises of these set of entities for each hardened entity xi . To prove that Algorithm 1 finds the optimal solution we make the following two assertions: First, consider any two sets Dxi and Dxj . It is implied from step 6 of Algorithm 1 that / A  B  is Dxi  Dxj = . Second, consider an entity xp  hardened. If xp fails when entities in A  B  fails initially then it would belong to some set Dxi . Thus hardening xp results in preventing the failure of entities that is a proper subset of Dxi . Hence the entities to be hardened must belong to A  B  only. Owing to the two assertions it directly follows that with a given budget k , hardening k highest cardinality sets from the set D ensures prevention of failure for the maximum number of entities. B. Case II: Problem Instance with One Minterm of Arbitrary Size The IDRs of Case II have a single minterm of arbitrary p size. This can be represented as xi  j =1 yj , where xi and yj are entities of network A(B ) and B (A) respectively and the size of the minterm is p. The Entity Hardening problem with respect to Case II is NP-complete and is proved in Theorem 2. An inapproximability proof for this case of the problem is given in Theorem 3 Theorem 2. The Entity Hardening problem for Case II is NP Complete Proof: The Entity Hardening problem for case II is proved to be NP complete by giving a reduction from the Densest pSubhypergraph problem [13], a known NP-complete problem. An instance of the Densest p-Subhypergraph problem includes a hypergraph G = (V, E ), a parameter p and a parameter M . The problem asks the question whether there exists a set of vertices |V  |  V and |V  |  p such that the subgraph induced with this set of vertices has at least M hyperedges. From an instance of the Densest p-Subhypergraph problem we create an instance of the ENH problem in the following way. For each vertex vi and each hyperedge ej an entity bi and aj are added to the set B and A respectively. For each hyperedge ej with ej = {vm , vn , vq } (say) an IDR of form aj  bm bn bq is created. It is assumed that the value of K is set of |V |. The values of k and EF are set to p and |V | + |E | - p - M (where |A| = |V | and |B | = |E |) respectively. In the constructed instance only entities of set A are dependent on entities of set B . Additionally the dependency for an entity ai consists of conjunction of entities in set B . Hence for an entity ai  A to fail, either it itself has to fail initially or all entities to which ai is dependent on has to fail. It is to be noted that the entities in set B has no induced failure i.e., there is no cascade. Following from this assertion, with K = p, the solution A =  and B  = B would fail all entities in set A  B . Moreover this is the single unique solution to the problem instance. This is because by including one entity

For an interdependent network I (A, B, F (A, B )) the IDRs can be represented in four different forms. We analyze the computational complexity of the ENH problem for each of these cases separately. A. Case I: Problem Instance with One Minterm of Size One The IDRs of Case I have a single minterm of size 1. This can be represented as xi  yj , where xi and yj are entities of network A(B ) and B (A) respectively. We show that the ENH problem for Case I can be solved optimally in polynomial time. Algorithm 1: Entity Hardening Algorithm for systems with Case I type interdependencies
Data: An interdependent network I (A, B, F (A, B )), set of K most vulnerable entities A  B  , A  A, B   B , hardening budget k and a set H = . Result: Set of hardened entities H. begin For each entity xi  (A  B  ) compute the set of kill sets C = {Cx1 , Cx2 , ..., CxK }, where Cxi = KillSet(xi ) ; Create a copy D = {Dx1 , Dx2 , ..., DxK } of set C ; for (i=1; i  K; i++) do for (j=1, j = i; j  K; j++) do if Cxj  Cxi then Dxi  Dxi \ Dxj ; Choose the top k sets from D with highest cardinality ; For each of the Dxi  D sets chosen in Step 8, H  H  xi ; return H

1 2 3 4 5 6 7

8 9 10

Theorem 1. Algorithm 1 solves the Entity Hardening problem for Case I optimally in polynomial time. Proof: It is shown in [9] that the kill set for all entities in the interdependent network can be computed in O(n3 ) where n = |A| + |B |, thus computing the kill sets for K entities takes O(Kn2 ). Step 4-7 of the algorithm runs in O(K2 ). Choosing the k highest cardinality sets can be found using any standard sorting algorithm in O(Klog (K)). Hence Algorithm 1 runs in O(Kn2 ). For two kill sets Cxi and Cxj it can be shown that either Cxi  Cxj =  or Cxi  Cxj = Cxi or Cxi  Cxj = Cxj

4

ai in the initial failure set would result in not failing at least one entity bj for a given budget K = p. Hence it won't fail the entire set of entities in A  B . If an entity in set A is hardened then it would have no effect in failure prevention of any other entities. Whereas hardening an entity bm  B might result in failure prevention of an entity ai  A with IDR aj  bm bn bq provided that entities bn , bq are also defended. With k = p (and K  |V | = |B |) it can be ensured that entities to be defended are from set B  . To prove the theorem consider that there is a solution to the Densest p-Subhypergraph problem. Then there exist p vertices which induces a subgraph which has at least M hyperedges. Hardening the entities bi  B  for each vertex vi in the solution of the Densest p-Subhypergraph problem would then ensure that at least M entities in set A are protected from failure. This is because the entities in set A for which the failure is prevented corresponds to the hyperedges in the induced subgraph. Thus the number of entities that fail after hardening p entities is at most |V | + |E | - p - M , solving the ENH problem. Now consider that there is a solution to the ENH problem. As previously stated, the entities to be hardened will always be from set B  . So defending p entities from set B  would result in failure prevention of at least M entities in set A such that EF  |V | + |E | - p - M . Hence, the vertex induced subgraph would have at least M hyperedges when vertices corresponding to the entities hardened are included in the solution of the Densest p-Subhypergraph problem, thus solving it. Theorem 3. For an interdependent network I (A, B, F (A, B )) with n = |A  B | and F (A, B ) having IDRs of form Case II, it is hard to approximate the ENH problem within a factor of 1 for some  > 0. log(n)
2

set S we add an entity bi in set B . For all subsets in S , say Sp , Sm , Sn , which has the element xi there is an IDR of form ai  bm + bn + bl . The values of positive integers k and EF are set to M and m - M respectively. It is assumed that the value of K = m. With similar reasoning as that of Case II it can be shown that for K = m the maximum number of node failures (i.e. failure of all entities in A  B ) would occur if A =  and B  = B . This is also the single unique solution to the problem instance. The constructed instance also ensures that the entities to be hardened are from set B  (A not considered as it is equal to ). This is because protecting an entity ai  A would only result in prevention of its own failure whereas protecting an entity bj  B would result in failure prevention of its own and all other entities in set A for which it appears in its IDR. To begin with the proof, consider that there is a solution to the Set Cover problem. Then there exist M subsets (or elements in set S ) whose union results in the set S . Hardening the entities in set B corresponding to the subsets selected would ensure that all entities in set A are prevented from failure. This is because for the dependency of each entity ai  A there exist at least one entity (in set B ) that is hardened. Hence the number of entities that fails after hardening is m-M which is equal to EF , thus solving the ENH problem. Now, consider that there is a solution to the ENH problem. As discussed above the entities to be hardened should be from set B  . To achieve EF = m - M with k = M , no entities in the set A must fail. Hence for each entity ai  A at least one entity in set B that appears in its IDR has to be hardened. Thus, it directly follows that the union of subsets in set S corresponding to the entities hardened is equal to the set S , solving the Set Cover Problem. 1) Approximation Scheme for Case 3: In this subsection we provide an approximation algorithm for Case 3 of the problem. For an interdependent network I (A, B, F (A, B )) with the initial failed set of entities as A  B  we define Protection Set of each entity as follows. Definition: For an entity xi  A  B the Protection Set is defined as the entities that would be prevented from failure by hardening the entity xi when all entities in A  B  fails initially. This is represented as P (xi |A  B  ). The Protection Set of each entity can be computed in O((n + m)2 ) where n and m are the number of entities and number of minterms respectively in an interdependent network I (A, B, F (A, B )) . Theorem 5. For two entities xi , xj  A  B , P (xi |A  B  )  P (xj |A  B  ) = P (xi , xj |A  B  ) when IDRs are of form Case III. Proof: Assume that defending two entities xi and xj would result in preventing failure of P (xi , xj |A  B  ) entities with |P (xi |A  B  )  P (xj |A  B  )| < |P (xi , xj |A  B  )|. Then there exist at least one entity xp  / P (xi |A  B  )  P (xj |A  B  ) such that it's failure is prevented only if xi and xj is protected together. So two entities xm and xn (with xm  P (xi |A B  ) and xn  P (xj |A B  ) or vice versa) have to be

Proof: From Theorem 2, Densest p-Subhypergraph problem has been shown to be a special case of the ENH problem with IDRs of form Case II. Densest p-Subhypergraph problem is proved to be inapproximable within a factor of log1 2 (n) ( > 0) in [13]. Hence the theorem follows. C. Case III: Problem Instance with an Arbitrary Number of Minterm of Size One The IDRs of Case III have arbitrary number of minterm of size 1. This can be represented as xi  p q=1 yq , where xi and yq are entities of network A(B ) and B (A) respectively and the number of minterms are p. The ENH problem with respect to Case III is NP-complete and is proved in Theorem 4. Theorem 4. The ENH problem for Case III is NP Complete Proof: The ENH problem for case III is proved to be NP complete by giving a reduction from the Set Cover Problem, a well known NP-complete problem. An instance of the Set Cover problem includes a set S = {x1 , x2 , ..., xn }, a set S = {S1 , S2 , ..., Sm } where Si  S and a positive integer M . The problem asks the question whether there exists at most M subsets from set S whose union would result in the set S . From an instance of the set cover problem we create an instance of the ENH problem in the following way. For each element xi in set S we add an entity ai in set A. For each subset Si in

5

present in the IDR of xp . As the IDRs are of form Case III so if any one of xm or xn is protected then xp is protected, hence a contradiction. On the other way round P (xi , xj |A  B  ) contains all entities which would be prevented from failure if xi or xj is defended alone. So it directly follows that |P (xi |A  B  )  P (xj |A  B  )| > |P (xi , xj |A  B  )| is not possible. Hence the theorem holds. Theorem 6. There exists an 1 - 1 e approximation algorithm that approximates the ENH problem for Case III. Proof: The approximation algorithm is constructed by modeling the problem as Maximum Coverage problem. An instance of the maximum coverage problem consists of a set S = {x1 , x2 , ..., xn }, a set S = {S1 , S2 , ..., Sm } where Si  S and a positive integer M . The objective of the problem is to find a set S   S and |S  |  M such that Si S Si is maximized. For a given initial failure set A  B  with |A | + |B  |  K, let P (xi |A  B  ) denote the protection set for each entity xi  A  B . We construct a set S = A  B and for each entity xi a set Sxi  S such that Sxi = P (xi |A  B  ). Each set Sxi is added as an element of a set S . The conversion of the problem to Maximum Coverage problem can be done in polynomial time. By Theorem 5 defending a set of entities X  S would result in failure prevention of xi X Sxi entities. Hence, with the constructed sets S and S and a positive integer M (with M = k ) finding the Maximum Coverage would ensure the failure protection of maximum number of entities in A  B . This is same as the ENH problem of Case III. As there exists an 1 - 1 e approximation algorithm for the Maximum Coverage problem hence the theorem holds. D. Case IV: Problem Instance with an Arbitrary Number of Minterms of Arbitrary Size The IDRs of Case IV have arbitrary number of minterm of arbitrary size. This can be represented as xi  qj1 p j2 =1 yj2 , where xi and yj2 are entities of network j1 =1 A(B ) and B (A) respectively and there are p minterms each of size qj1 . Theorem 7. The Entity Hardening problem for Case IV is NP Complete Proof: Case II and Case III are special cases of Case IV. Hence following from Theorem 2 and Theorem 4 the computational complexity of the Entity Hardening problem is NP-complete in Case IV. V. S OLUTIONS TO
THE

It is to be noted that the maximum number cascading steps is upper bounded by |A| + |B | - 1 = m + n - 1. The objective function can now be formulated as follows:
m n

min
i=1

xi(m+n-1) +
j =1

yj (m+n-1)

(1)

The objective in (1) minimizes the number of entities failed after the cascading failure with the respective constraints for the Entity Hardening problem as follows:
n m

Constraint Set 1:
i=1

qxi +
j =1

qyj = k , with qxi , qyj  [0, 1].

If an entity xi (yj ) is defended then qxi = 1 (qyj = 1) and 0 otherwise. Constraint Set 2: xi0  gi - qxi and yi0  hi - qyi . This constraint implies that only if an entity is not defended and gi (hi ) is 1 then the entity will fail at the initial time step. Constraint Set 3: xid  xi(d-1) , d, 1  d  m + n - 1, and yid  yi(d-1) , d, 1  d  m + n - 1, in order to ensure that for an entity which fails in a particular time step would remain in failed state at all subsequent time steps. Constraint Set 4: Modeling of the constraint to capture the cascade propagation for IIM is similar to the constraints established in [9]. A brief presentation of this constraint is provided here. Consider an IDR ai  bj bp bl + bm bn + bq of type Case IV. The following steps are enumerated to depict the cascade propagation: Step 1: Replace all minterms of size greater than one with a variable. In the example provided we have the transformed minterm as ai  c1 + c2 + bq with c1  bj bp bl and c2  bm bn (c1 , c2  {0, 1}) as the new IDRs. Note that after transformation, the original IDR is in the form of Case III and the introduced IDRs are in the form of Case II. Step 2: For each variable c, a constraints is added to capture the cascade propagation. Let N be the number of entities in the minterm on which c is dependent. In the example for the variable c1 with IDR c1  bj bp bl , constraints y +y d-1) +yl(d-1) c1d  j(d-1) p(N and c1d  yj (d-1) + yp(d-1) + yl(d-1) d, 1  d  m + n - 1 are introduced (with N = 3 in this case). If IDR of an entity is already in form of Case II, i.e.,ai  bj bp bl then constraints xid  yj(d-1) +yp(d-1) +yl(d-1) - qxi and xid  yj (d-1) + yp(d-1) + N yl(d-1) d, 1  d  m + n - 1 are introduced (with N = 3). These constraints satisfies that if the entity xi is hardened initially then it is not dead at any time step. Step 3: Let M be the number of minterms in the transformed IDR as described in Step 1. In the given example with IDR ai  c1 + c2 + bq constraints of form xid  c1(d-1) + c2(d-1) + yq(d-1) - (M - 1) - qxi and xid  c1(d-1) +c2(d-1) +yq(d-1) d, 1  d  m + n - 1 are introduced. M These constraints ensures that even if all the minterms of xi has at least one entity in dead state then it will be alive if the entity is hardened initially. For all IDRs of type Case I and Case III, the constraint discussed in this step is used.

E NTITY H ARDENING P ROBLEM

A. Optimal Solution using Integer Linear Programming We propose an Integer Linear Program (ILP) that solves the Entity Hardening problem optimally. Let [G, H ] with G = {g1 , g2 , ..., gn } and H = {h1 , h2 , ..., hm } denote the entities in set A and B respectively with hi = 0 (gj = 0) if entity ai (bj ) is alive and hi = 1 (gj = 1) otherwise. Given an integer k let [G, H ] be the solution (with value of 1 corresponding to entities failed initially) that cause maximum number of entity failure. Two variables xid and yjd are used in the ILP with xid = 1 (yjd = 1), when entity ai  A (bj  B ) is in a failed state at time step d, and 0 otherwise. The number of entities to be defended is considered to be k .

6

B. Heuristic In this subsection we provide a greedy heuristic solution to the Entity Hardening problem. For an interdependent network I (A, B, F (A, B )) with the initial failed set of entities as A  B  , Protection Set of each entity has been defined in the approximation scheme of Case III. To design the heuristic we define Minterm Coverage Number of each entity in A  B as follows: Definition: For an entity xi  A  B the Minterm Coverage Number is defined as the number of minterms that can be removed from F (A, B ) without affecting the cascading process by hardening the entity xi when all entities in A  B  fails initially. This is represented as M (xi |A  B  ). Similar to the computation of Protection Set the Minterm Coverage Number of each entity can be computed in O((n + m)2 ). With these definitions the heuristic is given in Algorithm 2. The algorithm takes in as input an interdependent network I (A, B, F (A, B )) with S = A  B . Step 4-5 is done to reduce the search space as it directly follows that the set of entities in Q wouldn't effect the hardening process. In each iteration of the while loop an entity xd is greedily selected which when hardened would prevent failure of maximum number of entities. This ensures that at each step the number of entities failed is minimized. In case of a tie, among all entities involved in the tie, the entity having the highest Minterm Coverage Number is included in the solution. This gives a higher priority to the entity which when hardened, has more impact on failure minimization in subsequent iterations of the while loop. The interdependent network I (A, B, F (A, B )) is updated in steps 13-16 of the algorithm. This takes into account the effect of hardening an entity in the current iteration on entities hardened in the following iterations. Run Time Analysis of Algorithm 2: For this analysis we consider n to be the total number of entities and m to be the total number of minterms. Updates in step 4 can be done in O(m) and step 5 in O(n). The while loop iterates for k times. In each iteration of the while loop step 7 and step 8 takes at most O((n + m)2 ) and O(nlog (n)) time respectively. On branching in step 9, step 10 and step 11 takes O((n + m)2 ) and O(nlog (n)) time respectively. Updates in step 13 takes O(n) time and in step 14 takes O(n + m) time. Step 12, 16 and 17 runs in constant time. Hence Algorithm 2 runs in O(k (n + m)2 ) time. VI. E XPERIMENTAL R ESULTS

Algorithm 2: Heuristic Solution to the ENH Problem
Data: An interdependent network I (A, B, F (A, B )) (with S = A  B ), set of entities A  B  failed initially causing maximum failure in the interdependent network with |A | + |B  | = K and hardening budget k. Result: Set of hardened entities H. begin Initialize S   A  B  ; Initialize H = ; Update F (A, B ) as follows -- (a) let Q be the set of entities that does not fail on failing K entities, (b) remove IDRs corresponding to entities in set Q, (c) remove from minterm of entities not in set Q all entities which are in set Q ; Update S = S \ Q ; while (k entities are not hardened) do For each entity xi  S compute the Protection Set P (xi |S  ); Choose the entity xd with highest cardinality of the set |P (xd |S  )|; if (more than one entity has the same highest cardinality value) then For each such entity xj compute the Minterm Coverage Number M (xj |S  ) ; Choose the entity xd with highest Minterm Coverage Number. ; In case of a tie choose arbitrarily; Update S  S - P (xd |S  ); Update F (A, B ) by removing (i) IDRs corresponding to all entities in P (xd |S  ), and (ii) occurrence of these entities in IDRs of entities not in P (xd |S  ); if (xd  S  ) then Update S   S  - {xd }; Update H = H  xd ; return H ;

1 2 3 4

5 6 7 8 9 10 11 12 13 14

15 16 17 18

located within the geographic region formed the set A and B respectively. Each region was represented by an interdependent network I (A, B, F (A, B )). We use the IDR construction rules as defined in [9] to generate F (A, B ).

In this section we present the experimental results of the Entity Hardening problem by comparing the optimal solution computed using an ILP, and the proposed heuristic algorithm. The experiments were conducted on real world power grid data obtained from Platts (www.platts.com), and communication network data obtained from GeoTel (www.geo-tel.com) of Maricopa County, Arizona. The data consisted of 70 power plants and 470 transmission lines in the power network, and 2, 690 cell towers, 7, 100 fiber-lit buildings and 42, 723 fiber links in the communication network. We identified five nonintersecting geographical regions from the data set and labeled them from regions 1 through 5. For each of the regions, the entities of the power and communication network that were

In all of our simulations IBM CPLEX Optimizer 12.5 to solve ILPs and Python 3 for heuristic is used. To analyze the Entity Hardening problem the value of K was set to 8. The ILP in [9] was used to compute the K most vulnerable nodes in the network, and the set of failed entities due to the failure of the K entities was also computed. For the five regions, when the K = 8 most vulnerable nodes failed, the total number of failed entities in the network were 28, 23, 28, 28 and 27 respectively. With the K most vulnerable nodes and final set of failed nodes as input, the ILP and heuristic of the Entity Hardening problem are compared with k = 1, 3, 5, 7. The results of these simulations are shown in Figure 1. It is observed that the heuristic solution differs more from optimal at higher values of k (factor of 0.5 and 0.67 for Regions 1 and 3 respectively with k = 7). This is primarily because of the greedy nature of Algorithm 2. However on an average the heuristic solution differs by a factor of 0.13 from the optimal.

7

14 Number of entities failed 12 10 8 6 4 2 0 1 12

13

Number of entities failed

12 10 8 6 4 2 0 1 3 7 7

Number of entities failed

ILP solution Heuristic

14 13

13

14 ILP solution Heuristic 12 10 8 6 4 2 0 1 12

13

ILP solution Heuristic

8 6 5 3 1 3 5 7 Number of entities hardened 3

7 6 4 3 2 1 3 5 7 Number of entities hardened

3 1 1

3 5 7 Number of entities hardened

(a) Region 1
12 Number of entities failed 10 8 6 4 2 0 1 6 5 4 3 2 1 3 5 7 Number of entities hardened 11 11

(b) Region 2
8 Number of entities failed ILP solution Heuristic 7 6 5 4 3 2 1 0 1 3 5 7 Number of entities hardened 1 3 5 7

(c) Region 3
ILP solution Heuristic

(d) Region 4

(e) Region 5

Fig. 1: Comparison chart of the optimal solution (ILP) with the heuristic by varying number of entities hardened for each identified region

VII.

C ONCLUSION

In this paper we studied the entity hardening problem in multi-layer networks. We modeled the interdependencies shared between the networks using IIM, and formulated the the Entity Hardening problem in this setting. We showed that the problem is solvable in polynomial time for some cases, whereas for others it is NP-complete. We evaluated the efficacy of our heuristic using power and communication network data of Maricopa County, Arizona. Our experiments showed that our heuristic almost always produces near optimal results. R EFERENCES
[1] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin, "Catastrophic cascade of failures in interdependent networks," Nature, vol. 464, no. 7291, pp. 1025­1028, 2010. J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, "Networks formed from interdependent networks," Nature Physics, vol. 8, no. 1, pp. 40­48, 2011. J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, "Cascade of failures in coupled network systems with multiple support-dependence relations," Physical Review E, vol. 83, no. 3, p. 036116, 2011. V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and R. Setola, "Modelling interdependent infrastructures using interacting dynamical models," International Journal of Critical Infrastructures, vol. 4, no. 1, pp. 63­79, 2008. P. Zhang, S. Peeta, and T. Friesz, "Dynamic game theoretic model of multi-layer infrastructure networks," Networks and Spatial Economics, vol. 5, no. 2, pp. 147­178, 2005. M. Parandehgheibi and E. Modiano, "Robustness of interdependent networks: The case of communication networks and the power grid," arXiv preprint arXiv:1304.0356, 2013. D. T. Nguyen, Y. Shen, and M. T. Thai, "Detecting critical nodes in interdependent power networks for vulnerability assessment," 2013.

[2]

[3]

[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman, "Power grid vulnerability to geographically correlated failures-analysis and control implications," arXiv preprint arXiv:1206.1099, 2012. [9] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, "Identification of k most vulnerable nodes in multi-layered network using a new model of interdependency," in Computer Communications Workshops (INFOCOM WKSHPS), 2014 IEEE Conference on. IEEE, 2014, pp. 831­836. [10] J.-F. Castet and J. H. Saleh, "Interdependent multi-layer networks: Modeling and survivability analysis with applications to space-based networks," PloS one, vol. 8, no. 4, p. e60402, 2013. [11] A. Das, J. Banerjee, and A. Sen, "Root cause analysis of failures in interdependent power-communication networks," in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910­915. [12] A. Mazumder, C. Zhou, A. Das, and A. Sen, "Progressive recovery from failure in multi-layered interdependent network using a new model of interdependency," in Conference on Critical Information Infrastructures Security (CRITIS), 2014. Springer, 2014. [13] M. Hajiaghayi, K. Jain, K. Konwar, L. Lau, I. Mandoiu, A. Russell, A. Shvartsman, and V. Vazirani, "The minimum k-colored subgraph problem in haplotyping and dna primer selection," in Proceedings of the International Workshop on Bioinformatics Research and Applications (IWBRA). Citeseer, 2006.

[4]

[5]

[6]

[7]

On the Entity Hardening Problem in Multi-layered
Interdependent Networks
Joydeep Banerjee, Arun Das, Chenyang Zhou, Anisha Mazumder and Arunabha Sen

arXiv:1412.6686v1 [cs.NI] 20 Dec 2014

Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {joydeep.banerjee, arun.das, czhou24, anisha.mazumder, asen}@asu.edu
AbstractâThe power grid and the communication network
are highly interdependent on each other for their well being.
In recent times the research community has shown significant
interest in modeling such interdependent networks and studying
the impact of failures on these networks. Although a number
of models have been proposed, many of them are simplistic in
nature and fail to capture the complex interdependencies that
exist between the entities of these networks. To overcome the
limitations, recently an Implicative Interdependency Model that
utilizes Boolean Logic, was proposed and a number of problems
were studied. In this paper we study the âentity hardeningâ
problem, where by âentity hardeningâ we imply the ability of the
network operator to ensure that an adversary (be it Nature or
human) cannot take a network entity from operative to inoperative
state. Given that the network operator with a limited budget
can only harden k entities, the goal of the entity hardening
problem is to identify the set of k entities whose hardening will
ensure maximum benefit for the operator, i.e. maximally reduce
the ability of the adversary to degrade the network. We show
that the problem is solvable in polynomial time for some cases,
whereas for others it is NP-complete. We provide the optimal
solution using ILP, and propose a heuristic approach to solve the
problem. We evaluate the efficacy of our heuristic using power
and communication network data of Maricopa County, Arizona.
The experiments show that our heuristic almost always produces
near optimal results.

I.

I NTRODUCTION

The critical infrastructures of a nation form a complex symbiotic ecosystem where individual infrastructures are heavily
interdependent on each other for being fully functional. Two
such critical systems that rely heavily on each other for their
well being are the power and communication network infrastructures. For instance, power grid entities such as SCADA
systems, that are used to remotely operate power generation
units, receive their control commands over the communication
network infrastructure, while communication network entities
such as routers and base stations are inoperable without electric
power. Thus, failure introduced in the system either by Nature
(hurricanes), or man (terrorist attacks), can trigger further
failures in the system due to interdependencies between the
entities of the two infrastructures.
Although a number of models have been proposed for modeling and analysis of interdependent multi-layered networks
[1], [2], [3], [4], [5], [6], [7], [8], many of these models are simplistic in nature and fail to capture the complex interdependencies that exists between the entities of these networks. As noted
in [9], these models fail to model complex interdependencies

that may exist between network entities, such as when entity ai
is operational, if entities (i) bj and bk and bl are operational, or
(ii) bm and bn are operational, or (iii) bp is operational. Graph
based interdependency models proposed in the literature such
as [3], [4], [5], [10], [6], [7] including [1], [2] cannot capture
such complex interdependency involving both conjunctive and
disjunctive terms between entities of multi-layer networks. To
overcome these limitations, an Implicative Interdependency
Model that utilizes Boolean Logic, was recently proposed in
[9], and a number of problems including computation of K
most vulnerable nodes [9], root cause of failure analysis [11],
and progressive recovery from failures [12], were studied using
this model.
In this paper we study the âentity hardeningâ problem in
the interdependent power-communication network using the
Implicative Interdependency Model (IIM). By âentity hardeningâ, we imply the ability of the network operator to ensure
that an adversary (be it Nature or human), cannot take a network entity from an operative (operational) to an inoperative
(failed) state. We assume that the adversary is clever and
is capable of identifying the most vulnerable entities in the
network that causes maximum damage to the interdependent
system. However, the adversary does not have an unlimited
budget and has the resources to destroy at most K entities
of the interdependent network. The network operator is also
aware of adversaryâs target entities for destruction. Since we
assume that once an entity is âhardenedâ by the network
operator it cannot be destroyed by the adversary, if all K
targets of the adversary are hardened by the network operator,
then the adversary cannot induce any failure in the network.
However, if due to resource limitations the network operator
is able to strengthen only k entities, where k < K, these k
entities have to be carefully chosen. The goal of the entity
hardening problem is to identify the set of k entities whose
hardening will ensure maximum benefit for the operator, i.e.
maximally reduce the ability of the adversary to degrade the
network.
We classify the entity hardening problem into four different
cases depending on the nature of the interdependency relationships. We show that the first case can be solved in polynomial
time, and all other cases are shown to be NP-complete. We
provide an inapproximability result for the second case, an
approximation algorithm for the third case, and a heuristic
for the fourth (general) case. We evaluate the efficacy of our
heuristic using power and communication network data of
Maricopa County, Arizona. The experiments show that our

2

heuristic almost always produces near optimal results.
The paper is organized as follows, the IIM model is
presented in Section II, in Sections III and IV we formally
state the entity hardening problem and analyze its computational complexity, Section V outlines the optimal and heuristic
solutions to the problem, Section VI shows the experimental
results, and finally Section VII concludes this paper.
II.

I NTERDEPENDENCY M ODEL

We now present an overview of the underlying IIM interdependency model [9]. IIM uses Boolean Logic to model
the interdependencies between network entities, these interdependent relationships are termed as Implicative Interdependency Relations (IDRs). We represent this interdependent network setting as I(A, B, F (A, B)), where sets A
and B are the power and communication network entities
respectively, and F (A, B) is the set of dependency relations,
or IDRs. Table I represents a sample interdependent network I(A, B, F (A, B)), where A = {a1 , a2 , a3 , a4 }, B =
{b1 , b2 , b3 } and F (A, B) is the set of IDRs (dependency
relations) between the entities of A and B. In this example,
the IDR b1 â a1 a3 + a2 implies that entity b1 is operational
when both the entities a1 and a3 are operational, or entity a2
is operational. The conjunction of entities, such as a1 a3 , is
also referred to as a minterm.
Power Network
a1 â b1 b2
a2 â b1 + b2
a3 â b1 + b2 + b3
a4 â b1 + b3

Comm. Network
b1 â a1 a3 + a2
b2 â a1 a2 a3
b3 â a1 + a2 + a3
ââ

TABLE I: Implicative Interdependency Relations of a sample network

Given a set of inoperable (failed) entities, a time stepped
failure cascade can be derived from the dependency relationships outlined in the IDR set. For example, for the interdependent network outlined in Table I, Table II shows the failure
propagation when entities {a2 , b3 } fail at the initial time step
(t = 0). It may be noted that the model assumes that dependent
entities fail immediately in the next time step, for example,
when {a2 , b3 } fail at t = 0, b2 fails at t = 1 as b2 is dependent
on a2 for its survival. The system reaches a steady state when
the failure propagation process stops. In this example, when
{a2 , b3 } fail at t = 0, the steady state is reached at time step
t = 4.
Entities
a1
a2
a3
a4
b1
b2
b3

0

1

0
1
0
0
0
0
1

0
1
0
0
0
1
1

Time Steps (t)
2
3
4
1
1
0
0
0
1
1

1
1
0
0
1
1
1

1
1
1
1
1
1
1

5

6

1
1
1
1
1
1
1

1
1
1
1
1
1
1

TABLE II: Failure cascade propagation when entities {a2 , b3 } fail at
time step t = 0. A value of 1 denotes entity failure, and 0 otherwise

A primary consideration for using this model is the accurate
formulation of the IDRs that is representative of the underlying
physical power and communication network infrastructures.
This can either be done by careful analysis as done in [8], or
by consultation with experts of these infrastructures. We utilize
IIM to model the interdependency between the two networks
and analyze the entity hardening problem in this setting.

III.

P ROBLEM F ORMULATION

Before we make a formal statement of the entity hardening
problem in the IIM setting, we explain it with the help of an
example. Consider an interdependent system as outlined in the
IDR set shown in Table I. It may be easily checked that when
the adversary budget is K= 2, the most vulnerable entities
of this system are {a2 , b3 }. If the network operator doesnât
harden any one of the entities a2 or b3 , then in this example
all the network entities eventually fail, as seen from the fault
propagation in Table II. When the network operator chooses
to harden both a2 and b3 then none of the entities in the
network fail if the adversary restricts the attack only to the two
most vulnerable entities of the network, which in this example
happens to be {a2 , b3 }. If the network operator has resources
to harden only one entity and the operator chooses to harden
a2 , the destruction of b3 by the adversary will eventually lead
to the failure of no other entities of the network, as shown in
Table III(a). If on the other hand, the network operator chooses
to harden b3 , destruction by the adversary of a2 will eventually
lead to the failure of the entities {a2 , b2 , a1 , b1 } as shown in
Table III(b). Clearly in this scenario the operator should harden
a2 instead of b3 .
Definition: Kill Set of a set of Entities(S): The kill set of a
set of entities S, is the set of all entities that will eventually
fail due to failure of S and the interdependencies between the
entities of the network as given by the set of IDRâs. The kill
set of a set of entities S is denoted by KillSet(S).
It may be noted that the search for k entities to be hardened
is restricted to the KillSet(S), where S is the set of K
most vulnerable entities in the network, because hardening any
entity not in KillSet(S) does not provide any benefit to the
network operator. In this study we also assume that the set of
K most vulnerable entities in the network is unique.
Entities
0
a1
a2
a3
a4
b1
b2
b3

0
â
0
0
0
0
1

Time Steps (t)
1
2
3
0
â
0
0
0
0
1

0
â
0
0
0
0
1

0
â
0
0
0
0
1

(a) Entity a2 is hardened

Entities
4
0
â
0
0
0
0
0

0
a1
a2
a3
a4
b1
b2
b3

0
1
0
0
0
0
â

Time Steps (t)
1
2
3
0
1
0
0
0
1
â

1
1
0
0
0
1
â

1
1
0
0
1
1
â

4
1
1
0
0
1
1
â

(b) Entity b3 is hardened

TABLE III: Failure cascade propagation with entity hardening. Entities {a2 , b3 } are attacked at time step t = 0. A value of 1 denotes
entity failure, 0 otherwise. â denotes a hardened entity.

We now proceed to formulate the entity hardening
problem formally. Given an interdependent network system
I(A, B, F (A, B)), and the set of K most vulnerable entities
of the system Aâ² âª B â² , where Aâ² â A and B â² â B:
The Entity Hardening (ENH) problem
INSTANCE: Given:
(i) An interdependent network system I(A, B, F (A, B)),
where the sets A and B represent the entities of the two
networks, and F (A, B) is the set of IDRs.
(ii) The set of K most vulnerable entities of the system
Aâ² âª B â² , where Aâ² â A and B â² â B
(iii) Two positive integers k, k < K and EF .

3

QUESTION:Is there a set of entities H = Aâ²â² âª B â²â² , Aâ²â² â
A, B â²â² â B, |H| â¤ k, such that hardening H entities results
in no more than EF entities to fail after entities Aâ² âª B â² fail
at time step t = 0.
We note some of the assumptions for the ENH problem:
First, we assume that once an entity is hardened, it is always
operational and does not fail at any time step of the observation, even when the entity is part of the K most vulnerable
entities. Second, we assume that k < K, as otherwise the
selection of K entities for hardening ensures that no entities
fail at all. Finally, as noted earlier, we assume that the set of
K most vulnerable entities in the network is unique. We now
proceed to analyze the computational complexity of the ENH
problem.
IV.

C OMPUTATIONAL C OMPLEXITY A NALYSIS

For an interdependent network I(A, B, F (A, B)) the IDRs
can be represented in four different forms. We analyze the
computational complexity of the ENH problem for each of
these cases separately.
A. Case I: Problem Instance with One Minterm of Size One
The IDRs of Case I have a single minterm of size 1. This
can be represented as xi â yj , where xi and yj are entities of
network A(B) and B(A) respectively. We show that the ENH
problem for Case I can be solved optimally in polynomial time.
Algorithm 1: Entity Hardening Algorithm for systems
with Case I type interdependencies

1
2
3
4
5
6
7

8
9
10

Data: An interdependent network I(A, B, F(A, B)), set of
K most vulnerable entities Aâ² âª B â² , Aâ² â A, B â² â B,
hardening budget k and a set H = â.
Result: Set of hardened entities H.
begin
For each entity xi â (Aâ² âª B â² ) compute the set of kill sets
C = {Cx1 , Cx2 , ..., CxK }, where Cxi = KillSet(xi ) ;
Create a copy D = {Dx1 , Dx2 , ..., DxK } of set C ;
for (i=1; i â¤ K; i++) do
for (j=1, j 6= i; j â¤ K; j++) do
if Cxj â Cxi then
Dxi â Dxi \ Dxj ;
Choose the top k sets from D with highest cardinality ;
For each of the Dxi â D sets chosen in Step 8,
H â H âª xi ;
return H

Theorem 1. Algorithm 1 solves the Entity Hardening problem
for Case I optimally in polynomial time.
Proof: It is shown in [9] that the kill set for all entities in
the interdependent network can be computed in O(n3 ) where
n = |A| + |B|, thus computing the kill sets for K entities takes
O(Kn2 ). Step 4-7 of the algorithm runs in O(K2 ). Choosing
the k highest cardinality sets can be found using any standard
sorting algorithm in O(Klog(K)). Hence Algorithm 1 runs in
O(Kn2 ).
For two kill sets Cxi and Cxj it can be shown that either
Cxi â© Cxj = â or Cxi â© Cxj = Cxi or Cxi â© Cxj = Cxj

[9]. So with two entities {xi , xj } â Aâ² âª B â² and Cxi â© Cxj =
Cxj i.e, Cxj â Cxi , if xi is hardened it prevents the failure
of Cxi â Cxj entities (provided that none of the entities in
Cxi â Cxj â {xi } are in Aâ² âª B â² ). With this assertion, for
an entity xi â Aâ² â© B â² , steps 4-7 of Algorithm 1 finds the
actual entities for which failure is prevented by hardening xi .
The set D = {Dx1 , Dx2 , ..., DxK } comprises of these set of
entities for each hardened entity xi .
To prove that Algorithm 1 finds the optimal solution we
make the following two assertions: First, consider any two sets
Dxi and Dxj . It is implied from step 6 of Algorithm 1 that
/ Aâ² âª B â² is
Dxi â© Dxj = â. Second, consider an entity xp â
hardened. If xp fails when entities in Aâ² âªB â² fails initially then
it would belong to some set Dxi . Thus hardening xp results
in preventing the failure of entities that is a proper subset of
Dxi . Hence the entities to be hardened must belong to Aâ² âª B â²
only. Owing to the two assertions it directly follows that with
a given budget k, hardening k highest cardinality sets from the
set D ensures prevention of failure for the maximum number
of entities.
B. Case II: Problem Instance with One Minterm of Arbitrary
Size
The IDRs of Case II have a single
Qpminterm of arbitrary
size. This can be represented as xi â j=1 yj , where xi and
yj are entities of network A(B) and B(A) respectively and the
size of the minterm is p. The Entity Hardening problem with
respect to Case II is NP-complete and is proved in Theorem
2. An inapproximability proof for this case of the problem is
given in Theorem 3
Theorem 2. The Entity Hardening problem for Case II is NP
Complete
Proof: The Entity Hardening problem for case II is proved
to be NP complete by giving a reduction from the Densest pSubhypergraph problem [13], a known NP-complete problem.
An instance of the Densest p-Subhypergraph problem includes
a hypergraph G = (V, E), a parameter p and a parameter M .
The problem asks the question whether there exists a set of
vertices |V â² | â V and |V â² | â¤ p such that the subgraph induced
with this set of vertices has at least M hyperedges. From an
instance of the Densest p-Subhypergraph problem we create
an instance of the ENH problem in the following way. For
each vertex vi and each hyperedge ej an entity bi and aj are
added to the set B and A respectively. For each hyperedge ej
with ej = {vm , vn , vq } (say) an IDR of form aj â bm bn bq is
created. It is assumed that the value of K is set of |V |. The
values of k and EF are set to p and |V | + |E| â p â M (where
|A| = |V | and |B| = |E|) respectively.
In the constructed instance only entities of set A are
dependent on entities of set B. Additionally the dependency
for an entity ai consists of conjunction of entities in set B.
Hence for an entity ai â A to fail, either it itself has to fail
initially or all entities to which ai is dependent on has to fail.
It is to be noted that the entities in set B has no induced failure
i.e., there is no cascade. Following from this assertion, with
K = p, the solution Aâ² = â and B â² = B would fail all entities
in set A âª B. Moreover this is the single unique solution to
the problem instance. This is because by including one entity

4

ai in the initial failure set would result in not failing at least
one entity bj for a given budget K = p. Hence it wonât fail
the entire set of entities in A âª B.
If an entity in set A is hardened then it would have no effect
in failure prevention of any other entities. Whereas hardening
an entity bm â B might result in failure prevention of an entity
ai â A with IDR aj â bm bn bq provided that entities bn , bq
are also defended. With k = p (and K â¤ |V | = |B|) it can be
ensured that entities to be defended are from set B â² .
To prove the theorem consider that there is a solution to the
Densest p-Subhypergraph problem. Then there exist p vertices
which induces a subgraph which has at least M hyperedges.
Hardening the entities bi â B â² for each vertex vi in the solution
of the Densest p-Subhypergraph problem would then ensure
that at least M entities in set A are protected from failure.
This is because the entities in set A for which the failure
is prevented corresponds to the hyperedges in the induced
subgraph. Thus the number of entities that fail after hardening
p entities is at most |V | + |E| â p â M , solving the ENH
problem. Now consider that there is a solution to the ENH
problem. As previously stated, the entities to be hardened will
always be from set B â² . So defending p entities from set B â²
would result in failure prevention of at least M entities in set
A such that EF â¤ |V | + |E| â p â M . Hence, the vertex
induced subgraph would have at least M hyperedges when
vertices corresponding to the entities hardened are included
in the solution of the Densest p-Subhypergraph problem, thus
solving it.
Theorem 3. For an interdependent network I(A, B, F (A, B))
with n = |A âª B| and F (A, B) having IDRs of form Case II,
it is hard to approximate the ENH problem within a factor of
1
for some Î» > 0.
log(n)Î»
2

Proof: From Theorem 2, Densest p-Subhypergraph problem has been shown to be a special case of the ENH problem
with IDRs of form Case II. Densest p-Subhypergraph problem
1
is proved to be inapproximable within a factor of log(n)
Î»
2
(Î» > 0) in [13]. Hence the theorem follows.
C. Case III: Problem Instance with an Arbitrary Number of
Minterm of Size One
The IDRs of Case III have arbitrary number
P of minterm of
size 1. This can be represented as xi â pq=1 yq , where xi
and yq are entities of network A(B) and B(A) respectively
and the number of minterms are p. The ENH problem with
respect to Case III is NP-complete and is proved in Theorem
4.
Theorem 4. The ENH problem for Case III is NP Complete
Proof: The ENH problem for case III is proved to be NP
complete by giving a reduction from the Set Cover Problem,
a well known NP-complete problem. An instance of the Set
Cover problem includes a set S = {x1 , x2 , ..., xn }, a set S =
{S1 , S2 , ..., Sm } where Si â S and a positive integer M . The
problem asks the question whether there exists at most M
subsets from set S whose union would result in the set S. From
an instance of the set cover problem we create an instance of
the ENH problem in the following way. For each element xi
in set S we add an entity ai in set A. For each subset Si in

set S we add an entity bi in set B. For all subsets in S, say
Sp , Sm , Sn , which has the element xi there is an IDR of form
ai â bm + bn + bl . The values of positive integers k and EF
are set to M and m â M respectively. It is assumed that the
value of K = m.
With similar reasoning as that of Case II it can be shown
that for K = m the maximum number of node failures (i.e.
failure of all entities in A âª B) would occur if Aâ² = â and
B â² = B. This is also the single unique solution to the problem
instance.
The constructed instance also ensures that the entities to
be hardened are from set B â² (Aâ² not considered as it is equal
to â). This is because protecting an entity ai â A would only
result in prevention of its own failure whereas protecting an
entity bj â B would result in failure prevention of its own and
all other entities in set A for which it appears in its IDR.
To begin with the proof, consider that there is a solution
to the Set Cover problem. Then there exist M subsets (or
elements in set S) whose union results in the set S. Hardening
the entities in set B corresponding to the subsets selected
would ensure that all entities in set A are prevented from
failure. This is because for the dependency of each entity
ai â A there exist at least one entity (in set B) that is hardened.
Hence the number of entities that fails after hardening is mâM
which is equal to EF , thus solving the ENH problem. Now,
consider that there is a solution to the ENH problem. As
discussed above the entities to be hardened should be from
set B â² . To achieve EF = m â M with k = M , no entities
in the set A must fail. Hence for each entity ai â A at least
one entity in set B that appears in its IDR has to be hardened.
Thus, it directly follows that the union of subsets in set S
corresponding to the entities hardened is equal to the set S,
solving the Set Cover Problem.
1) Approximation Scheme for Case 3: In this subsection we
provide an approximation algorithm for Case 3 of the problem.
For an interdependent network I(A, B, F (A, B)) with the
initial failed set of entities as Aâ² âª B â² we define Protection
Set of each entity as follows.
Definition: For an entity xi â A âª B the Protection Set is
defined as the entities that would be prevented from failure
by hardening the entity xi when all entities in Aâ² âª B â² fails
initially. This is represented as P (xi |Aâ² âª B â² ).
The Protection Set of each entity can be computed in
O((n + m)2 ) where n and m are the number of entities and
number of minterms respectively in an interdependent network
I(A, B, F (A, B)) .
Theorem 5. For two entities xi , xj â A âª B, P (xi |Aâ² âª B â² ) âª
P (xj |Aâ² âª B â² ) = P (xi , xj |Aâ² âª B â² ) when IDRs are of form
Case III.
Proof: Assume that defending two entities xi and xj
would result in preventing failure of P (xi , xj |Aâ² âª B â² ) entities
with |P (xi |Aâ² âª B â² ) âª P (xj |Aâ² âª B â² )| < |P (xi , xj |Aâ² âª B â² )|.
Then there exist at least one entity xp â
/ P (xi |Aâ² âª B â² ) âª
P (xj |Aâ² âª B â² ) such that itâs failure is prevented only if xi and
xj is protected together. So two entities xm and xn (with xm â
P (xi |Aâ² âªB â² ) and xn â P (xj |Aâ² âªB â² ) or vice versa) have to be

5

present in the IDR of xp . As the IDRs are of form Case III so if
any one of xm or xn is protected then xp is protected, hence
a contradiction. On the other way round P (xi , xj |Aâ² âª B â² )
contains all entities which would be prevented from failure
if xi or xj is defended alone. So it directly follows that
|P (xi |Aâ² âª B â² ) âª P (xj |Aâ² âª B â² )| > |P (xi , xj |Aâ² âª B â² )| is
not possible. Hence the theorem holds.
Theorem 6. There exists an 1 â 1e approximation algorithm
that approximates the ENH problem for Case III.
Proof: The approximation algorithm is constructed by
modeling the problem as Maximum Coverage problem. An
instance of the maximum coverage problem consists of a
set S = {x1 , x2 , ..., xn }, a set S = {S1 , S2 , ..., Sm } where
Si â S and a positive integer M . The objective of the problem
is to find a set S â² â S and |S â² | â¤ M such that âªSi âS Si
is maximized. For a given initial failure set Aâ² âª B â² with
|Aâ² |+|B â² | â¤ K, let P (xi |Aâ² âªB â² ) denote the protection set for
each entity xi â A âª B. We construct a set S = A âª B and for
each entity xi a set Sxi â S such that Sxi = P (xi |Aâ² âª B â² ).
Each set Sxi is added as an element of a set S. The conversion
of the problem to Maximum Coverage problem can be done
in polynomial time. By Theorem 5 defending a set of entities
X â S would result in failure prevention of âªxi âX Sxi entities.
Hence, with the constructed sets S and S and a positive integer
M (with M = k) finding the Maximum Coverage would
ensure the failure protection of maximum number of entities in
A âª B. This is same as the ENH problem of Case III. As there
exists an 1 â 1e approximation algorithm for the Maximum
Coverage problem hence the theorem holds.
D. Case IV: Problem Instance with an Arbitrary Number of
Minterms of Arbitrary Size
The IDRs of Case IV have arbitrary number of minterm
of
Pp arbitrary
Qqj1 size. This can be represented as xi â
j2 =1 yj2 , where xi and yj2 are entities of network
j1 =1
A(B) and B(A) respectively and there are p minterms each
of size qj1 .
Theorem 7. The Entity Hardening problem for Case IV is NP
Complete
Proof: Case II and Case III are special cases of Case
IV. Hence following from Theorem 2 and Theorem 4 the
computational complexity of the Entity Hardening problem is
NP-complete in Case IV.
V.

S OLUTIONS TO

THE

E NTITY H ARDENING P ROBLEM

A. Optimal Solution using Integer Linear Programming
We propose an Integer Linear Program (ILP) that solves
the Entity Hardening problem optimally. Let [G, H] with
G = {g1 , g2 , ..., gn } and H = {h1 , h2 , ..., hm } denote the
entities in set A and B respectively with hi = 0 (gj = 0)
if entity ai (bj ) is alive and hi = 1 (gj = 1) otherwise.
Given an integer k let [G, H] be the solution (with value of 1
corresponding to entities failed initially) that cause maximum
number of entity failure. Two variables xid and yjd are used
in the ILP with xid = 1 (yjd = 1), when entity ai â A
(bj â B) is in a failed state at time step d, and 0 otherwise.
The number of entities to be defended is considered to be k.

It is to be noted that the maximum number cascading steps is
upper bounded by |A| + |B| â 1 = m + n â 1. The objective
function can now be formulated as follows:
min

n
m

X
X
yj(m+nâ1)
xi(m+nâ1) +

(1)

j=1

i=1

The objective in (1) minimizes the number of entities failed
after the cascading failure with the respective constraints for
the Entity Hardening problem as follows:
Constraint Set 1:

n
P

i=1

qxi +

m
P

qyj = k , with qxi , qyj â [0, 1].

j=1

If an entity xi (yj ) is defended then qxi = 1 (qyj = 1) and 0
otherwise.
Constraint Set 2: xi0 â¥ gi â qxi and yi0 â¥ hi â qyi .
This constraint implies that only if an entity is not defended
and gi (hi ) is 1 then the entity will fail at the initial time step.
Constraint Set 3: xid â¥ xi(dâ1) , âd, 1 â¤ d â¤ m + n â 1, and
yid â¥ yi(dâ1) , âd, 1 â¤ d â¤ m + n â 1, in order to ensure
that for an entity which fails in a particular time step would
remain in failed state at all subsequent time steps.
Constraint Set 4: Modeling of the constraint to capture
the cascade propagation for IIM is similar to the constraints
established in [9]. A brief presentation of this constraint is
provided here. Consider an IDR ai â bj bp bl + bm bn + bq of
type Case IV. The following steps are enumerated to depict
the cascade propagation:
Step 1: Replace all minterms of size greater than one with a
variable. In the example provided we have the transformed
minterm as ai â c1 + c2 + bq with c1 â bj bp bl and
c2 â bm bn (c1 , c2 â {0, 1}) as the new IDRs. Note that after
transformation, the original IDR is in the form of Case III and
the introduced IDRs are in the form of Case II.
Step 2: For each variable c, a constraints is added to capture
the cascade propagation. Let N be the number of entities
in the minterm on which c is dependent. In the example
for the variable c1 with IDR c1 â bj bp bl , constraints
y
+y
+yl(dâ1)
c1d â¥ j(dâ1) p(dâ1)
and c1d â¤ yj(dâ1) + yp(dâ1) +
N
yl(dâ1) âd, 1 â¤ d â¤ m + n â 1 are introduced (with
N = 3 in this case). If IDR of an entity is already in
form of Case II, i.e.,ai â bj bp bl then constraints xid â¥
yj(dâ1) +yp(dâ1) +yl(dâ1)
â qxi and xid â¤ yj(dâ1) + yp(dâ1) +
N
yl(dâ1) âd, 1 â¤ d â¤ m + n â 1 are introduced (with N = 3).
These constraints satisfies that if the entity xi is hardened
initially then it is not dead at any time step.
Step 3: Let M be the number of minterms in the transformed IDR as described in Step 1. In the given example
with IDR ai â c1 + c2 + bq constraints of form xid â¥
c1(dâ1) + c2(dâ1) + yq(dâ1) â (M â 1) â qxi and xid â¤
c1(dâ1) +c2(dâ1) +yq(dâ1)
âd, 1 â¤ d â¤ m + n â 1 are introduced.
M
These constraints ensures that even if all the minterms of xi
has at least one entity in dead state then it will be alive if the
entity is hardened initially. For all IDRs of type Case I and
Case III, the constraint discussed in this step is used.

6

B. Heuristic

Algorithm 2: Heuristic Solution to the ENH Problem

In this subsection we provide a greedy heuristic solution to
the Entity Hardening problem. For an interdependent network
I(A, B, F (A, B)) with the initial failed set of entities as
Aâ² âª B â² , Protection Set of each entity has been defined in
the approximation scheme of Case III. To design the heuristic
we define Minterm Coverage Number of each entity in A âª B
as follows:

1
2
3
4

Definition: For an entity xi â A âª B the Minterm Coverage
Number is defined as the number of minterms that can be
removed from F (A, B) without affecting the cascading process
by hardening the entity xi when all entities in Aâ² âª B â² fails
initially. This is represented as M (xi |Aâ² âª B â² ).
Similar to the computation of Protection Set the Minterm
Coverage Number of each entity can be computed in O((n +
m)2 ). With these definitions the heuristic is given in Algorithm
2. The algorithm takes in as input an interdependent network
I(A, B, F (A, B)) with S = AâªB. Step 4-5 is done to reduce
the search space as it directly follows that the set of entities
in Q wouldnât effect the hardening process. In each iteration
of the while loop an entity xd is greedily selected which
when hardened would prevent failure of maximum number of
entities. This ensures that at each step the number of entities
failed is minimized. In case of a tie, among all entities involved
in the tie, the entity having the highest Minterm Coverage
Number is included in the solution. This gives a higher priority
to the entity which when hardened, has more impact on failure
minimization in subsequent iterations of the while loop. The
interdependent network I(A, B, F (A, B)) is updated in steps
13-16 of the algorithm. This takes into account the effect of
hardening an entity in the current iteration on entities hardened
in the following iterations.
Run Time Analysis of Algorithm 2: For this analysis we
consider n to be the total number of entities and m to be
the total number of minterms. Updates in step 4 can be done
in O(m) and step 5 in O(n). The while loop iterates for k
times. In each iteration of the while loop step 7 and step 8 takes
at most O((n + m)2 ) and O(nlog(n)) time respectively. On
branching in step 9, step 10 and step 11 takes O((n + m)2 )
and O(nlog(n)) time respectively. Updates in step 13 takes
O(n) time and in step 14 takes O(n + m) time. Step 12,
16 and 17 runs in constant time. Hence Algorithm 2 runs in
O(k(n + m)2 ) time.
VI.

E XPERIMENTAL R ESULTS

In this section we present the experimental results of the
Entity Hardening problem by comparing the optimal solution
computed using an ILP, and the proposed heuristic algorithm.
The experiments were conducted on real world power grid data
obtained from Platts (www.platts.com), and communication
network data obtained from GeoTel (www.geo-tel.com) of
Maricopa County, Arizona. The data consisted of 70 power
plants and 470 transmission lines in the power network, and
2, 690 cell towers, 7, 100 fiber-lit buildings and 42, 723 fiber
links in the communication network. We identified five nonintersecting geographical regions from the data set and labeled
them from regions 1 through 5. For each of the regions, the
entities of the power and communication network that were

5
6
7
8
9
10
11
12

Data: An interdependent network I(A, B, F(A, B)) (with
S = A âª B), set of entities Aâ² âª B â² failed initially
causing maximum failure in the interdependent network
with |Aâ² | + |B â² | = K and hardening budget k.
Result: Set of hardened entities H.
begin
Initialize S â² â Aâ² âª B â² ;
Initialize H = â;
Update F(A, B) as follows â (a) let Q be the set of
entities that does not fail on failing K entities, (b) remove
IDRs corresponding to entities in set Q, (c) remove from
minterm of entities not in set Q all entities which are in
set Q ;
Update S = S \ Q ;
while (k entities are not hardened) do
For each entity xi â S compute the Protection Set
P (xi |S â² );
Choose the entity xd with highest cardinality of the
set |P (xd |S â² )|;
if (more than one entity has the same highest
cardinality value) then
For each such entity xj compute the Minterm
Coverage Number M (xj |S â² ) ;
Choose the entity xd with highest Minterm
Coverage Number. ;
In case of a tie choose arbitrarily;

16

Update S â S â P (xd |S â² );
Update F(A, B) by removing (i) IDRs corresponding
to all entities in P (xd |S â² ), and (ii) occurrence of
these entities in IDRs of entities not in P (xd |S â² );
if (xd â S â² ) then
Update S â² â S â² â {xd };

17

Update H = H âª xd ;

13
14

15

18

return H ;

located within the geographic region formed the set A and B
respectively. Each region was represented by an interdependent
network I(A, B, F (A, B)). We use the IDR construction rules
as defined in [9] to generate F (A, B).

In all of our simulations IBM CPLEX Optimizer 12.5 to
solve ILPs and Python 3 for heuristic is used. To analyze
the Entity Hardening problem the value of K was set to 8.
The ILP in [9] was used to compute the K most vulnerable
nodes in the network, and the set of failed entities due to
the failure of the K entities was also computed. For the five
regions, when the K = 8 most vulnerable nodes failed, the
total number of failed entities in the network were 28, 23, 28,
28 and 27 respectively. With the K most vulnerable nodes and
final set of failed nodes as input, the ILP and heuristic of the
Entity Hardening problem are compared with k = 1, 3, 5, 7.
The results of these simulations are shown in Figure 1. It is
observed that the heuristic solution differs more from optimal
at higher values of k (factor of 0.5 and 0.67 for Regions 1
and 3 respectively with k = 7). This is primarily because of
the greedy nature of Algorithm 2. However on an average the
heuristic solution differs by a factor of 0.13 from the optimal.

7

14 13

ILP solution
Heuristic

10
8

7
6

6

4

4

3
2

2
0

1
1

14

13

ILP solution
Heuristic

12
10
8

7

4

3

Number of entities failed

3

2

1
1

1

13

ILP solution
Heuristic

12

12
10

8

8
6

6

5

4

3

2
1

3
5
7
Number of entities hardened

(b) Region 2
11

ILP solution
Heuristic

10
8
6
5

4

4

3
2

2

1
1

(c) Region 3

8

11

6

3
1

0

3
5
7
Number of entities hardened

(a) Region 1

0

7

6

0

3
5
7
Number of entities hardened

12

Number of entities failed

13
12

3
5
7
Number of entities hardened

(d) Region 4

Number of entities failed

12

Number of entities failed

Number of entities failed

14

7

ILP solution
Heuristic

7

6
5

5

4
3

3

2
1
0

1

1

3
5
7
Number of entities hardened

(e) Region 5

Fig. 1: Comparison chart of the optimal solution (ILP) with the heuristic by varying number of entities hardened for each identified region

VII.

C ONCLUSION

In this paper we studied the entity hardening problem
in multi-layer networks. We modeled the interdependencies
shared between the networks using IIM, and formulated the
the Entity Hardening problem in this setting. We showed that
the problem is solvable in polynomial time for some cases,
whereas for others it is NP-complete. We evaluated the efficacy
of our heuristic using power and communication network data
of Maricopa County, Arizona. Our experiments showed that
our heuristic almost always produces near optimal results.
R EFERENCES
[1]

[2]

[3]

[4]

[5]

[6]

[7]

S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
âCatastrophic cascade of failures in interdependent networks,â Nature,
vol. 464, no. 7291, pp. 1025â1028, 2010.
J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, âNetworks formed
from interdependent networks,â Nature Physics, vol. 8, no. 1, pp. 40â48,
2011.
J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, âCascade of
failures in coupled network systems with multiple support-dependence
relations,â Physical Review E, vol. 83, no. 3, p. 036116, 2011.
V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and
R. Setola, âModelling interdependent infrastructures using interacting
dynamical models,â International Journal of Critical Infrastructures,
vol. 4, no. 1, pp. 63â79, 2008.
P. Zhang, S. Peeta, and T. Friesz, âDynamic game theoretic model of
multi-layer infrastructure networks,â Networks and Spatial Economics,
vol. 5, no. 2, pp. 147â178, 2005.
M. Parandehgheibi and E. Modiano, âRobustness of interdependent
networks: The case of communication networks and the power grid,â
arXiv preprint arXiv:1304.0356, 2013.
D. T. Nguyen, Y. Shen, and M. T. Thai, âDetecting critical nodes in
interdependent power networks for vulnerability assessment,â 2013.

[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman,
âPower grid vulnerability to geographically correlated failures-analysis
and control implications,â arXiv preprint arXiv:1206.1099, 2012.
[9] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, âIdentification of k most vulnerable nodes in multi-layered network using a new
model of interdependency,â in Computer Communications Workshops
(INFOCOM WKSHPS), 2014 IEEE Conference on. IEEE, 2014, pp.
831â836.
[10] J.-F. Castet and J. H. Saleh, âInterdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,â PloS one, vol. 8, no. 4, p. e60402, 2013.
[11] A. Das, J. Banerjee, and A. Sen, âRoot cause analysis of failures in
interdependent power-communication networks,â in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910â915.
[12] A. Mazumder, C. Zhou, A. Das, and A. Sen, âProgressive recovery from
failure in multi-layered interdependent network using a new model of
interdependency,â in Conference on Critical Information Infrastructures
Security (CRITIS), 2014. Springer, 2014.
[13] M. Hajiaghayi, K. Jain, K. Konwar, L. Lau, I. Mandoiu, A. Russell,
A. Shvartsman, and V. Vazirani, âThe minimum k-colored subgraph
problem in haplotyping and dna primer selection,â in Proceedings of the
International Workshop on Bioinformatics Research and Applications
(IWBRA). Citeseer, 2006.

Identification of K Most Vulnerable Nodes in Multi-layered Network Using a New Model of Interdependency
Arunabha Sen, Anisha Mazumder, Joydeep Banerjee, Arun Das and Randy Compton
Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering Arizona State University Tempe, Arizona 85287 Email: {asen, amazumde, Joydeep.Banerjee, adas22, Randy.Compton}@asu.edu

arXiv:1401.1783v1 [cs.NI] 8 Jan 2014

Abstract--The critical infrastructures of the nation including the power grid and the communication network are highly interdependent. Recognizing the need for a deeper understanding of the interdependency in a multi-layered network, significant efforts have been made by the research community in the last few years to achieve this goal. Accordingly a number of models have been proposed and analyzed. Unfortunately, most of the models are over simplified and, as such, they fail to capture the complex interdependency that exists between entities of the power grid and the communication networks involving a combination of conjunctive and disjunctive relations. To overcome the limitations of existing models, we propose a new model that is able to capture such complex interdependency relations. Utilizing this model, we provide techniques to identify the K most vulnerable nodes of an interdependent network. We show that the problem can be solved in polynomial time in some special cases, whereas for some others, the problem is NP-complete. We establish that this problem is equivalent to computation of a fixed point of a multilayered network system and we provide a technique for its computation utilizing Integer Linear Programming. Finally, we evaluate the efficacy of our technique using real data collected from the power grid and the communication network that span the Maricopa County of Arizona.

I. I NTRODUCTION In the last few years there has been an increasing awareness in the research community that the critical infrastructures of the nation are closely coupled in the sense that the well being of one infrastructure depends heavily on the well being of another. A case in point is the interdependency between the electric power grid and the communication network. The power grid entities, such as the SCADA systems that control power stations and sub-stations, receive their commands through communication networks, while the entities of communication network, such as routers and base stations, cannot operate without electric power. Cascading failures in the power grid, are even more complex now because of the coupling between power grid and communication network. Due to this coupling, not only entities in power networks, such as generators and transmission lines, can trigger power failure, communication network entities, such as routers and optical fiber lines, can also trigger failure in power grid. Thus it is essential that the interdependency between different types of networks be understood well, so that preventive measures can be taken to

avoid cascading catastrophic failures in multi-layered network environments. Recognizing the need for a deeper understanding of the interdependency in a multi-layered network, significant efforts have been made in the research community in the last few years to achieve this goal [1], [2], [3], [4], [5], [6], [7], [8]. Accordingly a number of models have been proposed and analyzed. Unfortunately, many of the proposed models are overly simplistic in nature and as such they fail to capture the complex interdependency that exists between power grid and communication networks. In a highly cited paper [1], the authors assume that every node in one network depends on one and only one node of the other network. However, in a follow up paper [2], the same authors argue that this assumption may not be valid in the real world and a single node in one network may depend on more than one node in the other network. A node in one network may be functional ("alive") as long as one supporting node on the other network is functional. Although this generalization can account for disjunctive dependency of a node in the A network (say ai ) on more than one node in the B network (say, bj and bk ), implying that ai may be "alive" as long as either bi or bj is alive, it cannot account for conjunctive dependency of the form when both bj and bk has to be alive in order for ai to be alive. In a real network the dependency is likely to be even more complex involving both disjunctive and conjunctive components. For example, ai may be alive if (i) bj and bk and bl are alive, or (ii) bm and bn are alive, or (iii) bp is alive. The graph based interdependency models proposed in the literature [3], [4], [5], [9], [6], [7] including [1], [2] cannot capture such complex interdependency between entities of multilayer networks. In order to capture such complex interdependency, we propose a new model using Boolean logic. Utilizing this comprehensive model, we provide techniques to identify the K most vulnerable nodes of an interdependent multilayered network system. We show that the this problem can be solved in polynomial time for some special cases, whereas for some others, the problem is NP-complete. We also show that this problem is equivalent to computation of a fixed point [10] and we provide a technique utilizing Integer Linear Programming

2

to compute that fixed point. Finally, we evaluate the efficacy of our technique using real data collected from power grid and communication networks that span Maricopa County of Arizona.

Entities a1 a2 a3 a4 b1 b2 b3 t0 1 0 0 0 0 0 0 t1 1 0 0 0 0 0 1

Time Steps t2 t3 t4 1 1 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1 1 1 1 1

t5 1 1 1 1 1 1 1

t6 1 1 1 1 1 1 1

II. I NTERDEPENDENCY M ODEL We describe the model for an interdependent network with two layers. However, the concept can easily be generalized to deal with networks with more layers. Suppose that the network entities in layer 1 are referred to as the A type entities, A = {a1 , . . . , an } and entities in layer 2 are referred to as the B type entities, B = {b1 , . . . , bm }. If the layer 1 entity ai is operational if (i) the layer 2 entities bj , bk , bl are operational, or (ii) bm , bn are operational, or (iii) bp is operational, we express it in terms of live equations of the form ai  bj bk bl + bm bn + bp . The live equation for a B type entity br can be expressed in a similar fashion in terms of A type entities. If br is operational if (i) the layer 1 entities as , at , au , av are operational, or (ii) aw , az are operational, we express it in terms of live equations of the form br  as at au av + aw az . It may be noted that the live equations only provide a necessary condition for entities such as ai or br to be operational. In other words, ai or br may fail independently and may be not operational even when the conditions given by the corresponding live equations are satisfied. A live equation in general will have the following tj Ti form: xi  j =1 k=1 yj,k where xi and yj,k are elements of the set A (B ) and B (A) respectively, Ti represents the number of min-terms in the live equation and tj refers to the size of the j -th min-term (the size of a min-term is equal to the number of A or B elements in that min-term). In the example ai  bj bk bl + bm bn + bp , Ti = 3, t1 = 3, t2 = 2, t3 = 1, xi = ai , y2,1 = bm , y2,2 = bp . We refer to the live equations of the form ai  bj bk bl + bm bn + bp also as First Order Dependency Relations, because these relations express direct dependency of the A type entities on B type entities and vice-versa. It may be noted however that as A type entities are dependent on B type entities, which in turn depends on A type entities, the failure of some A type entities can trigger the failure of other A type entities, though indirectly, through some B type entities. Such interdependency creates a cascade of failures in multilayered networks when only a few entities of either A type or B type (or a combination) fails. We illustrate this with the help of an example. The live equations for this example is shown in table I.
Power Network a1  b1 + b2 a2  b1 b3 + b2 a3  b1 b2 b3 a4  b1 + b2 + b3 Communication Network b1  a1 + a2 a3 b2  a1 + a3 b3  a1 a2 --

TABLE II: Time Stepped Cascade Effect for a Multilayer Network

Fig. 1: Cascading failures reach steady state after p time steps

As shown in table II, the failure of only one entity a1 at time step t0 triggered a chain of failures that resulted in the failure of all the entities of the network after by timestep t4 . A table entry of 1 indicates that the entity is "dead". In this example, the failure of a1 at t0 triggered the failure of b3 at t1 , which in turn triggered the failure of a3 at t2 . The failure of b3 at t1 was due to the dependency relation b3  a1 a2 and the failure of a3 at t2 was due to the dependency relation a3  b1 b2 b3 . The cascading failure process initiated by failure (or death) of a subset of A type entities at timestep t0 , A0 d and 0 till it reaches its final steady a subset of B type entities Bd state is shown diagrammatically in figure 1. Accordingly, a multilayered network can be viewed as a "closed loop" control system. Finding the steady state after an initial failure in this case is equivalent of computing the fixed point of a function p p p F (.) such that F (Ap d  Bd ) = Ad  Bd , where p represents the number of steps when the system reaches the steady state. We define a set of K entities in a multi-layered network as most vulnerable, if failure of these K entities triggers the failure of the largest number of other entities. The goal of the K most vulnerable nodes problem is to identify this set of 0 nodes. This is equivalent to identifying A0 d  A, Bd  B , that p p 0 maximizes |Ad  Bd |, subject to the constraint that |A0 d  Bd |  K. The dependency relations (live equations) can be formed either after careful analysis of the multilayer network along the lines carried out in [8], or after consultation with the engineers of the local utility and internet service providers. III. C OMPUTATIONAL C OMPLEXITY AND A LGORITHMS Based on the number and the size of the min-terms in the dependency relations, we divide them into four different cases as shown in Table III. The algorithms for finding the K most vulnerable nodes in the multilayer networks and computation complexity for each of the cases are discussed in the following four subsections.
Case Case I Case II Case III Case IV No. of Min-terms 1 1 Arbitrary Arbitrary Size of Min-terms 1 Arbitrary 1 Arbitrary

TABLE I: Live equations for a Multilayer Network

TABLE III: Equation Types for Dependency Relations

3

A. Case I: Problem Instance with One Min-term of Size One In this case, a live equation in general will have the following form: xi  yj where xi and yj are elements of the set A (B ) and B (A) respectively. In the example ai  bj , xi = ai , y1 = bj . It may be noted that a conjunctive implication of the form ai  bj bk can also be written as two separate implications ai  bj and ai  bk . However, such cases are considered in Case II and is excluded from consideration in Case I. The exclusion of such implications implies that the entities that appear on the LHS of an implication in Case I are unique. This property enables us to develop a polynomial time algorithm for the solution of the K most vulnerable node problem for this case. We present the algorithm next. Algorithm 1 Input: (i) A set S of implications of the form of y  x, where x, y  A  B , (ii) An integer K. Output: A set V where |V | = K and V  A  B such that failure of entities in V at time step t0 results in failure of the largest number of entities in A  B when the steady state is reached. Step 1. We construct a directed graph G = (V, E ), where V = A  B . For each implication y  x in S , where x, y  A  B , we introduce a directed edge (x, y )  E . Step 2. For each node xi  V , we construct a transitive closure set Cxi as follows: If there is a path from xi to some node yi  V in G, then we include yi in Cxi . It may be recalled that |A| + |B | = n + m. So, we get n + m transitive closure sets Cxi , 1  i  (n + m). We call each xi to be the seed entity for the transitive closure set Cxi . Step 3. We remove all the transitive closure sets which are proper subsets of some other transitive closure set. Step 4. Sort the remaining transitive closure sets Cxi , where the rank of the closure sets is determined by the cardinality of the sets. The sets with a larger number of entities are ranked higher than the sets with a fewer number of entities. Step 5. Construct the set V by selecting the seed entities of the top K transitive closure sets. If the number of remaining transitive closure sets is less than K (say, K ), arbitrarily select the remaining entities. Time complexity of Algorithm 1: Step 1 takes O(n + m + |S |) time. Step 2 can be executed in O((n + m)3 ) time. Step 3 takes at most O((n + m)2 ) time. Step 4 sorts at most |S | entries, a standard sorting algorithm takes O(|S | log |S |) time. Selecting K entities in step 5 takes O(K) time. Since |S |  n + m, hence the overall time complexity is O((n + m)3 ) Theorem 1. For each pair of transitive closure sets Cxi and Cxj produced in step 2 of algorithm 1, either Cxi  Cxj =  or Cxi  Cxj = Cxi or Cxi  Cxj = Cxj , where xi = xj . Proof: Consider, if possible, that there is a pair of transitive closure sets Cxi and Cxj produced in step 2 of algorithm 1, such that Cxi  Cxj =  and Cxi  Cxj = Cxi and Cxi  Cxj =

Cxj . Let xk  Cxi  Cxj . This implies that there is a path from xi to xk (path1 ) as well as there is a path from xj to xk , (path2 ). Since, xi = xj and Cxi  Cxj = Cxi and Cxi  Cxj = Cxj , there is some xl in the path1 such that xl also belongs to path2 . W.l.o.g, let us consider that xl be the first node in path1 such that xl also belongs to path2 . This implies that xl has in-degree greater than 1. This in turn implies that there are two implications in the set of implications S such that xl appears in the L.H.S of both. This is a contradiction because this violates a characteristic of the implications in Case I. Hence, our initial assumption was wrong and the theorem is proven. Theorem 2. Algorithm 1 gives an optimal solution for the problem of selecting K most vulnerable entities in a multilayer network for case I dependencies. Proof: Consider that the set V returned by the algorithm is not optimal and the optimal solution is VOP T . Let us consider there is a entity xi  A  B such that xi  VOP T \ V . Evidently, (i) Cxi was either deleted in step 3 or (ii) |Cxi | is less than the cardinalities of all the transitive closure sets with seed entities xj  V , because our algorithm did not select xi . Hence, in both cases, replacing any entity xj  V by xi reduces the total number of entities killed. Thus, the number of dead entities by the failure of entities in VOP T is lesser than that caused by the failure of the entities in V , contradicting the optimality of VOP T . Hence, the algorithm does in fact return the optimal solution. B. Case II: Problem Instance with One Min-term of Arbitrary Size In this case, a live equation in general will have the q following form: xi  k=1 yj where xi and yj are elements of the set A (B ) and B (A) respectively, q represents the size of min-term. In the example ai  bj bk bl , q = 3, xi = ai , y1 = bj , y2 = bk , y3 = bk . 1) Computational Complexity: We show that computation of K most vulnerable nodes (K-MVN) in a multilayer network is NP-complete in Case II. We formally state the problem next. Instance: Given a set of dependency relations between A and B type entities in the form of live equations xi  q k=1 yj , integers K and L. Question: Is there a subset of A and B type entities of size at most K whose "death" (failure) at time t0 , triggers a cascade of failures resulting in failures of at least L entities, when the steady state is reached? Theorem 3. The K-MVN problem is NP-complete. Proof: We prove that the K-MVN problem is NP-complete by giving a transformation for the vertex cover (VC) problem. An instance of the vertex cover problem is specified by an undirected graph G = (V, E ) and an integer R. We want to know if there is a subset of nodes S  V of size at most R, so that every edge has at least one end point in S . From an instance of the VC problem, we create an instance of the

4

K-MVN problem in the following way. First, from the graph G = (V, E ), we create a directed graph G = (V, E ) by replacing each edge e  E by two oppositely directed edges e1 and e2 in E (the end vertices of e1 and e2 are same as the end vertices of e). Corresponding to a node vi in G that has incoming edges from other nodes (say) vj , vk and vl , we create a dependency relation (live equation) vi  vj vk vl . We set K = R and L = |V |. The corresponding death equation is of the form v ¯i  v ¯ ¯ ¯l (obtained by taking negation j +v k +v of the live equation). We set K = R and L = |V |. It can now easily be verified that if the graph G = (V, E ) has a vertex cover of size R iff in the created instance of K-MVN problem death (failure) of at most K entities at time t0 , will trigger a cascade of failures resulting in failures of at least L entities, when the steady state is reached. 2) Optimal Solution with Integer Linear Programming: In this case, we can find and optimal solution to the KMVN problem using Integer Linear Programming (ILP). We associate binary indicator variables xi (yi ) to capture the state of the entities ai (bi ). xi (yi ) is 1 when ai (bi ) is dead and 0 otherwise. Since we want find the set of K entities whose failure at time step t0 triggers cascading failure resulting in the failure of the largest number of entities, the objective of the n m ILP can be written as follows maximize i=1 xi + i=1 yi It may be noted that the variables in the objective function do not have any notion of time. However, cascading failure takes place in time steps, ai triggers failure of bj at time step t1 , which in turn triggers failure of ak in time step t2 and so on. Accordingly, in order to capture the cascading failure process, we need to introduce the notion of time into the variables of the ILP. If the numbers of A and B type entities are n and m respectively, the steady state must be reached by time step n + m - 1 (cascading process starts at time step 0, t0 ). Accordingly, we introduce n + m versions of the variables xi and yi , i.e., xi [0], . . . , xi [n + m - 1] and yi [0], . . . , yi [n + m - 1]. To indicate the state of entities ai and bi at times t0 , . . . , tn+m-1 . The objective of the ILP is now changed to
n m

The optimal solution to K-MVN problem for Case II can be found by solving the above ILP. C. Case III: Problem Instance with an Arbitrary Number of Min-terms of Size One A live equation in this special case will have the following q form: xi  j =1 yj where xi and yj are elements of the set A (B ) and B (A) respectively, q represents the number of minterms in the live equation. In the example ai  bj + bk + bl , q = 3, xi = ai , y1 = bj , y2 = bk , y3 = bl . 1) Computational Complexity: We show that a special case of the problem instances with an arbitrary number of min-terms of size one is same as the Subset Cover problem (defined below), which is proven to be NPcomplete. We define Implication Set(A) to be the Ti set of all implications of the form ai  j =1 bj and Implication Set(B ) to be the set of all implications of the Ti form bi  j =1 aj . Now consider a subset of the set of problem instances with an arbitrary number of min-terms of size one where either Implication Set(A) =  or Implication Set(B ) = . Let A = {ai |ai is the element on the LHS of an implication} in the Implication Set(A). The set B is defined accordingly. If Implication Set(B ) =  then B = . In this case, failure of any ai , 1  i  n type entities will not cause failure of any bj , 1  j  m type entities. Since an adversary can cause failure of only K entities, the adversary would like to choose only those K entities that will cause failure of the largest number of entities. In this scenario, there is no reason for the adversary to attack any ai , 1  i  n type entities as they will not cause failure of any bj , 1  j  m type entities. On the other hand, if the adversary attacks K bj type entities, not only those K bj type entities will be destroyed, some ai type entities will also be destroyed due to the implications in the Implication Set(A). As such the goal of the adversary will be to carefully choose K bj , 1  j  m type entities that will destroy the largest number of ai type entities. In its abstract form, the problem can be viewed as the Subset Cover problem. Subset Cover Problem Instance: A set S = {s1 , . . . , sm }, a set S of m subsets of S , i.e., S = {S1 , . . . , Sr }, where Si  S, i, 1  i  r, integers p and q . Question: Is there a p element subset S of S (p < n) that completely covers at least q elements of the set S ? (A set S is said to be completely covering an element Si , i, 1  i  m of the set S , if S  Si = Si , i, 1  i  m.) The set S in the subset cover problem corresponds to the set B = {b1 , . . . , bm }, and each set Si , 1  i  r corresponds to an implication in the ImplicationS et(A) and comprises of the bj 's that appear on the RHS of the implication. The goal of the problem is to select a subset B of B that maximizes the number of Si 's completely covered by B .

maximize
i=1

xi [n + m - 1] +
i=1

yi [n + m - 1]

Subject to the constraint that no more than K entities can fail at time t0 . n m Constraint 1: i=1 yi [0]  K In order i=1 xi [0] + to ensure that the cascading failure process conforms to the dependency relations between type A and B entities, additional constraints must be imposed. Constraint 2: If an entity fails at time fails at time step p, (i.e., tp ) it should continue to be in the failed state at all time steps t > p. That is xi (t)  xi (t - 1), t, 1  t  n + m - 1. Same constraint applies to yi (t). Constraint 3: The dependency relation (death equation) ¯ ¯ ¯ a ¯i  b j + bk + bl can be translated into a linear constraint in the following way xi (t)  yj (t - 1)+ yk (t - 1)+ yl (t - 1), t, 1  t  n + m - 1.

5

Theorem 4. The Subset Cover problem is NP-complete. Proof: We prove that the Subset Cover problem is NPcomplete by giving a transformation from the well known Clique problem. It may be recalled that an instance of the Clique problem is specified by a graph G = (V, E ) and an integer K . The decision question is whether or not a clique of size at least K exists in the graph G = (V, E ). We show that a clique of size K exists in graph G = (V, E ) iff the Subset Cover problem instance has a p element subset S of S that completely covers at least q elements of the set S . From an instance of the Clique problem, we create an instance of the Subset Cover problem in the following way. Corresponding to every vertex vi , 1  i  n of the graph G = (V, E ) (V = {v1 , . . . , vn }), we create an element in the set S = {s1 , . . . , sn }. Corresponding to every edge ei , 1  i  m, we create m subsets of S , i.e., S = {S1 , . . . , Sm }, where Si corresponds to a two element subset of nodes, corresponding to the end vertices of the edge ei . We set the parameters p = K and q = K (K - 1)/2. Next we show that in the instance of the subset cover problem created by the above construction process, a p element subset S of S exists that completely covers at least q elements of the set S , iff the graph G = (V, E ) has a clique of size at least K . Suppose that the graph G = (V, E ) has a clique of size K . It is clear that in the created instance of the subset cover problem, we will have K (K - 1)/2 elements in the set S , that will be completely covered by a K element subset of the set S . The K element subset of S corresponds to the set of K nodes that make up the clique in G = (V, E ) and the K (K - 1)/2 elements in the set S corresponds to the edges of the graph G = (V, E ) that corresponds to the edges of the clique. Conversely, suppose that the instance of the Subset Cover problem has K element subset of S that completely covers K (K - 1)/2 elements of the set S . Since the elements of S corresponds to the edges in G, in order to completely cover K (K - 1)/2 edges, at least K nodes (elements of the set S ) will be necessary. As such, this set of K nodes will constitute a clique in the graph G = (V, E ). 2) Optimal Solution with Integer Linear Programming: If q the live equation is in the form xi  k=1 yj then the "death equation" (obtained by taking negation of the live equation) q will be in the product form x ¯i  j =1 y ¯j . If the live equation is given as ai  bj + bk , then the death equation will be given ¯ ¯ as a ¯i  b j bk . By associating binary indicator variables xi and yi to capture the state of the entities ai and bi , we can follow almost identical procedure as in Case II, with only one exception. It may be recalled that in Case II, the death equations such ¯ ¯ as a ¯i  b j + bk was translated into a linear constraint xi (t)  yj (t - 1) + yk (t - 1), t, 1  t  n + m - 1. However a similar translation in Case III, with death equations such as ¯ ¯ a ¯i  b j bk , will result in a non-linear constraint of the form xi (t)  yj (t - 1)yk (t - 1), t, 1  t  n + m - 1. Fortunately, a non-linear constraint of this form can be replaced a linear constraint such as 2xi (t)  yj (t - 1) + yk (t - 1), t, 1 

t  n + m - 1. After this transformation, we can compute the optimal solution using integer linear programming. D. Case IV: Problem Instance with an Arbitrary Number of Min-terms of Arbitrary Size 1) Computational Complexity: Since both Case II and Case III are special cases of Case IV, the computational complexity of finding the K most vulnerable nodes in the multilayer network in NP-complete in Case IV also. 2) Optimal Solution with Integer Linear Programming: The optimal solution to this version of the problem can be computed by combining the techniques developed for the solution of the versions of the problems considered in Cases II and III. IV. E XPERIMENTAL RESULTS We applied our model to study multilayer vulnerability issues in Maricopa County, the most densely populated county of Arizona with approximately 60% of Arizonas population residing in it. Specifically, we wanted to find out if some regions of Maricopa County were more vulnerable to failure than some other regions. The data for our multi-layered network were obtained from different sources. We obtained the data for the power network (network A) from Platts (http://www.platts.com/). Our power network dataset consists of 70 power plants and 470 transmission lines. Our communication network (network B) data were obtained from GeoTel (http://www.geo-tel.com/). Our communication network data consists of 2, 690 cell towers and 7, 100 fiber-lit buildings as well as 42, 723 fiber links. Snapshots of our power network data and communication network data are shown in figure 2. In the power network snapshot of sub-figure(a), the orange markers show locations of powerplants while the yellow continuous lines represent the transmission lines. In the communication network snapshot of sub-figure (b) the pink markers show the location of fiber-lit buildings, the orange markers show the location of cell towers and the green continuous lines represent the fiber links. In our dataset, `load' in the Power Network is divided into Cell towers and Fiber-lit buildings. Although there exists various other physical entities which also draw electric power and hence can be viewed as load to the power network, as they are not relevant to our study on interdependency between power and communication networks, we ignore such entities. Thus in network A, we have the three types of Power Network Entities (PNE's) - Generators, Load (consisting of Cell towers and Fiber-lit buildings) and Transmission lines (denoted by a1 , a2 , a3 respectively). For the Communication Network, we have the following Communication Network Entities (CNE's) - Cell Towers, Fiber-lit buildings and Fiber links (denoted by b1 , b2 , b3 respectively). We consider the Fiber-lit buildings as a communication network entities as they house routers which definitely are communication network entities. From the raw data we construct Implication Set(A) and Implication Set(B), by following the rules stated below: Rules: We consider that a PNE is dependent on a set of CNEs for being in the active state (`alive') or being in the

6

(a) Snapshot of Power Network in Maricopa County

(b) Snapshot of Communication Network in Maricopa County

Fig. 2: Snapshots of power network and communication network in Maricopa County)

inactive state (`dead'). Similarly, a CNE is dependent on a set of PNEs for being active or inactive state. For simplicity we consider the live equations with at most two minterms. For the same reason we consider the size of each minterm is at most two. Generators (a1,i , 1  i  p, where p is the total number of generators): We consider that each generator (a1.i ) is dependent on the nearest Cell Tower (b1,j ) or the nearest Fiber-lit building (b2,k ) and the corresponding Fiber link (b3,l ) connecting b2,k and a1,i . Thus, we have a1,i  b1,j + b2,k × b3,l Load (a2,i , 1  i  q , where q is the total number of loads): We consider that the loads in the power network do not depend on any CNE. Transmission Lines (a3,i , 1  i  r, where r is the total number of transmission lines): We consider that the transmission lines do not depend on any CNE. Cell Towers (b1,i , 1  i  s, where s is the total number of cell towers): We consider the cell towers depend on the nearest pair of generators and the corresponding transmission line connecting the generator to the cell tower. Thus, we have b1,i  a1,j × a3,k + a1,j × a3,k Fiber-lit Buildings (b2,i , 1  i  t, where t is the total number of fiber-lit buildings): We consider that the fiber-lit buildings depend on the nearest pair of generators and the corresponding transmission lines connecting the generators to the cell tower. Thus, we have b2,i  a1,j × a3,k + a1,j × a3,k Fiber Links (b3,i , 1  i  u, where u is the total number of fiber links)): We consider that the fiber links do not depend on any PNE. Because of experimental resource limitation, we have considered 5 regions of Maricopa County for our experiments. We used IBM CPLEX Optimizer 12.5 to run the formulated ILP's on the experimental dataset. We show our results in the figure 3. We observe that in each of the regions there is a specific budget threshold beyond which each additional increment in budget results in the death of only one entity. The reason for this behavior is our assumption that entities such as the transmission lines and the fiberlinks are not dependent on any other entities. We notice that all the entities of the two networks can be destroyed with a budget of about 60%

of the number of entities of the two networks A and B . Most importantly, we find that the degree of vulnerability of all the five regions considered in our study are close and no one region stands out as being extremely vulnerable.

Fig. 3: Experimental results of failure vulnerability across five regions of Maricopa county

R EFERENCES
[1] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin, "Catastrophic cascade of failures in interdependent networks," Nature, vol. 464, no. 7291, pp. 1025­1028, 2010. [2] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, "Networks formed from interdependent networks," Nature Physics, vol. 8, no. 1, pp. 40­48, 2011. [3] J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, "Cascade of failures in coupled network systems with multiple support-dependence relations," Physical Review E, vol. 83, no. 3, p. 036116, 2011. [4] V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and R. Setola, "Modelling interdependent infrastructures using interacting dynamical models," International Journal of Critical Infrastructures, vol. 4, no. 1, pp. 63­79, 2008. [5] P. Zhang, S. Peeta, and T. Friesz, "Dynamic game theoretic model of multi-layer infrastructure networks," Networks and Spatial Economics, vol. 5, no. 2, pp. 147­178, 2005. [6] M. Parandehgheibi and E. Modiano, "Robustness of interdependent networks: The case of communication networks and the power grid," arXiv preprint arXiv:1304.0356, 2013. [7] D. T. Nguyen, Y. Shen, and M. T. Thai, "Detecting critical nodes in interdependent power networks for vulnerability assessment," 2013. [8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman, "Power grid vulnerability to geographically correlated failures-analysis and control implications," arXiv preprint arXiv:1206.1099, 2012. [9] J.-F. Castet and J. H. Saleh, "Interdependent multi-layer networks: Modeling and survivability analysis with applications to space-based networks," PloS one, vol. 8, no. 4, p. e60402, 2013. [10] A. Fudenberg and J. Tirole, Game Theory. Ane Books, 2010.

Soc. Netw. Anal. Min. DOI 10.1007/s13278-012-0072-x

ORIGINAL ARTICLE

A system for ranking organizations using social scale analysis
Sukru Tikves · Sujogya Banerjee · Hamy Temkit · Sedat Gokalp · Hasan Davulcu · Arunaba Sen · Steven Corman · Mark Woodward Shreejay Nair · Inayah Rohmaniyah · Ali Amin
·

Received: 20 December 2011 / Revised: 4 April 2012 / Accepted: 20 April 2012 Ó Springer-Verlag 2012

Abstract In this paper, we utilize feature extraction and model-fitting techniques to process the rhetoric found in the web sites of 23 Indonesian Islamic religious organizations to profile their ideology and activity patterns along a hypothesized radical/counter-radical scale, and present an end-to-end system that is able to help researchers to visualize the data in an interactive fashion on a timeline. The subject data of this study is 37,000 articles downloaded from the web sites of these organizations dating from 2001 to 2011. We develop algorithms to rank these organizations by assigning them to probable positions on the scale. We show that the developed Rasch model fits the data using Andersen's LR-test. We create a gold standard of the ranking of these organizations through an expertise elicitation tool. We compute expert-to-expert agreements, and we present experimental results comparing the performance of three baseline methods to show that the Rasch model not only outperforms the baseline methods, but it is also the only system that performs at expert-level accuracy.
S. Tikves (&) Á S. Banerjee Á H. Temkit Á S. Gokalp Á H. Davulcu Á A. Sen Á S. Corman Á M. Woodward Á S. Nair Arizona State University, P.O. Box 87-8809, Tempe, AZ 85281, USA e-mail: sukru@asu.edu; stikves@asu.edu S. Banerjee e-mail: sujogya@asu.edu H. Temkit e-mail: mtemkit@asu.edu S. Gokalp e-mail: sgokalp@asu.edu H. Davulcu e-mail: hdavulcu@asu.edu A. Sen e-mail: asen@asu.edu

1 Introduction Being able to asses information on radical and moderate actors in a geographic area is an important research topic for national security. Radicalism is the ideological conviction that it is acceptable, and in some cases, obligatory to use violence to effect profound political, cultural and religious transformations and change the existing social order fundamentally. Muslim radical movements have complex origins and depend on diverse factors that enable translation of their radical ideology into social, political and religious movements. Crelinste (2002), in his work, states that ``both violence and terrorism possess a logic and grammar that must be understood if we are to prevent or control them''. Therefore, analysis of Muslim radical and counter-radical movements requires attention to the global, national and local social, economic and political contexts in which they are located. Similarly, in the Islamic context, counter-radical discourse takes various different forms; discursive and narrative refutations of extremist claims, symbolic action such as ritual and other religious and
S. Corman e-mail: scorman@asu.edu M. Woodward e-mail: mataram@asu.edu S. Nair e-mail: snair8@asu.edu I. Rohmaniyah Center for Religious and Cross Cultural Studies, Gadjah Mada University, Yogyakarta, Indonesia e-mail: rochmaniyah@yahoo.com A. Amin State College of Islamic Studies (STAIN), Manado, Indonesia e-mail: aleejtr77@yahoo.com

123

S. Tikves et al.

cultural practices, and Islamic arguments for pluralism, peaceful relations with non-Muslims, democracy, etc. The most effective counter-radicals are likely to be religiously conservative Muslims. Effective containment and defeat of radicalism depends on our ability to recognize various levels of radicalization, and detection of counter-radical voices. In our previous work (Davulcu et al. 2010), we attempted a clustering approach to obtain ``natural groupings'' of a number of local non-government religious social movements and organizations in Indonesia. Social scientists on our team observed that clustering results were not fully able to separate all counter-radical or radical organizations into pure clusters. Pure radical clusters were easily identified due to high similarity among their support for violent practices. Pure counterradical clusters were identified due to their strong reactionary opposition to violent practices through protests and rhetoric. But the rest of the groupings were mixed. We realized that binary labeling as counter-radical or radical does not capture the overlap, movement and interactivity among these organizations. In this paper, we hypothesize that both counterradical and radical movements in Muslim societies exhibit distinct combinations of discrete states comprising various social, political, and religious beliefs, attitudes and practices, that can be mapped to a latent linear continuum or a scale. Using such a scale, an analyst can determine where exactly along the spectrum any particular group lies, and also potentially where it is heading with its rhetoric and activity. Given the complex nature of the task, such as regional differences in local cultures, beliefs and practices, and in the absence of readily available high-accuracy parsers, highly structured religio-social ontologies, and information extraction systems; we decided to devise a multi-lingual nonlinguistic text processing pipeline that relies on only statistical modeling of keyword frequency and co-occurrence information. However, we designed the system to be able to incorporate additional information extracted from the text, if available. For example, named entity recognition (NER), machine translation, and GIS-based location lookup information are part of the user-interface presentation. We (Tikves et al. 2011) worked with social scientists on our team to come up with an orthogonal model comprising of two primary dimensions. Both dimensions, (1) radical/ counter-radical and (2) violent/non-violent, are characterized as latent, partial orders of discrete beliefs and practices based on a generalization of item order in Guttman scaling (Guttman 1950) using a Rasch model (Andric 1988). A true Guttman scale is a deterministic process, i.e., if a social movement subscribes to a certain belief or practice, then it must also agree with all lower-order practices and beliefs on the scale. Of course, such perfect order is rare in the social world. The Rasch model provides a probabilistic framework for Guttman scales to accommodate for incomplete observations and measurement errors.

We have designed a web-based system to visualize this orthogonal model. The web tools provided by the system allows drilling down on specific data, and plotting the trends and trajectories of organizations on a timeline. It consists of several modules: an off-line web mining, and data-processing pipeline, two web services for application logic, and an AJAX-based presentation layer. The web-based interface built for this study can be accessed through the web site at http://www.demo.minerva-project.org. In this paper, we present several scenarios with this tool in Sect. 5. In this paper, we present feature extraction, feature selection, and model-fitting techniques to process the rhetoric found in the web sites of 23 religious Indonesian organizations--comprising a total of 37,000 articles dating from 2001 to 2011. We aim to identify their ideology and activity patterns along a hypothesized radical/counter-radical scale, and rank them to probable positions on this scale (McPhee 1995). The automated ordering of organizations is formed by ranking the organizations according to their estimated positions on the latent scale. We used the eRm1 package to fit the Rasch model on this data set, and identify organizations' positions based on maximum likelihood estimation (Le Cam 1990). We show that the model fits the data using the Andersen's likelihood ratio test (LR-test) (Hessen 2010). We also created a gold standard of the ranking of these organizations through an expert-opinion elicitation tool, and through the opinions of three ethnographers on our team who collectively possess 35 years of scholarly expertise on Indonesia and Islam. We computed expert-to-gold standard agreements, as well as compared the performance of three different baseline computational methods to show that the Rasch model presented here not only performs the best among the baseline methods but that it is also the only method that performs at an expert level of accuracy. 1.1 Organization of the paper Next section provides an introduction to the theory of Guttman scaling and Rash models. Section 3 defines the problem, presents the system architecture, and the methods used to solve the problem. Section 4 describes the Indonesian corpus, expert-opinion elicitation tool, baseline computational methods, and experimental evaluations. Section 5 discusses the user-interface designed for navigating our findings. Section 6 concludes the paper.

2 Introduction of Guttman scaling and Rasch model In social science, scaling is a process of measuring and ordering entities called subjects, based on their qualitative
1

http://www.r-forge.r-project.org/projects/erm/.

123

A system for ranking organizations using social scale analysis

attributes called items. In general, subjects are requested to respond to surveys conducted by means of structured interviews or questionnaires. Items are presented to the subjects in form of questions. Statistical analysis of the response of the subjects on the questions about items are used in scaling the subjects. Some of the widely followed scaling procedure in social science surveys are Likert scale (Likert 1932), Thurnstone scale (Thurnstone 1928), and Guttman scale (McIver 1981). In Likert scale, subjects indicate their magnitude of agreement or disagreement about an item (from strongly agree to strongly disagree) on a five- to ten-point scale. On the other hand, Thurnstone scale is a formal method of ordering the attitudes of the subjects toward the items. Guttman scaling procedure orders both the subjects and the items simultaneously with respect to some underlying cumulative continuum. In this paper, we follow the Guttman scaling process to rank the organizations based on their response on the radical and counter-radical keywords. 2.1 Guttman scaling A Guttman scale (Guttman 1950) presents a number of items to which each subject is requested to provide a dichotomous response, e.g., agree/disagree, yes/no, or 1/0. This scaling procedure is based on the premise that the items have strict orders (i.e., the items are presented to the subjects ranked according to the level of the item's difficulty). An item ``A'' is said to be ``more difficult'' than an item ``B'', if any subject answering ``yes'' on item ``A'' implies that the subject will also answer ``yes'' on item ``B''. A subject who responds to an item positively is expected to respond positively to all the items of lesser difficulty. For example, to find out how extreme a subject's view is on Guttman scale, the subject is presented with the following series of items in question form. (1) Are you willing to permit immigrants to live in your country? (2) Are you willing to permit immigrants to live in your community? (3) Are you willing to permit immigrants to live in your neighborhood? (4) Are you willing to permit immigrants to live to your next door? (5) Are you willing to permit your child to marry an immigrant? If the items form a Guttman scale, any subject agreeing with any item in this series, will also agree with other items of lower rank-order in this series. Guttman scale is a deterministic process and the score of a subject depends on the number of affirmative responses he has made on the items. So, a score of 2 for a subject in the above Guttman scale not only means he has given affirmative response to two of the questions or items but also indicates that he agrees with two particular questions, namely the first and second. Scores in Guttman scale can also be interpreted as the ``ability'' of a subject in answering questions sorted in increasing order of ``difficulty''. These scores when presented on an underlying scale,

give us an ordering of the subjects based on their ``ability'' also. The objective of our paper is to order the Indonesian Islamic organizations based on their views on religio-social keywords which have an inherent ordering. For example, two such keywords are ``Quran'' and ``Sharia''. An organization supporting ``Sharia'' will also likely to ``believe in Quran''. So it makes sense to use Guttman scaling procedure to rank the organizations and their beliefs and practices. One drawback of Guttman scale is that it is deterministic and assumes a strict ordering of the items. In real world, it is difficult to order all the items in such a strict level of increasing difficulty, therefore, perfect scales are not often observed in practice. Furthermore, many times, the order of the items are not known since they are not straightforwardly comparable. In addition, measurement errors might lead to responses that do not strictly fit the ordering. As a result, we can no longer conclude deterministically that if a subject answers a question affirmative, whether she will be able to give affirmative answers to other questions of lower order in the same questionnaire. We use Rasch model to overcome this drawback by taking into account measurement error. 2.2 Rasch model Rasch model (Andric 1988) provides a probabilistic framework for Guttman scales. In Rasch model, the probability of a specified binary response (e.g., a subject agreeing or disagreeing to an item) is modeled as a function of subject's and item's parameters. Specifically in the simple Rasch model, the probability of a positive response (yes) is modeled as a logistic function of the difference between the subject and item's parameters. Item parameters pertain to the difficulty of items while subject parameters pertain to the ability of subjects who are assessed. A subject of higher ability, related to the difficulty of an item, has higher probability to respond to a question affirmatively. In this paper, Rasch models are used to assess the organizations degree of being radical or counter-radical based on the religio-social keywords (items) appearing in their rhetoric. Rasch model also maps the responses of the subjects to the items in binary or dichotomous format, i.e., 1 or 0. Let Bernoulli variable Xvi denotes the response of a subject v to the item i, variable hv denotes the parameter of ``ability'' of the subject v and bi denotes the parameter of ``difficulty'' of an item i. According to the simple Rasch model, the probability that the subject v responds 1 for item i is given by: PðXvi ¼ 1jhv ; bi Þ ¼ expðhv À bi Þ : 1 þ expðhv À bi Þ

Rasch model assumes that the data under analysis have the following properties.

123

S. Tikves et al.

Unidimensionality P(xvi = 1|hv, bi, a) = P(xvi = 1|hv, bi), i.e., the response probability does not depend on other variable 2. Sufficiency sum of responses contains all information on ability of a subject, regardless which item it has responded 3. Conditional independence for a fixed subject, there is no correlation between any two items 4. Monotonicity response probability increases with higher values of h, i.e., subject's ability. P Items with si = n v xvi value of 0 or n, and subjects with Pk rv = i xvi value of 0 or k are removed prior to estimation, where n is the total number of subjects and k is the total number of items. Running Rasch model on the data gives us an item parameter estimate or a score for each item. In general, the estimation of bi or score for an item i is calculated through conditional maximum likelihood (CML) estimation (Pawitan 2001). The conditional likelihood function for measuring item parameter estimate is defined as: Y expðÀbi si Þ Lc ¼ Pðxvi jrv Þ ¼ Q P r xjr expðÀbi xvi Þ v 1. where r represents the sum over all combinations of r items. Similarly, the maximum likelihood is used to calculate subject parameter estimation hv or score for each subject. Expectation-maximization algorithms (Hunter 2004) are used in implementing CML estimation in Rasch model. We can also assess whether the data fit the model by looking at goodness of fit indices, such as the Andersen's LR-test. To evaluate the quality of these measurements, we run Anderson LR-test (Hessen 2010) on the set of data. The test gives us a goodness of fit of the data in Rasch model, i.e., it tells us whether the data follows the assumptions of Rasch model. A p value, returned by the test, indicates the goodness of fit and a p value2 higher than 0.05 indicates no presence of lack of fit. 2.3 Implementing Rasch model in the text mining domain In this paper, we use Guttman scaling and Rasch model to find a ranking of 23 religious organizations based on extremity of their views are on radicalism and counterradicalism. In our application, Rasch-model subjects correspond to a group of religious organizations, and items correspond to a set of keywords for socio-cultural, political, religious radical and counter-radical beliefs, and practices. An organization responding ``yes'' to a feature means the organization exhibits that feature in its narrative,
2

while an organization responding ``no'' to a feature indicates that the organization does not exhibit such a feature. Difficulty of an item translates to strength of the corresponding attitude in defining radical or counter-radical ideology of any organization. Similarly ability of a subject in this case means the degree of radicalism or counterradicalism exhibited by an organization's rhetoric. Other works in text-mining domain, such as sentiment analysis, have used Rasch model in their analysis (Drehmer et al. 2000). Details of keyword extraction and selection are presented in Sect. 3.3.

3 Methods 3.1 Problem definition The primary goal of this study is to build a semi-automated method to rank religious organizations from a certain geographical region on a scale of radicalism versus counterradicalism using their web sites. The efficacy of the generated model is evaluated by comparing it against baseline methods and expert-level performance. In addition to accomplishing these goals, we also present an end-to-end system architecture, and a graphical user-interface design to facilitate faceted search and browsing of this corpus. 3.2 System architecture A summary of the system architecture can be seen in Fig. 1. The system is a composition of four components: a data-gathering component, which does web crawling, and text extraction; a scale generation component, performing scaling algorithms; application services component, which consists of several web services, and finally, a web userinterface component, presenting the data to the end user. 3.2.1 Data gathering Initially, social scientists are invited to use their domain and area expertise to identify a set of organizations, and hypothesize any number of unipolar or bipolar scales that could explain the variance among their beliefs and practices. Next, a set of web crawling scripts are created for extraction of articles from those organizations' web sites. For each organization's corpus, we extract their top-k n-grams, and a union of all these phrases are presented to experts for feature selection. Downloaded articles are then converted into XML structures, containing their original text, their set of keywords, and extracted information such as person, location and organization names using a NER tool for Indonesian language, and their machine translations into English.

http://www.en.wikipedia.org/wiki/P-value.

123

A system for ranking organizations using social scale analysis

economic, and religious} keywords corresponding to beliefs, goals and practices. During this process, our team of experts screened a total of 790 candidate keywords and they selected 29 keywords for inclusion in the radical scale, and 26 keywords for inclusion in the counter-radical scale. 3.4 Debates and perspective analysis Upon inspecting the keywords selected by our team of experts, we observed that some of these keywords correspond to differing perspectives on a set of topics that are debated within these web sites. Definition of debate is ``a formal discussion on a particular topic in a public meeting or legislative assembly, in which opposing arguments are put forward''.3 During a debate on a particular topic, like education, both radical and counter-radical organizations discuss different perspectives such as ``secular multi-cultural education'' versus ``Sharia based religious education''. To design an automated perspective detection algorithm, we made the following simplifying assumptions. 1. 3.3 Keyword extraction and selection 2. To identify candidate keywords, one option was to translate the documents into English and apply readily available keyword-extraction methods (Michael 2010). However, it was preferable to preserve the original expression of the phrases in the original language. Hence, we utilized a nonlinguistic technique that relies only on statistical occurrence, and frequency information. Within each document, the words were separated by whitespace or punctuation marks. We considered each keyword to be an n-gram of one to three words. We treated each organization as one document and calculated the term frequency-inverse document frequency (TF-IDF) (Salton 1988) values for every single n-gram mentioned by these organizations. Top 100 n-grams with the highest TF-IDF values from each organization were used to generate a candidate list of topics that these organizations discuss most frequently. Next, we asked our team of experts to screen and manually select identify {social, political, Organizations will mostly discuss their own perspective in a debate. Organizations will occasionally mention others perspectives, however, then relate them back to their own perspective.

Fig. 1 An overview of the system architecture

An example document snippet is shown in Fig. 2. Here the original input (content, source), and a sample of the automatically extracted information corresponding to DATE, PERSON, and LOCATION can be seen. The corresponding XML versions for each input document are then stored in a document database for processing.

In the following sections, we present a mathematical formulation of the perspective keyword-generation problem for a given topic, provide an NP-completeness proof, and design an exact solution through an integer linear programming (ILP)-based solver. Our future work involves finding an efficient approximation algorithm for this problem. 3.4.1 Perspective keywords-generation problem Perspective keywords-generation problem (PKGP) is defined as follows. Given a topic (a keyword) T, and two sets of documents TR and TCR where TR contains n documents TR ¼ fDR;1 ; DR;2 ; . . .; DR;n g and TCR contains m documents TCR ¼ fDCR;1 ; DCR;2 ; . . .; DCR;m g: From each document DR;i 2 TR ðDCR;j 2 TCR Þ, we collect a set of words WR,i, V1 B i B n (WCR,j, V1 B j B m) which appear two words before and two words after each occurrence of the topic T in that document. Let us define W as the union of all the WR,i, V1 B i B n and WCR,j, V1 B j B m. If the cardinality of W is p, then W can be given as W ¼ fw1 ; w2 ; . . .; wp g ¼ fWR;1 [ WR;2 [ Á Á Á [ WR;n [ WCR;1 [ WCR;2 [ Á Á Á [ WCR;m g Let the frequency of word wk in document DR,i is given as fR,i(wk) and the frequency of word wk in document DCR,j as fCR,j(wk).
3

Fig. 2 A portion of a document represented in the system

Oxford online dictionary.

123

S. Tikves et al.

Question: Are there two non-empty disjoint subsets of W, named W 0 and W 00 and W 0 \ W 00 ¼ ;; such that for every DR,i V1 B i B n, X X fR;i ðwk Þ ! fR;i ðwl Þ ð 1Þ
wk 2 W 0 wl 2W 00

Since WPP is known to be NP-complete, PKGP is also NPcomplete. 3.4.3 Integer linear programming formulation for PKGP We formulate an ILP to solve the PKGP optimally. For each word wi 2 W , we use two variables xi and yi. xi is 1 if and only if the word wi is in W1 and yi is 1 if and only if the word wi is in W2. Then constraint (3) means sets W1 and W2 disjoint. Constraint (4) ensures that these sets (W1 and W2) are also non-empty. Constraints (5) and (6) ensure the constraints 1 and 2 in problem statement. The objective minimizes the summation of cardinality of W1 and W2. Variables: For each word wi, & 1; if word wi is assigned to set W1 xi ¼ 0; otherwise. & 1; if word wi is assigned to set W2 yi ¼ 0; otherwise. Pp min i ¼1 x i þ y i s: t : xi þ y i
n X i¼1 p X wk 2 W p X wk 2 W

and for every DCR,j V1 B j B m, X X fCR;j ðwk Þ fCR;j ðwl Þ
wk 2 W 0 wl 2W 00

ð 2Þ

and jW 0 j þ jW 00 j K ? In optimization version of the problem, we will try to minimize jW 0 j þ jW 00 j: 3.4.2 Computational complexity of PKGP Definition 1 [Weak Partition problem (WPP)] Instance A finite set A ¼ fa1 ; . . .; an g and a size sðai Þ 2 Z þ ; 8i; 1 i n: Question Does the set A contain two nonempty sub-sets A1 and A2 that (1) A1 \ A2 ¼ ;; (2) A1 [ P P A2  A and (3) ai 2A1 sðai Þ ¼ aj 2A2 sðaj Þ? WPP has been shown to be NP-complete in (van Emde Boa 1981). Theorem 1 PKGP is NP-complete.

1;
n X i ¼1

8i ¼ 1; . . .; p y i ! 1; 8i ¼ 1; . . .; p

ð 3Þ ð 4Þ

xi ! 1 and

Proof It is easy to see that PKGP is in NP since a nondeterministic algorithm needs only to guess a partition of the word set W into W 0 and W 00 and check in polynomial time if all the constraints hold for this partition and also if jW 0 j þ jW 00 j K : WPP is a restricted version of PKGP. First we create a restricted instance of PKGP as follows: let TR and TCR contains one documents each, i.e, TR = {DR,1} and TCR = {DCR,1}. Frequency of a word wi 2 W ; 81 i n in document DR,1 and DCR,1 is taken to be equal, i.e., fR,1(wi) = fCR,1(wi) = s(ai). The parameter K is taken to be equal to |W|. This instance of PKGP is similar to an instance of WP in the following way: the set A contains element ai for every word wi 2 W : So, |W| = |A|. In addition, s(ai) = fR,1(wi) = fCR,1(wi), V1 B i B n. If we find a weak partition of A, as sets A1 and A2 such that P P ai 2A1 sðai Þ ¼ aj 2A2 sðaj Þ, then we can find subsets of W, as sets W1 and W2, such that wi 2 W1 if ai 2 A1 and P wj 2 W2 if aj 2 A2 , respectively. In addition, ai 2A1 sðai Þ ¼ P P 0 aj 2A2 sðaj Þ; implies that both the constraints P P P wi 2 W fR;1 ðwi Þ ! wj 2W 00 fR;1 ðwj Þ and wi 2W 0 fCR;1 ðwi Þ wj 2W 00 fCR;1 ðwj Þ are true, because s(ai) = fR,1(wi) = fCR,1(wi), V1 B i B n. Since K = |W|, the constraint jW 0 j þ jW 00 j K will trivially hold. So, WPP is a restricted version of PKGP.

fR;i ðwk Þðxk À yk Þ ! 0;

8i ¼ 1; . . .; n

ð 5Þ

fR;i ðwk Þðxk À yk Þ

0;

8i ¼ 1; . . .; m 8i ¼ 1; . . .; p

ð 6Þ ð 7Þ

xi 2 f0; 1g; yi 2 f0; 1g;

3.4.4 Social scale generation Social scale generation is done by building response tables; a pair of tables for a bipolar scale, such as radical/counterradical (R/CR), or a single table for a unipolar scale, by thresholding the occurrence frequencies of the selected keywords in the organizations' web corpus. The scale-generation architecture is shown in Fig. 3. Here, the flow of the processes and data can be seen as interactions between experts and automated modules. The system works as follows. ­ Initially, area experts to identify a set of organizations, and hypothesize any number of unipolar or bipolar scales that could explain the variance among the beliefs and practices of the organizations. Next, we crawl and download the web sites of the organizations, and the system automatically extracts the top-k candidate keywords for consideration in the

­

123

A system for ranking organizations using social scale analysis

­

Two types of other information are collected for evaluation purposes. First, expert rankings of the organizations, using a graphical drag-and-drop expertopinion elicitation tool shown in Fig. 11. Expert rankings are merged into a consensus gold standard of rankings. Next, two other computational baseline methods; one based on simple sorting, and another based on principal component analysis (Jolliffe 2002), are used to generate alternative computational rankings shown in Fig. 12.

In addition, the data for the violence/non-violence are gathered using a separately developed tool, by collecting the opinion of the experts. A future work will also include automated generation of this dimension, as well. 3.5 Feature extraction After identifying the keywords for the analysis, we needed to search the web site corpus of the organizations for the matching items. This yielded a term-document matrix. This task was performed in a simple three-step procedure; initially, the occurrence frequencies of particular keywords were counted within each organization's corpus, then, a threshold matrix was calculated from the initial values, and finally, a binary response matrix was generated by applying these thresholds to the initial values. The frequency metric is shown in formula 8, where k is the keyword, o is the organization, and Do is the document set pertaining to that particular organization. fo;k ¼ jfdjk 2 d; d 2 Do gj jDo j ð 8Þ

Fig. 3 A model of the system architecture

­

­

hypothesized scale. Social scientists screen the list of extracted keywords, and select the relevant ones for inclusion in further analysis. The system builds response tables; a pair of tables for a bipolar scale (such as radical/counter-radical R/CR), or a single table for a unipolar scale, by thresholding the occurrence frequencies of the selected keywords in the organizations' web corpus. See Figs. 4 and 5 for the response tables for the R/CR scale. The response tables are fed as input to the Rasch Model building algorithm. The algorithm produces a metric to validate the fitness of the model, and rankings of the organizations and keywords. Figures 6 and 7 show the relative positions of the organizations and keywords on the latent scales. The algorithm also produces a metric to validate the fitness of the model.

A threshold value for each keyword is calculated by taking the median of the values in the related column. Median was preferred over mean as a threshold, since the distribution of the values did not fit Gaussian distribution, yet median empirically proved to be a better measure.

Fig. 4 Radical subset of organizations and keywords, sorted according to aggregate row values

123

S. Tikves et al. Fig. 5 Counter-radical subset of organizations and keywords, sorted according to aggregate row values

Fig. 6 Radical subset of organizations and keywords

Finally, each element was converted into a binary value by comparing it to the column's threshold. English translations of the keywords are presented for clarity in Figs. 4 and 5. 3.6 Model fitting We fit the Rasch model on two datasets: (1) radical organizations with radical keywords and (2) counter-radical organizations with counter-radical keywords. We used the eRm package in R, an open source statistical software package,4 to fit a Rasch model to the dataset, and obtain the organizations' scores on the latent scale, which are the the subject parameter estimates (hv) discussed in the previous section. The eRm package5 fits Rasch models and provide subjects or organizations parameter estimates based on maximum likelihood estimation.
4 5

The automated scale of the organizations is formed by ranking the organizations according to their estimates on the latent scale. Not only we can provide the organization estimates but we can also assess whether the model fits the data by looking at several goodness of fit indices, such as the Andersen's LR-test. 3.7 Application services We use two backend services in the application layer to present the data to the user interface. First, all the extracted textual information are stored in Apache Solr,6 providing facilities like full-text search and faceting (Tunkelang 2009), using an AJAX interface. In addition, a WCF-based scaling service is used to infer scales in real time. This particular service loads the response table, and the previously generated scale data, and estimates the R/CR scale
6

http://www.cran.r-project.org/. http://www.r-forge.r-project.org/projects/erm/.

http://www.lucene.apache.org/solr/.

123

A system for ranking organizations using social scale analysis

Fig. 7 Counter-radical subset of organizations and keywords

for a subset of the input. Number of positive responses are interpolated on the scale to generate the scale, and the expert opinion is used for a static violent/non-violent (V/NV) scale. While the interpolation is based on a sufficient statistics, future work on speeding up Rasch model generation for real-time use would be beneficial. 3.8 User interface The user interface is responsible for representing our input data, and the findings to the experts in an interactive fashion. Users should be in control of the selection of the data displayed, and filtering with organization names, or a specific date range, or using other parameters such as arbitrary keywords, or geographic locations. While performing these tasks, it should provide results to the user with a minimum of delay, allowing quick drilling down to interactively model the scenarios that users have in mind. The user interface is implemented as an interactive AJAX-based application, using ajaxsolr7 framework. In addition to the search and navigation capabilities provided with ajaxsolr, it also adds functional widgets for visualizing the organizations on a scale, mapping the intensity of the locations, displaying demographics trends, and so on. A more detailed discussion of the user interface is provided in Sect. 5. The presentation of the scale, however, brings the following challenges. ­ It would be preferable to plot the locations on the same range as the input collection. However, the Rasch scale is on a latent range (Figs. 8, 9).

­

Since this will be an interactive application, users would prefer to see almost instantaneous results. Yet, the eRm model generation is computationally expensive.

We resolve the first issue by uniformly scaling the ranges into [-10, 10], making it consistent with the inputs. The second issue requires a more specific solution. We make use of the fact that the raw person scores pertaining to number of positive responses is a sufficient statistics for the Rasch model (G 1961) to estimate scale values on the fly. Since we know the date range, and the selected organizations currently visible in the user interface, it is possible to quickly generate a response matrix for this subset of the data, and merge it with the previously known scale information to generate interpolated scale values.

Radical Scale

Person Parameters (Theta)

-4 0

-2

0

2

5

10

15

20

25

30

Person Raw Scores
7

http://www.evolvingweb.github.com/ajax-solr/.

Fig. 8 Radical scale

123

S. Tikves et al.

Here we have opted to include all the organizations in threshold calculations. This is because, the radical or counter-radical activity intensities are always measured relative to the other organizations participating in the same time period. However, while the scale is based on all the organizations, only the ones specifically asked will be presented to the user.

4 Experimental evaluation 4.1 Indonesian corpus The corpus domain is the online articles published by the web sites of the 23 religious organizations identified in Indonesia, in the Indonesian language. These sources are the web sites or blogs of the identified think tanks and organizations. As discussed in the Sect. 1, each source was classified as either radical or counter-radical by the area experts. We downloaded a total of 37,000 Indonesian articles published in these 23 web sites, dating from 2001 to 2011. For each web site, a specific REGEX filter was used to strip off the headers, footers, advertising sections and to extract the plain text from the HTML code. The psuedo-code for the subset scale-generation procedure is presented in Algorithm 1. The process starts with identifying the subset of documents in the (start, end) date range (lines 2­5). Then the keyword frequencies, and thresholds are calculated for the entire set of organizations on this document subset (lines 6­14). Finally, response tables for the subset of organizations is generated (lines 15­17), and then the sums need to be interpolated (lines 18­23), to be able to generate a scale on the [-10,10] range (line 24).
Counter-Radical Scale

4.2 The quadrants model Our project leverages the results of our previous work, which relied on social theory including Durkheim's (2004) research on collective representations, Simmel's (2008) work on conflict and social differentiation, Wallace's (1956) writings on revitalization movements, and Tilly and Bayat's studies on contemporary social movement theory (Tilly 2004; Bayat 2007). Our team has also developed, and is currently testing a theoretically based class model comprised of continuous latent scales. The first pair of scales focus on distinctions between the goals and methods of counter-radical and radical discourse, and capture the degree to which individuals, groups, and behaviors aim to influence the social order (change orientation) and the methods by which they attempt to do so (change strategies). Quadrants model (see Fig. 10) captures multiple social trends in four quadrants (A, B, C, and D), and it makes the significant distinction between violent and not-violent dimensions of both radicalisms and counter radicalisms. Using the quadrants model, a researcher can locate organizations, individuals, and discourses in broader categories while still considering subtle differences between groups within categories. A researcher can document movement and trends from category to category, and identify points where movement is likely to happen.

Person Parameters (Theta)

-4 0

-2

0

2

5

10

15

20

25

Person Raw Scores

Fig. 9 Counter-radical scale

123

A system for ranking organizations using social scale analysis

rankings. The individual scores for each organization were combined and averaged to obtain the consensus gold standard rankings along the hypothesized R/CR scale. A work is in progress for building a publicly accessible expert opinion collection toolkit. The preliminary version can be accessed at: http://www.minerva-project.org/ DataCollector. 4.4 Computationally generated scale The ranking discovered by the Rasch model fitting the corpus has been evaluated against the gold standard rankings of the organizations provided by the experts. The difference between two separate rankings have been calculated using the following misplacement error measure in Eq. (9). P jGðoÞÀRðoÞj error ðG; RÞ ¼
o 2O jOj

Fig. 10 The quadrants model

4.3 Expert opinion and gold standard of rankings We collaborated with three area experts, who collectively possess 35 years of scholarly expertise on Indonesia and Islam. To build a gold standard of orderings of the organizations, we built a graphical drag-and-drop user-interface tool to collect the opinions of each of the area experts. A screenshot of the tool is shown in Fig. 11. Each expert, separately evaluated and ranked the organizations in the dataset according to a two dimensional scale of radical/counter-radical (R/CR) and violent/nonviolent (V/NV) axis. The consensus among the experts was high; since per item standard deviations among the experts' scores along the R/CR axis over a range of [-10, 10], across all organizations were 2.75. In addition, 90 % of the items have less than 22.6 % difference in their

jO j

ð 9Þ

Here, O is the set of organizations, G and R are one to one mapping functions of rankings from set O to range [1,|O|]. For two exactly matching rankings, the error(G, R) will be zero, whereas for two inversely sorted rankings it is expected to be 0.5 (when the size of O is even). In addition, a random ranking is expected to have a error of 0.375. 4.5 Expert-to-gold standard error We calculated the error between each expert's ranking and their consensus gold standard of rankings. The first expert's error measure is 0.06, and the second and third expert's errors are 0.12 and 0.14 correspondingly as shown in the last row of the table in Fig. 12. The average error of our experts against their gold standard ranking is 0.11. 4.6 Baseline: sorting with aggregate score The first baseline we used was constructed by sorting the organizations according to the number of different keywords observed in their corpus. While this provided a pattern similar to a Guttman scale, and orderings of the organizations matched to a certain degree with the gold standard as shown in Fig. 12, the error for this baseline was 0.19, which is higher than the average expert's performance. 4.7 Baseline: principal component analysis A stronger baseline was built by employing principal component analysis (Jolliffe 2002), and sorting the organizations according to their projections in the first principal component of the term­document matrix. Since experts selected the R/CR scale relevant keywords only, it was expected that the first principal component would reflect the corresponding scale. PCA proved to be performing

Fig. 11 The visual interface of the expert-opinion collector for manually placing the organizations on the two dimensional scale

123

S. Tikves et al. Fig. 12 Computational and expert rankings

better than the aggregate score sorting, with an error measure of 0.18. However, this error rate is still higher than the error rate of each expert. 4.8 Performance of the Rasch model ranking system The p values from the Anderson LR goodness of fit test from model (1) and model (2) (mentioned in Sect. 3.6) are 0.85 and 0.669, respectively, suggesting no evidence of lack of fit. The Rasch models allow us to get a natural order of the organizations, according to their ``abilities'', i.e., radicalism and counter-radicalism in this case. This system had an error measure of 0.10, which actually provided a higher ranking performance than the average performance of our experts'-- performing better than the majority of our area experts. 4.9 Evaluations Our experiments showed that the hypothesized compatibility of the R/CR scale for the Indonesian corpus is valid. Not only the Rasch model was statistically fitting the response matrix but also the generated ranking performance was better than the average expert performance. Among our computational baseline methods, the Rasch Model was the only method producing expert-level performance as shown in Fig. 12. This preliminary analysis with the R/CR scale shows that when experts assist the system with keyword selection, the web corpus of organizations provides rich-enough

information and patterns to enable a computational method to rank them accurately.

5 Web application overview A sample snapshot of the web application can be seen in Fig. 13. It is composed of four main widgets for visualization and navigation. The top-left section which contains the Search and Navigation widget (1) that allows filtering of the document subset using parametric search queries and keyword based search criteria. The top-right section is the Quadrant widget (2) which displays the organizations active in the currently selected time frame on a twodimensional axis, using violence and radicalism scales. The bottom-left section consists of two Treemap widgets (3) which displays the demographics and the top keywords (markers) of the current selection. The bottom-right section has a Timeline widget (4) which provides a visualization of the keywords (markers) trends on a time line. The navigation in the user interface starts with the Navigation widget (top-left) of the web application. Here the user is able to filter down the corpus utilizing full-text search queries, or faceting using keywords, locations, demographics, or choosing a subset of organizations. The Quadrant widget (top-right) provides a plot of the currently selected organizations on the two dimensional scale. The radical/counter-radical (R/CR) axis is

123

A system for ranking organizations using social scale analysis

Fig. 13 A sample snapshot of the web application (color figure online)

dynamically calculated in real time, using the subset of organizations, and the time range of the current selection. The location change on the time range for each organization is shown as a color-coded path, with three markers, a light circle corresponding to the position at the beginning of the period, a dark circle corresponding to the end of the period, and a dark-small circle for the middle. A red line between the circle denotes the rise of radical activities in the organization's behavior. A blue line denotes the opposite. The smaller circle is useful to see the overall movement of an organization. For example, between the range Aug 2005 and Aug 2007, EraMuslim's activities were radical (center of A quadrant), then became almost counter radical (the smaller circle denotes this mid point in the movement), and then jumped up again. The V/NV axis is retrieved from expert opinion in the current version, and dynamic calculation of this axis is left for a future version. The Timeline widget (bottom-right) displays the trends of the most frequent markers on a time line. Initially the subset of markers presented defaults to all available, however it is possible to restrict the selection of markers to a more limited set among radical/counter-radical, economical, political, religious, or social domains. Timeline widget can also be used for selecting a date range of interest. The Treemap widgets (bottom-left) are used to display the relative frequencies of demographics and keywords

(markers). The displayed marker category selection for this widget is synchronized with the Timeline widget. In the following sections, we present some scenarios and findings to illustrate the capabilities of the web interface. 5.1 Scenario 1: radical organizations' trends In this scenario, we analyze both violent and non-violent radical organizations. Our web application shows the ideologies that these organizations are propagating. We can see8 the most prominent markers associated with these radical organizations. Markers such as ``infidel'', ``Sharia'', and ``violence'' show an increasing trend between 2001 and 2011. A very strict interpretation of ``Sharia'' is used by radical organizations to justify their actions (Widhiarto 2010; Hasan 2009). ``Sharia'' peaks during this period as shown in Fig. 14. 5.2 Scenario 2: C-quadrant organizations' trends We now analyze Front Pembela Islam (FPI), an Islamic organization in Indonesia established in 1998. FPI is well known for its violent acts (Frost et al. 2010; Rondonuwu
8

Select the filter ``Radical'' from the search options and then in the Markers Menu select [Religious ! Radical Markers].

123

S. Tikves et al.

Fig. 14 Trend of radical markers

2010) justified by a strict interpretation of Sharia (for the Study of Terrorism 2011). Our documents for FPI ranges between 2000 and 2010. Using our web application's plots of the movement of FPI in the C Quadrant, we found that FPI consistently rised higher on the radical scale as shown in Fig. 15. We selected the following time ranges, 2000­2003, 2002­2006, 2006­2010 and analyzed the trends of various markers associated with FPI. There was a substantial increase in the intensity of various radical markers such as ``infidel'', ``Mujahedin'', ``pornography''.9 Since 2006, we also saw a steep increase in the frequency of marker ``Ahmadiyya'', as shown in Fig. 16, which indicates FPI's increased opposition to this heretical sect (Rahmat and Sihaloho 2011). 5.3 Scenario 3: A-quadrant organizations' trends We analyze Hizb ut-Tahrir also known as Hizb ut-Tahrir Indonesia (HTI), a radical organization widely believed to be non-violent (Ward 2009), which has been active in Indonesia since 1982 (Osman 2011). Between 2007 and 2009, our web application shows various radical and nonradical markers associated with this organization.

Fig. 15 Consistent rise of FPI on the radical scale

Fig. 16 ``Ahmadiyya'' peaking during the period 2006­2010

Radical ``Sharia'', ``Infidel'', ``Caliph'', ``Violence''

Non-Radical ``Politics'', ``Indonesian Islam'', ``Election'', ``Liberal'', ``Democracy''

During the same period, we see a steady increase in the frequency of the radical marker ``Sharia''. This is consistent with one of HTI's goals of implementing Sharia in Indonesia (Hasan 2009). Hizb ut-Tahrir openly propagates
9

Fig. 17 ``Khilafah'' ideology of Hizb ut-Tahrir

Select ``Radical'' and ``FPI'' from the filters, then select the time range 2002­2006 or 2006­2010, then select ``radical'' markers under ``R/CR'' menu.

the ideology of Khilafah, which believes in unification of all Muslim countries as a single Islamic State (Zakaria 2011; Mohamed Osman 2010). Figure 17 shows

123

A system for ranking organizations using social scale analysis

Searching for the text ``suicide bombing'', we see that one of the related markers is ``ideology''. Adding the keyword ``ideology'' to the search filter reveals a new set of markers including the ``sin'' keyword. Adding ``sin'' to our search, we obtain a set of matching documents. One of the top matches, is titled ``Mengapa Saya Berubah?'' (english translation: ``Why I changed?'')13. This article is by a reformed terrorist, debunking the misinterpretation of the jihad-related verses used by violent groups.

6 Conclusions and future work In our experiments, not only did the data show fitness with the Rasch Model for the R/CR scale but also the Rasch rankings of the organizations are better than the output of the other baseline computational methods, and they are at expert-level performance when compared with the consensus gold standard rankings. Rasch model also provided us with another output, namely the ranking of selected keywords (items) on the R/CR scale. Although preliminary observations indicates that this can be a valuable asset by itself, we plan to further investigate the quality and utility of this ranking as future work. While the model has been demonstrated to fit on the R/CR scale, two major expansion points can be investigated in the future work, namely the violent/non-violent scale, and enhancement of feature selection. Although our experts have identified a second dimension, evaluating its correlation to R/CR axis, or existence of other significant ones could be beneficial. In addition, the features can be enhanced by experimenting with the significance of the radical keywords in the counter-radical organization corpora, and vice-versa. A practical method to increase the automation of keyword generation has been discussed in Sect. 3.4. Future work will involve finding an efficient approximation algorithm for this model, for decreasing the necessity of expert interaction for this particular step. Other interesting work includes making our expert opinion elicitation tool available online to a wider and more geographically distributed audience to crowdsource (Snow et al. 2008) the needed expertise for making lists of local organizations, identifying their web sources, and overcome the complex task of construction and validation of significant and fitting scales (work is currently underway to build this tool). Another interesting dimension is to look at synthesis and analysis of scales that do have a strict hierarchy of keywords, but adhere to more flexible partial order models (James and John 2002).
13

Fig. 18 Decline of the HTI in the radical scale

``Khilafah'' as the most prominent marker10 in Hizb utTahrir's discourse. By looking at the Quadrants widget (in Fig. 18), we can infer that HTI has been moderating its narrative. 5.4 Scenario 4: B-quadrant organizations' trends In this scenario, we discuss the trends of counter radical organizations like NU and DaarulUluum. We also show an interesting scenario on the topic of ``Suicide Bombing'' using the keyword based Navigation widget. The ``counter radical'' markers11 associated with these organizations are: ``politics'', ``election'', ``Indonesian Islam'', ``liberal'', ``human rights''. These organizations support democracy and elections, which is shown by the high frequency of the markers ``politics'' and ``election''. Their narrative has local interpretation of Islam at its core, which is shown by the marker ``Indonesian Islam''. On analyzing the occurrences of radical markers12 in B-Quadrant, we find that counter radical organizations are very vocal against all of radical markers. One of the interesting radical markers is ``Suicide Bombing''. Most of the counter radical organizations are against suicide bombings.(Malang 2006). We will now demonstrate how combination of parametric and keyword search, and various widgets in the web application can help reveal opposition to ``Suicide Bombing'' by counter-radical organizations.
10

Select ``Hizb ut-Tahrir'' and ``radical'' from filters. Select the time range 2007­2009. The markers can be seen by selecting the options of Markers Menu [Religious ! Religious Markers]. 11 Select CounterRadical filter in the search option, then from the Markers Menu select [R/CR ! Counter Radical]. 12 In the Markers Menu select [R/CR ! Radical].

http://www.islamlib.com/id/artikel/mengapa-saya-berubah/.

123

S. Tikves et al. Acknowledgments This research was supported by US DoDs Minerva Research Initiative Grant N00014-09-1-0815, Project leader: Prof. Mark Woodward, Arizona State University, and the project title is ``Finding Allies for the War of Words: Mapping the Diffusion and Influence of Counter-Radical Muslim Discourse''. doi:10.1080/09546553.2010.496317. http://www.tandfonline. com/doi/abs/10.1080/09546553.2010.496317 Osman MNM (2011) Preparing for the caliphate. Asian Stud Assoc Aust E-Bull 80:14­16. ISSN:1449-4418 Pawitan Y (2001) In all likelihood: statistical modelling and inference using likelihood. Oxford University Press, USA Rahmat Sihaloho M (2011) FPI vows to disband ahmadiyah 'whatever it takes'. http://www.thejakartaglobe.com/home/fpivows-to-disband-ahmadiyah-whatever-it-takes/423477 [Online accessed 21 Nov 2011] Rasch G (1961) On general laws and the meaning of measurement in psychology. In: Proceedings of the fourth Berkeley symposium on mathematical statistics and psychology, 4, p 332 Rondonuwu O, Creagh S (2010) Opposition grows to indonesia's hardline fpi islamists. http://www.in.reuters.com/article/ 2010/06/30/idINIndia-49777620100630 [Online accessed 21 Nov 2011] Salton G, Buckley C (1988) Term-weighting approaches in automatic text retrieval. In: Information Processing and Management, vol 25, pp 513­523 Simmel G (2008) Sociological theory. McGraw-Hill, New York Snow R, O'Connor B, Jurafsky D, Ng AY (2008) Cheap and fast-- but is it good?: evaluating non-expert annotations for natural language tasks. In: Proceedings of the conference on empirical methods in natural language processing, EMNLP '08, pp 254­263. Association for Computational Linguistics, Stroudsburg, PA, USA. http://www.portal.acm.org/citation.cfm? id=1613715.1613751 for the Study of Terrorism NC, to Terrorism R (2011) Terrorist organization profile: front for defenders of Islam. http://www. start.umd.edu/start/data_collections/tops/terrorist_organization_ profile.asp?id=4026 [Online accessed 21 Nov 2011] Thurstone LL (1928) Attitudes can be measured. Am J Sociol 33:529­554 Tikves S, Banerjee S, Temkit H, Gokalp S, Davulcu H, Sen A, Corman S, Woodward M, Rochmaniyah I, Amin A (2011) A system for ranking organizations using social scale analysis. In: EISIC. IEEE, pp 308­313. http://www.ieeexplore.ieee.org/xpl/ mostRecentIssue.jsp?punumber=6059524 Tilly C (2004) Social Movements. Paradigm Publishers, USA Tunkelang D (2009) Faceted Search: synthesis lectures on information concepts, retrieval, and services. Morgan & Claypool Publishers, UK. doi:10.2200/S00190ED1V01Y200904ICR005 van Emde Boas P (1981) Another NP-complete partition problem and the complexity of computing short vectors in a lattice: Tech. Rep. 81-04. Mathematisch Instituut, Amsterdam Wallace A (1956) Revitalization movements. Am Anthropol 58:264­281 Ward K (2009) Non-violent extremists? Hizbut Tahrir Indonesia. Aust J Intl Affairs 63(2):149­164 doi:10.1080/1035771090 2895103.http://www.tandfonline.com/doi/abs/10.1080/10357710 902895103 Widhiarto H (2010) Radical groups urge Bekasi administration to implement Sharia law. http://www.thejakartapost.com/news/2010/ 06/27/radical-groups-urge-bekasi-administration-implementsharia-law.html [Online accessed 21 Nov 2011] Zakaria Y (2011) A Global Caliphate: reality or fantasy? http://www. usa.mediamonitors.net/content/view/full/91207 [Online accessed 21 Nov 2011]

References
Andrich D (1988) Rasch models for measurement. Sage, USA Bayat A (2007) Making Islam Democratic: social movements and the post-Islamist turn. Stanford University Press, USA Crelinsten R (2002) Analysing terrorism and counter-terrorism: a communication model. Terror Political Violence 14:77­122 Davulcu H, Ahmed ST, Gokalp S, Temkit MH, Taylor T, Woodward M, Amin A (2010) Analyzing sentiment markers describing radical and counter-radical elements in online news. In: Proceedings of the 2010 IEEE second international conference on social computing, IEEE Computer Society, SOCIALCOM'10, pp 335­340 Drehmer D, Belohlav J, Coye R (2000) An exploration of employee participation using a scaling approach. Group Org Manage 25(4):397 Durkheim E (2004) The cultural logic of collective representations: social theory the multicultural and classic readings. Wesleyan University: Westview Press Frost F, Rann A, Chin A (2010) Terrorism in southeast asia. http://www.aph.gov.au/library/intguide/FAD/sea.html [Online accessed 21 Nov 2011] Guttman L (1950) The basis for scalogram analysis. Meas Predict 4:60­90 Hasan N (2009) Islamic militancy, Sharia, and democratic consolidation in post-Suharto Indonesia. RSIS Working Papers. 143/07 Hessen D (2010) Likelihood ratio tests for special Rasch models. J Edu Behav Stat 35(6):611 Hunter D, Lange K (2004) A tutorial on mm algorithms. Am Stat 58(1):30­37 James AW, John LM (2002) Algebraic representations of beliefs and attitudes: partial order models for item responses. Sociol Methodol 29:113­146 Jolliffe I (2002) Principal component analysis: Springer series in statistics. Springer, Germany Le Cam L (1990) Maximum likelihood an introduction. ISI Rev 58(2):153­171 Likert R (1932) A technique for the measurement of attitudes. Arch Psychol 140:1­55 Malang (2006) NU chairman deplores suicide bombing attempt. http://www.nu.or.id/page/en/dinamic_detil/15/28282/News/NU_ chairman_deplores_suicide_bombing_attempt.html [Online accessed 22 Nov 2011] McIver J, Carmines E (1981) Unidimensional scaling, vol 24. Sage Publications Inc, USA McPhee RD, Corman S (1995) An activity-based theory of communication networks in organizations, applied to the case of a local church. Commun Monogr 62:1­20 Michael WB, Kogan J (2010) Text mining: applications and theory. Wiley, London Mohamed Osman MN (2010) Reviving the Caliphate in the Nusantara: Hizbut Tahrir Indonesia's mobilization strategy and its impact in Indonesia. Terror Political Violence 22(4):601­622.

123

Spatio-Temporal Signal Recovery from Political Tweets in Indonesia
Anisha Mazumder, Arun Das, Nyunsu Kim, Sedat Gokalp, Arunabha Sen, Hasan Davulcu School of Computing, Informatics and Decision Systems Engineering Arizona State University Tempe, Arizona - 85287 Email: {Anisha.Mazumder, adas22, nkim30, Sedat.Gokalp, asen, hdavulcu}@asu.edu
Abstract--Online social network community now provides an enormous volume of data for analyzing human sentiment about people, places, events and political activities. It is increasingly clear that analysis of such data can provide great insights on the social, political and cultural aspect of the participants of these networks. As part of the Minerva project, currently underway at Arizona State University, we have analyzed a large volume of Twitter data to understand radical political activity in the provinces of Indonesia. Based on analysis of radical/counter radical sentiments expressed in tweets by Twitter users, we create a Heat Map of Indonesia which visually demonstrates the degree of radical activities in various provinces of Indonesia. We create the Heat Map of Indonesia by computing (i) the Radicalization Index and (ii) the Location Index of each Twitter user from Indonesia, who has expressed some radical sentiment in her tweets. The conclusions derived from our analysis matches significantly with the analysis of Wahid Institute, a leading political think tank of Indonesia, thus validating our results. Index Terms--radical, tweet, Radicalization Index, Location Index, Heat Map

I. I NTRODUCTION The sheer popularity of online social media nowadays is reflected by the immense amount of data being fed every second by people from all over the world. It is becoming increasingly evident that analysis of this huge online dataset can provide great insights on the social, political and cultural aspect of the Twitter users and possibly the non-Twitter users as well. In [2], the authors have developed Socioscope, a tool for extracting signal from noisy social media data. Utilizing a Socioscope like mechanism, we have developed a tool for recovering spatio-temporal signals from tweets generated in Indonesia. Our interest in analyzing tweets from Indonesia developed in the context of the Minerva1 project, currently underway at Arizona State University. The goal of this project is to increase the understanding of movements within Muslim communities towards radicalism or counter radicalism. Based on the support and opposition of certain beliefs and practices of an individual (as expressed in her tweet), we can assign a Radicalization Index to that individual. In addition, from the self declared home location of a Twitter user and the locations of her tweets, we can compute a distribution of Location Index for that user. The map of Indonesia is divided up into a set of regions and the Location Index of a user provides the
1A

probability of the user to be in a specific region at a specific time. For this analysis a region corresponds to a province of Indonesia. Finally, from the Radicalization Index and Location Index of individuals, Heat Index of a region , which is a composite measure of the number of radical tweeters of that region and their `degree of radicalism', is computed. In our model we have a set of tweeters (or users), U = {U1 , U2 , . . . , Un }. Each user Ui , 1  i  n creates a set of tweets Ti = {Ti,1 , Ti,2 , . . . , Ti,ti }. The set of all tweets by n all users is denoted by T = i=1 Ti . The geographic area from where the tweets originate is divided into a set of regions R = {R1 , R2 , . . . , Rm }. In our study m is equal to thirty four, the number of provinces and special administrative regions of Indonesia. Each user Ui , 1  i  n has a home location HLi , 1  i  n associated with her, which may or may not be declared. Each tweet Ti,k , 1  i  n, 1  k  ti has a geo-location GLi,k , 1  i  n, 1  k  ti associated with it. However, GLi,k for some tweets Ti,k may not be known as the user Ui might turn her GPS off. Accordingly, we can divide the set of users in four different classes: (i) Class 1: user Ui whose home location is declared and geo-location of at least one tweet is known, (ii) Class 2: Ui whose home location is not declared and geo-location of at least one tweet is known, (iii) Class 3: Ui whose home location is declared and geolocation of none of the tweets are known, and (iv) Class 4 : Ui whose home location is not declared and geo-location of none of the tweets are known. From the input data set (U, T, R ), we compute, (i) Location Index, Li of each user Ui , 1  i  n, (ii) Radicalization Index, RDi of each user Ui , 1  i  n, and finally, combining Li and RDi , we compute (iii) Heat Index, Hj of each region Rj , 1  j  m. It may be noted that whereas RDi , 1  i  n is a scalar value, Li is a vector of size m, (Li,1 , . . . , Li,m ), where Li,j indicates the probability of user Ui being located in region Rj i.e. Li,j indicates the probability of the Actual home location of Ui being Rj . Finally, the Heat Index Hj of n region Rj , 1  j  m is computed as Hj = i=1 RDi × Li,j , j, 1  j  m. We thus provide a generic technique for generating time-varying political Heat Maps of a geographical region based on the Twitter data analysis. Throughout this paper we have used `region ' and `location' interchangeably

project sponsored by the U.S. Department of Defense

to mean an `Indonesian Province'. It is to be noted that for our calculations, we have considered all Indonesian provinces including special administrative regions such as Yogyakarta and special capital region such as Jakarta. II. R ELATED W ORK Computation of Heat Map of Indonesia requires the computation of the following: First, we compute the Radicalization Index of a user Ui by analyzing the content of her tweets. Second, Location Index of the user Ui is computed from her geo-location containing tweets (if any) and also from her home location declared as a part of her Twitter profile (if at all provided). It is to be noted that we do not consider users who have neither of these two sources of location information present. Identification of the location of users using Twitter data has been quite a focus of recent research. Inferring location from tweets have been pursued by [14], [15], [16]. Studies conducted in [4], [5], [6], [7] combine location information and text from social-network data history to infer various questions such as user preferences and provide recommendations. However, we do not rely on any `checking in' information for our computations and providing recommendations is not our goal. We do employ the notion of regions - the thirty four provinces of Indonesia are the regions of interest for our problem. Thus, `geo-coding' (the use of gazetteers) is applicable to our problem. However, just as in [3], we too argue that location estimates are multi-modal probability distributions, rather than particular points or regions. However, it may be noted that in contrast to [3], we are interested only in Indonesia and in Indonesian provinces - thus our estimate of the location of the user must be the probability of each Indonesian province as the Actual home location of the user under consideration, rather than the probability of the user being located in each and every point on the surface of the earth. This implies that our world comprises of Indonesia only and individual geo-co-ordinates are bunched into the corresponding province of Indonesia. As a result, we apply the combination of `geocoding' and the modification of the techniques in [3]. Thus, we use gazetteers for the Declared Home Location of the Twitter Users to map those to a specific province of Indonesia (This is explained in further details in Section VII ). This combined with the geo-coordinate information about the user (obtained from her tweets containing geo-location) gives us the probability distribution of the user across the thirty four provinces of Indonesia. We thus obtain a simple yet effective means of computing the geo-location of the user as compared to other more complex methodologies such as Topic Detection Techniques [20], [21], [22]. Human mobility is modeled as a stochastic process in [8]. Following the studies of [8], in [1], the authors study the manner in which the movements of human beings are related to time of the day, geography as well as social ties. They intend to predict the exact location of a person based on various factors which the authors have identified, including impact of

social network. Similar problems have been studied by [9], [10], [11]. However, in our problem, there is no notion of prediction of location of users involved. Besides, we consider categorical distribution. However, we do use the concept of mixture of distributions in the lines of [1]. Another line of research which focuses on location estimation by content-analysis of the tweets of a user has been studied by [12], [13]. They use the techniques of feature selection from tweets of users, following it up with training and classification. However, we do not apply content based analysis in this current work, but rely on the geo-location containing tweets of users in our dataset and also the user declared home location to obtain the location distribution of the users. In [17], the authors analyze tweets generated during the United Kingdom 2010 General Election to measure political sentiments as well. They have identified the specific features of the political parties and their ultimate goal is to infer the political affiliation of a user based on her tweets. We also study a similar problem, however our goal is not to identify the political affiliations of users, rather we compute the `degree of radicalism' of the user. Besides, our technique is completely different from theirs. Unlike them, we apply a very simple yet effective term-frequency analysis of tweets and leverage heavily on our team of domain experts. We validated our classification of users into radicals and counter radicals by classifying some well-known counter radical leaders of Indonesia (Our validation process is discussed in further details in Section IX). The work in [35] which is followed by [18] is very relevant to our technique of Radicalization Index assignment to users. These works too deal with the recovery of radical signals from the online posts of social media users and thereby identify individuals as potential `lone wolf terrorists'. They specifically focus on presenting a framework for combining entity matching techniques for detecting extremist behavior on discussion boards. These `lone wolf terrorists' might leave weak signals of radicalism through their comments or posts on discussion boards, signals made further weaker by the use of aliases. Identification and analysis of such weak signals of radicalism by the use of topic-filtered web harvesting as well as application of natural language processing techniques, thereby fusing aliases for identifying the person form the basis of the works of [35]. Their work is fundamentally different from ours because we deal specifically with the users' publicly available tweets only - this eliminates the availability of the vital background information such as characteristic ( `radical internet forum', `capability internet forum' ) annotation of particular discussion boards that is leveraged in [35] . However, we also have used the technique of application of keyword analysis and crawling of web-sites of well-known radical/ counter radical organizations of Indonesia as discussed in later sections. Furthermore, [35] and [18] do not deal with location profiling of users which is one of the two major goals of our work.

Fig. 1: The flow diagram of our Heat Map computation technique. The Web data mentioned here refers to the documents generated by crawling the web pages of radical and counter radical organizations of Indonesia.

III. M OTIVATION AND D ISTINGUISHING F EATURES OF
OUR WORK

The motivation for our work is to provide a visualization of the spatio-temporal distribution of the radical population of Indonesia by recovering political signals from Twitter data. A pictorial description of our methodology is provided in Figure 1. Similar retrieval of signals using Twitter data is the motivation of the work of [2]. In [2], they find the location distribution of tweets mentioning roadkills using human beings as sensors. Similarity of our work with [2] is that we too use human beings as sensors to the extent that we use tweets of people of Indonesia to infer radicalism Heat Indices of the provinces of Indonesia. However, our work is significantly different from theirs. First, unlike [2], we intend to find the distribution of (radical) individuals, so we should not factor in any `human population bias' i.e variation of densities of people across the different provinces of Indonesia. Second, our problem is much more complex because we not only need to know from which location have the radical tweets come in greater number, but also the `degree of radicalism' of the tweets - so we need to comprehend the sentiment of the tweets. The major difference here is that in our case, we need a finer grained distinction among the radical tweets specifying which tweets are more radical and which tweets are less. So, questions of interest for us are(Qs1) the `degree of radicalism' of tweet tw (Qs2) the originating location of tweet tw Thus, Heat Index of a region factors in both the count of the radical tweets from the region as well as the `degree of radicalism' of the tweets. However, there are certain challenges in answering these questions. As for Qs1, a tweet can at most be 140 characters long. This is indeed too little information to ascertain the `degree of radicalism' of tweets on individual basis. Thus, we go one level up the hierarchy and consider individual users instead of individual tweets and try to answer the two questions in the context of individual users. We collect all the tweets from individual users and assign the `degree of radicalism' to the user based on her tweets. Now, Qs2 would have been easy to answer with respect to individual tweets if all the tweets had geo-co-ordinate information because Twitter API2 provides geo-location information of tweets if the user
2 https://dev.twitter.com/docs/streaming-apis and ter.com/docs/platform-objects/tweets have been used

https://dev.twit-

had chosen to reveal her location at the time of tweeting. However, there are certain problems with this approach - first, the percentage of tweets containing geo-location information is very scarce (such tweets constitute less than 1% of our dataset). Second, when we consider individual users, it is unjustified to assume that all her tweets containing geo-location information will point to a single region, even if all her tweets contained geo-location information. Thus, the best estimate of the location of the user is the probability distribution of the user's location over the Indonesian provinces. We consider categorical distribution of the users into the thirty four provinces of Indonesia. The motivation behind employing categorical distribution instead of say Gaussian distribution over the entire landscape of Indonesia is that we want to obtain a political Heat Map of Indonesia with the granularity level of a province. Another possibility, that is feasible however not pursued by us in this current work, is dividing Indonesia in the form of grids with varied granularity. Finer granularity poses the problem of insufficient data from every grid, because, as mentioned previously, most tweets do not contain geo-location information. So, most grids will have no geo-location containing tweet. Our technique of Location Index computation is discussed in further details in the following section. Sentiment Analysis using social media data has been attempted by works such as [31] which tries to exploit patterns in online social media communication and also by [32] which uses background lexical information and refining of the same for specific domains by supervised learning techniques. However, we have computed Radicalization Indices using simpler text regression techniques similar to [33] and [34]. Our technique of Radicalization Index computation, which is verified to be quite accurate is discussed in further details in Section V. In summary, individual Twitter users are our chosen level of granularity - we obtain all the necessary information pertaining to each user. Next we characterize the user based on those information, not only on the radicalization scale but we also obtain a location distribution of the user over the regions of Indonesia. Hence, there is no prediction of the location of the user involved as in [1]. It is to be noted that we consider only the users classified as radical by our Radicalization Index computation method. Hence, our major contribution is the

development of a robust technique to obtain the political Heat Map of any geographic area. IV. L OCATION I NDEX C OMPUTATION As discussed earlier, each user Ui , 1  i  n has a home location HLi , 1  i  n associated with her, which may or may not be declared. Each tweet Ti,k , 1  i  n, 1  k  ti has a geo-location GLi,k , 1  i  n, 1  k  ti associated with it. However, GLi,k for some tweets Ti,k may not be known as the user Ui might turn her GPS off. Even when user Ui has a Declared Home Location DHLi , it may not be accurate. User Ui might intentionally or inadvertently misstate her location. Accordingly, we do not accept the DHLi at its face value as the Actual home location of Ui . Instead, we compute a matrix, which we term as the general Computed Home Location matrix gCHL, from the entire dataset barring the timespan (month in our case) for which the Heat Map is being generated. The created matrix gCHL is an m × m matrix where gCHLa,b , 1  a  m, 1  b  m, is the conditional probability of the Actual home location of a user being region Rb , when her Declared Home Location is region Ra , as learnt from the dataset. The gCHL matrix is computed using the following three steps provided in Algorithm 1. Thus, gCHLa,b is given by: gCHLa,b = X Y where, X = The number of tweets in T such that the author of the tweet has Declared Home Location as Ra and geo-location of the tweet is Rb Y = The number of tweets in T such that the author of the tweet has Declared Home Location as Ra Let the ath row of the gCHL matrix be denoted by gCHLa . Now Computed Home Location vector for the user Ui denoted by CHLi is assigned the value of gCHLa if the Declared Home Location of Ui is region Ra . It is to be noted that the gCHL matrix is general (and not user specific) and is computed using the entire Twitter data set comprising all users. From those tweets Ti,k , 1  k  ti of user Ui , that contain the geo-location information GLi,k (i.e., when the GPS is not turned off at the time of the tweet), we compute the Computed Geo Location vector CGLi of length m, where CGLi,j , 1  j  m, is the probability of the Actual home location of user Ui being region Rj , as learnt from the tweets of Ui . The CGLi,j is computed in the following way:
A CGLi,j = B

CHLi and CGLi , where CGLi is completely user-specific. However, CHLi is partially user-specific - it does depend on the user because CHLi is based on her Declared Home Location, but it also depends on the general distribution which depends on the entire population mass. It is evident that both CHLi and CGLi are categorical distribution over the thirty four Indonesian provinces. Now, we know that a mixture of discrete distributions over any finite number of categories is just another distribution over those categories. In order to combine CGLi and CHLi we obtain a convex combination of the two to obtain Li,j in the following way: Li,j = (1 - i )  CHLi,j + i  CGLi,j (1)

Now, the mixture weights i for the user Ui is learnt from the data itself and is calculated as i = |Ti |/|Ti |. Li,j essentially is given by Li,j = |Ti |  CHLi,j + |Ti |  CGLi,j (2)

which gives equation (1) when normalized by |Ti | = |Ti | + |Ti | i.e the total number of tweets posted by the user Ui where, · Ti = set of tweets produced by user Ui · Ti = subset of Ti and represents the set of tweets by Ui that contains geo-location information · Ti = subset of Ti and represents the set of tweets by Ui that do not contain geo-location information The motivation behind this definition of the mixture weight is that for the Ti tweets which contain geo-location information, we consider the user-specific location distribution information inferred from the particular user's geo-location containing tweets. However, for the tweets of Ti , we have no location information except for the general information that given a Declared Home Location for any user Uv in our dataset as Ra , the location distribution for Uv is CHLv = gCHLa . Thus, if the Declared Home Location of Ui is given to be Ra , we consider CHLi = gCHLa . Evidently, we depend on this semi-user-specific location distribution information for the tweets Ti of Ui . This simple formulation of Li,j also captures the fact that we rely more on CGLi than on CHLi when the number of tweets with geo-location information, generated by Ui is high, however if that count is low ( or even absent), instead of discarding the particular user's information, we obtain the location distribution of Ui from her Declared Home Location. We experimented by using only geo-location containing tweets and we saw that the results are far more accurate if we included users of Type 3 - This is intuitively correct because the geo-location containing tweets form less than 1% of the entire dataset. As noted earlier, the set of users can be divided into four different classes. We do not try to compute Li,j values for the users belonging to Class 4. For users belonging to the other three classes, we compute Li,j using equation (1). For the users belonging to Class 3, we obtain i to be zero, as we do not have any geo-location data from the tweets to compute i .

where, A = The number of tweets in Ti whose geo-location is Rj and B = The number of tweets in Ti whose geo-location is known We thus obtain two pieces of information about the Actual home location of the user Ui in the form of two distributions:

Algorithm 1 Counting Algorithm for computation of the general Computed Home Location gCHL
· · ·

Step 1: Initialize gCHLa,b = 0, 1  a  m, 1  b  m Step 2: For each tweet tw in T , increment gCHLa,b if Declared home location of the author of tw and the geo-location of tw are Ra and Rb respectively Step 3: Make each row gCHLa of gCHL matrix row stochastic, 1  a  m

V. R ADICALIZATION I NDEX C OMPUTATION We intend to assign a Radicalization Index RDi to a Twitter user Ui based on the content of her tweets. Each tweet can contain up to 140 characters. Thus the content of a single tweet does not provide adequate information regarding the user's ideology. We collect tweets from users over a period of time (in our case a month) and for each user Ui we create a document Di that contains all the tweets Ti of that user, during that period of time. As there exists a one-to-one correspondence between Ui and Di , by assigning a Radicalization Index to Di , we essentially assign a Radicalization Index RDi to Ui . Classical predictive model Multiple Linear regression [23], [24], [25] fits our application, since it is a dichotomous classification problem with multiple predictor variables, where the predictor variables are the terms of our "vocabulary". Classical classification methods such as Logistic Regression which has applications in a wide variety of domains can also be used for document classification [26]. Thus, Logistic Regression can also be applied for our problem. However, Linear Regression was selected instead of Logistic Regression because it out-performed the Logistic one through 10-fold cross validation. This well-known technique divides the given dataset into 10 segments and then uses 90% of the data ( i.e nine segments) as training data and 10% of the data ( i.e. one segment ) as the test data. Linear Regression showed around 98% of accuracy, but Logistic Regression showed 83-85% of accuracy. The implementation of our approach proceeds in the following way: First, we identify a set of Indonesian political organizations. Next, social scientists in our Minerva team, who are domain experts for Indonesia, hypothesize a classification to label each organization as radical or counter radical based on these organizations beliefs and practices. Using web crawling tools, we download a large number of documents from the web sites of these organizations. We use the term "vocabulary" to mean the set of all unique terms that appear in all documents from all organizations. All the documents of an organization are assigned the same Radicalization Index as the Radicalization Index assigned to the organization by the domain experts in our team. This set of documents together with their Radicalization Indices form the training dataset for our model. After that we use the model to assign a Radicalization Index to the document Di created from the tweets of user Ui . This Radicalization Index of document Di is taken to be the Radicalization Index of user Ui . A. Problem Formulation: We formulate the problem in a general sparse learning framework and solve the following optimization problem (3)

using the techniques from [27] . This is indeed a sparse learning problem because the vocabulary is very large compared to the number of words used in a document. 1  2 2 Ax - y 2 + x 2+ x 2 2 where A  Rs×p , y  Rs×1 , and x  Rp×1 min
x 1

(3)

In our application, we have · A is Document × Term matrix which is constructed as follows: The set of terms (t1 , . . . tp ) includes all the terms from all the documents by all the organizations, barring the stop words. The size of the vocabulary in this case is p. If data is collected by crawling web sites of different organization (O1 , . . . , Oq ) and documents (di,1 , . . . , d1,ri ) are collected from the web site of organization Oi , 1  i  q , the total number of rows of the matrix A is s = q i=1 ri , and it has the following structure. Document/T erm d1 d2 .... ds
· ·

t1 .... .... .... ....

t2 .... .... .... ....

.... .... .... .... ....

tp .... .... .... ....

·

Aij = term f requency of the j th term in the ith document such that Aij  0, 1  i  s, 1  j  p. yi  {+1, -1} is the class of each document Di , 1  i  s. As indicated earlier, the Radicalization Index of a document is the same the Radicalization Index of the organization that created that document. Thus, when an organization is labeled as radical (or counter radical) by the domain experts, all the documents pertaining to that organization is marked as +1 (or -1). Thus yi = +1 if Di , 1  i  s belongs to an organization marked as radical by the experts, or yi = -1 if Di , 1  i  s belongs to an organization marked as counter radical by the experts. xj is the weight for each term tj , 1  j  p. This is the parameter estimated by optimizing the objective function (3). The xj 's thus form the predictor variables of the model.

Let us further clarify the three terms involved in the convex optimization problem:
·

Ax - y 2 - this first term is related to the sum of the squared errors to fit a straight line to a set of data

1 2

2

points. The objective function (3) thus is the optimization problem of minimizing this sum of squared-errors. 2  · 2 x 2 - this term deals with the ridge regression, which is an extra level of shrinkage. We set  = 0 as we were mainly driven by sparsity. ·  x 1 - this term involving the L1 norm deals with the sparsity of the solution vector x. For different values of  we obtain a solution vector x which represents the weights associated with each term tj , 1  j  p ( the same terms which are considered in the A matrix). Some of these weights are positive, some negative (values can be very close to 0). The terms with positive (or negative) weights are the radical (or counter radical) words. The top (ones with weights having high magnitude) radical and counter radical words are presented to the experts for validation. We experiment with several  values resulting in x vectors of various sparsity until the list of top radical and counter radical words are approved by the field experts. We use the Matlab implementation of the SLEP package [28] that utilizes gradient descent approach to solve the optimization problem (3). This package can handle matrices of 20M entries within a couple of seconds on a machine with standard configuration. The input to the SLEP package are the values of A, , and y . The SLEP model outputs the weight vector x. B. Assignment of Radicalization Index: For each time period (in our case one month), each user Ui will be assigned Radicalization Index RDi based on their tweets within that period. This is done as follows: · As mentioned earlier, each tweet which can only contain a maximum of 140 characters is too insufficient for inferring the radicalism of the user. Hence, from the tweets of each user Ui we form a User Document Di which is the conglomeration of all her tweets over a period of one month. It is to be noted here that many users choose to tweet quite infrequently, hence even if we collect tweets for one month, a user might have tweeted only once or twice during the entire one month which defeats the purpose of collecting tweets for a month. Hence, we further apply the constraint that we consider only those users who have tweeted at least seven times in a month. The value of this threshold has been arrived at empirically after experimentation with various values of the threshold. · With the help of the model that has been fitted using the organization documents, we classify the User Documents. Let each User Document Di which is a termf requency row be denoted by the row vector tc of count of terms from our "vocabulary". · Each user Ui receives a `score' which we refer to as Radicalization Index RDi of user Ui . RDi is given by
p

where p is the size of our vocabulary. This provides us a time-series of RDi values for the users, which will make it possible to analyze the transition dynamics for each user. It is evident that a high positive RDi indicates that Ui is highly radical whereas a high negative RDi indicates that Ui is highly counter radical. VI. H EAT I NDEX C OMPUTATION Once we have obtained the Location Indices Li , 1  i  n and Radicalization Indices RDi , 1  i  n, for all the users Ui , 1  i  n , the Heat Index Hj of region Rj , 1  j  m is computed as Hj =
n i=1

RDi × Li,j , j, 1  j  m.

The Heat Index Hj for a region Rj indicates the degree of prevalence of radical ideologies among the people of Rj by taking into account both the number of radical tweeters living in Rj and also their `degree of radicalism'. In Table I we present a time-varying Heat Map of Indonesia by computing the map in three different time intervals of October 10 November 10, November 11 - December 10 and December 11 - January 10. We found a drastic change in the heat indices during the interval of November 10 ­ December 10. But we could not discern any particular event which could have triggered the same. VII. DATA C OLLECTION Since our model requires the computation of both the Radicalization Index RDi as well as the Location Index Li for each user Ui , we followed a two step data collection procedure described as follows: · For the purpose collecting the training data set for computing the Radicalization Index, we crawled the websites of 36 well-known Indonesian organizations which are classified as radical or counter radical by our field experts. A few of the organizations are mentioned in Table II. We crawled the websites of all these different organizations and collected a total of 78,135 documents which after pre-processing and filtering resulted into 49,250 documents. The reason for the reduction from the number of crawled documents to the number of useful documents is that many of the crawled documents did not have any relevant information (for example documents having only advertisements) and hence were discarded during pre-processing. Each of the documents on a average contained 280 words i.e on an average 2880 characters. All documents pertaining to an organization were labeled as radical or counter radical depending on the outlook professed by the organization itself. These were then used for fitting our Radicalization Index computation model. · For our study on recovery of political signals pertaining to trend of radical activities in Indonesia, we chose Twitter as the data collection platform as Indonesia features as one of the top five global market segments of Twitter by reach, and accounts for 19.0% to 20.8% of Twitter's

RDi = tc .x =
j =1

tcj xj

TABLE I: The table provides the top 5 province or special region names based on their computed Heat Index values (also mentioned alongwith) for October 10 - November 10, November 11 - December 10, December 11- January 10
Province Name Jakarta East Java West Java Yogyakarta Central Java Heat Index 5.48 2.95 2.68 1.74 1.68 Province Name Jakarta East Java Yogyakarta Central Java West Java Heat Index 16.16 12.33 4.53 3.7 3.39 Province Name Jakarta Yogyakarta West Java East Java Central Java Heat Index 4.71 1.82 1.25 1.20 0.69

TABLE II: Table showing some of the well-known radical and counter radical organizations of Indonesia
Radical Organizations AdianHusaini PKS Arrahmah AbuJibriel EraMuslim HizbutTahrir MillahIbrahim Counter radical Organizations DaarulUluum Interfidei IslamLiberal NU PPIM Paramadina LKIS

TABLE III: Keyword markers used for filtering Twitter Stream API
Keyword "penegakan syariah" "jihad majelis" "mati syahid" "ajaran islam" "pendidikan agama di sekolah" "asasi manusia" "demokrasi yang" "kebebasan beragama" "sekularisme" "di negeri negeri islam" Interpretation enforcement of Sharia jihad assemblies martyrdom the teaching of Islam religious education in schools human rights democracy religious freedom secularism Islamic state in the country

total reach by country (Dec 2010)3 . No other publicly available portal offers access to opinions posted online by the Indonesian populace on a similar scale as does Twitter. For gathering tweets, we use Twitter's Stream API to access Twitter's global stream of publicly available tweet data. Since our goal is to recover "political signals", we setup a keyword filter on the Stream API to gather tweets that relate to radical and counter radical ideologies. The keywords used for this filtration have been identified by the social scientists in our Minerva project team and are considered to be significant markers of radical and counter radical ideologies in the Indonesian context. The keyword list includes radical markers and a few such markers are listed in Table III. We collected tweet data for a three-month interval and gathered a total of 12,152,874 tweets from October 10, 2012 to January 10, 2013 ( Figure 2) that matched the keyword filtration criteria ( Table III ). We used the three months of data to calculate the Radicalization Indices of the users. In this research, we are interested in the probability distribution
3 http://www.billhartzer.com/pages/comscore-twitter-latin-america-usage/

Location Index Li of user Ui over the thirty four provinces of Indonesia, thus we focus only on users from Indonesia. The keywords used are in Indonesian language and narrows down the tweets we obtained from the Twitter API. Thus, the geocode in majority of cases indicated a location in Indonesia. However, not all geo-codes are from Indonesia. We ignore those tweets in the current work. Thus, out of these 12 million tweets, 110,063 tweets contained geo-locations that mapped to regions within Indonesia. To apply this reverse geo-coding, we used the OpenStreetMap API4 . A user repository was constructed by including only those users whose Declared Home Locations matched with an identifiable Indonesian city or province. Now the user declared home location which the user mentions as a part of her profile could consist of any text according to the user's whim. We found texts such as "Dark side of the moon" or "somewhere in this big world" or "Here" or "infront of my laptop" and hence, there is a need for pre-processing of the text. Also, the users provided location information to varied degrees of granularity ranging from continents to towns, however we are interested in the fixed granularity level of Indonesian provinces and the special regions such as Jakarta and Yogyakarta. Hence we manually created a database of towns and cities of all of the Indonesian provinces. Each of the provinces were annotated with 42 cities/ towns on an average with Papua being the highest which was annotated with 70 cities/towns. Using this database we then assigned a legitimate Declared Home Location to as many users as possible. The final user repository consisted of 959,911 unique users.

Fig. 2: Figure showing the number of tweets collected over our observation period

http://www.comscoredatamine.com/2011/02/the-netherlands-leads-globalmarkets-in-twitter-reach/

4 The relevant information about the API could be found at http://wiki.openstreetmap.org/wiki/Nominatim

VIII. E XPERIMENTAL R ESULTS We created Heat Maps of Indonesia on a monthly basis. We computed the RDi of each user U i for each month from October 10 to January 10, as long as Ui sent at least 7 tweets in that month Again, for each user Ui we computed the Location Index Li by considering all her tweets over the period of the month.. For that we computed the general Computed Home Location gCHL matrix. The(i, j )th entry gives the probability of a user with a Declared Home Location of Ri being located in Rj , 1  i  m, 1  j  m.
·

IX. VALIDATION For the purpose of validation of the Radicalization Index, we computed the Radicalization Indices of some well-known counter radical leaders of Indonesia for the months that they had tweeted for more than 7 times which we consider as our threshold. Our classifier gave perfect accuracy. By accuracy of the classification we mean the percentage of time the leaders who are thus known to be counter radical were classified as counter radical by our classifier. We did not validate the Location Index computation technique because of the lack of the ground truth of the Actual home location of users. However, our results of Heat Index are validated by the findings of the Indonesia-based Wahid Institute5 (named after Abdurrahman Wahid, an Indonesian Muslim religious and political leader who served as the President of Indonesia from 1999 to 2001). Wahid Institute promotes a moderate version of Islam through dialogue events, publications, and public advocacies. The institute also releases an annual religious freedom report on religious life in Indonesia. According to the Wahid Institute's Annual Report of 20126 , the top four provinces of Indonesia where radical activities are most observable are West Java, Aceh, East Java, and Central Java. It may be noted here, that three out of the four most radical provinces identified by the Wahid Institute, also appear at the very top of our list. Also, our field experts have confirmed Jakarta to be a center of radical activities. It may be mentioned here that field studies7 in January 2012 by Setara Institute8 , a well-known NGO based in Indonesia, showed that the strong radicalism of the young muslim population in Yogyakarta and Central Java are making them hot targets to be recruited as Jihadists. In May 2012, a mob attack by Indonesian Mujahidin Council on a book launch of a well-known Canadian author, an advocate of LGBT, took place in Yogyakarta. In September 2012, there has been arrests of potential terrorists from Yogyakarta9 . Because, Wahid Institute has mentioned about Indonesian provinces only, it might be expected that Jakarta and Yogyakarta, being special administrative regions, are missing from their list however, we do not have access to their full report. The high radicalism of the Java provinces are also corroborated by reports of the Setara Institute. The only radically active province that shows up in the Wahid Institute report but does not appear at the top of our list is Aceh, located at the north west corner of Indonesia. It is worth mentioning here that Aceh was completely devastated by the 2004 Indian Ocean Tsunami and is still recovering from its effects. Aceh is also one of the least economically developed provinces of Indonesia. We believe that due to the lack of economic advancement in Aceh, the level of Internet penetration in Aceh is fairly small and not many people from Aceh are active tweeters. This may explain
5 http://berkleycenter.georgetown.edu/resources/organizations/wahidinstitute 6 Released on December 28, 2012 7 http://www.setara-institute.org/en/content/study-shows-how-youngradical-indonesian-muslims-become-terrorists 8 http://www.setara-institute.org/ 9 http://www.washingtontimes.com/multimedia/image/indonesia-terrorjpg/

gCHL matrix : The gCHL matrix provides interesting insights on the Indonesian population. We computed the gCHL - matrix on all possible doublets among the three months of observation period. i.e for each month for calculating the Location Indices Li of users, we have generated the gCHL matrix using the other two months of data. Thus, in each case, we had training data of two months and test data of one month. Among the three gCHL - matrices generated, we saw that the Declared Home Location of users give us a good insight on the Actual home location of the user. Thus, instead of merely depending on the geo-co-ordinates of users, we should consider the home location from the user's profile and home location declarations are much more abundant than geo-location containing tweets. However, depending solely on Declared Home Locations can be deceptive. We also observed that people with Declared Home Locations in various different provinces from all around Indonesia such as Bangka Belitung, Banten, Maluku, West Nusa Tenggara, East Nusa Tenggara and Papua have a very strong tendency to have high probability of having Actual home location in Jakarta (as observed from our results over three months). This is very intuitive because Jakarta being the Capital Region must have attracted people from different parts of Indonesia for prospective settlement. We further made an interesting observation that people with Declared Home Location of East Kalimantan have considerable geo-location containing tweets from Central Kalimantan.

The Heat Indices values for the thirty four Indonesian provinces are computed using our approach for three months of our observation period - namely October 10 - November 10, November 11 - December 10, December 11 -January 10. Among all Indonesian provinces the top five provinces and special regions along with their Heat Index values are presented in Table I for the three months. Color maps of Indonesia with Heat Indices is shown in Figure 3, where darker colors indicate a higher level of radical tweeting, and lighter colors indicate a lower level of radical tweeting. It may be seen from Figure 3 that the area around Jakarta and the Java provinces are highly active in radical tweet creation. According to our Twitter data analysis, the provinces Jakarta, East Java, Yogyakarta and Central Java, along with West Java are the top provinces that generate a high level of radical activities.

(a) Heat Map for October 10 to November 10

(b) Heat Map for November 11 to December 10

(c) Heat Map for December 11 to January 10

(d) Radicalism Scale

Index

Fig. 3: Heat Maps of Indonesia

the reason for Aceh not showing up among our list of top radically active provinces. X. C ONCLUSION We have developed a generic robust technique for recovering signals pertaining to a geographical area such as a country using Twitter Data. We have applied our technique to our Indonesian dataset and have observed high accuracy. The goal of our work is the generation of a political Heat Map of Indonesia which will clearly indicate the provinces of Indonesia where radical narrative is prominent. The granularity of the Radicalization Index Assignment used is a Twitter user, while the granularity of regions used is an Indonesian Province. Thus, we have analyzed tweets made by a user Ui in a month to assign a Radicalization Index RDi to Ui - Thus RDi indicates how radical is Ui in her political outlook . Also, by mining the tweets in our database we assign a Location Index Li to Ui . For computation of the Location Index, the sources of location information used are not only the geolocation tagging as provided by Twitter API for the tweets which contain that information (such tweets constitute less than 1% of our dataset), but we also use the user declared home location information from her profile (after considerable amount of pre-processing and cleansing). We have combined these two sources of information for inferring the probability distribution of the location of the users among the provinces of Indonesia. Thus, by considering the RDi 's for all the users over a period of one month, in conjunction with the Location Indices Li 's we generate the political Heat Maps of the likes

of Figure 3. We have got a time series of Heat Maps, which can help us in mapping trends by regions in radical discourse. Such Heat Maps can prove to be very useful in studying the spatio-temporal dynamics of the people of Indonesia so far as their political outlook is concerned R EFERENCES
[1] Cho, E., Myers, S. A., & Leskovec, J. (2011, August). Friendship and mobility: user movement in location-based social networks. In Proceedings of the 17th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1082-1090). ACM. [2] Xu, J. M., Bhargava, A., Nowak, R., & Zhu, X. (2012). Socioscope: spatio-temporal signal recovery from social media. In Machine Learning and Knowledge Discovery in Databases (pp. 644-659). Springer Berlin Heidelberg. [3] Priedhorsky, R., Culotta, A., & Del Valle, S. Y. (2013). Inferring the Origin Locations of Tweets with Quantitative Confidence. arXiv preprint arXiv:1305.3932. [4] Yang, D., Zhang, D., Yu, Z., & Wang, Z. (2013, May). A sentimentenhanced personalized location recommendation system. In Proceedings of the 24th ACM Conference on Hypertext and Social Media (pp. 119-128). ACM. [5] Rahimi, S. M., & Wang, X. (2013). Location Recommendation Based on Periodicity of Human Activities and Location Categories. In Advances in Knowledge Discovery and Data Mining (pp. 377-389). Springer Berlin Heidelberg. [6] Li, Y., Steiner, M., Wang, L., Zhang, Z. L., & Bao, J. Exploring Venue Popularity in Foursquare. [7] Li, Y., Steiner, M., Wang, L., Zhang, Z. L., & Bao, J. (2012, December). Dissecting foursquare venue popularity via random region sampling. In Proceedings of the 2012 ACM conference on CoNEXT student workshop (pp. 21-22). ACM. [8] Gonzalez, M. C., Hidalgo, C. A., & Barabasi, A. L. (2008). Understanding individual human mobility patterns. Nature, 453(7196), 779-782.

[9] Sadilek, A., Kautz, H., & Bigham, J. P. (2012, February). Finding your friends and following them to where you are. In Proceedings of the fifth ACM international conference on Web search and data mining (pp. 723-732). ACM. [10] Noulas, A., Scellato, S., Lambiotte, R., Pontil, M., & Mascolo, C. (2012). A tale of many cities: universal patterns in human urban mobility. PloS one, 7(5), e37027. [11] Lu, X., Bengtsson, L., & Holme, P. (2012). Predictability of population displacement after the 2010 Haiti earthquake. Proceedings of the National Academy of Sciences, 109(29), 11576-11581. [12] Chang, H. W., Lee, D., Eltaher, M., & Lee, J. (2012, August). @ Phillies Tweeting from Philly? Predicting Twitter User Locations with Spatial Word Usage. In Advances in Social Networks Analysis and Mining (ASONAM), 2012 IEEE/ACM International Conference on (pp. 111-118). IEEE. [13] Mahmud, J., Nichols, J., & Drews, C. (2012). Where is this tweet from? inferring home locations of twitter users. Proc AAAI ICWSM, 12. [14] Eisenstein, J., O'Connor, B., Smith, N. A., & Xing, E. P. (2010, October). A latent variable model for geographic lexical variation. In Proceedings of the 2010 Conference on Empirical Methods in Natural Language Processing (pp. 1277-1287). Association for Computational Linguistics. [15] Cheng, Z., Caverlee, J., & Lee, K. (2010, October). You are where you tweet: a content-based approach to geo-locating twitter users. In Proceedings of the 19th ACM international conference on Information and knowledge management (pp. 759-768). ACM. [16] Hecht, B., Hong, L., Suh, B., & Chi, E. H. (2011, May). Tweets from Justin Bieber's heart: the dynamics of the location field in user profiles. In Proceedings of the 2011 annual conference on Human factors in computing systems (pp. 237-246). ACM. [17] Boutet, A., Kim, H., & Yoneki, E. (2012, August). What's in Twitter: I Know What Parties are Popular and Who You are Supporting Now!. In Advances in Social Networks Analysis and Mining (ASONAM), 2012 IEEE/ACM International Conference on (pp. 132-139). IEEE. [18] Dahlin, J., Johansson, F., Kaati, L., Martenson, C., & Svenson, P. (2012, August). Combining Entity Matching Techniques for Detecting Extremist Behavior on Discussion Boards. In Advances in Social Networks Analysis and Mining (ASONAM), 2012 IEEE/ACM International Conference on (pp. 850-857). IEEE. [19] Eisenstein, J., Ahmed, A., & Xing, E. P. (2011, June). Sparse additive generative models of text. In International Conference on Machine Learning (ICML). [20] Hong, L., Ahmed, A., Gurumurthy, S., Smola, A. J., & Tsioutsiouliklis, K. (2012, April). Discovering geographical topics in the twitter stream. In Proceedings of the 21st international conference on World Wide Web (pp. 769-778). ACM. [21] O'Connor, B., Eisenstein, J., Xing, E. P., & Smith, N. A. (2010). Discovering demographic language variation. In Workshop on Machine Learning for Social Computing at NIPS. [22] Yin, Z., Cao, L., Han, J., Zhai, C., & Huang, T. (2011, March). Geographical topic discovery and comparison. In Proceedings of the 20th international conference on World wide web (pp. 247-256). ACM. [23] Zhang, T. (2009). Some sharp performance bounds for least squares regression with L1 regularization. The Annals of Statistics, 37(5A), 2109-2144. [24] S.Kim, K.Koh, M.Lustig, S.Boyd and D. Gorinevsky, An Interior-Point Method for Large-Scale l1 - Regularized Least Squares Journal of seleccted topics in Signal Processing, Vol. 1, No. 4, pages 606 - 617, Dec 2007 [25] Kolter, J. Z., & Ng, A. Y. (2009, June). Regularization and feature selection in least-squares temporal difference learning. In Proceedings of the 26th Annual International Conference on Machine Learning (pp. 521-528). ACM. [26] Brzezinski, J. R., & Knafl, G. J. (1999). Logistic regression modeling for context-based classification. In Database and Expert Systems Applications, 1999. Proceedings. Tenth International Workshop on (pp. 755-759). IEEE. [27] Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths for generalized linear models via coordinate descent. Journal of statistical software, 33(1), 1. [28] J. Liu, S. Ji and Jieping Ye. SLEP: Sparse Learning with Efficient Projections, Arizona State University (2009) [29] Lee, S. I., Lee, H., Abbeel, P., & Ng, A. Y. (2006, July). Efficient L~ 1 Regularized Logistic Regression. In Proceedings of the National

[30] [31]

[32]

[33]

[34]

[35]

Conference on Artificial Intelligence (Vol. 21, No. 1, p. 401). Menlo Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999. T. P. Minka, A comparison of numerical optimizaers for logistic regression, Technical report, 2007 Thelwall, M., Buckley, K., Paltoglou, G., Skowron, M., Garcia, D., Gobron, S., ... & Holyst, J. A. (2013). Damping Sentiment Analysis in Online Communication: Discussions, Monologs and Dialogs. In Computational Linguistics and Intelligent Text Processing (pp. 1-12). Springer Berlin Heidelberg. Melville, P., Gryc, W., & Lawrence, R. D. (2009, June). Sentiment analysis of blogs by combining lexical knowledge with text classification. In Proceedings of the 15th ACM SIGKDD international conference on Knowledge discovery and data mining (pp. 1275-1284). ACM. Joshi, M., Das, D., Gimpel, K., & Smith, N. A. (2010, June). Movie reviews and revenues: An experiment in text regression. In Human Language Technologies: The 2010 Annual Conference of the North American Chapter of the Association for Computational Linguistics (pp. 293-296). Association for Computational Linguistics. Kogan, S., Levin, D., Routledge, B. R., Sagi, J. S., & Smith, N. A. (2009, May). Predicting risk from financial reports with regression. In Proceedings of Human Language Technologies: The 2009 Annual Conference of the North American Chapter of the Association for Computational Linguistics (pp. 272-280). Association for Computational Linguistics Brynielsson, J., Horndahl, A., Johansson, F., Kaati, L., Martenson, C., & Svenson, P. (2012, August). Analysis of weak signals for detecting lone wolf terrorists. In Intelligence and Security Informatics Conference (EISIC), 2012 European (pp. 197-204). IEEE.

Spatio-Temporal Signal Recovery from Political
Tweets in Indonesia
Anisha Mazumder, Arun Das, Nyunsu Kim, Sedat Gokalp, Arunabha Sen, Hasan Davulcu
School of Computing, Informatics and Decision Systems Engineering
Arizona State University
Tempe, Arizona - 85287
Email: {Anisha.Mazumder, adas22, nkim30, Sedat.Gokalp, asen, hdavulcu}@asu.edu
AbstractâOnline social network community now provides an
enormous volume of data for analyzing human sentiment about
people, places, events and political activities. It is increasingly
clear that analysis of such data can provide great insights on the
social, political and cultural aspect of the participants of these
networks. As part of the Minerva project, currently underway
at Arizona State University, we have analyzed a large volume
of Twitter data to understand radical political activity in the
provinces of Indonesia. Based on analysis of radical/counter
radical sentiments expressed in tweets by Twitter users, we
create a Heat Map of Indonesia which visually demonstrates the
degree of radical activities in various provinces of Indonesia.
We create the Heat Map of Indonesia by computing (i) the
Radicalization Index and (ii) the Location Index of each Twitter
user from Indonesia, who has expressed some radical sentiment
in her tweets. The conclusions derived from our analysis matches
significantly with the analysis of Wahid Institute, a leading
political think tank of Indonesia, thus validating our results.
Index Termsâradical, tweet, Radicalization Index, Location
Index, Heat Map

I. I NTRODUCTION
The sheer popularity of online social media nowadays is
reflected by the immense amount of data being fed every
second by people from all over the world. It is becoming
increasingly evident that analysis of this huge online dataset
can provide great insights on the social, political and cultural
aspect of the Twitter users and possibly the non-Twitter users
as well. In [2], the authors have developed Socioscope, a tool
for extracting signal from noisy social media data. Utilizing
a Socioscope like mechanism, we have developed a tool for
recovering spatio-temporal signals from tweets generated in
Indonesia. Our interest in analyzing tweets from Indonesia
developed in the context of the Minerva1 project, currently
underway at Arizona State University. The goal of this project
is to increase the understanding of movements within Muslim
communities towards radicalism or counter radicalism. Based
on the support and opposition of certain beliefs and practices
of an individual (as expressed in her tweet), we can assign a
Radicalization Index to that individual. In addition, from the
self declared home location of a Twitter user and the locations
of her tweets, we can compute a distribution of Location
Index for that user. The map of Indonesia is divided up into
a set of regions and the Location Index of a user provides the
1A

project sponsored by the U.S. Department of Defense

probability of the user to be in a specific region at a specific
time. For this analysis a region corresponds to a province of
Indonesia. Finally, from the Radicalization Index and Location
Index of individuals, Heat Index of a region , which is a
composite measure of the number of radical tweeters of that
region and their âdegree of radicalismâ, is computed.
In our model we have a set of tweeters (or users), U =
{U1 , U2 , . . . , Un }. Each user Ui , 1 â¤ i â¤ n creates a set of
tweets Ti = {Ti,1 , Ti,2 , . . . , Ti,t
Sin}. The set of all tweets by
all users is denoted by T = i=1 Ti . The geographic area
from where the tweets originate is divided into a set of regions
R = {R1 , R2 , . . . , Rm }. In our study m is equal to thirty four,
the number of provinces and special administrative regions of
Indonesia. Each user Ui , 1 â¤ i â¤ n has a home location
HLi , 1 â¤ i â¤ n associated with her, which may or may not
be declared. Each tweet Ti,k , 1 â¤ i â¤ n, 1 â¤ k â¤ ti has a
geo-location GLi,k , 1 â¤ i â¤ n, 1 â¤ k â¤ ti associated with
it. However, GLi,k for some tweets Ti,k may not be known
as the user Ui might turn her GPS off. Accordingly, we can
divide the set of users in four different classes:
(i) Class 1: user Ui whose home location is declared and
geo-location of at least one tweet is known,
(ii) Class 2: Ui whose home location is not declared and
geo-location of at least one tweet is known,
(iii) Class 3: Ui whose home location is declared and geolocation of none of the tweets are known, and
(iv) Class 4 : Ui whose home location is not declared and
geo-location of none of the tweets are known.
From the input data set (U, T, R ), we compute, (i) Location
Index, Li of each user Ui , 1 â¤ i â¤ n, (ii) Radicalization Index,
RDi of each user Ui , 1 â¤ i â¤ n, and finally, combining Li
and RDi , we compute (iii) Heat Index, Hj of each region
Rj , 1 â¤ j â¤ m. It may be noted that whereas RDi , 1 â¤ i â¤ n
is a scalar value, Li is a vector of size m, (Li,1 , . . . , Li,m ),
where Li,j indicates the probability of user Ui being located
in region Rj i.e. Li,j indicates the probability of the Actual
home location of Ui being Rj . Finally, the HeatPIndex Hj of
n
region Rj , 1 â¤ j â¤ m is computed as Hj = i=1 RDi Ã
Li,j , âj, 1 â¤ j â¤ m. We thus provide a generic technique for
generating time-varying political Heat Maps of a geographical
region based on the Twitter data analysis. Throughout this
paper we have used âregion â and âlocationâ interchangeably

to mean an âIndonesian Provinceâ. It is to be noted that for
our calculations, we have considered all Indonesian provinces
including special administrative regions such as Yogyakarta
and special capital region such as Jakarta.
II. R ELATED W ORK
Computation of Heat Map of Indonesia requires the computation of the following: First, we compute the Radicalization
Index of a user Ui by analyzing the content of her tweets.
Second, Location Index of the user Ui is computed from her
geo-location containing tweets (if any) and also from her home
location declared as a part of her Twitter profile (if at all
provided). It is to be noted that we do not consider users
who have neither of these two sources of location information
present.
Identification of the location of users using Twitter data
has been quite a focus of recent research. Inferring location
from tweets have been pursued by [14], [15], [16]. Studies
conducted in [4], [5], [6], [7] combine location information
and text from social-network data history to infer various questions such as user preferences and provide recommendations.
However, we do not rely on any âchecking inâ information for
our computations and providing recommendations is not our
goal.
We do employ the notion of regions - the thirty four
provinces of Indonesia are the regions of interest for our problem. Thus, âgeo-codingâ (the use of gazetteers) is applicable to
our problem. However, just as in [3], we too argue that location
estimates are multi-modal probability distributions, rather than
particular points or regions. However, it may be noted that in
contrast to [3], we are interested only in Indonesia and in
Indonesian provinces - thus our estimate of the location of the
user must be the probability of each Indonesian province as the
Actual home location of the user under consideration, rather
than the probability of the user being located in each and every
point on the surface of the earth. This implies that our world
comprises of Indonesia only and individual geo-co-ordinates
are bunched into the corresponding province of Indonesia. As
a result, we apply the combination of âgeocodingâ and the
modification of the techniques in [3]. Thus, we use gazetteers
for the Declared Home Location of the Twitter Users to map
those to a specific province of Indonesia (This is explained in
further details in Section VII ). This combined with the geo-coordinate information about the user (obtained from her tweets
containing geo-location) gives us the probability distribution
of the user across the thirty four provinces of Indonesia. We
thus obtain a simple yet effective means of computing the
geo-location of the user as compared to other more complex
methodologies such as Topic Detection Techniques [20], [21],
[22].
Human mobility is modeled as a stochastic process in [8].
Following the studies of [8], in [1], the authors study the
manner in which the movements of human beings are related
to time of the day, geography as well as social ties. They intend
to predict the exact location of a person based on various
factors which the authors have identified, including impact of

social network. Similar problems have been studied by [9],
[10], [11]. However, in our problem, there is no notion of
prediction of location of users involved. Besides, we consider
categorical distribution. However, we do use the concept of
mixture of distributions in the lines of [1].
Another line of research which focuses on location estimation by content-analysis of the tweets of a user has been
studied by [12], [13]. They use the techniques of feature
selection from tweets of users, following it up with training
and classification. However, we do not apply content based
analysis in this current work, but rely on the geo-location
containing tweets of users in our dataset and also the user
declared home location to obtain the location distribution of
the users.
In [17], the authors analyze tweets generated during the
United Kingdom 2010 General Election to measure political
sentiments as well. They have identified the specific features
of the political parties and their ultimate goal is to infer
the political affiliation of a user based on her tweets. We
also study a similar problem, however our goal is not to
identify the political affiliations of users, rather we compute
the âdegree of radicalismâ of the user. Besides, our technique
is completely different from theirs. Unlike them, we apply a
very simple yet effective term-frequency analysis of tweets and
leverage heavily on our team of domain experts. We validated
our classification of users into radicals and counter radicals
by classifying some well-known counter radical leaders of
Indonesia (Our validation process is discussed in further details
in Section IX).
The work in [35] which is followed by [18] is very relevant
to our technique of Radicalization Index assignment to users.
These works too deal with the recovery of radical signals
from the online posts of social media users and thereby
identify individuals as potential âlone wolf terroristsâ. They
specifically focus on presenting a framework for combining
entity matching techniques for detecting extremist behavior
on discussion boards. These âlone wolf terroristsâ might leave
weak signals of radicalism through their comments or posts
on discussion boards, signals made further weaker by the use
of aliases. Identification and analysis of such weak signals
of radicalism by the use of topic-filtered web harvesting as
well as application of natural language processing techniques,
thereby fusing aliases for identifying the person form the basis
of the works of [35]. Their work is fundamentally different
from ours because we deal specifically with the usersâ publicly
available tweets only - this eliminates the availability of the
vital background information such as characteristic ( âradical
internet forumâ, âcapability internet forumâ ) annotation of particular discussion boards that is leveraged in [35] . However,
we also have used the technique of application of keyword
analysis and crawling of web-sites of well-known radical/
counter radical organizations of Indonesia as discussed in later
sections. Furthermore, [35] and [18] do not deal with location
profiling of users which is one of the two major goals of our
work.

Fig. 1: The flow diagram of our Heat Map computation technique. The Web data mentioned here refers to the documents
generated by crawling the web pages of radical and counter radical organizations of Indonesia.

III. M OTIVATION AND D ISTINGUISHING F EATURES OF
OUR WORK

The motivation for our work is to provide a visualization
of the spatio-temporal distribution of the radical population of
Indonesia by recovering political signals from Twitter data.
A pictorial description of our methodology is provided in
Figure 1. Similar retrieval of signals using Twitter data is the
motivation of the work of [2]. In [2], they find the location
distribution of tweets mentioning roadkills using human beings
as sensors. Similarity of our work with [2] is that we too use
human beings as sensors to the extent that we use tweets of
people of Indonesia to infer radicalism Heat Indices of the
provinces of Indonesia. However, our work is significantly
different from theirs. First, unlike [2], we intend to find the
distribution of (radical) individuals, so we should not factor
in any âhuman population biasâ i.e variation of densities of
people across the different provinces of Indonesia. Second,
our problem is much more complex because we not only
need to know from which location have the radical tweets
come in greater number, but also the âdegree of radicalismâ of
the tweets - so we need to comprehend the sentiment of the
tweets. The major difference here is that in our case, we need
a finer grained distinction among the radical tweets specifying
which tweets are more radical and which tweets are less. So,
questions of interest for us are(Qs1) the âdegree of radicalismâ of tweet tw
(Qs2) the originating location of tweet tw
Thus, Heat Index of a region factors in both the count of
the radical tweets from the region as well as the âdegree of
radicalismâ of the tweets. However, there are certain challenges
in answering these questions. As for Qs1, a tweet can at most
be 140 characters long. This is indeed too little information
to ascertain the âdegree of radicalismâ of tweets on individual
basis. Thus, we go one level up the hierarchy and consider
individual users instead of individual tweets and try to answer
the two questions in the context of individual users. We collect
all the tweets from individual users and assign the âdegree of
radicalismâ to the user based on her tweets. Now, Qs2 would
have been easy to answer with respect to individual tweets if
all the tweets had geo-co-ordinate information because Twitter
API2 provides geo-location information of tweets if the user
2 https://dev.twitter.com/docs/streaming-apis
and
ter.com/docs/platform-objects/tweets have been used

https://dev.twit-

had chosen to reveal her location at the time of tweeting.
However, there are certain problems with this approach - first,
the percentage of tweets containing geo-location information
is very scarce (such tweets constitute less than 1% of our
dataset). Second, when we consider individual users, it is unjustified to assume that all her tweets containing geo-location
information will point to a single region, even if all her tweets
contained geo-location information. Thus, the best estimate of
the location of the user is the probability distribution of the
userâs location over the Indonesian provinces.
We consider categorical distribution of the users into the
thirty four provinces of Indonesia. The motivation behind
employing categorical distribution instead of say Gaussian
distribution over the entire landscape of Indonesia is that we
want to obtain a political Heat Map of Indonesia with the
granularity level of a province. Another possibility, that is
feasible however not pursued by us in this current work, is
dividing Indonesia in the form of grids with varied granularity.
Finer granularity poses the problem of insufficient data from
every grid, because, as mentioned previously, most tweets
do not contain geo-location information. So, most grids will
have no geo-location containing tweet. Our technique of
Location Index computation is discussed in further details in
the following section.
Sentiment Analysis using social media data has been attempted by works such as [31] which tries to exploit patterns in online social media communication and also by [32]
which uses background lexical information and refining of the
same for specific domains by supervised learning techniques.
However, we have computed Radicalization Indices using
simpler text regression techniques similar to [33] and [34].
Our technique of Radicalization Index computation, which is
verified to be quite accurate is discussed in further details in
Section V.
In summary, individual Twitter users are our chosen level of
granularity - we obtain all the necessary information pertaining
to each user. Next we characterize the user based on those
information, not only on the radicalization scale but we also
obtain a location distribution of the user over the regions of
Indonesia. Hence, there is no prediction of the location of
the user involved as in [1]. It is to be noted that we consider
only the users classified as radical by our Radicalization Index
computation method. Hence, our major contribution is the

development of a robust technique to obtain the political Heat
Map of any geographic area.
IV. L OCATION I NDEX C OMPUTATION
As discussed earlier, each user Ui , 1 â¤ i â¤ n has a home
location HLi , 1 â¤ i â¤ n associated with her, which may or
may not be declared. Each tweet Ti,k , 1 â¤ i â¤ n, 1 â¤ k â¤ ti
has a geo-location GLi,k , 1 â¤ i â¤ n, 1 â¤ k â¤ ti associated
with it. However, GLi,k for some tweets Ti,k may not be
known as the user Ui might turn her GPS off. Even when
user Ui has a Declared Home Location DHLi , it may not be
accurate. User Ui might intentionally or inadvertently misstate
her location. Accordingly, we do not accept the DHLi at its
face value as the Actual home location of Ui . Instead, we
compute a matrix, which we term as the general Computed
Home Location matrix gCHL, from the entire dataset barring
the timespan (month in our case) for which the Heat Map
is being generated. The created matrix gCHL is an m Ã m
matrix where gCHLa,b , 1 â¤ a â¤ m, 1 â¤ b â¤ m, is the
conditional probability of the Actual home location of a user
being region Rb , when her Declared Home Location is region
Ra , as learnt from the dataset. The gCHL matrix is computed
using the following three steps provided in Algorithm 1. Thus,
gCHLa,b is given by:
gCHLa,b = X
Y
where,
X = The number of tweets in T such that the author of the
tweet has Declared Home Location as Ra and geo-location of
the tweet is Rb
Y = The number of tweets in T such that the author of the
tweet has Declared Home Location as Ra
Let the ath row of the gCHL matrix be denoted by gCHLa .
Now Computed Home Location vector for the user Ui denoted
by CHLi is assigned the value of gCHLa if the Declared
Home Location of Ui is region Ra . It is to be noted that
the gCHL matrix is general (and not user specific) and is
computed using the entire Twitter data set comprising all users.
From those tweets Ti,k , 1 â¤ k â¤ ti of user Ui , that contain
the geo-location information GLi,k (i.e., when the GPS is not
turned off at the time of the tweet), we compute the Computed
Geo Location vector CGLi of length m, where CGLi,j , 1 â¤
j â¤ m, is the probability of the Actual home location of
user Ui being region Rj , as learnt from the tweets of Ui . The
CGLi,j is computed in the following way:
A
CGLi,j = B

where,
A = The number of tweets in Ti whose geo-location is
Rj and
B = The number of tweets in Ti whose geo-location is
known
We thus obtain two pieces of information about the Actual
home location of the user Ui in the form of two distributions:

CHLi and CGLi , where CGLi is completely user-specific.
However, CHLi is partially user-specific - it does depend
on the user because CHLi is based on her Declared Home
Location, but it also depends on the general distribution which
depends on the entire population mass. It is evident that both
CHLi and CGLi are categorical distribution over the thirty
four Indonesian provinces. Now, we know that a mixture of
discrete distributions over any finite number of categories is
just another distribution over those categories. In order to
combine CGLi and CHLi we obtain a convex combination
of the two to obtain Li,j in the following way:
Li,j = (1 â Ïi ) â CHLi,j + Ïi â CGLi,j

(1)

Now, the mixture weights Ïi for the user Ui is learnt from
the data itself and is calculated as Ïi = |Ti0 |/|Ti |.
Li,j essentially is given by
00

0

Li,j = |Ti | â CHLi,j + |Ti | â CGLi,j

(2)
0

which gives equation (1) when normalized by |Ti | = |Ti | +
00
|Ti | i.e the total number of tweets posted by the user Ui
where,
â¢ Ti = set of tweets produced by user Ui
0
â¢ Ti = subset of Ti and represents the set of tweets by Ui
that contains geo-location information
00
â¢ Ti = subset of Ti and represents the set of tweets by Ui
that do not contain geo-location information
The motivation behind this definition of the mixture weight is
0
that for the Ti tweets which contain geo-location information,
we consider the user-specific location distribution information
inferred from the particular userâs geo-location containing
00
tweets. However, for the tweets of Ti , we have no location
information except for the general information that given a
Declared Home Location for any user Uv in our dataset as
Ra , the location distribution for Uv is CHLv = gCHLa .
Thus, if the Declared Home Location of Ui is given to be
Ra , we consider CHLi = gCHLa . Evidently, we depend
on this semi-user-specific location distribution information for
00
the tweets Ti of Ui . This simple formulation of Li,j also
captures the fact that we rely more on CGLi than on CHLi
when the number of tweets with geo-location information,
generated by Ui is high, however if that count is low ( or even
absent), instead of discarding the particular userâs information,
we obtain the location distribution of Ui from her Declared
Home Location. We experimented by using only geo-location
containing tweets and we saw that the results are far more
accurate if we included users of Type 3 - This is intuitively
correct because the geo-location containing tweets form less
than 1% of the entire dataset.
As noted earlier, the set of users can be divided into four
different classes. We do not try to compute Li,j values for the
users belonging to Class 4. For users belonging to the other
three classes, we compute Li,j using equation (1). For the
users belonging to Class 3, we obtain Ïi to be zero, as we
do not have any geo-location data from the tweets to compute
Ïi .

Algorithm 1 Counting Algorithm for computation of the general Computed Home Location gCHL
â¢
â¢
â¢

Step 1: Initialize gCHLa,b = 0, 1 â¤ a â¤ m, 1 â¤ b â¤ m
Step 2: For each tweet tw in T , increment gCHLa,b if Declared home location of the author of tw and the geo-location
of tw are Ra and Rb respectively
Step 3: Make each row gCHLa of gCHL matrix row stochastic, 1 â¤ a â¤ m

V. R ADICALIZATION I NDEX C OMPUTATION
We intend to assign a Radicalization Index RDi to a Twitter
user Ui based on the content of her tweets. Each tweet can
contain up to 140 characters. Thus the content of a single
tweet does not provide adequate information regarding the
userâs ideology. We collect tweets from users over a period
of time (in our case a month) and for each user Ui we create
a document Di that contains all the tweets Ti of that user,
during that period of time. As there exists a one-to-one correspondence between Ui and Di , by assigning a Radicalization
Index to Di , we essentially assign a Radicalization Index RDi
to Ui . Classical predictive model Multiple Linear regression
[23], [24], [25] fits our application, since it is a dichotomous
classification problem with multiple predictor variables, where
the predictor variables are the terms of our âvocabularyâ.
Classical classification methods such as Logistic Regression
which has applications in a wide variety of domains can
also be used for document classification [26]. Thus, Logistic
Regression can also be applied for our problem. However,
Linear Regression was selected instead of Logistic Regression
because it out-performed the Logistic one through 10-fold
cross validation. This well-known technique divides the given
dataset into 10 segments and then uses 90% of the data ( i.e
nine segments) as training data and 10% of the data ( i.e. one
segment ) as the test data. Linear Regression showed around
98% of accuracy, but Logistic Regression showed 83-85%
of accuracy. The implementation of our approach proceeds
in the following way: First, we identify a set of Indonesian
political organizations. Next, social scientists in our Minerva
team, who are domain experts for Indonesia, hypothesize a
classification to label each organization as radical or counter
radical based on these organizations beliefs and practices.
Using web crawling tools, we download a large number of
documents from the web sites of these organizations. We
use the term âvocabularyâ to mean the set of all unique
terms that appear in all documents from all organizations.
All the documents of an organization are assigned the same
Radicalization Index as the Radicalization Index assigned to
the organization by the domain experts in our team. This set of
documents together with their Radicalization Indices form the
training dataset for our model. After that we use the model to
assign a Radicalization Index to the document Di created from
the tweets of user Ui . This Radicalization Index of document
Di is taken to be the Radicalization Index of user Ui .
A. Problem Formulation:
We formulate the problem in a general sparse learning
framework and solve the following optimization problem (3)

using the techniques from [27] . This is indeed a sparse learning problem because the vocabulary is very large compared to
the number of words used in a document.
1
Ï
2
2
min kAx â yk2 + kxk2 + Î» kxk1
x 2
2
where A â RsÃp , y â RsÃ1 , and x â RpÃ1

(3)

In our application, we have
â¢ A is Document Ã Term matrix which is constructed as
follows: The set of terms (t1 , . . . tp ) includes all the terms
from all the documents by all the organizations, barring
the stop words. The size of the vocabulary in this case is
p.
If data is collected by crawling web sites of different organization (O1 , . . . , Oq ) and documents (di,1 , . . . , d1,ri ) are
collected from the web site of organization Oi , 1 â¤ i â¤ q,
the total number of rows of the matrix A is
s = Î£qi=1 ri ,
and it has the following structure.
Document/T erm
d1
d2
....
ds
â¢
â¢

â¢

t1
....
....
....
....

t2
....
....
....
....

....
....
....
....
....

tp
....
....
....
....

Aij = term f requency of the j th term in the ith
document such that Aij â¥ 0, 1 â¤ i â¤ s, 1 â¤ j â¤ p.
yi â {+1, â1} is the class of each document Di , 1 â¤
i â¤ s. As indicated earlier, the Radicalization Index of
a document is the same the Radicalization Index of the
organization that created that document. Thus, when an
organization is labeled as radical (or counter radical) by
the domain experts, all the documents pertaining to that
organization is marked as +1 (or â1). Thus yi = +1
if Di , 1 â¤ i â¤ s belongs to an organization marked as
radical by the experts, or yi = â1 if Di , 1 â¤ i â¤ s
belongs to an organization marked as counter radical by
the experts.
xj is the weight for each term tj , 1 â¤ j â¤ p. This is the
parameter estimated by optimizing the objective function
(3). The xj âs thus form the predictor variables of the
model.

Let us further clarify the three terms involved in the convex
optimization problem:
â¢

1
2

2

kAx â yk2 - this first term is related to the sum of
the squared errors to fit a straight line to a set of data

points. The objective function (3) thus is the optimization
problem of minimizing this sum of squared-errors.
2
Ï
â¢ 2 kxk2 - this term deals with the ridge regression, which
is an extra level of shrinkage. We set Ï = 0 as we were
mainly driven by sparsity.
â¢ Î» kxk1 - this term involving the L1 norm deals with the
sparsity of the solution vector x. For different values of
Î» we obtain a solution vector x which represents the
weights associated with each term tj , 1 â¤ j â¤ p ( the
same terms which are considered in the A matrix). Some
of these weights are positive, some negative (values can
be very close to 0). The terms with positive (or negative)
weights are the radical (or counter radical) words. The
top (ones with weights having high magnitude) radical
and counter radical words are presented to the experts for
validation. We experiment with several Î» values resulting
in x vectors of various sparsity until the list of top
radical and counter radical words are approved by the
field experts.
We use the Matlab implementation of the SLEP package [28]
that utilizes gradient descent approach to solve the optimization problem (3). This package can handle matrices of 20M
entries within a couple of seconds on a machine with standard
configuration. The input to the SLEP package are the values
of A, Î», and y. The SLEP model outputs the weight vector x.

For each time period (in our case one month), each user
Ui will be assigned Radicalization Index RDi based on their
tweets within that period. This is done as follows:
â¢ As mentioned earlier, each tweet which can only contain
a maximum of 140 characters is too insufficient for
inferring the radicalism of the user. Hence, from the
tweets of each user Ui we form a User Document Di
which is the conglomeration of all her tweets over a
period of one month. It is to be noted here that many
users choose to tweet quite infrequently, hence even if we
collect tweets for one month, a user might have tweeted
only once or twice during the entire one month which
defeats the purpose of collecting tweets for a month.
Hence, we further apply the constraint that we consider
only those users who have tweeted at least seven times
in a month. The value of this threshold has been arrived
at empirically after experimentation with various values
of the threshold.
â¢ With the help of the model that has been fitted using the
organization documents, we classify the User Documents.
Let each User Document Di which is a termf requency
row be denoted by the row vector tc of count of terms
from our âvocabularyâ.
â¢ Each user Ui receives a âscoreâ which we refer to as
Radicalization Index RDi of user Ui . RDi is given by
p
X
j=1

VI. H EAT I NDEX C OMPUTATION
Once we have obtained the Location Indices Li , 1 â¤ i â¤ n
and Radicalization Indices RDi , 1 â¤ i â¤ n, for all the users
Ui , 1 â¤ i â¤ n , the Heat Index Hj of region Rj , 1 â¤ j â¤ m
is computed as
Pn
Hj = i=1 RDi Ã Li,j , âj, 1 â¤ j â¤ m.
The Heat Index Hj for a region Rj indicates the degree of
prevalence of radical ideologies among the people of Rj by
taking into account both the number of radical tweeters living
in Rj and also their âdegree of radicalismâ. In Table I we
present a time-varying Heat Map of Indonesia by computing
the map in three different time intervals of October 10 November 10, November 11 - December 10 and December
11 - January 10. We found a drastic change in the heat
indices during the interval of November 10 â December 10.
But we could not discern any particular event which could
have triggered the same.
VII. DATA C OLLECTION

B. Assignment of Radicalization Index:

RDi = tc .x =

where p is the size of our vocabulary.
This provides us a time-series of RDi values for the users,
which will make it possible to analyze the transition dynamics
for each user. It is evident that a high positive RDi indicates
that Ui is highly radical whereas a high negative RDi indicates
that Ui is highly counter radical.

tcj xj

Since our model requires the computation of both the Radicalization Index RDi as well as the Location Index Li for each
user Ui , we followed a two step data collection procedure
described as follows:
â¢ For the purpose collecting the training data set for computing the Radicalization Index, we crawled the websites
of 36 well-known Indonesian organizations which are
classified as radical or counter radical by our field experts.
A few of the organizations are mentioned in Table II. We
crawled the websites of all these different organizations
and collected a total of 78,135 documents which after
pre-processing and filtering resulted into 49,250 documents. The reason for the reduction from the number of
crawled documents to the number of useful documents
is that many of the crawled documents did not have
any relevant information (for example documents having
only advertisements) and hence were discarded during
pre-processing. Each of the documents on a average
contained 280 words i.e on an average 2880 characters.
All documents pertaining to an organization were labeled
as radical or counter radical depending on the outlook
professed by the organization itself. These were then used
for fitting our Radicalization Index computation model.
â¢ For our study on recovery of political signals pertaining to
trend of radical activities in Indonesia, we chose Twitter
as the data collection platform as Indonesia features as
one of the top five global market segments of Twitter
by reach, and accounts for 19.0% to 20.8% of Twitterâs

TABLE I: The table provides the top 5 province or special region names based on their computed Heat Index values (also
mentioned alongwith) for October 10 - November 10, November 11 - December 10, December 11- January 10
Province Name
Jakarta
East Java
West Java
Yogyakarta
Central Java

Heat Index
5.48
2.95
2.68
1.74
1.68

Province Name
Jakarta
East Java
Yogyakarta
Central Java
West Java

TABLE II: Table showing some of the well-known radical and
counter radical organizations of Indonesia
Radical Organizations
AdianHusaini
PKS
Arrahmah
AbuJibriel
EraMuslim
HizbutTahrir
MillahIbrahim

Counter radical Organizations
DaarulUluum
Interfidei
IslamLiberal
NU
PPIM
Paramadina
LKIS

TABLE III: Keyword markers used for filtering Twitter Stream
API
Keyword
âpenegakan syariahâ
âjihad majelisâ
âmati syahidâ
âajaran islamâ
âpendidikan agama di sekolahâ
âasasi manusiaâ
âdemokrasi yangâ
âkebebasan beragamaâ
âsekularismeâ
âdi negeri negeri islamâ

Interpretation
enforcement of Sharia
jihad assemblies
martyrdom
the teaching of Islam
religious education in schools
human rights
democracy
religious freedom
secularism
Islamic state in the country

total reach by country (Dec 2010)3 . No other publicly
available portal offers access to opinions posted online
by the Indonesian populace on a similar scale as does
Twitter. For gathering tweets, we use Twitterâs Stream
API to access Twitterâs global stream of publicly available
tweet data. Since our goal is to recover âpolitical signalsâ,
we setup a keyword filter on the Stream API to gather
tweets that relate to radical and counter radical ideologies.
The keywords used for this filtration have been identified
by the social scientists in our Minerva project team and
are considered to be significant markers of radical and
counter radical ideologies in the Indonesian context. The
keyword list includes radical markers and a few such
markers are listed in Table III.

Heat Index
16.16
12.33
4.53
3.7
3.39

Province Name
Jakarta
Yogyakarta
West Java
East Java
Central Java

Heat Index
4.71
1.82
1.25
1.20
0.69

Location Index Li of user Ui over the thirty four provinces of
Indonesia, thus we focus only on users from Indonesia. The
keywords used are in Indonesian language and narrows down
the tweets we obtained from the Twitter API. Thus, the geocode in majority of cases indicated a location in Indonesia.
However, not all geo-codes are from Indonesia. We ignore
those tweets in the current work. Thus, out of these 12 million
tweets, 110,063 tweets contained geo-locations that mapped to
regions within Indonesia. To apply this reverse geo-coding,
we used the OpenStreetMap API4 . A user repository was
constructed by including only those users whose Declared
Home Locations matched with an identifiable Indonesian city
or province. Now the user declared home location which the
user mentions as a part of her profile could consist of any
text according to the userâs whim. We found texts such as
âDark side of the moonâ or âsomewhere in this big worldâ
or âHereâ or âinfront of my laptopâ and hence, there is a
need for pre-processing of the text. Also, the users provided
location information to varied degrees of granularity ranging
from continents to towns, however we are interested in the
fixed granularity level of Indonesian provinces and the special
regions such as Jakarta and Yogyakarta. Hence we manually
created a database of towns and cities of all of the Indonesian
provinces. Each of the provinces were annotated with 42 cities/
towns on an average with Papua being the highest which
was annotated with 70 cities/towns. Using this database we
then assigned a legitimate Declared Home Location to as
many users as possible. The final user repository consisted
of 959,911 unique users.

Fig. 2: Figure showing the number of tweets collected over
our observation period

We collected tweet data for a three-month interval and gathered a total of 12,152,874 tweets from October 10, 2012
to January 10, 2013 ( Figure 2) that matched the keyword
filtration criteria ( Table III ). We used the three months of
data to calculate the Radicalization Indices of the users. In
this research, we are interested in the probability distribution
3 http://www.billhartzer.com/pages/comscore-twitter-latin-america-usage/

http://www.comscoredatamine.com/2011/02/the-netherlands-leads-globalmarkets-in-twitter-reach/

4 The relevant information about the API could be found at http://wiki.openstreetmap.org/wiki/Nominatim

VIII. E XPERIMENTAL R ESULTS
We created Heat Maps of Indonesia on a monthly basis. We
computed the RDi of each user U i for each month from
October 10 to January 10, as long as Ui sent at least 7 tweets in
that month Again, for each user Ui we computed the Location
Index Li by considering all her tweets over the period of the
month.. For that we computed the general Computed Home
Location gCHL matrix. The(i, j)th entry gives the probability
of a user with a Declared Home Location of Ri being located
in Rj , 1 â¤ i â¤ m, 1 â¤ j â¤ m.
â¢

gCHL matrix : The gCHL matrix provides interesting
insights on the Indonesian population. We computed the
gCHL - matrix on all possible doublets among the
three months of observation period. i.e for each month
for calculating the Location Indices Li of users, we
have generated the gCHL matrix using the other two
months of data. Thus, in each case, we had training
data of two months and test data of one month. Among
the three gCHL - matrices generated, we saw that the
Declared Home Location of users give us a good insight
on the Actual home location of the user. Thus, instead of
merely depending on the geo-co-ordinates of users, we
should consider the home location from the userâs profile
and home location declarations are much more abundant
than geo-location containing tweets. However, depending
solely on Declared Home Locations can be deceptive. We
also observed that people with Declared Home Locations
in various different provinces from all around Indonesia
such as Bangka Belitung, Banten, Maluku, West Nusa
Tenggara, East Nusa Tenggara and Papua have a very
strong tendency to have high probability of having Actual
home location in Jakarta (as observed from our results
over three months). This is very intuitive because Jakarta
being the Capital Region must have attracted people from
different parts of Indonesia for prospective settlement.
We further made an interesting observation that people
with Declared Home Location of East Kalimantan have
considerable geo-location containing tweets from Central
Kalimantan.

The Heat Indices values for the thirty four Indonesian
provinces are computed using our approach for three months
of our observation period - namely October 10 - November
10, November 11 - December 10, December 11 -January
10. Among all Indonesian provinces the top five provinces
and special regions along with their Heat Index values are
presented in Table I for the three months. Color maps of
Indonesia with Heat Indices is shown in Figure 3, where darker
colors indicate a higher level of radical tweeting, and lighter
colors indicate a lower level of radical tweeting. It may be
seen from Figure 3 that the area around Jakarta and the Java
provinces are highly active in radical tweet creation. According
to our Twitter data analysis, the provinces Jakarta, East Java,
Yogyakarta and Central Java, along with West Java are the top
provinces that generate a high level of radical activities.

IX. VALIDATION
For the purpose of validation of the Radicalization Index,
we computed the Radicalization Indices of some well-known
counter radical leaders of Indonesia for the months that they
had tweeted for more than 7 times which we consider as our
threshold. Our classifier gave perfect accuracy. By accuracy of
the classification we mean the percentage of time the leaders
who are thus known to be counter radical were classified as
counter radical by our classifier. We did not validate the Location Index computation technique because of the lack of the
ground truth of the Actual home location of users. However,
our results of Heat Index are validated by the findings of the
Indonesia-based Wahid Institute5 (named after Abdurrahman
Wahid, an Indonesian Muslim religious and political leader
who served as the President of Indonesia from 1999 to 2001).
Wahid Institute promotes a moderate version of Islam through
dialogue events, publications, and public advocacies. The
institute also releases an annual religious freedom report on
religious life in Indonesia. According to the Wahid Instituteâs
Annual Report of 20126 , the top four provinces of Indonesia
where radical activities are most observable are West Java,
Aceh, East Java, and Central Java. It may be noted here,
that three out of the four most radical provinces identified
by the Wahid Institute, also appear at the very top of our
list. Also, our field experts have confirmed Jakarta to be a
center of radical activities. It may be mentioned here that field
studies7 in January 2012 by Setara Institute8 , a well-known
NGO based in Indonesia, showed that the strong radicalism
of the young muslim population in Yogyakarta and Central
Java are making them hot targets to be recruited as Jihadists. In
May 2012, a mob attack by Indonesian Mujahidin Council on a
book launch of a well-known Canadian author, an advocate of
LGBT, took place in Yogyakarta. In September 2012, there has
been arrests of potential terrorists from Yogyakarta9 . Because,
Wahid Institute has mentioned about Indonesian provinces
only, it might be expected that Jakarta and Yogyakarta, being
special administrative regions, are missing from their list however, we do not have access to their full report. The
high radicalism of the Java provinces are also corroborated
by reports of the Setara Institute. The only radically active
province that shows up in the Wahid Institute report but does
not appear at the top of our list is Aceh, located at the north
west corner of Indonesia. It is worth mentioning here that Aceh
was completely devastated by the 2004 Indian Ocean Tsunami
and is still recovering from its effects. Aceh is also one of
the least economically developed provinces of Indonesia. We
believe that due to the lack of economic advancement in Aceh,
the level of Internet penetration in Aceh is fairly small and not
many people from Aceh are active tweeters. This may explain
5 http://berkleycenter.georgetown.edu/resources/organizations/wahidinstitute
6 Released on December 28, 2012
7 http://www.setara-institute.org/en/content/study-shows-how-youngradical-indonesian-muslims-become-terrorists
8 http://www.setara-institute.org/
9 http://www.washingtontimes.com/multimedia/image/indonesia-terrorjpg/

(a) Heat Map for October 10 to November 10

(b) Heat Map for November 11 to December 10

(c) Heat Map for December 11 to January 10

(d) Radicalism
Scale

Index

Fig. 3: Heat Maps of Indonesia

the reason for Aceh not showing up among our list of top
radically active provinces.
X. C ONCLUSION
We have developed a generic robust technique for recovering signals pertaining to a geographical area such as a
country using Twitter Data. We have applied our technique
to our Indonesian dataset and have observed high accuracy.
The goal of our work is the generation of a political Heat
Map of Indonesia which will clearly indicate the provinces of
Indonesia where radical narrative is prominent. The granularity
of the Radicalization Index Assignment used is a Twitter
user, while the granularity of regions used is an Indonesian
Province. Thus, we have analyzed tweets made by a user Ui
in a month to assign a Radicalization Index RDi to Ui - Thus
RDi indicates how radical is Ui in her political outlook . Also,
by mining the tweets in our database we assign a Location
Index Li to Ui . For computation of the Location Index, the
sources of location information used are not only the geolocation tagging as provided by Twitter API for the tweets
which contain that information (such tweets constitute less
than 1% of our dataset), but we also use the user declared
home location information from her profile (after considerable
amount of pre-processing and cleansing). We have combined
these two sources of information for inferring the probability
distribution of the location of the users among the provinces
of Indonesia. Thus, by considering the RDi âs for all the users
over a period of one month, in conjunction with the Location
Indices Li âs we generate the political Heat Maps of the likes

of Figure 3. We have got a time series of Heat Maps, which
can help us in mapping trends by regions in radical discourse.
Such Heat Maps can prove to be very useful in studying the
spatio-temporal dynamics of the people of Indonesia so far as
their political outlook is concerned
R EFERENCES
[1] Cho, E., Myers, S. A., & Leskovec, J. (2011, August). Friendship
and mobility: user movement in location-based social networks. In
Proceedings of the 17th ACM SIGKDD international conference on
Knowledge discovery and data mining (pp. 1082-1090). ACM.
[2] Xu, J. M., Bhargava, A., Nowak, R., & Zhu, X. (2012). Socioscope:
spatio-temporal signal recovery from social media. In Machine Learning
and Knowledge Discovery in Databases (pp. 644-659). Springer Berlin
Heidelberg.
[3] Priedhorsky, R., Culotta, A., & Del Valle, S. Y. (2013). Inferring the
Origin Locations of Tweets with Quantitative Confidence. arXiv preprint
arXiv:1305.3932.
[4] Yang, D., Zhang, D., Yu, Z., & Wang, Z. (2013, May). A sentimentenhanced personalized location recommendation system. In Proceedings
of the 24th ACM Conference on Hypertext and Social Media (pp.
119-128). ACM.
[5] Rahimi, S. M., & Wang, X. (2013). Location Recommendation Based on
Periodicity of Human Activities and Location Categories. In Advances in
Knowledge Discovery and Data Mining (pp. 377-389). Springer Berlin
Heidelberg.
[6] Li, Y., Steiner, M., Wang, L., Zhang, Z. L., & Bao, J. Exploring Venue
Popularity in Foursquare.
[7] Li, Y., Steiner, M., Wang, L., Zhang, Z. L., & Bao, J. (2012, December).
Dissecting foursquare venue popularity via random region sampling.
In Proceedings of the 2012 ACM conference on CoNEXT student
workshop (pp. 21-22). ACM.
[8] Gonzalez, M. C., Hidalgo, C. A., & Barabasi, A. L. (2008). Understanding individual human mobility patterns. Nature, 453(7196), 779-782.

[9] Sadilek, A., Kautz, H., & Bigham, J. P. (2012, February). Finding your
friends and following them to where you are. In Proceedings of the
fifth ACM international conference on Web search and data mining (pp.
723-732). ACM.
[10] Noulas, A., Scellato, S., Lambiotte, R., Pontil, M., & Mascolo, C.
(2012). A tale of many cities: universal patterns in human urban mobility.
PloS one, 7(5), e37027.
[11] Lu, X., Bengtsson, L., & Holme, P. (2012). Predictability of population displacement after the 2010 Haiti earthquake. Proceedings of the
National Academy of Sciences, 109(29), 11576-11581.
[12] Chang, H. W., Lee, D., Eltaher, M., & Lee, J. (2012, August). @
Phillies Tweeting from Philly? Predicting Twitter User Locations with
Spatial Word Usage. In Advances in Social Networks Analysis and
Mining (ASONAM), 2012 IEEE/ACM International Conference on (pp.
111-118). IEEE.
[13] Mahmud, J., Nichols, J., & Drews, C. (2012). Where is this tweet from?
inferring home locations of twitter users. Proc AAAI ICWSM, 12.
[14] Eisenstein, J., OâConnor, B., Smith, N. A., & Xing, E. P. (2010,
October). A latent variable model for geographic lexical variation. In
Proceedings of the 2010 Conference on Empirical Methods in Natural
Language Processing (pp. 1277-1287). Association for Computational
Linguistics.
[15] Cheng, Z., Caverlee, J., & Lee, K. (2010, October). You are where
you tweet: a content-based approach to geo-locating twitter users. In
Proceedings of the 19th ACM international conference on Information
and knowledge management (pp. 759-768). ACM.
[16] Hecht, B., Hong, L., Suh, B., & Chi, E. H. (2011, May). Tweets from
Justin Bieberâs heart: the dynamics of the location field in user profiles.
In Proceedings of the 2011 annual conference on Human factors in
computing systems (pp. 237-246). ACM.
[17] Boutet, A., Kim, H., & Yoneki, E. (2012, August). Whatâs in Twitter: I
Know What Parties are Popular and Who You are Supporting Now!. In
Advances in Social Networks Analysis and Mining (ASONAM), 2012
IEEE/ACM International Conference on (pp. 132-139). IEEE.
[18] Dahlin, J., Johansson, F., Kaati, L., Martenson, C., & Svenson, P.
(2012, August). Combining Entity Matching Techniques for Detecting
Extremist Behavior on Discussion Boards. In Advances in Social Networks Analysis and Mining (ASONAM), 2012 IEEE/ACM International
Conference on (pp. 850-857). IEEE.
[19] Eisenstein, J., Ahmed, A., & Xing, E. P. (2011, June). Sparse additive
generative models of text. In International Conference on Machine
Learning (ICML).
[20] Hong, L., Ahmed, A., Gurumurthy, S., Smola, A. J., & Tsioutsiouliklis,
K. (2012, April). Discovering geographical topics in the twitter stream.
In Proceedings of the 21st international conference on World Wide Web
(pp. 769-778). ACM.
[21] OâConnor, B., Eisenstein, J., Xing, E. P., & Smith, N. A. (2010).
Discovering demographic language variation. In Workshop on Machine
Learning for Social Computing at NIPS.
[22] Yin, Z., Cao, L., Han, J., Zhai, C., & Huang, T. (2011, March).
Geographical topic discovery and comparison. In Proceedings of the
20th international conference on World wide web (pp. 247-256). ACM.
[23] Zhang, T. (2009). Some sharp performance bounds for least squares
regression with L1 regularization. The Annals of Statistics, 37(5A),
2109-2144.
[24] S.Kim, K.Koh, M.Lustig, S.Boyd and D. Gorinevsky, An Interior-Point
Method for Large-Scale l1 - Regularized Least Squares Journal of
seleccted topics in Signal Processing, Vol. 1, No. 4, pages 606 - 617,
Dec 2007
[25] Kolter, J. Z., & Ng, A. Y. (2009, June). Regularization and feature
selection in least-squares temporal difference learning. In Proceedings
of the 26th Annual International Conference on Machine Learning (pp.
521-528). ACM.
[26] Brzezinski, J. R., & Knafl, G. J. (1999). Logistic regression modeling for context-based classification. In Database and Expert Systems
Applications, 1999. Proceedings. Tenth International Workshop on (pp.
755-759). IEEE.
[27] Friedman, J., Hastie, T., & Tibshirani, R. (2010). Regularization paths for
generalized linear models via coordinate descent. Journal of statistical
software, 33(1), 1.
[28] J. Liu, S. Ji and Jieping Ye. SLEP: Sparse Learning with Efficient
Projections, Arizona State University (2009)
[29] Lee, S. I., Lee, H., Abbeel, P., & Ng, A. Y. (2006, July). Efficient
LË 1 Regularized Logistic Regression. In Proceedings of the National

[30]
[31]

[32]

[33]

[34]

[35]

Conference on Artificial Intelligence (Vol. 21, No. 1, p. 401). Menlo
Park, CA; Cambridge, MA; London; AAAI Press; MIT Press; 1999.
T. P. Minka, A comparison of numerical optimizaers for logistic regression, Technical report, 2007
Thelwall, M., Buckley, K., Paltoglou, G., Skowron, M., Garcia, D.,
Gobron, S., ... & Holyst, J. A. (2013). Damping Sentiment Analysis
in Online Communication: Discussions, Monologs and Dialogs. In
Computational Linguistics and Intelligent Text Processing (pp. 1-12).
Springer Berlin Heidelberg.
Melville, P., Gryc, W., & Lawrence, R. D. (2009, June). Sentiment analysis of blogs by combining lexical knowledge with text classification.
In Proceedings of the 15th ACM SIGKDD international conference on
Knowledge discovery and data mining (pp. 1275-1284). ACM.
Joshi, M., Das, D., Gimpel, K., & Smith, N. A. (2010, June). Movie
reviews and revenues: An experiment in text regression. In Human
Language Technologies: The 2010 Annual Conference of the North
American Chapter of the Association for Computational Linguistics (pp.
293-296). Association for Computational Linguistics.
Kogan, S., Levin, D., Routledge, B. R., Sagi, J. S., & Smith, N. A.
(2009, May). Predicting risk from financial reports with regression.
In Proceedings of Human Language Technologies: The 2009 Annual
Conference of the North American Chapter of the Association for
Computational Linguistics (pp. 272-280). Association for Computational
Linguistics
Brynielsson, J., Horndahl, A., Johansson, F., Kaati, L., Martenson, C.,
& Svenson, P. (2012, August). Analysis of weak signals for detecting
lone wolf terrorists. In Intelligence and Security Informatics Conference
(EISIC), 2012 European (pp. 197-204). IEEE.

A Network Planning and Management Tool for
Mitigating the Impact of Spatially Correlated Failures in
Infrastructure Networks
Arun Das, Arunabha Sen, Chunming Qiao, Nasir Ghani, Nathalie Mitton

To cite this version:
Arun Das, Arunabha Sen, Chunming Qiao, Nasir Ghani, Nathalie Mitton. A Network Planning
and Management Tool for Mitigating the Impact of Spatially Correlated Failures in Infrastructure Networks. International Conference on Design of Reliable Communication Networks
(DRCN), Mar 2016, Paris, France. <hal-01254982>

HAL Id: hal-01254982
https://hal.inria.fr/hal-01254982
Submitted on 21 Mar 2016

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

Lâarchive ouverte pluridisciplinaire HAL, est
destineÌe au deÌpoÌt et aÌ la diffusion de documents
scientifiques de niveau recherche, publieÌs ou non,
eÌmanant des eÌtablissements dâenseignement et de
recherche francÌ§ais ou eÌtrangers, des laboratoires
publics ou priveÌs.

A Network Planning and Management Tool for
Mitigating the Impact of Spatially Correlated
Failures in Infrastructure Networks
Arun Dasâ , Arunabha Senâ , Chunming Qiaoâ  , Nasir Ghaniâ¡ , Nathalie MittonÂ§

â School of Computing, Informatics and Decision System Engineering, Arizona State University, Tempe, Arizona 85287, USA
â  Department of Computer Science and Engineering, SUNY at Buffalo, Buffalo, NY 14201, USA
â¡ Department of Electrical Engineering, University of South Florida, Tampa, FL 33620, USA
Â§ Inria, 40 Avenue Halley, 59650 Villeneuve DâASCQ, France

Email: arun.das@asu.edu, asen@asu.edu, qiao@computer.org, nghani@usf.edu, nathalie.mitton@inria.fr
AbstractâCurrent practices of fault-tolerant network design
ignore the fact that most network infrastructure faults are
localized or spatially correlated (i.e., confined to regions). Network
operators require new tools to mitigate the impact of such
region based faults on their infrastructures. Utilizing the support
from the U.S. Department of Defense, and by consolidating a
wide range of theories and solutions developed in the last few
years, the authors of this paper have developed an advanced
Network Planning and Management Tool (NPMT) that facilitates
the design and provisioning of robust and resilient networks.
The tool provides multi-faceted network design, evaluation and
simulation capabilities for network planners. Future extensions
of the tool currently being worked upon not only expand the tools
capabilities, but also extend these capabilities to heterogeneous
interdependent networks such as communication, power, water
and satellite networks.

I.

I NTRODUCTION AND M OTIVATION

It is extremely important that planners for large wide area
networks have the right tools to design robust and resilient
networks that can effectively withstand large scale geographically correlated failures in their networks. Such failures can
be triggered by nature (hurricane or earthquake) or humans
(nuclear attack or conventional weapon attack over a large
geographical area). With research support from the U.S. Defense Threat Reduction Agency, an agency whose mission is to
protect the U.S. against Weapons of Mass Destruction (WMD),
such as nuclear, biological or chemical attacks, the authors
of this paper, over the last six years have developed a wide
ranging set of concepts and techniques for enhancing network
robustness against spatially correlated or region based faults.
We have recently incorporated these concepts and techniques
into a Network Planning and Management Tool (NPMT) [1]
for the benefit of network designers, planners and operators.
In this paper, we first describe the novel concepts developed
to design networks that are robust against region based faults,
and then describe how these concepts have been incorporated
into the NPMT. The goal of this paper is to bring to the
attention of the networking research community, and the
audience of the workshop on DRCN in particular, the existence
of NPMT as a tool that consolidates a large body of work on
spatially correlated failures, and as a tool that can be used by
This work was supported in part by the NSF grant 1441214, and by grant
HDTRA1-09-1-0032 from the U.S. Defense Threat Reduction Agency

the community to meet the needs for robust network design
against spatially correlated failures. In essence, this paperâs
contribution should not be measured in terms of new analytical
findings, but in terms of service to the networking community.
We use the term WMD attack to imply a large scale
geographically correlated failure such as failures caused by an
earthquake, hurricane or nuclear attack. The characteristic of a
WMD attack is massive but localized faults. The connectivity
of a network [2] is generally accepted as a metric for evaluating
the fault-tolerance capability of a network [3]. If a networkâs
connectivity is k + 1, then the network can tolerate up to k
faults, implying that the surviving network remains connected
even after k failures. The connectivity metric, however, has no
way of capturing locality, i.e., the faulty nodes/edges may be
close or far away from each other. Thus, the connectivity metric cannot distinguish between faults that are geographically
correlated (a WMD fault characteristic), and faults that are not.
Connectivity as a metric also fails to capture other important
structural properties of the network such as the number or
size of the connected components [2] into which a network
disintegrates when the number of failed nodes/edges exceeds
the node/edge connectivity of the network.
Recognizing the limitations of connectivity as a metric
for capturing the special characteristics of geographically correlated failures, the authors of [4] introduced the notion of
region-based connectivity. A region may be defined either with
reference to the network graph or to the network geometry (i.e.,
layout of the network in a two or three dimensional space). For
example, a region may be defined as a subgraph with diameter
d (where the diameter of a graph is defined as the maximum
of the shortest path distance between a pair of nodes, taken
over all source-destination node pairs). Or, a region may also
be defined as a collection of nodes and edges in the network
graph layout that is covered by a circular area in that layout.
Figure 1(a) shows an example of a circular region-based fault.
The NPMT described in this paper is intended to support
design and analysis of single layered and multi-layered interdependent heterogeneous networks. In essence, the NPMT
is particularly suitable for planning and design of critical
infrastructures. For example, from the single network layer
perspective, the NPMT enables backbone communication network providers, such as, AT&T, Sprint, Qwest and Level 3
Communications, to (i) identify the most vulnerable parts of

(a)

(b)

(c)

Fig. 1: (a) Network with circular fault region, (b) Optical fiber network of a major U.S. provider, (c) Optical Fiber network of a major European
provider disrupted by a WMD attack

their network against a WMD attack, and (ii) reinforce the
network with least cost to eliminate or significantly reduce
the threat of network disruption due to a WMD attack. Figure
1(b) shows the backbone network of a major U.S. provider
and Figure 1(c) shows how the backbone network of a major
European provider can potentially be disrupted by a WMD
attack. From a multi-layer perspective the NPMT can be used
for design and analysis of smart cities, where heterogeneous
networks ranging from disparate telecom networks (such as
2G, 3G, WiFi, Bluetooth, etc.) to water, electricity and gas
distribution networks, form a complex interdependent ecosystem. Subsequently, failures in one network, for example a leak
in the water distribution network, may deteriorate other nearby
(spatially correlated) infrastructures such as gas or electricity
whose pipes and cables may get affected due to the leak. In this
context, a tool like NPMT can be an asset for utility companies
and smart city planners to quickly perform (i) root cause
analysis of failure, and (ii) forecast fault evolution, to direct
repairs and maintenance towards specific network components
and restrict fault propagation. To the best of our knowledge no
such tool is available today that supports features for planning
and designing of single layer and multi-layer interdependent
networks in the presence of spatially correlated faults.
Several studies in the network research community have
focused on different aspects of spatially correlated or regionbased faults in networks [5-11], however, to the best of our
knowledge there does not exist an executable platform that
consolidates the findings and techniques of these studies into
a readily usable tool. The NPMT is intended to fill that gap
and be such a platform that can incorporate the outcomes
developed in [5-11] into executable modules to be integrated
into the NPMT. This will allow network designers, planners
and operators to use the results of these studies in their real
world operational networks.
The rest of the paper is organized as follows: In Section
II we present an overview of the underlying concepts and
theoretical results that the NPMT operates on. In Section III
we outline the capabilities of the NPMT and finally Section
IV concludes this paper.
II.

C ONCEPTS , M ETRICS AND S OLUTION T ECHNIQUES

In this section we give a brief overview of the underlying
concepts, metrics and solution techniques that the Network

Planning and Management Tool (NPMT) utilizes to carry out
its functional operations. The NPMT is built as a modular
execution engine that can execute smaller reusable modules
to perform desired operations on a network topology. In this
respect, the current version of the NPMT comprises of different
modules that deal with both static and dynamic aspects of
robust and resilient network design. The modular approach
allows design, development and testing of these modules to
be done independently and defers the integration into the
NPMT until a module meets itâs functional requirements. In
the following sub-sections we give a brief overview of the
analytical foundations of these modules. It may be noted that,
as of writing this paper not all modules have been implemented
and integrated into the NPMT. Accordingly, we highlight our
ongoing work in the discussion below.
A. Region-Based Fault Metrics Computation Module
As outlined in Section I, connectivity as a metric fails to
capture several characteristics of the network in presence of
spatially correlated failures. For instance, the number or size of
the connected components into which a network disintegrates
in the presence of a spatially correlated fault is not captured by
the traditional connectivity metric. In order to overcome these
gaps and capture such network state characteristics, several
metrics and their computation techniques have been proposed
by the research community. For a given network topology, the
NPMT can analyze the network and compute metrics pertinent
to network state in the presence of spatially correlated faults.
The following metrics are supported by the NPMT:
Region-based Connectivity Metric Computation
Region based connectivity can be considered under two fault
models â (i) Single Region Fault Model (sRFM) where faults
are confined to a single region [4], and (ii) Multiple Region
Fault Model (mRFM) where faults are confined to k regions
for some specified k [12].
Formally, in sRFM, the single-region-based (node) connectivity of graph G with a specified definition of region R,
sÎºR (G), is defined as follows: Suppose that {R1 , . . . , Rk } is
the set of all possible regions of the graph G. Consider a
k-dimensional vector T whose i-th entry, T [i], indicates the
number of nodes in region Ri whose failure will disconnect
the graph G. If the graph G remains connected even after the

failure of all nodes of the region Ri then T [i] is set equal to
â. The region-based connectivity of a graph G with region
R, is then computed as follows:
sÎºR (G) = min T [i]
1â¤iâ¤k

In mRFM, the multi-region-based (node) connectivity of graph
G with a specified definition of region R, mÎºR (G), is defined
as the minimum number of regions whose removal (i.e.,
removal of all nodes in the regions and edges incident on them)
will disconnect the graph.
Polynomial time algorithms to compute region-based connectivity in sRFM was presented in [4]. The NPMT contains an
implementation of this algorithm that can be used to compute
the Region-based Connectivity for a given network topology.
Region-based
Component
Decomposition
(RBCDN) Metric Computation

Number

Proposed by the authors of [13], the Region-Based Component Decomposition Number, or RBCDN of graph G = (V, E)
with a specified definition of region R is defined the following
way: Suppose that {R1 , . . . , Rk } is the set of all possible
regions of the graph G. Consider a k-dimensional vector C
whose i-th entry, C[i], indicates the number of connected
components in which G decomposes when all entities in Ri
fails. RBCDN of a graph G with region R is defined as follows:
Î´R (G) = max C[i]
1â¤iâ¤k

RBCDN as a metric provides a insight into the worst case
scenario on how fragmented a network can become in the
presence of a spatially correlated fault. In [13] the authors
propose techniques to compute the RBCDN and the NPMT
provides an implementation of this algorithm that can be used
on user selected network topologies.
Region-based Smallest/Largest Component Size Metric
Computation
The Region-Based Smallest (Largest) Component Size, or
RBSCS/RBLCS was proposed in [14], and is defined for
a graph G = (V, E) with a specified definition of region
R, as follows: Suppose that {R1 , . . . , Rk } is the set of all
possible regions of the graph G. Consider a k-dimensional
vector CS (CL ) whose i-th entry, CS [i] (CL [i]), indicates the
size of the smallest (largest) connected component in which G
decomposes when all nodes in Ri fails. The RBSCS Î±R (G)
and RBLCS Î²R (G) of graph G with region R is defined as:
Î±R (G) = min CS [i] and Î²R (G) = min CL [i]
1â¤iâ¤k

1â¤iâ¤k

The RBLCS and RBSCS metrics provide insights on how well
a networkâs performance degrades in the presence of regionbased faults. Depending on the needs of graceful performance
degradation, networks designers may choose to design networks that have a small value of RBCDN (Î´R (G)) and a high
value of either RBLCS (Î±R (G)) or RBSCS (Î²R (G)). The
NPMT allows the user to compute the RBLCS and RBSCS
metrics for a chosen network topology.

is a need for techniques to compute the set of regions, given
a network and some fault specification. In [14], given a graph
Gâs layout on a two-dimensional plane and a fault radius r,
the authors provide a polynomial time algorithm to compute all
distinguishable or distinct circular regions with radius r. Two
fault regions are considered indistinguishable if they contain
the same set of links and nodes. The authors considered both
wired networks, where nodes and edges can be part of a failure
region, and wireless networks, where only nodes can be part
of a failure region. It was shown in [14] that the number of
distinct regions in wireless and wired networks are O(n2 )
and O(n4 ) respectively, and that all distinct regions can be
computed in O(n6 ) time, where n is the number of nodes.
The NPMT is bundled with an implementation of the
technique outlined in [14]. Given a network topology and a
fault radius, the NPMT can compute all distinct regions of
the network which can then be used by other modules of
the NPMT, such as the Metric Computation Module and the
Region-disjoint Path Computation Module (discussed next).
C. Region-disjoint Paths Computation Module
For a graph G = (V, E), a set of region-disjoint paths
P between a source node s and destination node d with a
specified definition of region R, is defined as follows: Suppose
that {R1 , . . . , Rk } is the set of all possible regions of graph
G and path Pu â P contains a set of nodes and edges from
G such that Pu forms a path from s to d, {s, d} â V . Then,
for every pair of paths {Pu , Pv } â P, u 6= v, Pu and Pv are
region-disjoint, i.e. there is no region in R that both the paths
traverse. Formally, region-disjoint paths are defined as follows,
for all i = 1, . . . , k:
|(Pu â© Ri ) â© (Pv â© Ri )| = 0, â{Pu , Pv } â P, u 6= v
Although region-disjoint path computation has been addressed
in [8], the authors consider a model where faults do not cause
edges to fail unless a failed edge is associated with a failed
node. In this model an edge cannot fail on itâs own and can
only fail when one of the nodes incident on the edge fails. This
assumption is considerably restrictive and possibly unusable
for designers of larger networks where spatially correlated
faults can affect nodes and edges independently. In order to
overcome this limitation the NPMT supports computation of
region-disjoint paths in the presence of circular faults using an
Integer Linear Program (ILP) that doesnât presuppose any such
restrictions. The NPMT is capable of computing two regiondisjoint paths from given source and destination nodes such
that the sum of lengths of the two paths is minimum. Also, as
the source (destination) node is part of a region that is traversed
by both paths (as both paths have the same staring and ending
points), no region disjoint path may exist. To accommodate this
situation the NPMT accommodates the use of no-fault zones
â a circular area around the source and destination nodes that
is immune to faults. Future extensions of this module include
computing more than two paths, and including other selection
criteria such as minimizing the maximum path length.

B. Distinct Regions Computation Module

D. Region-based Fault Tolerant Distributed File Storage Module

It may be noted that all the previously defined metrics
operate on a given graph and a set of regions. Thus, there

In the preceding discussions the importance of a node
in keeping the network connected is emphasized, however,

individual nodes can also act as data stores of the network
and the removal of a node from a network (due to a regionbased fault), may not only cause connectivity losses, but also
data losses. To address such data loss risks, distributed storage
techniques are often employed that enhances data survivability
in the presence of faults. One such technique is redundancy,
such as by (i) storing multiple copies of the entire file, or
(ii) storing different fragments of the same file at different
nodes in the network. In the popular (N, K), N â¥ K file
distribution scheme, from a file F of size |F |, N segments
of size |F |/K are created in such a way that it is possible to
reconstruct the entire file by accessing any K segments. For
such a reconstruction scheme to work, it is essential that the
K segments of the file are stored in nodes that are connected
to each other in the network. However, in the event of failures,
the network may become disconnected (i.e., split into several
connected components) and K segments may not be accessible
in the residual network to reconstruct the file F .
From the context of data survivability in the presence of
spatially correlated faults in networks, the NPMT supports
a âRegion-based Distributed File Storage Moduleâ that implements an algorithm proposed in [11] that ensures that:
(i) even when the network is fractured into disconnected
components due to a region-based fault, at least one of the
largest components will have access to at least K distinct file
segments with which to reconstruct the entire file, and (ii)
the total storage requirement is minimized. As of writing this
paper, this module is currently under development and will be
part of the NPMT upon its completion.
E. Robust Multi-layer Interdependent Network Design Module
In todayâs world, a multitude of heterogeneous interconnected networks form a symbiotic ecosystem that supports all
of the economic, political and social aspects of human life.
For example, the critical infrastructures of the nation such
as the power grid and the communication network are highly
interdependent on each other, and any adverse effects on one
network can affect the other network. Thus, isolated network
analysis is no longer sufficient to design and operate such
interconnected and interdependent network systems.
Recognizing this need for a deeper understanding of the
interdependency in such multi-layered network systems, significant efforts have been made by the research community in the
last few years, and accordingly, a number of analytical models
have been proposed to analyze such interdependencies [15-17].
However, most of these models are simplistic and fail to
capture the complex interdependencies that may exist between
entities of the power grid and communication networks. To
overcome the limitations of existing models, the authors of
[18] have proposed an Implicative Interdependency Model that
is able to capture such complex interdependency. Utilizing
this model, several problems on multi-layer interdependent
networks have been studied, such as (i) identification of the
K most vulnerable nodes [18], (ii) root cause analysis of
failures [19], (iii) the entity hardening problem [20], (iv) the
smallest pseudo-target set identification problem [21], and (v)
the robustness analysis problem [22].
This module will support multi-layer network interdependency modeling using the Implicative Interdependency Model,

and analysis of multi-layer networks using the techniques proposed in [18-22]. The module is currently under development
and will be part of the NPMT upon its completion.
F. Module for Progressive Recovery from Region-based Failures
With this module, the NPMT addresses post-fault recovery
techniques in the aftermath of region-based faults on multilayer interdependent networks. To restore an interdependent
network system from a post-fault scenario to its pre-failure
state, all the faulty network entities (nodes/edges) have to be
repaired or replaced. However, resource limitations may prevent simultaneous restoration of all failed units of the network.
Accordingly, the failed units have to be restored in a sequenced
manner. As each network entity in its operational state adds
some utility value to the interdependent network system, when
a unit recovers from a failed state to an operational state,
the unit starts providing some âbenefitâ to the system. Since
different units have different utility values to the system, the
sequence in which the failed units are restored is important as
the recovery sequence determines the cumulative system utility
during the recovery process.
As discussed in Section II-E, the Implicative Interdependency Model provides a powerful technique for modeling
dependencies in multi-layer interdependent networks. Using
this model the authors of [23] studied the progressive recovery
problem in interdependent networks with the objective of
maximizing system utility during the system recovery process.
This module implements the progressive recovery algorithm of
[23], and can be used to sequence recovery of network entities
from a post-fault to a pre-fault network state that maximizes
system utility during the recovery process. The module is
currently under development and will be part of the NPMT
upon its completion.
III.

A RCHITECTURE AND S YSTEM C APABILITIES

In this section we first outline the system architecture, and
then discuss the different capabilities of the NPMT.
A. System Architecture
View

Service

Fault Analyzer

Path Analyzer

Topology Manager

Profile Manager

Traffic and Fault Simulator

Core Modules:
Network Topology Manager
Region-Based Fault Analysis

Controller

Disjoint Path Analysis

Visualization Engine

Simulation Engine

Execution Engine

Common Modules:

Repository

N/W Fault Impact Analyzer

Path Planning Algorithms

Model
Simulation Data

User/Roles

Path Archive

Fault Generation Engine

N/W Topologies

Fault Archive

Library Faults

Request Generation Engine

Fig. 2: The NPMT High-Level Architecture

The NPMT is implemented as a web-application that
allows the user to remotely connect and operate the tool from
a browser. The web-application follows the standard three-tier
architecture and has a client tier, application tier, and database

(a)
(b)
Fig. 3: (a) Topology Manager â create, edit and manage network topologies, (b) Fault Analyzer â generic fault analysis, metric computations

tier. The tool has been developed following the Model-ViewController (MVC) design pattern. Figure 2 outlines the high
level architecture and some of the components of the tool.

map tiles are rendered from OpenStreetMap [24]. The NPMT
uses the OpenLayers API to support an user interactive map
interface.

The tool is currently accessible from Arizona State Universityâs WAN, and runs from our testbed server. The toolâs webapplication is deployed on an Apache Tomcat 7 instance, and
the repository used is MySQL. The application tier business
logic for operations on network topologies, such as RegionBased Fault Analysis and Region Disjoint Path Analysis, are
implemented in Java. Additional packages and libraries, such
as IBM ILOG CPLEX Optimization Studio libraries (required
for solving Integer Linear Programs), are setup and made
available on the testbed server. Our testbed server is a 64bit Intel Core 2 Quad Core (2.66 GHz) system with 8 GB of
RAM running an Ubuntu 14.04 instance.

To create the topology and place nodes and edges on the
map, the user can either point-and-click on the map itself,
or can type in specific latitude and longitude coordinates and
then proceed to add the network entity. Capacities for each
edge (in Gigabits per second), can also be specified during the
edge creation process. Once a network topology is created, the
topology must be saved to be used for Network Analysis and
Network Simulation. The topologies are saved on the NPMT
server and can be loaded back into the Topology Manager to
edit any entity or attribute of the network.

B. System Capabilities
The NPMT is designed to be used by following a three
step workflow comprising of (i) Network Creation, (ii) Network Analysis, and (iii) Network Simulation. Accordingly, the
individual features and the executable modules of the NPMT
are bundled around these three workflow steps. The following
list enumerates the current high-level features of the tool and
the corresponding workflow step that each feature emulates:
1)
2)
3)
4)

Topology Management (Network Creation)
Fault Analysis (Network Analysis)
Path Analysis (Network Analysis)
Traffic and Fault Impact Simulation (Network Simulation)

Each of the above features are accessible from a tabbed
interface of the tool and can be navigated to from any part of
the web-application. In the following subsections we discuss
each of the features and provide a brief functional overview.
Topology Management
Network Creation is the first step of the NPMT workflow and
the Topology Manager interface allows the user to create, edit,
save and delete network topologies. The Topology Manager
presents the user with a geographical map interface that she
can interact with to manage network topologies. The displayed

Figure 3(a) shows a screen grab of the Topology Manager.
As seen in the figure, the map interface is on the right and the
user interact-able menu is on the left. The user can click on the
map to to add nodes and edges, or can alternatively type in the
latitude and longitude coordinates in the input fields available
on the menu. The menu also lists the nodes and edges that are
part of the topology. Selecting an edge or node from these lists
highlights the network entity on the map (in yellow), and the
user can then proceed to remove the entity from the network if
necessary. The displayed map overlays can be toggled from a
dropdown menu available on the map (in blue in Figure 3(a)).
Finally, as seen in Figure 3(a), options for saving, loading, and
deleting topologies are available to the user directly below the
displayed mapâs dimensions.
Fault Analysis
Once network topologies are created from the Topology Manager, the Fault Analyzer can be used to analyze the created
networks for their resilience in the presence of spatially correlated faults. In the context of the NPMT, network resiliency
is measured by how well the network performs when benchmarked against the metrics outlined in Section II-A. It may
be noted that the metrics of Section II-A emphasize resilience
from the aspect of connectivity in the presence of a spatially
correlated fault. For example, the more number of disconnected
components a network has due to a fault, the worse is the
networkâs resilience (as captured by the metric RBCDN). It

(a)
(b)
Fig. 4: Fault Analyzer - Specified Fault Analysis. (a) User specified fault coordinates, (b) Fault impact of the user defined fault and an imported
library fault (coordinates for the state of California, USA)

may be noted that, for the purpose of this analysis the NPMT
assumes that any network entity (nodes/edges), that fall within
the fault area are all rendered inoperable, i.e. the fault model
is deterministic and if a network entity falls within the fault
region, it necessarily fails. To carry out this analysis, the user
first selects a network topology and can then choose to either
perform a generic fault analysis, or a specified fault analysis.
These analyses are described below.
Generic Fault Analysis: In the generic fault analysis, for a
selected network topology, the user specifies a fault feature and
the tool computes the values of the individual metrics listed
in Section II-A. The NPMT can generically analyzes circular
faults, and the supported fault feature is the fault radius r.
As shown in Figure 3(b), the user can specify the fault
radius r from the left menu. The tool then performs the
generic fault analysis by (i) computing all the distinct regions with radius r using the techniques implemented in
the module âDistinct Regions Computation Moduleâ (Section
II-B), and (ii) computes the individual metrics using the
techniques implemented in the module âRegion-Based Fault
Metrics Computation Moduleâ (Section II-A). The results
are subsequently reported back to the user. For the network
selected in Figure 3(b) and radius r = 500 km., the computed
Region-based Component Decomposition Number (RBCDN)
is 2, the Region-based Largest Component Size (RBLCS) is 9
and the Region-based Smallest Component Size (RBSCS) is
1. Finally, the number of distinct regions computed is 112.
As shown in Figure 3(b), the user is also presented with
sample worst case fault scenarios where a distinct fault causes
the network to fragment into the same number of components
as the RBCDN. Selecting one of the listed faults updates
the displayed network with the faultâs impact. In Figure 3(b)
the fault centered at 36.249â¦ N , â85.696â¦ E is selected. The
nodes and edges rendered inoperable by the fault are grayed
out, while the surviving nodes and edges are shown in green
and black respectively. The connected components in the
fragmented network are highlighted by a light-green region. In
this example, the loss of the two grayed out nodes causes the
network to fragment into two disconnected components: one

with 9 components, and the other with 1 component. Options
for saving the analysis results are available from the menu.
Specified Fault Analysis: In the specified fault analysis, the
user can provide the exact coordinates of one or more faults
and visualize the impact of these faults on the selected network.
The user has the option to save and load faults to visualize the
impact of a fault on different networks. The NPMT also comes
bundled with a set of library faults that the user can choose
from to simulate fault impact on a network. The current set
of library faults consist of the coordinates of the 50 states
of the USA. The inclusion of a fault library in the NPMT is
to provide the user with pre-defined fault scenarios based on
known fault patterns, faults centered at a target of interest, or
recorded faults. For example, fault impact zones of Level 4
hurricanes such as hurricane Katrina or hurricane Sandy.
As shown in Figure 4(a), to specify the exact coordinates of
the fault region the user can either type in the exact coordinates
of the fault region coordinates, or can click on the map to add
such coordinates. The user also has the option for importing
library faults. Once all the fault regions are defined, the NPMT
can simulate the impact of the fault on the selected network.
In Figure 4(b), apart from the user specified fault region,
the boundary of the state of California has been imported
from the fault library and the selected network has been
analyzed for these two fault regions. The updated map shows
the impacted nodes and edges in gray, while the operable
nodes and edges are shown in green and black respectively.
The connected components are shown with a green region. As
seen in Figure 4(b) the menu displays impact statistics such
as, the number of surviving nodes/edges and the number of
connected components. The user is provided with the option
to save the analysis results for later reference, and also the
option to save the defined fault regions for later use.
Path Analyzer
The Path Analyzer allows the user to analyze the network by
computing paths between source and destination nodes that
provide protection against spatially correlated faults. As in the
Fault Analyzer, the Path Analyzer allows the user to specify
a fault feature, and the tool then proceeds to compute paths

(a)
(b)
Fig. 5: Path Analyzer - Region disjoint paths between a source and destination nodes for given fault radius (r) and no-fault zone radius (nfr )
(a) r = 100 km., nfr = 300 km. (b) r = 120 km., nfr = 300 km.

between a given source and destination node pair such that (i)
at least one of the paths survive in the presence of one or more
spatially correlated faults, and (ii) satisfy some other network
resource constraint.
In the current version of the tool the faults considered
are circular faults and the supported fault feature that can
be specified by the user is the fault radius r. The number
of spatially correlated faults considered for path analysis is
one, and the number of paths computed is two, i.e. the NPMT
computes two paths such that if a single circular fault with
radius r occurs anywhere in the network, at least one of the two
paths computed will not be affected by the fault. The network
resource constraint supported is that the sum of lengths of the
two paths computed must be minimum.
It may be noted that a single fault can also render inoperable either the source node, or the destination node, or both,
and thus there always exists a fault region that affects all paths
computed and no region disjoint paths can exist such that at
least one path remains immune to the fault. To accommodate
this case when the source and/or destination nodes themselves
are part of the fault region, the NPMT supports a âNo-Fault
Zoneâ parameter. The user can specify a no-fault zone radius
nfr for both the source and destination nodes that reserves
two circular areas with radius nfr centered at the source and
destination nodes such that network entities, or parts of a
network entity (such as an edge segment), that falls within
this no-fault zone are immune to faults.
Figures 5(a) and 5(b) show screen grabs of the path
analyzer computation for different input values of fault radius
(r). The no-fault zone set to a radius of nfr = 300 km. and
is shown as a white circular region centered at the source and
destination nodes. The computed paths are shown in orange
and blue, and the lengths of each of these to paths are reported
in the left menu. The effect of the path selection criteria, i.e.
the sum of the lengths of the two paths must be minimum,
is also visible in Figures 5(a) and 5(b). In Figure 5(a) when
r = 100 km., the sum of lengths of the two paths is 5793.24
km., however in Figure 5(b) increasing r to 120 km. the
previously computed paths are no longer feasible as a region
fault exists that can impact both these paths. Hence, new paths
are computed and the sum of the new lengths is 5921.69 km.

Traffic and Fault Impact Simulation
For a selected network, the Traffic and Impact Simulator allows
users to generate traffic and faults to analyze the impact of
faults on a load bearing network. To perform this analysis,
a simulation schedule consisting of bandwidth requests and
faults is generated by the NPMT using user provided simulation parameters. Parameters such as total number of time steps
in the schedule, total number of requests in the schedule, minimum/maximum request bandwidth and minimum/maximum
request hold times can be specified by the user. The source and
destination nodes for each request can be generated randomly,
or can be user specified. For introducing faults in the schedule,
the user can specify the number of faults to introduce and can
either specify the exact fault coordinates, or introduce random
circular faults from the set of all possible distinct circular faults
for a specified fault radius. Time intervals of the faults can be
user specified, or can be randomly generated by the NPMT.
Using the request and fault settings, the NPMT then generates
a time stepped simulation schedule of requests and faults. Once
the schedule is finalized, the user can specify the algorithm to
be used in the simulation to route requests from source and
destination nodes, and then proceed to run the simulation.
As shown in the screen grabs of Figures 6(a) and 6(b), the
left menu of the Traffic and Impact Simulator contains the fault
and simulation parameter fields that can be used to generate
the schedule and run the simulation. The tables below the
mapâs dimensions allow the user fine grained control over the
requests and faults that will be simulated. Once the simulation
is complete, for each time interval the network state can be
visualized from the âEvent Simulation Resultsâ table. The user
can click on a row of this table to visualize the network state
on the map for that specific time interval. The user can also
âplayâ the simulation results and the NPMT will iterate over
all the time steps and update the map with the network state
at each step. In Figures 6(a) and 6(b) the impact of a fault
and the corresponding response of the network is shown. In
Figure 6(a) the network is fault free, but in Figure 6(b) a fault
is introduced and an edge is rendered inoperable. It can be
seen that the red and yellow flows of Figure 6(a) are impacted
by the fault, however, as bandwidth is available, in Figure 6(b)
the flows are rerouted in response to this fault.

(a)
(b)
Fig. 6: Traffic and Fault Impact Simulator (a) Pre-Fault network state, (b) Post-Fault network state â rerouted red and yellow flows)

IV.

C ONCLUSION

In this paper we presented a summary of the work done
towards developing a Network Planning and Management Tool
(NPMT), intended to support design and analysis of single
layer and multi-layer networks in the presence of spatially
correlated faults. We highlighted that the NPMT is particularly
suitable for planning and design of critical infrastructures.
We described the underlying novel concepts that have been
developed to enhance robustness of networks in presence of
region based faults, and then described how those concepts
have been incorporated into the NPMT. The goal of this
paper was to bring to the attention of the networking research
community, and to the audience of the workshop on DRCN
in particular, about the existence of NPMT as a tool that
consolidates a large body of work on spatially correlated faults.
To the best of our knowledge no such tool is available today
that supports planning and designing of single layer and multilayer networks in the presence of spatially correlated faults.
R EFERENCES
[1] NEXT Lab, Arizona State University. The Network Planning and
Management Tool. [Online]. Available: http://netsci.asu.edu/networktool/
[2] R. Diestel, Graph Theory. Springer, 2005.
[3] E. Ganesan and D. K. Pradhan, âThe Hyper-deBruijn Networks: Scalable
Versatile Architecture,â IEEE Transactions on Parrallel and Distributed
Systems, vol. 4, no. 9, September 1993.
[4] A. Sen, B. H. Shen, L. Zhou, and B. Hao, âFault-tolerance in Sensor
Networks: A New Evaluation Metric,â in Proceedings of IEEE Infocom,
Barcelona, Spain, April 2006, pp. 1â12.
[5] S. Neumayer and E. Modiano, âNetwork reliability with geographically
correlated failures,â in INFOCOM, 2010 Proceedings IEEE. IEEE,
2010, pp. 1â9.
[6] Y. Cheng, M. T. Gardner, J. Li, R. May, D. Medhi, and J. P. Sterbenz,
âOptimised heuristics for a geodiverse routing protocol,â in 10th International Conference on the Design of Reliable Communication Networks
(DRCN), 2014, 2014, pp. 1â9.
[7] P. Agarwal, A. Efrat, S. Ganjugunte, D. Hay, S. Sankararaman, and
G. Zussman, âThe resilience of wdm networks to probabilistic geographical failures,â in Proceedings of IEEE INFOCOM, 2011.
[8] S. Trajanovski, F. Kuipers, A. Ilic, J. Crowcroft, and P. Van Mieghem,
âFinding critical regions and region-disjoint paths in a network,â
IEEE/ACM Transactions on Networking, vol. 23, no. 3, pp. 908â921,
2015.
[9] S. Banerjee, A. Das, A. Mazumder, Z. Derakhshandeh, and A. Sen, âOn
the impact of coding parameters on storage requirement of region-based
fault tolerant distributed file system design,â in Computing, Networking
and Communications (ICNC), International Conference on. IEEE, 2014,
pp. 78â82.

[10] A. Mazumder, A. Das, C. Zhou, and A. Sen, âRegion based fault-tolerant
distributed file storage system design under budget constraint,â in Reliable Networks Design and Modeling (RNDM), 2014 6th International
Workshop on. IEEE, 2014, pp. 61â68.
[11] A. Sen, A. Mazumder, S. Banerjee, A. Das, C. Zhou, and S. Shirazipourazad, âRegion-based fault-tolerant distributed file storage system
design in networks,â Networks, Wiley Online Library, 2015.
[12] A. Sen, S. Murthy, and S. Banerjee, âRegion-based connectivity-a new
paradigm for design of fault-tolerant networks,â in High Performance
Switching and Routing, 2009. HPSR 2009. International Conference on.
IEEE, 2009, pp. 1â7.
[13] S. Banerjee, S. Shirazipourazad, P. Ghosh, and A. Sen, âBeyond
connectivity-new metrics to evaluate robustness of networks,â in High
Performance Switching and Routing (HPSR), 2011 IEEE 12th International Conference on. IEEE, 2011, pp. 171â177.
[14] S. Banerjee, S. Shirazipourazad, and A. Sen, âDesign and analysis of
networks with large components in presence of region-based faults,â in
International Conference on Communications (ICC). IEEE, 2011.
[15] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
âCatastrophic cascade of failures in interdependent networks,â Nature,
vol. 464, no. 7291, pp. 1025â1028, 2010.
[16] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, âNetworks formed
from interdependent networks,â Nature Physics, vol. 8, no. 1, pp. 40â48,
2011.
[17] J.-F. Castet and J. H. Saleh, âInterdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,â PloS one, vol. 8, no. 4, p. e60402, 2013.
[18] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, âIdentification of k most vulnerable nodes in multi-layered network using a
new model of interdependency,â in NetSciCom Workshop (INFOCOM
WKSHPS), Conference on Computer Communications. IEEE, 2014,
pp. 831â836.
[19] A. Das, J. Banerjee, and A. Sen, âRoot cause analysis of failures in
interdependent power-communication networks,â in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910â915.
[20] J. Banerjee, A. Das, C. Zhou, A. Mazumder, and A. Sen, âOn the entity
hardening problem in multi-layered interdependent networks,â in WIDN
Workshop (INFOCOM WKSHPS), 2015 IEEE Conference on Computer
Communications. IEEE, 2015, pp. 648â653.
[21] A. Das, C. Zhou, J. Banerjee, and A. Sen, âOn the smallest pseudo
target set identification problem for targeted attack on interdependent
power-communication networks,â in Military Communications Conference (MILCOM), IEEE (To appear). IEEE, 2015.
[22] J. Banerjee, C. Zhou, A. Das, and A. Sen, âOn robustness in multilayer interdependent networks,â in Conference on Critical Information
Infrastructures Security (CRITIS) (To appear). Springer, 2015.
[23] A. Mazumder, C. Zhou, A. Das, and A. Sen, âProgressive recovery from
failure in multi-layered interdependent network using a new model of
interdependency,â in Conference on Critical Information Infrastructures
Security (CRITIS). Springer, 2014.
[24] OpenStreetMap Contributors. OpenStreetMap. [Online]. Available:
www.openstreetmap.org

Analysis of On-line Routing and Spectrum Allocation in Spectrum-sliced Optical Networks
Shahrzad Shirazipourazad, Zahra Derakhshandeh and Arunabha Sen
School of Computing, Informatics and Decision System Engineering Arizona State University Tempe, Arizona 85287 Email: {sshiraz1, zderakhs, asen}@asu.edu

Abstract--The orthogonal frequency division multiplexing (OFDM) technology provides an opportunity for efficient resource utilization in optical networks. It allows allocation of multiple sub-carriers to meet traffic demands of varying size. Utilizing OFDM technology, a spectrum efficient and scalable optical transport network called SLICE was proposed recently. The SLICE architecture enables sub-wavelength, super-wavelength resource allocation and multiple rate data traffic that results in efficient use of spectrum. However, the benefit is accompanied by additional complexities in resource allocation. In SLICE architecture, in order to minimize utilized spectrum, one has to solve the routing and spectrum allocation (RSA) problem, a generalization of the routing and wavelength allocation (RWA) problem. In this paper, we focus our attention to the on-line version of RSA problem and provide an algorithm for the ring network with a competitive ratio of min{O(log(dmax )), O(log(k))} where k is the total number of requests and dmax is the maximum demand in terms of the number of sub-carriers. Moreover, we provide a heuristic for the network with arbitrary topology and measure the effectiveness of the heuristic with extensive simulation.

I. I NTRODUCTION It is being increasingly recognized by the optical network designers that in order to meet the challenges posed by the explosive growth of the network traffic, the networks must be operated in the most innovative and efficient manner. The traditional WDM network operates at the granularity of a wavelength, which may lead to inefficient use of resources as some connection requests may not have enough traffic to utilize the full capacity of a wavelength. However such wastage of networking resources can be avoided if the optical network can be made to operate at a finer grain (i.e., subwavelength level) instead of the current practice of course grain operation (i.e., wavelength level). Recent introduction of Orthogonal Frequency Division Multiplexing (OFDM) technology in optical networks [1] offers an opportunity for operating optical networks at a much finer grain than what is currently possible. The advantages offered by the OFDM in terms of flexibility and scalability originate from the unique multicarrier nature of this technology [1]. Utilizing the OFDM technology, a spectrum efficient and scalable optical transport network called spectrum-sliced elastic optical path network (SLICE) was proposed recently [2]. Just as the ability to operate at a granularity finer than a wavelength (i.e., a sub-wavelength) will enable the network operator to manage resources more efficiently, the same is

true if the operator is provided with capability to operate at super-wavelength granularity. Such a capability will be useful for the network operator to meet large traffic demand. The goal of SLICE architecture is to allocate variable sized optical bandwidths that matches with the user traffic demands. It achieves that goal by slicing off spectral resources of a route and allocating only the requested amount to establish an endto-end optical path. Although the sub-wavelength (sub-carrier) level allocation capability of SLICE leads to more effective resource utilization, it also leads to additional complexities in network control and management. First, if a call requests for d sub-carriers, the network controller must allocate d consecutive sub-carriers to this request. Second, if the paths corresponding to two requests R1 and R2 share a fiber link, not only the set of carriers allocated to R1 and R2 must be disjoint, in order to avoid interference, they must be separated from each other in the spectrum domain by a few carriers, known as guard carriers or guard bands. The first and the second constraints are known as the sub-carrier consecutiveness constraint and the guardcarrier constraint respectively [3]. The introduction of the subcarrier consecutiveness constraint significantly increases the complexity of the Routing and Spectrum Assignment (RSA) problem that needs to be solved in SLICE. The RSA problem may be informally defined as follows: Given a network topology and a set of call requests with varying demands (in terms of the number of sub-carriers) find a route for each request and allocate a number of sub-carriers to each request (equal to their requested demand), so that the utilized part of the spectrum span is minimized. It may be noted that if the demand of each request is one sub-carrier, then the RSA problem reduces to the Routing and Wavelength Assignment (RWA) problem, which has been studied extensively. One can conceive of two different versions of the RSA problem - offline and on-line. In the off-line version all the requests are known ahead of time before path and spectrum allocation for any request is carried out. In the on-line version, the requests come in a sequence and path and spectrum allocation for a request has to carried out at the time of arrival of that request. Because the off-line version has the luxury of knowing all the requests, it can carry out better optimization of utilized spectrum span than its on-line counterpart. In this paper we study the on-line version of RSA problem. Previous studies on the on-line version of the RSA problem,

[4]­[9] primarily focus on the development of efficient heuristics for the problem. The effectiveness of these heuristics are primarily evaluated through simulation. To the best of our knowledge, very little analytical results are available in the literature regarding the performance of these heuristics. In this paper we present analytical results relating to the on-line version of the RSA problem when the network topology is a ring. The performance of an on-line algorithm is measured in terms of the metric competitive ratio. In this metric, the performance of an on-line algorithm is compared with the performance of an optimal off-line algorithm that knows the sequence of requests in advance. The maximum ratio between their respective performances, taken over all sequences, is known as the competitive ratio of the algorithm [10]. In this paper, we provide an algorithm for the on-line version of the RSA problem for the ring network with a competitive ratio of min{O(log(dmax )), O(log(k ))} where k is the total number of requests, dmax = max1ik di , and di is the demand in terms of the number of sub-carriers associated with request Ri . Moreover, we provide a heuristic for the network with arbitrary topology and measure the effectiveness of the heuristic with extensive simulation. The rest of the paper is organized as follows. We discuss related works in section II. In section III we introduce definitions and notations. We present problem statement for the on-line RSA problem in section IV. Analytical results for on-line RSA in rings is presented in section V. A heuristic and experimental results for the arbitrary network topology is presented in section VI. Section VII concludes the paper. II. R ELATED W ORK Utilizing the optical OFDM technology, the SLICE architecture proposes a novel scheme for slicing off the spectral resources of a route, resulting in more efficient utilization [2]. The fact that the sub-carriers in the SLICE architecture have to be assigned in a contiguous manner, led to the formulation of the RSA problem. To the best of our knowledge, the RSA problem was originally introduced in [4], [11], [12]. Since then a few other papers, [3], [13] have also studied the RSA problem. In most of these studies [3], [12]­[14], the authors propose an integer linear program based solution and a heuristic solution for the off-line RSA problem. Based on the experimental results, the authors claim effectiveness of their heuristics. The on-line version of RSA problem has been studied in [4]­[9]. In all of these papers, the objective of the on-line RSA problem is to maximize the number of requests that can be satisfied and minimize the blocking probability. In this version of on-line RSA problem, the number of available spectrum sub-carriers is limited. The authors of these papers proposed heuristic solutions mainly by modifying the Dijkstra shortest path algorithm or using K-shortest path algorithm accompanying with the First-Fit algorithm. To the best of our knowledge none of these papers consider the objective of minimizing the utilized spectrum while satisfying all the requests. It may be the case that all the requests should be satisfied while the utilized spectrum is minimized. In this paper

we propose a new heuristic for arbitrary network graphs. We also modify the K-shortest path approach for this version of the on-line RSA and through simulations we evaluate their performance. Most of the studies both on on-line and off-line RSA do not present any analytical results for the RSA problem, even for the simplest optical network topologies such as rings. The ring topology is of particular importance in the optical domain because of its application in metro networks and in some long haul networks. A major thrust of our effort is to present analytical results for the on-line RSA for optical networks with ring structure. III. D EFINITIONS AND N OTATIONS Spectrum Slice/Interval: A number of consecutive sub-carriers from ai to bi denoted by [ai , bi ], that is allocated to a specific request Ri to establish a connection between (si , ti ) with di sub-carriers. The length of this slice is bi - ai + 1 = di . Spectrum Span/Spread: The total amount of spectrum used for allocating a slice to all the requests; If Ri , 1  i  k is allocated the spectrum interval [ai , bi ] then the spectrum span is [ min ai , max bi ].
1ik 1ik

Chromatic Number: The Chromatic Number, (G), of a graph G = (V, E ) is the fewest number of colors necessary to color the nodes of the graph, such that no two adjacent nodes have the same color. Interval Chromatic Number (ICN): Consider a weighted graph G = (V, E, w) with a strictly positive integer weight w(v ) associated with each node v  V . An interval t-coloring of G = (V, E, w) is a function c from V to {1,2, . . . , t} such that c(x) + w(x) - 1  t and if both c(x)  c(y ) and (x, y )  E then c(x) + w(x) - 1 < c(y ). We can view an interval coloring c of G as assigning an interval [c(v ), . . . , c(v ) + w(v ) - 1] of w(v ) consecutive colors to each vertex v so that the intervals of colors assigned to two adjacent vertices (i.e., the pair of nodes that has an edge between them) do not overlap. If interval t-coloring is feasible for a graph G then G is said to be interval t-colorable. The interval chromatic number of G , denoted by int (G ) is the least t such that G has a interval t-coloring [15]. Interval Graph: Let F be a family of non-empty sets. The intersection graph of F is obtained by representing each set in F by a node and connecting the two nodes with an edge, if and only if the corresponding sets intersect. The intersection graph of a family of intervals on a linearly ordered set (such as the real line) is called Interval Graph. Path Intersection Graph: Consider a graph G = (V, E ) and a set of paths P = {P1 , . . . , Pk }, where each Pi is a path between a node pair (si , ti ), i, 1  i  k . A graph G = (V , E ) is a Path Intersection Graph corresponding to P , if each vertex pi  V corresponds to a path Pi  P and two nodes pi and pj in V have an edge between them, if the corresponding paths Pi and Pj in P have at least one common edge in E . IV. P ROBLEM F ORMULATION In this section we provide a formal statement of the on-line routing and spectrum allocation problem.

On-line Routing and Spectrum Allocation (RSA) Problem: A graph G = (V, E ) representing the network topology is given. The connection requests arrive in a sequence one by one where k is the total number of requests. The ith connection request is denoted by a triple Ri = (si , ti , di ), 1  i  k , where si represents a source node, ti represents a destination node, and di represents the demand between si and ti in terms of sub-carriers. Once a request Ri arrives without knowledge of the future requests, assign a path Pi from si to ti and assign a spectrum interval Ii = [ai , bi ] of length di to Pi , such that for every pair of requests i and j, j  i the intervals Ii and Ij do not overlap if the corresponding paths Pi and Pj share an edge between them in G = (V, E ). Moreover, if the paths Pi and Pj overlap, not only the corresponding intervals Ii and Ij must be non-overlapping, these two intervals must be separated by a fixed number of sub-carriers, known as the guard band. The objective is to minimize spectrum span, I = [min1ik ai , max1ik bi ]. Without loss of generality, we number the first available sub-carrier one and the rest are numbered accordingly. We note that guard-band constraint can be satisfied by increasing the demand values by guard-band value g . In other words, in an instance of RSA problem, RSA1 with guard-band g1 > 0 and requests {Ri = (si , ti , di )|1  i  k }, we can increase the demand values in each request by g1 and consider another instance of RSA, RSA2 where guard-band g2 = 0 and for every request Ri in RSA1 , request Ri = (si , ti , di + g1 ) is added to RSA2 . Then the optimal solution of RSA2 can be used to create the optimal solution of RSA1 by removing the last g1 sub-carriers from each spectrum slice assigned to each request (for the proof, reader is referred to the proof of the Observation 4 in [16]). As a result, from this point onward we assume that guard-band is zero. The RSA problem has two distinct components - the routing component and the spectrum allocation component. When routing is given and the paths for the requests are known then interval chromatic number (ICN) of the intersection graph of request paths finds the solution of the SA problem. Let G = (V , E , w) be the weighted path intersection graph of paths of all requests where V = {p1 , p2 , . . . , pk } and each node pi corresponds to the path of request Ri and the weight of pi is di ; i.e., w(pi ) = di . Let int (G ) be the ICN of graph G . In computation of int (G ), each node pi  V is assigned an interval [ai , bi ] of colors with length w(pi ) = di where the intervals of two adjacent vertices do not intersect and total number of distinct colors used is minimum. Therefore, interval [ai , bi ] can be allocated to the path Pi in G and no two paths with common edge intersect in their spectrum intervals. Hence, the spectrum span of int (G ) is sufficient for the spectrum allocation of requests in G with predefined set of paths P . Moreover, int (G ) is the minimum spectrum span needed in the SA problem; otherwise, it contradicts with int (G ) being the minimum interval chromatic number of G . It is known that computation of ICN of interval graphs is NP-complete (Problem SR2 in [17]). Fig. 1 shows an example of SA instance where the network graph is a ring with 8 nodes and requests are {R1 =

1 8 7 2 3

p1 p3 p4

6 5

4

p5

p2

(a)

(b)

Fig. 1. (a) An example of SA instance where the network graph is a ring (b) Path intersection graph G of SA instance in (a)

(1, 3, 15), R2 = (1, 6, 6), R3 = (2, 5, 6), R4 = (2, 8, 6), R5 = (4, 7, 12)}. Dashed lines show the paths for the requests. Fig. 1(b) depicts G , the path intersection graph of these paths where w(p1 ) = 15, w(p2 ) = 6, w(p3 ) = 6, w(p4 ) = 6 and w(p5 ) = 12. In this example, int (G ) is 24 where the requests R1 to R5 are assigned intervals [1, 15], [13, 18], [16, 21], [19, 24] and [1, 12] respectively. V. O N - LINE ROUTING AND S PECTRUM A LLOCATION P ROBLEM IN R INGS Theorem 1: RSA problem (the off-line case) is NPComplete when the optical network topology is a Ring. Proof: If the demands of the requests in the off-line RSA instance are all equal to one, then RSA problem becomes RWA problem. In [18], it is proven that the RWA problem for optical networks with a ring topology is NP-complete. Since RWA problem is a special case of the RSA problem, it follows that the RSA problem for optical networks with a ring topology is also NP-complete. Next, we propose an on-line algorithm for RSA problem when network topology is a ring. In this algorithm, first we use cut-one-link approach and after removing one link the induced graph is a chain. In the chain for every request there exists just one path. Therefore routing is trivial. For the spectrum assignment, we use First-Fit technique that finds the first free spectrum interval fit the demand of the current request. The steps of the algorithms are explained in Algorithm 1. Algorithm 1 On-line RSA in Ring 1: Remove an edge e  E randomly; Let Gp be the induced chain; 2: while A new request arrives do 3: Find the path for the request in graph Gp ; 4: Compute the first free spectrum interval fit the demand of the current request 5: end while Theorem 2: Algorithm 1 has competitive ratio of min{O(log(dmax )), O(log(k ))} where k is total number of requests and dmax = max1ik di . Proof: In order to compute the competitive ratio we need to compare the spectrum span of Algorithm 1 with the optimal spectrum span of off-line RSA where the sequence of requests is known in advance. After removing one edge randomly from G = (V, E ) in Algorithm 1, the induced graph Gp is a chain. Let OP T and OP Tp be the optimal spectrum

span in RSA problem when network graph is G and Gp , respectively, and I be the size of the spectrum computed by Algorithm 1. Clearly, the intersection graph of paths of the requests in Gp is an interval graph (a path from node i to node j in Gp can be interpreted as an interval from i to j ). Let Gp be the path intersection graph. Therefore, minimum spectrum needed to satisfy requests in Gp is equivalent to the int (Gp ). Based on the paper [10], First-Fit algorithm will have competitive ratio of min{O(log(dmax )), O(log(Gp ))} for on-line interval coloring in Gp . Also it is obvious Gp  k (i.e., chromatic number of Gp is at most as large as the number of nodes in Gp that is number of requests). Hence, we have (1) I  min{O(log(dmax )), O(log(k ))} · OP Tp . We denote the set of paths in the optimal solution of RSA (off-line) when network graph is G by POP T . The paths in POP T can be 1 2 1 partitioned into two subsets, Pe and Pe such that Pe is the set 2 of paths that include edge e and the paths in Pe do not include 1 2 edge e. Let OP Te and OP Te be the ICN of the intersection 1 2 graph of paths in Pe and Pe respectively. Then we have (2) 1 2 1 OP T  max(OP Te , OP Te ). Since all the paths in Pe have intersection in edge e, their intervals do not intersect. Clearly, 1 2 (3) OP Tp  OP Te + OP Te . The reason is that in the worst case, all requests that were routed through edge e in POP T are routed the other way in Gp and now they at most need 1 OP Te spectrum span not intersecting the spectrum allocated 2 to the paths in Pe . Therefore, using relations in (2) and (3) we have OP Tp  2OP T . Also, based on relation (1) we can conclude I  min{O(log(dmax )), O(log(k ))} · OP T . VI. A H EURISTIC AND R ESULTS FOR G ENERAL G RAPHS In this section first we present our heuristic for on-line RSA problem in general graphs. Then we present the results of our extensive simulation that demonstrate the efficacy of our heuristic for the on-line RSA problem by comparing it against (i) the optimal solution and (ii) the solution obtained by executing the heuristic based on K -shortest path and FirstFit technique. Minimum Sub-Carrier Path Heuristic (MSCP): The main idea in our heuristic is that it tries to find disjoint paths for routing the requests to increase the reuse of sub-carriers in spectrum allocation. Of this concern, we define a new weight function on the edges (fibers) of the network where weight of an edge e  E , w(e) will be largest sub-carrier number that is used in that edge. We also define the weight of a path, P from node s to node t to be maxeP {w(e)}. For each new request, M SCP selects the path with minimum weight. The minimum weight path can be computed by modifying the distance function in Dijkstra algorithm so that it considers the new weight function as the distance. After finding the path, M SCP uses First-Fit algorithm to find the first available spectrum slice with the length of the request demand in all the edges of the path. Then, M SCP updates the weight of every edge in the path to the largest sub-carrier so far used in that edge. For each request, time complexity of minimumweight path computation is O(|V |2 ) and First-Fit algorithm takes O(k |V |2 ) where k is the number of requests. Hence, time complexity of M SCP is O(k 2 |V |2 ).

K -Shortest Path Heuristic (KSP): In this heuristic, initially K shortest paths are computed between every pair of nodes in the network using [19] algorithm with O(k |V |3 ). When a request Ri arrives, for every path in the K shortest paths between si and ti we compute First-Fit algorithm to find the first available spectrum slice [ai , bi ] with the length of di . Then we select the path whose first available spectrum slice [ai , bi ] has the smallest bi . This algorithm takes O(Kk 2 |V |2 ) for satisfying all the k requests. We perform our experiments on the NSFnet (Fig. 2(a)) and the fiber network of Level-3 that spans Europe (Fig. 3(a)) [20]. We view the NSFnet and Level-3 networks as examples of a small and a large network respectively. In Fig. 2(b), we present the results obtained from ILP , M SCP and KSP when executed on the NSFnet. We find the optimal solution of the RSA problem (off-line) by solving an ILP using the software package CPLEX. Since computing the optimal solution by ILP takes considerable amount of time, we need to do this set of experiments for small number of requests. In this set of experiments, the number of requests, k , is varied from 2 to 6 with step of one. For each value of k , we generate 10 instances. In each instance we generate k requests randomly and consider them one at a time. For this set of experiments all the demand values are at most 5, (i.e., dmax  5). The average spectrum span computed by each of the three methods is shown in Fig. 2(b). It may be observed that the average spectrum span of M SCP is closest to the ILP almost in all cases. The ratio of the average spectrum span of M SCP to ILP is at most 1.28 demonstrating the closeness of the M SCP to the optimal. The results in these experiments also show that M SCP works better than KSP algorithm in almost all the cases even when number of paths in KSP is K = 3. We repeat similar experiments for larger value of k , where k = 10 and we change the value of dmax from 5 to 25. The result of these experiments is depicted in Fig. 2(c). It can be observed that spectrum span in M SCP is at least 12% smaller than the span in KSP where K = 1 and it is even smaller than the one in KSP where K = 2. When K = 3 in KSP , KSP needs smaller spectrum span than M SCP but its time complexity is at least 3 times M SCP . We perform our next set of experiments on the Level-3 network shown in Fig. 3(a). In these experiments, first, we vary k from 10 to 60 with step of 10. For a specific value of k we generate 10 instances. In all these instances, the maximum demand is limited to 10 (i.e., dmax  10). The average utilized spectrum span is shown in Fig. 3(b). These results show that M SCP efficacy with respect to utilized spectrum span is almost the same as KSP when K = 2. We also conduct experiments for the case that values of dmax is varied from 5 to 25 with step of 5, while keeping the number of requests k constant at 20. We compute the average spectrum span over 10 random instances for each value of dmax . The results are shown in Fig. 3(c). According to these results, M SCP 's performance is better than KSP when K = 2 especially for larger values of dmax . According to the last experiments we may conclude that M SCP outperforms KSP when K = 2 for larger values of dmax .

8
Average Spectrum Span
MI WA

50

Average Spectrum Span

NY

7 6 5 4

dmax  5

40

k = 10

CA1

UT NE CO

IL

NJ PA

3
2 1 2 3 4 5 Number of requests (k) 6

MD CA2 TX GA

KSP (K=1) KSP (K=2) KSP (K=3) MSCP ILP 7

30
20 10 0 0 5 10 15 20 25 Maximum demand (dmax) 30 KSP (K=1) KSP (K=2) KSP (K=3) MSCP

(a)

(b)

(c)

Fig. 2. (a) The 14-node NSF Network, (b) The average spectrum span in NSF Network for different values of k where dmax  5, (c) different values of dmax where k = 10
160 140 Average Spectrum Span 180

dmax  10
Average Spectrum Span

160 140 120 100 KSP (K=1) KSP (K=2) MSCP k = 20

120
100 80 60 40

KSP (K=1)
KSP (K=2) MSCP

80
60 40 20

20
0

0

20

40 60 Number of requests (k)

80

0 0 10 20 30 Maximum demand (dmax) 40

(a) Fig. 3.

(b)

(c)

(a) Level-3 network over Europe, (b) The average spectrum span in Level-3 network for dmax  10, (c) k = 20

VII. C ONCLUSION In this paper we study on-line version of Routing and Spectrum Allocation problem in OFDM-based optical networks. We propose an algorithm for the ring network with a competitive ratio of min{O(log(dmax )), O(log(k ))} where k is the total number of requests and dmax is the maximum demand. In addition, we provide a heuristic for networks with arbitrary topology and measure its effectiveness with extensive simulation. In future, we plan to develop efficient algorithms for on-line RSA in networks with tree and grid topologies. We also would like to extend our results to the case that different modulation models can be used. Acknowledgement The research was supported by the DTRA grant HDTRA1-09-1-0032 and the AFOSR grant FA9550-091-0120. R EFERENCES
[1] W. Shieh, "Ofdm for flexible high-speed optical networks," Journal of Lightwave Technology, vol. 29, no. 10, pp. 1560 ­1577, 2011. [2] M. Jinno, H. Takara, B. Kozicki, Y. Tsukishima, Y. Sone, and S. Matsuoka, "Spectrum-efficient and scalable elastic optical path network: architecture, benefits, and enabling technologies," IEEE Communications Magazine, vol. 47, no. 11, pp. 66 ­73, 2009. [3] Y. Wang, X. Cao, and Y. Pan, "A study of the routing and spectrum allocation in spectrum-sliced elastic optical path networks," in INFOCOM, 2011, pp. 1503­1511. [4] M. Jinno, B. Kozicki, H. Takara, A. Watanabe, Y. Sone, T. Tanaka, and A. Hirano, "Distance-adaptive spectrum resource allocation in spectrumsliced elastic optical path network [topics in optical communications]," IEEE Communications Magazine, vol. 48, no. 8, pp. 138 ­145, 2010. [5] K. Christodoulopoulos, I. Tomkos, and E. Varvarigos, "Dynamic bandwidth allocation in flexible ofdm-based networks," in OFC/NFOEC, 2011. [6] X. Wan, N. Hua, and X. Zheng, "Dynamic routing and spectrum assignment in spectrum-flexible transparent optical networks," IEEE/OSA Journal of Optical Communications and Networking, vol. 4, no. 8, pp. 603 ­613, 2012.

[7] T. Takagi, H. Hasegawa, K. Sato, Y. Sone, B. Kozicki, A. Hirano, and M. Jinno, "Dynamic routing and frequency slot assignment for elastic optical path networks that adopt distance adaptive modulation," in OFC/NFOEC, 2011. [8] A. Castro, L. Velasco, M. Ruiz, M. Klinkowski, J. P. Fern´ andez-Palacios, and D. Careglio, "Dynamic routing and spectrum (re)allocation in future flexgrid optical networks," Compututer Networks, vol. 56, no. 12, pp. 2869­2883, 2012. [9] G. Shen and Q. Yang, "From coarse grid to mini-grid to gridless: How much can gridless help contentionless?" in OFC/NFOEC, 2011. [10] M. G. Luby, "Tight bounds for dynamic storage allocation," SIAM Journal on Discrete Mathematics, vol. 9, no. 1, pp. 155­166, Feb. 1996. [11] A. N. Patel, P. N. Ji, J. P. Jue, and T. Wang, "Routing, wavelength assignment, and spectrum allocation in transparent flexible optical wdm (fwdm) networks," in Photonics in Switching, 2010. [12] K. Christodoulopoulos, I. Tomkos, and E. A. Varvarigos, "Routing and spectrum allocation in ofdm-based optical networks with elastic bandwidth allocation," in GLOBECOM, 2010, pp. 1­6. [13] M. Klinkowski and K. Walkowiak, "Routing and spectrum assignment inspectrum sliced elastic optical path network," IEEE Communications Letters, vol. 15, no. 8, pp. 884­886, 2011. [14] K. Christodoulopoulos, I. Tomkos, and E. Varvarigos, "Elastic bandwidth allocation in flexible ofdm-based optical networks," Journal of Lightwave Technology, vol. 29, no. 9, pp. 1354 ­1366, 2011. [15] H. A. Kierstead, "A polynomial time approximation algorithm for dynamic storage allocation," Discrete Mathematics, vol. 87, no. 2-3, pp. 231­237, 1991. [16] S. Shirazipourazad, C. Zhou, Z. Derakhshandeh, and A. Sen. On routing and spectrum allocation in spectrum-sliced optical networks. [Online]. Available: http://www.public.asu.edu/sshiraz1/RSA.pdf [17] M. R. Garey and D. S. Johnson, Computers and Intractability; A Guide to the Theory of NP-Completeness. W. H. Freeman & Co., 1990. [18] T. Erlebach and K. Jansen, "The complexity of path coloring and call scheduling," Theoretical Computer Science, vol. 255, no. 1-2, pp. 33­50, 2001. [19] J. Y. Yen, "Finding the k shortest loopless paths in a network," Management Science, vol. 17, no. 11, pp. 712­716, 1971. [20] Level 3 Communications, Network Map. [Online]. Available: http: //www.level3.com/en/resource-library/maps/level-3-network-map/

Identification of K Most Vulnerable Nodes in
Multi-layered Network Using a New Model of
Interdependency
Arunabha Sen, Anisha Mazumder, Joydeep Banerjee, Arun Das and Randy Compton
Computer Science and Engineering Program

arXiv:1401.1783v1 [cs.NI] 8 Jan 2014

School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {asen, amazumde, Joydeep.Banerjee, adas22, Randy.Compton}@asu.edu

AbstractâThe critical infrastructures of the nation including
the power grid and the communication network are highly
interdependent. Recognizing the need for a deeper understanding
of the interdependency in a multi-layered network, significant
efforts have been made by the research community in the last
few years to achieve this goal. Accordingly a number of models
have been proposed and analyzed. Unfortunately, most of the
models are over simplified and, as such, they fail to capture the
complex interdependency that exists between entities of the power
grid and the communication networks involving a combination of
conjunctive and disjunctive relations. To overcome the limitations
of existing models, we propose a new model that is able to
capture such complex interdependency relations. Utilizing this
model, we provide techniques to identify the K most vulnerable
nodes of an interdependent network. We show that the problem
can be solved in polynomial time in some special cases, whereas
for some others, the problem is NP-complete. We establish that
this problem is equivalent to computation of a fixed point of a
multilayered network system and we provide a technique for its
computation utilizing Integer Linear Programming. Finally, we
evaluate the efficacy of our technique using real data collected
from the power grid and the communication network that span
the Maricopa County of Arizona.

I. I NTRODUCTION
In the last few years there has been an increasing awareness
in the research community that the critical infrastructures of
the nation are closely coupled in the sense that the well being
of one infrastructure depends heavily on the well being of another. A case in point is the interdependency between the electric power grid and the communication network. The power
grid entities, such as the SCADA systems that control power
stations and sub-stations, receive their commands through
communication networks, while the entities of communication
network, such as routers and base stations, cannot operate
without electric power. Cascading failures in the power grid,
are even more complex now because of the coupling between
power grid and communication network. Due to this coupling,
not only entities in power networks, such as generators and
transmission lines, can trigger power failure, communication
network entities, such as routers and optical fiber lines, can
also trigger failure in power grid. Thus it is essential that
the interdependency between different types of networks be
understood well, so that preventive measures can be taken to

avoid cascading catastrophic failures in multi-layered network
environments.
Recognizing the need for a deeper understanding of the
interdependency in a multi-layered network, significant efforts
have been made in the research community in the last few
years to achieve this goal [1], [2], [3], [4], [5], [6], [7], [8].
Accordingly a number of models have been proposed and
analyzed. Unfortunately, many of the proposed models are
overly simplistic in nature and as such they fail to capture
the complex interdependency that exists between power grid
and communication networks. In a highly cited paper [1], the
authors assume that every node in one network depends on one
and only one node of the other network. However, in a follow
up paper [2], the same authors argue that this assumption may
not be valid in the real world and a single node in one network
may depend on more than one node in the other network. A
node in one network may be functional (âaliveâ) as long as
one supporting node on the other network is functional.
Although this generalization can account for disjunctive
dependency of a node in the A network (say ai ) on more
than one node in the B network (say, bj and bk ), implying
that ai may be âaliveâ as long as either bi or bj is alive,
it cannot account for conjunctive dependency of the form
when both bj and bk has to be alive in order for ai to
be alive. In a real network the dependency is likely to be
even more complex involving both disjunctive and conjunctive
components. For example, ai may be alive if (i) bj and bk and
bl are alive, or (ii) bm and bn are alive, or (iii) bp is alive. The
graph based interdependency models proposed in the literature
[3], [4], [5], [9], [6], [7] including [1], [2] cannot capture
such complex interdependency between entities of multilayer
networks. In order to capture such complex interdependency,
we propose a new model using Boolean logic. Utilizing this
comprehensive model, we provide techniques to identify the
K most vulnerable nodes of an interdependent multilayered
network system. We show that the this problem can be solved
in polynomial time for some special cases, whereas for some
others, the problem is NP-complete. We also show that this
problem is equivalent to computation of a fixed point [10] and
we provide a technique utilizing Integer Linear Programming

2

to compute that fixed point. Finally, we evaluate the efficacy
of our technique using real data collected from power grid
and communication networks that span Maricopa County of
Arizona.

II. I NTERDEPENDENCY M ODEL
We describe the model for an interdependent network with
two layers. However, the concept can easily be generalized
to deal with networks with more layers. Suppose that the
network entities in layer 1 are referred to as the A type
entities, A = {a1 , . . . , an } and entities in layer 2 are referred
to as the B type entities, B = {b1 , . . . , bm }. If the layer 1
entity ai is operational if (i) the layer 2 entities bj , bk , bl
are operational, or (ii) bm , bn are operational, or (iii) bp
is operational, we express it in terms of live equations of
the form ai â bj bk bl + bm bn + bp . The live equation for
a B type entity br can be expressed in a similar fashion
in terms of A type entities. If br is operational if (i) the
layer 1 entities as , at , au , av are operational, or (ii) aw , az
are operational, we express it in terms of live equations of
the form br â as at au av + aw az . It may be noted that the
live equations only provide a necessary condition for entities
such as ai or br to be operational. In other words, ai or br
may fail independently and may be not operational even when
the conditions given by the corresponding live equations are
satisfied. A P
live equation
in general will have the following
Ti Qtj
form: xi â j=1
y
k=1 j,k where xi and yj,k are elements
of the set A (B) and B (A) respectively, Ti represents the
number of min-terms in the live equation and tj refers to the
size of the j-th min-term (the size of a min-term is equal to the
number of A or B elements in that min-term). In the example
ai â bj bk bl + bm bn + bp , Ti = 3, t1 = 3, t2 = 2, t3 = 1,
xi = ai , y2,1 = bm , y2,2 = bp .
We refer to the live equations of the form ai â bj bk bl +
bm bn + bp also as First Order Dependency Relations, because
these relations express direct dependency of the A type entities
on B type entities and vice-versa. It may be noted however
that as A type entities are dependent on B type entities,
which in turn depends on A type entities, the failure of
some A type entities can trigger the failure of other A type
entities, though indirectly, through some B type entities. Such
interdependency creates a cascade of failures in multilayered
networks when only a few entities of either A type or B type
(or a combination) fails. We illustrate this with the help of
an example. The live equations for this example is shown in
table I.
Power Network
a1 â b1 + b2
a2 â b1 b3 + b2
a3 â b1 b2 b3
a4 â b1 + b2 + b3

Communication Network
b1 â a1 + a2 a3
b2 â a1 + a3
b3 â a1 a2
ââ

TABLE I: Live equations for a Multilayer Network

Entities
a1
a2
a3
a4
b1
b2
b3

t0
1
0
0
0
0
0
0

t1
1
0
0
0
0
0
1

Time Steps
t2
t3
t4
1
1
1
0
0
1
1
1
1
0
0
1
0
1
1
0
1
1
1
1
1

t5
1
1
1
1
1
1
1

t6
1
1
1
1
1
1
1

TABLE II: Time Stepped Cascade Effect for a Multilayer Network

Fig. 1: Cascading failures reach steady state after p time steps

As shown in table II, the failure of only one entity a1 at
time step t0 triggered a chain of failures that resulted in the
failure of all the entities of the network after by timestep t4 .
A table entry of 1 indicates that the entity is âdeadâ. In this
example, the failure of a1 at t0 triggered the failure of b3 at
t1 , which in turn triggered the failure of a3 at t2 . The failure
of b3 at t1 was due to the dependency relation b3 â a1 a2
and the failure of a3 at t2 was due to the dependency relation
a3 â b1 b2 b3 . The cascading failure process initiated by failure
(or death) of a subset of A type entities at timestep t0 , A0d and
a subset of B type entities Bd0 till it reaches its final steady
state is shown diagrammatically in figure 1. Accordingly, a
multilayered network can be viewed as a âclosed loopâ control
system. Finding the steady state after an initial failure in this
case is equivalent of computing the fixed point of a function
F(.) such that F(Apd âª Bdp ) = Apd âª Bdp , where p represents
the number of steps when the system reaches the steady state.
We define a set of K entities in a multi-layered network
as most vulnerable, if failure of these K entities triggers the
failure of the largest number of other entities. The goal of
the K most vulnerable nodes problem is to identify this set of
nodes. This is equivalent to identifying A0d â A, Bd0 â B, that
maximizes |Apd âªBdp |, subject to the constraint that |A0d âªBd0 | â¤
K.
The dependency relations (live equations) can be formed
either after careful analysis of the multilayer network along the
lines carried out in [8], or after consultation with the engineers
of the local utility and internet service providers.
III. C OMPUTATIONAL C OMPLEXITY AND A LGORITHMS
Based on the number and the size of the min-terms in the
dependency relations, we divide them into four different cases
as shown in Table III. The algorithms for finding the K most
vulnerable nodes in the multilayer networks and computation
complexity for each of the cases are discussed in the following
four subsections.
Case
Case I
Case II
Case III
Case IV

No. of Min-terms
1
1
Arbitrary
Arbitrary

Size of Min-terms
1
Arbitrary
1
Arbitrary

TABLE III: Equation Types for Dependency Relations

3

A. Case I: Problem Instance with One Min-term of Size One
In this case, a live equation in general will have the following form: xi â yj where xi and yj are elements of the set A
(B) and B (A) respectively. In the example ai â bj , xi = ai ,
y1 = bj . It may be noted that a conjunctive implication of
the form ai â bj bk can also be written as two separate
implications ai â bj and ai â bk . However, such cases are
considered in Case II and is excluded from consideration in
Case I. The exclusion of such implications implies that the
entities that appear on the LHS of an implication in Case I
are unique. This property enables us to develop a polynomial
time algorithm for the solution of the K most vulnerable node
problem for this case. We present the algorithm next.
Algorithm 1
Input: (i) A set S of implications of the form of y â x,
where x, y â A âª B, (ii) An integer K.
Output: A set V 0 where |V 0 | = K and V 0 â A âª B such
that failure of entities in V 0 at time step t0 results in failure
of the largest number of entities in A âª B when the steady
state is reached.
Step 1. We construct a directed graph G = (V, E), where
V = A âª B. For each implication y â x in S, where x, y â
A âª B, we introduce a directed edge (x, y) â E.
Step 2. For each node xi â V , we construct a transitive
closure set Cxi as follows: If there is a path from xi to some
node yi â V in G, then we include yi in Cxi . It may be
recalled that |A| + |B| = n + m. So, we get n + m transitive
closure sets Cxi , 1 â¤ i â¤ (n + m). We call each xi to be the
seed entity for the transitive closure set Cxi .
Step 3. We remove all the transitive closure sets which are
proper subsets of some other transitive closure set.
Step 4. Sort the remaining transitive closure sets Cxi ,
where the rank of the closure sets is determined by the
cardinality of the sets. The sets with a larger number of entities
are ranked higher than the sets with a fewer number of entities.
Step 5. Construct the set V 0 by selecting the seed entities
of the top K transitive closure sets. If the number of remaining
transitive closure sets is less than K (say, K0 ), arbitrarily select
the remaining entities.
Time complexity of Algorithm 1: Step 1 takes O(n + m + |S|)
time. Step 2 can be executed in O((n+m)3 ) time. Step 3 takes
at most O((n + m)2 ) time. Step 4 sorts at most |S| entries, a
standard sorting algorithm takes O(|S| log |S|) time. Selecting
K entities in step 5 takes O(K) time. Since |S| â¤ n+m, hence
the overall time complexity is O((n + m)3 )
Theorem 1. For each pair of transitive closure sets Cxi and
Cxj produced in step 2 of algorithm 1, either Cxi â© Cxj = â
or Cxi â© Cxj = Cxi or Cxi â© Cxj = Cxj , where xi 6= xj .
Proof: Consider, if possible, that there is a pair of transitive
closure sets Cxi and Cxj produced in step 2 of algorithm 1,
such that Cxi â©Cxj 6= â and Cxi â©Cxj 6= Cxi and Cxi â©Cxj 6=

Cxj . Let xk â Cxi â© Cxj . This implies that there is a path
from xi to xk (path1 ) as well as there is a path from xj to xk ,
(path2 ). Since, xi 6= xj and Cxi â©Cxj 6= Cxi and Cxi â©Cxj =
Cxj , there is some xl in the path1 such that xl also belongs to
path2 . W.l.o.g, let us consider that xl be the first node in path1
such that xl also belongs to path2 . This implies that xl has
in-degree greater than 1. This in turn implies that there are two
implications in the set of implications S such that xl appears in
the L.H.S of both. This is a contradiction because this violates
a characteristic of the implications in Case I. Hence, our initial
assumption was wrong and the theorem is proven.
Theorem 2. Algorithm 1 gives an optimal solution for the
problem of selecting K most vulnerable entities in a multilayer network for case I dependencies.
Proof: Consider that the set V 0 returned by the algorithm is
not optimal and the optimal solution is VOP T . Let us consider
there is a entity xi â A âª B such that xi â VOP T \ V 0 .
Evidently, (i) Cxi was either deleted in step 3 or (ii) |Cxi | is
less than the cardinalities of all the transitive closure sets with
seed entities xj â V 0 , because our algorithm did not select
xi . Hence, in both cases, replacing any entity xj â V 0 by xi
reduces the total number of entities killed. Thus, the number
of dead entities by the failure of entities in VOP T is lesser than
that caused by the failure of the entities in V 0 , contradicting
the optimality of VOP T . Hence, the algorithm does in fact
return the optimal solution.
B. Case II: Problem Instance with One Min-term of Arbitrary
Size
In this case, a liveQ equation in general will have the
q
following form: xi â k=1 yj where xi and yj are elements
of the set A (B) and B (A) respectively, q represents the size
of min-term. In the example ai â bj bk bl , q = 3, xi = ai ,
y1 = bj , y2 = bk , y3 = bk .
1) Computational Complexity: We show that computation
of K most vulnerable nodes (K-MVN) in a multilayer network
is NP-complete in Case II. We formally state the problem next.
Instance: Given a set of dependency relations between
A
Qqand B type entities in the form of live equations xi â
k=1 yj , integers K and L.
Question: Is there a subset of A and B type entities of
size at most K whose âdeathâ (failure) at time t0 , triggers a
cascade of failures resulting in failures of at least L entities,
when the steady state is reached?
Theorem 3. The K-MVN problem is NP-complete.
Proof: We prove that the K-MVN problem is NP-complete
by giving a transformation for the vertex cover (VC) problem.
An instance of the vertex cover problem is specified by an
undirected graph G = (V, E) and an integer R. We want to
know if there is a subset of nodes S â V of size at most
R, so that every edge has at least one end point in S. From
an instance of the VC problem, we create an instance of the

4

K-MVN problem in the following way. First, from the graph
G = (V, E), we create a directed graph G0 = (V, E 0 ) by
replacing each edge e â E by two oppositely directed edges
e1 and e2 in E 0 (the end vertices of e1 and e2 are same as
the end vertices of e). Corresponding to a node vi in G0 that
has incoming edges from other nodes (say) vj , vk and vl , we
create a dependency relation (live equation) vi â vj vk vl . We
set K = R and L = |V |. The corresponding death equation is
of the form vÂ¯i â vÂ¯j + vÂ¯k + vÂ¯l (obtained by taking negation
of the live equation). We set K = R and L = |V |. It can now
easily be verified that if the graph G = (V, E) has a vertex
cover of size R iff in the created instance of K-MVN problem
death (failure) of at most K entities at time t0 , will trigger a
cascade of failures resulting in failures of at least L entities,
when the steady state is reached.
2) Optimal Solution with Integer Linear Programming:
In this case, we can find and optimal solution to the KMVN problem using Integer Linear Programming (ILP). We
associate binary indicator variables xi (yi ) to capture the state
of the entities ai (bi ). xi (yi ) is 1 when ai (bi ) is dead and
0 otherwise. Since we want find the set of K entities whose
failure at time step t0 triggers cascading failure resulting in the
failure of the largest number of entities, the
the
Pnobjective
Pof
m
ILP can be written as follows maximize
x
+
i
i=1
i=1 yi
It may be noted that the variables in the objective function
do not have any notion of time. However, cascading failure
takes place in time steps, ai triggers failure of bj at time
step t1 , which in turn triggers failure of ak in time step t2
and so on. Accordingly, in order to capture the cascading
failure process, we need to introduce the notion of time into
the variables of the ILP. If the numbers of A and B type
entities are n and m respectively, the steady state must be
reached by time step n + m â 1 (cascading process starts at
time step 0, t0 ). Accordingly, we introduce n + m versions
of the variables xi and yi , i.e., xi [0], . . . , xi [n + m â 1] and
yi [0], . . . , yi [n+mâ1]. To indicate the state of entities ai and
bi at times t0 , . . . , tn+mâ1 . The objective of the ILP is now
changed to
maximize

n
X
i=1

xi [n + m â 1] +

m
X

yi [n + m â 1]

i=1

Subject to the constraint that no more than K entities can
fail at time t0 .
Pn
Pm
Constraint 1:
i=1 yi [0] â¤ K In order
i=1 xi [0] +
to ensure that the cascading failure process conforms to
the dependency relations between type A and B entities,
additional constraints must be imposed.
Constraint 2: If an entity fails at time fails at time step p,
(i.e., tp ) it should continue to be in the failed state at all time
steps t > p. That is xi (t) â¥ xi (t â 1), ât, 1 â¤ t â¤ n + m â 1.
Same constraint applies to yi (t).
Constraint 3: The dependency relation (death equation)
aÂ¯i â bÂ¯j +bÂ¯k +bÂ¯l can be translated into a linear constraint in the
following way xi (t) â¤ yj (tâ1)+yk (tâ1)+yl (tâ1), ât, 1 â¤
t â¤ n + m â 1.

The optimal solution to K-MVN problem for Case II can be
found by solving the above ILP.
C. Case III: Problem Instance with an Arbitrary Number of
Min-terms of Size One
A live equation
Pq in this special case will have the following
form: xi â j=1 yj where xi and yj are elements of the set
A (B) and B (A) respectively, q represents the number of minterms in the live equation. In the example ai â bj + bk + bl ,
q = 3, xi = ai , y1 = bj , y2 = bk , y3 = bl .
1) Computational Complexity: We show that a special
case of the problem instances with an arbitrary number
of min-terms of size one is same as the Subset Cover
problem (defined below), which is proven to be NPcomplete. We define Implication Set(A) Pto be the
Ti
set of all implications of the form ai â
j=1 bj and
ImplicationPSet(B) to be the set of all implications of the
Ti
form bi â
j=1 aj . Now consider a subset of the set of
problem instances with an arbitrary number of min-terms
of size one where either Implication Set(A) = â
or Implication Set(B)
=
â. Let A0
=
{ai |ai is the element on the LHS of an implication}
in the Implication Set(A). The set B 0 is defined
accordingly. If Implication Set(B) = â then B 0 = â. In
this case, failure of any ai , 1 â¤ i â¤ n type entities will not
cause failure of any bj , 1 â¤ j â¤ m type entities. Since an
adversary can cause failure of only K entities, the adversary
would like to choose only those K entities that will cause
failure of the largest number of entities. In this scenario, there
is no reason for the adversary to attack any ai , 1 â¤ i â¤ n type
entities as they will not cause failure of any bj , 1 â¤ j â¤ m
type entities. On the other hand, if the adversary attacks
K bj type entities, not only those K bj type entities will
be destroyed, some ai type entities will also be destroyed
due to the implications in the Implication Set(A). As
such the goal of the adversary will be to carefully choose
K bj , 1 â¤ j â¤ m type entities that will destroy the largest
number of ai type entities. In its abstract form, the problem
can be viewed as the Subset Cover problem.
Subset Cover Problem
Instance: A set S = {s1 , . . . , sm }, a set S of m subsets of S,
i.e., S = {S1 , . . . , Sr }, where Si â S, âi, 1 â¤ i â¤ r, integers
p and q.
Question: Is there a p element subset S 0 of S (p < n) that
completely covers at least q elements of the set S? (A set S 0 is
said to be completely covering an element Si , âi, 1 â¤ i â¤ m
of the set S, if S 0 â© Si = Si , âi, 1 â¤ i â¤ m.)
The set S in the subset cover problem corresponds to the
set B = {b1 , . . . , bm }, and each set Si , 1 â¤ i â¤ r corresponds
to an implication in the ImplicationS et(A) and comprises of
the bj âs that appear on the RHS of the implication. The goal
of the problem is to select a subset B 00 of B that maximizes
the number of Si âs completely covered by B 00 .

5

Theorem 4. The Subset Cover problem is NP-complete.
Proof: We prove that the Subset Cover problem is NPcomplete by giving a transformation from the well known
Clique problem. It may be recalled that an instance of the
Clique problem is specified by a graph G = (V, E) and an
integer K. The decision question is whether or not a clique of
size at least K exists in the graph G = (V, E). We show that
a clique of size K exists in graph G = (V, E) iff the Subset
Cover problem instance has a p element subset S 0 of S that
completely covers at least q elements of the set S.
From an instance of the Clique problem, we create an
instance of the Subset Cover problem in the following way.
Corresponding to every vertex vi , 1 â¤ i â¤ n of the graph
G = (V, E) (V = {v1 , . . . , vn }), we create an element
in the set S = {s1 , . . . , sn }. Corresponding to every edge
ei , 1 â¤ i â¤ m, we create m subsets of S, i.e., S =
{S1 , . . . , Sm }, where Si corresponds to a two element subset
of nodes, corresponding to the end vertices of the edge ei . We
set the parameters p = K and q = K(K â 1)/2. Next we
show that in the instance of the subset cover problem created
by the above construction process, a p element subset S 0 of
S exists that completely covers at least q elements of the set
S, iff the graph G = (V, E) has a clique of size at least K.
Suppose that the graph G = (V, E) has a clique of size
K. It is clear that in the created instance of the subset cover
problem, we will have K(K â 1)/2 elements in the set S,
that will be completely covered by a K element subset of
the set S. The K element subset of S corresponds to the set
of K nodes that make up the clique in G = (V, E) and the
K(K â 1)/2 elements in the set S corresponds to the edges
of the graph G = (V, E) that corresponds to the edges of
the clique. Conversely, suppose that the instance of the Subset
Cover problem has K element subset of S that completely
covers K(K â 1)/2 elements of the set S. Since the elements
of S corresponds to the edges in G, in order to completely
cover K(K â 1)/2 edges, at least K nodes (elements of the
set S) will be necessary. As such, this set of K nodes will
constitute a clique in the graph G = (V, E).
2) Optimal Solution with Integer Linear
Programming: If
Pq
the live equation is in the form xi â k=1 yj then the âdeath
equationâ (obtained by taking negation
of the live equation)
Qq
will be in the product form xÌi â j=1 yÌj . If the live equation
is given as ai â bj + bk , then the death equation will be given
as aÂ¯i â bÂ¯j bÂ¯k .
By associating binary indicator variables xi and yi to
capture the state of the entities ai and bi , we can follow almost
identical procedure as in Case II, with only one exception.
It may be recalled that in Case II, the death equations such
as aÂ¯i â bÂ¯j + bÂ¯k was translated into a linear constraint
xi (t) â¤ yj (t â 1) + yk (t â 1), ât, 1 â¤ t â¤ n + m â 1. However
a similar translation in Case III, with death equations such as
aÂ¯i â bÂ¯j bÂ¯k , will result in a non-linear constraint of the form
xi (t) â¤ yj (t â 1)yk (t â 1), ât, 1 â¤ t â¤ n + m â 1. Fortunately,
a non-linear constraint of this form can be replaced a linear
constraint such as 2xi (t) â¤ yj (t â 1) + yk (t â 1), ât, 1 â¤

t â¤ n + m â 1. After this transformation, we can compute the
optimal solution using integer linear programming.
D. Case IV: Problem Instance with an Arbitrary Number of
Min-terms of Arbitrary Size
1) Computational Complexity: Since both Case II and Case
III are special cases of Case IV, the computational complexity
of finding the K most vulnerable nodes in the multilayer
network in NP-complete in Case IV also.
2) Optimal Solution with Integer Linear Programming:
The optimal solution to this version of the problem can be
computed by combining the techniques developed for the
solution of the versions of the problems considered in Cases
II and III.
IV. E XPERIMENTAL RESULTS
We applied our model to study multilayer vulnerability
issues in Maricopa County, the most densely populated county
of Arizona with approximately 60% of Arizonas population
residing in it. Specifically, we wanted to find out if some
regions of Maricopa County were more vulnerable to failure
than some other regions. The data for our multi-layered
network were obtained from different sources. We obtained
the data for the power network (network A) from Platts
(http://www.platts.com/). Our power network dataset consists
of 70 power plants and 470 transmission lines. Our communication network (network B) data were obtained from GeoTel
(http://www.geo-tel.com/). Our communication network data
consists of 2, 690 cell towers and 7, 100 fiber-lit buildings as
well as 42, 723 fiber links. Snapshots of our power network
data and communication network data are shown in figure 2. In
the power network snapshot of sub-figure(a), the orange markers show locations of powerplants while the yellow continuous
lines represent the transmission lines. In the communication
network snapshot of sub-figure (b) the pink markers show the
location of fiber-lit buildings, the orange markers show the
location of cell towers and the green continuous lines represent
the fiber links. In our dataset, âloadâ in the Power Network is
divided into Cell towers and Fiber-lit buildings. Although there
exists various other physical entities which also draw electric
power and hence can be viewed as load to the power network,
as they are not relevant to our study on interdependency
between power and communication networks, we ignore such
entities. Thus in network A, we have the three types of Power
Network Entities (PNEâs) - Generators, Load (consisting of
Cell towers and Fiber-lit buildings) and Transmission lines
(denoted by a1 , a2 , a3 respectively). For the Communication
Network, we have the following Communication Network
Entities (CNEâs) - Cell Towers, Fiber-lit buildings and Fiber
links (denoted by b1 , b2 , b3 respectively). We consider the
Fiber-lit buildings as a communication network entities as they
house routers which definitely are communication network
entities. From the raw data we construct Implication Set(A)
and Implication Set(B), by following the rules stated below:
Rules: We consider that a PNE is dependent on a set of
CNEs for being in the active state (âaliveâ) or being in the

6

(a) Snapshot of Power Network in Maricopa County

(b) Snapshot of Communication Network in Maricopa County

Fig. 2: Snapshots of power network and communication network in Maricopa County)

inactive state (âdeadâ). Similarly, a CNE is dependent on a set
of PNEs for being active or inactive state. For simplicity we
consider the live equations with at most two minterms. For
the same reason we consider the size of each minterm is at
most two.

of the number of entities of the two networks A and B. Most
importantly, we find that the degree of vulnerability of all
the five regions considered in our study are close and no one
region stands out as being extremely vulnerable.

Generators (a1,i , 1 â¤ i â¤ p, where p is the total number
of generators): We consider that each generator (a1.i ) is
dependent on the nearest Cell Tower (b1,j ) or the nearest
Fiber-lit building (b2,k ) and the corresponding Fiber link (b3,l )
connecting b2,k and a1,i . Thus, we have
a1,i â b1,j + b2,k Ã b3,l
Load (a2,i , 1 â¤ i â¤ q, where q is the total number of loads):
We consider that the loads in the power network do not depend
on any CNE.
Transmission Lines (a3,i , 1 â¤ i â¤ r, where r is the total number of transmission lines): We consider that the transmission
lines do not depend on any CNE.
Cell Towers (b1,i , 1 â¤ i â¤ s, where s is the total number
of cell towers): We consider the cell towers depend on the
nearest pair of generators and the corresponding transmission
line connecting the generator to the cell tower. Thus, we have
b1,i â a1,j Ã a3,k + a1,j 0 Ã a3,k0
Fiber-lit Buildings (b2,i , 1 â¤ i â¤ t, where t is the total number
of fiber-lit buildings): We consider that the fiber-lit buildings
depend on the nearest pair of generators and the corresponding
transmission lines connecting the generators to the cell tower.
Thus, we have b2,i â a1,j Ã a3,k + a1,j 0 Ã a3,k0
Fiber Links (b3,i , 1 â¤ i â¤ u, where u is the total number of
fiber links)): We consider that the fiber links do not depend
on any PNE.
Because of experimental resource limitation, we have considered 5 regions of Maricopa County for our experiments.
We used IBM CPLEX Optimizer 12.5 to run the formulated
ILPâs on the experimental dataset. We show our results in
the figure 3. We observe that in each of the regions there
is a specific budget threshold beyond which each additional
increment in budget results in the death of only one entity. The
reason for this behavior is our assumption that entities such
as the transmission lines and the fiberlinks are not dependent
on any other entities. We notice that all the entities of the
two networks can be destroyed with a budget of about 60%

Fig. 3: Experimental results of failure vulnerability across five regions
of Maricopa county

R EFERENCES
[1] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
âCatastrophic cascade of failures in interdependent networks,â Nature,
vol. 464, no. 7291, pp. 1025â1028, 2010.
[2] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, âNetworks formed
from interdependent networks,â Nature Physics, vol. 8, no. 1, pp. 40â48,
2011.
[3] J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, âCascade of
failures in coupled network systems with multiple support-dependence
relations,â Physical Review E, vol. 83, no. 3, p. 036116, 2011.
[4] V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and
R. Setola, âModelling interdependent infrastructures using interacting
dynamical models,â International Journal of Critical Infrastructures,
vol. 4, no. 1, pp. 63â79, 2008.
[5] P. Zhang, S. Peeta, and T. Friesz, âDynamic game theoretic model of
multi-layer infrastructure networks,â Networks and Spatial Economics,
vol. 5, no. 2, pp. 147â178, 2005.
[6] M. Parandehgheibi and E. Modiano, âRobustness of interdependent
networks: The case of communication networks and the power grid,â
arXiv preprint arXiv:1304.0356, 2013.
[7] D. T. Nguyen, Y. Shen, and M. T. Thai, âDetecting critical nodes in
interdependent power networks for vulnerability assessment,â 2013.
[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman,
âPower grid vulnerability to geographically correlated failures-analysis
and control implications,â arXiv preprint arXiv:1206.1099, 2012.
[9] J.-F. Castet and J. H. Saleh, âInterdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,â PloS one, vol. 8, no. 4, p. e60402, 2013.
[10] A. Fudenberg and J. Tirole, Game Theory. Ane Books, 2010.

On the Entity Hardening Problem in Multi-layered
Interdependent Networks
Joydeep Banerjee, Arun Das, Chenyang Zhou, Anisha Mazumder and Arunabha Sen

arXiv:1412.6686v1 [cs.NI] 20 Dec 2014

Computer Science and Engineering Program
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {joydeep.banerjee, arun.das, czhou24, anisha.mazumder, asen}@asu.edu
AbstractâThe power grid and the communication network
are highly interdependent on each other for their well being.
In recent times the research community has shown significant
interest in modeling such interdependent networks and studying
the impact of failures on these networks. Although a number
of models have been proposed, many of them are simplistic in
nature and fail to capture the complex interdependencies that
exist between the entities of these networks. To overcome the
limitations, recently an Implicative Interdependency Model that
utilizes Boolean Logic, was proposed and a number of problems
were studied. In this paper we study the âentity hardeningâ
problem, where by âentity hardeningâ we imply the ability of the
network operator to ensure that an adversary (be it Nature or
human) cannot take a network entity from operative to inoperative
state. Given that the network operator with a limited budget
can only harden k entities, the goal of the entity hardening
problem is to identify the set of k entities whose hardening will
ensure maximum benefit for the operator, i.e. maximally reduce
the ability of the adversary to degrade the network. We show
that the problem is solvable in polynomial time for some cases,
whereas for others it is NP-complete. We provide the optimal
solution using ILP, and propose a heuristic approach to solve the
problem. We evaluate the efficacy of our heuristic using power
and communication network data of Maricopa County, Arizona.
The experiments show that our heuristic almost always produces
near optimal results.

I.

I NTRODUCTION

The critical infrastructures of a nation form a complex symbiotic ecosystem where individual infrastructures are heavily
interdependent on each other for being fully functional. Two
such critical systems that rely heavily on each other for their
well being are the power and communication network infrastructures. For instance, power grid entities such as SCADA
systems, that are used to remotely operate power generation
units, receive their control commands over the communication
network infrastructure, while communication network entities
such as routers and base stations are inoperable without electric
power. Thus, failure introduced in the system either by Nature
(hurricanes), or man (terrorist attacks), can trigger further
failures in the system due to interdependencies between the
entities of the two infrastructures.
Although a number of models have been proposed for modeling and analysis of interdependent multi-layered networks
[1], [2], [3], [4], [5], [6], [7], [8], many of these models are simplistic in nature and fail to capture the complex interdependencies that exists between the entities of these networks. As noted
in [9], these models fail to model complex interdependencies

that may exist between network entities, such as when entity ai
is operational, if entities (i) bj and bk and bl are operational, or
(ii) bm and bn are operational, or (iii) bp is operational. Graph
based interdependency models proposed in the literature such
as [3], [4], [5], [10], [6], [7] including [1], [2] cannot capture
such complex interdependency involving both conjunctive and
disjunctive terms between entities of multi-layer networks. To
overcome these limitations, an Implicative Interdependency
Model that utilizes Boolean Logic, was recently proposed in
[9], and a number of problems including computation of K
most vulnerable nodes [9], root cause of failure analysis [11],
and progressive recovery from failures [12], were studied using
this model.
In this paper we study the âentity hardeningâ problem in
the interdependent power-communication network using the
Implicative Interdependency Model (IIM). By âentity hardeningâ, we imply the ability of the network operator to ensure
that an adversary (be it Nature or human), cannot take a network entity from an operative (operational) to an inoperative
(failed) state. We assume that the adversary is clever and
is capable of identifying the most vulnerable entities in the
network that causes maximum damage to the interdependent
system. However, the adversary does not have an unlimited
budget and has the resources to destroy at most K entities
of the interdependent network. The network operator is also
aware of adversaryâs target entities for destruction. Since we
assume that once an entity is âhardenedâ by the network
operator it cannot be destroyed by the adversary, if all K
targets of the adversary are hardened by the network operator,
then the adversary cannot induce any failure in the network.
However, if due to resource limitations the network operator
is able to strengthen only k entities, where k < K, these k
entities have to be carefully chosen. The goal of the entity
hardening problem is to identify the set of k entities whose
hardening will ensure maximum benefit for the operator, i.e.
maximally reduce the ability of the adversary to degrade the
network.
We classify the entity hardening problem into four different
cases depending on the nature of the interdependency relationships. We show that the first case can be solved in polynomial
time, and all other cases are shown to be NP-complete. We
provide an inapproximability result for the second case, an
approximation algorithm for the third case, and a heuristic
for the fourth (general) case. We evaluate the efficacy of our
heuristic using power and communication network data of
Maricopa County, Arizona. The experiments show that our

2

heuristic almost always produces near optimal results.
The paper is organized as follows, the IIM model is
presented in Section II, in Sections III and IV we formally
state the entity hardening problem and analyze its computational complexity, Section V outlines the optimal and heuristic
solutions to the problem, Section VI shows the experimental
results, and finally Section VII concludes this paper.
II.

I NTERDEPENDENCY M ODEL

We now present an overview of the underlying IIM interdependency model [9]. IIM uses Boolean Logic to model
the interdependencies between network entities, these interdependent relationships are termed as Implicative Interdependency Relations (IDRs). We represent this interdependent network setting as I(A, B, F (A, B)), where sets A
and B are the power and communication network entities
respectively, and F (A, B) is the set of dependency relations,
or IDRs. Table I represents a sample interdependent network I(A, B, F (A, B)), where A = {a1 , a2 , a3 , a4 }, B =
{b1 , b2 , b3 } and F (A, B) is the set of IDRs (dependency
relations) between the entities of A and B. In this example,
the IDR b1 â a1 a3 + a2 implies that entity b1 is operational
when both the entities a1 and a3 are operational, or entity a2
is operational. The conjunction of entities, such as a1 a3 , is
also referred to as a minterm.
Power Network
a1 â b1 b2
a2 â b1 + b2
a3 â b1 + b2 + b3
a4 â b1 + b3

Comm. Network
b1 â a1 a3 + a2
b2 â a1 a2 a3
b3 â a1 + a2 + a3
ââ

TABLE I: Implicative Interdependency Relations of a sample network

Given a set of inoperable (failed) entities, a time stepped
failure cascade can be derived from the dependency relationships outlined in the IDR set. For example, for the interdependent network outlined in Table I, Table II shows the failure
propagation when entities {a2 , b3 } fail at the initial time step
(t = 0). It may be noted that the model assumes that dependent
entities fail immediately in the next time step, for example,
when {a2 , b3 } fail at t = 0, b2 fails at t = 1 as b2 is dependent
on a2 for its survival. The system reaches a steady state when
the failure propagation process stops. In this example, when
{a2 , b3 } fail at t = 0, the steady state is reached at time step
t = 4.
Entities
a1
a2
a3
a4
b1
b2
b3

0

1

0
1
0
0
0
0
1

0
1
0
0
0
1
1

Time Steps (t)
2
3
4
1
1
0
0
0
1
1

1
1
0
0
1
1
1

1
1
1
1
1
1
1

5

6

1
1
1
1
1
1
1

1
1
1
1
1
1
1

TABLE II: Failure cascade propagation when entities {a2 , b3 } fail at
time step t = 0. A value of 1 denotes entity failure, and 0 otherwise

A primary consideration for using this model is the accurate
formulation of the IDRs that is representative of the underlying
physical power and communication network infrastructures.
This can either be done by careful analysis as done in [8], or
by consultation with experts of these infrastructures. We utilize
IIM to model the interdependency between the two networks
and analyze the entity hardening problem in this setting.

III.

P ROBLEM F ORMULATION

Before we make a formal statement of the entity hardening
problem in the IIM setting, we explain it with the help of an
example. Consider an interdependent system as outlined in the
IDR set shown in Table I. It may be easily checked that when
the adversary budget is K= 2, the most vulnerable entities
of this system are {a2 , b3 }. If the network operator doesnât
harden any one of the entities a2 or b3 , then in this example
all the network entities eventually fail, as seen from the fault
propagation in Table II. When the network operator chooses
to harden both a2 and b3 then none of the entities in the
network fail if the adversary restricts the attack only to the two
most vulnerable entities of the network, which in this example
happens to be {a2 , b3 }. If the network operator has resources
to harden only one entity and the operator chooses to harden
a2 , the destruction of b3 by the adversary will eventually lead
to the failure of no other entities of the network, as shown in
Table III(a). If on the other hand, the network operator chooses
to harden b3 , destruction by the adversary of a2 will eventually
lead to the failure of the entities {a2 , b2 , a1 , b1 } as shown in
Table III(b). Clearly in this scenario the operator should harden
a2 instead of b3 .
Definition: Kill Set of a set of Entities(S): The kill set of a
set of entities S, is the set of all entities that will eventually
fail due to failure of S and the interdependencies between the
entities of the network as given by the set of IDRâs. The kill
set of a set of entities S is denoted by KillSet(S).
It may be noted that the search for k entities to be hardened
is restricted to the KillSet(S), where S is the set of K
most vulnerable entities in the network, because hardening any
entity not in KillSet(S) does not provide any benefit to the
network operator. In this study we also assume that the set of
K most vulnerable entities in the network is unique.
Entities
0
a1
a2
a3
a4
b1
b2
b3

0
â
0
0
0
0
1

Time Steps (t)
1
2
3
0
â
0
0
0
0
1

0
â
0
0
0
0
1

0
â
0
0
0
0
1

(a) Entity a2 is hardened

Entities
4
0
â
0
0
0
0
0

0
a1
a2
a3
a4
b1
b2
b3

0
1
0
0
0
0
â

Time Steps (t)
1
2
3
0
1
0
0
0
1
â

1
1
0
0
0
1
â

1
1
0
0
1
1
â

4
1
1
0
0
1
1
â

(b) Entity b3 is hardened

TABLE III: Failure cascade propagation with entity hardening. Entities {a2 , b3 } are attacked at time step t = 0. A value of 1 denotes
entity failure, 0 otherwise. â denotes a hardened entity.

We now proceed to formulate the entity hardening
problem formally. Given an interdependent network system
I(A, B, F (A, B)), and the set of K most vulnerable entities
of the system Aâ² âª B â² , where Aâ² â A and B â² â B:
The Entity Hardening (ENH) problem
INSTANCE: Given:
(i) An interdependent network system I(A, B, F (A, B)),
where the sets A and B represent the entities of the two
networks, and F (A, B) is the set of IDRs.
(ii) The set of K most vulnerable entities of the system
Aâ² âª B â² , where Aâ² â A and B â² â B
(iii) Two positive integers k, k < K and EF .

3

QUESTION:Is there a set of entities H = Aâ²â² âª B â²â² , Aâ²â² â
A, B â²â² â B, |H| â¤ k, such that hardening H entities results
in no more than EF entities to fail after entities Aâ² âª B â² fail
at time step t = 0.
We note some of the assumptions for the ENH problem:
First, we assume that once an entity is hardened, it is always
operational and does not fail at any time step of the observation, even when the entity is part of the K most vulnerable
entities. Second, we assume that k < K, as otherwise the
selection of K entities for hardening ensures that no entities
fail at all. Finally, as noted earlier, we assume that the set of
K most vulnerable entities in the network is unique. We now
proceed to analyze the computational complexity of the ENH
problem.
IV.

C OMPUTATIONAL C OMPLEXITY A NALYSIS

For an interdependent network I(A, B, F (A, B)) the IDRs
can be represented in four different forms. We analyze the
computational complexity of the ENH problem for each of
these cases separately.
A. Case I: Problem Instance with One Minterm of Size One
The IDRs of Case I have a single minterm of size 1. This
can be represented as xi â yj , where xi and yj are entities of
network A(B) and B(A) respectively. We show that the ENH
problem for Case I can be solved optimally in polynomial time.
Algorithm 1: Entity Hardening Algorithm for systems
with Case I type interdependencies

1
2
3
4
5
6
7

8
9
10

Data: An interdependent network I(A, B, F(A, B)), set of
K most vulnerable entities Aâ² âª B â² , Aâ² â A, B â² â B,
hardening budget k and a set H = â.
Result: Set of hardened entities H.
begin
For each entity xi â (Aâ² âª B â² ) compute the set of kill sets
C = {Cx1 , Cx2 , ..., CxK }, where Cxi = KillSet(xi ) ;
Create a copy D = {Dx1 , Dx2 , ..., DxK } of set C ;
for (i=1; i â¤ K; i++) do
for (j=1, j 6= i; j â¤ K; j++) do
if Cxj â Cxi then
Dxi â Dxi \ Dxj ;
Choose the top k sets from D with highest cardinality ;
For each of the Dxi â D sets chosen in Step 8,
H â H âª xi ;
return H

Theorem 1. Algorithm 1 solves the Entity Hardening problem
for Case I optimally in polynomial time.
Proof: It is shown in [9] that the kill set for all entities in
the interdependent network can be computed in O(n3 ) where
n = |A| + |B|, thus computing the kill sets for K entities takes
O(Kn2 ). Step 4-7 of the algorithm runs in O(K2 ). Choosing
the k highest cardinality sets can be found using any standard
sorting algorithm in O(Klog(K)). Hence Algorithm 1 runs in
O(Kn2 ).
For two kill sets Cxi and Cxj it can be shown that either
Cxi â© Cxj = â or Cxi â© Cxj = Cxi or Cxi â© Cxj = Cxj

[9]. So with two entities {xi , xj } â Aâ² âª B â² and Cxi â© Cxj =
Cxj i.e, Cxj â Cxi , if xi is hardened it prevents the failure
of Cxi â Cxj entities (provided that none of the entities in
Cxi â Cxj â {xi } are in Aâ² âª B â² ). With this assertion, for
an entity xi â Aâ² â© B â² , steps 4-7 of Algorithm 1 finds the
actual entities for which failure is prevented by hardening xi .
The set D = {Dx1 , Dx2 , ..., DxK } comprises of these set of
entities for each hardened entity xi .
To prove that Algorithm 1 finds the optimal solution we
make the following two assertions: First, consider any two sets
Dxi and Dxj . It is implied from step 6 of Algorithm 1 that
/ Aâ² âª B â² is
Dxi â© Dxj = â. Second, consider an entity xp â
hardened. If xp fails when entities in Aâ² âªB â² fails initially then
it would belong to some set Dxi . Thus hardening xp results
in preventing the failure of entities that is a proper subset of
Dxi . Hence the entities to be hardened must belong to Aâ² âª B â²
only. Owing to the two assertions it directly follows that with
a given budget k, hardening k highest cardinality sets from the
set D ensures prevention of failure for the maximum number
of entities.
B. Case II: Problem Instance with One Minterm of Arbitrary
Size
The IDRs of Case II have a single
Qpminterm of arbitrary
size. This can be represented as xi â j=1 yj , where xi and
yj are entities of network A(B) and B(A) respectively and the
size of the minterm is p. The Entity Hardening problem with
respect to Case II is NP-complete and is proved in Theorem
2. An inapproximability proof for this case of the problem is
given in Theorem 3
Theorem 2. The Entity Hardening problem for Case II is NP
Complete
Proof: The Entity Hardening problem for case II is proved
to be NP complete by giving a reduction from the Densest pSubhypergraph problem [13], a known NP-complete problem.
An instance of the Densest p-Subhypergraph problem includes
a hypergraph G = (V, E), a parameter p and a parameter M .
The problem asks the question whether there exists a set of
vertices |V â² | â V and |V â² | â¤ p such that the subgraph induced
with this set of vertices has at least M hyperedges. From an
instance of the Densest p-Subhypergraph problem we create
an instance of the ENH problem in the following way. For
each vertex vi and each hyperedge ej an entity bi and aj are
added to the set B and A respectively. For each hyperedge ej
with ej = {vm , vn , vq } (say) an IDR of form aj â bm bn bq is
created. It is assumed that the value of K is set of |V |. The
values of k and EF are set to p and |V | + |E| â p â M (where
|A| = |V | and |B| = |E|) respectively.
In the constructed instance only entities of set A are
dependent on entities of set B. Additionally the dependency
for an entity ai consists of conjunction of entities in set B.
Hence for an entity ai â A to fail, either it itself has to fail
initially or all entities to which ai is dependent on has to fail.
It is to be noted that the entities in set B has no induced failure
i.e., there is no cascade. Following from this assertion, with
K = p, the solution Aâ² = â and B â² = B would fail all entities
in set A âª B. Moreover this is the single unique solution to
the problem instance. This is because by including one entity

4

ai in the initial failure set would result in not failing at least
one entity bj for a given budget K = p. Hence it wonât fail
the entire set of entities in A âª B.
If an entity in set A is hardened then it would have no effect
in failure prevention of any other entities. Whereas hardening
an entity bm â B might result in failure prevention of an entity
ai â A with IDR aj â bm bn bq provided that entities bn , bq
are also defended. With k = p (and K â¤ |V | = |B|) it can be
ensured that entities to be defended are from set B â² .
To prove the theorem consider that there is a solution to the
Densest p-Subhypergraph problem. Then there exist p vertices
which induces a subgraph which has at least M hyperedges.
Hardening the entities bi â B â² for each vertex vi in the solution
of the Densest p-Subhypergraph problem would then ensure
that at least M entities in set A are protected from failure.
This is because the entities in set A for which the failure
is prevented corresponds to the hyperedges in the induced
subgraph. Thus the number of entities that fail after hardening
p entities is at most |V | + |E| â p â M , solving the ENH
problem. Now consider that there is a solution to the ENH
problem. As previously stated, the entities to be hardened will
always be from set B â² . So defending p entities from set B â²
would result in failure prevention of at least M entities in set
A such that EF â¤ |V | + |E| â p â M . Hence, the vertex
induced subgraph would have at least M hyperedges when
vertices corresponding to the entities hardened are included
in the solution of the Densest p-Subhypergraph problem, thus
solving it.
Theorem 3. For an interdependent network I(A, B, F (A, B))
with n = |A âª B| and F (A, B) having IDRs of form Case II,
it is hard to approximate the ENH problem within a factor of
1
for some Î» > 0.
log(n)Î»
2

Proof: From Theorem 2, Densest p-Subhypergraph problem has been shown to be a special case of the ENH problem
with IDRs of form Case II. Densest p-Subhypergraph problem
1
is proved to be inapproximable within a factor of log(n)
Î»
2
(Î» > 0) in [13]. Hence the theorem follows.
C. Case III: Problem Instance with an Arbitrary Number of
Minterm of Size One
The IDRs of Case III have arbitrary number
P of minterm of
size 1. This can be represented as xi â pq=1 yq , where xi
and yq are entities of network A(B) and B(A) respectively
and the number of minterms are p. The ENH problem with
respect to Case III is NP-complete and is proved in Theorem
4.
Theorem 4. The ENH problem for Case III is NP Complete
Proof: The ENH problem for case III is proved to be NP
complete by giving a reduction from the Set Cover Problem,
a well known NP-complete problem. An instance of the Set
Cover problem includes a set S = {x1 , x2 , ..., xn }, a set S =
{S1 , S2 , ..., Sm } where Si â S and a positive integer M . The
problem asks the question whether there exists at most M
subsets from set S whose union would result in the set S. From
an instance of the set cover problem we create an instance of
the ENH problem in the following way. For each element xi
in set S we add an entity ai in set A. For each subset Si in

set S we add an entity bi in set B. For all subsets in S, say
Sp , Sm , Sn , which has the element xi there is an IDR of form
ai â bm + bn + bl . The values of positive integers k and EF
are set to M and m â M respectively. It is assumed that the
value of K = m.
With similar reasoning as that of Case II it can be shown
that for K = m the maximum number of node failures (i.e.
failure of all entities in A âª B) would occur if Aâ² = â and
B â² = B. This is also the single unique solution to the problem
instance.
The constructed instance also ensures that the entities to
be hardened are from set B â² (Aâ² not considered as it is equal
to â). This is because protecting an entity ai â A would only
result in prevention of its own failure whereas protecting an
entity bj â B would result in failure prevention of its own and
all other entities in set A for which it appears in its IDR.
To begin with the proof, consider that there is a solution
to the Set Cover problem. Then there exist M subsets (or
elements in set S) whose union results in the set S. Hardening
the entities in set B corresponding to the subsets selected
would ensure that all entities in set A are prevented from
failure. This is because for the dependency of each entity
ai â A there exist at least one entity (in set B) that is hardened.
Hence the number of entities that fails after hardening is mâM
which is equal to EF , thus solving the ENH problem. Now,
consider that there is a solution to the ENH problem. As
discussed above the entities to be hardened should be from
set B â² . To achieve EF = m â M with k = M , no entities
in the set A must fail. Hence for each entity ai â A at least
one entity in set B that appears in its IDR has to be hardened.
Thus, it directly follows that the union of subsets in set S
corresponding to the entities hardened is equal to the set S,
solving the Set Cover Problem.
1) Approximation Scheme for Case 3: In this subsection we
provide an approximation algorithm for Case 3 of the problem.
For an interdependent network I(A, B, F (A, B)) with the
initial failed set of entities as Aâ² âª B â² we define Protection
Set of each entity as follows.
Definition: For an entity xi â A âª B the Protection Set is
defined as the entities that would be prevented from failure
by hardening the entity xi when all entities in Aâ² âª B â² fails
initially. This is represented as P (xi |Aâ² âª B â² ).
The Protection Set of each entity can be computed in
O((n + m)2 ) where n and m are the number of entities and
number of minterms respectively in an interdependent network
I(A, B, F (A, B)) .
Theorem 5. For two entities xi , xj â A âª B, P (xi |Aâ² âª B â² ) âª
P (xj |Aâ² âª B â² ) = P (xi , xj |Aâ² âª B â² ) when IDRs are of form
Case III.
Proof: Assume that defending two entities xi and xj
would result in preventing failure of P (xi , xj |Aâ² âª B â² ) entities
with |P (xi |Aâ² âª B â² ) âª P (xj |Aâ² âª B â² )| < |P (xi , xj |Aâ² âª B â² )|.
Then there exist at least one entity xp â
/ P (xi |Aâ² âª B â² ) âª
P (xj |Aâ² âª B â² ) such that itâs failure is prevented only if xi and
xj is protected together. So two entities xm and xn (with xm â
P (xi |Aâ² âªB â² ) and xn â P (xj |Aâ² âªB â² ) or vice versa) have to be

5

present in the IDR of xp . As the IDRs are of form Case III so if
any one of xm or xn is protected then xp is protected, hence
a contradiction. On the other way round P (xi , xj |Aâ² âª B â² )
contains all entities which would be prevented from failure
if xi or xj is defended alone. So it directly follows that
|P (xi |Aâ² âª B â² ) âª P (xj |Aâ² âª B â² )| > |P (xi , xj |Aâ² âª B â² )| is
not possible. Hence the theorem holds.
Theorem 6. There exists an 1 â 1e approximation algorithm
that approximates the ENH problem for Case III.
Proof: The approximation algorithm is constructed by
modeling the problem as Maximum Coverage problem. An
instance of the maximum coverage problem consists of a
set S = {x1 , x2 , ..., xn }, a set S = {S1 , S2 , ..., Sm } where
Si â S and a positive integer M . The objective of the problem
is to find a set S â² â S and |S â² | â¤ M such that âªSi âS Si
is maximized. For a given initial failure set Aâ² âª B â² with
|Aâ² |+|B â² | â¤ K, let P (xi |Aâ² âªB â² ) denote the protection set for
each entity xi â A âª B. We construct a set S = A âª B and for
each entity xi a set Sxi â S such that Sxi = P (xi |Aâ² âª B â² ).
Each set Sxi is added as an element of a set S. The conversion
of the problem to Maximum Coverage problem can be done
in polynomial time. By Theorem 5 defending a set of entities
X â S would result in failure prevention of âªxi âX Sxi entities.
Hence, with the constructed sets S and S and a positive integer
M (with M = k) finding the Maximum Coverage would
ensure the failure protection of maximum number of entities in
A âª B. This is same as the ENH problem of Case III. As there
exists an 1 â 1e approximation algorithm for the Maximum
Coverage problem hence the theorem holds.
D. Case IV: Problem Instance with an Arbitrary Number of
Minterms of Arbitrary Size
The IDRs of Case IV have arbitrary number of minterm
of
Pp arbitrary
Qqj1 size. This can be represented as xi â
j2 =1 yj2 , where xi and yj2 are entities of network
j1 =1
A(B) and B(A) respectively and there are p minterms each
of size qj1 .
Theorem 7. The Entity Hardening problem for Case IV is NP
Complete
Proof: Case II and Case III are special cases of Case
IV. Hence following from Theorem 2 and Theorem 4 the
computational complexity of the Entity Hardening problem is
NP-complete in Case IV.
V.

S OLUTIONS TO

THE

E NTITY H ARDENING P ROBLEM

A. Optimal Solution using Integer Linear Programming
We propose an Integer Linear Program (ILP) that solves
the Entity Hardening problem optimally. Let [G, H] with
G = {g1 , g2 , ..., gn } and H = {h1 , h2 , ..., hm } denote the
entities in set A and B respectively with hi = 0 (gj = 0)
if entity ai (bj ) is alive and hi = 1 (gj = 1) otherwise.
Given an integer k let [G, H] be the solution (with value of 1
corresponding to entities failed initially) that cause maximum
number of entity failure. Two variables xid and yjd are used
in the ILP with xid = 1 (yjd = 1), when entity ai â A
(bj â B) is in a failed state at time step d, and 0 otherwise.
The number of entities to be defended is considered to be k.

It is to be noted that the maximum number cascading steps is
upper bounded by |A| + |B| â 1 = m + n â 1. The objective
function can now be formulated as follows:
min

n
m

X
X
yj(m+nâ1)
xi(m+nâ1) +

(1)

j=1

i=1

The objective in (1) minimizes the number of entities failed
after the cascading failure with the respective constraints for
the Entity Hardening problem as follows:
Constraint Set 1:

n
P

i=1

qxi +

m
P

qyj = k , with qxi , qyj â [0, 1].

j=1

If an entity xi (yj ) is defended then qxi = 1 (qyj = 1) and 0
otherwise.
Constraint Set 2: xi0 â¥ gi â qxi and yi0 â¥ hi â qyi .
This constraint implies that only if an entity is not defended
and gi (hi ) is 1 then the entity will fail at the initial time step.
Constraint Set 3: xid â¥ xi(dâ1) , âd, 1 â¤ d â¤ m + n â 1, and
yid â¥ yi(dâ1) , âd, 1 â¤ d â¤ m + n â 1, in order to ensure
that for an entity which fails in a particular time step would
remain in failed state at all subsequent time steps.
Constraint Set 4: Modeling of the constraint to capture
the cascade propagation for IIM is similar to the constraints
established in [9]. A brief presentation of this constraint is
provided here. Consider an IDR ai â bj bp bl + bm bn + bq of
type Case IV. The following steps are enumerated to depict
the cascade propagation:
Step 1: Replace all minterms of size greater than one with a
variable. In the example provided we have the transformed
minterm as ai â c1 + c2 + bq with c1 â bj bp bl and
c2 â bm bn (c1 , c2 â {0, 1}) as the new IDRs. Note that after
transformation, the original IDR is in the form of Case III and
the introduced IDRs are in the form of Case II.
Step 2: For each variable c, a constraints is added to capture
the cascade propagation. Let N be the number of entities
in the minterm on which c is dependent. In the example
for the variable c1 with IDR c1 â bj bp bl , constraints
y
+y
+yl(dâ1)
c1d â¥ j(dâ1) p(dâ1)
and c1d â¤ yj(dâ1) + yp(dâ1) +
N
yl(dâ1) âd, 1 â¤ d â¤ m + n â 1 are introduced (with
N = 3 in this case). If IDR of an entity is already in
form of Case II, i.e.,ai â bj bp bl then constraints xid â¥
yj(dâ1) +yp(dâ1) +yl(dâ1)
â qxi and xid â¤ yj(dâ1) + yp(dâ1) +
N
yl(dâ1) âd, 1 â¤ d â¤ m + n â 1 are introduced (with N = 3).
These constraints satisfies that if the entity xi is hardened
initially then it is not dead at any time step.
Step 3: Let M be the number of minterms in the transformed IDR as described in Step 1. In the given example
with IDR ai â c1 + c2 + bq constraints of form xid â¥
c1(dâ1) + c2(dâ1) + yq(dâ1) â (M â 1) â qxi and xid â¤
c1(dâ1) +c2(dâ1) +yq(dâ1)
âd, 1 â¤ d â¤ m + n â 1 are introduced.
M
These constraints ensures that even if all the minterms of xi
has at least one entity in dead state then it will be alive if the
entity is hardened initially. For all IDRs of type Case I and
Case III, the constraint discussed in this step is used.

6

B. Heuristic

Algorithm 2: Heuristic Solution to the ENH Problem

In this subsection we provide a greedy heuristic solution to
the Entity Hardening problem. For an interdependent network
I(A, B, F (A, B)) with the initial failed set of entities as
Aâ² âª B â² , Protection Set of each entity has been defined in
the approximation scheme of Case III. To design the heuristic
we define Minterm Coverage Number of each entity in A âª B
as follows:

1
2
3
4

Definition: For an entity xi â A âª B the Minterm Coverage
Number is defined as the number of minterms that can be
removed from F (A, B) without affecting the cascading process
by hardening the entity xi when all entities in Aâ² âª B â² fails
initially. This is represented as M (xi |Aâ² âª B â² ).
Similar to the computation of Protection Set the Minterm
Coverage Number of each entity can be computed in O((n +
m)2 ). With these definitions the heuristic is given in Algorithm
2. The algorithm takes in as input an interdependent network
I(A, B, F (A, B)) with S = AâªB. Step 4-5 is done to reduce
the search space as it directly follows that the set of entities
in Q wouldnât effect the hardening process. In each iteration
of the while loop an entity xd is greedily selected which
when hardened would prevent failure of maximum number of
entities. This ensures that at each step the number of entities
failed is minimized. In case of a tie, among all entities involved
in the tie, the entity having the highest Minterm Coverage
Number is included in the solution. This gives a higher priority
to the entity which when hardened, has more impact on failure
minimization in subsequent iterations of the while loop. The
interdependent network I(A, B, F (A, B)) is updated in steps
13-16 of the algorithm. This takes into account the effect of
hardening an entity in the current iteration on entities hardened
in the following iterations.
Run Time Analysis of Algorithm 2: For this analysis we
consider n to be the total number of entities and m to be
the total number of minterms. Updates in step 4 can be done
in O(m) and step 5 in O(n). The while loop iterates for k
times. In each iteration of the while loop step 7 and step 8 takes
at most O((n + m)2 ) and O(nlog(n)) time respectively. On
branching in step 9, step 10 and step 11 takes O((n + m)2 )
and O(nlog(n)) time respectively. Updates in step 13 takes
O(n) time and in step 14 takes O(n + m) time. Step 12,
16 and 17 runs in constant time. Hence Algorithm 2 runs in
O(k(n + m)2 ) time.
VI.

E XPERIMENTAL R ESULTS

In this section we present the experimental results of the
Entity Hardening problem by comparing the optimal solution
computed using an ILP, and the proposed heuristic algorithm.
The experiments were conducted on real world power grid data
obtained from Platts (www.platts.com), and communication
network data obtained from GeoTel (www.geo-tel.com) of
Maricopa County, Arizona. The data consisted of 70 power
plants and 470 transmission lines in the power network, and
2, 690 cell towers, 7, 100 fiber-lit buildings and 42, 723 fiber
links in the communication network. We identified five nonintersecting geographical regions from the data set and labeled
them from regions 1 through 5. For each of the regions, the
entities of the power and communication network that were

5
6
7
8
9
10
11
12

Data: An interdependent network I(A, B, F(A, B)) (with
S = A âª B), set of entities Aâ² âª B â² failed initially
causing maximum failure in the interdependent network
with |Aâ² | + |B â² | = K and hardening budget k.
Result: Set of hardened entities H.
begin
Initialize S â² â Aâ² âª B â² ;
Initialize H = â;
Update F(A, B) as follows â (a) let Q be the set of
entities that does not fail on failing K entities, (b) remove
IDRs corresponding to entities in set Q, (c) remove from
minterm of entities not in set Q all entities which are in
set Q ;
Update S = S \ Q ;
while (k entities are not hardened) do
For each entity xi â S compute the Protection Set
P (xi |S â² );
Choose the entity xd with highest cardinality of the
set |P (xd |S â² )|;
if (more than one entity has the same highest
cardinality value) then
For each such entity xj compute the Minterm
Coverage Number M (xj |S â² ) ;
Choose the entity xd with highest Minterm
Coverage Number. ;
In case of a tie choose arbitrarily;

16

Update S â S â P (xd |S â² );
Update F(A, B) by removing (i) IDRs corresponding
to all entities in P (xd |S â² ), and (ii) occurrence of
these entities in IDRs of entities not in P (xd |S â² );
if (xd â S â² ) then
Update S â² â S â² â {xd };

17

Update H = H âª xd ;

13
14

15

18

return H ;

located within the geographic region formed the set A and B
respectively. Each region was represented by an interdependent
network I(A, B, F (A, B)). We use the IDR construction rules
as defined in [9] to generate F (A, B).

In all of our simulations IBM CPLEX Optimizer 12.5 to
solve ILPs and Python 3 for heuristic is used. To analyze
the Entity Hardening problem the value of K was set to 8.
The ILP in [9] was used to compute the K most vulnerable
nodes in the network, and the set of failed entities due to
the failure of the K entities was also computed. For the five
regions, when the K = 8 most vulnerable nodes failed, the
total number of failed entities in the network were 28, 23, 28,
28 and 27 respectively. With the K most vulnerable nodes and
final set of failed nodes as input, the ILP and heuristic of the
Entity Hardening problem are compared with k = 1, 3, 5, 7.
The results of these simulations are shown in Figure 1. It is
observed that the heuristic solution differs more from optimal
at higher values of k (factor of 0.5 and 0.67 for Regions 1
and 3 respectively with k = 7). This is primarily because of
the greedy nature of Algorithm 2. However on an average the
heuristic solution differs by a factor of 0.13 from the optimal.

7

14 13

ILP solution
Heuristic

10
8

7
6

6

4

4

3
2

2
0

1
1

14

13

ILP solution
Heuristic

12
10
8

7

4

3

Number of entities failed

3

2

1
1

1

13

ILP solution
Heuristic

12

12
10

8

8
6

6

5

4

3

2
1

3
5
7
Number of entities hardened

(b) Region 2
11

ILP solution
Heuristic

10
8
6
5

4

4

3
2

2

1
1

(c) Region 3

8

11

6

3
1

0

3
5
7
Number of entities hardened

(a) Region 1

0

7

6

0

3
5
7
Number of entities hardened

12

Number of entities failed

13
12

3
5
7
Number of entities hardened

(d) Region 4

Number of entities failed

12

Number of entities failed

Number of entities failed

14

7

ILP solution
Heuristic

7

6
5

5

4
3

3

2
1
0

1

1

3
5
7
Number of entities hardened

(e) Region 5

Fig. 1: Comparison chart of the optimal solution (ILP) with the heuristic by varying number of entities hardened for each identified region

VII.

C ONCLUSION

In this paper we studied the entity hardening problem
in multi-layer networks. We modeled the interdependencies
shared between the networks using IIM, and formulated the
the Entity Hardening problem in this setting. We showed that
the problem is solvable in polynomial time for some cases,
whereas for others it is NP-complete. We evaluated the efficacy
of our heuristic using power and communication network data
of Maricopa County, Arizona. Our experiments showed that
our heuristic almost always produces near optimal results.
R EFERENCES
[1]

[2]

[3]

[4]

[5]

[6]

[7]

S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
âCatastrophic cascade of failures in interdependent networks,â Nature,
vol. 464, no. 7291, pp. 1025â1028, 2010.
J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, âNetworks formed
from interdependent networks,â Nature Physics, vol. 8, no. 1, pp. 40â48,
2011.
J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, âCascade of
failures in coupled network systems with multiple support-dependence
relations,â Physical Review E, vol. 83, no. 3, p. 036116, 2011.
V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and
R. Setola, âModelling interdependent infrastructures using interacting
dynamical models,â International Journal of Critical Infrastructures,
vol. 4, no. 1, pp. 63â79, 2008.
P. Zhang, S. Peeta, and T. Friesz, âDynamic game theoretic model of
multi-layer infrastructure networks,â Networks and Spatial Economics,
vol. 5, no. 2, pp. 147â178, 2005.
M. Parandehgheibi and E. Modiano, âRobustness of interdependent
networks: The case of communication networks and the power grid,â
arXiv preprint arXiv:1304.0356, 2013.
D. T. Nguyen, Y. Shen, and M. T. Thai, âDetecting critical nodes in
interdependent power networks for vulnerability assessment,â 2013.

[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman,
âPower grid vulnerability to geographically correlated failures-analysis
and control implications,â arXiv preprint arXiv:1206.1099, 2012.
[9] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, âIdentification of k most vulnerable nodes in multi-layered network using a new
model of interdependency,â in Computer Communications Workshops
(INFOCOM WKSHPS), 2014 IEEE Conference on. IEEE, 2014, pp.
831â836.
[10] J.-F. Castet and J. H. Saleh, âInterdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,â PloS one, vol. 8, no. 4, p. e60402, 2013.
[11] A. Das, J. Banerjee, and A. Sen, âRoot cause analysis of failures in
interdependent power-communication networks,â in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910â915.
[12] A. Mazumder, C. Zhou, A. Das, and A. Sen, âProgressive recovery from
failure in multi-layered interdependent network using a new model of
interdependency,â in Conference on Critical Information Infrastructures
Security (CRITIS), 2014. Springer, 2014.
[13] M. Hajiaghayi, K. Jain, K. Konwar, L. Lau, I. Mandoiu, A. Russell,
A. Shvartsman, and V. Vazirani, âThe minimum k-colored subgraph
problem in haplotyping and dna primer selection,â in Proceedings of the
International Workshop on Bioinformatics Research and Applications
(IWBRA). Citeseer, 2006.

Identification of K Most Vulnerable Nodes in
Multi-layered Network Using a New Model of
Interdependency
Arunabha Sen, Anisha Mazumder, Joydeep Banerjee, Arun Das and Randy Compton
Computer Science and Engineering Program

arXiv:1401.1783v1 [cs.NI] 8 Jan 2014

School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {asen, amazumde, Joydeep.Banerjee, adas22, Randy.Compton}@asu.edu

AbstractâThe critical infrastructures of the nation including
the power grid and the communication network are highly
interdependent. Recognizing the need for a deeper understanding
of the interdependency in a multi-layered network, significant
efforts have been made by the research community in the last
few years to achieve this goal. Accordingly a number of models
have been proposed and analyzed. Unfortunately, most of the
models are over simplified and, as such, they fail to capture the
complex interdependency that exists between entities of the power
grid and the communication networks involving a combination of
conjunctive and disjunctive relations. To overcome the limitations
of existing models, we propose a new model that is able to
capture such complex interdependency relations. Utilizing this
model, we provide techniques to identify the K most vulnerable
nodes of an interdependent network. We show that the problem
can be solved in polynomial time in some special cases, whereas
for some others, the problem is NP-complete. We establish that
this problem is equivalent to computation of a fixed point of a
multilayered network system and we provide a technique for its
computation utilizing Integer Linear Programming. Finally, we
evaluate the efficacy of our technique using real data collected
from the power grid and the communication network that span
the Maricopa County of Arizona.

I. I NTRODUCTION
In the last few years there has been an increasing awareness
in the research community that the critical infrastructures of
the nation are closely coupled in the sense that the well being
of one infrastructure depends heavily on the well being of another. A case in point is the interdependency between the electric power grid and the communication network. The power
grid entities, such as the SCADA systems that control power
stations and sub-stations, receive their commands through
communication networks, while the entities of communication
network, such as routers and base stations, cannot operate
without electric power. Cascading failures in the power grid,
are even more complex now because of the coupling between
power grid and communication network. Due to this coupling,
not only entities in power networks, such as generators and
transmission lines, can trigger power failure, communication
network entities, such as routers and optical fiber lines, can
also trigger failure in power grid. Thus it is essential that
the interdependency between different types of networks be
understood well, so that preventive measures can be taken to

avoid cascading catastrophic failures in multi-layered network
environments.
Recognizing the need for a deeper understanding of the
interdependency in a multi-layered network, significant efforts
have been made in the research community in the last few
years to achieve this goal [1], [2], [3], [4], [5], [6], [7], [8].
Accordingly a number of models have been proposed and
analyzed. Unfortunately, many of the proposed models are
overly simplistic in nature and as such they fail to capture
the complex interdependency that exists between power grid
and communication networks. In a highly cited paper [1], the
authors assume that every node in one network depends on one
and only one node of the other network. However, in a follow
up paper [2], the same authors argue that this assumption may
not be valid in the real world and a single node in one network
may depend on more than one node in the other network. A
node in one network may be functional (âaliveâ) as long as
one supporting node on the other network is functional.
Although this generalization can account for disjunctive
dependency of a node in the A network (say ai ) on more
than one node in the B network (say, bj and bk ), implying
that ai may be âaliveâ as long as either bi or bj is alive,
it cannot account for conjunctive dependency of the form
when both bj and bk has to be alive in order for ai to
be alive. In a real network the dependency is likely to be
even more complex involving both disjunctive and conjunctive
components. For example, ai may be alive if (i) bj and bk and
bl are alive, or (ii) bm and bn are alive, or (iii) bp is alive. The
graph based interdependency models proposed in the literature
[3], [4], [5], [9], [6], [7] including [1], [2] cannot capture
such complex interdependency between entities of multilayer
networks. In order to capture such complex interdependency,
we propose a new model using Boolean logic. Utilizing this
comprehensive model, we provide techniques to identify the
K most vulnerable nodes of an interdependent multilayered
network system. We show that the this problem can be solved
in polynomial time for some special cases, whereas for some
others, the problem is NP-complete. We also show that this
problem is equivalent to computation of a fixed point [10] and
we provide a technique utilizing Integer Linear Programming

2

to compute that fixed point. Finally, we evaluate the efficacy
of our technique using real data collected from power grid
and communication networks that span Maricopa County of
Arizona.

II. I NTERDEPENDENCY M ODEL
We describe the model for an interdependent network with
two layers. However, the concept can easily be generalized
to deal with networks with more layers. Suppose that the
network entities in layer 1 are referred to as the A type
entities, A = {a1 , . . . , an } and entities in layer 2 are referred
to as the B type entities, B = {b1 , . . . , bm }. If the layer 1
entity ai is operational if (i) the layer 2 entities bj , bk , bl
are operational, or (ii) bm , bn are operational, or (iii) bp
is operational, we express it in terms of live equations of
the form ai â bj bk bl + bm bn + bp . The live equation for
a B type entity br can be expressed in a similar fashion
in terms of A type entities. If br is operational if (i) the
layer 1 entities as , at , au , av are operational, or (ii) aw , az
are operational, we express it in terms of live equations of
the form br â as at au av + aw az . It may be noted that the
live equations only provide a necessary condition for entities
such as ai or br to be operational. In other words, ai or br
may fail independently and may be not operational even when
the conditions given by the corresponding live equations are
satisfied. A P
live equation
in general will have the following
Ti Qtj
form: xi â j=1
y
k=1 j,k where xi and yj,k are elements
of the set A (B) and B (A) respectively, Ti represents the
number of min-terms in the live equation and tj refers to the
size of the j-th min-term (the size of a min-term is equal to the
number of A or B elements in that min-term). In the example
ai â bj bk bl + bm bn + bp , Ti = 3, t1 = 3, t2 = 2, t3 = 1,
xi = ai , y2,1 = bm , y2,2 = bp .
We refer to the live equations of the form ai â bj bk bl +
bm bn + bp also as First Order Dependency Relations, because
these relations express direct dependency of the A type entities
on B type entities and vice-versa. It may be noted however
that as A type entities are dependent on B type entities,
which in turn depends on A type entities, the failure of
some A type entities can trigger the failure of other A type
entities, though indirectly, through some B type entities. Such
interdependency creates a cascade of failures in multilayered
networks when only a few entities of either A type or B type
(or a combination) fails. We illustrate this with the help of
an example. The live equations for this example is shown in
table I.
Power Network
a1 â b1 + b2
a2 â b1 b3 + b2
a3 â b1 b2 b3
a4 â b1 + b2 + b3

Communication Network
b1 â a1 + a2 a3
b2 â a1 + a3
b3 â a1 a2
ââ

TABLE I: Live equations for a Multilayer Network

Entities
a1
a2
a3
a4
b1
b2
b3

t0
1
0
0
0
0
0
0

t1
1
0
0
0
0
0
1

Time Steps
t2
t3
t4
1
1
1
0
0
1
1
1
1
0
0
1
0
1
1
0
1
1
1
1
1

t5
1
1
1
1
1
1
1

t6
1
1
1
1
1
1
1

TABLE II: Time Stepped Cascade Effect for a Multilayer Network

Fig. 1: Cascading failures reach steady state after p time steps

As shown in table II, the failure of only one entity a1 at
time step t0 triggered a chain of failures that resulted in the
failure of all the entities of the network after by timestep t4 .
A table entry of 1 indicates that the entity is âdeadâ. In this
example, the failure of a1 at t0 triggered the failure of b3 at
t1 , which in turn triggered the failure of a3 at t2 . The failure
of b3 at t1 was due to the dependency relation b3 â a1 a2
and the failure of a3 at t2 was due to the dependency relation
a3 â b1 b2 b3 . The cascading failure process initiated by failure
(or death) of a subset of A type entities at timestep t0 , A0d and
a subset of B type entities Bd0 till it reaches its final steady
state is shown diagrammatically in figure 1. Accordingly, a
multilayered network can be viewed as a âclosed loopâ control
system. Finding the steady state after an initial failure in this
case is equivalent of computing the fixed point of a function
F(.) such that F(Apd âª Bdp ) = Apd âª Bdp , where p represents
the number of steps when the system reaches the steady state.
We define a set of K entities in a multi-layered network
as most vulnerable, if failure of these K entities triggers the
failure of the largest number of other entities. The goal of
the K most vulnerable nodes problem is to identify this set of
nodes. This is equivalent to identifying A0d â A, Bd0 â B, that
maximizes |Apd âªBdp |, subject to the constraint that |A0d âªBd0 | â¤
K.
The dependency relations (live equations) can be formed
either after careful analysis of the multilayer network along the
lines carried out in [8], or after consultation with the engineers
of the local utility and internet service providers.
III. C OMPUTATIONAL C OMPLEXITY AND A LGORITHMS
Based on the number and the size of the min-terms in the
dependency relations, we divide them into four different cases
as shown in Table III. The algorithms for finding the K most
vulnerable nodes in the multilayer networks and computation
complexity for each of the cases are discussed in the following
four subsections.
Case
Case I
Case II
Case III
Case IV

No. of Min-terms
1
1
Arbitrary
Arbitrary

Size of Min-terms
1
Arbitrary
1
Arbitrary

TABLE III: Equation Types for Dependency Relations

3

A. Case I: Problem Instance with One Min-term of Size One
In this case, a live equation in general will have the following form: xi â yj where xi and yj are elements of the set A
(B) and B (A) respectively. In the example ai â bj , xi = ai ,
y1 = bj . It may be noted that a conjunctive implication of
the form ai â bj bk can also be written as two separate
implications ai â bj and ai â bk . However, such cases are
considered in Case II and is excluded from consideration in
Case I. The exclusion of such implications implies that the
entities that appear on the LHS of an implication in Case I
are unique. This property enables us to develop a polynomial
time algorithm for the solution of the K most vulnerable node
problem for this case. We present the algorithm next.
Algorithm 1
Input: (i) A set S of implications of the form of y â x,
where x, y â A âª B, (ii) An integer K.
Output: A set V 0 where |V 0 | = K and V 0 â A âª B such
that failure of entities in V 0 at time step t0 results in failure
of the largest number of entities in A âª B when the steady
state is reached.
Step 1. We construct a directed graph G = (V, E), where
V = A âª B. For each implication y â x in S, where x, y â
A âª B, we introduce a directed edge (x, y) â E.
Step 2. For each node xi â V , we construct a transitive
closure set Cxi as follows: If there is a path from xi to some
node yi â V in G, then we include yi in Cxi . It may be
recalled that |A| + |B| = n + m. So, we get n + m transitive
closure sets Cxi , 1 â¤ i â¤ (n + m). We call each xi to be the
seed entity for the transitive closure set Cxi .
Step 3. We remove all the transitive closure sets which are
proper subsets of some other transitive closure set.
Step 4. Sort the remaining transitive closure sets Cxi ,
where the rank of the closure sets is determined by the
cardinality of the sets. The sets with a larger number of entities
are ranked higher than the sets with a fewer number of entities.
Step 5. Construct the set V 0 by selecting the seed entities
of the top K transitive closure sets. If the number of remaining
transitive closure sets is less than K (say, K0 ), arbitrarily select
the remaining entities.
Time complexity of Algorithm 1: Step 1 takes O(n + m + |S|)
time. Step 2 can be executed in O((n+m)3 ) time. Step 3 takes
at most O((n + m)2 ) time. Step 4 sorts at most |S| entries, a
standard sorting algorithm takes O(|S| log |S|) time. Selecting
K entities in step 5 takes O(K) time. Since |S| â¤ n+m, hence
the overall time complexity is O((n + m)3 )
Theorem 1. For each pair of transitive closure sets Cxi and
Cxj produced in step 2 of algorithm 1, either Cxi â© Cxj = â
or Cxi â© Cxj = Cxi or Cxi â© Cxj = Cxj , where xi 6= xj .
Proof: Consider, if possible, that there is a pair of transitive
closure sets Cxi and Cxj produced in step 2 of algorithm 1,
such that Cxi â©Cxj 6= â and Cxi â©Cxj 6= Cxi and Cxi â©Cxj 6=

Cxj . Let xk â Cxi â© Cxj . This implies that there is a path
from xi to xk (path1 ) as well as there is a path from xj to xk ,
(path2 ). Since, xi 6= xj and Cxi â©Cxj 6= Cxi and Cxi â©Cxj =
Cxj , there is some xl in the path1 such that xl also belongs to
path2 . W.l.o.g, let us consider that xl be the first node in path1
such that xl also belongs to path2 . This implies that xl has
in-degree greater than 1. This in turn implies that there are two
implications in the set of implications S such that xl appears in
the L.H.S of both. This is a contradiction because this violates
a characteristic of the implications in Case I. Hence, our initial
assumption was wrong and the theorem is proven.
Theorem 2. Algorithm 1 gives an optimal solution for the
problem of selecting K most vulnerable entities in a multilayer network for case I dependencies.
Proof: Consider that the set V 0 returned by the algorithm is
not optimal and the optimal solution is VOP T . Let us consider
there is a entity xi â A âª B such that xi â VOP T \ V 0 .
Evidently, (i) Cxi was either deleted in step 3 or (ii) |Cxi | is
less than the cardinalities of all the transitive closure sets with
seed entities xj â V 0 , because our algorithm did not select
xi . Hence, in both cases, replacing any entity xj â V 0 by xi
reduces the total number of entities killed. Thus, the number
of dead entities by the failure of entities in VOP T is lesser than
that caused by the failure of the entities in V 0 , contradicting
the optimality of VOP T . Hence, the algorithm does in fact
return the optimal solution.
B. Case II: Problem Instance with One Min-term of Arbitrary
Size
In this case, a liveQ equation in general will have the
q
following form: xi â k=1 yj where xi and yj are elements
of the set A (B) and B (A) respectively, q represents the size
of min-term. In the example ai â bj bk bl , q = 3, xi = ai ,
y1 = bj , y2 = bk , y3 = bk .
1) Computational Complexity: We show that computation
of K most vulnerable nodes (K-MVN) in a multilayer network
is NP-complete in Case II. We formally state the problem next.
Instance: Given a set of dependency relations between
A
Qqand B type entities in the form of live equations xi â
k=1 yj , integers K and L.
Question: Is there a subset of A and B type entities of
size at most K whose âdeathâ (failure) at time t0 , triggers a
cascade of failures resulting in failures of at least L entities,
when the steady state is reached?
Theorem 3. The K-MVN problem is NP-complete.
Proof: We prove that the K-MVN problem is NP-complete
by giving a transformation for the vertex cover (VC) problem.
An instance of the vertex cover problem is specified by an
undirected graph G = (V, E) and an integer R. We want to
know if there is a subset of nodes S â V of size at most
R, so that every edge has at least one end point in S. From
an instance of the VC problem, we create an instance of the

4

K-MVN problem in the following way. First, from the graph
G = (V, E), we create a directed graph G0 = (V, E 0 ) by
replacing each edge e â E by two oppositely directed edges
e1 and e2 in E 0 (the end vertices of e1 and e2 are same as
the end vertices of e). Corresponding to a node vi in G0 that
has incoming edges from other nodes (say) vj , vk and vl , we
create a dependency relation (live equation) vi â vj vk vl . We
set K = R and L = |V |. The corresponding death equation is
of the form vÂ¯i â vÂ¯j + vÂ¯k + vÂ¯l (obtained by taking negation
of the live equation). We set K = R and L = |V |. It can now
easily be verified that if the graph G = (V, E) has a vertex
cover of size R iff in the created instance of K-MVN problem
death (failure) of at most K entities at time t0 , will trigger a
cascade of failures resulting in failures of at least L entities,
when the steady state is reached.
2) Optimal Solution with Integer Linear Programming:
In this case, we can find and optimal solution to the KMVN problem using Integer Linear Programming (ILP). We
associate binary indicator variables xi (yi ) to capture the state
of the entities ai (bi ). xi (yi ) is 1 when ai (bi ) is dead and
0 otherwise. Since we want find the set of K entities whose
failure at time step t0 triggers cascading failure resulting in the
failure of the largest number of entities, the
the
Pnobjective
Pof
m
ILP can be written as follows maximize
x
+
i
i=1
i=1 yi
It may be noted that the variables in the objective function
do not have any notion of time. However, cascading failure
takes place in time steps, ai triggers failure of bj at time
step t1 , which in turn triggers failure of ak in time step t2
and so on. Accordingly, in order to capture the cascading
failure process, we need to introduce the notion of time into
the variables of the ILP. If the numbers of A and B type
entities are n and m respectively, the steady state must be
reached by time step n + m â 1 (cascading process starts at
time step 0, t0 ). Accordingly, we introduce n + m versions
of the variables xi and yi , i.e., xi [0], . . . , xi [n + m â 1] and
yi [0], . . . , yi [n+mâ1]. To indicate the state of entities ai and
bi at times t0 , . . . , tn+mâ1 . The objective of the ILP is now
changed to
maximize

n
X
i=1

xi [n + m â 1] +

m
X

yi [n + m â 1]

i=1

Subject to the constraint that no more than K entities can
fail at time t0 .
Pn
Pm
Constraint 1:
i=1 yi [0] â¤ K In order
i=1 xi [0] +
to ensure that the cascading failure process conforms to
the dependency relations between type A and B entities,
additional constraints must be imposed.
Constraint 2: If an entity fails at time fails at time step p,
(i.e., tp ) it should continue to be in the failed state at all time
steps t > p. That is xi (t) â¥ xi (t â 1), ât, 1 â¤ t â¤ n + m â 1.
Same constraint applies to yi (t).
Constraint 3: The dependency relation (death equation)
aÂ¯i â bÂ¯j +bÂ¯k +bÂ¯l can be translated into a linear constraint in the
following way xi (t) â¤ yj (tâ1)+yk (tâ1)+yl (tâ1), ât, 1 â¤
t â¤ n + m â 1.

The optimal solution to K-MVN problem for Case II can be
found by solving the above ILP.
C. Case III: Problem Instance with an Arbitrary Number of
Min-terms of Size One
A live equation
Pq in this special case will have the following
form: xi â j=1 yj where xi and yj are elements of the set
A (B) and B (A) respectively, q represents the number of minterms in the live equation. In the example ai â bj + bk + bl ,
q = 3, xi = ai , y1 = bj , y2 = bk , y3 = bl .
1) Computational Complexity: We show that a special
case of the problem instances with an arbitrary number
of min-terms of size one is same as the Subset Cover
problem (defined below), which is proven to be NPcomplete. We define Implication Set(A) Pto be the
Ti
set of all implications of the form ai â
j=1 bj and
ImplicationPSet(B) to be the set of all implications of the
Ti
form bi â
j=1 aj . Now consider a subset of the set of
problem instances with an arbitrary number of min-terms
of size one where either Implication Set(A) = â
or Implication Set(B)
=
â. Let A0
=
{ai |ai is the element on the LHS of an implication}
in the Implication Set(A). The set B 0 is defined
accordingly. If Implication Set(B) = â then B 0 = â. In
this case, failure of any ai , 1 â¤ i â¤ n type entities will not
cause failure of any bj , 1 â¤ j â¤ m type entities. Since an
adversary can cause failure of only K entities, the adversary
would like to choose only those K entities that will cause
failure of the largest number of entities. In this scenario, there
is no reason for the adversary to attack any ai , 1 â¤ i â¤ n type
entities as they will not cause failure of any bj , 1 â¤ j â¤ m
type entities. On the other hand, if the adversary attacks
K bj type entities, not only those K bj type entities will
be destroyed, some ai type entities will also be destroyed
due to the implications in the Implication Set(A). As
such the goal of the adversary will be to carefully choose
K bj , 1 â¤ j â¤ m type entities that will destroy the largest
number of ai type entities. In its abstract form, the problem
can be viewed as the Subset Cover problem.
Subset Cover Problem
Instance: A set S = {s1 , . . . , sm }, a set S of m subsets of S,
i.e., S = {S1 , . . . , Sr }, where Si â S, âi, 1 â¤ i â¤ r, integers
p and q.
Question: Is there a p element subset S 0 of S (p < n) that
completely covers at least q elements of the set S? (A set S 0 is
said to be completely covering an element Si , âi, 1 â¤ i â¤ m
of the set S, if S 0 â© Si = Si , âi, 1 â¤ i â¤ m.)
The set S in the subset cover problem corresponds to the
set B = {b1 , . . . , bm }, and each set Si , 1 â¤ i â¤ r corresponds
to an implication in the ImplicationS et(A) and comprises of
the bj âs that appear on the RHS of the implication. The goal
of the problem is to select a subset B 00 of B that maximizes
the number of Si âs completely covered by B 00 .

5

Theorem 4. The Subset Cover problem is NP-complete.
Proof: We prove that the Subset Cover problem is NPcomplete by giving a transformation from the well known
Clique problem. It may be recalled that an instance of the
Clique problem is specified by a graph G = (V, E) and an
integer K. The decision question is whether or not a clique of
size at least K exists in the graph G = (V, E). We show that
a clique of size K exists in graph G = (V, E) iff the Subset
Cover problem instance has a p element subset S 0 of S that
completely covers at least q elements of the set S.
From an instance of the Clique problem, we create an
instance of the Subset Cover problem in the following way.
Corresponding to every vertex vi , 1 â¤ i â¤ n of the graph
G = (V, E) (V = {v1 , . . . , vn }), we create an element
in the set S = {s1 , . . . , sn }. Corresponding to every edge
ei , 1 â¤ i â¤ m, we create m subsets of S, i.e., S =
{S1 , . . . , Sm }, where Si corresponds to a two element subset
of nodes, corresponding to the end vertices of the edge ei . We
set the parameters p = K and q = K(K â 1)/2. Next we
show that in the instance of the subset cover problem created
by the above construction process, a p element subset S 0 of
S exists that completely covers at least q elements of the set
S, iff the graph G = (V, E) has a clique of size at least K.
Suppose that the graph G = (V, E) has a clique of size
K. It is clear that in the created instance of the subset cover
problem, we will have K(K â 1)/2 elements in the set S,
that will be completely covered by a K element subset of
the set S. The K element subset of S corresponds to the set
of K nodes that make up the clique in G = (V, E) and the
K(K â 1)/2 elements in the set S corresponds to the edges
of the graph G = (V, E) that corresponds to the edges of
the clique. Conversely, suppose that the instance of the Subset
Cover problem has K element subset of S that completely
covers K(K â 1)/2 elements of the set S. Since the elements
of S corresponds to the edges in G, in order to completely
cover K(K â 1)/2 edges, at least K nodes (elements of the
set S) will be necessary. As such, this set of K nodes will
constitute a clique in the graph G = (V, E).
2) Optimal Solution with Integer Linear
Programming: If
Pq
the live equation is in the form xi â k=1 yj then the âdeath
equationâ (obtained by taking negation
of the live equation)
Qq
will be in the product form xÌi â j=1 yÌj . If the live equation
is given as ai â bj + bk , then the death equation will be given
as aÂ¯i â bÂ¯j bÂ¯k .
By associating binary indicator variables xi and yi to
capture the state of the entities ai and bi , we can follow almost
identical procedure as in Case II, with only one exception.
It may be recalled that in Case II, the death equations such
as aÂ¯i â bÂ¯j + bÂ¯k was translated into a linear constraint
xi (t) â¤ yj (t â 1) + yk (t â 1), ât, 1 â¤ t â¤ n + m â 1. However
a similar translation in Case III, with death equations such as
aÂ¯i â bÂ¯j bÂ¯k , will result in a non-linear constraint of the form
xi (t) â¤ yj (t â 1)yk (t â 1), ât, 1 â¤ t â¤ n + m â 1. Fortunately,
a non-linear constraint of this form can be replaced a linear
constraint such as 2xi (t) â¤ yj (t â 1) + yk (t â 1), ât, 1 â¤

t â¤ n + m â 1. After this transformation, we can compute the
optimal solution using integer linear programming.
D. Case IV: Problem Instance with an Arbitrary Number of
Min-terms of Arbitrary Size
1) Computational Complexity: Since both Case II and Case
III are special cases of Case IV, the computational complexity
of finding the K most vulnerable nodes in the multilayer
network in NP-complete in Case IV also.
2) Optimal Solution with Integer Linear Programming:
The optimal solution to this version of the problem can be
computed by combining the techniques developed for the
solution of the versions of the problems considered in Cases
II and III.
IV. E XPERIMENTAL RESULTS
We applied our model to study multilayer vulnerability
issues in Maricopa County, the most densely populated county
of Arizona with approximately 60% of Arizonas population
residing in it. Specifically, we wanted to find out if some
regions of Maricopa County were more vulnerable to failure
than some other regions. The data for our multi-layered
network were obtained from different sources. We obtained
the data for the power network (network A) from Platts
(http://www.platts.com/). Our power network dataset consists
of 70 power plants and 470 transmission lines. Our communication network (network B) data were obtained from GeoTel
(http://www.geo-tel.com/). Our communication network data
consists of 2, 690 cell towers and 7, 100 fiber-lit buildings as
well as 42, 723 fiber links. Snapshots of our power network
data and communication network data are shown in figure 2. In
the power network snapshot of sub-figure(a), the orange markers show locations of powerplants while the yellow continuous
lines represent the transmission lines. In the communication
network snapshot of sub-figure (b) the pink markers show the
location of fiber-lit buildings, the orange markers show the
location of cell towers and the green continuous lines represent
the fiber links. In our dataset, âloadâ in the Power Network is
divided into Cell towers and Fiber-lit buildings. Although there
exists various other physical entities which also draw electric
power and hence can be viewed as load to the power network,
as they are not relevant to our study on interdependency
between power and communication networks, we ignore such
entities. Thus in network A, we have the three types of Power
Network Entities (PNEâs) - Generators, Load (consisting of
Cell towers and Fiber-lit buildings) and Transmission lines
(denoted by a1 , a2 , a3 respectively). For the Communication
Network, we have the following Communication Network
Entities (CNEâs) - Cell Towers, Fiber-lit buildings and Fiber
links (denoted by b1 , b2 , b3 respectively). We consider the
Fiber-lit buildings as a communication network entities as they
house routers which definitely are communication network
entities. From the raw data we construct Implication Set(A)
and Implication Set(B), by following the rules stated below:
Rules: We consider that a PNE is dependent on a set of
CNEs for being in the active state (âaliveâ) or being in the

6

(a) Snapshot of Power Network in Maricopa County

(b) Snapshot of Communication Network in Maricopa County

Fig. 2: Snapshots of power network and communication network in Maricopa County)

inactive state (âdeadâ). Similarly, a CNE is dependent on a set
of PNEs for being active or inactive state. For simplicity we
consider the live equations with at most two minterms. For
the same reason we consider the size of each minterm is at
most two.

of the number of entities of the two networks A and B. Most
importantly, we find that the degree of vulnerability of all
the five regions considered in our study are close and no one
region stands out as being extremely vulnerable.

Generators (a1,i , 1 â¤ i â¤ p, where p is the total number
of generators): We consider that each generator (a1.i ) is
dependent on the nearest Cell Tower (b1,j ) or the nearest
Fiber-lit building (b2,k ) and the corresponding Fiber link (b3,l )
connecting b2,k and a1,i . Thus, we have
a1,i â b1,j + b2,k Ã b3,l
Load (a2,i , 1 â¤ i â¤ q, where q is the total number of loads):
We consider that the loads in the power network do not depend
on any CNE.
Transmission Lines (a3,i , 1 â¤ i â¤ r, where r is the total number of transmission lines): We consider that the transmission
lines do not depend on any CNE.
Cell Towers (b1,i , 1 â¤ i â¤ s, where s is the total number
of cell towers): We consider the cell towers depend on the
nearest pair of generators and the corresponding transmission
line connecting the generator to the cell tower. Thus, we have
b1,i â a1,j Ã a3,k + a1,j 0 Ã a3,k0
Fiber-lit Buildings (b2,i , 1 â¤ i â¤ t, where t is the total number
of fiber-lit buildings): We consider that the fiber-lit buildings
depend on the nearest pair of generators and the corresponding
transmission lines connecting the generators to the cell tower.
Thus, we have b2,i â a1,j Ã a3,k + a1,j 0 Ã a3,k0
Fiber Links (b3,i , 1 â¤ i â¤ u, where u is the total number of
fiber links)): We consider that the fiber links do not depend
on any PNE.
Because of experimental resource limitation, we have considered 5 regions of Maricopa County for our experiments.
We used IBM CPLEX Optimizer 12.5 to run the formulated
ILPâs on the experimental dataset. We show our results in
the figure 3. We observe that in each of the regions there
is a specific budget threshold beyond which each additional
increment in budget results in the death of only one entity. The
reason for this behavior is our assumption that entities such
as the transmission lines and the fiberlinks are not dependent
on any other entities. We notice that all the entities of the
two networks can be destroyed with a budget of about 60%

Fig. 3: Experimental results of failure vulnerability across five regions
of Maricopa county

R EFERENCES
[1] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
âCatastrophic cascade of failures in interdependent networks,â Nature,
vol. 464, no. 7291, pp. 1025â1028, 2010.
[2] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, âNetworks formed
from interdependent networks,â Nature Physics, vol. 8, no. 1, pp. 40â48,
2011.
[3] J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, âCascade of
failures in coupled network systems with multiple support-dependence
relations,â Physical Review E, vol. 83, no. 3, p. 036116, 2011.
[4] V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and
R. Setola, âModelling interdependent infrastructures using interacting
dynamical models,â International Journal of Critical Infrastructures,
vol. 4, no. 1, pp. 63â79, 2008.
[5] P. Zhang, S. Peeta, and T. Friesz, âDynamic game theoretic model of
multi-layer infrastructure networks,â Networks and Spatial Economics,
vol. 5, no. 2, pp. 147â178, 2005.
[6] M. Parandehgheibi and E. Modiano, âRobustness of interdependent
networks: The case of communication networks and the power grid,â
arXiv preprint arXiv:1304.0356, 2013.
[7] D. T. Nguyen, Y. Shen, and M. T. Thai, âDetecting critical nodes in
interdependent power networks for vulnerability assessment,â 2013.
[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman,
âPower grid vulnerability to geographically correlated failures-analysis
and control implications,â arXiv preprint arXiv:1206.1099, 2012.
[9] J.-F. Castet and J. H. Saleh, âInterdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,â PloS one, vol. 8, no. 4, p. e60402, 2013.
[10] A. Fudenberg and J. Tirole, Game Theory. Ane Books, 2010.

2014 IEEE 15th International Conference on High Performance Switching and Routing (HPSR)

On Shortest Single/Multiple Path Computation
Problems in Fiber-Wireless (FiWi) Access Networks
Chenyang Zhouâ , Anisha Mazumderâ , Arunabha Senâ , Martin Reissleinâ  and Andrea Richaâ
â School of Computing, Informatics and Decision Systems Engineering
â  School of Electrical, Computer, and Energy Engineering

Arizona State University
Email: {czhou24, anisha.mazumder, asen, reisslein, aricha}@asu.edu

AbstractâFiber-Wireless (FiWi) networks have received considerable attention in the research community in the last few
years as they offer an attractive way of integrating optical and
wireless technology. As in every other type of networks, routing
plays a major role in FiWi networks. Accordingly, a number of
routing algorithms for FiWi networks have been proposed. Most
of the routing algorithms attempt to ï¬nd the âshortest pathâ
from the source to the destination. A recent paper proposed
a novel path length metric, where the contribution of a link
towards path length computation depends not only on that link
but also every other link that constitutes the path from the
source to the destination. In this paper we address the problem
of computing the shortest path using this path length metric.
Moreover, we consider a variation of the metric and also provide
an algorithm to compute the shortest path using this variation.
As multipath routing provides a number of advantages over
single path routing, we consider disjoint path routing with the
new path length metric. We show that while the single path
computation problem can be solved in polynomial time in both
the cases, the disjoint path computation problem is NP-complete.
We provide optimal solution for the NP-complete problem using
integer linear programming and also provide two approximation
algorithms with a performance bound of 4 and 2 respectively.
The experimental evaluation of the approximation algorithms
produced a near optimal solution in a fraction of a second.

I. I NTRODUCTION
Path computation problems are arguably one of the most
well studied family of problems in communication networks.
In most of these problems, one or more weight is associated
with a link representing, among other things, the cost, delay or
the reliability of that link. The objective most often is to ï¬nd
a least weighted path (or âshortest pathâ) between a speciï¬ed
source-destination node pair. In most of these problems, if a
link l is a part of a path P , then the contribution of the link
l on the âlengthâ of the path P depends only on the weight
w(l) of the link l, and is oblivious of the weights of the links
traversed before or after traversing the link l on the path P .
However, in a recent paper on optical-wireless FiWi network
[5], the authors have proposed a path length metric, where the
contribution of the link l on the âlengthâ of the path P depends
not only on its own weight w(l), but also on the weights
of all the links of the path P . As the authors of [5] do not
present any algorithm for computing the shortest path between
the source-destination node pair using this new metric, we
present a polynomial time algorithm for this problem in this
paper. This result is interesting because of the nature of new

978-1-4799-1633-7/14/$31.00 Â©2014 IEEE

metric proposed in [5], one key property on which the shortest
path algorithm due to Dijkstra is based, that is, subpath of a
shortest path is shortest, is no longer valid. We show that even
without this key property, not only it is possible to compute
the shortest path in polynomial time using the new metric, it
is also possible to compute the shortest path in polynomial
time, with a variation of the metric proposed in [5].
The rest of the paper is organized as follows. In section
III, we present the path length metric proposed for the FiWi
network in [5] and a variation of it. In section IV we provide
algorithms for computing the shortest path using these two
metrics. As multi-path routing offers signiï¬cant advantage
over single path routing [6], [7], [8], [9], we also consider
the problem of computation of a pair of node disjoint paths
between a source-destination node pair using the metric proposed in [5]. We show that while the single path computation
problem can be solved in polynomial time in all these cases,
the disjoint path computation problem is NP-complete. The
contributions of the paper are as follows;
â¢ Polynomial time algorithm for single path routing (metric
1) in FiWi networks
â¢ Polynomial time algorithm for single path routing (metric
2) in FiWi networks
â¢ NP-completeness proof of disjoint path routing (metric
1) in FiWi networks
â¢ Optimal solution for disjoint path routing (metric 1) in
FiWi networks using Integer Linear Programming
â¢ One approximation algorithm for disjoint path routing in
FiWi networks with an approximation bound of 4 and
computation complexity O((n + m)log n)
â¢ One approximation algorithm for disjoint path routing in
FiWi networks with an approximation bound of 2 and
computation complexity O(m(n + m)log n)
â¢ Experimental evaluation results of the approximation
algorithm for disjoint path routing in FiWi networks
II. R ELATED W ORK
Fiber-Wirelss (FiWi) networks is a hybrid access network
resulting from the convergence of optical access networks
such as Passive Optical Networks (PONs) and wireless access
networks such as Wireless Mesh Networks (WMNs) capable of providing low cost, high bandwidth last mile access.

131

Because it provides an attractive way of integrating optical
and wireless technology, Fiber-Wireless (FiWi) networks have
received considerable attention in the research community in
the last few years [1], [2], [3], [4], [5], [8], [9]. The minimum
interference routing algorithm for the FiWi environment was
ï¬rst proposed in [4]. In this algorithm the path length was
measured in terms of the number of hops in the wireless
part of the FiWi network. The rationale for this choice was
that the maximum throughput of the wireless part is typically
much smaller than the throughput of the optical part, and
hence minimization of the wireless hop count should lead to
maximizing the throughout of the FiWi network. However,
the authors of [5] noted that minimization of the wireless
hop count does not always lead to throughput maximization.
Accordingly, the path length metric proposed by them in
[5] pays considerable importance to the trafï¬c intensity at a
generic FiWi network node. The results presented in this paper
are motivated by the path length metric proposed in [5].
III. P ROBLEM F ORMULATION
In the classical path problem, each edge e â E of the graph
G = (V, E), has a weight w(e) associated with it and if there
is a path P from the node v0 to vk in the graph G = (V, E)
w

w

w

w

v0 â1 v1 â2 v2 â3 v3 . . . âk vk
then the path length or the distance between the nodes v0 and
vk is given by
w(Pv0 ,vk ) = w1 + w2 + Â· Â· Â· + wk
However, in the path length metric proposed in [5] for
optical-wireless FiWi networks [1], [2], [3], the contribution
of ei to the path length computation depends not only on
the weight wi , but also on the weights of the other edges
that constitute the path. In the following section, we discuss
this metric and a variation of it. We also also formulate the
multipath computation problem using this metric.
The Optimized FiWi Routing Algorithm (OFRA) proposed
in [5] computes the âlengthâ (or weight) of a path P from v0
to vk using the following metric




(wu ) + max (wu )
w (Pv0 ,vk ) = min
P

âuâP

âuâP

where wu represents the trafï¬c intensity at a generic FiWi
network node u, which may be an optical node in the ï¬ber
backhaul or a wireless node in wireless mesh front-end. In
order to compute shortest path using this metric, in our
formulation, instead of associating a trafï¬c intensity âweightâ
(wu ) with nodes, we associate them with edges. This can easily
be achieved by replacing the node u with weight wu with two
nodes u1 and u2 , connecting them with an edge (u1 , u2 ) and
assigning the weight wu on this edge. In this scenario, if there
is a path P from the node v0 to vk in the graph G = (V, E)
w

w

w

w

v0 â1 v1 â2 v2 â3 . . . âk vk
then the path length between the nodes v0 and vk is given by

w+ (Pv0 ,vk )

=
=

w1 + w2 + . . . + wk + max(w1 , w2 , . . . wk )
k

wi + maxki=1 wi
i=1

In the second metric, the length a path Pv0 ,vk :
v0 âv1 âv2 â . . . âvk , between the nodes v0 and vk is given
by

wÌ(Pv0 ,vk )

=
=

k

i=1
k


wi + CN T (Pv0 ,vk ) â max(w1 , w2 , . . . wk )
wi + CN T (Pv0 ,vk ) â maxki=1 wi

i=1

where CN T (Pv0 ,vk ) is the count of the number of times
max (w1 , w2 , . . . wk ) appears on the path Pv0 ,vk . We study the
shortest path computation problems in FiWi networks using
the above metrics and provide polynomial time algorithms for
solution in subsections IV-A and IV-B.
If wmax = max(w1 , w2 , . . . wk ), we refer to the corresponding edge (link) as emax . If there are multiple edges having
the weight of wmax , we arbitrarily choose any one of them as
emax . It may be noted that both the metrics have an interesting
property in that in both cases, the contribution of an edge e
on the path length computation depends not only on the edge
e but also on every other edge on the path. This is so, because
if the edge e happens to be emax , contribution of this edge
in computation of w+ (Pv0 ,vk ) and wÌ(Pv0 ,vk ) will be 2 â w(e)
and CN T (Pv0 ,vk ) â w(e) respectively. If e is not emax , then
its contribution will be w(e) for both the metrics.
As multipath routing provides an opportunity for higher
throughput, lower delay, and better load balancing and resilience, its use have been proposed in ï¬ber networks [6],
wireless networks [7] and recently in integrated ï¬ber-wireless
networks [8], [9]. Accordingly, we study the problem of
computing a pair of edge disjoint paths between a sourcedestination node pair s and d, such that the length of the
longer path (path length computation using the ï¬rst metric)
is shortest among all edge disjoint path pairs between the
nodes s and d. In subsection IV-C we prove that this problem
is NP-complete, in subsection IV-D, we provide an optimal
solution for the problem using integer linear programming,
in subsections IV-E and IV-F we provide two approximation
algorithms for the problem with a performance bound of 4
and 2 respectively, and in subsection IV-F we provide results
of experimental evaluation of the approximation algorithms.
IV. PATH P ROBLEMS IN F I W I N ETWORKS
In this section, we present (i) two different algorithms for
shortest path computation using two different metrics, (ii)
NP-completeness proof for the disjoint path problem, (iii)
two approximation algorithms for the disjoint path problem,
and (iv) experimental evaluation results of the approximation
algorithms.

132

It may be noted that, in both metrics w+ (Pv0 ,vk ) and
wÌ(Pv0 ,vk ), we call an edge e â Pv0 ,vk crucial, if w(e) =
maxki=1 w(e ), âe â Pv0 ,vk .
A. Shortest Path Computation using Metric 1
It may be recalled that the path length
k metric used in this
case is the following: w+ (Pv0 ,vk ) = i=1 wi + maxki=1 wi . If
k
the path length metric was given as w(Pv0 ,vk ) = i=1 wi ,
algorithms due to Dijkstra and Bellman-Ford could have been
used to compute the shortest path between a source-destination
node pair. One important property of the path length metric
that is exploited by Dijkstraâs algorithm is that âsubpath of a
shortest
path is shortestâ. However, the new path length metric
k
k
i=1 wi +maxi=1 wi does not have this property. We illustrate
this with the example below.
Consider two paths P1 and P2 from the node v0 to v3 in the
w
w
w
graph G = (V, E), where P1 : v0 â1 v1 â2 v2 â3 v3 and P2 :
w4
w5
w3
v0 â v4 â v2 â v3 . If w1 = 0.25, w2 = 5, w3 = 4.75, w4 =
2, w5 = 4, the length of the path P1 , w+ (P1 ) = w1 +w2 +w3 +
max(w1 , w2 , w3 ) = 0.25 + 5 + 4.75 + max(0.25, 5, 4.75) = 15
and the length of the path P2 , w+ (P2 ) = w4 + w5 + w6 +
max(w4 , w5 .w6 ) = 2 + 4 + 4.75 + max(2, 4, 4.75) = 15.5.
Although P1 is shortest path in this scenario, the length of
w
w
its subpath v0 â1 v1 â2 v2 is 0.25 + 5 + max (0.25, 5) =
10.25, which is greater than the length of a subpath of P2
w
w
v0 â4 v4 â5 v2 2 + 4 + max (2, 4) = 10, demonstrating that
the assertion that âsubpath of a shortest path is shortestâ no
longer holds in this path length metric.
As the assertion âsubpath of a shortest path is shortestâ no
longer holds in this path length metric, we cannot use the
standard shortest path algorithm due to Dijkstra in this case.
However, we show that we can still compute the shortest path
between a source-destination node pair in polynomial time by
repeated application of the Dijkstraâs algorithm. The algorithm
is described next.
For a given graph G = (V, E), w.l.o.g, we assume |V | = n
and |E| = m. Deï¬ne Ge as subgraph of G by deleting edges
whose weight is greater than w(e).
Also, as Dijkstraâs algorithm does, we need to maintain
distance vector. We deï¬ne distv be distance (length of shortest
path) from s to v, Î v be predecessor of v and maxedgev be
weight of the crucial edge from s to v via the shortest path,
ansv be optimal solution (length) from s to v.
Different Ge can be treated as different layers of the
G. For any path P , we deï¬ne the function eâ (P ) as the
crucial edge along P . It is easy to observe that if
Pd is the
optimal path from s to node d then w(Pd ) =
eâP w(e)
and w+ (Pd ) = w(Pd ) + w(eâ (Pd )). It may be noted that
henceforth, we shorten Ps,d to Pd , because we consider that
the source is ï¬xed while the destination d is variable.
Lemma 1. w(Pd ) is minimum in Geâ (Pd ) .
Proof: It is obvious that Pd still exists in Geâ (Pd ) , since
edges on Pd are not abandoned. Suppose Pd is not shortest,
then there must be another path Pd s.t. w(Pd ) < w(Pd ).
Noting that the crucial edge on Pd , namely e , is no longer than

Algorithm 1 Modiï¬ed Dijkstraâs Algorithm
1: Initialize ansv = â for for all v â V
2: sort all edges according to w(e) in ascending order
3: for i = 1 to m do
4:
Initialize distv = â, Î v = nil, maxedgev = 0 for
all v â V
5:
dists = 0
6:
Q = the set of all nodes in graph
7:
while Q is not empty do
8:
u = Extract-Min(Q)
9:
for each neighbor v of u do
10:
if eu,v â E(Gei ) then
11:
t = MAX {maxedgeu , w(eu,v )}
12:
if distu + w(eu,v ) < distv then
13:
distv = distu + w(eu,v )
14:
maxedgev = t
15:
Î v = u
16:
else if distu + w(eu,v ) == distv then
17:
if maxedgev > t then
18:
maxedgev = t
19:
Î v = u
20:
end if
21:
end if
22:
end if
23:
end for
24:
end while
25:
for each node v do
26:
ansv = min{ansv , distv + maxedgev }
27:
end for
28: end for
eâ (Pd ) since they both belong to Geâ (Pd ) . Hence w+ (Pd ) =
w(Pd )+w(e ) < w(Pd )+w(eâ (Pd )) = w+ (Pd ), contradicting
Pd is optimal.
Lemma 2. Modiï¬ed Dijkstraâs Algorithm (MDA) computes
shortest path while keeping the crucial edge as short as
possible in every iteration.
Proof: Line 4 to 24 works similar to the standard Dijkstraâs algorithm does. Besides, when updating distance, MDA
also updates the crucial edge to guarantee that it lies on the
path and when there is a tie, MDA will choose the edge with
the smaller weight.
Theorem 1. Modiï¬ed Dijkstraâs Algorithm computes optimal
solution for every node v in O(m(n + m)logn) time.
Proof: Lemma 1 indicates for any node v â V , optimal
solution can be obtained by enumerating all possible crucial
edges eâ (Pv ) and computing shortest path on Geâ (Pv ) . By sorting all edges in nondecreasing order, every subgraph Geâ (Pv )
is considered and it is shown in lemma 2, MDA correctly
computes shortest path for every node v in every Geâ (Pv ) .
Then optimal solution is obtained by examining all shortest
path using the w() metric plus the corresponding crucial edge.
Dijkstraâs algorithm runs O((n + m)logn) time when using

133

binary heap, hence MDA runs in O(m(n+m)logn) time when
considering all layers.
B. Shortest Path Computation using Metric 2
Given a path P , let eâ (P ) be the crucial edge along the
P and CN T (P ) be the number of occurrence of such edge.
Now
a path Q, such that wÌ(P ) =
 our objective becomes to ï¬nd
â
w(e)
+
CN
T
(Q)
â
w(e
(Q))
is minimum.
eâQ
The layering technique can also be used in this problem.
However, shortest path under a ceratin layer may not become
a valid candidate for optimal solution. Here, we introduce a
dynamic programming algorithm that can solve the problem
optimally in O(n2 m2 ) time.
Input is a weighted graph G = (V, E), |V | = n, |E| = m
with a speciï¬ed source node s. In this paper, we only
consider nonnegative edge weight. As shown before, we use
Ge to represent the residue graph by deleting edges longer
than e in G. Different from MDA1, in order to consider
the number of crucial edges, distv is replaced by an array
dist0v , dist1v , ....distnv . One can think distcv be the shortest
distance from s to v by going through exactly c crucial edges
and possibly some shorter edges. Similarly, we replace Î v by
Î cv , 0 â¤ c â¤ n. Each Î cv records predecessor of v for the
path corresponding to distcv . Lastly, ansv is used as optimal
solution from s to v.
Lemma 3. If Pv is the best path from s to v, i.e., wÌ(Pv ) is
minimum among all s-v path, then Pv is computed in Geâ (Pv )
CN T (Pv )
and distv
= w(Pv ).
Proof: By deï¬nition, Pv exists in Geâ (Pv ) and
CN T (Pv ) â¥ 1 since any path should go through at least one
crucial edge. Noting wÌ(Pv ) = w(Pv )+CN T (Pv )âw(eâ (Pv )),
on one hand if we treat CN T (Pv ) as a ï¬xed number, then we
need to keep w(Pv ) as small as possible. Inspired by idea
of bellman-ford algorithm, we can achieve it by enumerating
|Pv |, i.e., number of edges on Pv . On the other hand, we need
to keep tracking number of crucial edges as well. Hence, distcv
is adopted to maintain such information, superscript c reï¬ects
exact number of crucial edges. From line 12 to line 25, distcv
is updated either when it comes from a neighbor who has
already witnessed c crucial edges or it comes from a neighbor
with c â 1 crucial edges and the edge between is crucial. In
either case, node v gets a path, say P  , with exact c crucial
edges on it and w(P ) is minimum. At last, Pv can be selected
by enumerating number of crucial edges and that is what line
30 to 32 does.
Lemma 4. Maxedge Shortest Path Algorithm(MSPA) runs in
O(n2 m) time for each Ge .
Proof: We can apply similar analysis of bellman-ford
algorithm. However, we need to update distcv array, it takes
extra O(n) time for every node v in every iteration when
enumerating |Pv |. Hence, total running time is O(n2 m).
Theorem 2. MSPA computes optimal path for every v â V
in O(n2 m2 ) time.

Algorithm 2 Maxedge Shortest Path Algorithm
1: Initialize ansv = â for for all v â V
2: sort all edges according to w(e) in ascending order, say
e1 , e2 , ..., em after sorting
3: for i = 1 to m do
4:
Initialize distcv = â, Î cv = nil for all v â V and all
0â¤câ¤n
5:
dist0s = 0
6:
for j = 1 to n â 1 do
7:
for k = 0 to j do
8:
for every node v â V do
9:
if distkv = â then
10:
continue
11:
end if
12:
for every neighbor u of v do
13:
if w(eu,v ) > w(ei ) then
14:
continue
15:
else if w(eu,v ) == w(eâ ) then
16:
if distkv + w(eu,v )
<
then
distk+1
u
17:
distk+1
= distkv +
u
w(eu,v )
18:
Î k+1
=v
u
19:
end if
20:
else
21:
if distkv + w(eu,v ) < distku
then
22:
distku = distkv +
w(eu,v )
23:
Î ku = v
24:
end if
25:
end if
26:
end for
27:
end for
28:
end for
29:
end for
30:
for i = 1 to n â 1 do
31:
ansv = min{ansv , distiv + i â w(ei )}
32:
end for
33: end for

Proof: By Lemma 3, if Pv is obtained when computing
Geâ Pv . Then, by considering all possible Geâ , we could get
Pv in one of these layering. It takes O(m) to generate all Geâ ,
by Lemma 4, MSPA runs in v â V in O(n2 m2 ) time.
C. Computational Complexity of Disjoint Path Problem
In this section, we study edge disjoint path in optical
wireless network. By reduction from well known Min-Max
2-Path Problem, i.e., min-max 2 edge disjoint path problem
under normal length measurement, we show it is also
NP-complete if we try to minimize the longer path when w+
length is applied. Then we give an ILP formulation to solve
this problem optimally. At last, we provide two approximation
algorithm, one with approximation ratio 4, running time

134

O((m + n)logn), the other one with approximation ratio 2
while running time is O(m(m + n)logn).

D. Optimal Solution for the Disjoint Path Problem
Here, we give an ILP formulation for MinMax2OWFN.
ILP for MinMax2OWFN

Min-Max 2 Disjoint Path Problem (MinMax2PP)
Instance: An undirected graph G = (V, E) with a positive
weight w( e) associated with each edge e â E, a source node
s â V , a destination node t â V , and a positive number X.
Question: Does there exist a pair of edge disjoint paths P1
and P2 from s to d in G such that w(P1 ) â¤ w(P2 ) â¤ X?

min
s.t.

MP



The MinMax2PP problem is shown to be NP-complete in
[10]. With a small modiï¬cation, we show NP-completeness
still holds if w+ length measurement is adopted.

fi,j,1 â

fj,i,1 =

(i,j)âE

(j,i)âE





fi,j,2 â

(i,j)âE

Min-Max 2 Disjoint Path Problem in Optical Wireless
Networks (MinMax2OWFN)
Instance: An undirected graph G = (V, E) with a positive
weight w( e) associated with each edge e â E, a source node
s â V , a destination node t â V , and a positive number X.
Question: Does there exist a pair of edge disjoint paths P1
and P2 from s to t in G such that w+ (P1 ) â¤ w+ (P2 ) â¤ X  ?



fj,i,2 =

(j,i)âE

â§
âª
â¨
âª
â©
â§
âª
â¨
âª
â©

1
â1
0

i=s
i=t
otherwise

1
â1
0

i=s
i=t
otherwise

fi,j,1 + fi,j,2 â¤ 1
w1 â¥ fi,j,1 â w(i, j)

â(i, j) â E

w2 â¥ fi,j,2 â w(i, j)

fi,j,1 â w(i, j)
M P â¥ w1 +

â(i, j) â E

â(i, j) â E

(i,j)âE



M P â¥ w2 +

Theorem 3. The MinMax2OWFN is NP-complete

fi,j,2 â w(i, j)

(i,j)âE

Proof: Evidently, MinMax2OWFN is in NP class, given
two edge joint path P1 and P2 , we can check if w+ (P1 ) â¤
w+ (P2 ) â¤ X  in polynomial time.
We then transfer from MinMax2PP to MinMax2OWFN.
Let graph G = (V, E) with source node s , destination t and
an integer X be an instance of MinMax2PP, we construct an
instance Gâ of MinMax2OWFN in following way.
1) Create an identical graph G with same nodes and edges
in G.
2) Add one node s0 to G .
3) Create two parallel edges e01 , e02 between s0 and s,
w(e01 ) = w(e02 ) = maxeâG(E) w(e)
4) Choose s0 to the source node in G and t to be the
destination.
5) Set X  = X + 2w(e01 )
It is easy to see, the construction takes polynomial time.
Now we need to show a instance of MinMax2OWFN have
two edge disjoint paths from s0 to t with length at most X  if
and only if the corresponding instance have two edge disjoint
paths from s to t with length at most X.
Suppose there are two edge disjoint paths P1 and P2 from
s0 to t in G , such that w+ (P1 ) â¤ w+ (P2 ) â¤ X  . By the
way we construct G , P1 and P2 must go through e01 and
e02 . W.l.o.g. we say e01 â P1 and e02 â P2 . Since w(e01 ) =
w(e02 ) = maxeâE(G ) {w(e)}, therefore e01 and e02 are the
crucial edge on P1 and P2 respectively. Hence, P1 â e01 and
P2 âe02 are two edge disjoint path in G, with length no greater
than X  â 2w(e01 ) = X.
Conversely, now suppose P1 and P2 are two edge joint paths
in G satisfying w(P1 ) â¤ w(P2 ) â¤ X. We follow the same
argument above, P1 + e01 and P2 + e02 are two desired paths,
with length not exceeding X + 2w(e01 ) = X  .

fi,j,1 = {0, 1},

fi,j,2 = {0, 1}

â(i, j) â E

The following is a brief description of this ILP formulation.
The ï¬rst two equation represent ï¬ow constraint as normal
shortest path problem does. fi,j,1 = 1 indicates path P1 goes
through edge (i, j), and 0 otherwise. So it is with fi,j,2 and
path P2 . Constraint 3 ensures two edges are disjoint, since
fi,j,1 and fi,j,2 cannot both be 1 at the same time. w1 , w2 act
as the weights of the crucial edges on P1 and P2 respectively.
Finally, we deï¬ne M P to be the maximum of w+ (P1 ) and
w+ (P2 ) and therefore try to minimize it.
E. Approximation Algorithm for the Disjoint Path Problem,
with approximation factor 4
Next we propose a 4-approximation algorithm which runs
in O((n + m)logn) time.
Given G = (V, E) with source s and destination t, the idea
of approximation algorithm is to ï¬nd two disjoint P1 and P2
such that w(P1 ) + w(P2 ) is minimized. Such P1 and P2 can
be found either using min cost max ï¬ow algorithm or the
algorithm due to Suurballe presented in [11]. And we need to
show both w+ (P1 ) and w+ (P2 ) are at most four times of the
optimal solution.
Algorithm 3 MinMax2OWFN Approximation Algorithm 1 (MAA1)
1: Run Suurballeâs algorithm on G, denote P1 , P2 be two
resulting path.
2: Compute w + (P1 ) and w + (P2 ).
3: Output max{w + (P1 ), w + (P2 )}.

135

Lemma 5. For any path P , w+ (P ) â¤ 2w(P ).
Proof: By deï¬nition, w+ (P ) = w(P ) + w(eâ (P )). Since
w(e (P )) â¤ w(P ), then w+ (P ) â¤ 2 â w(P ).
â

Lemma 6. If P1 and P2 are two edge joint path from s to
t such that w(P1 ) + w(P2 ) is minimum, then w+ (P1 ) and
w+ (P2 ) are at most four times of the optimal solution.
Proof: Say opt is the optimal value of a
M inM ax2OW F N instance and Q1 ,Q2 are two s â t
edge disjoint path in one optimal solution. W.l.o.g, we may
suppose w+ (P1 ) â¥ w+ (P2 ) and w+ (Q1 ) â¥ w+ (Q2 ). Let
w(P1 ) + w(P2 ) = p and w(Q1 ) + w(Q2 ) = q, by assumption,
p â¤ q. Also, we have w+ (P1 ) = w(P1 ) + eâ (P1 ) â¤ 2p,
opt = w+ (Q1 ) = w(Q1 ) + eâ (Q1 ) > 2q . Hence,
+
w+ (P2 )
(P1 )
2p
â¤ w opt
< q/2
â¤4
opt
Theorem 4. MAA1 is a 4-approximation algorithm running
in O((n + m)logn) time and 4 is a tight bound.
Proof: By Lemma 5 and 6, MAA1 has approximation
ratio at most 4.Then we show MAA1 has approximation at
least 4 for certain cases. Consider the following graph.

Algorithm 4 MinMax2OWFN Approximation Algorithm
2(MAA2)
1: set ans = â
2: for every Ge of G do
3:
Run Suurballeâs algorithm on Ge , denote P1 , P2 be
two resulting path.
4:
Compute w+ (P1 ) and w+ (P2 ).
5:
ans = min{ans, max{w+ (P1 ), w+ (P2 )}}.
6: end for
7: Output ans.
w(P1 )+w(e )
max{w+ (Q1 ), w+ (Q2 )} < 2. We
w(P1 )+w(e )
Suppose max{w
+ (Q ), w + (Q )} â¥ 2,
1
2

w(e ). It sufï¬ces to show

prove

it by contradiction.

then

w(P1 ) + w(e ) â¥ w+ (Q1 ) + w+ (Q2 )
Which follows,
w(P1 ) + w(e ) â¥ w(Q1 ) + w(eâ (Q1 )) + w(Q2 ) + w(eâ (Q2 ))
By deï¬nition, e is one of eâ (Q1 ), eâ (Q2 ). Hence,
w(P1 ) > w(Q1 ) + w(Q2 )
It is impossible since w(P1 ) + w(P2 ) is minimum in layer
G e .
Theorem 5. MAA2 is a 2-approximation algorithm running
in O(m(n + m)logn) time and 2 is a tight bound.

It is easy to check, P1 = {s â t}, P2 = {s â r â t} are
two edge disjoint path with minimum length 2k+2, w+ (P1 ) =
4k > w+ (P2 ) = 3. However, let Q1 = {s â u1 â u2 â
... â ukâ1 â uk â r â t}, Q2 = {s â r â v1 â v2 â
... â vkâ1 â vk â t}, then w(Q1 ) + w(Q2 ) = 2k + 4 while
+
4k
1)
w+ (Q1 ) = w+ (Q2 ) = k + 3. ww+(P
Q1 = k+3 â 4 when k is
sufï¬ciently large. Hence, 4 is a tight bound for MAA1.
We need O((n + m)logn) time running Suurballeâs algorithm and O(n) time computing w+ (P1 ) and w+ (P2 ).
Therefore total running time is O((n + m)logn).
F. Approximation Algorithm for the Disjoint Path Problem,
with approximation factor 2
In MAA1, layering technique is not used and we only
consider the original graph. However, by taking all Ge of G
into account, we can have a better approximation ratio.
Say Q1 , Q2 are two disjoint paths in one optimal solution.
Let e = max{eâ (Q1 ), eâ (Q2 )} and P1 , P2 be the resulting
paths when computing layer Ge ; w.l.o.g, we may assume
w(P1 ) > w(P2 ). Also, let anse = max{w+ (P1 ), w+ (P2 )}.
Lemma 7. anse < 2 max{w+ (Q1 ), w+ (Q2 )}.
Proof: Noting that w(eâ (P1 )) â¤ w(e ) and w(eâ (P2 )) â¤
w(e ) since they both belong to Ge . Then anse â¤ w(P1 ) +


Proof: By Lemma 7, in one of the layer, we guarantee
to have a 2-approximation solution. Since we take minimum
outcome among all layers, the ï¬nal result is no worse than
twice of the optimal solution. Now we need to show there
exists certain case, such that MAA2 is no good than twice of
the optimal solution. Consider the following graph

There is only one layer, and P1 = {s â x1 â x2 â
... â x2kâ1 â x2 k â t}, P2 = {s â r â t} are two
edge disjoint path with minimum length 2k + 3, w+ (P1 ) =
2k + 2 > w+ (P2 ) = 3. Again, set Q1 = {s â u1 â u2 â
... â ukâ1 â uk â r â t}, Q2 = {s â r â v1 â v2 â
... â vkâ1 â vk â t}, then w(Q1 ) + w(Q2 ) = 2k + 4 while
+
2k+2
1)
w+ (Q1 ) = w+ (Q2 ) = k + 3. ww+(P
Q1 = k+3 â 2 when k is
sufï¬ciently large. Hence, 2 is a tight bound for MAA2.
Finally, it is easy to see that the running time is O(m(n +
m)logn).

136

S
node
14
18
1
18
20
10
1
14
20
10
18
1
20
14
10
20
5

D
node
2
8
6
4
3
3
11
6
7
5
12
20
13
19
17
16
11

Opt
Sol
47
46
28
50
40
27
35
50
38
36
22
46
26
29
36
29
40

Approx
Sol 1
55
46
28
58
40
27
35
52
38
38
22
52
26
29
36
29
48

Approx
ratio 1
1.17
1
1
1.16
1
1
1
1.04
1
1.05
1
1.13
1
1
1
1
1.2

Approx
Sol 2
55
46
28
57
40
27
35
52
38
38
22
52
26
29
36
29
48

Approx
ratio 2
1.17
1
1
1.14
1
1
1
1.04
1
1.05
1
1.13
1
1
1
1
1.2

Fig. 1.

TABLE I
C OMPARISON OF THE A PPROXIMATE SOLUTIONS WITH THE O PTIMAL
SOLUTION FOR THE ARPANET GRAPH

The ARPANET graph with 20 nodes and 32 links

approximation algorithms. Both the approximation algorithms
have a constant factor approximation bound. However, there is
a trade-off between the quality of the solution (approximation
bound) and the execution time. Finally, we show that both the
approximation algorithms obtain near optimal results through
simulation using the ARPANET topology.

G. Experimental Results for the Disjoint Path Problem
In this section, we present the results of simulations for
comparing the performance of our approximation algorithms
with the optimal solution when w+ () metric is applied.
The simulation experiments have been carried out on the
ARPANET topology (as shown in Fig 1 with nodes and links
shown in black) which has twenty nodes and thirty two links.
The weights of the links have been randomly generated and
lie in the range of two and eleven (as shown in red in Fig
1) and we consider the graph to be undirected. The results of
the comparison is presented in Table I. We have compared the
lengths of the longer of the two edge disjoint paths computed
by the optimal and the approximate solutions for seventeen
different source-destination pairs. It may be noted that for
almost 65% of the cases, the approximate algorithms obtain
the optimal solution. In the remaining cases, the approximate
solutions lie within a factor of 1.2 of the optimal solution
Thus, even though the approximation ratio in the worst case
are proven to be 4 and 2, in practical cases, it is within 1.2.
From these experimental results, we can conclude that the
approximation algorithms produce optimal or near optimal
solutions in majority of the cases. It may be noted that the
two approximation algorithms perform in a similar fashion
in the ARPANET graph, however, as proven theoretically,
the two approximation algorithms differ in their worst case
approximation ratio.
V. C ONCLUSION

R EFERENCES
[1] N. Ghazisaidi, M. Maier, and C. M. Assi, âFiber-Wireless (FiWi) Access
Networks: A Surveyâ, IEEE Communications Magazine, vol. 47, no. 2,
pp 160-167, Feb. 2009.
[2] N. Ghazisaidi, and M. Maier, âFiber-Wireless (FiWi) Access Networks:
Challenges and Opportunitiesâ, IEEE Network, vol. 25, no. 1, pp 36-42,
Feb. 2011.
[3] Z. Zheng, J. Wang, X. Wang, âONU placement in ï¬ber-wireless (FiWi)
networks considering peer-to-peer communicationsâ, IEEE Globecom, 2009.
[4] Z. Zheng, J. Wang, X. Wang, âA study of network throughput gain
in optical-wireless (FiWi) networks subject to peer-to-peer commuincationsâ, IEEE ICC, 2009.
[5] F. Aurzada, M. Levesque, M. Maier, M. Reisslein, âFiWi Access
Networks Based on Next-Generation PON and Gigabit-Class WLAN
Technologies: A Capacity and Delay Analysisâ, IEEE/ACM Transactions
on Networking, to appear.
[6] A. Sen, B.Hao . B. Shen , L.Zhou and S. Ganguly, âOn maximum
available bandwidth through disjoint pathsâ, Proc. of IEEE Conf. on
High Performance Switching and Routing, 2005.
[7] M. Mosko, J.J. Garcia-Luna-Aceves, âMultipath routing in wireless
mesh networksâ, Proc. of IEEE Workshop on Wireless Mesh Networks, 2005.
[8] J. Wang, K. Wu, S. Li and C. Qiao ,âPerformance Modeling and Analysis
of Multi-Path Routing in Integrated Fiber-Wireless (FiWi) Networksâ,
IEEE Infocom mini conference, 2010.
[9] S. Li, J. Wang, C. Qiao, Y. Xu ,âMitigating Packet Reordering in
FiWi Networksâ, IEEE/OSA Journal of Optical Communications and
Networking, vol. 3, pp.134-144, 2011.
[10] C. Li, S.T. McCormick and D.Simchi-Levi, âComplexity of Finding Two
Disjoint Paths with Min- Max Objectiveâ, Discrete Applied Mathematics, vol. 26, pp. 105-115, 1990.
[11] J. W. Suurballe, âDisjoint paths in a networkâ, Networks, vol. 4, pp. 125145, 1974.

In this paper, we study the shortest path problem in FiWi
networks. Based on the path length metrics proposed in [3],
[5], we present polynomial time algorithms for the single
path scenario. In the disjoint path scenario, we prove that the
problem of ï¬nding a pair of disjoint paths, where the length
of the longer path is shortest, is NP-complete. We provide an
ILP solution for the disjoint path problem and propose two

137

Partitioning Signed Bipartite Graphs for
Classification of Individuals and Organizations
Sujogya Banerjee, Kaushik Sarkar, Sedat Gokalp,
Arunabha Sen, and Hasan Davulcu
Arizona State University
P.O. Box 87-8809, Tempe, AZ, 85281 USA
{sujogya,kaushik.sarkar,sedat.gokalp,asen,hdavulcu}@asu.edu

Abstract. In this paper, we use signed bipartite graphs to model opinions expressed by one type of entities (e.g., individuals, organizations)
about another (e.g., political issues, religious beliefs), and based on the
strength of that opinion, partition both types of entities into two clusters. The clustering is done in such a way that support for the second
type of entity by the first within a cluster is high and across the cluster
is low. We develop an automated partitioning tool that can be used to
classify individuals and/or organizations into two disjoint groups based
on their beliefs, practices and expressed opinions.

1

Introduction

The goal of the Minerva1 project, currently underway at Arizona State University is to increase understanding of movements within Muslim communities
actively working to counter violent extremism. As a part of this study, we have
collected over 800,000 documents from web sites various organizations in Indonesia. Based on the support and opposition of certain beliefs and practices, we can
partition the set of organizations O into two groups O1 and O2 and the set of
beliefs and practices B into two groups, B1 and B2 , such that organizations in O1
support B1 and oppose B2 , while the organizations O2 support B2 and oppose
B1 . With the domain knowledge of the social scientists in our team regarding
the beliefs and practices of Indonesian community, we can then label one group
as being radical and other as counter-radical.
Although the motivation for our work was driven by Minerva, the the problem
that is being addressed in this paper is much broader in nature. In the mathematical sociology community, the problem is known as the Signed two-mode network
partitioning problem [1]. In its mathematical abstraction, the problem is specified by a bipartite graph G = (U âª V, E) and label function Ï : E â {P, N }.
The node sets U and V may be representing the set of organizations O and the
set of beliefs B respectively. If the label of an edge from oi â O to bj â B is
P , it implies oi supports (or has positive opinion) about bj . If the label of an
edge is N , it implies oi opposes (or has negative opinion) about bj . The goal of
1

A project sponsored by the U.S. Department of Defense

(a)

(b)

Fig. 1: Partitioning of the node set U and V with the desired goal
the partitioning problem is to divide the node sets U and V into two subsets
(U1 , U2 ) and (V1 , V2 ) respectively, such that
1. number of P edges (positive opinion or support) between nodes within block
1 (P11 between U1 and V1 ) and block 2 (P22 between U2 and V2 ) is high,
2. number of P edges between nodes across block 1 and block 2 (edges P12
between U1 and V2 and P21 between U2 and V1 ) is low,
3. number of N edges (negative opinion or opposition) between nodes within
block 1 (N11 between U1 and V1 ) and block 2 (N22 between U2 and V2 ) is
low and
4. number N edges between nodes across block 1 and block 2 (edges N12 between U1 and V2 and N21 between U2 and V1 ) is high.
The goal of partitioning is depicted in Fig. 1, where the green edges indicate
support (i.e, P edges) and the red edges indicate opposition (i.e, N edges). We
can realize these goals by maximizing [(P 11 + P 22 + N 12 + N 21) â (P 12 + P 21 +
N 11 + N 22)].
Signed two-mode network partitioning problem can be applied in a multitude
of domains, where the node sets U and V can represent different entities. For
example, (i) U and V may represent the members of the U.S. Senate/House of
Representatives and the bills before the senate/house of representatives where
they cast their votes, either supporting or opposing the bill; (ii) U and V may
represent the political blogs/bloggers and various issues confronting the nation,
where they express their opinions either supporting or opposing issues. Clearly,
availability of an automated tool that will co-cluster the entities represented by
U and V , will be valuable to individuals and organizations that need a coarse
grain (two-modal) partitioning of the data set represented by the node set U
and V . This tool can help classify individuals or organizations as radicals vs.
counter-radicals, or liberals vs. conservatives or violent vs. non-violent, etc.
The main contribution of this effort is the development of a fast automated
tool (and associated algorithms) for co-clustering the entities represented by the
node sets U and V . We first compute an optimal solution of the partitioning
problem using an integer linear program to be used as a benchmark for our
heuristic solution. We then develop a heuristic solution and compare its performance using three real data sets. The real data sets include voting records
of the Republican and Democratic members of the 111th US Congress and the
opinions expressed in top twenty two liberal and conservative blogs. In all these

data sets our partitioning tool produces high quality solution (i.e., with low misclassification) at a low cost (in terms of computation time). To the best of our
knowledge, our Minerva research group is the first to present an efficient computational technique for partitioning of signed bipartite graph and apply it to
some real data sets.

2

Related works

As the literature on clustering, classification and partitioning is really vast, due
to page limitations, we only refer to the ones that are most relevant to this
paper [1â4, 8, 7]. The two key features of the partitioning problem addressed in
this paper are (i) the graph is bipartite and (ii) the weights on the edges are
signed (i.e., the weights are both positive and negative). Simultaneous clustering
of two sets of entities (represented by two sets of nodes in the bipartite graph)
was considered in the context of document clustering in [4, 8]. In these studies one
set of entities are the documents and the other set is terms or words. Although
these efforts study the bipartite graph partition problem, they are distinctly
different from our study in one respect. In our study, the edge weights are signed,
whereas the edges weights considered in [4, 8] are unsigned. Graph partitioning
problem with signed edge weights was studied in [2, 3]. However, these studies are
also distinctly different from our study in that, while they focus on partitioning
general (i.e., arbitrary) graphs, we focus our attention to partitioning bipartite
graphs. The study that comes closest to our study is [1, 7], where attention is
focused on partitioning of a signed bipartite graphs. However, neither [1] nor
[7] present any efficient algorithm to solve the partitioning problem in signed
bipartite graph.

3

Problem Formulation

In this section we formally define the partitioning problem.
Signed Bipartite Graph Partition Problem (SBGPP): An edge labeled weighted
bipartite graph G = (U âª V, E) where U = {u1 , u2 , . . . , un } represents entities
of type I and V = {v1 , v2 , . . . , vm } represents entities of type II. Each edge
(u, v) â E has two functions associated with it: (i) label function Ï : E â {P, N },
which indicates the type of opinion (positive or negative), and (ii) weight function
w : E â Z, which indicates the strength of that opinion. AN = [wn (u, v)] and
AP = [wp (u, v)] are the weighted adjacency matrix for edges with label N and P
respectively. If the node set U is partitioned into U1 and U2 and V is partitioned
into V1 and V2 , the strength of the positive and negative opinions of the entities
of type I regarding the entities of type II are defined as follows:
For all edges (u, v) â E,
X X
X X
X X
P11 =
wp (u, v), P12 =
wp (u, v), P22 =
wp (u, v)
P21 =

uâU1 vâV1

uâU1 vâV2

X X

X X

uâU2 vâV1

wp (u, v), N11 =

uâU1 vâV1

uâU2 vâV2

wn (u, v), N12 =

X X

uâU1 vâV2

wn (u, v)

N22 =

X X

wn (u, v), N21 =

uâU2 vâV2

X X

wn (u, v)

uâU2 vâV1

Problem: Find a partition of the node set U into U1 and U2 and V into V1 and V2
such that [(P 11 + P 22 + N 12 + N 21) â (P 12 + P 21 + N 11 + N 22)] is maximized.

4

Computational Techniques

In this section we give a mathematical programming technique to find the optimal solution for the SBGPP. Since computational time for finding optimal solution for large graphs is unacceptably high, we present a heuristic in subsequent
section to solve the SBGPP.
4.1

Optimal Solution for SBGPP

The goal of the SBGPP is to partition U into two disjoint sets U1 and U2
(similarly V into V1 and V2 ). For each node in u â U and each partition Ui , i =
1, 2, we use a variable bui . bui is 1 iff in u is in Ui . Similarly we define variable
pvi for all v â V . We will refer B1 = U1 âª V1 and B2 = U2 âª V2 as blocks 1 and
2 respectively.
V ariables: For each node u â U, v â V and each partition Ui , Vi , i = 1, 2
(
(
1, if node u is in partition Ui
1, if node v is in partition Vi
bui =
pvi =
0, otherwise.
0, otherwise.
The mathematical programming formulation is given as follows:
max

L=

2 X X
X

(wp (u, v) â wn (u, v))bui pvi

i=1 uâUi vâVi

+

2
X
X X

(wn (u, v) â wp (u, v))bui pvj

i,j=1 uâUi vâVj
i6=j

s.t

bu1 + bu2 = 1,

âu â U

(1)

pu1 + pu2 = 1,

âp â V

(2)

The objective function computes the objective value given by the expression L.
We want to maximize L. It may be noted that the above quadratic objective
function can easily be changed into a linear function by simple variable transformation [6]. Constraint 1 and 2 ensures that each node in U and V belongs to
one particular block.
4.2

Move-based Heuristics

We present a move-based heuristic to find an approximate solution of SBGPP.
The move-based heuristic is a variant of well known FM algorithm [5] for partitioning graphs. The algorithm starts with a random initial partition and iteratively moves nodes from one block to another such that the value of the objective

function is improved. The âgainâ of a node is defined as the value by which the
objective function increases if the node is moved from one block to the other.
In each iteration the node with the highest gain is moved from one block to
the other. In case of a tie a node is chosen arbitrarily. After a node is moved,
it is locked and is not moved until the next pass. The heuristic is presented in
Algorithm 1. It should be noted that original FM algorithm will not work for our
problem as SBGPP relates to signed bipartite graphs with a completely different
objective function and doesnât have any size constraints. As a result the node
gain computation routine Algorithm 2 is considerably different from the original
FM algorithm. Algorithm 1 runs for r different initial random partition of the
nodes to avoid the possibility of being stuck at a local maxima. In practice the
heuristic converges very fast, mostly in 2 to 3 passes.
Algorithm 1: Move-based Heuristic (MBH)

13

Input : A weighted signed bipartite graph H = (U âª V, E)
Output: A partition of the nodes U1 , U2 and V1 , V2 such that objective value L
is maximum
L ââ 0;
for i ââ 1 to r do
Generate a random partitioning of the nodes in U into U1 and U2 and nodes
in V into V1 and V2 ;
repeat
Compute gains of all nodes using Algorithm 2 ;
repeat
Among all the unlocked nodes select the node of highest gain. Move
the node to the other block and call it base node. Lock the base
node;
Update the node gains of all the free neighbors of the base node;
until Until all the nodes are locked ;
Change the current partition into a new partition that has the largest
value of the objective function in this pass ;
Unlock all the nodes;
until If the objective value Lâ² improves during the last pass;
if Lâ² â¥ L then Lâ² â L and save the current partition

14

return L and the final partition of nodes

1
2
3
4
5
6
7

8
9
10
11
12

5

Experimental Results and Discussions

To validate the effectiveness of our heuristic and benchmark its performance we
tested the heuristic both on synthetic and real world data. The real world data
consists of US Congress (SENATE, REP) and political blogosphere (BLOG) data
sets.
5.1

US Congress Data [SENATE, REP]

The US Congress has been collecting data since the very first congress of the US
history. This data has been encoded as XML files and publicly shared through

Algorithm 2: Node Gain Computation
Input : A weighted signed bipartite graph G = (U âª V, E)
Output: Gains of all nodes
foreach node u â U âª V do
gain(u) ââ 0;
// FBlock = "from block" of node u, ToBlock = "to block" of node
u, w(e) = weight of edge e and # = number
foreach edge e â E with l(e) = N of node u do
if # nodes of e in ToBlock is 0 then gain(u) â gain(u) + 2 â w(e);
if # nodes of e in FBlock is 1 then gain(u) â gain(u) â 2 â w(e);

1
2

3
4
5

foreach edge e â E with l(e) = P of node u do
if # nodes of e in ToBlock is 0 then gain(u) â gain(u) â 2 â w(e);
if # nodes of e in FBlock is 1 then gain(u) â gain(u) + 2 â w(e);

6
7
8

the govtrack.us project2 . From various types of data available at the project
site, we collected the roll call votes for the 111th US Congress which includes
The Senate and The House of Representatives and covers the years 2009-2010.
The 111th Senate data contains information about 108 senators and their votes
on 696 bills3 . The 111th Congress has 451 representatives and the data contains
their vote on 1655 bills.
We extracted the SENATE and REP data in adjacency matrices A|U|Ã|V | ,
with U vertices representing the congressmen, and the V vertices representing
the bills. The edge (ui , vj ), ui â U, vj â V has weight 1 if the congressman
ui votes âYeaâ for the bill vj , â1 if the congressman votes âNayâ, and 0 if he
did not attend the session. We have the original classification vector for both
the congressmen and the bills in terms of which party they represent (or which
party sponsored the bill). The first two columns of Table 1 provide information
about this data as well as the partitioning accuracies of the algorithms. Figure
2 depicts the partitioned vote matrices of the 111th US Congress data, where
rows representing the congressmen and the columns representing the bills. Also,
the light green color represents âYeaâ votes, and dark red represents âNayâ votes.
5.2

Blog Data [BLOG]

As Web 2.0 platforms gained popularity, it became easy for web users to be a
part of the web and express their opinions, mostly through blogs. Most blogs
are maintained by individuals, whereas there are also professional blogs with a
group of authors. In this study, we focus on a set of popular political liberal or
conservative blogs that have a clearly declared positions. These blogs contain
discussions about social, political, economic issues and related key individuals.
2
3

http://www.govtrack.us/data
Normally, each congress has 100 senators (2 from each state), however in many of
the congresses, there are unexpected changes on the seats caused by displacements
or deaths.

(a) 111th US House

(b) 111th US Senate

Fig. 2: Vote matrix of US Congress after partitioning

Table 1: Descriptive summaries of the graphs for each dataset with the Heuristic
accuracy

Vertices in V

111th US Senate
64 Democrat
42 Republican
Senator
696 Bills

111th US House
268 Democrat
183 Republican
Representatives
1655 Bills

Graph Density
Heuristic accuracy

88.36 %
100.00%

91.23 %
99.56%

Vertices in U

Political Blogosphere
13 Liberal
9 Conservative
Blogs
20 Liberal
14 Conservative People
39.04 %
98.21%

They express positive sentiment towards individuals whom they share ideologies
with, and negative sentiment towards the others. In these blogs, it is also common
to see criticism of people within the same camp, and also support for people from
the other camp.
In this experiment, we collected a list of 22 most popular liberal and conservative blogs from the Technorati4 rankings. For each blog, we fetched the posts
for the period of 6 months before the 2008 US presidential elections (May - October, 2008). We expected to have high intensity of the debates and discussions
and resulting in a bipolar clustering in the data. Table 2 shows the partial list
of blogs with their URLs, political camps and the number of posts for the given
period.
We use AlchemyAPI5 to run a named entity tagger to extract the people
names mentioned in the posts, and an entity-level sentiment analysis which provided us with weighted and signed sentiment (positive values indicating support,
and negative indicating opposition) for each person. This information was used
to synthesize a signed bipartite graph (the BLOG data), where the blogs and
people correspond to the two sets of vertices U and V . The aij values of the adjacency matrix A are the cumulative sum of sentiment values for each mention
of the person vj by the blog ui .
4
5

http://technorati.com
http://www.alchemyapi.com

To get a gold standard list of the most influential liberal and conservative
people, we used The Telegraph List6 for 2007. The third column of Table 1
provides information about this data as well as the partitioning accuracies of
the algorithm.
Table 2: Political Blogs
Blog name
URL
Huffington Post
http://www.huffingtonpost.com/
Daily Kos
http://www.dailykos.com/
Boing Boing
http://www.boingboing.net/
Crooks and Liars
http://www.crooksandliars.com/
Firedoglake
http://www.firedoglake.com/
Hot Air
http://hotair.com/
Reason - Hit and Run http://reason.com/blog
Little green footballs http://littlegreenfootballs.com/
Atlas shrugs
http://atlasshrugs2000.typepad.com/
Stop the ACLU
http://www.stoptheaclu.com/
Wizbangblog
http://wizbangblog.com/

6

Political view
Liberal
Liberal
Liberal
Liberal
Liberal
Conservative
Conservative
Conservative
Conservative
Conservative
Conservative

Size
3959
1957
1576
1497
1354
1579
1563
787
773
741
621

Conclusion

In this paper we study the problem of partitioning signed bipartite graph with
relevant application in political, religious and social domains. We provided a fast
heuristic to find the solution for this problem. We tested the high accuracy of
our heuristic on three sets of real data collected from political domain.

References
1. Andrej, M., Doreian, P.: Partitioning signed two-mode networks. Journal of Mathematical Sociology 33, 196â221 (2009)
2. Bansal, N., Blum, A., Chawla, S.: Correlation clustering. In: MACHINE LEARNING. pp. 238â247 (2002)
3. Charikar, M., Guruswami, V., Wirth, A.: Clustering with qualitative information.
In: Proceedings of the 44th Annual IEEE FOCS (2003)
4. Dhillon, I.S.: Co-clustering documents and word using bipartite spectral graph partitioning. In: Proceedings of the KDD. IEEE (2001)
5. Fiduccia, C., Mattheyses, R.: A linear-time heuristic for improving network partitions. In: Papers on Twenty-five years of electronic design automation. pp. 241â247.
ACM (1988)
6. Sen, A., Deng, H., Guha, S.: On a graph partition problem with application to vlsi
layout. Inf. Process. Lett. 43(2), 87â94 (1992)
7. Zaslavsky, T.: Frustration vs. clusterability in two-mode signed networks (signed
bipartite graphs) (2010)
8. Zha, H., He, X., Ding, C., Simon, H., Gu, M.: Bipartite graph partitioning and data
clustering. In: Proceedings of the 10th International Conference on Information and
Knowledge Management. pp. 25â32. ACM (2001)
6

The-top-US-conservatives-and-liberals.html

INL/EXT-06-11464

Critical Infrastructure Interdependency Modeling: A Survey of U.S. and International Research
P. Pederson D. Dudenhoeffer S. Hartley M. Permann August 2006

The INL is a U.S. Department of Energy National Laboratory operated by Battelle Energy Alliance

INL/EXT-06-11464

Critical Infrastructure Interdependency Modeling: A Survey of U.S. and International Research

P. Pedersona D. Dudenhoefferb S. Hartleyb M. Permannb
a

b

Technical Support Working Group, Washington D.C. Idaho National Laboratory

August 2006

Idaho National Laboratory Idaho Falls, Idaho 83415

Prepared for the Technical Support Working Group Under Work for Others Agreement 05734 Under DOE Idaho Operations Office Contract DE-AC07-05ID14517

ABSTRACT
"The Nation's health, wealth, and security rely on the production and distribution of certain goods and services. The array of physical assets, processes, and organizations across which these goods and services move are called critical infrastructures."1 This statement is as true in the U.S. as in any country in the world. Recent world events such as the 9-11 terrorist attacks, London bombings, and gulf coast hurricanes have highlighted the importance of stable electric, gas and oil, water, transportation, banking and finance, and control and communication infrastructure systems. Be it through direct connectivity, policies and procedures, or geospatial proximity, most critical infrastructure systems interact. These interactions often create complex relationships, dependencies, and interdependencies that cross infrastructure boundaries. The modeling and analysis of interdependencies between critical infrastructure elements is a relatively new and very important field of study. The U.S. Technical Support Working Group (TSWG) has sponsored this survey to identify and describe this current area of research including the current activities in this field being conducted both in the U.S. and internationally. The main objective of this study is to develop a single source reference of critical infrastructure interdependency modeling tools (CIIMT) that could be applied to allow users to objectively assess the capabilities of CIIMT. This information will provide guidance for directing research and development to address the gaps in development. The results will inform researchers of the TSWG Infrastructure Protection Subgroup of research and development efforts and allow a more focused approach to addressing the needs of CIIMT end-user needs. This report first presents the field of infrastructure interdependency analysis, describes the survey methodology, and presents the leading research efforts in both a cumulative table and through individual datasheets. Data was collected from open source material and when possible through direct contact with the individuals leading the research.

iii

iv

Table EX-1. Summary of areas surveyed.
Area Model by Infrastructure Sector Simulation Type System Model Hardware Platform Requirements PC HPC Windows Software Requirements User and Maturity Levels

Simulation Name X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X X Maturity: RS DV MI MC Research Development Mature Internal Mature Commercial X X X X X X X X X X X X X A A A A A X X X X X X X X X X X X X X X X X X X A X X X X X X X X X X X X X X X X X X X X X X X X X I X A X X X X I X X X X A X X A X X X X X X X X X X X X X X X X X X X X X X X X X X X X A X X X X X X X X X X X X X X X X X X X X X X X X A X A X X X

Developer

Electric Natural Drinking Power Gas Water Sewage Water SCADA Telecom Continuous Discrete Integrated Coupled Storm Water Human Activity Financial Networks Computer Networks Oil Pipeline Rail System Highway System Waterway System

Police/ Regulatory Constraints

Linux

Solaris

Users IA X X EA B B X X B X X B

Maturity Level RS MI MC MI DV MI MI

1

AIMS

UNB

2

Athena

On Target Technologies, Inc.

3

CARVER2

National Infrastructure Institute

4

CI3

ANL

5

CIMS

INL

6

CIP/DSS

LANL, SNL, ANL

7

CIPMA

Australia

8

CISIA

University Roma Tre

9

COMM-ASPEN

SNL

IA B IA X X X IA IA B X X IA IA IA

DV MC MI DV DV MC MI RS RS

10

DEW

EDD

11

EMCAS

ANL

12

FAIT

SNL

13

FINSIM

LANL

14

Fort Future

USACE

15

IEISS

LANL

16

IIM

UV

17

Knowledge Management and Visualization

CMU

18

MIN

Purdue

19

MUNICIPAL

RPI

20

N-ABLE

SNL

IA

MI DV IA B B RS MC MI RS

21

NEMO

SPARTA

22

Net-Centric GIS

York University

23

NEXUS Fusion Framework

IntePoint, LLC.

24

NGtools

ANL

25

NSRAM

JMU

26

PFNAM

ANL

27

TRAGIS

ORNL

MI X X X B IA IA MC MI DV

28

TRANSIMS

LANL

29

UIS

LANL

30

WISE

LANL

Simulation Type: I Input-Output Model A Agent-based

Users: IA EA B

Internal Analyst External Analyst Both

See Notes.

v

NOTES:

1

AIMS (Agent-Based Infrastructure Modeling and Simulation) is an agent-based system to simulate and model the (national and cross-border) interdependencies and survivability of Canada's critical infrastructures. Point of Contact (POC): Dr. Stephen Marsh, Adjunct Professor, University of New Brunswick, Canada, Stephen.Marsh@nrc-cnrc.gc.ca

2

Athena is an analysis and modeling tool that is designed to analyze a network of nodes (actors, concepts, and physical) as a "system of systems" by merging various political, military, economic, social, information, and infrastructure (PMESII) models and their associated crossdependencies. Athena incorporates several reasoning algorithms that allow sophisticated inter- and intra-dependency analysis between and through nodes. POC: Dr. Brian Drabble, On Target Technologies, brain@ontgttech.com

3

CARVER2 is a simple software program that provides a quick and easy way to prioritizes potential terrorist targets. It compares and rates the critical infrastructure and key assets in jurisdictions by producing a mathematical score for each potential target. It is the first step for conducting more in-depth vulnerability assessments. CARVER2 helps users make "apples vs. oranges" comparisons such as a water system vs. an energy grid vs. a bridge. POC: Ronald Peimer, National Infrastructure Institute Center for Infrastructure Expertise, rpeimer@ni2.org

4

CI3 (Critical Infrastructures Interdependencies Integrator) is a software tool for emulating (Monte Carlo simulation) the amount of time or cost (or both) needed for activities that must be completed to restore a given infrastructure component, a specific infrastructure system, or an interdependent set of infrastructures to an operational state. The software tool provides a framework for recognizing interdependencies and incorporating uncertainty into the analysis of critical infrastructures. POC: Dr. James Peerenboom, Argonne National Laboratory, jpeerenboom@anl.com

5

CIMS (Critical Infrastructure Modeling System) is a high level M&S tool that allows visualization in a 3D environment the cascading consequence of infrastructure perturbations. Events can be scripted or assets directly manipulated within the environment during a simulation run to illustrate consequence. POC: Don Dudenhoeffer, Idaho National Laboratory, Donald.Dudenhoeffer@inl.gov

6

CIP/DSS (Critical Infrastructure Protection Decision Support System) simulates the dynamics of individual infrastructures and couples separate infrastructures to each other according to their interdependencies. CIP/DSS models asset information at the aggregate level. For example, a focus area can estimate the number of hospital beds affected by an event, but it cannot directly retrieve information relative to a particular hospital. It utilizes the commercial simulation software Vensim. POC: Randy Michelsen, Los Alamos National Laboratory, rem@lanl.gov POC: Theresa Brown, Sandia National Laboratories, tjbrown@sandia.gov

7

CIPMA (Critical Infrastructure Protection Modeling and Analysis) is a computer based tool to support business and government decision making for critical infrastructure (CI) protection, counter-terrorism, and emergency management, especially with regard to prevention, preparedness, and planning and recovery. POC: Australian Government ­ Attorney General's Department (AGD), Michael Jerks ­ Director, Major Projects, Michael.Jerks@ag.gov.au

8

CISIA (Critical Infrastructure Simulation by Interdependent Agents) is described by the authors as a hybrid of the two modeling approaches; interdependency analysis and system analysis. It is a bottom-up complex adaptive systems (CAS) model using interactive agents. The CISIA simulator is designed to analyze short term effects of failures in terms of fault propagation and performance degradation (Panzieri, 2004). POC: Stefano Panzieri Universita Roma Tre, Italy, panzieri@uniroma3.it

9

DEW (Distribution Engineering Workstation) provides over 30 applications for analysis, design, and control of electrical and other physical network systems. DEW allows all of its components (data sets and algorithms) to be reused by a new application, allowing new solutions to build on top of existing work. This provides for cross collaborations among different groups and the emergence of solutions to complex problems. DEW is being used to identify and analyze interdependencies in large scale electrical power systems and fluid systems of aircraft carriers. DEW is open architecture, non-proprietary. POC: Electrical Distribution Design, Inc., Dr. Robert Broadwater, dew@vt.edu

10

EMCAS (Electricity Market Complex Adaptive System) combines engineering techniques with quantitative market analysis: DC load flow models allow you to simulate the actual operation of the physical system configuration as well as regulatory rules imposed on market operations. POC: Guenter Conzelmann, Argonne National Laboratory, guenter@anl.gov

11

FAIT (Fast Analysis Infrastructure Tool) is primarily an economic analysis tool utilizing REMI to conduct economic impact assessment across multiple sectors. It does promote interdependency discovery for first order relationships. The program resides on an SNL server and supports web access. POC: Theresa Brown, Sandia National Laboratories, tjbrown@sandia.gov

12

FINSIM is an agent-based model of cash and barter transactions that are dependant on contractual relationships and a network at the federal reserve level. Agent based models create transactions which rely on telecommunications and electric power. POC: Randy Michelsen, Los Alamos National Laboratory, rem@lanl.gov

13

Fort Future is a collaborative, web-based planning system that uses simulation to test plans for Department of Defense (DoD) installations. It uses an open, service-oriented architecture to allow multiple simulations to be run simultaneously from the same set of alternative, organized into a study. The web-based workbench provides geographic information system (GIS)-based plan editors, controls simulations, and organizes results into a decision matrix. Fort Future assesses the impact of critical infrastructure on mission using a "Virtual Installation" simulation that contains models for transportation, electrical power, water systems, including waterborne chemical/biological/radiological (CBR) agents, airborne CBR plume, facilities, mission tasks and processes, agents, and dynamic plans. POC: U.S. Army Corps of Engineers, Engineer Research and Development Center, Construction Engineering Research Laboratory (CERL), Dr. Michael P. Case, Michael.P.Case@erdc.usace.army.mil

14

IEISS (Interdependent Energy Infrastructure Simulation System) is an actor-based infrastructure modeling, simulation, and analysis tool designed to assist individuals in analyzing and understanding interdependent energy infrastructures. The actor-based infrastructure components were developed in IEISS to realistically simulate the dynamic interactions within each of the infrastructures, as well as, the interconnections between the infrastructures. POC: Randy Michelsen, Los Alamos National Laboratory, rem@lanl.gov

15

IIM (Inoperability input-output model) based on Leontief's input-output model, characterizes interdependencies among sectors in the economy and analyzes initial disruptions to a set of sectors and the resulting ripple effects. POC: Yacov Y. Haimes, University of Virginia, haimes@virginia.edu

16

Knowledge Management and Visualization is a research project to analyze vulnerabilities associated with delivery of fuel. It is designed to help ensure availability of supply and to visualize the impacts for decision support. The project has focused on coal deliveries to power plants because, while vulnerabilities at the power plant level (production) are easier to identify, vulnerabilities and impacts associated with delivery of fuel are more uncertain. Also, data on coal shipments is readily available. POC: Carnegie Mellon University, H. Scott Matthews, hsm@cmu.edu

17

MIN (multilayer infrastructure network) is a preliminary network flow equilibrium model of dynamic multilayer infrastructure networks in the form of a differential game involving two essential time scales. In particular, three coupled network layers--automobiles, urban freight, and data--are modeled as being comprised of Cournot-Nash dynamic agents. An agent-based simulation solution structure is introduced to solve the flow equilibrium and optimal budget allocation problem for these three layers under the assumption of a super authority that oversees investments in the infrastructure of all three technologies and thereby creates a dynamic Stackelberg leader-follower game. POC: Purdue School of Civil Engineering, Dr. Srinivas Peeta, peeta@purdue.edu

vi

18

MUNICIPAL (Multi-Network Interdependent Critical Infrastructure Program for Analysis of Lifelines) is a GIS user interface, built on a formal, mathematical representation of a set of civil infrastructure systems that explicitly incorporates the interdependencies among them. The mathematical foundation or decision support system is called the interdependent layered network (ILN) model. ILN is a mixed-integer, network-flow based model implemented in software drawing on a database containing infrastructure attributes. MUNICIPAL provides the capability to understand how a disruptive event affects the interdependent set of civil infrastructures. POC: Rensselaer Polytechnic Institute (RPI), Earl E. Lee II, Leee7@rpi.edu

19

N-ABLE (Next-generation agent-based economic laboratory) simulates the economy using an agent-based discrete-event model. Agents make economic decisions including purchasing products, hiring workers, selling bonds, collecting welfare payments, conducting open market operations, and others. N-ABLE has been used to evaluate electric power and rail transportation disruptions on commodity production. POC: Theresa Brown, Sandia National Laboratories, tjbrown@sandia.gov

20

NEMO (Net-Centric Effects-based operations MOdel) relies on the following domain specific legacy simulations: CitiLabs' Voyager simulation provides road and rail network analysis. Advantica provides the solver tools for electrical power networks, water and gas pipelines. Users define relationships between components. POC: Brent L. Goodwin, SPARTA Corporation, brent.goodwin@sparta.com

21

Net-Centric GIS is a framework for using GIS interoperability for supporting emergency management decision makers by providing effective data sharing and timely access to infrastructure interdependency information. POC: York University, Toronto, Ontario, Canada, Rifaat Abdalla, abdalla@yorku.ca

22

NEXUS Fusion FrameworkTM is a planning and response tool that visualizes intended and unintended effects and consequences of an event across multiple infrastructure, social, and population behavior models. It is a single framework that incorporates geospatial, graph based (social, economic), and population behavior models in the same simulation space for cross-infrastructure relationship analysis. The framework takes a holistic system-of-systems view to support cross system analyses of cascading events within and between complex networks. POC: IntePoint, LLC, Mark Armstrong, Mark.Armstrong@IntePoint.com

23

NGtools (natural gas infrastructure toolset). NGtools was developed to provide an analyst with a quick method to access, review, and display components of the natural gas network; perform varying levels of component and systems analysis, and display analysis results. POC: Argonne National Laboratory, Infrastructure Assurance Center (IAC), Dr. James Peerenboom, jpeerenboom@anl.gov

24

NSRAM (Network Security Risk Assessment Modeling) is centered on the analysis of large interconnected multi-infrastructure models. POC: Jim McManus, James Madison University, McManuJP@jmu.edu

25

PFNAM (Petroleum Fuels Network Analysis Model) was developed to perform hydraulic calculations of pipeline transport of crude oil and petroleum products. A network consists of links (pipe segments), nodes (pipe junctions), pump stations, valves, and pressure-reducing stations. The model tracks the flow of oil in each pipe and the pressure at each node. "Point-and-click" motions allow the analyst to create a representative model of the liquids pipeline network in order to set up and run a simulation. Graphical and tabular results provided for each simulation enable analysts to quantify the impact of infrastructure disruptions on the pipeline segment or system. This software tool provides a framework for introducing pipeline component dependencies into critical infrastructure analyses. POC: Argonne National Laboratory, Infrastructure Assurance Center, Steve Folga, sfolga@anl.gov

26

TRAGIS (Transportation Routing Analysis Geographic Information System), available via a client server architecture from a web server residing at ORNL. Calculates transportation rouge information based on regulatory guidance for shipping hazardous materials. POC: Paul E. Johnson, Oak Ridge National Laboratory, johnsonpe@ornl.gov

27

TRANSIM is an agent-based system capable of simulating a synthetic populations second-by-second movements of every person and vehicle through the transportation network of a large metropolitan area. TRANSIMS provides planners with a synthetic population's daily activity patterns (such as travel to work, shopping and recreation, etc.), simulates the movements of individual vehicles on a regional transportation network, and estimates the air pollution emissions generated by vehicle movements. POC: Randy Michelsen, Los Alamos National Laboratory, rem@lanl.gov

28

WISE (Water Infrastructure Simulation Environment) is an analytic framework supporting the evaluation of water infrastructure in terms of both infrastructure specific and interdependency issues. WISE involves the integration of geographic information systems with a wide range of infrastructure analysis tools including industry standard hydraulic simulation engines (e.g., EPANET and SWMM) as well as Los Alamos National Laboratory interdependency simulation systems such as the Interdependent Energy Infrastructure Simulation System (IEISS). POC: Randy Michelsen, Los Alamos National Laboratory, rem@lanl.gov

vii

viii

CONTENTS
ABSTRACT.................................................................................................................................................iii INTRODUCTION ........................................................................................................................................ 1 Technical Support Working Group .................................................................................................... 1 Background......................................................................................................................................... 1 INFRASTRUCTURE INTERDEPENDENCIES......................................................................................... 3 Interdependency Formalization .......................................................................................................... 5 Interdependency Types....................................................................................................................... 6 Problem Space .................................................................................................................................... 8 SURVEY METHODOLOGY ...................................................................................................................... 9 Infrastructures..................................................................................................................................... 9 Modeling and Simulation Technique ............................................................................................... 10 Integrated vs. Coupled Models......................................................................................................... 10 Hardware/Software Requirements.................................................................................................... 10 Intended user .................................................................................................................................... 10 Maturity level ................................................................................................................................... 10 STATE-OF-THE-ART REPORT ............................................................................................................... 11 POLITICAL, MILITARY, ECONOMIC, SOCIAL, INFORMATION, AND INFRASTRUCTURE MODELING ACTIVITIES.............................................................................................................. 12 DATA SOURCES ...................................................................................................................................... 13 U.S. RESEARCH AND SPONSORING ORGANIZATIONS .................................................................. 14 CHALLENGES AND RESEARCH NEEDS ............................................................................................. 16 CONCLUSIONS......................................................................................................................................... 19 ACKNOWLEDGEMENT .......................................................................................................................... 20 REFERENCES ......................................................................................................................................... 115

ix

FIGURES
1. 2. Infrastructure interdependencies......................................................................................................... 3 Thick, black smoke billows out of the railroad tunnel near Oriole Park at Camden Yards. Interstate 395 and the baseball park were closed, along with the Inner Harbor (see Reference 9). ............................................................................................................................... 4 An official surveys the gaping hole and broken 40-in. water main at Howard and Lombard streets (see Reference 10). .................................................................................................................. 4 Sample dependency matrix................................................................................................................. 5 Cascading consequence example (see Reference 13)......................................................................... 7 PMESII node and effects relation (see Reference 19)...................................................................... 12

3. 4. 5. 6.

TABLES
EX-1. Summary of areas surveyed................................................................................................................ v 1. 2. 3. Multiscale time hierarchy of power systems. ................................................................................... 16 Strengths and weaknesses of HLA. .................................................................................................. 17 Strengths and weaknesses of DIS..................................................................................................... 17

x

Critical Infrastructure Interdependency Modeling: A Survey of U.S. and International Research
INTRODUCTION
"The Nation's health, wealth, and security rely on the production and distribution of certain goods and services. The array of physical assets, processes, and organizations across which these goods and services move are called critical infrastructures."2 This statement is as true in the U.S. as in any country in the world. Recent world events such as the 9-11 terrorist attacks, London bombings, and gulf coast hurricanes have highlighted the importance of stable electric, gas and oil, water, transportation, banking and finance, and control and communication infrastructure systems. Be it through direct connectivity, policies and procedures, or geospatial proximity, most critical infrastructure systems interact. These interactions often create complex relationships, dependencies, and interdependencies that cross infrastructure boundaries. The modeling and analysis of interdependencies between critical infrastructure elements is a relatively new and very important field of study. Much effort is currently being spent to develop models that accurately simulate critical infrastructure behavior and identify interdependencies and vulnerabilities. The results of these simulations are used by private companies, government agencies, military, and communities to plan for expansion, reduce costs, enhance redundancy, improve traffic flow, and to prepare for and respond to emergencies. Modelers have developed various innovative modeling approaches including agent based modeling, effects-based operations (EBO) models, input-output models, models based on game theory, mathematical models, and models based on risk. These have been applied to infrastructure of shipboard systems, University campuses, large power grids, and waterways to name a few. Modeling is complicated by the quality and availability of data, intricacy of systems, complexity of interactions between infrastructure sectors, and implications and sensitivity of results. This survey identifies and catalogs much of the state-of-the-art research being conducted in the area of infrastructure interdependency modeling and analysis.

Technical Support Working Group
The U.S. Technical Support Working Group (TSWG) is the sponsor for this effort.3 TSWG is a national forum to identify, prioritize, and coordinate interagency and international research and development (R&D) requirements for combating terrorism. The aim of TSWG is to support rapidly developed technologies and product development to provide tools for combating terrorism. It supports multiple U.S. government agencies as well as major allies. The main objective of this study is to develop a single source reference of critical infrastructure interdependency modeling tools (CIIMT) that could be applied to allow users to objectively assess the capabilities of CIIMT. This information will provide guidance for directing R&D to address the gaps in development. The results will inform the R&D efforts of the TSWG Infrastructure Protection Subgroup of R&D efforts and allow a more focused approach to addressing the needs of CIIMT end-user needs.

Background
The study and analysis of infrastructure interdependencies is relatively new. The interdependencies between critical infrastructures received little attention in the early 1990s. However, in the mid 1990s events such as the Oklahoma City bombing in 1995 and the report from the Defense Science Board Task Force on Information Warfare in 1996, and the increased reliance on information and computerized control systems brought the increasing importance of

1

infrastructure interdependencies into focus. Also in 1996, President Clinton established the President's Commission on Critical Infrastructure Protection (PCCIP).4 The PCCIP report was released in 1997 and though it identified no immediate critical threats to national infrastructures, it did highlight the importance of interdependencies including those between power, transportation, emergency response, vital human services, banking and finance, and telecommunications, especially through digital means. A general recommendation of the commission was that since the lion's share (approximately 85%) of the nation's critical infrastructure is in private hands, there needs to be good cooperation and information sharing between government and private sector. In May of 1998, Presidential Decision Directive (PDD) no. 63 was released. That directive set a national goal to protect the nation's critical infrastructure from deliberate attacks by 2003. PDD-63 was followed by executive orders (E.O.s) by both Presidents Clinton (E.O. 131305 in July 1999) and Bush (E.O. 132316 in 2001) establishing Information Sharing and Analysis Centers that were largely private-sector run and a National Infrastructure Advisory Council (NIAC). While there were some changes in the wording of

the E.O.s, the functions of NIAC remained largely the same. We have since seen the establishment of the U.S. Department of Homeland Security (DHS) in November of 2002 and the National Infrastructure Simulation and Analysis Center (NISAC) in fall of 2001. NISAC is a partnership between Sandia National Laboratory (SNL) and Los Alamos National Laboratory (LANL) established to develop advanced infrastructure modeling and simulation techniques that identify vulnerabilities and interdependencies. This increased attention has been followed by increases in funding to universities, national laboratories, and private companies involved in modeling and simulation of critical interdependencies. Funding has come from national organizations, private investments, the Department of Defense (DoD), U.S. government agencies (DHS, U.S. Department of Energy [DOE], Department of Commerce, and others), and other governments and agencies. The increased funding and level of efforts has led to much innovative work in this area. Thus, while efforts focusing on modeling of critical infrastructure interdependencies have only begun recently, much valuable work has already been done.

2

INFRASTRUCTURE INTERDEPENDENCIES
"One of the most frequently identified shortfalls in knowledge related to enhancing critical infrastructure protection capabilities is the incomplete understanding of interdependencies between infrastructures. Because these interdependencies are complex, modeling efforts are commonly seen as a first step to answering persistent questions about the "real" vulnerability of infrastructures."7 The importance of "What are infrastructure inter-dependencies, and how are they modeled?" is addressed in this section. References to interdependent relationships in this paper are actually referring to as dependent relationships or influences between infrastructures. Figure 1 illustrates common representations of infrastructure based on the scenario of a flooding event and the subsequent response. Parallels to this scenario with the events in New Orleans during Hurricane Katrina can easily be drawn. Within the

figure, individual infrastructure networks are represented on a single plane. The parallel lines represent individual sectors or subsets within that particular infrastructure. The spheres or nodes represent key infrastructure components within that sector from the events in New Orleans The energy sector infrastructure, for example, during Hurricane Katrina contains the sectors of electrical generation and distribution, natural gas production and distribution, etc. Ties and dependencies exist within each infrastructure and between the different sectors. The solid lines in Figure 1, crossing sectors and connecting nodes, represent internal dependencies, while the dashed lines represent dependencies that also exist between different infrastructures (infrastructure interdependencies). The example in Figure 1 is a simple attempt to portray the complexity of dependencies that may exist between components. In chaotic environments such as emergency response to catastrophic events, decision makers should

Figure 1. Infrastructure interdependencies.

3

understand the dynamics underlying the infrastructures. Failure to understand those dynamics will result in ineffective response and poor coordination between decision makers and agencies responsible for rescue, recovery, and restoration. It could also cause the mismanagement of resources, including supplies, rescue personnel, and security teams. At best, emergency responders will lose public trust, at worst, human life. This interrelationship among infrastructures and its potential for cascading effects was never more evident than on July 19, 2001 when a 62-car freight train carrying hazardous chemicals derailed in Baltimore's Howard Street Tunnel, Figure 2. This disaster, in addition to its expected effect on rail system traffic, automobile traffic, and emergency services, caused a cascading degradation of infrastructure components not previously anticipated. For example, the tunnel fire caused a water main to break above the tunnel shooting geysers 20 ft into the air, Figure 3. The break caused localized flooding which exceeded a depth of three feet in some areas. Additionally, the flooding knocked out electricity to about 1,200 downtown Baltimore residences.8 Fiber optical cables running through the tunnel were also destroyed. This resulted in major disruptions to phone and cell phone service, email service, web services, and data services to major corporations including WorldCom Inc., Verizon Communications Inc., the Hearst Corp. in New York City, Nextel Communications Inc., and the Baltimore Sun newspaper.9 Disruption to rail services and its effects on the Middle Atlantic states were significant also.10 These effects included delays in coal delivery and also limestone delivery for steel. Figure 3. An official surveys the gaping hole and broken 40-in. water main at Howard and Lombard streets (see Reference 10). Figure 2. Thick, black smoke billows out of the railroad tunnel near Oriole Park at Camden Yards. Interstate 395 and the baseball park were closed, along with the Inner Harbor (see Reference 9).

4

A dependency matrix is another way to represent interdependencies between infrastructure networks and their relative impact. The Critical Infrastructure Protection Task Force of Canada used a dependency matrix (see Figure 4) to relate the interdependency among six sectors identified as crucial: Government, Energy and Utilities, Services, Transportation, Safety, and Communications.11 The matrix is their attempt to better understand the level of dependency and the potential impact among sectors. Infrastructure owners historically concerned with the operation of their own, often well defined domains must now contend with unbounded networks brought about by greater information technology connectivity. There is a growing need to analyze and better understand the chains of influence that cross multiple sectors that can induce potentially unforeseen secondary effects. This survey addresses a growing concern dealing

with the influence or impact, that one infrastructure can have, either directly or indirectly, upon another. The cross infrastructure effects continue to grow as information technology pushes interconnectivity between all aspects of business. Infrastructure interdependencies therefore refer to relationships or influences that an element in one infrastructure imparts upon another infrastructure.

Interdependency Formalization
Precisely how is an infrastructure interdependency relationship defined? Dudenhoeffer, Permann and Manic12 model the levels of infrastructure as a large graph in which nodes represent infrastructure components, and edges the relations between nodes.

Figure 4. Sample dependency matrix.

5

A formal model of this infrastructure and the interrelationships is presented in the following definitions: 1. An infrastructure network, I, is a set of nodes related to each other by a common function. The network may be connected or disjoint. It may be directional, bi-directional or have elements of both. Internal relationships/dependencies within the infrastructure I are represented by edge (a, b) with a, b  I. 2. Given Ii and Ij are infrastructure networks, i z j, a  Ii and b  Ij, an interdependency is defined as a relationship between infrastructures and represented as the edge (a,b) which implies that node b is dependent upon node a. Depending on the nature or type of the relationship, this relationship may be reflexive in that (a,b)  (b,a).

thunderstorm resulting in a loss of power to an office building and all the computers inside. x Informational Interdependency. An informational or control requirement between components. For example: a supervisory control and data acquisition (SCADA) system that monitors and controls elements on the electrical power grid. A loss of the SCADA system will not by itself shut down the grid, but the ability to remotely monitor and operate the breakers is lost. Likewise, this relationship may represent a piece of information or intelligence flowing from a node that supports a decision process elsewhere. An example is the dispatch of emergency services. While the responders may be fully capable of responding, an informational requirement exists as to answering where, what, and when to initiate response. x Geospatial Interdependency. A relationship that exists entirely because of the proximity of components. For example: flooding or a fire may affect all the assets located in one building or area. x Policy/Procedural Interdependency. An interdependency that exists due to policy or procedure that relates a state or event change in one infrastructure sector component to a subsequent effect on another component. Note that the impact of this event may still exist given the recovery of an asset. For example: after aircraft were flown into the World Trade Towers "all U.S. air transportation was halted for more than 24 hours, and commercial flights did not resume for three to four days."14 x Societal Interdependency. The interdependencies or influences that an infrastructure component event may have on societal factors such as public opinion, public confidence, fear, and cultural issues. Even if no physical linkage or relationship exists, consequences from events in one infrastructure may impact other infrastructures. This influence may also be time sensitive and decay over time from the original event grows. For example: air traffic following the 9-11 attack dropped significantly while the public evaluated the safety of travel. This resulted in layoffs within 6

Interdependency Types
Interdependencies can be of different types. Several taxonomies have been presented3 to categorize the types of interdependencies. Rinaldi, Peerenboom, and Kelly13 describe dependencies in terms of four general categories: x Physical ­ a physical reliance on material flow from one infrastructure to another x Cyber ­ a reliance on information transfer between infrastructure x Geographic ­ a local environmental event affects components across multiple infrastructures due to physical proximity x Logical ­ a dependency that exists between infrastructures that does not fall into one of the above categories. This study used a slightly expanded taxonomy developed by Dudenhoeffer and Permann.4 The categorization classifies the following types of relationships: x Physical. A requirement, often engineering reliance between components. For example: a tree falls on a power line during a

the airline industry and bankruptcy filings by some of the smaller airlines (see Reference 12). Again, while the dependencies within an individual infrastructure network are often well understood, the region of interest in interdependency and effects modeling is the influence or impact that one infrastructure can impart upon another. Therefore, the key effects to model and gain understanding of are the chains of influence that cross multiple sectors and induce potentially unforeseen n-ary effects. These chains, potentially composed of multiple interdependency types, compose the paths or arcs between infrastructure components or nodes denoted as {(a,b), (b,c), (c,d), ...(y,z)}. This particular path represents the cascading consequence of an event or the derived dependency of node z on node a,

further denoted (aDz). Likewise the genesis of the chain may not be singular in that the end effect is the influence of multiple nodes, denoted by (abc..Dz). These paths may not be unique in terms of effect, they may change over time, and their behavior may be cumulative in nature, i.e., the end effect may be the culmination of multiple predicated events. The intertwining of networks in this fashion represents a complex system where emergent behaviors are rarely fully understood. Rinaldi, Peerenboom and Kelly (see Reference 13) provide a nice visual representation of this intertwining and the potential cascading effects. This is shown in Figure 5.

Figure 5. Cascading consequence example (see Reference 13).

7

Problem Space
Thus given the realm of interdependency analysis, what are the goals for modeling and simulation efforts? In the analysis of infrastructure interdependencies and the subsequent emergent system behaviors, some of the major problem areas being examined include: 1. Given a set of initiating events {E(a), E(b), ...} what is the cascading impact on a subset of nodes {x, y, z , ...}? 2. Given a set of nodes {x, y, z,...} and a desired end state, what is a set of events {E(a), E(b), ...} that would cause this effect?

3. Given a set of events {E(a), E(b), ...} and a set of observed outcomes of on nodes {x, y, z,....}, is it possible to determine the derived interdependence (abDxyz)? 4. Given a set of infrastructure networks and a critical function, what is the subset of critical nodes {x, y, z , ...} across all networks that will adversely impact a specific mission functionality due to direct or derived dependency?

8

SURVEY METHODOLOGY
The areas included in this survey were selected because they focus on modeling and simulation across multiple infrastructure layers. Systems such as geographical information systems (GIS), which may provide geospatial relationships, are not included unless they possess additional analytical capabilities. Each model examined in the survey offers unique capabilities and provides specific insights into various aspects of the problem domain. The modeling approaches and the objectives of the efforts varied greatly. Specific parameters in the survey were of interest for comparison. One of the goals of the survey was to identify potential resources for a wide range of customers and domains. Six major categories were considered in the survey: x Infrastructures x Modeling and simulation technique x Integrated vs. coupled models x Hardware/software requirements x Intended user x Maturity level. Each of these categories is briefly discussed below.

government as well as the infrastructure relied upon for the defense and national security of the U.S. x Private business, government, and the national security apparatus increasingly depend on an interdependent network of critical physical and information infrastructures, including telecommunications, energy, financial services, water, and transportation sectors. x A continuous national effort is required to ensure the reliable provision of cyber and physical infrastructure services critical to maintaining the national defense, continuity of government, economic prosperity, and quality of life in the U.S.. x This national effort requires extensive modeling and analytic capabilities for purposes of evaluating appropriate mechanisms to ensure the stability of these complex and interdependent systems, and to underpin policy recommendations, so as to achieve the continuous viability and adequate protection of the critical infrastructure of the Nation.16 Although countries tend to have slightly different lists detailing their "critical sectors," most contain elements of the following: x Agriculture and food x Water x Public health and safety x Emergency services x Government x Defense industrial base x Information and telecommunications x Energy x Transportation x Banking and finance x Industry/manufacturing x Postal and shipping. These sectors in turn contain individual infrastructures such as highways, rail systems, electric power generation and distribution, etc.

Infrastructures
The U.S. Patriot Act defines critical infrastructure as "systems and assets, whether physical or virtual, so vital to the U.S. that the incapacity or destruction of such systems and assets would have a debilitating impact on security, national economic security, national public health or safety, or any combination of those matters."15 Further, congress set forth the following findings in Section 1016 of the U.S. Patriot Act: x The information revolution has transformed the conduct of business and the operations of

9

Some of these systems are managed by government agencies, but the majority resides with industry. This survey attempts to capture and describe the infrastructures/infrastructure sectors each program models. This report seeks to reflect only those infrastructures that have been actually modeled and not those presumed to be capable of being modeled.

integrated models tend to model at a much higher level than coupled models.

Hardware/Software Requirements
In an effort to identify possible tool sets, the survey captures the portability and exportability of programs and data.

Intended user
The survey categorizes products as internal analytical tools intended for internal use only or external analytical tools available for use outside the developing organization. This decision relates to the level of expertise required to use the product, the application requirements, and the analytical output of the product. The requirement is sometimes driven by the size, complexity, or proprietary nature underlying the data

Modeling and Simulation Technique
This category attempts to capture the modeling and simulation method used for the infrastructure and interdependencies. It has multiple dimensions that include those of time (continuous vs. discrete time step) and modeling technique (Markov chains, Petri Nets, dynamic simulation, agent-based, physics based, ordinary differential equations, input-output model, etc.).

Maturity level
The following four categories were used to identify the product's level of maturity: x Research ­ the product is still highly conceptual without vetted application in real-world domains. x Development ­ the product has been applied and validated against real-world infrastructure. Beyond conceptual, the product has been used by internal or external customers, but is still undergoing substantial development. x Mature analytic ­ the product has reached a high level of code stability and is part of a vested internal analytical process. The results of analysis may be an external report, but the tool usage is strictly internal to the organization. x Mature commercial ­ the tool is a commercially licensed product.

Integrated vs. Coupled Models
During the course of the survey it became apparent that two different approaches were often used to conduct cross infrastructure analysis. One approach was to create an integrated system model that attempted to model multiple infrastructures and their interdependencies within one framework. The other approach coupled a series of individual infrastructure simulations together, which then illustrated the cascading influence between them. An example of this approach would be an electric grid simulation that determines an outage area for a specific event. The electrical outage area is then fed to a telecommunication model used to determine the subsequent impact on message routing. This impact is fed to a financial simulation that determines the loss of telecommunication impact on commerce and financial transactions. As one might expect,

10

STATE-OF-THE-ART REPORT
Appendix A contains data on U.S. and international efforts and interdependency modeling tools. The information is presented at a high level with POC information for those desiring greater detail.

11

POLITICAL, MILITARY, ECONOMIC, SOCIAL, INFORMATION, AND INFRASTRUCTURE MODELING ACTIVITIES
A modeling area that closely follows infrastructure interdependency modeling is EBO modeling and analysis. War and conflict are rarely confined to only the battlefield and force-on-force engagement. Potential U.S. adversaries comprise a complex and interdependent system of systems, all of which contribute, to some degree, toward their societal coherence, will, and capability to pursue a course of action contrary to U.S. interests.17 Conflict, war, and reconstruction represent a complex set of influences, competing goals, and resources. The battle environment, and thus the means of victory, are often shaped by the intricate interactions between them. Many point to the emergence of a new generation of warfare termed fourth generation warfare (4GW). Retired Colonel Thomas Hammes, U.S. Military Complex, describes this concept: "4GW uses all available networks--political, economic, social, and military--to convince the enemy's political decision makers that their strategic goals are either unachievable or too costly for the perceived benefit. It is an evolved form of insurgency. Still rooted in the fundamental precept that superior political will, when properly employed, can defeat greater economic and military power, 4GW makes use of a society's networks to carry on its fight. Unlike previous

generations of warfare, it does not attempt to win by defeating the enemy's military forces. Instead, via the networks, it directly attacks the minds of enemy decision makers to destroy the enemy's political will."18 Operational Net Assessment (ONA) is the integration of people, processes, and tools that use multiple information sources and collaborative analysis to build shared knowledge of the adversary, the environment, and ourselves in understanding and effectively employing EBO. ONA analytical products are based on a system-of-systems analysis and the understanding of key relationships, dependencies, strengths, and vulnerabilities within and between the adversary's political, military, economic, social, information, and infrastructure (PMESII) elements. These products identify leverage points, key nodes, and links that we can act upon to decisively influence the adversary's behavior, capabilities, perceptions, and decisions.19 Within this operating environment, EBOs are actions that change the state of a system to achieve directed policy aims using the integrated application of the diplomatic, informational, military, and economic instruments of national power. In order to achieve EBO, however, it is imperative to understand the relationships and influences of the PMESII dimensions that shape the actions of the adversary, of allies, and of your organization. Figure 6 illustrates this concept showing a representation of the connectivity and interdependencies between these dimensions as both a strength and potential weakness.

Figure 6. PMESII node and effects relation (see Reference 19).

12

DATA SOURCES
The paradigm of modeling and simulation is "garbage in, garbage out." Having credible and traceable data available to use is key to infrastructure and interdependency modeling. Gathering information on a particular infrastructure is possibly the most significant challenge. Interdependency modeling also requires that gathered information (assets) be linked across multiple infrastructures. Supporting data for these analyses often spread across multiple data sets. The fact that most infrastructures data is held by private industry and, to a large extent, considered proprietary in nature complicates the situation further. The data is often accompanied by the analytical requirement for a certain level of domain expertise in identifying and validating cross infrastructure influences. The scale of the model also determines the possible sources of information. Consider, for example, the electrical power grid. If the goal is to model assets on a national scale, data equivalent to transmission level information may suffice with broad asset effects drawn from course outage area determination. If the goal is to evaluate a particular city, compound, or facility, distribution level information is required reflecting a far greater level of granularity. Commercial geospatial data sets such as those provided by ESRI, Platts, etc., provide coarse level data that may suffice for initial model development, but they lack the detail needed to construct a more precise model. Public census provides a good data source for an initial data set. Recall however, that the census data reflects nighttime residential demographics in terms of grid-wise statistics, which may not be adequate in terms of population mobility and granularity. To mitigate the shortcomings of data, several efforts have been made to compose and validate detail infrastructure and demographic data sources. Two of the data sets used by those surveyed are LandScan and National Asset Database:

x LandScan ­ The LandScan series of data sets have been developed and are maintained by Oak Ridge National Laboratory. They are a population distribution model, database, and tool developed from census data that incorporates other spatial information for greater accuracy and granularity. The LandScan series consist of LandScan Global representing data in 30 arc second grid cells for ~1 km resolution, LandScan Interim, which has a 15-arc (~450 m) second resolution, and LandScan USA with 3-arc-second resolution for ~90 m resolution with both day and night time population distributions and demographic and socioeconomic characteristics data.20 x National Asset Database ­ In July 2004, the Office of Infrastructure Protection (DHS/IP) initiated a data call to states and territories requesting a listing of assets deemed of national or local importance. The collection, named the National Asset Database, contains basic asset and facility information, including data associated with location, POC, and risk attributes. In addition to these specialized data sets, several DOE national laboratories maintain system expertise that includes detailed infrastructure data. These information sets are, to a large degree, the result of industry nondisclosure agreements and therefore are not generally releasable for public use. x LANL ­ National electrical generation and transmission data x Argonne National Laboratory (ANL) ­ Natural gas and oil pipeline data x Oak Ridge National Laboratory (ORNL) ­ National transportation sector information including rail systems, highway, and waterway data and models x Idaho National Laboratory (INL) ­ National electrical power SCADA system information.

13

U.S. RESEARCH AND SPONSORING ORGANIZATIONS
The modeling and simulation of infrastructure interdependencies is a substantial effort in terms of development resources such as infrastructure expertise, modeling and simulation, data accessibility, and so on. For this reason, U.S. government agencies are currently doing most of the research in this area. In order to understand the current focus on ongoing research, it is important to understand the thrust of these organizations. A brief description of the more prominent supporting agencies and their programs are described as follows: x Department of Homeland Security (DHS) ­ The NISAC provides advanced modeling and simulation capabilities for the analysis of critical infrastructures, their interdependencies, vulnerabilities, and complexities. These capabilities help improve the robustness of our nation's critical infrastructures by aiding decision makers in the areas of policy analysis, investment and mitigation planning, education and training, and near real-time assistance to crisis response organizations. The NISAC program is sponsored by the DHS Information Analysis and Infrastructure Protection Directorate. NISAC is a core partnership of Los Alamos and Sandia National Laboratories. NISAC integrates the modeling and simulation expertise of both laboratories to address the nation's potential vulnerabilities and the consequence of disruption among our critical infrastructures.21 x Department of Energy (DOE) ­ The Visualization and Modeling Working Group (VMWG) sponsored by DOE's Office of Electricity Delivery and Energy Reliability activates in response to national energy emergencies to provide data, analyses, and visualization tools as was done for Hurricanes Katrina and Rita. The VMWG was formed in September 2003 to improve the ability of DOE to perform quick turn-around analyses during energy emergencies. It is comprised of energy experts from several DOE offices and energy infrastructure and modeling experts from various DOE national laboratories. Their technical expertise is combined with modeling, GIS, data libraries on past energy disruptions, 14

and other tools to conduct in-depth analysis. DOE national laboratories provide the bulk of this modeling and analysis.22 x Technical Support Working Group (TSWG) ­ TSWG is an inter-agency organization tasked with providing technologies to a variety of government organizations. Their development and product deployment goals focus on identifying and answering specific programmatic needs versus sponsoring national infrastructure modeling and simulation initiatives. This study attempts to identify available and developing resources that may be utilized to address those needs.23 x Defense Advanced Research Projects Agency (DARPA) ­ DARPA is a central research and development organization for DoD. It manages and directs selected basic and applied research and development projects for DoD, and pursues research and technology where risk and payoff are both very high and where success may provide dramatic advances for traditional military roles and missions. DARPA also has a research program in the area of crossdimensional infrastructure influence modeling. By focusing on PMESII dimension interactions, DARPA is leading the Integrated Battle Command. The objective of this program is the development of decision aids to support the commander in conducting a future, complex, multidimensional, coalition, and effects-based campaign. The decision aids will assist the commander and staff in generating, assessing, and visualizing the consequences of employing diplomatic, military, information operations and economic actions, singularly or in combinations, to achieve effects against the adversary's PMESII systems. The decision aids will also assist the commander and staff in constructing, visualizing, and evaluating campaign plans that exploit the impact of multidimensional effects and the interaction among effects. http://www.darpa.mil/ato/solicit/IBC/index.htm. x Department of the Air Force, Air Force Materiel Command, (AFRL) ­ Similar to DARPA, AFRL is leading multiple research efforts in developing PMSEII analytical models. One effort is the Commander's Predictive Environment program, whose objective is to provide a decision support environment that

enables the joint force commander to anticipate and shape the future battlespace. Similar in view to the DARPA effort, the battlespace is seen as a complex and interrelated system of PMESII dimensions. A full understanding of the battlespace requires comprehension of how these interrelated factors affect not only the adversary, but also friendly forces. The focus of

this research program is to (1) model and analyze adversaries, self, and neutrals as a complex adaptive system; (2) understand key relationships, dependencies, and vulnerabilities of adversary/self/neutrals; and (3) identify leverage points that represent opportunities to influence capabilities, perceptions, decision making, and behavior.24

15

CHALLENGES AND RESEARCH NEEDS
Critical infrastructure interdependency modeling has many of the same challenges that one can expect with any modeling and simulation domain: data accessibility, model development, and model validation. Interdependency modeling is further complicated by the extremely large and disparate cross sector analysis required. Many extremely detailed single sector models have been developed. One driving research question asks: "How do we leverage these existing models into a common operating picture?" Such a question is further exasperated by the granularity and the time factors associated with the models. For example, Table 1 illustrates the multiple time scales that exist within the electrical power sector. While currently no standards exists that directly address infrastructure and specifically cross sector modeling, standards do exists for exchanging information between distributed simulations. The two most common methods are the High Level Architecture (HLA) and the Distributed Interactive Simulation (DIS) frameworks. HLA, developed under the leadership of the Defense Modeling and Simulation Office is a general purpose high-level simulation architecture/framework to facilitate the interoperability of multiple types of models and simulations. The purpose of its development is to support reuse and interoperability across the large numbers of different types of simulations developed and maintained by DoD. Within HLA, simulation objects exist as federates in a larger simulation federation. HLA was approved as an open standard through the Institute of Electrical and Electronic Engineers (IEEE) -- IEEE Standard 1516 -- in September 2000.

Table 1. Multiscale time hierarchy of power systems.25
Action/Operation Time frame

Wave effects (fast dynamics, lightning caused over voltages) Switching over voltages Fault protection

Microseconds to milliseconds Milliseconds

100 milliseconds or a few cycles Electromagnetic effects in machine Milliseconds to windings seconds Stability 60 cycles or 1 second Stability augmentation Seconds Electromechanical effects of Milliseconds to oscillations in motors & generators minutes Tie line load frequency control 1 to 10 seconds; ongoing Economic load dispatch 10 seconds to 1 hour; ongoing Seconds to Thermodynamic changes from hours boiler control action (slow dynamics) System structure monitoring (what Steady state; is energized & what is not) ongoing System state measurement and Steady state; estimation ongoing System security monitoring Steady state; ongoing Load management, load 1 hour to 1 day forecasting, generation scheduling. or longer; ongoing Maintenance scheduling Months to 1 year; ongoing. Expansion planning Years; ongoing Power plant site selection, design, construction, environmental impact, etc. 10 years or longer

16

Table 2 provides a listing of HLA strengths and weaknesses as detailed by Schmitz and Neubecker.26 Additional information on HLA can be found by contacting hla@dmso.mil or via the website https://www.dmso.mil/public/transition/hla/. Table 2. Strengths and weaknesses of HLA.
HLA Strengths HLA Weaknesses

program. Table 3 provides an assessment of the strengths and weakens of DIS by the IAPG. Further information on DIS can be found at http://www.sei.cmu.edu/architecture/Architectures_ for_DIS.html#291. Table 3. Strengths and weaknesses of DIS.
DIS Strengths DIS Weaknesses

HLA is an open standard that will be supported beyond 2006 (ref. IEEE 1516). The architecture can be implemented across different computing environments. Provides a documented process for developing distributed simulation systems, e.g., the federation development execution process. More "bandwidth" friendly.

HLA developments may be subject to significant changes in order to meet future needs. Changes to future HLA standards may have significant impact on local implementations. U.S. will continue to lead HLA development and thus there may be dependence on U.S. support for software implementations.

DIS is an open standard (ref: IEEE 1278.x). The architecture can be implemented across different computing environments.

Scalability ­ difficult to scale up to very large exercises, e.g., >500 simulation entities.

The resources and time required to implement an HLA federation can be Supports real-time, faster significant -- up to than real-time, and eventdouble that required for driven time domains. noncompliant Availability of implementations. commercial off the shelf HLA does not ensure (COTS) software support plug-and-play tools, e.g., data interoperability, it capture/replay, simulation facilitates (federation) exercise communication. management (reduces the requirements for bespoke HLA compliance cannot be established in developments). abstract, but only by reference to a defined federation. DIS is another framework for linking real-time and potentially distributed simulations. Defined under IEEE Standard 1278, the chief objective of DIS was to create real-time, synthetic, virtual representations of the warfare environment. This environment is created by interconnecting separate, distributed computers/simulators, called component simulator nodes. These nodes typically represent entities on the order of a military unit. DIS has its roots in the DARPA simulation networking 17

Efficiency ­ rigid structure of data protocols (PDUs) leads Provides a set of well to inefficiency of defined data protocols to network resources, e.g., support the interaction of wide area network real-time simulation (WAN) bandwidth. systems. IEEE standards will not Availability of COTS be developed to meet software support tools future simulation (e.g., DIS Stealth requirements. Viewers, DIS Data DIS only supports realLoggers) reduces the requirements for bespoke time simulations, it does not support event driven, developments. faster than real-time DIS is a stable "product." applications. Limited number of PDUs. HLA and DIS are examples of frameworks that integrate "real-time" simulation models. Information is passed actively between models and timing between models is synchronized. This method may support some aspects of infrastructure model integration. The issue may arise however when the computational time for processing a model makes this type of integration unrealistic, i.e., the computational requirements greatly exceed real-time. One potential method to address this issue and also to provide a more rapid response capability is to develop scenario libraries consisting of preprocessed scenarios with run profiles available for immediate access. Los Alamos National Laboratory utilizes this approach with their Scenario Library Visualizer. Another method of model integration consists of devising a common architecture to distribute

information between models. This method is currently used by Los Alamos National Laboratory and NISAC to relate impacts across different infrastructure models. In a broad sense, a damage profile based on expected physical damage is constructed first. An example of this is determining power outages based on projected high wind profiles, surge, and flooding models associated with hurricanes. The physical impact of the event is transformed into impact on the power grid in terms of outage areas. This information is then passed to other models (water, financial, transportation, etc.) such that the corresponding impact in the electrical power sector integrates into other sectors. In this way, impact cascades across infrastructure boundaries and presents potential effects via infrastructure interdependencies. This type of model integration works well when the timing between infrastructures precludes a true federation of simulations. Interdependency discovery and validation is another challenging area of research. Although physical interdependencies can be derived by subject matter experts, doing so on a large scale is a

resource challenge. Discovery methods and tools, including automated mapping, are essential for highfidelity models. Fast Analysis Infrastructure Tool (FAIT) by Sandia National Laboratory conducts rough first order interdependency mapping based on simple rule sets. The IEISS model and Los Alamos National Laboratory suite of models use outage areas to identify geospatial and gross order dependencies. The Critical Infrastructure Modeling System (CIMS) developed by Idaho National Laboratory likewise supports geospatial dependencies, but requires manual direct association for other dependencies. Identifying and mapping societal interdependencies is perhaps the most challenging aspect in terms of discovery, mapping, and validation. Identifying a multicultural response and the duration of impacts on a society is challenging. The impact of "like" events can be speculated, but drawing inferences to unforeseen and rare events relative to the other infrastructure sectors is a challenging area of active research. This is one of the main focuses of PMESII research that is underway.

18

CONCLUSIONS
Infrastructure interdependency modeling is a relatively new area of research and analysis, but recent events of both natural disasters and malicious acts have shown that the impact of these cross infrastructure relationships can be measured. Significant research efforts are underway in the U.S. and abroad. One observation resulting from this effort is that no cross program working group or forum is specifically dedicated to this critical area of research. Most research exchange occurs within

specific programs. Consequently, a limited exchange of ideas has occurred across the sponsoring agencies in this area. The strongest collaboration exists between DHS and DOE, mainly due to the fact that the same research teams are sponsored by both organizations. One suggestion from our study is the development, whether formally or informally, of a national or international working group with a central focus of infrastructure interdependency analysis. It is hoped that this state-of-the-art report will serve to not only report on current activities, but will also act as a catalysts for information exchange for such activities.

19

ACKNOWLEDGEMENT
We appreciate the many contributors to this report. Our preferred method has been to directly interact with the project leaders in collecting this information. All that have participated have been extremely supportive. Again, this is an ongoing project and we apologize to those efforts which were not recognized in this first report.

Please forward comments on material contained within this document and also points of contacts for those efforts not covered in this initial document to Donald.Dudenhoeffer@inl.gov. Finally, we would like to express our gratitude to Dr. Steve Fernandez of Los Alamos Laboratory who acted as a constant guide and source of data for this report.

20

Appendix A

21

Table Abbreviations: Infrastructure Sectors EP Electric Power NG Natural Gas DW Drinking Water SW Sewage Water ST Storm Water HA Human Activity FN Financial Networks SCADA Supervisory Control and Data Acquisition TC Telecom CN Computer Networks OL Oil Pipeline RL Rail System HW Highway System WW Waterway System POL Policy/Regulatory constraints Simulation Type I Input-Output Model A Agent-based Intended Users Types IA Internal Analyst EA External Analyst B Both Maturity Level RS Research DV Development MI Mature Internal MC Mature Commercial

22

Model Name Organization POC

Agent-Based Infrastructure Modeling and Simulation (AIMS) University of New Brunswick Infrastructures Dr. Ali Ghorbani, Professor Various ghorbani@unb.ca Dr. Stephen Marsh, Adjunct Professor Stephen.Marsh@nrc-cnrc.gc.ca

Description Overview ­ AIMS is an agent-based system to simulate and model the (national and cross-border) interdependencies and survivability of Canada's Critical Infrastructures. Development goals ­ Goals are to incorporate into complete critical infrastructure (CI) crisis management system and plan to model New Brunswick's Information and Communication Technology (ICT), power, water, etc. Intended users ­ Users will include CI managers, users, planners, and emergency services personnel. System output ­ Visualization for training monitoring and planning. It's a possibility to add mapping systems. Maturity ­ The system is in development. Areas modeled ­ New Brunswick Critical Infrastructures. Customers/sponsors ­ National Research Council Canada (CNRCC). Model Framework Underlying model ­ Agent-based modeling uses universal mark-up language (UML) and service oriented architectures. Plans are to use a multi-agent development kit in the future such as Agent Oriented Software Group (AOS) JACKTM, Java Agent DEvelopment (JADE) framework, or other agent software. Simulation ­ Simulation scenarios have included the Moncton area forest fire, the Saint John Port disaster, and a Border incident. Data format ­ The data and text are in UML and Environmental Systems Research Institute, Inc. (ESRI)'s ArcGIS formats. Sensor data ­ Not specified. Coupling with other models ­ This model is designed to couple with other models, but that capability has not been tested to date. Human activity modeling ­ Included in model. System Requirements Not specified. Hardware Not specified. Software Other Notes

References Stephen Marsh, Critical Infrastructure Interdependencies, http://iit-iti.nrc-cnrc.gc.ca/colloq/0405/0411-04_e.html, November 4, 2004, Webpage visited July 10, 2006.

23

24

Model Name Athena Organization POC

On Target Technologies, Inc. Dr. Brian Drabble brain@ontgttech.com Dr. Maris "Buster" McCrabb buster@dmmventures.com

Infrastructures All (physical to conceptual)

Description Overview ­ Athena is an analysis and modeling tool that is designed to analyze a network of nodes (actors, concepts and physical) as a "system of systems" by merging various political, military, economic, social, information, and infrastructure (PMESII) models and their associated crossdependencies. Athena incorporates several reasoning algorithms that allow sophisticated inter- and intra-dependency analysis between and through nodes. Model construction is quick and simple point and uses a simple point and click interface. Development goals ­ Automatic Network Extraction (engineering models), Semantic Reasoning across transitive dependencies & Interfacing to different information sources. Intended users ­ Military for in analyzing disruptive military effects, Law Enforcement for analyzing disruptions of criminal gangs and enterprises, Disaster/Network Recovery to determine repair priorities, and Economic for competitive analysis. Output ­ Graphical interface showing nodes and linkages with criticalities and interdependencies indicated. Multiple analytical capabilities. Can be linked to GIS data. Maturity ­ Evolving. Areas modeled ­ Athena is capable of modeling any entity including countries, states, cities, roads, and facilities. Customers/sponsors ­ Air Force Research Laboratory (AFRL) IFSA sponsored the original work. Funding is now provided by DARPA and USSTRATCOM who will deploy the tool in late 2006. Model Framework Underlying model(s) ­ Fusion of Barlow's model of horizontal cross-dependency with weighting, Warden's model of vertical cross-dependency, and the McCrabb-Drabble model of time-phased linkages between models. This is a fractal model that allows the description of a Strategic Entity through Centers of Gravity (COG) to Target Systems to Target sets and where appropriate targets. Simulation ­ System allows full-scale simulations. Data format ­ Accepts data in variety of formats. Sensor data ­ Accepts sensor data/feeds to update model nodes and changing interactions (e.g., strength) between nodes. Human activity ­ This tool models human activity/capability as part of the network (e.g., loss of plant manager may decrease network capability). Nodes may be humans or concepts. Coupling with other models ­ Couples readily with other engineering models, databases, sensor networks, etc. System Requirements Laptop 2 GB processor speed, 60 GB hard drive, 500 MB RAM. Hardware Windows XP or similar, program is written in C. Software Other Notes

25

References Athena: Effects-based Cross-Dependency Modeling for Target Systems Analysis Final Report. (Limited distribution) Final Athena Demonstration (Microsoft PowerPoint presentation).

26

Model Name Organization POC

CARVER2 TM National Infrastructure Institute Center for Infrastructure Expertise Ronald Peimer rpeimer@ni2.org

Infrastructures User defined

Description Overview ­ CARVER2 is a simple software program that provides a quick and easy way to prioritizes potential terrorist targets. It compares and rates the critical infrastructure and key assets in jurisdictions by producing a mathematical score for each potential target. It is the first step for conducting more indepth vulnerability assessments. CARVER2 helps users make "apples vs. oranges" comparisons such as a water system vs. an energy grid vs. a bridge. Development goals ­ None goals have been stated. Intended users ­ Federal, state and local government officials are the intended users for this program. Output ­ The CARVER2 tool outputs various reports with priority scores and background information for different infrastructure elements. Maturity ­ This is a free product by request. Areas modeled ­ Determined by user. Customers/sponsors ­ This tool is Sponsored by the US Department of Commerce, National Institute for Standards and Technology (NIST). Model Framework Underlying model(s) ­ The support is a relational database. Simulation ­ This tool has no simulation capability. Data format ­ Text. Sensor data ­ No sensor data has been incorporated in the tool. Human activity ­ Not modeled. Coupling with other models ­ There is no coupling with other models. System Requirements PC or laptop running Microsoft Windows operating system. Hardware Distributed via CD no other software needed. Software Other Notes

27

References NI2 Center for Infrastructure Expertise Critical Infrastructure Library, http://www.ni2ciel.org/,Webpage visited July 3, 2006. National Infrastructure Institute home page, http://www.ni2.org/default.asp, Webpage visited July 3, 2006 CARVER2 Project Page, http://www.ni2cie.org/CARVER2.asp, Webpage visited July 3, 2006. 28

Model Name Organization POC

COMM-ASPEN Sandia National Laboratory

Infrastructures FN, TEL

Description Overview ­ CommAspen is a new agent-based model for simulating the interdependent effects of market decisions and disruptions in the telecommunications infrastructure on other critical infrastructures in the U.S. economy such as banking and finance, and electric power. CommAspen extends and modifies the capabilities of Aspen-EE, an agent-based model previously developed by Sandia National Laboratories to analyze the interdependencies between the electric power system and other critical infrastructures. CommAspen has been tested on a series of scenarios in which the communications network has been disrupted, due to congestion and outages. Analysis of the scenario results indicates that communications networks simulated by the model behave as their counterparts do in the real world. Results also show that the model could be used to analyze the economic impact of communications congestion and outages. Development goals ­ To analyze interdependent infrastructure systems in a more holistic way, Sandia and other research institutions have developed models of critical infrastructure systems using agentbased approaches. Sandia's first agent-based model of the U.S. economy, developed in the mid-1990s, is called Aspen. This model is a Monte Carlo simulation that uses agents to represent various decisionmaking segments in the economy, such as banks, households, industries, and the Federal Reserve. An agent is a computational entity that receives information and acts on its environment in an autonomous way; that is, an agent's behavior depends at least partially on its own experience. Through the use of evolutionary learning techniques, Aspen allows us to examine the interactive behavior of these agents as they make real-life decisions in an environment where agents communicate with each other and adapt their behaviors to changing economic conditions, all the while learning from their past experience. In 2000, Sandia developed a new model of infrastructure interdependency called AspenEE. This model extended the capabilities of Aspen to include the impact of market structures and power outages in the electric power system, a critical infrastructure, on other infrastructures in the economy. One of the limitations of agent-based models in current development at Sandia and other research institutions is that communication is treated simply as a message passing between agents. Effectively, the telecommunications infrastructure is not specifically represented. None of the models simulates the differences in communication over telephone, computer, wireless, or other networks and therefore cannot model the impact of specific communication failures on the whole system. Nor can current models simulate the impact of other infrastructure failures on telecommunications. To address the communications deficiencies described above, Sandia revised and restructured the Aspen-EE model to include a more realistic representation of the telecommunications infrastructure. This new model of infrastructure interdependency is called CommAspen. In CommAspen, communication is treated as an integrated agent system capable of creating, transforming, sending, receiving, and storing information and messages over time and across distance. With CommAspen, we can model communication networks or medium-specific vulnerabilities to failures and their dependence on supporting infrastructures like power. Intended users ­ Internal analyst. System output ­ Not specified. Maturity ­ Development. Areas modeled ­ Not specified. Customers/sponsors ­ Not specified. 29

Model Framework Underlying model ­ There are several ways that we can implement the notion of infrastructures in CommAspen. One method of representing certain types of infrastructures in CommAspen is through the use of spigots and sinks. Such infrastructures are for commodities that run continuously, like water from a municipality and electricity from a local utility. A sink is where a producer puts product into an infrastructure. For example, a power company may have a natural gas-fired electric generating plant producing power. It would put power on the transmission lines by passing the power into the associated sink. A spigot is where a consumer gets the product, such as turning on the lights in a residence or getting water from a faucet. Simulation ­ Agent Based Model. Data format ­ Not specified. Sensor data ­ None. Coupling with other models ­ No. Human activity modeling ­ Not Known. System Requirements Not specified. Hardware Not specified. Software Other Notes

Images: None.

References Barton, Dianne C., Eric D. Edison, David A. Schoenwald, Roger G. Cox, and Rhonda K. Reinert, "Simulating Economic Effects of Disruptions in the Telecommunications Infrastructure", SAND REPORT, SAND2004-0101, Printed January 2004.

30

Model Name Organization POC

Critical Infrastructures Interdependencies Integrator (CI3) Argonne National Laboratory Infrastructures Dr. James Peerenboom EP,NG,SCADA,TC jpeerenboom@anl.com

Description Overview ­ CI3 is a software tool for emulating (Monte Carlo simulation) the amount of time or cost (or both) needed for activities that must be completed to restore a given infrastructure component, a specific infrastructure system, or an interdependent set of infrastructures to an operational state. The software tool provides a framework for recognizing interdependencies and incorporating uncertainty into the analysis of critical infrastructures. Development goals ­ No goals stated. Intended users ­ Infrastructure owners. System output ­ Graphs and tables of completion time and cost distributions for repairs to quantify the impacts of infrastructure disruptions. Maturity ­ The system is in development. Areas modeled ­ No specific areas are mentioned. Argonne has developed transition diagrams for repair of damages to the following: natural gas transmission pipeline, petroleum, oil, liquids (POL) pumping station, natural gas city gate station, propane air plant, natural gas compressor station, natural gas underground storage facility, supervisory control and data acquisition (SCADA) communications tower, electrical substation, transformer, and an optical telecommunications cable. Customers/sponsors ­ U.S. Department of Energy. Model Framework Underlying model ­ Transition diagrams coupled to Monte Carlo simulator. Simulation ­ Transition diagrams are easy to create via point-and-click techniques to simulate recovery and restoration activities for covered infrastructure. Data format ­ Not specified. Sensor data ­ Model does not accept sensor data. Coupling with other models ­ None. Human activity modeling ­ Human activities (travel, repair, assessment) are included in simulations. System Requirements Not specified. Hardware Not specified. Software Other Notes

31

References

32

Model Name Organization POC

Critical Infrastructure Modeling System (CIMS©) Idaho National Laboratory (INL) Donald Dudenhoeffer Donald.Dudenhoeffer@inl.gov

Infrastructures EP, SCADA, HW, HA, POL, PMESII

Description Overview ­ A modeling and simulation framework that combines geospatial information and a four dimensional (4D) environment (time-based) to support `what if' analysis. Development goals ­ Provide decision makers with a highly adaptable and easily constructed `wargaming' tool to assess infrastructure vulnerabilities including policy and response plans. Operating at a high level of simulation, it supports rapid `point and click' model development to allow the adaptation of models to rapidly changing environments. Intended users ­ Emergency planners and responders. System output ­ Four dimensional geospatial visualization in a VTK framework along with report generation. Maturity ­ Development ­ in the process of commercial licensing. Areas modeled ­ Idaho National Laboratory, New Orleans Louisiana. Customers/sponsors ­ Research has been ongoing for the past 4 years under the INL National Security Divisions. Sponsors have included the INL's internal research program, the Department of Energy, the U.S. Air Force Research Laboratory (AFRL), and negotiations are underway with the State of Louisiana. Model Framework Underlying model ­ The underlying model is a network representation of infrastructure utilizing nodes and edges for assets and relationships. Graphical objects such as aerial images, 3DS images, or VRML models can be tied to the assets. Additionally, information can be embedded within nodes such as documents, web site hyperlinks, web cams, avis, etc Simulation ­ Agent-based discrete event simulation. Data format ­ Flat files are used as direct feeds to the simulations. These files can be fed by a multitude of different databases including Access, GIS, etc Sensor data ­ Agent objects(nodes) can have autonomous behaviors or they can be fed by external sensor input. Coupling with other models ­ Yes. Human activity modeling ­ Human activity can be modeled directly or as the result of policy/procedure enactment. System Requirements Cross platform compatibility ­ Windows, UNIX/LINUX, and Solaris. Internet Hardware connectivity required to access embedded links. No external software to CIMS ~ requires < 5 meg of disk space. Software

33

Other Notes The objective of CIMS was to create a rapid modeling and analysis capability that did not require extensive data collection or proprietary GIS software. As such, CIMS allows the ability to create models and infrastructure simulations on the fly embedding new intelligence as it becomes available. Model development can start with an aerial image or a scanned/sketched chart/map image. All information is georeferenced. Models construction can occur via one of three methods. x Direct manipulation of the network descriptor flat files x Conversion from a database to the flat file format x Point and click network construction via the Model Builder Application. User interactivity with the Model. The models were developed with a wargaming approach to allow maximum user interaction with the simulation. Thus the user has several different ways to interact with the data: x An event script can be created to initiate specific events at a designated time x The user can select and directly manipulate the state of individual nodes and edges, i.e., shutting down an electrical substation or making a bridge impassible x The user can inject events during runtime, i.e., placing and detonating a bomb to observe cascading impacts.

34

New Orleans Model

Damage Profile due to flooding ­ illustrating loss of infrastructure.

3D Stereo Representation of downtown on SGI P i

Images 35

Model showing loss of an Electrical Substation

Rotated Side view showing building profiles at an angle with the electrical infrastructure separated from the buildings to highlight the connectivity. Multiple infrastructures can be displayed to show direct and spatial relationships.

36

References
Dudenhoeffer, D.D, M. R. Permann, and M. Manic, "CIMS: A Framework For Infrastructure Interdependency Modeling And Analysis." Submitted to Proceedings of the 2006 Winter Simulation Conference, ed L. F. Perrone, F. P. Wieland, J. Liu, B. G. Lawson, D. M. Nicol, and R. M. Fujimoto. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, 2006. Dudenhoeffer, D. D., M. R. Permann, and R. L. Boring, 2006. Decision consequence in complex environments: Visualizing decision impact. In Proceeding of Sharing Solutions for Emergencies and Hazardous Environments. American Nuclear Society Joint Topical Meeting: 9th Emergency Preparedness and Response/11th Robotics and Remote Systems for Hazardous Environments. Dudenhoeffer, D. D., M. R. Permann, and E.M. Sussman. 2002. A Parallel Simulation Framework For Infrastructure Modeling And Analysis. In Proceedings of the 2002 Winter Simulation Conference, ed E. Yücesan, C. H. Chen, J. L. Snowdon, and J. M. Charnes, 1971-1977. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers.

Critical Infrastructure Modeling System Fact Sheet <http://www.inl.gov/nationalsecurity/factsheets/docs/cims.pdf>

37

38

Model Name Organization POC

The Critical Infrastructure Protection Decision Support System (CIP/DSS) Los Alamos National Laboratory Infrastructures Randy Michelsen ALL rem@lanl.gov Sandia National Laboratories Theresa Brown tjbrown@sandia.gov

Description

Overview ­ The Critical Infrastructure Protection Decision Support System (CIP/DSS) simulates the dynamics of individual infrastructures and couples separate infrastructures to each other according to their interdependencies. For example, repairing damage to the electric power grid in a city requires transportation to failure sites and delivery of parts, fuel for repair vehicles telecommunications for problem diagnosis and coordination of repairs, and the availability of labor crews. The repair itself involves diagnosis, ordering parts, dispatching crews, and performing work. The electric power grid responds to the initial damage and to the completion of repairs with changes in its operating characteristics. Dynamic processes like these are represented in the CIP/DSS infrastructure sector simulations by differential equations, discrete events, and codified rules of operation. Many of these variables are output metrics estimating the human health, economic, or environmental effects of disturbances to the infrastructures. CIP/DSS will assist decision makers in making informed choices by:
x Functionally representing all 14 critical infrastructures with their interdependencies x Computing human health and safety, economic, public confidence, national security, and environmental impacts x Synthesizing a methodology that is technically sound, defensible, and extendable. Development goals ­ Charter is to model all infrastructures and key assets. Used for quick response on areas Los Alamos National Laboratory (LANL) doesn't have data for. Intended users ­ Internal analyst at LANL. System output ­ Graphs representing the impact on multiple state variables such as hospital beds occupied, etc.

Maturity ­ Development ­ Initiated as a proof-of-concept in August 2003. Completed a prototype model and two case studies in February2004.
Areas modeled ­ Not specified. Customers/sponsors ­ DHS. Model Framework Underlying model ­ The national and metropolitan consequence models are implemented using Vensim, which reads input parameters from and writes output time series to an Oracle relational database of "consequence" metrics, which are abstracted into a much smaller set of "decision" metrics. The decision support software (written in Visual Basic) accesses the decision database to compute utility values for various scenarios and alternatives. Simulation ­ Vensim is used for developing, analyzing, and packaging high quality dynamic feedback models. Models are constructed graphically or in a text editor. Features include dynamic functions, subscripting (arrays), Monte Carlo sensitivity analysis, optimization, data handling, and application interfaces. Data format ­ Vensim Model. Sensor data ­ No ability to input live data feeds. 39

Coupling with other models ­ No. Human activity modeling ­ Human activity can be modeled directly or as the result of policy/procedure enactment. System Requirements The Vensim family of software runs on Windows (95/98/Millennium/NT/2000/XP) Hardware and the Power Macintosh running System 7 or higher (in Classic mode under OSX). Vensim requires 8 MB of memory and 8 MB of disk space for a full installation. A demonstration version of Vensim is available free for either Windows or Macintosh. CIPDSS is a model built within Vensim simulation software by Ventura Software (http://www.vensim.com/brochure.html). Other Notes CIP/DSS (Critical Infrastructure Protection Decision Support System) simulates the dynamics of individual infrastructures and couples separate infrastructures to each other according to their interdependencies. CIP/DSS models asset information at the aggregate level. For example with a focus area, it can estimate the number of hospital beds affected by an event, but it cannot directly retrieve information relative to a particular hospital. It utilizes the commercial simulation software Vensim.

40

References Bush B, L. Dauelsberg, R. LeClaire, D. Powell (LANL), S. DeLand (SNL), and M. Samsa (ANL), Critical Infrastructure Protection Decision Support System (CIP/DSS) Project Overview, LA-UR-051870, July 2005.

41

42

Model Name Organization POC

Critical Infrastructure Protection (CIP) Modeling and Analysis (CIPMA) Program Australian Government ­ Attorney General's Infrastructures Department (AGD) FN, TC, EP, NG, OL Michael Jerks ­ Director, Major Projects Michael.Jerks@ag.gov.au

Description Overview ­ The Critical Infrastructure Protection Modeling and Analysis program (CIPMA) is a computer based tool to support business and government decision making for critical infrastructure (CI) protection, counter-terrorism and emergency management, especially with regard to prevention, preparedness, and planning and recovery. CIPMA is designed to examine the relationships and dependencies within and between critical infrastructure systems, and to demonstrate how a failure in one sector can greatly affect the operations of critical infrastructure in other sectors. CIPMA uses a vast array of data and information from a range of sources to model and simulate the behavior and dependency relationships of critical infrastructure systems. The capability will include a series of impact models to analyze the effects of a disruption to CI services. The CIPMA Program currently focuses on three priority sectors: banking and finance, communications, and energy. The capability was launched by the Attorney-General in February 2006. "Proof of concept" of the capability was successfully demonstrated to key business and government stakeholders in May 2006. Although CIPMA is still in development, results from the capability are already assisting the development and direction of government policy in national security and critical infrastructure protection (CIP), and helping owners and operators to better protect their critical infrastructure. Development goals ­ The current focus is on broadening and deepening CIPMA coverage of the three priority sectors, the Sydney commercial business district (CBD) precinct, and development of impact models for the Decision Support Module. The impact models will assess the flow-on consequences of a CI service disruption, the economic impacts of the disruption, the effects on population, time/duration and area of the disruption, and the behavior of networks and clusters of infrastructure as a result of the service interruption. Work on a fourth sector will commence by July 2007. Intended users ­ Users include CI owners and operators and Australian local governments. System output ­ Output will include geographic information system (GIS) functionality for data capture, management, and visualization. System behavior will determine dependencies and time-based impacts of disruptive events on infrastructure networks. Maturity ­ In development, some tools are complete. Areas modeled ­ Australian critical infrastructure networks and high priority precincts (e.g., capital cities). Customers/sponsors ­ Australian government, state and territory governments, CI owners and operators. Model Framework Underlying model(s) ­ System Dynamic Models. Simulation ­ Telecommunication connectivity matrix and expert systems. Data format ­ The format is geographic information system (GIS) and relational database. Sensor data ­ Not currently equipped for sensor input. Human activity ­ Contains human activity model. Coupling with other models ­ Model couples with earthquake, tsunami inundation, bomb blast, and plume models. System Requirements Not specified. Hardware ArcGIS, ArcSDE, Oracle, Vensim DSS, Dynamic Network System (DNS), CLIPS, Software 43

Java Runtime Environment (JRE). Other Notes CIPMA is a very detailed modeling and analysis initiative which contains sensitive business information about the operation of Australia's critical infrastructure networks, relationships and dependencies. The IP is owned and managed by Attorney-General Department (AGD) on behalf of the Australian Government. The CIPMA Development Team of AGD, Geoscience Australia (GA) and the Commonwealth Scientific and Industrial Research Organization (CSIRO) has been in discussions with the US Department of Homeland Security (DHS) and Argonne, Sandia, and Los Alamos National Laboratories regarding the Critical Infrastructure Decision Support System (CIP-DSS), and the similarities and differences between the two capabilities, since November 2004. AGD is currently preparing a Project Arrangement for ongoing consultation with DHS and the three National labs under the Homeland Security Science and Technology Treaty (HSST). References Fact sheet on CIPMA program http://www.tisn.gov.au/agd/WWW/rwpattach.nsf/VAP/(7A188806B7893EBA0402BC1472412E58)~ Overview+of+CIPMA.PDF/$file/Overview+of+CIPMA.PDF, Webpage visited July 3, 2006. AusGeo News, Protecting the Nation, http://www.ga.gov.au/ausgeonews/ausgeonews200509/cip.jsp, Issue No. 79, September 2005, Webpage visited July 3, 2006.

44

Model Name Organization POC

Critical Infrastructure Simulation by Interdependent Agents (CISIA) Universita Roma Tre Infrastructures Stefano Panzieri EP,SCADA panzieri@uniroma3.it Giovanni Ulivi ulivi@uniroma3.it

Description Overview ­ This model is described by the authors as a hybrid of the two modeling approaches; interdependency analysis and system analysis. It is a bottom-up complex adaptive systems (CAS) model using interactive agents. The critical infrastructure simulation by interdependent agents (CISIA) simulator is designed to analyze short term effects of failures in terms of fault propagation and performance degradation (Panzieri, 2004). The simulator is based on Recursive Porus Agent Simulation Toolkit, Repast, open-source agent-based development software with libraries of classes for creating, running, displaying and collecting data from a agent based simulations. It extends the Java classes of Repast defining a new class for each type of macro component present into any infrastructure: such as, electric power plant, transmission line, telecommunication channel, waste-water system, etc. Development goals ­Work is ongoing to further validate the CISIA approach and to analyze how intelligent reaction, and autonomy capabilities (e.g., decentralized control strategies), might be used to improve the robustness of the system of system's composed by different heterogeneous and interdependent infrastructures. Intended users ­ Infrastructure owners, planners, and emergency responders. System output ­ Graphic models showing the operative level incidence matrix and physical fault incidence matrices (FIMs) between elements in the model. In this case air conditioning, electric power, and SCADA. Maturity ­ The system is in development. Areas modeled ­ An unspecified (for security reasons) University Campus. Customers/sponsors ­ Not indicated. Model Framework Underlying model ­ Agent-based model based on Repast, in order to handle many heterogeneous infrastructures into a single framework. Agent behavior is abstracted to allow use of a small set of common quantities; operative level, requirements (needs), and faults. Agent interactions include; induced faults, input requirements, and input operative level. Outputs include: propagated faults, output requirements, and output operative level. Simulation ­ During simulation agents communicate via messages. An agent sends messages to its neighbors to specify its requirements to communicate its level of service (operative level), and to propagate faults (physical-faults, geographical-faults, and cyber faults). Data format ­ Relational database. Sensor data ­ Model does not accept sensor data. Coupling with other models ­ CISIA implements an easy-linkage/black box philosophy: any model obtains connecting together agents without any modification of their internal structure. Human activity modeling ­ Not incorporated.

45

System Requirements Not specified. Hardware Not specified. Software Other Notes Each agent class defines the behavioral roles of the element and its input/output quantities in term of which resources the agent needed and supply. Moreover, the class defines which type of failure can be propagated to (generated from) the agent. An agent may propagate different types of failure to a different set of neighbors.

References Panzieri, S., R. Setola, G. Ulivi (2004). An agent based simulator for critical interdependent infrastructures. Proc. 2nd International Conference on Critical Infrastructures, October 24-27, 2004. Panzieri, S., R. Setola, G. Ulivi , An Approach to Model Complex Interdependent Infrastructures, International Federation of Automatic Control (IFAC), http://www.dia.uniroma3.it/~panzieri/Articoli/WorldIFAC05CIIP.pdf#search='An%20Approach%20to%20Model%20Complex%20Interdependent%20Infrastructur es', Webpage visited July 10, 2006.

46

Model Name Organization POC

Distributed Engineering Workstation (DEW) Electrical Distribution Design, Inc. Dr. Robert Broadwater dew@vt.edu

Infrastructures EL, SCADA

Description Overview ­ The Distribution Engineering Workstation (DEW) provides over 30 applications for analysis, design, and control of electrical and other physical network systems. DEW allows all of its components (data sets and algorithms) to be reused by a new application, allowing new solutions to build on top of existing work. This provides for cross collaborations among different groups and the emergence of solutions to complex problems. DEW is being used to identify and analyze interdependencies in large scale electrical power systems and fluid systems of aircraft carriers. DEW is open architecture, non-proprietary. Development goals ­ Electrical Distribution Design, Inc. (EDD) continues to develop and support DEW. They aspire to achieve combined analysis of systems with millions of nodes and to develop a seamless approach to asset management. DEW's architecture provides an open platform for development. The DEW system model can be linked to asset management records, daily operational procedures, events, long- and short-term planning, and more. Intended users ­ Users are utilities, analysts, and military. System output ­ The system is used for operation and control of electrical system and analysis of reconfiguration of damaged systems. Maturity ­ Mature product is in broad use. Areas modeled ­ This model has been used in St. Louis, MO, Detroit, MI, Consolidated Edison, NY, Aircraft Carriers. Customers/sponsors ­ Electric Power Research Institute (EPRI) along with Department of Defense and Department of Energy sponsored the original development. Users include Northrop Grumman (naval applications), Detroit Edison (Detroit, MI), Ameren (St. Louis, MO), Orange and Rockland (Pearl River, NY), and Consolidated Edison (New York). Model Framework Underlying model ­ EDD's approach is built around a combination of concepts from graph theory, physical network modeling, and generic programming. The DEW model incorporates power flow, fault, reliability, reconfiguration for restoration, and over 30 other algorithms. Simulation ­ Simulations may be run manually with mouse and keyboard, automatically controlled from user developed applications, or set up to run in batch mode over numerous systems and/or time points. Data format ­ Model data is stored in relational SQL-compliant databases; real-time measurement data comes from common object request broker architecture (CORBA) interface or plant information (PI) time series databases. Sensor data ­ DEW can handle any number of measurements and any types of measurements that are modeled, through its PI or CORBA interface. Coupling with other models ­ DEW can attach to other models, such as geographic information system (GIS) models, via provided interface. System Requirements Laptop/Server/Circuit server. Hardware Win 2000, XP, User interface. Software Other Notes EDD is working with the utility industry, Virginia Tech, and other universities to develop a 47

comprehensive Integrated System Model (ISM) based design, operations and maintenance management system. This concept is being applied to critical infrastructures including naval ships and gas and water utilities. Through work with the utility industry and Department of Energy, EDD has demonstrated it is possible to use the same ISM for analysis, design, operations, and real-time control. EDD has also used ISM based analysis to manage reconfigurable system models with more than 3 million objects and 200 million attached historical measurement values. The ISM provides a complete, seamless view of a physical plant that forms a common context for multi-discipline team collaboration, distributed processing, synergistic research and development, and providing infinite extensibility. Any data or algorithm that can be attached to the ISM is also associated with all other data and algorithms attached to the ISM. The ISM uses linked list type traces to dynamically adapt data management and analysis whenever the system is changed through modification, maintenance or operation. EDD is structuring its current research and development work so that it that can eventually be combined into a generic integration platform for collaborative analysis, design, and operations for energy systems (CADOE). CADOE will directly support and structure low overhead collaboration among electric utilities, gas utilities, regulatory and policy making agencies, suppliers, integrators, aggregators, and customers. CADOE is envisioned to encompass simulation, analysis, alternative design evaluation, training, and real-time operations support.

Model of Ship Critical Infrastructure.

48

Dense Electrical Power System Model References Broadwater, Robert, et al., Power Engineering, http://www.ecpe.vt.edu/news/ar04/power2004.pdf#search='distributed%20engineering %20workstation%20epri', Webpage visited July 3, 2006. SAM Six, Products: Dew, http://www.samsix.com/dew.htm, Webpage visited July 3,2006. Tam, Kwa-Sur and Robert Broadwater, Virginia Tech Presentation, http://www.eng.vt.edu/research/dom_pres/TamBroadwater%20Systems%20Presentation.pdf#search='vt%20dew' Webpage visited July 3, 2006.

49

50

Model Name Organization POC

Electricity Market Complex Adaptive System (EMCAS) Argonne National Laboratory Guenter Conzelmann (ANL) guenter@anl.gov

Infrastructures Power Systems and Markets

Description Overview ­ Electricity Market Complex Adaptive System (EMCAS) uses agent-based modeling to simulate the operation of complex power systems. EMCAS can be used as an "electronic-laboratory" to probe the possible operational and economic impacts on the power system of various external events. Market participants are represented as "agents" with their own set of objectives, decisionmaking rules, and behavioral patterns. Agents are modeled as independent entities that make decisions and take actions using limited and/or uncertain information available to them, similar to how organizations and individuals operate in the real world. EMCAS includes all the entities participating in power markets, including consumers, generation companies (GenCos), Transmission Companies (TransCos), Distribution Companies (DisCos), Demand Companies (DemCos), Independent System Operators (ISO) or Regional Transmission Organizations (RTO), and regulators. Development goals ­ Continue to develop EMCAS as a new approach to model and simulate the operations of restructured electricity markets. Intended users ­ EMCAS was first applied for a regulatory commission in the mid-western United States. At the beginning of 2005, the software became commercially available and current clients include research institutes, power companies, transmission companies, and regulatory offices in South Korea, Portugal, and Spain. The Iberian EMCAS application includes the simulation of hydropower, wind power, and a variety of other renewable resources. System output ­ EMCAS utilizes a graphical user interface to develop market configurations, display model inputs, and analyze simulation results (see screen captures on next page). Results are stored in HDF format and can be exported in text and spreadsheet formats. In addition to the energy spot markets and bilateral financial contract markets, EMCAS also includes a simplified representation of ancillary services markets; Detailed representation of the transmission system, using a Direct Current Optimal Power Flow (DC OPF) algorithm to compute locational marginal prices (LMP) and identify transmission congestion and price impacts of congestion; Chronological simulation of hourly market prices over short or long time periods; Hourly bid-based market clearing, scheduling and dispatch in day-ahead and real-time markets; Representation of different bidding strategies, from production cost bidding to various forms of physical and economic withholding strategies; Ability to change prevailing market rules (regarding congestion management, pricing mechanisms, price caps etc.) provides the opportunity to test the robustness and vulnerability to gaming of different market designs; and Calculation of cost, revenues, and profits for all relevant agents in the system. Maturity ­ Commercial Product distributed by ADICA Consulting, LLC. Areas modeled ­ Illinois electrical market, Iberia, France, South Korea, Poland, Central Europe Customers/sponsors ­ At the beginning of 2005, the software became commercially available and current clients include research institutes, power companies, transmission companies, and regulatory offices in South Korea, Portugal, and Spain.

51

Model Framework Underlying model(s) ­ Agent-based modeling and simulation. Simulation ­ EMCAS simulates the operation of a power system and computes electricity prices for each hour and each location in the transmission network. Electricity prices are driven by demand for electricity, cost of electricity production, the extent of transmission congestion, external random or non-random events, such as unit outages or system disruptions, and company strategies. Model results include the economic impacts on individual companies and consumer groups under various scenarios. Data format ­ The user builds the system configuration either within the EMCAS graphical user interface or by preparing and importing a set of well-defined input files. Sensor data ­ The model also includes bilateral financial contracts. Real-time prices are calculated in a real-time dispatch using a DC optimal power flow model. Human activity ­ Model includes different types of consumers (e.g., residential, industrial, and commercial) with their respective electricity consumption profiles. Coupling with other models ­ Couples with hydropower models (e.g., VALORAGUA) and detailed power flow models (e.g., PowerWorld). System Requirements A network with 10 nodes (buses or locations), 70 aggregated thermal generating units, Hardware 13 generation companies, one transmission company, one ISO, and one regulator takes approximately 60 minutes for a one-year simulation (8760 hours) on a desktop PC with a 2.0 GHz AMD Athlon2000+ processor and 1 GB of RAM. For multi-year simulations, it is recommended to use a brand-new, high-end PC, preferably with dual core processors and 2+ GB of RAM. Commercial optimizer (LINGO), long-term hydro model (e.g., VALORAGUA). Software Other Notes Adaptability to Local Market and System Conditions: The EMCAS model is fully customizable and not hardwired to any particular system. Network configurations can be simple and aggregate consisting of a few to several dozen network nodes and links, or detailed bus-level representations with several thousand network elements. The level of detail largely depends on data availability and particular analysis objectives. References ADICA Consulting, LLC., Innovative Solutions for Analyzing Energy Markets, http://www.adica.com/media/downloads/ADICA_Overview_2006.pdf, Webpage visited July 3, 2006. ADICA Consulting, LLC., Electricity Market Complex Adaptive System (EMCAS) Software, http://www.adica.com/media/downloads/EMCAS_Model_Overview.pdf, Webpage visited July 3, 2006. ADICA Consulting, LLC., Electricity Market Complex Adaptive Systems (EMCAS), http://www.adica.com/media/downloads/EMCAS_Specifications_2006.pdf, Webpage visited July 3, 2006. Argonne National Laboratory, Simulating GenCo Bidding Strategies in Electricity Markets with an Agent-Based Model, http://www.iaee.org/documents/denver/Thimmapuram.pdf#search='emcas', Webpage visited July 3, 2006.

52

53

54

Model Name Fast Analysis Infrastructure Tool (FAIT) Sandia National Laboratory (SNL) Organization Infrastructures Theresa Brown POC EP, NG, POL, TL, Emergency Services tjbrown@sandia.gov Description Overview ­ National Infrastructure Simulation and Analysis Center (NISAC) analysts are regularly tasked by the Directorate for Preparedness in the Department of Homeland Security (DHS) with determining the significance and interdependencies associated with elements of the nation's critical infrastructure. The Fast Analysis Infrastructure Tool (FAIT) has been developed to meet this need. FAIT utilizes system expert-defined object-oriented interdependencies, encoded in a rule-based expert systems software language (JESS), to define relationships between infrastructure assets across different infrastructures. These interdependencies take into account proximity, known service boundaries, ownership, and other unique characteristics of assets found in their associated metadata. In a similar fashion, co-location of assets can be analyzed based exclusively on available spatial data. The association process is dynamic, allowing for the substitution of data sets and the inclusion of new rules reflecting additional infrastructures, as data accuracy is improved and infrastructure analysis requirements expand. FAIT also utilizes established Input/Output (I/O) methods for estimating the economic consequence of the disruption of an asset. Each of these analysis elements (interdependency, co-location, economic analysis) have been extended from their original `asset-level' analysis, to allow for the analysis of a specified region. Here, rules written for individual assets are executed en masse on classes of demand infrastructures, like assets of the emergency services (e.g., fire and police stations) and public health (e.g., hospitals) infrastructures, which lie in a defined analysis area, such as a hurricane damage zone, to identify those elements of supply infrastructures (e.g., electric power and telecommunications) which serve the largest number of particular sets of demand infrastructures. FAIT's regional economic analysis takes as input economic data (from the Bureau of the Census) for the disrupted area (as modeled by other NISAC capabilities). When coupled with other NISAC modeling results (estimates for the duration of the disruption and recovery, and the range of magnitude of disruption for the disrupted region), FAIT creates a regional economic analysis, an understanding of the direct and indirect economic consequences, for each sector of the economy in each county in the analysis area. Development goals ­ The FAIT development team is constantly modifying their development goals to best support the requirements of NISAC analysts, in responding to questions from DHS. Current goals include the following: Expansion of existing FAIT capabilities to cover infrastructures not in the current analysis set; Enhancement of economic analysis capability to more accurately represent the consequences of the loss of infrastructure services on the performance of individual industrial sectors; Incorporation of infrastructure-specific models to define areas of consequence due to the failure of asset(s) in a given infrastructure; and Development of a network `metacrawler' designed to associate sparse metadata (e.g., transportation system commodity throughput) with fragmented system elements (e.g., segments of the national rail network). Intended users ­ Analysts on NISAC's Fast Analysis and Simulation Team. System Output ­ Web-based, printer-friendly description of assets, their interdependencies, economic consequence of disruption, and other information associated with asset by system users. Maturity ­ In development, utilized by NISAC Fast Analysis and Simulation Team to support NISAC analyses for DHS/Preparedness. Areas modeled ­ First-order interdependencies for selected classes of assets in the energy, telecommunications, emergency services, and public health sectors, nationwide (based on data availability). Customers/sponsors ­ DHS/IP ­ NISAC. 55

Model Framework Underlying model ­ Dependency model is an object-oriented expert system model of infrastructure interdependencies. The economic model centers on the economic disruption over an area or region from a discrete event. Economic methodology best employed for disruptions with a timeframe of 1 week to 1 month. Simulation ­ For identification of interdependencies, FAIT utilizes an expert system developed in JESS. Economic analysis within FAIT is performed utilizing Input-Output methodologies. Both elements are coded in Java. Data format ­ FAIT utilizes spatial and tabular data Sensor data ­ None. Ability to couple with other models ­ None; though results of other models (documents, files) can be coupled through the FAIT architecture to particular assets, classes of assets, or infrastructures with which they are associated. Human Activity modeling ­ None. System Requirements None, for the end user. Program resides on a SNL server and supports web access. Hardware Internet Browser Software Other Notes FAIT allows for external information, (e.g. web addresses or files), to be `attached' to specific assets, classes of assets, or infrastructure sectors, such that when those areas are examined in the future, the associated information is accessible to future users.

References National Infrastructure Simulation and Analysis Center, Fait Analysis Infrastructure Tool Fact Sheet,http://www.sandia.gov/mission/homeland/factsheets/nisac/FAIT_factsheet.pdf.

56

Model Name Financial System Infrastructure (FinSim) Los Alamos National Laboratory Organization Infrastructures FIN Sam Flaim POC Description Overview ­ The Financial System Infrastructure (FinSim) is an agent-based model of cash and barter transactions that is dependant on contractual relationships and a network at the federal reserve level. Agent based models create transactions which rely on telecommunications and electric power. Dependencies can cause deadlocks in the situation where one is unable to pay until being paid. The MIITS module asks every transaction whether there is an electronic connection available to make the transaction. The payments and settlement systems (PSS) module makes the validity checks. Development goals ­ Development started in January 2005 to protect the physical infrastructure of payment and trading systems initiated by the events of 9-11. All current models didn't address the transaction system, just the economic impact. Intended users ­ Internal analyst. System output ­ The system output is the number of financial institutions affected. Output is in a textbased format. Maturity ­ Development. Areas modeled ­ National Federal Reserve Banking System -- Financial Customers/sponsors ­ Sponsor is the National Infrastructure Simulation and Analysis Center (NISAC), Department of Homeland Security (DHS). Model Framework Underlying model ­ Agent-based model. Simulation ­ FinSim models financial transactions modeling the 12 FRB, about 9,700 FedWire participants, and almost 28,000 financial institutions registered with FedACH. This includes the electronic PSS--networks with contractual as well as electronic links and nodes PSSs include: FedWire, FedNet, CHIPS, FedACH, Commercial ACHS ~50 Cash & barter (excluded from FinSim) Data format ­ Not specified. Sensor data ­ No direct sensor feeds. Coupling with other models ­ Yes, coupling is done indirectly. Electrical power failure (IEISS output)  Telecom failure (MIITS output)  PSS failures (FINSIM) Human activity modeling ­ None. System Requirements Larger models require a computer cluster. Hardware Java. Software Other Notes

57

References Financial System Infrastructure--FinSim, LAUR-05-9147.

58

Model Name Organization

POC

Fort Future U.S. Army Corps of Engineers, Engineer Research and Development Center, Construction Engineering Research Laboratory (CERL) Dr. Michael P. Case Michael.P.Case@erdc.usace.army.mil

Infrastructures All support infrastructures for a military installation

Description Overview ­ Fort Future is a collaborative, web-based planning system that uses simulation to test plans for Department of Defense (DoD) installations. It uses an open, service-oriented architecture to allow multiple simulations to be run simultaneously from the same set of alternative, organized into a study. The web-based workbench provides geographic information system (GIS)-based plan editors, controls simulations, and organizes results into a decision matrix. Fort Future assesses the impact of critical infrastructure on mission using a "Virtual Installation" simulation that contains models for transportation, electrical power, water systems, including waterborne chemical/biological/radiological (CBR) agents, airborne CBR plume, facilities, mission tasks and processes, agents, and dynamic plans. The Virtual Installation simulation was built using Argonne National Laboratory's Dynamic Information Architecture System (DIAS) framework and will be ported to the Repast agent modeling toolkit by September of 2006. Other models support analysis of encroachment, sustainability, and facility design. Development goals ­ Demonstrate the use of simulation to improve planning for DoD Installations. Incorporate scenario descriptions into Simulation Interoperability Standards Organization (SISO) Military Scenario Definition Language (MSDL). Intended users ­ Users will include installation and regional planners, US Army Corps of Engineers, and researchers. System output ­ Output of the simulations is collected by a web-based collaborative workbench and presented as a decision matrix. The workbench can be customized to present output specific to particular simulations. Maturity ­ This product is in development with some tools complete. The product will be complete by October, 2006. Areas modeled ­ Fort Benning, Fort Shafter, Fort Bragg, and Fort Carson. Customers/sponsors ­ United States Army. Model Framework Underlying model(s) ­ The agent-based Virtual Installation is based on DIAS and Repast. Water modeling uses EPAnet. CBR plume model uses the Defense Threat Reduction Agency (DTRA) Hazardous Prediction and Assessment Capability (HPAC) Tool. Simulation ­ This model supports complex and lengthy scenario simulations Data format ­ GIS ­ Environmental Systems Research Institute, Inc. (ESRI) Geodatabase and SHP files (Tri-service Spatial Data Standards). Scenarios ­ XML. Sensor data ­ Not accepted. Human activity ­ Human activities are modeled, however there are no humans in the simulation loop. Coupling with other models ­ Fort Future is built to collaborate with multiple models using simple object access protocol (SOAP).

59

System Requirements Fort Future is a server-based application, accessed over the internet using a Hardware web-browser. Fort Future has been tested on Windows and Linux servers. The workbench runs as a Software J2EE application on JBoss 3.x. Persistence is provided by MySQL or Oracle relational databases. Geospatial information is provided by ESRI ArcSDE and ArcGIS server. Users access the workbench using a web-browser. Other Notes Users of Fort Future at the installation, regional, or national level will be able to set up planning scenarios, conduct dynamic analyses over time periods of up to 30 years, and compare scenario results. Fort Future will allow decision makers to: · Provide an integrated sustainability planning capability to support mission-essential task list (METL) analysis, master planning, and natural and cultural resource planning. · Simulate the impact of critical infrastructure failure on the installation mission. · Simulate and optimize planning for force projection. Metrics will focus on risk-based evaluation of an installation's ability to project forces over time. · Simulate urban and regional growth around installations as a foundation for analysis of mission sustainability. Factors to be evaluated include encroachment, noise, traffic congestion, habitat, and threatened and endangered species. · Manage facility requirements to rapidly generate, visualize, and analyze facilities for the Objective Force. The analysis will include force protection and sustainability issues.

Electrical Infrastructure (capacity & interruption)

Water Infrastructure(flow & CBR)

CBR Plume Modeling

Collaborative Web-based Decision Support

60

National/Regional Scale

Installation Scale

Facility Scale

SIRRA Sustainable Installation Regional Resource Assessment

LEAM 30 Year Encroachment Simulation

Facility Composer:
Accelerating MILCON Transformation

SPiRiT & LEED
Sustainability Rating

AT Standards
References Fact sheet on Fort Future, www.erdc.usace.army.mil/pls/erdcpub/docs/erdc/images/ERDCFactSheet_ Research_FortFuture.pdf#search='fort%20future', Webpage accessed July 3, 2006. US Army Corps of Engineers, www.erdc.usace.army.mil, Webpage visited July 3, 2006. Discussion of US Army Corps of Engineers ongoing research, http://www.erdc.usace.army.mil/pls/erdcpub/WWW_WELCOME.NAVIGATION_ PAGE?tmp_next_page=61605&tmp_Main_Topic=51585, Webpage visited July 3, 2006.

61

62

Model Name Organization POC

Inoperability Input-Output Model (IIM) University of Virginia Center for Risk Management of Engineering Systems, Director and founder ­ Lawrence R. Quarles Professor of Systems and Information Engineering and Civil Engineering Yacov Y. Haimes Yyh4f@virginia.edu

Infrastructures Financial networks, highway networks

Description Overview ­ Inoperability Input-Output Model (IIM) is a computer-based analytical model capable of analyzing the impacts of an attack on an infrastructure and the cascading effects (in economic and inoperability terms) on all other interconnected and interdependent infrastructures. The model uses U.S. Bureau of Economic Analysis (BEA) data for assessing economic interdependencies. IIM allows systematic prioritization of infrastructure sectors that are economically critical and identifies sectors whose operability is critical during recovery. The model can be used to represent workforce recovery following a terrorist attack and identify essential response personnel. IIM also models recovery rates of different infrastructure sectors following an event. Development goals ­ Not specified. Intended users ­ Analysts and emergency planning and response organizations are the intended users. System output ­ The model outputs various data and metrics in text and graphically. Maturity ­ The model has been used with cooperation of various local and state governments. Areas modeled ­ IIM has been used to model Virginia's transportation systems in various cities (e.g., Hampton City, Norfolk, and Virginia Beach) and support Department of Homeland Security (DHS) security alert levels for the greater New York area and to support a commission on high-altitude electromagnetic pulse (HEMP) attacks. Customers/sponsors ­ Customers and sponsors of IIM include the State of Virginia, U.S. DHS, Defense Threat Reduction Agency (DTRA), the Department of Defense (DoD) and the Commission on High Altitude EMP Attacks on the U.S. Model Framework Underlying model ­ IIS is a mathematical model based on Wassily Leontif's input-output model for the U.S. economy which describes economic interdependencies. Simulation ­ IIM simulates the behaviors of multiple infrastructure sectors during and following perturbations (such as terrorist attacks on modeled infrastructure) using economic and other data to assess the criticality of the effects. Data format ­ Data are retrieved from and stored in relational databases containing information including employment and earnings data, commodity flow data, and geographic location data. Sensor data ­ Not specified. Coupling with other models ­ Not specified. Human activity modeling ­ IIM has been used to model human activity in response to transportation disruptions. System Requirements Not specified. Hardware Not specified. Software Other Notes

63

IIM Calculates Propagating Effects. References Haimes, Y. (2005), Risk-Based Framework for Modeling Infrastructure Interdependencies, University of Southern California Terrorism Risk Analysis Symposium, Los Angeles, California, January 14, 2005, http://www.usc.edu/dept/create/events/2005_01_31/Risk_Based_Framework_for_Modeling_Infrastructure_ Interdependencies.pdf#search='yacov%20Haimes%20interdependencies', Webpage visited July 12, 2006. Haimes, Yacov Y., et al., "Inoperability input-output model for interdependent infrastructure sectors. I: Theory and methodology," Journal of Infrastructure Systems, Vol. 11, No. 2, June 2005, pp. 67-79. Haimes, Y. (2004). Assessment and Management of Transportation Infrastructure Security using the Inoperability Input-Output Model (IIM), October 19, 2004, http://www.virginiadot.org/infoservice/resources/TransConfHaimes%20-%20VDOT-Roanoke-October-192004-2.pdf#search='Yacov%20Y.%20Haimes%20interdependencies', Webpage visited July 12, 2006.

64

Model Name Organization POC

Interdependent Energy Infrastructure Simulation System (IEISS) Los Alamos National Laboratory Infrastructures Joe Holland EP, NG

Description Overview ­ The Interdependent Energy Infrastructure Simulation System (IEISS) is an actor-based infrastructure modeling, simulation, and analysis tool designed to assist individuals in analyzing and understanding interdependent energy infrastructures. The actor-based infrastructure components were developed in IEISS to realistically simulate the dynamic interactions within each of the infrastructures, as well as, the interconnections between the infrastructures. In particular, it has the ability to analyze and simulate the interdependent electric power and natural gas infrastructures. IEISS Water is a water distribution simulation capability for simulating urban scale water infrastructures and their interdependencies. Development goals ­ The ultimate goal for IEISS is a multi-infrastructure modeling framework that can be used to analyze the complex, nonlinear interactions (interdependencies) among interdependent infrastructures including electric power, natural gas, petroleum, water, and other network based infrastructures that is scalable to multiple spatial (e.g., urban to regional) and temporal resolutions Intended users ­ Internal Analyst ­ IEISS used to support the development of an impact report on for specific infrastructure events (such as, hurricanes, terrorist attacks, etc.). System output ­ System output include the identification of outage areas (e.g., electrical outage areas). Output visualization is current in Java OpenMaps and is exportable to ESRI compatible shape files. Maturity ­ Mature Internal. Areas modeled ­ numerous US metropolitan areas. Customers/sponsors ­ Sponsor is NISAC ­ DHS. Model Framework Underlying model ­ IEISS is an actor-based infrastructure modeling, simulation, and analysis tool designed to assist individuals in analyzing and understanding interdependent energy infrastructures. Simulation ­ A continuous time based model with an underling physical engine for system dynamics. Data format ­ Data is input via xml format from a variety of databases. Sensor data ­ no direct sensor feeds. Coupling with other models ­ Yes, coupling is done indirectly. The output of IEISS will serve as the input to other infrastructure models to identify cross infrastructure effects. Human activity modeling ­ None at this time. System Requirements Cross platform compatibility ­ Windows and LINUX compatibility. Hardware Requires the Java Virtual Machine. Software Other Notes IEISS is coupled with other LANL modeling tools. Of particular note is the Scenario Library Visualizer (SLV). SLV is a scenario library of outage simulations, which includes a custom visualization tools to provide map-based view of scenarios that have been evaluated in IEISS. The goal has been to identify potential impacts to critical infrastructures dependent upon electric power. SLV has principally been used during fast-response exercises for analysis of hurricane impacts (restoration of hurricanes Charlie and Ivan in '04; Dennis, Katrina, Ophelia, Rita, Wilma in `05) SLV has also modeled electric power restoration during ice storms and during DOE-sponsored exercises involving low-voltage scenarios.

65

References "NISAC Energy Sector ­ IEISS," NISAC Capabilities Workshop, LA-UR-03-1159, Portland, Oregon, 26-27 March 2003. Los Alamos National Laboratory, "Energy Infrastructure Modeling at LANL," LALP-03-027, LA-UR-03-0658. Los Alamos National Laboratory, "Energy and Environmental Programs Compendium," LA-LP-02-216.

66

Model Name Organization POC

Knowledge Management and Visualization in Support of Vulnerability Assessment of Electricity Production Carnegie Mellon University Infrastructures H. Scott Matthews EP (RL, WW, HW limited) hsm@cmu.edu Department of Energy (DOE) National Energy Technology Laboratory (NETL) Pittsburgh and Morgantown Campuses

Description Overview ­ This is a research project to analyze vulnerabilities associated with delivery of fuel. It is designed to help ensure availability of supply and to visualize the impacts for decision support. The project has focused on coal deliveries to power plants because, while vulnerabilities at the power plant level (production) are easier to identify, vulnerabilities and impacts associated with delivery of fuel are more uncertain. Also, data on coal shipments is readily available. Development goals ­ The first phase of the project focused on the origin (mines) and destination (power plant) layers of the coal model. The middle (transportation) layer will be focused on in the future. Additional work will also be done to improve tools for data mining such as, classification of transportation assets, better prediction of impacts, and improved sequential pattern analysis tools. Intended users ­ Planners are the intended users. System output ­ Output includes maps and chart graphics showing mines, transportation routes, and affected (with degree of vulnerability to disruption) power plants. Maturity ­ This project is currently in the prototype stage with ongoing research. Areas modeled ­ United States with emphasis on coal mines in Wyoming. Customers/sponsors ­ Department of Energy (DOE) National Energy Technology Laboratory (NETL). Model Framework Underlying model(s) ­ Statistical data and analysis tools drawing on data derived from data warehouses. Simulation ­ Mines can be removed from the network and a simulation run to identify plants affected and the degree of the impact on production. Data format ­ Data were used from several databases including; Coaldat (developed by Platts containing ~ 2500 coal transactions per month), Coal Transportation Rate Database developed by the Energy Information Administration supplemented with data from the Department of Transportation (DOT) Surface Transportation Board (STB), National Transportation Atlas Database (NTAD), and PowerMAP, a geographical information system (GIS) developed by Platts containing map layers of power plants and mines. Sensor data ­ Not included. Human activity ­ Not modeled. Coupling with other models ­ Not specified. System Requirements Not specified. Hardware Not specified. Software

67

Other Notes The Transportation Routing Analysis Geographic Information System (TRAGIS) developed by Oak Ridge National Laboratory (ORNL) was evaluated to help with the problem of routing (of coal supplies). TRAGIS is designed to schedule possible routes by selecting the origin and destination with one transportation mode (e.g., highway, rail, and waterway modes) and route type (e.g., commercial [default], quickest, shortest, and others). Currently, it is not able to schedule routes for multimodal transportation as is often used to deliver coal. While the most frequently used mode of transporting coal is railroad, many transactions are shipped multimode, such as by barge then by railroad. Therefore, a multimodal route scheduling solution is necessary for acquiring more accurate transportation analyses.

68

References Information in the preceding section was obtained from draft report and communications with point of contact.

69

70

Model Name Organization POC

Multi-Layer Infrastructure Networks (MIN) Purdue School of Civil Engineering Dr. Srinivas Peeta peeta@purdue.edu George Mason University Dr. Terry Friesz tfriesz@gmu.edu

Infrastructures HW, HA

Description Overview ­ This is a preliminary network flow equilibrium model of dynamic multi-layer infrastructure networks (MIN) in the form of a differential game involving two essential time scales. In particular, three coupled network layers--automobiles, urban freight, and data--are modeled as being comprised of Cournot-Nash dynamic agents. An agent-based simulation solution structure is introduced to solve the flow equilibrium and optimal budget allocation problem for these three layers under the assumption of a super authority that oversees investments in the infrastructure of all three technologies and thereby creates a dynamic Stackelberg leader-follower game. Development goals ­ Continue to develop a generalized framework to address both equilibrium and disequilibrium scenarios. Intended users ­ Community planners and engineers. System output ­ Charts, graphs, behavioral trends. Maturity ­ Research. Areas modeled ­ Urban transportation (e.g., auto, urban freight, and data). Customers/sponsors ­ The National Science Foundation sponsored the work. Model Framework Underlying model ­ Agent based simulation of multi-layer infrastructure networks. The three-layer model consists of an auto, urban freight, and data layer flow sub models. These three sub models are combined and solved using an agent-based simulation approach. Simulation ­ Temporal dynamic flow model involving producers and consumers. Data format ­ Not specified. Sensor data ­ Not incorporated. Coupling with other models ­ Unknown. Human activity modeling ­ Models human activity as consumers. System Requirements Not specified. Hardware Not specified. Software Other Notes

References

71

72

Model Name Organization POC

Multi-Network Interdependent Critical Infrastructure Program for Analysis of Lifelines (MUNICIPAL) Rensselaer Polytechnic Institute (RPI) Infrastructures Earl E. Lee II TC, EP, RL Leee7@rpi.edu William A. Wallace wallaw@rpi.edu John E. Mitchell mitchj@rpi.edu David M. Mendonca mendonca@njit.edu

Description Overview ­ Multi-Network Interdependent Critical Infrastructure Program for Analysis of Lifelines (MUNICIPAL) is a geographic information system (GIS) user interface, built on a formal, mathematical representation of a set of civil infrastructure systems that explicitly incorporates the interdependencies among them. The mathematical foundation or decision support system is called the Interdependent Layered Network (ILN) model. ILN is a mixed-integer, network-flow based model implemented in software drawing on a database containing infrastructure attributes. MUNICIPAL provides the capability to understand how a disruptive event affects the interdependent set of civil infrastructures. This can help communities train for and respond to events that disrupt services required for their health, safety, and economic well being. It can be used to help assess the vulnerability of systems due to their reliance on other systems. The model is generic (applicable to more than one location) and not specific to a particular type of event, such as an earthquake or hurricane. Development goals ­ Once the Los Angeles and Manhattan data sets are complete, mathematical and technical assessments will be conducted. The system will also be evaluated by infrastructure system managers and emergency response organizations. Intended users ­ MUNICIPAL is intended for use by personnel in charge of response and restoration efforts following a disruptive event and as a training tool for personnel who guide response and restoration efforts. System output ­ A GIS interface displays systems and identifies affected areas. An operator can update the conditions of components of the set of systems modeled, add temporary systems during restoration, and display areas affected by inabilities to meet demands. Maturity ­ Prototype system. Areas modeled ­ Manhattan, NY and Los Angeles, CA. Customers/sponsors ­ National Science Foundation. Model Framework Underlying model ­ MUNICIPAL consists of a GIS interface for the user, a database with the attributes of the set of infrastructures, the ILN module, and the vulnerability module. Simulation ­ With identification of paths or components of concern, MUNICIPAL identifies components in the parent system which these paths or components rely on. For example, placing power supply components in a failed condition will identify telecommunications components that rely on these sections of power to fail. By proposing new connections within telecomm, MUNICIPAL can help to determine if a feasible path (or paths) exists and the set of nodes that constitute this path (or set of paths). MUNICIPAL can also be used for the addition of temporary or alternative power sources or any other analyses relating to improving reliability by adding redundancy. Data format ­ ESRI ArcGIS, relational database, text. 73

Sensor data ­ Not currently configured for sensor data. Coupling with other models ­ Not specified. Human activity modeling ­ Not specified. System Requirements Not specified. Hardware Not specified. Software Other Notes

References Lee, Earl E. II, et al., Decision Technologies for Protection of Critical Infrastructures, http://www.rpi.edu/~mitchj/papers/decisiontechnologies.pdf#search='decision%20technologies, Webpage visited July 10, 2006.

74

75

76

Model Name Organization POC

Natural Gas Infrastructure Toolset (NGtools ) Argonne National Laboratory, Infrastructure Assurance Center (IAC) Dr. James Peerenboom jpeerenboom@anl.gov

Infrastructures NG, EL

Description Overview ­ The Infrastructure Assurance Center (IAC) has developed a set of tools to represent the physical components of the natural gas network. The Natural Gas Infrastructure Toolset (NGtools) was developed to provide an analyst with a quick method to access, review, and display components of the natural gas network; perform varying levels of component and systems analysis, and display analysis results. Development goals ­ Not specified. Intended users ­ Natural gas suppliers and users (e.g., electric utilities). System output ­ Geographic (using GIS) or schematic view of pipeline system, charts and graphs showing failure sets (e.g., pumping stations and power stations) and the amount of time to gas depletion. The system allows various analyses on component and system level, and displays results. Maturity ­ The system is in development. Areas modeled ­ Not specified. Customers/sponsors ­ US Department of Energy. Model Framework Underlying model ­ Agent-based. Simulation ­ NGflow simulates steady-state gas network flows and provides gas flow movements under various operating conditions based on gas flow balancing algorithms and available system flow data. Data format ­ Not specified. Sensor data ­ Not specified. Coupling with other models ­ Not specified. Human activity modeling ­ Human activities are not modeled. System Requirements Not specified. Hardware Not specified. Software Other Notes There are four tools in the toolset; NGanalyzer, NGcut, NGflow, and NGdepletion as described in the following: NGanalyzer assists in analyzing gas system characteristics and vulnerabilities. Key considerations include the number of city-gates, available storage, and pipeline capacity and interconnections. The figure below shows an example of the shortest path distance from major gas supply areas to a sample site as calculated by the model. NGcut determines network component failure sets that could isolate a specific location or site from all supply sources. One of the advantages of using this model is that it significantly decreases the time needed to analyze site isolation issues by automating the construction of failure sets. The model also allows analysts to consider a larger number of failures and to broaden an analysis. Failure sets identified by NGcut provide an initial set of components that require closer examination. NGflow identifies critical links and nodes in a network topology. This tool provides an alternative to using very detailed, data-intensive commercial flow simulation models. The model also gives a unique snapshot of the gas transmission infrastructure that supports a certain location or site. NGdepletion addresses outage duration times and determines whether and when a component outage will affect a specific location or site. The model computes the amount of time that line pack can continue supplying gas to a site.

77

References

78

Model Name Organization POC

Net-Centric Effects-based operations MOdel (NEMO) SPARTA Brent L. Goodwin Brent.goodwin@sparta.com Laura Lee Laura.lee@sparta.com

Infrastructures TC, EP, NG, DW

Description Overview ­ Net-Centric Effects-based operations MOdel (NEMO) is an effects-based planning and analysis application for modeling the cascading effects of events across multiple infrastructure networks. It is a Net-Centric compliant application, relying on a service oriented architecture (SOA) approach to access infrastructure models, data repositories, and mapping tools. NEMO models interactions across electrical power, water, gas, and road networks using an on/off interaction behavior between the components of the different networks, and provides a solid foundation for advancement. NEMO provides a first of its kind capability for observing second and higher order effects of operations against opponents' infrastructure networks. Development goals ­ Efforts are underway to integrate social/political networks into the effects-based operation (EBO) process. Future development needs to enhance the program capabilities for integrating additional relationship definitions, multi-agent capabilities, and optimization. Intended users ­ Planners and analysts are the intended users. System output ­ NEMO displays maps overlaid with nodes and linkages between various infrastructures. Disruptions and cascading effects are highlighted during simulations. Maturity ­ This is a prototype system. Areas modeled ­ Not specified. Customers/sponsors ­ NEMO was internally developed by SPARTA. Model Framework Underlying model ­ The graphical user interface (GUI) is backed by an SOA consisting of two web services; one accesses to a geo-spatial database for storage and retrieval of network databases, and the other coordinates interaction with the various infrastructure models used to provide network status feedback. The geo-spatial database web service, Earth Resource Terrain Hierarchical Archive (ERTHA), contains nearly 200GB of network definitions that may be accessed via the NEMO GUI and used to support effects-based analysis. ERTHA is a geographical information system (GIS) database, based on ESRI products, of infrastructure data items (e.g., power lines, road networks) that were developed as an unclassified source. Abstracting access to data through a web service decouples NEMO from a specific database and specific vendors, making it possible to integrate other data sources in the future. Simulation ­ NEMO provides a basic capability for effects-based planning and performing "what if" analysis of actions. Data format ­ Data is in Environmental Systems Research Institute's (ESRI) ERTHA relational database format. Other models utilize a model interface client (MIC) translator and eXtensible Markup Language (XML). Sensor data ­ Not included. Human activity modeling ­ Changes to include human activity modeling are in progress. Coupling with other models ­ NEMO integrates four infrastructure models: lines of communications, electrical power, gas pipelines, and water pipelines. The models used to evaluate these networks are industry best-of-breed simulation tools for their domains. CitiLabs' Voyager simulation provides road and rail network analysis, while Advantica (formerly Stoner Engineering) provides the Solver tools for electrical power networks as well as the water and gas pipelines. System Requirements Windows XP/2000. Hardware Not Specified. Software 79

Other Notes Efforts are ongoing and mostly complete to integrate social/political networks into the EBO process. For the most part, these efforts are complete. We have integrated the Political Science-Identity (PSI) model (from University of Pennsylvania, Dr. Ian Lustick) into our architecture, and have developed operators that alter the contentedness of a population based on associated physical infrastructure. Further information on PS-I is available at http://jasss.soc.surrey.ac.uk/5/3/7.html.

· ERTHA Web Service ­ Interface for all GIS Infrastructure Models » "Get" shape files and associated attributes Web Service · ArcIMS and ArcXML ­ ESRI's interface to ArcSDE and its Data ­ ArcIMS : Internet Map Server ­ ArcXML : Layer Definition and Query Language for ArcIMS ArcXML · ArcSDE ­ Spatial Database Engine ­ Centralized management of geographic information in a DBMS » Vector, raster, table, annotation, relationships, CAD ­ Contains a subset of JIVA's data · DBMS ­ Oracle database ­ Features as objects » Geometry » Attributes » Behavior (rules, methods, relationships) ­ Uses ArcSDE for multi-user access and versioning

NEMO

ERTHA Web Service
Apache Java JRun

ArcXML ArcIMS ArcSDE
JIVA data

ArcIMS

ArcSDE

DBMS
Oracle Windows

80

References JASSS, PS-1: A User-Friendly Agent-Based Modeling Platform for Testing Theories of Political Identity and Political Stability, http://jasss.soc.surrey.ac.uk/5/3/7.html, Webpage visited July 3, 2006. Sparta, Planning and Assessing Effects Based Operations, www.sparta.com/sew/publications/NEMOfor-ICCRTS.pdf, Webpage visited July 3, 2006.

81

82

Model Name Organization POC

Network-Centric GIS York University, Toronto, Ontario, Canada Rifaat Abdalla abdalla@yorku.ca

Infrastructures RL, HW, WW

Description Overview ­ This system is a framework for using geographical information system (GIS) interoperability for supporting emergency management decision makers by providing effective data sharing and timely access to infrastructure interdependency information. Development goals ­ There are no development goals identified at present. Intended users ­ This is intended for emergency planners and responders. System output ­ Output are GeoServNet (York University GeoICT Lab Product) GIS 2 and 3D images. Maturity ­ Proof of principle. Areas modeled ­ This has been modeled in Vancouver, British Columbia (Earthquake scenario) and Toronto, Ontario (Flood scenario). Customers/sponsors ­ Ongoing research began under Canada's Joint Infrastructure Interdependencies Research program (JIIRP), which is jointly funded by the Natural Sciences and Engineering Research Council (NSERC) and the department of Public Safety and Emergency Preparedness Canada (PSEPC). Model Framework Underlying model ­ Underlying models are GIS technologies including ArcGIS 9 (desktop) and GeoServNet (web-based), GSNBuilder, GSNAdministrator, GSNServer, GSNPublisher, GSNViewer, and HEC-RAS (used for hydraulic simulation with ArcView GIS). Simulation ­ The system has GIS-based spatial-temporal simulations. Data format ­ Data formats are GIS data, graphics, and text. Knowledge-base information is stored in a specially designed object-oriented database. The project used Environmental Systems Research Institute's (ESRI) Geodatabase model. Sensor data ­ Hydraulic gauges provide information for water surface levels and there exists a capability for integrating other live sensor information. Coupling with other models ­ None. Human activity modeling ­ Not included. System Requirements Pentium 4 with 512 RAM, broadband connection. Hardware ESRI ArcGIS, GeoServNet. Software Other Notes Model creation process for the flood model: x Preparation of different data layers x Digitize floodplain, banks, stream centerline, and stream cross section using HEC-GeoRAS extension for ArcView x Input flood parameters using channel geometry created in ArcView and model a flood scenario using HEC-RAS, GIS interoperability is utilized for sharing and visualization of the disaster model x Delineate flood layers using HEC-RAS export ASCII data and data layers with help of ArcView and HEC-GeoRAS extension. Populate flood layers produced in GeoServNet using standard processing procedures. The following steps are useful for defining location based infrastructure interdependencies (LBII) for a particular area: 83

x Identify critical infrastructure sectors in the study area x Analyze processes and operations for each sector x Analyze dependencies x Determine Interdependencies x Collect data x Model and visualize (interoperable 3D internet-based). Earthquake scenario modeling is based on using a Geological Survey of Canada Shakemap for the city of Vancouver. Critical infrastructure at risk was identified based on GIS modeling. Building damage density was analyzed based on IKONOS satellite imagery. Population at risk was identified based on census information and the Shakemap. Location based infrastructure interdependency was modeled.

Spatial Model Showing Critical Infrastructures at Risk

84

GeoServNet 3D Damage Assessment Model of Downtown Vancouver References Abdalla, R. Tao, V. and H. Ali., 2005. "Location-Based Infrastructure Interdependency: New Term, New Modeling Approach," Proceedings of Geoinformatics 2005, Toronto, August 17-19, 2005 CD. Abdalla, R., K. Niall, and V. Tao, 2005. "A framework for modeling Critical Infrastructure Interdependency Using GIS," Canadian Risk and Hazard Symposium, Toronto, 19-21 November 2005. Joint Infrastructure Interdependency Research Program, Modeling Infrastructure Interdependency for Emergency Management Using a Network-Centric Spatial Decision Support System Approach, www.geoict.yorku.ca/JIIRP/Jiirp.htm, Webpage visited July 10, 2006.

85

86

Model Name Organization POC

Network Security Risk Assessment Model (NSRAM) Tool for Critical Infrastructure Protection (CIP) Project James Madison University (JMU), Institute for Infrastructures Infrastructure and Information Assurance EP, CN Philip Riley RileyPB@jmu.edu Jim McManus McManuJP@jmu.edu Samuel T. Redwine, Jr. RedwinST@jmu.edu George Baker BakerGH@jmu.edu Taz Daughtrey DaughtHT@jmu.edu

Description Overview ­ The network security risk assessment model (NSRAM) tool is a complex network system simulation modeling tool that emphasizes the analysis (including risk analysis) of large interconnected multi-infrastructure models. It is designed to be portable, and uses portable and expandable database and model structures. The tool also provides a framework to simulate large networks and analyze their behavior under conditions where the network suffers failures or structural breakdowns. In order to accurately portray the severity of network failures, repair variables (time to repair, cost to repair, repair priorities) must be considered. NSRAM's unique repair element set consists of repair entities with specialized functions that allow users to accurately simulate any configuration of fault detection and repair schemes. The intent of these repair element sets is to more accurately model the human response to perceived system damage. The repair element sets identify symptoms, test the system to determine the elements that are damaged, attempt to repair the damage, and then attempt system recovery. If symptoms are still present, the repair elements repeat the above cycle until the system is recovered. Inspection routines will also be accommodated so that preventative maintenance effects are accurately incorporated. The tool is flexible and can be used to model different infrastructure networks, such as computers, electrical systems, and highway systems. Development goals ­ James Madison University (JMU) is continuing development to add strong security features, improve the graphical user interface (GUI) and database efficiency, and to develop an emergency radio system element set. JMU is also developing the concept of sophisticated repair element sets that interact via predefined algorithms to more accurately simulate repair personnel reaction to system insults or malfunctions. These repair element sets are unique in that they interact with the simulation network model in a predetermined manner, but their operating rules can be changed to allow the user to optimize repair strategies. Intended users ­ Analysts are the intended users. System output ­ NSRAM contains a GUI for developing models and scenarios, and interpreting output. The data output is flexible to facilitate post simulation processing. Maturity ­ NSRAM is currently in development as part of the CIP project. Areas modeled ­ Not specified. Customers/sponsors ­ Not specified. Model Framework Underlying model ­ Not specified. Simulation ­ Developed simulation elements for computer and electrical power distribution networks. 87

Data format ­ Not specified. Sensor data ­ Not specified. Human Activity ­ NSRAM models human activities such as responses to system damage. Coupling with other models ­ Not specified. System Requirements Not specified. Hardware Not specified. Software Other Notes

References McManus, Jim, et al., Network Security Risk Assessment Model(NSRAM) Tool for Critical Infrastructure Protection Project, http://www.jmu.edu/iiia/webdocs/ppt_NSRAM_Tool.ppt, July 12, 2006. Redwine, Sam, et al., Network Security Risk Assessment Model Tool, http://www.jmu.edu/cisat/frd/abstracts04/redwine_sam.html, April 3, 2006.

Model Name Organization POC

Next-generation agent-based economic 'laboratory' (N-ABLE) Sandia National Laboratory Infrastructures Theresa Brown FIN, POL tjbrown@sandia.gov 88

Description Overview ­ The NISAC Agent-Based Laboratory for Economics (N-ABLE) is a software system for analyzing the economic factors, feedbacks, and downstream effects of infrastructure interdependencies. N-ABLE is a simulation environment in which hundreds of thousands to millions of individual economic actors simulate real-world manufacturing firms, households, and government agencies. NABLE can specifically address questions such as: 1. Which economic sectors are most vulnerable to infrastructure disruptions and interdependencies? 2. What firms are most affected  who does well, poorly? 3. What are the different qualitative and quantitative ways in which economic sectors use the energy, transportation, financial, and communication sectors? 4. What short-run infrastructure changes affect economic performance (and vice versa)? 5. How do systems of firms and individuals respond and adapt over time and over regions? 6. What economic mechanisms do national, state, and local governments have or need to have to assist firms and other economic sectors in their regions? Development goals ­ Developed to provide decision makers with a firm-level understanding of the interdependencies between infrastructure sectors and the economy. Intended users ­ Economic Analysts. System Output ­ Geographical charts and statistical output. Maturity ­ Mature Internal. Areas modeled ­ Examples: chemical, food, financial, manufacturing sectors. Customers/sponsors ­ Department of Homeland Security Model Framework Underlying model ­ N-ABLE models the economy at the level of the individual firm; each N-ABLE firm is complete with individual buyers, production supervisors, sellers, and strategic planners who collectively navigate through economic disruption and recovery. N-ABLE's simulations of thousands to millions of firms provide the fidelity necessary to understand and implement better infrastructure policies. Simulation ­ Agent Based. N-ABLE microsimulates the economy using an agent-based discrete-event model. This modeling approach is well suited for investigating the behavior of complex, nonlinear stochastic systems like the economy. Agents start each time increment making decisions much like their real-life counterparts. Decisions about what actions to take are based either on probabilities computed from actual microeconomic data or on results of learning models such as genetic algorithms. These decisions include setting sales prices, purchasing products, setting production schedules, hiring workers, buying and selling financial instruments, conducting open market operations, and others. Macroeconomic variables, such as gross domestic product, inflation (CPI), and the unemployment rate are computed as individual-firm and aggregate system measures of the performance of the economy. Data format ­ not specified. Sensor data ­ None. Ability to couple with other models ­ Not known. Human activity modeling ­ Human in the loop activity supported within the simulation. System Requirements Computer cluster Hardware Not specified. Software

89

Figure 1. Geographical Simulation Output

Figure 2. Statistical Simulation Output References NISAC Agent-Based Laboratory for Economics (N-ABLETM) Fact Sheet http://www.sandia.gov/mission/homeland/factsheets/nisac/NISAC_N-ABLE_factsheet.pdf

90

Model Name

Organization POC

NEXUS Fusion FrameworkTM ­ IntePoint, LLC Critical Infrastructure Integration Modeling and Simulation, University of North Carolina at Charlotte (UNCC) IntePoint, LLC ­ Commercial Product Infrastructures Mark Armstrong EP, TC, HW, HA, RL Mark.Armstrong@IntePoint.com University of North Carolina, UNCC Development wjtotone@uncc.edu

Description Overview ­ NEXUS Fusion FrameworkTM is a planning and response tool that visualizes intended and unintended effects and consequences of an event across multiple infrastructure, social, and population behavior models. It is a single framework that incorporates geospatial, graph based (social, economic), and population behavior models in the same simulation space for cross-infrastructure relationship analysis. The framework takes a holistic system-of-systems view to support cross system analyses of cascading events within and between complex networks. Development goals ­ Not specified. Intended users ­ Department of Defense (DoD) Leadership/Analysts. Output ­ Output includes 2, 2.5, and 3-D graphical and geospatial temporal views of modeled infrastructure. Maturity ­ Version 1.1 was delivered to DoD and accreditation is expected in the summer of 2006. Multiple infrastructure models have already been built & tested using DoD data. Version 2.0 is under development with delivery in summer 2006. Areas modeled ­ Model had been used in many areas including New Orleans, Houston, and Federal Emergency Management Agency (FEMA) Region 5. Customers/sponsors ­ The team is working on the sixth project in 3 years with DoD. Model Framework Underlying model(s) ­ Intelligent agent-based system within the context of a Geographic Information System (GIS) environment, open architecture Simulation ­ Simulation playback offers a foundation for heuristics and supports a collaborative, sharable simulation result that can be viewed by analysts and consumers. Visual display of cause/effect allows determination of rules and inferred relationships. The model supports network component validation and verification of data points. Facilitates identification of missing intelligence. Modification of rules and branching supports analysis of multiple scenarios based on initial starting boundaries. Additionally, multiple geographical regions can participate in the same simulation. Data format - Not specified. Sensor data ­ Architecture supports sensor data, not actively incorporated into the model. Human activity ­ Incorporates infrastructure-aware population behavior models. Coupling with other models ­ Flexible, scaleable, and extensible in that it allows "plug and play" of models of the same infrastructure, multiple models of the same infrastructure, and incorporation of other infrastructure models into the simulation. System Requirements Not specified. Hardware Not specified. Software Other Notes Leverages Environmental Systems Research Institute, Inc. (ESRI) ArcGIS capabilities for geospatial 91

display and analysis. Uses ESRI ArcGIS Geodatabase to capture: x Critical attributes x Critical relationships x Predictive analytics x Meta-driven inference engines x System-of-systems causality analysis x Temporal view x Incorporates specialized functionality off-the-shelf as needed.

92

References

93

94

Model Name Organization POC

Petroleum Fuels Network Analysis Model (PFNAM) Argonne National Laboratory, Infrastructure Assurance Center Steve Folga sfolga@anl.gov

Infrastructures NG, OL

Description Overview ­ Petroleum Fuels Network Analysis Model (PFNAM) was developed to perform hydraulic calculations of pipeline transport of crude oil and petroleum products. A network consists of links (pipe segments), nodes (pipe junctions), pump stations, valves, and pressure-reducing stations. The model tracks the flow of oil in each pipe and the pressure at each node. "Point-and-click" motions allow the analyst to create a representative model of the liquids pipeline network in order to set up and run a simulation. Graphical and tabular results provided for each simulation enable analysts to quantify the impact of infrastructure disruptions on the pipeline segment or system. This software tool provides a framework for introducing pipeline component dependencies into critical infrastructure analyses. Development goals ­ Not specified. Intended users ­ Not specified. System output ­ Results include graphs and tables for steady-state flow rate, pressure, and line capacity distributions. The hydraulic gradient along the pipeline is also displayed. After a simulation, the analysis results indicate the potential effect on pipeline operations. The diagram below indicates that the long-term loss of a specific pump station can lead to isolation or curtailment of the deliveries of petroleum fuels. Maturity ­ The system is in development into the DOT. NET framework. Areas modeled ­ Experts at Argonne have applied PFNAM to a number of crude oil and refined petroleum products pipelines. Other potential applications are being explored. Customers/sponsors ­ US Department of Defense. Model Framework Underlying model ­ Mathematical model. Simulation ­ PFNAM allows the analyst to address a wide range of "what if" questions. Two of the main outputs of a PFNAM simulation are pressure and pipeline capacity estimates along the pipeline. This allows the analyst to determine whether an outage of a pipeline component will result in pipeline shutdown or degradation in pipeline throughput. Data format ­ Access database. Sensor data ­ Accepts pipeline pressure and flow. Coupling with other models ­ This model is compatible with the NG Tool set developed at Argonne. Human activity modeling ­ Human activities are not modeled. System Requirements Not specified. Hardware Not specified. Software Other Notes

95

References

96

97

98

Model Name Organization POC

Transportation Routing Analysis Geographic Information System (TRAGIS) Oak Ridge National Laboratory Infrastructures Paul E. Johnson RL, HW, WW, POL johnsonpe@ornl.gov

Description Overview ­ The Transportation Routing Analysis Geographic Information System (TRAGIS) model is used to calculate highway, rail, or waterway routes within the United States. TRAGIS is a client-server application with the user interface and map data files residing on the user's personal computer and the routing engine and network data files on a network server. By default, the model calculates commercial highway routes; but with the change of the route type, the model can determine routes that meet the U.S. Department of Transportation (DOT) regulations for shipments of highway route-controlled quantities (HRCQ) of radioactive material, routes for shipments to the Waste Isolation Pilot Plant (WIPP), the shortest, or the quickest route. Development goals ­ The goal for WebTRAGIS is to have national 1:100,000-scale routing networks. The highway network developed for TRAGIS is a 1:100,000-scale database. The legacy HIGHWAY model used a stick figure network with nodes digitized at 1:250,000-scale. The TRAGIS highway network was developed from the U.S. Geological Survey (USGS) Digital Line Graphs and the U.S. Bureau of Census Topologically Integrated Geographic Encoding and Referencing (TIGER) system. The rail network used in the initial version of TRAGIS was the same database as that used in the INTERLINE model. This network also was a stick figure network with nodes that were digitized from variable scaled maps. A 1:100,000-scale rail network is now incorporated into TRAGIS. The current inland waterway network is based on the USGS 1:2,000,000-U.S. Geodata. Deep-water routes are depicted in WebTRAGIS as straight-line segments. It is planned to incorporate a 1:100,000-scale waterway database into the model at a future time so that all modes will be at a consistent scale. Intended users ­ internal and external transportation route planners. System output ­ Web-based Graphical 2D map display or textual reports. Maturity ­ Mature ­ commercial. Areas modeled ­ United States. Customers/sponsors ­ Funding for the development of TRAGIS has been provided by the National Transportation Program (NTP) of the U.S. Department of Energy (DOE). Model Framework Underlying model ­ TRAGIS is a client-server application where the user interface and map data files reside on the user's personal computer (PC) and the routing engine and its large data files reside on the server. The model uses the World Wide Web (WWW) for communications between the client and the server. There are two user interfaces for TRAGIS: WebTRAGIS, which is the primary client user interface, and BatchTRAGIS, which is a specialized user interface that allows multiple routes to be prepared and then calculated at one time. Simulation ­ The simulation utilizes a network flow model, which determines the optimal routes based upon an optimization of the impedance measures between endpoints. The impedance is a valued function based upon route type and requirements. Transportation between various sectors is modeled (such as, rail to road transfer, barge to rail, etc.). Population demographics is a component of the model to determine routing criteria for HAZMAT. Data format ­ Not specified. Sensor data ­ None. Coupling with other models ­ Not directly. Human activity modeling ­ No. System Requirements 99

Hardware Software Other Notes

PC with Internet Access. Windows.

100

References WebTragis https://tragis.ornl.gov/ Transportation RoutingAnalysis Geographic Information System (TRAGIS) User's Manual, Oak Ridge National Laboratory, ORNL/NTRC-006 Rev 0, June 2003. https://tragis.ornl.gov/TRAGISmanual.pdf.

101

102

Model Name TRANSIM Los Alamos National Laboratory Organization Infrastructures HW, HA Jim Smith POC Description Overview ­ TRANSIMS is an agent-based simulation system capable of simulating the second-by-second movements of every person and every vehicle through the transportation network of a large metropolitan area. It consists of mutually supporting simulations, models, and databases. By employing advanced computational and analytical techniques, it creates an integrated environment for regional transportation system analysis. TRANSIMS is an integrated suite of products containing an easy-to-use graphical user interface for the modeling functions, a GIS-based network editor, 3D data visualization and animation software, and a reporting system. TRANSIMS is designed to give transportation planners more accurate, complete information on: · Traffic impacts · Energy consumption · Traffic congestion · Land use planning. The core code version of TRANSIMS (TRANSIMS-LANL), developed at Los Alamos National Laboratory, is distributed for a nominal fee to universities on this Web site. The commercial version of TRANSIMS (TRANSIMS-DOT) was developed from the core software package especially for the Department of Transportation by IBM, and it has a more elaborate interface and specific features to meet requirements by the DOT. It is not available on this Web site. Development goals ­ Started as laboratory-directed research and development in the late 1980s for the Department of Transportation. Funding is continuing under Department of Homeland Security (DHS) National Infrastructure Simulation and Analysis Center (NISAC). TRANSIMS technology was developed under U.S. Department of Transportation and EPA funding at the Los Alamos National Laboratory (LANL) over the last eight years. It is a result of an effort to develop new transportation and air quality modeling methodologies required by the Clean Air Act, the Transportation Equity Act for the 21st Century (TEA 21), and other regulations. Intended users ­ Internal analyst ­used to support the development of an impact report on for specific infrastructure events (such as, hurricanes, terrorist attacks, etc.), external analysts. System output ­ Visualization of demographics data with a city or region illustrating the human activity such as traffic patterns and work patterns. Maturity ­ Mature internal and commercial. Areas modeled ­ Customers/sponsors ­ NISAC ­ DHS. Model Framework Underlying model ­ Cellular Autonoma. Simulation ­ Discrete event, agent based simulation. Data ­ Multiple data sources including Census data, Household Survey Data, Dunn and Bradstreet Data. Sensor data ­ No direct sensor feeds. Coupling with other models ­ Yes, coupling is done directly (EPISIM). Human activity modeling ­ Yes ­ social network and human mobility model. System Requirements Memory and disk requirements depend upon the scenario that is used, but large Hardware networks require a large Linux cluster. Some scenarios may consist of 10 ­ 100 103

Software

million agents. The TRANSIMS distribution requires that the user install the following software. Linux · X11R6 libraries (Xmu, Xi, X11, Xext, Xt) · OpenGL and the OpenGL Utilities Toolkit libraries (Mesa/Glut) · Linux libraries (stdc++, ld-linux, ICE, SM) · Perl. All of the third-party software used by TRANSIMS is available on Red Hat Linux distribution CDs. The latest versions of the following packages should be installed: kernel, kernel-headers, gcc, glibc, libstdc++, make, perl, XFree86, Mesa, Mesa-devel, Mesa-Glut, Mesa-Glut-devel, MPI, and PVM. Solaris XllR6 libraries in /usr/openwin, OpenGL, OpenGL Utilities Toolkit libraries (glut), and Perl. Metis, PVM, MPI, and SPRNG are supplied with the TRANSIMS distribution.

Other Notes Los Alamos National Laboratory's TRANSIMS software is based on a computationally intensive, agent-based simulation technology requiring significant multiprocessor computing hardware. Programs in the TRANSIMS software suite are distributed applications with components running on different hardware/software platforms. To install and run all of the components of the TRANSIMS suite, the customer must procure and set up the following three types of computer systems: 1. Unix/Linux server(s) for hosting the core TRANSIMS software, Oracle database, and serverside components of the TRANSIMS modeling interface. Customers who wish to execute largesize problems must have procured multiserver Linux computing cluster or an equivalent multiprocessor UNIX-based framework. 2. Windows workstation(s) for running the Network Editor, the client-side modeling interface, and Crystal Reports. 3. Optional Linux workstation(s) for running the Visualizer. Alternatively, the customer may wish to equip the Linux server with a high-end graphics card and use the server as the Visualizer platform. A version of the output Visualizer that operates on the Windows workstation is in development. TRANSIMS was tested in a Linux cluster environment on Red Hat Linux 6.2 and compiled with gcc/g++ 2.95.2. Limited tests in a single-CPU environment were done on Red Hat Linux 7.1 using gcc/g++ 2.96. To run the traffic microsimulator under PVM or MPI, the Linux kernel must be compiled with networking support and must have an assigned IP address and a host name. An actual network card is not required. The following options must be selected in the Linux kernel configuration: · Networking support (CONFIG_NET) · System V IPC (CONFIG_SYSVIPC) · TCP/IP networking (CONFIG_INET) · Dummy-net driver support (CONFIG_DUMMY) · The appropriate network card driver. The default kernel shipped with Red Hat 6.2 and 7.1 is configured with the appropriate options. The following package categories should be selected during Red Hat Linux installation to run the TRANSIMS components: 104

· X Window System · Mesa/GL · Glut. Additional package categories should be selected to compile the TRANSIMS components: · C Development · Development Libraries · C++ Development · X Development.

105

106

References http://www.transims.net/home.html Zoltán Toroczkai "Agent-Based Modeling as a Decision Making Tool: How to Halt a Smallpox Epidemic How to Halt a Smallpox Epidemic", Center for Nonlinear Studies, Theoretical Division, Los Alamos National Laboratory.

107

108

Model Name Organization POC

Water Infrastructure Simulation Environment (WISE) Los Alamos National Laboratory Joe Holland

Infrastructures DW, SW, ST

Description Overview ­ The Water Infrastructure Simulation Environment (WISE) is an analytic framework supporting the evaluation of water infrastructure in terms of both infrastructure specific and interdependency issues. Development goals ­ Not specified. Intended users ­ Internal analyst ­ IEISS used to support the development of an impact report on for specific infrastructure events (such as, hurricanes, terrorist attacks, etc.). System output ­ Key components in the WISE framework are ArcWISE, a GIS based graphical user interface, and IEISS Water, a water infrastructure interdependency simulation capability within IEISS. ArcWISE leverages the existing data management, analysis, and display capabilities within geographic information systems while also extending them to infer, improve, and amend water infrastructure data in support of running hydraulic simulation engines such as EPANET or IEISS Water. ArcWISE also provides tools for defining and simulating infrastructure damage events, such as a fire, and generating water demand/sewage production estimates. IEISS Water is an extension of the IEISS analysis software to water distribution infrastructure simulation. Maturity ­ Development. Areas modeled ­ Numerous U.S. metropolitan areas. Customers/sponsors ­ NISAC ­ DHS. Model Framework Underlying model ­ Flow and Dispersion Model. Simulation ­ A continuous time based model with an underling physical engine for system dynamics. WISE involves the integration of geographic information systems with a wide range of infrastructure analysis tools including industry standard hydraulic simulation engines (e.g., EPANET and SWMM) as well as Los Alamos National Laboratory interdependency simulation systems such the Urban Infrastructure Suite (UIS) and the Interdependent Energy Infrastructure Simulation System (IEISS). Data format ­ Not specified. Sensor data ­ No direct sensor feeds. Coupling with other models ­ Yes, coupling is done indirectly. The output of IEISS will serve as the input to other infrastructure models to identify cross infrastructure effects. Human activity modeling ­ None at this time. System Requirements Not specified. Hardware ArcWise, EPANET and SWMM. Software Other Notes

109

References

110

Model Name Organization

POC

MIT Screening Methodology--A Screening Methodology for the Identification and Ranking of Infrastructure Vulnerabilities Due to Terrorism Massachusetts Institute of Technology (MIT), Infrastructures Engineering Systems Division and Department of Electric power, Nuclear Science and Engineering natural gas, and George E. Apostolakis drinking water apostola@mit.edu Douglas M. Lemon

Description Overview ­ This research proposes a methodology for the identification and prioritization of vulnerabilities in infrastructures. Portions of the Massachusetts Institute of Technology (MIT) campus were assessed using this methodology. Infrastructures are modeled as digraphs and graph theory is employed to identify the candidate vulnerable scenarios. Screening of scenarios is performed to produce a prioritized list of vulnerabilities. Prioritization is based on multiattribute utility theory (MAUT). The value of a lost element is based on a rated impact of losing infrastructure services. Development goals ­ Professor Apostolakas and others are continuing to extend this work as described in the "Other Notes" section, which follows. Intended users ­ The intended users for this methodology include analysts and decision makers for evaluation and risk management. System output ­ The system provides numeric ranking values for infrastructure elements as output. Maturity ­ This is a research and development level method. Areas modeled ­ Portions of MIT campus including electric power, water, and natural gas infrastructures. Customers/sponsors ­ MIT and the U.S. Navy sponsored the work. Model Framework Underlying model ­ This methodology is based on graph theory, MAUT (for identifying and ranking vulnerabilities), and mathematical network analysis (for infrastructure modeling). Simulation ­ The method allows simulations based on perceived terrorist threats. Data format ­ Not specified. Sensor data ­ None. Coupling with other models ­ Not specified. Human activity modeling ­ None. System Requirements Not specified. Hardware Not Specified. Software Other Notes Since the publication of the subject methodology, MIT has continued similar work. A recent paper, "A Methodology for Ranking the Elements of Water-Supply Networks," co-written by David Michaud-- also of MIT--has been accepted for publication in the Journal of Infrastructure Systems in 2006. That work is based on a case study of a mid-sized city and presents a scenario-based methodology for ranking elements of water-supply networks.

111

References Apostolakas, G., and Lemon, D. (2005). "A Screening Methodology for the Identification and Ranking of Infrastructure Vulnerabilities Due to Terrorism," Risk Analysis, Vol. 25, No. 2, pp. 361-376.

112

Model Name Organization POC

The Urban Infrastructure Suite (UIS) Los Alamos National Laboratory Randy Michelsen rem@lanl.gov Infrastructures HW, HA, TC, AST, SW, DW

Description Overview ­ The Urban Infrastructure Suite (UIS) is a set of interoperable modules that employ advanced modeling and simulation methodologies to represent urban infrastructures and populations. These simulation-based modules are linked through a common interface for the flow of information between UIS sector simulations to model urban transportation, telecommunications, public health, energy, financial (commodity markets), and water-distribution infrastructures and their interdependencies. x Urban Population Mobility Simulation Technologies (UPMoST) Module x Epidemiological Simulation Systems (EpiSims) Module x Telecommunications Sector: AdHopNet Module x Transportation Analysis Simulation System (TRANSIMS) Module x Water Infrastructure Simulation Environment (WISE) x Generic Cities Project Development goals ­ The project objective (NISAC) is to understand the infrastructures' performance under unusual conditions, the effects of interdependencies, and the dynamics of their interconnections. To better understand the complexities of the interconnected infrastructures, the team has collaborated with private sector infrastructure experts to develop methodologies and tools for characterizing and simulating their performance. Intended users ­ LANL internal analysts. System Output ­ Graphical overlays and textual based output. Maturity ­ Development. Areas modeled ­ Multiple. Customers/sponsors ­ DHS ­ NISAC. Model Framework Underlying models: x Urban Population Mobility Simulation Technologies (UPMoST) Module x Epidemiological Simulation Systems (EpiSims) Module - a contact-based approach for evaluating the spread of disease among a populace. It looks at infection rates based on the assumed numbers of contacts people in different demographic groups might have with others in their families, workplaces, and communities. Interactions/contacts are based on the TRANSIM's mobility model. x Telecommunications Sector: MIITS Module (formerly AdHopNet) ­ end to end communications system simulation, agent based simulating individual packets, devices, connections, etc., input is TRANSIMS mobility model. x Transportation Analysis Simulation System (TRANSIMS) Module ­ synthetic population model, cellular automata microsimulation. The output is population mobility with demographics x Water Infrastructure Simulation Environment (WISE) ­ is an analytic framework supporting the evaluation of water infrastructure in terms of both infrastructure specific and interdependency issues. x Generic Cities Project ­ module to create representative but not necessarily accurate city representations in terms of demographic data. Simulation ­ TRANSIM mobility/social network model--agent-based, Epidemic model-- differential equation based. Data ­ Multiple sources. Sensor data ­ None. Ability to couple with other models ­ Suite of coupled modules for different infrastructure 113

sectors. Human Activity modeling ­ Yes. Mobility and Social Interaction Model. System Requirements Hardware Software Other Notes Large models require a Linux Cluster. Linux, various.

Images:

References Barrett, Christopher L, Stephen Eubank, V.S. Anil Kumar, and Madhav V. Marathe From The Mathematics of Networks, Understanding Large-Scale Social and Infrastructure Networks: A Simulation-Based Approach, SIAM News, Volume 37, Number 4, May 2004

114

REFERENCES
1. 2. 3. 4. 5. 6. 7. 8. Congressional Research Service Report for Congress. 2002 Critical Infrastructures: Background, Policy and Implementation. Available online at http://www.iwar.org.uk/cip/resources/pdd63/crs-report.pdf. Congressional Research Service Report for Congress. 2002 Critical Infrastructures: Background, Policy and Implementation. Available online at http://www.iwar.org.uk/cip/resources/pdd63/crs-report.pdf. http://www.tswg.gov Executive Order, 13010. Critical Infrastructure Protection. Federal Register. Vol. 61. No. 138. July 17, 1996. pp. 3747-3750. Executive Order, 13130 National Infrastructure Assurance Council, Federal Register, Vol. 64, No. 137, July 19, 1999. pp. 38535-38536. Executive order 13231 Critical Infrastructure Protection in the Information Age. Federal Register. Vol. 66. No. 202. October 18, 2001. pp. 53063­53071. The NIAC is established on page 53069. D. Mussington, "Concepts for Enhancing Critical Infrastructure Protection: Relating Y2K to CIP Research and Development." RAND:Science and Technology Institute, Santa Monica, CA, 2002, p 29. Layton, L. and D. Phillips. 2001. Train Sets Tunnel Afire, Shuts Down Baltimore. Available online via < http://www.washingtonpost.com/ac2/wp-dyn?pagename=article&node=&contentId=A175422001Jul18>, accessed March 28, 2002. Ratner, A. 2001. Train derailment severs communications. Available online via <http://www.baltimoresun.com/news/local/bal-email19.story?coll=bal-home-headlines>, accessed March 28, 2002.

9.

10. Little, R. and P. Adams. 2001, Tunnel fire choking East Coast rail freight. Available online via <http://www.baltimoresun.com/news/local/bal-te.bz.freight20jul20.story?coll= bal-home-headlines>, Accessed March 28, 2002. 11. M. Dunn, and I.Wigert. International CIIP Handbook 2004: An Inventory and Analysis of Protection Policies in Fourteen Countries. Zurich: Swiss Federal Institute of Technology: 2004, p. 243. 12. D. D. Dudenhoeffer, M. R. Permann, and M Manic, "CIMS: A Framework For Infrastructure Interdependency Modeling And Analysis." Submitted to Proceedings of the 2006 Winter Simulation Conference, L. F. Perrone, F. P. Wieland, J. Liu, B. G. Lawson, D. M. Nicol, and R. M. Fujimoto. Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, 2006. 13. S. Rinaldi, J. Peerenboom, and T. Kelly. "Identifying, Understanding, and Analyzing Critical Infrastructure Interdependencies," IEEE Control Systems Magazine, IEEE, December 2001, pp. 11-25. 14. U.S. Department Of Energy Office of Critical Infrastructure Protection. 2001. Critical Infrastructure Interdependencies: Impact of the September 11 Terrorist Attacks on the World Trade Center, A Case Study, 2001, p. 10. 15. UNITED STATES CONGRESS, "U.S.A. Patriot Act", 2001, http://www.epic.org/privacy/terrorism/hr3162.html. 16. UNITED STATES CONGRESS, "U.S.A. Patriot" Act, 2001, http://www.epic.org/privacy/terrorism/hr3162.html 115

17. United States Joint Forces Command, Operational Net Assessment (ONA) Concept Of Operations For Millennium Challenge 02 (MC-02), October 2001. 18. Hammes, T. The Sling and the Stone, Zenith Press, St. Paul MN, 2004, p.2. 19. United States Joint Forces Command, The Joint Warfighting Center, Joint Doctrine Series Pamphlet 4, Doctrinal Implications of Operational Net Assessment (ONA), 2004. 20. LandScan, http://www.ornl.gov/sci/landscan/index.html. 21. Los Alamos National Laboratory, http://lanl.gov/orgs/d/nisac/, http://www.sandia.gov/mission/homeland/programs/critical/nisac.html. 22. National Energy Technology Laboratory, http://www.netl.doe.gov/onsite_research/Facilities/energy.html. 23. Technical Support Working Group, http://www.tswg.gov. 24. Federal Business Opportunities, http://www.fbo.gov/spg/USAF/AFMC/ AFRLRRS/Reference%2DNumber%2DBAA%2D06%2D07%2DIFKA/SynopsisP.html. 25. Amin, M. "National Infrastructures as Complex Interactive Networks", Chapter 14 in Automation,Control, and Complexity: New Developments and Directions, Samad & Weyrauch (Eds.), John Wiley and Sons, March 2000, Table 14.1. 26. Schmitz, W. and K. A. Neubecker. 2003. Architecture of an Integrated Model Hierarchy: Work Package 6, Deliverable D6.2, ACIP Technical Report IST-2001-37257: 32. Available via http://www.iabg.de/acip/doc/wp6/D62_architecture.pdf, Accessed April 1, 2006.

116

arXiv:1404.6890v1 [cs.DS] 28 Apr 2014

Analysis of d-Hop Dominating Set Problem for
Directed Graph with Indegree Bounded by One
Joydeep Banerjee, Arun Das and Arunabha Sen
Computer Science and Engineering Program,
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
{Joydeep.Banerjee, adas,asen}@asu.edu
April 29, 2014
Abstract
d-hop dominating set problem was introduced for cluster formation in
wireless ad-hoc networks and is proved to be NP-complete. Dominating
set problem for directed graph with indegree of at most 1 can be solved
polynomially. In this article we perform an analysis of d-hop dominating
set problem for directed graph with indegree 1. We show that the problem
can be solved polynomially by exploiting certain properties of the graph
under consideration.

1

Introduction

d-hop minimum dominating set problem was introduced in [1] for cluster formation in wireless ad-hoc networks. The problem is proved to be NP complete.
The decision version of minimum d-hop dominating set problem is given as follows:
Instance: An undirected graph G = (V, E), two positive integers d and K.
â²
â²
Question: Is there a subset |V | â V with |V | â¤ K such that for every vertex
â²
â²
vâ
/ V â V is at most d hops away from at least one vertex in V .
In a directed graph G = (V, E) a node u dominates a node v if the edge
(u, v) â E. For a directed graph dominating set is proved to NP complete[2].
But for directed graph with indegree of at most 1, the problem can be solved
polynomially [2].
In this article we analyze the problem d-hop minimum dominating set problem
for directed graph with indegree bounded by 1. We show the existence of a poly-

1

nomial time algorithm of the problem by exploiting certain properties of the
graph under consideration.

2

Analysis of the Problem

Certain properties and definitions of a directed graph with indegree bounded
by one (denoted as the graph GD = (VD , ED ) is introduced before analysis of
the problem.
Property 2.1 Each weakly connected subgraph of the directed graph GD consists of at most one cycle with no incoming edge to a node in the cycle from a
node not in the cycle.
Thus a weakly connected component is either a Directed Acyclic Graph (DAG)
or has a cycle with all nodes not in the cycle having a directed path from all
nodes in the cycle to it.
Definition 2.1 A leaf node of a weakly connected component is defined
as the node with an incoming edge and no outgoing edge.
Definition 2.2 The farthest leaf node of a weakly connected component is
defined as the node which is at furthest hops from the root node (if the graph is
a DAG) or from a node in the cycle.
Definition 2.3 An isolated strongly connected component is defined as
the component which is not a subgraph of a weakly connected component of the
graph GD
Property 2.2 Each isolated strongly connected subgraph of the directed graph
GD is composed of exactly one cycle.
Exploiting the properties of the graph GD an algorithm (Algorithm 1) is
framed. The proof of optimality and time complexity analysis of the algorithm
are done in Theorem 1 and Theorem 2 respectively.
Theorem 2.1 Algorithm 1 gives the optimum solution of d-hop dominating set
problem for the graph GD .
Proof For an isolated strongly connected component GS = (VS , ES ) at least
â |VdS | â nodes has to be included in the solution. Algorithm 1 selects â |VdS | â nodes
with at least â |VdS | â â 1 nodes separated by exactly d hops. Thus it includes
the optimum nodes in the solution. For a weakly connected component a leaf
node has to be dominated by a node within d hops away from it. Selecting
the farthest leaf node would ensure that after updating the graph GW with
node removals would not result in segregation of the updated graph with more
than one weakly connected component. Moreover selecting a leaf node which
2

Algorithm 1 Algorithm for finding d-hop dominating set of graph GD
Set D = â;
Compute all weakly connected component and isolated strongly connected
component of graph GD ;
for (Each weakly connected component GW = (VW , EW ) of graph GD ) do
while (Graph GW is not empty) do
if (Graph GW is a isolated strongly connected component) then
For all n nodes in graph GW , include â nd â nodes in set D with at
least â nd â â 1 nodes separated by exactly d hops;
break;
end if
Pick the farthest leaf node v from graph GW ;
Include node u in set D such that the number of hops from u to v is
maximum but is less than d;
Update graph GW by removing all nodes (and their edges) which are
within d hops from u, including node u;
end while
end for
for (Each isolated strongly connected component GS = (VS , ES ) of graph
GD ) do
Include â |VdS | â nodes in set D with at least â |VdS | â â 1 nodes separated by
exactly d hops;
end for
is not farthest might result in excluding the node which d-hop dominates it
along with the farthest leaf node. Thus another node needs to be included in
the solution to d-hop dominate the farthest leaf node and so the solution would
not be optimum. Hence a node that is maximum hops away from the farthest
leaf node (with number of hops less than equal to d) for the weakly connected
graph GW (with or without updating with node removals) has to be included
in the optimum solution of the problem. Additionally consider computing the
d-hop dominating set for a weakly connected component GW with one cycle.
This may result in an isolated strongly connected component of original graph
GW (i.e. the cycle) after subsequent update of graph GW with node removals
(as in Algorithm 1). The d-hop dominating set of the resultant graph then can
be solved similarly to that of strongly connected component, which is optimum.
Hence Algorithm 1 computes the optimum d-hop dominating set problem for
each weakly connected component and strongly connected component of graph
GD polynomially.
Theorem 2.2 Algorithm 1 solves d-hop dominating set problem for the graph
GD polynomially with time complexity of order O
Proof All strongly connected components can be found using Tarjanâs strongly
connected components algorithm [3] in O(|VD | + |ED |) time. The isolated
3

strongly connected component from the computed strongly connected components can be found in another O(|VD |) time. Hence computing the isolated
strongly connected component takes O(|VD |(|VD | + |ED |)) time. Deleting all
nodes in the strongly connected component would result in segregation of the
graph GD into weakly connected components. All weakly connected components can be found by computing existence of pairwise path between two nodes.
It can be done in O(|VD | + |ED |) time for each pair and each weakly connected
component can be computed in O(|VD |(|VD |+ED |)) time. Both the first for loop
and the while loop inside it iterates for at most O(|VD |) times. The operation
inside the if branch can be computed O(|VD |) time. The farthest leaf node can
be found in O(|VD |2 ) time. The d-hop dominating node of the farthest leaf node
can be found in O(|VD |) time. Hence the d-hop dominating set computation
for all weakly connected component takes O(|VD |4 ) time. For computing the
d-hop dominating set for all strongly connected component, the for loop takes
O(|VD |) time. The computation inside the for loop is same as the computation
inside the if branch of the weakly connected component d-hop dominating set
computation and takes O(|VD |) time. So the total time complexity for finding
d-hop dominating set for strongly connected component is O(|VD |2 ). Hence
Algorithm 1 takes in total an O(|VD |4 ) time to compute the optimum solution
of the problem.

3

Conclusion

In this article analysis of d-hop dominating set for directed graph with indegree
bounded by one is performed. It is found that exploiting certain properties of
the graph under consideration an algorithm can solve the problem in polynomial
time, with run time complexity bounded by four times the number of vertices
in the graph.

References
[1] Amis, Alan D., et al. âMax-min d-cluster formation in wireless ad hoc networks.â INFOCOM 2000. Nineteenth Annual Joint Conference of the IEEE
Computer and Communications Societies. Proceedings. IEEE. Vol. 1. IEEE,
2000.
[2] Albers, Susanne, and Tomasz Radzik, eds. AlgorithmsâESA 2004: 12th Annual European Symposium, Bergen, Norway, September 14-17, 2004, Proceedings. Vol. 12. Springer, 2004.
[3] Tarjan, R. E. âDepth-first search and linear graph algorithmsâ, SIAM Journal on Computing 1 (2): 146160, 1972.

4

arXiv:1404.6890v1 [cs.DS] 28 Apr 2014

Analysis of d-Hop Dominating Set Problem for
Directed Graph with Indegree Bounded by One
Joydeep Banerjee, Arun Das and Arunabha Sen
Computer Science and Engineering Program,
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
{Joydeep.Banerjee, adas,asen}@asu.edu
April 29, 2014
Abstract
d-hop dominating set problem was introduced for cluster formation in
wireless ad-hoc networks and is proved to be NP-complete. Dominating
set problem for directed graph with indegree of at most 1 can be solved
polynomially. In this article we perform an analysis of d-hop dominating
set problem for directed graph with indegree 1. We show that the problem
can be solved polynomially by exploiting certain properties of the graph
under consideration.

1

Introduction

d-hop minimum dominating set problem was introduced in [1] for cluster formation in wireless ad-hoc networks. The problem is proved to be NP complete.
The decision version of minimum d-hop dominating set problem is given as follows:
Instance: An undirected graph G = (V, E), two positive integers d and K.
â²
â²
Question: Is there a subset |V | â V with |V | â¤ K such that for every vertex
â²
â²
vâ
/ V â V is at most d hops away from at least one vertex in V .
In a directed graph G = (V, E) a node u dominates a node v if the edge
(u, v) â E. For a directed graph dominating set is proved to NP complete[2].
But for directed graph with indegree of at most 1, the problem can be solved
polynomially [2].
In this article we analyze the problem d-hop minimum dominating set problem
for directed graph with indegree bounded by 1. We show the existence of a poly-

1

nomial time algorithm of the problem by exploiting certain properties of the
graph under consideration.

2

Analysis of the Problem

Certain properties and definitions of a directed graph with indegree bounded
by one (denoted as the graph GD = (VD , ED ) is introduced before analysis of
the problem.
Property 2.1 Each weakly connected subgraph of the directed graph GD consists of at most one cycle with no incoming edge to a node in the cycle from a
node not in the cycle.
Thus a weakly connected component is either a Directed Acyclic Graph (DAG)
or has a cycle with all nodes not in the cycle having a directed path from all
nodes in the cycle to it.
Definition 2.1 A leaf node of a weakly connected component is defined
as the node with an incoming edge and no outgoing edge.
Definition 2.2 The farthest leaf node of a weakly connected component is
defined as the node which is at furthest hops from the root node (if the graph is
a DAG) or from a node in the cycle.
Definition 2.3 An isolated strongly connected component is defined as
the component which is not a subgraph of a weakly connected component of the
graph GD
Property 2.2 Each isolated strongly connected subgraph of the directed graph
GD is composed of exactly one cycle.
Exploiting the properties of the graph GD an algorithm (Algorithm 1) is
framed. The proof of optimality and time complexity analysis of the algorithm
are done in Theorem 1 and Theorem 2 respectively.
Theorem 2.1 Algorithm 1 gives the optimum solution of d-hop dominating set
problem for the graph GD .
Proof For an isolated strongly connected component GS = (VS , ES ) at least
â |VdS | â nodes has to be included in the solution. Algorithm 1 selects â |VdS | â nodes
with at least â |VdS | â â 1 nodes separated by exactly d hops. Thus it includes
the optimum nodes in the solution. For a weakly connected component a leaf
node has to be dominated by a node within d hops away from it. Selecting
the farthest leaf node would ensure that after updating the graph GW with
node removals would not result in segregation of the updated graph with more
than one weakly connected component. Moreover selecting a leaf node which
2

Algorithm 1 Algorithm for finding d-hop dominating set of graph GD
Set D = â;
Compute all weakly connected component and isolated strongly connected
component of graph GD ;
for (Each weakly connected component GW = (VW , EW ) of graph GD ) do
while (Graph GW is not empty) do
if (Graph GW is a isolated strongly connected component) then
For all n nodes in graph GW , include â nd â nodes in set D with at
least â nd â â 1 nodes separated by exactly d hops;
break;
end if
Pick the farthest leaf node v from graph GW ;
Include node u in set D such that the number of hops from u to v is
maximum but is less than d;
Update graph GW by removing all nodes (and their edges) which are
within d hops from u, including node u;
end while
end for
for (Each isolated strongly connected component GS = (VS , ES ) of graph
GD ) do
Include â |VdS | â nodes in set D with at least â |VdS | â â 1 nodes separated by
exactly d hops;
end for
is not farthest might result in excluding the node which d-hop dominates it
along with the farthest leaf node. Thus another node needs to be included in
the solution to d-hop dominate the farthest leaf node and so the solution would
not be optimum. Hence a node that is maximum hops away from the farthest
leaf node (with number of hops less than equal to d) for the weakly connected
graph GW (with or without updating with node removals) has to be included
in the optimum solution of the problem. Additionally consider computing the
d-hop dominating set for a weakly connected component GW with one cycle.
This may result in an isolated strongly connected component of original graph
GW (i.e. the cycle) after subsequent update of graph GW with node removals
(as in Algorithm 1). The d-hop dominating set of the resultant graph then can
be solved similarly to that of strongly connected component, which is optimum.
Hence Algorithm 1 computes the optimum d-hop dominating set problem for
each weakly connected component and strongly connected component of graph
GD polynomially.
Theorem 2.2 Algorithm 1 solves d-hop dominating set problem for the graph
GD polynomially with time complexity of order O
Proof All strongly connected components can be found using Tarjanâs strongly
connected components algorithm [3] in O(|VD | + |ED |) time. The isolated
3

strongly connected component from the computed strongly connected components can be found in another O(|VD |) time. Hence computing the isolated
strongly connected component takes O(|VD |(|VD | + |ED |)) time. Deleting all
nodes in the strongly connected component would result in segregation of the
graph GD into weakly connected components. All weakly connected components can be found by computing existence of pairwise path between two nodes.
It can be done in O(|VD | + |ED |) time for each pair and each weakly connected
component can be computed in O(|VD |(|VD |+ED |)) time. Both the first for loop
and the while loop inside it iterates for at most O(|VD |) times. The operation
inside the if branch can be computed O(|VD |) time. The farthest leaf node can
be found in O(|VD |2 ) time. The d-hop dominating node of the farthest leaf node
can be found in O(|VD |) time. Hence the d-hop dominating set computation
for all weakly connected component takes O(|VD |4 ) time. For computing the
d-hop dominating set for all strongly connected component, the for loop takes
O(|VD |) time. The computation inside the for loop is same as the computation
inside the if branch of the weakly connected component d-hop dominating set
computation and takes O(|VD |) time. So the total time complexity for finding
d-hop dominating set for strongly connected component is O(|VD |2 ). Hence
Algorithm 1 takes in total an O(|VD |4 ) time to compute the optimum solution
of the problem.

3

Conclusion

In this article analysis of d-hop dominating set for directed graph with indegree
bounded by one is performed. It is found that exploiting certain properties of
the graph under consideration an algorithm can solve the problem in polynomial
time, with run time complexity bounded by four times the number of vertices
in the graph.

References
[1] Amis, Alan D., et al. âMax-min d-cluster formation in wireless ad hoc networks.â INFOCOM 2000. Nineteenth Annual Joint Conference of the IEEE
Computer and Communications Societies. Proceedings. IEEE. Vol. 1. IEEE,
2000.
[2] Albers, Susanne, and Tomasz Radzik, eds. AlgorithmsâESA 2004: 12th Annual European Symposium, Bergen, Norway, September 14-17, 2004, Proceedings. Vol. 12. Springer, 2004.
[3] Tarjan, R. E. âDepth-first search and linear graph algorithmsâ, SIAM Journal on Computing 1 (2): 146160, 1972.

4

Analysis of On-line Routing and Spectrum
Allocation in Spectrum-sliced Optical Networks
Shahrzad Shirazipourazad, Zahra Derakhshandeh and Arunabha Sen
School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {sshiraz1, zderakhs, asen}@asu.edu

AbstractâThe orthogonal frequency division multiplexing
(OFDM) technology provides an opportunity for efficient resource utilization in optical networks. It allows allocation of
multiple sub-carriers to meet traffic demands of varying size.
Utilizing OFDM technology, a spectrum efficient and scalable optical transport network called SLICE was proposed recently. The
SLICE architecture enables sub-wavelength, super-wavelength
resource allocation and multiple rate data traffic that results
in efficient use of spectrum. However, the benefit is accompanied
by additional complexities in resource allocation. In SLICE
architecture, in order to minimize utilized spectrum, one has
to solve the routing and spectrum allocation (RSA) problem, a
generalization of the routing and wavelength allocation (RWA)
problem. In this paper, we focus our attention to the on-line
version of RSA problem and provide an algorithm for the ring
network with a competitive ratio of min{O(log(dmax )), O(log(k))}
where k is the total number of requests and dmax is the maximum
demand in terms of the number of sub-carriers. Moreover, we
provide a heuristic for the network with arbitrary topology
and measure the effectiveness of the heuristic with extensive
simulation.

I. I NTRODUCTION
It is being increasingly recognized by the optical network
designers that in order to meet the challenges posed by the
explosive growth of the network traffic, the networks must
be operated in the most innovative and efficient manner. The
traditional WDM network operates at the granularity of a
wavelength, which may lead to inefficient use of resources
as some connection requests may not have enough traffic
to utilize the full capacity of a wavelength. However such
wastage of networking resources can be avoided if the optical
network can be made to operate at a finer grain (i.e., subwavelength level) instead of the current practice of course
grain operation (i.e., wavelength level). Recent introduction of
Orthogonal Frequency Division Multiplexing (OFDM) technology in optical networks [1] offers an opportunity for
operating optical networks at a much finer grain than what
is currently possible. The advantages offered by the OFDM in
terms of flexibility and scalability originate from the unique
multicarrier nature of this technology [1].
Utilizing the OFDM technology, a spectrum efficient and
scalable optical transport network called spectrum-sliced elastic optical path network (SLICE) was proposed recently [2].
Just as the ability to operate at a granularity finer than a
wavelength (i.e., a sub-wavelength) will enable the network
operator to manage resources more efficiently, the same is

true if the operator is provided with capability to operate at
super-wavelength granularity. Such a capability will be useful
for the network operator to meet large traffic demand. The
goal of SLICE architecture is to allocate variable sized optical
bandwidths that matches with the user traffic demands. It
achieves that goal by slicing off spectral resources of a route
and allocating only the requested amount to establish an endto-end optical path.
Although the sub-wavelength (sub-carrier) level allocation
capability of SLICE leads to more effective resource utilization, it also leads to additional complexities in network control
and management. First, if a call requests for d sub-carriers, the
network controller must allocate d consecutive sub-carriers to
this request. Second, if the paths corresponding to two requests
R1 and R2 share a fiber link, not only the set of carriers
allocated to R1 and R2 must be disjoint, in order to avoid
interference, they must be separated from each other in the
spectrum domain by a few carriers, known as guard carriers
or guard bands. The first and the second constraints are known
as the sub-carrier consecutiveness constraint and the guardcarrier constraint respectively [3]. The introduction of the subcarrier consecutiveness constraint significantly increases the
complexity of the Routing and Spectrum Assignment (RSA)
problem that needs to be solved in SLICE. The RSA problem may be informally defined as follows: Given a network
topology and a set of call requests with varying demands (in
terms of the number of sub-carriers) find a route for each
request and allocate a number of sub-carriers to each request
(equal to their requested demand), so that the utilized part
of the spectrum span is minimized. It may be noted that if
the demand of each request is one sub-carrier, then the RSA
problem reduces to the Routing and Wavelength Assignment
(RWA) problem, which has been studied extensively. One can
conceive of two different versions of the RSA problem - offline and on-line. In the off-line version all the requests are
known ahead of time before path and spectrum allocation for
any request is carried out. In the on-line version, the requests
come in a sequence and path and spectrum allocation for a
request has to carried out at the time of arrival of that request.
Because the off-line version has the luxury of knowing all
the requests, it can carry out better optimization of utilized
spectrum span than its on-line counterpart.
In this paper we study the on-line version of RSA problem.
Previous studies on the on-line version of the RSA problem,

[4]â[9] primarily focus on the development of efficient heuristics for the problem. The effectiveness of these heuristics are
primarily evaluated through simulation. To the best of our
knowledge, very little analytical results are available in the
literature regarding the performance of these heuristics. In
this paper we present analytical results relating to the on-line
version of the RSA problem when the network topology is
a ring. The performance of an on-line algorithm is measured
in terms of the metric competitive ratio. In this metric, the
performance of an on-line algorithm is compared with the
performance of an optimal off-line algorithm that knows the
sequence of requests in advance. The maximum ratio between
their respective performances, taken over all sequences, is
known as the competitive ratio of the algorithm [10].
In this paper, we provide an algorithm for the on-line version
of the RSA problem for the ring network with a competitive
ratio of min{O(log(dmax )), O(log(k))} where k is the total
number of requests, dmax = max1â¤iâ¤k di , and di is the
demand in terms of the number of sub-carriers associated with
request Ri . Moreover, we provide a heuristic for the network
with arbitrary topology and measure the effectiveness of the
heuristic with extensive simulation.
The rest of the paper is organized as follows. We discuss
related works in section II. In section III we introduce definitions and notations. We present problem statement for the
on-line RSA problem in section IV. Analytical results for
on-line RSA in rings is presented in section V. A heuristic
and experimental results for the arbitrary network topology is
presented in section VI. Section VII concludes the paper.
II. R ELATED W ORK
Utilizing the optical OFDM technology, the SLICE architecture proposes a novel scheme for slicing off the spectral
resources of a route, resulting in more efficient utilization [2].
The fact that the sub-carriers in the SLICE architecture have
to be assigned in a contiguous manner, led to the formulation
of the RSA problem. To the best of our knowledge, the
RSA problem was originally introduced in [4], [11], [12].
Since then a few other papers, [3], [13] have also studied
the RSA problem. In most of these studies [3], [12]â[14], the
authors propose an integer linear program based solution and a
heuristic solution for the off-line RSA problem. Based on the
experimental results, the authors claim effectiveness of their
heuristics.
The on-line version of RSA problem has been studied in
[4]â[9]. In all of these papers, the objective of the on-line
RSA problem is to maximize the number of requests that
can be satisfied and minimize the blocking probability. In
this version of on-line RSA problem, the number of available
spectrum sub-carriers is limited. The authors of these papers
proposed heuristic solutions mainly by modifying the Dijkstra
shortest path algorithm or using K-shortest path algorithm
accompanying with the First-Fit algorithm. To the best of
our knowledge none of these papers consider the objective
of minimizing the utilized spectrum while satisfying all the
requests. It may be the case that all the requests should be
satisfied while the utilized spectrum is minimized. In this paper

we propose a new heuristic for arbitrary network graphs. We
also modify the K-shortest path approach for this version of
the on-line RSA and through simulations we evaluate their
performance.
Most of the studies both on on-line and off-line RSA do
not present any analytical results for the RSA problem, even
for the simplest optical network topologies such as rings. The
ring topology is of particular importance in the optical domain
because of its application in metro networks and in some
long haul networks. A major thrust of our effort is to present
analytical results for the on-line RSA for optical networks with
ring structure.
III. D EFINITIONS AND N OTATIONS
Spectrum Slice/Interval: A number of consecutive sub-carriers
from ai to bi denoted by [ai , bi ], that is allocated to a specific
request Ri to establish a connection between (si , ti ) with di
sub-carriers. The length of this slice is bi â ai + 1 = di .
Spectrum Span/Spread: The total amount of spectrum used
for allocating a slice to all the requests; If Ri , 1 â¤ i â¤ k is
allocated the spectrum interval [ai , bi ] then the spectrum span
is [ min ai , max bi ].
1â¤iâ¤k

1â¤iâ¤k

Chromatic Number: The Chromatic Number, Ï(G), of a graph
G = (V, E) is the fewest number of colors necessary to color
the nodes of the graph, such that no two adjacent nodes have
the same color.
Interval Chromatic Number (ICN): Consider a weighted graph
Gâ = (V, E, w) with a strictly positive integer weight w(v)
associated with each node v â V . An interval t-coloring of
Gâ = (V, E, w) is a function c from V to {1,2, . . . , t} such
that c(x) + w(x) â 1 â¤ t and if both c(x) â¤ c(y) and (x, y) â
E then c(x) + w(x) â 1 < c(y). We can view an interval
coloring c of Gâ as assigning an interval [c(v), . . . , c(v) +
w(v) â 1] of w(v) consecutive colors to each vertex v so
that the intervals of colors assigned to two adjacent vertices
(i.e., the pair of nodes that has an edge between them) do not
overlap. If interval t-coloring is feasible for a graph Gâ then
Gâ is said to be interval t-colorable. The interval chromatic
number of Gâ , denoted by Ïint (Gâ ) is the least t such that
Gâ has a interval t-coloring [15].
Interval Graph: Let F be a family of non-empty sets. The
intersection graph of F is obtained by representing each set
in F by a node and connecting the two nodes with an edge, if
and only if the corresponding sets intersect. The intersection
graph of a family of intervals on a linearly ordered set (such
as the real line) is called Interval Graph.
Path Intersection Graph: Consider a graph G = (V, E) and
a set of paths P = {P1 , . . . , Pk }, where each Pi is a path
between a node pair (si , ti ), âi, 1 â¤ i â¤ k. A graph G0 =
(V 0 , E 0 ) is a Path Intersection Graph corresponding to P, if
each vertex pi â V 0 corresponds to a path Pi â P and two
nodes pi and pj in V 0 have an edge between them, if the
corresponding paths Pi and Pj in P have at least one common
edge in E.
IV. P ROBLEM F ORMULATION
In this section we provide a formal statement of the on-line
routing and spectrum allocation problem.

On-line Routing and Spectrum Allocation (RSA) Problem:
A graph G = (V, E) representing the network topology is
given. The connection requests arrive in a sequence one by one
where k is the total number of requests. The ith connection
request is denoted by a triple Ri = (si , ti , di ), 1 â¤ i â¤ k,
where si represents a source node, ti represents a destination
node, and di represents the demand between si and ti in terms
of sub-carriers. Once a request Ri arrives without knowledge
of the future requests, assign a path Pi from si to ti and
assign a spectrum interval Ii = [ai , bi ] of length di to Pi ,
such that for every pair of requests i and j, j â¤ i the intervals
Ii and Ij do not overlap if the corresponding paths Pi and Pj
share an edge between them in G = (V, E). Moreover, if the
paths Pi and Pj overlap, not only the corresponding intervals
Ii and Ij must be non-overlapping, these two intervals must
be separated by a fixed number of sub-carriers, known as
the guard band. The objective is to minimize spectrum span,
I = [min1â¤iâ¤k ai , max1â¤iâ¤k bi ]. Without loss of generality,
we number the first available sub-carrier one and the rest are
numbered accordingly.
We note that guard-band constraint can be satisfied by
increasing the demand values by guard-band value g. In other
words, in an instance of RSA problem, RSA1 with guard-band
g1 > 0 and requests {Ri = (si , ti , di )|1 â¤ i â¤ k}, we can
increase the demand values in each request by g1 and consider
another instance of RSA, RSA2 where guard-band g2 = 0 and
for every request Ri in RSA1 , request Ri0 = (si , ti , di + g1 )
is added to RSA2 . Then the optimal solution of RSA2 can
be used to create the optimal solution of RSA1 by removing
the last g1 sub-carriers from each spectrum slice assigned to
each request (for the proof, reader is referred to the proof of
the Observation 4 in [16]). As a result, from this point onward
we assume that guard-band is zero.
The RSA problem has two distinct components - the routing
component and the spectrum allocation component. When
routing is given and the paths for the requests are known
then interval chromatic number (ICN) of the intersection graph
of request paths finds the solution of the SA problem. Let
G0 = (V 0 , E 0 , w) be the weighted path intersection graph of
paths of all requests where V 0 = {p1 , p2 , . . . , pk } and each
node pi corresponds to the path of request Ri and the weight
of pi is di ; i.e., w(pi ) = di . Let Ïint (G0 ) be the ICN of graph
G0 . In computation of Ïint (G0 ), each node pi â V 0 is assigned
an interval [ai , bi ] of colors with length w(pi ) = di where the
intervals of two adjacent vertices do not intersect and total
number of distinct colors used is minimum. Therefore, interval
[ai , bi ] can be allocated to the path Pi in G and no two paths
with common edge intersect in their spectrum intervals. Hence,
the spectrum span of Ïint (G0 ) is sufficient for the spectrum
allocation of requests in G with predefined set of paths P.
Moreover, Ïint (G0 ) is the minimum spectrum span needed in
the SA problem; otherwise, it contradicts with Ïint (G0 ) being
the minimum interval chromatic number of G0 . It is known
that computation of ICN of interval graphs is NP-complete
(Problem SR2 in [17]).
Fig. 1 shows an example of SA instance where the network
graph is a ring with 8 nodes and requests are {R1 =

p1

1
8

2

p3

p4

3

7

6

4

p2

p5

5

(a)

(b)

Fig. 1. (a) An example of SA instance where the network graph is a ring
(b) Path intersection graph G0 of SA instance in (a)

(1, 3, 15), R2 = (1, 6, 6), R3 = (2, 5, 6), R4 = (2, 8, 6), R5 =
(4, 7, 12)}. Dashed lines show the paths for the requests. Fig.
1(b) depicts G0 , the path intersection graph of these paths
where w(p1 ) = 15, w(p2 ) = 6, w(p3 ) = 6, w(p4 ) = 6
and w(p5 ) = 12. In this example, Ïint (G0 ) is 24 where
the requests R1 to R5 are assigned intervals [1, 15], [13, 18],
[16, 21], [19, 24] and [1, 12] respectively.
V. O N - LINE ROUTING AND S PECTRUM A LLOCATION
P ROBLEM IN R INGS
Theorem 1: RSA problem (the off-line case) is NPComplete when the optical network topology is a Ring.
Proof: If the demands of the requests in the off-line RSA
instance are all equal to one, then RSA problem becomes RWA
problem. In [18], it is proven that the RWA problem for optical
networks with a ring topology is NP-complete. Since RWA
problem is a special case of the RSA problem, it follows that
the RSA problem for optical networks with a ring topology is
also NP-complete.
Next, we propose an on-line algorithm for RSA problem
when network topology is a ring. In this algorithm, first we use
cut-one-link approach and after removing one link the induced
graph is a chain. In the chain for every request there exists
just one path. Therefore routing is trivial. For the spectrum
assignment, we use First-Fit technique that finds the first free
spectrum interval fit the demand of the current request. The
steps of the algorithms are explained in Algorithm 1.
Algorithm 1 On-line RSA in Ring
1: Remove an edge e â E randomly; Let Gp be the induced
chain;
2: while A new request arrives do
3:
Find the path for the request in graph Gp ;
4:
Compute the first free spectrum interval fit the demand
of the current request
5: end while
Theorem 2: Algorithm 1 has competitive ratio of
min{O(log(dmax )), O(log(k))} where k is total number of
requests and dmax = max1â¤iâ¤k di .
Proof: In order to compute the competitive ratio we need to
compare the spectrum span of Algorithm 1 with the optimal
spectrum span of off-line RSA where the sequence of requests
is known in advance. After removing one edge randomly
from G = (V, E) in Algorithm 1, the induced graph Gp
is a chain. Let OP T and OP Tp be the optimal spectrum

span in RSA problem when network graph is G and Gp ,
respectively, and I be the size of the spectrum computed by
Algorithm 1. Clearly, the intersection graph of paths of the
requests in Gp is an interval graph (a path from node i to
node j in Gp can be interpreted as an interval from i to j).
Let G0p be the path intersection graph. Therefore, minimum
spectrum needed to satisfy requests in Gp is equivalent to the
Ïint (G0p ). Based on the paper [10], First-Fit algorithm will
have competitive ratio of min{O(log(dmax )), O(log(ÏG0p ))}
for on-line interval coloring in G0p . Also it is obvious ÏG0p â¤ k
(i.e., chromatic number of G0p is at most as large as the number
of nodes in G0p that is number of requests). Hence, we have (1)
I â¤ min{O(log(dmax )), O(log(k))} Â· OP Tp . We denote the
set of paths in the optimal solution of RSA (off-line) when
network graph is G by POP T . The paths in POP T can be
partitioned into two subsets, Pe1 and Pe2 such that Pe1 is the set
of paths that include edge e and the paths in Pe2 do not include
edge e. Let OP Te1 and OP Te2 be the ICN of the intersection
graph of paths in Pe1 and Pe2 respectively. Then we have (2)
OP T â¥ max(OP Te1 , OP Te2 ). Since all the paths in Pe1 have
intersection in edge e, their intervals do not intersect. Clearly,
(3) OP Tp â¤ OP Te1 + OP Te2 . The reason is that in the worst
case, all requests that were routed through edge e in POP T
are routed the other way in Gp and now they at most need
OP Te1 spectrum span not intersecting the spectrum allocated
to the paths in Pe2 . Therefore, using relations in (2) and (3)
we have OP Tp â¤ 2OP T . Also, based on relation (1) we can
conclude I â¤ min{O(log(dmax )), O(log(k))} Â· OP T .
VI. A H EURISTIC AND R ESULTS FOR G ENERAL G RAPHS
In this section first we present our heuristic for on-line
RSA problem in general graphs. Then we present the results
of our extensive simulation that demonstrate the efficacy of
our heuristic for the on-line RSA problem by comparing it
against (i) the optimal solution and (ii) the solution obtained
by executing the heuristic based on K-shortest path and FirstFit technique.
Minimum Sub-Carrier Path Heuristic (MSCP): The main
idea in our heuristic is that it tries to find disjoint paths for
routing the requests to increase the reuse of sub-carriers in
spectrum allocation. Of this concern, we define a new weight
function on the edges (fibers) of the network where weight
of an edge e â E, w(e) will be largest sub-carrier number
that is used in that edge. We also define the weight of a path,
P from node s to node t to be maxeâP {w(e)}. For each
new request, M SCP selects the path with minimum weight.
The minimum weight path can be computed by modifying the
distance function in Dijkstra algorithm so that it considers
the new weight function as the distance. After finding the
path, M SCP uses First-Fit algorithm to find the first available
spectrum slice with the length of the request demand in all
the edges of the path. Then, M SCP updates the weight of
every edge in the path to the largest sub-carrier so far used
in that edge. For each request, time complexity of minimumweight path computation is O(|V |2 ) and First-Fit algorithm
takes O(k|V |2 ) where k is the number of requests. Hence,
time complexity of M SCP is O(k 2 |V |2 ).

K-Shortest Path Heuristic (KSP): In this heuristic, initially
K shortest paths are computed between every pair of nodes
in the network using [19] algorithm with O(k|V |3 ). When
a request Ri arrives, for every path in the K shortest paths
between si and ti we compute First-Fit algorithm to find the
first available spectrum slice [ai , bi ] with the length of di .
Then we select the path whose first available spectrum slice
[ai , bi ] has the smallest bi . This algorithm takes O(Kk 2 |V |2 )
for satisfying all the k requests.
We perform our experiments on the NSFnet (Fig. 2(a)) and
the fiber network of Level-3 that spans Europe (Fig. 3(a)) [20].
We view the NSFnet and Level-3 networks as examples of a
small and a large network respectively.
In Fig. 2(b), we present the results obtained from ILP ,
M SCP and KSP when executed on the NSFnet. We find the
optimal solution of the RSA problem (off-line) by solving an
ILP using the software package CPLEX. Since computing the
optimal solution by ILP takes considerable amount of time,
we need to do this set of experiments for small number of
requests. In this set of experiments, the number of requests,
k, is varied from 2 to 6 with step of one. For each value of
k, we generate 10 instances. In each instance we generate k
requests randomly and consider them one at a time. For this
set of experiments all the demand values are at most 5, (i.e.,
dmax â¤ 5). The average spectrum span computed by each of
the three methods is shown in Fig. 2(b). It may be observed
that the average spectrum span of M SCP is closest to the
ILP almost in all cases. The ratio of the average spectrum
span of M SCP to ILP is at most 1.28 demonstrating the
closeness of the M SCP to the optimal. The results in these
experiments also show that M SCP works better than KSP
algorithm in almost all the cases even when number of paths
in KSP is K = 3. We repeat similar experiments for larger
value of k, where k = 10 and we change the value of dmax
from 5 to 25. The result of these experiments is depicted in
Fig. 2(c). It can be observed that spectrum span in M SCP is
at least 12% smaller than the span in KSP where K = 1 and
it is even smaller than the one in KSP where K = 2. When
K = 3 in KSP , KSP needs smaller spectrum span than
M SCP but its time complexity is at least 3 times M SCP .
We perform our next set of experiments on the Level-3
network shown in Fig. 3(a). In these experiments, first, we
vary k from 10 to 60 with step of 10. For a specific value of k
we generate 10 instances. In all these instances, the maximum
demand is limited to 10 (i.e., dmax â¤ 10). The average utilized
spectrum span is shown in Fig. 3(b). These results show that
M SCP efficacy with respect to utilized spectrum span is
almost the same as KSP when K = 2. We also conduct
experiments for the case that values of dmax is varied from 5
to 25 with step of 5, while keeping the number of requests
k constant at 20. We compute the average spectrum span
over 10 random instances for each value of dmax . The results
are shown in Fig. 3(c). According to these results, M SCP âs
performance is better than KSP when K = 2 especially for
larger values of dmax . According to the last experiments we
may conclude that M SCP outperforms KSP when K = 2
for larger values of dmax .

50

NJ

IL

UT

CA1

PA

NE
CO
MD

dmax â¤ 5

7
6

KSP (K=1)
KSP (K=2)
KSP (K=3)
MSCP
ILP

5
4

3

Average Spectrum Span

NY

WA

Average Spectrum Span

8
MI

2

CA2

30
KSP (K=1)
KSP (K=2)
KSP (K=3)
MSCP

20
10
0

1

TX

k = 10

40

2

GA

3
4
5
Number of requests (k)

(a)

6

7

0

5

(b)

10
15
20
25
Maximum demand (dmax)

30

(c)

Fig. 2. (a) The 14-node NSF Network, (b) The average spectrum span in NSF Network for different values of k where dmax â¤ 5, (c) different values of
dmax where k = 10
180

160

dmax â¤ 10

160

100
80

KSP (K=1)

60

KSP (K=2)

40

MSCP

20
0

(a)

120
100
KSP (K=1)
KSP (K=2)
MSCP

80
60
40
20

0

Fig. 3.

k = 20

140

120

Average Spectrum Span

Average Spectrum Span

140

20

40
60
Number of requests (k)

(b)

80

0
0

10
20
30
Maximum demand (dmax)

40

(c)

(a) Level-3 network over Europe, (b) The average spectrum span in Level-3 network for dmax â¤ 10, (c) k = 20

VII. C ONCLUSION
In this paper we study on-line version of Routing and
Spectrum Allocation problem in OFDM-based optical networks. We propose an algorithm for the ring network with
a competitive ratio of min{O(log(dmax )), O(log(k))} where
k is the total number of requests and dmax is the maximum
demand. In addition, we provide a heuristic for networks with
arbitrary topology and measure its effectiveness with extensive
simulation. In future, we plan to develop efficient algorithms
for on-line RSA in networks with tree and grid topologies. We
also would like to extend our results to the case that different
modulation models can be used.
Acknowledgement The research was supported by the DTRA
grant HDTRA1-09-1-0032 and the AFOSR grant FA9550-091-0120.
R EFERENCES
[1] W. Shieh, âOfdm for flexible high-speed optical networks,â Journal of
Lightwave Technology, vol. 29, no. 10, pp. 1560 â1577, 2011.
[2] M. Jinno, H. Takara, B. Kozicki, Y. Tsukishima, Y. Sone, and S. Matsuoka, âSpectrum-efficient and scalable elastic optical path network: architecture, benefits, and enabling technologies,â IEEE Communications
Magazine, vol. 47, no. 11, pp. 66 â73, 2009.
[3] Y. Wang, X. Cao, and Y. Pan, âA study of the routing and spectrum allocation in spectrum-sliced elastic optical path networks,â in INFOCOM,
2011, pp. 1503â1511.
[4] M. Jinno, B. Kozicki, H. Takara, A. Watanabe, Y. Sone, T. Tanaka, and
A. Hirano, âDistance-adaptive spectrum resource allocation in spectrumsliced elastic optical path network [topics in optical communications],â
IEEE Communications Magazine, vol. 48, no. 8, pp. 138 â145, 2010.
[5] K. Christodoulopoulos, I. Tomkos, and E. Varvarigos, âDynamic bandwidth allocation in flexible ofdm-based networks,â in OFC/NFOEC,
2011.
[6] X. Wan, N. Hua, and X. Zheng, âDynamic routing and spectrum assignment in spectrum-flexible transparent optical networks,â IEEE/OSA
Journal of Optical Communications and Networking, vol. 4, no. 8, pp.
603 â613, 2012.

[7] T. Takagi, H. Hasegawa, K. Sato, Y. Sone, B. Kozicki, A. Hirano,
and M. Jinno, âDynamic routing and frequency slot assignment for
elastic optical path networks that adopt distance adaptive modulation,â
in OFC/NFOEC, 2011.
[8] A. Castro, L. Velasco, M. Ruiz, M. Klinkowski, J. P. FernaÌndez-Palacios,
and D. Careglio, âDynamic routing and spectrum (re)allocation in future
flexgrid optical networks,â Compututer Networks, vol. 56, no. 12, pp.
2869â2883, 2012.
[9] G. Shen and Q. Yang, âFrom coarse grid to mini-grid to gridless: How
much can gridless help contentionless?â in OFC/NFOEC, 2011.
[10] M. G. Luby, âTight bounds for dynamic storage allocation,â SIAM
Journal on Discrete Mathematics, vol. 9, no. 1, pp. 155â166, Feb. 1996.
[11] A. N. Patel, P. N. Ji, J. P. Jue, and T. Wang, âRouting, wavelength
assignment, and spectrum allocation in transparent flexible optical wdm
(fwdm) networks,â in Photonics in Switching, 2010.
[12] K. Christodoulopoulos, I. Tomkos, and E. A. Varvarigos, âRouting
and spectrum allocation in ofdm-based optical networks with elastic
bandwidth allocation,â in GLOBECOM, 2010, pp. 1â6.
[13] M. Klinkowski and K. Walkowiak, âRouting and spectrum assignment
inspectrum sliced elastic optical path network,â IEEE Communications
Letters, vol. 15, no. 8, pp. 884â886, 2011.
[14] K. Christodoulopoulos, I. Tomkos, and E. Varvarigos, âElastic bandwidth allocation in flexible ofdm-based optical networks,â Journal of
Lightwave Technology, vol. 29, no. 9, pp. 1354 â1366, 2011.
[15] H. A. Kierstead, âA polynomial time approximation algorithm for
dynamic storage allocation,â Discrete Mathematics, vol. 87, no. 2-3,
pp. 231â237, 1991.
[16] S. Shirazipourazad, C. Zhou, Z. Derakhshandeh, and A. Sen. On
routing and spectrum allocation in spectrum-sliced optical networks.
[Online]. Available: http://www.public.asu.edu/â¼sshiraz1/RSA.pdf
[17] M. R. Garey and D. S. Johnson, Computers and Intractability; A Guide
to the Theory of NP-Completeness. W. H. Freeman & Co., 1990.
[18] T. Erlebach and K. Jansen, âThe complexity of path coloring and call
scheduling,â Theoretical Computer Science, vol. 255, no. 1-2, pp. 33â50,
2001.
[19] J. Y. Yen, âFinding the k shortest loopless paths in a network,â
Management Science, vol. 17, no. 11, pp. 712â716, 1971.
[20] Level 3 Communications, Network Map. [Online]. Available: http:
//www.level3.com/en/resource-library/maps/level-3-network-map/

A Network Planning and Management Tool for
Mitigating the Impact of Spatially Correlated Failures in
Infrastructure Networks
Arun Das, Arunabha Sen, Chunming Qiao, Nasir Ghani, Nathalie Mitton

To cite this version:
Arun Das, Arunabha Sen, Chunming Qiao, Nasir Ghani, Nathalie Mitton. A Network Planning
and Management Tool for Mitigating the Impact of Spatially Correlated Failures in Infrastructure Networks. International Conference on Design of Reliable Communication Networks
(DRCN), Mar 2016, Paris, France. <hal-01254982>

HAL Id: hal-01254982
https://hal.inria.fr/hal-01254982
Submitted on 21 Mar 2016

HAL is a multi-disciplinary open access
archive for the deposit and dissemination of scientific research documents, whether they are published or not. The documents may come from
teaching and research institutions in France or
abroad, or from public or private research centers.

Lâarchive ouverte pluridisciplinaire HAL, est
destineÌe au deÌpoÌt et aÌ la diffusion de documents
scientifiques de niveau recherche, publieÌs ou non,
eÌmanant des eÌtablissements dâenseignement et de
recherche francÌ§ais ou eÌtrangers, des laboratoires
publics ou priveÌs.

A Network Planning and Management Tool for
Mitigating the Impact of Spatially Correlated
Failures in Infrastructure Networks
Arun Dasâ , Arunabha Senâ , Chunming Qiaoâ  , Nasir Ghaniâ¡ , Nathalie MittonÂ§

â School of Computing, Informatics and Decision System Engineering, Arizona State University, Tempe, Arizona 85287, USA
â  Department of Computer Science and Engineering, SUNY at Buffalo, Buffalo, NY 14201, USA
â¡ Department of Electrical Engineering, University of South Florida, Tampa, FL 33620, USA
Â§ Inria, 40 Avenue Halley, 59650 Villeneuve DâASCQ, France

Email: arun.das@asu.edu, asen@asu.edu, qiao@computer.org, nghani@usf.edu, nathalie.mitton@inria.fr
AbstractâCurrent practices of fault-tolerant network design
ignore the fact that most network infrastructure faults are
localized or spatially correlated (i.e., confined to regions). Network
operators require new tools to mitigate the impact of such
region based faults on their infrastructures. Utilizing the support
from the U.S. Department of Defense, and by consolidating a
wide range of theories and solutions developed in the last few
years, the authors of this paper have developed an advanced
Network Planning and Management Tool (NPMT) that facilitates
the design and provisioning of robust and resilient networks.
The tool provides multi-faceted network design, evaluation and
simulation capabilities for network planners. Future extensions
of the tool currently being worked upon not only expand the tools
capabilities, but also extend these capabilities to heterogeneous
interdependent networks such as communication, power, water
and satellite networks.

I.

I NTRODUCTION AND M OTIVATION

It is extremely important that planners for large wide area
networks have the right tools to design robust and resilient
networks that can effectively withstand large scale geographically correlated failures in their networks. Such failures can
be triggered by nature (hurricane or earthquake) or humans
(nuclear attack or conventional weapon attack over a large
geographical area). With research support from the U.S. Defense Threat Reduction Agency, an agency whose mission is to
protect the U.S. against Weapons of Mass Destruction (WMD),
such as nuclear, biological or chemical attacks, the authors
of this paper, over the last six years have developed a wide
ranging set of concepts and techniques for enhancing network
robustness against spatially correlated or region based faults.
We have recently incorporated these concepts and techniques
into a Network Planning and Management Tool (NPMT) [1]
for the benefit of network designers, planners and operators.
In this paper, we first describe the novel concepts developed
to design networks that are robust against region based faults,
and then describe how these concepts have been incorporated
into the NPMT. The goal of this paper is to bring to the
attention of the networking research community, and the
audience of the workshop on DRCN in particular, the existence
of NPMT as a tool that consolidates a large body of work on
spatially correlated failures, and as a tool that can be used by
This work was supported in part by the NSF grant 1441214, and by grant
HDTRA1-09-1-0032 from the U.S. Defense Threat Reduction Agency

the community to meet the needs for robust network design
against spatially correlated failures. In essence, this paperâs
contribution should not be measured in terms of new analytical
findings, but in terms of service to the networking community.
We use the term WMD attack to imply a large scale
geographically correlated failure such as failures caused by an
earthquake, hurricane or nuclear attack. The characteristic of a
WMD attack is massive but localized faults. The connectivity
of a network [2] is generally accepted as a metric for evaluating
the fault-tolerance capability of a network [3]. If a networkâs
connectivity is k + 1, then the network can tolerate up to k
faults, implying that the surviving network remains connected
even after k failures. The connectivity metric, however, has no
way of capturing locality, i.e., the faulty nodes/edges may be
close or far away from each other. Thus, the connectivity metric cannot distinguish between faults that are geographically
correlated (a WMD fault characteristic), and faults that are not.
Connectivity as a metric also fails to capture other important
structural properties of the network such as the number or
size of the connected components [2] into which a network
disintegrates when the number of failed nodes/edges exceeds
the node/edge connectivity of the network.
Recognizing the limitations of connectivity as a metric
for capturing the special characteristics of geographically correlated failures, the authors of [4] introduced the notion of
region-based connectivity. A region may be defined either with
reference to the network graph or to the network geometry (i.e.,
layout of the network in a two or three dimensional space). For
example, a region may be defined as a subgraph with diameter
d (where the diameter of a graph is defined as the maximum
of the shortest path distance between a pair of nodes, taken
over all source-destination node pairs). Or, a region may also
be defined as a collection of nodes and edges in the network
graph layout that is covered by a circular area in that layout.
Figure 1(a) shows an example of a circular region-based fault.
The NPMT described in this paper is intended to support
design and analysis of single layered and multi-layered interdependent heterogeneous networks. In essence, the NPMT
is particularly suitable for planning and design of critical
infrastructures. For example, from the single network layer
perspective, the NPMT enables backbone communication network providers, such as, AT&T, Sprint, Qwest and Level 3
Communications, to (i) identify the most vulnerable parts of

(a)

(b)

(c)

Fig. 1: (a) Network with circular fault region, (b) Optical fiber network of a major U.S. provider, (c) Optical Fiber network of a major European
provider disrupted by a WMD attack

their network against a WMD attack, and (ii) reinforce the
network with least cost to eliminate or significantly reduce
the threat of network disruption due to a WMD attack. Figure
1(b) shows the backbone network of a major U.S. provider
and Figure 1(c) shows how the backbone network of a major
European provider can potentially be disrupted by a WMD
attack. From a multi-layer perspective the NPMT can be used
for design and analysis of smart cities, where heterogeneous
networks ranging from disparate telecom networks (such as
2G, 3G, WiFi, Bluetooth, etc.) to water, electricity and gas
distribution networks, form a complex interdependent ecosystem. Subsequently, failures in one network, for example a leak
in the water distribution network, may deteriorate other nearby
(spatially correlated) infrastructures such as gas or electricity
whose pipes and cables may get affected due to the leak. In this
context, a tool like NPMT can be an asset for utility companies
and smart city planners to quickly perform (i) root cause
analysis of failure, and (ii) forecast fault evolution, to direct
repairs and maintenance towards specific network components
and restrict fault propagation. To the best of our knowledge no
such tool is available today that supports features for planning
and designing of single layer and multi-layer interdependent
networks in the presence of spatially correlated faults.
Several studies in the network research community have
focused on different aspects of spatially correlated or regionbased faults in networks [5-11], however, to the best of our
knowledge there does not exist an executable platform that
consolidates the findings and techniques of these studies into
a readily usable tool. The NPMT is intended to fill that gap
and be such a platform that can incorporate the outcomes
developed in [5-11] into executable modules to be integrated
into the NPMT. This will allow network designers, planners
and operators to use the results of these studies in their real
world operational networks.
The rest of the paper is organized as follows: In Section
II we present an overview of the underlying concepts and
theoretical results that the NPMT operates on. In Section III
we outline the capabilities of the NPMT and finally Section
IV concludes this paper.
II.

C ONCEPTS , M ETRICS AND S OLUTION T ECHNIQUES

In this section we give a brief overview of the underlying
concepts, metrics and solution techniques that the Network

Planning and Management Tool (NPMT) utilizes to carry out
its functional operations. The NPMT is built as a modular
execution engine that can execute smaller reusable modules
to perform desired operations on a network topology. In this
respect, the current version of the NPMT comprises of different
modules that deal with both static and dynamic aspects of
robust and resilient network design. The modular approach
allows design, development and testing of these modules to
be done independently and defers the integration into the
NPMT until a module meets itâs functional requirements. In
the following sub-sections we give a brief overview of the
analytical foundations of these modules. It may be noted that,
as of writing this paper not all modules have been implemented
and integrated into the NPMT. Accordingly, we highlight our
ongoing work in the discussion below.
A. Region-Based Fault Metrics Computation Module
As outlined in Section I, connectivity as a metric fails to
capture several characteristics of the network in presence of
spatially correlated failures. For instance, the number or size of
the connected components into which a network disintegrates
in the presence of a spatially correlated fault is not captured by
the traditional connectivity metric. In order to overcome these
gaps and capture such network state characteristics, several
metrics and their computation techniques have been proposed
by the research community. For a given network topology, the
NPMT can analyze the network and compute metrics pertinent
to network state in the presence of spatially correlated faults.
The following metrics are supported by the NPMT:
Region-based Connectivity Metric Computation
Region based connectivity can be considered under two fault
models â (i) Single Region Fault Model (sRFM) where faults
are confined to a single region [4], and (ii) Multiple Region
Fault Model (mRFM) where faults are confined to k regions
for some specified k [12].
Formally, in sRFM, the single-region-based (node) connectivity of graph G with a specified definition of region R,
sÎºR (G), is defined as follows: Suppose that {R1 , . . . , Rk } is
the set of all possible regions of the graph G. Consider a
k-dimensional vector T whose i-th entry, T [i], indicates the
number of nodes in region Ri whose failure will disconnect
the graph G. If the graph G remains connected even after the

failure of all nodes of the region Ri then T [i] is set equal to
â. The region-based connectivity of a graph G with region
R, is then computed as follows:
sÎºR (G) = min T [i]
1â¤iâ¤k

In mRFM, the multi-region-based (node) connectivity of graph
G with a specified definition of region R, mÎºR (G), is defined
as the minimum number of regions whose removal (i.e.,
removal of all nodes in the regions and edges incident on them)
will disconnect the graph.
Polynomial time algorithms to compute region-based connectivity in sRFM was presented in [4]. The NPMT contains an
implementation of this algorithm that can be used to compute
the Region-based Connectivity for a given network topology.
Region-based
Component
Decomposition
(RBCDN) Metric Computation

Number

Proposed by the authors of [13], the Region-Based Component Decomposition Number, or RBCDN of graph G = (V, E)
with a specified definition of region R is defined the following
way: Suppose that {R1 , . . . , Rk } is the set of all possible
regions of the graph G. Consider a k-dimensional vector C
whose i-th entry, C[i], indicates the number of connected
components in which G decomposes when all entities in Ri
fails. RBCDN of a graph G with region R is defined as follows:
Î´R (G) = max C[i]
1â¤iâ¤k

RBCDN as a metric provides a insight into the worst case
scenario on how fragmented a network can become in the
presence of a spatially correlated fault. In [13] the authors
propose techniques to compute the RBCDN and the NPMT
provides an implementation of this algorithm that can be used
on user selected network topologies.
Region-based Smallest/Largest Component Size Metric
Computation
The Region-Based Smallest (Largest) Component Size, or
RBSCS/RBLCS was proposed in [14], and is defined for
a graph G = (V, E) with a specified definition of region
R, as follows: Suppose that {R1 , . . . , Rk } is the set of all
possible regions of the graph G. Consider a k-dimensional
vector CS (CL ) whose i-th entry, CS [i] (CL [i]), indicates the
size of the smallest (largest) connected component in which G
decomposes when all nodes in Ri fails. The RBSCS Î±R (G)
and RBLCS Î²R (G) of graph G with region R is defined as:
Î±R (G) = min CS [i] and Î²R (G) = min CL [i]
1â¤iâ¤k

1â¤iâ¤k

The RBLCS and RBSCS metrics provide insights on how well
a networkâs performance degrades in the presence of regionbased faults. Depending on the needs of graceful performance
degradation, networks designers may choose to design networks that have a small value of RBCDN (Î´R (G)) and a high
value of either RBLCS (Î±R (G)) or RBSCS (Î²R (G)). The
NPMT allows the user to compute the RBLCS and RBSCS
metrics for a chosen network topology.

is a need for techniques to compute the set of regions, given
a network and some fault specification. In [14], given a graph
Gâs layout on a two-dimensional plane and a fault radius r,
the authors provide a polynomial time algorithm to compute all
distinguishable or distinct circular regions with radius r. Two
fault regions are considered indistinguishable if they contain
the same set of links and nodes. The authors considered both
wired networks, where nodes and edges can be part of a failure
region, and wireless networks, where only nodes can be part
of a failure region. It was shown in [14] that the number of
distinct regions in wireless and wired networks are O(n2 )
and O(n4 ) respectively, and that all distinct regions can be
computed in O(n6 ) time, where n is the number of nodes.
The NPMT is bundled with an implementation of the
technique outlined in [14]. Given a network topology and a
fault radius, the NPMT can compute all distinct regions of
the network which can then be used by other modules of
the NPMT, such as the Metric Computation Module and the
Region-disjoint Path Computation Module (discussed next).
C. Region-disjoint Paths Computation Module
For a graph G = (V, E), a set of region-disjoint paths
P between a source node s and destination node d with a
specified definition of region R, is defined as follows: Suppose
that {R1 , . . . , Rk } is the set of all possible regions of graph
G and path Pu â P contains a set of nodes and edges from
G such that Pu forms a path from s to d, {s, d} â V . Then,
for every pair of paths {Pu , Pv } â P, u 6= v, Pu and Pv are
region-disjoint, i.e. there is no region in R that both the paths
traverse. Formally, region-disjoint paths are defined as follows,
for all i = 1, . . . , k:
|(Pu â© Ri ) â© (Pv â© Ri )| = 0, â{Pu , Pv } â P, u 6= v
Although region-disjoint path computation has been addressed
in [8], the authors consider a model where faults do not cause
edges to fail unless a failed edge is associated with a failed
node. In this model an edge cannot fail on itâs own and can
only fail when one of the nodes incident on the edge fails. This
assumption is considerably restrictive and possibly unusable
for designers of larger networks where spatially correlated
faults can affect nodes and edges independently. In order to
overcome this limitation the NPMT supports computation of
region-disjoint paths in the presence of circular faults using an
Integer Linear Program (ILP) that doesnât presuppose any such
restrictions. The NPMT is capable of computing two regiondisjoint paths from given source and destination nodes such
that the sum of lengths of the two paths is minimum. Also, as
the source (destination) node is part of a region that is traversed
by both paths (as both paths have the same staring and ending
points), no region disjoint path may exist. To accommodate this
situation the NPMT accommodates the use of no-fault zones
â a circular area around the source and destination nodes that
is immune to faults. Future extensions of this module include
computing more than two paths, and including other selection
criteria such as minimizing the maximum path length.

B. Distinct Regions Computation Module

D. Region-based Fault Tolerant Distributed File Storage Module

It may be noted that all the previously defined metrics
operate on a given graph and a set of regions. Thus, there

In the preceding discussions the importance of a node
in keeping the network connected is emphasized, however,

individual nodes can also act as data stores of the network
and the removal of a node from a network (due to a regionbased fault), may not only cause connectivity losses, but also
data losses. To address such data loss risks, distributed storage
techniques are often employed that enhances data survivability
in the presence of faults. One such technique is redundancy,
such as by (i) storing multiple copies of the entire file, or
(ii) storing different fragments of the same file at different
nodes in the network. In the popular (N, K), N â¥ K file
distribution scheme, from a file F of size |F |, N segments
of size |F |/K are created in such a way that it is possible to
reconstruct the entire file by accessing any K segments. For
such a reconstruction scheme to work, it is essential that the
K segments of the file are stored in nodes that are connected
to each other in the network. However, in the event of failures,
the network may become disconnected (i.e., split into several
connected components) and K segments may not be accessible
in the residual network to reconstruct the file F .
From the context of data survivability in the presence of
spatially correlated faults in networks, the NPMT supports
a âRegion-based Distributed File Storage Moduleâ that implements an algorithm proposed in [11] that ensures that:
(i) even when the network is fractured into disconnected
components due to a region-based fault, at least one of the
largest components will have access to at least K distinct file
segments with which to reconstruct the entire file, and (ii)
the total storage requirement is minimized. As of writing this
paper, this module is currently under development and will be
part of the NPMT upon its completion.
E. Robust Multi-layer Interdependent Network Design Module
In todayâs world, a multitude of heterogeneous interconnected networks form a symbiotic ecosystem that supports all
of the economic, political and social aspects of human life.
For example, the critical infrastructures of the nation such
as the power grid and the communication network are highly
interdependent on each other, and any adverse effects on one
network can affect the other network. Thus, isolated network
analysis is no longer sufficient to design and operate such
interconnected and interdependent network systems.
Recognizing this need for a deeper understanding of the
interdependency in such multi-layered network systems, significant efforts have been made by the research community in the
last few years, and accordingly, a number of analytical models
have been proposed to analyze such interdependencies [15-17].
However, most of these models are simplistic and fail to
capture the complex interdependencies that may exist between
entities of the power grid and communication networks. To
overcome the limitations of existing models, the authors of
[18] have proposed an Implicative Interdependency Model that
is able to capture such complex interdependency. Utilizing
this model, several problems on multi-layer interdependent
networks have been studied, such as (i) identification of the
K most vulnerable nodes [18], (ii) root cause analysis of
failures [19], (iii) the entity hardening problem [20], (iv) the
smallest pseudo-target set identification problem [21], and (v)
the robustness analysis problem [22].
This module will support multi-layer network interdependency modeling using the Implicative Interdependency Model,

and analysis of multi-layer networks using the techniques proposed in [18-22]. The module is currently under development
and will be part of the NPMT upon its completion.
F. Module for Progressive Recovery from Region-based Failures
With this module, the NPMT addresses post-fault recovery
techniques in the aftermath of region-based faults on multilayer interdependent networks. To restore an interdependent
network system from a post-fault scenario to its pre-failure
state, all the faulty network entities (nodes/edges) have to be
repaired or replaced. However, resource limitations may prevent simultaneous restoration of all failed units of the network.
Accordingly, the failed units have to be restored in a sequenced
manner. As each network entity in its operational state adds
some utility value to the interdependent network system, when
a unit recovers from a failed state to an operational state,
the unit starts providing some âbenefitâ to the system. Since
different units have different utility values to the system, the
sequence in which the failed units are restored is important as
the recovery sequence determines the cumulative system utility
during the recovery process.
As discussed in Section II-E, the Implicative Interdependency Model provides a powerful technique for modeling
dependencies in multi-layer interdependent networks. Using
this model the authors of [23] studied the progressive recovery
problem in interdependent networks with the objective of
maximizing system utility during the system recovery process.
This module implements the progressive recovery algorithm of
[23], and can be used to sequence recovery of network entities
from a post-fault to a pre-fault network state that maximizes
system utility during the recovery process. The module is
currently under development and will be part of the NPMT
upon its completion.
III.

A RCHITECTURE AND S YSTEM C APABILITIES

In this section we first outline the system architecture, and
then discuss the different capabilities of the NPMT.
A. System Architecture
View

Service

Fault Analyzer

Path Analyzer

Topology Manager

Profile Manager

Traffic and Fault Simulator

Core Modules:
Network Topology Manager
Region-Based Fault Analysis

Controller

Disjoint Path Analysis

Visualization Engine

Simulation Engine

Execution Engine

Common Modules:

Repository

N/W Fault Impact Analyzer

Path Planning Algorithms

Model
Simulation Data

User/Roles

Path Archive

Fault Generation Engine

N/W Topologies

Fault Archive

Library Faults

Request Generation Engine

Fig. 2: The NPMT High-Level Architecture

The NPMT is implemented as a web-application that
allows the user to remotely connect and operate the tool from
a browser. The web-application follows the standard three-tier
architecture and has a client tier, application tier, and database

(a)
(b)
Fig. 3: (a) Topology Manager â create, edit and manage network topologies, (b) Fault Analyzer â generic fault analysis, metric computations

tier. The tool has been developed following the Model-ViewController (MVC) design pattern. Figure 2 outlines the high
level architecture and some of the components of the tool.

map tiles are rendered from OpenStreetMap [24]. The NPMT
uses the OpenLayers API to support an user interactive map
interface.

The tool is currently accessible from Arizona State Universityâs WAN, and runs from our testbed server. The toolâs webapplication is deployed on an Apache Tomcat 7 instance, and
the repository used is MySQL. The application tier business
logic for operations on network topologies, such as RegionBased Fault Analysis and Region Disjoint Path Analysis, are
implemented in Java. Additional packages and libraries, such
as IBM ILOG CPLEX Optimization Studio libraries (required
for solving Integer Linear Programs), are setup and made
available on the testbed server. Our testbed server is a 64bit Intel Core 2 Quad Core (2.66 GHz) system with 8 GB of
RAM running an Ubuntu 14.04 instance.

To create the topology and place nodes and edges on the
map, the user can either point-and-click on the map itself,
or can type in specific latitude and longitude coordinates and
then proceed to add the network entity. Capacities for each
edge (in Gigabits per second), can also be specified during the
edge creation process. Once a network topology is created, the
topology must be saved to be used for Network Analysis and
Network Simulation. The topologies are saved on the NPMT
server and can be loaded back into the Topology Manager to
edit any entity or attribute of the network.

B. System Capabilities
The NPMT is designed to be used by following a three
step workflow comprising of (i) Network Creation, (ii) Network Analysis, and (iii) Network Simulation. Accordingly, the
individual features and the executable modules of the NPMT
are bundled around these three workflow steps. The following
list enumerates the current high-level features of the tool and
the corresponding workflow step that each feature emulates:
1)
2)
3)
4)

Topology Management (Network Creation)
Fault Analysis (Network Analysis)
Path Analysis (Network Analysis)
Traffic and Fault Impact Simulation (Network Simulation)

Each of the above features are accessible from a tabbed
interface of the tool and can be navigated to from any part of
the web-application. In the following subsections we discuss
each of the features and provide a brief functional overview.
Topology Management
Network Creation is the first step of the NPMT workflow and
the Topology Manager interface allows the user to create, edit,
save and delete network topologies. The Topology Manager
presents the user with a geographical map interface that she
can interact with to manage network topologies. The displayed

Figure 3(a) shows a screen grab of the Topology Manager.
As seen in the figure, the map interface is on the right and the
user interact-able menu is on the left. The user can click on the
map to to add nodes and edges, or can alternatively type in the
latitude and longitude coordinates in the input fields available
on the menu. The menu also lists the nodes and edges that are
part of the topology. Selecting an edge or node from these lists
highlights the network entity on the map (in yellow), and the
user can then proceed to remove the entity from the network if
necessary. The displayed map overlays can be toggled from a
dropdown menu available on the map (in blue in Figure 3(a)).
Finally, as seen in Figure 3(a), options for saving, loading, and
deleting topologies are available to the user directly below the
displayed mapâs dimensions.
Fault Analysis
Once network topologies are created from the Topology Manager, the Fault Analyzer can be used to analyze the created
networks for their resilience in the presence of spatially correlated faults. In the context of the NPMT, network resiliency
is measured by how well the network performs when benchmarked against the metrics outlined in Section II-A. It may
be noted that the metrics of Section II-A emphasize resilience
from the aspect of connectivity in the presence of a spatially
correlated fault. For example, the more number of disconnected
components a network has due to a fault, the worse is the
networkâs resilience (as captured by the metric RBCDN). It

(a)
(b)
Fig. 4: Fault Analyzer - Specified Fault Analysis. (a) User specified fault coordinates, (b) Fault impact of the user defined fault and an imported
library fault (coordinates for the state of California, USA)

may be noted that, for the purpose of this analysis the NPMT
assumes that any network entity (nodes/edges), that fall within
the fault area are all rendered inoperable, i.e. the fault model
is deterministic and if a network entity falls within the fault
region, it necessarily fails. To carry out this analysis, the user
first selects a network topology and can then choose to either
perform a generic fault analysis, or a specified fault analysis.
These analyses are described below.
Generic Fault Analysis: In the generic fault analysis, for a
selected network topology, the user specifies a fault feature and
the tool computes the values of the individual metrics listed
in Section II-A. The NPMT can generically analyzes circular
faults, and the supported fault feature is the fault radius r.
As shown in Figure 3(b), the user can specify the fault
radius r from the left menu. The tool then performs the
generic fault analysis by (i) computing all the distinct regions with radius r using the techniques implemented in
the module âDistinct Regions Computation Moduleâ (Section
II-B), and (ii) computes the individual metrics using the
techniques implemented in the module âRegion-Based Fault
Metrics Computation Moduleâ (Section II-A). The results
are subsequently reported back to the user. For the network
selected in Figure 3(b) and radius r = 500 km., the computed
Region-based Component Decomposition Number (RBCDN)
is 2, the Region-based Largest Component Size (RBLCS) is 9
and the Region-based Smallest Component Size (RBSCS) is
1. Finally, the number of distinct regions computed is 112.
As shown in Figure 3(b), the user is also presented with
sample worst case fault scenarios where a distinct fault causes
the network to fragment into the same number of components
as the RBCDN. Selecting one of the listed faults updates
the displayed network with the faultâs impact. In Figure 3(b)
the fault centered at 36.249â¦ N , â85.696â¦ E is selected. The
nodes and edges rendered inoperable by the fault are grayed
out, while the surviving nodes and edges are shown in green
and black respectively. The connected components in the
fragmented network are highlighted by a light-green region. In
this example, the loss of the two grayed out nodes causes the
network to fragment into two disconnected components: one

with 9 components, and the other with 1 component. Options
for saving the analysis results are available from the menu.
Specified Fault Analysis: In the specified fault analysis, the
user can provide the exact coordinates of one or more faults
and visualize the impact of these faults on the selected network.
The user has the option to save and load faults to visualize the
impact of a fault on different networks. The NPMT also comes
bundled with a set of library faults that the user can choose
from to simulate fault impact on a network. The current set
of library faults consist of the coordinates of the 50 states
of the USA. The inclusion of a fault library in the NPMT is
to provide the user with pre-defined fault scenarios based on
known fault patterns, faults centered at a target of interest, or
recorded faults. For example, fault impact zones of Level 4
hurricanes such as hurricane Katrina or hurricane Sandy.
As shown in Figure 4(a), to specify the exact coordinates of
the fault region the user can either type in the exact coordinates
of the fault region coordinates, or can click on the map to add
such coordinates. The user also has the option for importing
library faults. Once all the fault regions are defined, the NPMT
can simulate the impact of the fault on the selected network.
In Figure 4(b), apart from the user specified fault region,
the boundary of the state of California has been imported
from the fault library and the selected network has been
analyzed for these two fault regions. The updated map shows
the impacted nodes and edges in gray, while the operable
nodes and edges are shown in green and black respectively.
The connected components are shown with a green region. As
seen in Figure 4(b) the menu displays impact statistics such
as, the number of surviving nodes/edges and the number of
connected components. The user is provided with the option
to save the analysis results for later reference, and also the
option to save the defined fault regions for later use.
Path Analyzer
The Path Analyzer allows the user to analyze the network by
computing paths between source and destination nodes that
provide protection against spatially correlated faults. As in the
Fault Analyzer, the Path Analyzer allows the user to specify
a fault feature, and the tool then proceeds to compute paths

(a)
(b)
Fig. 5: Path Analyzer - Region disjoint paths between a source and destination nodes for given fault radius (r) and no-fault zone radius (nfr )
(a) r = 100 km., nfr = 300 km. (b) r = 120 km., nfr = 300 km.

between a given source and destination node pair such that (i)
at least one of the paths survive in the presence of one or more
spatially correlated faults, and (ii) satisfy some other network
resource constraint.
In the current version of the tool the faults considered
are circular faults and the supported fault feature that can
be specified by the user is the fault radius r. The number
of spatially correlated faults considered for path analysis is
one, and the number of paths computed is two, i.e. the NPMT
computes two paths such that if a single circular fault with
radius r occurs anywhere in the network, at least one of the two
paths computed will not be affected by the fault. The network
resource constraint supported is that the sum of lengths of the
two paths computed must be minimum.
It may be noted that a single fault can also render inoperable either the source node, or the destination node, or both,
and thus there always exists a fault region that affects all paths
computed and no region disjoint paths can exist such that at
least one path remains immune to the fault. To accommodate
this case when the source and/or destination nodes themselves
are part of the fault region, the NPMT supports a âNo-Fault
Zoneâ parameter. The user can specify a no-fault zone radius
nfr for both the source and destination nodes that reserves
two circular areas with radius nfr centered at the source and
destination nodes such that network entities, or parts of a
network entity (such as an edge segment), that falls within
this no-fault zone are immune to faults.
Figures 5(a) and 5(b) show screen grabs of the path
analyzer computation for different input values of fault radius
(r). The no-fault zone set to a radius of nfr = 300 km. and
is shown as a white circular region centered at the source and
destination nodes. The computed paths are shown in orange
and blue, and the lengths of each of these to paths are reported
in the left menu. The effect of the path selection criteria, i.e.
the sum of the lengths of the two paths must be minimum,
is also visible in Figures 5(a) and 5(b). In Figure 5(a) when
r = 100 km., the sum of lengths of the two paths is 5793.24
km., however in Figure 5(b) increasing r to 120 km. the
previously computed paths are no longer feasible as a region
fault exists that can impact both these paths. Hence, new paths
are computed and the sum of the new lengths is 5921.69 km.

Traffic and Fault Impact Simulation
For a selected network, the Traffic and Impact Simulator allows
users to generate traffic and faults to analyze the impact of
faults on a load bearing network. To perform this analysis,
a simulation schedule consisting of bandwidth requests and
faults is generated by the NPMT using user provided simulation parameters. Parameters such as total number of time steps
in the schedule, total number of requests in the schedule, minimum/maximum request bandwidth and minimum/maximum
request hold times can be specified by the user. The source and
destination nodes for each request can be generated randomly,
or can be user specified. For introducing faults in the schedule,
the user can specify the number of faults to introduce and can
either specify the exact fault coordinates, or introduce random
circular faults from the set of all possible distinct circular faults
for a specified fault radius. Time intervals of the faults can be
user specified, or can be randomly generated by the NPMT.
Using the request and fault settings, the NPMT then generates
a time stepped simulation schedule of requests and faults. Once
the schedule is finalized, the user can specify the algorithm to
be used in the simulation to route requests from source and
destination nodes, and then proceed to run the simulation.
As shown in the screen grabs of Figures 6(a) and 6(b), the
left menu of the Traffic and Impact Simulator contains the fault
and simulation parameter fields that can be used to generate
the schedule and run the simulation. The tables below the
mapâs dimensions allow the user fine grained control over the
requests and faults that will be simulated. Once the simulation
is complete, for each time interval the network state can be
visualized from the âEvent Simulation Resultsâ table. The user
can click on a row of this table to visualize the network state
on the map for that specific time interval. The user can also
âplayâ the simulation results and the NPMT will iterate over
all the time steps and update the map with the network state
at each step. In Figures 6(a) and 6(b) the impact of a fault
and the corresponding response of the network is shown. In
Figure 6(a) the network is fault free, but in Figure 6(b) a fault
is introduced and an edge is rendered inoperable. It can be
seen that the red and yellow flows of Figure 6(a) are impacted
by the fault, however, as bandwidth is available, in Figure 6(b)
the flows are rerouted in response to this fault.

(a)
(b)
Fig. 6: Traffic and Fault Impact Simulator (a) Pre-Fault network state, (b) Post-Fault network state â rerouted red and yellow flows)

IV.

C ONCLUSION

In this paper we presented a summary of the work done
towards developing a Network Planning and Management Tool
(NPMT), intended to support design and analysis of single
layer and multi-layer networks in the presence of spatially
correlated faults. We highlighted that the NPMT is particularly
suitable for planning and design of critical infrastructures.
We described the underlying novel concepts that have been
developed to enhance robustness of networks in presence of
region based faults, and then described how those concepts
have been incorporated into the NPMT. The goal of this
paper was to bring to the attention of the networking research
community, and to the audience of the workshop on DRCN
in particular, about the existence of NPMT as a tool that
consolidates a large body of work on spatially correlated faults.
To the best of our knowledge no such tool is available today
that supports planning and designing of single layer and multilayer networks in the presence of spatially correlated faults.
R EFERENCES
[1] NEXT Lab, Arizona State University. The Network Planning and
Management Tool. [Online]. Available: http://netsci.asu.edu/networktool/
[2] R. Diestel, Graph Theory. Springer, 2005.
[3] E. Ganesan and D. K. Pradhan, âThe Hyper-deBruijn Networks: Scalable
Versatile Architecture,â IEEE Transactions on Parrallel and Distributed
Systems, vol. 4, no. 9, September 1993.
[4] A. Sen, B. H. Shen, L. Zhou, and B. Hao, âFault-tolerance in Sensor
Networks: A New Evaluation Metric,â in Proceedings of IEEE Infocom,
Barcelona, Spain, April 2006, pp. 1â12.
[5] S. Neumayer and E. Modiano, âNetwork reliability with geographically
correlated failures,â in INFOCOM, 2010 Proceedings IEEE. IEEE,
2010, pp. 1â9.
[6] Y. Cheng, M. T. Gardner, J. Li, R. May, D. Medhi, and J. P. Sterbenz,
âOptimised heuristics for a geodiverse routing protocol,â in 10th International Conference on the Design of Reliable Communication Networks
(DRCN), 2014, 2014, pp. 1â9.
[7] P. Agarwal, A. Efrat, S. Ganjugunte, D. Hay, S. Sankararaman, and
G. Zussman, âThe resilience of wdm networks to probabilistic geographical failures,â in Proceedings of IEEE INFOCOM, 2011.
[8] S. Trajanovski, F. Kuipers, A. Ilic, J. Crowcroft, and P. Van Mieghem,
âFinding critical regions and region-disjoint paths in a network,â
IEEE/ACM Transactions on Networking, vol. 23, no. 3, pp. 908â921,
2015.
[9] S. Banerjee, A. Das, A. Mazumder, Z. Derakhshandeh, and A. Sen, âOn
the impact of coding parameters on storage requirement of region-based
fault tolerant distributed file system design,â in Computing, Networking
and Communications (ICNC), International Conference on. IEEE, 2014,
pp. 78â82.

[10] A. Mazumder, A. Das, C. Zhou, and A. Sen, âRegion based fault-tolerant
distributed file storage system design under budget constraint,â in Reliable Networks Design and Modeling (RNDM), 2014 6th International
Workshop on. IEEE, 2014, pp. 61â68.
[11] A. Sen, A. Mazumder, S. Banerjee, A. Das, C. Zhou, and S. Shirazipourazad, âRegion-based fault-tolerant distributed file storage system
design in networks,â Networks, Wiley Online Library, 2015.
[12] A. Sen, S. Murthy, and S. Banerjee, âRegion-based connectivity-a new
paradigm for design of fault-tolerant networks,â in High Performance
Switching and Routing, 2009. HPSR 2009. International Conference on.
IEEE, 2009, pp. 1â7.
[13] S. Banerjee, S. Shirazipourazad, P. Ghosh, and A. Sen, âBeyond
connectivity-new metrics to evaluate robustness of networks,â in High
Performance Switching and Routing (HPSR), 2011 IEEE 12th International Conference on. IEEE, 2011, pp. 171â177.
[14] S. Banerjee, S. Shirazipourazad, and A. Sen, âDesign and analysis of
networks with large components in presence of region-based faults,â in
International Conference on Communications (ICC). IEEE, 2011.
[15] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
âCatastrophic cascade of failures in interdependent networks,â Nature,
vol. 464, no. 7291, pp. 1025â1028, 2010.
[16] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, âNetworks formed
from interdependent networks,â Nature Physics, vol. 8, no. 1, pp. 40â48,
2011.
[17] J.-F. Castet and J. H. Saleh, âInterdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,â PloS one, vol. 8, no. 4, p. e60402, 2013.
[18] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton, âIdentification of k most vulnerable nodes in multi-layered network using a
new model of interdependency,â in NetSciCom Workshop (INFOCOM
WKSHPS), Conference on Computer Communications. IEEE, 2014,
pp. 831â836.
[19] A. Das, J. Banerjee, and A. Sen, âRoot cause analysis of failures in
interdependent power-communication networks,â in Military Communications Conference (MILCOM), 2014 IEEE. IEEE, 2014, pp. 910â915.
[20] J. Banerjee, A. Das, C. Zhou, A. Mazumder, and A. Sen, âOn the entity
hardening problem in multi-layered interdependent networks,â in WIDN
Workshop (INFOCOM WKSHPS), 2015 IEEE Conference on Computer
Communications. IEEE, 2015, pp. 648â653.
[21] A. Das, C. Zhou, J. Banerjee, and A. Sen, âOn the smallest pseudo
target set identification problem for targeted attack on interdependent
power-communication networks,â in Military Communications Conference (MILCOM), IEEE (To appear). IEEE, 2015.
[22] J. Banerjee, C. Zhou, A. Das, and A. Sen, âOn robustness in multilayer interdependent networks,â in Conference on Critical Information
Infrastructures Security (CRITIS) (To appear). Springer, 2015.
[23] A. Mazumder, C. Zhou, A. Das, and A. Sen, âProgressive recovery from
failure in multi-layered interdependent network using a new model of
interdependency,â in Conference on Critical Information Infrastructures
Security (CRITIS). Springer, 2014.
[24] OpenStreetMap Contributors. OpenStreetMap. [Online]. Available:
www.openstreetmap.org

Partitioning Signed Bipartite Graphs for
Classification of Individuals and Organizations
Sujogya Banerjee, Kaushik Sarkar, Sedat Gokalp,
Arunabha Sen, and Hasan Davulcu
Arizona State University
P.O. Box 87-8809, Tempe, AZ, 85281 USA
{sujogya,kaushik.sarkar,sedat.gokalp,asen,hdavulcu}@asu.edu

Abstract. In this paper, we use signed bipartite graphs to model opinions expressed by one type of entities (e.g., individuals, organizations)
about another (e.g., political issues, religious beliefs), and based on the
strength of that opinion, partition both types of entities into two clusters. The clustering is done in such a way that support for the second
type of entity by the first within a cluster is high and across the cluster
is low. We develop an automated partitioning tool that can be used to
classify individuals and/or organizations into two disjoint groups based
on their beliefs, practices and expressed opinions.

1

Introduction

The goal of the Minerva1 project, currently underway at Arizona State University is to increase understanding of movements within Muslim communities
actively working to counter violent extremism. As a part of this study, we have
collected over 800,000 documents from web sites various organizations in Indonesia. Based on the support and opposition of certain beliefs and practices, we can
partition the set of organizations O into two groups O1 and O2 and the set of
beliefs and practices B into two groups, B1 and B2 , such that organizations in O1
support B1 and oppose B2 , while the organizations O2 support B2 and oppose
B1 . With the domain knowledge of the social scientists in our team regarding
the beliefs and practices of Indonesian community, we can then label one group
as being radical and other as counter-radical.
Although the motivation for our work was driven by Minerva, the the problem
that is being addressed in this paper is much broader in nature. In the mathematical sociology community, the problem is known as the Signed two-mode network
partitioning problem [1]. In its mathematical abstraction, the problem is specified by a bipartite graph G = (U âª V, E) and label function Ï : E â {P, N }.
The node sets U and V may be representing the set of organizations O and the
set of beliefs B respectively. If the label of an edge from oi â O to bj â B is
P , it implies oi supports (or has positive opinion) about bj . If the label of an
edge is N , it implies oi opposes (or has negative opinion) about bj . The goal of
1

A project sponsored by the U.S. Department of Defense

(a)

(b)

Fig. 1: Partitioning of the node set U and V with the desired goal
the partitioning problem is to divide the node sets U and V into two subsets
(U1 , U2 ) and (V1 , V2 ) respectively, such that
1. number of P edges (positive opinion or support) between nodes within block
1 (P11 between U1 and V1 ) and block 2 (P22 between U2 and V2 ) is high,
2. number of P edges between nodes across block 1 and block 2 (edges P12
between U1 and V2 and P21 between U2 and V1 ) is low,
3. number of N edges (negative opinion or opposition) between nodes within
block 1 (N11 between U1 and V1 ) and block 2 (N22 between U2 and V2 ) is
low and
4. number N edges between nodes across block 1 and block 2 (edges N12 between U1 and V2 and N21 between U2 and V1 ) is high.
The goal of partitioning is depicted in Fig. 1, where the green edges indicate
support (i.e, P edges) and the red edges indicate opposition (i.e, N edges). We
can realize these goals by maximizing [(P 11 + P 22 + N 12 + N 21) â (P 12 + P 21 +
N 11 + N 22)].
Signed two-mode network partitioning problem can be applied in a multitude
of domains, where the node sets U and V can represent different entities. For
example, (i) U and V may represent the members of the U.S. Senate/House of
Representatives and the bills before the senate/house of representatives where
they cast their votes, either supporting or opposing the bill; (ii) U and V may
represent the political blogs/bloggers and various issues confronting the nation,
where they express their opinions either supporting or opposing issues. Clearly,
availability of an automated tool that will co-cluster the entities represented by
U and V , will be valuable to individuals and organizations that need a coarse
grain (two-modal) partitioning of the data set represented by the node set U
and V . This tool can help classify individuals or organizations as radicals vs.
counter-radicals, or liberals vs. conservatives or violent vs. non-violent, etc.
The main contribution of this effort is the development of a fast automated
tool (and associated algorithms) for co-clustering the entities represented by the
node sets U and V . We first compute an optimal solution of the partitioning
problem using an integer linear program to be used as a benchmark for our
heuristic solution. We then develop a heuristic solution and compare its performance using three real data sets. The real data sets include voting records
of the Republican and Democratic members of the 111th US Congress and the
opinions expressed in top twenty two liberal and conservative blogs. In all these

data sets our partitioning tool produces high quality solution (i.e., with low misclassification) at a low cost (in terms of computation time). To the best of our
knowledge, our Minerva research group is the first to present an efficient computational technique for partitioning of signed bipartite graph and apply it to
some real data sets.

2

Related works

As the literature on clustering, classification and partitioning is really vast, due
to page limitations, we only refer to the ones that are most relevant to this
paper [1â4, 8, 7]. The two key features of the partitioning problem addressed in
this paper are (i) the graph is bipartite and (ii) the weights on the edges are
signed (i.e., the weights are both positive and negative). Simultaneous clustering
of two sets of entities (represented by two sets of nodes in the bipartite graph)
was considered in the context of document clustering in [4, 8]. In these studies one
set of entities are the documents and the other set is terms or words. Although
these efforts study the bipartite graph partition problem, they are distinctly
different from our study in one respect. In our study, the edge weights are signed,
whereas the edges weights considered in [4, 8] are unsigned. Graph partitioning
problem with signed edge weights was studied in [2, 3]. However, these studies are
also distinctly different from our study in that, while they focus on partitioning
general (i.e., arbitrary) graphs, we focus our attention to partitioning bipartite
graphs. The study that comes closest to our study is [1, 7], where attention is
focused on partitioning of a signed bipartite graphs. However, neither [1] nor
[7] present any efficient algorithm to solve the partitioning problem in signed
bipartite graph.

3

Problem Formulation

In this section we formally define the partitioning problem.
Signed Bipartite Graph Partition Problem (SBGPP): An edge labeled weighted
bipartite graph G = (U âª V, E) where U = {u1 , u2 , . . . , un } represents entities
of type I and V = {v1 , v2 , . . . , vm } represents entities of type II. Each edge
(u, v) â E has two functions associated with it: (i) label function Ï : E â {P, N },
which indicates the type of opinion (positive or negative), and (ii) weight function
w : E â Z, which indicates the strength of that opinion. AN = [wn (u, v)] and
AP = [wp (u, v)] are the weighted adjacency matrix for edges with label N and P
respectively. If the node set U is partitioned into U1 and U2 and V is partitioned
into V1 and V2 , the strength of the positive and negative opinions of the entities
of type I regarding the entities of type II are defined as follows:
For all edges (u, v) â E,
X X
X X
X X
P11 =
wp (u, v), P12 =
wp (u, v), P22 =
wp (u, v)
P21 =

uâU1 vâV1

uâU1 vâV2

X X

X X

uâU2 vâV1

wp (u, v), N11 =

uâU1 vâV1

uâU2 vâV2

wn (u, v), N12 =

X X

uâU1 vâV2

wn (u, v)

N22 =

X X

wn (u, v), N21 =

uâU2 vâV2

X X

wn (u, v)

uâU2 vâV1

Problem: Find a partition of the node set U into U1 and U2 and V into V1 and V2
such that [(P 11 + P 22 + N 12 + N 21) â (P 12 + P 21 + N 11 + N 22)] is maximized.

4

Computational Techniques

In this section we give a mathematical programming technique to find the optimal solution for the SBGPP. Since computational time for finding optimal solution for large graphs is unacceptably high, we present a heuristic in subsequent
section to solve the SBGPP.
4.1

Optimal Solution for SBGPP

The goal of the SBGPP is to partition U into two disjoint sets U1 and U2
(similarly V into V1 and V2 ). For each node in u â U and each partition Ui , i =
1, 2, we use a variable bui . bui is 1 iff in u is in Ui . Similarly we define variable
pvi for all v â V . We will refer B1 = U1 âª V1 and B2 = U2 âª V2 as blocks 1 and
2 respectively.
V ariables: For each node u â U, v â V and each partition Ui , Vi , i = 1, 2
(
(
1, if node u is in partition Ui
1, if node v is in partition Vi
bui =
pvi =
0, otherwise.
0, otherwise.
The mathematical programming formulation is given as follows:
max

L=

2 X X
X

(wp (u, v) â wn (u, v))bui pvi

i=1 uâUi vâVi

+

2
X
X X

(wn (u, v) â wp (u, v))bui pvj

i,j=1 uâUi vâVj
i6=j

s.t

bu1 + bu2 = 1,

âu â U

(1)

pu1 + pu2 = 1,

âp â V

(2)

The objective function computes the objective value given by the expression L.
We want to maximize L. It may be noted that the above quadratic objective
function can easily be changed into a linear function by simple variable transformation [6]. Constraint 1 and 2 ensures that each node in U and V belongs to
one particular block.
4.2

Move-based Heuristics

We present a move-based heuristic to find an approximate solution of SBGPP.
The move-based heuristic is a variant of well known FM algorithm [5] for partitioning graphs. The algorithm starts with a random initial partition and iteratively moves nodes from one block to another such that the value of the objective

function is improved. The âgainâ of a node is defined as the value by which the
objective function increases if the node is moved from one block to the other.
In each iteration the node with the highest gain is moved from one block to
the other. In case of a tie a node is chosen arbitrarily. After a node is moved,
it is locked and is not moved until the next pass. The heuristic is presented in
Algorithm 1. It should be noted that original FM algorithm will not work for our
problem as SBGPP relates to signed bipartite graphs with a completely different
objective function and doesnât have any size constraints. As a result the node
gain computation routine Algorithm 2 is considerably different from the original
FM algorithm. Algorithm 1 runs for r different initial random partition of the
nodes to avoid the possibility of being stuck at a local maxima. In practice the
heuristic converges very fast, mostly in 2 to 3 passes.
Algorithm 1: Move-based Heuristic (MBH)

13

Input : A weighted signed bipartite graph H = (U âª V, E)
Output: A partition of the nodes U1 , U2 and V1 , V2 such that objective value L
is maximum
L ââ 0;
for i ââ 1 to r do
Generate a random partitioning of the nodes in U into U1 and U2 and nodes
in V into V1 and V2 ;
repeat
Compute gains of all nodes using Algorithm 2 ;
repeat
Among all the unlocked nodes select the node of highest gain. Move
the node to the other block and call it base node. Lock the base
node;
Update the node gains of all the free neighbors of the base node;
until Until all the nodes are locked ;
Change the current partition into a new partition that has the largest
value of the objective function in this pass ;
Unlock all the nodes;
until If the objective value Lâ² improves during the last pass;
if Lâ² â¥ L then Lâ² â L and save the current partition

14

return L and the final partition of nodes

1
2
3
4
5
6
7

8
9
10
11
12

5

Experimental Results and Discussions

To validate the effectiveness of our heuristic and benchmark its performance we
tested the heuristic both on synthetic and real world data. The real world data
consists of US Congress (SENATE, REP) and political blogosphere (BLOG) data
sets.
5.1

US Congress Data [SENATE, REP]

The US Congress has been collecting data since the very first congress of the US
history. This data has been encoded as XML files and publicly shared through

Algorithm 2: Node Gain Computation
Input : A weighted signed bipartite graph G = (U âª V, E)
Output: Gains of all nodes
foreach node u â U âª V do
gain(u) ââ 0;
// FBlock = "from block" of node u, ToBlock = "to block" of node
u, w(e) = weight of edge e and # = number
foreach edge e â E with l(e) = N of node u do
if # nodes of e in ToBlock is 0 then gain(u) â gain(u) + 2 â w(e);
if # nodes of e in FBlock is 1 then gain(u) â gain(u) â 2 â w(e);

1
2

3
4
5

foreach edge e â E with l(e) = P of node u do
if # nodes of e in ToBlock is 0 then gain(u) â gain(u) â 2 â w(e);
if # nodes of e in FBlock is 1 then gain(u) â gain(u) + 2 â w(e);

6
7
8

the govtrack.us project2 . From various types of data available at the project
site, we collected the roll call votes for the 111th US Congress which includes
The Senate and The House of Representatives and covers the years 2009-2010.
The 111th Senate data contains information about 108 senators and their votes
on 696 bills3 . The 111th Congress has 451 representatives and the data contains
their vote on 1655 bills.
We extracted the SENATE and REP data in adjacency matrices A|U|Ã|V | ,
with U vertices representing the congressmen, and the V vertices representing
the bills. The edge (ui , vj ), ui â U, vj â V has weight 1 if the congressman
ui votes âYeaâ for the bill vj , â1 if the congressman votes âNayâ, and 0 if he
did not attend the session. We have the original classification vector for both
the congressmen and the bills in terms of which party they represent (or which
party sponsored the bill). The first two columns of Table 1 provide information
about this data as well as the partitioning accuracies of the algorithms. Figure
2 depicts the partitioned vote matrices of the 111th US Congress data, where
rows representing the congressmen and the columns representing the bills. Also,
the light green color represents âYeaâ votes, and dark red represents âNayâ votes.
5.2

Blog Data [BLOG]

As Web 2.0 platforms gained popularity, it became easy for web users to be a
part of the web and express their opinions, mostly through blogs. Most blogs
are maintained by individuals, whereas there are also professional blogs with a
group of authors. In this study, we focus on a set of popular political liberal or
conservative blogs that have a clearly declared positions. These blogs contain
discussions about social, political, economic issues and related key individuals.
2
3

http://www.govtrack.us/data
Normally, each congress has 100 senators (2 from each state), however in many of
the congresses, there are unexpected changes on the seats caused by displacements
or deaths.

(a) 111th US House

(b) 111th US Senate

Fig. 2: Vote matrix of US Congress after partitioning

Table 1: Descriptive summaries of the graphs for each dataset with the Heuristic
accuracy

Vertices in V

111th US Senate
64 Democrat
42 Republican
Senator
696 Bills

111th US House
268 Democrat
183 Republican
Representatives
1655 Bills

Graph Density
Heuristic accuracy

88.36 %
100.00%

91.23 %
99.56%

Vertices in U

Political Blogosphere
13 Liberal
9 Conservative
Blogs
20 Liberal
14 Conservative People
39.04 %
98.21%

They express positive sentiment towards individuals whom they share ideologies
with, and negative sentiment towards the others. In these blogs, it is also common
to see criticism of people within the same camp, and also support for people from
the other camp.
In this experiment, we collected a list of 22 most popular liberal and conservative blogs from the Technorati4 rankings. For each blog, we fetched the posts
for the period of 6 months before the 2008 US presidential elections (May - October, 2008). We expected to have high intensity of the debates and discussions
and resulting in a bipolar clustering in the data. Table 2 shows the partial list
of blogs with their URLs, political camps and the number of posts for the given
period.
We use AlchemyAPI5 to run a named entity tagger to extract the people
names mentioned in the posts, and an entity-level sentiment analysis which provided us with weighted and signed sentiment (positive values indicating support,
and negative indicating opposition) for each person. This information was used
to synthesize a signed bipartite graph (the BLOG data), where the blogs and
people correspond to the two sets of vertices U and V . The aij values of the adjacency matrix A are the cumulative sum of sentiment values for each mention
of the person vj by the blog ui .
4
5

http://technorati.com
http://www.alchemyapi.com

To get a gold standard list of the most influential liberal and conservative
people, we used The Telegraph List6 for 2007. The third column of Table 1
provides information about this data as well as the partitioning accuracies of
the algorithm.
Table 2: Political Blogs
Blog name
URL
Huffington Post
http://www.huffingtonpost.com/
Daily Kos
http://www.dailykos.com/
Boing Boing
http://www.boingboing.net/
Crooks and Liars
http://www.crooksandliars.com/
Firedoglake
http://www.firedoglake.com/
Hot Air
http://hotair.com/
Reason - Hit and Run http://reason.com/blog
Little green footballs http://littlegreenfootballs.com/
Atlas shrugs
http://atlasshrugs2000.typepad.com/
Stop the ACLU
http://www.stoptheaclu.com/
Wizbangblog
http://wizbangblog.com/

6

Political view
Liberal
Liberal
Liberal
Liberal
Liberal
Conservative
Conservative
Conservative
Conservative
Conservative
Conservative

Size
3959
1957
1576
1497
1354
1579
1563
787
773
741
621

Conclusion

In this paper we study the problem of partitioning signed bipartite graph with
relevant application in political, religious and social domains. We provided a fast
heuristic to find the solution for this problem. We tested the high accuracy of
our heuristic on three sets of real data collected from political domain.

References
1. Andrej, M., Doreian, P.: Partitioning signed two-mode networks. Journal of Mathematical Sociology 33, 196â221 (2009)
2. Bansal, N., Blum, A., Chawla, S.: Correlation clustering. In: MACHINE LEARNING. pp. 238â247 (2002)
3. Charikar, M., Guruswami, V., Wirth, A.: Clustering with qualitative information.
In: Proceedings of the 44th Annual IEEE FOCS (2003)
4. Dhillon, I.S.: Co-clustering documents and word using bipartite spectral graph partitioning. In: Proceedings of the KDD. IEEE (2001)
5. Fiduccia, C., Mattheyses, R.: A linear-time heuristic for improving network partitions. In: Papers on Twenty-five years of electronic design automation. pp. 241â247.
ACM (1988)
6. Sen, A., Deng, H., Guha, S.: On a graph partition problem with application to vlsi
layout. Inf. Process. Lett. 43(2), 87â94 (1992)
7. Zaslavsky, T.: Frustration vs. clusterability in two-mode signed networks (signed
bipartite graphs) (2010)
8. Zha, H., He, X., Ding, C., Simon, H., Gu, M.: Bipartite graph partitioning and data
clustering. In: Proceedings of the 10th International Conference on Information and
Knowledge Management. pp. 25â32. ACM (2001)
6

The-top-US-conservatives-and-liberals.html

Soc. Netw. Anal. Min.
DOI 10.1007/s13278-012-0072-x

ORIGINAL ARTICLE

A system for ranking organizations using social scale analysis
Sukru Tikves â¢ Sujogya Banerjee â¢ Hamy Temkit â¢ Sedat Gokalp â¢
Hasan Davulcu â¢ Arunaba Sen â¢ Steven Corman â¢ Mark Woodward
Shreejay Nair â¢ Inayah Rohmaniyah â¢ Ali Amin

â¢

Received: 20 December 2011 / Revised: 4 April 2012 / Accepted: 20 April 2012
Ã Springer-Verlag 2012

Abstract In this paper, we utilize feature extraction and
model-fitting techniques to process the rhetoric found in
the web sites of 23 Indonesian Islamic religious organizations to profile their ideology and activity patterns along a
hypothesized radical/counter-radical scale, and present an
end-to-end system that is able to help researchers to visualize the data in an interactive fashion on a timeline. The
subject data of this study is 37,000 articles downloaded
from the web sites of these organizations dating from 2001
to 2011. We develop algorithms to rank these organizations
by assigning them to probable positions on the scale. We
show that the developed Rasch model fits the data using
Andersenâs LR-test. We create a gold standard of the
ranking of these organizations through an expertise elicitation tool. We compute expert-to-expert agreements,
and we present experimental results comparing the performance of three baseline methods to show that the Rasch model not only outperforms the baseline methods, but it
is also the only system that performs at expert-level
accuracy.

1 Introduction

S. Tikves (&)  S. Banerjee  H. Temkit  S. Gokalp 
H. Davulcu  A. Sen  S. Corman  M. Woodward  S. Nair
Arizona State University, P.O. Box 87-8809,
Tempe, AZ 85281, USA
e-mail: sukru@asu.edu; stikves@asu.edu

S. Corman
e-mail: scorman@asu.edu

S. Banerjee
e-mail: sujogya@asu.edu

S. Nair
e-mail: snair8@asu.edu

H. Temkit
e-mail: mtemkit@asu.edu

I. Rohmaniyah
Center for Religious and Cross Cultural Studies,
Gadjah Mada University, Yogyakarta, Indonesia
e-mail: rochmaniyah@yahoo.com

S. Gokalp
e-mail: sgokalp@asu.edu
H. Davulcu
e-mail: hdavulcu@asu.edu
A. Sen
e-mail: asen@asu.edu

Being able to asses information on radical and moderate actors
in a geographic area is an important research topic for national
security. Radicalism is the ideological conviction that it is
acceptable, and in some cases, obligatory to use violence to
effect profound political, cultural and religious transformations and change the existing social order fundamentally.
Muslim radical movements have complex origins and depend
on diverse factors that enable translation of their radical ideology into social, political and religious movements. Crelinste
(2002), in his work, states that ââboth violence and terrorism
possess a logic and grammar that must be understood if we are
to prevent or control themââ. Therefore, analysis of Muslim
radical and counter-radical movements requires attention to
the global, national and local social, economic and political
contexts in which they are located. Similarly, in the Islamic
context, counter-radical discourse takes various different
forms; discursive and narrative refutations of extremist
claims, symbolic action such as ritual and other religious and

M. Woodward
e-mail: mataram@asu.edu

A. Amin
State College of Islamic Studies (STAIN),
Manado, Indonesia
e-mail: aleejtr77@yahoo.com

123

S. Tikves et al.

cultural practices, and Islamic arguments for pluralism,
peaceful relations with non-Muslims, democracy, etc. The
most effective counter-radicals are likely to be religiously
conservative Muslims. Effective containment and defeat of
radicalism depends on our ability to recognize various levels
of radicalization, and detection of counter-radical voices.
In our previous work (Davulcu et al. 2010), we attempted a
clustering approach to obtain âânatural groupingsââ of a number
of local non-government religious social movements and
organizations in Indonesia. Social scientists on our team
observed that clustering results were not fully able to separate
all counter-radical or radical organizations into pure clusters.
Pure radical clusters were easily identified due to high similarity among their support for violent practices. Pure counterradical clusters were identified due to their strong reactionary
opposition to violent practices through protests and rhetoric.
But the rest of the groupings were mixed. We realized that
binary labeling as counter-radical or radical does not capture
the overlap, movement and interactivity among these organizations. In this paper, we hypothesize that both counterradical and radical movements in Muslim societies exhibit
distinct combinations of discrete states comprising various
social, political, and religious beliefs, attitudes and practices,
that can be mapped to a latent linear continuum or a scale.
Using such a scale, an analyst can determine where exactly
along the spectrum any particular group lies, and also potentially where it is heading with its rhetoric and activity.
Given the complex nature of the task, such as regional
differences in local cultures, beliefs and practices, and in the
absence of readily available high-accuracy parsers, highly
structured religio-social ontologies, and information
extraction systems; we decided to devise a multi-lingual nonlinguistic text processing pipeline that relies on only statistical modeling of keyword frequency and co-occurrence
information. However, we designed the system to be able to
incorporate additional information extracted from the text, if
available. For example, named entity recognition (NER),
machine translation, and GIS-based location lookup information are part of the user-interface presentation.
We (Tikves et al. 2011) worked with social scientists on
our team to come up with an orthogonal model comprising
of two primary dimensions. Both dimensions, (1) radical/
counter-radical and (2) violent/non-violent, are characterized as latent, partial orders of discrete beliefs and practices
based on a generalization of item order in Guttman scaling
(Guttman 1950) using a Rasch model (Andric 1988). A true
Guttman scale is a deterministic process, i.e., if a social
movement subscribes to a certain belief or practice, then it
must also agree with all lower-order practices and beliefs
on the scale. Of course, such perfect order is rare in the
social world. The Rasch model provides a probabilistic
framework for Guttman scales to accommodate for
incomplete observations and measurement errors.

123

We have designed a web-based system to visualize this
orthogonal model. The web tools provided by the system
allows drilling down on specific data, and plotting the trends
and trajectories of organizations on a timeline. It consists of
several modules: an off-line web mining, and data-processing pipeline, two web services for application logic, and an
AJAX-based presentation layer. The web-based interface
built for this study can be accessed through the web site at
http://www.demo.minerva-project.org. In this paper, we
present several scenarios with this tool in Sect. 5.
In this paper, we present feature extraction, feature
selection, and model-fitting techniques to process the rhetoric found in the web sites of 23 religious Indonesian organizationsâcomprising a total of 37,000 articles dating from
2001 to 2011. We aim to identify their ideology and activity
patterns along a hypothesized radical/counter-radical scale,
and rank them to probable positions on this scale (McPhee
1995). The automated ordering of organizations is formed by
ranking the organizations according to their estimated
positions on the latent scale. We used the eRm1 package to fit
the Rasch model on this data set, and identify organizationsâ
positions based on maximum likelihood estimation (Le Cam
1990). We show that the model fits the data using the
Andersenâs likelihood ratio test (LR-test) (Hessen 2010). We
also created a gold standard of the ranking of these organizations through an expert-opinion elicitation tool, and
through the opinions of three ethnographers on our team who
collectively possess 35 years of scholarly expertise on
Indonesia and Islam. We computed expert-to-gold standard
agreements, as well as compared the performance of three
different baseline computational methods to show that the
Rasch model presented here not only performs the best
among the baseline methods but that it is also the only
method that performs at an expert level of accuracy.
1.1 Organization of the paper
Next section provides an introduction to the theory of
Guttman scaling and Rash models. Section 3 defines the
problem, presents the system architecture, and the methods
used to solve the problem. Section 4 describes the Indonesian corpus, expert-opinion elicitation tool, baseline
computational methods, and experimental evaluations.
Section 5 discusses the user-interface designed for navigating our findings. Section 6 concludes the paper.

2 Introduction of Guttman scaling and Rasch model
In social science, scaling is a process of measuring and
ordering entities called subjects, based on their qualitative
1

http://www.r-forge.r-project.org/projects/erm/.

A system for ranking organizations using social scale analysis

attributes called items. In general, subjects are requested to
respond to surveys conducted by means of structured
interviews or questionnaires. Items are presented to the
subjects in form of questions. Statistical analysis of the
response of the subjects on the questions about items are
used in scaling the subjects. Some of the widely followed
scaling procedure in social science surveys are Likert scale
(Likert 1932), Thurnstone scale (Thurnstone 1928), and
Guttman scale (McIver 1981). In Likert scale, subjects
indicate their magnitude of agreement or disagreement
about an item (from strongly agree to strongly disagree) on
a five- to ten-point scale. On the other hand, Thurnstone
scale is a formal method of ordering the attitudes of the
subjects toward the items. Guttman scaling procedure
orders both the subjects and the items simultaneously with
respect to some underlying cumulative continuum. In this
paper, we follow the Guttman scaling process to rank the
organizations based on their response on the radical and
counter-radical keywords.
2.1 Guttman scaling
A Guttman scale (Guttman 1950) presents a number of items
to which each subject is requested to provide a dichotomous
response, e.g., agree/disagree, yes/no, or 1/0. This scaling
procedure is based on the premise that the items have strict
orders (i.e., the items are presented to the subjects ranked
according to the level of the itemâs difficulty). An item ââAââ
is said to be ââmore difficultââ than an item ââBââ, if any subject
answering ââyesââ on item ââAââ implies that the subject will
also answer ââyesââ on item ââBââ. A subject who responds to
an item positively is expected to respond positively to all the
items of lesser difficulty. For example, to find out how
extreme a subjectâs view is on Guttman scale, the subject is
presented with the following series of items in question form.
(1) Are you willing to permit immigrants to live in your
country? (2) Are you willing to permit immigrants to live in
your community? (3) Are you willing to permit immigrants
to live in your neighborhood? (4) Are you willing to permit
immigrants to live to your next door? (5) Are you willing to
permit your child to marry an immigrant? If the items form a
Guttman scale, any subject agreeing with any item in this
series, will also agree with other items of lower rank-order in
this series. Guttman scale is a deterministic process and the
score of a subject depends on the number of affirmative
responses he has made on the items. So, a score of 2 for a
subject in the above Guttman scale not only means he has
given affirmative response to two of the questions or items
but also indicates that he agrees with two particular questions, namely the first and second. Scores in Guttman scale
can also be interpreted as the ââabilityââ of a subject in
answering questions sorted in increasing order of ââdifficultyââ. These scores when presented on an underlying scale,

give us an ordering of the subjects based on their ââabilityââ
also.
The objective of our paper is to order the Indonesian
Islamic organizations based on their views on religio-social
keywords which have an inherent ordering. For example,
two such keywords are ââQuranââ and ââShariaââ. An organization supporting ââShariaââ will also likely to ââbelieve in
Quranââ. So it makes sense to use Guttman scaling procedure to rank the organizations and their beliefs and practices. One drawback of Guttman scale is that it is
deterministic and assumes a strict ordering of the items. In
real world, it is difficult to order all the items in such a
strict level of increasing difficulty, therefore, perfect scales
are not often observed in practice. Furthermore, many
times, the order of the items are not known since they are
not straightforwardly comparable. In addition, measurement errors might lead to responses that do not strictly fit
the ordering. As a result, we can no longer conclude
deterministically that if a subject answers a question
affirmative, whether she will be able to give affirmative
answers to other questions of lower order in the same
questionnaire. We use Rasch model to overcome this
drawback by taking into account measurement error.
2.2 Rasch model
Rasch model (Andric 1988) provides a probabilistic framework for Guttman scales. In Rasch model, the probability of a
specified binary response (e.g., a subject agreeing or disagreeing to an item) is modeled as a function of subjectâs and
itemâs parameters. Specifically in the simple Rasch model,
the probability of a positive response (yes) is modeled as a
logistic function of the difference between the subject and
itemâs parameters. Item parameters pertain to the difficulty
of items while subject parameters pertain to the ability of
subjects who are assessed. A subject of higher ability, related
to the difficulty of an item, has higher probability to respond
to a question affirmatively. In this paper, Rasch models are
used to assess the organizations degree of being radical or
counter-radical based on the religio-social keywords (items)
appearing in their rhetoric.
Rasch model also maps the responses of the subjects to the
items in binary or dichotomous format, i.e., 1 or 0. Let
Bernoulli variable Xvi denotes the response of a subject v to
the item i, variable hv denotes the parameter of ââabilityââ of
the subject v and bi denotes the parameter of ââdifficultyââ of
an item i. According to the simple Rasch model, the probability that the subject v responds 1 for item i is given by:
PÃ°Xvi Â¼ 1jhv ; bi Ã Â¼

expÃ°hv  bi Ã
:
1 Ã¾ expÃ°hv  bi Ã

Rasch model assumes that the data under analysis have
the following properties.

123

S. Tikves et al.

Unidimensionality P(xvi = 1|hv, bi, a) = P(xvi = 1|hv,
bi), i.e., the response probability does not depend on
other variable
2. Sufficiency sum of responses contains all information
on ability of a subject, regardless which item it has
responded
3. Conditional independence for a fixed subject, there is
no correlation between any two items
4. Monotonicity response probability increases with
higher values of h, i.e., subjectâs ability.
P
Items with si = nv xvi value of 0 or n, and subjects with
Pk
rv = i xvi value of 0 or k are removed prior to estimation,
where n is the total number of subjects and k is the total
number of items. Running Rasch model on the data gives us
an item parameter estimate or a score for each item. In
general, the estimation of bi or score for an item i is calculated through conditional maximum likelihood (CML) estimation (Pawitan 2001). The conditional likelihood function
for measuring item parameter estimate is defined as:
Y
expÃ°bi si Ã
Lc Â¼
PÃ°xvi jrv Ã Â¼ Q P
r
xjr expÃ°bi xvi Ã
v
1.

where r represents the sum over all combinations of
r items. Similarly, the maximum likelihood is used to
calculate subject parameter estimation hv or score for each
subject. Expectation-maximization algorithms (Hunter
2004) are used in implementing CML estimation in Rasch
model. We can also assess whether the data fit the model
by looking at goodness of fit indices, such as the Andersenâs LR-test.
To evaluate the quality of these measurements, we run
Anderson LR-test (Hessen 2010) on the set of data. The test
gives us a goodness of fit of the data in Rasch model, i.e., it
tells us whether the data follows the assumptions of Rasch
model. A p value, returned by the test, indicates the
goodness of fit and a p value2 higher than 0.05 indicates no
presence of lack of fit.
2.3 Implementing Rasch model in the text mining
domain
In this paper, we use Guttman scaling and Rasch model to
find a ranking of 23 religious organizations based on
extremity of their views are on radicalism and counterradicalism. In our application, Rasch-model subjects correspond to a group of religious organizations, and items
correspond to a set of keywords for socio-cultural, political, religious radical and counter-radical beliefs, and
practices. An organization responding ââyesââ to a feature
means the organization exhibits that feature in its narrative,
2

http://www.en.wikipedia.org/wiki/P-value.

123

while an organization responding âânoââ to a feature indicates that the organization does not exhibit such a feature.
Difficulty of an item translates to strength of the corresponding attitude in defining radical or counter-radical
ideology of any organization. Similarly ability of a subject
in this case means the degree of radicalism or counterradicalism exhibited by an organizationâs rhetoric. Other
works in text-mining domain, such as sentiment analysis,
have used Rasch model in their analysis (Drehmer et al.
2000). Details of keyword extraction and selection are
presented in Sect. 3.3.

3 Methods
3.1 Problem definition
The primary goal of this study is to build a semi-automated
method to rank religious organizations from a certain
geographical region on a scale of radicalism versus counterradicalism using their web sites. The efficacy of the generated model is evaluated by comparing it against baseline
methods and expert-level performance. In addition to
accomplishing these goals, we also present an end-to-end
system architecture, and a graphical user-interface design to
facilitate faceted search and browsing of this corpus.
3.2 System architecture
A summary of the system architecture can be seen in
Fig. 1. The system is a composition of four components: a
data-gathering component, which does web crawling, and
text extraction; a scale generation component, performing
scaling algorithms; application services component, which
consists of several web services, and finally, a web userinterface component, presenting the data to the end user.
3.2.1 Data gathering
Initially, social scientists are invited to use their domain
and area expertise to identify a set of organizations, and
hypothesize any number of unipolar or bipolar scales that
could explain the variance among their beliefs and practices. Next, a set of web crawling scripts are created for
extraction of articles from those organizationsâ web sites.
For each organizationâs corpus, we extract their top-k
n-grams, and a union of all these phrases are presented to
experts for feature selection. Downloaded articles are then
converted into XML structures, containing their original
text, their set of keywords, and extracted information such
as person, location and organization names using a NER
tool for Indonesian language, and their machine translations into English.

A system for ranking organizations using social scale analysis

economic, and religious} keywords corresponding to
beliefs, goals and practices. During this process, our team
of experts screened a total of 790 candidate keywords and
they selected 29 keywords for inclusion in the radical scale,
and 26 keywords for inclusion in the counter-radical scale.
3.4 Debates and perspective analysis

Fig. 1 An overview of the system architecture

An example document snippet is shown in Fig. 2. Here
the original input (content, source), and a sample of the
automatically extracted information corresponding to
DATE, PERSON, and LOCATION can be seen. The corresponding XML versions for each input document are then
stored in a document database for processing.

Upon inspecting the keywords selected by our team of
experts, we observed that some of these keywords correspond to differing perspectives on a set of topics that are
debated within these web sites. Definition of debate is ââa
formal discussion on a particular topic in a public meeting or
legislative assembly, in which opposing arguments are put
forwardââ.3 During a debate on a particular topic, like education, both radical and counter-radical organizations discuss different perspectives such as ââsecular multi-cultural
educationââ versus ââSharia based religious educationââ.
To design an automated perspective detection algorithm,
we made the following simplifying assumptions.
1.

3.3 Keyword extraction and selection
2.
To identify candidate keywords, one option was to translate
the documents into English and apply readily available
keyword-extraction methods (Michael 2010). However, it
was preferable to preserve the original expression of the
phrases in the original language. Hence, we utilized a nonlinguistic technique that relies only on statistical occurrence, and frequency information.
Within each document, the words were separated by
whitespace or punctuation marks. We considered each
keyword to be an n-gram of one to three words. We treated
each organization as one document and calculated the term
frequency-inverse document frequency (TF-IDF) (Salton
1988) values for every single n-gram mentioned by these
organizations. Top 100 n-grams with the highest TF-IDF
values from each organization were used to generate a
candidate list of topics that these organizations discuss
most frequently. Next, we asked our team of experts to
screen and manually select identify {social, political,

Fig. 2 A portion of a document represented in the system

Organizations will mostly discuss their own perspective in a debate.
Organizations will occasionally mention others perspectives, however, then relate them back to their own
perspective.

In the following sections, we present a mathematical
formulation of the perspective keyword-generation problem
for a given topic, provide an NP-completeness proof, and
design an exact solution through an integer linear programming (ILP)-based solver. Our future work involves finding
an efficient approximation algorithm for this problem.
3.4.1 Perspective keywords-generation problem
Perspective keywords-generation problem (PKGP) is
defined as follows. Given a topic (a keyword) T, and two sets
of documents TR and TCR where TR contains n documents
TR Â¼ fDR;1 ; DR;2 ; . . .; DR;n g and TCR contains m documents
TCR Â¼ fDCR;1 ; DCR;2 ; . . .; DCR;m g: From each document
DR;i 2 TR Ã°DCR;j 2 TCR Ã, we collect a set of words
WR,i, V1 B i B n (WCR,j, V1 B j B m) which appear two
words before and two words after each occurrence of the
topic T in that document. Let us define W as the union of all
the WR,i, V1 B i B n and WCR,j, V1 B j B m. If the cardinality of W is p, then W can be given as W Â¼ fw1 ; w2 ; . . .;
wp g Â¼ fWR;1 [ WR;2 [    [ WR;n [ WCR;1 [ WCR;2 [    [
WCR;m g
Let the frequency of word wk in document DR,i is given
as fR,i(wk) and the frequency of word wk in document DCR,j
as fCR,j(wk).
3

Oxford online dictionary.

123

S. Tikves et al.

Question: Are there two non-empty disjoint subsets of
W, named W 0 and W 00 and W 0 \ W 00 Â¼ ;; such that for
every DR,i V1 B i B n,
X
X
fR;i Ã°wk Ã 
fR;i Ã°wl Ã
Ã°1Ã
wk 2W 0

wl 2W 00

and for every DCR,j V1 B j B m,
X
X
fCR;j Ã°wk Ã 
fCR;j Ã°wl Ã
wk 2W 0

Ã°2Ã

wl 2W 00

and jW 0 j Ã¾ jW 00 j  K?
In optimization version of the problem, we will try to
minimize jW 0 j Ã¾ jW 00 j:
3.4.2 Computational complexity of PKGP
Definition 1 [Weak Partition problem (WPP)] Instance
A finite set A Â¼ fa1 ; . . .; an g and a size sÃ°ai Ã 2
Z Ã¾ ; 8i; 1  i  n: Question Does the set A contain two nonempty sub-sets A1 and A2 that (1) A1 \ A2 Â¼ ;; (2) A1 [
P
P
A2  A and (3) ai 2A1 sÃ°ai Ã Â¼ aj 2A2 sÃ°aj Ã?
WPP has been shown to be NP-complete in (van Emde
Boa 1981).
Theorem 1

PKGP is NP-complete.

Proof It is easy to see that PKGP is in NP since a nondeterministic algorithm needs only to guess a partition of
the word set W into W 0 and W 00 and check in polynomial
time if all the constraints hold for this partition and also if
jW 0 j Ã¾ jW 00 j  K:
WPP is a restricted version of PKGP. First we create a
restricted instance of PKGP as follows: let TR and TCR
contains one documents each, i.e, TR = {DR,1} and
TCR = {DCR,1}. Frequency of a word wi 2 W; 81  i  n in
document DR,1 and DCR,1 is taken to be equal, i.e.,
fR,1(wi) = fCR,1(wi) = s(ai). The parameter K is taken to be
equal to |W|. This instance of PKGP is similar to an
instance of WP in the following way: the set A contains
element ai for every word wi 2 W: So, |W| = |A|. In addition, s(ai) = fR,1(wi) = fCR,1(wi), V1 B i B n.
If we find a weak partition of A, as sets A1 and A2 such that
P
P
ai 2A1 sÃ°ai Ã Â¼
aj 2A2 sÃ°aj Ã, then we can find subsets of
W, as sets W1 and W2, such that wi 2 W1 if ai 2 A1 and
P
wj 2 W2 if aj 2 A2 , respectively. In addition, ai 2A1 sÃ°ai Ã Â¼
P
P
0
aj 2A2 sÃ°aj Ã; implies that both the constraints
P
P
P wi 2W
fR;1 Ã°wi Ã  wj 2W 00 fR;1 Ã°wj Ã and wi 2W 0 fCR;1 Ã°wi Ã  wj 2W 00
fCR;1 Ã°wj Ã are true, because s(ai) = fR,1(wi) = fCR,1(wi),
V1 B i B n. Since K = |W|, the constraint jW 0 j Ã¾ jW 00 j  K
will trivially hold. So, WPP is a restricted version of PKGP.

123

Since WPP is known to be NP-complete, PKGP is also NPcomplete.
3.4.3 Integer linear programming formulation for PKGP
We formulate an ILP to solve the PKGP optimally. For
each word wi 2 W, we use two variables xi and yi. xi is 1 if
and only if the word wi is in W1 and yi is 1 if and only if the
word wi is in W2. Then constraint (3) means sets W1 and W2
disjoint. Constraint (4) ensures that these sets (W1 and W2)
are also non-empty. Constraints (5) and (6) ensure the
constraints 1 and 2 in problem statement. The objective
minimizes the summation of cardinality of W1 and W2.
Variables: For each word wi,

1; if word wi is assigned to set W1
xi Â¼
0; otherwise.

1; if word wi is assigned to set W2
yi Â¼
0; otherwise.
Pp
min
iÂ¼1 xi Ã¾ yi
s:t: xi Ã¾ yi  1;
n
X

xi  1 and

iÂ¼1

n
X

8i Â¼ 1; . . .; p
yi  1;

8i Â¼ 1; . . .; p

Ã°3Ã
Ã°4Ã

iÂ¼1

p
X

fR;i Ã°wk ÃÃ°xk  yk Ã  0;

8i Â¼ 1; . . .; n

Ã°5Ã

fR;i Ã°wk ÃÃ°xk  yk Ã  0;

8i Â¼ 1; . . .; m

Ã°6Ã

wk 2W
p
X
wk 2W

xi 2 f0; 1g; yi 2 f0; 1g;

8i Â¼ 1; . . .; p

Ã°7Ã

3.4.4 Social scale generation
Social scale generation is done by building response tables;
a pair of tables for a bipolar scale, such as radical/counterradical (R/CR), or a single table for a unipolar scale, by
thresholding the occurrence frequencies of the selected
keywords in the organizationsâ web corpus.
The scale-generation architecture is shown in Fig. 3.
Here, the flow of the processes and data can be seen as
interactions between experts and automated modules. The
system works as follows.
â

â

Initially, area experts to identify a set of organizations,
and hypothesize any number of unipolar or bipolar
scales that could explain the variance among the beliefs
and practices of the organizations.
Next, we crawl and download the web sites of the
organizations, and the system automatically extracts
the top-k candidate keywords for consideration in the

A system for ranking organizations using social scale analysis

â

Two types of other information are collected for
evaluation purposes. First, expert rankings of the
organizations, using a graphical drag-and-drop expertopinion elicitation tool shown in Fig. 11. Expert
rankings are merged into a consensus gold standard
of rankings. Next, two other computational baseline
methods; one based on simple sorting, and another
based on principal component analysis (Jolliffe 2002),
are used to generate alternative computational rankings
shown in Fig. 12.

In addition, the data for the violence/non-violence are
gathered using a separately developed tool, by collecting
the opinion of the experts. A future work will also include
automated generation of this dimension, as well.
3.5 Feature extraction

Fig. 3 A model of the system architecture

â

â

hypothesized scale. Social scientists screen the list of
extracted keywords, and select the relevant ones for
inclusion in further analysis.
The system builds response tables; a pair of tables for a
bipolar scale (such as radical/counter-radical R/CR), or
a single table for a unipolar scale, by thresholding the
occurrence frequencies of the selected keywords in
the organizationsâ web corpus. See Figs. 4 and 5 for the
response tables for the R/CR scale.
The response tables are fed as input to the Rasch Model
building algorithm. The algorithm produces a metric to
validate the fitness of the model, and rankings of the
organizations and keywords. Figures 6 and 7 show the
relative positions of the organizations and keywords on
the latent scales. The algorithm also produces a metric
to validate the fitness of the model.

After identifying the keywords for the analysis, we
needed to search the web site corpus of the organizations
for the matching items. This yielded a term-document
matrix.
This task was performed in a simple three-step procedure; initially, the occurrence frequencies of particular
keywords were counted within each organizationâs corpus, then, a threshold matrix was calculated from the
initial values, and finally, a binary response matrix was
generated by applying these thresholds to the initial
values.
The frequency metric is shown in formula 8, where k is
the keyword, o is the organization, and Do is the document
set pertaining to that particular organization.
fo;k Â¼

jfdjk 2 d; d 2 Do gj
jDo j

Ã°8Ã

A threshold value for each keyword is calculated by taking
the median of the values in the related column. Median was
preferred over mean as a threshold, since the distribution of
the values did not fit Gaussian distribution, yet median
empirically proved to be a better measure.

Fig. 4 Radical subset of
organizations and keywords,
sorted according to aggregate
row values

123

S. Tikves et al.
Fig. 5 Counter-radical subset
of organizations and keywords,
sorted according to aggregate
row values

Fig. 6 Radical subset of organizations and keywords

Finally, each element was converted into a binary value by
comparing it to the columnâs threshold. English translations of
the keywords are presented for clarity in Figs. 4 and 5.
3.6 Model fitting
We fit the Rasch model on two datasets: (1) radical organizations with radical keywords and (2) counter-radical
organizations with counter-radical keywords. We used the
eRm package in R, an open source statistical software
package,4 to fit a Rasch model to the dataset, and obtain the
organizationsâ scores on the latent scale, which are the the
subject parameter estimates (hv) discussed in the previous
section. The eRm package5 fits Rasch models and provide
subjects or organizations parameter estimates based on
maximum likelihood estimation.
4
5

http://www.cran.r-project.org/.
http://www.r-forge.r-project.org/projects/erm/.

123

The automated scale of the organizations is formed by
ranking the organizations according to their estimates on
the latent scale. Not only we can provide the organization
estimates but we can also assess whether the model fits the
data by looking at several goodness of fit indices, such as
the Andersenâs LR-test.
3.7 Application services
We use two backend services in the application layer to
present the data to the user interface. First, all the extracted
textual information are stored in Apache Solr,6 providing
facilities like full-text search and faceting (Tunkelang
2009), using an AJAX interface. In addition, a WCF-based
scaling service is used to infer scales in real time. This
particular service loads the response table, and the previously generated scale data, and estimates the R/CR scale
6

http://www.lucene.apache.org/solr/.

A system for ranking organizations using social scale analysis

Fig. 7 Counter-radical subset of organizations and keywords

â

It would be preferable to plot the locations on the same
range as the input collection. However, the Rasch scale
is on a latent range (Figs. 8, 9).

We resolve the first issue by uniformly scaling the
ranges into [-10, 10], making it consistent with the inputs.
The second issue requires a more specific solution. We
make use of the fact that the raw person scores pertaining
to number of positive responses is a sufficient statistics for
the Rasch model (G 1961) to estimate scale values on the
fly. Since we know the date range, and the selected organizations currently visible in the user interface, it is possible to quickly generate a response matrix for this subset
of the data, and merge it with the previously known scale
information to generate interpolated scale values.

0

2

Radical Scale

â2

The user interface is responsible for representing our
input data, and the findings to the experts in an interactive fashion. Users should be in control of the selection
of the data displayed, and filtering with organization
names, or a specific date range, or using other parameters
such as arbitrary keywords, or geographic locations.
While performing these tasks, it should provide results to
the user with a minimum of delay, allowing quick drilling down to interactively model the scenarios that users
have in mind.
The user interface is implemented as an interactive
AJAX-based application, using ajaxsolr7 framework. In
addition to the search and navigation capabilities provided
with ajaxsolr, it also adds functional widgets for visualizing
the organizations on a scale, mapping the intensity of the
locations, displaying demographics trends, and so on. A
more detailed discussion of the user interface is provided in
Sect. 5.
The presentation of the scale, however, brings the following challenges.

Since this will be an interactive application, users
would prefer to see almost instantaneous results. Yet,
the eRm model generation is computationally
expensive.

â4

3.8 User interface

â

Person Parameters (Theta)

for a subset of the input. Number of positive responses are
interpolated on the scale to generate the scale, and the
expert opinion is used for a static violent/non-violent
(V/NV) scale. While the interpolation is based on a sufficient statistics, future work on speeding up Rasch model
generation for real-time use would be beneficial.

0

5

10

15

20

25

30

Person Raw Scores
7

http://www.evolvingweb.github.com/ajax-solr/.

Fig. 8 Radical scale

123

S. Tikves et al.

Here we have opted to include all the organizations in
threshold calculations. This is because, the radical or
counter-radical activity intensities are always measured
relative to the other organizations participating in the same
time period. However, while the scale is based on all the
organizations, only the ones specifically asked will be
presented to the user.

4 Experimental evaluation
4.1 Indonesian corpus
The corpus domain is the online articles published by the
web sites of the 23 religious organizations identified in
Indonesia, in the Indonesian language. These sources are
the web sites or blogs of the identified think tanks and
organizations. As discussed in the Sect. 1, each source
was classified as either radical or counter-radical by the
area experts. We downloaded a total of 37,000 Indonesian articles published in these 23 web sites, dating from
2001 to 2011. For each web site, a specific REGEX filter
was used to strip off the headers, footers, advertising
sections and to extract the plain text from the HTML
code.
The psuedo-code for the subset scale-generation procedure is presented in Algorithm 1. The process starts with
identifying the subset of documents in the (start, end) date
range (lines 2â5). Then the keyword frequencies, and
thresholds are calculated for the entire set of organizations
on this document subset (lines 6â14). Finally, response
tables for the subset of organizations is generated (lines
15â17), and then the sums need to be interpolated (lines
18â23), to be able to generate a scale on the [-10,10] range
(line 24).

2
0
â2
â4

Person Parameters (Theta)

CounterâRadical Scale

0

5

10

15

Person Raw Scores

Fig. 9 Counter-radical scale

123

20

25

4.2 The quadrants model
Our project leverages the results of our previous work,
which relied on social theory including Durkheimâs (2004)
research on collective representations, Simmelâs (2008)
work on conflict and social differentiation, Wallaceâs
(1956) writings on revitalization movements, and Tilly and
Bayatâs studies on contemporary social movement theory
(Tilly 2004; Bayat 2007). Our team has also developed,
and is currently testing a theoretically based class model
comprised of continuous latent scales. The first pair of
scales focus on distinctions between the goals and methods
of counter-radical and radical discourse, and capture the
degree to which individuals, groups, and behaviors aim to
influence the social order (change orientation) and the
methods by which they attempt to do so (change
strategies).
Quadrants model (see Fig. 10) captures multiple social
trends in four quadrants (A, B, C, and D), and it makes the
significant distinction between violent and not-violent
dimensions of both radicalisms and counter radicalisms.
Using the quadrants model, a researcher can locate organizations, individuals, and discourses in broader categories
while still considering subtle differences between groups
within categories. A researcher can document movement
and trends from category to category, and identify points
where movement is likely to happen.

A system for ranking organizations using social scale analysis

rankings. The individual scores for each organization were
combined and averaged to obtain the consensus gold
standard rankings along the hypothesized R/CR scale.
A work is in progress for building a publicly accessible
expert opinion collection toolkit. The preliminary version
can be accessed at: http://www.minerva-project.org/
DataCollector.
4.4 Computationally generated scale

Fig. 10 The quadrants model

4.3 Expert opinion and gold standard of rankings
We collaborated with three area experts, who collectively
possess 35 years of scholarly expertise on Indonesia and
Islam. To build a gold standard of orderings of the organizations, we built a graphical drag-and-drop user-interface
tool to collect the opinions of each of the area experts.
A screenshot of the tool is shown in Fig. 11.
Each expert, separately evaluated and ranked the organizations in the dataset according to a two dimensional
scale of radical/counter-radical (R/CR) and violent/nonviolent (V/NV) axis. The consensus among the experts was
high; since per item standard deviations among the expertsâ
scores along the R/CR axis over a range of [-10,
10], across all organizations were 2.75. In addition, 90 %
of the items have less than 22.6 % difference in their

The ranking discovered by the Rasch model fitting the corpus
has been evaluated against the gold standard rankings of the
organizations provided by the experts. The difference
between two separate rankings have been calculated using
the following misplacement error measure in Eq. (9).
P
jGÃ°oÃRÃ°oÃj
errorÃ°G; RÃ Â¼

o2O

jOj

jOj

Ã°9Ã

Here, O is the set of organizations, G and R are one to one
mapping functions of rankings from set O to range [1,|O|].
For two exactly matching rankings, the error(G, R) will be
zero, whereas for two inversely sorted rankings it is
expected to be 0.5 (when the size of O is even). In addition,
a random ranking is expected to have a error of 0.375.
4.5 Expert-to-gold standard error
We calculated the error between each expertâs ranking and
their consensus gold standard of rankings. The first expertâs
error measure is 0.06, and the second and third expertâs
errors are 0.12 and 0.14 correspondingly as shown in the
last row of the table in Fig. 12. The average error of our
experts against their gold standard ranking is 0.11.
4.6 Baseline: sorting with aggregate score
The first baseline we used was constructed by sorting the
organizations according to the number of different keywords
observed in their corpus. While this provided a pattern
similar to a Guttman scale, and orderings of the organizations
matched to a certain degree with the gold standard as shown
in Fig. 12, the error for this baseline was 0.19, which is
higher than the average expertâs performance.
4.7 Baseline: principal component analysis

Fig. 11 The visual interface of the expert-opinion collector for
manually placing the organizations on the two dimensional scale

A stronger baseline was built by employing principal
component analysis (Jolliffe 2002), and sorting the organizations according to their projections in the first principal
component of the termâdocument matrix. Since experts
selected the R/CR scale relevant keywords only, it was
expected that the first principal component would reflect
the corresponding scale. PCA proved to be performing

123

S. Tikves et al.
Fig. 12 Computational and
expert rankings

better than the aggregate score sorting, with an error
measure of 0.18. However, this error rate is still higher than
the error rate of each expert.

information and patterns to enable a computational method
to rank them accurately.

4.8 Performance of the Rasch model ranking system

5 Web application overview

The p values from the Anderson LR goodness of fit test from
model (1) and model (2) (mentioned in Sect. 3.6) are 0.85 and
0.669, respectively, suggesting no evidence of lack of fit. The
Rasch models allow us to get a natural order of the organizations, according to their ââabilitiesââ, i.e., radicalism and
counter-radicalism in this case. This system had an error
measure of 0.10, which actually provided a higher ranking
performance than the average performance of our expertsââ
performing better than the majority of our area experts.

A sample snapshot of the web application can be seen in
Fig. 13. It is composed of four main widgets for visualization and navigation. The top-left section which contains
the Search and Navigation widget (1) that allows filtering
of the document subset using parametric search queries and
keyword based search criteria. The top-right section is the
Quadrant widget (2) which displays the organizations
active in the currently selected time frame on a twodimensional axis, using violence and radicalism scales. The
bottom-left section consists of two Treemap widgets (3)
which displays the demographics and the top keywords
(markers) of the current selection. The bottom-right section
has a Timeline widget (4) which provides a visualization
of the keywords (markers) trends on a time line.
The navigation in the user interface starts with the
Navigation widget (top-left) of the web application. Here
the user is able to filter down the corpus utilizing full-text
search queries, or faceting using keywords, locations,
demographics, or choosing a subset of organizations.
The Quadrant widget (top-right) provides a plot of
the currently selected organizations on the two dimensional scale. The radical/counter-radical (R/CR) axis is

4.9 Evaluations
Our experiments showed that the hypothesized compatibility
of the R/CR scale for the Indonesian corpus is valid. Not only
the Rasch model was statistically fitting the response matrix
but also the generated ranking performance was better than
the average expert performance. Among our computational
baseline methods, the Rasch Model was the only method
producing expert-level performance as shown in Fig. 12.
This preliminary analysis with the R/CR scale shows that
when experts assist the system with keyword selection, the
web corpus of organizations provides rich-enough

123

A system for ranking organizations using social scale analysis

Fig. 13 A sample snapshot of the web application (color figure online)

dynamically calculated in real time, using the subset of
organizations, and the time range of the current selection.
The location change on the time range for each organization is shown as a color-coded path, with three markers, a
light circle corresponding to the position at the beginning
of the period, a dark circle corresponding to the end of the
period, and a dark-small circle for the middle. A red line
between the circle denotes the rise of radical activities in
the organizationâs behavior. A blue line denotes the
opposite. The smaller circle is useful to see the overall
movement of an organization. For example, between the
range Aug 2005 and Aug 2007, EraMuslimâs activities
were radical (center of A quadrant), then became almost
counter radical (the smaller circle denotes this mid point in
the movement), and then jumped up again. The V/NV axis
is retrieved from expert opinion in the current version, and
dynamic calculation of this axis is left for a future version.
The Timeline widget (bottom-right) displays the trends of
the most frequent markers on a time line. Initially the subset
of markers presented defaults to all available, however it is
possible to restrict the selection of markers to a more limited
set among radical/counter-radical, economical, political,
religious, or social domains. Timeline widget can also be
used for selecting a date range of interest.
The Treemap widgets (bottom-left) are used to display
the relative frequencies of demographics and keywords

(markers). The displayed marker category selection for this
widget is synchronized with the Timeline widget.
In the following sections, we present some scenarios and
findings to illustrate the capabilities of the web interface.
5.1 Scenario 1: radical organizationsâ trends
In this scenario, we analyze both violent and non-violent
radical organizations. Our web application shows the ideologies that these organizations are propagating. We can
see8 the most prominent markers associated with these
radical organizations. Markers such as ââinfidelââ, ââShariaââ,
and ââviolenceââ show an increasing trend between 2001 and
2011. A very strict interpretation of ââShariaââ is used by
radical organizations to justify their actions (Widhiarto
2010; Hasan 2009). ââShariaââ peaks during this period as
shown in Fig. 14.
5.2 Scenario 2: C-quadrant organizationsâ trends
We now analyze Front Pembela Islam (FPI), an Islamic
organization in Indonesia established in 1998. FPI is well
known for its violent acts (Frost et al. 2010; Rondonuwu
8

Select the filter ââRadicalââ from the search options and then in the
Markers Menu select [Religious ! Radical Markers].

123

S. Tikves et al.

Fig. 14 Trend of radical markers

2010) justified by a strict interpretation of Sharia (for the
Study of Terrorism 2011). Our documents for FPI ranges
between 2000 and 2010. Using our web applicationâs plots of
the movement of FPI in the C Quadrant, we found that FPI
consistently rised higher on the radical scale as shown in
Fig. 15. We selected the following time ranges, 2000â2003,
2002â2006, 2006â2010 and analyzed the trends of various
markers associated with FPI. There was a substantial increase
in the intensity of various radical markers such as ââinfidelââ,
ââMujahedinââ, ââpornographyââ.9 Since 2006, we also saw a
steep increase in the frequency of marker ââAhmadiyyaââ, as
shown in Fig. 16, which indicates FPIâs increased opposition
to this heretical sect (Rahmat and Sihaloho 2011).

Fig. 15 Consistent rise of FPI on the radical scale

5.3 Scenario 3: A-quadrant organizationsâ trends
We analyze Hizb ut-Tahrir also known as Hizb ut-Tahrir
Indonesia (HTI), a radical organization widely believed to
be non-violent (Ward 2009), which has been active in
Indonesia since 1982 (Osman 2011). Between 2007 and
2009, our web application shows various radical and nonradical markers associated with this organization.

Radical

Non-Radical

ââShariaââ, ââInfidelââ,
ââCaliphââ, ââViolenceââ

ââPoliticsââ, ââIndonesian Islamââ,
ââElectionââ, ââLiberalââ,
ââDemocracyââ

During the same period, we see a steady increase in the
frequency of the radical marker ââShariaââ. This is consistent with one of HTIâs goals of implementing Sharia in
Indonesia (Hasan 2009). Hizb ut-Tahrir openly propagates
9

Select ââRadicalââ and ââFPIââ from the filters, then select the time
range 2002â2006 or 2006â2010, then select ââradicalââ markers under
ââR/CRââ menu.

123

Fig. 16 ââAhmadiyyaââ peaking during the period 2006â2010

Fig. 17 ââKhilafahââ ideology of Hizb ut-Tahrir

the ideology of Khilafah, which believes in unification of
all Muslim countries as a single Islamic State (Zakaria
2011; Mohamed Osman 2010). Figure 17 shows

A system for ranking organizations using social scale analysis

Searching for the text ââsuicide bombingââ, we see that
one of the related markers is ââideologyââ. Adding the
keyword ââideologyââ to the search filter reveals a new set of
markers including the ââsinââ keyword. Adding ââsinââ to our
search, we obtain a set of matching documents. One of the
top matches, is titled ââMengapa Saya Berubah?ââ (english
translation: ââWhy I changed?ââ)13. This article is by a
reformed terrorist, debunking the misinterpretation of the
jihad-related verses used by violent groups.

6 Conclusions and future work

Fig. 18 Decline of the HTI in the radical scale

ââKhilafahââ as the most prominent marker10 in Hizb utTahrirâs discourse.
By looking at the Quadrants widget (in Fig. 18), we can
infer that HTI has been moderating its narrative.
5.4 Scenario 4: B-quadrant organizationsâ trends
In this scenario, we discuss the trends of counter radical
organizations like NU and DaarulUluum. We also show an
interesting scenario on the topic of ââSuicide Bombingââ
using the keyword based Navigation widget.
The ââcounter radicalââ markers11 associated with these
organizations are: ââpoliticsââ, ââelectionââ, ââIndonesian
Islamââ, ââliberalââ, ââhuman rightsââ. These organizations
support democracy and elections, which is shown by the
high frequency of the markers ââpoliticsââ and ââelectionââ.
Their narrative has local interpretation of Islam at its core,
which is shown by the marker ââIndonesian Islamââ.
On analyzing the occurrences of radical markers12 in
B-Quadrant, we find that counter radical organizations are
very vocal against all of radical markers. One of the
interesting radical markers is ââSuicide Bombingââ. Most of
the counter radical organizations are against suicide
bombings.(Malang 2006). We will now demonstrate how
combination of parametric and keyword search, and various widgets in the web application can help reveal opposition to ââSuicide Bombingââ by counter-radical
organizations.
10

Select ââHizb ut-Tahrirââ and ââradicalââ from filters. Select the time
range 2007â2009. The markers can be seen by selecting the options of
Markers Menu [Religious ! Religious Markers].
11
Select CounterRadical filter in the search option, then from the
Markers Menu select [R/CR ! Counter Radical].
12
In the Markers Menu select [R/CR ! Radical].

In our experiments, not only did the data show fitness with
the Rasch Model for the R/CR scale but also the Rasch
rankings of the organizations are better than the output of
the other baseline computational methods, and they are at
expert-level performance when compared with the consensus gold standard rankings.
Rasch model also provided us with another output,
namely the ranking of selected keywords (items) on the
R/CR scale. Although preliminary observations indicates
that this can be a valuable asset by itself, we plan to further
investigate the quality and utility of this ranking as future
work.
While the model has been demonstrated to fit on the
R/CR scale, two major expansion points can be investigated in the future work, namely the violent/non-violent
scale, and enhancement of feature selection. Although our
experts have identified a second dimension, evaluating its
correlation to R/CR axis, or existence of other significant
ones could be beneficial. In addition, the features can be
enhanced by experimenting with the significance of the
radical keywords in the counter-radical organization corpora, and vice-versa.
A practical method to increase the automation of keyword generation has been discussed in Sect. 3.4. Future
work will involve finding an efficient approximation
algorithm for this model, for decreasing the necessity of
expert interaction for this particular step.
Other interesting work includes making our expert
opinion elicitation tool available online to a wider and
more geographically distributed audience to crowdsource
(Snow et al. 2008) the needed expertise for making lists of
local organizations, identifying their web sources, and
overcome the complex task of construction and validation
of significant and fitting scales (work is currently underway
to build this tool). Another interesting dimension is to look
at synthesis and analysis of scales that do have a strict
hierarchy of keywords, but adhere to more flexible partial
order models (James and John 2002).
13

http://www.islamlib.com/id/artikel/mengapa-saya-berubah/.

123

S. Tikves et al.
Acknowledgments This research was supported by US DoDs
Minerva Research Initiative Grant N00014-09-1-0815, Project leader:
Prof. Mark Woodward, Arizona State University, and the project title
is ââFinding Allies for the War of Words: Mapping the Diffusion and
Influence of Counter-Radical Muslim Discourseââ.

References
Andrich D (1988) Rasch models for measurement. Sage, USA
Bayat A (2007) Making Islam Democratic: social movements and the
post-Islamist turn. Stanford University Press, USA
Crelinsten R (2002) Analysing terrorism and counter-terrorism: a
communication model. Terror Political Violence 14:77â122
Davulcu H, Ahmed ST, Gokalp S, Temkit MH, Taylor T, Woodward
M, Amin A (2010) Analyzing sentiment markers describing
radical and counter-radical elements in online news. In:
Proceedings of the 2010 IEEE second international conference
on social computing, IEEE Computer Society, SOCIALCOMâ10, pp 335â340
Drehmer D, Belohlav J, Coye R (2000) An exploration of employee
participation using a scaling approach. Group Org Manage
25(4):397
Durkheim E (2004) The cultural logic of collective representations:
social theory the multicultural and classic readings. Wesleyan
University: Westview Press
Frost F, Rann A, Chin A (2010) Terrorism in southeast asia.
http://www.aph.gov.au/library/intguide/FAD/sea.html [Online
accessed 21 Nov 2011]
Guttman L (1950) The basis for scalogram analysis. Meas Predict
4:60â90
Hasan N (2009) Islamic militancy, Sharia, and democratic consolidation in post-Suharto Indonesia. RSIS Working Papers. 143/07
Hessen D (2010) Likelihood ratio tests for special Rasch models.
J Edu Behav Stat 35(6):611
Hunter D, Lange K (2004) A tutorial on mm algorithms. Am Stat
58(1):30â37
James AW, John LM (2002) Algebraic representations of beliefs and
attitudes: partial order models for item responses. Sociol
Methodol 29:113â146
Jolliffe I (2002) Principal component analysis: Springer series in
statistics. Springer, Germany
Le Cam L (1990) Maximum likelihood an introduction. ISI Rev
58(2):153â171
Likert R (1932) A technique for the measurement of attitudes. Arch
Psychol 140:1â55
Malang (2006) NU chairman deplores suicide bombing attempt.
http://www.nu.or.id/page/en/dinamic_detil/15/28282/News/NU_
chairman_deplores_suicide_bombing_attempt.html [Online
accessed 22 Nov 2011]
McIver J, Carmines E (1981) Unidimensional scaling, vol 24. Sage
Publications Inc, USA
McPhee RD, Corman S (1995) An activity-based theory of communication networks in organizations, applied to the case of a local
church. Commun Monogr 62:1â20
Michael WB, Kogan J (2010) Text mining: applications and theory.
Wiley, London
Mohamed Osman MN (2010) Reviving the Caliphate in the
Nusantara: Hizbut Tahrir Indonesiaâs mobilization strategy and
its impact in Indonesia. Terror Political Violence 22(4):601â622.

123

doi:10.1080/09546553.2010.496317.
http://www.tandfonline.
com/doi/abs/10.1080/09546553.2010.496317
Osman MNM (2011) Preparing for the caliphate. Asian Stud Assoc
Aust E-Bull 80:14â16. ISSN:1449-4418
Pawitan Y (2001) In all likelihood: statistical modelling and inference
using likelihood. Oxford University Press, USA
Rahmat Sihaloho M (2011) FPI vows to disband ahmadiyah
âwhatever it takesâ. http://www.thejakartaglobe.com/home/fpivows-to-disband-ahmadiyah-whatever-it-takes/423477 [Online
accessed 21 Nov 2011]
Rasch G (1961) On general laws and the meaning of measurement in
psychology. In: Proceedings of the fourth Berkeley symposium
on mathematical statistics and psychology, 4, p 332
Rondonuwu O, Creagh S (2010) Opposition grows to indonesiaâs
hardline fpi islamists. http://www.in.reuters.com/article/
2010/06/30/idINIndia-49777620100630 [Online accessed 21
Nov 2011]
Salton G, Buckley C (1988) Term-weighting approaches in automatic
text retrieval. In: Information Processing and Management, vol
25, pp 513â523
Simmel G (2008) Sociological theory. McGraw-Hill, New York
Snow R, OâConnor B, Jurafsky D, Ng AY (2008) Cheap and fastâ
but is it good?: evaluating non-expert annotations for natural
language tasks. In: Proceedings of the conference on empirical
methods in natural language processing, EMNLP â08,
pp 254â263. Association for Computational Linguistics, Stroudsburg, PA, USA. http://www.portal.acm.org/citation.cfm?
id=1613715.1613751
for the Study of Terrorism NC, to Terrorism R (2011) Terrorist
organization profile: front for defenders of Islam. http://www.
start.umd.edu/start/data_collections/tops/terrorist_organization_
profile.asp?id=4026 [Online accessed 21 Nov 2011]
Thurstone LL (1928) Attitudes can be measured. Am J Sociol
33:529â554
Tikves S, Banerjee S, Temkit H, Gokalp S, Davulcu H, Sen A,
Corman S, Woodward M, Rochmaniyah I, Amin A (2011) A
system for ranking organizations using social scale analysis. In:
EISIC. IEEE, pp 308â313. http://www.ieeexplore.ieee.org/xpl/
mostRecentIssue.jsp?punumber=6059524
Tilly C (2004) Social Movements. Paradigm Publishers, USA
Tunkelang D (2009) Faceted Search: synthesis lectures on information concepts, retrieval, and services. Morgan & Claypool
Publishers, UK. doi:10.2200/S00190ED1V01Y200904ICR005
van Emde Boas P (1981) Another NP-complete partition problem and
the complexity of computing short vectors in a lattice: Tech.
Rep. 81-04. Mathematisch Instituut, Amsterdam
Wallace A (1956) Revitalization movements. Am Anthropol
58:264â281
Ward K (2009) Non-violent extremists? Hizbut Tahrir Indonesia.
Aust J Intl Affairs 63(2):149â164 doi:10.1080/1035771090
2895103.http://www.tandfonline.com/doi/abs/10.1080/10357710
902895103
Widhiarto H (2010) Radical groups urge Bekasi administration to
implement Sharia law. http://www.thejakartapost.com/news/2010/
06/27/radical-groups-urge-bekasi-administration-implementsharia-law.html [Online accessed 21 Nov 2011]
Zakaria Y (2011) A Global Caliphate: reality or fantasy? http://www.
usa.mediamonitors.net/content/view/full/91207 [Online accessed
21 Nov 2011]

Identification of K Most Vulnerable Nodes in
Multi-layered Network Using a New Model of
Interdependency
Arunabha Sen, Anisha Mazumder, Joydeep Banerjee, Arun Das and Randy Compton
Computer Science and Engineering Program

arXiv:1401.1783v1 [cs.NI] 8 Jan 2014

School of Computing, Informatics and Decision System Engineering
Arizona State University
Tempe, Arizona 85287
Email: {asen, amazumde, Joydeep.Banerjee, adas22, Randy.Compton}@asu.edu

AbstractâThe critical infrastructures of the nation including
the power grid and the communication network are highly
interdependent. Recognizing the need for a deeper understanding
of the interdependency in a multi-layered network, significant
efforts have been made by the research community in the last
few years to achieve this goal. Accordingly a number of models
have been proposed and analyzed. Unfortunately, most of the
models are over simplified and, as such, they fail to capture the
complex interdependency that exists between entities of the power
grid and the communication networks involving a combination of
conjunctive and disjunctive relations. To overcome the limitations
of existing models, we propose a new model that is able to
capture such complex interdependency relations. Utilizing this
model, we provide techniques to identify the K most vulnerable
nodes of an interdependent network. We show that the problem
can be solved in polynomial time in some special cases, whereas
for some others, the problem is NP-complete. We establish that
this problem is equivalent to computation of a fixed point of a
multilayered network system and we provide a technique for its
computation utilizing Integer Linear Programming. Finally, we
evaluate the efficacy of our technique using real data collected
from the power grid and the communication network that span
the Maricopa County of Arizona.

I. I NTRODUCTION
In the last few years there has been an increasing awareness
in the research community that the critical infrastructures of
the nation are closely coupled in the sense that the well being
of one infrastructure depends heavily on the well being of another. A case in point is the interdependency between the electric power grid and the communication network. The power
grid entities, such as the SCADA systems that control power
stations and sub-stations, receive their commands through
communication networks, while the entities of communication
network, such as routers and base stations, cannot operate
without electric power. Cascading failures in the power grid,
are even more complex now because of the coupling between
power grid and communication network. Due to this coupling,
not only entities in power networks, such as generators and
transmission lines, can trigger power failure, communication
network entities, such as routers and optical fiber lines, can
also trigger failure in power grid. Thus it is essential that
the interdependency between different types of networks be
understood well, so that preventive measures can be taken to

avoid cascading catastrophic failures in multi-layered network
environments.
Recognizing the need for a deeper understanding of the
interdependency in a multi-layered network, significant efforts
have been made in the research community in the last few
years to achieve this goal [1], [2], [3], [4], [5], [6], [7], [8].
Accordingly a number of models have been proposed and
analyzed. Unfortunately, many of the proposed models are
overly simplistic in nature and as such they fail to capture
the complex interdependency that exists between power grid
and communication networks. In a highly cited paper [1], the
authors assume that every node in one network depends on one
and only one node of the other network. However, in a follow
up paper [2], the same authors argue that this assumption may
not be valid in the real world and a single node in one network
may depend on more than one node in the other network. A
node in one network may be functional (âaliveâ) as long as
one supporting node on the other network is functional.
Although this generalization can account for disjunctive
dependency of a node in the A network (say ai ) on more
than one node in the B network (say, bj and bk ), implying
that ai may be âaliveâ as long as either bi or bj is alive,
it cannot account for conjunctive dependency of the form
when both bj and bk has to be alive in order for ai to
be alive. In a real network the dependency is likely to be
even more complex involving both disjunctive and conjunctive
components. For example, ai may be alive if (i) bj and bk and
bl are alive, or (ii) bm and bn are alive, or (iii) bp is alive. The
graph based interdependency models proposed in the literature
[3], [4], [5], [9], [6], [7] including [1], [2] cannot capture
such complex interdependency between entities of multilayer
networks. In order to capture such complex interdependency,
we propose a new model using Boolean logic. Utilizing this
comprehensive model, we provide techniques to identify the
K most vulnerable nodes of an interdependent multilayered
network system. We show that the this problem can be solved
in polynomial time for some special cases, whereas for some
others, the problem is NP-complete. We also show that this
problem is equivalent to computation of a fixed point [10] and
we provide a technique utilizing Integer Linear Programming

2

to compute that fixed point. Finally, we evaluate the efficacy
of our technique using real data collected from power grid
and communication networks that span Maricopa County of
Arizona.

II. I NTERDEPENDENCY M ODEL
We describe the model for an interdependent network with
two layers. However, the concept can easily be generalized
to deal with networks with more layers. Suppose that the
network entities in layer 1 are referred to as the A type
entities, A = {a1 , . . . , an } and entities in layer 2 are referred
to as the B type entities, B = {b1 , . . . , bm }. If the layer 1
entity ai is operational if (i) the layer 2 entities bj , bk , bl
are operational, or (ii) bm , bn are operational, or (iii) bp
is operational, we express it in terms of live equations of
the form ai â bj bk bl + bm bn + bp . The live equation for
a B type entity br can be expressed in a similar fashion
in terms of A type entities. If br is operational if (i) the
layer 1 entities as , at , au , av are operational, or (ii) aw , az
are operational, we express it in terms of live equations of
the form br â as at au av + aw az . It may be noted that the
live equations only provide a necessary condition for entities
such as ai or br to be operational. In other words, ai or br
may fail independently and may be not operational even when
the conditions given by the corresponding live equations are
satisfied. A P
live equation
in general will have the following
Ti Qtj
form: xi â j=1
y
k=1 j,k where xi and yj,k are elements
of the set A (B) and B (A) respectively, Ti represents the
number of min-terms in the live equation and tj refers to the
size of the j-th min-term (the size of a min-term is equal to the
number of A or B elements in that min-term). In the example
ai â bj bk bl + bm bn + bp , Ti = 3, t1 = 3, t2 = 2, t3 = 1,
xi = ai , y2,1 = bm , y2,2 = bp .
We refer to the live equations of the form ai â bj bk bl +
bm bn + bp also as First Order Dependency Relations, because
these relations express direct dependency of the A type entities
on B type entities and vice-versa. It may be noted however
that as A type entities are dependent on B type entities,
which in turn depends on A type entities, the failure of
some A type entities can trigger the failure of other A type
entities, though indirectly, through some B type entities. Such
interdependency creates a cascade of failures in multilayered
networks when only a few entities of either A type or B type
(or a combination) fails. We illustrate this with the help of
an example. The live equations for this example is shown in
table I.
Power Network
a1 â b1 + b2
a2 â b1 b3 + b2
a3 â b1 b2 b3
a4 â b1 + b2 + b3

Communication Network
b1 â a1 + a2 a3
b2 â a1 + a3
b3 â a1 a2
ââ

TABLE I: Live equations for a Multilayer Network

Entities
a1
a2
a3
a4
b1
b2
b3

t0
1
0
0
0
0
0
0

t1
1
0
0
0
0
0
1

Time Steps
t2
t3
t4
1
1
1
0
0
1
1
1
1
0
0
1
0
1
1
0
1
1
1
1
1

t5
1
1
1
1
1
1
1

t6
1
1
1
1
1
1
1

TABLE II: Time Stepped Cascade Effect for a Multilayer Network

Fig. 1: Cascading failures reach steady state after p time steps

As shown in table II, the failure of only one entity a1 at
time step t0 triggered a chain of failures that resulted in the
failure of all the entities of the network after by timestep t4 .
A table entry of 1 indicates that the entity is âdeadâ. In this
example, the failure of a1 at t0 triggered the failure of b3 at
t1 , which in turn triggered the failure of a3 at t2 . The failure
of b3 at t1 was due to the dependency relation b3 â a1 a2
and the failure of a3 at t2 was due to the dependency relation
a3 â b1 b2 b3 . The cascading failure process initiated by failure
(or death) of a subset of A type entities at timestep t0 , A0d and
a subset of B type entities Bd0 till it reaches its final steady
state is shown diagrammatically in figure 1. Accordingly, a
multilayered network can be viewed as a âclosed loopâ control
system. Finding the steady state after an initial failure in this
case is equivalent of computing the fixed point of a function
F(.) such that F(Apd âª Bdp ) = Apd âª Bdp , where p represents
the number of steps when the system reaches the steady state.
We define a set of K entities in a multi-layered network
as most vulnerable, if failure of these K entities triggers the
failure of the largest number of other entities. The goal of
the K most vulnerable nodes problem is to identify this set of
nodes. This is equivalent to identifying A0d â A, Bd0 â B, that
maximizes |Apd âªBdp |, subject to the constraint that |A0d âªBd0 | â¤
K.
The dependency relations (live equations) can be formed
either after careful analysis of the multilayer network along the
lines carried out in [8], or after consultation with the engineers
of the local utility and internet service providers.
III. C OMPUTATIONAL C OMPLEXITY AND A LGORITHMS
Based on the number and the size of the min-terms in the
dependency relations, we divide them into four different cases
as shown in Table III. The algorithms for finding the K most
vulnerable nodes in the multilayer networks and computation
complexity for each of the cases are discussed in the following
four subsections.
Case
Case I
Case II
Case III
Case IV

No. of Min-terms
1
1
Arbitrary
Arbitrary

Size of Min-terms
1
Arbitrary
1
Arbitrary

TABLE III: Equation Types for Dependency Relations

3

A. Case I: Problem Instance with One Min-term of Size One
In this case, a live equation in general will have the following form: xi â yj where xi and yj are elements of the set A
(B) and B (A) respectively. In the example ai â bj , xi = ai ,
y1 = bj . It may be noted that a conjunctive implication of
the form ai â bj bk can also be written as two separate
implications ai â bj and ai â bk . However, such cases are
considered in Case II and is excluded from consideration in
Case I. The exclusion of such implications implies that the
entities that appear on the LHS of an implication in Case I
are unique. This property enables us to develop a polynomial
time algorithm for the solution of the K most vulnerable node
problem for this case. We present the algorithm next.
Algorithm 1
Input: (i) A set S of implications of the form of y â x,
where x, y â A âª B, (ii) An integer K.
Output: A set V 0 where |V 0 | = K and V 0 â A âª B such
that failure of entities in V 0 at time step t0 results in failure
of the largest number of entities in A âª B when the steady
state is reached.
Step 1. We construct a directed graph G = (V, E), where
V = A âª B. For each implication y â x in S, where x, y â
A âª B, we introduce a directed edge (x, y) â E.
Step 2. For each node xi â V , we construct a transitive
closure set Cxi as follows: If there is a path from xi to some
node yi â V in G, then we include yi in Cxi . It may be
recalled that |A| + |B| = n + m. So, we get n + m transitive
closure sets Cxi , 1 â¤ i â¤ (n + m). We call each xi to be the
seed entity for the transitive closure set Cxi .
Step 3. We remove all the transitive closure sets which are
proper subsets of some other transitive closure set.
Step 4. Sort the remaining transitive closure sets Cxi ,
where the rank of the closure sets is determined by the
cardinality of the sets. The sets with a larger number of entities
are ranked higher than the sets with a fewer number of entities.
Step 5. Construct the set V 0 by selecting the seed entities
of the top K transitive closure sets. If the number of remaining
transitive closure sets is less than K (say, K0 ), arbitrarily select
the remaining entities.
Time complexity of Algorithm 1: Step 1 takes O(n + m + |S|)
time. Step 2 can be executed in O((n+m)3 ) time. Step 3 takes
at most O((n + m)2 ) time. Step 4 sorts at most |S| entries, a
standard sorting algorithm takes O(|S| log |S|) time. Selecting
K entities in step 5 takes O(K) time. Since |S| â¤ n+m, hence
the overall time complexity is O((n + m)3 )
Theorem 1. For each pair of transitive closure sets Cxi and
Cxj produced in step 2 of algorithm 1, either Cxi â© Cxj = â
or Cxi â© Cxj = Cxi or Cxi â© Cxj = Cxj , where xi 6= xj .
Proof: Consider, if possible, that there is a pair of transitive
closure sets Cxi and Cxj produced in step 2 of algorithm 1,
such that Cxi â©Cxj 6= â and Cxi â©Cxj 6= Cxi and Cxi â©Cxj 6=

Cxj . Let xk â Cxi â© Cxj . This implies that there is a path
from xi to xk (path1 ) as well as there is a path from xj to xk ,
(path2 ). Since, xi 6= xj and Cxi â©Cxj 6= Cxi and Cxi â©Cxj =
Cxj , there is some xl in the path1 such that xl also belongs to
path2 . W.l.o.g, let us consider that xl be the first node in path1
such that xl also belongs to path2 . This implies that xl has
in-degree greater than 1. This in turn implies that there are two
implications in the set of implications S such that xl appears in
the L.H.S of both. This is a contradiction because this violates
a characteristic of the implications in Case I. Hence, our initial
assumption was wrong and the theorem is proven.
Theorem 2. Algorithm 1 gives an optimal solution for the
problem of selecting K most vulnerable entities in a multilayer network for case I dependencies.
Proof: Consider that the set V 0 returned by the algorithm is
not optimal and the optimal solution is VOP T . Let us consider
there is a entity xi â A âª B such that xi â VOP T \ V 0 .
Evidently, (i) Cxi was either deleted in step 3 or (ii) |Cxi | is
less than the cardinalities of all the transitive closure sets with
seed entities xj â V 0 , because our algorithm did not select
xi . Hence, in both cases, replacing any entity xj â V 0 by xi
reduces the total number of entities killed. Thus, the number
of dead entities by the failure of entities in VOP T is lesser than
that caused by the failure of the entities in V 0 , contradicting
the optimality of VOP T . Hence, the algorithm does in fact
return the optimal solution.
B. Case II: Problem Instance with One Min-term of Arbitrary
Size
In this case, a liveQ equation in general will have the
q
following form: xi â k=1 yj where xi and yj are elements
of the set A (B) and B (A) respectively, q represents the size
of min-term. In the example ai â bj bk bl , q = 3, xi = ai ,
y1 = bj , y2 = bk , y3 = bk .
1) Computational Complexity: We show that computation
of K most vulnerable nodes (K-MVN) in a multilayer network
is NP-complete in Case II. We formally state the problem next.
Instance: Given a set of dependency relations between
A
Qqand B type entities in the form of live equations xi â
k=1 yj , integers K and L.
Question: Is there a subset of A and B type entities of
size at most K whose âdeathâ (failure) at time t0 , triggers a
cascade of failures resulting in failures of at least L entities,
when the steady state is reached?
Theorem 3. The K-MVN problem is NP-complete.
Proof: We prove that the K-MVN problem is NP-complete
by giving a transformation for the vertex cover (VC) problem.
An instance of the vertex cover problem is specified by an
undirected graph G = (V, E) and an integer R. We want to
know if there is a subset of nodes S â V of size at most
R, so that every edge has at least one end point in S. From
an instance of the VC problem, we create an instance of the

4

K-MVN problem in the following way. First, from the graph
G = (V, E), we create a directed graph G0 = (V, E 0 ) by
replacing each edge e â E by two oppositely directed edges
e1 and e2 in E 0 (the end vertices of e1 and e2 are same as
the end vertices of e). Corresponding to a node vi in G0 that
has incoming edges from other nodes (say) vj , vk and vl , we
create a dependency relation (live equation) vi â vj vk vl . We
set K = R and L = |V |. The corresponding death equation is
of the form vÂ¯i â vÂ¯j + vÂ¯k + vÂ¯l (obtained by taking negation
of the live equation). We set K = R and L = |V |. It can now
easily be verified that if the graph G = (V, E) has a vertex
cover of size R iff in the created instance of K-MVN problem
death (failure) of at most K entities at time t0 , will trigger a
cascade of failures resulting in failures of at least L entities,
when the steady state is reached.
2) Optimal Solution with Integer Linear Programming:
In this case, we can find and optimal solution to the KMVN problem using Integer Linear Programming (ILP). We
associate binary indicator variables xi (yi ) to capture the state
of the entities ai (bi ). xi (yi ) is 1 when ai (bi ) is dead and
0 otherwise. Since we want find the set of K entities whose
failure at time step t0 triggers cascading failure resulting in the
failure of the largest number of entities, the
the
Pnobjective
Pof
m
ILP can be written as follows maximize
x
+
i
i=1
i=1 yi
It may be noted that the variables in the objective function
do not have any notion of time. However, cascading failure
takes place in time steps, ai triggers failure of bj at time
step t1 , which in turn triggers failure of ak in time step t2
and so on. Accordingly, in order to capture the cascading
failure process, we need to introduce the notion of time into
the variables of the ILP. If the numbers of A and B type
entities are n and m respectively, the steady state must be
reached by time step n + m â 1 (cascading process starts at
time step 0, t0 ). Accordingly, we introduce n + m versions
of the variables xi and yi , i.e., xi [0], . . . , xi [n + m â 1] and
yi [0], . . . , yi [n+mâ1]. To indicate the state of entities ai and
bi at times t0 , . . . , tn+mâ1 . The objective of the ILP is now
changed to
maximize

n
X
i=1

xi [n + m â 1] +

m
X

yi [n + m â 1]

i=1

Subject to the constraint that no more than K entities can
fail at time t0 .
Pn
Pm
Constraint 1:
i=1 yi [0] â¤ K In order
i=1 xi [0] +
to ensure that the cascading failure process conforms to
the dependency relations between type A and B entities,
additional constraints must be imposed.
Constraint 2: If an entity fails at time fails at time step p,
(i.e., tp ) it should continue to be in the failed state at all time
steps t > p. That is xi (t) â¥ xi (t â 1), ât, 1 â¤ t â¤ n + m â 1.
Same constraint applies to yi (t).
Constraint 3: The dependency relation (death equation)
aÂ¯i â bÂ¯j +bÂ¯k +bÂ¯l can be translated into a linear constraint in the
following way xi (t) â¤ yj (tâ1)+yk (tâ1)+yl (tâ1), ât, 1 â¤
t â¤ n + m â 1.

The optimal solution to K-MVN problem for Case II can be
found by solving the above ILP.
C. Case III: Problem Instance with an Arbitrary Number of
Min-terms of Size One
A live equation
Pq in this special case will have the following
form: xi â j=1 yj where xi and yj are elements of the set
A (B) and B (A) respectively, q represents the number of minterms in the live equation. In the example ai â bj + bk + bl ,
q = 3, xi = ai , y1 = bj , y2 = bk , y3 = bl .
1) Computational Complexity: We show that a special
case of the problem instances with an arbitrary number
of min-terms of size one is same as the Subset Cover
problem (defined below), which is proven to be NPcomplete. We define Implication Set(A) Pto be the
Ti
set of all implications of the form ai â
j=1 bj and
ImplicationPSet(B) to be the set of all implications of the
Ti
form bi â
j=1 aj . Now consider a subset of the set of
problem instances with an arbitrary number of min-terms
of size one where either Implication Set(A) = â
or Implication Set(B)
=
â. Let A0
=
{ai |ai is the element on the LHS of an implication}
in the Implication Set(A). The set B 0 is defined
accordingly. If Implication Set(B) = â then B 0 = â. In
this case, failure of any ai , 1 â¤ i â¤ n type entities will not
cause failure of any bj , 1 â¤ j â¤ m type entities. Since an
adversary can cause failure of only K entities, the adversary
would like to choose only those K entities that will cause
failure of the largest number of entities. In this scenario, there
is no reason for the adversary to attack any ai , 1 â¤ i â¤ n type
entities as they will not cause failure of any bj , 1 â¤ j â¤ m
type entities. On the other hand, if the adversary attacks
K bj type entities, not only those K bj type entities will
be destroyed, some ai type entities will also be destroyed
due to the implications in the Implication Set(A). As
such the goal of the adversary will be to carefully choose
K bj , 1 â¤ j â¤ m type entities that will destroy the largest
number of ai type entities. In its abstract form, the problem
can be viewed as the Subset Cover problem.
Subset Cover Problem
Instance: A set S = {s1 , . . . , sm }, a set S of m subsets of S,
i.e., S = {S1 , . . . , Sr }, where Si â S, âi, 1 â¤ i â¤ r, integers
p and q.
Question: Is there a p element subset S 0 of S (p < n) that
completely covers at least q elements of the set S? (A set S 0 is
said to be completely covering an element Si , âi, 1 â¤ i â¤ m
of the set S, if S 0 â© Si = Si , âi, 1 â¤ i â¤ m.)
The set S in the subset cover problem corresponds to the
set B = {b1 , . . . , bm }, and each set Si , 1 â¤ i â¤ r corresponds
to an implication in the ImplicationS et(A) and comprises of
the bj âs that appear on the RHS of the implication. The goal
of the problem is to select a subset B 00 of B that maximizes
the number of Si âs completely covered by B 00 .

5

Theorem 4. The Subset Cover problem is NP-complete.
Proof: We prove that the Subset Cover problem is NPcomplete by giving a transformation from the well known
Clique problem. It may be recalled that an instance of the
Clique problem is specified by a graph G = (V, E) and an
integer K. The decision question is whether or not a clique of
size at least K exists in the graph G = (V, E). We show that
a clique of size K exists in graph G = (V, E) iff the Subset
Cover problem instance has a p element subset S 0 of S that
completely covers at least q elements of the set S.
From an instance of the Clique problem, we create an
instance of the Subset Cover problem in the following way.
Corresponding to every vertex vi , 1 â¤ i â¤ n of the graph
G = (V, E) (V = {v1 , . . . , vn }), we create an element
in the set S = {s1 , . . . , sn }. Corresponding to every edge
ei , 1 â¤ i â¤ m, we create m subsets of S, i.e., S =
{S1 , . . . , Sm }, where Si corresponds to a two element subset
of nodes, corresponding to the end vertices of the edge ei . We
set the parameters p = K and q = K(K â 1)/2. Next we
show that in the instance of the subset cover problem created
by the above construction process, a p element subset S 0 of
S exists that completely covers at least q elements of the set
S, iff the graph G = (V, E) has a clique of size at least K.
Suppose that the graph G = (V, E) has a clique of size
K. It is clear that in the created instance of the subset cover
problem, we will have K(K â 1)/2 elements in the set S,
that will be completely covered by a K element subset of
the set S. The K element subset of S corresponds to the set
of K nodes that make up the clique in G = (V, E) and the
K(K â 1)/2 elements in the set S corresponds to the edges
of the graph G = (V, E) that corresponds to the edges of
the clique. Conversely, suppose that the instance of the Subset
Cover problem has K element subset of S that completely
covers K(K â 1)/2 elements of the set S. Since the elements
of S corresponds to the edges in G, in order to completely
cover K(K â 1)/2 edges, at least K nodes (elements of the
set S) will be necessary. As such, this set of K nodes will
constitute a clique in the graph G = (V, E).
2) Optimal Solution with Integer Linear
Programming: If
Pq
the live equation is in the form xi â k=1 yj then the âdeath
equationâ (obtained by taking negation
of the live equation)
Qq
will be in the product form xÌi â j=1 yÌj . If the live equation
is given as ai â bj + bk , then the death equation will be given
as aÂ¯i â bÂ¯j bÂ¯k .
By associating binary indicator variables xi and yi to
capture the state of the entities ai and bi , we can follow almost
identical procedure as in Case II, with only one exception.
It may be recalled that in Case II, the death equations such
as aÂ¯i â bÂ¯j + bÂ¯k was translated into a linear constraint
xi (t) â¤ yj (t â 1) + yk (t â 1), ât, 1 â¤ t â¤ n + m â 1. However
a similar translation in Case III, with death equations such as
aÂ¯i â bÂ¯j bÂ¯k , will result in a non-linear constraint of the form
xi (t) â¤ yj (t â 1)yk (t â 1), ât, 1 â¤ t â¤ n + m â 1. Fortunately,
a non-linear constraint of this form can be replaced a linear
constraint such as 2xi (t) â¤ yj (t â 1) + yk (t â 1), ât, 1 â¤

t â¤ n + m â 1. After this transformation, we can compute the
optimal solution using integer linear programming.
D. Case IV: Problem Instance with an Arbitrary Number of
Min-terms of Arbitrary Size
1) Computational Complexity: Since both Case II and Case
III are special cases of Case IV, the computational complexity
of finding the K most vulnerable nodes in the multilayer
network in NP-complete in Case IV also.
2) Optimal Solution with Integer Linear Programming:
The optimal solution to this version of the problem can be
computed by combining the techniques developed for the
solution of the versions of the problems considered in Cases
II and III.
IV. E XPERIMENTAL RESULTS
We applied our model to study multilayer vulnerability
issues in Maricopa County, the most densely populated county
of Arizona with approximately 60% of Arizonas population
residing in it. Specifically, we wanted to find out if some
regions of Maricopa County were more vulnerable to failure
than some other regions. The data for our multi-layered
network were obtained from different sources. We obtained
the data for the power network (network A) from Platts
(http://www.platts.com/). Our power network dataset consists
of 70 power plants and 470 transmission lines. Our communication network (network B) data were obtained from GeoTel
(http://www.geo-tel.com/). Our communication network data
consists of 2, 690 cell towers and 7, 100 fiber-lit buildings as
well as 42, 723 fiber links. Snapshots of our power network
data and communication network data are shown in figure 2. In
the power network snapshot of sub-figure(a), the orange markers show locations of powerplants while the yellow continuous
lines represent the transmission lines. In the communication
network snapshot of sub-figure (b) the pink markers show the
location of fiber-lit buildings, the orange markers show the
location of cell towers and the green continuous lines represent
the fiber links. In our dataset, âloadâ in the Power Network is
divided into Cell towers and Fiber-lit buildings. Although there
exists various other physical entities which also draw electric
power and hence can be viewed as load to the power network,
as they are not relevant to our study on interdependency
between power and communication networks, we ignore such
entities. Thus in network A, we have the three types of Power
Network Entities (PNEâs) - Generators, Load (consisting of
Cell towers and Fiber-lit buildings) and Transmission lines
(denoted by a1 , a2 , a3 respectively). For the Communication
Network, we have the following Communication Network
Entities (CNEâs) - Cell Towers, Fiber-lit buildings and Fiber
links (denoted by b1 , b2 , b3 respectively). We consider the
Fiber-lit buildings as a communication network entities as they
house routers which definitely are communication network
entities. From the raw data we construct Implication Set(A)
and Implication Set(B), by following the rules stated below:
Rules: We consider that a PNE is dependent on a set of
CNEs for being in the active state (âaliveâ) or being in the

6

(a) Snapshot of Power Network in Maricopa County

(b) Snapshot of Communication Network in Maricopa County

Fig. 2: Snapshots of power network and communication network in Maricopa County)

inactive state (âdeadâ). Similarly, a CNE is dependent on a set
of PNEs for being active or inactive state. For simplicity we
consider the live equations with at most two minterms. For
the same reason we consider the size of each minterm is at
most two.

of the number of entities of the two networks A and B. Most
importantly, we find that the degree of vulnerability of all
the five regions considered in our study are close and no one
region stands out as being extremely vulnerable.

Generators (a1,i , 1 â¤ i â¤ p, where p is the total number
of generators): We consider that each generator (a1.i ) is
dependent on the nearest Cell Tower (b1,j ) or the nearest
Fiber-lit building (b2,k ) and the corresponding Fiber link (b3,l )
connecting b2,k and a1,i . Thus, we have
a1,i â b1,j + b2,k Ã b3,l
Load (a2,i , 1 â¤ i â¤ q, where q is the total number of loads):
We consider that the loads in the power network do not depend
on any CNE.
Transmission Lines (a3,i , 1 â¤ i â¤ r, where r is the total number of transmission lines): We consider that the transmission
lines do not depend on any CNE.
Cell Towers (b1,i , 1 â¤ i â¤ s, where s is the total number
of cell towers): We consider the cell towers depend on the
nearest pair of generators and the corresponding transmission
line connecting the generator to the cell tower. Thus, we have
b1,i â a1,j Ã a3,k + a1,j 0 Ã a3,k0
Fiber-lit Buildings (b2,i , 1 â¤ i â¤ t, where t is the total number
of fiber-lit buildings): We consider that the fiber-lit buildings
depend on the nearest pair of generators and the corresponding
transmission lines connecting the generators to the cell tower.
Thus, we have b2,i â a1,j Ã a3,k + a1,j 0 Ã a3,k0
Fiber Links (b3,i , 1 â¤ i â¤ u, where u is the total number of
fiber links)): We consider that the fiber links do not depend
on any PNE.
Because of experimental resource limitation, we have considered 5 regions of Maricopa County for our experiments.
We used IBM CPLEX Optimizer 12.5 to run the formulated
ILPâs on the experimental dataset. We show our results in
the figure 3. We observe that in each of the regions there
is a specific budget threshold beyond which each additional
increment in budget results in the death of only one entity. The
reason for this behavior is our assumption that entities such
as the transmission lines and the fiberlinks are not dependent
on any other entities. We notice that all the entities of the
two networks can be destroyed with a budget of about 60%

Fig. 3: Experimental results of failure vulnerability across five regions
of Maricopa county

R EFERENCES
[1] S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin,
âCatastrophic cascade of failures in interdependent networks,â Nature,
vol. 464, no. 7291, pp. 1025â1028, 2010.
[2] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin, âNetworks formed
from interdependent networks,â Nature Physics, vol. 8, no. 1, pp. 40â48,
2011.
[3] J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley, âCascade of
failures in coupled network systems with multiple support-dependence
relations,â Physical Review E, vol. 83, no. 3, p. 036116, 2011.
[4] V. Rosato, L. Issacharoff, F. Tiriticco, S. Meloni, S. Porcellinis, and
R. Setola, âModelling interdependent infrastructures using interacting
dynamical models,â International Journal of Critical Infrastructures,
vol. 4, no. 1, pp. 63â79, 2008.
[5] P. Zhang, S. Peeta, and T. Friesz, âDynamic game theoretic model of
multi-layer infrastructure networks,â Networks and Spatial Economics,
vol. 5, no. 2, pp. 147â178, 2005.
[6] M. Parandehgheibi and E. Modiano, âRobustness of interdependent
networks: The case of communication networks and the power grid,â
arXiv preprint arXiv:1304.0356, 2013.
[7] D. T. Nguyen, Y. Shen, and M. T. Thai, âDetecting critical nodes in
interdependent power networks for vulnerability assessment,â 2013.
[8] A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman,
âPower grid vulnerability to geographically correlated failures-analysis
and control implications,â arXiv preprint arXiv:1206.1099, 2012.
[9] J.-F. Castet and J. H. Saleh, âInterdependent multi-layer networks:
Modeling and survivability analysis with applications to space-based
networks,â PloS one, vol. 8, no. 4, p. e60402, 2013.
[10] A. Fudenberg and J. Tirole, Game Theory. Ane Books, 2010.

INL/EXT-06-11464

Critical Infrastructure
Interdependency
Modeling: A Survey of
U.S. and International
Research
P. Pederson
D. Dudenhoeffer
S. Hartley
M. Permann
August 2006

The INL is a U.S. Department of Energy National Laboratory
operated by Battelle Energy Alliance

INL/EXT-06-11464

Critical Infrastructure Interdependency Modeling: A
Survey of U.S. and International Research

P. Pedersona
D. Dudenhoefferb
S. Hartleyb
M. Permannb
a

Technical Support Working Group, Washington D.C.
Idaho National Laboratory

b

August 2006

Idaho National Laboratory
Idaho Falls, Idaho 83415

Prepared for the
Technical Support Working Group Under
Work for Others Agreement 05734
Under DOE Idaho Operations Office
Contract DE-AC07-05ID14517

ABSTRACT
âThe Nationâs health, wealth, and security rely on the production and
distribution of certain goods and services. The array of physical assets, processes,
and organizations across which these goods and services move are called critical
infrastructures.â1 This statement is as true in the U.S. as in any country in the
world. Recent world events such as the 9-11 terrorist attacks, London bombings,
and gulf coast hurricanes have highlighted the importance of stable electric, gas
and oil, water, transportation, banking and finance, and control and
communication infrastructure systems.
Be it through direct connectivity, policies and procedures, or geospatial
proximity, most critical infrastructure systems interact. These interactions often
create complex relationships, dependencies, and interdependencies that cross
infrastructure boundaries. The modeling and analysis of interdependencies
between critical infrastructure elements is a relatively new and very important
field of study.
The U.S. Technical Support Working Group (TSWG) has sponsored this
survey to identify and describe this current area of research including the current
activities in this field being conducted both in the U.S. and internationally. The
main objective of this study is to develop a single source reference of critical
infrastructure interdependency modeling tools (CIIMT) that could be applied to
allow users to objectively assess the capabilities of CIIMT. This information will
provide guidance for directing research and development to address the gaps in
development. The results will inform researchers of the TSWG Infrastructure
Protection Subgroup of research and development efforts and allow a more
focused approach to addressing the needs of CIIMT end-user needs.
This report first presents the field of infrastructure interdependency
analysis, describes the survey methodology, and presents the leading research
efforts in both a cumulative table and through individual datasheets. Data was
collected from open source material and when possible through direct contact
with the individuals leading the research.

iii

iv

Athena

CARVER2

CI3

CIMS

CIP/DSS

CIPMA

CISIA

COMM-ASPEN

DEW

EMCAS

FAIT

FINSIM

Fort Future

IEISS

IIM

Knowledge Management and
Visualization

MIN

MUNICIPAL

N-ABLE

NEMO

Net-Centric GIS

NEXUS Fusion Framework

NGtools

NSRAM

PFNAM

TRAGIS

TRANSIMS

UIS

WISE

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

21

22

23

24

25

26

27

28

29

30

See Notes.

Simulation Type:
I
Input-Output Model
A
Agent-based

AIMS

1

Simulation Name

Users:
IA
EA
B

LANL

LANL

LANL

ORNL

ANL

JMU

ANL
X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

X

Maturity:
RS
DV
MI
MC

X

X

X

X

X

X

X

X

X

X

X

Sewage
Water

X

X

X

X

X

Storm
Water

Research
Development
Mature Internal
Mature Commercial

Electric Natural Drinking
Power
Gas
Water

Internal Analyst
External Analyst
Both

IntePoint, LLC.

York University

SPARTA

SNL

RPI

Purdue

CMU

UV

LANL

USACE

LANL

SNL

ANL

EDD

SNL

University Roma Tre

Australia

LANL, SNL, ANL

INL

ANL

National Infrastructure
Institute

On Target Technologies,
Inc.

UNB

Developer

Table EX-1. Summary of areas surveyed.

X

X

X

X

X

X

X

X

Human
Activity

X

X

X

X

X

X

X

X

X

Financial
Networks

X

X

X

X

X

X

X

SCADA

X

X

X

X

X

X

X

X

X

X

X

X

Telecom

X

X

X

Computer
Networks

Area Model by Infrastructure Sector

v

X

X

X

X

X

Oil
Pipeline

X

X

X

X

X

X

X

X

X

X

Rail
System

X

X

X

X

X

X

X

X

X

Highway
System

X

X

X

X

X

X

X

X

X

X

Waterway
System

X

X

X

X

X

Police/
Regulatory
Constraints

X

X

X

I

X

X

I

X

X

X

X

X

Continuous

A

A

X

A

A

A

A

Discrete

Simulation
Type

X

X

X

X

X

X

X

X

X

X

X

Integrated

A

A

A

A

A

A

Coupled

System
Model

X

X

X

X

X

X

X

X

PC

X

X

X

HPC

Hardware
Platform
Requirements

X

X

X

X

X

X

X

X

Windows

X

X

X

X

X

Linux

Software
Requirements

X

Solaris

IA

IA

B

B

B

IA

IA

IA

IA

IA

B

IA

IA

IA

B

IA

B

B

B

B

EA

IA

Users

DV

MI

MC

MI

RS

MI

MC

RS

DV

MI

RS

RS

MI

MC

DV

DV

MI

MC

DV

MI

MI

DV

MI

MC

MI

RS

Maturity
Level

User and
Maturity Levels

Athena is an analysis and modeling tool that is designed to analyze a network of nodes (actors, concepts, and physical) as a âsystem of systemsâ by merging various political, military, economic, social, information, and infrastructure (PMESII) models and their associated crossdependencies. Athena incorporates several reasoning algorithms that allow sophisticated inter- and intra-dependency analysis between and through nodes.
POC: Dr. Brian Drabble, On Target Technologies, brain@ontgttech.com

CARVER2 is a simple software program that provides a quick and easy way to prioritizes potential terrorist targets. It compares and rates the critical infrastructure and key assets in jurisdictions by producing a mathematical score for each potential target. It is the first step for
conducting more in-depth vulnerability assessments. CARVER2 helps users make âapples vs. orangesâ comparisons such as a water system vs. an energy grid vs. a bridge.
POC: Ronald Peimer, National Infrastructure Institute Center for Infrastructure Expertise, rpeimer@ni2.org

CI3 (Critical Infrastructures Interdependencies Integrator) is a software tool for emulating (Monte Carlo simulation) the amount of time or cost (or both) needed for activities that must be completed to restore a given infrastructure component, a specific infrastructure system, or an
interdependent set of infrastructures to an operational state. The software tool provides a framework for recognizing interdependencies and incorporating uncertainty into the analysis of critical infrastructures.
POC: Dr. James Peerenboom, Argonne National Laboratory, jpeerenboom@anl.com

CIMS (Critical Infrastructure Modeling System) is a high level M&S tool that allows visualization in a 3D environment the cascading consequence of infrastructure perturbations. Events can be scripted or assets directly manipulated within the environment during a simulation run to
illustrate consequence.
POC: Don Dudenhoeffer, Idaho National Laboratory, Donald.Dudenhoeffer@inl.gov

CIP/DSS (Critical Infrastructure Protection Decision Support System) simulates the dynamics of individual infrastructures and couples separate infrastructures to each other according to their interdependencies. CIP/DSS models asset information at the aggregate level. For example,
a focus area can estimate the number of hospital beds affected by an event, but it cannot directly retrieve information relative to a particular hospital. It utilizes the commercial simulation software Vensim.
POC: Randy Michelsen, Los Alamos National Laboratory, rem@lanl.gov
POC: Theresa Brown, Sandia National Laboratories, tjbrown@sandia.gov

CIPMA (Critical Infrastructure Protection Modeling and Analysis) is a computer based tool to support business and government decision making for critical infrastructure (CI) protection, counter-terrorism, and emergency management, especially with regard to prevention,
preparedness, and planning and recovery.
POC: Australian Government â Attorney Generalâs Department (AGD), Michael Jerks â Director, Major Projects, Michael.Jerks@ag.gov.au

CISIA (Critical Infrastructure Simulation by Interdependent Agents) is described by the authors as a hybrid of the two modeling approaches; interdependency analysis and system analysis. It is a bottom-up complex adaptive systems (CAS) model using interactive agents. The CISIA
simulator is designed to analyze short term effects of failures in terms of fault propagation and performance degradation (Panzieri, 2004).
POC: Stefano Panzieri Universita Roma Tre, Italy, panzieri@uniroma3.it

DEW (Distribution Engineering Workstation) provides over 30 applications for analysis, design, and control of electrical and other physical network systems. DEW allows all of its components (data sets and algorithms) to be reused by a new application, allowing new solutions to
build on top of existing work. This provides for cross collaborations among different groups and the emergence of solutions to complex problems. DEW is being used to identify and analyze interdependencies in large scale electrical power systems and fluid systems of aircraft
carriers. DEW is open architecture, non-proprietary.
POC: Electrical Distribution Design, Inc., Dr. Robert Broadwater, dew@vt.edu

EMCAS (Electricity Market Complex Adaptive System) combines engineering techniques with quantitative market analysis: DC load flow models allow you to simulate the actual operation of the physical system configuration as well as regulatory rules imposed on market
operations.
POC: Guenter Conzelmann, Argonne National Laboratory, guenter@anl.gov

FAIT (Fast Analysis Infrastructure Tool) is primarily an economic analysis tool utilizing REMI to conduct economic impact assessment across multiple sectors. It does promote interdependency discovery for first order relationships. The program resides on an SNL server and
supports web access.
POC: Theresa Brown, Sandia National Laboratories, tjbrown@sandia.gov

FINSIM is an agent-based model of cash and barter transactions that are dependant on contractual relationships and a network at the federal reserve level. Agent based models create transactions which rely on telecommunications and electric power.
POC: Randy Michelsen, Los Alamos National Laboratory, rem@lanl.gov

Fort Future is a collaborative, web-based planning system that uses simulation to test plans for Department of Defense (DoD) installations. It uses an open, service-oriented architecture to allow multiple simulations to be run simultaneously from the same set of alternative, organized
into a study. The web-based workbench provides geographic information system (GIS)-based plan editors, controls simulations, and organizes results into a decision matrix. Fort Future assesses the impact of critical infrastructure on mission using a âVirtual Installationâ simulation
that contains models for transportation, electrical power, water systems, including waterborne chemical/biological/radiological (CBR) agents, airborne CBR plume, facilities, mission tasks and processes, agents, and dynamic plans.
POC: U.S. Army Corps of Engineers, Engineer Research and Development Center, Construction Engineering Research Laboratory (CERL), Dr. Michael P. Case, Michael.P.Case@erdc.usace.army.mil

IEISS (Interdependent Energy Infrastructure Simulation System) is an actor-based infrastructure modeling, simulation, and analysis tool designed to assist individuals in analyzing and understanding interdependent energy infrastructures. The actor-based infrastructure components
were developed in IEISS to realistically simulate the dynamic interactions within each of the infrastructures, as well as, the interconnections between the infrastructures.
POC: Randy Michelsen, Los Alamos National Laboratory, rem@lanl.gov

IIM (Inoperability input-output model) based on Leontiefâs input-output model, characterizes interdependencies among sectors in the economy and analyzes initial disruptions to a set of sectors and the resulting ripple effects.
POC: Yacov Y. Haimes, University of Virginia, haimes@virginia.edu

Knowledge Management and Visualization is a research project to analyze vulnerabilities associated with delivery of fuel. It is designed to help ensure availability of supply and to visualize the impacts for decision support. The project has focused on coal deliveries to power plants
because, while vulnerabilities at the power plant level (production) are easier to identify, vulnerabilities and impacts associated with delivery of fuel are more uncertain. Also, data on coal shipments is readily available.
POC: Carnegie Mellon University, H. Scott Matthews, hsm@cmu.edu

MIN (multilayer infrastructure network) is a preliminary network flow equilibrium model of dynamic multilayer infrastructure networks in the form of a differential game involving two essential time scales. In particular, three coupled network layersâautomobiles, urban freight,
and dataâare modeled as being comprised of Cournot-Nash dynamic agents. An agent-based simulation solution structure is introduced to solve the flow equilibrium and optimal budget allocation problem for these three layers under the assumption of a super authority that oversees
investments in the infrastructure of all three technologies and thereby creates a dynamic Stackelberg leader-follower game.
POC: Purdue School of Civil Engineering, Dr. Srinivas Peeta, peeta@purdue.edu

2

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

vi

AIMS (Agent-Based Infrastructure Modeling and Simulation) is an agent-based system to simulate and model the (national and cross-border) interdependencies and survivability of Canadaâs critical infrastructures.
Point of Contact (POC): Dr. Stephen Marsh, Adjunct Professor, University of New Brunswick, Canada, Stephen.Marsh@nrc-cnrc.gc.ca

1

NOTES:

N-ABLE (Next-generation agent-based economic laboratory) simulates the economy using an agent-based discrete-event model. Agents make economic decisions including purchasing products, hiring workers, selling bonds, collecting welfare payments, conducting open market
operations, and others. N-ABLE has been used to evaluate electric power and rail transportation disruptions on commodity production.
POC: Theresa Brown, Sandia National Laboratories, tjbrown@sandia.gov

NEMO (Net-Centric Effects-based operations MOdel) relies on the following domain specific legacy simulations: CitiLabsâ Voyager simulation provides road and rail network analysis. Advantica provides the solver tools for electrical power networks, water and gas pipelines. Users
define relationships between components.
POC: Brent L. Goodwin, SPARTA Corporation, brent.goodwin@sparta.com

Net-Centric GIS is a framework for using GIS interoperability for supporting emergency management decision makers by providing effective data sharing and timely access to infrastructure interdependency information.
POC: York University, Toronto, Ontario, Canada, Rifaat Abdalla, abdalla@yorku.ca

NEXUS Fusion FrameworkTM is a planning and response tool that visualizes intended and unintended effects and consequences of an event across multiple infrastructure, social, and population behavior models. It is a single framework that incorporates geospatial, graph based
(social, economic), and population behavior models in the same simulation space for cross-infrastructure relationship analysis. The framework takes a holistic system-of-systems view to support cross system analyses of cascading events within and between complex networks.
POC: IntePoint, LLC, Mark Armstrong, Mark.Armstrong@IntePoint.com

NGtools (natural gas infrastructure toolset). NGtools was developed to provide an analyst with a quick method to access, review, and display components of the natural gas network; perform varying levels of component and systems analysis, and display analysis results.
POC: Argonne National Laboratory, Infrastructure Assurance Center (IAC), Dr. James Peerenboom, jpeerenboom@anl.gov

NSRAM (Network Security Risk Assessment Modeling) is centered on the analysis of large interconnected multi-infrastructure models.
POC: Jim McManus, James Madison University, McManuJP@jmu.edu

PFNAM (Petroleum Fuels Network Analysis Model) was developed to perform hydraulic calculations of pipeline transport of crude oil and petroleum products. A network consists of links (pipe segments), nodes (pipe junctions), pump stations, valves, and pressure-reducing
stations. The model tracks the flow of oil in each pipe and the pressure at each node. âPoint-and-clickâ motions allow the analyst to create a representative model of the liquids pipeline network in order to set up and run a simulation. Graphical and tabular results provided for each
simulation enable analysts to quantify the impact of infrastructure disruptions on the pipeline segment or system. This software tool provides a framework for introducing pipeline component dependencies into critical infrastructure analyses.
POC: Argonne National Laboratory, Infrastructure Assurance Center, Steve Folga, sfolga@anl.gov

TRAGIS (Transportation Routing Analysis Geographic Information System), available via a client server architecture from a web server residing at ORNL. Calculates transportation rouge information based on regulatory guidance for shipping hazardous materials.
POC: Paul E. Johnson, Oak Ridge National Laboratory, johnsonpe@ornl.gov

TRANSIM is an agent-based system capable of simulating a synthetic populations second-by-second movements of every person and vehicle through the transportation network of a large metropolitan area. TRANSIMS provides planners with a synthetic population's daily activity
patterns (such as travel to work, shopping and recreation, etc.), simulates the movements of individual vehicles on a regional transportation network, and estimates the air pollution emissions generated by vehicle movements.
POC: Randy Michelsen, Los Alamos National Laboratory, rem@lanl.gov

WISE (Water Infrastructure Simulation Environment) is an analytic framework supporting the evaluation of water infrastructure in terms of both infrastructure specific and interdependency issues. WISE involves the integration of geographic information systems with a wide range
of infrastructure analysis tools including industry standard hydraulic simulation engines (e.g., EPANET and SWMM) as well as Los Alamos National Laboratory interdependency simulation systems such as the Interdependent Energy Infrastructure Simulation System (IEISS).
POC: Randy Michelsen, Los Alamos National Laboratory, rem@lanl.gov

19

20

21

22

23

24

25

26

27

28

vii

MUNICIPAL (Multi-Network Interdependent Critical Infrastructure Program for Analysis of Lifelines) is a GIS user interface, built on a formal, mathematical representation of a set of civil infrastructure systems that explicitly incorporates the interdependencies among them. The
mathematical foundation or decision support system is called the interdependent layered network (ILN) model. ILN is a mixed-integer, network-flow based model implemented in software drawing on a database containing infrastructure attributes. MUNICIPAL provides the
capability to understand how a disruptive event affects the interdependent set of civil infrastructures.
POC: Rensselaer Polytechnic Institute (RPI), Earl E. Lee II, Leee7@rpi.edu

18

viii

CONTENTS
ABSTRACT.................................................................................................................................................iii
INTRODUCTION ........................................................................................................................................ 1
Technical Support Working Group .................................................................................................... 1
Background......................................................................................................................................... 1
INFRASTRUCTURE INTERDEPENDENCIES......................................................................................... 3
Interdependency Formalization .......................................................................................................... 5
Interdependency Types....................................................................................................................... 6
Problem Space .................................................................................................................................... 8
SURVEY METHODOLOGY ...................................................................................................................... 9
Infrastructures..................................................................................................................................... 9
Modeling and Simulation Technique ............................................................................................... 10
Integrated vs. Coupled Models......................................................................................................... 10
Hardware/Software Requirements.................................................................................................... 10
Intended user .................................................................................................................................... 10
Maturity level ................................................................................................................................... 10
STATE-OF-THE-ART REPORT ............................................................................................................... 11
POLITICAL, MILITARY, ECONOMIC, SOCIAL, INFORMATION, AND INFRASTRUCTURE
MODELING ACTIVITIES.............................................................................................................. 12
DATA SOURCES ...................................................................................................................................... 13
U.S. RESEARCH AND SPONSORING ORGANIZATIONS .................................................................. 14
CHALLENGES AND RESEARCH NEEDS ............................................................................................. 16
CONCLUSIONS......................................................................................................................................... 19
ACKNOWLEDGEMENT .......................................................................................................................... 20
REFERENCES ......................................................................................................................................... 115

ix

FIGURES
1.

Infrastructure interdependencies......................................................................................................... 3

2.

Thick, black smoke billows out of the railroad tunnel near Oriole Park at Camden Yards.
Interstate 395 and the baseball park were closed, along with the Inner Harbor
(see Reference 9). ............................................................................................................................... 4

3.

An official surveys the gaping hole and broken 40-in. water main at Howard and Lombard
streets (see Reference 10). .................................................................................................................. 4

4.

Sample dependency matrix................................................................................................................. 5

5.

Cascading consequence example (see Reference 13)......................................................................... 7

6.

PMESII node and effects relation (see Reference 19)...................................................................... 12

TABLES
EX-1. Summary of areas surveyed................................................................................................................ v
1.

Multiscale time hierarchy of power systems. ................................................................................... 16

2.

Strengths and weaknesses of HLA. .................................................................................................. 17

3.

Strengths and weaknesses of DIS..................................................................................................... 17

x

Critical Infrastructure Interdependency Modeling:
A Survey of U.S. and International Research
complexity of interactions between infrastructure
sectors, and implications and sensitivity of results.

INTRODUCTION
âThe Nationâs health, wealth, and security rely
on the production and distribution of certain goods
and services. The array of physical assets,
processes, and organizations across which these
goods and services move are called critical
infrastructures.â2 This statement is as true in the
U.S. as in any country in the world. Recent world
events such as the 9-11 terrorist attacks, London
bombings, and gulf coast hurricanes have
highlighted the importance of stable electric, gas
and oil, water, transportation, banking and finance,
and control and communication infrastructure
systems.

This survey identifies and catalogs much of
the state-of-the-art research being conducted in the
area of infrastructure interdependency modeling
and analysis.

Technical Support Working
Group
The U.S. Technical Support Working Group
(TSWG) is the sponsor for this effort.3 TSWG is a
national forum to identify, prioritize, and
coordinate interagency and international research
and development (R&D) requirements for
combating terrorism. The aim of TSWG is to
support rapidly developed technologies and
product development to provide tools for
combating terrorism. It supports multiple U.S.
government agencies as well as major allies.

Be it through direct connectivity, policies and
procedures, or geospatial proximity, most critical
infrastructure systems interact. These interactions
often create complex relationships, dependencies,
and interdependencies that cross infrastructure
boundaries. The modeling and analysis of
interdependencies between critical infrastructure
elements is a relatively new and very important
field of study.

The main objective of this study is to develop
a single source reference of critical infrastructure
interdependency modeling tools (CIIMT) that
could be applied to allow users to objectively
assess the capabilities of CIIMT. This information
will provide guidance for directing R&D to
address the gaps in development. The results will
inform the R&D efforts of the TSWG
Infrastructure Protection Subgroup of R&D efforts
and allow a more focused approach to addressing
the needs of CIIMT end-user needs.

Much effort is currently being spent to
develop models that accurately simulate critical
infrastructure behavior and identify
interdependencies and vulnerabilities. The results
of these simulations are used by private
companies, government agencies, military, and
communities to plan for expansion, reduce costs,
enhance redundancy, improve traffic flow, and to
prepare for and respond to emergencies.

Background

Modelers have developed various innovative
modeling approaches including agent based
modeling, effects-based operations (EBO) models,
input-output models, models based on game
theory, mathematical models, and models based on
risk. These have been applied to infrastructure of
shipboard systems, University campuses, large
power grids, and waterways to name a few.
Modeling is complicated by the quality and
availability of data, intricacy of systems,

The study and analysis of infrastructure
interdependencies is relatively new. The
interdependencies between critical infrastructures
received little attention in the early 1990s.
However, in the mid 1990s events such as the
Oklahoma City bombing in 1995 and the report
from the Defense Science Board Task Force on
Information Warfare in 1996, and the increased
reliance on information and computerized control
systems brought the increasing importance of

1

the E.O.s, the functions of NIAC remained largely
the same.

infrastructure interdependencies into focus. Also
in 1996, President Clinton established the
Presidentâs Commission on Critical Infrastructure
Protection (PCCIP).4

We have since seen the establishment of the
U.S. Department of Homeland Security (DHS) in
November of 2002 and the National Infrastructure
Simulation and Analysis Center (NISAC) in fall of
2001. NISAC is a partnership between Sandia
National Laboratory (SNL) and Los Alamos
National Laboratory (LANL) established to
develop advanced infrastructure modeling and
simulation techniques that identify vulnerabilities
and interdependencies.

The PCCIP report was released in 1997 and
though it identified no immediate critical threats to
national infrastructures, it did highlight the
importance of interdependencies including those
between power, transportation, emergency
response, vital human services, banking and
finance, and telecommunications, especially
through digital means. A general recommendation
of the commission was that since the lionâs share
(approximately 85%) of the nationâs critical
infrastructure is in private hands, there needs to be
good cooperation and information sharing between
government and private sector.

This increased attention has been followed by
increases in funding to universities, national
laboratories, and private companies involved in
modeling and simulation of critical
interdependencies. Funding has come from
national organizations, private investments, the
Department of Defense (DoD), U.S. government
agencies (DHS, U.S. Department of Energy
[DOE], Department of Commerce, and others),
and other governments and agencies.

In May of 1998, Presidential Decision
Directive (PDD) no. 63 was released. That
directive set a national goal to protect the nationâs
critical infrastructure from deliberate attacks by
2003. PDD-63 was followed by executive orders
(E.O.s) by both Presidents Clinton (E.O. 131305 in
July 1999) and Bush (E.O. 132316 in 2001)
establishing Information Sharing and Analysis
Centers that were largely private-sector run and a
National Infrastructure Advisory Council (NIAC).
While there were some changes in the wording of

The increased funding and level of efforts has
led to much innovative work in this area. Thus,
while efforts focusing on modeling of critical
infrastructure interdependencies have only begun
recently, much valuable work has already been
done.

2

figure, individual infrastructure networks are
represented on a single plane. The parallel lines
represent individual sectors or subsets within that
particular infrastructure. The spheres or nodes
represent key infrastructure components within
that sector from the events in New Orleans

INFRASTRUCTURE
INTERDEPENDENCIES
âOne of the most frequently identified
shortfalls in knowledge related to enhancing
critical infrastructure protection capabilities is the
incomplete understanding of interdependencies
between infrastructures. Because these
interdependencies are complex, modeling efforts
are commonly seen as a first step to answering
persistent questions about the ârealâ vulnerability
of infrastructures.â7

The energy sector infrastructure, for example,
during Hurricane Katrina contains the sectors of
electrical generation and distribution, natural gas
production and distribution, etc. Ties and
dependencies exist within each infrastructure and
between the different sectors. The solid lines in
Figure 1, crossing sectors and connecting nodes,
represent internal dependencies, while the dashed
lines represent dependencies that also exist
between different infrastructures (infrastructure
interdependencies).

The importance of âWhat are infrastructure
inter-dependencies, and how are they modeled?â is
addressed in this section. References to
interdependent relationships in this paper are
actually referring to as dependent relationships or
influences between infrastructures. Figure 1
illustrates common representations of
infrastructure based on the scenario of a flooding
event and the subsequent response. Parallels to this
scenario with the events in New Orleans during
Hurricane Katrina can easily be drawn. Within the

The example in Figure 1 is a simple attempt to
portray the complexity of dependencies that may
exist between components. In chaotic
environments such as emergency response to
catastrophic events, decision makers should

Figure 1. Infrastructure interdependencies.

3

understand the dynamics underlying the
infrastructures. Failure to understand those
dynamics will result in ineffective response and
poor coordination between decision makers and
agencies responsible for rescue, recovery, and
restoration. It could also cause the
mismanagement of resources, including supplies,
rescue personnel, and security teams. At best,
emergency responders will lose public trust, at
worst, human life.
This interrelationship among infrastructures
and its potential for cascading effects was never
more evident than on July 19, 2001 when a 62-car
freight train carrying hazardous chemicals derailed
in Baltimoreâs Howard Street Tunnel, Figure 2.
Figure 2. Thick, black smoke billows out of the
railroad tunnel near Oriole Park at Camden
Yards. Interstate 395 and the baseball park were
closed, along with the Inner Harbor (see
Reference 9).

This disaster, in addition to its expected effect
on rail system traffic, automobile traffic, and
emergency services, caused a cascading
degradation of infrastructure components not
previously anticipated. For example, the tunnel
fire caused a water main to break above the tunnel
shooting geysers 20 ft into the air, Figure 3. The
break caused localized flooding which exceeded a
depth of three feet in some areas.
Additionally, the flooding knocked out
electricity to about 1,200 downtown Baltimore
residences.8 Fiber optical cables running through
the tunnel were also destroyed. This resulted in
major disruptions to phone and cell phone service,
email service, web services, and data services to
major corporations including WorldCom Inc.,
Verizon Communications Inc., the Hearst Corp. in
New York City, Nextel Communications Inc., and
the Baltimore Sun newspaper.9 Disruption to rail
services and its effects on the Middle Atlantic
states were significant also.10 These effects
included delays in coal delivery and also limestone
delivery for steel.

Figure 3. An official surveys the gaping hole and
broken 40-in. water main at Howard and
Lombard streets (see Reference 10).

4

with the influence or impact, that one
infrastructure can have, either directly or
indirectly, upon another. The cross infrastructure
effects continue to grow as information technology
pushes interconnectivity between all aspects of
business.

A dependency matrix is another way to
represent interdependencies between infrastructure
networks and their relative impact. The Critical
Infrastructure Protection Task Force of Canada
used a dependency matrix (see Figure 4) to relate
the interdependency among six sectors identified
as crucial: Government, Energy and Utilities,
Services, Transportation, Safety, and
Communications.11 The matrix is their attempt to
better understand the level of dependency and the
potential impact among sectors.

Infrastructure interdependencies therefore
refer to relationships or influences that an element
in one infrastructure imparts upon another
infrastructure.

Interdependency Formalization

Infrastructure owners historically concerned
with the operation of their own, often well defined
domains must now contend with unbounded
networks brought about by greater information
technology connectivity. There is a growing need
to analyze and better understand the chains of
influence that cross multiple sectors that can
induce potentially unforeseen secondary effects.
This survey addresses a growing concern dealing

Precisely how is an infrastructure
interdependency relationship defined?
Dudenhoeffer, Permann and Manic12 model the
levels of infrastructure as a large graph in which
nodes represent infrastructure components, and
edges the relations between nodes.

Figure 4. Sample dependency matrix.

5

thunderstorm resulting in a loss of power to an
office building and all the computers inside.

A formal model of this infrastructure and the
interrelationships is presented in the following
definitions:

x Informational Interdependency. An
informational or control requirement between
components. For example: a supervisory
control and data acquisition (SCADA) system
that monitors and controls elements on the
electrical power grid. A loss of the SCADA
system will not by itself shut down the grid,
but the ability to remotely monitor and operate
the breakers is lost. Likewise, this relationship
may represent a piece of information or
intelligence flowing from a node that supports
a decision process elsewhere. An example is
the dispatch of emergency services. While the
responders may be fully capable of
responding, an informational requirement
exists as to answering where, what, and when
to initiate response.

1. An infrastructure network, I, is a set of nodes
related to each other by a common function.
The network may be connected or disjoint. It
may be directional, bi-directional or have
elements of both. Internal
relationships/dependencies within the
infrastructure I are represented by edge (a, b)
with a, b Â I.
2. Given Ii and Ij are infrastructure networks, i z
j, a Â Ii and b Â Ij, an interdependency is
defined as a relationship between
infrastructures and represented as the edge
(a,b) which implies that node b is dependent
upon node a. Depending on the nature or type
of the relationship, this relationship may be
reflexive in that (a,b) Äº (b,a).

x Geospatial Interdependency. A relationship
that exists entirely because of the proximity of
components. For example: flooding or a fire
may affect all the assets located in one
building or area.

Interdependency Types
Interdependencies can be of different types.
Several taxonomies have been presented3 to
categorize the types of interdependencies.

x Policy/Procedural Interdependency. An
interdependency that exists due to policy or
procedure that relates a state or event change
in one infrastructure sector component to a
subsequent effect on another component. Note
that the impact of this event may still exist
given the recovery of an asset. For example:
after aircraft were flown into the World Trade
Towers âall U.S. air transportation was halted
for more than 24 hours, and commercial
flights did not resume for three to four days.â14

Rinaldi, Peerenboom, and Kelly13 describe
dependencies in terms of four general categories:
x Physical â a physical reliance on material flow
from one infrastructure to another
x Cyber â a reliance on information transfer
between infrastructure
x Geographic â a local environmental event
affects components across multiple
infrastructures due to physical proximity

x Societal Interdependency. The
interdependencies or influences that an
infrastructure component event may have on
societal factors such as public opinion, public
confidence, fear, and cultural issues. Even if
no physical linkage or relationship exists,
consequences from events in one
infrastructure may impact other
infrastructures. This influence may also be
time sensitive and decay over time from the
original event grows. For example: air traffic
following the 9-11 attack dropped
significantly while the public evaluated the
safety of travel. This resulted in layoffs within

x Logical â a dependency that exists between
infrastructures that does not fall into one of the
above categories.
This study used a slightly expanded taxonomy
developed by Dudenhoeffer and Permann.4 The
categorization classifies the following types of
relationships:
x Physical. A requirement, often engineering
reliance between components. For example: a
tree falls on a power line during a

6

further denoted (aDz). Likewise the genesis of the
chain may not be singular in that the end effect is
the influence of multiple nodes, denoted by
(abc..Dz).

the airline industry and bankruptcy filings by
some of the smaller airlines (see
Reference 12).
Again, while the dependencies within an
individual infrastructure network are often well
understood, the region of interest in
interdependency and effects modeling is the
influence or impact that one infrastructure can
impart upon another. Therefore, the key effects to
model and gain understanding of are the chains of
influence that cross multiple sectors and induce
potentially unforeseen n-ary effects. These chains,
potentially composed of multiple interdependency
types, compose the paths or arcs between
infrastructure components or nodes denoted as
{(a,b), (b,c), (c,d), ...(y,z)}. This particular path
represents the cascading consequence of an event
or the derived dependency of node z on node a,

These paths may not be unique in terms of
effect, they may change over time, and their
behavior may be cumulative in nature, i.e., the end
effect may be the culmination of multiple
predicated events. The intertwining of networks in
this fashion represents a complex system where
emergent behaviors are rarely fully understood.
Rinaldi, Peerenboom and Kelly (see Reference 13)
provide a nice visual representation of this
intertwining and the potential cascading effects.
This is shown in Figure 5.

Figure 5. Cascading consequence example (see Reference 13).

7

3. Given a set of events {E(a), E(b), â¦} and a set
of observed outcomes of on nodes {x, y, z,â¦.},
is it possible to determine the derived
interdependence (abDxyz)?

Problem Space
Thus given the realm of interdependency
analysis, what are the goals for modeling and
simulation efforts? In the analysis of infrastructure
interdependencies and the subsequent emergent
system behaviors, some of the major problem
areas being examined include:

4. Given a set of infrastructure networks and a
critical function, what is the subset of critical
nodes {x, y, z , â¦} across all networks that will
adversely impact a specific mission
functionality due to direct or derived
dependency?

1. Given a set of initiating events {E(a), E(b), â¦}
what is the cascading impact on a subset of
nodes {x, y, z , â¦}?
2. Given a set of nodes {x, y, z,â¦} and a desired
end state, what is a set of events {E(a), E(b),
â¦} that would cause this effect?

8

government as well as the infrastructure relied
upon for the defense and national security of
the U.S.

SURVEY METHODOLOGY
The areas included in this survey were
selected because they focus on modeling and
simulation across multiple infrastructure layers.
Systems such as geographical information systems
(GIS), which may provide geospatial relationships,
are not included unless they possess additional
analytical capabilities.

x Private business, government, and the national
security apparatus increasingly depend on an
interdependent network of critical physical
and information infrastructures, including
telecommunications, energy, financial
services, water, and transportation sectors.

Each model examined in the survey offers
unique capabilities and provides specific insights
into various aspects of the problem domain. The
modeling approaches and the objectives of the
efforts varied greatly. Specific parameters in the
survey were of interest for comparison. One of the
goals of the survey was to identify potential
resources for a wide range of customers and
domains.

x A continuous national effort is required to
ensure the reliable provision of cyber and
physical infrastructure services critical to
maintaining the national defense, continuity of
government, economic prosperity, and quality
of life in the U.S..
x This national effort requires extensive
modeling and analytic capabilities for
purposes of evaluating appropriate
mechanisms to ensure the stability of these
complex and interdependent systems, and to
underpin policy recommendations, so as to
achieve the continuous viability and adequate
protection of the critical infrastructure of the
Nation.16

Six major categories were considered in the
survey:
x Infrastructures
x Modeling and simulation technique
x Integrated vs. coupled models

Although countries tend to have slightly
different lists detailing their âcritical sectors,â
most contain elements of the following:

x Hardware/software requirements
x Intended user

x Agriculture and food

x Maturity level.

x Water

Each of these categories is briefly discussed
below.

x Public health and safety
x Emergency services

Infrastructures

x Government

The U.S. Patriot Act defines critical
infrastructure as âsystems and assets, whether
physical or virtual, so vital to the U.S. that the
incapacity or destruction of such systems and
assets would have a debilitating impact on
security, national economic security, national
public health or safety, or any combination of
those matters.â15

x Defense industrial base
x Information and telecommunications
x Energy
x Transportation
x Banking and finance
x Industry/manufacturing

Further, congress set forth the following
findings in Section 1016 of the U.S. Patriot Act:

x Postal and shipping.
These sectors in turn contain individual
infrastructures such as highways, rail systems,
electric power generation and distribution, etc.

x The information revolution has transformed
the conduct of business and the operations of

9

integrated models tend to model at a much higher
level than coupled models.

Some of these systems are managed by
government agencies, but the majority resides with
industry.

Hardware/Software Requirements

This survey attempts to capture and describe
the infrastructures/infrastructure sectors each
program models. This report seeks to reflect only
those infrastructures that have been actually
modeled and not those presumed to be capable of
being modeled.

In an effort to identify possible tool sets, the
survey captures the portability and exportability of
programs and data.

Intended user
The survey categorizes products as internal
analytical tools intended for internal use only or
external analytical tools available for use outside
the developing organization. This decision relates
to the level of expertise required to use the
product, the application requirements, and the
analytical output of the product. The requirement
is sometimes driven by the size, complexity, or
proprietary nature underlying the data

Modeling and Simulation
Technique
This category attempts to capture the
modeling and simulation method used for the
infrastructure and interdependencies. It has
multiple dimensions that include those of time
(continuous vs. discrete time step) and modeling
technique (Markov chains, Petri Nets, dynamic
simulation, agent-based, physics based, ordinary
differential equations, input-output model, etc.).

Maturity level

Integrated vs. Coupled Models

The following four categories were used to
identify the productâs level of maturity:

During the course of the survey it became
apparent that two different approaches were often
used to conduct cross infrastructure analysis. One
approach was to create an integrated system model
that attempted to model multiple infrastructures
and their interdependencies within one framework.
The other approach coupled a series of individual
infrastructure simulations together, which then
illustrated the cascading influence between them.
An example of this approach would be an electric
grid simulation that determines an outage area for
a specific event. The electrical outage area is then
fed to a telecommunication model used to
determine the subsequent impact on message
routing. This impact is fed to a financial
simulation that determines the loss of
telecommunication impact on commerce and
financial transactions. As one might expect,

x Research â the product is still highly
conceptual without vetted application in
real-world domains.
x Development â the product has been applied
and validated against real-world infrastructure.
Beyond conceptual, the product has been used
by internal or external customers, but is still
undergoing substantial development.
x Mature analytic â the product has reached a
high level of code stability and is part of a
vested internal analytical process. The results
of analysis may be an external report, but the
tool usage is strictly internal to the
organization.
x Mature commercial â the tool is a
commercially licensed product.

10

STATE-OF-THE-ART REPORT
Appendix A contains data on U.S. and
international efforts and interdependency
modeling tools. The information is presented at a
high level with POC information for those desiring
greater detail.

11

POLITICAL, MILITARY, ECONOMIC,
SOCIAL, INFORMATION, AND
INFRASTRUCTURE MODELING
ACTIVITIES
A modeling area that closely follows
infrastructure interdependency modeling is EBO
modeling and analysis. War and conflict are rarely
confined to only the battlefield and force-on-force
engagement. Potential U.S. adversaries comprise a
complex and interdependent system of systems, all
of which contribute, to some degree, toward their
societal coherence, will, and capability to pursue a
course of action contrary to U.S. interests.17
Conflict, war, and reconstruction represent a
complex set of influences, competing goals, and
resources. The battle environment, and thus the
means of victory, are often shaped by the intricate
interactions between them.

generations of warfare, it does not attempt to
win by defeating the enemyâs military forces.
Instead, via the networks, it directly attacks
the minds of enemy decision makers to
destroy the enemyâs political will.â18
Operational Net Assessment (ONA) is the
integration of people, processes, and tools that use
multiple information sources and collaborative
analysis to build shared knowledge of the
adversary, the environment, and ourselves in
understanding and effectively employing EBO.
ONA analytical products are based on a
system-of-systems analysis and the understanding
of key relationships, dependencies, strengths, and
vulnerabilities within and between the adversaryâs
political, military, economic, social, information,
and infrastructure (PMESII) elements. These
products identify leverage points, key nodes, and
links that we can act upon to decisively influence
the adversaryâs behavior, capabilities, perceptions,
and decisions.19

Many point to the emergence of a new generation of warfare termed fourth generation warfare
(4GW). Retired Colonel Thomas Hammes, U.S.
Military Complex, describes this concept:

Within this operating environment, EBOs are
actions that change the state of a system to achieve
directed policy aims using the integrated
application of the diplomatic, informational,
military, and economic instruments of national
power. In order to achieve EBO, however, it is
imperative to understand the relationships and
influences of the PMESII dimensions that shape
the actions of the adversary, of allies, and of your
organization. Figure 6 illustrates this concept
showing a representation of the connectivity and
interdependencies between these dimensions as
both a strength and potential weakness.

â4GW uses all available networksâpolitical,
economic, social, and militaryâto convince
the enemyâs political decision makers that
their strategic goals are either unachievable
or too costly for the perceived benefit. It is an
evolved form of insurgency. Still rooted in
the fundamental precept that superior
political will, when properly employed, can
defeat greater economic and military power,
4GW makes use of a societyâs networks to
carry on its fight. Unlike previous

Figure 6. PMESII node and effects relation (see Reference 19).

12

x LandScan â The LandScan series of data sets
have been developed and are maintained by Oak
Ridge National Laboratory. They are a
population distribution model, database, and
tool developed from census data that
incorporates other spatial information for greater
accuracy and granularity. The LandScan series
consist of LandScan Global representing data in
30 arc second grid cells for ~1 km resolution,
LandScan Interim, which has a 15-arc (~450 m)
second resolution, and LandScan USA with
3-arc-second resolution for ~90 m resolution
with both day and night time population
distributions and demographic and socioeconomic characteristics data.20

DATA SOURCES
The paradigm of modeling and simulation is
âgarbage in, garbage out.â Having credible and
traceable data available to use is key to
infrastructure and interdependency modeling.
Gathering information on a particular infrastructure
is possibly the most significant challenge.
Interdependency modeling also requires that
gathered information (assets) be linked across
multiple infrastructures. Supporting data for these
analyses often spread across multiple data sets. The
fact that most infrastructures data is held by private
industry and, to a large extent, considered
proprietary in nature complicates the situation
further. The data is often accompanied by the
analytical requirement for a certain level of domain
expertise in identifying and validating cross
infrastructure influences.

x National Asset Database â In July 2004, the
Office of Infrastructure Protection (DHS/IP)
initiated a data call to states and territories
requesting a listing of assets deemed of national
or local importance. The collection, named the
National Asset Database, contains basic asset
and facility information, including data
associated with location, POC, and risk
attributes.

The scale of the model also determines the
possible sources of information. Consider, for
example, the electrical power grid. If the goal is to
model assets on a national scale, data equivalent to
transmission level information may suffice with
broad asset effects drawn from course outage area
determination. If the goal is to evaluate a particular
city, compound, or facility, distribution level
information is required reflecting a far greater level
of granularity.

In addition to these specialized data sets, several
DOE national laboratories maintain system expertise
that includes detailed infrastructure data. These
information sets are, to a large degree, the result of
industry nondisclosure agreements and therefore are
not generally releasable for public use.

Commercial geospatial data sets such as those
provided by ESRI, Platts, etc., provide coarse level
data that may suffice for initial model development,
but they lack the detail needed to construct a more
precise model. Public census provides a good data
source for an initial data set. Recall however, that
the census data reflects nighttime residential
demographics in terms of grid-wise statistics, which
may not be adequate in terms of population mobility
and granularity.

x LANL â National electrical generation and
transmission data
x Argonne National Laboratory (ANL) â Natural
gas and oil pipeline data
x Oak Ridge National Laboratory (ORNL) â
National transportation sector information
including rail systems, highway, and waterway
data and models
x Idaho National Laboratory (INL) â National
electrical power SCADA system information.

To mitigate the shortcomings of data, several
efforts have been made to compose and validate
detail infrastructure and demographic data sources.
Two of the data sets used by those surveyed are
LandScan and National Asset Database:

13

and other tools to conduct in-depth analysis.
DOE national laboratories provide the bulk of
this modeling and analysis.22

U.S. RESEARCH AND
SPONSORING ORGANIZATIONS
The modeling and simulation of infrastructure
interdependencies is a substantial effort in terms of
development resources such as infrastructure
expertise, modeling and simulation, data
accessibility, and so on. For this reason, U.S.
government agencies are currently doing most of the
research in this area. In order to understand the
current focus on ongoing research, it is important to
understand the thrust of these organizations. A brief
description of the more prominent supporting
agencies and their programs are described as
follows:
x Department of Homeland Security (DHS) â The
NISAC provides advanced modeling and
simulation capabilities for the analysis of critical
infrastructures, their interdependencies,
vulnerabilities, and complexities. These
capabilities help improve the robustness of our
nationâs critical infrastructures by aiding
decision makers in the areas of policy analysis,
investment and mitigation planning, education
and training, and near real-time assistance to
crisis response organizations. The NISAC
program is sponsored by the DHS Information
Analysis and Infrastructure Protection
Directorate. NISAC is a core partnership of Los
Alamos and Sandia National Laboratories.
NISAC integrates the modeling and simulation
expertise of both laboratories to address the
nationâs potential vulnerabilities and the
consequence of disruption among our critical
infrastructures.21
x Department of Energy (DOE) â The
Visualization and Modeling Working Group
(VMWG) sponsored by DOEâs Office of
Electricity Delivery and Energy Reliability
activates in response to national energy
emergencies to provide data, analyses, and
visualization tools as was done for Hurricanes
Katrina and Rita. The VMWG was formed in
September 2003 to improve the ability of DOE
to perform quick turn-around analyses during
energy emergencies. It is comprised of energy
experts from several DOE offices and energy
infrastructure and modeling experts from
various DOE national laboratories. Their
technical expertise is combined with modeling,
GIS, data libraries on past energy disruptions,

x Technical Support Working Group (TSWG) â
TSWG is an inter-agency organization tasked
with providing technologies to a variety of
government organizations. Their development
and product deployment goals focus on
identifying and answering specific
programmatic needs versus sponsoring national
infrastructure modeling and simulation
initiatives. This study attempts to identify
available and developing resources that may be
utilized to address those needs.23
x Defense Advanced Research Projects Agency
(DARPA) â DARPA is a central research and
development organization for DoD. It manages
and directs selected basic and applied research
and development projects for DoD, and pursues
research and technology where risk and payoff
are both very high and where success may
provide dramatic advances for traditional
military roles and missions. DARPA also has a
research program in the area of crossdimensional infrastructure influence modeling.
By focusing on PMESII dimension interactions,
DARPA is leading the Integrated Battle
Command. The objective of this program is the
development of decision aids to support the
commander in conducting a future, complex,
multidimensional, coalition, and effects-based
campaign. The decision aids will assist the
commander and staff in generating, assessing,
and visualizing the consequences of employing
diplomatic, military, information operations and
economic actions, singularly or in combinations,
to achieve effects against the adversaryâs
PMESII systems. The decision aids will also
assist the commander and staff in constructing,
visualizing, and evaluating campaign plans that
exploit the impact of multidimensional effects
and the interaction among effects.
http://www.darpa.mil/ato/solicit/IBC/index.htm.
x Department of the Air Force, Air Force Materiel
Command, (AFRL) â Similar to DARPA, AFRL
is leading multiple research efforts in
developing PMSEII analytical models. One
effort is the Commander's Predictive
Environment program, whose objective is to
provide a decision support environment that

14

enables the joint force commander to anticipate
and shape the future battlespace. Similar in view
to the DARPA effort, the battlespace is seen as a
complex and interrelated system of PMESII
dimensions. A full understanding of the
battlespace requires comprehension of how
these interrelated factors affect not only the
adversary, but also friendly forces. The focus of

15

this research program is to (1) model and
analyze adversaries, self, and neutrals as a
complex adaptive system; (2) understand key
relationships, dependencies, and vulnerabilities
of adversary/self/neutrals; and (3) identify
leverage points that represent opportunities to
influence capabilities, perceptions, decision
making, and behavior.24

Table 1. Multiscale time hierarchy of power
systems.25

CHALLENGES AND RESEARCH
NEEDS

Action/Operation

Critical infrastructure interdependency modeling
has many of the same challenges that one can expect
with any modeling and simulation domain: data
accessibility, model development, and model
validation. Interdependency modeling is further
complicated by the extremely large and disparate
cross sector analysis required. Many extremely
detailed single sector models have been developed.
One driving research question asks: âHow do we
leverage these existing models into a common
operating picture?â Such a question is further
exasperated by the granularity and the time factors
associated with the models. For example, Table 1
illustrates the multiple time scales that exist within
the electrical power sector.
While currently no standards exists that directly
address infrastructure and specifically cross sector
modeling, standards do exists for exchanging
information between distributed simulations. The
two most common methods are the High Level
Architecture (HLA) and the Distributed Interactive
Simulation (DIS) frameworks.
HLA, developed under the leadership of the
Defense Modeling and Simulation Office is a
general purpose high-level simulation
architecture/framework to facilitate the
interoperability of multiple types of models and
simulations. The purpose of its development is to
support reuse and interoperability across the large
numbers of different types of simulations developed
and maintained by DoD. Within HLA, simulation
objects exist as federates in a larger simulation
federation. HLA was approved as an open standard
through the Institute of Electrical and Electronic
Engineers (IEEE) â IEEE Standard 1516 â in
September 2000.

Wave effects (fast dynamics,
lightning caused over voltages)
Switching over voltages

Microseconds
to milliseconds
Milliseconds

Fault protection

100
milliseconds or
a few cycles
Electromagnetic effects in machine Milliseconds to
windings
seconds
Stability
60 cycles or 1
second
Stability augmentation
Seconds
Electromechanical effects of
Milliseconds to
oscillations in motors & generators minutes
Tie line load frequency control
1 to 10
seconds;
ongoing
Economic load dispatch
10 seconds to 1
hour; ongoing
Seconds to
Thermodynamic changes from
hours
boiler control action (slow
dynamics)
System structure monitoring (what Steady state;
is energized & what is not)
ongoing
System state measurement and
Steady state;
estimation
ongoing
System security monitoring
Steady state;
ongoing
Load management, load
1 hour to 1 day
forecasting, generation scheduling. or longer;
ongoing
Maintenance scheduling
Months to 1
year; ongoing.
Expansion planning
Years; ongoing
Power plant site selection, design,
construction, environmental
impact, etc.

16

Time frame

10 years or
longer

Table 2 provides a listing of HLA strengths and
weaknesses as detailed by Schmitz and Neubecker.26
Additional information on HLA can be found by
contacting hla@dmso.mil or via the website
https://www.dmso.mil/public/transition/hla/.

program. Table 3 provides an assessment of the
strengths and weakens of DIS by the IAPG. Further
information on DIS can be found at
http://www.sei.cmu.edu/architecture/Architectures_
for_DIS.html#291.

Table 2. Strengths and weaknesses of HLA.

Table 3. Strengths and weaknesses of DIS.

HLA Strengths

HLA Weaknesses

DIS Strengths

HLA is an open standard
that will be supported
beyond 2006 (ref. IEEE
1516).

HLA developments may
be subject to significant
changes in order to meet
future needs.

DIS is an open standard
(ref: IEEE 1278.x).

The architecture can be
implemented across
different computing
environments.

Changes to future HLA
standards may have
significant impact on
local implementations.

Provides a documented
process for developing
distributed simulation
systems, e.g., the
federation development
execution process.

U.S. will continue to
lead HLA development
and thus there may be
dependence on U.S.
support for software
implementations.

Efficiency â rigid
structure of data
protocols (PDUs) leads
Provides a set of well
to inefficiency of
defined data protocols to
network resources, e.g.,
support the interaction of
wide area network
real-time simulation
(WAN) bandwidth.
systems.
IEEE standards will not
Availability of COTS
be developed to meet
software support tools
future simulation
(e.g., DIS Stealth
requirements.
Viewers, DIS Data
DIS only supports realLoggers) reduces the
requirements for bespoke time simulations, it does
not support event driven,
developments.
faster than real-time
DIS is a stable âproduct.â
applications.

The architecture can be
implemented across
different computing
environments.

The resources and time
required to implement an
HLA federation can be
Supports real-time, faster
significant â up to
than real-time, and eventdouble that required for
driven time domains.
noncompliant
Availability of
implementations.
commercial off the shelf
HLA does not ensure
(COTS) software support
plug-and-play
tools, e.g., data
interoperability, it
capture/replay, simulation
facilitates
(federation) exercise
communication.
management (reduces the
requirements for bespoke HLA compliance cannot
be established in
developments).
abstract, but only by
reference to a defined
federation.
More âbandwidthâ
friendly.

DIS is another framework for linking real-time
and potentially distributed simulations. Defined
under IEEE Standard 1278, the chief objective of
DIS was to create real-time, synthetic, virtual
representations of the warfare environment. This
environment is created by interconnecting separate,
distributed computers/simulators, called component
simulator nodes. These nodes typically represent
entities on the order of a military unit. DIS has its
roots in the DARPA simulation networking

DIS Weaknesses

Scalability â difficult to
scale up to very large
exercises, e.g., >500
simulation entities.

Limited number of
PDUs.
HLA and DIS are examples of frameworks that
integrate âreal-timeâ simulation models. Information
is passed actively between models and timing
between models is synchronized. This method may
support some aspects of infrastructure model
integration. The issue may arise however when the
computational time for processing a model makes
this type of integration unrealistic, i.e., the
computational requirements greatly exceed
real-time.
One potential method to address this issue and
also to provide a more rapid response capability is to
develop scenario libraries consisting of preprocessed
scenarios with run profiles available for immediate
access. Los Alamos National Laboratory utilizes this
approach with their Scenario Library Visualizer.
Another method of model integration consists of
devising a common architecture to distribute

17

information between models. This method is
currently used by Los Alamos National Laboratory
and NISAC to relate impacts across different
infrastructure models. In a broad sense, a damage
profile based on expected physical damage is
constructed first. An example of this is determining
power outages based on projected high wind
profiles, surge, and flooding models associated with
hurricanes. The physical impact of the event is
transformed into impact on the power grid in terms
of outage areas. This information is then passed to
other models (water, financial, transportation, etc.)
such that the corresponding impact in the electrical
power sector integrates into other sectors. In this
way, impact cascades across infrastructure
boundaries and presents potential effects via
infrastructure interdependencies. This type of model
integration works well when the timing between
infrastructures precludes a true federation of
simulations.
Interdependency discovery and validation is
another challenging area of research. Although
physical interdependencies can be derived by
subject matter experts, doing so on a large scale is a

18

resource challenge. Discovery methods and tools,
including automated mapping, are essential for highfidelity models. Fast Analysis Infrastructure Tool
(FAIT) by Sandia National Laboratory conducts
rough first order interdependency mapping based on
simple rule sets. The IEISS model and Los Alamos
National Laboratory suite of models use outage
areas to identify geospatial and gross order
dependencies. The Critical Infrastructure Modeling
System (CIMS) developed by Idaho National
Laboratory likewise supports geospatial
dependencies, but requires manual direct association
for other dependencies.
Identifying and mapping societal
interdependencies is perhaps the most challenging
aspect in terms of discovery, mapping, and
validation. Identifying a multicultural response and
the duration of impacts on a society is challenging.
The impact of âlikeâ events can be speculated, but
drawing inferences to unforeseen and rare events
relative to the other infrastructure sectors is a
challenging area of active research. This is one of
the main focuses of PMESII research that is
underway.

CONCLUSIONS
Infrastructure interdependency modeling is a
relatively new area of research and analysis, but
recent events of both natural disasters and malicious
acts have shown that the impact of these cross
infrastructure relationships can be measured.
Significant research efforts are underway in the U.S.
and abroad.
One observation resulting from this effort is that
no cross program working group or forum is
specifically dedicated to this critical area of
research. Most research exchange occurs within

19

specific programs. Consequently, a limited exchange
of ideas has occurred across the sponsoring agencies
in this area. The strongest collaboration exists
between DHS and DOE, mainly due to the fact that
the same research teams are sponsored by both
organizations. One suggestion from our study is the
development, whether formally or informally, of a
national or international working group with a
central focus of infrastructure interdependency
analysis. It is hoped that this state-of-the-art report
will serve to not only report on current activities, but
will also act as a catalysts for information exchange
for such activities.

ACKNOWLEDGEMENT
We appreciate the many contributors to this
report. Our preferred method has been to directly
interact with the project leaders in collecting this
information. All that have participated have been
extremely supportive. Again, this is an ongoing
project and we apologize to those efforts which were
not recognized in this first report.

20

Please forward comments on material contained
within this document and also points of contacts for
those efforts not covered in this initial document to
Donald.Dudenhoeffer@inl.gov.
Finally, we would like to express our gratitude
to Dr. Steve Fernandez of Los Alamos Laboratory
who acted as a constant guide and source of data for
this report.

Appendix A

21

Table Abbreviations:
Infrastructure Sectors
EP
Electric Power
NG
Natural Gas
DW
Drinking Water
SW
Sewage Water
ST
Storm Water
HA
Human Activity
FN
Financial Networks
SCADA
Supervisory Control and Data Acquisition
TC
Telecom
CN
Computer Networks
OL
Oil Pipeline
RL
Rail System
HW
Highway System
WW
Waterway System
POL
Policy/Regulatory constraints
Simulation Type
I
Input-Output Model
A
Agent-based
Intended Users Types
IA
Internal Analyst
EA
External Analyst
B
Both
Maturity Level
RS
Research
DV
Development
MI
Mature Internal
MC
Mature Commercial

22

Model Name
Organization
POC

Agent-Based Infrastructure Modeling and Simulation (AIMS)
University of New Brunswick
Infrastructures
Dr. Ali Ghorbani, Professor
Various
ghorbani@unb.ca
Dr. Stephen Marsh, Adjunct Professor
Stephen.Marsh@nrc-cnrc.gc.ca

Description
Overview â AIMS is an agent-based system to simulate and model the (national and cross-border)
interdependencies and survivability of Canadaâs Critical Infrastructures.
Development goals â Goals are to incorporate into complete critical infrastructure (CI) crisis
management system and plan to model New Brunswickâs Information and Communication Technology
(ICT), power, water, etc.
Intended users â Users will include CI managers, users, planners, and emergency services personnel.
System output â Visualization for training monitoring and planning. Itâs a possibility to add mapping
systems.
Maturity â The system is in development.
Areas modeled â New Brunswick Critical Infrastructures.
Customers/sponsors â National Research Council Canada (CNRCC).
Model Framework
Underlying model â Agent-based modeling uses universal mark-up language (UML) and service
oriented architectures. Plans are to use a multi-agent development kit in the future such as Agent
Oriented Software Group (AOS) JACKTM, Java Agent DEvelopment (JADE) framework, or other
agent software.
Simulation â Simulation scenarios have included the Moncton area forest fire, the Saint John Port
disaster, and a Border incident.
Data format â The data and text are in UML and Environmental Systems Research Institute, Inc.
(ESRI)âs ArcGIS formats.
Sensor data â Not specified.
Coupling with other models â This model is designed to couple with other models, but that capability
has not been tested to date.
Human activity modeling â Included in model.
System Requirements
Not specified.
Hardware
Not specified.
Software
Other Notes

References
Stephen Marsh, Critical Infrastructure Interdependencies, http://iit-iti.nrc-cnrc.gc.ca/colloq/0405/0411-04_e.html, November 4, 2004, Webpage visited July 10, 2006.

23

24

Model Name Athena
Organization
POC

On Target Technologies, Inc.
Dr. Brian Drabble
brain@ontgttech.com
Dr. Maris âBusterâ McCrabb
buster@dmmventures.com

Infrastructures
All (physical to
conceptual)

Description
Overview â Athena is an analysis and modeling tool that is designed to analyze a network of nodes
(actors, concepts and physical) as a âsystem of systemsâ by merging various political, military,
economic, social, information, and infrastructure (PMESII) models and their associated crossdependencies. Athena incorporates several reasoning algorithms that allow sophisticated inter- and
intra-dependency analysis between and through nodes. Model construction is quick and simple point
and uses a simple point and click interface.
Development goals â Automatic Network Extraction (engineering models), Semantic Reasoning
across transitive dependencies & Interfacing to different information sources.
Intended users â Military for in analyzing disruptive military effects, Law Enforcement for analyzing
disruptions of criminal gangs and enterprises, Disaster/Network Recovery to determine repair
priorities, and Economic for competitive analysis.
Output â Graphical interface showing nodes and linkages with criticalities and interdependencies
indicated. Multiple analytical capabilities. Can be linked to GIS data.
Maturity â Evolving.
Areas modeled â Athena is capable of modeling any entity including countries, states, cities, roads,
and facilities.
Customers/sponsors â Air Force Research Laboratory (AFRL) IFSA sponsored the original work.
Funding is now provided by DARPA and USSTRATCOM who will deploy the tool in late 2006.
Model Framework
Underlying model(s) â Fusion of Barlowâs model of horizontal cross-dependency with weighting,
Wardenâs model of vertical cross-dependency, and the McCrabb-Drabble model of time-phased
linkages between models. This is a fractal model that allows the description of a Strategic Entity
through Centers of Gravity (COG) to Target Systems to Target sets and where appropriate targets.
Simulation â System allows full-scale simulations.
Data format â Accepts data in variety of formats.
Sensor data â Accepts sensor data/feeds to update model nodes and changing interactions (e.g.,
strength) between nodes.
Human activity â This tool models human activity/capability as part of the network (e.g., loss of plant
manager may decrease network capability). Nodes may be humans or concepts.
Coupling with other models â Couples readily with other engineering models, databases, sensor
networks, etc.
System Requirements
Laptop 2 GB processor speed, 60 GB hard drive, 500 MB RAM.
Hardware
Windows XP or similar, program is written in C.
Software
Other Notes

25

References
Athena: Effects-based Cross-Dependency Modeling for Target Systems Analysis Final Report.
(Limited distribution) Final Athena Demonstration (Microsoft PowerPoint presentation).

26

Model Name
Organization
POC

CARVER2 TM
National Infrastructure Institute Center for
Infrastructure Expertise
Ronald Peimer
rpeimer@ni2.org

Infrastructures
User defined

Description
Overview â CARVER2 is a simple software program that provides a quick and easy way to prioritizes
potential terrorist targets. It compares and rates the critical infrastructure and key assets in jurisdictions
by producing a mathematical score for each potential target. It is the first step for conducting more indepth vulnerability assessments. CARVER2 helps users make âapples vs. orangesâ comparisons such
as a water system vs. an energy grid vs. a bridge.
Development goals â None goals have been stated.
Intended users â Federal, state and local government officials are the intended users for this program.
Output â The CARVER2 tool outputs various reports with priority scores and background information
for different infrastructure elements.
Maturity â This is a free product by request.
Areas modeled â Determined by user.
Customers/sponsors â This tool is Sponsored by the US Department of Commerce, National Institute
for Standards and Technology (NIST).
Model Framework
Underlying model(s) â The support is a relational database.
Simulation â This tool has no simulation capability.
Data format â Text.
Sensor data â No sensor data has been incorporated in the tool.
Human activity â Not modeled.
Coupling with other models â There is no coupling with other models.
System Requirements
PC or laptop running Microsoft Windows operating system.
Hardware
Distributed via CD no other software needed.
Software
Other Notes

27

References
NI2 Center for Infrastructure Expertise Critical Infrastructure Library,
http://www.ni2ciel.org/,Webpage visited July 3, 2006.
National Infrastructure Institute home page, http://www.ni2.org/default.asp, Webpage visited July
3, 2006
CARVER2 Project Page, http://www.ni2cie.org/CARVER2.asp, Webpage visited July 3, 2006.
28

Model Name
Organization
POC

COMM-ASPEN
Sandia National Laboratory

Infrastructures
FN, TEL

Description
Overview â CommAspen is a new agent-based model for simulating the interdependent effects of
market decisions and disruptions in the telecommunications infrastructure on other critical
infrastructures in the U.S. economy such as banking and finance, and electric power. CommAspen
extends and modifies the capabilities of Aspen-EE, an agent-based model previously developed by
Sandia National Laboratories to analyze the interdependencies between the electric power system and
other critical infrastructures. CommAspen has been tested on a series of scenarios in which the
communications network has been disrupted, due to congestion and outages. Analysis of the scenario
results indicates that communications networks simulated by the model behave as their counterparts do
in the real world. Results also show that the model could be used to analyze the economic impact of
communications congestion and outages.
Development goals â To analyze interdependent infrastructure systems in a more holistic way, Sandia
and other research institutions have developed models of critical infrastructure systems using agentbased approaches. Sandiaâs first agent-based model of the U.S. economy, developed in the mid-1990s,
is called Aspen. This model is a Monte Carlo simulation that uses agents to represent various decisionmaking segments in the economy, such as banks, households, industries, and the Federal Reserve. An
agent is a computational entity that receives information and acts on its environment in an autonomous
way; that is, an agentâs behavior depends at least partially on its own experience. Through the use of
evolutionary learning techniques, Aspen allows us to examine the interactive behavior of these agents
as they make real-life decisions in an environment where agents communicate with each other and
adapt their behaviors to changing economic conditions, all the while learning from their past
experience. In 2000, Sandia developed a new model of infrastructure interdependency called AspenEE. This model extended the capabilities of Aspen to include the impact of market structures and
power outages in the electric power system, a critical infrastructure, on other infrastructures in the
economy.
One of the limitations of agent-based models in current development at Sandia and other research
institutions is that communication is treated simply as a message passing between agents. Effectively,
the telecommunications infrastructure is not specifically represented. None of the models simulates the
differences in communication over telephone, computer, wireless, or other networks and therefore
cannot model the impact of specific communication failures on the whole system. Nor can current
models simulate the impact of other infrastructure failures on telecommunications.
To address the communications deficiencies described above, Sandia revised and restructured the
Aspen-EE model to include a more realistic representation of the telecommunications infrastructure.
This new model of infrastructure interdependency is called CommAspen. In CommAspen,
communication is treated as an integrated agent system capable of creating, transforming, sending,
receiving, and storing information and messages over time and across distance. With CommAspen, we
can model communication networks or medium-specific vulnerabilities to failures and their
dependence on supporting infrastructures like power.
Intended users â Internal analyst.
System output â Not specified.
Maturity â Development.
Areas modeled â Not specified.
Customers/sponsors â Not specified.
29

Model Framework
Underlying model â There are several ways that we can implement the notion of infrastructures in
CommAspen. One method of representing certain types of infrastructures in CommAspen is through
the use of spigots and sinks. Such infrastructures are for commodities that run continuously, like water
from a municipality and electricity from a local utility. A sink is where a producer puts product into an
infrastructure. For example, a power company may have a natural gas-fired electric generating plant
producing power. It would put power on the transmission lines by passing the power into the associated
sink. A spigot is where a consumer gets the product, such as turning on the lights in a residence or
getting water from a faucet.
Simulation â Agent Based Model.
Data format â Not specified.
Sensor data â None.
Coupling with other models â No.
Human activity modeling â Not Known.
System Requirements
Not specified.
Hardware
Not specified.
Software
Other Notes

Images: None.

References
Barton, Dianne C., Eric D. Edison, David A. Schoenwald, Roger G. Cox, and Rhonda K.
Reinert, "Simulating Economic Effects of Disruptions in the Telecommunications Infrastructure",
SAND REPORT, SAND2004-0101, Printed January 2004.

30

Model Name
Organization
POC

Critical Infrastructures Interdependencies Integrator (CI3)
Argonne National Laboratory
Infrastructures
Dr. James Peerenboom
EP,NG,SCADA,TC
jpeerenboom@anl.com

Description
Overview â CI3 is a software tool for emulating (Monte Carlo simulation) the amount of time or cost
(or both) needed for activities that must be completed to restore a given infrastructure component, a
specific infrastructure system, or an interdependent set of infrastructures to an operational state. The
software tool provides a framework for recognizing interdependencies and incorporating uncertainty
into the analysis of critical infrastructures.
Development goals â No goals stated.
Intended users â Infrastructure owners.
System output â Graphs and tables of completion time and cost distributions for repairs to quantify the
impacts of infrastructure disruptions.
Maturity â The system is in development.
Areas modeled â No specific areas are mentioned. Argonne has developed transition diagrams for
repair of damages to the following: natural gas transmission pipeline, petroleum, oil, liquids (POL)
pumping station, natural gas city gate station, propane air plant, natural gas compressor station, natural
gas underground storage facility, supervisory control and data acquisition (SCADA) communications
tower, electrical substation, transformer, and an optical telecommunications cable.
Customers/sponsors â U.S. Department of Energy.
Model Framework
Underlying model â Transition diagrams coupled to Monte Carlo simulator.
Simulation â Transition diagrams are easy to create via point-and-click techniques to simulate
recovery and restoration activities for covered infrastructure.
Data format â Not specified.
Sensor data â Model does not accept sensor data.
Coupling with other models â None.
Human activity modeling â Human activities (travel, repair, assessment) are included in simulations.
System Requirements
Not specified.
Hardware
Not specified.
Software
Other Notes

31

References

32

Model Name
Organization
POC

Critical Infrastructure Modeling System (CIMSÂ©)
Idaho National Laboratory (INL)
Donald Dudenhoeffer
Donald.Dudenhoeffer@inl.gov

Infrastructures
EP, SCADA, HW,
HA, POL, PMESII

Description
Overview â A modeling and simulation framework that combines geospatial information and a four
dimensional (4D) environment (time-based) to support âwhat ifâ analysis.
Development goals â Provide decision makers with a highly adaptable and easily constructed
âwargamingâ tool to assess infrastructure vulnerabilities including policy and response plans. Operating
at a high level of simulation, it supports rapid âpoint and clickâ model development to allow the
adaptation of models to rapidly changing environments.
Intended users â Emergency planners and responders.
System output â Four dimensional geospatial visualization in a VTK framework along with report
generation.
Maturity â Development â in the process of commercial licensing.
Areas modeled â Idaho National Laboratory, New Orleans Louisiana.
Customers/sponsors â Research has been ongoing for the past 4 years under the INL National
Security Divisions. Sponsors have included the INLâs internal research program, the Department of
Energy, the U.S. Air Force Research Laboratory (AFRL), and negotiations are underway with the State
of Louisiana.
Model Framework
Underlying model â The underlying model is a network representation of infrastructure utilizing
nodes and edges for assets and relationships. Graphical objects such as aerial images, 3DS images, or
VRML models can be tied to the assets. Additionally, information can be embedded within nodes such
as documents, web site hyperlinks, web cams, avis, etc
Simulation â Agent-based discrete event simulation.
Data format â Flat files are used as direct feeds to the simulations. These files can be fed by a
multitude of different databases including Access, GIS, etc
Sensor data â Agent objects(nodes) can have autonomous behaviors or they can be fed by external
sensor input.
Coupling with other models â Yes.
Human activity modeling â Human activity can be modeled directly or as the result of
policy/procedure enactment.
System Requirements
Cross platform compatibility â Windows, UNIX/LINUX, and Solaris. Internet
Hardware
connectivity required to access embedded links.
No external software to CIMS ~ requires < 5 meg of disk space.
Software

33

Other Notes
The objective of CIMS was to create a rapid modeling and analysis capability that did not require
extensive data collection or proprietary GIS software. As such, CIMS allows the ability to create
models and infrastructure simulations on the fly embedding new intelligence as it becomes available.
Model development can start with an aerial image or a scanned/sketched chart/map image. All
information is georeferenced.
Models construction can occur via one of three methods.
x Direct manipulation of the network descriptor flat files
x Conversion from a database to the flat file format
x Point and click network construction via the Model Builder Application.
User interactivity with the Model. The models were developed with a wargaming approach to allow
maximum user interaction with the simulation. Thus the user has several different ways to interact with
the data:
x An event script can be created to initiate specific events at a designated time
x The user can select and directly manipulate the state of individual nodes and edges, i.e., shutting
down an electrical substation or making a bridge impassible
x The user can inject events during runtime, i.e., placing and detonating a bomb to observe cascading
impacts.

34

New Orleans Model

Damage Profile due to flooding â illustrating loss of infrastructure.

3D Stereo Representation of downtown on SGI
P i

Images
35

Model showing loss of an Electrical Substation

Rotated Side view showing building profiles at an
angle with the electrical infrastructure separated from
the buildings to highlight the connectivity. Multiple
infrastructures can be displayed to show direct and
spatial relationships.

36

References
Dudenhoeffer, D.D, M. R. Permann, and M. Manic, âCIMS: A Framework For Infrastructure Interdependency
Modeling And Analysis.â Submitted to Proceedings of the 2006 Winter Simulation Conference, ed L. F.
Perrone, F. P. Wieland, J. Liu, B. G. Lawson, D. M. Nicol, and R. M. Fujimoto. Piscataway, New Jersey:
Institute of Electrical and Electronics Engineers, 2006.
Dudenhoeffer, D. D., M. R. Permann, and R. L. Boring, 2006. Decision consequence in complex environments:
Visualizing decision impact. In Proceeding of Sharing Solutions for Emergencies and Hazardous
Environments. American Nuclear Society Joint Topical Meeting: 9th Emergency Preparedness and
Response/11th Robotics and Remote Systems for Hazardous Environments.
Dudenhoeffer, D. D., M. R. Permann, and E.M. Sussman. 2002. A Parallel Simulation Framework For
Infrastructure Modeling And Analysis. In Proceedings of the 2002 Winter Simulation Conference, ed E.
YÃ¼cesan, C. H. Chen, J. L. Snowdon, and J. M. Charnes, 1971-1977. Piscataway, New Jersey: Institute of
Electrical and Electronics Engineers.

Critical Infrastructure Modeling System Fact Sheet
<http://www.inl.gov/nationalsecurity/factsheets/docs/cims.pdf>

37

38

Model Name
Organization
POC

The Critical Infrastructure Protection Decision Support System (CIP/DSS)
Los Alamos National Laboratory
Infrastructures
Randy Michelsen
ALL
rem@lanl.gov
Sandia National Laboratories
Theresa Brown
tjbrown@sandia.gov

Description

Overview â The Critical Infrastructure Protection Decision Support System (CIP/DSS)
simulates the dynamics of individual infrastructures and couples separate infrastructures to
each other according to their interdependencies. For example, repairing damage to the electric
power grid in a city requires transportation to failure sites and delivery of parts, fuel for repair
vehicles telecommunications for problem diagnosis and coordination of repairs, and the
availability of labor crews. The repair itself involves diagnosis, ordering parts, dispatching
crews, and performing work. The electric power grid responds to the initial damage and to the
completion of repairs with changes in its operating characteristics. Dynamic processes like
these are represented in the CIP/DSS infrastructure sector simulations by differential
equations, discrete events, and codified rules of operation. Many of these variables are output
metrics estimating the human health, economic, or environmental effects of disturbances to the
infrastructures.
CIP/DSS will assist decision makers in making informed choices by:
x Functionally representing all 14 critical infrastructures with their interdependencies
x Computing human health and safety, economic, public confidence, national security, and
environmental impacts
x Synthesizing a methodology that is technically sound, defensible, and extendable.
Development goals â Charter is to model all infrastructures and key assets. Used for quick response on
areas Los Alamos National Laboratory (LANL) doesnât have data for.
Intended users â Internal analyst at LANL.
System output â Graphs representing the impact on multiple state variables such as hospital beds
occupied, etc.

Maturity â Development â Initiated as a proof-of-concept in August 2003. Completed a
prototype model and two case studies in February2004.
Areas modeled â Not specified.
Customers/sponsors â DHS.
Model Framework
Underlying model â The national and metropolitan consequence models are implemented using
Vensim, which reads input parameters from and writes output time series to an Oracle relational
database of âconsequenceâ metrics, which are abstracted into a much smaller set of âdecisionâ metrics.
The decision support software (written in Visual Basic) accesses the decision database to compute
utility values for various scenarios and alternatives.
Simulation â Vensim is used for developing, analyzing, and packaging high quality dynamic feedback
models. Models are constructed graphically or in a text editor. Features include dynamic functions,
subscripting (arrays), Monte Carlo sensitivity analysis, optimization, data handling, and application
interfaces.
Data format â Vensim Model.
Sensor data â No ability to input live data feeds.
39

Coupling with other models â No.
Human activity modeling â Human activity can be modeled directly or as the result of
policy/procedure enactment.
System Requirements
The Vensim family of software runs on Windows (95/98/Millennium/NT/2000/XP)
Hardware
and the Power Macintosh running System 7 or higher (in Classic mode under OSX).
Vensim requires 8 MB of memory and 8 MB of disk space for a full installation. A
demonstration version of Vensim is available free for either Windows or Macintosh.
CIPDSS is a model built within Vensim simulation software by Ventura
Software
(http://www.vensim.com/brochure.html).
Other Notes
CIP/DSS (Critical Infrastructure Protection Decision Support System) simulates the dynamics of
individual infrastructures and couples separate infrastructures to each other according to their
interdependencies. CIP/DSS models asset information at the aggregate level. For example with a focus
area, it can estimate the number of hospital beds affected by an event, but it cannot directly retrieve
information relative to a particular hospital. It utilizes the commercial simulation software Vensim.

40

References
Bush B, L. Dauelsberg, R. LeClaire, D. Powell (LANL), S. DeLand (SNL), and M. Samsa (ANL),
Critical Infrastructure Protection Decision Support System (CIP/DSS) Project Overview, LA-UR-051870, July 2005.

41

42

Model Name
Organization
POC

Critical Infrastructure Protection (CIP) Modeling and Analysis (CIPMA) Program
Australian Government â Attorney Generalâs
Infrastructures
Department (AGD)
FN, TC, EP, NG, OL
Michael Jerks â Director, Major Projects
Michael.Jerks@ag.gov.au

Description
Overview â The Critical Infrastructure Protection Modeling and Analysis program (CIPMA) is a
computer based tool to support business and government decision making for critical infrastructure
(CI) protection, counter-terrorism and emergency management, especially with regard to prevention,
preparedness, and planning and recovery. CIPMA is designed to examine the relationships and
dependencies within and between critical infrastructure systems, and to demonstrate how a failure in
one sector can greatly affect the operations of critical infrastructure in other sectors. CIPMA uses a vast
array of data and information from a range of sources to model and simulate the behavior and
dependency relationships of critical infrastructure systems. The capability will include a series of
impact models to analyze the effects of a disruption to CI services. The CIPMA Program currently
focuses on three priority sectors: banking and finance, communications, and energy. The capability was
launched by the Attorney-General in February 2006. âProof of conceptâ of the capability was
successfully demonstrated to key business and government stakeholders in May 2006. Although
CIPMA is still in development, results from the capability are already assisting the development and
direction of government policy in national security and critical infrastructure protection (CIP), and
helping owners and operators to better protect their critical infrastructure.
Development goals â The current focus is on broadening and deepening CIPMA coverage of the three
priority sectors, the Sydney commercial business district (CBD) precinct, and development of impact
models for the Decision Support Module. The impact models will assess the flow-on consequences of a
CI service disruption, the economic impacts of the disruption, the effects on population, time/duration
and area of the disruption, and the behavior of networks and clusters of infrastructure as a result of the
service interruption. Work on a fourth sector will commence by July 2007.
Intended users â Users include CI owners and operators and Australian local governments.
System output â Output will include geographic information system (GIS) functionality for data
capture, management, and visualization. System behavior will determine dependencies and time-based
impacts of disruptive events on infrastructure networks.
Maturity â In development, some tools are complete.
Areas modeled â Australian critical infrastructure networks and high priority precincts (e.g., capital
cities).
Customers/sponsors â Australian government, state and territory governments, CI owners and
operators.
Model Framework
Underlying model(s) â System Dynamic Models.
Simulation â Telecommunication connectivity matrix and expert systems.
Data format â The format is geographic information system (GIS) and relational database.
Sensor data â Not currently equipped for sensor input.
Human activity â Contains human activity model.
Coupling with other models â Model couples with earthquake, tsunami inundation, bomb blast, and
plume models.
System Requirements
Not specified.
Hardware
ArcGIS, ArcSDE, Oracle, Vensim DSS, Dynamic Network System (DNS), CLIPS,
Software
43

Java Runtime Environment (JRE).
Other Notes
CIPMA is a very detailed modeling and analysis initiative which contains sensitive business
information about the operation of Australia's critical infrastructure networks, relationships and
dependencies. The IP is owned and managed by Attorney-General Department (AGD) on behalf of the
Australian Government. The CIPMA Development Team of AGD, Geoscience Australia (GA) and the
Commonwealth Scientific and Industrial Research Organization (CSIRO) has been in discussions with
the US Department of Homeland Security (DHS) and Argonne, Sandia, and Los Alamos National
Laboratories regarding the Critical Infrastructure Decision Support System (CIP-DSS), and the
similarities and differences between the two capabilities, since November 2004. AGD is currently
preparing a Project Arrangement for ongoing consultation with DHS and the three National labs under
the Homeland Security Science and Technology Treaty (HSST).
References
Fact sheet on CIPMA program
http://www.tisn.gov.au/agd/WWW/rwpattach.nsf/VAP/(7A188806B7893EBA0402BC1472412E58)~
Overview+of+CIPMA.PDF/$file/Overview+of+CIPMA.PDF, Webpage visited July 3, 2006.
AusGeo News, Protecting the Nation, http://www.ga.gov.au/ausgeonews/ausgeonews200509/cip.jsp,
Issue No. 79, September 2005, Webpage visited July 3, 2006.

44

Model Name
Organization
POC

Critical Infrastructure Simulation by Interdependent Agents (CISIA)
Universita Roma Tre
Infrastructures
Stefano Panzieri
EP,SCADA
panzieri@uniroma3.it
Giovanni Ulivi
ulivi@uniroma3.it

Description
Overview â This model is described by the authors as a hybrid of the two modeling approaches;
interdependency analysis and system analysis. It is a bottom-up complex adaptive systems (CAS)
model using interactive agents. The critical infrastructure simulation by interdependent agents (CISIA)
simulator is designed to analyze short term effects of failures in terms of fault propagation and
performance degradation (Panzieri, 2004). The simulator is based on Recursive Porus Agent
Simulation Toolkit, Repast, open-source agent-based development software with libraries of classes for
creating, running, displaying and collecting data from a agent based simulations. It extends the Java
classes of Repast defining a new class for each type of macro component present into any
infrastructure: such as, electric power plant, transmission line, telecommunication channel, waste-water
system, etc.
Development goals âWork is ongoing to further validate the CISIA approach and to analyze how
intelligent reaction, and autonomy capabilities (e.g., decentralized control strategies), might be used to
improve the robustness of the system of systemâs composed by different heterogeneous and
interdependent infrastructures.
Intended users â Infrastructure owners, planners, and emergency responders.
System output â Graphic models showing the operative level incidence matrix and physical fault
incidence matrices (FIMs) between elements in the model. In this case air conditioning, electric power,
and SCADA.
Maturity â The system is in development.
Areas modeled â An unspecified (for security reasons) University Campus.
Customers/sponsors â Not indicated.
Model Framework
Underlying model â Agent-based model based on Repast, in order to handle many heterogeneous
infrastructures into a single framework. Agent behavior is abstracted to allow use of a small set of
common quantities; operative level, requirements (needs), and faults. Agent interactions include;
induced faults, input requirements, and input operative level. Outputs include: propagated faults, output
requirements, and output operative level.
Simulation â During simulation agents communicate via messages. An agent sends messages to its
neighbors to specify its requirements to communicate its level of service (operative level), and to
propagate faults (physical-faults, geographical-faults, and cyber faults).
Data format â Relational database.
Sensor data â Model does not accept sensor data.
Coupling with other models â CISIA implements an easy-linkage/black box philosophy: any model
obtains connecting together agents without any modification of their internal structure.
Human activity modeling â Not incorporated.

45

System Requirements
Not specified.
Hardware
Not specified.
Software
Other Notes
Each agent class defines the behavioral roles of the element and its input/output quantities in term of
which resources the agent needed and supply. Moreover, the class defines which type of failure can be
propagated to (generated from) the agent. An agent may propagate different types of failure to a
different set of neighbors.

References
Panzieri, S., R. Setola, G. Ulivi (2004). An agent based simulator for critical interdependent
infrastructures. Proc. 2nd International Conference on Critical Infrastructures, October 24-27, 2004.
Panzieri, S., R. Setola, G. Ulivi , An Approach to Model Complex Interdependent Infrastructures,
International Federation of Automatic Control (IFAC),
http://www.dia.uniroma3.it/~panzieri/Articoli/WorldIFAC05CIIP.pdf#search='An%20Approach%20to%20Model%20Complex%20Interdependent%20Infrastructur
es', Webpage visited July 10, 2006.

46

Model Name
Organization
POC

Distributed Engineering Workstation (DEW)
Electrical Distribution Design, Inc.
Dr. Robert Broadwater
dew@vt.edu

Infrastructures
EL, SCADA

Description
Overview â The Distribution Engineering Workstation (DEW) provides over 30 applications for
analysis, design, and control of electrical and other physical network systems. DEW allows all of its
components (data sets and algorithms) to be reused by a new application, allowing new solutions to
build on top of existing work. This provides for cross collaborations among different groups and the
emergence of solutions to complex problems. DEW is being used to identify and analyze
interdependencies in large scale electrical power systems and fluid systems of aircraft carriers. DEW is
open architecture, non-proprietary.
Development goals â Electrical Distribution Design, Inc. (EDD) continues to develop and support
DEW. They aspire to achieve combined analysis of systems with millions of nodes and to develop a
seamless approach to asset management. DEW's architecture provides an open platform for
development. The DEW system model can be linked to asset management records, daily operational
procedures, events, long- and short-term planning, and more.
Intended users â Users are utilities, analysts, and military.
System output â The system is used for operation and control of electrical system and analysis of
reconfiguration of damaged systems.
Maturity â Mature product is in broad use.
Areas modeled â This model has been used in St. Louis, MO, Detroit, MI, Consolidated Edison, NY,
Aircraft Carriers.
Customers/sponsors â Electric Power Research Institute (EPRI) along with Department of Defense
and Department of Energy sponsored the original development. Users include Northrop Grumman
(naval applications), Detroit Edison (Detroit, MI), Ameren (St. Louis, MO), Orange and Rockland
(Pearl River, NY), and Consolidated Edison (New York).
Model Framework
Underlying model â EDDâs approach is built around a combination of concepts from graph theory,
physical network modeling, and generic programming. The DEW model incorporates power flow,
fault, reliability, reconfiguration for restoration, and over 30 other algorithms.
Simulation â Simulations may be run manually with mouse and keyboard, automatically controlled
from user developed applications, or set up to run in batch mode over numerous systems and/or time
points.
Data format â Model data is stored in relational SQL-compliant databases; real-time measurement
data comes from common object request broker architecture (CORBA) interface or plant information
(PI) time series databases.
Sensor data â DEW can handle any number of measurements and any types of measurements that are
modeled, through its PI or CORBA interface.
Coupling with other models â DEW can attach to other models, such as geographic information
system (GIS) models, via provided interface.
System Requirements
Laptop/Server/Circuit server.
Hardware
Win 2000, XP, User interface.
Software
Other Notes
EDD is working with the utility industry, Virginia Tech, and other universities to develop a
47

comprehensive Integrated System Model (ISM) based design, operations and maintenance
management system. This concept is being applied to critical infrastructures including naval ships and
gas and water utilities. Through work with the utility industry and Department of Energy, EDD has
demonstrated it is possible to use the same ISM for analysis, design, operations, and real-time control.
EDD has also used ISM based analysis to manage reconfigurable system models with more than 3
million objects and 200 million attached historical measurement values. The ISM provides a complete,
seamless view of a physical plant that forms a common context for multi-discipline team collaboration,
distributed processing, synergistic research and development, and providing infinite extensibility. Any
data or algorithm that can be attached to the ISM is also associated with all other data and algorithms
attached to the ISM. The ISM uses linked list type traces to dynamically adapt data management and
analysis whenever the system is changed through modification, maintenance or operation.
EDD is structuring its current research and development work so that it that can eventually be
combined into a generic integration platform for collaborative analysis, design, and operations for
energy systems (CADOE). CADOE will directly support and structure low overhead collaboration
among electric utilities, gas utilities, regulatory and policy making agencies, suppliers, integrators,
aggregators, and customers. CADOE is envisioned to encompass simulation, analysis, alternative
design evaluation, training, and real-time operations support.

Model of Ship Critical Infrastructure.

48

Dense Electrical Power System Model
References
Broadwater, Robert, et al., Power Engineering,
http://www.ecpe.vt.edu/news/ar04/power2004.pdf#search='distributed%20engineering
%20workstation%20epri', Webpage visited July 3, 2006.
SAM Six, Products: Dew, http://www.samsix.com/dew.htm, Webpage visited July 3,2006.
Tam, Kwa-Sur and Robert Broadwater, Virginia Tech Presentation,
http://www.eng.vt.edu/research/dom_pres/TamBroadwater%20Systems%20Presentation.pdf#search='vt%20dew' Webpage visited July 3, 2006.

49

50

Model Name
Organization
POC

Electricity Market Complex Adaptive System (EMCAS)
Argonne National Laboratory
Guenter Conzelmann (ANL)
guenter@anl.gov

Infrastructures
Power Systems and
Markets

Description
Overview â Electricity Market Complex Adaptive System (EMCAS) uses agent-based modeling to
simulate the operation of complex power systems. EMCAS can be used as an âelectronic-laboratoryâ
to probe the possible operational and economic impacts on the power system of various external
events. Market participants are represented as âagentsâ with their own set of objectives, decisionmaking rules, and behavioral patterns. Agents are modeled as independent entities that make decisions
and take actions using limited and/or uncertain information available to them, similar to how
organizations and individuals operate in the real world. EMCAS includes all the entities participating
in power markets, including consumers, generation companies (GenCos), Transmission Companies
(TransCos), Distribution Companies (DisCos), Demand Companies (DemCos), Independent System
Operators (ISO) or Regional Transmission Organizations (RTO), and regulators.
Development goals â Continue to develop EMCAS as a new approach to model and simulate the
operations of restructured electricity markets.
Intended users â EMCAS was first applied for a regulatory commission in the mid-western United
States. At the beginning of 2005, the software became commercially available and current clients
include research institutes, power companies, transmission companies, and regulatory offices in South
Korea, Portugal, and Spain. The Iberian EMCAS application includes the simulation of hydropower,
wind power, and a variety of other renewable resources.
System output â EMCAS utilizes a graphical user interface to develop market configurations, display
model inputs, and analyze simulation results (see screen captures on next page). Results are stored in
HDF format and can be exported in text and spreadsheet formats. In addition to the energy spot
markets and bilateral financial contract markets, EMCAS also includes a simplified representation of
ancillary services markets; Detailed representation of the transmission system, using a Direct Current
Optimal Power Flow (DC OPF) algorithm to compute locational marginal prices (LMP) and identify
transmission congestion and price impacts of congestion; Chronological simulation of hourly market
prices over short or long time periods; Hourly bid-based market clearing, scheduling and dispatch in
day-ahead and real-time markets; Representation of different bidding strategies, from production cost
bidding to various forms of physical and economic withholding strategies; Ability to change prevailing
market rules (regarding congestion management, pricing mechanisms, price caps etc.) provides the
opportunity to test the robustness and vulnerability to gaming of different market designs; and
Calculation of cost, revenues, and profits for all relevant agents in the system.
Maturity â Commercial Product distributed by ADICA Consulting, LLC.
Areas modeled â Illinois electrical market, Iberia, France, South Korea, Poland, Central Europe
Customers/sponsors â At the beginning of 2005, the software became commercially available and
current clients include research institutes, power companies, transmission companies, and regulatory
offices in South Korea, Portugal, and Spain.

51

Model Framework
Underlying model(s) â Agent-based modeling and simulation.
Simulation â EMCAS simulates the operation of a power system and computes electricity prices for
each hour and each location in the transmission network. Electricity prices are driven by demand for
electricity, cost of electricity production, the extent of transmission congestion, external random or
non-random events, such as unit outages or system disruptions, and company strategies. Model results
include the economic impacts on individual companies and consumer groups under various scenarios.
Data format â The user builds the system configuration either within the EMCAS graphical user
interface or by preparing and importing a set of well-defined input files.
Sensor data â The model also includes bilateral financial contracts. Real-time prices are calculated in a
real-time dispatch using a DC optimal power flow model.
Human activity â Model includes different types of consumers (e.g., residential, industrial, and
commercial) with their respective electricity consumption profiles.
Coupling with other models â Couples with hydropower models (e.g., VALORAGUA) and detailed
power flow models (e.g., PowerWorld).
System Requirements
A network with 10 nodes (buses or locations), 70 aggregated thermal generating units,
Hardware
13 generation companies, one transmission company, one ISO, and one regulator takes
approximately 60 minutes for a one-year simulation (8760 hours) on a desktop PC
with a 2.0 GHz AMD Athlon2000+ processor and 1 GB of RAM. For multi-year
simulations, it is recommended to use a brand-new, high-end PC, preferably with dual
core processors and 2+ GB of RAM.
Commercial optimizer (LINGO), long-term hydro model (e.g., VALORAGUA).
Software
Other Notes
Adaptability to Local Market and System Conditions:
The EMCAS model is fully customizable and not hardwired to any particular system. Network
configurations can be simple and aggregate consisting of a few to several dozen network nodes and
links, or detailed bus-level representations with several thousand network elements. The level of detail
largely depends on data availability and particular analysis objectives.
References
ADICA Consulting, LLC., Innovative Solutions for Analyzing Energy Markets,
http://www.adica.com/media/downloads/ADICA_Overview_2006.pdf, Webpage visited July 3, 2006.
ADICA Consulting, LLC., Electricity Market Complex Adaptive System (EMCAS) Software,
http://www.adica.com/media/downloads/EMCAS_Model_Overview.pdf, Webpage visited
July 3, 2006.
ADICA Consulting, LLC., Electricity Market Complex Adaptive Systems (EMCAS),
http://www.adica.com/media/downloads/EMCAS_Specifications_2006.pdf, Webpage visited July 3,
2006.
Argonne National Laboratory, Simulating GenCo Bidding Strategies in Electricity Markets with an
Agent-Based Model, http://www.iaee.org/documents/denver/Thimmapuram.pdf#search='emcas',
Webpage visited July 3, 2006.

52

53

54

Model Name Fast Analysis Infrastructure Tool (FAIT)
Sandia National Laboratory (SNL)
Organization
Infrastructures
Theresa Brown
POC
EP, NG, POL, TL,
Emergency Services
tjbrown@sandia.gov
Description
Overview â National Infrastructure Simulation and Analysis Center (NISAC) analysts are regularly
tasked by the Directorate for Preparedness in the Department of Homeland Security (DHS) with
determining the significance and interdependencies associated with elements of the nationâs critical
infrastructure. The Fast Analysis Infrastructure Tool (FAIT) has been developed to meet this need.
FAIT utilizes system expert-defined object-oriented interdependencies, encoded in a rule-based expert
systems software language (JESS), to define relationships between infrastructure assets across different
infrastructures. These interdependencies take into account proximity, known service boundaries,
ownership, and other unique characteristics of assets found in their associated metadata. In a similar
fashion, co-location of assets can be analyzed based exclusively on available spatial data. The
association process is dynamic, allowing for the substitution of data sets and the inclusion of new rules
reflecting additional infrastructures, as data accuracy is improved and infrastructure analysis
requirements expand. FAIT also utilizes established Input/Output (I/O) methods for estimating the
economic consequence of the disruption of an asset. Each of these analysis elements (interdependency,
co-location, economic analysis) have been extended from their original âasset-levelâ analysis, to allow
for the analysis of a specified region. Here, rules written for individual assets are executed en masse on
classes of demand infrastructures, like assets of the emergency services (e.g., fire and police stations)
and public health (e.g., hospitals) infrastructures, which lie in a defined analysis area, such as a
hurricane damage zone, to identify those elements of supply infrastructures (e.g., electric power and
telecommunications) which serve the largest number of particular sets of demand infrastructures.
FAITâs regional economic analysis takes as input economic data (from the Bureau of the Census) for
the disrupted area (as modeled by other NISAC capabilities). When coupled with other NISAC
modeling results (estimates for the duration of the disruption and recovery, and the range of magnitude
of disruption for the disrupted region), FAIT creates a regional economic analysis, an understanding of
the direct and indirect economic consequences, for each sector of the economy in each county in the
analysis area.
Development goals â The FAIT development team is constantly modifying their development goals to
best support the requirements of NISAC analysts, in responding to questions from DHS. Current goals
include the following:
Expansion of existing FAIT capabilities to cover infrastructures not in the current analysis set;
Enhancement of economic analysis capability to more accurately represent the consequences of the
loss of infrastructure services on the performance of individual industrial sectors;
Incorporation of infrastructure-specific models to define areas of consequence due to the failure of
asset(s) in a given infrastructure; and
Development of a network âmetacrawlerâ designed to associate sparse metadata (e.g., transportation
system commodity throughput) with fragmented system elements (e.g., segments of the national rail
network).
Intended users â Analysts on NISACâs Fast Analysis and Simulation Team.
System Output â Web-based, printer-friendly description of assets, their interdependencies, economic
consequence of disruption, and other information associated with asset by system users.
Maturity â In development, utilized by NISAC Fast Analysis and Simulation Team to support NISAC
analyses for DHS/Preparedness.
Areas modeled â First-order interdependencies for selected classes of assets in the energy,
telecommunications, emergency services, and public health sectors, nationwide (based on data
availability).
Customers/sponsors â DHS/IP â NISAC.
55

Model Framework
Underlying model â Dependency model is an object-oriented expert system model of infrastructure
interdependencies. The economic model centers on the economic disruption over an area or region
from a discrete event. Economic methodology best employed for disruptions with a timeframe of 1
week to 1 month.
Simulation â For identification of interdependencies, FAIT utilizes an expert system developed in
JESS. Economic analysis within FAIT is performed utilizing Input-Output methodologies. Both
elements are coded in Java.
Data format â FAIT utilizes spatial and tabular data
Sensor data â None.
Ability to couple with other models â None; though results of other models (documents, files) can be
coupled through the FAIT architecture to particular assets, classes of assets, or infrastructures with
which they are associated.
Human Activity modeling â None.
System Requirements
None, for the end user. Program resides on a SNL server and supports web access.
Hardware
Internet Browser
Software
Other Notes
FAIT allows for external information, (e.g. web addresses or files), to be âattachedâ to specific assets,
classes of assets, or infrastructure sectors, such that when those areas are examined in the future, the
associated information is accessible to future users.

References
National Infrastructure Simulation and Analysis Center, Fait Analysis Infrastructure Tool Fact
Sheet,http://www.sandia.gov/mission/homeland/factsheets/nisac/FAIT_factsheet.pdf.

56

Model Name Financial System Infrastructure (FinSim)
Los Alamos National Laboratory
Organization
Infrastructures
FIN
Sam Flaim
POC
Description
Overview â The Financial System Infrastructure (FinSim) is an agent-based model of cash and barter
transactions that is dependant on contractual relationships and a network at the federal reserve level.
Agent based models create transactions which rely on telecommunications and electric power.
Dependencies can cause deadlocks in the situation where one is unable to pay until being paid. The
MIITS module asks every transaction whether there is an electronic connection available to make the
transaction. The payments and settlement systems (PSS) module makes the validity checks.
Development goals â Development started in January 2005 to protect the physical infrastructure of
payment and trading systems initiated by the events of 9-11. All current models didnât address the
transaction system, just the economic impact.
Intended users â Internal analyst.
System output â The system output is the number of financial institutions affected. Output is in a textbased format.
Maturity â Development.
Areas modeled â National Federal Reserve Banking System â Financial
Customers/sponsors â Sponsor is the National Infrastructure Simulation and Analysis Center
(NISAC), Department of Homeland Security (DHS).
Model Framework
Underlying model â Agent-based model.
Simulation â FinSim models financial transactions modeling the 12 FRB, about 9,700 FedWire
participants, and almost 28,000 financial institutions registered with FedACH.
This includes the electronic PSSânetworks with contractual as well as electronic links and nodes
PSSs include: FedWire, FedNet, CHIPS, FedACH, Commercial ACHS ~50
Cash & barter (excluded from FinSim)
Data format â Not specified.
Sensor data â No direct sensor feeds.
Coupling with other models â Yes, coupling is done indirectly. Electrical power failure (IEISS
output) â  Telecom failure (MIITS output) â  PSS failures (FINSIM)
Human activity modeling â None.
System Requirements
Larger models require a computer cluster.
Hardware
Java.
Software
Other Notes

57

References
Financial System InfrastructureâFinSim, LAUR-05-9147.

58

Model Name
Organization

POC

Fort Future
U.S. Army Corps of Engineers, Engineer
Research and Development Center, Construction
Engineering Research Laboratory (CERL)
Dr. Michael P. Case
Michael.P.Case@erdc.usace.army.mil

Infrastructures
All support
infrastructures for a
military installation

Description
Overview â Fort Future is a collaborative, web-based planning system that uses simulation to test plans
for Department of Defense (DoD) installations. It uses an open, service-oriented architecture to allow
multiple simulations to be run simultaneously from the same set of alternative, organized into a study.
The web-based workbench provides geographic information system (GIS)-based plan editors, controls
simulations, and organizes results into a decision matrix. Fort Future assesses the impact of critical
infrastructure on mission using a âVirtual Installationâ simulation that contains models for
transportation, electrical power, water systems, including waterborne chemical/biological/radiological
(CBR) agents, airborne CBR plume, facilities, mission tasks and processes, agents, and dynamic plans.
The Virtual Installation simulation was built using Argonne National Laboratoryâs Dynamic
Information Architecture System (DIAS) framework and will be ported to the Repast agent modeling
toolkit by September of 2006. Other models support analysis of encroachment, sustainability, and
facility design.
Development goals â Demonstrate the use of simulation to improve planning for DoD Installations.
Incorporate scenario descriptions into Simulation Interoperability Standards Organization (SISO)
Military Scenario Definition Language (MSDL).
Intended users â Users will include installation and regional planners, US Army Corps of Engineers,
and researchers.
System output â Output of the simulations is collected by a web-based collaborative workbench and
presented as a decision matrix. The workbench can be customized to present output specific to
particular simulations.
Maturity â This product is in development with some tools complete. The product will be complete by
October, 2006.
Areas modeled â Fort Benning, Fort Shafter, Fort Bragg, and Fort Carson.
Customers/sponsors â United States Army.
Model Framework
Underlying model(s) â The agent-based Virtual Installation is based on DIAS and Repast. Water
modeling uses EPAnet. CBR plume model uses the Defense Threat Reduction Agency (DTRA)
Hazardous Prediction and Assessment Capability (HPAC) Tool.
Simulation â This model supports complex and lengthy scenario simulations
Data format â GIS â Environmental Systems Research Institute, Inc. (ESRI) Geodatabase and SHP
files (Tri-service Spatial Data Standards). Scenarios â XML.
Sensor data â Not accepted.
Human activity â Human activities are modeled, however there are no humans in the simulation loop.
Coupling with other models â Fort Future is built to collaborate with multiple models using simple
object access protocol (SOAP).

59

System Requirements
Fort Future is a server-based application, accessed over the internet using a
Hardware
web-browser.
Fort Future has been tested on Windows and Linux servers. The workbench runs as a
Software
J2EE application on JBoss 3.x. Persistence is provided by MySQL or Oracle relational
databases. Geospatial information is provided by ESRI ArcSDE and ArcGIS server.
Users access the workbench using a web-browser.
Other Notes
Users of Fort Future at the installation, regional, or national level will be able to set up planning
scenarios, conduct dynamic analyses over time periods of up to 30 years, and compare scenario results.
Fort Future will allow decision makers to:
â¢
Provide an integrated sustainability planning capability to support mission-essential task list
(METL) analysis, master planning, and natural and cultural resource planning.
â¢
Simulate the impact of critical infrastructure failure on the installation mission.
â¢
Simulate and optimize planning for force projection. Metrics will focus on risk-based
evaluation of an installation's ability to project forces over time.
â¢
Simulate urban and regional growth around installations as a foundation for analysis of mission
sustainability. Factors to be evaluated include encroachment, noise, traffic congestion, habitat, and
threatened and endangered species.
â¢
Manage facility requirements to rapidly generate, visualize, and analyze facilities for the
Objective Force. The analysis will include force protection and sustainability issues.

Electrical Infrastructure (capacity & interruption)

Water Infrastructure(flow & CBR)

CBR Plume Modeling

Collaborative Web-based Decision Support

60

National/Regional
Scale

SIRRA
Sustainable Installation
Regional Resource
Assessment

Installation
Scale

LEAM
30 Year
Encroachment
Simulation

Facility
Scale

Facility Composer:
Accelerating MILCON
Transformation

SPiRiT & LEED
Sustainability Rating

AT Standards
References
Fact sheet on Fort Future, www.erdc.usace.army.mil/pls/erdcpub/docs/erdc/images/ERDCFactSheet_
Research_FortFuture.pdf#search='fort%20futureâ, Webpage accessed July 3, 2006.
US Army Corps of Engineers, www.erdc.usace.army.mil, Webpage visited July 3, 2006.
Discussion of US Army Corps of Engineers ongoing research,
http://www.erdc.usace.army.mil/pls/erdcpub/WWW_WELCOME.NAVIGATION_
PAGE?tmp_next_page=61605&tmp_Main_Topic=51585, Webpage visited July 3, 2006.

61

62

Model Name
Organization
POC

Inoperability Input-Output Model (IIM)
University of Virginia Center for Risk
Management of Engineering Systems,
Director and founder â Lawrence R. Quarles
Professor of Systems and Information
Engineering and Civil Engineering
Yacov Y. Haimes
Yyh4f@virginia.edu

Infrastructures
Financial networks,
highway networks

Description
Overview â Inoperability Input-Output Model (IIM) is a computer-based analytical model capable of
analyzing the impacts of an attack on an infrastructure and the cascading effects (in economic and
inoperability terms) on all other interconnected and interdependent infrastructures. The model uses
U.S. Bureau of Economic Analysis (BEA) data for assessing economic interdependencies. IIM allows
systematic prioritization of infrastructure sectors that are economically critical and identifies sectors
whose operability is critical during recovery. The model can be used to represent workforce recovery
following a terrorist attack and identify essential response personnel. IIM also models recovery rates of
different infrastructure sectors following an event.
Development goals â Not specified.
Intended users â Analysts and emergency planning and response organizations are the intended users.
System output â The model outputs various data and metrics in text and graphically.
Maturity â The model has been used with cooperation of various local and state governments.
Areas modeled â IIM has been used to model Virginiaâs transportation systems in various cities (e.g.,
Hampton City, Norfolk, and Virginia Beach) and support Department of Homeland Security (DHS)
security alert levels for the greater New York area and to support a commission on high-altitude
electromagnetic pulse (HEMP) attacks.
Customers/sponsors â Customers and sponsors of IIM include the State of Virginia, U.S. DHS,
Defense Threat Reduction Agency (DTRA), the Department of Defense (DoD) and the Commission on
High Altitude EMP Attacks on the U.S.
Model Framework
Underlying model â IIS is a mathematical model based on Wassily Leontifâs input-output model for
the U.S. economy which describes economic interdependencies.
Simulation â IIM simulates the behaviors of multiple infrastructure sectors during and following
perturbations (such as terrorist attacks on modeled infrastructure) using economic and other data to
assess the criticality of the effects.
Data format â Data are retrieved from and stored in relational databases containing information
including employment and earnings data, commodity flow data, and geographic location data.
Sensor data â Not specified.
Coupling with other models â Not specified.
Human activity modeling â IIM has been used to model human activity in response to transportation
disruptions.
System Requirements
Not specified.
Hardware
Not specified.
Software
Other Notes

63

IIM Calculates Propagating Effects.
References
Haimes, Y. (2005), Risk-Based Framework for Modeling Infrastructure Interdependencies, University of
Southern California Terrorism Risk Analysis Symposium, Los Angeles, California, January 14, 2005,
http://www.usc.edu/dept/create/events/2005_01_31/Risk_Based_Framework_for_Modeling_Infrastructure_
Interdependencies.pdf#search='yacov%20Haimes%20interdependencies', Webpage visited July 12, 2006.
Haimes, Yacov Y., et al., âInoperability input-output model for interdependent infrastructure sectors. I:
Theory and methodology,â Journal of Infrastructure Systems, Vol. 11, No. 2, June 2005, pp. 67-79.
Haimes, Y. (2004). Assessment and Management of Transportation Infrastructure Security using the
Inoperability Input-Output Model (IIM), October 19, 2004,
http://www.virginiadot.org/infoservice/resources/TransConfHaimes%20-%20VDOT-Roanoke-October-192004-2.pdf#search='Yacov%20Y.%20Haimes%20interdependencies', Webpage visited July 12, 2006.

64

Model Name
Organization
POC

Interdependent Energy Infrastructure Simulation System (IEISS)
Los Alamos National Laboratory
Infrastructures
Joe Holland
EP, NG

Description
Overview â The Interdependent Energy Infrastructure Simulation System (IEISS) is an actor-based
infrastructure modeling, simulation, and analysis tool designed to assist individuals in analyzing and
understanding interdependent energy infrastructures. The actor-based infrastructure components were
developed in IEISS to realistically simulate the dynamic interactions within each of the infrastructures,
as well as, the interconnections between the infrastructures. In particular, it has the ability to analyze
and simulate the interdependent electric power and natural gas infrastructures. IEISS Water is a water
distribution simulation capability for simulating urban scale water infrastructures and their
interdependencies.
Development goals â The ultimate goal for IEISS is a multi-infrastructure modeling framework that
can be used to analyze the complex, nonlinear interactions (interdependencies) among interdependent
infrastructures including electric power, natural gas, petroleum, water, and other network based
infrastructures that is scalable to multiple spatial (e.g., urban to regional) and temporal resolutions
Intended users â Internal Analyst â IEISS used to support the development of an impact report on for
specific infrastructure events (such as, hurricanes, terrorist attacks, etc.).
System output â System output include the identification of outage areas (e.g., electrical outage areas).
Output visualization is current in Java OpenMaps and is exportable to ESRI compatible shape files.
Maturity â Mature Internal.
Areas modeled â numerous US metropolitan areas.
Customers/sponsors â Sponsor is NISAC â DHS.
Model Framework
Underlying model â IEISS is an actor-based infrastructure modeling, simulation, and analysis tool
designed to assist individuals in analyzing and understanding interdependent energy infrastructures.
Simulation â A continuous time based model with an underling physical engine for system dynamics.
Data format â Data is input via xml format from a variety of databases.
Sensor data â no direct sensor feeds.
Coupling with other models â Yes, coupling is done indirectly. The output of IEISS will serve as the
input to other infrastructure models to identify cross infrastructure effects.
Human activity modeling â None at this time.
System Requirements
Cross platform compatibility â Windows and LINUX compatibility.
Hardware
Requires the Java Virtual Machine.
Software
Other Notes
IEISS is coupled with other LANL modeling tools. Of particular note is the Scenario Library
Visualizer (SLV). SLV is a scenario library of outage simulations, which includes a custom
visualization tools to provide map-based view of scenarios that have been evaluated in IEISS. The goal
has been to identify potential impacts to critical infrastructures dependent upon electric power. SLV
has principally been used during fast-response exercises for analysis of hurricane impacts (restoration
of hurricanes Charlie and Ivan in â04; Dennis, Katrina, Ophelia, Rita, Wilma in â05) SLV has also
modeled electric power restoration during ice storms and during DOE-sponsored exercises involving
low-voltage scenarios.

65

References
"NISAC Energy Sector â IEISS," NISAC Capabilities Workshop, LA-UR-03-1159, Portland, Oregon,
26-27 March 2003.
Los Alamos National Laboratory, "Energy Infrastructure Modeling at LANL," LALP-03-027,
LA-UR-03-0658.
Los Alamos National Laboratory, "Energy and Environmental Programs Compendium,"
LA-LP-02-216.

66

Model Name
Organization
POC

Knowledge Management and Visualization in Support of Vulnerability Assessment
of Electricity Production
Carnegie Mellon University
Infrastructures
H. Scott Matthews
EP (RL, WW, HW
limited)
hsm@cmu.edu
Department of Energy (DOE) National Energy
Technology Laboratory (NETL) Pittsburgh and
Morgantown Campuses

Description
Overview â This is a research project to analyze vulnerabilities associated with delivery of fuel. It is
designed to help ensure availability of supply and to visualize the impacts for decision support. The
project has focused on coal deliveries to power plants because, while vulnerabilities at the power plant
level (production) are easier to identify, vulnerabilities and impacts associated with delivery of fuel are
more uncertain. Also, data on coal shipments is readily available.
Development goals â The first phase of the project focused on the origin (mines) and destination
(power plant) layers of the coal model. The middle (transportation) layer will be focused on in the
future. Additional work will also be done to improve tools for data mining such as, classification of
transportation assets, better prediction of impacts, and improved sequential pattern analysis tools.
Intended users â Planners are the intended users.
System output â Output includes maps and chart graphics showing mines, transportation routes, and
affected (with degree of vulnerability to disruption) power plants.
Maturity â This project is currently in the prototype stage with ongoing research.
Areas modeled â United States with emphasis on coal mines in Wyoming.
Customers/sponsors â Department of Energy (DOE) National Energy Technology Laboratory
(NETL).
Model Framework
Underlying model(s) â Statistical data and analysis tools drawing on data derived from data
warehouses.
Simulation â Mines can be removed from the network and a simulation run to identify plants affected
and the degree of the impact on production.
Data format â Data were used from several databases including; Coaldat (developed by Platts
containing ~ 2500 coal transactions per month), Coal Transportation Rate Database developed by the
Energy Information Administration supplemented with data from the Department of Transportation
(DOT) Surface Transportation Board (STB), National Transportation Atlas Database (NTAD), and
PowerMAP, a geographical information system (GIS) developed by Platts containing map layers of
power plants and mines.
Sensor data â Not included.
Human activity â Not modeled.
Coupling with other models â Not specified.
System Requirements
Not specified.
Hardware
Not specified.
Software

67

Other Notes
The Transportation Routing Analysis Geographic Information System (TRAGIS) developed by Oak
Ridge National Laboratory (ORNL) was evaluated to help with the problem of routing (of coal
supplies). TRAGIS is designed to schedule possible routes by selecting the origin and destination with
one transportation mode (e.g., highway, rail, and waterway modes) and route type (e.g., commercial
[default], quickest, shortest, and others). Currently, it is not able to schedule routes for multimodal
transportation as is often used to deliver coal. While the most frequently used mode of transporting
coal is railroad, many transactions are shipped multimode, such as by barge then by railroad.
Therefore, a multimodal route scheduling solution is necessary for acquiring more accurate
transportation analyses.

68

References
Information in the preceding section was obtained from draft report and communications with point of
contact.

69

70

Model Name
Organization
POC

Multi-Layer Infrastructure Networks (MIN)
Purdue School of Civil Engineering
Dr. Srinivas Peeta
peeta@purdue.edu
George Mason University
Dr. Terry Friesz
tfriesz@gmu.edu

Infrastructures
HW, HA

Description
Overview â This is a preliminary network flow equilibrium model of dynamic multi-layer
infrastructure networks (MIN) in the form of a differential game involving two essential time scales. In
particular, three coupled network layersâautomobiles, urban freight, and dataâare modeled as being
comprised of Cournot-Nash dynamic agents. An agent-based simulation solution structure is
introduced to solve the flow equilibrium and optimal budget allocation problem for these three layers
under the assumption of a super authority that oversees investments in the infrastructure of all three
technologies and thereby creates a dynamic Stackelberg leader-follower game.
Development goals â Continue to develop a generalized framework to address both equilibrium and
disequilibrium scenarios.
Intended users â Community planners and engineers.
System output â Charts, graphs, behavioral trends.
Maturity â Research.
Areas modeled â Urban transportation (e.g., auto, urban freight, and data).
Customers/sponsors â The National Science Foundation sponsored the work.
Model Framework
Underlying model â Agent based simulation of multi-layer infrastructure networks. The three-layer
model consists of an auto, urban freight, and data layer flow sub models. These three sub models are
combined and solved using an agent-based simulation approach.
Simulation â Temporal dynamic flow model involving producers and consumers.
Data format â Not specified.
Sensor data â Not incorporated.
Coupling with other models â Unknown.
Human activity modeling â Models human activity as consumers.
System Requirements
Not specified.
Hardware
Not specified.
Software
Other Notes

References

71

72

Model Name
Organization
POC

Multi-Network Interdependent Critical Infrastructure Program for Analysis of
Lifelines (MUNICIPAL)
Rensselaer Polytechnic Institute (RPI)
Infrastructures
Earl E. Lee II
TC, EP, RL
Leee7@rpi.edu
William A. Wallace
wallaw@rpi.edu
John E. Mitchell
mitchj@rpi.edu
David M. Mendonca
mendonca@njit.edu

Description
Overview â Multi-Network Interdependent Critical Infrastructure Program for Analysis of Lifelines
(MUNICIPAL) is a geographic information system (GIS) user interface, built on a formal,
mathematical representation of a set of civil infrastructure systems that explicitly incorporates the
interdependencies among them. The mathematical foundation or decision support system is called the
Interdependent Layered Network (ILN) model. ILN is a mixed-integer, network-flow based model
implemented in software drawing on a database containing infrastructure attributes. MUNICIPAL
provides the capability to understand how a disruptive event affects the interdependent set of civil
infrastructures. This can help communities train for and respond to events that disrupt services required
for their health, safety, and economic well being. It can be used to help assess the vulnerability of
systems due to their reliance on other systems. The model is generic (applicable to more than one
location) and not specific to a particular type of event, such as an earthquake or hurricane.
Development goals â Once the Los Angeles and Manhattan data sets are complete, mathematical and
technical assessments will be conducted. The system will also be evaluated by infrastructure system
managers and emergency response organizations.
Intended users â MUNICIPAL is intended for use by personnel in charge of response and restoration
efforts following a disruptive event and as a training tool for personnel who guide response and
restoration efforts.
System output â A GIS interface displays systems and identifies affected areas. An operator can
update the conditions of components of the set of systems modeled, add temporary systems during
restoration, and display areas affected by inabilities to meet demands.
Maturity â Prototype system.
Areas modeled â Manhattan, NY and Los Angeles, CA.
Customers/sponsors â National Science Foundation.
Model Framework
Underlying model â MUNICIPAL consists of a GIS interface for the user, a database with the
attributes of the set of infrastructures, the ILN module, and the vulnerability module.
Simulation â With identification of paths or components of concern, MUNICIPAL identifies
components in the parent system which these paths or components rely on. For example, placing power
supply components in a failed condition will identify telecommunications components that rely on
these sections of power to fail. By proposing new connections within telecomm, MUNICIPAL can help
to determine if a feasible path (or paths) exists and the set of nodes that constitute this path (or set of
paths). MUNICIPAL can also be used for the addition of temporary or alternative power sources or any
other analyses relating to improving reliability by adding redundancy.
Data format â ESRI ArcGIS, relational database, text.
73

Sensor data â Not currently configured for sensor data.
Coupling with other models â Not specified.
Human activity modeling â Not specified.
System Requirements
Not specified.
Hardware
Not specified.
Software
Other Notes

References
Lee, Earl E. II, et al., Decision Technologies for Protection of Critical Infrastructures,
http://www.rpi.edu/~mitchj/papers/decisiontechnologies.pdf#search='decision%20technologies,
Webpage visited July 10, 2006.

74

75

76

Model Name
Organization
POC

Natural Gas Infrastructure Toolset (NGtools )
Argonne National Laboratory, Infrastructure
Assurance Center (IAC)
Dr. James Peerenboom
jpeerenboom@anl.gov

Infrastructures
NG, EL

Description
Overview â The Infrastructure Assurance Center (IAC) has developed a set of tools to represent the
physical components of the natural gas network. The Natural Gas Infrastructure Toolset (NGtools) was
developed to provide an analyst with a quick method to access, review, and display components of the
natural gas network; perform varying levels of component and systems analysis, and display analysis
results.
Development goals â Not specified.
Intended users â Natural gas suppliers and users (e.g., electric utilities).
System output â Geographic (using GIS) or schematic view of pipeline system, charts and graphs
showing failure sets (e.g., pumping stations and power stations) and the amount of time to gas
depletion. The system allows various analyses on component and system level, and displays results.
Maturity â The system is in development.
Areas modeled â Not specified.
Customers/sponsors â US Department of Energy.
Model Framework
Underlying model â Agent-based.
Simulation â NGflow simulates steady-state gas network flows and provides gas flow movements
under various operating conditions based on gas flow balancing algorithms and available system flow
data.
Data format â Not specified.
Sensor data â Not specified.
Coupling with other models â Not specified.
Human activity modeling â Human activities are not modeled.
System Requirements
Not specified.
Hardware
Not specified.
Software
Other Notes
There are four tools in the toolset; NGanalyzer, NGcut, NGflow, and NGdepletion as described in the
following:
NGanalyzer assists in analyzing gas system characteristics and vulnerabilities. Key considerations
include the number of city-gates, available storage, and pipeline capacity and interconnections. The
figure below shows an example of the shortest path distance from major gas supply areas to a sample
site as calculated by the model.
NGcut determines network component failure sets that could isolate a specific location or site from all
supply sources. One of the advantages of using this model is that it significantly decreases the time
needed to analyze site isolation issues by automating the construction of failure sets. The model also
allows analysts to consider a larger number of failures and to broaden an analysis. Failure sets
identified by NGcut provide an initial set of components that require closer examination.
NGflow identifies critical links and nodes in a network topology. This tool provides an alternative to
using very detailed, data-intensive commercial flow simulation models. The model also gives a unique
snapshot of the gas transmission infrastructure that supports a certain location or site.
NGdepletion addresses outage duration times and determines whether and when a component outage
will affect a specific location or site. The model computes the amount of time that line pack can
continue supplying gas to a site.

77

References

78

Model Name
Organization
POC

Net-Centric Effects-based operations MOdel (NEMO)
SPARTA
Brent L. Goodwin
Brent.goodwin@sparta.com
Laura Lee
Laura.lee@sparta.com

Infrastructures
TC, EP, NG, DW

Description
Overview â Net-Centric Effects-based operations MOdel (NEMO) is an effects-based planning and
analysis application for modeling the cascading effects of events across multiple infrastructure
networks. It is a Net-Centric compliant application, relying on a service oriented architecture (SOA)
approach to access infrastructure models, data repositories, and mapping tools. NEMO models
interactions across electrical power, water, gas, and road networks using an on/off interaction behavior
between the components of the different networks, and provides a solid foundation for advancement.
NEMO provides a first of its kind capability for observing second and higher order effects of
operations against opponentsâ infrastructure networks.
Development goals â Efforts are underway to integrate social/political networks into the effects-based
operation (EBO) process. Future development needs to enhance the program capabilities for integrating
additional relationship definitions, multi-agent capabilities, and optimization.
Intended users â Planners and analysts are the intended users.
System output â NEMO displays maps overlaid with nodes and linkages between various
infrastructures. Disruptions and cascading effects are highlighted during simulations.
Maturity â This is a prototype system.
Areas modeled â Not specified.
Customers/sponsors â NEMO was internally developed by SPARTA.
Model Framework
Underlying model â The graphical user interface (GUI) is backed by an SOA consisting of two web
services; one accesses to a geo-spatial database for storage and retrieval of network databases, and the
other coordinates interaction with the various infrastructure models used to provide network status
feedback. The geo-spatial database web service, Earth Resource Terrain Hierarchical Archive
(ERTHA), contains nearly 200GB of network definitions that may be accessed via the NEMO GUI and
used to support effects-based analysis. ERTHA is a geographical information system (GIS) database,
based on ESRI products, of infrastructure data items (e.g., power lines, road networks) that were
developed as an unclassified source. Abstracting access to data through a web service decouples
NEMO from a specific database and specific vendors, making it possible to integrate other data sources
in the future.
Simulation â NEMO provides a basic capability for effects-based planning and performing âwhat ifâ
analysis of actions.
Data format â Data is in Environmental Systems Research Instituteâs (ESRI) ERTHA relational
database format. Other models utilize a model interface client (MIC) translator and eXtensible Markup
Language (XML).
Sensor data â Not included.
Human activity modeling â Changes to include human activity modeling are in progress.
Coupling with other models â NEMO integrates four infrastructure models: lines of communications,
electrical power, gas pipelines, and water pipelines. The models used to evaluate these networks are
industry best-of-breed simulation tools for their domains. CitiLabsâ Voyager simulation provides road
and rail network analysis, while Advantica (formerly Stoner Engineering) provides the Solver tools for
electrical power networks as well as the water and gas pipelines.
System Requirements
Windows XP/2000.
Hardware
Not Specified.
Software
79

Other Notes
Efforts are ongoing and mostly complete to integrate social/political networks into the EBO process.
For the most part, these efforts are complete. We have integrated the Political Science-Identity (PSI)
model (from University of Pennsylvania, Dr. Ian Lustick) into our architecture, and have developed
operators that alter the contentedness of a population based on associated physical infrastructure.
Further information on PS-I is available at http://jasss.soc.surrey.ac.uk/5/3/7.html.

â¢ ERTHA Web Service
â Interface for all GIS Infrastructure
Models
Â» âGetâ shape files and
associated attributes

NEMO

Web Service
â¢ ArcIMS and ArcXML
â ESRIâs interface to ArcSDE and its
Data
â ArcIMS : Internet Map Server
â ArcXML : Layer Definition and
Query Language for ArcIMS

ERTHA Web Service
Java
JRun

Apache

ArcXML

ArcIMS

ArcSDE

â¢ ArcSDE
â Spatial Database Engine
â Centralized management of
geographic information in a DBMS
Â» Vector, raster, table,
annotation, relationships, CAD
â Contains a subset of JIVAâs data
â¢ DBMS
â Oracle database
â Features as objects
Â» Geometry
Â» Attributes
Â» Behavior (rules, methods,
relationships)
â Uses ArcSDE for multi-user access
and versioning

80

ArcXML
ArcIMS
ArcSDE
JIVA data

DBMS
Oracle

Windows

References
JASSS, PS-1: A User-Friendly Agent-Based Modeling Platform for Testing Theories of Political
Identity and Political Stability, http://jasss.soc.surrey.ac.uk/5/3/7.html, Webpage visited July 3, 2006.
Sparta, Planning and Assessing Effects Based Operations, www.sparta.com/sew/publications/NEMOfor-ICCRTS.pdf, Webpage visited July 3, 2006.

81

82

Model Name
Organization
POC

Network-Centric GIS
York University, Toronto, Ontario, Canada
Rifaat Abdalla
abdalla@yorku.ca

Infrastructures
RL, HW, WW

Description
Overview â This system is a framework for using geographical information system (GIS)
interoperability for supporting emergency management decision makers by providing effective data
sharing and timely access to infrastructure interdependency information.
Development goals â There are no development goals identified at present.
Intended users â This is intended for emergency planners and responders.
System output â Output are GeoServNet (York University GeoICT Lab Product) GIS 2 and 3D
images.
Maturity â Proof of principle.
Areas modeled â This has been modeled in Vancouver, British Columbia (Earthquake scenario) and
Toronto, Ontario (Flood scenario).
Customers/sponsors â Ongoing research began under Canadaâs Joint Infrastructure Interdependencies
Research program (JIIRP), which is jointly funded by the Natural Sciences and Engineering Research
Council (NSERC) and the department of Public Safety and Emergency Preparedness Canada (PSEPC).
Model Framework
Underlying model â Underlying models are GIS technologies including ArcGIS 9 (desktop) and
GeoServNet (web-based), GSNBuilder, GSNAdministrator, GSNServer, GSNPublisher, GSNViewer,
and HEC-RAS (used for hydraulic simulation with ArcView GIS).
Simulation â The system has GIS-based spatial-temporal simulations.
Data format â Data formats are GIS data, graphics, and text. Knowledge-base information is stored in
a specially designed object-oriented database. The project used Environmental Systems Research
Instituteâs (ESRI) Geodatabase model.
Sensor data â Hydraulic gauges provide information for water surface levels and there exists a
capability for integrating other live sensor information.
Coupling with other models â None.
Human activity modeling â Not included.
System Requirements
Pentium 4 with 512 RAM, broadband connection.
Hardware
ESRI ArcGIS, GeoServNet.
Software
Other Notes
Model creation process for the flood model:
x Preparation of different data layers
x Digitize floodplain, banks, stream centerline, and stream cross section using HEC-GeoRAS
extension for ArcView
x Input flood parameters using channel geometry created in ArcView and model a flood scenario
using HEC-RAS, GIS interoperability is utilized for sharing and visualization of the disaster model
x Delineate flood layers using HEC-RAS export ASCII data and data layers with help of ArcView
and HEC-GeoRAS extension.
Populate flood layers produced in GeoServNet using standard processing procedures.
The following steps are useful for defining location based infrastructure interdependencies (LBII) for a
particular area:
83

x Identify critical infrastructure sectors in the study area
x Analyze processes and operations for each sector
x Analyze dependencies
x Determine Interdependencies
x Collect data
x Model and visualize (interoperable 3D internet-based).
Earthquake scenario modeling is based on using a Geological Survey of Canada Shakemap for the city
of Vancouver.
Critical infrastructure at risk was identified based on GIS modeling.
Building damage density was analyzed based on IKONOS satellite imagery.
Population at risk was identified based on census information and the Shakemap.
Location based infrastructure interdependency was modeled.

Spatial Model Showing Critical Infrastructures at Risk

84

GeoServNet 3D Damage Assessment Model of Downtown Vancouver
References
Abdalla, R. Tao, V. and H. Ali., 2005. âLocation-Based Infrastructure Interdependency: New Term,
New Modeling Approach,â Proceedings of Geoinformatics 2005, Toronto, August 17-19, 2005 CD.
Abdalla, R., K. Niall, and V. Tao, 2005. âA framework for modeling Critical Infrastructure
Interdependency Using GIS,â Canadian Risk and Hazard Symposium, Toronto, 19-21 November 2005.
Joint Infrastructure Interdependency Research Program, Modeling Infrastructure Interdependency for
Emergency Management Using a Network-Centric Spatial Decision Support System Approach,
www.geoict.yorku.ca/JIIRP/Jiirp.htm, Webpage visited July 10, 2006.

85

86

Model Name
Organization
POC

Network Security Risk Assessment Model (NSRAM) Tool for Critical Infrastructure
Protection (CIP) Project
James Madison University (JMU), Institute for
Infrastructures
Infrastructure and Information Assurance
EP, CN
Philip Riley
RileyPB@jmu.edu
Jim McManus
McManuJP@jmu.edu
Samuel T. Redwine, Jr.
RedwinST@jmu.edu
George Baker
BakerGH@jmu.edu
Taz Daughtrey
DaughtHT@jmu.edu

Description
Overview â The network security risk assessment model (NSRAM) tool is a complex network system
simulation modeling tool that emphasizes the analysis (including risk analysis) of large interconnected
multi-infrastructure models. It is designed to be portable, and uses portable and expandable database
and model structures. The tool also provides a framework to simulate large networks and analyze their
behavior under conditions where the network suffers failures or structural breakdowns. In order to
accurately portray the severity of network failures, repair variables (time to repair, cost to repair, repair
priorities) must be considered. NSRAMâs unique repair element set consists of repair entities with
specialized functions that allow users to accurately simulate any configuration of fault detection and
repair schemes. The intent of these repair element sets is to more accurately model the human response
to perceived system damage. The repair element sets identify symptoms, test the system to determine
the elements that are damaged, attempt to repair the damage, and then attempt system recovery. If
symptoms are still present, the repair elements repeat the above cycle until the system is recovered.
Inspection routines will also be accommodated so that preventative maintenance effects are accurately
incorporated. The tool is flexible and can be used to model different infrastructure networks, such as
computers, electrical systems, and highway systems.
Development goals â James Madison University (JMU) is continuing development to add strong
security features, improve the graphical user interface (GUI) and database efficiency, and to develop an
emergency radio system element set. JMU is also developing the concept of sophisticated repair
element sets that interact via predefined algorithms to more accurately simulate repair personnel
reaction to system insults or malfunctions. These repair element sets are unique in that they interact
with the simulation network model in a predetermined manner, but their operating rules can be changed
to allow the user to optimize repair strategies.
Intended users â Analysts are the intended users.
System output â NSRAM contains a GUI for developing models and scenarios, and interpreting
output. The data output is flexible to facilitate post simulation processing.
Maturity â NSRAM is currently in development as part of the CIP project.
Areas modeled â Not specified.
Customers/sponsors â Not specified.
Model Framework
Underlying model â Not specified.
Simulation â Developed simulation elements for computer and electrical power distribution networks.
87

Data format â Not specified.
Sensor data â Not specified.
Human Activity â NSRAM models human activities such as responses to system damage.
Coupling with other models â Not specified.
System Requirements
Not specified.
Hardware
Not specified.
Software
Other Notes

References
McManus, Jim, et al., Network Security Risk Assessment Model(NSRAM) Tool for Critical
Infrastructure Protection Project, http://www.jmu.edu/iiia/webdocs/ppt_NSRAM_Tool.ppt, July 12,
2006.
Redwine, Sam, et al., Network Security Risk Assessment Model Tool,
http://www.jmu.edu/cisat/frd/abstracts04/redwine_sam.html, April 3, 2006.

Model Name
Organization
POC

Next-generation agent-based economic 'laboratory' (N-ABLE)
Sandia National Laboratory
Infrastructures
Theresa Brown
FIN, POL
tjbrown@sandia.gov
88

Description
Overview â The NISAC Agent-Based Laboratory for Economics (N-ABLE) is a software system for
analyzing the economic factors, feedbacks, and downstream effects of infrastructure interdependencies.
N-ABLE is a simulation environment in which hundreds of thousands to millions of individual
economic actors simulate real-world manufacturing firms, households, and government agencies. NABLE can specifically address questions such as: 1. Which economic sectors are most vulnerable to
infrastructure disruptions and interdependencies? 2. What firms are most affected  who does well,
poorly? 3. What are the different qualitative and quantitative ways in which economic sectors use the
energy, transportation, financial, and communication sectors? 4. What short-run infrastructure changes
affect economic performance (and vice versa)? 5. How do systems of firms and individuals respond
and adapt over time and over regions? 6. What economic mechanisms do national, state, and local
governments have or need to have to assist firms and other economic sectors in their regions?
Development goals â Developed to provide decision makers with a firm-level understanding of the
interdependencies between infrastructure sectors and the economy.
Intended users â Economic Analysts.
System Output â Geographical charts and statistical output.
Maturity â Mature Internal.
Areas modeled â Examples: chemical, food, financial, manufacturing sectors.
Customers/sponsors â Department of Homeland Security
Model Framework
Underlying model â N-ABLE models the economy at the level of the individual firm; each N-ABLE
firm is complete with individual buyers, production supervisors, sellers, and strategic planners who
collectively navigate through economic disruption and recovery. N-ABLEâs simulations of thousands
to millions of firms provide the fidelity necessary to understand and implement better infrastructure
policies.
Simulation â Agent Based. N-ABLE microsimulates the economy using an agent-based discrete-event
model. This modeling approach is well suited for investigating the behavior of complex, nonlinear
stochastic systems like the economy. Agents start each time increment making decisions much like
their real-life counterparts. Decisions about what actions to take are based either on probabilities
computed from actual microeconomic data or on results of learning models such as genetic algorithms.
These decisions include setting sales prices, purchasing products, setting production schedules, hiring
workers, buying and selling financial instruments, conducting open market operations, and others.
Macroeconomic variables, such as gross domestic product, inflation (CPI), and the unemployment rate
are computed as individual-firm and aggregate system measures of the performance of the economy.
Data format â not specified.
Sensor data â None.
Ability to couple with other models â Not known.
Human activity modeling â Human in the loop activity supported within the simulation.
System Requirements
Computer cluster
Hardware
Not specified.
Software

89

Figure 1. Geographical Simulation Output

Figure 2. Statistical Simulation Output
References
NISAC Agent-Based Laboratory for Economics (N-ABLEâ¢) Fact Sheet http://www.sandia.gov/mission/homeland/factsheets/nisac/NISAC_N-ABLE_factsheet.pdf

90

Model Name

Organization
POC

NEXUS Fusion FrameworkTM â IntePoint, LLC
Critical Infrastructure Integration Modeling and Simulation, University of North
Carolina at Charlotte (UNCC)
IntePoint, LLC â Commercial Product
Infrastructures
Mark Armstrong
EP, TC, HW, HA, RL
Mark.Armstrong@IntePoint.com
University of North Carolina, UNCC
Development wjtotone@uncc.edu

Description
Overview â NEXUS Fusion FrameworkTM is a planning and response tool that visualizes intended and
unintended effects and consequences of an event across multiple infrastructure, social, and population
behavior models. It is a single framework that incorporates geospatial, graph based (social, economic),
and population behavior models in the same simulation space for cross-infrastructure relationship
analysis. The framework takes a holistic system-of-systems view to support cross system analyses of
cascading events within and between complex networks.
Development goals â Not specified.
Intended users â Department of Defense (DoD) Leadership/Analysts.
Output â Output includes 2, 2.5, and 3-D graphical and geospatial temporal views of modeled
infrastructure.
Maturity â Version 1.1 was delivered to DoD and accreditation is expected in the summer of 2006.
Multiple infrastructure models have already been built & tested using DoD data. Version 2.0 is under
development with delivery in summer 2006.
Areas modeled â Model had been used in many areas including New Orleans, Houston, and Federal
Emergency Management Agency (FEMA) Region 5.
Customers/sponsors â The team is working on the sixth project in 3 years with DoD.
Model Framework
Underlying model(s) â Intelligent agent-based system within the context of a Geographic Information
System (GIS) environment, open architecture
Simulation â Simulation playback offers a foundation for heuristics and supports a collaborative,
sharable simulation result that can be viewed by analysts and consumers. Visual display of cause/effect
allows determination of rules and inferred relationships. The model supports network component
validation and verification of data points. Facilitates identification of missing intelligence.
Modification of rules and branching supports analysis of multiple scenarios based on initial starting
boundaries. Additionally, multiple geographical regions can participate in the same simulation.
Data format - Not specified.
Sensor data â Architecture supports sensor data, not actively incorporated into the model.
Human activity â Incorporates infrastructure-aware population behavior models.
Coupling with other models â Flexible, scaleable, and extensible in that it allows âplug and playâ of
models of the same infrastructure, multiple models of the same infrastructure, and incorporation of
other infrastructure models into the simulation.
System Requirements
Not specified.
Hardware
Not specified.
Software
Other Notes
Leverages Environmental Systems Research Institute, Inc. (ESRI) ArcGIS capabilities for geospatial
91

display and analysis.
Uses ESRI ArcGIS Geodatabase to capture:
x Critical attributes
x Critical relationships
x Predictive analytics
x Meta-driven inference engines
x System-of-systems causality analysis
x Temporal view
x Incorporates specialized functionality off-the-shelf as needed.

92

References

93

94

Model Name
Organization
POC

Petroleum Fuels Network Analysis Model (PFNAM)
Argonne National Laboratory, Infrastructure
Assurance Center
Steve Folga
sfolga@anl.gov

Infrastructures
NG, OL

Description
Overview â Petroleum Fuels Network Analysis Model (PFNAM) was developed to perform hydraulic
calculations of pipeline transport of crude oil and petroleum products. A network consists of links (pipe
segments), nodes (pipe junctions), pump stations, valves, and pressure-reducing stations. The model
tracks the flow of oil in each pipe and the pressure at each node. âPoint-and-clickâ motions allow the
analyst to create a representative model of the liquids pipeline network in order to set up and run a
simulation. Graphical and tabular results provided for each simulation enable analysts to quantify the
impact of infrastructure disruptions on the pipeline segment or system. This software tool provides a
framework for introducing pipeline component dependencies into critical infrastructure analyses.
Development goals â Not specified.
Intended users â Not specified.
System output â Results include graphs and tables for steady-state flow rate, pressure, and line
capacity distributions. The hydraulic gradient along the pipeline is also displayed. After a simulation,
the analysis results indicate the potential effect on pipeline operations. The diagram below indicates
that the long-term loss of a specific pump station can lead to isolation or curtailment of the deliveries of
petroleum fuels.
Maturity â The system is in development into the DOT. NET framework.
Areas modeled â Experts at Argonne have applied PFNAM to a number of crude oil and refined
petroleum products pipelines. Other potential applications are being explored.
Customers/sponsors â US Department of Defense.
Model Framework
Underlying model â Mathematical model.
Simulation â PFNAM allows the analyst to address a wide range of âwhat ifâ questions. Two of the
main outputs of a PFNAM simulation are pressure and pipeline capacity estimates along the pipeline.
This allows the analyst to determine whether an outage of a pipeline component will result in pipeline
shutdown or degradation in pipeline throughput.
Data format â Access database.
Sensor data â Accepts pipeline pressure and flow.
Coupling with other models â This model is compatible with the NG Tool set developed at Argonne.
Human activity modeling â Human activities are not modeled.
System Requirements
Not specified.
Hardware
Not specified.
Software
Other Notes

95

References

96

97

98

Model Name
Organization
POC

Transportation Routing Analysis Geographic Information System (TRAGIS)
Oak Ridge National Laboratory
Infrastructures
Paul E. Johnson
RL, HW, WW, POL
johnsonpe@ornl.gov

Description
Overview â The Transportation Routing Analysis Geographic Information System (TRAGIS) model is
used to calculate highway, rail, or waterway routes within the United States. TRAGIS is a client-server
application with the user interface and map data files residing on the userâs personal computer and the
routing engine and network data files on a network server. By default, the model calculates commercial
highway routes; but with the change of the route type, the model can determine routes that meet the
U.S. Department of Transportation (DOT) regulations for shipments of highway route-controlled
quantities (HRCQ) of radioactive material, routes for shipments to the Waste Isolation Pilot Plant
(WIPP), the shortest, or the quickest route.
Development goals â The goal for WebTRAGIS is to have national 1:100,000-scale routing networks.
The highway network developed for TRAGIS is a 1:100,000-scale database. The legacy HIGHWAY
model used a stick figure network with nodes digitized at 1:250,000-scale. The TRAGIS highway
network was developed from the U.S. Geological Survey (USGS) Digital Line Graphs and the U.S.
Bureau of Census Topologically Integrated Geographic Encoding and Referencing (TIGER) system.
The rail network used in the initial version of TRAGIS was the same database as that used in the
INTERLINE model. This network also was a stick figure network with nodes that were digitized from
variable scaled maps. A 1:100,000-scale rail network is now incorporated into TRAGIS. The current
inland waterway network is based on the USGS 1:2,000,000-U.S. Geodata. Deep-water routes are
depicted in WebTRAGIS as straight-line segments. It is planned to incorporate a 1:100,000-scale
waterway database into the model at a future time so that all modes will be at a consistent scale.
Intended users â internal and external transportation route planners.
System output â Web-based Graphical 2D map display or textual reports.
Maturity â Mature â commercial.
Areas modeled â United States.
Customers/sponsors â Funding for the development of TRAGIS has been provided by the National
Transportation Program (NTP) of the U.S. Department of Energy (DOE).
Model Framework
Underlying model â TRAGIS is a client-server application where the user interface and map data files
reside on the userâs personal computer (PC) and the routing engine and its large data files reside on the
server. The model uses the World Wide Web (WWW) for communications between the client and the
server. There are two user interfaces for TRAGIS: WebTRAGIS, which is the primary client user
interface, and BatchTRAGIS, which is a specialized user interface that allows multiple routes to be
prepared and then calculated at one time.
Simulation â The simulation utilizes a network flow model, which determines the optimal routes based
upon an optimization of the impedance measures between endpoints. The impedance is a valued
function based upon route type and requirements. Transportation between various sectors is modeled
(such as, rail to road transfer, barge to rail, etc.). Population demographics is a component of the model
to determine routing criteria for HAZMAT.
Data format â Not specified.
Sensor data â None.
Coupling with other models â Not directly.
Human activity modeling â No.
System Requirements
99

Hardware
Software
Other Notes

PC with Internet Access.
Windows.

100

References
WebTragis https://tragis.ornl.gov/
Transportation RoutingAnalysis Geographic Information System (TRAGIS) Userâs Manual, Oak Ridge
National Laboratory, ORNL/NTRC-006 Rev 0, June 2003. https://tragis.ornl.gov/TRAGISmanual.pdf.

101

102

Model Name TRANSIM
Los Alamos National Laboratory
Organization
Infrastructures
HW, HA
Jim Smith
POC
Description
Overview â TRANSIMS is an agent-based simulation system capable of simulating the
second-by-second movements of every person and every vehicle through the transportation network of
a large metropolitan area.
It consists of mutually supporting simulations, models, and databases. By employing advanced
computational and analytical techniques, it creates an integrated environment for regional
transportation system analysis. TRANSIMS is an integrated suite of products containing an easy-to-use
graphical user interface for the modeling functions, a GIS-based network editor, 3D data visualization
and animation software, and a reporting system. TRANSIMS is designed to give transportation
planners more accurate, complete information on:
Â·
Traffic impacts
Â·
Energy consumption
Â·
Traffic congestion
Â·
Land use planning.
The core code version of TRANSIMS (TRANSIMS-LANL), developed at Los Alamos National
Laboratory, is distributed for a nominal fee to universities on this Web site. The commercial version of
TRANSIMS (TRANSIMS-DOT) was developed from the core software package especially for the
Department of Transportation by IBM, and it has a more elaborate interface and specific features to
meet requirements by the DOT. It is not available on this Web site.
Development goals â Started as laboratory-directed research and development in the late 1980s for the
Department of Transportation. Funding is continuing under Department of Homeland Security (DHS)
National Infrastructure Simulation and Analysis Center (NISAC). TRANSIMS technology was
developed under U.S. Department of Transportation and EPA funding at the Los Alamos National
Laboratory (LANL) over the last eight years. It is a result of an effort to develop new transportation
and air quality modeling methodologies required by the Clean Air Act, the Transportation Equity Act
for the 21st Century (TEA 21), and other regulations.
Intended users â Internal analyst âused to support the development of an impact report on for specific
infrastructure events (such as, hurricanes, terrorist attacks, etc.), external analysts.
System output â Visualization of demographics data with a city or region illustrating the human
activity such as traffic patterns and work patterns.
Maturity â Mature internal and commercial.
Areas modeled â Customers/sponsors â NISAC â DHS.
Model Framework
Underlying model â Cellular Autonoma.
Simulation â Discrete event, agent based simulation.
Data â Multiple data sources including Census data, Household Survey Data, Dunn and Bradstreet
Data.
Sensor data â No direct sensor feeds.
Coupling with other models â Yes, coupling is done directly (EPISIM).
Human activity modeling â Yes â social network and human mobility model.
System Requirements
Memory and disk requirements depend upon the scenario that is used, but large
Hardware
networks require a large Linux cluster. Some scenarios may consist of 10 â 100
103

Software

million agents.
The TRANSIMS distribution requires that the user install the following software.
Linux
Â·
X11R6 libraries (Xmu, Xi, X11, Xext, Xt)
Â·
OpenGL and the OpenGL Utilities Toolkit libraries (Mesa/Glut)
Â·
Linux libraries (stdc++, ld-linux, ICE, SM)
Â·
Perl.
All of the third-party software used by TRANSIMS is available on Red Hat Linux
distribution CDs. The latest versions of the following packages should be installed:
kernel, kernel-headers, gcc, glibc, libstdc++, make, perl, XFree86, Mesa, Mesa-devel,
Mesa-Glut, Mesa-Glut-devel, MPI, and PVM.
Solaris
XllR6 libraries in /usr/openwin, OpenGL, OpenGL Utilities Toolkit libraries (glut),
and Perl.
Metis, PVM, MPI, and SPRNG are supplied with the TRANSIMS distribution.

Other Notes
Los Alamos National Laboratory's TRANSIMS software is based on a computationally intensive,
agent-based simulation technology requiring significant multiprocessor computing hardware. Programs
in the TRANSIMS software suite are distributed applications with components running on different
hardware/software platforms. To install and run all of the components of the TRANSIMS suite, the
customer must procure and set up the following three types of computer systems:
1.
Unix/Linux server(s) for hosting the core TRANSIMS software, Oracle database, and serverside components of the TRANSIMS modeling interface. Customers who wish to execute largesize problems must have procured multiserver Linux computing cluster or an equivalent
multiprocessor UNIX-based framework.
2.
Windows workstation(s) for running the Network Editor, the client-side modeling interface, and
Crystal Reports.
3.
Optional Linux workstation(s) for running the Visualizer. Alternatively, the customer may wish
to equip the Linux server with a high-end graphics card and use the server as the Visualizer
platform. A version of the output Visualizer that operates on the Windows workstation is in
development.
TRANSIMS was tested in a Linux cluster environment on Red Hat Linux 6.2 and compiled with
gcc/g++ 2.95.2. Limited tests in a single-CPU environment were done on Red Hat Linux 7.1 using
gcc/g++ 2.96.
To run the traffic microsimulator under PVM or MPI, the Linux kernel must be compiled with
networking support and must have an assigned IP address and a host name. An actual network card is
not required. The following options must be selected in the Linux kernel configuration:
Â·
Networking support (CONFIG_NET)
Â·
System V IPC (CONFIG_SYSVIPC)
Â·
TCP/IP networking (CONFIG_INET)
Â·
Dummy-net driver support (CONFIG_DUMMY)
Â·
The appropriate network card driver.
The default kernel shipped with Red Hat 6.2 and 7.1 is configured with the appropriate options. The
following package categories should be selected during Red Hat Linux installation to run the
TRANSIMS components:
104

Â·
X Window System
Â·
Mesa/GL
Â·
Glut.
Additional package categories should be selected to compile the TRANSIMS components:
Â·
C Development
Â·
Development Libraries
Â·
C++ Development
Â·
X Development.

105

106

References
http://www.transims.net/home.html
ZoltÃ¡n Toroczkai âAgent-Based Modeling as a Decision Making Tool: How to Halt a Smallpox
Epidemic How to Halt a Smallpox Epidemicâ, Center for Nonlinear Studies, Theoretical Division, Los
Alamos National Laboratory.

107

108

Model Name
Organization
POC

Water Infrastructure Simulation Environment (WISE)
Los Alamos National Laboratory
Joe Holland

Infrastructures
DW, SW, ST

Description
Overview â The Water Infrastructure Simulation Environment (WISE) is an analytic framework
supporting the evaluation of water infrastructure in terms of both infrastructure specific and
interdependency issues.
Development goals â Not specified.
Intended users â Internal analyst â IEISS used to support the development of an impact report on for
specific infrastructure events (such as, hurricanes, terrorist attacks, etc.).
System output â Key components in the WISE framework are ArcWISE, a GIS based graphical user
interface, and IEISS Water, a water infrastructure interdependency simulation capability within IEISS.
ArcWISE leverages the existing data management, analysis, and display capabilities within geographic
information systems while also extending them to infer, improve, and amend water infrastructure data
in support of running hydraulic simulation engines such as EPANET or IEISS Water. ArcWISE also
provides tools for defining and simulating infrastructure damage events, such as a fire, and generating
water demand/sewage production estimates. IEISS Water is an extension of the IEISS analysis
software to water distribution infrastructure simulation.
Maturity â Development.
Areas modeled â Numerous U.S. metropolitan areas.
Customers/sponsors â NISAC â DHS.
Model Framework
Underlying model â Flow and Dispersion Model.
Simulation â A continuous time based model with an underling physical engine for system dynamics.
WISE involves the integration of geographic information systems with a wide range of infrastructure
analysis tools including industry standard hydraulic simulation engines (e.g., EPANET and SWMM) as
well as Los Alamos National Laboratory interdependency simulation systems such the Urban
Infrastructure Suite (UIS) and the Interdependent Energy Infrastructure Simulation System (IEISS).
Data format â Not specified.
Sensor data â No direct sensor feeds.
Coupling with other models â Yes, coupling is done indirectly. The output of IEISS will serve as the
input to other infrastructure models to identify cross infrastructure effects.
Human activity modeling â None at this time.
System Requirements
Not specified.
Hardware
ArcWise, EPANET and SWMM.
Software
Other Notes

109

References

110

Model Name
Organization

POC

MIT Screening MethodologyâA Screening Methodology for the Identification and
Ranking of Infrastructure Vulnerabilities Due to Terrorism
Massachusetts Institute of Technology (MIT),
Infrastructures
Engineering Systems Division and Department of Electric power,
Nuclear Science and Engineering
natural gas, and
George E. Apostolakis
drinking water
apostola@mit.edu
Douglas M. Lemon

Description
Overview â This research proposes a methodology for the identification and prioritization of
vulnerabilities in infrastructures. Portions of the Massachusetts Institute of Technology (MIT) campus
were assessed using this methodology. Infrastructures are modeled as digraphs and graph theory is
employed to identify the candidate vulnerable scenarios. Screening of scenarios is performed to
produce a prioritized list of vulnerabilities. Prioritization is based on multiattribute utility theory
(MAUT). The value of a lost element is based on a rated impact of losing infrastructure services.
Development goals â Professor Apostolakas and others are continuing to extend this work as described
in the âOther Notesâ section, which follows.
Intended users â The intended users for this methodology include analysts and decision makers for
evaluation and risk management.
System output â The system provides numeric ranking values for infrastructure elements as output.
Maturity â This is a research and development level method.
Areas modeled â Portions of MIT campus including electric power, water, and natural gas
infrastructures.
Customers/sponsors â MIT and the U.S. Navy sponsored the work.
Model Framework
Underlying model â This methodology is based on graph theory, MAUT (for identifying and ranking
vulnerabilities), and mathematical network analysis (for infrastructure modeling).
Simulation â The method allows simulations based on perceived terrorist threats.
Data format â Not specified.
Sensor data â None.
Coupling with other models â Not specified.
Human activity modeling â None.
System Requirements
Not specified.
Hardware
Not Specified.
Software
Other Notes
Since the publication of the subject methodology, MIT has continued similar work. A recent paper, âA
Methodology for Ranking the Elements of Water-Supply Networks,â co-written by David Michaudâ
also of MITâhas been accepted for publication in the Journal of Infrastructure Systems in 2006. That
work is based on a case study of a mid-sized city and presents a scenario-based methodology for
ranking elements of water-supply networks.

111

References
Apostolakas, G., and Lemon, D. (2005). âA Screening Methodology for the Identification and Ranking
of Infrastructure Vulnerabilities Due to Terrorism,â Risk Analysis, Vol. 25, No. 2, pp. 361-376.

112

Model Name
Organization
POC

The Urban Infrastructure Suite (UIS)
Los Alamos National Laboratory
Randy Michelsen
rem@lanl.gov

Infrastructures
HW, HA, TC, AST,
SW, DW

Description
Overview â The Urban Infrastructure Suite (UIS) is a set of interoperable modules that employ
advanced modeling and simulation methodologies to represent urban infrastructures and
populations. These simulation-based modules are linked through a common interface for the flow
of information between UIS sector simulations to model urban transportation,
telecommunications, public health, energy, financial (commodity markets), and water-distribution
infrastructures and their interdependencies.
x Urban Population Mobility Simulation Technologies (UPMoST) Module
x Epidemiological Simulation Systems (EpiSims) Module
x Telecommunications Sector: AdHopNet Module
x Transportation Analysis Simulation System (TRANSIMS) Module
x Water Infrastructure Simulation Environment (WISE)
x Generic Cities Project
Development goals â The project objective (NISAC) is to understand the infrastructuresâ
performance under unusual conditions, the effects of interdependencies, and the dynamics of their
interconnections. To better understand the complexities of the interconnected infrastructures, the
team has collaborated with private sector infrastructure experts to develop methodologies and
tools for characterizing and simulating their performance.
Intended users â LANL internal analysts.
System Output â Graphical overlays and textual based output.
Maturity â Development.
Areas modeled â Multiple.
Customers/sponsors â DHS â NISAC.
Model Framework
Underlying models:
x Urban Population Mobility Simulation Technologies (UPMoST) Module
x Epidemiological Simulation Systems (EpiSims) Module - a contact-based approach for
evaluating the spread of disease among a populace. It looks at infection rates based on the
assumed numbers of contacts people in different demographic groups might have with others
in their families, workplaces, and communities. Interactions/contacts are based on the
TRANSIMâs mobility model.
x Telecommunications Sector: MIITS Module (formerly AdHopNet) â end to end
communications system simulation, agent based simulating individual packets, devices,
connections, etc., input is TRANSIMS mobility model.
x Transportation Analysis Simulation System (TRANSIMS) Module â synthetic population
model, cellular automata microsimulation. The output is population mobility with
demographics
x Water Infrastructure Simulation Environment (WISE) â is an analytic framework supporting
the evaluation of water infrastructure in terms of both infrastructure specific and
interdependency issues.
x Generic Cities Project â module to create representative but not necessarily accurate city
representations in terms of demographic data.
Simulation â TRANSIM mobility/social network modelâagent-based, Epidemic modelâ
differential equation based.
Data â Multiple sources.
Sensor data â None.
Ability to couple with other models â Suite of coupled modules for different infrastructure
113

sectors.
Human Activity modeling â Yes. Mobility and Social Interaction Model.
System Requirements
Hardware
Software
Other Notes

Large models require a Linux Cluster.
Linux, various.

Images:

References
Barrett, Christopher L, Stephen Eubank, V.S. Anil Kumar, and Madhav V. Marathe From
The Mathematics of Networks, Understanding Large-Scale Social and Infrastructure Networks: A
Simulation-Based Approach, SIAM News, Volume 37, Number 4, May 2004

114

REFERENCES
1.

Congressional Research Service Report for Congress. 2002 Critical Infrastructures: Background, Policy
and Implementation. Available online at http://www.iwar.org.uk/cip/resources/pdd63/crs-report.pdf.

2.

Congressional Research Service Report for Congress. 2002 Critical Infrastructures: Background, Policy
and Implementation. Available online at http://www.iwar.org.uk/cip/resources/pdd63/crs-report.pdf.

3.

http://www.tswg.gov

4.

Executive Order, 13010. Critical Infrastructure Protection. Federal Register. Vol. 61. No. 138. July 17,
1996. pp. 3747-3750.

5.

Executive Order, 13130 National Infrastructure Assurance Council, Federal Register, Vol. 64, No. 137,
July 19, 1999. pp. 38535-38536.

6.

Executive order 13231 Critical Infrastructure Protection in the Information Age. Federal Register. Vol.
66. No. 202. October 18, 2001. pp. 53063â53071. The NIAC is established on page 53069.

7.

D. Mussington, âConcepts for Enhancing Critical Infrastructure Protection: Relating Y2K to CIP
Research and Development.â RAND:Science and Technology Institute, Santa Monica, CA, 2002, p 29.

8.

Layton, L. and D. Phillips. 2001. Train Sets Tunnel Afire, Shuts Down Baltimore. Available online via <
http://www.washingtonpost.com/ac2/wp-dyn?pagename=article&node=&contentId=A175422001Jul18>, accessed March 28, 2002.

9.

Ratner, A. 2001. Train derailment severs communications. Available online via
<http://www.baltimoresun.com/news/local/bal-email19.story?coll=bal-home-headlines>, accessed
March 28, 2002.

10. Little, R. and P. Adams. 2001, Tunnel fire choking East Coast rail freight. Available online via
<http://www.baltimoresun.com/news/local/bal-te.bz.freight20jul20.story?coll= bal-home-headlines>,
Accessed March 28, 2002.
11. M. Dunn, and I.Wigert. International CIIP Handbook 2004: An Inventory and Analysis of Protection
Policies in Fourteen Countries. Zurich: Swiss Federal Institute of Technology: 2004, p. 243.
12. D. D. Dudenhoeffer, M. R. Permann, and M Manic, âCIMS: A Framework For Infrastructure
Interdependency Modeling And Analysis.â Submitted to Proceedings of the 2006 Winter Simulation
Conference, L. F. Perrone, F. P. Wieland, J. Liu, B. G. Lawson, D. M. Nicol, and R. M. Fujimoto.
Piscataway, New Jersey: Institute of Electrical and Electronics Engineers, 2006.
13. S. Rinaldi, J. Peerenboom, and T. Kelly. âIdentifying, Understanding, and Analyzing Critical
Infrastructure Interdependencies,â IEEE Control Systems Magazine, IEEE, December 2001, pp. 11-25.
14. U.S. Department Of Energy Office of Critical Infrastructure Protection. 2001. Critical Infrastructure
Interdependencies: Impact of the September 11 Terrorist Attacks on the World Trade Center, A Case
Study, 2001, p. 10.
15. UNITED STATES CONGRESS, âU.S.A. Patriot Actâ, 2001,
http://www.epic.org/privacy/terrorism/hr3162.html.
16. UNITED STATES CONGRESS, âU.S.A. Patriotâ Act, 2001,
http://www.epic.org/privacy/terrorism/hr3162.html
115

17. United States Joint Forces Command, Operational Net Assessment (ONA) Concept Of Operations For
Millennium Challenge 02 (MC-02), October 2001.
18. Hammes, T. The Sling and the Stone, Zenith Press, St. Paul MN, 2004, p.2.
19. United States Joint Forces Command, The Joint Warfighting Center, Joint Doctrine Series Pamphlet 4,
Doctrinal Implications of Operational Net Assessment (ONA), 2004.
20. LandScan, http://www.ornl.gov/sci/landscan/index.html.
21. Los Alamos National Laboratory, http://lanl.gov/orgs/d/nisac/,
http://www.sandia.gov/mission/homeland/programs/critical/nisac.html.
22. National Energy Technology Laboratory,
http://www.netl.doe.gov/onsite_research/Facilities/energy.html.
23. Technical Support Working Group, http://www.tswg.gov.
24. Federal Business Opportunities, http://www.fbo.gov/spg/USAF/AFMC/
AFRLRRS/Reference%2DNumber%2DBAA%2D06%2D07%2DIFKA/SynopsisP.html.
25. Amin, M. âNational Infrastructures as Complex Interactive Networksâ, Chapter 14 in
Automation,Control, and Complexity: New Developments and Directions, Samad & Weyrauch (Eds.),
John Wiley and Sons, March 2000, Table 14.1.
26. Schmitz, W. and K. A. Neubecker. 2003. Architecture of an Integrated Model Hierarchy: Work Package
6, Deliverable D6.2, ACIP Technical Report IST-2001-37257: 32. Available via
http://www.iabg.de/acip/doc/wp6/D62_architecture.pdf, Accessed April 1, 2006.

116

Influence Propagation in Adversarial Setting: How to
Defeat Competition with Least Amount of Investment
Shahrzad Shirazipourazad, Brian Bogard, Harsh Vachhani, Arunabha Sen
School of Computing, Informatics and Decision Systems Engineering
Arizona State University
Tempe, AZ 85287

{sshiraz1, bbogard, hvachhan, asen}@asu.edu
Paul Horn
Department of Mathematics
Harvard University
Cambridge, MA 09322

phorn@math.harvard.edu
ABSTRACT
It has been observed that individualsâ decisions to adopt a
product or innovation are often influenced by the recommendations of their friends and acquaintances. Motivated
by this observation, the last few years have seen a number
of studies on influence maximization in social networks. The
primary goal of these studies is identification of k most influential nodes in a network. A major limitation of these
studies is that they focus on a non-adversarial environment,
where only one player is engaged in influencing the nodes.
However, in a realistic scenario multiple players attempt to
influence the nodes in a competitive fashion. The proposed
model considers a competitive environment where a node
that has not yet adopted an innovation, can adopt only one
of the several competing innovations and once it adopts an
innovation, it does not switch. The paper studies the scenario where the first player has already chosen a set of k
nodes and the second player, with the knowledge of the
choice of the first, attempts to identify a smallest set of
nodes (excluding the ones already chosen by the first) so
that when the influence propagation process ends, the number of nodes influenced by the second player is larger than
the number of nodes influenced by the first.
The paper studies two propagation models and shows that
in both the models, the identification of the smallest set of
nodes to defeat the adversary is NP-Hard. It provides an
approximation algorithm and proves that the performance
bound is tight. It also presents the results of extensive experimentation using the collaboration network data. Experimental results show that the second player can easily
defeat the first with this algorithm, if the first utilizes the
node degree or closeness centrality based algorithms for the
selection of influential nodes. The proposed algorithm also

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
CIKMâ12, October 29âNovember 2, 2012, Maui, HI, USA.
Copyright 2012 ACM 978-1-4503-1156-4/12/10 ...$15.00.

provides better performance if the second player utilizes it
instead of the greedy algorithm to maximize its influence.

Categories and Subject Descriptors
F.2.2 [Analysis of Algorithms and Problem Complexity]: [Non-numerical Algorithms and Problems]

General Terms
Algorithms, Experimentation, Performance

Keywords
Social Networks, Influence Maximization, Adversarial Environment

1.

INTRODUCTION

It has been widely observed in various studies in social sciences and economics that an individualsâ decision to adopt
a product, behavior or innovation is often influenced by the
recommendations of their friends and acquaintances. Motivated by this observation, the last few years have seen a
number of studies on influence maximization problem in social networks [2, 3, 4, 6, 7, 11, 13]. One major goal of several
of these studies is identification of k most influential nodes
in a network. A product manufacturer may want to identify
the k most influential nodes in the network, as she may want
to incentivize these nodes to buy the new product by providing free samples to them, on the expectation that once
these nodes are convinced about the quality of the product, they will recommend it to their friends on the social
network and encourage them to buy the product. This set
of k nodes, being the most influential on the network, will
have the largest impact on convincing the rest of the nodes
about the quality of the product. Since the manufacturer
has a fixed budget for advertising, she can provide free samples only to a limited number of nodes in the network. The
size of the advertising budget determines the value of the
parameter k.
It may be noted that most of the studies on influence
propagation are geared toward a non-adversarial environment, where only one manufacturer (player) is attempting

to influence the nodes of a social network to buy her product. However, in a realistic market scenario, most often
there exists multiple players, each attempting to sell their
competing products or innovations. For example, just as
Coke attempts to convince customers in an emerging market about the quality of their beverage, its main competitor,
Pepsi, also does the same. Both the competitors have only a
finite advertisement budget and both of them want to derive
the greatest benefit out of their advertising campaign. The
goal of both the players often is to capture a share of this
emerging market that is larger than its competition.
The non-adversarial influence propagation models consider scenarios where a user (a node u in a social network
graph G = (V, E)) adopts (or does not adopt) an innovation based on how her acquaintances have adopted the innovation. In these models each node u in the social network
graph is in one of the following two states: (i) u has adopted
innovation A, and (ii) u has not adopted innovation A but u
is open to the idea of adoption. One can visualize such a scenario by coloring the nodes of the social network graph with
red if they have adopted the innovation A and with white if
they have not adopted A yet, but are open to the idea of
adopting A in the future. As the diffusion process progresses
with time, by observing changing color of the nodes of the
graph one can infer if innovation A is being adopted by the
members of the social network. Although, this paper focus
on influence propagation in social networks, conceptually,
the scenario is identical for spread of any contagion through
a network - be it spread of diseases through a human contact
network or spread of worms through the Internet.
The influence (contagion) propagation models can be divided into three distinct classes:
â¢ Class I: Non-adversarial
â¢ Class II: Adversarial with passive adversary
â¢ Class III Adversarial with active adversary
The problems in classes I and II can be stated as follows:
â¢ Class I: How to identify a set of k initial (seed) nodes,
so that once they are influenced/infected, they will infect the largest number of uninfected nodes in the network?
â¢ Class II: Given that a subset of the nodes is already influenced/infected, how to identify a set of k uninfected
nodes, so that when they are immunized, they will have
the largest impact in preventing the uninfected nodes
from being infected.
In most of the influence propagation models, influence
propagates in a step-by-step fashion and as such there is
a notion of time step (or propagation step) involved. The
expected number of nodes influenced at the end of time step
D is at most the expected number of nodes influenced at the
end of time step D + 1. In other words, expected number
of nodes influenced at the end of time step D is a nondecreasing function of D.
The Class I influence propagation problem considered in
[11] may be viewed to have three dimensions, (i) the number
of seed nodes activated at the beginning (budget or cost of
influence), (ii) the expected number of activated nodes at
the end of propagation (impact or coverage of initial seed
nodes), and (iii) time steps for propagation. The objective
of the influence maximization problem considered in [11], is

to maximize the coverage subject to a budget constraint but
without any constraint on the number of time steps.
The Class I problem considered in [11] can be stated in
the following way: âWhich k white nodes should be colored
red initially, so that the largest number of white nodes turn
to red at the end of propagation process?â. The Class II
problems can be stated in the following way: âGiven that
some nodes are already colored red, which k white nodes
should be colored blue, so that this set of nodes will have the
largest impact in preventing the white nodes from turning
red.
In Class I, there is no notion of an adversary. The red
nodes are trying to convert all the white nodes into red
nodes and there is no agent that is actively trying to prevent this conversion. The Class II, although it has a notion
of an adversary (i.e., the blue nodes) which is trying to slow
down (or stop) white-to-red conversion, at best this agent
can be viewed as a passive adversary, because its goal is
to prevent white-to-red conversion, and it is not engaged in
white-to-blue conversion. This gives rise to Class III, a truly
adversarial scenario, where the red agent is trying to convert
all the white nodes into red, while the blue agent is trying
to convert all the white nodes into blue. In this case, the
blue agent can be viewed as an active adversary of the red
agent.
The Class III models the scenario where a node u is being
actively encouraged by an adversary not only not to adopt
the innovation but also to adopt a competing innovation. In
this case, each node u in the social network graph can be
in one of following three states: (i) u has adopted innovation A , (ii) u has adopted innovation B, and (iii) u has not
adopted any innovation A or B but is open to the idea of
adopting either one of them. This adversarial scenario can
be viewed as a classic case of a strategic conflict game between the proponent(s) and the opponent(s) of adoption of
an innovation and a game is won by the proponent(s) if u
decides to adopt the innovation A.
This paper studies a Class III scenario where two vendors
(players) are trying to sell their competing products by influencing the nodes of a social network. The goal of both
the players is to have a market share that is larger than its
competition. It considers the scenario where the first player
(P1 ) has already chosen the k nodes to have a large influence (coverage) on the social network. The second player is
aware of the first playerâs choice and the goal of the second
player (P2 ) is to identify a smallest set of nodes (excluding
the ones already chosen by the first player) so that the number of nodes influenced by the second player will be larger
than the number of nodes influenced by the first player within
D time steps. In other words, the objective of the problem
is to minimze the cost subject to the constraint that the coverage of the second player is larger than the coverage of the
first player within D time steps. Since the goal of the second
player is to win the âgameâ (i.e., to have a larger coverage or
market share), with influencing (incentivizing) as few nodes
as possible, the problem under study in this paper is referred
to as the âWinning with Minimum Investmentâ (WMI) problem. In [3], the authors study a similar problem belonging
to class III. However, the objective of the problem studied in
[3] is different from the one being studied in this paper. The
goal of the second player in the problem studied in [3] is not
to defeat the first player with least amount of investment,
but to maximize its own influence.

Using the same two influence propagation models introduced in [3], the contributions of the paper may be listed as
follows:
â¢ Introduction of a new influence propagation problem
in an adversarial setting where the goal of the second
player is to defeat the first within D time steps and
least amount of cost (i.e., number of seed nodes)
â¢ NP-Hardness proof for the problem under both the
influence propagation models
â¢ Approximation algorithm for the problem with a tight
performance bound.
â¢ Experimental evaluation of the Approximation algorithm with collaboration network data
Experimental results show that utilizing the proposed algorithm, the second player can easily defeat the first, if the first
player utilizes the node degree or closeness centrality based
algorithms for the selection of the initial (seed) nodes. The
proposed algorithm also provides better performance for the
second player if she utilizes it instead of the algorithm to
maximize influence proposed in [3], in the sense that it requires selection of a fewer number of seed nodes to defeat
the first player.
The rest of the paper is organized as follows. The section II summarizes related work on influence propagation.
The section III describes the propagation models used in
the paper in detail. The sections IV, V and VI discuss the
problem statement, computational complexity and approximation algorithm results respectively. The results of experimental evaluation is presented in section VII and section
VIII concludes the paper.

2.

BACKGROUND AND RELATED WORK

The studies on identification of influential nodes in a social
network were triggered by a paper authored by Domingos
and Richardson [6]. They introduced the notion of ânetwork
valueâ of a node in a social network and using a Markov
random field model where a joint distribution over all node
behavior is specified, computed the network value of the
nodes. Kempe, Kleinberg and Tardos followed up the work
in [6] by providing new models derived from mathematical
sociology and interacting particle systems [11]. They made
a number of important contributions by providing approximation algorithms for maximizing the spread of influence
in these models by utilizing the submodularity property of
the objective functions. In addition to providing algorithms
with provable performance guarantee, they also presented
experimental results on large collaboration networks. Their
experimental results showed that their greedy approximation algorithm significantly out-performed the node selection
heuristics based on degree centrality and distance centrality
[18].
The approximation algorithm proposed in [11] is computeintensive. Accordingly, several researchers approached the
issue of scalability from different directions. Chen et. al. in
[4] provided improvement of the original greedy algorithm of
[11] and proposed a degree discount heuristic to improve influence spread. Mathioudakis in [13] introduced the notion
of sparsification of influence networks and presented an algorithm, SPINE, to compute the âbackboneâ of the influence
network. Utilizing SPINE as a pre-processing step for the

influence maximization problem, they showed that computation on the sparsified model provided significant improvements in terms of speedup without compromising accuracy.
Wang et. al. in [17] considered the influential node identification problem in a mobile social network and presented a
two step process, where in the first step, communities in the
social network are detected and in the second step a subset
of communities is selected to identify the influential nodes.
Experimental results with data from large real world mobile
social network showed that their algorithm performed an
order of magnitude faster than the state-of-the-art greedy
algorithm for finding the top-k influential nodes. A simulated annealing (SA) based algorithm for finding the top-k
influential nodes was presented in [10]. It has been reported
in [10], that using data from four real networks, the SA based
algorithm performed 2-3 orders of magnitude faster than the
state-of-the-art greedy algorithm.
In addition to attempts to address the scalability issue of
the greedy algorithm in [11], efforts on variations of the original problem formulation and also the computation model is
underway in the research community. In [7] two new problem formulations are provided. In the first formulation, the
goal is to minimize the cost, subject to the constraint that
coverage exceeds a minimum threshold Î½ without any constraint on the number of time steps. The goal of the second
formulation is to minimize the number of time steps, subject to a budget constraint k and a coverage constraint Î½.
For the first version of the problem, the authors provide a
simple greedy algorithm and show that it provides a bicriteria approximation. For the second version, they show that
even bicriteria or tricriteria approximations are hard under
several conditions. In [1], the authors argue that a user (a
node in the social network) may be influenced by positive
recommendations from a group of friends (neighbors in the
network) but that does not necessarily imply that she will
adopt the product herself. However, she may pass on her
positive impression about the product to another group of
friends. Clearly, such a model departs from the model considered in [11]. The authors in [1] consider an âadoption
maximizationâ problem instead of âinfluence maximizationâ
problem and present both analytical and experimental results for the new problem. The authors in [12] argue that a
limitation of the traditional influence analysis technique is
that they only consider positive relations (agreement, trust)
and ignore the negative relations (distrust, disagreement).
Moreover, the traditional techniques also ignore conformity
of people, i.e., an individualâs inclination to be influenced.
The paper studies the interplay between influence and conformity of each individual and computes the influence and
conformity indices of individuals. The authors in [5] suggest
an alternate way of measuring the influencing capability of
an individual on her peers, through the individuals reach
within the social network for certain actions.
All the references discussed in the last three paragraphs
pertain to the class I (non-adversarial) problems as defined
in the previous section. Results on study of class II problems
(adversarial with passive adversary) is presented in [8]. It
focuses on identification of blockers, the nodes that are most
effective in blocking the spread of a dynamic process through
a social network, and reports that simple local measures such
as the degree of a node are good indicators of its effectiveness
as a blocker. The blocker identification problem has been
extensively studied in the public health community, where

the goal is to stop or slow down progress of an infectious
disease by immunizing a small set of key individuals in the
community.
As indicated in the previous section, the WMI problem
studied in this paper belongs to Class III (adversarial with
active adversary). Unfortunately, there exists only a handful of studies on problems belonging to Class III. Bharathi
et. al. were one of the earliest to study a Class III problem
[2]. They proposed a mathematical model for diffusion of
multiple innovations in a network, an approximation algorithm with a (1 â 1/e) performance guarantee for computing the best response to an opponentâs strategy. In addition
they prove that the âprice of competitionâ of the game is
at most 2. While game theoretic framework was utilized
for deriving the results in [2], Carnes et al. used an algorithmic framework to study a Class III problem [3]. Their
research primarily extends the problem studied in [11] from
the Class I domain to the Class III domain. They study
the followerâs perspective (i.e., the player who entered the
market after the first player) and investigate how a follower
can maximize her influence in the network with a limited
budget, given that the first player has already entered the
market and influenced a certain number of key individuals
(nodes in the network). They prove that the influence maximization problem for the second player is NP-complete and
provide an approximation algorithm that is guaranteed to
produce a solution within 63% of the optimal. Adversarial models in evolutionary game dynamics was studied by
Istrate em et al. in [9].
In all the problems discussed in [2, 3] once a node adopts
an innovation (i.e., changes its color from white to red or
white to blue), it is not allowed to change its color, i.e., the
model precludes the possibility of an individual changing her
mind. However, the model considered by Nowak et al. in
[16] there are only red and blue nodes (no white nodes) and
the model allows a node to change its color from red to blue
and vice-versa. Although this model was developed to capture a biological phenomenon involving viruses and cells, this
model can be equally effective in capturing the phenomenon
of the spread of ideas and behaviors in human population.
Using evolutionary game theoretic and evolutionary graph
theoretic techniques, the authors establish fundamental laws
that govern choices of competing players regarding strategies.

3.

INFLUENCE PROPAGATION MODELS

A number of influence propagation models for the
non-adversarial scenario have been proposed in the literature [11]. Among these, the Linear Threshold Model (LTM)
and the Independent Cascade Model (ICM) have drawn most
attention in the research community. As indicated earlier,
the literature on influence propagation in adversarial scenario with active adversaries is very sparse [2, 3]. Bharati
et al. in [2] and Carnes et al. in [3] have studied influence
propagation in adversarial scenario with active adversaries,
and have proposed two different models for it. Both of these
two models are generalizations of the Independent Cascade
Model. The model proposed in [2] is suitable for a multiplayer scenario, whereas the model proposed in [3] is for two
competing players. Bharati et al. in [2] study the problem from a game-theoretic perspective and focus on finding
best response strategies for the players. Carnes et al. on
the other hand study the problem from an algorithmic per-

spective. Since this paper studies the problem with only
two competing players, the models proposed in [3] are more
relevant for this study than the one proposed in [2]. Accordingly, the influence propagation models of [3] are used here.
Since these models, Distance-based Model (DBM) and Wavepropagation Model (WPM), are generalization of the ICM,
the paper first discusses ICM and then DBM and WPM.

3.1

Independent Cascade Model

The social network is modeled as a graph G = (V, E),
where each node represents an individual. Each individual
may either be active (i.e., has adopted innovation) or inactive. A node can switch from an inactive state to an active
state but cannot switch back in the other direction. The
propagation process from the perspective of an inactivate
node v â V can be described in the following way: With
passage of time, more and more of vâs neighbors become active and this may cause v to become active at some time step.
The activation of v in turn may trigger activation of some
of vâs inactive neighbors. In the ICM model there exists a
set of nodes V 0 â V that are active (seed nodes) initially
and the rest of the nodes are inactive. Influence propagation unfolds in discrete steps following a randomized process.
When a node v first becomes active in time step d, it has
a single chance to activate each of its inactive neighbors w
with probability pv,w at time step d + 1. If v succeeds, w
become active at d + 1. However, if v fails, it doesnât get
another chance to turn w active. The process of conversion
of nodes from the inactive to the active state continues, till
no further activation is possible. Since v influences w with
probability pv,w , the v â w edge is considered active with
probability pv,w . The set of active edges is denoted by Ea .

3.2

Generalized ICM for Adversarial Scenario

The ICM can be adapted to handle adversarial scenario by
allowing the nodes to be in one of the following three states
- (i) active by adopting innovation A, (ii) active by adopting
innovation B, and (iii) inactive. We use the notation IA and
IB to indicate the initial adopters (seed nodes) of technologies A and B respectively. The nodes in the set V â(IA âªIB )
are the nodes that are inactive initially. The sets IA and IB
are disjoint, i.e., IA â© IB = â. Just as in ICM, an active
node v may influence each one of its inactive neighbors w
with probability pv,w . However, in an adversarial scenario,
an inactive node w, may be in a situation where one of its
active neighbor v attempts to influence w with innovation A,
whereas another active neighbor u attempts to influence w
with innovation B. In order to deal with this situation, the
authors in [3] proposed two new models - (i) Distance-based
Model, and (ii) Wave-propagation Model. The models specify the probability with which the node w will be influenced,
when its active neighbors attempt to influence w with two
competing technologies. The GICM operates on a random
subgraph of the social network graph G = (V, E), where each
edge is included independently with probability pv,w . The
details of these two models are described in the following
two subsections.

3.3

Distance-based Model

Suppose that du (I, Ea ) denotes the shortest path distance
from the node u to the node set I where I = IA âªIB along the
active edges in the edge set Ea . If u is not connected to any
node of I using only the active edges Ea , then du (I, Ea ) =

uâV

where j = 1 if i = A; else j = 2 and the expectation is over
the set of active edges.

3.4

Wave-propagation Model

In this model, in step d < D all nodes that are at distance
d â 1 from some node in I have adopted technology A or B
and all nodes that are farther than d â 1 from I have not
adopted any technology yet(where the distance is measured
with respect to active edges). Every node at distance d
from I chooses one of its neighbors at distance d â 1 from
I independently at random and adopt the same technology
as its neighbor. For every node u, S denotes the set of
neighbors of u that are closer to I than u; i.e., their distance
from I is du (I, Ea ) â 1. In this model Pi (u|IA , IB , Ea , D),
the probability that node u adopts innovation i â {A, B} in
at most D steps, is computed as follows:
If du (I, Ea ) â¤ D,
P
P (v|I ,I ,E ,D)
Pi (u|IA , IB , Ea , D) = vâS i |S|A B a ;
otherwise, it is zero.
In this model the expected number of nodes which adopt
i â {A, B} will be computed in the following way:
"
#
X
Ïj (IA , IB , D) = E
Pi (u|IA , IB , Ea , D)
uâV

where j = 1 if i = A; else j = 2 and the expectation is over
the set of active edges.

4.

PROBLEM STATEMENT

The WMI problem can be stated informally as follows:
Given a diffusion model and the information that a subset of network nodes IA have already adopted innovation A
marketed by player P1 , what is the fewest number of nodes
should player P2 (marketing innovation B) target so that
by the end of D time steps, the number of nodes that adopt
innovation B will exceed the number of nodes that adopt
innovation A? If Ï1 (IA , IB , D) and Ï2 (IA , IB , D) denote the
expected number of nodes that adopt innovations A and B
respectively within D time steps, the objective of the WMI
problem is to
minimize | IB |
subject to
Ï2 (IA , IB , D) > Ï1 (IA , IB , D)

5.

COMPUTATIONAL COMPLEXITY

In this section, we prove that W M I problem is NP-hard
for both propagation models.

L2

...

...

e1

e2

L3

Ln

...

e3

...

...

y1

en
a
x2

...
s1

s2

s3

y2

xn

sm

x1

...

L1

...

â. Let Î½u (IA , du (I, Ea )) and Î½u (IB , du (I, Ea )) be the number of nodes in IA and IB respectively, at distance du (I, Ea )
from u along edges in Ea . The probability that node u
adopts innovation i â {A, B} when maximum number of
propagation steps is D is denoted by Pi (u|IA , IB , Ea , D) and
is computed in the following way:
if du (I, Ea ) â¤ D,
Î½u (Ii ,du (I,Ea ))
;
Pi (u|IA , IB , Ea , D) = Î½u (IA ,du (I,E
a ))+Î½u (IB ,du (I,Ea ))
otherwise, it is zero.
In this model the expected number of nodes which adopt
i â {A, B} will be computed in the following way:
#
"
X
Ïj (IA , IB , D) = E
Pi (u|IA , IB , Ea , D)

ynr

Figure 1: Graph G = (V, E) of WMI instance in set
cover reduction

5.1

Distance-based Model

Decision version of WMI: Is there a set IB where |IB | â¤ M
and Ï2 (IA , IB , D) > Ï1 (IA , IB , D)?
Theorem 1. WMI is NP-hard for the distance-based model.
Proof: In order to prove that WMI is NP-hard when diffusion is based on distance based model, we reduce the NPcompete Set Cover problem to W M I. The decision version
of the Set Cover problem is defined in the following way: A
ground set of elements S = {e1 , e2 , . . . , en }, a collection of
sets C = {s1 , s2 , . . . , sm } such that si â S and a positive
integer K â¤ |C| are given. The question is whether there
exists a collection Q â C that covers all the elements in S
and |Q| â¤ K.
Given an instance of set cover problem we construct an
instance of W M I. We compute G = (V, E) in the following
way. For every element ei â S we add a node ei and for
every set sj â C we add a node sj to V . We add an edge
(ei , sj ) to E for every ei and sj if ei â sj . Also, we add a
node a and nodes x1 , . . . , xn to V . Then, for every ei we
add edges (a, xi ) and (xi , ei ) to E. Moreover, for every ei
we add a set of r nodes, Li = {li,j |1 â¤ j â¤ r} to V and
we connect them directly to ei . We identify the value of r
later in the proof. Finally, we add n Ã r additional nodes,
y1 , . . . , ynÃr , to V and edges (yt , a), 1 â¤ t â¤ n Ã r (Fig. 1).
We consider that all edges are active; i.e., pu,v = 1 for all
edges in E. We assign D = 4 equal to the diameter of the
graph G, M = K and IA = {a}.
Now, we show that the set cover problem has a solution if
and only if there is a set IB â V âIA such that |IB | â¤ M and
Ï2 (IA , IB , D) > Ï1 (IA , IB , D). First we consider that there
is a collection Q â C that covers S and |Q| â¤ K. Then IB
includes all nodes sj corresponding to the sets in Q. In this
case, all ei will be at distance one from IB and two from IA .
So, all ei and the nodes in Li will adopt IB with probability
one. Moreover, the nodes sj â
/ IB are two hops away from
IB while 3 hops away from IA . Hence, all nodes sj will adopt
IB . Therefore, we have Ï2 (IA , IB , D) = m + n(1 + r); so,
Ï2 (IA , IB , D) > Ï1 (IA , IB , D).
Next, we show that if there is no collection Q of size K
that covers all elements then there is no set IB â V â IA
of size M where Ï2 (IA , IB , D) > Ï1 (IA , IB , D). Considering that set cover does not have a solution, there should be
at least one ei whose distance from IB cannot be one; so,
there is an ei and consequently nodes in Li that choose A
1
and the probability that
with the probability at least K+1
K
they choose B is at most K+1 . Also, at most K nodes from
x1 , . . . , xn can be at distance less than or equal to 1 from
IB . Hence n â K of them will adopt A with probability one.
Therefore, we have

K
Ï2 (IA , IB , D) â¤ m + (n â 1)(1 + r) + K+1
(r + 1) + K and
1
Ï1 (IA , IB , D) â¥ 1 + nr + n â K + K+1 (r + 1). We choose r in
(m+2Kâ2)(K+1)+Kâ1
.
2

our instance large enough such that r >
1
Then we have 1 + nr + n â K + K+1
(r + 1) > m + (n â 1)(1 +
K
r) + K+1 (r + 1) + K; so Ï2 (IA , IB , D) < Ï1 (IA , IB , D).

5.2

Wave Propagation Model

Theorem 2. WMI is NP-hard for the wave propagation
model.
Proof: Similar to Theorem 1, we reduce decision version of
Set Cover problem to decision version of W M I when wave
propagation model is used for diffusion. We construct an
instance of W M I in the same way as in Theorem 1. The
only change that should be made to this instance is the value
of r which will be computed later.
We need to show that the set cover problem has a solution
if and only if there is a set IB â V â IA such that |IB | â¤ M
and Ï2 (IA , IB , D) > Ï1 (IA , IB , D). First we consider that
there is a collection Q â C that covers S and |Q| â¤ K.
Then IB includes all nodes sj corresponding to the sets in Q.
Similar to the proof of Theorem 1 we have Ï2 (IA , IB , D) =
m + n(1 + r); so, Ï2 (IA , IB , D) > Ï1 (IA , IB , D).
Next, we show that if there is no collection Q of size K
that covers all elements then there is no set IB â V â IA
of size M where Ï2 (IA , IB , D) > Ï1 (IA , IB , D). Considering the construction of G and the fact that set cover does
not have a solution , there should be at least one ei whose
distance from IB cannot be one or smaller. Since the node
xi connected to this ei will have probability 1 to accept A
and the maximum number of nodes in first hop neighborhood of ei that are at distance one from IA âª IB is m + 1,
there is an ei and consequently nodes in Li that choose A
1
and the probability that
with the probability at least m+1
m
they choose B is at most m+1 . Also, at most K nodes from
x1 , . . . , xn or y1 , . . . , ynÃr can be at distance less than or
equal to 1 from IB . Hence n(r + 1) â K of them will adopt
A with probability one. Therefore, we have
m
(r + 1) + K and
Ï2 (IA , IB , D) â¤ m + (n â 1)(1 + r) + m+1
1
Ï1 (IA , IB , D) â¥ 1+n(r+1)âK + m+1 (r+1). We choose r in
2

our instance large enough such that r > m2 + K(m + 1) â 23 .
1
Then we have 1 + n(r + 1) â K + m+1
(r + 1) > m + (n â
m
1)(1+r)+ m+1 (r +1)+K; so Ï2 (IA , IB , D) < Ï1 (IA , IB , D).

6.

APPROXIMATION ALGORITHM

Since we proved that finding the optimal solution for W M I
is hard, in this section we propose a greedy algorithm called
GW M I. In this algorithm either of the two propagation
models discussed before can be used as the diffusion process.
Let Ï(IA , IB , D) be (Ï2 (IA , IB , D) â Ï1 (IA , IB , D)). We
define Fi to denote the amount of increase in the value of Ï
when node i is added to IB ; i.e., Fi = Ï(IA , IB âª {i}, D) â
Ï(IA , IB , D). Initially IB is empty. Hence, Ï(IA , IB , D) â¤
0. The algorithm executes through iterations and in each
iteration node i â V â IA with the maximum Fi is selected.
The steps of the algorithm GW M I has been shown in Algorithm 1.

Algorithm 1 GWMI
Input: G = (V, E), IA , D
Output: IB
1: while Ï(IA , IB , D) â¤ 0 do
2: for every node i â V â (IA âª IB ) do
3:
Compute Fi
4: end for
5: Select node j with maximum Fj
6: IB = IB âª {j}
7: end while
8: return IB

In [11], it is mentioned that computing the exact value of
Ï1 (IA , â, D) efficiently is an open question. Similarly, there
is no known way to compute Ï1 (IA , IB , D), Ï2 (IA , IB , D)
in both propagation models efficiently. However, by sampling the active sets we can get a close approximation with
high probability. Given IA , IB and a set of active edges Ea ,
computation of Ï1 and Ï2 in both propagation models has
O(n3 ) time complexity since it needs computation of single all-pairs shortest paths. Given IA , IB and input graph
G, using sampling, we can then approximate Ï1 and Ï2 to
within (1+Î³) for any Î³ > 0 where the running time depends
on 1/Î³ [3].

6.1

Upper Bound Computation

Theorem 3. GWMI has a log n approximation ratio.
t
be the set of Bâs initial adopters selected by
Proof. Let IB
0
, D) =
GW M I at step t. Initially, IB is empty and Ï(IA , IB
âÏ1 (IA , â, D). In every iteration t, the nodes in the optiopt
tâ1
âª
mal set of Bâs initial adopters, IB
, will make Ï(IA , IB
opt
opt
IB , D) positive. We denote the size of IB by OP T and
the size of the solution of GW M I by H. Therefore, There
tâ1
will be at least one node in V â {IA âª IB
} that increases
tâ1
, D) at least by
Ï(IA , IB

tâ1
|Ï(IA ,IB
,d)|
.
OP T

Let, vt be the node

selected by GW M I at iteration t. Then, Fvt â¥
Therefore, for t < H we have
t
tâ1
, D)| â
|Ï(IA , IB
, D)| â¤ |Ï(IA , IB

0
â¤ |Ï(IA , IB
, D)|(1 â

tâ1
|Ï(IA ,IB
,D)|
.
OP T

tâ1
|Ï(IA , IB
, D)|
OP T

1 t
)
OP T

0
, D)| = Ï1 (IA , â, D) â¤ n. Hence
Also, we know that |Ï(IA , IB
we have
ât
1 t
t
|Ï(IA , IB
, D)| â¤ n(1 â
) â¤ ne OP T .
OP T
Since adding a node to IB will increase Ï(IA , IB , D) at least
t
by one, we need to find the smallest t that |Ï(IA , IB
, D)| < 1.
Then adding at most one more node will make Ï(IA , IB , D)
positive. Therefore, H â¤ 1 + OP T ln n. We note that this
proof holds for both propagation models.

6.2

Lower Bound Computation

We now give a construction giving the lower bound for
GWMI when distance-based propagation model is used. Let
X and Y be disjoint sets of n2 vertices and G(n, 3/4) be the
ErdoÌs-Renyi random graph on X âª Y with p = 3/4.
We take two new vertices u and v, connect u to all vertices
of X and v to all vertices of Y . Now, we add a disjoint star
S with n + 2 leaves and connect the center of the star to u
and v. This yields our graph G (Fig. 2).

larly v) is chosen, then increase is at most
G(n, 3/4)
X

Y

n/2

n/2

u

1
1
|X 0 | +
|Y 0 |
(1)
k+1
(k + 1)(k + 2)
1
n
1
n
=
(1/4)k +
(1/4)k + O(n3/4 ).
k+1
2
(k + 1)(k + 2)
2

1+

v
Red set

Figure 2: Construction of G.

We consider that the center of the star is the only initial
adopter of A (red node), and pu,v is uniform and it is 1 for
all the edges of G and D = 3. An optimal set of initial
adopters of B (initial blue nodes) includes u, v and any of
the leaves of S. We claim that the greedy algorithm GW M I
will select â¦(log n) vertices with high probability, assuming
n is large enough.
In order to prove this we first state a technical lemma
giving a condition that G satisfies with high probability. Let
S â X âª Y . We say S is fair if

On the other hand, if a vertex x in X 0 âª Y 0 is chosen, the
increase is at least
1
1
|â(x) â© X 0 | +
|â(x) â© Y 0 |
(2)
k+1
k+1
1
|X 0 âª Y 0 \ â(X)|
+
(k + 1)(k + 2)
1
3
n
1
n
=2Â·
Â· (1/4)k +
(1/4)k+1 + O(n3/4 );
k+1 4
2
(k + 1)(k + 2)
2
therefore, (2) - (1) is positive and hence the vertex in X 0 âª
Y 0 will be chosen as desired. We note that this construction
is for sufficiently large n and (1/4)k n >> n3/4 .
1
Proof of Lemma 4. Let S â XâªY , with |S| < 100
ln n.
Then
n
E[|X \ âS|] = (1/4)|S| (|X| â |X â© S|) = (1/4)|S| + O(ln n).
2

Let XS = |X \ âS|. Chernoff bounds imply that
1. |X \ â(S)| = (1/4)|S| n2 + O(n3/4 ) and |Y \ â(S)| =
(1/4)|S| n2 + O(n3/4 ).
where â(S) is the set of one hop neighbors of vertices in
S.
We claim the following lemma, whose proof we defer:
Lemma 4. With probability 1 â o(1) every set S â X âª Y
1
ln(n) is fair. Furthermore, the induced graph
with |S| < 100
on X âª Y has diameter 2, every vertex in Y is at distance at
most 2 from u and every vertex in X is at distance at most
2 from v.
Assuming Lemma 4 we prove the lower bound. In particular we prove the following: The greedy algorithm selects at
1
least 100
ln n vertices from X âª Y . We proceed by induction.
At the first step, the greedy algorithm has to choose between
a vertex in X âª Y , one of u or v, or one of the vertices in the
star. Selecting a vertex in the star will cause the number of
blue vertices to increase by one and red vertices to decrease,
a net change of two. Selecting u (or resp. v) will increase
blue (and decrease red) by a total of 1 + n2 + n4 ; since every
vertex in X will be at distance 1 from a blue vertex and
every vertex in Y will be at distance 2 from both u and the
red vertex if u is selected. On the other hand, by fairness, if
a vertex x in X âªY is selected; the increase in blue is at least
3n
+ n8 + O(n3/4 ); since 3n
+ O(n3/4 ) vertices are at distance
4
4
n
1 from x and the other 4 + O(n3/4 ) are at distance 2 from
both x and the red vertex. Therefore the greedy algorithm
will select from X âª Y at the first time.
Now suppose that the greedy algorithm has selected from
1
X âªY a total of k < 100
ln n times. Let B denote the selected
0
set, and X = X \ â(B) and Y 0 = Y \ â(B). Every vertex in
X 0 âª Y 0 is at distance two from all k blue vertices, and hence
k
they are currently blue with probability k+1
. Furthermore
0
0
by fairness X and Y are both of size (1/4)k n2 + O(n3/4 ).
Again, the greedy algorithm must choose: If u (or simi-

P(|XS âE[XS ]| > n3/4 ) â¤ exp(ââ¦(

n3/2
)) â¤ exp(ââ¦(n1/2 )).
E[XS ]

Bounds for |Y \âS| follow similarly. On the other hand there
are at most
1 ln n
!
100
X
n
1
â¤
ln(n) Â· nln n ,
i
100
i=1
sets S. Thus union bounds imply every set is fair with probability 1 â exp(ââ¦(n1/2 )).
Note that the expected number of common neighbors between x and y in X 0 âª Y 0 is 9n
, and Chernoff bounds plus
16
union bounds imply every pair x and y is of distance 2 (and
in fact has (1 â o(1)) 9n
common neighbors). Likewise, u
16
expected neighbors and Cherand a vertex in Y have 3n
8
noff bounds imply that every pair has (1 + o(1)) 3n
common
8
neighbors. Likewise, for v and vertices in X. A union bound
over all events completes the proof.

7.

SIMULATION

In this section we evaluate the performance of our approximation algorithm, GW M I, on a real network data set.
It has been suggested in [15] that the co-authorship graphs
are representative of typical social networks. As such, we
use the real collaboration network data set of the scientists posting preprints on the high-energy theory archive
at www.arxiv.org, 1995-1999 [14]. This network has 8361
nodes (authors) and 15751 edges. The largest connected
component has 5835 number of nodes (authors) and maximum distance between the nodes in a connected component
is 19.
Our experiments were conducted on a high performance
computer which is a 5K processor Dell Linux Cluster. The
program is parallelized with OpenMP, optimized with Intel
compiler and was executed on an 8 core compute node. The
cores in the node have equal access to a common pool of
shared memory. Each node is comprised of 2.66/2.83 GHz

Number of Initial Adopters of B

200
160

120
80

Degree-Closeness
Degree-Degree
Degree-SPIM
Degree-GWMI

40

0
0

20

40
60
80
100
Number of Initial Adopters of A

120

Figure 3: Number of initial adopters of B for different values of |IA |

350
300
Coverage of A

processors, 8MB cache, 16GB memory and 8 cores. Since
our experiments required execution of the algorithm on a
large number of instantiation of a social network (the graphs
were different as their set of active edges were different), we
used OpenMP for parallelization of the graph instances for
the simulation with one data set.
In the first set of experiments we evaluate the performance of GWMI algorithm against the results obtained from
the heuristics based on node degree and closeness centrality.
These heuristics are most often used in social networks to
identify most influential nodes [11]. We also compare performance of GWMI with the greedy algorithm proposed in
[3] for selection of seed nodes for the second player P2 . In
our model the first player P1 is trying to market product
A and the second player P2 is trying to market product B.
Since WMI problem is NP-hard and the input data set is
large, computation of the optimal solution within a reasonable amount of time is unlikely. It may be noted that there is
no known way of computing the exact value of Ï1 (IA , IB , D)
and Ï2 (IA , IB , D) efficiently [11]. Accordingly, we use sampling of the active edge sets to obtain close approximation
of Ï1 (IA , IB , D), Ï2 (IA , IB , D) with high probability. As in
the experiments reported in papers [11, 3], we assign the
edge probabilities to be 0.1. In all the experiments we use
WPM as the diffusion model.
The node degree based heuristic selects the nodes in the
decreasing order of their degrees and the closeness centrality
based heuristic selects the nodes in the increasing order of
their average distance to other nodes. The distance between
two nodes that are not in the same connected component is
taken to be n, where n is number of nodes in the network.
In the greedy algorithm proposed in [3], in every iteration
the node that increases Ï2 (IA , IB , D) the most is selected.
We refer to this algorithm as Second Player Influence Maximization (SPIM) algorithm. In these experiments, maximum number of propagation steps is taken to be 10, i.e.,
D = 10. In the experiments, the player P1 used node degree
based heuristic to select its k initial adopters. In our experiments, the size of initial adopters of A is varied from 20 to
100. The results of this set of experiments using the WPM
is shown in Fig. 3. The Fig. 3 shows that all five sizes of
the initial adopters of A (20, 40, 60, 80, 100), the GWMI
algorithm required the fewest number of initial adopters of
B necessary to defeat Aâs influence at the end of time step
10. The legend Degree-Degree in Fig. 3 denotes that both
the players are using the node degree based heuristics to select the initial adopters. Similarly,the legend Degree-GWMI
denotes that while P1 is using the node degree based heuristics to select the initial adopters, P2 is using the GWMI
algorithm to do the same.
The Figs. 4 and 5 show the coverage (i.e., the number of
nodes influenced at the end of 10 time steps) for players P1
and P2 respectively. Although the GWMI algorithm does
not make an effort to minimize the coverage of P1 , it may
be observed from the Fig. 4, the coverage of P1 is less if
P2 uses GWMI instead of SPIM. Thus P2 is better off using
GWMI instead of SPIM, if in addition to be able to defeat
P1 with least investment (i.e., initial adopters), P2 wants
to have a smaller market share for P1 . The Fig. 5 shows
the coverage of P2 at the end of ten time steps. It may be
observed from the Fig. 5, that at all five data points the coverage for P2 is highest when she uses the SPIM algorithm.
This is not surprising as the stated goal of SPIM is to maxi-

250

Degree-Degree
Degree-Closeness
Degree-GWMI
Degree-SPIM

200
150
100
20

40
60
Number of Initial Adopters of A

80

Figure 4: Expected number of nodes adopting A
after 10 propagation steps

mize P2 âs coverage (influence). However, this figure may be
somewhat misleading because it does not provide the information pertaining to the number of initial adopters required
by the SPIM algorithm to achieve the higher coverage. By
its stated objective, the number of initial adopters required
by GWMI to defeat P1 cannot be higher than the the number of initial adopters required by SPIM. Once this is factored in, and we compute the coverage per initial adopter,
we find that the coverage per initial adopter of the SPIM algorithm is very close to that of the GWMI algorithm. This
is shown in Fig. 6.
From Fig. 3 it is clear that the node degree and centrality
based heuristics and the SPIM algorithm require a larger
number of initial adopters of B to beat A than is needed
by the GW M I algorithm. While this is a negative aspect
of SPIM (cost), it also has a positive aspect in the sense
that at the end of ten time steps, it also secures a larger
coverage for B (benefit). We compute the additional benefit
provided by the additional initial adopters. Let IB(X) be
the smallest set of initial adopters of B that is required by
algorithm X to defeat A and Ï2(X) be the expected number
of nodes that adopt B after D propagation steps. Here X
can be node-degree or centrality based heuristic or the SPIM
algorithm. In the case, (Ï2(X) â Ï2(GW M I) ) indicates the
additional benefit and (|IB(X) | â |IB(GW M I) |) indicates the
additional cost. In this case, (Ï2(X) â Ï2(GW M I) )/(|IB(X) | â
|IB(GW M I) |) indicates the average market share gain of B
with each additional initial adopter when using algorithm

0.4

350
Extended Benefit of B per
Additional Initial Adopter

Coverage of B

300

Degree-Degree
Degree-Closeness
Degree-GWMI
Degree-SPIM

250

200
150

Degree-Degree
Degree-Closeness
Degree-SPIM

0.3
0.2

0.1
0
20

100
20

40
60
Number of Initial Adopters of A

Coverage of B per Initial Adopter of B

9
Degree-Degree
Degree-Closeness
Degree-GWMI
Degree-SPIM

7
6

but also (Ï1(GW M I) â Ï1(X) ). It introduces a notion of extended benefit by combining these two factors in the following
way: (Ï2(X) â Ï2(GW M I) ) â (Ï1(GW M I) â Ï1(X) ). With this
notion of extended benefit,
((Ï2(X) â Ï2(GW M I) ) + (Ï1(GW M I) â Ï1(X) ))
|IB(X) | â |IB(GW M I) |

4
3

2
1
0
40
60
Number of Initial Adopters of A

80

Figure 6: Expected number of nodes adopting B per
initial adopter of B after 10 propagation steps

X. The Fig. 7 depicts the results for the heuristics and
SPIM. The negative gains are not shown. It may be observed
from Fig. 7 that the average market share gain of B with
each additional initial adopter diminishes with increase of
the number of initial adopters of A, when it uses the SPIM
algorithm.
While the stated objective of P2 is to have a larger market
share than P1 with the fewest number of initial adopters, it
may also have two other unstated objectives - (i) to have a
large Ï2(X) and (ii) a small Ï1(X) for all X (Ï1(X) be the
expected number of nodes that adopt A after D time steps).
Therefore while considering the benefit of the additional initial adopters, we can consider not only (Ï2(X) â Ï2(GW M I) )

Average Increase in Market Share
of B per additional initial adopter

Figure 8: Extended benefit that B can capture per
additional initial adopter with respect to GW M I

5

20

12
Degree-Degree
10

Degree-Closeness
Degree-SPIM

8

6

indicates the average market share gain of B with each additional initial adopter when using algorithm X. The Fig.
8 depicts the results for the heuristics and SPIM. It may be
observed from Fig. 7 that when extended benefit is considered, the average market share gain of B with each additional initial adopter diminishes even more drastically with
increase of the number of initial adopters of A, when it uses
the SPIM algorithm. Moreover, the gain of each additional
initial adopter is smaller than 1 and implies that the additional adopter is not worth its cost.
In the second set of experiments we investigate different
strategies for selection of initial adopters of A when P2 uses
GW M I. The strategies that we consider for selection of
initial adopters of A includes the greedy algorithm proposed
in [11] and heuristics based on node degree and closeness
centrality. In these experiments WPM is used as diffusion
model and D = 10.
Fig. 9 depicts the results of these experiments. We observe that the closeness-centrality based heuristic performs
poorly in comparison to other two algorithms. This is true
because the number of initial adopters of B that it needs
to defeat Aâs overall influence (coverage) is much smaller
than the size of initial adopters of A. More specifically, for
closeness-centrality based heuristic, for |IA | values greater
than 60, the number of initial adopters of B is less than
50% of |IA |. This set of results show that if the influence
maximization algorithm (IM) proposed in [11] is used for
the selection of IA , it forces P2 to select a large set for IB in
order to be able to defeat P1 within D time steps.

8.

4

2
0
20

40
60
Number of Initial Adopters of A

80

80

Figure 5: Expected number of nodes adopting B
after 10 propagation steps

8

40
60
Number of Initial Adopters of A

80

Figure 7: Average market share increase that innovation B can capture per additional initial adopter
with respect to GW M I

CONCLUSION

In this paper we have introduced a new influence propagation problem in an adversarial setting where the goal
of the second player is to defeat the first within D time
steps and least cost, measured in terms of the number of
seed nodes. Considering two different influence propagation
models, we provided the NP-Hardness proof for the problem
and an approximation algorithm with a tight performance
bound. In addition, we evaluated the performance of the
approximation algorithm with collaboration network data.

Number of Initial Adopters of B

120

Degree-GWMI
IM-GWMI
Closeness-GWMI

100
80
60
40
20
0
0

20

40
60
80
Number of Initial adopters of A

100

120

Figure 9: Size of initial adopters of B for different
values of |IA |
We can envisage at least two new directions of research with
this problem. In the first direction, P2 is not aware of P1 âs
choice. In the second direction, back and forth transition of
the nodes between two competing products is allowed.

9.

ACKNOWLEDGMENTS

The research was supported in part by a grant to the Center for the Study of Religion and Conflict at Arizona State
University (N00014-09-1-0815). The award was funded through
the Office of the Secretary of Defense Minerva program, and
managed out of the Office of Naval Research. The content
is solely the responsibility of the authors and does not necessarily represent the views of the Office of Naval Research.
In addition, it was also supported in part by the DTRA
grant HDTRA1-09-1-0032 and the AFOSR grant FA955009-1-0120.

10.

REFERENCES

[1] S. Bhagat, A. Goyal, and L. V. Lakshmanan.
Maximizing product adoption in social networks. In
Proceedings of the fifth ACM international conference
on Web search and data mining, WSDM â12, pages
603â612, 2012.
[2] S. Bharathi, D. Kempe, and M. Salek. Competitive
influence maximization in social networks. In
Proceedings of the 3rd international conference on
Internet and network economics, WINEâ07, pages
306â311, 2007.
[3] T. Carnes, C. Nagarajan, S. M. Wild, and A. van
Zuylen. Maximizing influence in a competitive social
network: a followerâs perspective. In Proceedings of the
ninth international conference on Electronic
commerce, ICEC â07, pages 351â360, 2007.
[4] W. Chen, Y. Wang, and S. Yang. Efficient influence
maximization in social networks. In Proceedings of the
15th ACM SIGKDD International Conference on
Knowledge Discovery and Data Mining, KDD â09,
pages 199â208, 2009.
[5] K. Dave, R. Bhatt, and V. Varma. Modelling action
cascades in social networks. 2011.

[6] P. Domingos and M. Richardson. Mining the network
value of customers. In Proceedings of the seventh ACM
SIGKDD international conference on Knowledge
discovery and data mining, KDD â01, pages 57â66,
2001.
[7] A. Goyal, F. Bonchi, L. V. S. Lakshmanan, and
S. Venkatasubramanian. Approximation analysis of
influence spread in social networks.
arXiv:1008.2005v4, 2011.
[8] H. Habiba, Y. Yu, T. Y. Berger-Wolf, and J. Saia.
Finding spread blockers in dynamic networks. In
Proceedings of the Second international conference on
Advances in social network mining and analysis,
SNAKDDâ08, pages 55â76, 2010.
[9] G. Istrate, M. V. Marathe, and S. S. Ravi. Adversarial
models in evolutionary game dynamics. In Proceedings
of the twelfth annual ACM-SIAM symposium on
Discrete algorithms, SODA â01, pages 719â720, 2001.
[10] Q. Jiang, G. Song, C. Gao, Y. Wang, W. Si, and
K. Xie. Simulated annealing based influence
maximization in social networks. 2011.
[11] D. Kempe, J. Kleinberg, and E. Tardos. Maximizing
the spread of influence through a social network. In
Proceedings of the ninth ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD â03, pages 137â146, 2003.
[12] H. Li, S. S. Bhowmick, and A. Sun. Casino: towards
conformity-aware social influence analysis in online
social networks. In Proceedings of the 20th ACM
international conference on Information and
knowledge management, CIKM â11, pages 1007â1012,
2011.
[13] M. Mathioudakis, F. Bonchi, C. Castillo, A. Gionis,
and A. Ukkonen. Sparsification of influence networks.
In Proceedings of the 17th ACM SIGKDD
international conference on Knowledge discovery and
data mining, KDD â11, pages 529â537, 2011.
[14] M. Newman.
http://networkdata.ics.uci.edu/data/hep-th/.
[15] M. E. J. Newman. The structure of scientific
collaboration networks. Proceedings of the National
Academy of Sciences of the United States of America,
98(2):404â409, 2001.
[16] M. A. Nowak, C. E. Tarnita, and T. Antal.
Evolutionary dynamics in structured populations.
Philosophical Transactions of the Royal Society B:
Biological Sciences, 365(1537):19â30, 2010.
[17] Y. Wang, G. Cong, G. Song, and K. Xie.
Community-based greedy algorithm for mining top-k
influential nodes in mobile social networks. In
Proceedings of the 16th ACM SIGKDD international
conference on Knowledge discovery and data mining,
KDD â10, pages 1039â1048, 2010.
[18] S. Wasserman and K. Faust. Social Network Analysis:
Methods and Applications. Number 8 in Structural
analysis in the social sciences. Cambridge University
Press, 1 edition, 1994.

