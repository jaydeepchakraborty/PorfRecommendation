ON HARDWARE SUPPORT FOR INTERVAL COMPUTATIONS
AND FOR SOFT COMPUTING: THEOREMS
Hung T. Nguyen, Vladik Kreinovich, Member, IEEE, Vyacheslav Nesterov,
and Mutsumi Nakamura

Abstract. This paper provides a rationale for providing hardware supported functions of
more than two variables for processing incomplete knowledge and fuzzy knowledge. The
result is in contrast to Kolmogorov's theorem in numerical (non-fuzzy) case.

1. INTRODUCTION
In this paper, we show that for interval computations and for processing fuzzy data (i.e.,
for soft computing), it is desirable to have hardware supported operations with more than
two operands.
Before we formulate the problem and go into technical details, we would like to emphasize the importance of this problem by briey describing in Subsection 1.1 the practical
origin (and practical necessity) of interval computations and soft computing.

1.1. Estimating accuracy of the results of data processing:
crisp and fuzzy cases
Data processing: why? To make decisions, we must have some information about the

values of the physical quantities. For example, in order to decide whether to approve
the scheduled launch of a Space Shuttle, we must know the characteristics of the Shuttle
(to make sure that all its systems work), and weather conditions around the launch site
during the launch time. Some of these characteristics can be measured directly: e.g.,
characteristics of the Shuttle's electric systems can be measured by testers. Some of the
desired characteristics can be estimated by experts: e.g., some experts meteorologists can
provide us with reasonably good short-time weather predictions for a given area.
Hung T. Nguyen is with the Department of Mathematical Sciences, New Mexico
State University, Las Cruces, NM 88003, USA, email hunguyen@nmsu.edu
Vladik Kreinovich is with the Department of Computer Science Department, University of Texas at El Paso, El Paso, TX 79968, USA, email vladik@cs.utep.edu
Vyacheslav Nesterov is with the Institute of New Technologies, P. O. Box 52, St.
Petersburg 256, 195256 Russia, email nest@into.nit.spb.su
Mutsumi Nakamura is with the Department of Mathematics, University of Texas at
Austin, Austin, TX 78712, USA, email mutsumin@math.utexas.edu
1

In some cases, however, it is very dicult (or even impossible) to measure the characteristic y that we are interested in, and there are no experts who can predict the values of
these characteristics. For example, it is very dicult to directly measure the temperature
inside the jet chamber (because this temperature is extremely high); if we are planning a
mission to a new planet, it is simply impossible to directly measure the characteristics of
the new environment before the mission actually gets there, and often, no expert can help.
If we are interested in the value of such a characteristic y, and we cannot estimate y
directly (either by measurement, or by using experts), then a natural idea is to estimate y
indirectly, i.e.:
 to estimate some other (easier to estimate) quantities x1 ; :::; xn that are related to y,
and then
 to compute the estimate y~ for y based on the estimates x~1 ; ::; x~n for x1 ; :::; xn.
This process is called data processing, and this is what super-computers are doing most of
the time: from measured characteristics x~i of observed collisions in the accelerators, they
reconstruct the (directly unobservable) properties of the elementary particles; from the
results x~i of geophysical measurements, computers predict the amount y of oil (or other
mineral) in a given area, etc.
In this paper, we will assume that we already know what characteristics xi to measure,
and how to reconstruct y from xi . In other words, we will assume that we know an algorithm
f (x1; :::; xn) that transforms the values of xi into an estimate for y. This algorithm is
not necessarily simple: e.g., in geophysics, it may involve solving a complicated non-linear
integral equation (\inverse problem"); in elementary particle physics, it may involve solving
a system of non-linear operator quantum equations, etc.

The result of data processing is never absolutely accurate. The assumption that

we know the algorithm f means that if we know the exact values of the variables x1 ; :::; xn,
we can then apply the algorithm f and compute the exact value of y. In reality, however, we only know some estimates x~i for xi that are obtained either by measurements
or by an expert estimation. Measurements are never 100% precise; expert estimates are
not absolutely precise either. As a result, the available values x~i dier from the actual
(unknown) values xi ; therefore, the estimate y~ = f (~x1; :::; x~n) that we obtain by processing
the available data may dier from the desired value y = f (x1; :::; xn).

In real-life applications, we must know the accuracy of the result of data processing. For practical purposes, it is important to know how dierent the result y~ of data
processing can be from the actual value y.

2

For example, if we want to decide whether a particular well is worth drilling, and the
estimate for the amount of oil is y~ = 100 mln. tons, then before we start drilling, we
would like to know whether this is, say, 100  1, in which case, we should probably start
drilling, or it is 100  100 (maybe 100, maybe 0, maybe 200), in which case we would rather
undertake further (and more accurate) measurements.
In this paper, we consider the problem of nding this accuracy.

Simplest case: measurements only. Before we start analyzing the general case, where

both measurement results and expert estimates are present, let us consider the simplest
case, where there is no expert knowledge, and all the data come from the measurements.
In traditional measurement theory (see, e.g., [8,47,35]), it is usually assumed that we know
the probabilities of dierent values of a measurement error. These values can be obtained
if we calibrate the measuring instrument, i.e.:
 we use the calibrated instrument in conjunction with a much more accurate one (called
a standard) in several measurements;
 for each measurement, we compute a dierence e(k) = x~(k) ? x(k) between the results
of these two instruments, and use this dierence as an estimate of the error of the
measurement performed by the calibrated instrument;
 reconstruct the error probability distribution from the recorded sample errors e(1) , ...,
e(N ) .
For the situations in which we know the probabilities of dierent errors, there exist numerous methods that compute statistical characteristics of the resulting error.
In many real-life situations, however, the values of the probabilities are not known:
 in advanced measurements (in radio-astronomy, in elementary particle physics, etc),
we are using measuring instruments that have the highest accuracy possible, so, there
is simply no \more accurate" measuring instrument that we can for calibrating;
 in manufacturing applications, we can potentially calibrate all the sensors that we
use, but this calibration would cost much more than the sensors themselves, so it is
usually not done.
In these situations, the manufacturer of the measuring instrument provides us with the
guaranteed accuracy , i.e., with a guaranteed upper bound of the error x = x~ ? x (e.g.,
\error cannot exceed 0.1"). If our measurement results is x~, then the possible values of
x = x~ ? x form an interval [~x ? ; x~ + ]. Since we are dealing with intervals, the entire
area is called interval computations (see, e.g., [29,11,10,18,14]).
3

The set of possible values of an error is not necessarily an interval. For example,
suppose that we are measuring the current inside the computer, and we know that the
error cannot exceed a certain value . If we know nothing else about the error, then we
may conclude that the error belongs to the interval [?; ]. However, we may know that
the error is caused by the inuence of a nearby magnetic memory element, which can be
in two possible states (corresponding to \0" and \1"). In this case, the error is either
positive, or negative (depending on the state), but never 0; actually, the error can never
be smaller than some value . In this case, the set X of possible values of the error is
not an interval, but a union of two intervals: X = [?; ?] [ [; ]. There can be more
complicated cases, in which the error can be described by more complicated (crisp) sets X
of possible values.
In this case, our problem takes the following form:
We know:
 an algorithm f that transform n real numbers x1; :::; xn into a real number y =
f (x1; :::; xn);
 sets X1  R, ..., Xn  R that contain the actual values of xi ;
We must compute: The set Y of possible values of y:

Y = ff (x1; :::; xn) j x1 2 X1; :::; xn 2 Xng

(1:1)

This set Y is usually denoted by f (X1; :::; Xn).
A measuring instrument can measure several dierent quantities x1; :::; xn at a time.
In this case, in addition to the information about the possible errors of each measurement,
the manufacturer can guarantee that certain combinations of errors are impossible: e.g.,
it can happen that x1 attains its largest possible value , and it can happen that x2
attains its largest possible value , but they can never attain these extreme values at the
same time, because of the restriction that x21 + x22  2 (this situation happens, e.g.,
if we measure geographical coordinates of a point). Such an information can be described
by a set X  Rn of all the tuples that the manufacturer believes can be possible values of
errors (x1; :::; xn). In this case, the problem takes the following form:

4

We know:
 an algorithm f that transform n real numbers x1; :::; xn into a real number y =
f (x1; :::; xn);
 a set X  Rn that contains the actual value of ~x = (x1; ::; xn);
We must compute: The set Y of possible values of y:

Y = ff (x1; :::; xn) j (x1; :::; xn) 2 X g

(1:2)

This set Y is usually denoted by f (X ).
The previous formulation is a particular case of this one if we take X = X1  :::  Xn.

General case: processing data that includes expert knowledge as well. In many

cases, in addition to measurements results, we have expert's knowledge about the variables
x1 ; :::; xn. For example, in order to make a decision on what doze of radiation to assign to
a patient, we must know not only the characteristics that are measurable (like blood count,
tumor size, etc), but the characteristics that can only be estimated by an expert (e.g., the
granularity of a tumor can be \small" or \medium"). We want to process this informal
information automatically, therefore, we must be able to represent it in the computer.
A word like \medium-size" does not describe one particular value; it can correspond to
several dierent values; some of them are more reasonably described as \medium-size",
some values can be in principle described as such, but only occasionally. To describe the
meaning of each word, we ascribe to every real number x a value (x) 2 [0; 1] that describe
to what extent it is reasonable to assume that x is, say, medium-size (1 means that it is
absolutely reasonable, 0 that it is not reasonable at all). The resulting function is called a
membership function, or a fuzzy set.
In some cases, the expert's informal statement describes not one variable, but several
of them. For example, if we say that a point with coordinates x1 ; x2 is close to 0, this
means that both x1 and x2 are close to 0. Such knowledge can be represented by a function
from R2 to [0,1]. This membership function is called a fuzzy subset of R2. If we have such
information about xi , and we want to estimate y, we get the following problem:
We know:
 a function f of n variables;
 a fuzzy set X  Rn that describes our knowledge about ~x = (x1 ; :::; xn).
We want: to describe the resulting knowledge about y in terms of a fuzzy set Y .
This problem was formulated, e.g., in [1,46,39], and it has appeared in many practical
cases, including:
5

 testing jet engines [22,20,21];
 seismic analysis [3,4];
 image processing [19,20,21],
etc.

The desired description of the set Y is known as extension principle. This principle was
proposed by Zadeh in his pioneer paper [53] (see also [54] and [5]), and it is based on the
following idea:
A real number y~ is a reasonable value of y if and only if
there exist values x~1; :::x~n for which
x~1 is a possible values of x1 , x~2 is a possible value of x2 , ..., and f (~x1; :::; x~n) = y~.
If we follow the traditional fuzzy set theory and interpret \and" as min, and \there
exists" as sup, then we arrive at the following formula:

Y (y) = sup (min(X (~x); f (~x; y));
~x2Rn

where f is a characteristic function of the graph of the function f (i.e., (~x; y) = 1 if
f (~x) = y, and 0 otherwise). Due to this formula, values ~x for which f (~x) 6= y, do not
inuence on Y (y). Therefore, this formula can be rewritten as follows:

Y (y) = sup X (~x):

(1:3)

~x:f (~x)=y

This formula is called the extension principle, and the resulting fuzzy set Y is denoted by
f (X ).
If instead of X , we have n separate fuzzy sets X1 ; :::; Xn that describe our knowledge
about x1 ; :::; xn, then similar arguments lead to a formula

Y (y) = sup min(X1 (x1); :::; X (xn)):
~x:f (~x)=y

n

(1:4)

The resulting fuzzy set Y is denoted by f (X1; :::; Xn).
Comments.
1. In particular, if we take elementary arithmetic operations (+; ; ?, etc) as f , we get
the denition of arithmetic operations with fuzzy operands X1; :::; Xn.

2. The statement that we have just formalized contains two logical terms: \and" and
\there exists". So, to formalize it, we must formalize what these two logical terms mean.
6

\There exists" can be viewed as an innite \or": namely, \there exist x~1 ; :::; x~n with
a certain property" means that this property is either true for, say, (0:0; :::; 0:0), or for
(1:1; 0:2; :::; 2:3), or for any of innitely many tuples of n real numbers. Therefore, to get
an interpretation of \there exists", we must choose an appropriate fuzzy interpretation
f_(a; b) of _ and apply the resulting _?operation f_ innitely many times (i.e., apply it
to N tuples and then take N ! 1).
In fuzzy logic, many dierent _?operations have been proposed (e.g., f_(a; b) =
a + b ? a  b); these operations are also called t?conorms. A usual (and natural) restriction
on a t?conorm f_ comes from the fact that for every two statement A and B , our degree
of belief in A _ B must be at least as big as the degree of belief in each of these statements
A and B . If we denote the degree of belief in A by t(A), then this condition turns into
f_(t(A); t(B ))  t(A) and f_ (t(A); t(B ))  t(B ). In other words, for every a and b,
f_(a; b)  max(a; b).
If we choose one of the known t?conorms for which f_(a; b) > max(a; b) (e.g., if we
choose f_ (a; b) = a + b ? ab), then, the more times we apply f_ , the larger the resulting
degree of belief, and in the limit, we get 1. For example, for f_ (a; b) = a + b ? ab =
1 ? (1 ? a)  (1 ? b), disjunction of N formulas with the same degree of belief a 2 (0; 1) leads
to f_(a; : : :; a) = 1 ? (1 ? a)N , and this expression ! 1 as N ! 1. (Here, f_(a; b; :::; c)
stands for f_(:::(f_(a; b); :::); c), i.e., it means that we apply the _?operation N times.)
A similar result can be proven for any (strict or non-strict) Archimedean t?conorm (see,
e.g., [16,34]). Hence, for such operations, we will have Y (y) = 1 for all y, which makes
no sense.
Therefore, when we dene operations with fuzzy operands, the only meaningful interpretation of \there exists" is through the _?operation f_ (a; b) = max(a; b), for which for
every property A(x), the degree of belief in \there exists x such that A(x)" is equal to the
\maximum" (or, to use the precise mathematical term, supremum) of all the degrees of
belief t(A(x)) for all x.
As far as an &?operation is concerned, we can use an arbitrary function f& : [0; 1] 
[0; 1] ! [0; 1] that extends a usual & operation dened for binary values (with 0 as false
as 1 as true), i.e., an arbitrary function f& for which f& (0; 0) = f& (0; 1) = f& (1; 0) = 0
and f& (1; 1) = 1.
For such interpretation of \and" and \there exists", formula (2) takes the following
form:
Y (y) = sup f& (X1 (x1); :::; X (xn ));
(1:4a)
~x:f (~x)=y

n

7

where f& (a; b; :::; c) stands for f& (:::(f&(a; b); :::); c) (i.e., it means that we apply the
&?operation several times).
Our results will be true for an arbitrary choice of an &?operation.

1.2. How is the problem of
estimating accuracy of the results of data processing
solved now?
In general, the problem is computationally intractable even for crisp sets (even
for intervals). It has been proven that even for the case when the sets Xi are crisp (and
are intervals), and the algorithm f is actually a polynomial, the problem of computing the
set Y exactly is computationally intractable (NP-hard) [9].

This result does not mean, of course, that the problem of computing Y is not practically solvable. The sets Xi describe the inaccuracy of the measuring devices, or the
inaccuracy of an expert. These inaccuracies are never known precisely, therefore, it would
be quite sucient to have an approximate description of Y . Several methods are known
for that:

Case when estimates are pretty accurate: linearization technique. If the esti-

mates x~i for xi are pretty accurate, then we can neglect the terms that are quadratic
(or of higher order) in xi = x~i ? xi , and thus, for given estimates x~i , describe
y = f (x1; :::; xn) = f (~x1 ? x1 ; :::; x~n ? xn ) by the following approximate formula:
y  flin = y~ ? f;1x1 ? ::: ? f;nxn ; where by f;i , we denoted the partial derivative of f
w.r.t. xi :
@f (~x ; :::; x~ ):
f;i = @x
1
n
i

In this case, for interval Xi = [~xi ? i ; x~i + i ], we have Y  Xlin = flin (X1; :::; Xn) =
[~y ? ; y~ + ], where  = jf;1j1 + ::: + jf;njn (see, e.g., [35]).

Another case when we can estimate Y is when the function f is monotonic in each
of the variables; in this case, if, e.g., f is monotonically increasing, and Xi = [x?i ; x+i ], we
have Y = [f (x?1 ; :::; x?n ); f (x+1; :::; x+n )].
For these two cases (small errors or monotonic f ), there also exist ecient techniques
for fuzzy data processing [4,51].

Expert estimates are rarely very accurate, so, other methods are needed. Mea-

surements typically lead to accurate estimates of xi , so, if all the estimates come from
8

measurements, we can usually apply linearization techniques. Expert estimates, on the
other hand, are rarely very accurate. So, if we have expert estimates, we usually cannot
neglect the squares of the errors, and therefore, we need other methods for estimating the
error of the result of data processing. For this case, the following idea has been proposed
by R. Moore [27,28] (see also [29,11,10,14]). No matter what high-level language we use
to describe an algorithm f , inside a computer, this algorithm is translated into a sequence
of elementary operations (usually, +, ?, , :, etc).
For example, a function f (x1; x2; x3) = (x1 + x2 )2 + 2  x3 is computed as follows (we will
enumerate all the input and intermediate values by r1 ; r2, ...): rst, we have r1 = x1,
r2 = x2 , and r3 = x3 ; then, we start computing further values:
 we apply + and get r4 = r1 + r2 = x1 + x2 ;
 we apply the square operation and get r5 = r42 (so, r5 = (x1 + x2 )2).
 we take r6 = 2;
 we compute r7 = r6  r3 ;
 nally, we compute r8 = r5 + r7 ; this is the desired value y.
The idea is to repeat the same sequence of operations, but with intervals instead of numbers.
Elementary operations g are usually monotonic, so, we can explicitly compute g(X1; :::; Xn)
for intervals Xi : e.g., for addition g(x1; x2) = x1 + x2 , we have g([x?1 ; x+1]; [x?2 ; x+2]) =
[x?1 + x?2 ; x+1 + x+2 ].
For example, if we start with the intervals R1 = X1 = [0; 1], R2 = X2 = [0; 1], and
R3 = X3 = [1; 2], we get the following sequence of computations:
 we apply + and get R4 = R1 + R2 = [0; 1] + [0; 1] = [0; 2];
 we apply the square operation and get R5 = R42 = [0; 4];
 take R6 = [2; 2];
 compute R7 = R6  R3 = [2; 2]  [1; 2] = [2; 4];
 nally, we compute Y~ = R8 = R5 + R7 = [0; 4] + [2; 4] = [2; 8]; this is the desired
estimate for Y .
It has been proven that for intervals Xi , the resulting estimate Y~ contains the desired
interval Y .
A similar procedure can be used for fuzzy processing: here, we implement elementary
operations g using extension principle.

These new methods do not always lead to accurate results. The results of these

computations do not always lead to the exact value of Y ; even for intervals Xi , the result
9

depends on the exact order of the operations performed to compute f . For example, we can
compute f (x1; x2) = x1  x2 by simply multiplying the two numbers, or we can compute the
same product by using a more complicated formula x1  x2 = (1=4)  [(x1 + x2 )2 ? (x1 ? x2 )2 ].
If X1 = X2 = [1; 2], then the rst algorithm leads to the estimate Y~ = [1; 4] that coincides
with the desired interval Y = f (X1; X2). However, the second algorithm leads to a dierent
result: indeed, this algorithm can be represented as a following sequence of computations:
 r3 := r1 + r2.
 r4 := r32 .
 r5 := r1 ? r2.
 r6 := r52 .
 r7 := r4 ? r6.
 r8 := 4.
 r9 := r7 =r8.
So, for Xi = [1; 2], we get R3 = [2; 4], R4 = [4; 16], R5 = [?1; 1], R6 = [0; 1], R7 = [3; 16],
R7 = [4; 4], and Y~ = R9 = [0:75; 4] 6= Y = [1; 4]. The resulting interval Y~ contains extra
points [0:75; 1).

1.3. Going from numbers to intervals and fuzzy sets
drastically increases computation time,
so hardware support is in order
Hardware support is necessary. We have already mentioned that even for real num-

bers, data processing algorithm that computes f (x1; :::; xn) can be quite complicated and
time-consuming. When we analyze accuracy of data processing, we must go from operations with numbers to operations with intervals, crisp sets, or fuzzy sets. For example,
when we go from precise numbers to fuzzy sets, then instead of processing n numbers
according to the known algorithm f , we have to solve complicated optimization problems
to nd Y (y). This increases computation time drastically: for example, in a crisp case,
to compute a sum of two numbers x1 + x2 , we must process these two numbers xi only;
all it takes is one arithmetic operation. To compute a sum of two fuzzy operands, instead
of processing two numbers, we must take as input the values X1 (x1 ) and X2 (x2 ) that
correspond to dierent xi . Just because of the necessity to process such a long input, these
computations are inevitably long. How to speed up fuzzy computations?
One known way to speed up computations in general is to design a hardware support
for them. This idea worked perfectly well, e.g., for oating point operations that had
initially been implemented in software. So, it is desirable to design hardware support for
interval and fuzzy computations as well.
10

A word of warning: Hardware support is not sucient. Hardware support does

bring a speed-up, and is, therefore, a great idea. However, the very fact that we have a
hardware support of several basic operations with intervals, crisp sets, and/or with fuzzy
sets, does not necessarily mean that we have improved the quality of the result (we are
thankful to the anonymous referee who helped us clarify this point).
As we have shown in Subsection 1.2, if we start with an expression
(1=4)  [(x1 + x2)2 ? (x1 ? x2)2 ]
and simply substitute interval operations instead of operations with numbers, we will get
an overestimation irrespective on whether we implement these operations in software or in
hardware. The only way to get the exact estimate is to transform this expression into an
equivalent one x1  x2 for which interval computations give the exact estimate.
An even more striking example is a function of one variable dened as f (x1) = x1 ? x1 .
This function is, of course, identically equal to 0, so, for every interval X1, we should get
f (X1) = f0g. However, if we take X1 = [0; 1], and apply interval subtraction, we will get
X1 ? X1 = [0; 1] ? [0; 1] = [?1; 1]. Whether we are implementing interval subtractions in
hardware or in software, we get an overestimation. Again, the only way to get the exact
estimate is take into the consideration that the two terms in the original expression cancel
each other, and thus, to transform the original expression x1 ? x1 into an equivalent one
0.
So, in addition to hardware, we need some appropriate symbolic reasoning engine (of
the type implemented in Macsyma or in Mathematica) that would transform the original
expression into an equivalent one that will lead to the precise (or at least to a more
accurate) estimate.
At present, designing such an engine is a more urgent and more potentially rewarding
task than working on hardware. Currently, computers only allow unary and binary operations. So, what this engine will do is, given a function, transform it into a sequence of
unary and binary operations. The perfect engine will output a transformation that leads to
the best possible estimate (e.g., to the interval with the smallest possible overestimation).
However, this \the best" does not necessarily mean that we will have no overestimation
at all: As we will see later, for some functions of three and more variables, no matter
how we transform these functions into a sequence of unary and binary operations, interval
computations will always lead to an overestimation. This overestimation result is true not
only for the existing computers, where only elementary arithmetic operations are hardware
supported; this result (as we will show) is true for any computer that hardware supports

11

only unary and binary operations. So, for such functions, the only way to avoid overestimation is to implement operations with three or more interval (corr., fuzzy) operands in
hardware.
For the resulting new computer, the computation scheme will include not only standard unary and binary operations, but some new operations (with interval or fuzzy
operands) as well. Again, the very fact that we have added these new operations does
not automatically mean that we will achieve the exact estimate: before we apply interval
computations, we must transform the original computation scheme (that may be overestimating) into a new (equivalent) scheme with no overestimation. So, we will again need
an appropriate symbolic reasoning engine for the new computer.
Summarizing, we can say that achieving precise interval and fuzzy computations is a threestep task:
 First, we must design a symbolic reasoning engine based on the existing hardware
supported operations (namely, elementary arithmetic operations). This engine must
transform the original expression (in terms of these elementary operations) for which
interval and fuzzy computations overestimate into an equivalent expression that leads
to more precise result Y .
 Second, we must select operations with three or more interval or fuzzy operands that
need to be hardware supported in order to get precise results.
 Third, for these new operations included, we must design a new symbolic reasoning
engine that will transform every algorithm into a sequence of elementary operations
(arithmetic or new ones) for which interval (fuzzy) computations will lead to the
precise result.
The rst task { the design of the original engine is, in eect, being currently done in interval
computations community (see, e.g., [10] and [14]). In the present paper, we consider the
second task: the choice of the hardware operations to be hardware supported. We give
only a partial answer to this task. As soon as this problem will be solved, the need will
come for the third task: designing a new symbolic reasoning engine for the new computer.

Hardware support of interval and fuzzy operations: a little bit of history. The

existing hardware support of interval operations is described in [23,36,37], and references
therein. Usually, the supported operations are arithmetic operations, and the scalar (dot)
product a1  b1 + ::: + an  bn .
The rst hardware implementation of operations with fuzzy sets has been developed
by Yamakawa. For the current state of research, see, e.g., [49,50]; for applications, see,
12

e.g., [43] and [52]. This rst implementation included several chips. The rst single-chip
hardware implementation of fuzzy operations have been proposed in [45] (for more recent
results, see, e.g., [44] and [48]). Parallel hardware implementation has been proposed
and described in [2]; see also [38] and [42]. These implementations, however, are mainly
oriented towards fuzzy control problems (and not fuzzy data processing).

What interval and fuzzy operations should we support for data processing? It

is impossible to implement in hardware fuzzy operations that correspond to all possible
functions f (x1; :::; xn). So, it is necessary to choose.

The natural idea is to describe all functions f that are hardware supported on the
existing computers, and to support the corresponding operations with fuzzy sets. Since
usually, only unary and binary operations are hardware supported, we will thus have
hardware implementation only of operations of one and two operands.

What we are planning to do. In this paper, we show that for intervals and for fuzzy sets,

an implementation of only unary and binary operations is not sucient in the following
sense: for some functions f and for some intervals (fuzzy sets) Xi , no matter how we
represent the function f as a composition of unary and binary operations, if we then
apply the above-described methodology, the resulting estimate Y~ will be dierent from the
desired value Y = f (X1; :::; Xn). Therefore, if we want fuzzy data processing to be precise,
operations with three or more fuzzy operands should also be implemented in hardware.
Some crisp operations with three or more crisp operands are already hardware supported: e.g., many computers contain a math co-processor that, among other things, hardware supports matrix operations, i.e., operations whose operands include an entire matrix
(i.e., lots of numerical operands). For example, it is possible, given two arrays a1 ; :::; an
and b1; :::; bn, to compute their dot (scalar) product a1  b1 + a2  b2 + ::: + an  bn by using
a single operation of a math co-processor. For crisp numbers, the main purpose of using
such operations is to speed up computations: in principle, we can use operations with two
operands (in this case, addition and multiplication) and compute the same expression in
several steps.
We are planning to show that in interval and fuzzy case, if we restrict ourselves to
unary and binary operations only, then not only computations will slow down, but for
some functions f , and for some intervals (fuzzy sets) Xi , we will not get the desired value
f (X1; :::; Xn) at all.
13

1.4. Structure of the paper
In Section 2, we will give some general denitions. In Section 3, we will consider the simplest case when inputs are intervals, and when all operations that are hardware supported
are smooth (dierentiable) functions. For this case, we will prove that operations with
one or two variables are not sucient. The fact that functions are dierentiable makes it
possible not only to prove the negative results but also to describe the smallest possible
order of the error that can be caused by such computations (linear, quadratic, etc). We
also give a list of operations that need to be hardware supported so that we will be able
to compute Y with a better accuracy: this list consists of arithmetic operations and a
weighted scalar product. In Section 4, these results will be generalized from crisp intervals
to fuzzy sets. In Section 5 and 6, we show that even if we allow operations that are not
dierentiable, still operations with one or two operands will not be sucient. In Section
7, we discuss the relation between these results and Kolmogorov's theorem (well known in
mathematics) that every continuous function of three or more real variables can be represented as a composition of real-valued functions of one and two variables. For reader's
convenience, all the proofs are placed in Section 8.

14

2. GENERAL DEFINITIONS
In this Section, we give general denitions that will be used in the following text.

2.1. Computation scheme
In this subsection, we will formalize the description of a general computation process
(presented in Subsection 1.2) into a formal denition, and show (on an example) how this
formalization is related to the original description.

Denition 2.1. By a computation scheme S with n initial values, and with operations
with one or two operands (or, for short, simply a computation scheme), we mean a nite
sequence (Sn+1; Sn+2 ; :::; SN ) of expressions Si (called steps). Each step Si is an expression
of one of the following three types:
 ri := ci , where ci is a real number;
 ri := fi (rj ), where fi is a function of one variable (not necessarily everywhere dened),
and j < i;
 ri := fi (rj ; rk ), where fi is a function of two variables (not necessarily everywhere
dened), j < i, and k < i.
If S is a computation scheme with n initial values, and x1; :::; xn are n real numbers, we
say that the result of applying S to xi is y, if rN = y, where the sequence ri is dened as
follows:
 if i  n, then ri = xi :
 if i > n, then depending on the type of the rule Si , we dene ri as follows:
 if the rule if ri := ci , then ri = ci ;
 if the rule is ri := fi (rj ), then ri = fi (rj );
 if the rule is ri := fi (rj ; rk ), then ri = fi (rj ; rk ).

Denotation. For some x1; :::; xn, and for each i  N , this Denition provides us with a

certain value of ri . This value will be denoted by ri (x1; :::; xn).

Comment. We have already mentioned that inside a computer, every algorithm is translated into a sequence of elementary operations. Since in the majority of computers, all elementary operations correspond to functions of one or two variables, an arbitrary algorithm
computing y = f (x1; :::; xn) can be thus represented as a computation scheme. For example, how in the above-given representation of the function f (x1; x2; x3) = (x1 + x2 )2 +2  x3,
we have:

15

 r4 = r1 + r2 (here, f4 = +, j = 1, k = 2);
 r5 = r42 (here, f5(x) = x2, j = 4);
 r6 = 2 (here, we have a function of 0 variables, that computes a constant 2);
 r 7 = r6  r3 ;
 r 8 = r5 + r7 .
Denition 2.2. Let K  Rn, and let f be a function from K to R. We say that a
computation scheme S computes f if for every (x1; :::; xn) 2 K , the value that S computes
(is dened and) is equal to f (x1; :::; xn).
2.2. Applying computation scheme to crisp sets
(in particular, to intervals)
In this subsection, we will describe how a computation scheme can be applied to crisp sets;
in particular, we will describe how it can be applied to intervals.

Denition 2.3. Let f (x1; :::; xn) be a function of n real variables, and let X1  R, ...,
Xn  R be (crisp) sets. Then, the result f (X1; :::; Xn) of applying f to the sets X1 ; :::; Xn
is dened by the formula (1.1). If instead of the sets Xi, we have a (crisp) set X  Rn,
then the result f (X ) of applying f to X is dened by the formula (1.2).

Denition 2.4. Assume that S is a computation scheme with n input values, and that
Xq  R, ..., Xn  R are (crisp) subsets of R. We say that the result of applying S to

X1; :::; Xn is Y if RN = Y , where the sequence of (crisp) sets R1; :::; RN is dened as
follows:
 if i  n, then Ri = Xi ;
 if i > n, then, depending of the type of the rule Si , Ri is dened as follows:
 if Si is ri := ci , then Ri = fci g;
 if the rule is ri := fi (rj ), then Ri = fi (Rj );
 if the rule is ri := fi (rj ; rk ), where j > n or k > n, then Ri = fi (Rj ; Rk ).

16

Denition 2.5. Assume that S is a computation scheme with n input values, and that
X is a (crisp) subset of Rn. We say that the result of applying S to X is Y if RN = Y ,

where the sequence of (crisp) sets R1 ; :::; RN is dened as follows:
 if i  n, then Ri = i (X ), where i is a projection on i?th component (i.e., i (~x) =
xi );
 if i > n, then, depending of the type of the rule Si , Ri is dened as follows:
 if Si is ri := ci , then Ri = fci g;
 if the rule is ri := fi (rj ), then Ri = fi (Rj );
 if the rule is ri := fi(rj ; rk ), where j  n and k  n, then we take
Ri = f (jk (X )), where jk is a projection to j ?th and k?th components (i.e.,
jk (x1 ; :::; xj ; :::; xk ; :::; xn) = (xj ; xk ));
 if the rule is ri := fi (rj ; rk ), where j > n or k > n, then Ri = fi (Rj ; Rk ).
Comment. The main dierence between these two denitions is that when we have a set
X  Rn , and we want to compute the set fi (X ) of possible values of fi (x1 ; :::; xn), we
must take into consideration the fact that not all values (x1 ; :::; xn) with xi 2 i (X ) are
possible.

2.3. Applying computation scheme to fuzzy sets

In the previous subsection, we described the result of applying a computation scheme to
crisp sets. Let us now describe what a computation scheme will do if we apply it to fuzzy
sets.

Denition 2.6. Let f (x1; :::; xn) be a function of n real variables, and let X1  R, ...,
Xn  R be fuzzy sets. Then, the result f (X1; :::; Xn) of applying f to the sets X1; :::; Xn is
dened by the formula (1.4a). If instead of the fuzzy sets Xi , we have a fuzzy set X  Rn,

then the result f (X ) of applying f to X is dened by the formula (1.3).

Denition 2.7. Assume that S is a computation scheme with n input values, and that
X1  R, ..., Xn  R are fuzzy subsets of R. We say that the result of applying S to

X1; :::; Xn is Y if RN = Y , where the sequence of fuzzy sets R1 ; :::; RN is dened as
follows:
 if i  n, then Ri = Xi ;
 if i > n, then, depending of the type of the rule Si , Ri is dened as follows:
 if Si is ri := ci , then Ri = fci g (a crisp set);
 if the rule is ri := fi (rj ), then Ri = fi (Rj );
 if the rule is ri := fi (rj ; rk ), where j > n or k > n, then Ri = fi (Rj ; Rk ).
17

Denition 2.8. Assume that S is a computation scheme with n input values, and that
X is a fuzzy subset of Rn . We say that the result of applying S to X is Y if RN = Y ,

where the sequence of fuzzy sets R1; :::; RN is dened as follows:
 if i  n, then Ri = i (X ), where i is a projection on i?th component (i.e., i (~x) =
xi );
 if i > n, then, depending of the type of the rule Si , Ri is dened as follows:
 if Si is ri := ci , then Ri = fci g (a crisp set);
 if the rule is ri := fi (rj ), then Ri = fi (Rj );
 if the rule is ri := fi(rj ; rk ), where j  n and k  n, then we take
Ri = f (jk (X )), where jk is a projection to j ?th and k?th components (i.e.,
jk (x1 ; :::; xj ; :::; xk ; :::; xn) = (xj ; xk ));
 if the rule is ri := fi (rj ; rk ), where j > n or k > n, then Ri = fi (Rj ; Rk ).

3. SIMPLEST CASE:
SMOOTH OPERATIONS, INTERVAL UNCERTAINTY;
HARDWARE SUPPORT OF UNARY AND BINARY OPERATIONS
IS NOT SUFFICIENT;
WEIGHTED SCALAR PRODUCT MUST ALSO BE SUPPORTED

In this section, we will consider the simplest case when inputs are intervals, and when
all operations that are hardware supported are smooth (dierentiable) functions. For this
case, we will prove that hardware operations with one or two variables are not sucient.
The fact that functions are dierentiable makes it possible not only to prove the
negative results but also to describe the smallest possible order of the error that can be
caused by such computations (linear, quadratic, etc). We will also give a list of operations
that need to be hardware supported so that we will be able to compute Y with a better
accuracy: this list consists of arithmetic operations and a weighted scalar product.

Denition 3.1.
 We say that a computation scheme S is continuous if all the function fi are continuous.
 We say that a function f is smooth if it is dened on an open set, is three times
dierentiable, and all its third order derivatives are continuous.
 We say that a computation scheme S is smooth if all the function fi are smooth.

3.1. The main (negative) result:
unary and binary operations are not sucient

In this subsection, we will describe our rst negative result: that for smooth operations on
intervals, hardware unary and binary operations are not sucient.
18

Denition 3.2. Assume that a smooth computation scheme S computes a smooth function f dened on an open set K  Rn. We say that S is precise for intervals if for all
intervals X1; :::; Xn for which X1  :::  Xn  K , the result of applying S to X1 ; :::; Xn
coincides with f (X1; :::; Xn).

For example, a 1-step computation scheme that computes f (x1; x2) = x1  x2 by
multiplying x1 and x2 is precise for intervals, while the computation scheme based on the
expression (1=4)  [(x1 + x2 )2 ? (x1 ? x2 )2] is not. It can be proven that the above-given
computation scheme for f (x1; x2; x3) = (x1 + x2 )2 +2  x3 is precise for intervals. As we have
already mentioned, for one and the same function, some computation scheme are precise,
and some others are not. The question is: for a give function f , is there any computation
scheme that is precise?

Denition 3.3. We say that for a smooth function f : Rn ! R, smooth interval com-

putations are precise, if there exists a smooth computation scheme that computes f and
that is precise for intervals. If such a smooth computation scheme does not exist, then we
say that for this function f , smooth interval computations cannot be always precise.

We will show that for many reasonable smooth functions, smooth interval computations cannot be always precise. To formulate our result, we will need a few denotations
and denitions.

Denotations.
 By f;i, we will denote i?th partial derivative of a function f .
 By f;ij , we will denote the second partial derivative
2f
:
f;ij = @x@ @x
i

j

Denition 3.4.
 A point ~s is called a stationary point of a function f if f;i (~s) = 0 for all i.
 A stationary point ~s of a function f is called non-degenerate if the following two

conditions are satised:
 at this point ~s, all components of the Hessian matrix f;ij (~s) are dierent from 0;
 at this point ~s, the determinant of the Hessian matrix is dierent from 0.

Comment. Almost all matrices satisfy these two properties (in the sense that the set of
symmetric matrices for which they are not true forms a subspace of co-dimension 1 in
the n(n + 1)=2?dimensional set of all matrices). So, we can say that almost all smooth
functions with a stationary point have a non-degenerate stationary point.

19

THEOREM 3.1. If a smooth function f (x1; :::; xn), n  3, has a non-degenerate stationary point, then for this function f , smooth interval computations cannot be always
precise.

Reformulation of this result in more informal (and hopefully, more understandable) terms. In view of the previous comment, this result means that for almost
all smooth functions with a stationary point, smooth interval computations cannot be always precise.

Our result and known mathematical results. The very fact that there exist smooth
functions for which smooth interval computations are not always precise is not surprising:
indeed, it is known (see, e.g., a survey [26]), that there exists a smooth function of three
variables that cannot be represented as a composition of smooth functions of one or two
variables. For such functions, a smooth computation scheme cannot be precise even for
real numbers, and of course, it is not precise for intervals, because every real number xi
can be viewed as a (degenerate) interval [xi ; xi]. Our negative result is much broader than
that: namely:
 Functions that cannot be represented as a composition of smooth unary and binary
operations can be viewed rather as an exception: e.g., every polynomial can denitely
be represented as such a composition.
 For smooth interval computations, we have shown that (in some reasonable sense)
almost all functions (to be more precise, almost all functions with a stationary point)
cannot be represented as compositions of smooth unary and binary interval operations.
This result includes functions f (x1; :::; xn) like quadratic polynomials, that can be
represented as a composition for numerical xi .

What does this theorem tell us about the choice of operations for hardware
implementation. With respect to hardware support, Theorem 3.1 says the following:
suppose that we have chosen a list of operations that we intend to implement in hardware
(both as operations with numbers and as operations with intervals). On the resulting
computer, every algorithm f (x1; :::; xn) will thus be represented as a composition of the
hardware supported operations fi . If we want to compute the interval f (X1; :::; Xn),
we will apply the same sequence of operations as in computing f (x1; :::; xn), but with
intervals instead of numbers (i.e., we will apply the computation scheme S that computes
f to intervals X1; :::; Xn). For some computation schemes, e.g., for

x1  x2 = (1=4)  [(x1 + x2)2 ? (x1 ? x2)2 ];
20

we may get an overestimate of f (X1; :::; Xn); for some others, hopefully, we will get a
precise estimate. In view of the previously mentioned result, for some smooth functions,
no smooth computation scheme will return the precise interval f (X1; :::; Xn).
We would like to choose a set of hardware supported operations in such a way that
for each smooth function that has smooth computation schemes, at least one of these
schemes will be precise for intervals (i.e., the above-described method will give exactly
f (X1; :::; Xn)). Theorem 3.1 tell us that we cannot achieve this goal if we only support
unary and binary operations; so, we must also implement some operations with three or
more operands in hardware.

3.2. Second negative result: if we only use unary and binary operations,
we cannot even compute the main term correctly
According to Theorem 3.1, if a smooth function f (x1; :::; xn) has a non-degenerate stationary point, and if we are only using unary and binary operations, then it is impossible to
always compute f (X1; :::; Xn) precisely for intervals Xi. So, if a computation scheme S
computes f , then the result Y~ of applying S to some intervals X1; :::; Xn will be dierent
from the desired interval Y = f (X1; :::; Xn). How dierent can it be?
Since we consider smooth functions, we can try to describe this dierence in terms of
the order: is it linear (rst order), quadratic (second order), cubic (third order), etc, in
i ? It could be that the dierence is, say, of fth order w.r.t. errors i , and therefore,
for practical purposes (this dierence being much smaller than the interval itself) we could
safely neglect it, and treat Y~ as a good approximation for Y . Alas, the reality is not so
good: it turns out that smooth interval computations (with unary and binary operations)
do not even give a correct main term for Y .
To describe this result in mathematical terms, let us rst describe the asymptotic of Y :

PROPOSITION 3.1. Let f (x1; :::; xn) be a smooth function. Then, the lower f ? and
upper f + bounds of the interval f ([x1 ? 1 ; x1 + 1 ]; :::; [xn ? n ; xn + n ]) satisfy the
property f  = f (x1; :::; xn)  (jf;1(~x)j1 + ::: + jf;n (~x)jn ) + O(2i ).
Denition 3.5. Assume that S is a smooth computation scheme for a smooth function
f (x1; :::; xn). We say that S always computes the main error term correctly if for every ~x,
the dierence between the actual endpoints of the interval
Y = f ([x1 ? 1; x1 + 1 ]; :::; [xn ? n ; xn + n ])
and the values computed by applying S to intervals Xi = [xi ? i ; xi + i ] is O(2i ).
21

Comment. In other words, we allow smooth interval computations not to be precise in the
sense that their result can dier in terms that are quadratic (or of higher order) in i , but
we require that the main term in i be computed precisely.
THEOREM 3.2. If a smooth function f (x1; :::; xn), n  3, has a non-degenerate stationary point, then for this function f , smooth interval computations cannot always compute
the main error term correctly.

3.3. Third negative result:
if we only use unary and binary operations,
we cannot even be locally asymptotically correct

An even stronger negative result can be proved. Namely, in our denition of what it means
to compute the main term correctly, we required that the main term should be computed
exactly. In general, the fact that a function is several times dierentiable, means that we
can approximate it by its Taylor polynomial. For example, if f is twice dierentiable, then
in the neighborhood of a point (s1; :::; sn), we can approximate the function f (x1; :::; xn)
P
by a linear function: f (x1; :::; xn) = f (s1; :::; sn) + f;i (s1; :::; sn)(xi ? si ) + O((xi ? si )2 ).
If the function is analytical, these Taylor polynomials actually converge to the function f .
So, if by using smooth interval computations, we cannot compute the main term precisely,
then maybe, we can at least compute correctly the rst few terms in the expansion of the
main term? I.e., e.g., we may be able to compute the main term with the accuracy of
O((xi ? si )2) terms? Alas, even this, we cannot compute, as the following result shows.
Denition 3.6. Let f (~x) be a smooth function, ~s = (s1; :::; sn) is a point in Rn, and
S is a smooth computation scheme for f . We say that S is locally asymptotically correct
(in computing the main error term) in the neighborhood of ~s, if the dierence between the
actual endpoints of the interval f ([x1 ? 1 ; x1 +1 ]; :::; [xn ? n ; xn +n ]) and the values
computed by applying S to the intervals Xi = [xi ? i ; xi +i ] is O(2i )+ O(i  (xj ? sj )2 ).
THEOREM 3.3. If a smooth function f (x1; :::; xn), n  3, has a non-degenerate stationary point, then for this function f , there exists a point ~s such that no matter what smooth
computation scheme S we choose to compute f , the resulting smooth interval computations
are not locally asymptotically correct in the neighborhood of ~s.

3.4. Positive result: if we add weighted scalar product,
smooth interval computations become locally asymptotically correct

Indeed, assume that in our denition of a computation scheme, we allow, in addition to
unary and binary operations, weighted scalar product, i.e., an operation
a1 ; :::; an; b1; :::; bn ! w1  a1  b1 + ::: + wn  an  bn ;
(3:1)
22

where n is an arbitrary positive integer, and wi are arbitrary real numbers (called weights).
Before we give a formal denition, we must make one comment.
In operations with real numbers, if we can implement a binary operation f (x1; x2),
then we can automatically implement the function g(x) = f (x; x) that is obtained by
applying f to two equal values: namely, to implement g, we can simply apply f to two
equal values. For interval operations, this idea does not always lead to an implementation
of g. As an example, we can take subtraction f (x1; x2) = x1 ? x2. For subtraction,
g(x) = f (x; x) = x ? x = 0; the corresponding interval operation is f ([x?1 ; x+1]; [x?2 ; x+2]) =
[x?1 ? x+2 ; x+1 ? x?2 ]. If we apply this operation to [x?1 ; x+1 ] = [x?2 ; x+2] = [0; 1], we get
f ([0; 1]; [0; 1]) = [?1; 1]. This result is dierent from the desired g([0; 1]) = f0g. Because
of this comment, when we describe a computation scheme that involves a certain interval
operation f , we must specically include interval analogues of all functions that can be
obtained from f by applying it to equal values of the variables. As a result, for weighted
scalar product, we arrive at the following denition:

Denition 3.7.
 Let a function f (x1; :::; xi?1; xi ; xi+1; :::; xj?1; xj ; xj+1; :::; xn) be given. We say that a

function g(x1; :::; xj?1; xj+1; :::; xn) = f (x1; :::; xi?1; xi; xi+1 ; :::; xj?1; xi; xj+1 ; :::; xn)
with n ? 1 variables is a simplied version of a function f ; transition from f to g will
be called a simplication.
 We say that a function g is a version of a function f if g can be obtained from f by
a sequence of simplications.
 By a weighted scalar product, we mean an operation (3.1).
 By a computation scheme with weighted scalar products, we mean a sequence S of
expressions each of which is either a function of zero, one, or two variables (like in
Denition 2.1), or an application of a weighted scalar product or of one of its versions.

Example. If we use weighted scalar product and its versions, we can simplify the computation of the above-given expression (x1 + x2 )2 + 2  x3 as follows: we still have ri = xi for
i = 1; 2; 3; and then:
 r4 = r2 + r3 (this is a binary operation);
 r5 = 2;
 r6 = r42 + r5  r3; here, to the values r4 , r5 , and r6 , we apply a function f6(y1; y2; y3) =
y12 + y2  y3 obtained by simplifying a weighted scalar product: f6 (y1; y2; y3) =
f  (y1; y1; y2; y3), where f (y1; y4; y2; y3) = y1  y4 + y2  y3 is a weighted scalar product
with both weights equal to 1.

23

For such computation schemes, we can repeat denitions 2.2 (what it means that a scheme
computes a function f ), 2.3 (how to apply a scheme to intervals), and 3.6 (what it means
to be locally asymptotically correct). Now, we can formulate our positive result:

THEOREM 3.4. For every smooth function f (x1; :::; xn), n  3, and for every point
~s in its domain, there exists a computation scheme S with weighted scalar products for
which the resulting smooth interval computations are locally asymptotically correct in the
neighborhood of ~s.

Comments.
 Good news: the result is applicable not only to intervals, but to arbitrary crisp sets as
well. From the proof of this theorem, one can easily see that the designed computation
scheme is locally asymptotically correct not only for intervals Xi = [xi ? i ; xi + i ],
but also for arbitrary crisp sets Xi  [xi ? i ; xi + i ].

 Not so good news: We know what operations we need to implement in hardware, but

we do not yet know how to implement them all. Theorem 3.4 says that if we hardware
support all weighted scalar products, then smooth interval computation becomes locally asymptotically correct. A word of warning: this result does not mean that we can
immediately implement all these operations and make computations (asymptotically)
precise, because we do not yet know how to implement all the necessary operations.
Indeed, according to Denition 3.7, we need separate versions of a function to deal
with each simplication obtained when two or more arguments to the function represent the same variable. For a function with n arguments, this will require n! hardware
implementations of the function. When n is large, n! is so unrealistically large that
it is practically impossible to have n! dierent hardware devices that compute n! different simplications. So, to implement all these simplications, we need a exible
implementation that will change when some of the arguments represent the same variable. At present, we do not know how to design such a exible implementation. This
implementation issue is an interesting open problem.

4. SMOOTH OPERATIONS, INDEPENDENT FUZZY SETS X1; :::; Xn:
HARDWARE SUPPORT OF UNARY AND BINARY OPERATIONS
IS NOT SUFFICIENT;
WEIGHTED SCALAR PRODUCT MUST ALSO BE SUPPORTED
In the previous section, we proved that unary and binary operations are not sucient
to support arbitrary smooth operations on intervals. In this section, we will show that,
24

similarly, unary and binary operations are not sucient to describe smooth operations on
(independent) fuzzy sets.

Denition 4.1. Assume that a smooth computation scheme S computes a function f
dened on an open set K  Rn. We say that S is precise for independent fuzzy sets if for
all fuzzy sets X1 ; :::; Xn for which X1  :::  Xn  K , the result of applying S to X1 ; :::; Xn

coincides with f (X1; :::; Xn).

Denition 4.2. We say that for a smooth function f : Rn ! R, smooth computations

are precise for independent fuzzy sets, if there exists a smooth computation scheme that
computes f and that is precise for fuzzy sets. If such a smooth computation scheme does
not exist, then we say that for this function f , smooth computations cannot be always
precise for independent fuzzy sets.

THEOREM 4.1. If a smooth function f (x1; :::; xn), n  3, has a non-degenerate sta-

tionary point, then for this function f , smooth computations cannot be always precise for
independent fuzzy sets.
Comments.
 Theorem 4.1 follows directly from Theorem 3.1: indeed, if a smooth computation
scheme is precise for arbitrary independent fuzzy sets, then, in particular, it must be
precise for crisp intervals, and this (according to Theorem 3.1) is impossible.
 Similarly, Theorems 3.2 and 3.3 show that smooth fuzzy computations with only unary
and binary operations cannot even describe the main term in f (X1; :::; Xn) correctly.
 In many reasonable cases, extension principle that denes Y = f (X1; :::; Xn) for
fuzzy sets X1; :::; Xn can be reformulated in terms of their (crisp) ?cuts Xi () =
fxjX (x)  g: namely, Y () = f (X1(); ::; Xn()) (see [33]; for counterexamples,
see [33] and [7]). So, if we add weighted scalar product to the list of hardware supported operations, then, due to Theorem 3.4, we will be able to get all ?cuts Y ()
locally asymptotically correctly, and in this sense, we will be able to compute the
fuzzy set Y itself with the same accuracy.
i

5. GENERIC (NOT NECESSARY SMOOTH) OPERATIONS, CRISP SETS:
HARDWARE SUPPORT OF UNARY AND BINARY OPERATIONS
IS NOT SUFFICIENT
Let us now consider the general case of operations that are not necessarily smooth, and of
the information that is not necessarily representable by independent sets X1 ; :::; Xn.
25

Denition 5.1. Assume that a computation scheme S computes a function f dened on
a set K  Rn. We say that S is precise for all (crisp) sets if for every (crisp) set X  K ,
the result of applying S to X coincides with f (X ).
Comment. We are going to prove that if our list of elementary operations includes only
operations with one and two operands, and a function f of three and more variables is
(in some reasonable sense) non-degenerate, then no computation scheme is applicable to
fuzzy processing (i.e., none of them will provide the exact fuzzy result). Let us describe
what we mean by non-degenerate.

Denition 5.2. Let K  Rn , n  3. We say that a function f : K ! R is degenerate
if K can be subdivided into nitely many subsets Ki so that on each subset, f coincides
with some function of one or two variables (i.e., on which f (x1; :::; xn) = g(xj ; xk ) for
some j and k and for some function g). A function that is not degenerate will be called
non-degenerate.

Comment. As an example of a degenerate function, we can take f (x1; :::; xn) =
max(x1 ; :::; xn). For this function, K = Rn can be subdivided into the subsets Ki in
which xi is greater than or equal to all other values, and on each Ki , f (x1; :::; xn) = xi
(i.e., is equal to a function of one variable).

The following two results show that many functions of three or more variables are
non-degenerate:

PROPOSITION 5.1. If a function f (x1; :::; xn) of three or more variables is dened
on a domain K , and on some subset M  K with a non-empty interior, f is strictly
monotonic in each variable, then f is non-degenerate.

Comment. To describe non-degenerate functions, we need to recall a notion of a real
analytic function: this means a function that can be represented as a sum of its Taylor
series. All known elementary functions (arithmetic operations, sin, cos, exp, ln, etc) and
their compositions are real analytic functions. It turns out that a real analytical function
is non-degenerate if and only if it actually depends on all of its variables:

PROPOSITION 5.2. If f (x1; : : :; xn) is a real analytic function of n  3 variables, and
f is not equal to a function of < n variables, then the function f is non-degenerate (in the
sense of Denition 5.2).

26

Examples.
 A function f (x1; x2; x3) = sin(x1 + exp(x2  x3)) is non-degenerate, because it actually
depends on each of its variables.
 In contrast, a function f (x1; x2; x3) = x1  x2 is degenerate, because it does not depend
on the variable x3 at all and is thus equal to the function of two variables.

THEOREM 5.1. If a function f of three and more variables is non-degenerate, then no

computation scheme that computes f is precise for all crisp sets.

Comment. This result is based on our Denition 2.1, in which we assumed that all elementary operations are operations with one or two (set-valued) operands. Thus, in case
of incomplete knowledge, when we have sets of possible values of the variables, operations
with one and two operands are not sucient. This suggests that for this case, we need to
implement hardware operations with 3 or more operands.

6. GENERIC (NOT NECESSARY SMOOTH) OPERATIONS, FUZZY SETS:
HARDWARE SUPPORT OF UNARY AND BINARY OPERATIONS
IS NOT SUFFICIENT
In the previous section, we proved that unary and binary operations are not sucient to
support generic (not necessarily smooth) operations on crisp sets. In this section, we will
show that, similarly, unary and binary operations are not sucient to describe generic
operations on fuzzy sets.

Denition 6.1. Assume that a computation scheme S computes a function f dened on
a set K  Rn. We say that S is precise for all fuzzy sets if for every fuzzy subset X  K ,
the result of applying S to X coincides with f (X ).
THEOREM 6.1. If a function f of three and more variables is non-degenerate, then no

computation scheme that computes f is precise for all fuzzy sets.

Comments.
 Since crisp sets are a particular case of fuzzy sets, this theorem is a corollary of
Theorem 5.1 (just like Theorem 4.1. is a corollary of Theorem 3.1).
 This result is based on our Denition 2.1, in which we assumed that all elementary
operations are operations with one or two (fuzzy) operands. Thus, for soft computing,
operations with one and two operands are not sucient. This suggests that for soft
computing, we need to implement hardware operations with 3 or more operands.

27

7. OUR RESULTS AND KOLMOGOROV'S THEOREM
7.1 What is Kolmogorov's theorem: in brief
The fact that every continuous function of three and more variables can be represented
as a composition of functions of one or two variables (and can be thus computed by an
appropriate computation scheme) has been rst proved by Kolmogorov [15] as a solution
to the famous Hilbert's problem: one of 22 problems that Hilbert has proposed in 1900 as a
challenge to the XX century mathematics [13]. Kolmogorov's result was later improved in
[40,41], and turned out to be applicable to theoretical and practical aspects of computation
(see, e.g., [6,12,24,25,31,30]).

7.2. In some reasonable sense, Kolmogorov's theorem
may not be extended to interval and fuzzy computations
Our negative theorems (3.1{3.3, 4.1, 5.1, and 6.1) show that Kolmogorov's theorem might
not be possible to extend to interval or fuzzy cases: there are functions that cannot be
represented as a composition of operations with one or two interval (resp. fuzzy) operands.

7.3. In some other sense (less computationally straightforward)
we can extend Kolmogorov's theorem to intervals
The above results are about the following: we have a function; we describe its computations
step-by-step, and substitute operations with intervals instead of operations with numbers.
In [32], the following result have been proven: if we do not follow the algorithm
f step-by-step, i.e., if we do not require that interval operations follow the ow of numerical computations, then Kolmogorov's theorem is true: namely, an arbitrary interval
function can be represented as a composition of functions of one and two variables. The
proof is rather simple: suppose that we have an interval-valued function f (x1 ; :::; xn) =
[f ? (x1 ; :::); f +(x1 ; :::)] of n interval variables x1 = [x?1 ; x+1]; :::; xn = [x?n ; x+n ] . This means
that we have two functions f ? and f + of 2n real variables x?1 ; :::; x?n ; x+1 ; :::; x+m. Each
of these functions can be (due to Kolmogorov's theorem) represented as a composition of
functions of one and two variables. So, we can do the following:
 rst, apply the interval functions ? and > that transform an interval [x? ; x+] into
[x? ; x? ] and, correspondingly, [x+ ; x+].
 follow the operations from Kolmogorov's theorem with these degenerate intervals, and
get the numerical-valued functions [f ?; f ?] and [f +; f +] as the desired composition;
 apply a combination operation comb([a; a]; [b; b]) = [a; b] to [f ?; f ? ] and [f +; f +] and
get the desired interval f = [f ?; f +].
28

With respect to hardware operations it means that in principle, we can restrict ourselves to
unary and binary operations only, but in this case, to compute the interval f (X1; :::; Xn),
we will not be able to simply follow the algorithm f step-by-step: for each f , we will have
to design a new method of interval (and therefore, for fuzzy) data processing.
Comment. An important open question:
 the result from [32] (described in this section) is proven for intervals;
 what happens in the fuzzy case?

8. PROOFS
Proof of Proposition 3.1
Since the function f is smooth, if xi are such that jxi j  i , then
f (x1 + 1; :::; xn + xn ) = flin + O(2i );
where we denoted flin = f (x1; :::; xn) + x1  f;1 + ::: + xn  f;n. The maximum of
flin is attained when each of the component terms xi  f;i attains the largest value for
xi 2 [?i ; i ]: for f;i > 0, the maximum is attained when xi = i , and for f;i < 0, the
maximum is attained when xi = ?i . In both cases, the maximum of i?th term is equal
to jf;iji . Therefore, the maximum of flin is equal to f (x1; :::; xn) + jf;1j1 + ::: + jf;njn .
Hence, the maximum f + of f is equal to this expression plus O(2i ). The result for f ? is
proved similarly. Q.E.D.

Proof of Theorems 3.1{3.3
Theorems 3.1 and 3.2 follow from Theorem 3.3, so it is sucient to prove Theorem 3.3.
We will prove that the statement of this theorem is true for the non-degenerate stationary
point ~s. This will be proven by reduction to a contradiction. Namely, let us assume that
there exist a smooth function with a non-degenerate stationary point ~s and a computation
scheme S for which smooth interval computations are locally asymptotically correct in the
neighborhood of ~s. Each such scheme S can be characterized by an integer: namely, by
the total number of computation steps. Among such schemes, there exists a scheme with
the smallest possible value of this integer. Let us denote this scheme by S , the function
that is computed by this scheme by f (x1; :::; xn), and the stationary point for this function
by ~s = (s1; ::; sn).
First, let us describe what type of expression we can get after each step of the smooth
computation scheme.
29

Denition 8.1. By a generalized linear function of n variables x1; :::; xn, we mean a
linear combination of functions 1, x1, ..., xn , and absolute values ja1x1 + ::: + an xn j of

homogeneous linear functions a1x1 + ::: + an xn .
Example. A function 1 + x1 + x2 + j2x1 ? x2j + jx + 3j is a generalized linear function.
LEMMA 1. For every smooth computation scheme S that computes a function
f (x1; :::; xn), and for every point ~s 2 Rn , the endpoints f~ of the result of smooth interval computations can be represented as
X
f (x1; :::; xn)  f~i (x1 ? s1; :::; xn ? sn)i + O(2i ) + O((xi ? si )2j )
for some generalized linear functions f~i .

Proof of Lemma 1. We will prove this lemma using induction over the number of steps

in a smooth computation scheme.
Induction base. If a smooth computation scheme has not steps at all, this means that the
function that we are computing simply coincides with one of the input variables. For an
input variable xi , each endpoint xi of the interval [xi ? i ; xi + i ] is already represented
in the desired form with f~i = 1, and f~j = 0 for j 6= i.
Induction step. Assume now that we have proved this result for all smooth computation
schemes of length  k, and we want to prove it for smooth computation schemes of length
k +1. Let S be any smooth computation scheme of length k +1. By denition of a smooth
computation scheme, the nal result of S is obtained in the last ((k + 1)?st) step by
applying a smooth function of one or two variables to the results of previous computations.
These results of previous computations are thus computable by computation schemes of
length  k. Therefore, for these results, due to the induction assumption, the result can be
represented in the desired form. Let us use these forms to describe the result of applying
(k + 1)?st step.
To prove it, we will consider two possible cases:
 The rst case if when the last step of S consists of applying a smooth function of one
variable.
 The second case if when the last step of S consists of applying a smooth function of
two variables.
In the rst case, f (x1; :::; xn) = F (r(x1; :::; xn)), and the result of applying smooth interval
computations to r is already known to be expressible in the form
[r(x1; :::; xn) ? l + O(2i ) + O((xi ? si )2  j ); r(x1; :::; xn) + l + O(2i ) + O((xi ? si )2  j )];
30

where l = r~1 1 + ::: + r~n n is a linear expression in i , with coecients r~i that are
generalized linear function in xi ? si . Similarly to the proof of Proposition 3.1, we can
prove that applying F results in an interval [y?; y+], where

y = F (r(x1; :::; xn))  jF 0 (r(x1; :::; xn))j  l + O((xi ? si )2  j ):
To prove the desired result, let us rst approximate jF 0 (r(x1; :::; xn))j by a generalized
linear function. To do that, we can rst approximate the composition F 0 (r(x1; :::; xn))
by a generalized linear function. This part is easy: Since both F 0 and r are smooth
functions, we conclude that F 0 (r(x1; :::; xn)) is also a smooth function, and therefore, it
can be represented as a0 + a1 (x1 ? s1) + ::: + an(xn ? sn ) + O((xi ? si )2).
 If a0 = 0, then the absolute value of this function can be represented as

ja1(x1 ? s1) + ::: + an (xn ? sn )j + O((xi ? si)2 );
i.e., as a generalized linear function plus O(:::).
 If a0 6= 0, then a0 +P ai (xi ?si ) = a0 (1+P(ai =a0)(xi ?si )). The absolute value of the
product can be represented as the product of the absolute values. When xi ! si , then
P(ai=a0)(xi ?si) ! 0, hence P(ai=a0)(xi ?si) > ?1, and so, j1+P(ai=a0)(xi ?si)j =
P
1+ (ai =a0)(xi ? si )+ O((xi ? si )2). Therefore, ja0 + a1 (x1 ? s1)+ ::: + an(xn ? sn )j =
ja0j(1 + P(ai =a0)(xi ? si)) + O((xi ? si )2) is a linear (and thus, generalized linear)
function of xi ? si .
In both cases, we represent jF 0(r(x1; :::; xn))j as a sum of the generalized linear function g~ and an O((xi ? si)2 ) term. Therefore, the product jF 0 (r(x1; :::; xn)))j  l =
jF 0 (r(x1; :::; xn)))j  r~1 1 + ::: + jF 0(r(x1; :::; xn)))j  r~n n can be represented as

g~  r~1  1 + ::: + g~  r~n  n + O((xi ? si )2j ):
For every i, the coecient g~  r~i at i is a product of two generalized linear functions.
By denition, each generalized linear function is a sum of a constant (maybe 0), and a
homogeneous rst order part (i.e., terms ai xi and ja1x1 + ::: + an xn j). The product g~  r~i of
two generalized linear functions g~ and r~i is thus a product of two sums and can, therefore,
be represented as a sum of four terms:
 a product of two constants, which is a constant;
 a product of a constant term of g~ and a homogeneous rst order part of r~i , which is
a generalized linear function;
 a product of a constant term of r~i and a homogeneous rst order part of g~, which is
also a generalized linear function;
31

 a product of a homogeneous rst order parts of g~ and r~i , which is O((xi ? si)2 ).
The sum f~i of the rst three products is generalized linear function; the fourth term lead
to the term O((xi ? si )2 i ) in g~  r~i  i. Therefore, g~  r~i i = f~i i + O((xi ? si )2  j ) for
a generalized linear function f~i . So, jF 0 (r(x1; :::; xn))j  l = L + O((xi ? si )2  j ), where
L = f~11 + ::: + f~n n , f~i are generalized linear, and therefore, the resulting interval is
indeed equal to the desired expression [f (x1; :::; xn) ? L + O(:::); f (x1; :::; xn) + L + O(:::)].
The second case can be described in a similar manner. In this case, instead of f =
F (r), we have f (x1; :::; xn) = F (r(x1; :::; xn); t(x1; :::; xn)) for some functions r and t that
have been computed on the previous steps. The resulting proof is similar. The lemma is
proved. Q.E.D.

LEMMA 2. Let S be a smooth computation scheme that computes a function f (x1; :::; xn),
and let ~s 2 Rn. Then, jf;ij  f~i + O((xi ? si)2 ), where f~i are generalized linear functions
that appear (as coecients at i ) in the description of the result of applying smooth interval
computations.

Proof of Lemma 2. For interval computations, the resulting interval always contains the

actual interval of values of f . Due to Lemma 1, the endpoints of the resulting interval are
f  (f~1 1 + ::: + f~n n )+ O(:::), and due to the Proposition, the endpoints of the interval of
values of f are f  (jf;1j1 + ::: + jf;n jn )+ O(:::). The fact that the rst interval contains
the second one means, in particular, that the width of the rst interval is  than the width
of the second interval, i.e., that f~1 1 + ::: + f~n n  jf;1j1 + ::: + jf;n jn + O(:::). If
we x i, choose i = 1 for this i, and j = 0 for all j 6= i, we get the desired inequality.
Q.E.D.

LEMMA 3. Let S be a smooth computation scheme that computes a function f (x1; :::; xn),
and let ~s 2 Rn . Then, the following statements are equivalent to each other:
 for S , smooth interval computations are locally asymptotically correct in the neighborhood of ~s;
 jf;i j = f~i + O((xi ? si )2), where f~i are generalized linear functions that appear (as
coecients at i ) in the description of the result of applying smooth interval computations.

Proof of Lemma 3. This result immediately follows from Lemma 1 and Proposition 3.1.

Q.E.D.

Proof of Theorem 3.3 itself. Let us now prove Theorem 3.3 itself. By denition, each
step of a smooth computation scheme S that does not assign a constant value to a variable
32

(i.e., that is not of the type Ri = fci g), consists of applying a function of one or two
variables either to initial data xi , or to the values computed on one of the previous steps.
The last step of the scheme S cannot be of the form Ri = fci g since otherwise the
function would have no non-degenerate stationary point. Therefore, the last step of the
scheme S can consists of applying either:
 a function of one variable, or
 a function of two variables.
Let us prove the theorem for these two cases.
First case: the last step consists of applying a function of one variable. Let us
rst show that this last step cannot consist of applying a smooth function of one variable.
We will prove this statement by reduction to a contradiction. Assume that the last step of
S does consist in applying a smooth function of one variable F to the result r(x1 ; :::; xn)
of some previous step. In this case, f (x1; :::; xn) = F (r(x1; :::; xn)). Let us prove that r(~x)
has a non-degenerate stationary point (at the same ~s), and that for this function r, smooth
interval computations are locally asymptotically correct in the neighborhood of ~s.
Indeed, we know that ~s is a stationary point for f , i.e., that f;i (s1; :::; sn) = 0. Since
f (x1; :::; xn) = F (r(x1; :::; xn)), we have f;i (s1; :::; sn) = F 0 (r(s1; :::; sn))  r;i (s1; :::; sn).
Similarly, f;ij (~s) = F 00 (r(~s))r;i(~s)r;j (~s)+ F 0 (r(~s))  r;ij (~s). Since f;i(~s) = 0, we can conclude
that either F 0 (r(~s)) = 0, or r;i (~s) = 0. In the rst case, f;ij = F 00 (~s)r;i r;j , and the
determinant of the Hessian at ~s is 0, which contradicts to our assumption that ~s is a nondegenerate point. Therefore, F 0 (~s) 6= 0, and r;i (~s) = 0, and ~s is a stationary point of the
function r(x1; :::; xn).
Since F 0 (r(~s)) 6= 0 and r;i (~s) = 0, from f;ij (~s) = F 00(r(~s))r;i (~s)r;j (~s)+ F 0 (r(~s))  r;ij (~s),
we can conclude that f;ij (~s) = F 0 (r(~s))  r;ij (~s), and r;ij (~s) = C  f;ij (~s), where C =
1=F 0(r(~s)). Since ~s is a non-degenerate stationary point of f , we have f;ij 6= 0, hence
r;ij (~s) = C  f;ij (~s) 6= 0. Similarly, from det jf;ij j 6= 0, we conclude that det jr;ij j 6= 0. So,
~s is a non-degenerate stationary point for the function r.
Let us now show that for the smooth computation scheme that leads to r, smooth
interval computations are locally asymptotically correct in the neighborhood of ~s. Indeed,
let r~i be the generalized linear functions that appear (as coecients at i ) in the description of the result of applying smooth interval computations to r. This means that after
we reach r, the resulting interval is equal to r  (~r11 + ::: + r~n n ) + O(:::). Due to
Proposition 3.1, after applying the function F to this interval, we get
F (r)  jF 0 (r(x1; :::; xn))j(~r11 + ::: + r~n n ) + O(:::):
33

Now, since F 0 (r) is a smooth function, we get

F 0 (r(x1; :::; xn)) = F 0 (r(~s)) +

X F 00(r(~s))r (x ? s ) + O((x ? s )2):
;i i

i

i

i

Since ~s is a stationary point for r, we have r;i (~s) = 0, and therefore, F 0 (r(x1; :::; xn)) =
F 0 (r(~s))+ O((xi ? si )2) and jF 0(r(x1; :::; xn))j = jF 0 (r(~s))j + O((xi ? si )2). So, the result of
applying smooth interval computations to S is equal to F (r)jF 0 (r(~s))j(~r11 +:::+~rn n )+
O(:::). Since for f and S , smooth interval computations are locally asymptotically correct,
we can (due to Lemma 3) conclude that for all i, jF 0 (r(~s))j  r~i (~x) = jf;i(~x)j + O(:::). But,
since f (~x) = F (r(~x)), we have f;i (~x) = F 0 (r(~x))r;i (~x) and jf;i(~x)j = jF 0 (r(~x))j  jr;i(~x)j.
We already know that jF 0(r(x1; :::; xn))j = jF 0(r(~s))j + O((xi ? si )2). Therefore, jf;i (~x)j =
jF 0 (r(~s))j  jr;i(~x)j + O(:::). So, from

jF 0 (r(~s))j  r~i (~x) = jf;i(~x)j + O(:::) = jF 0 (r(~s))j  jr;i(~x)j + O(:::);
we can conclude that r~i (~x) = jr;i (~x)j + O(:::), and hence, due to Lemma 3, that for r,
smooth interval computations are locally asymptotically correct.
Since r is computed on a previous step of the smooth computation scheme S , the
number of steps that lead to r is smaller than the number of computation steps in a scheme
S , and this contradicts to our choice of S as the shortest smooth computation scheme that
leads to a smooth non-degenerate function for which smooth interval computations are
locally asymptotically correct. So, the last step of our smooth computation scheme S
cannot consist of applying a function of one variable.

Second case: the last step consists of applying a function of two variables. To

complete our proof, let us show that the last step cannot consist of applying a function of
two variables. Indeed, suppose that the last step consists of applying a smooth function
F of two variables to two functions r(~x) and t(~x) that have been computed on a previous
computation step. In this case, f;i = F;r  r;i + F;t  t;i , where by F;r and F;t, we denoted
partial derivatives of F w.r.t. r and t.

Let us rst show that for ~s, both partial derivatives of F cannot be 0. Indeed, suppose
that they are. For f = F (r; t), the Hessian matrix is equal to f;ij (~x) = F;rr (~x)r;ir;j +
2F;rt (~x)r;i t;j + F;tt (~x)t;i t;j + F;r (~x)r;ij + F;t (~x)t;ij . In particular, for ~x = ~s, taking into
consideration that F;r = F;t = 0, we conclude that f;ij (~s) = F;rr (~s)r;ir;j + 2F;rt (~s)r;i t;j +
F;tt (~s)t;i t;j . If we choose vectors r;i (~s) and t;i(~s) as the rst two elements of the base, then
the Hessian matrix will only have 11, 12, and 22 components. Therefore, the determinant
34

of the Hessian matrix f;ij (~s) will be 0, which contradicts to our assumption that the
stationary point ~s is non-degenerate. This contradiction shows that the derivatives F;r (~s)
and F;t (~s) cannot be both equal to 0. Hence, we only need to consider the following three
subcases:
 F;r (r(~s); t(~s)) 6= 0 and F;t(r(~s); t(~s)) 6= 0.
 F;r (r(~s); t(~s)) 6= 0 and F;t(r(~s); t(~s)) = 0.
 F;r (r(~s); t(~s)) = 0 and F;t(r(~s); t(~s)) 6= 0.
We will analyze these three subcases separately.

First subcase of the second case: both partial derivatives of F are dierent
from 0 at ~s. Let us rst consider the case when both partial derivatives of F are dierent

from 0 at ~s. Let us show that in this case, r;i (~s) = t;i (~s) = 0. Indeed, the interval that
P
corresponds to r is equal to r  r~i i + O(:::), and the interval that corresponds to t is
P
equal to t  t~i i + O(:::). Due to Proposition 3.1, the interval that corresponds to f is
P
thus equal to f  f~i i +O(:::), where f~i (~x) = jF;r (~x)jr~i(~x)+jF;t(~x)jt~i(~x)+O((xi ?si )2 ).
Let us analyze this equality for ~x = ~s. For this ~x, the O term is equal to 0, so we have an
exact equality f~i (~x) = jF;r (~x)j  r~i (~x) + jF;t(~x)j  t~i (~x). Since smooth interval computations
are locally asymptotically correct for f , we have (due to Lemma 2) f~i = jf;ij + O((xi ? si )2 ).
In particular, for ~x = ~s, we have f~;i (~s) = jf;i(~x)j = 0. So, the left-hand side of the equality
is 0: 0 = jF;r (~s)j r~i(~s)+ jF;t(~s)j t~i(~s). The coecients r~i and t~i are non-negative for all ~x.
So, 0 is equal to the sum of two non-negative products (jF;r (~s)j  r~i (~s) and jF;t(~s)j  t~i (~s)).
The only way for this to happen is when both products are equal to 0. Since F;r 6= 0 for
~x = ~s, from jF;r (~s)j  r~i (~s) = 0, it follows that r;i(~s) = 0. Similarly, we can conclude that
t;i (~s) = 0.
Due to r;i (~s) = t;i (~s) = 0, we can (similarly to the case of f = F (r)) conclude that
F (~x) = F (~s)+ O((xi ? si )2 ). Therefore, f~i (~x) = jF;r (~s)jr~i (~x)+ jF;t (~s)jt~i(~x)+ O(:::). On the
other hand, f~i = jf;ij + O(:::), where f;i (~x) = F;r (~x)r;i(~x)+ F;t (~x)t;i (~x), and due to F (~x) =
F (~s)+ O((xi ? si )2), we can rewrite this equality as f;i (~x) = F;r (~s)r;i (~x)+ F;t (~s)t;i(~x). So,

f~i = jf;ij + O(:::) = jF;r (~s)r;i(~x) + F;t (~s)t;i(~x)j + O(:::):
Since jp + qj  jpj + jqj, we conclude that

f~i  jF;r  r;i + F;t  t;ij + O(:::)  jF;r j  jr;i j + jF;tj  jt;i j + O(:::):
Due to Lemma 2, jr;ij  r~i + O(:::) and jt;i j  t~i + O(:::). Therefore,

f~i  jF;r  r;i + F;t  t;ij + O(:::)  jF;r jjr;ij + jF;tjjt;ij + O(:::)  jF;r j r~i + jF;t j t~i + O(:::):
35

But the right-hand side of this inequality is already known to be equal to f~i (~x) (modulo
quadratic terms). Therefore,

f~i  jF;r  r;i + F;t  t;ij + O(:::)  jF;r j  jr;ij + jF;tj  jt;ij + O(:::)  f~i + O(::):
In this chain of inequalities the starting and the ending expressions coincide, and hence,
they are all equalities. So,

jF;r  r;i + F;t  t;i j = jF;r j  jr;i j + jF;t j  jt;i j + O(:::):
The only case when jp + qj = jpj + jqj is when p and q are of the same sign. So, we can
conclude that the linear parts of the expressions F;r (~s)  r;i (~x) and F;t (~s)  t;i(~x) are of the
same sign. Clearly, the sign of the linear part of

f;i = F;r (~s)  r;i(~x) + F;t(~s)  t;i(~x) + O(:::)

P

will be of the same sign. This linear part is f;ij (~s)(xj ? sj ). Similarly, we can express
the linear parts of r;i and t;i . We know that the numbers F;r (~s) and F;t (~s) are dierent
from 0. Let us assume that both numbers are positive (the cases when one of them
is negative can be treated in a similar manner). In this case, if a linear term in f;i
then the linear terms in r;i and t;i are also non-negative. In other words, if
Pis positive,
P
f
;ij (~s)(xj ? sj ) > 0, then f;ij (~s)(xj ? sj )  0. Turning to a limit, we can conclude that
P
P
P
if f;ij (~s)(xj ? sj )  0, then f;ij (~s)(xj ? sj )  0. Similarly, if f;ij (~s)(xj ? sj )  0,
P
P
then f;ij (~s)(xj ? sj )  0. Therefore, if f;ij (~s)(xj ? sj ) = 0, then we have both
P f;ij (~s)(xj ?sj )  0 and P f;ij (~s)(xj ?sj )  0, hence, we have both P r;ij (~s)(xj ?sj )  0
P
P
and r;ij (~s)(xj ? sj )  0, and thence, r;ij (~s)(xj ? sj ) = 0.

P

P

So, if f;ij (~s)(xj ? sj ) = 0, then r;ij (~s)(xj ? sj ) = 0. In geometric terms, the
condition means that a vector xj ? sj is orthogonal to the vector f~i with coordinates
f;i1(~s); :::; f;in(~s). Similarly, the conclusion means that a vector xj ? sj is orthogonal to
the vector ~ri with coordinates r;ij (~s). Since ~s is a non-degenerate stationary point, the
vector f~i is not zero, and therefore, vectors that are orthogonal to f~i form a (hyper)plane.
A vector ~ri is orthogonal to every vector from this (hyper)plane, and is, therefore, collinear
with f~i . This means that for every i, there exists a constant ci such that r;ij (~s) = ci f;ij (~s)
for all i and j .
The matrix of second derivatives is symmetric: r;ij = r;ji . Therefore, for every i and
j , ci f;ij = cj f;ji. Since second derivatives of f;ij also form a symmetric matrix, we have
36

ci f;ij = cj f;ij . Due to the fact that ~s is a non-degenerate stationary point for f , we conclude
that f;ij =
6 0 and therefore, ci = cj for all i and j . Let us denote the common value of all ci
by c. Then, r;ij (~s) = cf;ij (~s). Similarly, t;ij (~s) = c0 f;ij (~s). The coecients c and c0 cannot
be both equal to 0, because then, we would have f;ij (~s) = F;r (~s)rij (~s) + F;t (~s)t;ij (~s) = 0.
So, either c =
6 0, or c0 =
6 0. If c =6 0, then r;ij (~s) = cf;ij (~s) is a non-degenerate matrix
with non-zero determinant and all elements non-zero. Therefore, r is a function that has
a non-degenerate stationary point at ~s, and that can be computed in fewer steps than
f , which contradicts to our choice of f . Similarly, if c0 =
6 0, then t is a function that
contradicts to our choice of f . In both cases, we get a contradiction.

Second and third subcases of the second case: only one partial derivative of
F is dierent from 0 at ~s. We have derived a contradiction for the case when both
partial derivatives F;r (~s) =
6 0 and F;t (~s) =
6 0. To complete the proof of the theorem, we

must deduce a contradiction for the case when one of these partial derivatives is equal to
0. Without losing generality, we can assume that F;r (~s) 6= 0 and F;t (~s) = 0.
In this subcase, from the equality f~i (~x) = jF;r (~x)j  r~i (~x) + jF;t (~x)j  t~i (~x) (that can be
proven exactly like in the rst subcase), we conclude that r;i (~s) = 0.

P

Since F;t (~s) = 0, we can conclude that F;t (~s) = aj (xj ?sj )+O((xi ?si )2 ). Therefore,
only zeroth-order terms in t;i (~x) need to be taken into consideration, and we have

f;i (~x) =

Xf

;ij (xj ? sj )+ O(:::) = F;r (~s)

Xr

;ij (~s)(xj ? sj )+(

X a (x ? s ))t (~s)+ O(:::):
j j

j

;i

Similarly to the rst subcase, we can prove that the two linear components of this sum
must be of the same sign as fi , and therefore, that r;ij = cf;ij and aj t;i = c0 f;ij . The
coecient c0 cannot be dierent from 0, because then, we would have f;ij = (1=c0)aj t;i,
and det jf;ij j = 0,which contradicts to our assumption that the Hessian matrix f;ij is nondegenerate. Therefore, c0 = 0, and hence, c = 0 is impossible. Therefore, r;ij = (1=c)f;ij ,
and r is a function that has a non-degenerate stationary point at ~s, and that can be
computed in fewer steps than f , which contradicts to our choice of f .
We have proven the result for the rst case, and for all subcases of the second case;
therefore, the theorem is proven. Q.E.D.

Proof of Theorem 3.4
Since the function f is smooth, for every point ~s, we have f (~x) = fquadr(~x)+ O((xi ? si )3 ),
where
X
X
fquadr = f (~s) + f;i(~s)(xi ? si ) + 21 f;ij (xi ? si )(xj ? sj ):
i;j

i

37

Therefore (similarly to the proof of Proposition 3.1), we can conclude that the dierence
between the intervals f (X1; :::; Xn) and fquadr (X1; :::; Xn) is also O((xi ? si )3). The function fquadr is quadratic in xi , so, we can represent it as

fquadr(x1; :::; xn) = a0 +

Xa x + Xa
i

i i

i;j

ij xi xj

for some real numbers ai and aij . If we dene a0i = ai for i = 0; 1; :::; n, then we can
represent this expression as fquadr(x1 ; :::; xn) = g(x0; x1; :::; xn) for x0 = 1 and

g(x0; :::; xn) =

Xn Xn a
i=0 j =0

ij xi xj :

The function g, in its turn, can be represented as a result of applying n simplications
xi = yi , 1  i  n, to a weighted scalar product

f (x0; x1; :::; xn; y0; y1; :::; yn) =

Xn Xn a
i=0 j =0

ij xi yj

with weights aij .
So, fquadr can be computed using a computation scheme S with weighted scalar products. Since the dierence between the intervals f (X1; :::; Xn) and fquadr(X1; :::; Xn) is
O((xi ? si )3 ), we can conclude that smooth interval computations described by the scheme
S are locally asymptotically correct for the original function f . Q.E.D.

Proof of Proposition 5.1
1. Let us prove this Proposition by reduction to a contradiction. Namely, assume that
f : K ! R is strictly monotonic in each variable, and that f is degenerate. By denition,
this means that K can be represented as a union of nitely many sets K1 ; :::; Kp on each
of which f is equal to a function of two of fewer variables. Let's deduce a contradiction
from here.
2. Since the interior of M  K is non-empty, the set M contains a ball.
Inside the ball, we can place a cube [a1; b1]  [a2; b2]  :::  [an ; bn]. Let's divide each
side [ai ; bi] of the cube into p + 1 equally distanced values of xi : ai , ai + (bi ? ai )=p,
ai + 2  (bi ? ai)=p, ..., ai + p  (bi ? ai )=p = bi . By combining these values for dierent i,
we get (p + 1)n dierent points that all belong to the cube and therefore, to K . Let us
denote the set of all these points by P .
38

3. Let us take one of the sets Ki , and let us estimate how many of the points from P
belong to Ki . Since f is degenerate of Ki , it means that on Ki f is equal to a function
of two or fewer variables. But f is a function of n  3 variables. So, this means, that for
~x 2 Ki , f cannot depend on all the variables. Hence, it does not depend on one of the
variables. Let us denote one of such variables by xj . The fact that for ~x 2 Ki , f does not
depend on xj , means that if we have two dierent points with dierent values of xj and
equal values of all other coordinates, then the value of f for both point will be the same.
4. Formally, if ~x = (x1; :::; xj?1; xj ; xj+1; :::; xn) 2 P  M and

~y = (x1 ; :::; xj?1; xj + xj ; xj+1; :::; xn) 2 M;
where xj 6= 0, then f (~x) = f (~y).
But on M , f is strictly monotonic in each variable. This means, in particular, that f
is either strictly increasing in xj , or strictly decreasing in xj . In both cases, f (~x) 6= f (~y).
This contradiction with f (~x) = f (~y) shows that no such pairs (~x; ~y) are possible. In other
words, if we x the values of n ? 1 coordinates (all of them except for xj ), i.e., if we
x the values x1; :::; xj?1; xj+1; :::; xn, then at most one point with these values of n ? 1
coordinates belongs to Ki .
5. For P , if we x the values of all the coordinates but one, then, we can choose p + 1
dierent values of xj and hence, have p + 1 dierent points from P  K . At most one of
them belongs to Ki. Therefore, Ki contains at most 1=(p + 1) part of all the points from
P.
6. The same inequality is true for each of the sets Ki . So, totally, p sets K1, ..., Kp contain
 p=(p + 1) of all the points from P . Since p=(p + 1) < 1, it means that there are points
from P (and hence from K ) that are not covered by any of the sets Ki . This contradicts
to our assumption that f is degenerate and K = [Ki .
This contradiction shows that our assumption was false, and so f is not degenerate.
Q.E.D.

Proof of Proposition 5.2
Let us show that if a real analytic function f (x1; :::; xn) does not coincide with a function
of less than n variables, then it is non-degenerate in the sense of Denition 5.2. Indeed, an
arbitrary real analytic function g(x1; :::; xn) has the following property (similar to complex
analytical functions): it is either identically equal to 0, or it is equal to 0 on a set of
(Lebesgue) measure 0 (i.e., it is dierent from 0 on a set of full measure) (see, e.g., [17]).
39

Also, for an arbitrary real analytic function, each of its derivatives is also real analytic.
In particular, the partial derivative @f=@x1 of the given function is real analytic. If it was
identically equal to 0, then f would not depend on x1 at all. Therefore, according to the
above-cited result, the set of points (x1 ; :::; xn) on which this derivative is equal to 0 is of
Lebesgue measure 0. Similarly, for each i = 2; :::; n, the set of all points (x1 ; :::; xn), at
which i?th partial derivative @f=@xi is equal to zero, is also of measure 0.
Therefore, the union of these n sets, i.e., the set of all points on which at least one of
the partial derivatives is dierent from 0, is also of measure 0 (as a union of measure-zero
(0)
sets). Therefore, there exists a point (x(0)
1 ; :::; xn ) that does not belong to this union. By
denition of the union it means that in this point, all n partial derivatives are dierent
from 0.
These derivatives are real analytic and therefore, continuous. Therefore, there exists
(0)
a spherical neighborhood M of this point (x(0)
1 ; :::; xn ) in which sign of each of the partial
(0)
derivatives coincides with its sign at the point (x(0)
1 ; :::; xn ). For those i for which this
sign is positive (@f=@xi > 0), f is strictly increasing in xi . For those i for which this sign
is negative (@f=@xi < 0), f is strictly decreasing in xi . So, for all points from a set M
with a non-empty interior, the function f is strictly monotonic in each variable. Therefore,
according to Proposition 5.1, this function f is non-degenerate. Q.E.D.

Proof of Theorem 5.1
1. Let us prove this theorem by reduction to a contradiction. Namely, we will assume that
there exists a non-degenerate function f and a computation scheme S that computes f and
that is precise for all (crisp) sets, and we will deduce a contradiction from this assumption.
To deduce this contradiction, we will use the following observation: According to our
denitions, the phrase \S is precise for all (crisp) sets" means that for every (crisp) set
X  Rn , the result RN of applying S to X coincides with f (X ).
In this proof, we will need the following two Lemmas:

LEMMA 4. Assume that a function f : K ! R (K  Rn ) is computed by a composition
scheme S , and X  K . Let's denote by RN the result of applying S to X . Then, f (X ) 
RN .

Proof of Lemma 4. This Lemma is similar to the Main Theorem of Interval Computa-

tions (see, e.g., [29]), and is proved similarly: namely, using induction, one can then prove
that for every i, ri (X )  Ri. For i = N , we get the desired result. Q.E.D.
40

LEMMA 5. Assume that k is an integer, f : R ! R, g : Rk ! R, and h : Rk ! R
is a composition of f and g (i.e., h(~x) = f (g(~x))). Then, for every (crisp) set X  Rk ,

h(X ) = f (g(X )).

Proof of Lemma 5 follows directly from the denitions. Q.E.D.
2. We will need the following auxiliary notion: by a complexity of a computation scheme
S = (Sn+1 ; :::; SN ), we will understand the number N .
We assumed that there exists a computation scheme that computes a non-degenerate
function of n variables and that is precise for all (crisp) sets. Out of all computation
schemes with this property, there exists a one with the smallest possible complexity N .
Let's choose one of these \simplest" schemes. In the following text, this chosen scheme
will be denoted by Ss (s stands for simplest). The corresponding function will be denoted
by fs .
Comment. A computation scheme Ss consists of the rules of the type ri := ci , ri := fi (rj ),
and ri := fi (rj ; rk ). In principle, the corresponding functions fi can be dened everywhere.
However, when we apply this computation scheme to compute the value of the function fs
that is dened on some set K , we will use only the values of fi for rj 2 ri (K ); or, for the
function of two variables, only the values for (rj ; rk ) 2 Djk , where

Djk = f(a; b) j 9~x (~x 2 K & rj (~x) = a & rk (~x) = bg:
Therefore, to simplify our proofs, we will assume in the following text that fi is dened
only for these values.
It is easy to check that if initially Ss computed fs and was precise for all (crisp) sets,
then after such a restriction on fi it still has the same properties.
3. Depending on what the nal step SN of Ss is, we have the following ve possibilities:
 N  n;
 N > n and rN := cN ;
 N > n and rN := fN (rj ), where j < N ;
 N > n and rN := fN (rj ; rk ), where j  n and k  n;
 N > n and rN := fN (rj ; rk ), where j > n or k > n.
In the following ve subsections, we will prove that in all these ve cases, we have a
contradiction with our initial assumption.
4. N  n.
41

In this case, fs (x1; :::; xn) = rN (x1; :::; xn) = xN . So, fs depends only on one of its
variables. Hence, fs is degenerate, which contradicts to the assumption that it is nondegenerate.
5. N > n and rN := cN .
In this case, fs (x1; :::; xn) = rN (x1; :::; xn) = cN does not depend on any variables at
all. Therefore, it is degenerate.
6. N > n and rN := fN (rj ), where j < N .
Let us prove (by considering all possible cases) that this case is impossible. Depending
on the case, we will prove it either directly, or by \merging" the last step with one of the
previous ones, and thus coming up with a new computation scheme that is simpler than
the scheme Ss that is by denition the simplest possible (so, we have a contradiction).
6.1. If j  n, then rj = xj , and fs = fN (rj ) = fN (xj ) is a function of one variable (namely,
xj ) and is thus degenerate, which contradicts to our choice of fs.
6.2. If j > n, and j ?th step is rj := cj , then fs = rN = fN (cj ) =const, i.e., fs is also
degenerate.
6.3. If j > n, and j ?th step is rj := fj (rk ) for some k < j , then rN = fN (fj (rk )). In
other words, rN = f~N (rk ), where by f~N , we denoted the composition of fN and fj .
6.3.1. If k  n, then fs is again a function of one variable xk (i.e., degenerate).
6.3.2. If k > n, then, we can replace the original computation scheme S with the
following simplied one: S~ = (Sn+1 ; :::; Sk ; S~N ), where by S~N , we denoted the
following step: rN := f~N (rk ). Because of our formulas, this scheme computes
exactly the same function fs . The fact that this scheme computes the same
function and is precise for all (crisp) sets, follows from Lemma 5.
Since we deleted at least one step (Sj ), this new scheme has a smaller complexity
that Ss . This contradicts to our choice of Ss as the computation scheme with
the smallest possible complexity that computes a non-degenerate function of 3 or
more variables and that is precise for all (crisp) sets.
6.4 If j > n, and j ?th step is rj := fj (rk ; rl ) for some k; l < j , then rN = fN (fj (rk ; rl)) =
f~N (rk ; rl ), where by f~N , we denoted a composition f~N (a; b) = fN (fj (a; b)). In this
case, we can also delete one step from the computation scheme and thus arrive at the
contradiction.
42

7. N > n and rN := fN (rj ; rk ), where j  n and k  n.
In this case, fs (x1; :::; xn) = rN (x1; :::; xn) = fN (xj ; xk ). Hence, fs depends on only
two of its variables, and is therefore degenerate.
8. N > n and rN := fN (rj ; rk ), where j > n or k > n.
8.1. We assumed that Ss computes fs , and that Ss is precise for all (crisp) sets. This
means, in particular, that for each input set X  K , RN = fs(X ). In particular, if we
take arbitrary two elements ~x 2 K and ~y 2 K , then this equality must be true for a 2-point
set X = f~x; ~yg.
8.2. For this choice of X , the right-hand side of the equality RN = fs (X ) (i.e., the
set fs(X )) is easy to describe: it consists of two values fs (~x) = fN (rj (~x); rk (~x)) and
fs(~y) = fN (rj (~y); rk (~y)).
8.3. Now, let's nd an element of the left-hand side. By denition of RN , this set is
equal to RN = fN (Rj ; Rk ). Due to Lemma 4, Rj  rj (X ), and Rk  rk (X ). Since
rj (~x) 2 rj (X ), we can thus conclude that rj (~x) 2 Rj . Similarly, we can conclude that
rk (~y) 2 Rk . Therefore, fN (rj (~x); rk (~y)) 2 fN (Rj ; Rk ) = RN .
Since RN = fs(X ), every element of RN must coincide with one of the two elements
of fs (X ). In particular, for the above-discovered element, it means the following:
For every ~x 2 K and ~y 2 K , fN (rj (~x); rk (~y)) is either equal to fN (rj (~x); rk (~x)), or it is
equal to fN (rj (~y); rk (~y)).

8.4. This statement enables us to make the following conclusion about the function
fN (a; b):
If we can nd ~x and ~y such that a = rj (~x) and b = rk (~y), then either fN (a; b) =
fN (a; rk (~x)), or fN (a; b) = fN (rj (~y); b).

8.5. For each a 2 rj (K ), there exists a vector ~x for which rj (~x) = a (it is possible that
several such vectors exist). For dierent vectors ~x, the value rk (~x) may also be dierent.
For each a, let us pick one of these values rk (~x) and denote it by gkj (a).
Similarly, for each b 2 rk (K ), we will pick a vector ~x with the property that rk (~x) = b,
and denote the value rj (~x) for thus picked ~x by gjk (b).
8.6. Using these denotations, we can reformulate the statement from 8.4 as follows:
43

For every a 2 rj (K ) and b 2 rk (K ), either fN (a; b) = fN (a; gkj (a)), or fN (a; b) =
fN (gjk (b); b).

If we denote h1 (a) = fN (a; gkj (a)) and h2 (b) = fN (gjk (b); b), then we can further
simplify this conclusion:
For every (a; b) 2 Djk , either fN (a; b) = h1 (a), or fN (a; b) = h2 (b).
8.7. In particular, this property is true if we choose an arbitrary ~x 2 K and take a = rj (~x)
and b = rk (~x).
Let us denote by K1 the set of all ~x 2 K for which fN (rj (~x); rk (~x)) = h1 (rj (~x)), and
by K2, the set of all values ~x 2 K for which fN (rj (~x); rk (~x)) = h2 (rk (~x)). Then, this
property means that K = K1 [ K2.
8.8. Since the function f dened on K is non-degenerate, its restriction to either K1 or
K2 is also non-degenerate.
Indeed, if it were not true, then we would be able to describe both K1 and K2 (and
hence, their union) as the union of nitely many subsets on which fs is degenerate. On
the other hand, we assumed that fs : K ! R is non-degenerate, which means that for K ,
such a representation is impossible.
8.9. Let us choose a set Ki (i.e., K1 or K2) for which the restriction of fs is non-degenerate.
For this set, we can form the restriction of fs and Ss . For this restriction, we can take
rN := h1 (rj ) (or rN := h2 (rk )) as a last step of the computation scheme (instead of the step
rN := fN (rj ; rk )), and the resulting computation scheme will still compute the restriction
of fs , and it will still precise for all (crisp) sets.
If we were able to compute a non-degenerate function fs by another computation
scheme S1 whose complexity is < N computation steps, then we would get a contradiction with our choice of N as the smallest possible complexity of a computation scheme
that computes a non-degerenate function. Therefore, the resulting computation scheme is
still the \simplest" in the sense that its complexity N is the smallest possible among all
computation schemes that compute non-degenerate functions.
So, we arrive at the situation where we have the \simplest" computation scheme, and
the function at the last step is a function of one variable; we have already proved (in part
6 of this proof) that such situation leads to a contradiction.
9. So, in all ve cases, we arrive at a contradiction. Hence, our initial assumption is
false, namely, the assumption that a non-degenerate function of 3 or more variables can
44

be computed by a computation scheme (with unary and binary operations only) that is
precise for all (crisp) sets. Q.E.D.

CONCLUSIONS
Theoretical. The main theoretical result of this paper is that functions of several interval

or fuzzy variables cannot always be computed precisely if we use only operations with one or
two interval (resp. fuzzy) variables. Moreover, for smooth functions f , we show that even
the main term in the result of fuzzy (interval) computations cannot always be computed
correctly. The accuracy of interval and fuzzy data processing drastically improves if we
add weighted scalar product to the list of elementary (hardware supported) operations.
For numerical operands, scalar product is already hardware supported in modern
computers: by math co-processors. There have been successful attempts to hardware
support scalar product of interval operands.

Practical. Therefore, our main practical recommendation is that for fuzzy data processing,

it is desirable to hardware support (weighted) scalar product of fuzzy operands as well (at
least, some operations with three or more fuzzy operands).

Acknowledgments. This work was partially supported by NSF grants No. CDA-9015006

and No. EEC-9322370, by a Grant No. PF90{018 from the General Services Administration (GSA), administered by the Materials Research Institute and the Institute for
Manufacturing and Materials Management, and by a NASA grant No. NAG 9-757. The
authors are greatly thankful to Paul Kainen, Baker Kearfott, Vera Kurkova, and Reza Langari for valuable discussions, and to the anonymous referees for their extremely valuable
suggestions and help.

45

REFERENCES
[1] O. Artbauer, \Application of interval, statistical, and fuzzy methods to the evaluation
of measurements", Metrologia, 1988, Vol. 25, pp. 81{86.
[2] H. Dhirf and D. Sarkar, \Fuzzy arithmetic on systolic arrays", Parallel Computing,
1993, Vol. 19, pp. 1283{1301.
[3] W. M. Dong, W. L. Chiang, H. C. Shah, \Fuzzy information processing in seismic
hazard analysis and decision making", International Journal of Soil Dynamics and
Earthquake Engineering, 1987, Vol. 6, No. 4., pp. 220{226.
[4] W. Dong and F. Wong, \Fuzzy weighted averages and implementation of the extension
principle", Fuzzy Sets and Systems, 1987, Vol. 21, pp. 183{199.
[5] D. Dubois and H. Prade. Fuzzy sets and systems: theory and applications, Academic
Press, N.Y., London, 1980.
[6] H. L. Frisch, C. Borzi, G. Ord, J. K. Percus, and G. O. Williams, \Approximate Representation of Functions of Several Variables in Terms of Functions of One Variable",
Physical Review Letters, 1989, Vol. 63, No. 9, pp. 927{929.
[7] R. Fuller and T. Keresztfalvi, \On generalization of Nguyen's theorem", Fuzzy Sets
and Systems, 1990, Vol. 4, pp. 371{374.
[8] W. A. Fuller, Measurement error models, J. Wiley & Sons, New York, 1987.
[9] A. A. Gaganov, \Computational complexity of the range of the polynomial in several
variables", Cybernetics, 1985, pp. 418{421.
[10] R. Hammer, M. Hocks, U. Kulisch, D. Ratz, Numerical toolbox for veried computing.
I. Basic numerical problems, Springer Verlag, Heidelberg, N.Y., 1993.
[11] E. R. Hansen, Global optimization using interval analysis, Marcel Dekker, N.Y., 1992.
[12] R. Hecht-Nielsen, \Kolmogorov's Mapping Neural Network Existence Theorem",
IEEE International Conference on Neural Networks, San Diego, SOS Printing, 1987,
Vol. 2, pp. 11{14.
[13] D. Hilbert, \Mathematical Problems, lecture delivered before the International
Congress of Mathematics in Paris in 1900", translated in Bull. Amer. Math, Soc.,
1902, Vol. 8, pp. 437{479.
[14] R. B. Kearfott and V. Kreinovich (eds.), Applications of Interval Computations,
Kluwer, Dordrecht, 1996.
[15] A. N. Kolmogorov, \On the Representation of Continuous Functions of Several Variables by Superposition of Continuous Functions of One Variable and Addition", Dokl.
Akad. Nauk SSSR, 1957, Vol. 114, pp. 369{373.
[16] G. Klir and B. Yuan, Fuzzy sets and fuzzy logic: theory and applications, Prentice
46

[17]

[18]
[19]

[20]

[21]
[22]

[23]
[24]
[25]
[26]
[27]

Hall, Upper Saddle River, NJ, 1995.
V. Kreinovich, \On the problem of recovering the ?function in non-relativistic quantum mechanics", Teoreticheskaya i Mathematicheskaya Fizika, 1976, Vol. 28, No. 1,
pp. 56{64 (in Russian); English translation: Theoretical and Mathematical Physics,
1976, Vol. 8, No. 7, pp. 56{64.
V. Kreinovich (ed.), Reliable Computing, 1995, Supplement (Extended Abstracts of
APIC'95: International Workshop on Applications of Interval Computations, El Paso,
TX, Febr. 23{25, 1995).
V. Kreinovich, Ching-Chuang Chang, L. Reznik, G. N. Solopchenko, \Inverse problems: fuzzy representation of uncertainty generates a regularization", Proceedings
of NAFIPS'92: North American Fuzzy Information Processing Society Conference,
Puerto Vallarta, Mexico, December 15{17, 1992, NASA Johnson Space Center, Houston, TX, 1992, Vol. II, pp. 418{426.
V. Kreinovich, C. Quintana, L. Reznik, Gaussian membership functions are most
adequate in representing uncertainty in measurements, In: Proceedings of NAFIPS'92:
North American Fuzzy Information Processing Society Conference, Puerto Vallarta,
Mexico, December 15{17, 1992, NASA Johnson Space Center, Houston, TX, 1992,
Vol. II, pp. 618{624.
V. Kreinovich et al, What non-linearity to choose? Mathematical foundations of fuzzy
control, Proceedings of the 1992 International Conference on Fuzzy Systems and Intelligent Control, Louisville, KY, 1992, pp. 349{412.
V. Kreinovich and L. K. Reznik, \Methods and models of formalizing a priori information (on the example of processing measurements results)", In: Analysis and
Formalization of Computer Experiments, Proceedings of the Mendeleev Metrology Institute, 1986, pp.37{41 (in Russian).
U. Kulisch, G. Bohlender, \Features of a hardware implementation of an optimal
arithmetic", In: U. Kulisch, W. L. Miranker (eds), A new approach to scientic computation, Academic Press, Orlando, FL, 1983, pp. 269{290.
V. Kurkova, \Kolmogorov's Theorem Is Relevant", Neural Computation, 1991, Vol.
3, pp. 617{622.
V. Kurkova, \Kolmogorov's Theorem and Multilayer Neural Networks", Neural Networks, 1992, Vol. 5, pp. 501{506.
G. G. Lorentz, \The 13-th problem of Hilbert", in: F. E. Browder (ed.), Mathematical
Developments Arizing from Hilbert's Problems, American Math. Society, Providence,
RI, 1976, Part 2, pp. 419{430.
R. E. Moore, Automatic error analysis in digital computation, Lockheed Missiles and
47

[28]
[29]
[30]
[31]
[32]
[33]
[34]
[35]
[36]
[37]

[38]
[39]
[40]
[41]
[42]
[43]

Space Co. Technical Report LMSD-48421, Palo Alto, CA, 1959.
R. E. Moore, C. T. Yang, Interval analysis, Lockheed Missiles and Space Co. Technical
Report LMSD-285875, Palo Alto, CA, 1959.
R. E. Moore, Methods and applications of interval analysis, SIAM, Philadelphia, 1979.
M. Nakamura, R. Mines, V. Kreinovich, \Guaranteed Intervals for Kolmogorov's Theorem (and Their Possible Relation to Neural Networks)", Interval Computations, 1993,
No. 3, pp. 183{199.
M. Ness, \Approximative versions of Kolmogorov's superposition theorem, proved
constructively", J. Comput. Appl. Math., 1993.
V. M. Nesterov, \Interval analogues of Hilbert's 13th problem", In: Abstracts of the
Int'l Conference Interval'94, St. Petersburg, Russia, March 7{10, 1994, 185{186.
H. T. Nguyen, \A note on the extension principle for fuzzy sets", J. Math. Anal. and
Appl., 1978, Vol. 64, pp. 359{380.
H. T. Nguyen and E. A. Walker, A First Course in Fuzzy Logic, CRC Press, Boca
Raton, Florida, 1996 (to appear).
S. Rabinovich, Measurement errors: theory and practice, American Institute of
Physics, N.Y., 1993.
M. J. Schulte and E. E. Swartzlander, Jr., \Parallel Hardware Designs for Correctly
Rounded Elementary Functions", Interval Computations, 1993, No. 4, pp. 65{88.
M. J. Schulte and E. E. Swartzlander, Jr., \Design and applications for variableprecision, interval arithmetic coprocessors", In: V. Kreinovich (ed.), Reliable Computing, 1995, Supplement (Extended Abstracts of APIC'95: International Workshop
on Applications of Interval Computations, El Paso, TX, Febr. 23{25, 1995), pp.
166{172.
S. M. Shah and R. Horvath, A hardware digital fuzzy inference engine using standard
integrated circuits, Information Sciences, 1994, Vol. 1, pp. 1{7.
G. N. Solopchenko, \Formal metrological components of measuring systems", Measurement, 1994, Vol. 13, pp. 1{12.
D. A. Sprecher, \On the Structure of Continuous Functions of Several Variables",
Transactions Amer. Math. Soc., 1965, Vol. 115, No. 3, pp. 340{355.
D. A. Sprecher, \An Improvement in the Superposition Theorem of Kolmogorov",
Journal of Mathematical Analysis and Applications, 1972, Vol. 38, pp. 208{213.
H. Surmann et al., \What kind of hardware is necessary for a fuzzy rule based system?", In: Proceedings of the FUZZ-IEEE'94 International Conference, Orlando, FL,
July 1994, Vol. 1, pp. 274{278.
M. Takahashi, E. Sanchez, R. Bartolin, J. P. Aurrand-Lions, E. Akaiwa, T. Yamakawa,
48

[44]
[45]
[46]
[47]
[48]
[49]
[50]
[51]
[52]
[53]
[54]

and J. R. Monties, \Biomedical applications of fuzzy logic controllers", In: Intl. Conference on Fuzzy Logic and Neural Networks, Iizuka, Fukuoka, Japan, 1990, pp. 553{
556.
M. Togai and S. Chiu, \A fuzzy accelerator and a programming environment for realtime fuzzy control", In: Second IFSA Congress, Tokyo, Japan, 1987, pp. 147{151.
M. Togai and H. Watanabe, \Expert systems on a chip: an engine for real-time
approximate reasoning", IEEE Experts Systems Magazine, 1986, No. 1, pp. 55{62.
M. J. Tretter, \Interval analysis isn`t fuzzy is it?", Abstracts for an International
Conference on Numerical Analysis with Automatic Result Verication: Mathematics,
Application and Software, February 25 { March 1, 1993, Lafayette, LA, 1993, p. 104.
H. M. Wadsworth, Jr. (editor), Handbook of statistical methods for engineers and
scientists, McGraw-Hill Publishing Co., N.Y., 1990.
H. Watanabe and W. Detlo, \Recongurable fuzzy logic processor: a full custom
digital VLSI", In: Intl. Workshop on Fuzzy Systems Applications, Iizuka, Japan,
1988, pp. 49{50.
T. Yamakawa, \Fuzzy microprocessors { rule chip and defuzzier chip", In: Intl.
Workshop on Fuzzy Systems Applications, Iizuka, Japan, 1988, pp. 51{52.
T. Yamakawa, \Intrinsic fuzzy electronic circuits for sixth generation computer", In:
M. M. Gupta and T. Yamakawa (eds.), Fuzzy Computing, Elsevier, 1988, pp. 157{181.
H. Q. Yang, H. Yao, and J. D. Jones, \Calculating functions of fuzzy numbers", Fuzzy
Sets and Systems, 1993, Vol. 55, pp. 273{283.
Y. Yoshikawa, T. Deguchi, and T. Yamakawa, \Exclusive fuzzy hardware systems for
the appraisal of orthodentic results", In: Intl. Conference on Fuzzy Logic and Neural
Networks, Iizuka, Fukuoka, Japan, 1990, pp. 939{942.
L. A. Zadeh, \Fuzzy sets", Information and control, 1965, Vol. 8, pp. 338{353.
L. A. Zadeh, Outline of a new approach to the analysis of complex systems and decision
processes, IEEE Transactions on Systems, Man and Cybernetics, 1973, Vol. 3, pp.
28{44.

49
The author has requested enhancement of the downloaded file. All in-text references underlined in blue are linked to publications on ResearchGate.

