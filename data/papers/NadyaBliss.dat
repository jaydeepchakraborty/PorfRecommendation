MOMENTS OF PARAMETER ESTIMATES FOR CHUNG-LU RANDOM GRAPH MODELS
Nicholas Arcolano1 , Karl Ni1 , Benjamin A. Miller1 , Nadya T. Bliss1 , and Patrick J. Wolfe2
1

MIT Lincoln Laboratory, Lexington, MA 02420, USA
{arcolano, karl.ni, bamiller, nt}@ll.mit.edu
2
Statistics and Information Sciences Laboratory, Harvard University, Cambridge, MA 02138, USA
patrick@seas.harvard.edu
ABSTRACT
As abstract representations of relational data, graphs and networks
Ô¨Ånd wide use in a variety of Ô¨Åelds, particularly when working in nonEuclidean spaces. Yet for graphs to be truly useful in in the context
of signal processing, one ultimately must have access to Ô¨Çexible and
tractable statistical models. One model currently in use is the ChungLu random graph model, in which edge probabilities are expressed
in terms of a given expected degree sequence. An advantage of this
model is that its parameters can be obtained via a simple, standard
estimator. Although this estimator is used frequently, its statistical
properties have not been fully studied. In this paper, we develop
a central limit theory for a simpliÔ¨Åed version of the Chung-Lu
parameter estimator. We then derive approximations for moments
of the general estimator using the delta method, and conÔ¨Årm the
effectiveness of these approximations through empirical examples.
Index Terms‚Äî graphs and networks, central limit theory, delta
method, given expected degree models, parameter estimation
1. INTRODUCTION
A graph is deÔ¨Åned as a set of nodes (or vertices) and edges,
where each edge indicates a relationship between a pair of nodes
(or a node and itself, if self-loops are allowed). The simplicity of
this deÔ¨Ånition, however, belies a mathematical object of immense
Ô¨Çexibility and expressive power. Today, graphs arise in a wide
variety of application domains, and graph analysis has become a
pervasive and critical area of research. (For an extensive review
of existing graph models, methods, and applications, we direct the
reader to [1, 2].)
Let G be an undirected, unweighted graph with n nodes. In
order to take full advantage of G as an abstract representation of
underlying data, one often will choose to characterize G in terms of a
statistical model. One of the simplest approaches is to assume G is a
realization of an ErdoÃãs-ReÃÅnyi random graph, where the existence of
each edge is determined by an independent Bernoulli trial with Ô¨Åxed
success probability p ‚àà (0, 1). Equivalently, if we deÔ¨Åne A = {aij }
to be the adjacency matrix of G, (i.e. A is a binary symmetric matrix
where aij = 1 if and only if there exists an edge between the i-th
and j-th nodes), the ErdoÃãs-ReÃÅnyi model assumes {aij : i ‚â• j} are
independent and identically distributed Bernoulli random variables
with parameter p.
This work is sponsored by the United States Air Force under contract
FA8721-05-C-0002. Opinions, interpretations, recommendations, and conclusions are those of the authors and are not necessarily endorsed by the
United States Government.

978-1-4673-0046-9/12/$26.00 ¬©2012 IEEE

3961

Unfortunately, though simple and tractable, the ErdoÃãs-ReÃÅnyi
model often is not practical for real-world applications.
For example,
n
consider the degrees of each node, given by di =
j=1 aij for
i = 1, . . . , n. Under the ErdoÃãs-ReÃÅnyi model, d1 , . . . , dn are
identically distributed binomial random variables, with expected
value E (di ) = np (assuming self-loops are allowed). Consequently,
this model is not effective at representing data whose corresponding
degree sequence is far from constant, such as any data that exhibits
power-law behavior.
One way to address this shortcoming is to deÔ¨Åne a set of node
weights {w1 , . . . , wn } where wi ‚àà (0, 1) for i = 1, . . . , n, and
assume that edges exist independently between pairs of nodes i and
j with probability pij = wi wj . This approach‚Äîwhich can be
viewed as a generalization of the ErdoÃãs-ReÃÅnyi model‚Äîis known as
the Chung-Lu model [3, 4]. Since its introduction, the Chung-Lu
model has proved useful in a number of settings, particularly in the
context of modularity theory, where it has been applied to problems
such as Ô¨Ånding community structure [5], graph partitioning [6], and
detection of dense subgraphs in large graphs [7].
As is true for any parametric statistical model, practical application of the Chung-Lu model requires either that we know the model
parameters a priori, or (more typically) that we can estimate them
given observed data. The standard estimator for the node weights is
wÃÇi = 
n

di

j=1 dj

1/2 ,

(1)

where d1 , . . . , dn denote the observed degrees of each node. Though
frequently used in practice [3‚Äì7], this estimator has not been well
studied, and its theoretical properties have received little attention.
In this paper, we investigate statistics of the estimator in (1),
including results for both Ô¨Ånite n and asymptotically as n ‚Üí ‚àû.
We begin by formally deÔ¨Åning the Chung-Lu model in Section 2.
In Section 3, we develop a central limit theory for a simpliÔ¨Åed
version of the estimator, and derive approximations for moments
of the general estimator using an approach known as the delta
method. Finally, we validate our theoretical results and study the
effectiveness of our approximations through a series of empirical
examples in Section 4, and close with a summary.
2. THE CHUNG-LU RANDOM GRAPH MODEL
Let G be an undirected, unweighted random graph with n nodes, and
let w = [w1 ¬∑ ¬∑ ¬∑ wn ]T be a vector of weights such that wi ‚àà (0, 1)
for i = 1, . . . , n. To simplify computation we allow G to have selfloops, although our results may be extended to the case where such
edges are prohibited.

ICASSP 2012

We say that G follows a Chung-Lu random graph model with n
nodes and parameter w, denoted G(n, w), if the presence or absence
of each edge is determined by an independent Bernoulli trial, with
the probability pij of an edge between the i-th and j-th nodes given
by pij = wi wj . Alternatively, if A = {aij } is the adjacency
matrix of G, then an equivalent deÔ¨Ånition is to say that G follows a
Chung-Lu model if {aij : i ‚â• j} are independent Bernoulli random
variables with corresponding parameters {pij }.
Let di denote the degree of the i-th node, for i = 1, . . . , n. A
basic property of the Chung-Lu model is that
E (di ) =

n


E (aij ) =

j=1

n


wi wj = wi w1 ,

3.1. Preliminaries
In addition to the expression for E (di ) given in (2), we will Ô¨Ånd it
necessary to compute second moments of the form E (di dj ) both
when i = j and i = j, as well as the variance var (di ) and the
covariance cov (di , dj ). When i = j, we have
‚é°


 n
‚é§
n


 2
aij
aij  ‚é¶
E di = E ‚é£
j  =1
n




j=1

j=1

j  =j

n


n 


 
E a2ij +
wi wj +

j=1

j=1

= wi w1 ‚àí wi2

wi2 wj wj 

j  =j

n


wi2

wj2 + wi2

w22

n




E (aij  ai j ) I i = i , j = j 

n


n




wi wj  wi wj I i = i , j = j 

n
n 


wj wj 

+

w21

,

wi wj 

i =1 j  =1

= wi wj ‚àí wi2 wj2 + wi wj w21 ,
where I (¬∑) denotes the indicator function (i.e. I (¬∑) = 1 when its
argument is true, and I (¬∑) = 0 otherwise). Finally, we have
 
var (di ) = E d2i ‚àí E2 (di ) = wi w1 ‚àí wi2 w22 ,
and
cov (di , dj ) = E (di dj ) ‚àí E (di ) E (dj ) = wi wj ‚àí wi2 wj2 .

3.2. Error distribution for a simpliÔ¨Åed estimator

n
To proceed, let us temporarily assume that w1 =
i=1 wi is
known, as this restricted setting will provide greater insight into the
behavior of the general estimator. In this case, we could obtain
a simpler estimator by replacing n
j=1 dj in (1) with its expected
value, yielding
di
di
di
wÃÇi =  
.
1/2 = 
1/2 =
n
w1
n
E
j=1 dj
j=1 E (dj )
Thus, when w1 is known the traditional estimator reduces to a
re-scaling of the observed degrees. This estimator is unbiased, as
E (wÃÇi ‚àí wi ) =

E (di )
‚àí wi = 0.
w1

Its mean squared error (MSE) is given by


 
MSE (wÃÇi ) = E (wÃÇi ‚àí wi )2 = E wÃÇi2 ‚àí wi2
=
where
E




wÃÇi2

=

 
E d2i
w21

=

w22
wi
‚àí wi2
,
w1
w21
2

w2
wi
‚àí wi2
+ wi2 ,
w1
w21

Œºj ‚â° E (aij ) = wi wj ,

j=1 j  =1

wi2

n
n 


= wi wj ‚àí wi2 wj2 + wi wj

from (3). Consequently, as long as w1 grows without bound as
n ‚Üí ‚àû (i.e. as long as w is not an absolutely summable sequence),
we will have MSE (wÃÇi ) ‚Üí 0 as n ‚Üí ‚àû.
We also can prove a central limit theorem (CLT) for this
estimator. Recall that for Ô¨Åxed i, {ai1 , . . . , ain } is a sequence of
independent Bernoulli random variables with corresponding probabilities {wi w1 , . . . , wi wn }. Next, deÔ¨Åne

E (aij aij  )

j=1

= wi w1 ‚àí

= wi wj +

p 1/p

For the remainder of the paper, we assume G is an observed random
graph generated according to the Chung-Lu model with n nodes
and weight vector w. We denote the adjacency matrix of G by the
n √ó n binary symmetric matrix A = {aij },
and denote the degree
sequence of G by {d1 , . . . , dn } where di = n
j=1 aij .
As a function of the degree sequence of a random graph,
the parameter estimator given by (1) is itself a random variable.
Accordingly, to determine the effectiveness of wÃÇ = [wÃÇ1 ¬∑ ¬∑ ¬∑ wÃÇn ]T
as an estimator, we will want to study its distribution for Ô¨Ånite n,
and determine whether it converges to the true weight vector w as
n ‚Üí ‚àû.

j=1

i =1
n


i =1 j  =1

(2)

3. STATISTICS OF CHUNG-LU PARAMETER ESTIMATES

=

 
= E a2ij +

j=1

where wp denotes the vector p-norm wp ‚â°
.
i=1 |wi |
Since the parameter vector w is proportional to the expected degree
sequence, the Chung-Lu model is also referred to as the ‚Äúgiven
expected degree‚Äù model [3].
As discussed in Section 1, in practice one rarely has prior
knowledge of w, and thus it must be estimated from data. In the
case of the Chung-Lu model, the estimator given in (1) is commonly
used to compute wÃÇ1 . . . , wÃÇn given a sequence of observed degrees
d1 , . . . , dn . Although this approach is used frequently in practice
for Ô¨Åtting the Chung-Lu model to empirical data, its characteristics
as a parameter estimator have yet to be properly studied. Thus, we
proceed by determining expressions for statistics of this estimator
and studying some of its asymptotic error properties.

=

j  =1

i =1 j  =1

n

n


and when i = j, we have
‚é°


 n
‚é§
n


aij 
ai j ‚é¶
E (di dj ) = E ‚é£

(3)

3962

œÉj2 ‚â° var (aij ) = wi wj ‚àí wi2 wj2 ,

and
s2n

‚â°

n


where gi denotes the i-th element of the gradient of g,
œÉj2

= wi w1 ‚àí

wi2

w22

= var (di ) .

gi (d) ‚â°

j=1

It is relatively straightforward to show that Lyapunov‚Äôs condition [8]
lim

n‚Üí‚àû

1

n


s2+Œ¥
n

j=1



E |aij ‚àí Œºj |

2+Œ¥



= 0,

(4)

holds for Œ¥ = 1, in particular by noting that 0 < wi wj < 1 implies


E |aij ‚àí Œºj |3 = (1 ‚àí wi wj )3 wi wj + (wi wj )3 (1 ‚àí wi wj )
‚â§ (1 ‚àí wi wj ) wi wj + (wi wj ) (1 ‚àí wi wj )
= 2 wi wj ‚àí 2 wi2 wj2 ,
and thus
n

1  
E |aij ‚àí Œºj |3 ‚â§
3
sn j=1

n
j=1

for i = 1, . . . , n. Using (7), we can approximate the expected value
of g (d) as


n


gi (Œæ) (di ‚àí Œæi ) = g (Œæ) , (8)
E (g (d)) ‚âà E g (Œæ) +
i=1

and the second moment of g (d) as







2

E g (d) ‚âà E

2 wi wj ‚àí 2 wi2 wj2
s3n

=

‚àÇ
g (d) ,
‚àÇdi

2
,
sn

g (Œæ) +

n


i=1
n 
n


= g 2 (Œæ) +

2 
gi

gi (Œæ) gj (Œæ) cov (di , dj ) .

i=1 j=1

which tends to zero as n ‚Üí ‚àû (again, as long as w1 grows
without bound). Consequently, we can apply the Lyapunov CLT [8],
which states that under the condition in (4),

Combining these two expressions, we can approximate the variance
of g (d) as
var (g (d)) ‚âà

n
di ‚àí wi w1
wÃÇi w1 ‚àí wi w1
1 
(aij ‚àí Œºj ) =
=
sn j=1
sn
sn

n
n 


gj (Œæ) gj (Œæ) cov (di , dj ) .

To obtain approximate moments of the Chung-Lu estimator, we
can apply the approximations in (8)‚Äì(9) using the deÔ¨Ånition of g
given in (6). For the expected value of wÃÇi , this yields
E (wÃÇi ) = E (g (d | i)) ‚âà g (Œæ | i)
= 


3.3. Approximate error statistics for general estimator

j=1

dj

n


Œæj

1/2 ,

(6)

gi (Œæ) (di ‚àí Œæi ) ,

(7)

i=1

3963

1/2

wi w1
wj w1

j=1

now denoted as a function g : Rn ‚Üí R of the observed degree vector
d = [d1 ¬∑ ¬∑ ¬∑ dn ]T (conditioned on the node index i). Computing
the distribution of this estimator is challenging, as g is nonlinear
function of a vector whose elements are neither independent nor
identically distributed. One potential approach would be to employ
the multivariate delta method [9], which involves formulating a
CLT through a Taylor series expansion of g (d). Unfortunately, this
approach requires us to begin with a multivariate CLT for d, which
is itself difÔ¨Åcult to establish, as the sequence {d1 , . . . , dn } is both
dependent and non-stationary.
Nevertheless, we can still use the Taylor series to obtain approximations of the mean and variance of g (d) under the ChungLu model. Furthermore, numerical examples suggest that d does
exhibit CLT-like behavior, and thus normal approximations using
these moments will tend to be quite good for large n. (Empirical
results will be discussed further in Section 4.)
We begin by expanding g as a multivariate Taylor series,
centered at the mean vector Œæ = E (d). We can approximate g (d)
by retaining only the Ô¨Årst-order terms, yielding
g (d) ‚âà g (Œæ) +

Œæi
n
j=1

= 
n

We now return to the standard Chung-Lu parameter estimator
di

(9)

i=1 j=1

converges in distribution to a standard normal random variable. In
other words, for sufÔ¨Åciently large n the distribution of wÃÇi can be
approximated arbitrarily well by the normal distribution



2
wi
2 w2
.
(5)
‚àí wi
N wi ,
w1
w21

wÃÇi = g (d | i) = 
n

(Œæ) (di ‚àí Œæi )

1/2

= wi .
where Œæi = E (di ) = wi w1 from (2). Thus, the standard ChungLu estimator is approximately unbiased for Ô¨Ånite n.
To compute the approximate variance of the estimator, we Ô¨Årst
need to calculate partial derivatives gj for j = 1, . . . , n. We have
gj

(d | i) =

‚éß
‚é®

n

(
‚é©‚àí

1

k=1

2(

dk )
di

n

k=1

1/2

dk )

‚àí
3/2

2(

n

di

k=1

dk )

3/2

,

,

i = j,
otherwise,

and thus

gj

(Œæ | i) =

wi
1
‚àí 2w
2,
w1
1
wi
‚àí 2w2 ,
1

i = j,
otherwise.

Next, we substitute these partial derivatives and the degree covariances from Section 3.1 into the approximation
var (wÃÇi ) ‚âà

n 
n


gj (Œæ | i) gk (Œæ | i) cov (dj , dk ) .

j=1 k=1

When evaluating this expression, it will be helpful to consider four
distinct cases: (a) i = j = k, (b) i = j = k, (c) i = j or i = k with

1

j=i

‚àí2


j=i

+

1
w1

  w
j=i k=i,j

‚àí

wk
2w2
1

j wk
4w4
1







wi
wi wj ‚àí wi2 wj2
2w2

f1 (w) =

f2 (w) =

Empirical density
Normal approximation

9
8
7
6
5
4
3
2
1
0
0

1

2

0.05

0.1

0.15

0.2

Estimate of w1

0.25

0.3

0.35

0.4

wj wk ‚àí wj2 wk .

Simplifying this expression, we obtain


wi2 w22 + 1
wi
‚àí
+ f1 (w) ‚àí f2 (w) , (10)
var (wÃÇi ) ‚âà
w1
w21
where

10

Probability density function

j = k, and (d) i = j, k with j = k. Partitioning the summation
according to these cases yields
2 


wi
1
var (wÃÇi ) ‚âà w
‚àí 2w
wi w1 ‚àí wi2 w22
2
1
1
  w 2 

j
+
wj w1 ‚àí wj2 w22
2w2





w33 1 + 4wi2 ‚àí 4 w22 wi ‚àí wi3 + 4wi3 ‚àí 4wi5
4 w31

w22 w44 ‚àí w42 + w63 + w44 ‚àí w66
4 w41

,

.

Fig. 1. Kernel density estimate and normal approximation for
distribution of wÃÇ1 .
for each of the 1000 generated graphs and a Gaussian kernel with
a bandwidth of œÉ = 0.01. Also plotted is a normal distribution
with mean 0.1629 and variance 0.0016. From the Ô¨Ågure, we see
that the normal distribution appears to provide a good Ô¨Åt to the
empirical distribution. This assertion is supported by noting that the
skewness and the excess kurtosis of the samples are low (0.2328 and
0.0398, respectively), as is the Kullback-Leibler divergence between
the empirical and normal distributions (with a value of 0.0076).
5. SUMMARY

Since this expression tends to zero with increasing w1 , we have
that the MSE of wÃÇi approximately tends to zero as n ‚Üí ‚àû.
A simpler approximation can be obtained by noting that as w1
grows large, f1 (w) and f2 (w) quickly become negligible. Ignoring
these terms, we obtain


wi2 w22 + 1
wi
‚àí
,
(11)
var (wÃÇi ) ‚âà
w1
w21
which is quite close to the variance obtained for the simpliÔ¨Åed
estimator given in (5), differing only by 1/ w21 . As we will
see in the next section, empirical results suggest that all three
variance expressions‚Äî(5), (10), and (11)‚Äîcan be used as effective
approximations of the true estimator variance.
4. EMPIRICAL RESULTS
We conclude by investigating the previously stated moment approximations through an empirical example. Let n = 1000, and let w
be a n-length vector whose elements are generated by independent
draws from the uniform
over (0, 0.2). Thus, we have
 distribution

E (wi ) = 0.1 and E w1 = 100. Given w Ô¨Åxed, we generated
1000 graphs according to the Chung-Lu model G (n, w), and for
each graph, we computed the set of Chung-Lu parameter estimates
according to (1), given its observed degree sequence.
Let wÃÇ1 denote the estimate of the Ô¨Årst node weight, which for
this example was w1 = 0.1629. Computed over the 1000 sample
estimates, the empirical mean of wÃÇ1 was 0.1617, and the empirical
variance was 0.001630. By comparison, the approximate variances
as computed by (10) and (11) were 0.001627 and 0.001628. These
values are close the variance of the simpliÔ¨Åed estimator given in (5),
which is equal to 0.001631.
In addition to having mean and variance consistent with our
approximations, the sample estimates appear to follow CLT-like behavior. Figure 1 shows the empirical distribution of wÃÇ1 , obtained by
constructing a kernel density estimate using the estimates computed

3964

In this paper, we explored statistical properties of a standard estimator used for determining the weight parameter of Chung-Lu random
graph models. In addition to developing a central limit theory for a
simpliÔ¨Åed version of the estimator, we derived approximations for
moments of the general estimator using the delta method. We also
illustrated through an empirical example that these approximations
can be effective in practice.
6. REFERENCES
[1] R. Albert and A.-L. BarabaÃÅsi, ‚ÄúStatistical mechanics of complex
networks,‚Äù Rev. Mod. Phys., vol. 74, no. 1, pp. 47‚Äì97, 2002.
[2] M. E. J. Newman, ‚ÄúThe structure and function of complex
networks,‚Äù SIAM Rev., vol. 45, no. 2, pp. 167‚Äì256, 2003.
[3] F. Chung and L. Lu, ‚ÄúConnected components in random graphs
with given expected degree sequences,‚Äù Ann. Comb., vol. 6, no.
2, pp. 125‚Äì145, 2002.
[4] F. Chung and L. Lu, Complex graphs and networks, American
Mathematical Society, 2006.
[5] M. E. J. Newman, ‚ÄúFinding community structure in networks
using the eigenvectors of matrices,‚Äù Phys. Rev. E, vol. 74, pp.
1‚Äì19, 2006.
[6] Y.-T. Chang, D. Pantazis, H. B. Hui, and R. M. Leahy, ‚ÄúStatistically optimal graph partition method based on modularity,‚Äù
in Proc. IEEE Int. Symp. on Biomed. Imaging, 2010, pp. 1193‚Äì
1196.
[7] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúToward signal
processing theory for graphs and non-Euclidean data,‚Äù in Proc.
ICASSP, 2010, pp. 5414‚Äì5417.
[8] P. Billingsley, Probability and measure, John Wiley & Sons,
1995.
[9] G. Casella and R. L. Berger, Statistical inference, Duxbury,
2002.

2015 IEEE International Conference on Data Mining

On the Connectivity of Multi-layered Networks:
Models, Measures and Optimal Control
Chen Chen, Jingrui He, Nadya Bliss and Hanghang Tong
Arizona State University
Tempe, Arizona 85287, USA
Email: {chen chen, jingrui.he, nadya.bliss, hanghang.tong}@asu.edu
so does the information network. In this application, the
different layers form a tree-structured dependency graph.
Compared with singlelayered networks, multilayered networks are even
more vulnerable to external
attacks analogous to the
ButterÔ¨Çy Effect in the
atmosphere system. That is,
even a small disturbance on
one supporting layer/network
might cause a ripple effect
to
all
the
dependent
layers,
leading
to
a
catastrophic/cascading
failure of the entire system.
In 2012, Hurricane Sandy
Fig. 1. A simpliÔ¨Åed example of
disabled several major power
multi-layered network.
generator facilities in the New York area, which not only
put tens of thousands of people in dark for a long time,
but also paralyzed the telecom network and caused a great
interruption on the transportation network. Therefore, it is of
key importance to identify crucial nodes in the supporting
layer/network, whose loss would lead to a catastrophic failure
of the entire system, so that counter measures can be taken
proactively.

Abstract‚ÄîNetworks appear naturally in many high-impact
real-world applications. In an increasingly connected and coupled
world, the networks arising from many application domains
are often collected from different channels, forming the socalled multi-layered networks, such as cyber-physical systems,
organization-level collaboration platforms, critical infrastructure
networks and many more. Compared with single-layered networks, multi-layered networks are more vulnerable as even a
small disturbance on one supporting layer/network might cause
a ripple effect to all the dependent layers, leading to a catastrophic/cascading failure of the entire system. The state-of-theart has been largely focusing on modeling and manipulating the
cascading effect of two-layered interdependent network systems
for some speciÔ¨Åc type of network connectivity measure.
This paper generalizes the challenge to multiple dimensions.
First, we propose a new data model for multi-layered networks
(M U L A N), which admits an arbitrary number of layers with a
much more Ô¨Çexible dependency structure among different layers,
beyond the current pair-wise dependency. Second, we unify a wide
range of classic network connectivity measures (S UB L INE). Third,
we show that for any connectivity measure in the S UB L INE family,
it enjoys the diminishing returns property which in turn lends
itself to a family of provable near-optimal control algorithms
with linear complexity. Finally, we conduct extensive empirical
evaluations on real network data, to validate the effectiveness of
the proposed algorithms.
Keywords‚Äîmulti-layered network; connectivity control;

I. I NTRODUCTION
Networks are ubiquitous and naturally appear in many
high-impact applications. Moreover, in a way reminiscent of the famous quote from Leonardo da Vinci1 , the networks arising from these application domains are often interconnected/interwined with each other, forming the so-called
multi-layered networks [3], [8], [16], [18]. Cyber-physical
systems are a classic example of multi-layered networks,
where the control layer controls the physical layer (e.g.,
power grid) through the communication layer (e.g., computer
networks); and in the meanwhile, the fully functioning of the
communication layers depends on the sufÔ¨Åcient power supply
from the physical layer. Here, these three interdependent layers
naturally form a line-structured dependency graph. Another example is the organization-level collaboration platforms
(Fig. 1), where the team network is supported by the social
network, connecting its employee pool, which further interacts
with the information network, linking to its knowledge base.
Furthermore, the social network layer could have an embedded
multi-layered structure (e.g., each of its layers represents a
different collaboration type among different individuals); and
1 ‚ÄúLearn

In response to such an imminent need, a recent trend in
multi-layered networks research community has been focusing
on modeling and manipulating the cascading effect of twolayered interdependent network systems [3], [15], [18], [19],
[8]. Although much progress has been made, several key
challenges have largely remained open. First (modeling), most,
if not all, of these existing work is devoted to two-layered
networks with a pair-wise dependency structure; and thus
it is not clear how to represent and model multiple (more
than two) layers with a more generic dependency structure.
Second (connectivity measures), there does not exist one
single network connectivity measure that is superior to all
other measures; but rather several connectivity measures are
prevalent in the literature (e.g., robustness, vulnerability, triangle counts). Each of the existing controlling algorithms on
multi-layered networks is tailored for one speciÔ¨Åc connectivity
measure. It is not clear if an algorithm designed for one speciÔ¨Åc
connectivity measure is still applicable to other measures. So
how can we design a generic control strategy that applies to
a variety of prevalent network connectivity measures? Third
(optimal control), an optimal control strategy tailored for twolayered networks might be sub-optimal, or even misleading to

how to see. Realize that everything is connected to everything else.‚Äù

1550-4786/15 $31.00 ¬© 2015 IEEE
DOI 10.1109/ICDM.2015.104

715

network G, where G(i, j) = 1 indicates layer j depends
on layer i (or layer i supports layer j), G(i, j) = 0 means
no direct dependency from layer i to layer j; (2) a set of
within-layer adjacency matrices A = {A1 , . . . , Ag }; (3) a
set of inter-layer node-node dependency matrices D, indexed
by pair (i, j), i, j ‚àà [1, . . . , g], such that for a pair (i, j),
if G(i, j) = 1, then D(i,j) is an ni √ó nj matrix; otherwise
D(i,j) = Œ¶ (i.e., an empty set); (4) Œ∏ is a one-to-one mapping function that maps each node in layer-layer dependency
network G to the corresponding within-layer adjacency matrix
Ai (i = 1, ..., g); (5) œï is another one-to-one mapping function
that maps each edge in G to the corresponding inter-layer
node-node dependency matrix D(i,j) . We deÔ¨Åne a multi-layered
network as a quintuple Œì =< G, A, D, Œ∏, œï >.
For simplicity, we restrict the within-layer adjacency matrices Ai to be simple (i.e., no self-loops), symmetric and
binary; and the extension to the weighted, asymmetric case
is straight-forward. In this paper, we require inter-layer dependency network G to be an un-weighted directed acyclic graph
(DAG). Notice that compared with the existing pair-wise twolayered model, it allows a much more Ô¨Çexible and complicated
dependency structure among different layers. For the interlayer node-node dependency matrix D(i,j) , D(i,j) (s, t) = 1
indicates that node s in layer i supports node t in layer j.

multi-layered networks, e.g., in case we want to simultaneously
optimize the connectivity of multiple layers by manipulating
one common supporting layer. On the theoretic side, the
optimality of the connectivity control problem of generic
multi-layered networks is largely unknown.
This paper aims to address all these challenges, and the
main contributions can be summarized as
‚Ä¢ New Data Models. We propose a novel multi-layered
network model (M U L A N), which admits an arbitrary
number of layers with a much more Ô¨Çexible node-level
dependency structure among different layers, beyond
the current pair-wise dependency (Section II).
‚Ä¢ Connectivity Measures. We unify a family of prevalent
network connectivity measures (S UB L INE), in close
relation to a variety of important network parameters
(e.g., epidemic threshold, network robustness, triangle
counting) (Section III).
‚Ä¢ Optimal Control. We show that for any network connectivity measure in the S UB L INE family, the optimal
connectivity control problem with the proposed M U L A N model enjoys the diminishing returns property,
which naturally lends itself to a family of provable
near-optimal control algorithms with linear complexity
(Section IV).
‚Ä¢ Empirical Evaluations. We perform extensive experiments based on real data sets to validate the effectiveness of the proposed algorithms. (Section V).
II. A N EW M ULTI - LAYERED N ETWORK M ODEL
In this section, we propose our new multi-layered network
model that admits an arbitrary number of layers with a more
generic dependency structure among different layers. We start
with the main symbols used throughout the paper ( Table I). We
use bold upper case letters for matrices (e.g., A, B), bold lower
case letters for column vectors (e.g., a, b) and calligraphic font
for sets (e.g., A, B). The transpose of a matrix is denoted with
a prime, i.e., A is the transpose of matrix A.
TABLE I.

Symbol
A, B
a, b
A, B
A(i, j)
A(i, :)
A(:, j)
A
G
A
D
Œ∏, œà
Œì
Si , T i , . . .
Si‚Üíj
N (Si )
m i , ni
Œª<A,j> , u<A,j>
ŒªA , uA
C(A)
IA (Si )
I(Si )

Fig. 2(a) presents an example of a four-layered network. In
this example, Layer 1 (e.g., the control layer) is the supporting
layer (i.e., the root node in the layer-layer dependency network
G). Layer 2 and Layer 3 directly depend on Layer 1 (e.g.,
one represents a communication layer by satellites and the
other represents another communication layer in landlines,
respectively), while Layer 4 (e.g., the physical layer) depends
on both communication layers (Layer 2 and Layer 3). The
abstracted layer-layer dependency network (G) is shown in
Fig. 2(b). A = {A1 , A2 , A3 , A4 } denotes the within-layer
adjacency matrices, each of which describes the network
topology in the corresponding layer. In this example, D is a set
of matrices containing only four non-empty matrices: D(1,2) ,
D(1,3) , D(2,4) , and D(3,4) . For example, D(3,4) describes the
node-node dependency between Layer 3 and Layer 4. The oneto-one mapping function Œ∏ maps node 1 (i.e., Layer 1) in G
to the within-layer adjacency matrix of Layer 1 (A1 ); and the
one-to-one mapping function œï maps edge < 3, 4 > in G to
the inter-layer node-node dependency matrix D(3,4) as shown
in Fig. 2(b).

M AIN S YMBOLS .

DeÔ¨Ånition and Description
the adjacency matrices (bold upper case)
column vectors (bold lower case)
sets (calligraphic)
the element at ith row j th column
in matrix A
the ith row of matrix A
the j th column of matrix A
transpose of matrix A
the layer-layer dependency matrix
networks at each layer of M U L A N
A = {A1 , . . . , Ag }
inter-layer node-node dependency matrices
one to one mapping functions
multi-layered network M U L A N
Œì =< G, A, D, Œ∏, œà >
node sets in layer Ai (calligraphic)
nodes in Aj that depend on nodes S in Ai
nodes and inter-layer links that depend on Si
number of edges and nodes in layer Ai
j th largest eigenvalue (in module) and its
corresponding eigenvector of network A
Ô¨Årst eigenvalue and eigenvector of network A
connectivity function of network A
impact of node set Si on network A
overall impact of node set Si to M U L A N

III. U NIFICATION OF C ONNECTIVITY M EASURES
In this section, we present a uniÔ¨Åed view for a variety of
prevalent network connectivity measures.
The key of our uniÔ¨Åed connectivity measure (referred to as
S UB L INE in this paper) is to view the connectivity of the entire
network as an aggregation over the connectivity measures of
its sub-networks (e.g., subgraphs), that is,

f (œÄ)
(1)
C(A) =
œÄ‚äÜA

where œÄ is a subgraph of A. The non-negative function f :
œÄ ‚Üí R+ maps any subgraph in A to a non-negative real
number and f (Œ¶) = 0 for empty set Œ¶. In other words, we
view the connectivity of the entire network (C(A)) as the sum
of the connectivity of all the subgraphs (f (œÄ)). Based on such
a connectivity deÔ¨Ånition, we further deÔ¨Åne the impact function

With the above notation, we introduce a new data model
for multi-layered networks as follows.
DeÔ¨Ånition 1. A Multi-layered Network Model (M U L A N).
Given (1) a binary g √ó g abstract layer-layer dependency
716

(b) The corrsponding layer-layer dependency network G

(a) A four-layered network
Fig. 2.

An illustrative example of M U L A N model

of a given set of nodes S as follows, where A\S is the residual
network after removing the set of nodes S from the original
network A.
I(S) = C(A) ‚àí C(A \ S)
(2)

attacked/deleted (e.g., shaded circle nodes), all the nodes from
Layer 2 and Layer 3 that are dependent on S (e.g., shaded
parallelogram and triangle nodes) will be disabled/deleted,
which will in turn cause the disfunction of the nodes in Layer
4 (e.g., shaded diamond nodes) that depend on these affected
nodes in Layer 2 or Layer 3. Our goal is to choose k nodes
from Layer 1 that have the maximal impact on both Layer 2
and Layer 4, i.e., to simultaneously decrease the connectivity
C(A2 ) and C(A4 ) as much as possible.
B. O PERA: Theory
In this subsection, we present the major theoretic results of
the optimal connectivity control problem (O PERA) on multilayered networks deÔ¨Åned in Problem 1. It says that for any
connectivity function C(A) in the S UB L INE family (eq. (1)),
for any multi-layered network in the M U L A N family (DeÔ¨Ånition 1), the optimal connectivity control problem (O PERA,
Problem 1) bears diminishing returns property.

Based on eq. (2), we can deÔ¨Åne the overall impact of node
set Si in Ai on the multi-layered network system as
I(Si ) =

g

j=1

Œ±j I(Si‚Üíj ) =

g


Œ±j (C(Aj ) ‚àí C(Aj \ Si‚Üíj ))

j=1

(3)
where Œ± = [Œ±1 , ..., Œ±g ] is a g √ó 1 non-negative weight vector
that assigns different weights to different layers in the system.
Si‚Üíj denotes the set of nodes in layer-j that depend on nodes
S in layer-i.
It turns out many prevalent network connectivity measures
can be interpreted from this perspective. Examples include
path capacity, loop capacity and triangle capacity. We omit
the detailed discussions due to space limit.
IV. O PTIMAL C ONNECTIVITY C ONTROL
In this section, we Ô¨Årst deÔ¨Åne the optimal connectivity control problem (O PERA) on the proposed multi-layered network
model (M U L A N); then unveil its major theoretic properties;
and Ô¨Ånally propose a generic algorithmic framework to solve
it.

Theorem 1. Diminishing Returns Property of M U L A N.
For any connectivity function C(A) in the S UB L INE family
(eq. (1)), for any multi-layered network in the M U L A N family
(DeÔ¨Ånition 1); the
overall impact of node set Sl in the control
g
layer l, I(Sl ) = i=1 Œ±i I(Sl‚Üíi ), is (a) monotonically nondecreasing; (b) sub-modular; and (c)normalized.
Proof: Omitted for space.
C. O PERA: Algorithms
In this subsection, we introduce our algorithm to solve
O PERA (Problem 1).

A. O PERA: Problem Statement
We formally deÔ¨Åne the optimal connectivity control problem (O PERA) on the proposed M U L A N model for multilayered networks as follows.
Given: (1) a multi-layered network Œì =< G, A, D, Œ∏, œà > (2)
a control layer Al , (3) an impact function I(.), and (4) an
integer k (budget);

A Generic Solution Framework. Finding out the global
optimal solution for Problem 1 by a brute-force method
would be computationally intractable, due to the exponential
enumeration. Nonetheless, the diminishing returns property of
the impact function I(.) (Theorem 1) immediately lends itself
to a greedy algorithm for solving O PERA with any arbitrary
connectivity function in the S UB L INE family and an arbitrary
member in the M U L A N family, summarized in Algorithm 1.

Output: a set of k nodes Sl from the control layer (Al ) such
that I(Sl ) (the overall impact of Sl ) is maximized.
In the above deÔ¨Ånition, the control layer Al indicates the
sources of the ‚Äòattack‚Äô; and the g √ó 1 vector Œ± indicates the
target layer(s) as well as their relative weights. For instance,
in Figure 2(a), we can choose Layer 1 as the control layer
(indicated by the strike sign); and set Œ± = [0 1 0 1 ] , which
means that both Layer 2 and Layer 4 are the target layers
(indicated by the star signs) with equal weights between them.
In this example, once a subset of nodes S in Layer 1 are

In Algorithm 1, Steps 2-4 calculate the impact score
I(v0 ) (v0 = 1, 2, ...) for each node in the control layer Al .
Step 5 selects the node with the maximum impact score.
In each iteration in Steps 7-19, we select one of the remaining (k ‚àí 1) nodes, which would make the maximum
marginal increase in terms of the current impact score (Step
12, margin(v0 ) = I(S ‚à™ {v0 }) ‚àí I(S)). In order to further
speed-up the computation, the algorithm admits an optional
lazy evaluation strategy (adopted from [11]) by activating an
optional ‚Äòif‚Äô condition in Step 11.

Problem 1. O PERA on M U L A N

717

nodes in target layer(s), and then trace back to its supporting
layer through the inter-layer dependency links (i.e., D). For
both strategies, we need a node importance measure. In our
evaluations, we compare three such measures, including (1)
node degree; (2) pagerank measure [13]; and (3) Netshield
values [21]. In addition, for comparison purposes, we also
randomly select nodes either from the control layer (for
the forward propagation strategy) or from the target layer(s)
(for the backward propagation strategy). Altogether, we have
eight baseline methods (four for each strategy, respectively),
including (1) ‚ÄòDegree-FP‚Äô, (2) ‚ÄòPageRank-FP‚Äô, (3) ‚ÄòNetshieldFP‚Äô, (4) ‚ÄòRand-FP‚Äô, (5) ‚ÄòDegree-BP‚Äô, (6) ‚ÄòPageRank-BP‚Äô, (7)
‚ÄòNetshield-BP‚Äô, (8) ‚ÄòRand-BP‚Äô.

We can show that Algorithm 1 leads to a near-optimal
solution with linear complexity, thanks to the diminishing
returns property in Theorem 1. We omit the detailed algorithm
analysis due to space limit.
Algorithm 1 O PERA: A Generic Solution Framework
Input: (1) A multi-layered network Œì, (2) a control layer Al ,
(3) an overall impact function I(Sl ) and (4) an integer k
Output: a set of k nodes S from the control layer Al .
1: initialize S to be empty
2: for each node v0 in layer Al do
3:
calculate margin(v0 ) ‚Üê I(v0 )
4: end for
5: Ô¨Ånd v = argmaxv0 margin(v0 ) and add v to S
6: set margin(v) ‚Üê ‚àí1
7: for i = 2 to k do
8:
set maxMargin ‚Üê ‚àí1
9:
for each node v0 in layer Al do
10:
/*an optional ‚Äòif‚Äô for lazy eval.*/
11:
if margin(v0 ) > maxMargin then
12:
calculate margin(v0 ) ‚Üê I(S ‚à™ {v0 }) ‚àí I(S)
13:
if margin(v0 ) > maxMargin then
14:
set maxMargin ‚Üê margin(v0 ) and v ‚Üê v0
15:
end if
16:
end if
17:
end for
18:
add v to S and set margin(v) ‚Üê ‚àí1
19: end for
20: return S
V. E XPERIMENTAL R ESULTS
In this section, we empirically evaluate the proposed
O PERA algorithms. All experiments are designed to show the
effectiveness of the proposed O PERA algorithms at optimizing
the connectivity measures (deÔ¨Åned in the proposed S UB L INE
family) of a multi-layered network (from the proposed M U L A N family).

O PERA Algorithms and Variants. We evaluate three prevalent network connectivity measures, including (1) the leading eigenvalue of the (within-layer) adjacency matrix, which
relates to the epidemic threshold of a variety of cascading
models; (2) the loop capacity (LC), which relates to the robustness of the network; and (3) the triangle capacity (TC), which
relates to the local connectivity of the network. As mentioned
in Section III, both the loop capacity and the triangle capacity
are members of the S UB L INE family. Strictly speaking, the
leading eigenvalue does not belong to the S UB L INE family.
Instead, it approximates the path capacity (PC), and the latter
(PC) is a member of the S UB L INE family. Correspondingly,
we have three instances of the proposed O PERA algorithm
(each corresponding to one speciÔ¨Åc connectivity measures) including ‚ÄòO PERA-PC‚Äô, ‚ÄòO PERA-LC‚Äô, and ‚ÄòO PERA-TC‚Äô. Recall
that there is an optional lazy evaluation step (Step 11) in the
proposed O PERA algorithm, thanks to the diminishing returns
property of the S UB L INE connectivity measures. When the
leading eigenvalue is chosen as the connectivity function, such
a diminishing returns property does not hold any more. To
address this issue, we introduce a variant of O PERA-PC as
follows. At each iteration, after the algorithm chooses a new
node v (Step 18, Algorithm 1), we (1) update the network by
removing all the nodes that depend on node v, and (2) update
the corresponding leading eigenvalues and eigenvectors. We
refer to this variant as ‚ÄòO PERA-PC-Up‚Äô. For each of the three
connectivity measures, we run all four O PERA algorithms.

A. Experimental Setup
Data Sets Summary. We perform the evaluations on three
application domains, including (D1) a multi-layered Internet
topology at the autonomous system level (M ULTI AS); and
(D2) critical infrastructure networks (I NFRA N ET). For each
application domain, we use real networks to construct the
within-layer networks (i.e., A in the M U L A N model) and
construct one or more inter-layer dependency based on real
application scenarios (i.e., G and D in the M U L A N model).
A summary of these data sets is shown in Table II. We will
present the detailed description of each application domain in
Subsection V-B.
TABLE II.
Data Sets
D1
D2

Application Domains
M ULTI AS
I NFRA N ET

Machines and Repeatability. All the experiments are performed on a machine with 2 processors Intel Xeon 3.5GHz
with 256GB of RAM. The algorithms are programmed with
MATLAB using single thread. All data sets used in this paper
are publicly available. Due to the space limit, we omit the
actual Ô¨Ågures for some experimental results. We will include
these additional results in an extended technical report.

DATA S ETS S UMMARY.
# of Layers
2‚àº4
3

# of Nodes
5,929‚àº24,539
19,235

B. Effectiveness Results
D1 - M ULTI AS. This data set contains the Internet topology
at the autonomous system level. The data set is available at
http://snap.stanford.edu/data/. It has 9 different network snapshots, with 633 ‚àº 13, 947 nodes and 1, 086 ‚àº 30, 584 edges.
In our evaluations, we treat these snapshots as the withinlayer adjacency matrices A. For a given supporting layer,
we generate the inter-layer node-node dependency matrices
D by randomly choosing 3 nodes from its dependent layer
as the direct dependents for each supporting node. For this
application domain, we have experimented with different layerlayer dependency structures (G), including a two-layered network, a three-layered line-structured network, a three-layered

# of Links
11,183‚àº50,778
46,926

Baseline Methods. To our best knowledge, there is no
existing method which can be directly applied to the connectivity optimization problem (Problem 1) of the proposed
M U L A N model. We generate the baseline methods using
two complementary strategies, including forward propagation
(‚ÄòFP‚Äô for short) and backward propagation (‚ÄòBP‚Äô for short).
The key idea behind the forward propagation strategy is that
an important node in control layer might have more impact
on its dependent networks as well. On the other hand, for
the backward propagation strategy, we Ô¨Årst identify important
718

Fig. 3. Evaluations on the M ULTI AS data set, with a four-layered diamond-shaped dependency network. The connectivity change vs. budget. Larger is better.
All the four instances of the proposed O PERA algorithm (in red) outperform the baseline methods.

and (b) multi-layered network analysis.
Network Connectivity Control. Connectivity is a fundamental property of networks, and has been a core research
theme in graph theory and mining for decades. Depending on
the speciÔ¨Åc applications, many network connectivity measures
have been proposed in the past. Examples include the size
of giant connected component (GCC), graph diameter, the
mixing time [9], the vulnerability measure [1], the epidemic
thresholds [4], the natural connectivity [10] and number of
triangles in the network, each of which often has its own,
different mathematical deÔ¨Ånitions.

tree-structured network and a four-layered diamond shaped
network. Figure 3 shows the results on the diamond shaped network. All the four instances of the proposed O PERA algorithm
perform better than the baseline methods. Among the baseline
methods, the backward propagation methods are better than
the forward propagation methods. This is because the length of
the back tracking path on the dependency network G (from the
target layer to the control layer) is short. Therefore compared
with other baseline methods, the node set returned from the
BP strategy is able to affect more important nodes in the target
layer. The results on the other dependent networks are similar
and omitted due to the space limit. In all these scenarios, the
proposed O PERA algorithms perform best consistently.
D2 - I NFRA N ET. This data set contains three types of
critical infrastructure networks, including (1) the power grid,
(2) the communication network; and (3) the airport networks. The power grid is an undirected, un-weighted network representing the topology of the Western States Power Grid of the United State [23]. It has 4,941 nodes and
6,594 edges. We use one snapshot from the M ULTI AS data set as the communication network with 11,461 nodes
and 32,730 edges. The airport network represents the internal US air trafÔ¨Åc lines between 2,649 airports and has
13,106 links (available at http://www.levmuchnik.net/Content/
Networks/NetworkData.html). We construct a triangle-shaped
layer-layer dependency network G (see the icon of Figure 4)
based on the following observation. The operation of an airport
depends on both the electricity provided by the power grid and
the Internet support provided by the communication network.
In the meanwhile, the full functioning of the communication
network depends on the support of power grid. We use the
similar strategy as M ULTI AS to generate the inter-layer nodenode dependency matrices D. The results are summarized in
Figure 4. Again, the proposed O PERA algorithms outperform
all the baseline methods. Similar to the M ULTI AS network,
the back tracking path from the airport layer to the power grid
layer is also very short. Therefore the backward propagation
strategies perform relatively better than other baseline methods.
In addition, we also change the density of the inter-layer
node-node dependency matrices and evaluate its impact on the
optimization results (detailed results are omitted for space).
We found that (1) across different dependency densities, the
proposed O PERA algorithms still outperform the baseline
methods; and (2) when the dependency density increases, the
algorithms lead to a larger decrease of the corresponding
connectivity measures with the same budget.
VI. R ELATED W ORK
In this section, we review the related work, which can be
categorized into two groups: (a) network connectivity control,

From algorithm‚Äôs perspective, network connectivity control
aims to optimize (e.g., maximize or minimize) the corresponding connectivity measure by manipulating the underlying
topology (e.g., add/remove nodes/links). Recent work tries
to solve this problem by collectively Ô¨Ånding a subset of nodes/links with the highest impact on the network connectivity
measure. For example, Tong et al. [21], [20] proposed both
node-level and edge-level manipulation strategies to optimize
the leading eigenvalue of the network, which is the key
network connectivity measure behind a variety of cascading
models. In [5], Chan et al. further generalized these strategies
to manipulate the network robustness measure through the
truncated loop capacity [10]. Another important aspect of
network connectivity control lies in the network dynamics.
Chen et al. in [6] proposed an efÔ¨Åcient online algorithm to
track some important network connectivity measures (e.g., the
leading eigenvalue, the robustness measure) on a temporal
dynamic network.
Multi-Layered Network Analysis. Multi-layered networks have been attracting a lot of research attention in recent
years. In [16] and [7], the authors presented an in-depth
introduction on the fundamental concepts of interdependent,
multi-layered networks as well as the key research challenges.
In a multi-layered network, the failure of a small number of
the nodes might lead to catastrophic damages on the entire
system as shown in [3] and [22]. In [3], [15], [19], [18], [8],
different types of two-layered interdependent networks were
thoroughly analyzed. In [7], Gao et al. analyzed the robustness
of multi-layered networks with star- and loop-shaped dependency structures. Similar to the robustness measures in [17],
most of the current works use the size of GCC (giant connected
component) in the network as the evaluation standard [14],
[12], [2]. Nonetheless, the Ô¨Åne-granulated connectivity details
might not be captured by the GCC measure.
VII. C ONCLUSION
In this paper, we study the connectivity control problem
on multi-layered networks (O PERA). Our main contributions
719

Fig. 4. Evaluations on the I NFRA N ET data set, with a three-layered triangle-shaped dependency network. The connectivity change vs. budget. Larger is better.
All the four instances of the proposed O PERA algorithm (in red) outperform the baseline methods.

are as follows. First, we propose a new data model for
multi-layered networks (M U L A N), which admits an arbitrary
number of layers with a much more Ô¨Çexible dependency
structure among different layers, beyond the current pair-wise
dependency. Second, we unify a family of prevalent network
connectivity measures (S UB L INE). Third, we show that for
any network connectivity measure in the S UB L INE family,
the optimal connectivity control problem with the proposed
M U L A N model enjoys the diminishing returns property, which
naturally lends itself to a family of provable near-optimal
control algorithms with linear complexity. Finally, we conduct extensive empirical evaluations on real network data, to
validate the effectiveness of the proposed algorithms. In the
future, we plan to generalize M U L A N to an arbitrary layerlayer dependency network as well as the dynamic setting.
ACKNOWLEDGEMENT
This material is supported by the National Science Foundation under Grant No. IIS1017415, by the Army Research
Laboratory under Cooperative Agreement Number W911NF09-2-0053, by Defense Advanced Research Projects Agency
(DARPA) under Contract Number W911NF-11-C-0200 and
W911NF-12-C-0028, by National Institutes of Health under
the grant number R01LM011986, Region II University Transportation Center under the project number 49997-33 25.

[7]

J. Gao, S. V. Buldyrev, S. Havlin, and H. E. Stanley. Robustness of a
network of networks. Physical Review Letters, 107(19):195701, 2011.
[8] J. Gao, S. V. Buldyrev, H. E. Stanley, and S. Havlin. Networks formed
from interdependent networks. Nature physics, 8(1):40‚Äì48, 2012.
[9] M. Jerrum and A. Sinclair. Conductance and the rapid mixing property
for markov chains: the approximation of permanent resolved. In
Proceedings of the twentieth annual ACM symposium on Theory of
computing, pages 235‚Äì244. ACM, 1988.
[10] W. Jun, M. Barahona, T. Yue-Jin, and D. Hong-Zhong. Natural connectivity of complex networks. Chinese Physics Letters, 27(7):078902,
2010.
[11] J. Leskovec, A. Krause, C. Guestrin, C. Faloutsos, J. VanBriesen,
and N. Glance. Cost-effective outbreak detection in networks. In
Proceedings of the 13th ACM SIGKDD international conference on
Knowledge discovery and data mining, pages 420‚Äì429. ACM, 2007.
[12] D. T. Nguyen, Y. Shen, and M. T. Thai. Detecting critical nodes
in interdependent power networks for vulnerability assessment. IEEE
Trans. Smart Grid, 4(1):151‚Äì159, 2013.
[13] L. Page, S. Brin, R. Motwani, and T. Winograd. The PageRank
citation ranking: Bringing order to the web. Technical report, Stanford
Digital Library Technologies Project, 1998. Paper SIDL-WP-1999-0120
(version of 11/11/1999).
[14] M. Parandehgheibi and E. Modiano. Robustness of interdependent
networks: The case of communication networks and the power grid. In
Global Communications Conference (GLOBECOM), 2013 IEEE, pages
2164‚Äì2169. IEEE, 2013.
[15] R. Parshani, S. V. Buldyrev, and S. Havlin. Interdependent networks:
Reducing the coupling strength leads to a change from a Ô¨Årst to second
order percolation transition. Physical review letters, 105(4):048701,
2010.
[16] S. M. Rinaldi, J. P. Peerenboom, and T. K. Kelly. Identifying,
understanding, and analyzing critical infrastructure interdependencies.
Control Systems, IEEE, 21(6):11‚Äì25, 2001.
[17] C. M. Schneider, A. A. Moreira, J. S. Andrade, S. Havlin, and H. J.
Herrmann. Mitigation of malicious attacks on networks. Proceedings
of the National Academy of Sciences, 108(10):3838‚Äì3841, 2011.
[18] A. Sen, A. Mazumder, J. Banerjee, A. Das, and R. Compton. Multilayered network using a new model of interdependency. arXiv preprint
arXiv:1401.1783, 2014.
[19] J. Shao, S. V. Buldyrev, S. Havlin, and H. E. Stanley. Cascade of
failures in coupled network systems with multiple support-dependent
relations. arXiv preprint arXiv:1011.0234, 2010.
[20] H. Tong, B. A. Prakash, T. Eliassi-Rad, M. Faloutsos, and C. Faloutsos.
Gelling, and melting, large graphs by edge manipulation. In Proceedings
of the 21st ACM international conference on Information and knowledge
management, pages 245‚Äì254. ACM, 2012.
[21] H. Tong, B. A. Prakash, C. Tsourakakis, T. Eliassi-Rad, C. Faloutsos,
and D. H. Chau. On the vulnerability of large graphs. In Data Mining
(ICDM), 2010 IEEE 10th International Conference on, pages 1091‚Äì
1096. IEEE, 2010.
[22] A. Vespignani. Complex networks: The fragility of interdependency.
Nature, 464(7291):984‚Äì985, 2010.
[23] D. J. Watts and S. H. Strogatz. Collective dynamics of smallworldnetworks. nature, 393(6684):440‚Äì442, 1998.

The content of the information in this document does not
necessarily reÔ¨Çect the position or the policy of the Government,
and no ofÔ¨Åcial endorsement should be inferred. The U.S. Government is authorized to reproduce and distribute reprints for
Government purposes notwithstanding any copyright notation
here on.
R EFERENCES
[1]
[2]

[3]

[4]

[5]

[6]

R. Albert, H. Jeong, and A.-L. BarabaÃÅsi. Error and attack tolerance of
complex networks. Nature, 406(6794):378‚Äì382, 2000.
A. Bernstein, D. Bienstock, D. Hay, M. Uzunoglu, and G. Zussman.
Power grid vulnerability to geographically correlated failuresanalysis
and control implications. In INFOCOM, 2014 Proceedings IEEE, pages
2634‚Äì2642. IEEE, 2014.
S. V. Buldyrev, R. Parshani, G. Paul, H. E. Stanley, and S. Havlin.
Catastrophic cascade of failures in interdependent networks. Nature,
464(7291):1025‚Äì1028, 2010.
D. Chakrabarti, Y. Wang, C. Wang, J. Leskovec, and C. Faloutsos. Epidemic thresholds in real networks. ACM Transactions on Information
and System Security (TISSEC), 10(4):1, 2008.
H. Chan, L. Akoglu, and H. Tong. Make it or break it: manipulating robustness in large networks. In Proceedings of 2014 SIAM International
Conference on Data Mining, pages 325‚Äì333. SIAM, 2014.
C. Chen and H. Tong. Fast eigen-functions tracking on dynamic graphs.
In Proceedings of the 2015 SIAM International Conference on Data
Mining. SIAM, 2015.

720

Spectral Anomaly Detection in Very Large Graphs
Models, Noise, and Computational Complexity
Benjamin A. Miller, Nicholas Arcolano‚àó, Michael M. Wolf‚Ä†, and Nadya T. Bliss‚Ä°

arXiv:1412.4411v1 [cs.SI] 14 Dec 2014

MIT Lincoln Laboratory
bamiller@ll.mit.edu
Data analysis tasks considering entities and their connections and interactions are inherent in numerous
application domains. For example, computer security applications may consider communication between
machines to determine abnormal behavior in the network [6], and medical imaging may analyze the functional and anatomical connectivity between brain regions to determine the regions affected by a neurological
condition [12]. These tasks are naturally formulated as graph analysis problems, and in applications as
varied as biology, sociology, and network security, one technical problem of interest is the detection of a
small subregion of the graph in which the connections are significantly different than expected under normal
conditions.
Signal processing for graphs (SPG) is a recent technical effort to address this problem in an applicationagnostic setting [9]. SPG provides a statistical framework addressing the subgraph detection problem in the
context of traditional detection theory, where the objective is to resolve a binary hypothesis test. Under the
null hypothesis, the observed data‚Äîi.e., an observed graph‚Äîis drawn from a ‚Äúnoise‚Äù distribution representing
typical activity, whereas under the alternative hypothesis the graph also contains a subset of vertices whose
connectivity significantly deviates from the expectation. Inspired by modularity analysis [11], the framework
is based on analysis of graph residuals‚Äîthe difference between the observed and expected topology of the
graph‚Äîand is designed in a modular way to enable a variety of models and techniques. The processing chain
includes (1) fitting (possibly dynamic) graph data to an expected connectivity model, (2) computing the
graph residuals (possibly aggregating over time), (3) projection into a low-dimensional space, (4) computation
of a detection statistic to determine the presence of an anomaly, and (5) identification of the vertices (entities)
exhibiting the anomalous behavior. Specific attention is paid to spectral methods [8], as they provide natural
metrics for signal and noise power. Thus, the methods used here focus on computation and analysis of the
principal eigenvectors of A ‚àí E [A], where A is the adjacency matrix of the graph.
A recent series of studies aimed to expand the basis of the framework in three technical dimensions. One
area of interest was the development of techniques that incorporate graph dynamics and attributes into the
residuals analysis, in a way that scales well to very large graphs. As graphs of interest are often obtained
through noisy or unreliable sources, another important issue is the impact of uncertainty and corruption
on subgraph detection performance. Finally, since many application areas consider graphs that will not fit
into memory on a desktop computer, understanding how high-performance computing technologies can best
assist large-scale graph residuals analysis is an important aspect of this work. This abstract provides a brief
summary outlining results with respect to each of these technical challenges.
To integrate attributes into the residuals analysis framework, we use a generalized linear model for edge
probabilities, based on the vertex metadata [10]. We formulate the model of the probability of an edge
occurring from vertex i to vertex j as

pij = g xTi Œ≤1 + xTj Œ≤2 + xTij Œ≤3 ,
(1)
where g is a link function, xi and xj represent attribute vectors for the vertices, xij is a feature vector
for the pair, and the Œ≤ vectors appropriately weight the attributes and are learned from the data. For g,
‚àí1
we use the logistic function g(x) = (1 + exp (‚àíx)) . This presents a problem for spectral analysis when
graphs become very large: A will typically be sparse, but E [A] will be dense. In a restricted setting and
assuming that probabilities are low, however, we can leverage a structure that will allow fast matrix-vector
‚àó Currently

at FitnessKeeper, Inc.: nicholas.arcolano@runkeeper.com
at Sandia National Laboratories: mmwolf@sandia.gov
‚Ä° Currently at Arizona State University: nadya.bliss@asu.edu
‚Ä† Currently

Fused Detection Performance

Detection with 20% Edge Errors

1

0.8
0.6
0.4
0.2
45% Dense, With Attributes
75% Dense, No Attributes

0
0

0.2
0.4
0.6
0.8
1
Probability of False Alarm

Probability of Detection

1
Probability of Detection

Probability of Detection

1

0.8
0.6
0.4

Uniform Corruption
Degree‚àíBased Corruption
Similarity‚àíBased Errors
Random Subgraph
Uniform Deletion
Snowball Sampling

0.2
0
‚àí4

10

‚àí3

‚àí2

‚àí1

0.8
0.6
Degree‚àíBased Corruption
Uniform Deletion
Latent Graph
Fusion w/ ER Prior
Weighted Sum Fusion

0.4
0.2
0

0

10
10
10
10
Probability of False Alarm

‚àí4

10

‚àí3

‚àí2

‚àí1

0

10
10
10
10
Probability of False Alarm

Figure 1: Detection performance with attributes and uncertainty. (left) Subgraph detection performance
is markedly improved when attributes are considered in the expected value model. (center) Different error
mechanisms yield substantially different results with respect to operating on the true graph (black dashed
line). (right) Performance can be recovered using either a Bayesian method or by weighting the measurements
from different uncertainty mechanisms.
multiplications, thus enabling efficient computation of the principal residuals space. First, if we approximate
the logistic function as an exponential function (a reasonable approximation for small probabilities), the
edge probability becomes the product of three terms: one based entirely on the source vertex, one on the
destination vertex, and one on the properties of the pair. If we also restrict the vertex-pair attributes to
pairs of vertex categories (e.g., a math paper cites a physics paper), then the probability matrix E [A] will
have a low-rank structure that can be exploited to achieve fast matrix-vector multiplications. This will have
the same structure as the degree-corrected stochastic blockmodel [15]. This model also yields an efficient
parameter estimation procedure based on moment matching.
For models of uncertainty and corruption, we considered recent experience with real datasets as well as
those from the open literature [5]. To model issues such as sensor dropouts, we remove edges uniformly
at random. To model sensor noise, we corrupt the graph with random edge addition and removal. In this
situation, we consider each pair of edges and, with some probability, either remove the edge if it is present
or add it if it is absent. The probability may either be uniform over the graph or based on the degrees of
the vertices. Since many graphs are samplings of a population, we consider two vertex-sampling methods:
selecting a random subset of vertices and the edges within that subset, and a ‚Äúsnowball‚Äù sampling method
in which a random set of seed vertices is chosen and the observed graph is determined by following the
links of the currently observed vertices with some probability. With attributed graphs, nodes with similar
attributes can sometimes be mistaken for one another, so we consider a model in which each vertex has a
feature vector, and the distance between two vertices‚Äô vectors determines the probability that a given edge
connects to one when the intention is to connect to the other.
The impact of data attributes and data corruption are demonstrated in the plots in Figure 1. On the left,
a 15-vertex subgraph that densifies and disperses over time is possibly embedded into a background with a
skewed degree distribution and 3 categories for each vertex, and the objective is to determine whether the
embedding occurred. The background is drawn independently at random using the same model parameters
at each of 8 time samples. Full experimental details are provided in [10]. As demonstrated by the receiver
operating characteristic (ROC) curves, when the subgraph reaches 75% density before dispersing, but attributes are not considered, performance is not much better than chance. However, if the vertex attributes
are taken into account, we achieve near-perfect performance even when the subgraph is much smaller. To
quantify the impact of uncertainty and corruption, each of the uncertainty mechanisms defined previously
were applied to the specific problem of detecting a 12-vertex, 85%-dense subgraph in a 1024-vertex R-MAT
background [3] with average degree of about 10. Normalizing each method so that the edge errors (i.e.,
number of missing edges plus number of incorrect edges, divided by the number of edges in the true graph)
is 20%, the impact of these mechanisms is provided in the center plot. A detailed analysis of 5 of the 6
mechanisms (all except snowball sampling) is given in [7]. Uniform corruption actually slightly improves
performance over using the true graph, since it makes the background more similar to the assumed model.
Degree-based corruption adds noise that is correlated with the random background fluctuations, thus reducing performance. Similarity-based errors, as implemented here where all vertices are given a random

Time%to%Par55on%and%Compute%SpMV%opera5ons%
1.00E+05&
2D&random&

4.5E%10'

2D'random'

2D&hypergraph&

Time%(s)%

1.00E+03&

1.00E+02&

~40,000 SpMVs

1.00E+01&

1.00E+00&

R-Mat, 223 vertices
1024 cores

1.00E%01&

SpMV Time / |ei| (seconds)

1.00E+04&

3.5E%10'

3.0E%10'

2.5E%10'

2.0E%10'

1.5E%10'

1&

10&

100&

1000&

10000&

Number%of%SpMV%Opera5ons%

100000&

1000000&

2D'Hypergraph'

4.0E%10'

0.330'

40'
0.4

50'
0.5

60'
0.6

70'
0.7

79'
0.8

89'
0.9

99'
1.0

Fraction of total edges (|ei|/|en|)

Figure 2: Impact of partitioning with reduced data. (left) In order to make hypergraph partitioning worth
its computational cost, about 40,000 matrix-vector multiplications must be performed. (right) Even when
partitioning is done with only 30% of edges are available, the hypergraph partitioning method provides a
significant speedup over random partitioning.
3-dimensional vector in the unit hypercube, has a slight ‚Äúwhitening‚Äù effect that degrades performance less
than in the case of random vertex and edge removal. Snowball sampling yields the worst subgraph detection
performance, as it is highly biased by the seed vertices. However, as shown in the righthand plot, performance
can be recovered using multiple corrupt sources. In this example, edge deletion and degree-based corruption
are fused to improve performance. In a situation where there is knowledge of the uncertainty mechanism, a
Bayesian approach can be used to actually achieve better performance than on the latent graph, since the
algorithm is given knowledge of the expected number of edges. If this information is not available, using a
weighted sum of the individual adjacency matrices still provides a substantial increase in detection power,
improving the false alarm probability at an 80% detection rate by about 2 orders of magnitude.
To apply these methods to large-scale graphs, we require the ability to efficiently compute the eigenvectors
of residuals matrices for large-scale graphs, which necessitates methods for appropriately dividing the data
among many processes. Recent methods for 2D partitioning have shown promise for large graphs with
skewed degree distributions [1], and even randomly partitioning in such a way allows computation of the
top eigenvectors of multi-billion-vertex graphs in minutes [14]. A data-dependent hypergraph partitioning
algorithm [4] has potential to provide a substantial performance gain in this context. This method can speed
up eigenvector computation, possibly by a significant factor, but it is an expensive procedure and its cost
must be amortized in order to be effective. This is demonstrated on the left in Figure 2. Implementing the
eigensolver in Anasazi [2], we computed the time required to partition the graph‚Äîeither randomly or using
the hypergraph method‚Äîand perform a number of matrix-vector multiplications for the residuals matrix of
an R-MAT graph with 223 vertices and an average degree of 8. It requires approximately 40,000 matrixvector multiplications to make the cost of performing the more sophisticated partitioning method worth the
cost over random partitioning [13]. In a dynamic setting, however, if a partition remains valid over time, it
may be possible to recover the cost by reusing at several time instances, and preliminary results suggest that
this may be the case. As an experiment, in the process of generating an R-MAT graph, we partitioned the
vertices after only 30% of the edges have been added. This simulates the process of partitioning a network
while it is growing. As shown on the right in Figure 2, a substantial gap between the running time of
the matrix-vector multiplication is maintained as edges are added. When all of the edges in the graph are
present, the multiplication performed on the randomly partitioned graph takes about 40% longer than on
the graph using the hypergraph partitioning method. This opens up an interesting line of research for static
graphs as well: If a large graph can be sampled such that hypergraph partitioning still substantially improves
performance at lower cost, it will become a more attractive option for parallel big data analysis. Future
work will focus on the area of large graph sampling, both for effective partitioning in a parallel computing
context and within resource-constrained data analysis environments.

Acknowledgments
This work is sponsored by the Intelligence Advanced Research Projects Activity (IARPA) under Air Force
Contract FA8721-05-C-0002. The U.S. Government is authorized to reproduce and distribute reprints for

Governmental purposes notwithstanding any copyright annotation thereon. Disclaimer: The views and
conclusions contained herein are those of the author and should not be interpreted as necessarily representing
the official policies or endorsements, either expressed or implied, of IARPA or the U.S. Government.
This research used resources of the National Energy Research Scientific Computing Center, which is supported by the Office of Science of the U.S. Department of Energy under Contract No. DE-AC02-05CH11231.

References
[1] A. Yoo et al. A scalable eigensolver for large scale-free graphs using 2D graph partitioning. In Proc.
Supercomputing, pages 63(1‚Äì11), 2011.
[2] C. G. Baker et al. Anasazi software for the numerical solution of large-scale eigenvalue problems. ACM
Trans. Math. Softw., 36(3):13(1‚Äì23), July 2009.
[3] Deepayan Chakrabarti, Yiping Zhan, and Christos Faloutsos. R-MAT: A recursive model for graph
mining. In Proc. SIAM Int. Conf. Data Mining, pages 442‚Äì446, 2004.
[4] E. G. Boman et al. Scalable matrix computations on large scale-free graphs using 2D graph partitioning.
In Proc. Supercomputing, pages 50(1‚Äì12), 2013.
[5] Mark S. Handcock and Krista J. Gile. Modeling social networks from sampled data. Ann. Appl. Stat.,
4(1):5‚Äì25, 2010.
[6] Tsuyoshi IdeÃÅ and Hisashi Kashima. Eigenspace-based anomaly detection in computer systems. In Proc.
ACM Int. Conf. Knowledge Discovery and Data Mining, pages 440‚Äì449, 2004.
[7] B. A. Miller and N. Arcolano. Spectral subgraph detection with corrupt observations. In Proc. IEEE
Int. Conf. Acoust., Speech and Signal Process., pages 3449‚Äì3453, 2014.
[8] B. A. Miller, M. S. Beard, P. J. Wolfe, and N. T. Bliss. A spectral framework for anomalous subgraph
detection. Preprint: arXiv:1401.7702, 2014.
[9] B. A. Miller, N. T. Bliss, P. J. Wolfe, and M. S. Beard. Detection theory for graphs. Lincoln Laboratory
J., 20(1), 2013.
[10] Benjamin A. Miller, Nicholas Arcolano, and Nadya T. Bliss. Efficient anomaly detection in dynamic,
attributed graphs. In Proc. IEEE Intelligence and Security Informatics, pages 179‚Äì184, 2013.
[11] M. E. J. Newman. Finding community structure in networks using the eigenvectors of matrices. Phys.
Rev. E, 74(3), 2006.
[12] A. Venkataraman, M. Kubicki, and P. Golland. From connectivity models to region labels: Identifying
foci of a neurological disorder. IEEE Trans. Med. Imag., 32:2078‚Äì2098, November 2013.
[13] M. M. Wolf and B. A. Miller. Detecting anomalies in very large graphs. SIAM Workshop Combinatorial
Scientific Computing, pages 43‚Äì44, 2014.
[14] M. M. Wolf and B. A. Miller. Sparse matrix partitioning for parallel eigenanalysis of large static and
dynamic graphs. In Proc. IEEE High Performance Extreme Computing Conf., 2014.
[15] X. Zhang, R. R. Nadakuditi, and M. E. J. Newman. Spectra of random graphs with community structure
and arbitrary degrees. Phys. Rev. E, 89:042816, April 2014.

J. Parallel Distrib. Comput. 71 (2011) 641‚Äì650

Contents lists available at ScienceDirect

J. Parallel Distrib. Comput.
journal homepage: www.elsevier.com/locate/jpdc

Time-division-multiplexed arbitration in silicon nanophotonic networks-on-chip
for high-performance chip multiprocessors
Gilbert Hendry a,‚àó , Eric Robinson b , Vitaliy Gleyzer b , Johnnie Chan a , Luca P. Carloni c , Nadya Bliss b ,
Keren Bergman a
a

Lightwave Research Laboratory, Department of Electrical Engineering, Columbia University, 500 W 120th St, Mudd 1300, New York, NY 10027, United States

b

Lincoln Laboratory, Massachusetts Institute of Technology, 244 Wood St, Lexington, MA 02420, United States

c

Department of Computer Science, Columbia University, 450 Computer Science Building, 1214 Amsterdam Ave., Mailcode: 0401, New York, NY 10027, United States

article

info

Article history:
Received 12 April 2010
Received in revised form
15 August 2010
Accepted 14 September 2010
Available online 8 October 2010
Keywords:
Networks-on-chip
Photonic interconnection networks
Silicon photonics
Time division multiplexing
Memory systems

abstract
As the computational performance of microprocessors continues to grow through the integration of an
increasing number of processing cores on a single die, the interconnection network has become the central
subsystem for providing the communications infrastructure among the on-chip cores as well as to offchip memory. Silicon nanophotonics as an interconnect technology offers several promising benefits for
future networks-on-chip, including low end-to-end transmission energy and high bandwidth density of
waveguides using wavelength division multiplexing. In this work, we propose the use of time-divisionmultiplexed distributed arbitration in a photonic mesh network composed of silicon micro-ring resonator
based photonic switches, which provides round-robin fairness to setting up photonic circuit paths. Our
design sustains over 10√ó more bandwidth and uses less power than the compared network designs. We
also observe a 2√ó improvement in performance for memory-centric application traces using the MORE
modeling system.
¬© 2010 Elsevier Inc. All rights reserved.

1. Introduction
Current trends in computer architecture indicate that the
network-on-chip will play a critical role in determining future
high-performance microprocessors. How this role is played out
will impact many areas of computing, including the programming
models, architecture designs and manufacturing.
It is becoming apparent that electronics may not be able to
solve all the communications challenges in high-performance
computing. Increased off-chip bandwidth means higher IO pin
counts, a requirement that may become unrealistic if memory
bandwidth is to be balanced with computational capabilities. Using
electronic links to connect a microprocessor to memory on a
board presents critical design trade-offs between the wire lengths,
memory capacity, datarate, and power consumed by the IO.
Photonics offers key advantages in these areas, and deserves serious consideration as the leading communications technology of
future high-performance chip multiprocessors. Using wavelength
division multiplexing (WDM), a technique of transmitting many
optical signals on different wavelengths simultaneously in the

‚àó

Corresponding author.
E-mail addresses: gilbert@ee.columbia.edu, gilbert.hendry@gmail.com
(G. Hendry), erobinson@ll.mit.edu (E. Robinson), vgleyzer@ll.mit.edu (V. Gleyzer),
johnnie@ee.columbia.edu (J. Chan), luca@cs.columbia.edu (L.P. Carloni),
nt@ll.mit.edu (N. Bliss), bergman@ee.columbia.edu (K. Bergman).
0743-7315/$ ‚Äì see front matter ¬© 2010 Elsevier Inc. All rights reserved.
doi:10.1016/j.jpdc.2010.09.009

same transmission medium, photonics can achieve a bandwidth
density orders of magnitude higher than electronics, which can
greatly alleviate package IO pin constraints. Additionally, photonics provides an extremely energy efficient end-to-end transmission
technology that is largely independent of the datarate and distance.
Unlike electronics, the distance traveled in a waveguide or optical
fiber is virtually independent of the energy spent, which is also decoupled from datarate.
Recent numerous advances in silicon photonic integration and
the emerging field of CMOS photonics [3,10,21,36,23] allows us
to consider practical designs for full-scale first generation interconnects in this technology platform. Many such novel photonicenabled network architectures have been recently proposed that
can deliver performance improvement over equivalent electronic
interconnect designs [35,26,17,1,5,28,14].
In this work, we propose an improvement to an all-optical
broadband network that uses time division multiplexing (TDM) to
arbitrate setting up communication circuit-paths, first proposed
in [11]. This network architecture is able to achieve high bandwidths between communicating pairs and better network resource
utilization, while providing round-robin fairness to network requests through distributed control of photonic switches.
We evaluate an instantiation of the network connecting 256
cores with 128 GB of memory using both random network traffic
and a detailed trace of an embedded computing application. We
find that our design achieves over 10√ó higher total network

642

G. Hendry et al. / J. Parallel Distrib. Comput. 71 (2011) 641‚Äì650

bandwidth over other solutions, including previously proposed
circuit-switched and TDM-arbitrated solutions leading to lower
latencies at high loads and lower power consumption. In addition,
our design is 2√ó faster when running a projective transform, a
key high-performance embedded computing kernel for signal and
image processing.
2. Related work
Research into photonic circuit-switched networks-on-chip has
progressed in the past few years, leading to a more complete
understanding of the challenges both at the system level and the
device level.
Many advances have been made towards the integration of
silicon nanophotonic devices into the traditional CMOS production
line. Ring resonators have become a prevalent building block for
broadband spatial switches, wavelength filters, and modulators
because of their low area and power consumption [38].
However, device temperature stability and manufacturing defects still remain as significant barriers to full-fledged integration.
Currently, both of these problems are solved by heating the individual device, changing the effective index of refraction of the
material and tuning the ring to the correct resonance [4]. Other
solutions include manufacturing and design techniques to make
more athermal devices [10].
System-level implications of photonics has made a large impact
on the way architects are thinking of future CMPs. Reducing
the cost of cross-chip, off-chip, and chip‚Äìchip communication
allows a system designer to rethink programming models, memory
hierarchy, and cost-performance optimization.
Next-generation NoC designs using silicon nanophotonic technology have been proposed in other works. The Corona network
is an example of a network that uses optical arbitration via a
wavelength-routed token ring to reserve access to a full serpentine crossbar made from redundant waveguides, modulators, and
detectors [35]. Similarly, wavelength-routed bus based architectures have been proposed which take advantage of WDM for arbitration [26,17].
Batten et al. proposed an architecture using source routing and
wavelength arbitration for off-chip communications which takes
advantage of WDM to dedicate wavelengths to different DRAM
banks, forming a large wavelength-tuned ring resonator matrix as
a central crossbar [1]. Phastlane was designed for a cache-coherent
CMP, enabling snoop broadcasts and cache line transfers in the
optical domain [5].
3. Photonic circuit switching
On-chip hybrid circuit-switched photonic networks using an
electronic control plane have been proposed by Shacham et al. [33]
and Petracca [28]. The fundamental switching unit in these designs
is the photonic switching element (PSE), which is a micro-ring
resonator that is able to shift its periodic resonance to align with
the optical signals present in the nearby waveguide by injecting
carriers through a p‚Äìi‚Äìn junction. This operation is shown in Fig. 1.
These PSEs are strictly spatial switches, much like conventional
electronic ones, which means paths from one port to another must
be arbitrated before data can be sent through them. This issue is
further complicated by the fact that no photonic equivalent of a
buffer exists, making it a requirement that the path be completely
set up from source to destination before data can be passed
through the network. This has been solved in the past by using
a conventional packet-switched electronic control network which
circuit-switches a photonic data plane [33].
The idea behind this method is that once the optical path
is set up between two nodes, the transmission of the data can

Fig. 1. PSE operation, switching from OFF to ON state, shifting wavelengths.

amortize the setup latency with high bandwidth WDM. Also, PSEs
are transparent to bit rate, meaning that energy is only dissipated
at the modulators and detectors for each bit, making the end-toend transmission energy practically distance independent.
However, circuit switching in this way contains no implicit
mechanism which ensures fairness, which can lead to degraded
performance due to path blocking if messages are short or require
the same photonic resources [12].
This work improves on these designs by removing the electronic
control network responsible for allocating network resources, replacing it with a time-division-multiplexing distributed arbitration
of photonic switches.
4. TDM arbitration
We propose using time division multiplexing (TDM) to arbitrate
end-to-end photonic circuit paths in a network of ring resonator
based photonic switches. The basic concept behind this is that
during a specified amount of time, or time slot, switches in the
network are configured to allow communication between one or
more pairs of access points. Each time slot is of length
tslot = tsetup + ttransmission + tpropagation

(1)

where tsetup is the time it takes to change the state of all PSEs at
once, ttransmission is the time each node is allowed to transmit data
per time slot, and tpropagation is the worst-case propagation latency
between any two valid communicating pairs. If each switch is able
to keep track of the current time slot using a global clock, this
allows the control of the switches to be completely distributed in
that they need not communicate with each other.
This concept should be distinguished from TDM mechanisms
in other networks. Typically, requests to use network resources is
arbitrated by sources or individual network nodes to dynamically
allocate a temporal schedule for access to virtual channels,
physical links, switches, or virtual circuits, thus providing fairness
guarantees to latency and bandwidth [25,9,31,22,27].
Our method aims at providing the same fairness, but because
there are no equivalent of buffers in photonic technology, we must
apply TDM arbitration through the entire network creating endto-end optical circuit paths. Here, the scheduling of all the nodes‚Äô
accesses to network resources is done statically at design time. If
there are Nslot time slots, each of duration tslot , then the total TDM
frame, TTDM is
TTDM = Nslot √ó tslot .

(2)

G. Hendry et al. / J. Parallel Distrib. Comput. 71 (2011) 641‚Äì650

For the design proposed in this paper, we require that full network
communication coverage is implemented, or that every network
node is able to send messages to every other node within TTDM .
During TDM arbitration, the network repeatedly cycles through
every time slot. If a network node has data to send to another node,
it waits for the correct time slot. If a node has multiple messages
to different destinations queued up, it can send them out of order.
Also, by statically selecting different values for ttransmission , we can
vary the granularity of the arbitration. If, for instance, the system
architecture specifies that only fixed-length messages may be sent
on the network (i.e. cache lines), then we can adjust ttransmission to
exactly match that size.
The naive way to accomplish this is to assign a time slot to
every possible communicating pair in the network. Thus, we would
require
Nslot = N √ó (N ‚àí 1)

(3)

643

Fig. 2. Row communication TDM slot examples for four nodes. (For interpretation
of the references to colour in this figure legend, the reader is referred to the web
version of this article.)

time slots to implement full coverage, where N is the number of
nodes in the network. A 64-node network would therefore require
4032 time slots. This naive scheduling of one path per time slot
in the network achieves the worst-case network utilization. As we
will see, it is easily possible to statically allocate the network to
many transmissions during a single time slot.
4.1. Enhanced TDM arbitration
We can improve on the naive implementation by scheduling
more than one transmission per time slot, thus reducing the total
number of time slots, and the worst-case latency of a message
waiting for its slot. In order to maintain correct operation we must
adhere to the following constraints during a single time slot:
1. Source contention‚ÄîA node can only send to one destination at a
time, assuming a single set of modulators at an access point.
2. Destination contention‚ÄîA node can only receive from one source
at a time, assuming a single set of detectors at an access point.
3. Topology contention‚ÄîTransmission cannot overlap in the same
waveguide.
A method for statically scheduling end-to-end TDM-arbitrated
optical transmissions was discussed previously by Hendry [11],
using a genetic algorithm to search the solution space. In this
work, we aim to improve on that implementation by decreasing
the number of time slots required. Instead of searching the solution
space, we will simplify the problem and describe a method we can
apply manually to a mesh topology for scheduling which is scalable
and results in significantly fewer time slots.
To simplify the problem, let us first concede that photonic
transmission will no longer be entirely end-to-end for every
node pair. Rather, the mesh X -dimension transmission is first
completed, converted to the electronic domain, and stored in a
buffer until the Y -dimension transmission can be completed. This
means that we will pay optical to electrical conversion energy costs
twice. This simplification reduces the energy per bit benefits that
end-to-end photonic transmission technology provides, but we
will leave this to our discussion of our results in Section 7.
We can first observe that two transmissions can always take
place in a row during a time slot, for any size row, where the two
sending nodes are on opposite sides of the row. This is illustrated
in Fig. 2 for one row of four nodes, assuming bidirectional links
connecting neighboring nodes consisting of two uni-directional
waveguides. The red nodes are the sending nodes, and exhaust
all possible combinations of destinations (green) in the row. The
process repeats for all other nodes being designated as the sending nodes. Note that communications are shown to be symmetric
across the midpoint of the row in Fig. 2, though this is not required.
We now make it our goal to schedule communications similar
to Fig. 2 such that two transmissions occur in every row and

Fig. 3. Control matrix for a 4 √ó 4 network.

every column in each time slot. Since each node in a row must
communicate with every other in its row (R ‚àí 1 of them), and two
nodes are communicating at once per row, we would require
Nslot = (R ‚àí 1) √ó

Óµù Óµ°
R
2

(4)

time slots, where R is the number of nodes in a row (and column,
assuming a square network), R is even, and R ‚â• 4. For an 8 √ó
8 64-node network, this is merely 28 time slots, a significant
improvement over the previous end-to-end implementation with
142 time slots [11].
Fig. 3 illustrates an example of how to schedule a 4 √ó 4
TDM network, which requires 6 time slots. We represent the
transmission possibilities as a 16 √ó 16 control matrix. Each entry in
the matrix is color-coded to indicate which sender‚Äìreceiver pair is
enabled during a time slot. Note that a node may only send and
receive once per time slot, which translates into the rule that a
color may only appear once in a row and column in the control
matrix. Also note that not all node combinations are necessary
because we conceded that optical circuit paths only travel in one
mesh dimension during a slot, which is why many control matrix
entries are blank (white).
Some visual and numerical patterns are useful when specifying
the control matrix for any size network. For instance, the 4 √ó 4
squares lying on the black diagonal indicate row communications.
Other diagonal stripes represent column communication. First, all
row communications are added, each block (row) utilizing every
time slot exactly twice, as shown in Fig. 2. The block pattern shifts
slightly to accommodate column communications, and is mirrored
across the network bisection line (row R/2).

644

G. Hendry et al. / J. Parallel Distrib. Comput. 71 (2011) 641‚Äì650

Fig. 5. Layout of photonic switch, showing waveguides and ring resonators. Units
in microns.
Table 1
Switch functionality.

Fig. 4. Network architecture.

5. Network implementation
Fig. 4 shows a 64-core example of a CMP using a 4 √ó 4
instantiation of our photonic TDM network, consisting of three
basic components: a photonic switch, switch controller, and
network gateway. Switches are arranged in a mesh, each controlled
by their controller. Each gateway connects four cores, known
as gateway concentration. In addition, our network design has
an added advantage that it is tiled, aligning with today‚Äôs chip
design flow and manufacturing techniques. In this section, we
also describe the design for an optically attached DRAM memory
module attached to each gateway.
5.1. Photonic switch
Fig. 5 shows the layout for the photonic switches in the network.
It consists of waveguide paths and PSEs, operating as in Fig. 1. Ports
are labeled as North, South, East, West, and Gateway.
Because we optimized our arbitration for fewer TDM slots at
the cost of paying O‚ÄìE‚ÄìO energy by doing X -then-Y routing, the
switch does not need to implement full connectivity between the
ports. Table 1 shows the port combinations, and the PSE number
that implements the path, referring to Fig. 5. For example, we can
see in Fig. 5 that the PSE labeled as 1 can switch a signal from the
gateway (modulator bank) to the North port. Note that the signal
must pass through a ring only when coming from a gateway and
entering a gateway, which saves on insertion loss when traveling
in straight lines.
5.2. Switch controller
In the proposed network architecture, each switch is controlled
by a local controller which is aware of the current TDM slot by
tracking ticks of a global TDM clock, and is therefore aware of how
the switch should be set. A global, synchronous TDM clock can
be implemented with waterfall clock distribution, synchronous
latency-insensitive design [7], or optical clock distribution [39].

Inport

Outport

PSE

Mod
Mod
Mod
Mod
N
E
S
W
E/W
N/S

N
E
S
W
Det
Det
Det
Det
W/E
S/N

1
2
3
4
5
6
7
8
N/A
N/A

The period of this clock must be the TDM period, tslot . As indicated
later in Section 7, tslot should be set to an expected average
message transmission time, so that time slots are just big enough to
allow end-to-end transmission. Taking into account the time slot
overheads, this value could be at least ten nanoseconds equating
to less than 100 MHz TDM clock frequency (depending on tslot ), a
very feasible implementation by today‚Äôs standards.
The output logic can be implemented as a single lookup table
(LUT) which takes the switch ID register as an input, allowing
identical ROM instantiation among network tiles. In practice, only
the fraction of the table that is necessary to run the local switch
would be instantiated to save area and power.
The size of the output logic is proportional to the number of
TDM slots, which is dictated by the number of network nodes.
Specifically, there is one bit per PSE per TDM slot, indicating
whether the PSE is on or off. Since there are 8 PSEs per switch, this
means that the ROM of each switch controller contains Nslot bytes
of information. Referring to Eq. (4), a 64-node network needs a
28-byte LUT per switch.
5.3. Network gateway
Fig. 6 shows the microarchitecture of a network gateway,
providing network and memory access to four cores. This is
accomplished through the use of a main TDM controller, which
arbitrates network and memory resources and acts as a memory
controller by keeping a master schedule of events that occur during
each time slot.
Each gateway has two vertically coupled [32] connections to a
memory bank. Local reads and writes are serviced by scheduling
row and column accesses during free slots in the master schedule.

G. Hendry et al. / J. Parallel Distrib. Comput. 71 (2011) 641‚Äì650

645

a

b
IDLE

Receive Col
Address (and
Burst Length)

Receive Row
Address (and
Bank ID)

c

DLL

Read/Write
Data

Control

Receivers
Modulators

Fig. 6. Network gateway microarchitecture.

Remote memory accesses are sent to the destination gateway,
where they are then scheduled in a similar fashion. Remote reads
are read directly from memory into the network to save on
buffering power.
The following describes an example of the gateway operation,
numbered in Fig. 6:
1. Communication requests are made to the TDM controller,
which controls an electronic crossbar that connects the various
gateway components.
2. When the network is in the correct TDM slot, depending on
the type of communication (memory read, memory write,
MPI-send, etc.), the TDM controller sets the broadband rings
that control access to and from the modulators and detectors.
This can also be done ahead of time when the time slot switches,
if the transaction has been queued up.
3. The TDM controller also sets the crossbar from the requesting
core to the serializer, which ramps the data up to 10 Gb/s
modulation. The transmission clock is also transmitted on a
separate wavelength.
4. When a signal is received, it is first deserialized, clocked by the
received transmission clock.
5. If the data has reached its destination, it sits in a temporary
buffer, waiting for access to the electronic crossbar. Access will
be immediately available unless cores in the same gateway are
communicating locally through the crossbar.
6. If the data is using the gateway as an intermediate point while
switching dimensions, it sits in the X ‚ÄìY buffer and notifies the
TDM controller. It can then transmit during the correct TDM
slot.
The sizes of the buffers can be exactly specified based on the size
of the network. The X ‚ÄìY buffer is used to hold transmissions that
have arrived at this gateway to continue through the network in
a different direction, and are waiting for their time slot. Therefore,
they must hold a maximum of 2 √ó (R ‚àí 1) transmissions, which is
the number of time slots in one TDM frame in which a message
could be received. A 64-node network will therefore require a
buffer of size 14 √ó Stransmission , where Stransmission is the maximum
message size that can be transmitted in one time slot.
The temporary buffer is only used to store received transmissions that are destined for the cores in the gateway. The TDM controller gives priority to the temporary buffer over local core‚Äìcore

Fig. 7. Photonic Circuit-Accessed Memory Module design (a) Photonic CAMM (b)
P-CAMM control logic (c) P-CAMM Transceiver.

communication, therefore it needs to hold a maximum of 2 transmissions: one for receiving incoming transmissions, and one for
sending the last received transmission on to the correct core.
A key characteristic of the gateway design is its ability to handle
cores‚Äô requests out of order. For example, if core 0 requests that
a message be sent but must wait for the correct time slot, the
controller is free to grant access to subsequent requests from any
of the other cores connected by the gateway. In a purely circuitswitched network with a dynamic path setup implementation,
the first request must be handled first, potentially head-of-line
blocking other cores.
5.4. Circuit-Accessed Memory Module
Our proposed memory access architecture uses a DRAM module
in a less conventional way, which requires a redesign of the basic
memory module discussed in the previous work [13]. Fig. 7(a)
shows the Photonic Circuit-Accessed Memory Module (P-CAMM)
design. Individual DRAM chips are connected via a local electronic
bus to a central optical controller/transceiver, shown in Fig. 7(c).
The controller (Fig. 7(b)) is responsible for demultiplexing the
single optical channel into the address and data bus much in
the same way as Rambus RDRAM memory technology [29], using
the simple control flowchart shown. This shift from electrical
to photonic technology presents significant advantages for the
physical design and implementation of off-chip signaling.
Although the P-CAMM shown in Fig. 7(a) retains the contemporary SDRAM DIMM form factor, this is not required due to the
alleviated pinning requirements. The memory module can then be
designed for larger, smaller, or more dense configurations of DRAM
chips. Furthermore, the memory module can be placed arbitrarily
distant from the processor using low-loss optical fiber without incurring any additional power or optical loss. Latency is also minimal, paying 4.9 ns/m [6].
Additionally, the driver and receiver banks use much less power
for photonics using ring resonator based modulators and SiGe
detectors than for off-chip electronic I/O wires [3].
6. Experimental setup
We evaluate a 64-node enhanced TDM photonic network
implementation (P-ETDM) using external concentration [18] for
a total of 256 cores and compare it against a circuit-switched

646

G. Hendry et al. / J. Parallel Distrib. Comput. 71 (2011) 641‚Äì650

Table 2
Optical device energy parameters.
Parameter

Value

Datarate (per wavelength)
PSE dynamic energy
PSE static (OFF) energy
Modulation switching energy
Modulation static energy
Detector energy
Thermal tuning energy

2.5 Gb/s
375 fJa
400 uJ/sb
25 fJ/bitc
30 ¬µWd
50 fJ/bite
1 uW/Kf

a
Dynamic energy calculation based on carrier density, 50-¬µm ring, 320 √ó 250
nm waveguide, 75% exposure, 1-V bias.
b
Based on switching energy, including photon lifetime for re-injection.
c
Same asa that for a 3 ¬µm ring modulator.
d
Based on experimental measurements in [36]. Calculated for half a 10 GHz clock
cycle, with 50% probability of a 1-bit.
e
Conservative approximation assuming femto-farad class receiverless SiGe
detector with C < 1 fF.
f
Same value as used in [14]. Average of 20¬∞ thermal tuning required.

Table 3
Optical device loss parameters.

a
b
c

Device

Insertion loss

Waveguide propagation
Waveguide crossing
Waveguide bend
Passing by ring (Off)
Insertion into ring (On)

1.5 dB/cma
0.05b
0.005 dB/90¬∞a
‚âà0c
0.5c

Optical power budget

35 dB

From [37].
Projections based on [8].
From [19].

photonic mesh (P-mesh) designated PS-1 in [13] and the original
TDM network design (P-TDM) [11]. We describe the relevant
modeling and parameters below.
6.1. Simulation environment
We use a simulation and CAD environment called PhoenixSim
[2], developed for the analysis of electronic and photonic
networks-on-chip. PhoenixSim includes a cycle-accurate network
simulator which captures physical-layer details, such as physical
dimensions and layout, of both electronic and nanophotonic
devices to accurately execute various traffic models.
Photonic devices. Modeling of optical components is built on a detailed physical-layer library that has been validated through the
physical measurement of fabricated devices. The modeled components are fabricated in silicon at the nano-scale, and include modulators, photodetectors, waveguides (straight, bending, crossing),
filters, and PSEs. These devices are characterized and modeled at
runtime by attributes such as insertion loss, crosstalk, delay, and
power dissipation. Tables 2 and 3 show the most important optical
parameters used.
Photonic network physical-layer analysis. The number of available
wavelengths is obtained through an insertion loss analysis using
PhoenixSim [2]. Fig. 8 shows the relationship between network
insertion loss and the number of wavelengths that can be used.
The following equations specify the constraints that must be met
in order to achieve reliable optical communication:
Ptot < PNT

(5)

Pinj ‚àí Ploss > Pdet .

(6)

Eq. (5) states that the total injected power at the first modulator
(Ptot ) must be below the threshold at which nonlinear effects
are induced (PNT ), which would corrupt the data (or introduce
significantly more optical loss). A reasonable value for PNT is
around 10‚Äì15 dBm [20]. Eq. (6) states that the power received at
the detectors (Pdet ) must be greater than the detector sensitivity

Fig. 8. Number of wavelengths dictated by insertion loss and optical power budget.

(usually about ‚àí20 dBm) to reliably distinguish between zeros and
ones. To ensure this, every wavelength must inject at least enough
power (Pinj ) to overcome the worst-case optical loss through the
network (Ploss ). From these relationships, we can see that the
number of wavelengths that can be used in a network can be
limited by the worst-case insertion loss through it.
The three photonic networks that we consider have different
insertion loss characteristics. We determine the worst-case Ploss
for each network and find that it equates to 9.1, 10.1, and 6.3 dB
for the P-mesh, P-TDM and P-ETDM, respectively. All networks
can support a large number of wavelengths with a 35 dB optical
power budget, though we limit this number at 128 because
of modulator free spectral range (FSR) and inter-wavelength
crosstalk limitations.
Simulation parameters. Each network uses 2.5 Gb/s signaling to
reduce SerDes and driver power costs for an ideal link bandwidth
of 320 Gb/s in and out of every gateway in each network. A bit-rate
clock is sent with the data on a separate channel to lock on to the
data at the receiver, and we allocate 16 clock cycles of overhead for
each transmission for locking.
For power dissipation modeling, the ORION 2.0 electronic
router model [15] is integrated into PhoenixSim, which provides
detailed technology node-specific modeling of router components
such as buffers, crossbars, arbiters, clock tree, and wires. The
technology point is specified as 32 nm, and the VDD and Vth ORION
parameters are set according to frequency (lower voltage, higher
threshold for lower frequencies). The ORION model also calculates
the area of these components, which is used to determine the
lengths of interconnecting wires for the P-mesh. The P-mesh uses
a 1 GHz control plane with small (128-bit) buffers and narrow
(32-bit) channels.
DRAM modeling. We employ the same DRAM subsystem modeling used in the previous work [13]. This model cycle accurately enforces all timing constraints of real DRAM chips,
including row access time, row‚Äìcolumn delay, column access latency, and precharge time. Because access to the memory modules is arbitrated by the on-chip path setup mechanism, only one
transaction must be sustained by a MAP, which greatly simplifies
the control logic as previously discussed. For the TDM networks,
the gateway control logic handles memory transactions, scheduling them in empty time slots.
We base our model parameters around a Micron 1 Gb DDR3
chip [24], with (tRCD ‚ÄìtRP ‚ÄìtCL ) chosen as (12.5‚Äì12.5‚Äì12.5) (ns).
To normalize the three different network architectures for
experiment, we assign them the same amount of similarly
configured DDR3 DRAM around the periphery.
7. Evaluation
7.1. Synthetic traffic
To test the network characteristics, we use PhoenixSim to
run Uniform, Neighbor, Tornado, Bitreverse, and Hotspot random

G. Hendry et al. / J. Parallel Distrib. Comput. 71 (2011) 641‚Äì650

647

Fig. 9. Latency vs. bandwidth under synthetic traffic.

(a) P-mesh.

(b) P-TDM.

(c) P-ETDM.
Fig. 10. Zero-load latency breakdown under Uniform traffic.

traffic in the network for 5 ms with 8, 128, and 2 kbyte messages, representing control, cache line, and application-level message sizes, respectively. We set tslot at 10 ns, requiring 1 ns each
for tsetup and tpropagation , making Stransmission equal to 10 240 bits, or
about 1.2 kB.
Fig. 9 shows the average read latency vs. total bandwidth
in the network. The two TDM networks show higher zeroload latency than the P-mesh, as expected from the overhead
of waiting for the correct slot. However, the enhanced TDM
network shows significant zero-load latency improvement over
the original TDM design. Both TDM networks also show higher
throughput compared to the P-mesh for all message sizes, mostly
due to their ability to service message requests that arrive at
the gateway‚Äôs controller out of order, thus increasing network
utilization. Bandwidth gains are most profound in the traffic
patterns with more chances of circuit-path blocking in the
P-mesh, either from long communication (Uniform, Bitreverse) or
predictably conflicting resources (Tornado).
Fig. 10 shows the sources of zero-load latency under Uniform
traffic for each network as message size increases. The P-mesh
is superior in this respect, as it is entirely dependent on the

electronic router hop latency. The original TDM design‚Äôs latency
comes entirely from the slot latency, or when a message is next in
line for a time slot, but is waiting for that slot. Again, our design
improves the zero-load latency over the original TDM design by
decreasing the time slot count, despite additional delay when
changing dimensions (XY -buffer queuing and slot latency). The
TDM networks also show a significant increase in latency for the
larger 2 kB messages because the message must be sent in multiple
slots. Though the slot period could have been changed to match
the message size for the different simulations, we chose to keep
a single slot period to illustrate the effects of its relationship to
expected message size.
To illustrate the effects of contention on network latency, Fig. 11
shows the sources of latency while loaded at half capacity. For
the P-mesh, blocking latency enters the picture, forcing queuing at
the network gateways. The original TDM design is still dominated
by slot latency, where queuing latency is dictated by the traffic
pattern. The E-TDM method has a similar relationship, though
much less severe because of the reduced slot count. An extra
traffic-dependent queuing latency is introduced at the XY -buffer,
though it is small compared to the total.

648

G. Hendry et al. / J. Parallel Distrib. Comput. 71 (2011) 641‚Äì650

(a) P-mesh.

(b) P-TDM.

(c) P-ETDM.
Fig. 11. Half-load latency breakdown under Uniform traffic.

MORE consists of the following primary components:

‚Ä¢ The program analysis component is responsible for converting
the user program, taken as input, into a parse graph, a
description of the high-level operations and their dependences
on one another.
‚Ä¢ The data mapping component is responsible for distributing
the data of each variable specified in the user code across the
processors in the architecture.
‚Ä¢ The operations analysis component is responsible for taking the
parse graph and data maps and forming the dependency graph, a
description of the low-level operations and their dependences
on one another.

Fig. 12. Power breakdown.

Fig. 12 shows a coarse network power breakdown under Uniform traffic near saturation, assuming around 12% integrated laser
efficiency [30]. Electronic power is still a large part of all the networks, mainly in the electronic crossbar necessary to implement
external concentration, which must match the bandwidth of the
photonic links using many parallel wires. The TDM control circuitry
contributes minimal power overhead to the two TDM networks. An
advantage of the E-TDM network is that is has less insertion loss,
and therefore requires less laser power. Instead of laser power, the
P-ETDM consumes power in the XY -buffer (‚àº2 W) which is necessary to implement dimension-only transmission. Regardless, the
P-ETDM consumes the lowest total power.
7.2. Case study: embedded application
We evaluate the proposed network architectures using the application modeling framework, Mapping and Optimization Runtime
Environment (MORE) to collect traces from the execution of highperformance embedded signal and image processing applications.
The MORE system, based on pMapper [34], is designed to
project a user program written in Matlab onto a distributed
or parallel architecture and provide performance results and
analysis. The MORE framework translates application code into a
dependency based instruction trace, which captures the individual
operations performed as well as their interdependences. By
creating an instruction trace interface for PhoenixSim, we were
able to accurately model the execution of applications on the
proposed architectures.

PhoenixSim then reads the dependency graphs produced by MORE,
generating computation and communication events. Combining
PhoenixSim with MORE in this way allows us to characterize
photonic networks on the physical level by generating traffic
which exactly describes the communication, memory access, and
computation of the given application.
7.2.1. Projective transform
When registering multiple images taken from various aerial
surveillance platforms, it is frequently advantageous to change the
perspective of these images so that they are all registered from a
common angle and orientation (typically straight down with North
being at the top of the image). In order to do this, a process known
as projective transform is used [16].
Projective transform takes as input a two-dimensional image
M as well as a transformation matrix t that expresses the
transformational component between the angle and orientation
of the image presented and the desired image. The projective
transform algorithm outputs M ‚Ä≤ , or the image M after projection
through t. To populate a pixel p‚Ä≤ in M ‚Ä≤ , its x and y positions are
back-projected through t to get their relative position in M , p. This
position likely does not fall directly on a pixel in M, but rather
somewhere between a set of four pixels. Using the distance from p
to each of its corners as well as the corner values themselves, the
value for p‚Ä≤ can be obtained.
We consider this application on an image size of 256 √ó 256
pixels. We simulate a simple case, where the image orientation
is rotated by ninety degrees. While the result of this transform
is simply a corner turn on the matrix representing the image, it
allows for identical image and projection sizes while still inducing
data movement in the projection process. Also, with the use

G. Hendry et al. / J. Parallel Distrib. Comput. 71 (2011) 641‚Äì650

649

Table 4
Performance and power results for projective transformation.
Network

Network power (W)

Performance (GOPS)

Efficiency (GOPS/W)

P-mesh
P-TDM
P-ETDM

9.92
10.95
8.84

7.55
3.7
15.7

1√ó
0.44√ó
2.35√ó

of MORE, analyzing different projections is simple. It requires
only changing the transformation matrix in the source code and
rerunning the simulations.
7.2.2. Simulation results
During the simulations, we collected average network power
and system performance. These results are reported in Table 4.
The P-ETDM is the superior solution, outperforming the P-mesh
by around 2√ó while using slightly less power. Though the original
TDM network can sustain higher total network bandwidth, it
is inferior to the P-mesh for this particular application, directly
illustrating the usefulness of the design improvement proposed in
this paper.
8. Conclusions
TDM arbitration of photonic circuits proves to be an effective
way to increase network utilization, which increases performance
and energy efficiency for both random traffic and real applications.
Key characteristics of the architecture are the ability to bypass
head-of-line blocking at the gateways, and very low insertion loss
due to single dimension transmission. In this paper, we presented
an improvement over previous methods to decrease the number
of time slots needed to implement full network coverage in one
frame, which reduces zero-load latency and improves throughput,
having a significant impact on the performance of a key embedded
application kernel in real-time image processing, the projective
transform.
Acknowledgments
This work is sponsored in part by Defense Advanced Research
Projects Agency (DARPA) under Air Force contract FA8721-05-C0002, DARPA MTO under grant ARL-W911NF-08-1-0127, the NSF
(Award #: 0811012), and the FCRP Interconnect Focus Center (IFC).
Opinions, interpretations, conclusions and recommendations are
those of the author and are not necessarily endorsed by the United
States Government.
References
[1] C. Batten, A. Joshi, J. Orcutt, A. Khilo, B. Moss, C. Holzwarth, M. Popovic,
H. Li, H. Smith, J. Hoyt, F. Kartner, R. Ram, V. Stojanovic, K. Asanovic,
Building manycore processor-to-DRAM networks with monolithic silicon
photonics, in: HOTI‚Äô08: Proceedings of the 2008 16th IEEE Symposium on
High Performance Interconnects, IEEE Computer Society, Washington, DC,
USA, 2008, pp. 21‚Äì30.
[2] J. Chan, G. Hendry, A. Biberman, K. Bergman, L.P. Carloni, PhoenixSim: a
simulator for physical-layer analysis of chip-scale photonic interconnection
networks, in: DATE: Design, Automation, and Test in Europe, 2010.
[3] L. Chen, K. Preston, S. Manipatruni, M. Lipson, Integrated GHz silicon photonic
interconnect with micrometer-scale modulators and detectors, Optics Express
17 (17) (2009).
[4] L. Chen, N. Sherwood-Droz, M. Lipson, Compact bandwidth-tunable microring
resonators, Optics Letters 32 (22) (2007) 3361‚Äì3363.
[5] M.J. Cianchetti, J.C. Kerekes, D.H. Albonesi, Phastlane: a rapid transit optical
routing network, SIGARCH Computer Architecture News 37 (3) (2009)
441‚Äì450.
[6] Corning Inc., Datasheet: Corning SMF-28e Optical Fiber Product Information.
URL: http://www.princetel.com/datasheets/SMF28e.pdf (accessed 2010).
[7] A. Edman, C. Svensson, Timing closure through a globally synchronous, timing
partitioned design methodology, in: DAC‚Äô04: Proceedings of the 41st Annual
Design Automation Conference, ACM, New York, NY, USA, 2004, pp. 71‚Äì74.
[8] T. Fukazawa, T. Hirano, F. Ohno, T. Baba, Low loss intersection of Si photonic
wire waveguides, Japanese Journal of Applied Physics 43 (2) (2004) 646‚Äì647.

[9] K. Goossens, J. Dielissen, A. Radulescu, √Üthereal network on chip: concepts,
architectures, and implementations, IEEE Design & Test 22 (5) (2005) 414‚Äì421.
[10] B. Guha, B.B.C. Kyotoku, M. Lipson, CMOS-compatible athermal silicon
microring resonators, Optics Express 18 (4) (2010).
[11] G. Hendry, J. Chan, S. Kamil, L. Oliker, J. Shalf, L.P. Carloni, K. Bergman, Silicon
nanophotonic network-on-chip using TDM arbitration, in: Proceedings of IEEE
Symposium on High-Performance Interconnects, 2010.
[12] G. Hendry, S. Kamil, A. Biberman, J. Chan, B.G. Lee, M. Mohiyuddin, A. Jain,
K. Bergman, L.P. Carloni, J. Kubiatowicz, L. Oliker, J. Shalf, Analysis of photonic
networks for a chip multiprocessor using scientific applications, in: NOCS‚Äô09:
Proceedings of the 2009 3rd ACM/IEEE International Symposium on Networkson-Chip, IEEE Computer Society, Washington, DC, USA, 2009, pp. 104‚Äì113.
[13] G. Hendry, E. Robinson, V. Gleyzer, J. Chan, L.P. Carloni, N. Bliss, K. Bergman,
Circuit-switched memory access in photonic interconnection networks for
high-performance embedded computing, in: Proceedings of Supercomputing,
2010.
[14] A. Joshi, C. Batten, Y.-J. Kwon, S. Beamer, I. Shamim, K. Asanovic,
V. Stojanovic, Silicon-photonic clos networks for global on-chip communication, in: NOCS‚Äô09: Proceedings of the 2009 3rd ACM/IEEE International
Symposium on Networks-on-Chip, IEEE Computer Society, Washington, DC,
USA, 2009, pp. 124‚Äì133.
[15] A.B. Kahng, B. Li, L.-S. Peh, K. Samadi, ORION 2.0: a fast and accurate NoC power
and area model for early-stage design space exploration, 2009, pp. 423‚Äì428.
[16] H. Kim, E. Rutledge, S. Sacco, S. Mohindra, M. Marzilli, J. Kepner, R. Haney,
J. Daly, N. Bliss, PVTOL: providing productivity, performance and portability
to DoD signal processing applications on multicore processors, in: HPCMPUGC‚Äô08: Proceedings of the 2008 DoD HPCMP Users Group Conference, IEEE
Computer Society, Washington, DC, USA, ISBN: 978-0-7695-3515-9, 2008,
pp. 327‚Äì333.
[17] N. Kirman, et al., Leveraging optical technology in future bus-based chip
multiprocessors, in: MICRO 39: Proceedings of the 39th Annual IEEE/ACM
International Symposium on Microarchitecture, IEEE Computer Society,
Washington, DC, USA, 2006, pp. 492‚Äì503.
[18] P. Kumar, Y. Pan, J. Kim, G. Memik, A. Choudhary, Exploring concentration
and channel slicing in on-chip network router, in: NOCS‚Äô09: Proceedings of
the 2009 3rd ACM/IEEE International Symposium on Networks-on-Chip, IEEE
Computer Society, Washington, DC, USA, ISBN: 978-1-4244-4142-6, 2009,
pp. 276‚Äì285.
[19] B.G. Lee, A. Biberman, P. Dong, M. Lipson, K. Bergman, All-optical comb
switch for multiwavelength message routing in silicon photonic networks,
IEEE Photonics Technology Letters 20 (10) (2008) 767‚Äì769.
[20] B.G. Lee, X. Chen, A. Biberman, X. Liu, I.-W. Hsieh, C.-Y. Chou, J. Dadap, R.M.
Osgood, K. Bergman, Ultra-high-bandwidth WDM signal integrity in siliconon-insulator nanowire waveguides, IEEE Photonics Technology Letters 20 (6)
(2007) 398‚Äì400.
[21] H.L.R. Lira, S. Manipatruni, M. Lipson, Broadband hitless silicon electro-optic
switch for on-chip optical networks, Optics Express 17 (25) (2009).
[22] Z. Lu, A. Jantsch, TDM virtual-circuit configuration for network-on-chip, IEEE
Transactions on Very Large Scale Integration (VLSI) Systems 16 (8) (2008)
1021‚Äì1034.
[23] A. Melloni, F. Morichetti, R. Costa, G.C. an dP Boffi, M. Martinelli, The ring-based
optical resonant router, in: IEEE ICC, 2006, pp. 2799‚Äì2804.
[24] Micron Technology Inc., Product Specification. 1 Gb DDR3 SDRAM Chip, 2010.
URL: http://www.micron.com/products/partdetail?part=MT41J256M4JP-125.
[25] M. Millberg, E. Nilsson, R. Thid, A. Jantsch, Guaranteed bandwidth using looped
containers in temporally disjoint networks within the nostrum network on
chip, in: DATE‚Äô04: Proceedings of the Conference on Design, Automation and
Test in Europe, IEEE Computer Society, Washington, DC, USA, 2004, p. 20890.
[26] Y. Pan, P. Kumar, J. Kim, G. Memik, Y. Zhang, A. Choudhary, Firefly: illuminating
future network-on-chip with nanophotonics, SIGARCH Computer Architecture
News 37 (3) (2009) 429‚Äì440.
[27] C. Paukovits, H. Kopetz, Concepts of switching in the time-triggered networkon-chip, in: RTCSA‚Äô08: Proceedings of the 2008 14th IEEE International Conference on Embedded and Real-Time Computing Systems and Applications, IEEE
Computer Society, Washington, DC, USA, 2008, pp. 120‚Äì129.
[28] M. Petracca, B.G. Lee, K. Bergman, L.P. Carloni, Design exploration of optical
interconnection networks for chip multiprocessors, in: HOTI‚Äô08: Proceedings
of the 2008 16th IEEE Symposium on High Performance Interconnects, IEEE
Computer Society, Washington, DC, USA, 2008, pp. 31‚Äì40.
[29] Rambus, RDRAM Memory Technology. Online at:
http://www.rambus.com/us/products/rdram/index.html (accessed 2010).
[30] G. Roelkens, D.V. Thourhout, R. Baets, Continuous-wave lasing from DVSBCB heterogeneously integrated laser diodes, in: Integrated Photonics and
Nanophotonics Research and Applications, Optical Society of America, 2007.
[31] M. Schoeberl, A time-triggered network-on-chip, in: International Conference
on Field-Programmable Logic and its Applications, FPL 2007, Amsterdam,
Netherlands, 2007, pp. 377‚Äì382.

650

G. Hendry et al. / J. Parallel Distrib. Comput. 71 (2011) 641‚Äì650

[32] J. Schrauwen, F.V. Laere, D.V. Thourhout, R. Baets, Focused-ion-beam
fabrication of slanted grating couplers in silicon-on-insulator waveguides,
IEEE Photonics Technology Letters 19 (11) (2007) 816‚Äì818.
[33] A. Shacham, K. Bergman, L.P. Carloni, Photonic networks-on-chip for future
generations of chip multiprocessors, IEEE Transactions on Computers 57 (9)
(2008) 1246‚Äì1260.
[34] N. Travinin, H. Hoffmann, R. Bond, H. Chan, J. Kepner, E. Wong, PMapper: automatic mapping of parallel matlab programs, in: DOD_UGC‚Äô05: Proceedings
of the 2005 Users Group Conference on 2005 Users Group Conference, IEEE
Computer Society, Washington, DC, USA, ISBN: 0-7695-2496-6, 2005, p. 254.
[35] D. Vantrease, R. Schreiber, M. Monchiero, M. McLaren, N.P. Jouppi,
M. Fiorentino, A. Davis, N. Binkert, R.G. Beausoleil, J.H. Ahn, Corona: system implications of emerging nanophotonic technology, in: Computer
Architecture, International Symposium on, vol. 0, 2008, pp. 153‚Äì164.
[36] M.R. Watts, Ultralow power silicon microdisk modulators and switches, in: 5th
Annual Conference on Group IV Photonics, 2008, pp. 4‚Äì6.
[37] F. Xia, L. Sekaric, Y. Vlasov, Ultracompact optical buffers on a silicon chip,
Nature Photonics 1 (2007) 65‚Äì71.
[38] Q. Xu, B. Schmidt, J. Shakya, M. Lipson, Cascaded silicon micro-ring modulators
for WDM optical interconnection, Optics Express 14 (20) (2006).
[39] J.-F. Zheng, et al., On-chip optical clock signal distribution, in: OSA Topical
Meeting on Optics in Computing, 2003.

Gilbert Hendry received the B.S. and M.S. degrees in
computer engineering from the Rochester Institute of
Technology, Rochester, NY, in 2007. He is currently a Ph.D.
candidate in the Department of Electrical Engineering at
Columbia University, New York, NY. His research interests
include the design of computing systems using silicon
photonics, and the software tools used in this endeavor.

Eric Robinson graduated from Northeastern University
with a Ph.D. in computer science. His doctoral work as part
of the high-performance and distributed computing group
focused on parallel disk based enumeration of terascale
state spaces. He is now employed by MIT Lincoln Lab
where he focuses on graph exploitation in real world
graphs. He is working on developing effective highperformance algorithms and architectures for mining
valuable intelligence from graphs generated from real
world surveillance data. In addition, he continues to
pursue interests in computational group theory, disk
based computation, parallel computation, and software‚Äìhardware co-design.
Vitaliy Gleyzer has been a staff member in the Embedded
Digital Systems group at MIT Lincoln Laboratory for two
years. Prior to joining MIT Lincoln Laboratory, he received
his Masters in Electrical and Computer Engineering from
Carnegie Mellon University, with the research concentrated on network architecture and network modeling. His
current work is primarily focused on high-performance
computing systems and embedded systems engineering.

Johnnie Chan received the B.S. degree (with high distinction) in computer and electrical engineering and M.S.
degree in electrical engineering from the University of
Virginia, Charlottesville, in 2005 and 2007, respectively.
He is currently a Ph.D. candidate in the Department of
Electrical Engineering at Columbia University, New York,
NY. His research interests include the design of photonic
networks-on-chip in chip multiprocessor systems, and the
modeling of the nanophotonic devices used to enable onand off-chip communications.
Luca P. Carloni received the Laurea degree (summa cum
laude) in electrical engineering from the Universit√† di
Bologna, Italy, in 1995, and the M.S. and Ph.D. degrees
in electrical engineering and computer sciences from
the University of California, Berkeley, in 1997 and 2004,
respectively.
He is currently an Associate Professor with the
Department of Computer Science, Columbia University,
New York, NY. He has authored over 70 publications and
is the holder of one patent. His research interests are in
the area of design tools and methodologies for integrated
circuits and systems, distributed embedded systems design, and design of highperformance computer systems.
Dr. Carloni received the Faculty Early Career Development (CAREER) Award
from the National Science Foundation in 2006 and was selected as an Alfred P. Sloan
Research Fellow in 2008, and received the ONR Young Investigator Award in 2010.
He is the recipient of the 2002 Demetri Angelakos Memorial Achievement Award
‚Äò‚Äòin recognition of altruistic attitude towards fellow graduate students‚Äô‚Äô. In 2002,
one of his papers was selected for ‚Äò‚ÄòThe Best of ICCAD‚Äô‚Äô, a collection of the best IEEE
International Conference on Computer-Aided Design papers of the past 20 years. He
is a senior member of the ACM and IEEE.
Nadya Bliss is the Assistant Leader of the Embedded
and High Performance Computing Group at MIT Lincoln
Laboratory. She earned her bachelor and master degrees
in Computer Science from Cornell University, where her
research focus was on developing Bayesian techniques
for natural language processing. After completing her
degrees, she joined the Laboratory in 2002 and, as a
member of technical staff, was one of the principal innovators and developers of the pMatlab: Parallel Matlab Toolbox and the pMapper automated parallelization
system.
Her technical interests are in parallel and distributed computing, specifically
program analysis and optimization, scalable intelligent/cognitive algorithms, representations for multi-INT data, and software/hardware co-design methodologies.
Keren Bergman is a Professor of Electrical Engineering at
Columbia University where she also directs the Lightwave
Research Laboratory (http://lightwave.ee.columbia.edu/).
She leads multiple research programs on optical interconnection networks for advanced computing systems, data
centers, optical packet-switched routers, and chip multiprocessor nanophotonic networks-on-chip. Dr. Bergman
holds a Ph.D. from MIT and is a Fellow of the IEEE and of
the OSA. She currently serves as the co-Editor-in-Chief of
the IEEE/OSA Journal of Optical Communications and Networking.

TOWARD SIGNAL PROCESSING THEORY FOR GRAPHS AND NON-EUCLIDEAN DATA
Benjamin A. Miller and Nadya T. Bliss

Patrick J. Wolfe

Lincoln Laboratory
Massachusetts Institute of Technology
Lexington, Massachusetts 02420
Email: {bamiller, nt}@ll.mit.edu

Statistics and Information Sciences Laboratory
Harvard University
Cambridge, Massachusetts 02138
Email: wolfe@stat.harvard.edu

ABSTRACT
Graphs are canonical examples of high-dimensional non-Euclidean
data sets, and are emerging as a common data structure in many
Ô¨Åelds. While there are many algorithms to analyze such data, a signal
processing theory for evaluating these techniques akin to detection
and estimation in the classical Euclidean setting remains to be developed. In this paper we show the conceptual advantages gained by
formulating graph analysis problems in a signal processing framework by way of a practical example: detection of a subgraph embedded in a background graph. We describe an approach based on
detection theory and provide empirical results indicating that the test
statistic proposed has reasonable power to detect dense subgraphs in
large random graphs.
Index Terms‚Äî Chi-squared test, community detection, graph
algorithms, high-dimensional data, signal detection theory

1. INTRODUCTION
A graph G = (V, E) is deÔ¨Åned as a set of vertices V and a
set of edges E, where each edge connects two vertices. In essence,
there is a number of entities (the vertices) with relationships deÔ¨Åned
between them. Due to their ubiquitous structure, graphs are used
in a wide variety of application domains, including the natural sciences, medicine and social network analysis. In biology, graphs have
been used to represent interactions between proteins [1, 2] and reproduction within a population in an evolutionary model [3]. Social
network analysis, where the data of interest are people and the relationships among them, is another very natural setting for graph processing. SigniÔ¨Åcant work has been done on the detection of communities [4, 5] and inÔ¨Çuential Ô¨Ågures [6] in social networks, frequently
using a graph as the primary data structure.
The graph has been an important data structure for the signal
processing community. Analysis of graphs derived from radio frequency or image data is common [7,8], as a graph structure can help
discriminate and classify interesting entities. In this context, however, the graphs are typically derived from Euclidean data.
In general graphs are non-Euclidean, which complicates the application of standard signal processing to graph problems. Still, it is
natural to seek a framework in which graph processing algorithms
can be studied and evaluated in much the same way as classical signal processing methods. Some effort has been made to deÔ¨Åne signal
This work is sponsored by the Department of the Air Force under Air Force
Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the author and are not necessarily endorsed by the
United States Government.

978-1-4244-4296-6/10/$25.00 ¬©2010 IEEE

5414

processing techniques for graphs [9], but this has focused primarily on smoothing (i.e., low-pass Ô¨Åltering) of geometric meshes represented by graphs, and no general theory exists. At a high level,
many graph problems can be cast in a signal processing context.
For example, the problem of Ô¨Ånding a speciÔ¨Åc subgraph in a larger
graph [10] seems naturally coupled with matched Ô¨Åltering for signal
detection, and other problems such as detecting a very dense subgraph [11], a frequently-occurring subgraph [12] or a certain behavioral pattern [13] all have a strong signal processing Ô¨Çavor to them.
As an example, there has been a substantial amount of work in
the area of anomaly detection in graphs. An algorithm is presented
in [14] for Ô¨Ånding anomalies that bridge highly connected subgraphs.
In [15], the authors use measures of entropy on a graph to determine
whether a given subgraph is anomolous. The authors of [16] propose and evaluate several algorithms for detecting anomolous occurrences in graph-based data, using metrics that are common in signal
processing. Since graphs are often used for detection of anomolous
occurrences or behavior, such problems could be presented in the
context of classical detection theory. Indeed, [16‚Äì18] present detection problems using graphical data and evaluate their techniques
with metrics common in signal processing, such as receiver operating characteristic (ROC) analysis.
These problems all have a similar underlying structure: Given a
graph G, we want to Ô¨Ånd GS ‚äÇ G such that GS is anomolous, dense
or equal to some template. Each problem resembles a classical detection problem, but due to the non-Euclidean nature of graphical
data (e.g., the lack of well-deÔ¨Åned vector operations), the same theoretical frameworks no longer exist. In most domains, there is some
natural ordering of the data, such as a time series or a frequency
spectrum. A matched Ô¨Ålter, for example, assumes a certain temporal
ordering of the data and thus operates in geometric, rather than exponential, time. While the non-Euclidean nature of graphs may prevent
anything quite so simple from being applied (since in general subgraph detection is an NP-hard problem), an important eventual goal
is to provide analogous theoretical structure.
In this paper we demonstrate the practical and conceptual advantages to be gained by formulating graph analysis problems in a
signal processing framework with a concrete example: the detection
of a subgraph embedded in a large background graph. In our problem formulation and empirical results, we demonstrate that presenting such problems in the framework of classical signal processing
not only provides a basis for algorithm comparison, but also enables
mature ideas in signal processing to be directly applied to the new
and growing Ô¨Åeld of graph analysis. The remainder of the paper is
organized as follows. Section 2 presents a subgraph detection algorithm including a deÔ¨Ånition of a test statistic, the noise and signal
models, and ROC analysis. In Section 3 we analyze the performance

ICASSP 2010

of the detection algorithm using different background models and
signal models of varying density. Section 4 summarizes the results
and highlights future directions.
2. DETECING ANOMOLOUS SUBGRAPHS
As an example of developing and applying signal processing
theory for graph data, we focus in the sequel on the problem of detecting an anomalous subgraph in a random graph. To formulate
the problem of subgraph detectability in the framework of classical
detection, we consider the background ‚Äúnoise‚Äù graph GB to be random, and the ‚Äúsignal‚Äù graph GS to be Ô¨Åxed. Akin to a classical
hypothesis testing scenario in a vector space, we may then deÔ¨Åne a
set of null and alternate hypotheses as follows:
(
H0 : The observed graph is ‚Äúnoise‚Äù GB
(1)
H1 : The observed graph is ‚Äúsignal+noise‚Äù GB ‚à™ GS .
We present the algorithm for detecting the presence of a subgraph,
deÔ¨Åne noise and signal models, and analyze its performance.
2.1. Test Statistic
To formulate our detection problem, we consider the spectral
decomposition of the modularity matrix described in [4]. The modularity matrix B of an unweighted, undirected graph G is deÔ¨Åned
as
B = A ‚àí kk T /2 |E| ,
where A is the adjacency matrix of G and k is a column vector whose
ith row contains the degree of vertex i. Essentially, it is a matrix of
the difference between the actual and expected number of edges between pairs of vertices. Since G is undirected, B will be symmetric
and thus will have a spectral decomposition
B = U ŒõU T
that has orthogonal eigenvectors corresponding to distinct eigenvalues, all of which will be real. We will consider B in the space of its
two principal eigenvectors‚Äîu1 and u2 , both unit vectors‚Äîto examine the statistics of these background models in a low-dimensional
space.
Our Chi-squared test statistic is calcuated using a 2 √ó 2 contingency table. Considering the two principal eigenvectors as points in
a plane, we determine how many of these two-dimensional points
(i.e., those deÔ¨Åned by the rows of [u1 u2 ]) fall into each quadrant.
This yields a 2 √ó 2 observation matrix O = {oij }, which is then
used to compute the expected number of points in each quadrant,
resulting in the matrix M = {mij }, where

2.2. Distribution of the Test Statistic: Noise Model
In signal processing, noise is typically modeled as a stochastic process where the distribution may or may not be i.i.d. For our
‚Äúbackground‚Äù graph models, i.e., the noise in the system, we consider two random graph models. In addition to the random graphs
deÔ¨Åned by Erd√∂s and R√©nyi [19], which are reminiscent of an i.i.d.
process since each edge occurs with equal independent probability,
recent work has focused on alternative models that exhibit phenomena frequently seen in real-world graphs, such as power-law degree
distributions. As an example of the latter type, we consider here
the R-MAT graph model [20], which uses a recursion on Kronecker
products to formulate edge probabilities that can yield heavy-tailed
degree distributions. We will thus use both Erd√∂s-R√©nyi (E-R) and
R-MAT as canonical models for our background graph, GB .
The distribution of test statistics for 10000 1024-vertex graphs
generated using the R-MAT method is shown in Fig. 1(a). A Gamma
probability density function with shape parameter 2, which appears
to be a good Ô¨Åt for the distribution, has been Ô¨Åt to the test statistics
and overlayed on the histogram. The distribution of test statistics
for 10000 E-R graphs is shown in Fig. 1(b). Again, the distribution
resembles the overlayed Gamma process.
2.3. Distribution of the Test Statistic: Signal Model
The ‚Äúforeground‚Äù (or signal) model in our graph signal processing problem is our subgraph of interest, GS . In signal processing,
the weaker the signal is, or the more it resembles the background,
the more difÔ¨Åcult it is to process, i.e., when the signal-to-noise ratio
(SNR) is low, signals are harder to detect, estimate, and classify. Our
intuition tells us that a similar property exists in a graphical setting,
and our initial investigation has conÔ¨Årmed this.
Given a foreground GS , we create a ‚Äúsignal + noise‚Äù model using the union operation on the two edge sets, i.e., G = GS ‚à™ GB
with G = (V, E), E = EB ‚à™ ES . We are interested in detecting
subgraphs that are highly interconnected, where detectability seems
intuitively apparent given the foreground‚Äôs anomolous structure in a
sparse, random background. In a 1024-vertex graph, we choose 12
vertices at `random
to comprise VS and and select a substantial frac¬¥
tion of the 12
=
66
possible edges to use as ES . After creating G,
2
we perform the same statistical analysis on its modularity matrix as
we did with GB . Fig. 1(c) demonstrates the markedly different distribution in the test statistics (again for 10000 randomly-generated
graphs) when there is a highly-connected embedding, in this case
containing all possible edges. Using an E-R background, where
such a dense subgraph is much more unlikely since the model is less
structured, we observe even greater separation of the test statistics
between GB and G, as shown in Fig. 1(d).

mij = (oi1 + oi2 )(o1j + o2j )/ |V | .
We then compute the deviation X = {xij } from the expected value
as
(oij ‚àí mij )2
.
xij =
mij
P
P
The resulting test statistic œá2 ([u1 u2 ]T ) = i j xij is then maximized with respect to rotation in the plane, i.e., for each graph we
compute
‚Äû¬ª
‚Äì
¬´
cos Œ∏ ‚àí sin Œ∏
œá2max = max œá2
[u1 u2 ]T .
(2)
sin Œ∏
cos Œ∏
Œ∏
We determine the presence of a ‚Äúsignal‚Äù graph by comparing œá2max
to a threshold.

5415

3. DETECTION PERFORMANCE
A Monte Carlo simulation was run in which we evaluated the
performance of the detection algorithm. For each case, the background graph GB has 1024 vertices and an average degree of approximately 12. The signal graph GS , as mentioned in Section 2.3,
has 12 vertices and we evaluated detection performance as the subgraph density was increased from 70% to 100% in increments of 5%.
We used both E-R and R-MAT backgrounds in this experiment. For
each combination of background model and signal density, we generated 10000 different background graphs, each time choosing 12
different vertices at random to comprise the signal graph. Based on
the desired subgraph density, we then randomly selected the edges

E‚àíR Background: Distribution of Test Statistics

R‚àíMAT Background: Distribution of Test Statistics

400

400

350

300

Sample Proportion

Sample Proportion

350

250
200
150
100

300
250
200
150
100
50

50
0
0

10

20

œá2max

30

40

0
0

50

10

20

(a)

40

50

E‚àíR w/ Embedding: Distribution of Test Statistics

R‚àíMAT w/ Embedding: Distribution of Test Statistics

400

350

350

300

300

Sample Proportion

Sample Proportion

30

(b)

400

250
200
150
100

250
200
150
100
50

50
0
0

œá2max

50

100

150

200
œá2max

250

300

350

0
0

400

(c)

50

100

150

200
œá2max

250

300

350

400

(d)

Fig. 1. Distribution of the test statistic œá2max of equation (2) shown under the null and alternate models (top and bottom rows, respectively),
for cases of E-R and R-MAT background graphs (left and right columns, respectively). Note that the horizontal axes are scaled differently for
the null and alternate models.
for the` signal
model, ES , such that |ES | is the subgraph density
¬¥
times 12
.
2
After creating the 10000 background and signal models, we
computed the test statistic from Section 2.1 for each case. This
resulted in distributions similar to those in Fig. 1, with clearer
separation between the null and alternate hypotheses as the signal
graphs get more dense. Considering a range of thresholds to declare
a detection, we demonstrate with ROC curves the detectability of
subgraphs with varying density using this algorithm.
As demonstrated in Fig. 2(a), when the 12-vertex subgraph has
only 70% of all possible edges, it is undetectable. By increasing the
number of edges in the subgraph, detection performance increases
until our subgraph becomes a 12-vertex clique, where we achieve
near-perfect detection performance. The distributions of test statistics for both the ‚Äúnoise‚Äù and ‚Äúsignal+noise‚Äù for the case of a complete subgraph are displayed on the same plot in Fig. 2(b). The two
distributions are highly seperable, with an equal-error rate of 2.12%
achieved by setting the threshold to 24.716. When using an E-R
background with average degree of 12, perfect detection is achieved
for false alarm probabilities greater than zero for all subgraph densities of 70% and higher. While we used backgrounds with the same
average degree, the more structured R-MAT model creates a back-

5416

ground in which clustering is less anomolous, and detection is more
difÔ¨Åcult.
4. SUMMARY
In this article we have demonstrated some of the practical and
conceptual advantages to be gained by formulating the graph processing problem of subgraph detection in a classical signal processing framework. This formulation provides not only a basis for the
performance comparison of various algorithms, by way of comparative ROC curve analysis, but also a means of relating new data types
and problem domains to the more mature setting of signal processing
in linear vector spaces.
In the case at hand, we provided empirical results indicating that
the test statistic we propose has reasonable power to detect dense
subgraphs in large random graphs. More broadly, this problem scenario can be viewed as a proxy for more general tasks involving
high-dimensional data sets that patently do not conform to the classical Euclidean setting. We are encouraged by the initial successes
documented in this article, and hope they will similarly encourage
others to join in the challenge of developing a more general signal
processing theory for graphs and other non-Euclidean data.

ROC: R‚àíMAT Background

R‚àíMAT w/ Embedding: Distribution of Test Statistics

500

Background Alone
Background and Foreground

450
400

0.8
Sample Proportion

Probability of Detection

1

0.6
70%
75%

0.4

80%
85%
90%

0.2

0

0.2
0.4
0.6
0.8
Probability of False Alarm

300
250
200
150
100

95%
100%

0

350

50
0
0

1

(a)

50

100

150
200
œá2max

250

300

350

(b)

Fig. 2. Operating characteristics of the subgraph detection test, shown for various subgraph densities (left), with empirical sampling distributions of the test statistic shown for the case of a 12-vertex clique (right).

5. REFERENCES
[1] Dongbo Bu, Yi Zhao, Lun Cai, Hong Xue, Xiaopeng Zhu,
Hongchao Lu, Jingfen Zhang, Shiwei Sun, Lunjiang Ling, Nan
Zhang, Guojie Li, and Runsheng Chen, ‚ÄúTopological structure
analysis of the protein‚Äìprotein interaction network in budding
yeast,‚Äù Nucleic Acids Research, vol. 31, no. 9, pp. 2443‚Äì2450,
2003.
[2] Nizar N. Batada, Teresa Reguly, Ashton Breitkreutz, Lorrie
Boucher, Bobby-Joe Breitkreutz, Laurence D. Hurst, and Mike
Tyers, ‚ÄúStratus not altocumulus: A new view of the yeast protein interaction network,‚Äù PLoS Biology, vol. 4, no. 10, pp.
1720‚Äì1731, 2006.
[3] Erez Lieberman, Christoph Hauert, and Martin A. Nowak,
‚ÄúEvolutionary dynamics on graphs,‚Äù Nature, , no. 433, pp.
312‚Äì316, 20 January 2005.
[4] M. E. J. Newman, ‚ÄúFinding community structure in networks
using the eigenvectors of matrices,‚Äù Phys. Rev. E, vol. 74, no.
3, 2006.
[5] Nan Du, Bin Wu, Xin Pei, Bai Wang, and Liutong Xu, ‚ÄúCommunity detection in large-scale social networks,‚Äù in Int‚Äôl Conf.
on Knowledge Discovery and Data Mining, 2007, pp. 16‚Äì25.
[6] Jon M. Kleinberg, ‚ÄúAuthoritative sources in a hyperlinked environment,‚Äù Journal of the ACM, vol. 46, no. 5, pp. 604‚Äì632,
September 1999.
[7] Keming Chen, Chunlei Huo, Zhixin Zhou, and Hanqing Lu,
‚ÄúUnsupervised change detection in SAR image using graph
cuts,‚Äù in IEEE Int‚Äôl Geoscience and Remote Sensing Symposium, July 2008, vol. 3, pp. 1162‚Äì1165.
[8] Anthony Krivanek and Milan Sonka, ‚ÄúOvarian ultrasound image analysis: Follicle segmentation,‚Äù IEEE Trans. on Medical
Imaging, vol. 17, no. 6, 6 December 1998.
[9] Gabriel Taubin, Tong Zhang, and Gene H. Golub, ‚ÄúOptimal
surface smoothing as Ô¨Ålter design,‚Äù in Proc. European Conference on Computer Vision, 1996, pp. 283‚Äì292.
[10] Boaz Gelbord, ‚ÄúGraphical techniques in intrusion detection
systems,‚Äù in Proc. Int‚Äôl Conf. on Information Networking,
2001, pp. 253‚Äì258.

5417

[11] Yuichi Asahiro, Refael Hassin, and Kazuo Iwama, ‚ÄúComplexity of Ô¨Ånding dense subgraphs,‚Äù Discrete Applied Mathematics,
vol. 121, no. 1‚Äì3, pp. 15‚Äì26, 2002.
[12] Mukund Deshpande, Michihiro Kuramochi, Nikil Wale, and
George Karypis, ‚ÄúFrequent substructure-based approaches for
classifying chemical compounds,‚Äù IEEE Trans. on Knowledge
and Data Engineering, vol. 17, no. 8, pp. 1036‚Äì1050, August
2005.
[13] Thayne R. Coffman and Sherry E. Marcus, ‚ÄúPattern classiÔ¨Åcation in social network analysis: A case study,‚Äù in Proc. IEEE
Aerospace Conf., 2004, pp. 3162‚Äì3175.
[14] Jimeng Sun, Juiming Qu, Deepayan Chakrabarti, and Christos
Faloutsos, ‚ÄúNeighborhood formation and anomaly detection in
bipartite graphs,‚Äù in Proc. IEEE Int‚Äôl. Conf. on Data Mining,
Nov. 2005.
[15] Caleb C. Noble and Diane J. Cook, ‚ÄúGraph-based anomaly
detection,‚Äù in Proc. ACM SIGKDD Int‚Äôl. Conf. on Knowledge
Discovery and Data Mining, 2003, pp. 631‚Äì636.
[16] William Eberle and Lawrence Holder, ‚ÄúAnomaly detection in
data represented as graphs,‚Äù Intelligent Data Analysis, vol. 11,
no. 6, pp. 663‚Äì689, December 2007.
[17] Tung Le and Christoforos N. Hadjicostis, ‚ÄúGraphical inference
for multiple intrusion detection,‚Äù IEEE Trans. on Information
Forensics and Security, vol. 3, no. 3, pp. 370‚Äì380, September
2008.
[18] Hsun-Hsien Chang, Jos√© M. F. Moura, Yijen L. Wu, and Chien
Ho, ‚ÄúEarly detection of rejection in cardiac MRI: A spectral
graph approach,‚Äù in Proc. 3rd IEEE Int‚Äôl Symp. on Biomedical
Imaging, April 2006, pp. 113‚Äì116.
[19] Paul Erd√∂s and Alfr√©d R√©nyi, ‚ÄúOn random graphs,‚Äù Publicationes Mathematicae, vol. 6, pp. 290‚Äì297, 1959.
[20] Deepayan Chakrabarti, Yiping Zhan, and Christos Faloutsos,
‚ÄúR-MAT: A recursive model for graph mining,‚Äù in Proc. Fourth
SIAM Int‚Äôl Conference on Data Mining, 2004, vol. 6, pp. 442‚Äì
446.

2012 IEEE Statistical Signal Processing Workshop (SSP)

TOWARD MATCHED FILTER OPTIMIZATION FOR SUBGRAPH DETECTION IN
DYNAMIC NETWORKS
Benjamin A. Miller and Nadya T. Bliss
MIT Lincoln Laboratory
Lexington, Massachusetts 02420
{bamiller, nt}@ll.mit.edu
ABSTRACT
This paper outlines techniques for optimization of filter coefficients in a spectral framework for anomalous subgraph detection. Restricting the scope to the detection of a known signal in i.i.d. noise, the optimal coefficients for maximizing the
signal‚Äôs power are shown to be found via a rank-1 tensor approximation of the subgraph‚Äôs dynamic topology. While this
technique optimizes our power metric, a filter based on average degree is shown in simulation to work nearly as well in
terms of power maximization and detection performance, and
better separates the signal from the noise in the eigenspace.
Index Terms‚Äî community detection, dynamic graphs,
graph algorithms, matched filtering, signal detection theory

exhibiting known behavior in a temporally independent and
identically distributed background graph. One key observation is that, for our chosen power metric, the optimal filter coefficients can be computed via a tensor decomposition. Such
decompositions have been used with dynamic graphs [1, 4],
but, to our knowledge, this technique has not been applied to
subgraph detection before.
The remainder of this paper is organized as follows. In
Section 2 we define the subgraph detection problem setting.
Section 3 shows that statistics of the noise are restricted by
placing a norm constraint on the filter coefficients, and in Section 4 we show that, given this constraint, we can optimally
determine the filter coefficients using a rank-1 tensor approximation. Simulation results for signal power maximization
and detection performance are provided in Section 5, and in
Section 6 we summarize and discuss future research.

1. INTRODUCTION
A graph G = (V, E) is a pair of sets: a set of vertices, V , and
a set of edges, E, which connect pairs of vertices. Graphs
are used in a host of applications in which the data of interest
include relationships (the edges) between entities (vertices),
including physics, social network analysis, and cyber security. Since connections between entities will vary over time
in many applications, a significant amount of recent research
has focused on time-varying graphs (e.g., [1]).
While graphs are broadly useful, their non-Euclidean nature complicates the applications of traditional signal processing paradigms. Recent work has focused on developing a detection theory framework for data in the form of graphs, akin
to that for Euclidean data [2]. An extension of signal detection theory to network data would be desirable for a variety
of disciplines.
The original detection framework of [2] was extended to
dynamic graphs via a temporal matched filtering technique
in [3]. In this paper, we take steps toward optimizing the filter coefficients in a restricted setting: detection of a subgraph
This work is sponsored by the Department of the Air Force under Air Force
Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the authors and are not necessarily endorsed by
the United States Government.

978-1-4673-0183-1/12/$31.00 ¬©2012 IEEE

113

2. PROBLEM MODEL
The signal detection problem has many flavors, and in this paper we focus on the detection of a known signal in independent, identically distributed noise. Our observation is an unweighted, undirected, time-varying graph, with G(n) denoting the graph at discrete time step n. This consists of a background GB (n), the noise in our observation, and may or may
not include a signal subgraph GS (n). As in [2, 3], we cast the
problem as a binary hypothesis test, in which the null hypothesis H0 is that the observation is only noise, G(n) = GB (n),
and the alternative hypothesis H1 is that there is also a signal
present, i.e., G(n) = G(n) ‚à™ GS (n). In our formulation, the
vertex set V remains constant throughout the time window of
interest; only the edge set E(n) changes.
The noise consists of i.i.d. Bernoulli graphs, meaning
graphs where edges occur based on the outcome of independent Bernoulli trials. The probabilities are not identical across
all pairs of vertices‚Äîas in ErdoÃãs‚ÄìR√©nyi random graphs‚Äîbut
at each individual time step the probability of an edge between vertex i and vertex j is the same, denoted pij = pji .
Thus, under H0 , the expected value of the adjacency matrix of the graph at any time instance, E [A(n)], is given by

P = {pij }, a |V | √ó |V | matrix of edge probabilities.
Under H1 , a dynamic subgraph that is unlikely to appear
under H0 is embedded into the background on a randomly
selected subset of the vertices, VS ‚äÇ V . In this formulation,
we know the subgraph‚Äôs temporal evolution pattern, but do
not know its location in the background. While this could
potentially be solved by a brute-force search, such an approach would be a form of the subgraph isomorphism problem, which is known to be NP-hard.
The temporal integration technique used is outlined in [3].
This method is based on spectral analysis of integrated graph
residuals. Let B(n) = A(n) ‚àí E[A(n)], the difference between the adjacency matrix at time n and its expected value.
The first step is to integrate these residuals over a defined time
window, computing
e
B(n)
=

L‚àí1
X

R|V | . The analysis in this section assumes knowledge of the
probability matrix P . We will analyze the moments of the
e
quantity uT B(n)u,
and assume an arbitary, fixed u of unit
e
magnitude. The first, simple observation is that,h since B(n)
i is
T e
a random variable minus its expected value, E u B(n)u =
e
is centered at the origin.
0, i.e., the distribution of uT B(n)u
The second-order moment of this quantity is given by
2
!2 3
¬ª‚Äú
L‚àí1
‚Äù2 ‚Äì
X T
T e
E u B(n)u
=E4
u (AB (n ‚àí i) ‚àí P ) uhi 5
i=0

=E

T

u B(n ‚àí i)uhi

i=0

=

L‚àí1
X

L‚àí1
X

#
T

u B(n ‚àí j)uhj

j=0

h2i E

¬ª‚Äú
‚Äù2 ‚Äì
uT B(n ‚àí i)u

i=0

(A(n ‚àí i) ‚àí E[A(n ‚àí i)]) hi ,

i=0

where L is the length of the time window and h is a sequence
of i real numbers. This has the form of a classical finite ime
pulse response (FIR) filter. We then consider B(n)
in the
space of its two principal eigenvectors. As in [2], we use a
chi-squared statistic based on a 2 √ó 2 contingency table for
detection of the presence of the subgraph. This table contains
e
the number of vertices (i.e., columns of B(n))
that are projected into each quadrant, and is maximized over rotation in
the 2D plane. In the experiments of [2], under H0 this projection was rather radially symmetric, and, thus, the detection
statistic was smaller than when the symmetry was skewed by
the embedding of an anomalous subgraph.
As a metric of signal and noise power, we use the spectral norm, i.e., the absolute value of the largest eigenvalue,
denoted by k ¬∑ k. To best detect the presence of the anomalous subgraph, our goal is to maximize signal power while
restricting noise power, that is, to use coefficients
‚ÄöL‚àí1
‚Äö
‚ÄöX
‚Äö
‚Äö
‚Äö
h‚àó = arg max ‚Äö
AS (n ‚àí i)hi ‚Äö
‚Äö
‚Äö
h
i=0
‚Äö
‚ÄöL‚àí1
‚Äö
‚ÄöX
‚Äö
‚Äö
subject to ‚Äö
(AB (n) ‚àí E[AB (n)]) hi ‚Äö = Œ∑.
‚Äö
‚Äö

"L‚àí1
X

(1)

=

L‚àí1
X

20
h2i E

4@

=

=

i=0

uj uk (ajk (n ‚àí i) ‚àí pjk )A 5

2
h2i 4

3
X

i=0
L‚àí1
X

12 3

j=1 k=1

i=0
L‚àí1
X

|V | |V |
X
X

ÀÜ
2u2j u2k E (ajk

2Àú

‚àí pjk ) ‚àí

X

ÀÜ
u4j E (ajj

2

3
X

4

2u2j u2k (pjk

‚àí

p2jk )

‚àí

X

u4j (pjj

‚àí

p2jj )5.

j

j,k

e
Regardless of the direction of u, the variance of uT B(n)u
scales linearly with the sum of the squares of the filter coefficients. To restrict the expected noise power, therefore, we
will fix the `2 norm of the vector of filter coefficients to be 1.
4. SIGNAL MAXIMIZATION
In this section, we determine coefficients that solve the optimization problem as stated in equation (1), and consider two
other formulations.
As discussed in Section 3, we restrict the
PL‚àí1
noise by setting i=0 h2i = 1, so the focus is on finding
‚Äö
‚ÄöL‚àí1
‚ÄöX
‚Äö
‚Äö
‚Äö
h = arg max ‚Äö
AS (n ‚àí i)hi ‚Äö .
‚Äö
h:khk2 =1 ‚Äö
‚àó

i=0

‚àí pjj )

j

j,k

h2i

2 Àú5

(2)

i=0

Here AS (n) is the |VS | √ó |VS | adjacency matrix of the dynamic foreground only, and AB (n) is the adjacency matrix of
the background alone. In the next two sections, we focus on
coefficient optimization in this problem setting.

This can be rewritten as
h‚àó = arg max max

L‚àí1
X

h:khk2 =1 kuk=1 i=0

hi uT AS (n ‚àí i)u.

(3)

Let A be a 3-way tensor in which A(i, j, k) contains the value
from the jth row and kth column of AS (n‚àíi). For symmetric
(undirected) subgraphs, (3) is equivalent to maximizing

3. NOISE REDUCTION
To restrict the noise power after integration, we use the property that
Àõ
Àõ
Àõ e
Àõ
e
kB(n)k
= max ÀõuT B(n)u
Àõ.

|V | |V |
L‚àí1
XX
X

A(i, j, k)hi uj wk ,

i=0 j=1 k=1

kuk2 =1

Rather than truly limit the maximum eigenvalue, we will ree
strict the variance of B(n)
in any 1-dimensional subspace of

114

with the `2 norms of h, u and w all constrained to be 1. This
can be solved by finding the rank-1 approximation of A, i.e.,

to compute h, u and w, and a scalar Œª, such that
A ‚âà Œª(h ‚ó¶ u ‚ó¶ w),

where ‚ó¶ denotes the 3-way tensor outer product, with the
(i, j, k)th entry of h‚ó¶u‚ó¶w equal to hi uj wk . This is analogous
to approximating a matrix M by the scaled outer product of
its principal left and right singular vectors
P P u and w, which also
maximize the quantity uT M w = i j mij ui wj . We can,
thus, solve (2) by computing the rank-1 tensor approximation
of A (via a CP decomposition using the Matlab Tensor Toolbox1 ) and use the resulting vector h as the filter coefficients.
In [3], the filter coefficients used were proportional to the
largest eigenvalues (in magnitude) of the associated adjacency
matrices, i.e., the instantaneous signal power. While this provided adequate integration gain in the simulations, it is only
the optimal solution for (2) when the principal eigenvector of
AS (n) is constant across n. We provide results with such a
filter as a point of comparison.
Finally, if the task requires not only detection of anomalous activity but also localization, i.e., determining which vertices are exhibiting the activity of interest, then maximizing
the largest eigenvalue may not be optimal. In this case, it may
be ideal to emphasize the cross section of the integrated residuals space that points equally in the direction of all subgraph
vertices. To do this, we maximize the quantity
L‚àí1
X
i=0

L‚àí1
1T|V |
1|V |
1 X
hi p S AS (n ‚àí i) p S =
hi Vol(GS (n ‚àí i)),
|VS | i=0
|VS |
|VS |

where 1N is a column vector of N ones and Vol(¬∑) is the volume of the graph (the sum of the vertex degrees). Thus, a filter
based on the subgraph‚Äôs average degree will most emphasize
the portion of the residuals space aligned with the subgraph.
5. RESULTS
We ran several Monte Carlo simulations to demonstrate the
filter optimization techniques. In each experiment, we consider a time window of 32 samples. At each sample, the
background consists of an independent R-MAT Kronecker
graph [5] with 1024 vertices, a typical average degree of about
10, and a base probability matrix
¬ª
PRMAT =

0.5
0.125

0.125
0.25

‚Äì
.

We run the R-MAT algorithm for a fixed number of iterations,
so it is a Bernoulli graph as discussed in Section 3.
The foreground in these simulations consists of a 20vertex subgraph, divided into 2 portions. The behavior of
the subgraph involves one subset of the vertices densifying
over the first half of the window, with edges then shifting
to the other portion over the second half, e.g., a community
forming, then bringing in new members as others leave. Two
subsets V1 , V2 ‚äÇ VS both have 12 vertices, with 4 of them
1 Available

online at http://www.sandia.gov/‚àºtgkolda/TensorToolbox/.

115

Fig. 1. Spectral norms of integrated adjacency matrices of the
shifting subgraph using maximum eigenvalue and average degree for filter coefficients. Norms are presented as a fraction
of the norm obtained when using the tensor decomposition.

overlapping. The subgraph starts with no edges and, over the
first half of the time window, adds edges within V1 until it
reaches a density d. In the second half of the window, edges
are removed from V1 and added to V2 , while maintaining the
total number of edges, until edges only exist within V2 .
We first demonstrate the difference in the spectral norm
of the integrated signal subgraph using the three filters specified in Section 4. The results of a 10,000-trial Monte Carlo
simulation are summarized in Fig. 1. In each trial, a different foreground is randomly generated. As expected, the tensor decomposition always provides the greatest spectral norm,
and this figure shows the relative magnitudes using the other
techniques. The filter based on average degree performs quite
well, always achieving a norm of at least 96.5% of the optimal value. The filter based on maximum eigenvalues does
not always underperform the average degree filter, obtaining
a larger norm in just over 4% of the trials, but does frequently
take on significantly smaller values.
When embedding the subgraph into the R-MAT background, we fixed the foreground across all experiments for
each density to maintain consistency of the filter coefficients.
As an esimate for E[A], we simply use the average of the adjacency matrices over the time window. (This does preclude
e being a zero
using a uniform filter, as it would result in B
matrix.) For each filter, we computed chi-squared statistics
(as described in Section 2) for 10,000 graphs under H0 and
H1 . Receiver operating characteristic curves are presented
in Fig. 2, with detection performance in all cases increasing
as the maximum subgraph density increases from 20% to
35%. The plots confirm that the filter using average degree
yields substantially similar results to the optimal tensor decomposition, while both outperforming the filter based on
maximum eigenvalues. Indeed, the equal error rates for when
using filters derived from the average degree and the tensor
decomposition differ by at most 2%.
While the tensor decomposition maximizes signal power,
other metrics are worth considering as well. In Fig. 3, we

Fig. 2. ROC curves for detection of the deterministic foreground in i.i.d. R-MAT backgrounds. Performance is shown for filters
derived using maximum eigenvalue (left), average degree (center) and the tensor decomposition (right).

demonstrate that average degree may be better for the task of
subgraph identification. The figure shows the principal twoe when the highest density is set
dimensional subspace of B
to 60%. The tensor decomposition method achieves a larger
maximum eigenvalue, but, as shown in the figure, about 8 of
the vertices are buried within the background noise. These
vertices comprise V2 \ V1 , the vertices that have no edges until the second half of the window. Using the average degree
filter, on the other hand, allows near-perfect separation in the
first eigenvector. Since empirical detection performance between these methods is extremely similar, it is possible that an
average degree filter would be preferable in some situations.

6. SUMMARY
This paper provides techniques for optimization of filter coefficients in a spectral framework for subgraph detection. The
optimal coefficients are shown to be found via a rank-1 tensor approximation of the subgraph, but a filter based on average degree is shown in simulation to work nearly as well,
and does a better job separating the signal from the noise in
the eigenspace. Since these results depend on the assumption
that the noise is i.i.d., determining a noise whitening process
for models such as [6] will be useful future work. Expanding
this analysis to non-Bernoulli graphs and random signals with
known distributions will also be of significant interest.
7. REFERENCES
[1] J. Sun, D. Tao, and C. Faloutsos, ‚ÄúBeyond streams
and graphs: Dynamic tensor analysis,‚Äù in Proc. KDD,
pp. 374‚Äì383, 2006.
[2] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúToward signal
processing theory for graphs and non-Euclidean data,‚Äù in
Proc. ICASSP, pp. 5414‚Äì5417, 2010.
[3] B. A. Miller, M. S. Beard, and N. T. Bliss, ‚ÄúMatched filtering for subgraph detection in dynamic networks,‚Äù in
Proc. SSP, pp. 509‚Äì512, 2011.
[4] D. M. Dunlavy, T. G. Kolda, and E. Acar, ‚ÄúTemporal link
prediction using matrix and tensor factorizations,‚Äù ACM
Trans. Knowl. Discovery from Data, vol. 5, Feb. 2011.

Fig. 3. Scatterplots of the principal 2-dimensional subspace
e While the tensor decomposition (top) obtains the greatof B.
est signal power, the average degree filter (bottom) provides
better foreground/background separation in this space.

116

[5] D. Chakrabarti, Y. Zhan, and C. Faloutsos, ‚ÄúR-MAT: A
recursive model for graph mining,‚Äù in Proc. SIAM Int.
Conf. Data Mining, vol. 6, pp. 442‚Äì446, 2004.
[6] E. Wang, J. Silva, R. Willett, and L. Carin, ‚ÄúTimeevolving modeling of social networks,‚Äù in Proc. ICASSP,
pp. 2184‚Äì2187, 2011.

A SCALABLE SIGNAL PROCESSING ARCHITECTURE FOR MASSIVE GRAPH ANALYSIS
Benjamin A. Miller1 , Nicholas Arcolano1 , Michelle S. Beard1 , Jeremy Kepner1 , Matthew C. Schmidt1 ,
Nadya T. Bliss1 and Patrick J. Wolfe2
1

Lincoln Laboratory, Massachusetts Institute of Technology, Lexington, MA, 02420
{bamiller, arcolano, michelle.beard, kepner, matthew.schmidt, nt}@ll.mit.edu
2
Statistics and Information Sciences Laboratory, Harvard University, Cambridge, MA, 02138
wolfe@stat.harvard.edu
ABSTRACT
In many applications, it is convenient to represent data as a graph,
and often these datasets will be quite large. This paper presents an
architecture for analyzing massive graphs, with a focus on signal
processing applications such as modeling, Ô¨Åltering, and signal detection. We describe the architecture, which covers the entire processing chain, from data storage to graph construction to graph analysis and subgraph detection. The data are stored in a new format
that allows easy extraction of graphs representing any relationship
existing in the data. The principal analysis algorithm is the partial
eigendecomposition of the modularity matrix, whose running time
is discussed. A large document dataset is analyzed, and we present
subgraphs that stand out in the principal eigenspace of the timevarying graphs, including behavior we regard as clutter as well as
small, tightly-connected clusters that emerge over time.
Index Terms‚Äî Graph theory, large data analysis, processing
architectures, residuals analysis, emergent behavior

of graphs, a focus on detectability of anomalies and scalability of
architectures and algorithms will become increasingly important.
In this paper, we introduce a signal processing architecture
speciÔ¨Åcally designed for the analysis of massive graphs. This architecture encompasses the entire graph processing procedure, from
storage of the raw data to extraction of relational structure to analysis
of the resulting graph. Building on recently-developed technologies,
this architecture provides a framework with which we can easily
analyze large datasets containing complex relationships.
The remainder of this paper is organized as follows. In Section 2, we describe the architecture and discuss the data storage format, the graph construction procedure, and the analysis algorithms
and their complexity. Section 3 introduces our dataset of interest‚Äî
a large document database‚Äîand outlines graphs of interest derived
from this dataset. In Section 4, we analyze the data using the algorithms described in Section 2, and describe emerging clusters found
in the data, as well as some ‚Äúclutter‚Äù structures that can obscure more
interesting behavior. In Section 5 we summarize and outline future
research.

1. INTRODUCTION

2. SYSTEM ARCHITECTURE

As data collection capabilities improve, the amount of data available
for analysis rapidly increases. While improved processing enables us
to work with much larger datasets, it is at the same time important to
develop scalable algorithms and architectures to handle and analyze
massive data.
In many applications, the data of interest can be represented as
a graph. A graph G = (V, E) is a pair of sets: a set of vertices, V ,
representing entities, and a set of edges, E, that represent relationships between the entities. This data representation is used in a wide
variety of domains, from the social sciences to physics and engineering. While convenient and intuitive, the analysis of graphs is complicated, as they are combinatorial structures of non-Euclidean data
and, thus, cannot be exactly analyzed in the context of Euclidean
vector spaces. To address this, recent work has focused on developing a statistical detection theory framework for graphs, akin to that
for Euclidean data [1]. As more applications use data in the form

Our processing chain consists of 3 stages. First, the data are stored
in the D4M format [2]. From this data, we construct graphs representing a variety of relationships. We then run analysis algorithms
on the resulting graphs. In this section we describe each component
in detail.

This work is supported by the Intelligence Advanced Research Projects Activity (IARPA) via Air Force Contract FA8721-05-C-0002. The U.S. Government is authorized to reproduce and distribute reprints for Governmental
purposes notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the ofÔ¨Åcial
policies or endorsements, either expressed or implied, of IARPA or the U.S.
Government.

978-1-4673-0046-9/12/$26.00 ¬©2012 IEEE

5329

2.1. Data into D4M
Given a large dataset, D4M provides a convenient, intuitive interface
for accessing subsets of the data. In this format, the data are stored
in a 2-dimensional associative array, which is a sparse matrix whose
rows and columns are indexed by keys rather than integers. Consider, for example, an associative array holding information about
papers at a conference. If each record contains the paper title, authors, and session (e.g., My Paper Title, A. Researcher, Session 5),
there will be nonzero values in row "title/My Paper Title" only in
columns "author/Researcher, A." and "session/Session 5".
To extract a subset of the data, we can index into ranges of the
associative array, just as is done with matrices in Matlab. For example, to extract the work by all authors with the name Researcher in
the example database, we simply index into the column range "author/Researcher, A,:,author/Researcher, Z,". This operation returns
an associative array with its columns restricted to the authors of interest.

ICASSP 2012

2.2. Graph Extraction
With the data in the D4M format, we can easily extract a number
of graphs based on relationships in the data. The D4M architecture
allows for linear algebraic operations on the associative arrays, enabling easy construction of the adjacency matrices of a graph, i.e., a
matrix A representing a graph G = (V, E) wherein the entry in the
ith row and jth column of A is nonzero only if (vi , vj ) ‚àà E.
Returning to the conference program example, with an associative array A with rows indexed by titles and columns indexed by
authors and sessions. To build a graph in which the vertices are papers and an edge occurs between two vertices if the corresponding
papers are in the same session, the procedure is simple. First extract
the columns corresponding to sessions, i.e., create a new associative
array
b = A(:, ‚Äôsession/0,:,session/z,‚Äô),
A
which covers the range of sessions that begin with alphanumeric valb is a title-to-session associative array with exactly one
ues. Now A
nonzero value per row. To create the desired graph, we simply take
b T A,
b and A is the
the inner product of this array with itself, A = A
adjacency matrix of the session-cooccurrence graph.
2.3. Graph Analysis
Once the data are in network form, we run graph analysis algorithms
to detect interesting or anomalous behavior. In this paper, we focus
on the eigenspace analysis techniques outlined in [1, 3, 4]. These
algorithms are all based on spectral analysis of the modularity matrix [5], which we interpret as a graph-based residuals matrix. The
modularity matrix for an unweighted, undirected graph is given by
B =A‚àí

b = A ‚àí kout kin ,
B
|E|
where kout and kin are, respectively, a column vector of out degrees
(the edge counts going out of the vertices) and a row vector of in
degrees (the edge counts going into the vertices). Note that in this
case the ‚Äúexpected‚Äù term has |E| rather than 2|E| in the denominator, since the edges go in only one direction. As in [7], we use the
b+B
b T )/2. The running
‚Äúsymmetrized‚Äù modularity matrix B = (B
time will be greater than for undirected graphs, since it requires mulT T
tiplication at each iteration by A, AT , kout kin and kin
kout , but the
asymptotic running time will not change, so it will scale similarly.
For dynamic graphs, we consider the Ô¨Åltered modularity matrices over time, as in [4]. Let B(n) be the modularity matrix at discrete time step n. We accumulate the residuals over a time window
of  samples by applying a Ô¨Ånite impulse response Ô¨Ålter h to create
an aggregated residuals matrix
BÃÉ(n) =

‚àí1
X

h(i)B(n ‚àí i),

(1)

i=0

kk
.
2|E|
T

Here A is the adjacency matrix of the graph and k is the degree
vector, i.e., a vector in which the ith entry corresponds to the number
of edges adjacent to vertex vi . The term kkT /(2|E|) represents a
Ô¨Årst-order Ô¨Åt of the graph to the Chung‚ÄìLu random graph model, in
which the probability that an edge occurs between two vertices is
proportional to the product of their expected degrees [6]. We thus
treat B as a matrix of residuals‚Äîof the ‚Äúobserved‚Äù edges minus the
‚Äúexpected‚Äù edges‚Äîand analyze it to Ô¨Ånd anomalies.
The major computation in this analysis is the calculation of the
eigenvectors and eigenvalues of B. This is done using eigs in Matlab, which uses the implicitly restarted Lanczos method to compute
extreme eigenvalues and their corresponding eigenvectors. One issue in doing this computation is that, while a large A is likely very
sparse, the corresponding B is dense (indeed, an entry in B will
only be zero if an associated vertex is isolated). However, as Newman points out in [5], since B is the sum of a sparse matrix and
a rank-1 matrix, we can compute the eigenvectors efÔ¨Åciently without computing B. The function eigs can compute eigenvalues and
eigenvectors of a function, so by creating f : R|V | ‚Üí R|V | such that
f (x) = Bx = Ax ‚àí k

method computes the eigenvalues via iterative matrix-vector multiplication, or in our case iteratively evaluating f (x). Assuming
|V | < |E|, computing Ax costs O(|E|) operations, and adding the
computation and subtraction of k(kT x)/(2|E|) adds an additional
4|V | operations, which does not increase the asymptotic running
time. This algorithm, therefore, scales linearly in |E| for a constant m and linearly in |E|m if m grows no faster than the average
degree kavg . The space requirement for this method is O(|V |m),
which scales linearly in |E| if m is O(kavg ).
We are also interested in analyzing directed and dynamic graphs.
The modularity matrix for a directed graph is given by

kT x
,
2|E|

we can exploit the form of B for a much less intense computation.
The running time of eigs on an adjacency matrix is O(|E|m +
|V |m2 + m3 ) per restart, where m is the number of eigenvectors
computed. (The number of restarts is dependent upon the gap in
magnitude between consecutive eigenvalues of B.) The Lanczos

5330

where h(i) is a scalar for all integers 0 ‚â§ i < . Each iteration now
requires multiplication by the adjacency matrix and degree vectors
at each time step within the window. Letting E(n) be the edge set at
P‚àí1
time n, this process has running time O(|V | + i=0
|E(n ‚àí i)|).
(The vertex set is assumed to be Ô¨Åxed to maintain the matrix dimensions.) This slightly alters the running time of the Lanczos procedure, making it O(EÃÑm + |V |m + |V |m2 + m3 ), where EÃÑ is the
average cardinality of E over the time window. For a Ô¨Åxed window
length, this algorithm scales the same as in the static case.
3. DATASET AND GRAPH CONSTRUCTION
The dataset we analyze is the commercially available Thomson
R
Reuters Web of Science
(WoS) database [8]. This dataset is
comprised of records, compiled for research purposes, representing scholarly publications of the international scientiÔ¨Åc community,
published between 1900 and present in public commercial and open
source journals and conference proceedings. Each record represents
an individual document, and Ô¨Åelds include document title and type,
journal name, author names and institutional afÔ¨Åliations (as provided in publication), cited references, and publication date. There
are several interesting dynamic graphs we can extract from this data,
including coauthorship graphs, citation graphs, and graphs associated with some notion of document similarity, such as common
n-grams.
We obtained a snapshot of the database that contains over 42
million records. The raw data were parsed and inserted into a
database that can be accessed via the D4M interface. The data are
stored in associative arrays in which the rows correspond to unique

Fig. 1. Subject-to-subject citation counts in the Ô¨Årst 50 years of WoS
data (log10 scale).

identiÔ¨Åers and the columns are the document metadata. The majority of the data are in a single associative array, with the more
text-intensive data, such as titles and abstracts, in a separate table.
This data consumes about 300 GB and Ô¨Åts on a single database node.
Of the many potential graphs we can construct from the WoS
dataset, we focus on two in this paper: a coauthorship graph and a
citation graph. Both graphs are dynamic, and we use a temporal resolution of one year (since in many cases month and day are not available). In the coauthorship graph Gauth , the vertices represent authors
and two vertices share an edge in Gauth (n) if they coauthor a document published in year n. This is an unweighted graph, although the
edges could include weights corresponding to the number of documents coauthored. In the citation graph Gcite , the vertices represent
documents and a directed edge occurs from u ‚àà V to v ‚àà V in
Gcite (n) if u cites v in year n or earlier. The rationale for using a cumulative graph for citations but not authors is that citations are, for
the most part, static (i.e., the documents cited are Ô¨Åxed at the time of
publication), while authors may change collaborators over time.
These graphs are easily constructed from the associative arrays
in which we store the data. To create a coauthorship graph, select
the author columns and take the inner product of the resulting associative array with itself. To break up the graph by year, we can Ô¨Årst
select subsets of rows with nonzero entries in columns corresponding
to a certain date range. For citation graphs, the construction process
is even more straightforward. Since there are columns corresponding to the identiÔ¨Åer for the cited documents, we simply extract the
columns for the references.
It is worth noting at this point that, in addition to graphs, we
can construct matrices of useful data statistics in a similar fashion. For example, in an anomaly detection problem, we may want
to determine the likelihood that an article published in one subject
will cite an article in another. To get the subject-to-subject citation
counts, we extract a document-to-subject array Ads and a documentto-document citation array Acite , and compute C = ATds Acite Ads .
The subject-to-subject citation count matrix for the Ô¨Årst 50 years of
WoS records is shown in Fig. 1. The largest value in a given row or
column of C tends to fall on the main diagonal, indicating that docu-

5331

Fig. 2. The largest 30 eigenvalues of the coauthorship graph (top)
and citation graph. There is a slow upward trend in both cases with
a number of excursions, which in most cases correspond to clutter.

ments are often most likely to cite (or be cited by) other documents in
the same subject area. There are also clusters corresponding to subÔ¨Åelds of a topic area; for example, chemistry is split into analytical,
applied, and other subÔ¨Åelds. Using this information has the potential
to increase our modeling ability, providing a better expected value
model than the one discussed in Section 2.3 using tools from link
prediction and the point process model of [9].

4. DATA ANALYSIS
At the time of this writing, we have extracted Gauth and Gcite over the
course of the Ô¨Årst 60 years of WoS records. In the 2 million records
in the database over this time period, there are 549,726 unique authors and 4,668,824 documents (including cited documents that are
not in the database), so these are the sizes of the vertex sets in the corresponding graphs. We analyze each dynamic graph over a sliding
5-year time window, using a ramp Ô¨Ålter (i.e., h(i) in (1) decreases
linearly as i increases) to emphasize emerging connectivity. The 30
largest eigenvalues and eigenvectors of BÃÉ are computed, which, for
the (larger) citation graph, takes approximately one hour per time
window on a single processor.
The largest eigenvalues of the integrated modularity matrices are
shown in Fig. 2. In both graphs, the eigenvalues gradually increase
over the course of the 60 years (although more quickly in the citation
graph since it is cumulative). There are several points in which the
largest eigenvalues deviate substantially from the general trend. We
will consider one window from each dynamic graph, as indicated in
the Ô¨Ågure: the window centered at 1948 for Gauth and the window
centered at 1952 for Gcite . In both of these windows, several of the

Fig. 3. Emerging clusters in the WoS graphs. Adjacency matrices are shown for subsets of the coauthorship graph (top row) and the citation
graph (bottom row) over 5-year time windows (increasing year from left to right). In both cases, tightly connected clusters emerge over time.

eigenvalues are signiÔ¨Åcantly larger than eigenvalues of similar rank
in earlier and later windows.
4.1. Clutter
Upon deeper inspection, we Ô¨Ånd that many of the vertices that stand
out in the space of largest modularity are somewhat uninteresting,
which we see as analogous to clutter in radar processing. In Gauth ,
several of the eigenvectors with large eigenvalues are aligned with
large cliques in which no authors had previously collaborated. These
large cliques (on the order of 50 vertices) often occur for documents
of type ‚Äúdiscussion‚Äù, and could be easily detected by Ô¨Ånding documents with large author lists.
In the citation graph we see a different kind of clutter. The residuals space of Gcite is often dominated by review articles that cite
many hundreds (sometimes thousands) of documents. Again, we do
not really beneÔ¨Åt from analyzing the data as a dynamic graph, as
these vertices could be found by simply looking for documents with
long lists of references.
4.2. Emerging Clusters
Looking under the clutter, however, we see some interesting behavior involving emerging clusters of nodes. Looking in the space of
eigenvectors that do not correspond to clutter behavior, we Ô¨Ånd a
few vertex subsets that stand out from the rest of the graph. Sparsity
patterns of the adjacency matrices corresponding to these subgraphs
are shown in Fig. 3. In Gauth , we see two sets of authors (publishing
mostly in medical journals), each of which gradually increases the
number of connections within the subset over the window. These
subgraphs are smaller than the large cliques that appear suddenly in
the same window: one has 20 vertices and the other has 32, and neither is ever a clique. However, our temporal integration technique
emphasizes the increasing connectivity over time and brings these
subgraphs into the space of eigenvectors with larger eigenvalues.
We see similar behavior in Gcite . In this case, as time passes,
we see two subsets of documents accumulating a signiÔ¨Åcant amount
of internal citation, with some citation between the two subsets.
Most documents in these subsets are in biochemistry and microbiology, with a few in medicine and other areas, and largely focus
on metabolic properties of various acids and proteins. Again, the
emerging connectivity is emphasized by the ramp Ô¨Ålter and these
relatively small subsets are brought into the space of large residuals.

5332

5. SUMMARY
In this paper, we outline an architecture for analyzing large graph
data. The architecture is supported by the D4M framework, allowing
easy and intuitive construction of graphs from databases, and uses an
efÔ¨Åcient eigenspace analysis technique for signal-processing-based
analysis on the constructed graphs. We analyze two large, dynamic
graphs derived from the Web of Science database (one with over 500
thousand and one with over 4 million nodes), and Ô¨Ånd different types
of clutter in the different graphs, as well as small, emerging clusters
in the eigenspace of graph residuals. Future work will include developing automated methods to Ô¨Ålter away clutter in the graph data,
incorporating metadata into our models of graph residuals, and analyzing multi-graphs in which different types of edges correspond to
different relationships.
6. REFERENCES
[1] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúToward signal processing theory for graphs and non-Euclidean data,‚Äù in Proc.
ICASSP, 2010, pp. 5414‚Äì5417.
[2] J. Kepner, ‚ÄúMassive database analysis on the cloud with D4M,‚Äù
in Proc. HPEC Workshop, 2011.
[3] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúSubgraph detection
using eigenvector L1 norms,‚Äù in Advances in Neural Inform.
Process. Syst. 23, J. Lafferty, C. K. I. Williams, J. Shawe-Taylor,
R. Zemel, and A. Culotta, Eds., 2010, pp. 1633‚Äì1641.
[4] B. A. Miller, M. S. Beard, and N. T. Bliss, ‚ÄúMatched Ô¨Åltering for
subgraph detection in dynamic networks,‚Äù in Proc. IEEE Statistical Signal Process. Workshop, 2011.
[5] M. E. J. Newman, ‚ÄúFinding community structure in networks
using the eigenvectors of matrices,‚Äù Phys. Rev. E, vol. 74, no. 3,
2006.
[6] F. Chung, L. Lu, and V. Vu, ‚ÄúThe spectra of random graphs
with given expected degrees,‚Äù PNAS, vol. 100, no. 11, pp. 6313‚Äì
6318, 2003.
[7] E. A. Leicht and M. E. J. Newman, ‚ÄúCommunity structure in
directed networks,‚Äù Physics Review Letters, vol. 100, 2008.
[8] ‚ÄúThomson Reuters Web of Science,‚Äù http://thomsonreuters.com/
products_services/science/science_products/a-z/web_of_science.
[9] P. O. Perry and P. J. Wolfe, ‚ÄúPoint process modeling for directed
interaction networks,‚Äù 2010, http://arxiv.org/abs/1011.1703.

PMATLAB: PARALLEL MATLAB LIBRARY FOR SIGNAL PROCESSING APPLICATIONS'

Nadya T. Bliss, Jeremy Kepner, Hahn Kim, Albert Reuther
{nt, kepner, hgk, reuther}@ll.mit.edu
MIT Lincoln Laboratory
ABSTRACT

MATLABR is one of the most commonly used languages for
scientific computing with approximately one million users
worldwide. At MIT Lincoln Laboratory, MATLAB is used by
technical staff to develop sensor processing algorithms.
MATLAB'S popularity is based on availability of high-level
abstractions leading to reduced code development time. Due
to the compute intensive nature of scientific computing,
these applications often require long running times and
would benefit greatly from increased performance offered

by parallel computing. pMatlab (,,it.edu,

)
a
implements partitioned global address space (PGAS)
support via standard operator overloading techniques. The
core data structures in pMatlab are distributed arrays and
maps, which simplify parallel programming by removing
the need for explicit message passing. This paper presents
the pMaltab design and results for the HPC Challenge
benchmark suite. Additionally, two case studies of pMatlab
use are described.
Index Terms - data processing, parallel languages,
parallelprogramming, software
1. INTRODUCTION

MATLAB has emerged as one of the predominant languages
for scientific and technical computing and is widely used at
MIT Lincoln Laboratory for signal, image, and sensor
processing. MATLAB's popularity is largely dependent on

the expressiveness of the language and powerful graphics
that allow visualization of multi-dimensional data sets. The
users of MATLAB tend to be engineers and scientists, and
high-level languages allow them to concentrate on their core
competency and not implementations details. However, to
fully test the validity of the algorithms, test runs on large
data sets, with broader range of parameters are required.
This often causes the codes to run for hours and even days
and parallel capability without significant increase in

programming complexity is beneficial. The pMatlab library
provides this capability by implementing partitioned global
address space (PGAS) support in MATLAB by introducing
two core data structures: distributed arrays and maps. This
paper describes the design of pMatlab and performance
results of pMatlab implementations of the HPC Challenge
benchmarks. Additionally, it highlights two pMatlab case
studies at MIT Lincoln Laboratory.
The paper is organized as follows: Section 2 highlights
related work; Section 3 discusses pMatlab design along with
programming models. Section 4 presents benchmark results,
while Section 5 discusses pMatlab use at the Laboratory.
Finally, Section 6 summarizes and concludes the paper.
2. RELATED WORK

Parallel MATLAB has been an active area of research for a
number of years and many different approaches have been
developed. These different approaches can be roughly
divided into three categories: message passing, client/server
and PGAS (partitioned global address space).
The message passing approach [4, 10] requires the user to
explicitly send messages within the code. These approaches
often implement a variant of the Message Passing Interface
(MPI) standard [16]. While MPI approaches are powerful,
they significantly increase coding complexity. Nonetheless,
a message passing functionality is the minimum requirement
for parallel programming. Among the available MATLAB
message passing implementations, MatlabMPI is currently
the most popular implementation with thousands of users
worldwide. More recently, the incorporation of MPI into
The MathWorks' Distributed Computing Toolbox (DCT)
[6] makes message passing available to a much broader
range of users.

Client/server approaches [2, 15] use MATLAB as the user's
front-end to a distributed library. For example, Star-P keeps

IThis work is sponsored by the Department of the Air Force under Air Force contract FA8721-05-C-0002. Opinions, interpretations,
conclusions and recommendations are those of the author and are not necessarily endorsed by the United States Government.
MATLABR is a registered trademark of the MathWorks. Reference to commercial products, trade names, trademarks or manufacturer does not constitute or
imply endorsement.

1-4244-0728-1/07/$20.00 C2007 IEEE

IV - 1189

ICASSP 2007

the distributed arrays on a parallel server, which calls the
necessary routines from parallel libraries. These approaches
often provide the best performance once the data are
transferred to the server. However, these approaches are
limited to those functions that have been specifically linked
to a parallel library and require installation of the additional
libraries.
pMatlab falls into the third category, the PGAS approach.
Star-P and Falcon [9] also fall into this category. These
approaches provide a mechanism for creating global arrays,
which are distributed across multiple processors. Global
arrays have a long history in other languages, for example
Fortran [11, 18] and C [8], as well as in many C++ libraries
such as POOMA [5], GA Toolkit [17], PVL and VSIPL++
[12]. The global array approach allows the user to view a
distributed object as a single entity. This approach allows
operations on the array as a whole or on local parts of the
array.
pMatlab is a unique parallel MATLAB implementation for a
number of reasons. pMatlab supports global arrays and
allows combining global arrays with direct message passing
for optimized performance. While pMatlab does use
message passing in the library routines, a typical user does
not have to explicitly incorporate messages into the code.
pMatlab does not link in any external libraries, nor does it
compile the language into an executable. Our library is
implemented entirely in MATLAB, which significantly
reduces the size of the library while providing support for
distributions and redistributions of up to four-dimensional
arrays distributed with any combination of block-cyclic
distributions.
3. PMATLAB DESIGN AND IMPLEMENTATION

The pMatlab library is designed and implemented at MIT
Lincoln Laboratory and builds upon concepts from the
Parallel Vector Library (PVL) and Star-P, and uses
MatlabMPI as the communication layer. Figure 1 illustrates
the layered architecture of the parallel library. In the
architecture, the pMatlab library implements distributed
array constructs. In addition, a subset of functions, such as
plus, minus, fft, mtimes, and all element-wise
operations are implemented to operate on distributed arrays.
If a user requires additional functionality, s/he has the
flexibility of implementing specialized functions that are
optimized for the required data sizes and distributions.

pMatlab uses standard operator overloading techniques.
pMatlab map objects (see Section 3.1) can be passed to a
MATLAB constructor, such as rand, or zeros. The
constructors are overloaded and when a map object is
passed into a constructor, the library creates a variable of
type dmat, or a distributed array. pMatlab supports
numerical arrays of up to four dimensions of different

numerical data types and allows creation of distributed
sparse matrices.
ApplicatLion

M*

Inpu

_
M*

Aril yii

Oipat

- - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - - -

User interface

Parallelhbrary

-

- -

-

-

-

- -

-

-

-

- -

-

-

- -

-

-

-

- -

-

-

-

- -

-

-

-

- -

-

-

-

- -

-

-

- -

-

-

--Hardware interfac

Parallel hatdwae|

Figure 1. Layered architecture.

3.1. Maps

The concept of using maps to describe array distributions
has a long history. The ideas for pMatlab maps are
principally drawn from the High Performance Fortran (HPF)
community [13, 20], MIT Lincoln Laboratory Space-Time
Adaptive Processing Library (STAPL) [2], and Parallel
Vector Library (PVL). A map for a numerical array defines
how and where the array is distributed (Figure 2).
The pMatlab map construct is defined by three components:
(1) grid description, (2) distribution description, and (3)
processor list. The grid description together with the
processor list describes where the data object is distributed,
while the distribution describes how the object is
distributed. pMatlab supports any combination of blockcyclic distributions up to four dimensions. Data overlap,
required for some image processing applications, is also
supported through the map interface. The addition of maps
to the API represents the only major change to the general
MATLAB syntax.

I oo

wvith

-

_40I

pecifcation together

wocessor Wltt describe

the data is distributed.

a

aj i tyi U;q I ;.
describe hW the da Is
distributed (deftult is b

(4,66 m ; )
POP

1iIMATLAB onstruddra are overloaded
to take a mp as an argument, and
returni a dmat, a distributed array.

,.

A

=i

go

Y

W

Figure 2: Anatomy of a map. A map is defined as an assignment
of blocks of data to processing elements.

While maps introduce a new construct, they have significant
advantages over both message passing approaches and
predefined limited distribution approaches. Specifically,
pMatlab maps are scalable and allow the user to separate the
task of mapping the application from the task of writing the
applications. Additionally, maps make it easy to specify
different distributions for different algorithms. Finally, maps
support pipelining via mapping of different computations on
different subsets of processors.

IV- 1190

3.2. Programming Models

A

pMatlab supports both pure global array and fragmented
global array programming models (see Figure 3). Pure
global arrays provide the highest level of abstraction and
require minimum changes to the code.
It is impractical to provide optimized implementations of the
approximately 8,000 built-in functions for every
combination of array distributions. Instead, pMatlab also
supports fragmented global array programming style. This
style is less elegant but provides strict guarantees on
performance. Here, distributed arrays are used as containers
- the data is extracted from the distributed array, operated
on, and then inserted back into the distributed array.

A(Q)

A(i,)

A.Ica!(Q) A|I{cij)
P

sor

A({jJ

A(s

Processor.. Processor Npe

Proc ssor

Proceor

A

Pr1ocs

PIocal(lj

A|haca(ijI

Proesor

morrads
FraFmenteda Glorlog

ProcesrN

Figure 3. pMatlab programming models.

4. BENCHMARK RESULTS

This section focuses on pMatlab benchmark results.
Performance is compared to serial MATLAB and C+MPI
implementations. We have chosen to use the HPC Challenge
Benchmark suite [14] for this comparison.
The four primary HPC Challenge benchmarks (STREAM,
FFT, Top500 and RandomAccess) are implemented using
pMatlab and run on a commodity cluster system [19]. Both
the pMatlab and C+MPI reference implementation of the
benchmarks are run on up to 128 processors. At each
processor count the largest problem size is run that would fit
in the main memory. The collected data measures the
relative compute performance and memory overhead of
pMatlab with respect to C+MPI. In addition, code sizes are

Figure 4. HPC Challenge Results.
1000

compared.

100

In general, the pMatlab implementations can run problems
that are typically 1/2 the size of C+MPI implementation
problem size (Figure 4). This is mostly due to the need to
create temporary arrays when using high-level expressions.
The pMatlab performance ranges from being comparable to
the C+MPI code (FFT and STREAM), to somewhat slower
(Top500), to a lot slower (RandomAccess). In contrast, the
pMatlab code is typically 3x to 40x smaller than the
equivalent C+MPI code (Figure 5). For more details on
pMatlab implementations of the benchmarks, see [1].

5. USER EXPERIENCES

The true measure of success for any technology is its
effectiveness for real users. Table 1 highlights several
projects that use pMatlab on the MIT Lincoln Laboratory
interactive LLGrid system [19]. The projects are drawn
from the approximately one hundred and fifty current users
and are representative of the user base. The following two
sub-sections discuss specific case studies.

10
a

ideal

eed

1

STRA

SC MPI
+

Ser Mal
Matlab
pMat ab

...................
....
$P

iFFT

¬£n4)
0.01

IHPL'

I FFT

FFT

HPL

wTREAM...

W

z

0.01

0o001
0.0001
0.001

i Random

.Access

Random

IAcclos

Random

Access

0.01

0.1

1

10

Relative Code Size
Figure 5. HPC Challenge speedup vs code size comparison.

Table 1. Selected pMatlab

Code Description

applications.

Serial / Parallel
Dev lime (hours)
Missile & Sensor Simulations
2000 8
1300 /1
First-principles LADAR
40 0.4
Analytic TOM Leakage
Hercules Metric TOMN
900 0.75
40 /I
Coherent laser propagation
700 / 8
Polynomial coefcient approx.
600 / 3
Ground motion tracker
650 40
Automatic target recognition
960 6
Hyper-spectral linage Analysis

IV- 1191

Parallelization Enables
More or Faster

Hligher fidelity radar
Speckle image simulations
Parameter space studies

Monte Carlos
Run time
Faster training algorithm,
Faster & larger data sets
Target classes & scenarios
Larger datasets of images

5.1. Case Study 1: Terminal Doppler Weather Radar
The Terminal Doppler Weather Radar Data Quality
Improvement program is developing signal-processing
algorithms to mitigate range-velocity ambiguity [2]. For
example, gust fronts that are moving radially with respect to
the weather radar can be obscured by the inability of the
radar signal processing algorithms to distinguish its Doppler
velocity from weather that is not moving. The team needs to
rapidly write, evaluate, and revise these algorithms. Running
the algorithms on simulation data sets on a desktop
workstation typically executed for eight to ten hours. The
results of each simulation direct the parameter and algorithm
choices for subsequent simulations, and they usually could
only execute two of these simulations in a 24-hour period.
After parallelizing the simulations, the team now runs the
simulations on the desktop machines with eight to sixteen
processors. These simulations now complete in 30 to 60
minutes, affording eight to ten engineering turns per day.
5.2. Case Study 2: Optical Synthetic Aperture Radar

Optical synthetic aperture radar (OSAR) is a method of
generating images with laser radar that can resolve features
smaller than real-aperture spot size. Developing OSAR
algorithms requires simulating the return signals of a laser
radar, which is computationally intensive. Initially, the
parallel code distributed radar pulses across multiple
processors, with a serial to parallel development time ratio
of 100 to 1. Applying the OSAR simulation to new
applications revealed that the number of pulses is often
small containing many time bins. Due to pMatlab's map
approach, modifying the code to distribute along time bins
was trivial.
6. CONCLUSIONS

pMatlab combines the productivity inherent in the MATLAB
programming language with PGAS, allowing MATLAB users
to exploit distributed systems with only minor changes to
the code. The implementation of the HPC Challenge
benchmark suite using the pMatlab library allows for
comparison with equivalent C+MPI codes. These results
indicate that pMatlab can achieve comparable performance
to C+MPI at usually one tenth the code size. Finally,
implementation data collected from pMatlab applications at
the Laboratory indicate that users are typically able to go
from a serial code to a well-performing pMatlab code in
about 3 hours while changing less than 1% of their code.
7. REFERENCES

[1] N.T. Bliss and J. Kepner, "pMatlab Parallel Matlab Library,"
To be published in the Special Issue on High Productivity
Programming Languages and Models, Int. Journal of High
Performance Computing Applications.
[2] J.Y.N. Cho, G. R. Elkin, and N.G. Parker, "Enhanced Radar
Data Acquisition System and Signal Processing Algorithms for the

Terminal Doppler Weather Radar," Proc. AMS 32nd Conf on
Radar Meteorology, Albuquerque, NM, 24-29 Oct 2005.
[3] R. Choy and A. Edelman, "Parallel MATLAB: Doing It Right,"
Proc. of the IEEE 93(2), pp. 331-341, 2005.
Cornell Multitask Toolbox for MATLAB (CMTM),
[4]
[5] J. C. Cummings et al, "Rapid Application Development and
Enhanced Code Interoperability Using POOMA Framework,"
Proc. SIAM Workshop on Object-Oriented Methods and Code
Interoperability in Scientific and Engineering Computing (0098),
Yorktown Heights, NY, 21-23 Oct. 1998.
[6] L. Dean, S. Grad-Freilich, J. Kepner, A. Reuther, "Distributed
and Parallel Computing with MATLAB," tutorial presented at
ACMIEEE Conf on Supercomputing, Seattle, WA, 12-18 Nov.
2005.
[7] C.M. DeLuca, C.W. Heisey, R.A. Bond, and J.M. Daly, "A
Portable Object-Based Parallel Library and Layered Framework
for Real-Time Radar Signal Processing," Proc. Is' Conf Int.
Scientific Computing in Object-Oriented Parallel Environments
(ISCOPE '97), Marina del Rey, CA, pp.241-248, 8-11 Dec, 1997.
[8] T. El-Ghazawi, W. Carlson, T. Sterling, and K. Yelick, UPC:
Distributed Shared Memory Programming, Wiley, Hoboken, NJ,
May 2005.
[9] Falcon Project: Fast Array Language Computation,

[10] J. Kepner and S. Ahalt, "MatlabMPI," Journal ofParallel and
Distributed Computing 64(8), pp. 997-1005, 2004.
[11] C.H. Koelbel, D.B. Loveman, R.S. Schreiber, G.L. Steel, Jr.,
and M.E. Zosel, The High Performance Fortran Handbook, MIT
Press, Cambridge, MA, 1994.
[12] J. Lebak, J. Kepner, H. Hoffmann, and E. Rutledge, "Parallel
VSIPL++: An Open Standard Software Library for HighPerformance Parallel Signal Processing," Proc. IEEE 93(2), pp.
313-330, 2005.
[13] D.B. Loveman, "High Performance Fortran," IEEE Parallel
and Distributed Technology: Systems and Applications 1(1), pp.
25-42, 1993.
[14] P. Luszczek, J.J. Dongarra, D. Koester, R. Rabenseifher, B.
Lucas, J. Kepner, J. McCalpin, D. Bailey, and D. Takahashi,
"Introduction to the HPC Challenge Benchmark Suite," Lawrence
Berkley National Laboratory, Paper LBNL-57493, 25 Apr. 2005.
[15] G. Morrow and R. van de Geijn, "A Parallel Linear Algebra
Server for Matlab-Like Environments," Proc. ACMIIEEE Conf on
Supercomputing, Orlando, FL, 7-13 Nov. 1998.
[16] Message Passing Interface (MPI),
i,<W
,,; , o,
[17] J. Nieplocha, R.J. Harrison, M.K. Kumar, B. Palmer, V.
Tipparaju, and H. Trease, "Combining Shared and Disitrbuted
Memory Models: Approach and Evolution of the Global Arrays
Toolkit," Workshop on Performance Optimization for High Level
Languages and Libraries (POHLL-02), Int. Conf on
Supercomputing, New York, NY, 22-26 June, 2002.
[18] R.W. Numrich and J. Reid, "Co-Array Fortran for Parallel
Programming," ACM SIGPLAN Fortran Forum 17(2), pp.1-31,
1998.
[19] A. Reuther et. al., "LLGrid: Enabling On-Demand Grid
Computing with gridMatlab and pMatlab," Proc. of High
Performance Embedded Computing Workshop (HPEC 2004),
Lexington, MA, 28-30 September, 2004.
[20] M.E. Zosel, "High Performance Fortran: An Overview,"
Compcon Spring '93, Digest of Papers, San Francisco, CA, 22-26
Feb. 1993.

IV- 1192

Subgraph Detection Using Eigenvector L1 Norms

Nadya T. Bliss
Lincoln Laboratory
Massachusetts Institute of Technology
Lexington, MA 02420
nt@ll.mit.edu

Benjamin A. Miller
Lincoln Laboratory
Massachusetts Institute of Technology
Lexington, MA 02420
bamiller@ll.mit.edu

Patrick J. Wolfe
Statistics and Information Sciences Laboratory
Harvard University
Cambridge, MA 02138
wolfe@stat.harvard.edu

Abstract
When working with network datasets, the theoretical framework of detection theory for Euclidean vector spaces no longer applies. Nevertheless, it is desirable to
determine the detectability of small, anomalous graphs embedded into background
networks with known statistical properties. Casting the problem of subgraph detection in a signal processing context, this article provides a framework and empirical results that elucidate a ‚Äúdetection theory‚Äù for graph-valued data. Its focus is
the detection of anomalies in unweighted, undirected graphs through L1 properties
of the eigenvectors of the graph‚Äôs so-called modularity matrix. This metric is observed to have relatively low variance for certain categories of randomly-generated
graphs, and to reveal the presence of an anomalous subgraph with reasonable reliability when the anomaly is not well-correlated with stronger portions of the
background graph. An analysis of subgraphs in real network datasets confirms the
efficacy of this approach.

1

Introduction

A graph G = (V, E) denotes a collection of entities, represented by vertices V , along with some
relationship between pairs, represented by edges E. Due to this ubiquitous structure, graphs are used
in a variety of applications, including the natural sciences, social network analysis, and engineering.
While this is a useful and popular way to represent data, it is difficult to analyze graphs in the
traditional statistical framework of Euclidean vector spaces.
In this article we investigate the problem of detecting a small, dense subgraph embedded into an
unweighted, undirected background. We use L1 properties of the eigenvectors of the graph‚Äôs modularity matrix to determine the presence of an anomaly, and show empirically that this technique has
reasonable power to detect a dense subgraph where lower connectivity would be expected.
In Section 2 we briefly review previous work in the area of graph-based anomaly detection. In
Section 3 we formalize our notion of graph anomalies, and describe our experimental regime. In
Section 4 we give an overview of the modularity matrix and observe how its eigenstructure plays
a role in anomaly detection. Sections 5 and 6 respectively detail subgraph detection results on
simulated and actual network data, and in Section 7 we summarize and outline future research.
1

2

Related Work

The area of anomaly detection has, in recent years, expanded to graph-based data [1, 2]. The work of
Noble and Cook [3] focuses on finding a subgraph that is dissimilar to a common substructure in the
network. Eberle and Holder [4] extend this work using the minimum description length heuristic to
determine a ‚Äúnormative pattern‚Äù in the graph from which the anomalous subgraph deviates, basing
3 detection algorithms on this property. This work, however, does not address the kind of anomaly
we describe in Section 3; our background graphs may not have such a ‚Äúnormative pattern‚Äù that
occurs over a significant amount of the graph. Research into anomaly detection in dynamic graphs
by Priebe et al [5] uses the history of a node‚Äôs neighborhood to detect anomalous behavior, but this
is not directly applicable to our detection of anomalies in static graphs.
There has been research on the use of eigenvectors of matrices derived from the graphs of interest
to detect anomalies. In [6] the angle of the principal eigenvector is tracked in a graph representing
a computer system, and if the angle changes by more than some threshold, an anomaly is declared
present. Network anomalies are also dealt with in [7], but here it is assumed that each node in the
network has some highly correlated time-domain input. Since we are dealing with simple graphs,
this method is not general enough for our purposes. Also, we want to determine the detectability of
small anomalies that may not have a significant impact on one or two principal eigenvectors.
There has been a significant amount of work on community detection through spectral properties of
graphs [8, 9, 10]. Here we specifically aim to detect small, dense communities by exploiting these
same properties. The approach taken here is similar to that of [11], in which graph anomalies are
detected by way of eigenspace projections. We here focus on smaller and more subtle subgraph
anomalies that are not immediately revealed in a graph‚Äôs principal components.

3

Graph Anomalies

As in [12, 11], we cast the problem of detecting a subgraph embedded in a background as one of
detecting a signal in noise. Let GB = (V, E) denote the background graph; a network in which
there exists no anomaly. This functions as the ‚Äúnoise‚Äù in our system. We then define the anomalous subgraph (the ‚Äúsignal‚Äù) GS = (VS , ES ) with VS ‚äÇ V . The objective is then to evaluate the
following binary hypothesis test; to decide between the null hypothesis H0 and alternate hypothesis
H1 :

H0 : The observed graph is ‚Äúnoise‚Äù GB
H1 : The observed graph is ‚Äúsignal+noise‚Äù GB ‚à™ GS .
Here the union of the two graphs GB ‚à™ GS is defined as GB ‚à™ GS = (V, E ‚à™ ES ).
In our simulations, we formulate our noise and signal graphs as follows. The background graph GB
is created by a graph generator, such as those outlined in [13], with a certain set of parameters. We
then create an anomalous ‚Äúsignal‚Äù graph GS to embed into the background. We select the vertex
subset VS from the set of vertices in the network and embed GS into GB by updating the edge set
to be E ‚à™ ES . We apply our detection algorithm to graphs with and without the embedding present
to evaluate its performance.

4

The Modularity Matrix and its Eigenvectors

Newman‚Äôs notion of the modularity matrix [8] associated with an unweighted, undirected graph G
is given by
1
B := A ‚àí
KK T .
(1)
2|E|
Here A = {aij } is the adjacency matrix of G, where aij is 1 if there is an edge between vertex i
and vertex j and is 0 otherwise; and K is the degree vector of G, where the ith component of K
is the number of edges adjacent to vertex i. If we assume that edges from one vertex are equally
likely to be shared with all other vertices, then the modularity matrix is the difference between the
‚Äúactual‚Äù and ‚Äúexpected‚Äù number of edges between each pair of vertices. This is also very similar to
2

(a)

(b)

(c)

Figure 1: Scatterplots of an R-MAT generated graph projected into spaces spanned by two eigenvectors of its modularity matrix, with each point representing a vertex. The graph with no embedding
(a) and with an embedded 8-vertex clique (b) look the same in the principal components, but the
embedding is visible in the eigenvectors corresponding to the 18th and 21st largest eigenvalues (c).
the matrix used as an ‚Äúobserved-minus-expected‚Äù model in [14] to analyze the spectral properties of
random graphs.
Since B is real and symmetric, it admits the eigendecomposition B = U ŒõU T , where U ‚àà R|V |√ó|V |
is a matrix where each column is an eigenvector of B, and Œõ is a diagonal matrix of eigenvalues.
We denote by Œªi , 1 ‚â§ i ‚â§ |V |, the eigenvalues of B, where Œªi ‚â• Œªi+1 for all i, and by ui the
unit-magnitude eigenvector corresponding to Œªi .
Newman analyzed the eigenvalues of the modularity matrix to determine if the graph can be split
into two separate communities. As demonstrated in [11], analysis of the principal eigenvectors of
B can also reveal the presence of a small, tightly-connected component embedded in a large graph.
This is done by projecting B into the space of its two principal eigenvectors, calculating a Chisquared test statistic, and comparing this to a threshold. Figure 1(a) demonstrates the projection of
an R-MAT Kronecker graph [15] into the principal components of its modularity matrix.
Small graph anomalies, however, may not reveal themselves in this subspace. Figure 1(b) demonstrates an 8-vertex clique embedded into the same background graph. In the space of the two principal eigenvectors, the symmetry of the projection looks the same as in Figure 1(a). The foreground
vertices are not at all separated from the background vertices, and the symmetry of the projection has
not changed (implying no change in the test statistic). Considering only this subspace, the subgraph
of interest cannot be detected reliably; its inward connectivity is not strong enough to stand out in
the two principal eigenvectors.
The fact that the subgraph is absorbed into the background in the space of u1 and u2 , however, does
not imply that it is inseparable in general; only in the subspace with the highest variance. Borrowing
language from signal processing, there may be another ‚Äúchannel‚Äù in which the anomalous signal
subgraph can be separated from the background noise. There is in fact a space spanned by two
eigenvectors in which the 8-vertex clique stands out: in the space of the u18 and u21 , the two
eigenvectors with the largest components in the rows corresponding to VS , the subgraph is clearly
separable from the background, as shown in Figure 1(c).
4.1

Eigenvector L1 Norms

The subgraph detection technique we propose here is based on L1 properties of the eigenvectors
of the graph‚Äôs modularity matrix, where the L1 norm of a vector x = [x1 ¬∑ ¬∑ ¬∑ xN ]T is kxk1 :=
PN
i=1 |xi |. When a vector is closely aligned with a small number of axes, i.e., if |xi | is only large for
a few values of i, then its L1 norm will be smaller than that of a vector of the same magnitude where
this is not the case. For example, if x ‚àà R1024
‚àö has unit magnitude and only has nonzero components
along two of the 1024 axes, then kxk1 ‚â§ 2. If it has a component of equal magnitude along all
axes, then kxk1 = 32. This property has been exploited in the past in a graph-theoretic setting, for
finding maximal cliques [16, 17].
This property can also be useful when detecting anomalous clustering behavior. If there is a subgraph
GS that is significantly different from its expectation, this will manifest itself in the modularity
3

(a)

(b)

Figure 2: L1 analysis of modularity matrix eigenvectors. Under the null model, ku18 k has the
distribution in (a). With an 8-vertex clique embedded, ku18 k1 falls far from its average value, as
shown in (b).
matrix as follows. The subgraph GS has a set of vertices VS , which is associated with a set of indices
corresponding to rows and columns of the adjacency matrix A. Consider the vector x ‚àà {0, 1}N ,
where xi is 1 if vi ‚àà VS and xi = 0 otherwise. For any S ‚äÜ V and v ‚àà V , letP
dS (v) denote the
number of edges between the vertex v and the vertex set S. Also, let dS (S 0 ) := v‚ààS 0 dS (v) and
d(v) := dV (v). We then have
2
X
d(VS )
2
kBxk2 =
,
dVS (v) ‚àí d(v)
(2)
d(V )
v‚ààV

xT Bx = dVS (VS ) ‚àí

d2 (VS )
,
d(V )

(3)

p
and kxk2 = |VS |. Note that d(V ) = 2|E|. A natural interpretation of (2) is that Bx represents the difference between the actual and expected connectivity to VS across the entire graph,
and likewise (3) represents this difference within the subgraph. If x is an eigenvector of B, then
of course xT Bx/(kBxk2 kxk2 ) = 1. Letting
internal and
P each subgraph vertex have uniform
external degree, this ratio approaches 1 as v‚ààV
(dVS (v) ‚àí d(v)d(VS )/d(V ))2 is dominated by
/
S
P
2
v‚ààVS (dVS (v) ‚àí d(v)d(VS )/d(V )) . This suggests that if VS is much more dense than a typical
subset of background vertices, x is likely to be well-correlated with an eigenvector of B. (This becomes more complicated when there are several eigenvalues that are approximately dVS (VS )/|VS |,
but this typically occurs for smaller graphs than are of interest.) Newman made a similar observation: that the magnitude of a vertex‚Äôs component in an eigenvector is related to the ‚Äústrength‚Äù with
which it is a member of the associated community. Thus if a small set of vertices forms a community, with few belonging to other communities, there will be an eigenvector well aligned with this
set, and this implies that the L1 norm of this eigenvector would be smaller than that of an eigenvector
with a similar eigenvalue when there is no anomalously dense subgraph.
4.2

Null Model Characterization

To examine the L1 behavior of the modularity matrix‚Äôs eigenvectors, we performed the following
experiment. Using the R-MAT generator we created 10,000 graphs with 1024 vertices, an average
degree of 6 (the result being an average degree of about 12 since we make the graph undirected),
and a probability matrix


0.5 0.125
P =
.
0.125 0.25
For each graph, we compute the modularity matrix B and its eigendecomposition. We then compute
kui k1 for each i and store this value as part of our background statistics. Figure 2(a) demonstrates
the distribution of ku18 k1 . The distribution has a slight left skew, but has a tight variance (a standard
deviation of 0.35) and no large deviations from the mean under the null (H0 ) model.
After compiling background data, we computed the mean and standard deviation of the L1 norms
for each ui . Let ¬µi be the average of kui k1 and œÉi be its standard deviation. Using the R-MAT graph
with the embedded 8-vertex clique, we observed eigenvector L1 norms as shown in Figure 2(b). In
4

the figure we plot kui k1 as well as ¬µi , ¬µi + 3œÉi and ¬µi ‚àí 3œÉi . The vast majority of eigenvectors
have L1 norms close to the mean for the associated index. There are very few cases with a deviation
from the mean of greater than 3œÉ. Note also that ¬µi decreases with decreasing i. This suggests that
the community formation inherent in the R-MAT generator creates components strongly associated
with the eigenvectors with larger eigenvalues.
The one outlier is u18 , which has an L1 norm that is over 10 standard deviations away from the mean.
Note that u18 is the horizontal axis in Figure 1(c), which by itself provides significant separation
between the subgraph and the background. Simple L1 analysis would certainly reveal the presence
of this particular embedding.

5

Embedded Subgraph Detection

With the L1 properties detailed in Section 4 in mind, we propose the following method to determine
the presence of an embedding. Given a graph G, compute the eigendecomposition of its modularity
matrix. For each eigenvector, calculate its L1 norm, subtract its expected value (computed from the
background statistics), and normalize by its standard deviation. If any of these modified L1 norms
is less than a certain threshold (since the embedding makes the L1 norm smaller), H1 is declared,
and H0 is declared otherwise. Pseudocode for this detection algorithm is provided in Algorithm 1.
Algorithm 1 L1S UBGRAPH D ETECTION
Input: Graph G = (V, E), Integer k, Numbers `1MIN , ¬µ[1..k], œÉ[1..k]
B ‚Üê M OD M AT(G)
U ‚Üê EIGENVECTORS(B, k) hhk eigenvectors of Bii
for i ‚Üê 1 to k do
m[i] ‚Üê (kui k1 ‚àí ¬µ[i])/œÉ[i]
if m[i] < `1MIN then
return H1 hhdeclare the presence of an embeddingii
end if
end for
return H0 hhno embedding foundii
We compute the eigenvectors of B using eigs in MATLAB, which has running time O(|E|kh +
|V |k 2 h + k 3 h), where h is the number of iterations required for eigs to converge [10]. While
the modularity matrix is not sparse, it is the sum of a sparse matrix and a rank-one matrix, so we
can still compute its eigenvalues efficiently, as mentioned in [8]. Computing the modified L1 norms
and comparing them to the threshold takes O(|V |k) time, so the complexity is dominated by the
eigendecomposition.
The signal subgraphs are created as follows. In all simulations in this section, |VS | = 8. For each
simulation, a subgraph density of 70%, 80%, 90% or 100% is chosen. For subraphs of this size and
density, the method of [11] does not yield detection performance better than chance.
The subgraph

is created by, uniformly at random, selecting the chosen proportion of the 82 possible edges. To
determine where to embed the subgraph into the background, we find all vertices with at most 1, 3
or 5 edges and select 8 of these at random. The subgraph is then induced on these vertices.
For each density/external degree pair, we performed a 10,000-trial Monte Carlo simulation in which
we create an R-MAT background with the same parameters as the null model, embed an anomalous
subgraph as described above, and run Algorithm 1 with k = 100 to determine whether the embedding is detected. Figure 3 demonstrates detection performance in this experiment. In the receiver
operating characteristic (ROC), changing the L1 threshold (`1MIN in Algorithm 1) changes the position on the curve. Each curve corresponds to a different subgraph density. In Figure 3(a), each
vertex of the subgraph has 1 edge adjacent to the background. In this case the subgraph connectivity
is overwhelmingly inward, and the ROC curve reflects this. Also, the more dense subgraphs are
more detectable. When the external degree is increased so that a subgraph vertex may have up to
3 edges adjacent to the background, we see a decline in detection performance as shown in Figure
3(b). Figure 3(c) demonstrates the additional decrease in detection performance when the external
subgraph connectivity is increased again, to as much as 5 edges per vertex.
5

(a)

(b)

(c)

Figure 3: ROC curves for the detection of 8-vertex subgraphs in a 1024-vertex R-MAT background.
Performance is shown for subgraphs of varying density when each foreground vertex is connected
to the background by up to 1, 3 and 5 edges in (a), (b) and (c), respectively.

6

Subgraph Detection in Real-World Networks

To verify that we see similar properties in real graphs that we do in simulated ones, we analyzed
five data sets available in the Stanford Network Analysis Package (SNAP) database [18]. Each network is made undirected before we perform our analysis. The data sets used here are the Epinions
who-trusts-whom graph (Epinions, |V | = 75,879, |E| = 405,740) [19], the arXiv.org collaboration
networks on astrophysics (AstroPh, |V | = 18,722, |E| = 198,050) and condensed matter (CondMat,
|V |=23,133, |E|=93,439) [20], an autonomous system graph (asOregon, |V |=11,461, |E|=32,730)
[21] and the Slashdot social network (Slashdot, |V |=82,168, |E|=504,230) [22]. For each graph, we
compute the top 110 eigenvectors of the modularity matrix and the L1 norm of each. Comparing
each L1 sequence to a ‚Äúsmoothed‚Äù (i.e., low-pass filtered) version, we choose the two eigenvectors that deviate the most from this trend, except in the case of Slashdot, where there is only one
significant deviation.
Plots of the L1 norms and scatterplots in the space of the two eigenvectors that deviate most are
shown in Figure 4. The eigenvectors declared are highlighted. Note that, with the exception of the
asOregon, we see as similar trend in these networks that we did in the R-MAT simulations, with
the L1 norms decreasing as the eigenvalues increase (the L1 trend in asOregon is fairly flat). Also,
with the exception of Slashdot, each dataset has a few eigenvectors with much smaller norms than
those with similar eigenvalues (Slashdot decreases gradually, with one sharp drop at the maximum
eigenvalue).
The subgraphs detected by L1 analysis are presented in Table 1. Two subgraphs are chosen for each
dataset, corresponding to the highlighted points in the scatterplots in Figure 4. For each subgraph
we list the size (number of vertices), density (internal degree divided by the maximum number of
edges), external degree, and the eigenvector that separates it from the background. The subgraphs
are quite dense, at least 80% in each case.
To determine whether a detected subgraph is anomalous with respect to the rest of the graph, we
sample the network and compare the sample graphs to the detected subgraphs in terms of density
and external degree. For each detected subgraph, we take 1 million samples with the same number
of vertices. Our sampling method consists of doing a random walk and adding all neighbors of each
vertex in the path. We then count the number of samples with density above a certain threshold
and external degree below another threshold. These thresholds are the parenthetical values in the
4th and 5th columns of Table 1. Note that the thresholds are set so that the detected subgraphs
comfortably meet them. The 6th column lists the number of samples out of 1 million that satisfy
both thresholds. In each case, far less than 1% of the samples meet the criteria. For the Slashdot
dataset, no sample was nearly as dense as the two subgraphs we selected by thresholding along the
principal eigenvector. After removing samples that are predominantly correlated with the selected
eigenvectors, we get the parenthetical values in the same column. In most cases, all of the samples
meeting the thresholds are correlated with the detected eigenvectors. Upon further inspection, those
remaining are either correlated with another eigenvector that deviates from the overall L1 trend, or
correlated with multiple eigenvectors, as we discuss in the next section.
6

(a) Epinions L1 norms

(b) Epinions scatterplot

(c) AstroPh L1 norms

(d) AstroPh scatterplot

(e) CondMat L1 norms

(f) CondMat scatterplot

(g) asOregon L1 norms

(h) asOregon scatterplot

(i) Slashdot L1 norms

(j) Slashdot scatterplot

Figure 4: Eigenvector L1 norms in real-world network data (left column), and scatterplots of the
projection into the subspace defined by the indicated eigenvectors (right column).

7

dataset

eigenvector

subgraph
size

Epinions
Epinions
AstroPh
AstroPh
CondMat
CondMat
asOregon
asOregon
Slashdot
Slashdot

u36
u45
u57
u106
u29
u36
u6
u32
u1 > 0.08
u1 > 0.07

34
27
30
24
19
20
15
6
36
51

subgraph
(sample)
density
80% (70%)
83% (75%)
100% (90%)
100% (90%)
100% (90%)
83% (75%)
96% (85%)
93% (80%)
95% (90%)
89% (80%)

subgraph
(sample)
external degree
721 (1000)
869 (1200)
93 (125)
73 (100)
2 (50)
70 (120)
1089 (1500)
177 (200)
10570 (‚àû)
12713 (‚àû)

# samples
that meet
threshold
46 (0)
261 (6)
853 (0)
944 (0)
866 (0)
1596 (0)
23 (0)
762 (393)
0 (0)
0 (0)

Table 1: Subgraphs detected by L1 analysis, and a comparison with randomly-sampled subgraphs
in the same network.

Figure 5: An 8-vertex clique that does not create an anomalously small L1 norm in any eigenvector.
The scatterplot looks similar to one in which the subgraph is detectable, but is rotated.

7

Conclusion

In this article we have demonstrated the efficacy of using eigenvector L1 norms of a graph‚Äôs modularity matrix to detect small, dense anomalous subgraphs embedded in a background. Casting the
problem of subgraph detection in a signal processing context, we have provided the intuition behind
the utility of this approach, and empirically demonstrated its effectiveness on a concrete example:
detection of a dense subgraph embedded into a graph generated using known parameters. In real
network data we see trends similar to those we see in simulation, and examine outliers to see what
subgraphs are detected in real-world datasets.
Future research will include the expansion of this technique to reliably detect subgraphs that can be
separated from the background in the space of a small number of eigenvectors, but not necessarily
one. While the L1 norm itself can indicate the presence of an embedding, it requires the subgraph to
be highly correlated with a single eigenvector. Figure 5 demonstrates a case where considering multiple eigenvectors at once would likely improve detection performance. The scatterplot in this figure
looks similar to the one in Figure 1(c), but is rotated such that the subgraph is equally aligned with
the two eigenvectors into which the matrix has been projected. There is not significant separation in
any one eigenvector, so it is difficult to detect using the method presented in this paper. Minimizing
the L1 norm with respect to rotation in the plane will likely make the test more powerful, but could
prove computationally expensive. Other future work will focus on developing detectability bounds,
the application of which would be useful when developing detection methods like the algorithm
outlined here.
Acknowledgments
This work is sponsored by the Department of the Air Force under Air Force Contract FA8721-05-C0002. Opinions, interpretations, conclusions and recommendations are those of the author and are
not necessarily endorsed by the United States Government.
8

References
[1] J. Sun, J. Qu, D. Chakrabarti, and C. Faloutsos, ‚ÄúNeighborhood formation and anomaly detection in bipartite graphs,‚Äù in Proc. IEEE Int‚Äôl. Conf. on Data Mining, Nov. 2005.
[2] J. Sun, Y. Xie, H. Zhang, and C. Faloutsos, ‚ÄúLess is more: Compact matrix decomposition for
large sparse graphs,‚Äù in Proc. SIAM Int‚Äôl. Conf. on Data Mining, 2007.
[3] C. C. Noble and D. J. Cook, ‚ÄúGraph-based anomaly detection,‚Äù in Proc. ACM SIGKDD Int‚Äôl.
Conf. on Knowledge Discovery and Data Mining, pp. 631‚Äì636, 2003.
[4] W. Eberle and L. Holder, ‚ÄúAnomaly detection in data represented as graphs,‚Äù Intelligent Data
Analysis, vol. 11, pp. 663‚Äì689, December 2007.
[5] C. E. Priebe, J. M. Conroy, D. J. Marchette, and Y. Park, ‚ÄúScan statistics on enron graphs,‚Äù
Computational & Mathematical Organization Theory, vol. 11, no. 3, pp. 229‚Äì247, 2005.
[6] T. IdeÃÅ and H. Kashima, ‚ÄúEigenspace-based anomaly detection in computer systems,‚Äù in Proc.
KDD ‚Äô04, pp. 440‚Äì449, 2004.
[7] S. Hirose, K. Yamanishi, T. Nakata, and R. Fujimaki, ‚ÄúNetwork anomaly detection based on
eigen equation compression,‚Äù in Proc. KDD ‚Äô09, pp. 1185‚Äì1193, 2009.
[8] M. E. J. Newman, ‚ÄúFinding community structure in networks using the eigenvectors of matrices,‚Äù Phys. Rev. E, vol. 74, no. 3, 2006.
[9] J. Ruan and W. Zhang, ‚ÄúAn efficient spectral algorithm for network community discovery and
its applications to biological and social networks,‚Äù in Proc. IEEE Int‚Äôl Conf. on Data Mining,
pp. 643‚Äì648, 2007.
[10] S. White and P. Smyth, ‚ÄúA spectral clustering approach to finding communities in graphs,‚Äù in
Proc. SIAM Data Mining Conf., 2005.
[11] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúToward signal processing theory for graphs and other
non-Euclidean data,‚Äù in Proc. IEEE Int‚Äôl Conf. on Acoustics, Speech and Signal Processing,
pp. 5414‚Äì5417, 2010.
[12] T. Mifflin, ‚ÄúDetection theory on random graphs,‚Äù in Proc. Int‚Äôl Conf. on Information Fusion,
pp. 954‚Äì959, 2009.
[13] D. Chakrabarti and C. Faloutsos, ‚ÄúGraph mining: Laws, generators, and algorithms,‚Äù ACM
Computing Surveys, vol. 38, no. 1, 2006.
[14] F. Chung, L. Lu, and V. Vu, ‚ÄúThe spectra of random graphs with given expected degrees,‚Äù Proc.
of National Academy of Sciences of the USA, vol. 100, no. 11, pp. 6313‚Äì6318, 2003.
[15] D. Chakrabarti, Y. Zhan, and C. Faloutsos, ‚ÄúR-MAT: A recursive model for graph mining,‚Äù in
Proc. Fourth SIAM Int‚Äôl Conference on Data Mining, vol. 6, pp. 442‚Äì446, 2004.
[16] T. S. Motzkin and E. G. Straus, ‚ÄúMaxima for graphs and a new proof of a theorem of TuraÃÅn,‚Äù
Canad. J. Math., vol. 17, pp. 533‚Äì540, 1965.
[17] C. Ding, T. Li, and M. I. Jordan, ‚ÄúNonnegative matrix factorization for combinatorial optimization: Spectral clustering, graph matching, and clique finding,‚Äù in Proc. IEEE Int‚Äôl Conf.
on Data Mining, pp. 183‚Äì192, 2008.
[18] J. Leskovec, ‚ÄúStanford network analysis package.‚Äù http://snap.stanford.edu.
[19] M. Richardson, R. Agrawal, and P. Domingos, ‚ÄúTrust management for the semantic web,‚Äù in
Proc. ISWC, 2003.
[20] J. Leskovec, J. Kleinberg, and C. Faloutsos, ‚ÄúGraph evolution: Densification and shinking
diameters,‚Äù ACM Trans. on Knowledge Discovery from Data, vol. 1, no. 1, 2007.
[21] J. Leskovec, J. Kleinberg, and C. Faloutsos, ‚ÄúGraphs over time: Densification laws, shinking
diameters and possible explanations,‚Äù in Proc. KDD ‚Äô05, 2005.
[22] J. Leskovec, K. Lang, A. Dasgupta, and M. Mahoney, ‚ÄúCommunity structure in large networks:
Natural cluster sizes and the absence of large well-defined clusters.‚Äù arXiv.org:0810.1355,
2008.

9

REAL-TIME GLOBAL MOTION BLUR DETECTION
Karl S. Ni, Zachary Z. Sun, Nadya T. Bliss
MIT Lincoln Laboratory
E-mails: karl.ni@ll.mit.edu, zsun86@ll.mit.edu, nt@ll.mit.edu

ABSTRACT
Most video exploitation algorithms operate on individual
frames. To effect good results in such applications, the algorithms require good frames with which to work. However,
videos may contain artifacts such as blur which inhibit the
extraction of inherent and useful information. This paper
proposes an algorithm that detects poor video frames induced
by global motion blur. The proposed algorithm is divided
into two steps: the Ô¨Årst of which creates a single image blur
metric, and the second of which adds temporal information.
The blur metric is derived from a linear least squares Ô¨Åt to the
log distribution of the highest subbands in a wavelet-based
Haar Ô¨Ålters. The second part of the algorithm correlates adjacent frames to boost performance. The ideas presented in
this paper are low in complexity yet high in performance.
Additionally, the proposed algorithm has been tested on natural video data as well as synthesized blur, and comparisons
to state of the art show an advantage in using wavelet-based
thresholding.
1. INTRODUCTION
In the last couple of decades, there has been a surge of computer vision algorithms to address applications requiring object detection, recognition, 3-D geo-registration, image labeling, and information organization. The expectation of such
algorithms is that their feature space is derived from uncorrupted, high-resolution images. While frame enhancement
through post-processing is possible, videos captured real-time
at 30 to 40 frames per second for applications like structure
from motion (in which SIFT features are extracted) hardly require operation on each and every frame. Moreover, doing
so real-time would require an inordinate amount of computational resources that is often unavailable.
One of the most common afÔ¨Çictions in video capture is
image blur, and more speciÔ¨Åcally, motion blur. The source
of blurring artifacts are due to insufÔ¨Åcient exposure in image capture, where the basic properties of sensors are quite
This work is sponsored by the Department of the Air Force under Air
Force contract FA8721-05-C-0002. Opinions, interpretations, conclusions,
and recommendations are those of the author and are not necessarily endorsed
by the United States Government.

978-1-4673-2533-2/12/$26.00 ¬©2012 IEEE

3101

complex and difÔ¨Åcult to model. Again, there has been a considerable effort to remedy these problems in the form of blind
deconvolution, kernel estimation, total variational optimization problems, etc., but the literature relating to the actual detection and characterization of video frame quality is surprisingly sparse. Additionally, among the work addressing blur
detection, the vast majority discuss methods to detect image
blur rather than video blur. That is, blur detection is computed
in the absence of a reference frame. Because computational
resources are frequently limited while images from videos are
abundant, the proposed algorithm in this work lies under the
speciÔ¨Åc domain of global motion blur detection in video.
The following section and beyond, explores current solutions to this problem and attempts to improve upon them.
Related work is Ô¨Årst provided in Sec. 2 to provide a basic
foundation of our contributions. Then, Sec 3.2 describes the
algorithm in an unreferenced frame setting, and Sec. 3.1 adds
temporal improvements using adjacent frame information. In
Sec. 4, in addition to standardized test sets, real-world data
based on CCD-captured video data (where blurring artifacts
are easily created) promotes an applicable and useful piece
of work. Lastly, Sec. 5 concludes with some summarizing
remarks and future directions.
2. RELATED WORK
The literature can be categorized into either modeling approaches or discriminant approaches. A common model [4]
for natural images in general [5] is the power law probability
density function described by:
S(f ) = Af Œ±

(1)

where S(f ) is the power spectral density integrated over all
angles, f denotes the frequency, and Œ± is a parameter called
the slope of the power spectrum. (S(f ) is derived from the
polar coordinates of S(f, Œ∏) = S(f1 , f2 ) = N1 |F{I(x, y)}|,
with I denoting the image.) Typically, Œ± ‚âà 2 for clear images; blurred images have larger Œ± values while the opposite
is true for sharper images.
Determining the relative shape of the frequency domain
is a studied theme in extracting blur information; several image evaluation papers include similar measurements on the

ICIP 2012

kurtosis on DCT coefÔ¨Åcients to determine the sharpness of
images [2]. Unfortunately, models determined via empirical studies ultimately lack precision and are heuristic at best.
Moreover, as stated by [4], metrics on Œ± and kurtoses are insufÔ¨Åciently descriptive of blur in and of itself.
Further steps in [4] rely on similar properties of blurred
images, including gradient histogram Ô¨Åtting. Correspondingly, additional modeling with Gaussian Mixture Models
(GMM) takes place. Yet, Ô¨Åtting GMM‚Äôs to data involves an
iterative procedure, whose computational complexity precludes it from use in real-time video applications.
On the other hand, discriminant methods provide a
straightforward function (or functional) based decision. The
expressive power of the decision in discriminant methods lies
in the user‚Äôs choice of feature space. Because the overall
blurriness of an image, according to the human visual system (HVS), is primarily given by the sharpness of its edges,
the logical feature space is the collection of edges within an
image. One well-cited algorithm [6] relies on the discrete
wavelet transform (DWT) for feature extraction. Based on
the type of edges that naturally occur in images, [6] decides
upon a set of rules (though without much justiÔ¨Åcation), and
from them develops a procedure. The edge extraction process
comes in the form of an aggregate norm-like combination:

2 (x, y) + E 2 (x, y) + E 2 (x, y)
ELH
HL
HH
(2)
where ELH , EHL , and EHH are the low/high, high/low, and
high/high wavelet subbands, respectively. As will be discussed in Sec. 3, using (2) reduces the separability potential
that DWT inherently introduces. Moreover, while somewhat
intuitive, the results, at least from our data sets, have been less
than stellar.
Emap (x, y) =

Finally, there is a modest set of discriminant algorithms
that refer to a metric called the blur extent or something similar. The review of such papers culminate in a study that is
cognitive in nature, and the most recent SPIE contribution [3]
centers on the relative blur ‚Äúannoyance‚Äù, a term that is speciÔ¨Åc to the HVS. According to the results in [3], there is a
successful way to automatically extract a blur metric based
on the same criteria as perceptual systems. Although fascinating, our study is interested in an absolute value so that other
post-processing architectures have use of the metric.
The proposed algorithm falls under the latter discriminant
category and is most similar to [6], where the Haar wavelet
basis set is used to transform the feature space. However,
we borrow some concepts in modeling from [4] and others.
The remainder of this paper explores the improvements on
both paradigms and describes a procedure that is both featurebased and computationally tractable in real-time, derived for
both image and video applications.

3102

3. VIDEO BLUR DETECTION
This section (and the proposed algorithm) is broken up into
two portions: the within-frame evaluation in Sec. 3.1, designed to be a standalone blur metric for images, and a
frame-to-frame evaluation in Sec. 3.2, designed to boost performance with aid from adjacent frames.
3.1. Unreferenced Global Edge ClassiÔ¨Åcation
Our evaluation of image data proposes to condense millions
of pixels into a single discriminating value representing the
presence of blur within a single frame. As mentioned before,
we can optimize this operation with a change of basis to a
space more representative of image blur: the wavelet scale
space. A subset of the wavelet scale space with especially
conducive properties is the Haar wavelet basis over two dimensions:
1 1
hL = { , }
2 2

;

1 1
hH = { , ‚àí }
2 2

(3)

for low and high subbands, respectively.
There is an entire Ô¨Åeld devoted to the term blur perception [3], which broaches the issue of quantifying directional
blur. Motion blur, particularly in our experiments where
cameras were shaken or moved, is insidious in that it has
the potential to affect a single direction, where certain edges
are blurred and others are left untouched. ([3] describes low
scores in HVS when sharpness is not perceived in the nonblurred direction.) In this situation, an overall metric like (2)
is misleading because it loses discriminating information.
Instead, the proposed algorithm considers subbands for each
horizontal and vertical Ô¨Ålter separately.
Haar wavelets, as implemented, are most useful in the
variational information they provide (in terms of gradients,
etc.) Hence, vertical and horizontal subbands together preserve both degrees of freedom. The histogram in Fig. 1 is
from a video where the camera pans to the left. For a given
image, a single axis of orientation is eliminated when using (2), even though the blur has two degrees of freedom in
afÔ¨Åne space. Therefore, a naƒ±Ãàve approach in maintaining discrete metrics for each subband would be to threshold on edge
strength and number of edges. In contrast to (2), where an
equivalent ‚Äúor‚Äù is implemented, an ‚Äúand‚Äù operation is used:
|{EHL (x, y) : |EHL (x, y)| > thloc }| > thtot &
|{EHL (x, y) : |EHL (x, y)| > thloc }| > thtot

(4)

where & denotes an ‚Äúand‚Äù operation, and two different thresholds are used with thloc referring to the relative strength of an
edge and thtot referring to the number of strong edges. Note
that we have left out the HH band, which is less useful due to
the sampling grid. (The HH band requires a re-scale as pixel
distances are different as well.)
However, edge counting is, regrettably, a relativistic metric, subject to the content of the evaluated image. That is,

a single threshold is not conducive to all possible images.
While the number of edges vary considerably from image to
image, taking advantage of the distribution of an image tends
to mitigate the effect.
As it turns out, the distribution of coefÔ¨Åcients in wavelet
based transforms contain a large amount of information. The
DWT is the result of linear operations producing a Laplace
distribution [1], which gives a simple to approximate log distribution for LH and HL bands, seen in Fig. 1. (Whereas Ô¨Åtting the exponential models of Fourier and DCT-based image
transformations are somewhat unwieldy in terms of computational cycles.)

the solution is


m
b




‚àí1
,
= GzT zzT

(5)




DWT CoefÔ¨Åcient #
where z =
, G can be either Gv or
1T
Gh (un-normalized), and we can threshold m. Because the
tail end of the subbands are noisy and possibly negative inÔ¨Ånite (and we are also taking an absolute value), we need
only model a subset of the points near the start of the curve.
Additionally, without loss of generality, we can safely ignore
zeros (where no interesting content is present) that comprise
the majority of HL and LH bands. Like [4] and [5], the
slope of the spectrum is considered, though unlike them, we
have considered subbands and sampling grids while reducing
complexity. Furthermore, the intuition behind thresholding
on edge strength is simultaneously preserved while considering some outliers.
Therefore, m becomes the metric by which we apply a
threshold for each band. In other words, only when
(mLH > th)

&

(mHL > th)

(6)

is true, do we call image I blur-free.
3.2. Semi-referenced Frame Motion Blur

Fig. 1: Histograms of subbands

Fig. 1 shows the log histogram of the horizontal and vertical bands of blurred and unblurred images. As evident from
a curve, the video was panning to the left (horizontally), and
the global motion blur causes a sharper drop in the CH-1 histogram of the blurred image than in other wavelet subbands
in both sharp and blurred images.
Fortunately, the Laplace distribution is linear in logarithmic space, which allows for the proposed Ô¨Åt. Let Gv be the
sorted histogram of the set of non-zero, vertically-oriented
wavelet coefÔ¨Åcients {log(|gv |) : gv ‚àà ELH (x, y) = 0},
and likewise, Gh the histogram of the horizontally-oriented
coefÔ¨Åcients {log(|gh |) : gh ‚àà EHL (x, y) = 0}. We are primarily interested in the negative slope of Gv and Gh , shown
in Fig. 1. Then, it is possible to calculate the maximimum
likelihood estimate of the front end of the slope, ignoring the
outlier high-frequencies in the curve shape.
The maximum likelihood estimate of the slope is, of
course,the least
 squares solution to a linearly curve Ô¨Åtted som
lution,
, where our interest lies primarily in m. Here,
b

3103

Because a frame can always be brought in and buffered, the
miss rate for the detection can be higher than most applications. In contrast, the complexity of most exploitation algorithms by virtue of their intended purpose are usually high, so
once a frame is read, ensuring its quality is paramount (i.e.,
the probability of a false alarm must be kept relatively low.)
This section describes some low-cost support mechanisms
to alter thresholds in Sec. 3.1 or ignore frames altogether.
From the Ô¨Årst order decomposition of the wavelet transform,
it is possible to locate edges from an initial frame and determine its relative strength and location with respect to the next
frame. Depending on the available resources, this can be done
on a quadrant basis or adaptively.
i
i
(p) and NLH
(p)
Let us deÔ¨Åne a spatial neighborhood NHL
in HL and LH bands of point p in frame i. Then frames i and
i + n for small n should have edges of comparable strength
and location. Frame i is similar to frame i + n if
‚àÄE(pi ) > thin , ‚àÉ pi+n ‚àà Ii+n

 i

i+n
NHL
(p
)
‚à©
N
/ ‚àÖ
i
HL (pi+n ) ‚àà
 i
i+n
(pi+n ) ‚àà
/‚àÖ
and
NLH (pi ) ‚à© NLH

(7)

where thin is akin to (4). (Though to keep computation cycles
low, thin must be kept high while thout is low.) Therefore, if
reference frame i is unblurred, then should frame j be similar
to it, then it is likely an unblurred image.

4. COMPARATIVE RESULTS
We tested the application and generalization of the proposed
algorithm with real global motion blur and synthetically
blurred images. Real-data motion blur was taken from a
Canon D5000 CCD camera in 1MPix video mode in Cambridge, MA. As sensor heated up after twenty minutes, the
camera was increasingly sensitive to blur. For synthetically
blur, the image corpus included non-blurred ‚Äúreal data‚Äù,
CalPhotos, and Corel data sets. The blur was applied to
full-resolution images degraded with a 1D Gaussian kernel
under a sweep from 10 to 100 pixel œÉ deviation to simulate
horizontal or vertical directional blur. Blur was induced in
2D through orthogonal convolution of the Gaussian kernel.
ROC curves shown in Fig. 2 exemplify performance on
Fig. 4, shots of real-world blur. In the ‚Äúreal-blur‚Äù curves of
Fig. 2, positive (H1 ) and negative (H0 ) hypothesis sets are
subjectively selected from various blurred video frames over
various extent from images originating from a CCD captured
video. The H1 hypotheses in Fig. 3 were indiscriminately
chosen as to blur was applied to all data.

(a) Real-data: unidirectional blur

(b) Sample sharp image (in Fig. 1)

Fig. 4: Real motion-blurred images used for ROC curves.

5. CONCLUSIONS AND FUTURE WORK
Real-time blur detection through non-iterative image characterization rapidly and accurately classiÔ¨Åes blur detection, as
demonstrated in an applications-based setting. Due to extended degrees of freedom, it improves over state of the art.
The proposed algorithm falls underneath the umbrella of
a general collection of algorithms involved in frame selection,
or alternatively, frame exclusion Though motion blur is a primary consequence through CCD capture, interesting subband
behavior in other types of artifacts was noted, including those
originating from compression, speckle noise, and ghosting.
Extensions to these problems fall under general analysis of
the Haar Ô¨Ålter as a variational study, where [7] is a good place
to begin. In conjunction with the proposed algorithm, understanding speciÔ¨Åc conditions of video data will aid in the
advancement of frame-based detection algorithms.
6. REFERENCES

Fig. 2: Canon D5000 Captured Motion Blur ROC

Fig. 3: Open Source Synthetic Motion Blur ROC
Surprisingly, [6] performs poorly over the swept thresholds. In terms of real-world data, where Gaussian kernels
may not necessarily be an accurate depiction of the blurring
phenomenon, the proposed algorithm works well while keeping the computational burden low. The algorithm somewhat
breaks down at lower blur extents, though the state of the art
also functions poorly at those levels.

3104

[1] N. Al-Jawad and S. Jassim. Wavelet based image quality self
measurements. In Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, volume 7708, 2010.
[2] J. E. Caviedes and S. Gurbuz. No-reference sharpness metric
based on local edge kurtosis. In IEEE International Conference
on Signal Proecssing (ICIP), volume 3, pages 53‚Äì56, 2002.
[3] F. Crete, T. Dolmiere, P. Ladret, and M. Nicolas. The blur effect:
perception and estimation with a new no-reference perceptual
blur metric. In Society of Photo-Optical Instrumentation Engineers (SPIE) Conference Series, volume 6492 of Presented at
the Society of Photo-Optical Instrumentation Engineers (SPIE)
Conference, Mar. 2007.
[4] R. Liu, Z. Li, and J. Jia. Image partial blur detection and classiÔ¨Åcation. IEEE Computer Society Conference on Computer Vision
and Pattern Recognition, CVPR, 0:1‚Äì8, 2008.
[5] R. P. Millane, S. Alzaidi, and W. H. Hsiao. Scaling and power
spectra of natural images. In Proceedings of Image and Vision
Computing, pages 148‚Äì153. D.G.Bailey, ed. (Massey University, Palmerston North, New Zealand, 2003), 2003.
[6] H. Tong, M. Li, H. Zhang, and C. Zhang. Blur detection for
digital images using wavelet transform. In Proceedings of the
2004 IEEE International Conference on Multimedia and Expo
(ICME), pages 17‚Äì20, 2004.
[7] M. Vetterli and J. Kovacevic. Wavelets and Subband Coding.
Prentice-Hall, Englewood Cliffs, NJ, 1995.

Cluster-based 3D Reconstruction of Aerial Video
Scott M. Sawyer, Karl Ni, and Nadya T. Bliss
MIT Lincoln Laboratory
Emails: {scott.sawyer, karl.ni, nt}@ll.mit.edu

Abstract‚ÄîLarge-scale 3D scene reconstruction using Structure
from Motion (SfM) continues to be very computationally challenging despite much active research in the area. We propose
an efficient, scalable processing chain designed for cluster computing and suitable for use on aerial video. The sparse bundle
adjustment step, which is iterative and difficult to parallelize,
is accomplished by partitioning the input image set, generating
independent point clouds in parallel, and then fusing the clouds
and combining duplicate points. We compare this processing
chain to a leading parallel SfM implementation, which exploits
fine-grained parallelism in various matrix operations and is not
designed to scale beyond a multi-core workstation with GPU. We
show our cluster-based approach offers significant improvement
in scalability and runtime while producing comparable point
cloud density and more accurate point location estimates.

I. I NTRODUCTION
Detailed 3D reconstructions can be automatically generated
from photos and video by inferring the geometry of the realworld scene based on motion between multiple 2D viewpoints.
This process of creating Structure from Motion (SfM) of
imagery is a very active area of research, with many techniques
and applications being developed in recent years. The task is
especially challenging because determining each camera pose
and the matching points among the images represent an extraordinarily large solution space requiring much computation
to converge on a reasonable estimate.
Solutions to the problem have gained traction in the past
decade in commercial and military applications, including
mapping [1], robotic navigation [2], and most prevalently, 3D
geo-registration of photos and video [3]‚Äì[5]. SfM techniques
are well-suited to geo-registration because they work with
low-cost sensors and platforms and are robust to outliers
in telemetry data, which could propagate errors in other
techniques.
Automated 3D reconstruction represents a considerable
computational challenge, exercising the state-of-the-art in
computer vision, optimization, and parallel computing. SfM
techniques rely on identifying and matching common features from multiple images, a process whose runtime grows
quadratically with the input size. Moreover, a solution requires
an estimate of each camera‚Äôs parameters, which include both
extrinsics (e.g. location and pointing direction) and intrinsics
(e.g. focal length and lens distortion). Parameter estimation
presents a highly non-linear and non-convex optimization
This work is sponsored by the Assistant Secretary of Defense for Research
and Engineering under Air Force contract FA8721-05-C-0002. Opinions,
interpretations, conclusions, and recommendations are those of the author and
are not necessarily endorsed by the United States Government.

978-1-4673-1576-0/12/$31.00 ¬©2012 IEEE

problem. Compounding the computational difficulties are large
data considerations and real-time processing requirements
imposed by certain applications. The volume of imagery at
high-resolution is quickly outpacing the current capability to
process it. Furthermore, building models with high density and
accuracy requires large photo sets, and computation quickly
becomes intractable without efficient, parallel techniques.
Hence, scalable and automated algorithms are required to
process aerial imagery in appropriate computing environments.
Attempts to improve SfM efficiency have been previously
proposed [6]‚Äì[8], and multi-core processors and Graphics
Processing Units (GPUs) have been employed to accelerate
computation [9]. However, while some work has considered SfM in a cluster environment [10], little attention has
been given to processing datasets relevant to wide-area georegistration, such as aerial video, on a large-scale computing
grid.
This paper focuses on the problem of computing SfM from
aerial video for the purpose of fully automated geo-registration
of objects in the video. Applications require large input
datasets to produce accurate results, so the implementation
must be able to scale to an appropriately sized cluster in
order to meet a real-time processing budget. For this study, we
consider a dataset collected by MIT Lincoln Laboratory over
the MIT campus in 2011. The GPS location of the sensor
is available for all frames, and video frames are sampled
at several rates to produce photo sets of various sizes. We
propose an SfM processing chain that uses state-of-the-art
SfM methodologies that can run on a large-scale cluster. We
compare this chain‚Äôs runtime and reconstruction quality with
a freely available parallel SfM implementation [9] designed
to run on a high performance desktop workstation. Results
are shown for the sequential baseline implementation, the
multi-core workstation processing chain, and our cluster-based
processing chain. Our contributions are a novel technique for
parallelizing SfM and an in-depth analysis on the impact of
two parallelization methods on 3D reconstruction results.
The remaining sections will focus on describing SfM in
more detail, presenting our proposed solution, and evaluating
performance and scalability. Sec. II begins with a brief review
of the SfM problem for geo-registration. The next section,
Sec. III is a survey that outlines specific SfM implementations
in the literature and on the internet. Sec. IV introduces our
cluster-based SfM processing chain, while Sec. V compares it
with the alternative parallel and sequential processing chains.
Finally, we conclude in Sec. VI.

(a)

(b)

(c)
Fig. 1. (a) The baseline SfM processing chain consists of several steps of
varying computational complexity. (b) Key points based on SIFT are extracted
and matched among images. (c) Bundle adjustment creates a 3D point cloud
from the implicit scene geometry.

II. R EVIEW OF S TRUCTURE FROM M OTION (S F M)
The SfM problem infers the 3D geometry of a scene based
on multiple 2D views of the same scene from different aspect
angles. SfM techniques and applications have been active areas
of computer vision research in recent years. In general, SfM
techniques follow this this series of steps: feature extraction,
feature matching, and bundle adjustment (Fig. 1) [11]. The
processing chain takes a series of photos as its input and it
outputs a 3D model (generally a point cloud) and the estimated
position and viewing frustum of each camera. In addition to
the 3D model, SfM identifies a graph connecting each 3D
point to a set of views corresponding to features in the input
photos.
Feature extraction relates to the identification of distinct
features in each photo that can be robustly matched to features
in the other photos. Identifying distinctive image features is a
difficult problem, with many SfM techniques relying on the
Scale-Invariant Feature Transform (SIFT) developed by David
Lowe, who offers an implementation that accepts an image as
input and returns a list of key points described by their location

in the image, scale, orientation, and a 128-byte identifying
vector [12]. The computational cost of the feature extraction
step grows linearly (O(N )) with the number of input photos.
Computation also increases with photo resolution, resulting
in the identification of more key points per photo. Feature
extraction is embarrassingly parallel, as each photos feature set
can be computed independently of the others. Additionally, the
filters and dense operations performed within feature matching
can be parallelized further or implemented to take advantage
of hardware acceleration.
Following feature extraction, features must be matched
between images. A matched feature indicates two photos
have captured views of the same real-world point. In the
case of SIFT, two features would ideally be considered a
match when their 128-byte feature vectors have a Euclidean
distance below some threshold. For efficiency, matching is
generally performed using Approximate Nearest Neighbor
(ANN) techniques to avoid computing the distance between all
combinations of extracted features. In either case, the features
from any given image must be matched against the features of
every other image. This nominally leads to quadratic growth
(O(N 2 )) of computation with respect to the number of input
images. Although this step is computationally intensive, it is
also easily parallelized, as the feature comparisons can be
performed simultaneously.
Then 3D geometry is iteratively estimated from the matched
(and often sparse) features in a process termed sparse bundle
adjustment. As more views of the same point are added to
the bundle of points, the 3D location of each point is reestimated in a least-squares sense. To first order, bundle adjustment has O(N ) complexity, but runtimes generally approach
O(N 2 ) in reality. The process is iterative and is not trivially
parallelized. Nonetheless, the following section will discuss
several approaches for improving runtime performance. Upon
completion of bundle adjustment, a 3D point cloud has been
generated in an arbitrary 3D coordinate system. Compared
to a Cartesian geographic coordinate system, this coordinate
system has an unknown scale, rotation and translation.
Finally, many SfM implementations conclude with a step to
generate a denser point cloud by identifying likely surfaces in
the 3D model. This step results in appealing dense 3D models;
however, it generally does not maintain the graph relationship
between 3D points and features in the original photos. In the
remainder of this paper, we do not consider this step because
it is not relevant for most of our applications of interest.
III. I MPLEMENTATION S URVEY
Implementing SfM has been the focus of much research
aiming to improve the reconstruction quality and the computational efficiency of the processing chain. This section surveys
some notable work in the field but in no way constitutes
a complete list. We consider Bundler [13] as the baseline
SfM implementation. Bundler and its source code are freely
available on the web, and it has been cited in an abundance
of SfM papers. The tool makes no assumptions about its
input photo set; rather, it has been designed to handle a

completely unorganized collection, such as photos downloaded
from various sources on the web. Bundler runs all steps on a
single processor, but the feature extraction and matching steps
can be parallelized with relatively simple modifications to the
source code.
In recent years, many groups, including some of the authors
of Bundler, have proposed techniques for improving efficiency.
In [7] a sparse point cloud is formed using a ‚Äúskeletal set‚Äù
of the most important points, and then subsequent points
are added after significantly reducing the solution space for
remaining camera parameters. Additionally, in [14], they propose matching features against a SIFT vocabulary database,
which realizes computational savings compared to matching
against the entire input data set. Most recently, [8] suggests
finding a coarse initial estimate using a Markov random field
formulation and refining the initial solution using LevenbergMarquardt.
While Bundler has been designed for unstructured photo
sets, we are considering images from a video source. In this
case, one can assume the camera travels at a reasonable speed,
thereby limiting the change in camera location and pose from
frame to frame. It has been demonstrated that feature matching
can be reduced to O(N ) complexity by applying these video
constraints [3].
Another freely available tool, VisualSFM, is a highly optimized, parallel implementation of SfM that runs on multiprocessor workstations and can use a GPU if available [9].
VisualSFM is particularly efficient at SIFT feature extraction.
While Lowe‚Äôs implementation operates using a single processor, VisualSFM can take advantage of a GPU to greatly
accelerate extraction [15]. The tool also performs multithreaded matching and uses Multi-Core Bundle Adjustment
(MCBA) [16], which also uses a GPU if available, to generate
point clouds with considerable speed-up compared to Bundler.
MCBA uses a different approach for solving the non-linear
least squares problem underlying bundle adjustment, and it
exploits the fine-grained parallelism in the bottleneck matrix
operations (e.g. computation of the Jacobian matrix and several
matrix-vector multiplications).
Out-of-Core Bundle Adjustment [10] partitions the set of
feature matches using graph analysis techniques. The algorithm then runs several bundle adjustment processes in
parallel on different machines to create multiple point clouds.
The point clouds are then combined into a single model by
exploiting common points between the clouds. In addition
to offering speed-up via parallelization, this implementation
offers more scalability than VisualSFM because each bundle
adjustment process is on a separate machine, thereby allowing
the computation of large-scale point clouds that require more
memory than available to a single system. Our solution is
similar to [10], but we utilize the assumptions about the aerial
video to partition the image set and fuse the resulting point
clouds using different techniques, which achieve very high
point accuracy for our particular input data constraints.

Fig. 2. Camera locations and pointing directions for N aerial video samples
forming SfM input image set x0 , x1 , ..., xN ‚àí1 . The sensor continuously
points at a ground point of interest while the platform completes one full
revolution.

IV. C LUSTER - BASED S F M
We seek to efficiently generate 3D reconstructions from
large-scale datasets of aerial video frames on a compute
cluster. We assume an aerial platform captures the video while
completing a full revolution around a single ground point of
interest, continuously fixing the camera on the ground point, as
shown in Fig. 2. Given these data constraints and the available
SfM implementations (detailed in the previous section), this
section describes our scalable cluster-based approach.
A cluster is defined to be a collection of commodity Linuxbased machines connected via Ethernet. For the purposes of
this paper, communication between processes is available only
through a shared file system. A master node submits tasks
to worker nodes, which are managed by a central scheduler.
This computing model is scalable to thousands of nodes.
In general, an algorithm may share the cluster with other
processes. Therefore, the processing chain is constrained to use
a maximum number of processors, Np , specified as a runtime
parameter. The algorithm is implemented in a custom software
framework that dispatches each step on a set of cluster
nodes, enforcing barrier synchronization between steps. Input
imagery, intermediate results, and the final reconstruction are
persisted in a common database, enabling results to be re-used
by other processing chains.
The proposed processing chain is shown in Fig. 3. Clusterbased SfM can begin as soon as aerial video from a complete
revolution becomes available for ground processing. The video
is sampled at a constant rate to form a collection of N
images. Each image, along with its GPS metadata, is sent to
the processing chain input. Feature extraction and matching
follow the same technique as [13], but use modified scripts and
binaries to run in parallel. The feature matching step produces
a graph G = (V, E) whose vertices are the SIFT key points

|Œì(A) ‚à© B| > 0

(3)

where Œì(A) is the neighborhood of set A.
Given the point correspondences among the partitions, we
can transform the point clouds to a common 3D space. Each
point cloud is transformed to the coordinate system of the first
partition by solving the orthogonal Procrustes problem to find
the rotation matrix, Ri , scale, si , and translation, ti , between
the corresponding points in X0 and Xi . The transformation is
then applied to all points in the cloud Xi to form X0i for all
i > 0:
Fig. 3. Cluster-based SfM processing chain is comprised of multiple steps,
each dispatched on the number of CPUs shown above. Barrier synchronization
is enforced between each step.

(i.e. vertex vi,j ‚àà V corresponds to the j th SIFT point found
in image xi ). Edge vm vn exists if SIFT points vm and vn
were found to match in the feature matching step.
Following feature matching, the N input images
(x0 , x1 , ..., xN ‚àí1 )
are
partitioned
into
M
sets
(P0 , P1 , ..., PM ‚àí1 ), each to undergo independent bundle
adjustment in parallel. M cannot exceed Np (the maximum
number of processors available), but we apply the additional
constraint that N/M ‚â• 25 such that each partition has
enough images to ensure a reconstruction of reasonable
quality. Images are partitioned in a block-cyclic pattern with
C cycles. Photos are partitioned such that xi ‚àà Pj , where:


i
j=
mod M
(1)
bN/(M + C)c
Generally, C > 12 for large datasets (N  12M ) to limit
the maximum aspect angle between consecutive photos in the
same partition to no more than 30‚ó¶ , which promotes SIFT
matching.
We then perform parallel bundle adjustment by running the
Bundler binary independently on each partition. This results
in M point clouds each in its own arbitrary 3D coordinate
system. The point clouds can be described by the matrices
X0 , X1 , ..., XM ‚àí1 , each comprised of 3D points such that:
Ô£Æ
Ô£π
x0 x1
xNi ‚àí1
(2)
Xi = Ô£∞ y0 y1 ... yNi ‚àí1 Ô£ª
z0 z1
zNi ‚àí1
where Ni is the size of the particular point cloud and each
column contains the coordinates for a 3D point.
In order to combine them, we first must identify corresponding points among the partitions. Each reconstructed 3D point
has a view list, A ‚äÇ V , which is the set of SIFT points used
to determine its location. View lists from different partitions
will always be disjoint because each photo‚Äîand thus each
SIFT point‚Äîbelongs to only one partition. However, point
correspondences can be determined by using the match graph
G. Two 3D points from different partitions with view lists A
and B, respectively, are considered corresponding if:

X0i = si Ri Xi + ti 11√óNi

(4)

The point correspondences also indicate redundant key
point observations. Prior to combining the point clouds, we
must identify and remove duplicate points. Bundle adjustment
iteratively improves the estimate of each point‚Äôs position,
x = {x, y, z}. Thus, we can combine the d duplicate points
to form an improved estimate, x0 , using weight, w, equal to
the number of views in the view list (i.e. wi = |A|):
d
P
0

x =

wi2 xi

i=1
d
P

i=1

(5)
wi2

The duplicate points are removed from all containing clouds
except one, such that each duplicate x0 is replaced by its x0 .
The point clouds are then combined.
Finally, the last step transforms the point cloud into Universal Transverse Mercator (UTM) coordinates. We again estimate rotation, scale and translation between the arbitrary SfM
coordinate space and UTM, this time using the estimated camera locations from bundle adjustment and the known camera
GPS locations converted to Cartesian UTM coordinates. Poor
camera position estimates are removed using a RANSACbased technique. We then apply the transformation to the
cloud, resulting in a single point cloud in UTM coordinates.
This technique coarsely geo-registers the point cloud with
error contributions from the GPS and SfM camera placement
estimates. For applications requiring precise geo-registration,
a subsequent step would match the point cloud against reliable
reference data, such as LIDAR or satellite imagery, using this
coarse geo-registration as an initial estimate.
V. R ESULTS
We compare two parallel SfM processing chains against the
sequential baseline implementation [13]. The first chain is VisualSFM [9], which is freely available online and optimized to
use a multi-core processor and GPU (see Sec. III). The second
chain is the novel cluster-based technique described in Sec.
IV. We run each chain for various numbers of input images,
evaluating the chains in terms of runtime and reconstruction
quality. The cluster-based chain is also run for various numbers
of compute cores.

Bundler and VisualSFM benchmarks were run on a quadcore Intel Xeon (2.4GHz) workstation with 6GB RAM and an
nVIDIA Quadro FX 1800 GPU (64 compute cores) for math
kernel acceleration. Cluster-based benchmarks were run on
various numbers of compute cores, where each cluster machine
contained dual-socket Intel Xeon processors (3.2GHz) with
8GB RAM and was interconnected by gigabit Ethernet.
Fig. 4 shows runtime as a function of the number of input
images, for the baseline Bundler processing chain, VisualSFM,
and cluster-based SfM. Feature matching and bundle adjustment dominate runtime, with feature extraction barely visible
at the bottom of (a) and (b). VisualSFM offers significant
speed-up of the bundle adjustment step. However, clusterbased SfM ultimately offers much faster runtime, in particular
by scaling feature matching to more processing nodes. In (c),
for input sizes 100, 200, 300 and 400, M values of 4, 8, 12,
and 16 were used respectively.
Fig. 5 shows cluster-based SfM speed-up for various cluster
sizes. For small image set sizes, the overhead of the processing
chain actually increases runtime. However, for beyond 100
input images, cluster-based SfM provides significant speedup. For the largest set sizes and cluster sizes evaluated (top
right corner of plot) speed-up has converged to approximately
26x for a 64-node cluster. Speed-up is less than linear for
two primary reasons: in some cases M < Np , and in all
cases additional processing is required to fuse the point clouds
generated in parallel.

40
Speed-up (t(1)/t(Np))

A. Runtime Benchmarks

Cluster-based SfM Speed-up
45

35
30

N=75

25

100

20

200

15

300

10

400

5
0
0

16
32
48
Number of Processors (Np)

64

Fig. 5. Cluster-based SfM speed-up shown for various numbers of input
images, N .

Point Cloud Density
300
250
Points (x1,000)

The input image sets are comprised of extracted frames from
video of one aerial revolution around the MIT Stata Center.
The platform flew an approximate circle of two-mile radius at
an altitude of roughly 7500 ft. Each image is 1280√ó720 pixels
with latitude, longitude and altitude available as metadata. The
camera remained fixed on the ground target with a constant
field-of-view capturing the entire building and some immediate
surroundings.

200
150
100
50
0
50

100

Bundler

Fig. 6.

150

200
250
Image Set Size

VisualSFM

300

350

400

Cluster (N/M=25)

Point cloud density increases with larger input image set size.

B. Reconstruction Quality
We have demonstrated the significant runtime acceleration
achieved by both processing chains in the previous section.
However, parallelization does not come without trade-offs in
reconstruction quality. Generally, it is desirable to run SfM
on large data sets because the number of reconstructed 3D
points grows superlinearly with the size of the input image set.
Moreover, bundle adjustment is an iterative process that refines
the location of each point incrementally as more matched
features are evaluated.
Fig. 6 shows the growth in the point cloud size as a
function of image set size. Point cloud density for both parallel
implementations grow at slower rates than the baseline. Here
we see a clear trade-off of point cloud density for runtime
speed-up.
Although parallelization compromises point cloud density,
Table I shows that cluster-based SfM provides better convergence to the baseline point locations than VisualSFM.

This is likely a result of the different optimization problem
formulation employed by VisualSFM to improve parallelism
and runtime efficiency. Error was calculated for each point
cloud by comparing the 3D point location to that found by the
baseline implementation. Corresponding points in the clouds
produced by each processing chain were identified based on
intersecting SIFT points in each points view list. Error is
defined the distance (in meters) between the Bundler point
TABLE I
RMS E RROR OF R ECONSTRUCTED P OINTS ( METERS )
Input size (N )
VisualSFM
Cluster-based
SfM

M =4
M =8
M = 12
M = 16

100
0.20
0.10

200
7.15

300
1.05

400
0.44

0.27

0.26
0.26

0.25
0.24

Cluster-based SfM (32 cores)

VisualSFM (multi-core & GPU)

Bundler (single-core)

2

40
20

Runtime (hours)

60

Runtime (hours)

Runtime (hours)

30
20
10

100

Feat. Extr.

200
300
400
Image Set Size
Feat. Match

500

Bundle Adj.

(a)

1
0.5
0

0

0

1.5

100

Feat. Extr.

200
300
400
Image Set Size
Feat. Match

(b)

500

Bundle Adj.

100

200
300
Image Set Size

Feat. Match & Extr.

Bundle Adj.

400

Fuse

(c)

Fig. 4. Processing chain runtime (broken down by step) vs. number of input images for (a) Bundler, (b) VisualSFM and (c) cluster-based SfM (shown here
for Np = 32).

location and the corresponding point after scaling the clouds
using the coarse geo-registration technique described in Sec.
IV. Finally, RMS error is calculated for each point cloud
generated.
VI. C ONCLUSION
We have introduced a scalable, parallel SfM processing
chain suitable for use with aerial video of a single ground point
of interest. We have compared our technique to an alternative
parallel implementation and evaluated both in terms of runtime
efficiency and reconstruction quality. Practical SfM applications require scalable processing chains that can compute 3D
reconstructions for very large data sets in a fixed processing
time budget. We conclude that cluster-based SfM offers better
scalability than VisualSFM. Our cluster-based approach can
scale to run on a much larger system than VisualSFM, which
is restricted to the number of CPU and GPU cores than
can be integrated into a single machine and memory address
space. The fine-grained parallelization exploited by multicore bundle adjustment offers very impressive speed-up, but
ultimately it is the feature matching step that poses the biggest
bottleneck to both chains. Future work will focus on feature
matching optimizations for different types of input data sets.
Additionally, integration of fully automated precise point cloud
geo-registration could be made possible by matching point
clouds with reference data such as LIDAR or satellite imagery.
R EFERENCES
[1] ‚ÄúPhotosynth,‚Äù Microsoft Corporation, 2012, [Accessed April-2012].
[Online]. Available: http://photosynth.net
[2] H. Temeltas and D. Kayak, ‚ÄúSLAM for robot navigation,‚Äù Aerospace
and Electronic Systems Magazine, IEEE, vol. 23, no. 12, pp. 16 ‚Äì19,
Dec. 2008.
[3] A. N. Vasile, L. J. Skelly, K. Ni, R. Heinrichs, and O. Camps, ‚ÄúEfficient
city-sized 3D reconstruction from ultra-high resolution aerial and ground
video imagery,‚Äù in Proceedings of the 7th International Conference on
Advances in Visual Computing - Volume Part I, ser. ISVC‚Äô11, 2011, pp.
347‚Äì358.
[4] K. Ni, Z. Sun, and N. Bliss, ‚Äú3D image geo-registration using visionbased modeling,‚Äù in Acoustics, Speech and Signal Processing (ICASSP),
2011 IEEE International Conference on, May 2011, pp. 1573 ‚Äì1576.

[5] R. Kaminsky, N. Snavely, S. Seitz, and R. Szeliski, ‚ÄúAlignment of 3D
point clouds to overhead images,‚Äù in Proc. IEEE Conf. on Computer
Vision and Pattern Recognition, June 2009, pp. 63 ‚Äì70.
[6] R. Gherardi, M. Farenzena, and A. Fusiello, ‚ÄúImproving the efficiency
of hierarchical structure-and-motion,‚Äù in Proc. IEEE Conf. on Computer
Vision and Pattern Recognition, 2010.
[7] N. Snavely, S. M. Seitz, and R. Szeliski, ‚ÄúSkeletal sets for efficient
structure from motion,‚Äù in Proc. IEEE Conf. on Computer Vision and
Pattern Recognition, 2008.
[8] D. Crandall, A. Owens, N. Snavely, and D. P. Huttenlocher, ‚ÄúDiscretecontinuous optimization for large-scale structure from motion,‚Äù in Proc.
IEEE Conf. on Computer Vision and Pattern Recognition, 2011.
[9] C. Wu, ‚ÄúVisualSFM: A visual structure from motion system,‚Äù 2011.
[Online]. Available: http://www.cs.washington.edu/homes/ccwu/vsfm/
[10] K. Ni, D. Steedly, and F. Dellaert, ‚ÄúOut-of-core bundle adjustment for
large-scale 3D reconstruction,‚Äù in Computer Vision, 2007. ICCV 2007.
IEEE 11th International Conference on, Oct. 2007, pp. 1 ‚Äì8.
[11] R. Hartley and A. Zisserman, Multiple View Geometry in Computer
Vision, 2nd ed. Cambridge University Press, 2003.
[12] D. G. Lowe, ‚ÄúDistinctive image features from scale-invariant keypoints,‚Äù
Int. J. Comput. Vision, vol. 60, no. 2, pp. 91‚Äì110, Nov. 2004.
[13] N. Snavely, S. M. Seitz, and R. Szeliski, ‚ÄúPhoto tourism: exploring photo
collections in 3D,‚Äù in ACM SIGGRAPH 2006 Papers, ser. SIGGRAPH
‚Äô06. New York, NY, USA: ACM, 2006, pp. 835‚Äì846.
[14] Y. Li, N. Snavely, and D. P. Huttenlocher, ‚ÄúLocation recognition using
prioritized feature matching,‚Äù in Proceedings of the 11th European
conference on Computer vision: Part II, ser. ECCV‚Äô10, 2010, pp. 791‚Äì
804.
[15] C. Wu, ‚ÄúSiftGPU: A GPU implementation of scale invariant feature
transform (SIFT),‚Äù http://cs.unc.edu/ ccwu/siftgpu, 2007.
[16] C. Wu, S. Agarwal, B. Curless, and S. Seitz, ‚ÄúMulticore bundle adjustment,‚Äù in Proc. IEEE Conf. on Computer Vision and Pattern Recognition,
June 2011, pp. 3057 ‚Äì3064.

Efficient Anomaly Detection in Dynamic, Attributed
Graphs
Emerging Phenomena and Big Data
Benjamin A. Miller and Nicholas Arcolano

Nadya T. Bliss

Lincoln Laboratory
Massachusetts Institute of Technology
Lexington, MA 02420
Email: {bamiller, arcolano}@ll.mit.edu

Arizona State University
Tempe, AZ 85287
Email: nadya.bliss@asu.edu

Abstract‚ÄîWhen working with large-scale network data, the
interconnected entities often have additional descriptive information. This additional metadata may provide insight that can
be exploited for detection of anomalous events. In this paper, we
use a generalized linear model for random attributed graphs to
model connection probabilities using vertex metadata. For a class
of such models, we show that an approximation to the exact model
yields an exploitable structure in the edge probabilities, allowing
for efficient scaling of a spectral framework for anomaly detection
through analysis of graph residuals, and a fast and simple
procedure for estimating the model parameters. In simulation,
we demonstrate that taking into account both attributes and
dynamics in this analysis has a much more significant impact
on the detection of an emerging anomaly than accounting for
either dynamics or attributes alone. We also present an analysis
of a large, dynamic citation graph, demonstrating that taking
additional document metadata into account emphasizes parts of
the graph that would not be considered significant otherwise.
Index Terms‚ÄîSubgraph detection, network modularity, signal
detection theory, attributed graph modeling, generalized linear
models

I. I NTRODUCTION
In numerous big data applications, relationships between
entities are of interest. Connections between computers may
be analyzed for a computer security application [1], for
example, and large social networks are frequently analyzed
to find communities and influential figures [2], [3]. In these
applications, while the entities themselves may be of interest, it
is their connections and relationships that provide real insight
and situational awareness.
When working with relational data, a graph is a natural
mathematical structure for data representation. A graph G =
(V, E) is a pair of sets: a set of vertices V representing the
This work is sponsored by the Intelligence Advanced Research Projects
Activity (IARPA) under Air Force Contract FA8721-05-C-0002. The U.S.
Government is authorized to reproduce and distribute reprints for Governmental purposes notwithstanding any copyright annotation thereon.
Disclaimer: The views and conclusions contained herein are those of the
author and should not be interpreted as necessarily representing the official
policies or endorsements, either expressed or implied, of IARPA or the U.S.
Government.

978-1-4673-6213-9/13/$31.00 ¬©2013 IEEE

entities, and a set of edges E denoting their relationships
or connections. Traditionally, graphs consist of only vertices
and their connections, possibly with weights on the edges.
In many application domains, relational datasets also contain
contextual information on both edges and vertices. We receive
raw collections not as a graph, but as a (potentially multisource) set of heterogeneous data points. When a graph is
constructed, a large fraction of the data is not considered. If
these usually-ignored portions of the data are retained, they
can be useful for many purposes. In particular, these metadata
provide extra dimensions to the graph that can help determine
which observed edges are likely to occur, and which are out of
the ordinary. This enables the uncued detection of anomalies in
the graph, which is the primary objective of many applications.
In earlier work, we developed a framework for uncued
anomaly detection in large, dynamic graphs [4]. Detection
of anomalous subgraphs is an important capability across a
variety of applications, such as detection of malicious software
in a computer network, or the detection of threat actors in a
communication network [5]. This framework treats a graph
as an instance drawn from a distribution of random graphs,
and performs spectral analysis of graph residuals (i.e., the
difference between the observed graph and its expected value)
to determine the presence of anomalies. This analysis uses a
given expected degree model, which has a low-rank expected
value structure, to facilitate efficient residuals analysis. The
parameters of this model have a simple, closed-form estimator
that is approximately asymptotically unbiased, as shown in [6].
The residuals were analyzed over time, with a filter applied
to emphasize certain behaviors. Steps toward optimizing such
filters were later presented in [7].
In this paper, we demonstrate a new technique for detection
of anomalies in large, attributed graphs. Leveraging recent
work in subgraph detection, we show that this technique
enables the detection of emergent activity that would not be
detectable in a static graph, or one without attributes. This
technique is based on a generalized linear model (GLM)
framework for data modeling. Since this model has some
properties that would make it intractable for real-time analysis

179

ISI 2013, June 4-7, 2013, Seattle, Washington, USA

of large datasets, we provide an approximation that enables
fast parameter estimation and efficient analysis of attributed
graphs at large scales.
The remainder of this paper is organized as follows. In
Section II, we outline basic graph properties, and the attributes
considered in our analysis. Section III presents the anomaly
detection problem model and our temporal integration technique. In Section IV, we present the analysis of an efficient
method for incorporating the GLM into the subgraph detection
framework, which allows for an eigendecomposition of the
residuals matrix to be performed in linear time for a constant
number of categorical attributes. Section V presents results
of this technique in simulation, demonstrating the power of
using both dynamics and attributes in the analysis of graph
residuals, and in Section VI we present findings when this
technique is applied to a large document corpus. In Section
VII, we summarize and outline future research directions.
II. G RAPHS AND ATTRIBUTES
As mentioned previously, a graph is a pair of sets G =
(V, E). A graph may be either directed, where an edge goes
from one vertex to another, or undirected, where the two
vertices are connected with no notion of ordering. In the
former case, E ‚äÇ V √óV , where ordering denotes the direction,
whereas in the latter case E consists of 2-vertex subsets
of V . For the purpose of this paper, we will consider only
unweighted graphs, though the analysis can be extended to
graphs with weights. For convenience, let N = |V | and
M = |E|. An important notion in our analysis is that of
vertex degree. The degree of a vertex is the number of other
vertices with which it shares an edge, i.e., the degree of v ‚àà V
is |{u|{u, v} ‚àà E}| for undirected graphs, and in a directed
setting, v has in- and out-degrees of |{u|(u, v) ‚àà E}| and
|{u|(v, u ‚àà E}|, respectively.
In our detection framework, we make use of matrix representations of a graph. The adjacency matrix A of the graph is
a matrix in which the entry in row i and column j is nonzero
only if there is an edge from vertex i to vertex j. (This requires
an arbitrary labeling of the vertices with integers from 1 to N .)
The value of an entry will be the edge weight in the case of
a weighted graph, or 1 for an unweighted graph. Also, if the
graph is undirected, the adjacency matrix will be symmetric.
Finally, let k ‚àà ZN denote the vector of observed degrees,
with the ith component ki being the degree of vertex i. For
directed graphs, there will be two vectors, kin and kout , for inand out-degrees.
We will consider graph attributes that can be divided into
the following cases:
‚Ä¢ Vertex attributes: properties of the entities, which may be
real-valued or categorical;
‚Ä¢ Edge attributes: properties of the relationships, which
may be real-valued or categorical;
‚Ä¢ Vertex pair attributes: regardless of the presence of an
edge, an attribute regarding a pair of vertices.
Within this context, edge attributes are typically part of the
observed relationships, e.g., duration of a connection in a com-

puter network or amount of money transferred in a financial
graph. Thus, real-valued attributes can be expressed as edge
weights, and categorical attributes can be encoded through the
use of multi-graphs, i.e., graphs with multiple edges allowed
between one pair of vertices, in this case denoting different
kinds of relationships.
With this in mind, we will limit the scope of attributes
we consider to those related to vertices and pairs of vertices. That is, the attributes of interest will be those that
manifest themselves as parameters to the model (intrinsic
vertex properties) rather than a specific network instance
(observed relationships). For the purpose of the approximation
introduced in Section IV, we will also only consider vertex
pair attributes that are categorical. While this ignores some
potentially interesting behavior‚Äîsuch as cases where past
observations impact the probability of future ones‚Äîit enables
a convenient, exploitable model that enables efficient residuals
analysis in a broad space of random graphs.
III. P ROBLEM M ODEL
We pose the subgraph detection problem as one of traditional signal detection theory, as first discussed in [8]. The
objective, given an observed graph over time, G(n), is to
resolve the following binary hypothesis test:
(
H0 : The observed graph is ‚Äúnoise‚Äù GB
H1 : The observed graph is ‚Äúsignal+noise‚Äù GB ‚à™ GS .
Here, GB = (V, E) is drawn from a distribution of random
graphs, and GS = (VS , ES ) is a small subgraph (NS =
|VS |  N ) that is embedded into the background. This
subgraph will be unlikely to occur under the background
distribution, and, therefore, is an anomaly and will serve as
the signal. Only cases where the subgraph is embedded on
vertices already existing in the background, i.e., VS ‚äÇ V and
GB ‚à™ GS = (V, E ‚à™ ES ), are considered.
The present work is focused on graphs with attributes on
the vertices, and between pairs of vertices. For this purpose,
we will assume that the metadata are not affected by the
embedding procedure. That is, when embedding a subgraph
into a background graph GB , which has fixed attributes,
the vertices of selected to comprise GS will maintain their
attribute values, and only the topology will change.
We resolve the hypothesis test through analysis of graph
residuals. Filter coefficients h are given to integrate the residuals over a time window. An approximation of E[A(n)] is
derived from the observed dynamic graph, and we consider
the eigendecomposition of the integrated residuals matrix,
U ŒõU T =

L‚àí1
X

h` (A(n ‚àí `) ‚àí E [A(n ‚àí `)]),

`=0

where L is the length of the time window. By considering
only a few eigenvectors, we can reduce the dimensionality
of the problem, and efficiently determine the presence of an
anomalous subgraph.

180

As in [7], we will consider a known signal model, where
the subgraph behavior of interest is known, but its position
within the background is not. To compute these coefficients,
a 3-way tensor of the subgraph adjacency matrix AS (n) is
formed, where two dimensions correspond to vertices and one
dimension corresponds to time, and a rank-1 approximation
for the tensor is computed, as described in [9]. The factor of
this approximation along the temporal dimension is used for
the filter coefficients. As noted in [7], this will maximize the
spectral norm of the integrated subgraph
L‚àí1
X

AS (n ‚àí `)h` ,

`=0

thus improving its detectability in the eigenspace of the residuals matrix. In the subsequent sections, we will demonstrate
that, while temporal integration provides a significant benefit
in terms of subgraph detection ability, knowledge of vertex
attributes allows for detection of much subtler anomalies.

probabilities. That is, we allow the approximation

pij ‚âà exp xTout Œ≤i + xTin Œ≤j + xTpair Œ≤ij ,
since for small values of x, ex ‚âà (1 + e‚àíx )‚àí1 . The log-linear
model is frequently used as a generalized linear model when
a distribution can have an arbitrary positive expected value,
such as a Poisson distribution. For small expected values, a
Poisson distribution approaches a Bernoulli distribution, so
this model is a good approximation for sparse graphs where
any individual edge is fairly unlikely. As we demonstrate in
the remainder of this section, this approximation has properties
that can be exploited for computational efficiency.
In the case of categorical vertex pair attributes, i.e., cases in
which the vertex pair attributes are dependent on the categories
of the associated vertices, the rank of the expected value matrix
will be equal to the number of categories. Let C be the number
of categories, Ic be the indices of vertices in category c, and
I(i) be the indices of all vertices in the same category as
vertex i. The probability matrix P = {pij } has the form

IV. R ESIDUALS A NALYSIS IN ATTRIBUTED G RAPHS

P = diag (Œ±out ) ŒòXŒòT diag (Œ±in ) .

To model a random graph without edge weights, each
possible edge will be modeled as a Bernoulli random variable.
Let the probability of an edge occurring between vertices i and
j be pij . For an attributed graph, we use a logistic regression
model based on vertex and edge attributes. Let Œ≤i be a vector
of real-valued attributes for vertex i, and Œ≤ij be the attributes
of vertex pair (i, j). Using a logistic regression framework,
we can use the same principle as linear regression to map a
linear combination of attributes to edge probabilities, i.e.,

Here, Œò is an N √ó C assignment matrix, where each row
contains a single entry of 1, with all other entries being 0.
The position of the nonzero entry corresponds to the category
of the vertex, i.e., if vertex i is in category c, the ith row of
Œò will have a 1 only in column c. The matrix X = {xij }
consists of the terms from (1) dependent upon the categories
of the vertex pairs, meaning that xij = xTpair Œ≤Ii Ij , and the
vectors Œ±out and Œ±in are defined such that Œ±out (i) = xTout Œ≤i and
Œ±in (i) = xTin Œ≤i . This matrix will have rank C.
The relatively low rank of the probability matrix allows for
fast computation of the principal eigenvectors of A ‚àí P . As
discussed in [4], matrix-vector multiplications are at the heart
of algorithms for computing extreme eigenvectors and eigenvalues of matrices. With a probability matrix in the form of (2),
eigenvalues of the residuals matrix can be computed without
computing the full probability matrix, and the running time
will scale tractably for large graphs. To compute the product
(A‚àíP )z for an arbitrary vector z ‚àà RN , we first compute Az,
which is a sparse matrix-vector multiplication taking O(M )
time. Computing P z requires two entry-wise scalings (by Œ±in
and Œ±out , O(N ) time each), two multiplications by the sparse
Œò matrices (also O(N ) time), and multiplication of a vector by
X (O(C 2 ) time). Finally, P z is subtracted from Az, costing
O(N ) time, yielding a total running time for the matrixvector multiplication of O(M +N +C 2 ). Using the implicitlyrestarted Lanczos algorithm to compute m eigenvectors of the
residuals matrix, the matrix-vector multiplication running time
implies a total time complexity of O((M +C 2 )m+N m2 +m3 )
per restart, where the number of restarts depends on the
relative distance between consecutive eigenvalues. Thus, to
compute a fixed number of eigenvectors, if the number of
categories remains fixed, the per-iteration cost of the algorithm
will scale linearly in the number of edges.
In addition to efficient computation of the eigenspace, this
model enables a fast parameter approximation scheme based

1

pij =
1 + exp



‚àíxTout Œ≤i

‚àí xTin Œ≤j ‚àí xTpair Œ≤ij

.

(1)

Here, xout , xin and xpair are the attribute weights for source vertices, destination vertices, and pairs of vertices, respectively.
Note that, for undirected graphs, xout = xin . This sort of model
has been used in link prediction [10], to quantify the impact
of attributes on whether or not an edge occurs.
While this generalized linear model allows a direct mapping
of attributes to probabilities, this comes at the expense of a
relatively expensive fitting procedure. A maximum likelihood
technique can be used to estimate the coefficients in xin , xout
and xpair , but this algorithm will cost O(N 2 ) time per iteration,
which is not tractable for large graphs. Also, to enable scaling
to very large graphs, which are typically sparse, it is beneficial
to have an exploitable structure in the expected value matrix
that enables fast calculation of residuals without storing a
large, dense matrix. For example, when using Newman‚Äôs
modularity matrix, as described in [11], the residuals matrix is
the sum of a sparse matrix and a rank-1 matrix, which allows
efficient computation of eigenvalues. No such exploitable
structure exists for the GLM, meaning that the residuals matrix
may be a dense matrix with no special structure, which will
require O(N 2 ) time and space simply for storage.
Since we are typically interested in sparse graphs, we use
a log-linear model rather than a logistic-linear model for edge

181

(2)

on moment matching. Consider the sum over the entries of
the observed adjacency matrix corresponding to edges going
from category a to category b. In expectation, we have
Ô£Æ
Ô£π
XX
XX
EÔ£∞
aij Ô£ª =
Œ±out (i)Œ±in (j)XIa Ib
i‚ààIa j‚ààIb

i‚ààIa j‚ààIb

= kŒ±out (Ia )k1 kŒ±in (Ib )k1 XIa Ib .
Consider also the expected out-degree of vertex i. We have
Ô£Æ
Ô£π
N
N
X
X
EÔ£∞
aij Ô£ª =
Œ±out (i)Œ±in (j)XI(i)I(j)
j=1

2

10000
X = 4 3500
4000

C
X

kŒ±in (Ic )k1 XI(i)Ic

c=1

Similarly,
PC the expected in-degree is given by
Œ±in (i) c=1 kŒ±out (Ic )k1 XIc I(i) . By assuming that each
block of Œ±in and Œ±out corresponding to a distinct category
has unit L1 norm, we can estimate the probability matrix by
letting the estimate of X be a matrix of the observed volumes
of the portions of the graph corresponding to different
b = ŒòT AŒò, and the estimates of Œ±in and
category pairs, i.e., X
Œ±out be the observed in- and out-degree vectors, normalized
in each category block to have an L1 norm of 1. Thus, we
have estimates
kin (i)
j‚ààI(i) kin (j)

and

kout (i)
.
j‚ààI(i) kout (j)

Œ±
d
out (i) = P

The restriction of the Œ± vectors having unit L1 norm does not
disallow any probability matrices of the form (2). Indeed, for
arbitrary vectors Œ±out and Œ±in , the same P can be computed
e =
with block-normalized vectors using a new X matrix, X
{xÃÉij }, such that
xÃÉij = xij kŒ±out (Ii )k1 kŒ±out (Ij )k1 .
Another interesting feature of this approximation is that, like
the modularity matrix of [11], it creates a residuals matrix
where the vector of all 1s, denoted 1, is in the nullspace.
Multiplying the adjacency matrix on the right by 1, we get the
vector of out degrees. The ith row of the estimated probability
matrix times 1 will yield
N
X
kout (i)
kin (j)
bI(i)I(j)
X
kkout (I(i))k1 j=1 kkin (I(j))k1

=

3500
10200
3100

3
4000
3100 5 ,
9900

j=1

= Œ±out (i)

Œ±
cin (i) = P

V. S IMULATION R ESULTS
To demonstrate the impact of this additional information
on subgraph detection, a Monte Carlo simulation was run,
generating dynamic random graphs from a model based on
(2). The graphs generated are undirected, with 3 categories,
and 1000 vertices in each category. The Œ± vector is created
according to a power law, since many large, real-world graphs
have powerlaw distributions. The categorical parameters create
a homophily effect, causing higher probability of connectivity
between vertices in the same category, similarly to the model
for interaction rates proposed in [12]. In particular, we use

C
X
kout (i)
bI(i)I = kout (i).
X
j
kkout (I(i))k1 j=1

T
This holds similarly, yielding kin
, when multiplying by 1T
on the left. Thus, any eigenvectors (for undirected graphs) or
singular vectors (for directed) that are not in the nullspace
will be orthogonal to 1, and the projection into the principal
singular vectors will be centered at the origin.

making connection within a category about 2.5‚Äì3 times as
likely as between categories. The Œ± vector was block-unitnormalized, so the values in X correspond to the expected
volumes within the corresponding parts of the graph.
To incorporate dynamics, we generate 8 samples from this
distribution independently. This is used for the H0 case. For
the H1 case, 5 vertices are chosen uniformly at random
from each category, and these 15 vertices comprise the signal
subgraph. The signal starts with no edges, then adds edges at
a constant rate over the course of the first 7 time steps, until it
reaches a prescribed density. At the last time step, edges are
removed, so that the subgraph has only 30% of the edges it
had in the previous sample. This could represent an increase in
communication leading up to an event, followed by dispersion
to avoid detection. The filter coefficients are optimized using
the Matlab Tensor Toolbox1 . After integration according to
this filter, a chi-squared test for independence is performed in
the principal 2-dimensional subspace, as described in [8]. The
result of this test is used as our detection statistic.
Four separate scenarios were run. In one case, neither
dynamics nor attributes were considered. In this scenario, we
take the graph at time step n = 7, i.e., the point in time
where the subgraph is the densest. The residuals matrix used
is the modularity matrix of [11], i.e., A ‚àí (kk T )/kkk1 . Thus,
the categories of the vertices are not considered. In the second
case, we use dynamics, but not attributes, estimating k without
considering categories, and integrating (A(n) ‚àí (kk T )/kkk1 )
using the given filter coefficients. Third, we consider attributes,
but not dynamics. Again, only time step n = 7 is used, but
P is estimated using the method described in Section IV.
Finally, we use both dynamics and attributes. In this case,
P is estimated using measurements at all time steps, and we
integrate the residuals over time using the filter.
Detection performance in a 2000-trial simulation is presented in Figure 1. When neither dynamics nor attributes
are considered (top left), even when the subgraph is 100%
dense, detection performance is fairly mediocre. For densities
below 100%, performance is not much better than chance.
When dynamics, but not attributes, are taken into account
(top right), we achieve perfect detection performance when the
subgraph grows to 90% density, with performance decreasing
1 Available

182

online at http://www.sandia.gov/‚àºtgkolda/TensorToolbox/.

With Dynamics, Without Attributes
1

0.8

0.8

Probability of Detection

Probability of Detection

Without Dynamics, Without Attributes
1

0.6

0.4

0.2

100% dense
95% dense
90% dense
85% dense

0
0

0.2
0.4
0.6
0.8
Probability of False Alarm

0.6

0.4

0.2

90% dense
85% dense
80% dense
75% dense

0

1

0

1

0.8

0.8

0.6

0.4

0.2

80% dense
75% dense
70% dense
65% dense

0
0

0.2
0.4
0.6
0.8
Probability of False Alarm

1

With Dynamics, With Attributes

1

Probability of Detection

Probability of Detection

Without Dynamics, With Attributes

0.2
0.4
0.6
0.8
Probability of False Alarm

0.6

0.4

0.2

45% dense
40% dense
35% dense
30% dense

0

1

0

0.2
0.4
0.6
0.8
Probability of False Alarm

1

Figure 1. Detection performance in simulated graphs. Simulations were run in which the detection algorithm either includes (right column) or does not
include (left column) dynamics, and includes (bottom row) or does not include (top row) vertex attributes. Densities listed are the greatest density of the
signal subgraph over an 8-sample window. Detection performance increases as the subgraph gets denser, and as more features of the graph are considered.

as the density is reduced. With attributes and not dynamics
(bottom left), even weaker subgraphs, with density of only
80%, are detected with near-perfect accuracy. Taking into
account both attributes and dynamics (bottom right), however,
provides a substantial benefit well beyond using either feature
individually. We see near-perfect detection when the subgraph
grows to only 45% density, which is undetectable in any of the
other cases, demonstrating that accounting for dynamics and
attributes has the potential to drastically increase performance.
VI. DATA A NALYSIS
Using the commercially available Thomson Reuters Web of
R
Science
(WoS) database [13], we built a directed, dynamic
graph based on document citations. This dataset is comprised
of records, compiled for research purposes, representing scholarly publications of the international scientific community,
published between 1900 and present in public commercial
and open source journals and conference proceedings. Each
record represents an individual document, and fields include
document title and type, journal name, author names and
institutional affiliations (as provided in publication), cited
references, and publication date.
In the citation graph, each vertex represents a document,
with a directed edge going from vertex i to vertex j if

document i cites document j. We consider a citation graph for
each year, since this is the most reliable time resolution. For
each year, we fit the model in (2) to the graph at that time slice,
using the procedure outlined in Section IV, with the vertex
category determined by the ‚Äúsubject‚Äù field in the database.
Over the years from 1937 to 1986, the yearly graph increases
from about 1500 documents and 80,000 citations per year to
nearly 500,000 documents and 7.8 million citations per year.
At each year, after fitting the graph to the model, we integrate
the residuals over a 6-year window, using a linear ramp filter to
emphasize emergent behavior. The top 30 singular values are
computed, which are shown in the lefthand plot in Figure 2.
In 1976, there is a substantial uptick in several of the singular
values, so we will investigate this period of time in detail.
Looking at the window from 1971 to 1976, the second and
third right singular vectors (corresponding to cited documents)
are shown in the scatterplot on the right hand side of Figure 2.
(The first singular vector is dominated by a single, high-degree
vertex, seen in the lower left in the plot.) The highlighted
outliers toward the top of the plot all correspond to analytical
chemistry papers written in the 1950s and ‚Äô60s [14]‚Äì[18], all
of which have accumulated thousands of citations over the
years. Each of these documents has somewhat high degree

183

Web of Science Citation Graph, 1937‚àí1986
1200

Singular Value

1000
800
600
400
200
0

1940

1950

1960
Year

1970

1980

1990

Figure 2. The Web of Science citation graph over 50 years. Singular values of the integrated residuals matrices grow over time, with a large spike in values
highlighted in 1976 (left). The singular vectors in 1976 show 5 outliers corresponding to analytical chemistry papers with high cross-subject citation (right).

in 1976, but not as high as some other vertices that do not
stand out in the residuals space. As it turns out, the dynamics
and the attributes of the graph cause these papers to stand out
over vertices that would be stronger otherwise. These 5 articles
increase their annual citation count over the course of the 6year time window, from less than 700 collective citations in
1971 to about 1000 in 1976. The ramp filter emphasizes this
growth to make these vertices stand out more prominently.
These documents also received citations from a broader range
of subjects than other documents. The 5 outliers are cited
by documents from between 63 and 76 of the 290 different
subject labels over the 6 years, while other analytical chemistry
papers with even higher degree were cited by documents from
47 to 55 different subjects over the same period. Without
considering document subjects, these 5 documents are buried
in the background noise, but their substantial cross-subject
citation brings them into the front of the residuals space,
demonstrating the power of this approach.
VII. S UMMARY
In this paper, we describe a simple model for incorporating vertex attributes into spectral analysis of dynamic
graph residuals. An approximation to the logistic regression
model enables both efficient computation of residuals and
simple fitting of model parameters. Simulations demonstrate
the benefit to detection performance of taking both dynamics
and vertex attributes into consideration, and an analysis of a
dynamic citation network shows that accounting for attribute
information emphasizes portions of the graph that would not
be strong otherwise. It is clear that using vertex metadata is
a powerful technique for subgraph detection, and future work
will focus on extending the approximation used here to an
even broader class of models, analyzing the statistics of the
suggested estimator, and determining detectability when the
observations are corrupted or obfuscated.
ACKNOWLEDGMENT
The authors wish to thank J. Kepner and M. S. Beard for
their tremendous help in setting up the data ingestion and

access architecture, enabling the analysis in Section VI, and
A. Reuther, R. A. Bond and D. Martinez for their support of
this research.
R EFERENCES
[1] T. IdeÃÅ and H. Kashima, ‚ÄúEigenspace-based anomaly detection in computer systems,‚Äù in KDD, pp. 440‚Äì449, 2004.
[2] M. E. J. Newman and M. Girvan, ‚ÄúFinding and evaluating community
structure in networks,‚Äù Phys. Rev. E, vol. 69, no. 2, 2004.
[3] J. M. Kleinberg, ‚ÄúAuthoritative sources in a hyperlinked environment,‚Äù
J. ACM, vol. 46, pp. 604‚Äì632, September 1999.
[4] B. A. Miller et al., ‚ÄúA scalable signal processing architecture for massive
graph analysis,‚Äù in ICASSP, pp. 5329‚Äì5332, 2012.
[5] C. Weinstein, W. Campbell, B. Delaney, and G. O‚ÄôLeary, ‚ÄúModeling
and detection techniques for counter-terror social network analysis and
intent recognition,‚Äù in Proc. IEEE Aerospace Conf., pp. 1‚Äì16, 2009.
[6] N. Arcolano et al., ‚ÄúMoments of parameter estimates for Chung‚ÄìLu
random graph models,‚Äù in ICASSP, pp. 3961‚Äì3964, 2012.
[7] B. A. Miller and N. T. Bliss, ‚ÄúToward matched filter optimization for
subgraph detection in dynamic networks,‚Äù in SSP, pp. 113‚Äì116, 2012.
[8] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúToward signal processing
theory for graphs and non-Euclidean data,‚Äù in ICASSP, pp. 5414‚Äì5417,
2010.
[9] D. M. Dunlavy, T. G. Kolda, and E. Acar, ‚ÄúTemporal link prediction
using matrix and tensor factorizations,‚Äù ACM Trans. Knowl. Discovery
from Data, vol. 5, Feb. 2011.
[10] K. T. Miller, T. L. Griffiths, and M. I. Jordan, ‚ÄúNonparametric latent
feature models for link prediction,‚Äù in Advances in Neural Inform.
Process. Syst. 22 (Y. Bengio, D. Schuurmans, J. Lafferty, C. K. I.
Williams, and A. Culotta, eds.), pp. 1276‚Äì1284, 2009.
[11] M. E. J. Newman, ‚ÄúFinding community structure in networks using the
eigenvectors of matrices,‚Äù Phys. Rev. E, vol. 74, no. 3, 2006.
[12] S. T. Smith et al., ‚ÄúNetwork detection theory and performance.‚Äù Preprint:
arXiv:1303.5613v1, 2013.
[13] ‚ÄúThomson Reuters Web of Science.‚Äù http://thomsonreuters.com/ products services/science/science products/a-z/web of science.
[14] M. DuBois et al., ‚ÄúColorimetric method for determination of sugars and
related substances,‚Äù Anal. Chem., vol. 28, no. 3, pp. 350‚Äì356, 1956.
[15] P. S. Chen, T. Y. Toribara, and H. Warner, ‚ÄúMicrodetermination of
phosphorus,‚Äù Anal. Chem., vol. 28, no. 11, pp. 1756‚Äì1758, 1956.
[16] T. Bitter and H. M. Muir, ‚ÄúA modified uronic acid carbazole reaction,‚Äù
Anal. Biochem., vol. 4, pp. 330‚Äì334, October 1962.
[17] R. S. Nicholson and I. Shain, ‚ÄúTheory of stationary electrode polarography: Single scan and cyclic methods applied to reversible, irreversible,
and kinetic systems,‚Äù Anal. Chem., vol. 36, no. 4, pp. 706‚Äì723, 1964.
[18] M. S. Patterson and R. C. Greene, ‚ÄúMeasurement of low energy
beta-emitters in aqueous solution by liquid scintillation counting of
emulsions,‚Äù Anal. Chem., vol. 37, no. 7, pp. 854‚Äì857, 1965.

184

18th International Conference on Information Fusion
Washington, DC - July 6-9, 2015

Temporal and Multi-Source Fusion for Detection of
Innovation in Collaboration Networks
Benjamin A. Miller‚àó , Michelle S. Beard‚Ä† , Manfred D. Laubichler‚Ä° and Nadya T. Bliss¬ß
‚àó MIT

Lincoln Laboratory, Lexington, Massachusetts 02420
Email: bamiller@ll.mit.edu
‚Ä† Charles Stark Draper Laboratory, Cambridge, Massachusetts 02139
Email: mbeard@draper.com
‚Ä° School of Life Sciences, Arizona State University, Tempe, Arizona 85287‚Äì4501
Santa Fe Institute, Santa Fe, New Mexico 87501
Marine Biological Laboratory, Woods Hole, MA 02543
Email: manfred.laubichler@asu.edu
¬ß School of Computing, Informatics and Decision Systems Engineering
Simon A. Levin Mathematical, Computational and Modeling Sciences Center
Global Security Initiative
Arizona State University, Tempe, Arizona 85287‚Äì7205
Email: nadya.bliss@asu.edu
Abstract‚ÄîA common problem in network analysis is detecting
small subgraphs of interest within a large background graph.
This includes multi-source fusion scenarios where data from
several modalities must be integrated to form the network. This
paper presents an application of novel techniques leveraging
the signal processing for graphs algorithmic framework, to
well-studied collaboration networks in the field of evolutionary
biology. Our multi-disciplinary approach allows us to leverage
case studies of transformative periods in this scientific field as
truth. We build on previous work by optimizing the temporal
integration filters with respect to truth data using a tensor
decomposition method that maximizes the spectral norm of the
integrated subgraph‚Äôs adjacency matrix. We also demonstrate
that we can mitigate data corruption via fusion of different data
sources, demonstrating the power of this analysis framework for
incomplete and corrupted data.

I. I NTRODUCTION
In numerous applications, the data of interest are entities and
the relationships, connections, and interactions between them.
We may be interested in interactions between individuals,
communication between computers, or interaction between
proteins. Across these diverse application areas, the data of
interest are naturally represented as a graph.
One of the application domains where casting the data as a
graph is widely used is the analysis of social networks. Analyzing the interactions between people allows for identification
of community structure and influential figures. A network
of scientific collaborators is a particularly interesting type of
social network. Understanding the way innovation manifests
itself within the public record via collaborative publications
may lead to new insights into the evolution of scientific
research.
In this paper, we analyze such a network in the context of
a novel anomaly detection framework called signal processing
for graphs (SPG) [1]. This framework enables the detection of

978-0-9824-4386-6/15/$31.00 ¬©2015 IEEE

small, anomalous clusters within large, dynamic background
graphs. Within this framework, a filtering technique can be
used to emphasize certain patterns of behavior and increase
the power of these ‚Äúsignal‚Äù components of the graph within
the ‚Äúnoise‚Äù of the background. This paper considers an optimization technique with respect to a known, rigorously-studied
innovation period, and demonstrates that the optimal filter
does in fact bring a significant subset of data to a prominent
position within the analytical space. This framework can be
applied when the graph is derived from many fused sources,
which can also improve detection performance by considering
multiple ‚Äúlooks‚Äù at the data. Since network data are often
noisy or incomplete, we also consider observation of corrupted
data within this context. While data corruption significantly
hinders performance, we can leverage the diversity of multiple
measurements and recover the signal by fusing the corrupted
observations.
The remainder of this paper is organized as follows. Section II reviews the subgraph detection problem and defines
notation. Section III discusses the filtering technique for analyzing dynamic graphs. Our dataset of interest‚Äîco-authorship
networks of authors who all cite a seminal paper within a large,
dynamic collaboration network‚Äîis described in Section IV.
Section V presents the results of a set of experiments on this
dataset, including filter optimization to best emphasize the
innovation subnetwork and methods to fuse multiple corrupted
observations and still maintain signal power. In Section VI, we
summarize and provide possible avenues for further investigation.
II. S UBGRAPH D ETECTION P ROBLEM
A graph G = (V, E) consists of a vertex set V , a set of
entities, and an edge set E, a set of edges which represent relationships between vertices. The subgraph detection problem

659

is a classical detection problem studied in [2] [3] with a graph
as its observation. We cast the problem of subgraph detection
as detecting a signal embedded in noise, where our objective
is to resolve the binary hypothesis test

filter h over the length of a time window ‚Ñì and aggregate the
residuals, obtaining
BÃÉ(n) =

‚Ñì‚àí1
P
i=0

H0 : The observed graph is ‚Äúnoise‚Äù GB
H1 : The observed graph is ‚Äúsignal+noise‚Äù GB ‚à™ GS .
Let H0 denote the null hypothesis, an undirected, unweighted graph GB = (VB , EB ) generated by some random
model. The alternative hypothesis, H1 , has an additional graph
GS = (VS , ES ) embedded into GB . The problem is to decide
whether or not the null hypothesis is true based on whether the
observed graph deviates significantly from normal background
behavior.
While optimal detection is possible in some scenarios [4],
we focus on cases where this would be computationally
intractable. We take a spectral approach, which has the benefit
of analyzing the data in a space where there are known
metrics for power and detectability [5]. Our subgraph detection
procedure is based on the spectral analysis of modularity. Modularity is commonly used to detect communities in graphs [6],
but in the context of this paper‚Äîand in the SPG framework
more broadly‚Äîwe analyze modularity to detect the presence
of an anomaly. The modularity matrix B of an unweighted,
undirected graph G is given by
B =A‚àí

kkT
2|E| ,

where A is the adjacency matrix of G (i.e., Aij is 1 if vertices
vi and vj share an edge and is 0 otherwise) and k is the
observed degree vector of G, where ki is the number of
edges connected to vi . The matrix B can be considered the
residuals matrix, a matrix consisting of the difference between
the observed edges A and the expected edges kk T /(2 |E|) (the
expectation under the Chung‚ÄìLu model [7], which assumes no
community structure).
The algorithms described in [2] [3] analyze the residuals
under H0 and H1 by studying the eigendecomposition of B
(i.e. B = U ŒõU T ) and compute a test statistic to discriminate
between the two hypotheses. In [2], we determine the presence
of an anomaly by analyzing only the first two eigenvectors of
B. To compute the test statistic, matrix B is projected into
the space of its 2 principal eigenvectors u1 and u2 . This is
the linear subspace in which the residuals are largest, and,
intuitively, a subgraph with particularly large residuals will
separate from the rest of the vertices in this space.
III. D ETECTION IN DYNAMIC N ETWORKS
Extending the SPG framework to dynamic graphs, our
observation is a sequence of time-varying graphs G(n) =
(V, E(n)) where the vertex sets remain constant and the edge
sets vary over time [8], [9]. Dealing with time-series graphs,
we consider the residuals integrated over a time window.
At each discrete time step n we have a graph G(n) and a
modularity matrix B(n). We apply a finite impulse response

B(n ‚àí i)h(i).

Let BÃÉ(n) be the aggregated residuals matrix for the graph at
time n filtered by h. Thus BÃÉ(n) is a matrix where in each
vertex entry is the result of a vertex pair having its modularity
filtered by h. The sequence of filter coefficients h is designed
to effectively emphasize the subgraph and de-emphasize the
background. The problem of choosing the appropriate filter
coefficients is discussed in further detail later, where h will
be computed to maximize the integrated signal power over
time for a particular subgraph of interest. We perform the
same analysis on BÃÉ(n) as performed on B for static graphs
to discriminate between H0 and H1 .
IV. DATASET
One of the significant challenges in developing and evaluating subgraph detection techniques is lack of truth for many
of the applications of interest. In this work, as in [10], we
leverage rigorously studied period of scientific innovation
in evolutionary and developmental biology. This multidisciplinary approach allows us to refine our algorithmic techniques
while also potentially providing insight into emergence of
innovation in scientific literature. In this section, we describe
the dataset that is used for our analysis throughout the rest of
the paper.
The case study we consider is the emergence of the concept
of gene regulatory networks in developmental biology. As
discussed in [11], [12], [13], gene regulatory networks are one
of the main explanatory concepts in today‚Äôs evolutionary and
developmental biology. The history and emergence of this idea
are also well-studied [13]. This includes early conceptual ideas
from the beginning of the 20th century and more specifically,
the recent developments based on the formulation by Roy
Britten and Eric Davidson published in Science in 1969
[14]. The Britten-Davidson (BD) model is a clear study of
transformative innovation in a scientific field. As discussed in
[10], the citations to the 1969 BD paper illustrate its persistent
impact. Specifically, the citations rapidly increased throughout
the 1970s, dropping somewhat during 1980s and 1990s, and
again increasing in the 2000s and 2010s. Furthermore, second
order citations, or citations of papers that cite the BD paper,
tell a similar story - including a sharp increase in second
order citations in 2000s and 2010s. Second order citations are
good indicators of broader impact of the idea, especially when
combined with first order citation patterns.
Study of the BD model and its impact on the field has
allowed for an observation of the fact that scientific innovation
at least in this case, leads to re-wiring of patterns of collaboration. Based on the analysis of this case study, truth or signal
subgraphs were created by co-authorship graphs of the authors
that have directly cited the BD paper. The signal subgraphs
span 1969 to 2000. All citation graphs were extracted from the
Web of Science database and covered a representative sample

660

of the field of developmental biology, specifically, the top 12
journals in the field plus Science, Nature, and Proceeding of
the National Academy of Sciences. For each year, the graphs
considered were unweighted and undirected (co-authorship
is fundamentally undirected) yielding a symmetric adjacency
matrices. Total number of unique authors (number of vertices
in the graph) was help consistently at 294,700 (representing
the number of unique authors in the entire time period). The
ordering of authors in the graph, while arbitrary, was preserved
(as is necessary) in each year.
V. E XPERIMENTS
A. Temporal Filter Optimization
Within the SPG framework, the spectral norm is a good
power metric for signal and noise power [15]. When an
embedded subgraph‚Äôs spectral norm is large, its vertices are
more likely to stand out in the eigenspace. When working
with the temporal integration technique described in Section
III, this means that it is desirable to choose filter coefficients
that maximize the spectral norm of the principal submatrix of
the adjacency matrix associated with the subgraph vertices.
As originally discussed in [16], the subgraph‚Äôs spectral
norm can be maximized by forming a 3-way tensor from
the subgraph adjacency matrix, and computing a low-rank
approximation for this tensor. Let AS be an NS √óNS √ó‚Ñì tensor
for the subgraph vertices, where NS = |VS |. The first two
dimensions represent vertices and the third dimension represents time. Much like approximating a matrix with its singular
value decomposition, a low-rank tensor decomposition can be
used to approximate AS . For a rank-1 approximation, this is
achieved by solving
arg maxŒª,x,y,z

NS X
NS X
‚Ñì
X
i=1 j=1 t=1

(AS (i, j, t) ‚àí Œªxi yj zt )

2

(1)

B. Data Corruption

subject to kxk2 = 1, kyk2 = 1, kzk2 = 1.
Here x, y ‚àà RNS and z ‚àà R‚Ñì are vectors, and Œª ‚àà R is a
scalar. Our objective is to maximize the spectral norm of the
integrated adjacency matrix whose ijth entry is given by
ahij =

‚Ñì
X
t=1

subgraph of interest. In each case, the eigenvectors associated
with the largest 20 (nonnegative) eigenvalues were computed,
with smaller indices corresponding to larger eigenvalues. The
values of the plots are the components of the (unit-normalized)
eigenvectors that are associated with the subgraph vertices.
Without any knowledge of truth, one may assume that simply
averaging over time would be a reasonable approach, or that
integrating using a ramp filter (where the weight on each
successive time step increases in a linear fashion) would
detect interesting subgraphs, given that this would emphasize
emerging behavior. Using these strategies, as shown in the
figure, there is only one vertex that is particularly strong within
the eigenvectors with the largest eigenvalues. Using a method
that considers the spectral norm of the subgraph at each point
in time (i.e., using weights corresponding to the instantaneous
power of the foreground) provides some additional benefit,
as a few additional vertices stand out more prominently in
eigenvector 14. Using a filter that is optimized via the tensor
decomposition, on the other hand, brings out several more
vertices. When this filter is applied, nine vertices from the
subgraph stand out significantly in eigenvector four. Looking
back at the data used to optimize the graph (i.e., the authors
citing the seminal BD paper), these nine vertices comprise the
largest connected component in any given year, and in fact
form a clique (a graph with all possible edges) in 1977. Two
of the authors in this cluster are also part of a larger clique
with nine other authors in the background, who also stand out
in the same eigenvector. This is a significant finding: the most
interconnected that authors citing the BD paper ever become
in a given year, as well as other close collaborators. Without
this temporal integration technique, the subgraph would not
stand out from the background within this low-dimensional
space. We will focus on this subgraph for the remainder of
the experiments.

AS (i, j, ‚Ñì + 1 ‚àí t)h(t).

It turns out that this quantity is optimized‚Äîunder the constraint that the squares of the filter weights sum to 1‚Äî
by setting the filter weights h(t) equal to the time-reversed
temporal factor z‚Ñì+1‚àít from (1). This computation can be
done in Matlab using the PARAFAC decomposition [17] in
the Tensor Toolbox [18].
The effect of tuning the filter with respect to the vertices of
interest has been demonstrated in simulation [16], but here we
demonstrate application to the well-studied period of scientific
innovation described in Section IV: we optimize the filter
applied to the coauthorship graph from 1969 to 1980. As
demonstrated in Fig. 1, the impact is extremely significant.
Within each plot, there is one curve for each vertex in the

In many applications, the datasets from which the graphs are
derived are inherently incomplete or noisy. When forming the
graph, these errors can have a significant impact on detection
performance. Interference or noise can lead to incorrect or
missing edges. Clerical errors can lead to an edge being
switched from one vertex to another. And data are frequently
sampled from a population, giving an inherently incomplete
view of the individual interactions. All of these factors can
significantly hinder performance of detection algorithms.
The impact of network uncertainty on subgraph detection
has been of interest in recent years [19], [20]. In this paper,
we consider the impact of two of the corruption mechanisms
from [20] on detection of the subgraph pulled out of the
noise in Section V-A. One mechanism is a simple missing
data model, in which each edge that exists in the true graph
exists in the observed graph with equal probability. As noted in
[20], this mechanism reduces the power of random background
behavior more slowly than it reduces the power of clusters that
stand out in the eigenspace. This is a consequence of Wigner‚Äôs
semicircle law. Considering an ErdoÃãs‚ÄìReÃÅnyi random graph‚Äî

661

Equal Weights

Ramp Filter
0.4
Eigenvector Component Value

Eigenvector Component Value

0.15

0.1

0.05

0

‚àí0.05
0

5

10
15
Eigenvector Index

0.3
0.2
0.1
0
‚àí0.1
‚àí0.2
0

20

5

Eigenvalue‚àíBased Filter
0.4
Eigenvector Component Value

Eigenvector Component Value

20

Optimized Filter

0.2

0.1

0

‚àí0.1

‚àí0.2
0

10
15
Eigenvector Index

5

10
15
Eigenvector Index

0.3
0.2
0.1
0
‚àí0.1
0

20

5

10
15
Eigenvector Index

20

Fig. 1. Projections of subgraph vertices onto principal eigenvectors with various temporal integration techniques. Within the space of the principal eigenvectors,
only one vertex is particularly prominent when using equal weights (top left), linearly increasing weights (top right), or weights determines by eigenvalues
(bottom left). Only when an optimized filter is applied (bottom right) do a substantial number of subgraph vertices become prominent in the eigenspace.

where all possible edges are equally probable‚Äîthe range of
eigenvalues is proportional p
to the standard deviation of the
edge presence probability, p(1 ‚àí p). For sparse graphs, p
will be small and thus changing p will change the largest
‚àö
eigenvalues by approximately p. Meanwhile, the subgraph
that does not fit the background model will have its spectral
norm reduced by a factor of p, reducing the signal-to-noise
ratio and making the detection problem more difficult. As
shown in [20], this phenomenon also occurs in more realistic
models that incorporate arbitrary degree structure.
The other corruption mechanism we consider is an edgeflipping mechanism, where there is a random model for data
corruption based on vertex degree. For each vertex in the
graph, we assume that the number of errors is proportional to
the number of edges the vertex has. Similarly to the Chung‚ÄìLu
model, we assign a weight wi to vertex vi , where
Œ±
ki .
wi = qP
N
j=1 kj
The probability of an edge error between vertices vi and vj
is then pcorr
= wi wj . If there is an edge in the true graph
ij

between these two vertices, then it will not be observed with
probability pcorr
ij , and if there is no such edge, then this is the
probability with which an edge will be incorrectly observed.
The scalar Œ± controls the overall number of errors. In this
paper, the corruption is focused on those vertices‚Äîin the
entire graph, including the subgraph of interest‚Äîthat are most
prominent in the principal eigenspace for the true graph. This
concentrates the effects of the corruption on the portion of the
graph that we analyze, to better demonstrate the impact of this
mechanism on eigenvector analysis.
A typical example of the impact of these data corruption
mechanisms on subgraph detection ability is illustrated in
Fig. 2. Each scatter plot in the figure is created using the two
eigenvectors (among the principal 10) that most prominently
feature the subgraph vertices. The nine-vertex clique from
the data of interest, and the nine other close collaborators,
clearly stand out in the fourth eigenvector when the graph
is uncorrupted. For missing data, we consider a case where
only 15% of the edges are observed. In the plotted instance,
the subgraph vertices were most prominent in the eighth and
ninth eigenvectors. While some of the vertices still stand out

662

True Graph

0.1

0

0.1

0.3

0

0.2
Eigenvector 6

Eigenvector 9

Eigenvector 5

0.2

‚àí0.1
‚àí0.4

Degree‚àíBased Corruption

Missing Data

0.3

‚àí0.1
‚àí0.2

‚àí0.2

‚àí0.1
0
0.1
Eigenvector 4

0.2

‚àí0.4
‚àí0.4

0
‚àí0.1

‚àí0.3

‚àí0.3

0.1

‚àí0.2

0
0.2
Eigenvector 8

0.4

0.6

‚àí0.2
‚àí0.3

‚àí0.2

‚àí0.1
0
Eigenvector 5

0.1

0.2

Fig. 2. Scatter plots emphasizing the subgraph from the known period of innovation. Background vertices are in blue, while the nine-vertex clique and its
close collaborators are in red. When working with the true graph (left), the vertices all stand out in the fourth eigenvector. When only observing 15% of
the edges, the subgraph partially stands out in the eighth and ninth eigenvectors, but many vertices are buried in the background (center). The degree-based
corruption method similarly has a few vertices standing out in the 5th and 6th eigenvectors, but many of them are overpowered by background noise.

in this space, most of them are subsumed by other activity, and
many vertices are very close to the origin. The degree-based
corruption model (where about half of the observed vertices
are errors) has a different effect on performance, but the result
is similar. In the case plotted in the figure, the subgraph
vertices stand out the most in the 5th and 6th eigenvectors.
The background is much noisier due to the extra activity, and
many of the subgraph vertices are buried within the noise. In
both of these cases, the loss in power will reduce detection
performance.

structure) that enables efficient eigenvector analysis at scale.
In practice, it may also be difficult to estimate the model
parameters, and there may be mismatch with the true model.
We therefore focus on a method for fusing based on a weighted
sum.
When given the two observed graphs, they will be fused as
follows. For each pair of vertices, a fused observation will be
computed as

C. Fusion of Corrupted Data

Here the Œ≤ parameters are the weights of the corrupted observations. We are operating in the context of logistic regression,
where a linear function of the observations is mapped to an
expected value via the logistic function. Within this context,
values for aÃÇij only need to be computed if an edge exists
between vi and vj in one of the observations. Otherwise,
the probability is assumed to be 1/(1 + e‚àíŒ≤0 ), which can
be accounted for by adding a rank-1 matrix to the fused
observations.
Fusing the observations in this way improves the representation of our subgraph of interest in the eigenspace, as
demonstrated in Fig. 3. Under the same corruption scenarios as
in Section V-B, we measured the ‚Äúpower‚Äù of the subgraph in
the first 10 eigenvectors. Let U be the N √ó 10 matrix where
each column is an eigenvector of the integrated modularity
matrix for the observed (or fused) graph, and let x ‚àà {0, 1}N
be an indicator vector for the subgraph that is emphasized by
the optimized filter. We measure the power of the subgraph
in this space as kU T xk22 , i.e., the L2 norm squared of the orthogonal projection into the space spanned by the 10 principal
eigenvectors. Using the optimized filter, this will be reduced by
the corruption mechanisms, but can be recovered by fusing the
two observations. Let Ptrue be the power when U is computed
from the true graph, and we will compare the power P from
other cases to this quantity. Fig. 3 provides cumulative density
functions (CDF) demonstrating the probability that a corrupted
(or fused) observation will provide the signal power within
its principal eigenspace, as determined via a Monte Carlo
simulation. As shown in the figure, working with only 15% of

While the medium through which we observe a network
can create artifacts that hinder detection performance, it will
sometimes be possible to get multiple ‚Äúlooks‚Äù at the data. If the
error mechanisms are not correlated, it is possible to use the
diversity of the measurement domains to recover performance.
As alluded to in [20], this can be done via a Bayesian fusion
method or by weighting the individual observations based on
the level of trust in the source.
At relatively small scale, a Bayesian fusion method can be
quite powerful for performance recovery. With the two corruption mechanisms considered in this paper, we can estimate
that an edge exists in the latent graph in the following way.
Let pprior be the prior probability of edge existence in the
corr
be the ijth entry in
latent graph, and let aij , amiss
ij , and aij
the adjacency matrix of the true graph, the graph with missing
data, and the graph with degree-based corruption, respectively.
is 1, then aij is 1, since edges can only be taken away
If amiss
ij
with the missing data mechanism. If amiss
is zero, then the
ij
probability that the edge exists in the true graph is
Ô£±
prior
(1‚àípobs )pcorr
ij p
Ô£¥
if acorr = 0
Ô£¥
Ô£≤ (1‚àípobs )pcorr
+
1‚àíp
( corr
ij
ij )
prior
P [aij = 1] = (1‚àípobs )(1‚àípcorr
(2)
ij )p
if acorr = 1.
Ô£¥
obs ) 1‚àípcorr +pcorr
Ô£¥
(1‚àíp
( ij ) ij
Ô£≥

While fusing in this fashion has the potential to completely
recover detection performance‚Äîas demonstrated in simulation
in [20]‚Äîthe posterior expected value of A will be dense, and
may not have the sort of exploitable structure (e.g., low-rank

aÃÇij =

663

1
.
1 + exp ‚àíŒ≤0 ‚àí Œ≤1 amiss
‚àí Œ≤2 acorr
ij
ij

(3)

would be to study optimization of filter coefficients when there
are missing data in the training set, as in [21]. At a higher level,
it would be interesting to determine what other subgraphs can
be emphasized by this technique, and to find what subgraphs
are detected using the same filters in more recent publication
data. A comparative study of which filters detect patterns of
innovation in different scientific fields might also contribute
to a better understanding of the structure of different scientific
practices.

Subgraph Representation in Eigenspace
1
Missing Edges

Prob(P/Ptrue‚â§ t)

0.8

Degree‚àíBased Corruption
Equal Weights
Logistic Weights

0.6
0.4

ACKNOWLEDGMENT
0.2
0
0

0.2

0.4
0.6
Power Threshold t

0.8

1

Fig. 3. Cumulative density functions for the signal power maintained in
various scenarios. The power level considered is the norm squared of the
projection of the indicator vector for the subgraph. The horizontal axis is the
ratio of this power level for observed or fused data to the same quantity with
the true (uncorrupted) graph.

the edges can significantly reduce the power of the subgraph
in the top eigenvectors: about half the time less than 50% of
the power remains. The degree-based corruption mechanism in
which about half of the observations are incorrect also reduces
performance, but not usually as drastically, maintaining, on
average, over 62% of the power. By simply averaging the two
observations together, we shift the CDF by over 10%. Finally,
by using the fusion technique of (3), we improve upon this
result, increasing the signal power maintained by an additional
5%.
VI. C ONCLUSION
This paper investigates the use of temporal and multi-source
integration to enable detection of known innovation patterns
in scientific literature. Dynamic collaboration networks are
analyzed with the signal processing for graphs framework,
focused principally on eigenspace analysis of graph residuals
integrated over time. The temporal weights are optimized with
respect to a known innovation period surrounding the BrittenDavidson model for gene regulation, specifically among authors that cite the seminal paper on the model. We demonstrate
that this technique boosts the power of the largest connected
component of this subset of the data to a point where it
can be detected within a low-dimensional projection of the
data. Using two simple error models for graph data, we show
the negative impact of working with a corrupted graph, with
the detected subgraph having its power reduced while being
subsumed by other activity in the principal eigenspace. Using
a simple weighting procedure, we demonstrate that we can
recover the power of the subgraph within this space.
There are numerous potential areas for future development.
One interesting area would be determining an approximation
to the Bayesian fusion method in (2) that would allow the
technique to scale to very large graphs. Another possibility

The Lincoln Laboratory portion of this work is sponsored by
the Assistant Secretary of Defense for Research & Engineering
under Air Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the
authors and are not necessarily endorsed by the United States
Government. M. D. L. is supported by the NSF SES Grant
No. 1243575.
R EFERENCES
[1] B. A. Miller, N. T. Bliss, P. J. Wolfe, and M. S. Beard, ‚ÄúDetection theory
for graphs,‚Äù Lincoln Laboratory J., vol. 20, no. 1, 2013.
[2] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúToward signal processing
theory for graphs and non-Euclidean data,‚Äù in ICASSP, 2010, pp. 5414‚Äì
5417.
[3] ‚Äî‚Äî, ‚ÄúSubgraph detection using eigenvector L1 norms,‚Äù in Advances in
Neural Inform. Process. Syst. 23, J. Lafferty, C. K. I. Williams, J. ShaweTaylor, R. Zemel, and A. Culotta, Eds., 2010, pp. 1633‚Äì1641.
[4] T. Mifflin, ‚ÄúDetection theory on random graphs,‚Äù in Proc. Int. Conf.
Inform. Fusion, 2009, pp. 954‚Äì959.
[5] R. R. Nadakuditi, ‚ÄúOn hard limits of eigen-analysis based planted clique
detection,‚Äù in Proc. IEEE Statistical Signal Process. Workshop, 2012, pp.
129‚Äì132.
[6] M. E. J. Newman, ‚ÄúFinding community structure in networks using the
eigenvectors of matrices,‚Äù Phys. Rev. E, vol. 74, no. 3, 2006.
[7] F. Chung, L. Lu, and V. Vu, ‚ÄúThe spectra of random graphs with given
expected degrees,‚Äù Proc. of National Academy of Sciences of the USA,
vol. 100, no. 11, pp. 6313‚Äì6318, 2003.
[8] B. A. Miller, M. S. Beard, and N. T. Bliss, ‚ÄúMatched filtering for
subgraph detection in dynamic networks,‚Äù in Proc. IEEE Statistical
Signal Process. Workshop, 2011, pp. 509‚Äì512.
[9] ‚Äî‚Äî, ‚ÄúEigenspace analysis for threat detection in social networks,‚Äù in
Int. Conf. Inform. Fusion, 2011, pp. 1‚Äì7.
[10] N. T. Bliss, B. R. E. Peirson, D. Painter, and M. D. Laubichler,
‚ÄúAnomalous subgraph detection in publication networks: Leveraging
truth,‚Äù in Proc. 48th Asilomar Conf. Signals, Syst. and Comput., 2014.
[11] E. H. Davidson, The regulatory genome: Gene regulatory networks in
development and evolution. Academic Press, 2010.
[12] D. C. Krakauer, J. P. Collins, D. Erwin, J. C. Flack, W. Fontana, M. D.
Laubichler, S. J. Prohaska, G. B. West, and P. F. Stadler, ‚ÄúThe challenges
and scope of theoretical biology,‚Äù Journal of Theoretical Biology, vol.
276, no. 1, pp. 269‚Äì276, 2011.
[13] M. D. Laubichler, J. Maienschein, and J. Renn, ‚ÄúComputational perspectives in the history of science: To the memory of Peter Damerow,‚Äù
Isis, vol. 104, no. 1, pp. 119‚Äì130, 2013.
[14] R. J. Britten and E. H. Davidson, ‚ÄúGene regulation for higher cells: A
theory,‚Äù Science, vol. 165, no. 891, pp. 349‚Äì357, 1969.
[15] B. A. Miller, M. S. Beard, P. J. Wolfe, and N. T. Bliss, ‚ÄúA spectral framework for anomalous subgraph detection,‚Äù 2014, preprint:
arXiv:1401.7702.
[16] B. A. Miller and N. T. Bliss, ‚ÄúToward matched filter optimization for
subgraph detection in dynamic networks,‚Äù in Proc. IEEE Statistical
Signal Process. Workshop, 2012, pp. 113‚Äì116.
[17] E. Acar, D. M. Dunlavy, and T. G. Kolda, ‚ÄúA scalable optimization approach for fitting canonical tensor decompositions,‚Äù Journal of
Chemometrics, vol. 25, no. 2, pp. 67‚Äì86, February 2011.

664

[18] B. W. Bader, T. G. Kolda et al., ‚ÄúMatlab tensor toolbox
version 2.5,‚Äù Available online, January 2012. [Online]. Available:
http://www.sandia.gov/‚àºtgkolda/TensorToolbox/
[19] J. B. Collins and S. T. Smith, ‚ÄúNetwork discovery for uncertain graphs,‚Äù
in Proc. Int. Conf. Inform. Fusion, 2014.
[20] B. A. Miller and N. Arcolano, ‚ÄúSpectral subgraph detection with corrupt

observations,‚Äù in Proc. IEEE Int. Conf. Acoust., Speech and Signal
Process., 2014, pp. 3449‚Äì3453.
[21] E. Acar, D. M. Dunlavy, T. G. Kolda, and M. M√∏rup, ‚ÄúScalable
tensor factorizations for incomplete data,‚Äù Chemometrics and Intelligent
Laboratory Systems, vol. 106, no. 1, pp. 41‚Äì56, March 2011.

665

TRAINING IMAGE CLASSIFIERS WITH SIMILARITY METRICS, LINEAR
PROGRAMMING, AND MINIMAL SUPERVISION
Karl Ni‚Ä† , Ethan Phelps‚Ä† . Katherine L. Bouman‚Ä° , and Nadya Bliss‚Ä†
MIT Lincoln Laboratory‚Ä† , Massachusetts Institute of Technology‚Ä°
E-mails: {karl.ni, ethan.phelps, nt}@ll.mit.edu, klbouman@mit.edu

ABSTRACT
Image classification is a classical computer vision problem
with applications to semantic image annotation, querying, and
indexing. Recent and effective generative techniques assume
Gaussianity, rely on distance metrics, and estimate distributions, but are unfortunately not convex nor keep computational architecture in mind. We propose image content classification through convex linear programming using similarity
metrics rather than commonly-used Mahalanobis distances.
The algorithm is solved through a hybrid iterative method
that takes advantage of optimization space properties. Our
optimization problem uses dot products in the feature space
exclusively, and therefore can be extended to non-linear kernel functions in the transductive setting.
1. INTRODUCTION
Image classifiers and content recognition is an age-old computer vision problem, the most prominent applications being labeling and retrieving images semantically. The literature has consistently employed learning algorithms involving
parameter estimation built from training sets. Training and
classification methods almost universally rely on two components: feature extraction and matching.
Both feature extraction and matching require low noise
levels in the training data, and therefore, significant manual
involvement in either labeling or segmentation. Additionally, extensive cross-validation procedures must drive down
false alarms. Finally, there may be multiple instances of
a single concept that are not addressed. To ensure relevant and accurate features at such massive scales, training
data fidelity and segmentation truth is often manually performed with crowd-sourcing tools like Antonio Torralba‚Äôs LabelMe [1],the now-retired Google labels, and most face/object
detection/recognition training sets [2, 3]. While effective, the
gain in accuracy has not yet offset the needed throughput.
This has inspired a more recent push towards multiinstance, unsupervised learning [4, 5, 6], in which the proThis work is sponsored by the Assistant Secretary of Defense for Research & Engineering under Air Force Contract # FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the author and are not necessarily endorsed by the United States Government.

978-1-4673-5051-8/12/$31.00 ¬©2012 IEEE

1979

posed algorithm is grouped. The paradigm reflects the notion
that with enough quantity, where current data rates and accessibility are outpacing processing capabilities, training quality
can be improved naturally via large numbers and through
noise integration.
Popular multi-instance learning techniques approach classifier construction generatively by modeling the conditional
distributions of various semantic classes [7, 8, 9]. The most
mature parameter estimation for distribution parameters were
effected with multi-modal Gaussian mixtures (GMM‚Äôs). Unfortunately, without correct choices in the number of clusters, assumptions on noise behavior, and good initialization,
maximum likelihood parameter estimates through expectation maximization (EM, a.k.a, iterative annealing, [10]) will
produce irrecoverable and incorrect feature prototypes. Furthermore, GMMs have small sample bias and are often instable with respect to parameterization. Subsequently, iteratively
determined optimal values are sensitive to initialization. Online or incremental clustering is also limited through EM and
may require respecification of variables. The problem is augmented by the number of parameters to be updated, which
significantly impacts the objective function. Finally, convergence speed depends on dimensionality as GMMs and similar
techniques traditionally (and logically) utilize difference metrics, often the Mahalanobis distance.
Instead of modeling the representation generatively, we
propose to determine prototype features for comparing images directly by finding a small subset through sparsity constraints in a linear programming (LP) problem. Replacing
distance metrics and using only dot products, nonlinearity
may be introduced with kernel matrices representing a positive definite kernel space. The resultant system classifier relies on normalized cross-correlation (similarity) between features derived from a query image and those from a trained
subset of prototypes. The implementation as matched filter
bank will fit many system architectures.
This paper will discuss these issues and the resultant classifying prototypes along with the many practical aspects that
allow minimal supervision. Sec. 2 describes the convex problem required to train images. Sec. 3 discusses associated theoretical results and problems, and Sec. 4 promotes more practical procedures.

Asilomar 2012

2. TRAINING IMAGE CLASSIFIERS WITH
SIMILARITY METRICS
The empirical determination of optimal filters in a training
set is based on solutions that find the best prototype or feature
set that is least redundant. The optimal features are used as
templates, that can eventually serve as matched filters during
runtime. This section details the convex optimization problem
that can be used to determine both linear and nonlinear filters
for discrimination.

(a) min

P

`‚àû rows

(b) min

P

`2 columns

Fig. 1. Fig. 1(a) is the proposed algorithm while Fig. 1(b) is the `2
Group Lasso penalty

2.1. Linear Classification in Euclidean Space
Let a d dimensional feature be denoted by x, and X be the
collection of N features, and xi is organized as the ith column
of X. Then an LP problem that determines the vectors for a
data set that are the most representative and least redundant
can be specified as follows.
arg min

X

‚àítr X T XŒ≤ + Œª ¬∑
ti

Œ≤,t

such that
and

i

0 ‚â§ Œ≤ij ‚â§ ti ‚â§ 1
Œ≤T 1 = 1

(1)

In (1), the selector matrix to determine which features to use
as prototypes is embedded in Œ≤; the tuning parameter Œª determines the extent to how much we‚Äôd like to reduce redundancy
by inducing sparsity; and the training set for a single class
is written in matrix form, X ‚àà Rd√óN . Intuitively, the solution matrix Œ≤ will indicate the smallest set of features in X
that best represent it by indicating them with nonzero values.
Each column vector Œ≤i ‚àà Œ≤ selects the candidate prototypes
for every xi . As the full paper will discuss, the natural tendency of the elements of Œ≤ will tend toward 1 or zero, but occasionally it can take on a value v in between. In such cases,
a single ‚Äúbest‚Äù vector is chosen through maximum likelihood.
Regardless of the values in Œ≤i , the final matrix Œ≤ will have a
rank equal to the number of classes as its optimum value.
Correctly framed sparsity solutions not only induce efficiency in computation and class depiction, but can reduce
noise and improve error rates. Optimization in (1) is reminiscent of research on sparse feature representation (i.e., dictionary learning techniques)1 . While classifying images is often
formulated with the construction of learned features, recent
surveys on such work has proven such methodology unnecessary and inefficient. Nevertheless, similar techniques to enforce sparse class structure 2 , have appeared in convex grouping problems [11], though are less efficient and intuitive as
seen in Fig. 2.1.
1 It is important to note that the proposed algorithm does not solve this
problem
2 As opposed to feature representation, our problem addresses this problem instead.

1980

2.2. Nonlinear Classification via Kernel Matrix
Note that (1) consists solely of dot products with respect to
vectors in X, which we can use to improve and extend the
proposed problem in (1). Similar to SVM‚Äôs, nonlinearity
may be introduced in the form of kernel optimization with
the reproducing kernel Hilbert space (RKHS) in the general
form of (2), where K is a positive definite kernel function
(or convex grouping of kernel functions) in the RKHS. The
logical extension to (1) is the straightforward assignment of
K(xi , xj ) = hœÜ(xi ), œÜ(xj )i, where we can bypass the calculation and knowledge of the high-dimensional mapping of
œÜ : Rd ‚Üí Rt in the transductive setting. When t  d, (2) is
exceedingly useful.

arg min

X

Œ≤

ij

such that
and

Œ≤ij K(xi , xj ) + Œª ¬∑

X

ti

i

0 ‚â§ Œ≤ij ‚â§ ti ‚â§ 1
Œ≤T 1 = 1

(2)

3. CLASSIFIER ANALYSIS
For most learning frameworks, an instance x is classified by
comparing to prototypes or probabilistic models to determine
the likeliest solution based on a distribution in some feature
space. That is, the feature vector x belongs to class i of C
classes if it is closest to the prototypes in the set {yj }i characterizing the ith class. Take a simplistic view of classification
in GMMs, where each mixture component relates to a single
class:


arg max
K exp (x ‚àí yi )T Œ£‚àí1 (x ‚àí yi )
(3)
i‚àà{1,...,C}

= arg min
i‚àà{1,...,C}

‚àí2xT Œ£‚àí1 y + kyi k2Œ£‚àí1 .

(4)

Solutions to (4) are the same as (3); that is, the classification
of an input x relies on the Mahalanobis distance to all class
prototypes. It is not uncommon to normalize yTi Œ£‚àí1 yi to a
scalar value (say unity) for every class, though we constrain
feature vectors to the unit ellipse (or ball, depending on Œ£).

One will find normalized class representations in many applications in biological datasets, image processing applications,
detection-theory, etc., where a signal processing paradigm
places significant emphasis on the relationship between feature dimensions rather than the actual values themselves. For
example, pre-processing in images for computer vision-based
applications often involves DC subtraction and division by
pixel variance.
Under such an assumption, (4) can be written as the dot
product of x and yi :
arg max

< x, yi >

(5)

i‚àà{1,...,C}

This is an important result because the classifier is broken
down to a simple cross-correlation between x and yi , where
yi ‚àà {y1 , y2 ¬∑ ¬∑ ¬∑ yC }, each vector a known prototype of a
given class. The process of matching x with a bank of filters is frequently called categorization by matched filters,
‚àó
where
 the Coverand Hart inequality holds, R ‚â§ R ‚â§
C
R‚àó , where R‚àó is the Bayes error rate.
R‚àó 2 ‚àí C‚àí1

4.1. Between Class Filter Optimization
Clustering for each class will naturally yield similar recurrent
filters among classes that, while representative of a portion
of a single class, are not discriminative between them. For
example, one will often find that a large portion of most images contain the sky. This is true whether or not one wishes
to differentiate between images of, say, mountains or buildings, two completely unrelated concepts that happen to share
a similar feature in the images. Analogously, the discriminating power in ‚Äúsky features‚Äù, which the within feature
optimization will invariably produce, will be low because
P (mountain|sky) and P (buildings|sky) values are small.
Deletion of similar filters is then a logical step, and the
choices of which filters to remove are simply those with high
correlation occurring across a pair of classes. We can define a
threshold tkeep for features that we wish to keep. Let X (r) ‚äÜ
X and Y (r) ‚äÜ Y be the collection of within-class representative features for classes c1 and c2 , respectively, where
xi ‚àà X (r) , yj ‚àà Y (r) . The final set of pair-wise betweenclass filters discriminating c1 and c2 is:




T
max xi yj < tkeep
{(fc1 , fc2 )} = (xi , yi ) :
(6)
yj

3.1. Asymptotic Consistency
The infinity norm regularization in the proposed optimization relies on naturally clustered events, where xi is not
unique within X, suggesting inconsistent (and initializationdependent) Œ≤ estimators. For example, take X(Œæ) = Y + Œæ,
where Œæ is additive noise. If Y contains several instances
of the same vector, then Œ≤ÃÇ can represent X(Œæ) with any yi ,
where Œ≤ÃÇ is the estimated solution. Or, it can represent all of
them in the unlikely event that X(Œæ)T Y = 11T with Œª improperly chosen. This scenario is rare for sufficiently large Œª
since the `1 -norm of `‚àû -norms tends toward a single selector
value as opposed to the 2-norm, seen in Fig. 2.1.
However, there are sufficient conditions for asymptotic
consistency, which may not necessarily satisfy kŒ≤ÃÇ ‚àí Œ≤k2 =
oP (1), but may guarantee properties about the grouping of
features for a given Œª and the total number of clusters C(Œª).
Under our penalization in (1), Œ≤ promotes a unique and consistent grouping, namely that rank(Œ≤ g ) = 1, with Œ≤ g being a
submatrix of Œ≤ for group g. Therefore, the number of clusters
C(Œª) equals rank(Œ≤).

4. APPLICATION CONSIDERATIONS
In order to apply the algorithm to discriminate between
classes (in either one versus all scenario) and at scale, we
can apply simple yet effective common methodologies. Previously, Sec. 2 proposes a solution to create within-class
filters. This section discusses best filters to use between the
classes as well as how to train filters hierarchically.

1981

4.2. Hierarchical Filters
As discussed in [8], hierarchical methods are especially useful
for groupings that may appear different in different situations.
Filter hierarchies address scenarios where groupings reflect
some semantic organization. In [12], image patch-based clustering of objects taken at several angles, times of days, etc.,
may appear different for each instance. Furthermore, mixture hierarchies are useful for complexity reasons because we
have relied on the covariance matrix, where memory can grow
according to M N . Since the proposed algorithm aims to remove redundancy, we prune especially large data sets (M and
N on the order of millions) to a few relevant features to take
advantage of central limit behavior, a property enabling [8] to
automatically segment images without explicitly specifying
boundaries.
Hierarchical training operates over several data subsets
(e.g., images), effectively partitioning the class data. We optimize over each subset, and then between each subset. According to [9], irrelevant features (noise) will occur infrequently
while class features will arise; normalization will asymptotically integrate noise to zero in distribution. The procedure is,
then, to first find Œ≤ in data subsets and between data subset.
After this optimization, the rows of Œ≤ corresponding to the
highest frequency features relate to class structure.
5. RESULTS
Of the large set of features to choose from (e.g. SIFT [13],
SURF, Cosine Transforms, GIST [14]), our classification

leverages uniformly extracted, multi-channel (RGB/YBR)
DCT feature vectors, much like Carneiro et al. [4]. Though it
is an isometric transform (with DCT/pixel `2 -distance equal),
we take advantage of DCT‚Äôs energy compaction property
with the first 45 dimensions while weighting color components higher to improve illumination-invariance. Classification accuracy for individual image patches are shown in
Table 1. A separate application in Table 2, the localization
of images, stresses the multiple instance learning potential of
the proposed algorithm by classifying images into particular
locations. The latent features (which we have conceptually
labeled) underscore another capability that by training for semantic concepts, image segmentation is gained for free. The
segmentation and labeling of a location is visually shown in
Fig. 5. This is further evidenced by the automatic extraction
of faces in Fig. 5.
Table 1. Classification accuracy for synthetic and corel image
datasets [5]. Below are the performances under various initialization
conditions. The metrics are probability values of Correct Detection,
Correct Rejection, False Alarm, and Misses.

Synth

Corel

Methodology
GMM X-Val‚Äôd Init
GMM Under-Init
GMM Over-Init
Best k-means
Group Lasso
LP Estimate
Best GMM (26 inits)
GMM Under-Init
GMM Over-Init
LP Estimate

HC1 vs HC2 Performance (%)
Pdet
Prej
Pf a
Pmiss
97.24
92.51
7.49
2.76
87.84
17.95
82.05
12.16
85.24
86.25
13.75
14.76
89.84
90.49
9.51
10.16
93.17
90.05
9.95
6.83
94.84
92.04
7.96
5.16
87.24
76.76
23.24
12.76
70.36
64.09
35.81
29.64
75.57
65.92
34.18
24.43
87.32
74.51
25.49
12.68

(a) Kendall Square

(c) Indoor Room

classification is the geo-location problem, where a collection of images at various locations are gathered for training. The performance
is based on how well a classifier places the images at the correct locations in the training set. Below is the Multi-class Cross-validation
Confusion Matrix.
Test Set
MIT Kendall
Lubbock, TX
Dubrovnik
Vienna

MIT Kendall
0.930
0.019
0.014
0.036

Training Set Locations
Lubbock, TX
Dubrovnik
0.062
0.028
0.902
0.039
0.024
0.879
0.013
0.057

Vienna
0.083
0.041
0.038
0.838

(d) Room Prototypes

Fig. 2. In the classification of scenes, different prototypes typically
dominate in identifying different portions of a scene. The segmentation seen in the scene is a natural result of correlation and relevancy.
The top scene is classified as the Kendall Square area of Cambridge,
MA, 2,317 low-resolution images trained from Table 2. The bottom scene is an example derived from training data in the Corel data
set [5].

(a) Single image

Table 2. An example application of semantic or concept image

(b) Kendall Prototypes

(b) Class/Cluster #14

(c) Overlaid Class

Fig. 3. Completely unsupervised clustering trained on a single image in Fig. 3(a) of a crowd producing several selected features: including one of faces Fig. 3(b) and Fig. 3(c)
‚Ä¢ An approximation to the LP relaxation solves an optimization problem to obtain representative features.
‚Ä¢ Class prototypes based on their covariance matrix are
sparse and can be tuned with a Œª parameter.
‚Ä¢ Filter hierarchies can be built and similar filters between classes should be removed.

6. CONCLUSIONS

‚Ä¢ Results generalize well to several data sets.

We have proposed a sparse data representation procedure that
can determine prototypes quickly and efficiently. This representation can be used for clustering, classification, and feature
selection with the advantages of fast matched filtering. The
algorithm has several contributions which are enumerated as
follows.

1982

Further research is still warranted in understanding and characterizing our solution, most notably selection of Œª and consistency modeling. Rigor and statistical justification will also
be the subject of extended papers in the future. Finally, as
evident in recent talks by Google and Torralba et. al., classification performance is directly associated with the context

in which is applied, where assessing global properties of the
feature space could boost performance.
7. ACKNOWLEDGEMENTS
We would like to thank Andrew Bolstad at MIT Lincoln Laboratory for all the help, advice, and good ideas he has given
us in regard to convex optimization for sparse regularization
techniques.

[10] A. P. Dempster, N. M. Laird, and D. B. Rubin, ‚ÄúMaximum likelihood from incomplete data via the em algorithm,‚Äù Journal of the Royal Statistical Society, Series
B, vol. 39, no. 1, pp. 1‚Äì38, 1977.
[11] Han Liu and Jian Zhang, ‚ÄúEstimation consistency of the
group lasso and its applications,‚Äù Journal of Machine
Learning Research - Proceedings Track, vol. 5, pp. 376‚Äì
383, 2009.
[12] Nuno Vasconcelos, ‚ÄúImage indexing with mixture hierarchies,‚Äù Proceedings of IEEE Conference on Computer
Vision and Pattern Recognition, June 2001.

8. REFERENCES
[1] B. C. Russell, A. Torralba, K. P. Murphy, and W. T. Freeman, ‚ÄúLabelme: a database and web-based tool for image annotation,‚Äù International Journal of Computer Vision, vol. 77, no. 1-3, pp. 157‚Äì173, May 2008.
[2] Paul Viola and Michael Jones, ‚ÄúRapid object detection
using a boosted cascade of simple features,‚Äù 2001, pp.
511‚Äì518.
[3] P. F. Felzenszwalb, R. B. Girshick, and D. McAllester,
‚ÄúDiscriminatively trained deformable part models,
release 4,‚Äù http://people.cs.uchicago.edu/ pff/latentrelease4/.
[4] G. Carneiro, A. Chan, P. Moreno, and N. Vasconcelos,
‚ÄúSupervised learning of semantic classes for image annotation and retrieval,‚Äù IEEE Transactions on Pattern
Analysis and Machine Intelligence, vol. 29, no. 9, pp.
394‚Äì410, March 2007.
[5] Pinar Duygulu, Kobus Barnard, Nando de Freitas, and
David Forsyth, ‚ÄúObject recognition as machine translation: Learning a lexicon for a fixed image vocabulary,‚Äù
Seventh European Conference on Computer Vision, vol.
IV, pp. 97‚Äì112, 2002.
[6] K. Barnard and D. Forsyth, ‚ÄúLearning the semantics of
words and pictures,‚Äù Proceedings of the International
Conference on Computer Vision, vol. 2, pp. 408‚Äì415,
2001.
[7] D. Blei and M. Jordan, ‚ÄúModeling annotated data,‚Äù Proceedings of the ACM SIGIR Conference on Research
and Development in Information Retrieval, 2003.
[8] Nuno Vasconcelos and Andrew Lippman, ‚ÄúLearning
mixture hierarchies,‚Äù in Neural Information Processing
Systems, Denver, Colorado, 1998, vol. 11.
[9] N. Rasiwasia and N. Vasconcelos, ‚ÄúHolistic context
modeling using semantic co-occurrences,‚Äù In IEEE
Conference on Computer Vision and Pattern Recognition (CVPR), June 2009.

1983

[13] David G. Lowe, ‚ÄúDistinctive image features from scaleinvariant keypoints,‚Äù International Journal of Computer
Vision, vol. 60, pp. 91‚Äì110, 2004.
[14] Aude Oliva and Antonio Torralba, ‚ÄúModeling the shape
of the scene: a holistic representation of the spatial envelope,‚Äù International Journal of Computer Vision, vol.
42, no. 3, pp. 145‚Äì175, 2001.

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 63, NO. 16, AUGUST 15, 2015

4191

A Spectral Framework for Anomalous
Subgraph Detection
Benjamin A. Miller, Senior Member, IEEE, Michelle S. Beard, Patrick J. Wolfe, Senior Member, IEEE, and
Nadya T. Bliss, Senior Member, IEEE

Abstract‚ÄîA wide variety of application domains is concerned
with data consisting of entities and their relationships or connections, formally represented as graphs. Within these diverse application areas, a common problem of interest is the detection of a
subset of entities whose connectivity is anomalous with respect to
the rest of the data. While the detection of such anomalous subgraphs has received a substantial amount of attention, no application-agnostic framework exists for analysis of signal detectability
in graph-based data. In this paper, we describe a framework that
enables such analysis using the principal eigenspace of a graph‚Äôs
residuals matrix, commonly called the modularity matrix in community detection. Leveraging this analytical tool, we show that the
framework has a natural power metric in the spectral norm of the
anomalous subgraph‚Äôs adjacency matrix (signal power) and of the
background graph‚Äôs residuals matrix (noise power). We propose
several algorithms based on spectral properties of the residuals
matrix, with more computationally expensive techniques providing
greater detection power. Detection and identiÔ¨Åcation performance
are presented for a number of signal and noise models, including
clusters and bipartite foregrounds embedded into simple random
backgrounds, as well as graphs with community structure and realistic degree distributions. The trends observed verify intuition
gleaned from other signal processing areas, such as greater detection power when the signal is embedded within a less active portion of the background. We demonstrate the utility of the proposed
techniques in detecting small, highly anomalous subgraphs in real
graphs derived from Internet trafÔ¨Åc and product co-purchases.
Index Terms‚ÄîGraph theory, signal detection theory, spectral
analysis, residuals analysis, principal components analysis.

I. INTRODUCTION

I

N numerous applications, the data of interest consist of entities and the relationships between them. In social network
analysis, for example, the data are connections between individManuscript received April 23, 2014; revised September 10, 2014 and March
19, 2015; accepted May 01, 2015. Date of publication May 26, 2015; date of
current version July 02, 2015. The associate editor coordinating the review
of this manuscript and approving it for publication was Dr. Pengfei Xia. This
work is sponsored by the Assistant Secretary of Defense for Research & Engineeringunder Air Force Contract FA8721-05-C-0002. Opinions, interpretations,
conclusions and recommendations are those of the authors and are not necessarily endorsed by the United States Government.
B. A. Miller is with Lincoln Laboratory, Massachusetts Institute of Technology, Lexington, MA 02420 USA (e-mail: bamiller@ll.mit.edu).
M. S. Beard is with Charles Stark Draper Laboratory, Cambridge, MA 02139
USA (e-mail: mbeard@draper.com).
P. J. Wolfe is with the Department of Statistical Science, University College
London, London WC1E 6BT U.K. (e-mail: p.wolfe@ucl.ac.uk).
N. T. Bliss is with Arizona State University, Tempe, AZ 85287 USA (e-mail:
nadya.bliss@asu.edu).
Color versions of one or more of the Ô¨Ågures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object IdentiÔ¨Åer 10.1109/TSP.2015.2437841

uals, such as who knows whom personally, who is in the same
organization, or who is connected on a social networking website. In computer networks, we are often interested in which
computers communicate with one another. In the natural sciences, we may want to know which chemicals interact in a reaction. Across these varied domains, data regarding connections,
relationships, and interactions between discrete entities enhance
situational awareness and diversify analysis by incorporating
additional contextual information.
When working with relational data, it is common to formally
is a
represent the relationships as a graph. A graph
pair of sets: a set of vertices, , comprising the entities, and a set
of edges, , denoting relationships between them. Graph theory
provides an abstract mathematical object that has been applied
in all of the above contexts. Indeed, graphs have been used to
model protein interactions [1] and to represent communication
between computers [2]. Graphs‚Äîcommonly called networks in
practice‚Äîare used extensively in social network analysis, with
many graph algorithms focused on detection of communities
[3], [4] and inÔ¨Çuential Ô¨Ågures [5].
As a data structure, graphs have long been utilized by signal
processing practitioners. Analysis of graphs derived from radio
frequency or image data is common, as a graph structure can
help classify similar measurements (see, e.g., [6]). Recent research has also deÔ¨Åned traditional digital signal processing kernels‚Äîsuch as Ô¨Åltering and Fourier transforms‚Äîfor signals that
propagate along edges in a graph [7], [8]. When the graph comprises the data itself, rather than a means of storage, signiÔ¨Åcant
complications arise. Graphs are discrete, combinatorial structures, and, thus, they lack the convenient mathematical context of Euclidean vector spaces. The ability to perform linear
transformations and the analytical tractability of working with
Gaussian noise are not available in general when working with
relational data. Deriving an optimal detector for a small signal
subgraph buried within a large network, then, becomes potentially intractable, as it may require the solution to an NP-hard
problem.
Despite these complications, it is desirable to understand
notions of detectability of small subgraphs embedded within
a large background. The ability to detect small signals in
these contexts would be useful in many domains, from the
detection of malicious trafÔ¨Åc in a computer network to the
discovery of threatening activity in a social network. Recent
work in this area has considered subgraph detection from a
variety of perspectives. Work has been done on detection of
speciÔ¨Åc target subgraphs in random backgrounds [9], with
special attention paid in the computer science and statistics

1053-587X ¬© 2015 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

4192

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 63, NO. 16, AUGUST 15, 2015

communities to planted cliques [10], [11] and planted clusters
[12], [13]. Other work assumes common substructures over
the graph, and detects anomalies based on deviations from the
‚Äúnormative pattern‚Äù via methods such as minimum description
length [14] or analysis of the graph Laplacian [15]. Techniques
such as threat propagation [16], [17] and vertex nomination
[18] consider a cue vertex as a knowledge prior, giving an
initial indication of which vertices are of interest, the objective
then being to Ô¨Ånd the remainder of the subgraph. Community
detection in graphs is a widely studied related problem [19],
where the communities in the graph are sometimes cast as
deviations from a null hypothesis in which the graph has no
community structure [20].
The objective of the present contribution is to develop a
broadly applicable detection framework for graph-based data.
To apply in these varied domains, this framework should be independent of the speciÔ¨Åc application. We focus speciÔ¨Åcally on
the uncued anomalous subgraph detection problem, where the
goal is to detect the presence of a subgraph that is a statistical
outlier without a ‚Äútip‚Äù vertex provided as a cue. As graphs of
interest are often extremely large, the framework should have
favorable scaling properties as the number of vertices and edges
grows. To gain insight into properties that inÔ¨Çuence subgraph
detectability, the framework will ideally have a natural metric
for signal and noise power to enable discussion of quantities
like signal-to-noise ratio that are intrinsic to signal processing
applications.
In this paper, we present a spectral framework to address the
uncued subgraph detection problem. This framework is based
on a regression-style analysis of residuals in which an observed
random graph is compared to its expected value to Ô¨Ånd outliers.
We analyze the graph in the space of the principal eigenvectors
of its residuals matrix, which offers two advantages: it allows us
to use results from spectral graph theory to elucidate the notion
of subgraph detectability, and it works within a linear algebraic
framework with which many signal processing researchers are
familiar. Within this framework, the spectral norm provides a
good metric for signal and noise power, as we demonstrate analytically and empirically. This framework also enables the development of algorithms that work in a low-dimensional space
to detect small anomalies, several of which are discussed in this
paper.
The remainder of this paper is organized as follows. In
Section II, we formally deÔ¨Åne the subgraph detection problem.
Section III provides a brief summary of related work on subgraph detection and graph residuals analysis. Section IV details
our proposed subgraph detection framework. In Section V, we
outline several algorithms for anomaly detection within the
framework. Section VI presents detection results for several
simulated datasets, and in Section VII we demonstrate these
techniques on real datasets. Finally, in Section VIII, we summarize and discuss open problems and ongoing work.
II. PROBLEM MODEL

of is a graph in which
and
, where the Cartesian product
is the set of
all possible edges in a graph with vertex set . In this paper, we
consider graphs whose edges are unweighted and undirected.
We will allow the possibility of self-loops, meaning an edge
may connect a vertex to itself. Since edges have no weight,
two graphs will be combined via their union. The union of two
and
, is deÔ¨Åned as
graphs,
.
Working in a spectral framework, we will make use of matrix
representations for graphs. The adjacency matrix
of
is a binary
matrix. Each row and column is associated
with a vertex in . This implies an arbitrary ordering of the vertices with integers from 1 to , and we will denote the th vertex
. Then
is 1 if there is an edge connecting and , and is
be the adjacency matrix
0 otherwise. Similarly, let
for the signal subgraph. Since we consider undirected graphs,
and
are symmetric. Matrix norms will also be used in the
discussion of signal and noise power. Unless otherwise noted,
the matrix norm will be the spectral norm, i.e., the induced
norm,
(1)
which is equivalent to the absolute value of the largest-magnitude eigenvalue of the matrix.
Our framework is focused on detection of signals within a
random background. The analysis presented in this paper is
based on the assumption of Bernoulli random graphs, where
and
is a Bernoulli
the probability of an edge between
random variable with expected value
. Note that the edge
probabilities may be different for all pairs of vertices. Since
the presence of each edge is a Bernoulli random variable, the
. We refer to as
expected value of is given by
the probability matrix of the graph.
Another important notion when dealing with graphs is degree.
A vertex‚Äôs degree is the number of edges adjacent to the vertex.
The observed degree of vertex will be denoted , and its expected degree is denoted
. Note that
and
.1 The vectors of the observed and expected
degrees will be denoted and , respectively. The volume of
, is the sum of the degrees over all vertices.
the graph,
B. The Subgraph Detection Problem
In some cases, the observed graph will consist of only typical background activity. This is the ‚Äúnoise only‚Äù scenario. In
exhibits typical behavior, but a small
other cases, most of
subgraph has an anomalous topology. This is the ‚Äúsignal-plusnoise‚Äù scenario. In this case, the noise graph, denoted
, and the signal subgraph,
, are combined via union.
The objective, given the observation , is to discriminate between the two scenarios. Formally, we want to resolve the following binary hypothesis test:

A. Definitions and Notation
In the subgraph detection problem, the observation is a graph
. We will denote the sizes of the vertex and edge
sets as
and
, respectively. A subgraph

(2)
1Using

this convention, a self-loop only increases a vertex‚Äôs degree by 1.

MILLER et al.: A SPECTRAL FRAMEWORK FOR ANOMALOUS SUBGRAPH DETECTION

Thus, we have the classical signal detection problem: under
the null hypothesis
, the observation is purely noise, while
, a signal is also present.
under the alternative hypothesis
Here
and
are both random graphs, with
drawn from
the noise distribution and
drawn from the signal distribution. We will only consider cases in which the vertex set of the
signal subgraph is a subset of the vertices in the background,
.
i.e.,
III. RELATED WORK
While there are many Ô¨Çavors of subgraph detection research,
not all of them work under the same assumptions as in this paper.
For example, we consider a variety of noise models, which may
not have the ‚Äúnormative pattern‚Äù required to use techniques
based on common subgraphs [14], [15]. Research into anomaly
detection in dynamic graphs by Priebe et al. [21] uses the history
of a node‚Äôs neighborhood to detect anomalous behavior, but this
would not apply in the case of static graphs, which is the focus of
this work. As our interest is in uncued techniques, we operate in
a different context from the work in [16]‚Äì[18]. These methods
are complementary to the techniques outlined in this paper, as
a set of outlier vertices could be used to seed a cued algorithm
and do further exploration.
Previous work has considered optimal detection in the same
context we consider in this paper, though in a restricted setting.
In [9], the authors consider the detection of a speciÔ¨Åc foreground
embedded (via union) into a large graph in which each possible edge occurs with equal probability (i.e., the random graph
model of Erd≈ës and R√©nyi). In this setting, the likelihood ratio
can be written in closed form, as demonstrated by the following
theorem.
Theorem 1 (Mifflin et al. [9]): Let denote the random graph
where each possible edge occurs with equal probability , and
denote the target graph. The likelihood ratio of an oblet
served graph is
(3)
Here
denotes the number of occurrences of
in the
graph. The applicability of this result, therefore, requires a
tractable way to count all subgraphs of the observation that
are isomorphic with the target. This is NP-hard in general [22],
although there may be feasible methods to accomplish this for
certain targets within sparse backgrounds.
While the previous example requires a complicated procedure, detection of random subgraphs embedded into random
backgrounds may be an even harder problem. Take, for example, the detection problem where the background and foreground are both Erd≈ës-R√©nyi, i.e., when the null and alternative
hypotheses are given by

(4)
In this situation, we can derive an optimal detection statistic.

4193

, let
be a
Theorem 2: For an observed graph
subset of of size
, and
be the set of all edges
existing between the vertices in . The likelihood ratio for resolving the hypothesis test in (4) is given by
(5)
where
.
A proof of Theorem 2 is provided in Appendix A. Even in
this relatively simple scenario, computing the likelihood ratio
-vertex induced
in (5) requires, at least, knowing how many
subgraphs contain each possible number of edges. In [12], it is
shown that some computable tests asymptotically achieve the
information-theoretic bound for dense backgrounds, but there
are no known polynomial-time algorithms that achieve the
bound in a sparse graph [13]. For more complicated models,
calculating the optimal detection statistic is likely to be even
more difÔ¨Åcult.
The subgraph detection framework presented in this paper is
based on graph residuals analysis. The residuals of a random
graph are the difference between the observed graph and its expected value.2 For a random graph , we analyze its residuals
matrix
(6)
In the area of community detection, a widely used quantity to
evaluate the quality of separation of a graph into communities
is modularity, deÔ¨Åned in [20]. The modularity of a partition
is deÔ¨Åned as
(7)
are disjoint subsets of covering the entire set,
is
where
the proportion of edges entirely within , and is the proportion of edge connections in , i.e.,
(8)
with
denoting half the number of edges between
and
for
(half to prevent from counting the edge in both
and
). Note that is the expected proportion of edges within if
the edges were randomly rewired (i.e., the degree of each vertex
is preserved, but edges are cut and reconnected at random). Indeed, if the edge proportions are the only thing maintained in the
rewiring, the fraction of edges from any community that conwill be . Thus, the proportion of the
nect to a vertex in
to
will be
. Taken as an analysis
total edges from
of deviations from an expected topology, modularity is a residuals-based quantity.
In the community detection literature, numerous algorithms
exist to maximize for a given number of communities. In [3],
an algorithm is proposed by casting modularity maximization as
optimization of a vector with respect to a matrix. The modularity
2This is distinct, it should be noted, from the notion of residual networks when
computing network Ô¨Çow [22].

4194

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 63, NO. 16, AUGUST 15, 2015

matrix is given as the observed minus the expected adjacency
matrices, i.e., a matrix of the form in (6). To divide the graph into
two partitions in which modularity is maximized, we can solve
(9)
and declare the vertices corresponding to the positive entries of
to be in one community, with the negative entries indicating
the other. This technique will optimize for a partition into two
communities. Since this is a hard problem, it is suggested that
the principal eigenvector of
(10)
is computed‚Äîthereby relaxing the problem into the real numbers‚Äîwith the same strategy of discriminating based on the
sign of eigenvector components used to divide the graph into
communities.
This is an example of a community detection algorithm based
on spectral properties of a graph, which have inspired a signiÔ¨Åcant amount of work in the detection of communities [3],
[23]‚Äì[25] and global anomalies [2], [26], [27]. In this paper, we
leverage these same properties within a novel framework for
detection of small subgraphs whose behavior is distinct from
background activity.
IV. DETECTION FRAMEWORK
A. Framework Overview
The subgraph detection framework we propose is based on
the analysis of graph residuals, as expressed by (6). We may be
, or it may be estimated from the observed data. This
given
is similar to analysis of variance in linear regression: We compare the observed data to its expectation, and if the deviations
from the expected value are not consistent with variations due
to noise, then this may indicate the presence of a signal (in this
case an anomalous subgraph).
To reduce the dimensionality of the problem, this framework
deals with a graph‚Äôs spectral properties. Using the principal
components of the residuals matrix, we can consider a graph in
the linear subspace in which its residuals are largest. For some
established models, there is also theory regarding the eigenvalues and eigenvectors of these matrices [28]. This technique
is used in community detection, and is similar to models in
which each vertex has a position in a latent Euclidean space
(see, e.g., [29]). The presence of certain anomalous subgraphs
will alter the projection of a graph into this Euclidean residuals
space. Working within this space, we can compute test statistics
and, from these, resolve the hypothesis test (2). While these will
not be optimal detection statistics as in Theorems 1 and 2, this
framework can be applied to a wide variety of random graph
models, is computationally tractable, and, as we demonstrate in
subsequent sections, is quite useful for resolving the subgraph
detection problem in a variety of scenarios.
We use the modularity matrix from (9) as our baseline
residuals model. This has several advantages. First, the ‚Äúgiven
expected degree‚Äù model has been well-studied, and we know
properties of its eigenvalues and eigenvectors [30]. Second, the
model‚Äôs expected value term is low-rank, which allows easy

without computing a
computation of the eigenvectors of
dense
matrix (as noted in [3] and described in [31]).
This makes the model computationally tractable for large
can be
graphs where algorithms more expensive than
prohibitive. This model also has a simple Ô¨Åtting procedure. The
observed degree is, in fact, the maximum likelihood estimate
for the expected degree in the version of this model where each
possible edge is a Poisson random variable [32]. For small
edge probabilities, this is a good approximation for Bernoulli
random variables. Finally, this model has demonstrated utility
for intercommunity behavior; i.e., the probability of connections between vertices in different communities seems to follow
such a model (the reason that observed degree was added as a
covariate in [33]).
B. Power Metrics
As mentioned previously, one important aspect of a signal
processing framework is a metric for signal and noise power.
This provides a quantity that enables an intuitive assessment of
the detectability of a signal in a given background. Again, vector
signals with Gaussian noise provide an intuitive metric based on
vector norms, while such quantities are less clear in the context
of random graphs.
There are several intuitive quantities that could be used for
signal or noise power in the context of random graphs. One natural choice would be number of edges, or perhaps average degree. It seems intuitive that a signal graph with a large number
of edges would be easier to detect, and that greater variance in
the number of edges in the background would make this more
difÔ¨Åcult. A related linear algebraic quantity would be the Frobenius norm of the residuals matrix, i.e., the sum of the squared
residuals over all ordered pairs of vertices. This would consider
each edge probability separately, emphasizing the presence of
less-likely edges.
These metrics, however, have a few shortcomings. In both
cases, the signal power measurement will be exactly the same
for any subgraph with the same number of edges. Consider two
different trees: a path, in which each edge can be traversed while
visiting each vertex exactly once; and a star, where one vertex
edges and a
is connected to all others. Both will have
Frobenius norm of
. The star, however, is much more
concentrated on one vertex, and this will cause it to stand out
more in the eigenspace (it is also much less likely to occur by
chance if edges are randomly placed). The power metric we use
should provide an indication of a subgraph‚Äôs likelihood to stand
apart from the background in the eigenspace, since this is the
space in which we consider the data.
Working within a spectral framework, the spectral norm deÔ¨Åned in (1) provides a natural power metric. Using
as a metric for noise power and
as a metric for signal
power, we can determine the detectability of a subgraph in the
principal eigenspace. To see this, we Ô¨Årst deÔ¨Åne a new matrix,
, which is the adjacency matrix of
,
i.e., the edges of the anomaly that do not appear in the backis 1, then
ground. For deterministic foreground graphs, if
is a random variable whose value is 1 with probability
and 0 with probability . For a random Bernoulli foreground,
if
, then
is 1 with probability
. Thus,

MILLER et al.: A SPECTRAL FRAMEWORK FOR ANOMALOUS SUBGRAPH DETECTION

4195

when the subgraph is embedded within vertices where the interaction level is low,
. For convenience, we will
also denote a partition of the residuals matrix as
(11)
where the rows and columns have been permuted so that the
subgraph vertices are those with the smallest indices. The subis the background residuals within the subgraph vermatrix
tices,
is the residuals occurring between the subgraph and
the rest of the graph, and
includes only the residuals within
the complement of the subgraph vertices.
If the spectral norm of the signal subgraph is sufÔ¨Åciently large
with respect to the background power, the subgraph will dominate the principal eigenvector of the residuals matrix. This is
captured in the following theorem, a proof of which is provided
in Appendix B.
Theorem 3: Let be the residuals matrix of a graph drawn
from an arbitrary Bernoulli graph process, and be the adjacency matrix of the subgraph that does not include edges in the
background graph. If is the unit eigenvector associated with
the largest positive eigenvalue of
(the residuals matrix
after embedding), then assuming
, the
components of associated with only the signal vertices, denoted , is bounded below as
, where

(12)
Consider the implication of Theorem 3 for a Ô¨Åxed background, when embedding on a Ô¨Åxed subset of vertices. The
theorem states that as the difference between the signal power
and the power of the noise among the non-signal vertices
becomes much larger than the noise power
, the principal
involving subgraph vertices
eigenvector will become concentrated on the foreground vertices. A few aspects of this theorem conÔ¨Årm intuition from
other signal processing areas. First, if there is signiÔ¨Åcant
may be
noise activity within the subgraph vertices, then
, and
may be relatively
signiÔ¨Åcantly smaller than
large. This means that a signal placed in strong noise will be
difÔ¨Åcult to detect, which is always the case in detection problems. Also, the bound in the theorem shows that if a relatively
strong subgraph is embedded where there is typically very
little activity, and where there is relatively little interaction
and
),
with the remainder of the graph (i.e., small
the subgraph will be much easier to detect. Put in traditional
signal processing language, the signal will be much easier to
detect when it is less correlated with the noise. Working within
this framework, we see the same properties of the interaction
between signal and noise that affect detectability in domains
like radar and communications.
An empirical example is provided in Fig. 1. In this case, a
4096-vertex Erd≈ës-R√©nyi graph (see Section VI.A1) is generated, with a 15-vertex subgraph with 90% edge probability em, where
is the expresbedded. The horizontal axis is
sion in (35) in Appendix B. The bound holds for all cases considered, and the empirical results often are an order of magnitude

Fig. 1. Empirical comparison to bound in Theorem 3. The bound holds for each
case in this scenario with a 4096-vertex random background and a 15-vertex
.
dense signal subgraph, though it is only tight for cases where

Fig. 2. Distributions of vertex components in principal eigenvectors: a histogram of components in the Ô¨Årst eigenvector (left), with a comparison to a
Laplace distribution, and a scatterplot (right) in the principal two-dimensional
subspace, demonstrating its radial symmetry.

below the maximum for both the higher and lower edge probabilities (
and
, respectively). Only
when a case is considered where there is no background connectivity within the subgraph vertices is the bound approached
more closely.
V. DETECTION ALGORITHMS
For relatively large subgraph anomalies, a simple ‚Äúenergy detector‚Äù based on the spectral norm of the residuals matrix will
provide good detection performance. It is desirable, however,
to be able to detect much smaller subgraphs, which may not
stand out in the principal eigenvector. A few techniques have
been developed within this framework to detect subtler anomalies [34]‚Äì[37], which we outline in this section.
A. Chi-Squared Statistic in Principal Components
The Ô¨Årst algorithm is based on the symmetry of the projecinto its two principal components. This will enable
tion of
the detection of subgraphs that do not stand out in the Ô¨Årst
eigenvector. We have empirically observed for several random
graph models that, when projecting the residuals into their principal two components, the result is rather radially symmetric.
For sparse graphs, the entries in the principal eigenvectors resemble a Laplace distribution, as shown on the left in Fig. 2,
which is consistent with behavior observed in sparse Erd≈ësR√©nyi graphs. The right-hand plot in Fig. 2 demonstrates the
symmetry of the residuals in the top two eigenvectors.

4196

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 63, NO. 16, AUGUST 15, 2015

When an anomaly is embedded within the graph, as previously discussed, the subgraph vertices will stand apart from the
background. Therefore, we compute a statistic that is based on
symmetry in this space to detect the presence of an anomaly.
The detection statistic is a chi-squared statistic based on a 2
2 contingency table, where the table contains the number of
vertices projected into each quadrant of the two-dimensional
space. (That is, the number of rows of
, where and
are (column) eigenvectors of , fall into each quadrant.) This
yields a 2 2 matrix
of the observed numbers of
points in each section. From the observation, we compute the
expected number of points under the assumption of indepen, where
dence,

Fig. 3. An example of using eigenvector
norms for subgraph detection.
When a small, dense subgraph is embedded into a background with a skewed
norm of one of the eigenvectors of the residuals madegree distribution, the
trix becomes much smaller than usual, as shown on the left. Under the null hypothesis, the largest negative deviation from the mean will resemble a Gumbel
distribution, plotted on the right.

(13)
The chi-squared statistic is then calculated as
(14)
and, to favor radial symmetry, we maximize the statistic over
rotation in the plane, computing

compute the eigenvectors corresponding to the largest eigenvalues. By measuring cases with no embedding present, we obtain the mean and standard deviation for the norm of the
th eigenvector. For each of the eigenvectors
, we
subtract the mean and normalize by the standard deviation. The
smallest (i.e., most negative) value is then used as a test statistic,
since we are interested in cases where the norm is small. The test
statistic is given by

(15)
is used to detect an anomalous subgraph.
The statistic
When the spectral norm is a reliable detection statistic,
thresholding along the principal eigenvalue is often an effective
method to identify the vertices that are exhibiting anomalous
behavior. Working in multiple dimensions, while it enables the
detection of smaller subgraphs, makes the process of identiÔ¨Åcation more complicated. In this setting, we use a method based
on -means clustering to identify the subgraph vertices. Within
the two-dimensional space, we compute a small number of
clusters and declare the smallest cluster with at least a minimum
number of vertices to be the signal subgraph.
B. Eigenvector

Norms

It is also desirable to detect signal subgraphs that do not stand
out in the principal two components of the residuals matrix, and
extending the algorithm of Section V.A to an arbitrary number
of dimensions may not be feasible. One method to detect such
anomalies relies on the subgraphs being separable in the space
of a single eigenvector. As mentioned previously, the entries
in the eigenvectors of the background alone resemble numbers
drawn from a Laplace distribution. Thus, if a subgraph were to
stand out in a single eigenvector, that eigenvector will have a
norm than for the background alone.
substantially smaller
norm of a vector
, is much smaller
The
when it is concentrated on a small subset of entries, provided
that it is unit-normalized in an
sense. For this reason, the
norm serves as a proxy for sparsity in applications such as
compressed sensing [38].
The following algorithm enables detection when an eigenvector is concentrated on the vertices of the subgraph. This will
occur when, for example, a dense subgraph is embedded on relatively low degree vertices, as discussed in Appendix C. We

(16)
An example demonstrating this method is provided in Fig. 3.
The example uses a 4096-vertex graph with a skewed degree
distribution (using the CL model described in Section VI.A2),
with a 15-vertex subgraph with average degree 10.5 randomly
embedded into the background. The analysis is run on the 100
eigenvectors associated with the largest positive eigenvalues.
norms of most eigenvectors in the resulting maWhile the
trix fall within three standard deviations of the mean for their
norm of the 6th eigenvector is over 10 standard
index, the
deviations below the mean, which is extremely unlikely to occur
under the null hypothesis. Under the null hypothesis, the test
statistic (16) will resemble a Gumbel distribution (commonly
used to model extreme values), as shown in the plot on the right.
When an embedding occurs that creates a deviation as large as
that in the left-hand plot, it will take on a value much larger than
the maximum under normal circumstances.
The occurrence of tightly connected subgraphs highly aligned
with eigenvectors was documented independently in [39], and
a similar anomaly detection method using eigenvector kurtosis
in [40]. Here, we use this phenomenon to Ô¨Ånd subgraphs whose
internal connectivity is much larger than the expectation, given
the background model. When an anomaly is detected according
to (16), the corresponding eigenvector is thresholded to determine the subgraph vertices.
C. Sparse Principal Component Analysis
norms enables the detection
While analysis of eigenvector
of some subgraphs that do not separate in the principal components of the residuals space, this technique has some shortcomings. In particular, as consecutive eigenvalues get closer
together, the direction of the eigenvectors becomes unstable.
Therefore, we cannot rely on the test statistic being sufÔ¨Åciently

MILLER et al.: A SPECTRAL FRAMEWORK FOR ANOMALOUS SUBGRAPH DETECTION

changed because an eigenvector points in the direction of the
subgraph.
There is, however, a similar technique that enables the detection of small subgraphs with large residuals. Rather than Ô¨Årst
computing the eigenvectors of the residuals matrix and then
norm, we can Ô¨Ånd a
Ô¨Ånding an eigenvector with a small
vector that is nearly an eigenvector whose
norm is constrained. This is a technique known as sparse principal component analysis (sparse PCA) [41]. This method has been used
in the statistics literature to Ô¨Ånd high variance in the space of a
limited number of variables. We utilize it here for a similar goal:
to Ô¨Ånd large residuals in the space of a small number of vertices.
The problem is formulated as follows. The goal is to Ô¨Ånd a
vector that is projected substantially onto itself by the residuals
matrix, but with few nonzero components. Put formally, the objective is to solve

(17)
where
denotes the
quasi-norm (the number of nonzero
components in a vector). This, however, is an integer programming problem and is NP-hard. We therefore use a relaxation
constraint, recast as a penalized optimization:
with an
(18)
This problem is still not in an easily solvable form, due to the
quadratic equality constraint. We use an additional relaxation,
following the method of [41], to achieve a semideÔ¨Ånite program
that can be solved using well-documented techniques:

(19)
where
denotes the matrix trace,
replaces each entry
is the set of posiin a matrix with its absolute value, and
tive semideÔ¨Ånite matrices in
. The principal eigenvector
of , denoted , is then returned (and should be sparse, given
the constraints). The subgraph detection statistic is
. If no
small subgraph has sufÔ¨Åciently large residuals, the vector should
norm. For
be relatively diffuse and have a relatively large
vertex identiÔ¨Åcation, the sparse principal component is thresholded, and the vertices corresponding to the components of the
vector greater than the threshold are declared to be part of the
anomalous subgraph.
One drawback of this technique is its computational complexity. As mentioned in the introduction, one goal of this work
is to develop techniques that scale to very large graphs. The
algorithms described in Sections V.A and V.B rely on a partial eigendecomposition. Using the Lanczos method for computing eigenvectors and eigenvalues of a matrix, and leveraging sparseness of the graphs, this requires a running time of
, where is the number of restarts
in the algorithm [42]. Thus, if the number of eigenvectors to
compute is Ô¨Åxed, this algorithm scales linearly in the number of
edges in its per-restart running time. Sparse PCA, as described
, where
in [41], has a running time that is
controls accuracy of the solution. This implies that sparse PCA

4197

will not scale to extremely large datasets without additional optimization, which is a problem for future work. We present results using this technique to demonstrate the feasibility of detecting exceptionally small anomalies using the framework outlined in this paper.
VI. SIMULATION RESULTS
A. Noise Models
There are many models for random graphs, with varying
degrees of complexity. In this section, we outline three
random models that will be used for background noise in our
experiments.
1) Erd≈ës-R√©nyi (ER) Random Graphs: The simplest random
graph model was proposed by Erd≈ës and R√©nyi in [43]. In this
, an edge
model, given a vertex set and a number
with probability . In
occurs between any two vertices in
for all and . This model is subsumed
matrix form,
by the model for a random graph with a given expected degree
sequence assumed by (9), where, in this case, all vertices have
the same expected degree.
2) Chung-Lu (CL) Random Graphs: The ‚Äúgiven expected
degree‚Äù model has been studied extensively by Chung and Lu
[30]. Similarly to the dynamic preferential attachment model of
[44], in this model, the probability of two nodes sharing a connection increases with their popularity. Formally, each vertex
is given an expected degree , and the probability of vertices
and
sharing an edge is given by
,
yielding a rank-1 probability matrix
(20)
Using the observed degree as the expected degree‚Äîshown
to be an approximately asymptotically unbiased estimator in
[45]‚Äîthe standard formulation of the modularity matrix (10)
perfectly Ô¨Åts this model for background behavior.
3) R-MAT Stochastic Kronecker Graphs: To include a
slightly more complicated model, we also consider the Recursive Matrix (R-MAT) stochastic Kronecker graph [46]. In this
model, a base probability matrix
(21)
is given, where
and are nonnegative values that sum
to 1, and edge probabilities are deÔ¨Åned by the -fold Kronecker
product of , denoted
. This results in
matrices with vertices. The graph is generated by an iterative
method where one edge is added at each iteration with probabilities deÔ¨Åned by . If the total number of iterations is , the
edge probabilities are given by
(22)
If the base probability matrix has rank 1, this generator will produce graphs with a similar structure to the CL model. When this
is not the case, however, this model creates graphs with mild
community structure, as shown in [46], thereby presenting a
more challenging noisy background for our subgraph detection
framework.

4198

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 63, NO. 16, AUGUST 15, 2015

C. Monte Carlo Simulations

Fig. 4. Sparsity patterns for background graphs: an R-MAT graph (left), a
Chung-Lu graph (center), and an Erd≈ës-R√©nyi graph (right).

These three models represent varying degrees of complexity
for the detection framework. The ER model is overspeciÔ¨Åed by
the given expected degree model used in the modularity matrix, the CL model matches the formula exactly, and the R-MAT
model is mismatched due to its mild community structure. In
the simulations in Section VI.C, the R-MAT graphs are generated using a base probability matrix with
, and
, and the algorithm is run for
iterations, resulting in an average degree of approximately 12. The
graph is unweighted and its directionality is removed via the
‚Äúclip-and-Ô¨Çip‚Äù procedure as in [46], i.e., the edges below the
main diagonal in the adjacency matrix are removed, and those
above the main diagonal are made undirected. For CL backgrounds, the expected degree sequence is deÔ¨Åned by the edge
,
probabilities of the R-MAT background, i.e.,
where
is deÔ¨Åned in (22). The ER backgrounds use an edge
probability that yields an average degree the same as the more
complicated models.
Example sparsity patterns of the adjacency matrices, each
with 1024 vertices, are shown in Fig. 4. Note the moderate community structure in the R-MAT graph. While the CL graph has
vertices of varying degree, it does not have the same structure
of the R-MAT. One particularly visible difference is the lack of
connections between low-degree vertices and high-degree vertices in the R-MAT graph, seen in the upper-right and lower-left
corners of the matrix. Both of these graphs contain more variation than the ER graph, where the uniform randomness can be
seen in its sparsity pattern.
B. Signal Subgraph
Two random graph models are used for the anomalous signal
subgraph. In one case, an ER graph with probability parameter
is generated and combined with randomly selected vertices
from the background. Here, the expected adjacency matrix is an
matrix where every entry is , and thus has spectral
norm
. The second subgraph we consider is a random bipartite graph, where the vertex set is split into two subsets and
no edge can occur between vertices in the same subset. Letting
and
be the numbers of vertices in each subset, there are
possible edges between the two vertex subsets, and, as
in the ER subgraph case, each of these possible edges is generated with equal probability . For the bipartite subgraph, the
expected adjacency matrix has the form
(23)
. This subgraph provides us
which has spectral norm
with a signal where the average degree does not equal the spec), demonstrating that the spectral
tral norm (unless
norm is a more appropriate power metric.

The results in this section detail the outcomes of several
10 000-trial Monte Carlo simulations. In each simulation, a
background graph is generated and may or may not have a
signal subgraph embedded on a subset of its vertices. The
subgraph may be a 15-vertex cluster or a bipartite graph with
and
. Test statistics outlined in Section V are
computed on the resulting graph, creating several empirical
distributions that can be used to discriminate between
and
. Residuals matrices are formed using either the exact
expected value,3 or a rank-1 approximation based on the
observed degrees, as in (9). The expected degree sequence
from the R-MAT model is used for CL backgrounds, and ER
backgrounds use the same average degree. For R-MAT and CL
backgrounds, we consider cases where the foreground vertices
are selected uniformly at random from all background vertices,
and cases where they are randomly selected from the set of
vertices with expected degree at most Ô¨Åve.
For 4096-vertex graphs, ER graphs always achieved nearperfect detection performance. IdentiÔ¨Åcation and detection performance for CL and R-MAT backgrounds are summarized in
Fig. 5. A few phenomena in the results conÔ¨Årm our intuition.
First, note that CL backgrounds have extremely similar performance, whether the expected value term is given or estimated.
This is because the observed degree is a good estimate for expected degree, and the small embedding has a minimal effect on
the expected value term, as shown in Appendix D. (The small
but noticeable difference when using a bipartite foreground emphasizes the impact of the number of subgraph vertices.) The
R-MAT backgrounds have much more substantial performance
differences, due to the model mismatch. In fact, when the true
expected value is given, performance is better than with the CL
background. This is likely due to the lower variance in the noise,
caused by smaller connection probabilities among low-degree
vertices. Detection performance improves going from the spectral norm statistic to the chi-squared statistic, and improves further when analyzing the eigenvector
norms. Also, when the
subgraph is embedded only on vertices with expected degree at
most Ô¨Åve, performance signiÔ¨Åcantly increases for
norm analysis, while it degrades for the other statistics (since it is likely
to be more orthogonal to the principal eigenvectors). Note also
that, for the spectral norm and chi-squared statistics, the bipartite embedding is more detectable than the cluster with the same
average degree, since the bipartite foreground has a higher spectral norm. This does not hold for the
norm statistic, since
the cluster embedding, while less powerful, is concentrated on
a smaller subset of vertices, making it more detectable using this
statistic.
One interesting aspect of the
norm technique is its
non-monotonic behavior when using the estimated rank-1 expected value. In both detection and identiÔ¨Åcation, performance
improves as the subgraphs increase in density up to a certain
point, after which performance degrades and then improves
again. This is due to clustering of eigenvalues caused by the
model mismatch, as shown in Fig. 6. The Ô¨Ågure presents a
3Due to time and memory constraints, a rank-100 approximation for the
R-MAT expected value was used instead of the true probability matrix.

MILLER et al.: A SPECTRAL FRAMEWORK FOR ANOMALOUS SUBGRAPH DETECTION

4199

Fig. 5. A summary of detection and identiÔ¨Åcation performance. The equal error rate (EER) for each background and foreground is shown as the average foreground
degree increases from 6 to 15. Results are shown for cluster subgraphs (solid line) and bipartite subgraphs (dashed line), for R-MAT graphs with the true expected
, R-MAT graphs with an estimated rank-1 expected value
, CL graphs with given expected degrees
, and CL graphs using observed degrees
.
value
norm
Performance improves as the test statistic goes from the spectral norm (left column), to the chi-squared statistic (center column), to the largest deviation in
norm-based statistic improves when the subgraph is embedded on low-degree vertices (second row), rather
(right column). Detection performance with the
than choosing the vertices uniformly at random (Ô¨Årst row). The same performance trends typically hold for the vertex identiÔ¨Åcation algorithms (uniform random
embedding in third row, degree-biased embedding in fourth row), shown here in terms of precision at a 35% recall rate. The non-monotone behavior using
norms is caused by a cluster of larger eigenvalues in the R-MAT background, which, as discussed in Appendix C, makes detection more difÔ¨Åcult with this method.

histogram of eigenvalues for the R-MAT graph minus the estimated rank-1 expected value matrix,
. (The vertical
axis is the average number of eigenvalues that fell into a given
bin over the 10 000 Monte Carlo trials.) Most of the eigenvalues
are below 12, while there is always 1 that is greater than 16
and 11 in the cluster that spans approximately 12 to 15. Since,

as discussed in Appendix C, having eigenvalues that are close
together hinders performance with this method, performance
improves when the subgraph can be localized in an eigenvector
as its eigenvalue approaches the gap at 12, but it will be more
difÔ¨Åcult once it falls in the cluster of larger values. Using the
true expected value instead of the rank-1 approximation does

4200

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 63, NO. 16, AUGUST 15, 2015

Fig. 6. Histogram of eigenvalues from an R-MAT matrix using an estimated
rank-1 expected value. The two clusters of larger eigenvalues are responsible
for the non-monotonic behavior in the -norm statistic shown in Fig. 5.

not yield this behavior, since there is no model mismatch.
The mismatch between R-MAT and the rank-1 expected value
also causes the slight degradation in performance using the
chi-squared statistic before it rapidly improves. This may be
because the embedded subgraph actually improves the symmetry of the projection by balancing out the mismatch, before
Ô¨Ånally overpowering it.
The identiÔ¨Åcation results on the bottom half of Fig. 5 follow
similar trends, with one notable exception. Performance is
shown in terms of precision at a 35% recall rate (precision is
emphasized since the foreground vertex set is much smaller
than the background). While the -means-based identiÔ¨Åcation
method (center column, using three clusters and a subgraph
threshold of Ô¨Åve vertices) typically improves performance
over thresholding of the principal eigenvector (Ô¨Årst column)
for cases where precision is relatively low, it actually hinders
performance in cases where precision is high. This shows that
a subgraph that separates well along the Ô¨Årst eigenvector will
not necessarily be equally detectable via -means, possibly due
to spreading in the second dimension.
Since sparse PCA has a much greater computational burden,
we carried out a more limited set of experiments on smaller
graphs. In each trial, a 512-vertex background graph is generated according to either an R-MAT or ER model. The R-MAT
graphs use the same probability matrix as in the previous
experiment, and the ER graphs have equal expected volume. In
each case, we use an estimated rank-1 expected value, and use
the DSPCA software package [47] to solve (19). Detection and
identiÔ¨Åcation performance are shown in Fig. 7. These results
demonstrate the detection of a 7-vertex, 80% dense subgraph
in the R-MAT background or a 5-vertex, 85% dense subgraph
in an ER background. Sparse PCA yields markedly superior
performance to the three methods used in Fig. 5. By using this
more costly technique, much smaller, subtler anomalies can
be detected, using the same principles as the less expensive
algorithms.
VII. RESULTS ON APPLICATION DATA
Two network datasets were downloaded from the Stanford
Network Analysis Project (SNAP) large graph dataset collection (available at http://snap.stanford.edu/data). One dataset
consists of product co-purchase records on amazon.com, where

Fig. 7. Detection and identiÔ¨Åcation results using sparse PCA. In both an
Erd≈ës-R√©nyi background (top row) and an R-MAT background (bottom row),
sparse PCA signiÔ¨Åcantly outperforms the other algorithms. Similar performance gaps are seen in detection performance (left column) and identiÔ¨Åcation
(right column).

Fig. 8. Eigenvector
norms in application datasets: an amazon.com product
co-purchase network (left) and an autonomous system network (right).

each of the 548 552 vertices represents a product, and a directed
edge from vertex to vertex denotes that when product is
purchased, product is frequently also purchased [48]. The
other dataset has 1 696 415 vertices, representing nodes on the
Internet, taken from autonomous system traceroutes in 2005
[49]. The edges in this graph are undirected and represent
communication links between nodes. In both cases, the 150
eigenvectors corresponding to the largest positive eigenvalues
of the residuals matrices were computed, and subgraphs were
analyzed that align with eigenvectors with small
norms.
In the amazon.com co-purchase network, edges are directed,
and each vertex has at most Ô¨Åve outward edges. We use the symmetrized modularity matrix introduced in [50] as a residuals matrix. As shown on the left in Fig. 8, many of the eigenvectors
have small
norms, due to frequent co-purchase of small, relatively isolated sets of products. We consider the two smallest
norms, corresponding to the 23rd and 135th largest eigenvectors. These eigenvectors are concentrated, respectively, on a
53-vertex subgraph with the maximum possible number of internal edges (265) and a 44-vertex subgraph with 215 internal

MILLER et al.: A SPECTRAL FRAMEWORK FOR ANOMALOUS SUBGRAPH DETECTION

edges of a possible 220. Neither subgraph has any outgoing
edges, and both have fewer than 20 incoming edges. To compare this to the graph as a whole, we took one million samples
of comparable size by performing random walks on the graph.
Of all 53-vertex samples, only 609 have average internal degree
greater than 4.5, and of those, none has fewer than 20 external
edges. Similarly, among the random samples with 44 vertices,
108 have average internal degree greater than 4.4 and fewer
than 40 external edges. Each of these 108 samples, however,
is primarily outside of the 150-dimensional space spanned by
the computed eigenvectors‚Äîan indicator vector for the sample
vertices in each case is nearly in the null space of the matrix
of eigenvectors. Thus, both of these subgraphs are anomalous
with respect to random samples of similar size, when considering portions of the graph that are well-represented in the computed subspace.
The eigenvector
norms in the autonomous system graph
generally follow a trend, getting larger as the eigenvalues get
smaller (indices increasing). The two vectors highlighted in the
Ô¨Ågure‚Äîthe 10th and 94th‚Äîwere considered for further investigation, since they have the largest local deviations. The 10th
eigenvector is aligned with a 70-vertex subgraph with over 99%
of its possible edges, and the 94th eigenvector is aligned with a
28-vertex subgraph with over 81% of its possible edges. These
subgraphs consisted of primarily high-degree vertices, with average external degrees of about 957 and 577 for the 70- and
28-vertex subgraphs, respectively. We took one million random
samples from among the vertices with degree greater than 500,
with sizes commensurate with the number of high-degree vertices in each subgraph (68 of 70 and 17 of 28). Among the three
68-vertex samples with density greater than 80%, all share at
least 55 vertices with the detected subgraph. Of the 17-vertex
samples, 713 are at least 75% dense and have fewer than 16 000
external edges (the 17-vertex subset is 93% dense and has about
12 500 external edges). Of these 713 samples, all are signiÔ¨Åcantly aligned with eigenvectors 10 and 18, both of which also
have extremely small
norms as shown in the Ô¨Ågure. Thus, the
only subgraphs among the samples with similar densities and
external degrees would be detected through analysis of eigenvector
norms.
VIII. CONCLUSION
In this paper, we present a spectral framework for the uncued
detection of small anomalous signals within large, noisy background graphs. This framework is based on analysis of graph
residuals in their principal eigenspace. We propose the spectral
norm as a power metric, and several algorithms are outlined,
with varying degrees of complexity. In simulation, we demonstrate the utility of the algorithms for detection and identiÔ¨Åcation
of two foregrounds within three background models, with the
more computationally complex methods providing better detection performance. In two real networks, subgraphs detected via
one of the algorithms are shown to be anomalous with respect
to random samples of the background.
The framework presented in this paper demonstrates the
utility of considering the anomalous subgraph detection
problem in a signal processing context. There are myriad avenues of investigation from this point. Recent work has focused

4201

on extending this framework to time-varying graphs [51], [52]
and attributed graphs [53]. Non-spectral statistics have also
been of interest, in particular for detecting anomalously sparse
(rather than anomalously dense) subgraphs [54], though this
complicates the analysis since embedding the signal involves
subtracting edges rather than adding them. Another interesting
area is detection using supervised learning based on subgraph
features, as in [55]. Performance bounds in spectral detection
of cliques and communities have recently been studied [11],
[56], as have computational limits of detection [57], [58]. Also,
while the presented framework relies on analysis of residuals,
considering normalized residuals may improve detection for
subgraphs where the edges are extremely unlikely [30], [59].
This analysis, however, may be intractable for more complicated graph models, since it requires normalizing each observed
vertex pair and may not allow the computational tricks mentioned in Section IV.A. As the detection of anomalous behavior
in relational datasets continues to be a problem of interest, the
Ô¨Åeld of signal processing for graphs will continue to pose a rich
set of challenges for the research community.
APPENDIX A
PROOF OF THEOREM 2
‚Äîthe hypothesis that the observed graph was genUnder
erated by an Erd≈ës-R√©nyi process‚Äîthe likelihood of the observed graph is given by
(24)
-vertex subset was seUnder the alternative hypothesis, an
lected uniformly at random to serve as the subgraph. Suppose
that
, was chosen as the subset. Each pair of
still has probability of sharing an edge due
vertices within
to background activity. If there is no edge in the background,
however, an edge will be added with probability . Thus, the
probability of an edge occurring between a given pair of veris
tices both in
(25)
All other vertex pairs still have probability of sharing an edge.
Therefore, we have

(26)
is the number of ‚Äúnon-edges‚Äù
Note that
that are not within the subgraph vertices. Since only one vertex
subset is chosen for the signal embedding, the likelihood of
under the alternative hypothesis is
(27)
Each of the
lihood ratio is

possible subsets is equally likely, so the like-

(28)

4202

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 63, NO. 16, AUGUST 15, 2015

or, equivalently,
(29)

Using the triangle inequality to remove the radical in (35) and
substituting the matrix norms back into the equation yields the
bound in (12). This completes the proof.
APPENDIX C
CONCENTRATION OF EIGENVECTORS ON SUBGRAPH VERTICES

The ratio in (29) can be further simpliÔ¨Åed as

(30)
Replacing the ratio in (29) with the expression in (30), and
moving the non-subgraph-dependent portion outside of the
summation, yields the expression in (5). This completes the
proof.
APPENDIX B
PROOF OF THEOREM 3

Here we provide an example of an embedding on which a
single eigenvector will be concentrated. Consider a subgraph
that is regular, i.e., each vertex has the same degree . Such
a subgraph will have a spectral norm
, and the principal eigenvector will be a vector in which all components on
subgraph vertices are equal. Let be a unit-normalized indicator
vector for the subgraph, i.e., a vector where the th component
is
if corresponds to a subgraph vertex and is 0 otherwise. Further consider
and
. We
have
(36)

be the (unit-normalized) principal eigenvector of .
Let
Since is the eigenvector corresponding to the largest eigenvalue of
, we have

where
(37)

(31)
Since only has nonzero entries in rows corresponding to subgraph vertices, we can bound this quantity below by
.
The vector can be decomposed as
, where
the only nonzero components of
correspond to the signal
may only be nonzero in the rows corsubgraph vertices and
responding to
. Let
. Since has unit
norm, and
and
are orthogonal, we have
and
. The largest eigenvalue of the residuals matrix
is then given by

is a random variable whose mean is 0 and variance is less than
, that is, the expected fraction of possible edges between the subgraph vertices that exist in the background. If the embedding occurs on vertices where the expected
connectivity is low, then
will likely be very small. We also
have

(38)
Note that

, which can be rewritten as

(32)
are zero, since is only nonzero
Both terms that include
within the subgraph vertices. To get an upper bound for this
quantity, we bound each term in (32), yielding
(39)

(33)
For convenience, let
, and
. Combining the
upper bound in (33) with the lower bound yields

For
only

, the expectation of the summand is 0. Considering
, we have
(40)

(34)
We can verify that, for
and
, (34) will
achieve equality at the lesser of the two roots of the parabola
obtained by squaring both sides of the expression. Therefore,
(34) holds whenever
(35)

(41)
where the upper bound is the average expected degree of the
subgraph vertices before the embedding occurs. Again, if the
subgraph is embedded on vertices with low expected degree,
this quantity is likely to be small.

MILLER et al.: A SPECTRAL FRAMEWORK FOR ANOMALOUS SUBGRAPH DETECTION

Let
uals matrix, with
), and let

be the eigendecomposition of the residdenoting the th eigenvector (
for
. We have

4203

Combining (50) and (53) and performing some algebraic manipulation yields the system of equations
(54)
(55)
(56)
which, solving for , gives us

(42)

(43)
If the quantities in (42) and (43) were the same, then would
be an eigenvector of
. Since their difference is very small
(i.e., assuming and
are small, as they are in expectation),
then may be highly correlated with a single eigenvector. That
is, for some
may be quite large, so that concentrates
most of its magnitude on the th eigenvector. Let
be the
, and
. Then
eigenvalue closest to
we have

(44)
, let
For
following substitutions:

. For convenience, deÔ¨Åne the

(45)
(46)

(57)
are spread far apart, then
If the eigenvalues around
, and
will be relatively large, the fractions in (57) will
be small, and will be heavily concentrated on a single eigenvector. This is supported by the empirical results in Section VI,
where embedding clusters onto vertices with low expected degree yields separation in a single eigenvector.
APPENDIX D
CHANGE IN MODULARITY DUE TO SUBGRAPH EMBEDDING
When using observed degree to estimate expected degree, the
difference in the expected value terms caused by the signal is as
follows. If no embedding occurs, the estimated expected value
, where is the observed degree vector resulting
is
from the background noise. If an anomalous subgraph is embedded into the background, the degree vector is changed by
. Since consists of only edges within the subgraph
that do not appear due to noise, the degree vector after embed, and the volume is
. Thus,
ding is
the difference between the modularity matrix with estimated exand
is
pected degrees under

(47)
(58)
(48)
(49)
Thus,
and
eigenvalues greater than
can then express (44) as

are convex combinations of the
and less than
, respectively. We

To bound the strength of
, we will bound the spectral norm
of each summand in the numerator of (58) and ignore the
in the denominator, yielding

(50)

(59)
To show that the strength of this quantity will grow more slowly
than the signal strength, given certain conditions, we will show
that
is
, i.e., that

(51)

(60)

(52)

, we will ignore the
term, as the other
Since
terms will dominate it. Thus, we must bound

Similarly, letting

(43) can be rewritten as
(53)

(61)

4204

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 63, NO. 16, AUGUST 15, 2015

In many applications, the graphs of interest have degree sequences that follow a power law; i.e., the number of vertices
for constants
.
with degree is approximately
Using this model, we can analyze the ratio of
and
norms
in graphs with a realistic growth pattern. Let
be the largest
and
norms
degree in the graph. Then the squares of the
of can be approximated as
(62)

(63)
respectively. Their ratio is then approximated, assuming
not exactly equal 1 or 2, as

does

(64)
In practice, is typically greater than 1 and less than 3 (see,
e.g., [60]), so the constant
will be positive. As
increases, the ratio on the right will tend to
. If we let
the maximum degree increase, however, should be allowed to
increase as well, since this controls the number of vertices with
a given degree. Assume
is a degree that will probably not
occur in the graph. SpeciÔ¨Åcally, for a small, constant threshold
, let
. Since this means that
(65)
we have
(66)
Using the approximation in (64), the ratio of the
and
norms of is approximately
.
To bound the term dependent on the subgraph, we have
(67)
This upper bound can be achieved if the subgraph is a clique or
, we substitute (66) and
a star. Noting that
(67) into (61) to obtain
(68)
is
if
is
. Using (65) as
meaning that
will vanish
a lower bound for , this implies that
grows more slowly than
.
as the graph grows if

ACKNOWLEDGMENT
The authors would like to thank Dr. B. Johnson and the Lincoln Laboratory Technology OfÔ¨Åce for supporting this work,
and R. Bond, Dr. J. Ward, and D. Martinez for their managerial support. We would also like to thank N. Singh, for his early
work on the method in Section V.C. Finally, we would like to
thank Dr. R. S. Caceres, Dr. R. J. Crouser, D. Goodwin, Prof.
A. O. Hero III, Dr. S. Kelley, Dr. A. Reuther, Dr. M. C. Schmidt,
Dr. M. M. Wolf, and the anonymous referees for many helpful
comments on this paper.
REFERENCES
[1] D. Bu, Y. Zhao, L. Cai, H. Xue, X. Zhu, and H. Lu et al., ‚ÄúTopological
structure analysis of the protein-protein interaction network in budding
yeast,‚Äù Nucleic Acids Research, vol. 31, no. 9, pp. 2443‚Äì2450, 2003.
[2] T. Id√© and H. Kashima, ‚ÄúEigenspace-based anomaly detection in computer systems,‚Äù in Proc. ACM Int. Conf. Knowl. Discov. Data Min.,
2004, pp. 440‚Äì449.
[3] M. E. J. Newman, ‚ÄúFinding community structure in networks using the
eigenvectors of matrices,‚Äù Phys. Rev. E, vol. 74, no. 3, 2006, Article
ID 036104.
[4] K. S. Xu and A. O. Hero, III, ‚ÄúDynamic stochastic blockmodels for
time-evolving social networks,‚Äù IEEE J. Sel. Topics Signal Process.,
pp. 552‚Äì562, Aug. 2014.
[5] J. M. Kleinberg, ‚ÄúAuthoritative sources in a hyperlinked environment,‚Äù
J. ACM, vol. 46, no. 5, pp. 604‚Äì632, Sept. 1999.
[6] K. Chen, C. Huo, Z. Zhou, and H. Lu, ‚ÄúUnsupervised change detection
in SAR image using graph cuts,‚Äù in Proc. IEEE Int. Geosci. Remote
Sens. Symp., Jul. 2008, vol. 3, pp. 1162‚Äì1165.
[7] A. Sandryhaila and J. M. F. Moura, ‚ÄúDiscrete signal processing on
graphs,‚Äù IEEE Trans. Signal Process., vol. 61, pp. 1644‚Äì1656, Apr.
2013.
[8] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, ‚ÄúThe emerging Ô¨Åeld of signal processing on graphs:
Extending high-dimensional data analysis to networks and other irregular domains,‚Äù IEEE Signal Process. Mag., vol. 30, pp. 83‚Äì98, May
2013.
[9] T. L. MifÔ¨Çin, C. Boner, G. A. Godfrey, and J. Skokan, ‚ÄúA random graph
model for terrorist transactions,‚Äù in Proc. IEEE Aerosp. Conf., 2004,
pp. 3258‚Äì3264.
[10] N. Alon, M. Krivelevich, and B. Sudakov, ‚ÄúFinding a large hidden
clique in a random graph,‚Äù in Proc. ACM-SIAM Symp. Discrete Algorithms, 1998, pp. 594‚Äì598.
[11] R. R. Nadakuditi, ‚ÄúOn hard limits of eigen-analysis based planted
clique detection,‚Äù in Proc. IEEE Statist. Signal Process. Workshop,
2012, pp. 129‚Äì132.
[12] E. Arias-Castro and N. Verzelen, ‚ÄúCommunity detection in random
networks,‚Äù 2013, preprint: arXiv.org:1302.7099 [Online]. Available:
http://arxiv.org/abs/1302.7099
[13] N. Verzelen and E. Arias-Castro, ‚ÄúCommunity detection in sparse
random networks,‚Äù 2013, preprint: arXiv:1308.2955 [Online]. Available: http://arxiv.org/abs/1308.2955
[14] W. Eberle and L. Holder, ‚ÄúAnomaly detection in data represented as
graphs,‚Äù Intell. Data Anal., vol. 11, no. 6, pp. 663‚Äì689, Dec. 2007.
[15] D. B. Skillicorn, ‚ÄúDetecting anomalies in graphs,‚Äù in Proc. IEEE Intell.
Secur. Informatics, 2007, pp. 209‚Äì216.
[16] S. T. Smith, S. Philips, and E. K. Kao, ‚ÄúHarmonic space-time threat
propagation for graph detection,‚Äù in Proc. IEEE Int. Conf. Acoust.,
Speech, Signal Process., 2012, pp. 3933‚Äì3936.
[17] S. T. Smith, E. K. Kao, K. D. Senne, G. Bernstein, and S. Philips,
‚ÄúBayesian discovery of threat networks,‚Äù IEEE Trans. Signal Process.,
vol. 62, pp. 5324‚Äì5338, Oct. 2014.
[18] G. A. Coppersmith and C. E. Priebe, ‚ÄúVertex nomination via content
and context,‚Äù 2012, preprint: arXiv.org:1201.4118v1 [Online]. Available: http://arxiv.org/abs/1201.4118
[19] S. Fortunato, ‚ÄúCommunity detection in graphs,‚Äù Phys. Rep., vol. 486,
pp. 75‚Äì174, Feb. 2010.
[20] M. E. J. Newman and M. Girvan, ‚ÄúFinding and evaluating community
structure in networks,‚Äù Phys. Rev. E, vol. 69, no. 2, 2004, Article ID
026113.
[21] C. E. Priebe, J. M. Conroy, D. J. Marchette, and Y. Park, ‚ÄúScan statistics
on Enron graphs,‚Äù Comput. Math. Organiz. Theory, vol. 11, no. 3, pp.
229‚Äì247, 2005.

MILLER et al.: A SPECTRAL FRAMEWORK FOR ANOMALOUS SUBGRAPH DETECTION

[22] T. H. Cormen, C. E. Leiserson, and R. L. Rivest, Introduction to Algorithms. Cambridge, MA, USA: MIT Press, 1990.
[23] J. Ruan and W. Zhang, ‚ÄúAn efÔ¨Åcient spectral algorithm for network
community discovery and its applications to biological and social networks,‚Äù in Proc. IEEE Int. Conf. Data Min., 2007, pp. 643‚Äì648.
[24] S. White and P. Smyth, ‚ÄúA spectral clustering approach to Ô¨Ånding communities in graphs,‚Äù in Proc. SIAM Int. Conf. Data Min., 2005, pp.
274‚Äì285.
[25] D. Fasino and F. Tudisco, ‚ÄúAn algebraic analysis of the graph modularity,‚Äù SIAM. J. Matrix Anal. Appl., vol. 35, no. 3, pp. 997‚Äì1018, 2014.
[26] Q. Ding and E. D. Kolaczyk, ‚ÄúA compressed PCA subspace method for
anomaly detection in high-dimensional data,‚Äù IEEE Trans. Inf. Theory,
vol. 59, no. 11, pp. 7419‚Äì7433, Nov. 2013.
[27] S. Hirose, K. Yamanishi, T. Nakata, and R. Fujimaki, ‚ÄúNetwork
anomaly detection based on eigen equation compression,‚Äù in Proc.
ACM Int. Conf. Knowl. Discov. Data Min., 2009, pp. 1185‚Äì1193.
[28] F. R. K. Chung, Spectral Graph Theory. Providence, RI, USA: Amer.
Math. Soc., 1997.
[29] S. J. Young and E. R. Scheinerman, ‚ÄúRandom dot product graph models
for social networks,‚Äù in Algorithms and Models for the Web-Graph, ser.
Lecture Notes in Computer Science, A. Bonato and F. R. K. Chung,
Eds. New York, NY, USA: Springer, 2007, vol. 4863, pp. 138‚Äì149.
[30] F. Chung, L. Lu, and V. Vu, ‚ÄúThe spectra of random graphs with given
expected degrees,‚Äù in Proc. Nat. Acad. Sci. USA, 2003, vol. 100, no.
11, pp. 6313‚Äì6318.
[31] B. A. Miller, N. Arcolano, M. S. Beard, J. Kepner, M. C. Schmidt, N.
T. Bliss, and P. J. Wolfe, ‚ÄúA scalable signal processing architecture
for massive graph analysis,‚Äù in Proc. IEEE Int. Conf. Acoust., Speech,
Signal Process., 2012, pp. 5329‚Äì5332.
[32] P. O. Perry and P. J. Wolfe, ‚ÄúNull models for network data,‚Äù 2012,
preprint: arXiv:1201.5871v1 [Online]. Available: http://arxiv.org/abs/
1201.5871
[33] D. S. Choi, P. J. Wolfe, and E. M. Airoldi, ‚ÄúStochastic blockmodels
with a growing number of classes,‚Äù Biometrika, vol. 99, no. 2, pp.
273‚Äì284, 2012.
[34] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúToward signal processing
theory for graphs and non-Euclidean data,‚Äù in Proc. IEEE Int. Conf.
Acoust., Speech, Signal Process., 2010, pp. 5414‚Äì5417.
[35] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúSubgraph detection using
eigenvector L1 norms,‚Äù in Proc. Adv. Neural Inf. Process. Syst., J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta,
Eds., 2010, vol. 23, pp. 1633‚Äì1641.
[36] N. Singh, B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúAnomalous
subgraph detection via sparse principal component analysis,‚Äù in Proc.
IEEE Statist. Signal Process. Workshop, 2011, pp. 485‚Äì488.
[37] B. A. Miller, N. T. Bliss, P. J. Wolfe, and M. S. Beard, ‚ÄúDetection
theory for graphs,‚Äù Lincoln Lab. J., vol. 20, no. 1, pp. 10‚Äì30, 2013.
[38] D. Donoho, ‚ÄúCompressed sensing,‚Äù IEEE Trans. Inf. Theory, vol. 52,
no. 4, pp. 1289‚Äì1306, 2006.
[39] B. A. Prakash, A. Sridharan, M. Seshadri, S. Machiraju, and C.
Faloutsos, ‚ÄúEigenSpokes: Surprising patterns and scalable community
chipping in large graphs,‚Äù in Advances in Knowledge Discovery and
Data Mining, ser. LNCS, M. J. Zaki, J. X. Yu, B. Ravindran, and V.
Pudi, Eds. New York, NY, USA: Springer, 2010, vol. 6119, ch. 14,
pp. 435‚Äì448.
[40] L. Wu, X. Wu, A. Lu, and Z.-H. Zhou, ‚ÄúA spectral approach to detecting subtle anomalies in graphs,‚Äù J. Intell. Inf. Syst., vol. 41, no. 2,
pp. 313‚Äì337, 2013.
[41] A. d‚ÄôAspremont, L. E. Ghaoui, M. I. Jordan, and G. R. G. Lanckriet, ‚ÄúA
direct formulation for sparse PCA using semideÔ¨Ånite programming,‚Äù
SIAM Rev., vol. 49, no. 3, pp. 434‚Äì448, 2007.
[42] R. Lehoucq and D. Sorensen, ‚ÄúImplicitly restarted Lanczos method,‚Äù in
Templates for the Solution of Algebraic Eigenvalue Problems: A Practical Guide, Z. Bai, J. Demmel, J. Dongarra, A. Ruhe, and H. van der
Vorst, Eds. Philadelphia, PA, USA: SIAM, 2000, ch. 4.5.
[43] P. Erd≈ës and A. R√©nyi, ‚ÄúOn random graphs,‚Äù Publicationes Mathematicae Debrecen, vol. 6, pp. 290‚Äì297, 1959.
[44] A. Barab√°si and R. Albert, ‚ÄúEmergence of scaling in random networks,‚Äù Science, vol. 286, no. 5439, pp. 509‚Äì512, 1999.
[45] N. Arcolano, K. Ni, B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúMoments
of parameter estimates for Chung-Lu random graph models,‚Äù in Proc.
IEEE Int. Conf. Acoust., Speech, Signal Process., 2012, pp. 3961‚Äì3964.
[46] D. Chakrabarti, Y. Zhan, and C. Faloutsos, ‚ÄúR-MAT: A recursive
model for graph mining,‚Äù in Proc. SIAM Int. Conf. Data Min., 2004,
pp. 442‚Äì446.
[47] R. Luss, A. d‚ÄôAspremont, and L. E. Ghaoui, DSPCA: Sparse PCA
using semideÔ¨Ånite programming, ver. 0.6, Dec. 2008 [Online]. Available: http://www.di.ens.fr/~aspremon/DSPCA.html

4205

[48] J. Leskovec, L. A. Adamic, and B. A. Huberman, ‚ÄúThe dynamics of
viral marketing,‚Äù ACM Trans. Web, vol. 1, pp. 1‚Äì39, May 2007.
[49] J. Leskovec, J. Kleinberg, and C. Faloutsos, ‚ÄúGraphs over time: DensiÔ¨Åcation laws, shinking diameters and possible explanations,‚Äù in Proc.
Int. Conf. Knowl. Discov. Data Min., 2005, pp. 177‚Äì187.
[50] E. A. Leicht and M. E. J. Newman, ‚ÄúCommunity structure in directed
networks,‚Äù Phys. Rev. Lett., vol. 100, pp. 118703-1‚Äì118703-4, Mar.
2008.
[51] B. A. Miller, M. S. Beard, and N. T. Bliss, ‚ÄúMatched Ô¨Åltering for subgraph detection in dynamic networks,‚Äù in Proc. IEEE Statist. Signal
Process. Workshop, 2011, pp. 509‚Äì512.
[52] B. A. Miller and N. T. Bliss, ‚ÄúToward matched Ô¨Ålter optimization for
subgraph detection in dynamic networks,‚Äù in Proc. IEEE Statist. Signal
Process. Workshop, 2012, pp. 113‚Äì116.
[53] B. A. Miller, N. Arcolano, and N. T. Bliss, ‚ÄúEfÔ¨Åcient anomaly detection
in dynamic, attributed graphs,‚Äù in Proc. IEEE Intell. Secur. Inform.,
2013, pp. 179‚Äì184.
[54] B. A. Miller, L. H. Stephens, and N. T. Bliss, ‚ÄúGoodness-of-Ô¨Åt statistics
for anomaly detection in Chung-Lu random graphs,‚Äù in Proc. IEEE Int.
Conf. Acoust., Speech, Signal Process., 2012, pp. 3265‚Äì3268.
[55] S. Pan and X. Zhu, ‚ÄúGraph classiÔ¨Åcation with imbalanced class distributions and noise,‚Äù in Proc. Int. Joint Conf. Artif. Intell., 2013, pp.
1586‚Äì1592.
[56] R. R. Nadakuditi and M. E. J. Newman, ‚ÄúGraph spectra and the detectability of community structure in networks,‚Äù Phys. Rev. Lett., vol.
108, no. 18, pp. 188701-1‚Äì188701-5, 2012.
[57] Q. Berthet and P. Rigollet, ‚ÄúComplexity theoretic lower bounds for
sparse principal component detection,‚Äù in Conf. Learn. Theory, S.
Shalev-Shwartz and I. Steinwart, Eds., 2013, vol. 30, pp. 1046‚Äì1066,
ser. JMLR W&CP.
[58] Y. Chen and J. Xu, ‚ÄúStatistical-computational tradeoffs in planted
problems and submatrix localization with a growing number of
clusters and submatrices,‚Äù 2014, preprint arXiv:1402.1267 [Online].
Available: http://arxiv.org/abs/1402.1267
[59] R. R. Nadakuditi and M. E. J. Newman, ‚ÄúSpectra of random graphs with
arbitrary expected degrees,‚Äù Phys. Rev. E, vol. 87, no. 1, pp. 0128031‚Äì012803-12, 2013.
[60] M. Faloutsos, P. Faloutsos, and C. Faloutsos, ‚ÄúOn power-law relationships of the Internet topology,‚Äù in Proc. SIGCOMM, 1999.

Benjamin A. Miller (M‚Äô10‚ÄìSM‚Äô15) received the
B.S. degree (with highest honors) and the M.S. degree in computer science in 2005 from the University
of Illinois at Urbana-Champaign. In 2005, he joined
Lincoln Laboratory at the Massachusetts Institute
of Technology as an Associate Staff member in
the Embedded Digital Systems (later Embedded
and High Performance Computing) group. In this
role, he developed novel algorithms for real-time
linearization of radio-frequency electronics, researched methods and models for signal recovery in
multi-sensor compressed sensing, and developed efÔ¨Åcient spectral techniques
for the detection of anomalies in large graphs. Since 2012, he has been a
Technical Staff member at Lincoln Laboratory, currently in the Cyber Analytics
and Decision Systems group, where he continues to focus his research on the
theoretical and computational aspects of anomaly detection in large networks
and other dynamic, combinatorial structures. Mr. Miller is a member of the
IEEE Signal Processing Society, the Association for Computing Machinery,
and the Society for Industrial and Applied Mathematics. He holds 6 patents and
is author or co-author of 36 peer-reviewed conference and journal papers on
nonlinear signal processing, compressive sensing, and detection and estimation
theory for graph-based data.

Michelle S. Beard is a member of the technical staff
in the Product Assurance division at Draper Laboratory. Previously, she was a research staff member
in the Computing and Analytics Group at MIT Lincoln Laboratory. She received her bachelor‚Äôs degree
in computer science at Bryn Mawr College in 2010
and is currently pursuing her Master‚Äôs degree in computer science at Tufts University. Her technical experience includes software engineering and test, visual
analytics, and HCI.

4206

IEEE TRANSACTIONS ON SIGNAL PROCESSING, VOL. 63, NO. 16, AUGUST 15, 2015

Patrick J. Wolfe (S‚Äô96‚ÄìM‚Äô03‚ÄìSM‚Äô08) holds chairs
in statistics and computer science at University College London (UCL), where his research is focused
on statistical theory and methods for network data
analysis and signal processing. He received the
B.S.E.E. degree from the University of Illinois at
Champaign-Urbana in 1998 and the Ph.D. degree
from Cambridge University in 2003, after which he
joined the faculty of Harvard University, receiving
the Presidential Early Career Award from the White
House in 2008 for contributions to signal and image
processing. In addition to serving as founding Executive Director of the UCL
Big Data Institute, he is currently a U.K. Royal Society Research Fellow and
an EPSRC Established Career Fellow in the Mathematical Sciences, and leads
several major research initiatives in network modeling and inference.

Nadya T. Bliss (M‚Äô08‚ÄìSM‚Äô10) received the Bachelor of Science degree in computer science from Cornell University, Ithaca, NY in 2002, Master of Engineering degree in computer science from Cornell
University in 2002, and the Ph.D. degree in applied
mathematics for the life and social sciences (complex
adaptive systems science) from Arizona State University, Tempe, AZ, USA in 2015.
From 2002 to 2012, she was with MIT Lincoln
Laboratory, most recently as the Group Leader of
the Computing and Analytics Group. Currently, she
holds the following appointments at Arizona State University: Director, Global
Security Initiative; Professor of Practice, School of Computing, Informatics,
and Decision Systems Engineering; and Senior Sustainability Scientist, Julie
Ann Wrigley Global Institute of Sustainability. She leads interdisciplinary
research teams addressing global challenges in cyber security and digital
identity, mitigation and adaptation to climate change, and human security. Her
personal research both at MIT Lincoln Laboratory and ASU has been focused
on analysis of large networks for wide range of applications.
Dr. Bliss was awarded the inaugural MIT Lincoln Laboratory Early Career
Technical Achievement award recognizing her work in parallel computing,
computer architectures, and graph processing architectures and her leadership
in anomaly detection in graph-based data (presented to 2 employees under 35)
in 2011.

3D IMAGE GEO-REGISTRATION USING VISION-BASED MODELING
Karl Ni, Zachary Sun, Nadya Bliss
M.I.T. Lincoln Laboratories
E-mails: karl.ni@ll.mit.edu, zsun86@ll.mit.edu, nt@ll.mit.edu

ABSTRACT
Image geo-registration is the process of relating a photograph and
its pose to referenced world coordinates. The application is relevant, especially to the social networking, photo-sharing, and intelligence communities, where the locations of objects of interest are to
be determined. This paper proposes an algorithm that identiÔ¨Åes and
geo-registers query photographs in the absence of any meta-data that
are spatially co-located with training sets of user-collected images.
Training images construct a 3D model using inherent structure from
motion between images. Using SIFT features, the 3D model fuses
co-located points and generates an averaged SIFT comparison feature. This paper also advises on training set augmentation to mitigate
deleterious illumination, seasonal, and weather effects by introducing methods of merging separate point clouds. As such, probability
of detection (and therefore, registration performance) is enhanced.
After the 3D model is generated, each test photograph is matched
to the 3D point cloud and registered using similar camera reÔ¨Ånement
and positional optimization techniques. The results provide accuracy
to within a few meters of the absolute geo-coordinates.
Index Terms‚Äî Structure from Motion, Image Registration
1. INTRODUCTION
For at least a decade, the quantity of electro-optically collected media has been exponentially surpassing industry and academia‚Äôs ability to process them. With so much data, tasks such as searching
through images for speciÔ¨Åc attributes, monitoring a vast network of
surveillance cameras, and organizing collections of photos into albums have become both important and unmanageable without automation. Associating relevant information to visual media via tagging often makes it easier to perform these tasks by assigning meaningful and tangible attributes. Physical location in the form of absolute world geo-coordinates (latitude, longitude, and elevation) can
be included in the potential list of attributes that may be used to organize, search, and index digital imagery, and this paper is concerned
with the automatic geo-registration of images and video in the absence of any meta-data.
Recently, geolocating massive amounts of data has been attacked through correlation and feature-based models. Of particular
interest in this paper are structure from motion (SfM) techniques, to
recover underlying 3D information from multiple views. Obtaining
SfM from unorganized images has been extensively studied, though
much of the literature focuses on data sets that have been obtained
from data-rich and varied sources (e.g., the internet), without the
explicit purpose of image geo-registration. In contrast, this work
This work is sponsored by the Department of the Air Force under Air
Force contract FA8721-05-C-0002. Opinions, interpretations, conclusions,
and recommendations are those of the author and are not necessarily endorsed
by the United States Government.

978-1-4577-0539-7/11/$26.00 ¬©2011 IEEE

1573

focuses on a more constrained problem where SfM procedures are
used in collaboratively collected photos in unmapped areas. Hence,
the integration of multiple processed 3D data may be required to
boost the invariance of the geo-registration problem to illumination.
The proposed algorithm addresses this issue while leveraging
techniques in multiview geometry to determine the maximum likelihood 3D information that is useful in the geo-registration problem.
The remainder of this paper discusses the application of SfM and
related issues as they apply in geo-registering a single image. Sec. 2
describes the setup that facilitates exploitation of image properties
with reference to geo-registration. Next, Sec. 3 aligns multiple data
collections in voxel space, a common dilemma in collaboratively
collected 2D imagery. Sec. 4 describes the exploitation process, and
Ô¨Ånally results and future work are broached in Sec. 5 and Sec. 6
2. 3D MODEL CONSTRUCTION
In order to geo-register images, there must be a model that provides a
positional reference. The model can be provided by the vast amount
of published SfM work, and hence, the initialization phase of our
work utilizes Noah Snavely‚Äôs Photo Tourism research [1] to create a
voxel space that describes the 3D structure derived from the motion
in a large set of co-located 2D images. It is the 3D point cloud that
will provide the reference to which query images are geo-registered.
The point cloud construction begins with the extraction and matching of features in Sec. 2.1 followed by SfM in Sec. 2.2. Then, a
single format that is conducive to future matches serves as the Ô¨Ånal
representation in Sec. 2.3.
2.1. Feature Matching
David Lowe‚Äôs SIFT keypoint detector [2] is especially useful across
multiple viewpoints due to its invariance to image transformation
and robustness in matching. SIFT is an n-dimensional descriptor
vector that can be used for matching, where n = 128 typically gives
the best balance between speed and performance when attempting to
reliably match images.
Once all of the features have been extracted, correspondences
must be established. For each pair of images, keypoints are matched
by Ô¨Ånding the nearest neighbor vector in the corresponding image.
To speed up the matching, Arya and Mount‚Äôs approximate nearest
neighbor (ANN) package [3] was used.
For image I and J, ANN builds a kd-tree of the features in image J and then queries the tree for the best match of each feature in
image I. Instead of deÔ¨Åning a valid match by thresholding the distance, valid matches are determined using Lowe‚Äôs ratio test [2]: Ô¨Ånd
the best two nearest neighbors in image I with distances d1 and d2
where d1 < d2 . Accept as a match if dd12 < 0.6.
(I)

Let fi

be a feature obtained from image I. Then the nearest

ICASSP 2011

(I)

(J)

neighbor match to fi in the set of features {fj } obtained from
image J is deÔ¨Åned as:
(J)

fmatch,i

=

d

=

(I)

argminfi
fj

(I)

minfi
fj

(J)

‚àí fj 2
(J)

‚àí fj 

(1)
(2)

2.2. Structure From Motion
Photo Tourism [1] utilizes (1) and (2) to compute the voxel locations
of reliably matched features, while estimating the camera parameters. Tracks of matching features are then built to be triangulated
later. Photo Tourism initializes with a reconstruction of an initial
two images by triangulating the tracks of two images with the most
number of images and largest baseline. After an initial reconstruction it then proceeds to add images that observe a large set of what
has already been reconstructed and adds in new tracks that are observed by already reconstructed images. Like other SfM projects,
Photo Tourism also runs a bundle adjustment on the reconstructed
scene after each image is added. Snavely in [4] describes this process in greater detail.
2.3. 3D Feature Representation

m‚àí1
1 
fkj , j = 1, ..., n
m

(i)

(i)

xi = PB XB = PA XA , ‚àÄi = 1, ¬∑ ¬∑ ¬∑ M

(4)

T XB = XA

(5)

Because any pseudoinverse underdetermined, it is impossible to determine a single point from a camera single XA =
+

(i)
(i)
PB XB + Œ±v (where v is a vector in the null space of
PA
(i)

Packages from [1] create a constraint such that each voxel is a track
of features from multiple images. Each feature in these tracks (because of the matching criterium) can be considered nearly identical.
As a result each voxel has its own set of descriptor vectors that can
be used to match with the new image. For ease of computation, we
take an average of the feature descriptors for a given voxel into a
representative descriptor (3), however any other data reduction techniques could work (median, min, max). We then store these descriptors in the same format as Lowe keypoint Ô¨Åles. While we no longer
have a concept of scale nor an absolute orientation, we modify the
storage format to reduce his four keypoint values (scale, orientation,
location) down to just three (location).
Fj =

section discusses methods to arbitrarily augment a 3D point cloud
given and maximize the temporal coverage enabling a model‚Äôs immediate exploitability.
Say for the sake of convenience that we have taken a set of
pictures at time A and another set at time B. Let the illumination
conditions at time A be signiÔ¨Åcantly different from those at time
B. Also let there be at least a few images register between both
A and B clouds in C = A ‚à© B. With the photos sets, we create
(i)
point clouds, A and B, where PA ‚àà R3√ó4 denotes the projection matrix that maps a feature XA ‚àà R4√ó1 in point cloud A to a
two dimensional feature x(i) ‚àà R3√ó1 in the ith image. Likewise,
(i)
(i)
x(i) = PB XB = PA XA , where image i is in C.
Using techniques from [5], a set of M images in C can be found,
where image i relates to both A and B. To merge point clouds, we
must determine the transformation T that brings points in B into A.
The two conditions that arise from these statements is:

PA and Œ± is a scalar multiple), but from (4) and (5), we can write
(i)

PB XB
(i)

min
T

k=0

Here, Fj is the j th representative descriptor in the point cloud, and
fkj is the kth feature corresponding to the j th voxel.

Photos from the internet can be integrated with structure from motion since signiÔ¨Åcant quantities of pictures have been taken at random times, days, seasons, and precipitation conditions. Such algorithms are conservative by nature of training set attributes, rejecting
data that may otherwise be useful. However, for any data miss, the
internet provides an abundant source that will more than adequately
compensate.
On the other hand, for controlled experiments and applications
where users must collect the training set themselves over discrete
time periods, luminance variations and feature variation may be difÔ¨Åcult to match to a 3D point cloud. Because users collect data at discrete times, the model may not be immediately available or useful in
the interim between collects. Moreover, once new data is collected, a
3D structure that incorporates a large proportion of images may not
be feasibly constructed, or more likely and even worse, would exclude a large amount of information during bundle adjustment. This

1574

PA T XB

(6)

=

0

(7)

In order to automate the process as much as possible, the mapping effort can be done solely from cameras in C. However, because
C is small, there are still some slight errors in the alignment, so
manually picking one or two tie points is necessary. The joint optimization problem can be written as

(3)

3. POINT CLOUD ALIGNMENT

(i)

PB ‚àí P A T

(i)

=


s.t.

M


(i)

(i)

||PB ‚àí PA T ||F

i=1



(j)

(j)

T XB = XA

(8)

j

and we have minimized the Frobenius norm (denoted by the subscript F) of the difference in M projection matrices while constraining handpicked 3-D points i. Also, N is a small number of hand(j)
picked point correspondences {XA X (j) } ‚àà
/ C.
The relaxation on the constraint reduces to a minimization problem on both errors simulatenously, while making sure every column
of T is minimized by post-multiplying by vk , an indicator vector
with kth element one and the rest zero. The optimization problem is
thus:

min
T

4
N
M 




(i)
(i)
(j)
(j)
|| PB ‚àí PA T vk ||2 + Œª
||T XB ‚àí XA ||2
i=1 k=1

j=1

(9)
This convex intersection of quadratic functions can be solved
analytically in the Lagrangian, with the end solution appearing similar to some combination of the Moore Penrose equations. Solving
for Œª is entirely possible, but we wish to have some control over the

weighting of points, given that our choice of point correspondences
(in the constraint) may not be accurate. (We‚Äôve only chosen seven.)
Let L be the objective function in (9).Then
‚àÇL(T )/‚àÇT = 0
‚áì
T
T
0 = PAT PA T + ŒªT XB XB
‚àí PAT PB ‚àí ŒªXA XB
‚áì
T
T
T
PA PA T + ŒªT XB XB = PAT PB + ŒªXA XB

(10)

As it turns out, the Ô¨Ånal differential matrix equations can be reduced to the form AT + T B = C, with A and B being symmetric
rank 4 matrices. Solving for this equation is often of interest, and
has been solved in a variety of ways, depending on the properties of
A and B. T can be solved by considering that the sum of the eigenvalues of A and B are incorporated into C [6]. The end result can be
written as
Àú TB
T = Œ¶A CŒ¶
(11)
where Œ¶ are the collective eigenvector decompositions with eigenvalues ŒΩi and Œºj in the substitution:
Œ¶T CŒ¶B
CÀúij = A
ŒΩi + Œºj

(12)

the point cloud. As an additional part to our initialization stage we
compute the 3D feature for each voxel and store it as a keypoint Ô¨Åle
(Sec. 2.3) and then the new image‚Äôs 2D features are matched to 3D
voxel features (Sec. 4.1).
Recall that the problem statement is to determine whether or not
a test picture T is co-located with a set of images S, which we have
represented by a number of keypoint descriptors in the set {Fj }.
Therefore, the framework of our problem necessitates two hypotheses, where the events, labeled H0 and H1, are deÔ¨Åned as
‚éß
H0:
‚é™
‚é®
‚é™
‚é© H1:

Image T is not taken of speciÔ¨Åed area and
hence does not belong to set S
Image T is taken of the speciÔ¨Åed area and
hence belongs to set S

Given a representative set of feature descriptors, we build a kdtree (Arya and Mount‚Äôs ANN package [3]) with the representative
features. The new image‚Äôs features are extracted using SIFT as well,
and then matches are determined with the ratio test.
(T )
Let {fi } be the set of features obtained in the test image T .
We once again search for the smallest and second smallest distances,
d1 and d2 , and test the ratio dd12 < 0.6 to Ô¨Ånd the match deÔ¨Åned by
(Equ. 15)
(T )

‚àí Fj 2

(T )

‚àí Fj 2

Fmatch,i = argminfi
Fj

and
d1 , d2 =
A
B
C

=
=
=

PAT PA
T
ŒªXB XB
T
T
PA PB + ŒªXA XB

(13)

min fi

Fj,1 ,Fj,2

After sufÔ¨Åciently training the algorithm using setup procedures detailed in Sec. 2 and Sec. 3, images can be geo-registered. This section describes the process of extracting geo-coordinates from a query
image in the absence of any meta-data. Like recognition approaches,
image geo-registration requires two steps. Images must Ô¨Årst be detected as belonging to a location or 3D scene. In our case, the scene
has been generated as a 3D point cloud, where points are classiÔ¨Åed
with techniques similar to Sec. 2.1 using average SIFT vectors. The
second step builds 3D geometry from known matches and is analogous to Sec. 2.3, and results in the Ô¨Ånal exploitation product.
4.1. 3D Feature Matching
Given an area (for example, a section of a residential street, a courtyard, a city block, etc.), let S be the set of all images of that area. We
then deÔ¨Åne a subset R ‚àà S where R is a collection of images of the
area. We want to determine whether a new image, T , belongs to set
S. As Lowe demonstrated in [2], one way to go about this would be
to take T and match it to every image in R to decide whether or not T
belongs to S. However this is excessively expensive in both storage
and time as R could potentially be very large. In an effort to reduce
the cost of subsequent detections, we leverage Photo Tourism to give
us a new way of representing this set: the real world geometry.
We Ô¨Årst extract the SIFT features from R, match them, and then
run SfM on the matches to create a point cloud. Since the reconstructed point cloud is based on the matched features from the images themselves, this lends itself to a different way of matching. Instead of matching to each image, we choose to match the image to

1575

(15)

Once the number of matches in the new image are determined,
we utilize a percentage as our test statistic in (Eqn. 16).
T est Statistic =

4. OBTAINING 3D INFORMATION FROM A 2D IMAGE

(14)

|{Fmatch,i }|

(16)
(T )
|{fi }|
If the test statistic is above a certain threshold, then the image is
said to match the 3D scene.
One interesting outcome of using matches among images is the
ability to create an image graph, constructed in the same way as [4],
though utilized differently. By determining which images are most
similar to the query, should the geo-registration process described
in the next subsection fail, the approximate location can still be potentially discerned based on the images that may have matched. An
example graph can be seen in Sec. 5 in Fig. 1.
4.2. Registering the query image
Once matches have been found, RANSAC determines the maximum
number of inliers for the homography and projective transformation
matrix between the 3D scene (point cloud) and 2D image. The result is a linear regression determined from a minimum of 6 points.
However, an optimal linear regression as a minimum solution may
not be sufÔ¨Åciently accurate, because there are constraints such as focal length, camera center, etc., that are not considered. Therefore, an
additional camera reÔ¨Ånement is required, and through single point
camera bundle adjustment (reÔ¨Åning only motion), the image can be
properly aligned in 3D space so that that pointing direction, translation, scale all correspond well with 3D to 2D reprojection error.
5. RESULTS
Point clouds have been created and merged from pictures collected
during two times of day from winter and summer seasons (labeled

events A and B) to boost detection capability. The collections occured on MIT campus, near the Kendall Square subway station and
extended a couple of blocks North and South. The improvement in
capability is shown in Fig. 2(a). Meanwhile, geo-registration results
are veriÔ¨Åed on detected images in Fig. 2(b) with GPS positioning
and inspection.

(a) Overall graph with new node

(b) New node with related images in
graph
(a) Detection Rates for Sec. 4.1

Fig. 1. Image relationship graph containing new matched node.
Event H0 data sets were created with images in and around Cambridge and Boston that are similar in content to the reconstructed
area of interest. From the ROC curves, after detection, images sometimes did not register due to constraints in reÔ¨Åning camera parameters. Depending on the image content, resolution, etc., this occurred
roughly 40% of the time. Nevertheless, when images did register,
the point scatter shown reÔ¨Çects that image error was negligible and
very close.
6. CONCLUSIONS AND FUTURE WORK
The proposed algorithm provides the capability to geo-register an
image to an absolute 3D world coordinate system through structure
from motion. The system model is capable of being updated without re-optimizing data every time co-located images are gathered for
training. This allows for discrete time periods to be incrementally
incorporated from disparate luminance conditions. Should the registration procedure fail, we are able to detect images through matching
criterion and build data structures that would aid in image analysis.
Improvements on the image geo-registration problems are ongoing. Pose estimation could be utilized to further reÔ¨Åne the matches
and reject some erroneous matches to improve our matching accuracy. Further analysis of luminance conditions may be simulated so
that time of day information may be potentially discriminated. Finally, boosting performance in terms of registration and detection is
of great importance, and we are currently reviewing techniques to
promote denser point cloud matching.

(b) Camera Registration Error for Sec. 4.2

Fig. 2. Image Positional Geo-registration Performance.

[2]

[3]
7. ACKNOWLEDGEMENTS
We would like to thank Noah Snavely for all the help and advice
he has given us in regard to working with his Photo Tourism structure from motion project. A copy of his code that we have been
working with can be downloaded from the Photo Tourism website
(http://phototour.cs.washington.edu/). In addition we would also like
to thank Peter Cho at Lincoln Labs for his insights into applications
with the Photo Tourism work.
8. REFERENCES

[4]

[5]

[6]

[1] Noah Snavely, Steven M. Seitz, and Richard Szeliski, ‚ÄúPhoto
tourism: Exploring photo collections in 3d,‚Äù in SIGGRAPH

1576

Conference Proceedings, New York, NY, USA, 2006, pp. 835‚Äì
846, ACM Press.
David G. Lowe, ‚ÄúDistinctive image features from scale-invariant
keypoints,‚Äù International Journal of Computer Vision, vol. 60,
pp. 91‚Äì110, 2004.
Sunil Arya, David M. Mount, Nathan S. Netanyahu, Ruth Silverman, and Angela Y. Wu, ‚ÄúAn optimal algorithm for approximate nearest neighbor searching in Ô¨Åxed dimensions,‚Äù in ACMSIAM Symposium on Discrete Algorithms, 1994, pp. 573‚Äì582.
N. Snavely, S. M. Seitz, and R. Szeliski, ‚ÄúModeling the world
from Internet photo collections,‚Äù International Journal of Computer Vision, vol. 80, no. 2, pp. 189‚Äì210, November 2008.
Zachary Sun, Nadya Bliss, and Karl Ni, ‚ÄúA 3-d feature model
for image matching,‚Äù in Proceedings of the IEEE International
Conference on Acoustics, Speech, and Signal Processing, 2010,
pp. 2194‚Äì2197.
Antony Jameson, ‚ÄúSolution of equation ax + xb = c by inversion of an m x m or n x n matrix,‚Äù SIAM Journal of Applied
Mathematics, vol. 16, no. 5, September 1968.

1

pMATLAB PARALLEL MATLAB
LIBRARY
N. Travinin Bliss
J. Kepner
MIT LINCOLN LABORATORY, 244 WOOD STREET,
LEXINGTON, MA 02420 (NT@LL.MIT.EDU)

Abstract
MATLAB¬Æ has emerged as one of the languages most commonly used by scientists and engineers for technical computing, with approximately one million users worldwide.
The primary benefits of MATLAB are reduced code development time via high levels of abstractions (e.g. first class
multi-dimensional arrays and thousands of built in functions), interpretive, interactive programming, and powerful
mathematical graphics. The compute intensive nature of
technical computing means that many MATLAB users have
codes that can significantly benefit from the increased performance offered by parallel computing. pMatlab provides
this capability by implementing parallel global array semantics using standard operator overloading techniques. The
core data structure in pMatlab is a distributed numerical array
whose distribution onto multiple processors is specified
with a ‚Äúmap‚Äù construct. Communication operations between
distributed arrays are abstracted away from the user and
pMatlab transparently supports redistribution between any
block-cyclic-overlapped distributions up to four dimensions. pMatlab is built on top of the MatlabMPI communication library and runs on any combination of heterogeneous
systems that support MATLAB, which includes Windows,
Linux, MacOS X, and SunOS. This paper describes the
overall design and architecture of the pMatlab implementation. Performance is validated by implementing the HPC
Challenge benchmark suite and comparing pMatlab performance with the equivalent C+MPI codes. These results
indicate that pMatlab can often achieve comparable performance to C+MPI, usually at one tenth the code size.
Finally, we present implementation data collected from a
sample of real pMatlab applications drawn from the
approximately one hundred users at MIT Lincoln Laboratory. These data indicate that users are typically able to go
from a serial code to an efficient pMatlab code in about 3
hours while changing less than 1% of their code.
Key words: parallel computing, parallel programming models, parallel MATLAB, HPC challenge

The International Journal of High Performance Computing Applications,
Volume 21, No. 3, Fall 2007, pp. 336‚Äì359
DOI: 10.1177/1094342007078446
¬© 2007 SAGE Publications

336

COMPUTING APPLICATIONS

Introduction

MATLAB¬Æ1 has emerged as one of the predominant languages of technical computing. Its popularity for data
analysis, simulation, and modeling is largely due to the
expressiveness of the language. Additionally, MATLAB
provides its users with powerful graphics that allow visualization of multi-dimensional datasets. The users of
MATLAB tend to be engineers and scientists. High-level
languages allow them to concentrate on their core competency and spend less effort on implementation details. It is
common for scientists and engineers to test the validity of
data processing algorithms or physical simulations by
employing larger data sets, higher resolution models, or a
broader range of input parameters. This need for greater
fidelity causes the execution times to reach hours or even
days. Thus, a parallel capability that provides good speed
up without sacrificing the ease of programming is highly
beneficial. pMatlab seeks to provide this capability by
implementing parallel global array semantics (PGAS2;
see Figure 1) using operator overloading techniques. See
Sections 1.1 and 2.2 for a discussion of pMatlab PGAS
support. pMatlab (www.ll.mit.edu/pMatlab) follows a
SPMD (single program multiple data) model of computation, i.e. every processor executes the same program,
however, different processors work on different data.
The core data structures in pMatlab are distributed
arrays and maps, which will be discussed in greater detail
later in the paper. These data structures and the syntax for
creating them are illustrated in the pMatlab code fragment
(see Figure 2) of the STREAM benchmark (McCalpin
2005). STREAM is a simple, embarrassingly parallel
code that uses basic vector operations, such as scale and
add, to measure main memory bandwidth. Distributed
arrays allow the serial STREAM program to be quickly
transformed into a parallel program by simply adding a
‚Äúmap‚Äù object to selected arrays. The map describes how
the distributed array is to be broken up among multiple
processors. Additionally, pMatlab also abstracts the communication layer from the application developer. While
writing a parallel MATLAB program with pMatlab, the
user does not have to worry about parallel programming
concepts such as deadlocks and synchronization.
This paper describes the design, implementation and
performance results of the pMatlab library used to create
the constructs shown in Figure 2: the rest of this section
highlights related work and different approaches to
developing a parallel MATLAB capability. Section 2
addresses the details of the pMatlab design. Section 3
describes the implementation of the pMatlab library. Sec*

This work is sponsored by the Department of the Air Force under Air
Force contract FA8721-05-C-0002. Opinions, interpretations, conclusions, and recommendations are those of the author and are not necessarily endorsed by the United States Government.

tion 4 presents the HPC Challenge implementations and
benchmark results. Section 5 presents results from real
applications. Section 6 presents our conclusions.
1.1

Fig. 1 Parallel global array semantics (PGAS). The
top half of the figure illustrates pure PGAS. Matrix A is
distributed among Np processors. Element i,j is referenced on all the processors. In pure PGAS, the index i,j
is a global index which references the same element
on all processors (on Processor 1 that element is local,
on all other processors it is remote). The lower half of
the figure illustrates fragmented PGAS. Here, each
processor references element i,j local to the processor, thus each processor references a different element in the global matrix. pMatlab supports both pure
and fragmented PGAS.

Related Work

Parallel MATLAB has been an active area of research for a
number of years and many different approaches have
been developed (see Choy 2003 for a comprehensive survey). These different approaches can be roughly divided
into three categories: message passing, client/server and
global arrays.
The message passing approach (CMTM; Kepner and
Ahalt 2004) requires the user to explicitly send messages
within the code. These approaches often implement a
variant of the Message Passing Interface (MPI) standard
(http://www.mpi-forum.org/). Message passing allows any
processor to directly communicate with any other processor and provides the minimum required functionality to
implement any parallel program. Users that are already
familiar with MPI find these approaches powerful. However, the learning curve is steep for the typical user
because explicit message passing approaches significantly lower the level of abstraction and require users to
deal directly with deadlocks, synchronization, and other
low level parallel programming concepts. In addition, the
impact on code size is significant. Serial programs converted to parallel programs with MPI typically increase
in size by 25% to 50%; in contrast, OpenMP and PGAS
approaches typically only increase the code size by ~5%
(Funk et al. 2005). In spite of these difficulties, a message
passing capability is a requirement for both the client/
server and global arrays approaches. Furthermore, message
passing is often the most efficient way to implement a

Fig. 2 STREAM benchmark code highlights. The first three lines set the various constants required by the program
such as the number of processors and the size of the row vector. The next line creates a map, which will cause the
second dimension of a distributed array to be broken up equally among all the processors. The next three lines use
this map to create three row vectors. The last line performs the basic STREAM triad arithmetic operations in parallel.
No communication is required in this example because A, B and C are all mapped the same.

PMATLAB PARALLEL MATLAB LIBRARY

337

program and there are certain programs with complex communication patterns that can only be implemented with
direct message passing. Thus, any complete parallel solution must provide a mechanism for accessing the underlying
messaging layer. Among the available MATLAB message
passing implementations MatlabMPI (Kepner and Ahalt
2004; www.ll.mit.edu/MatlabMPI) is currently the most
popular implementation with thousands of users worldwide (see Section 3.4 for a more detailed discussion on
MatlabMPI). More recently, the incorporation of MPI
into MathWorks‚Äô Distributed Computing Toolbox (DCT;
Dean et al. 2005; MathWorks Inc. 2005) makes message
passing available to a much broader range of users.
Client/server approaches (RTExpress; Morrow and
van de Geijn 1998; Choy and Edelman 2005; Dean et al.
2005) use MATLAB as the user‚Äôs front-end to a distributed library. For example, Star-P (Choy and Edelman
2005) keeps the distributed arrays on a parallel server,
which calls the necessary routines from parallel libraries
such as ScaLAPACK and FFTW. These approaches often
provide the best performance once the data are transferred to the server. However, these approaches are limited to those functions that have been specifically linked
to a parallel library and require the users to install additional libraries. Another potential disadvantage to these
solutions is that backend libraries often require specific data
distributions for their algorithms. For example, ScaLAPACK requires that the arrays be distributed in a 2-D
block-cyclic distribution (see Section 3.2, Maps). If the
parallel MATLAB library does not support this type of distribution, extra communication overhead is incurred when
redistributing the data for submission to the ScaLAPACK
routine. We have included DCT in this category, although
in this instance the back-end server is MATLAB running
on each processor and the user is responsible for breaking up the calculation into embarrassingly parallel tasks
that can be independently scheduled onto the workers.
pMatlab falls into the third category, the global arrays
approach. Star-P (Choy and Edelman 2005), Falcon (Falcon), and MATLAB hierarchical array extensions (Bikshandi et al. 2006) also fall into this category. These
approaches provide a mechanism for creating arrays,
which are distributed across multiple processors. Global
arrays have a long history in other languages, for example Fortran (Koelbel 1994; Numrich and Reid 1998) and
C (El-Ghazawi et al. 2005), as well as in many C++ libraries such as POOMA (Cummings et al. 1998), GA Toolkit
(Nieplocha et al. 2002), PVL (Lebak et al. 2005) and Parallel VSIPL++ (Lebak et al. 2005). The global array
approach allows the user to view a distributed object as a
single entity, as opposed to multiple pieces as is the case
with message passing. This approach allows operations on
the array as a whole or on local parts of the array. Additionally, these libraries are compatible with MPI and are

338

COMPUTING APPLICATIONS

amenable to hybrid shared/distributed memory implementations. Parallel VSIPL++ is implemented for C++.
The GA toolkit is implemented for a number of languages including Fortran, C, and C++.
pMatlab supports both pure PGAS and fragmented
PGAS programming models (see Figure 1). The pure PGAS
model presents an entirely global view of a distributed
array. Specifically, once created with an appropriate map
object, distributed arrays are treated the same as nondistributed ones. When using this programming model,
the user never accesses the local part of the array and all
operations (such as matrix multiplies, FFTs, convolutions,
etc.) are performed on the global structure. The benefits
of pure PGAS are ease of programming and the highest
level of abstraction. The drawbacks include the need to
implement parallel versions of serial operations and library
performance overhead.
Fragmented PGAS maintains a high level of abstraction but allows access to local parts of the arrays. Specifically, a distributed array is created in the same manner
as in pure PGAS, however, the operations can be performed
on just the local part of the array. Later, the global structure
can be updated with locally computed results. This allows
greater flexibility. Additionally, this approach does not
require function coverage or implementation of parallel
versions of all existing serial functions. Furthermore, fragmented PGAS programs often achieve better performance
by eliminating the library overhead on local computations.
pMatlab is a unique parallel MATLAB implementation
for a number of reasons. pMatlab supports both pure and
fragmented PGAS programming models, and allows combining PGAS with direct message passing for optimized
performance. Both fragmented and pure PGAS programming models use the same distributed array data structure, allowing the conversion between the two. pMatlab
provides more control over data distribution than any
PGAS or client/server library. While pMatlab does use
message passing in the library routines, a typical user
does not have to explicitly incorporate messages into the
code. Currently, pMatlab does not support one-sided communication due to the limitations imposed on pMatlab
by the underlying communication library. Nothing in the
implementation of pMatlab explicitly prevents this functionality. It is possible to envision replacing the current
communication layer with one that would support onesided communication in the future. pMatlab supports
embarrassingly parallel computation, but is not limited to it.
pMatlab does not link in any external libraries, nor does
it compile into an executable. Our library is implemented
entirely in MATLAB, which sets pMatlab apart from client/server approaches. This significantly reduces the
size of the library. Additionally, it has allowed pMatlab
to become the most complete implementation of PGAS
available in any language, since it is the only PGAS

implementation that supports distributions and redistributions of up to four dimensional arrays distributed with
any combination of block, cyclic, and block-cyclic distributions.
2 pMatlab Interface and Architecture
Design
The primary challenge in implementing a parallel computation library is how to balance the conflicting goals of
ease of use, high performance, and ease of implementation. With respect to pMatlab, we have specifically defined
each of these goals in a measurable way (see Table 1). The
performance metrics are typical of those used throughout
the high performance computing community and primarily look at the computation and memory overhead of
programs written with pMatlab relative to serial pro-

grams written using MATLAB and parallel programs written using C with MPI. The metrics for ease of use and
ease of implementation are derived from the software
engineering community (see Kepner 2004 and Johnson
2004, 2005 and papers therein) and look at code size,
programmer effort, and required programmer expertise.
These metrics are not perfect, but they are useful tools for
measuring progress towards these goals. In the rest of
this section we will discuss the particular choices made
in pMatlab to satisfy these goals.
2.1

Ease of Use

The first step in writing a parallel program is to start with
a functionally correct serial program. The conversion from
serial to parallel requires the users to add new constructs
to their code. pMatlab adopts a separation-of-concerns

Table 1
pMatlab design goals. Metrics were defined for each of the high-level pMatlab design goals: ease of
use, performance, and ease of implementation. These metrics led to specific approaches for
addressing the goals in a measurable way.
Goal

Ease of use

Metrics

‚Äì Time for a user to produce a well performing parallel code from a serial code.
‚Äì Fraction of serial code that had to be modified.
‚Äì Expertise required to achieve good performance.

Approach

‚Äì Separate algorithm development from mapping onto a parallel architecture.
‚Äì Abstract message passing away from the user.
‚Äì Ensure that simple (embarrassingly) parallel programs are simple to express.
‚Äì Provide a simple mechanism for globally turning pMatlab constructs on and off.
‚Äì Ensure backward compatibility with serial MATLAB.
‚Äì Provide a well-defined and repeatable process for migrating from serial to parallel code.

Goal

High performance

Metrics

‚Äì Execution time and memory overhead as compared with serial MATLAB, the underlying MatlabMPI
communication library and C+MPI benchmarks.

Approach

‚Äì Use underlying serial MATLAB routines wherever possible (even if it means slightly larger user code).
‚Äì Minimize the use of overloaded functions whose performance depends upon how distributed arrays
are mapped.
‚Äì Provide a simple mechanism for using lower level communication when necessary.

Goal

Ease of implementation

Metrics

‚Äì Time to implement a well performing parallel library.
‚Äì Size of library code.
‚Äì Number of objects.
‚Äì Number of overloaded functions.
‚Äì Functional and performance test coverage.

Approach

‚Äì Utilize a layered design that separates math and communication.
‚Äì Leverage well-understood PGAS and data redistribution constructs.
‚Äì Minimize the use of overloaded functions.
‚Äì Develop a ‚Äúpure‚Äù MATLAB implementation to minimize code size and maximize portability.

PMATLAB PARALLEL MATLAB LIBRARY

339

Fig. 3 Anatomy of a map. A map for a numerical array
is an assignment of blocks of data to processing elements. It consists of a grid specification (in this case a
2 √ó 2 arrangement), a distribution (in this case {}
implies that the default block distribution should be
used), and a processor list (in this case the array is
mapped to processors 0, 1, 2, and 3).

approach to this process which seeks to make program
correctness and mapping a program to a parallel architecture orthogonal. A serial program is made parallel by adding maps to arrays. Maps only contain information about
how an array is broken up onto multiple processors and
the addition of a map does not change the functional correctness of a program. A map (see Figure 3) is composed
of a grid specifying how each dimension is partitioned, a
distribution that selects either a block, cyclic or blockcyclic partitioning, and a list of processor ids that defines
which processors actually hold the data.
The next step in writing a parallel program is implementing communications. Perhaps the largest benefit of
PGAS is the ability to abstract complex message pass-

ing away from the user. More specifically, redistribution
between any two distributed arrays in pMatlab is accomplished with the subscripted assignment (subsasgn)
operator (‚Äú=‚Äù with an indexed left hand side). In the
STREAM benchmark example (see Figure 2) the ‚Äú=‚Äù
operator was used in the statement: A(:,:) = B + s*C,
but since the arrays A, B and C all have the same map, no
communication was required. The overloaded subscripted assignment operator in pMatlab figures this out
and correctly performs a simple assignment of the local
data on the right hand side to the local data on left hand
side. A more complex example is the HPC Challenge
FFT benchmark (see Figure 4). This benchmark computes
the fast Fourier transform of a large 1-D vector. The
standard parallel algorithm for this benchmark is to transform the 1-D vector into a row distributed matrix, FFT
the rows of the matrix, multiply by a set of weights,
redistribute into a column distributed matrix, and FFT the
columns. A key step in the process is the redistribution
which is performed by the statement: Z(:,:) = X, which
2
determines and executes the Np messages that need to be
sent to complete this operation. (Note: pMatlab maps also
allow this operation to be performed using a pipeline by
using different processor sets in the maps; this capability
is discussed further in Section 3.2.)
PGAS enables complex data movements to be expressed
compactly without making parallelism a burden to code.
For example, removing the maps from either the STREAM
or FFT examples returns the program to a valid serial program that simply uses standard built-in operations. This is
a direct result of the orthogonality of mapping and functionality, and allows the pMatlab library to be ‚Äúturned off‚Äù
by simply setting all the maps equal to the scalar value of

Fig. 4 FFT benchmark code highlights. The first two lines set the various constants required by the program such
as the number of processors and the size of the matrix. The next two lines create two map objects for breaking the
matrix up into rows and into columns. The next two lines use the maps to create two matrices. The next four lines
FFT the rows, multiply by a set of local pre-computed weights, redistribute the data (using the ‚Äú=‚Äù operator) into the
matrix broken up by columns, and then perform the FFT on the columns.

340

COMPUTING APPLICATIONS

Fig. 5 The four step serial to parallel process. Step 1 adds distributed matrices to the serial program, then assigns
all the maps a value of 1 and runs with Np = 1 on the local machine. Step 2 turns the maps on and runs the program
again with Np = 1. Step 3 runs with Np > 1 on the local machine. Step 4 runs with Np > 1 on multiple machines. Debugging should always be performed at the lowest numbered step where a problem occurs.

one. This feature exploits a side effect of MATLAB constructors (e.g. zeros and rand), which ignore a trailing
argument equal to one. This ability to turn the library on
and off is a key debugging feature and allows users to
determine whether the bugs are from problems in their
serial code or due to their use of pMatlab constructs.
All of these steps: making the code parallel, managing
the communication and debugging, need to be directly
supported by the library. Our experience with many
pMatlab users has resulted in a standardized and repeatable process (see Figure 5) for quickly going from a serial
code to a well-performing parallel code. This process is
very important, as the natural tendency of new pMatlab
users is to add parallel functions and immediately
attempt to run large problems on a large number of processors.
The four step process begins by adding distributed
matrices to the serial program, but then assigning all
the maps to a value of one and verifying the program with
Np = 1 on the local machine. The second step is to turn
the maps on and to run the program again with Np = 1,
which will verify that the pMatlab overloading constructs
are working properly. It is also important to look at the
relative performance of the first and second steps, as this
will indicate if any unforeseen overheads are incurred by
using the pMatlab constructs. The third step is to run with
Np > 1 on the local machine, which will verify that the
pMatlab communication constructs are working properly. The fourth and final step is to run with Np > 1 on
multiple machines, which validates that the remote communication is working properly. Only after these four
steps have been performed it is worthwhile to attempt to
run large problems on many processors. In addition, it is
important to always debug problems at the lowest numbered step.
2.2

High Performance

The primary goal of using a parallel computer is to
improve runtime performance. The first step in achieving
high performance is to minimize the overhead of using

pMatlab constructs as compared with their serial equivalents. The previous examples (Figures 2 and 4) show the
ideal ‚Äúpure‚Äù PGAS case when all the required functions
have been overloaded to work well with pMatlab distributed arrays. It is impractical (and unnecessary) to provide
optimized implementations of the approximately 8000
built-in functions for every combination of array distributions. Instead, we adopt a coding style that uses some
fragmented PGAS constructs (see Section 1.1 for definition of fragmented PGAS). This style is less elegant but
provides strict guarantees on performance. More specifically, distributed arrays are used as little as possible and
only when interprocessor communication is required.
Figure 6 shows examples of the STREAM and FFT
benchmarks written using fragmented PGAS constructs
that minimize the use of overloaded functions by employing the local and put_local functions (see Section 3.5). The local function extracts the local part of
the distributed array and returns a regular MATLAB array
that will work with any serial MATLAB function. The
put_local function replaces the local part of a distributed array with a regular serial MATLAB array. Thus, in the
STREAM and FFT examples the key expression: Alocal
= Blocal + s*Clocal, and fft(local(X),[],2)
are guaranteed to have the same performance as the equivalent serial function calls and eliminate the need for
pMatlab to overload +, * and fft. In addition to providing a local performance guarantee this style of coding
minimizes the potential for ‚Äúaccidental‚Äù communication,
which is easy to do with the subscripted assignment operator. This style of coding has proven to be very effective
and most users are able to adapt their code to this style
with minimum effort. In support of this style, the pMatlab library also provides serial equivalents of the local
and put_local functions so that the code will still
work if parallel arrays are turned off. The put_local
operation operates exclusively on the data local to the
processor. Because of this, the fact that pMatlab follows
a SPMD model of programming, and the fact that local
MATLABs are single threaded, pMatlab avoids race conditions.

PMATLAB PARALLEL MATLAB LIBRARY

341

Fig. 6 Optimized STREAM and FFT code highlights. Programs have been rewritten to minimize the number of overloaded functions required by using the local and put_local functions. These programs are guaranteed to provide
the same local performance as their serial equivalents.

The power of PGAS is its ability to hide underlying
communication from the user and eliminate the need for
writing lengthy and complex message passing code. Unfortunately, PGAS constructs are not appropriate for all circumstances. There are communication patterns that simply
would be more efficient if direct message passing can be
employed. Thus, it is important to have mechanisms that
allow PGAS and the underlying communication constructs to interact easily. pMatlab provides this ability by
allowing the user to directly access the underlying MatlabMPI library and its data structures. At any time in the
program the user, if he or she so desires, can choose to
send messages directly with MatlabMPI. In fact, we have
found that PGAS and message passing work very well
together since the PGAS constructs can still be used to
quickly figure out which data to send and where to
send it.
Several of the HPC Challenge benchmarks fall into the
class of codes that do best by allowing some use of direct
message passing. In the case of the FFT code, we have
used a special function called transpose_grid (see
Figure 6) that directly uses MatlabMPI messaging to
optimally perform the all-to-all communication for going
from a row-distributed matrix to a column-distributed
matrix. This function is able to use memory more efficiently and to optimize the order in which messages are
sent and received. The RandomAccess benchmark (see
Section 4.3) requires that all processors are able to randomly communicate with all other processors and is a
more explicit example of using messaging and PGAS
together. The HPL Top500 benchmark (see Section 4.4)
requires that one processor be able to broadcast to a subset of all the other processors, which is also most easily
dealt with using direct message passing.

342

COMPUTING APPLICATIONS

2.3

Ease of Implementation

The ease of use and high performance goals are well
understood by the HPC community. Unfortunately, implementing these goals in a middleware library often proves to
be quite costly. A typical PGAS C++ library can be
50,000 lines of code and requires several programmer-years
to implement. pMatlab has adopted several strategies to
reduce implementation costs. The common theme among
these strategies is finding the minimum set of features that
will still allow users to write well performing programs.
One of the key choices in implementing a PGAS library
is which data distributions (see Section 3.2) to support.
At one extreme it can be argued that most users are satisfied by 1-D block distributions. At the other extreme, one
can find applications that require truly arbitrary distributions of array indices to processors. pMatlab has chosen
to support all 4-D block-cyclic distributions with overlap
because the problem of redistribution between any two
such distributions (see Section 3.3) has been solved a
number of times by different parallel computing technologies. This allows pMatlab users to create four-dimensional arrays with all four dimensions distributed.
The pMatlab subsasgn (subscripted assignment)
operator supports data redistribution between arrays. The
next question is what other functions to support and for
which distributions? Table 2 shows an enumeration of
different levels of PGAS support. The ability to work
with the local part of a distributed array and its indices
has also been demonstrated repeatedly. The big challenge
is overloading all mathematical functions in a library to
work well with every combination of input distributions.
As discussed in Section 2.2, this capability is extremely
difficult to implement and is not entirely necessary if
users are willing to tolerate the slightly less elegant coding style associated with fragmented PGAS. Thus, pMatlab

Table 2
Lebak levels. Levels of parallel support for data and functions. Note: Support for data distribution
is assumed to include support for overlap in any distributed dimension. Data4/Op1 has been
successfully implemented many times. Data1/Op2 may be possible but has not yet been
demonstrated.
Data level

Description of support

Data0

Distribution of data is not supported (not a parallel implementation)

Data1

One dimension of data may be block distributed

Data2

Two dimensions of data may be block distributed

Data3

Any and all dimensions of data may be block distributed

Data4

Any and all dimensions of data may be block or cyclically distributed.

Operations level

Description of support

Op0

No distributed operations supported (not a parallel implementation)

Op1

Distributed assignment, get, and put operations, and support for obtaining data and indices
of local data from a distributed object.

Op2

Distributed operation support (the implementation must state which operations those are)

provides a rich set of data distributions, but a relatively
modest set of overloaded functions, which are mainly
focused on array construction functions, array index support functions, and the various element-wise operations
(+,‚Äì,.*,./, ‚Ä¶).
The final implementation choice was to implement
pMatlab purely in MATLAB without relying on binding to
other languages. This has minimized code size and maximized portability. For example, pMatlab is the most
complete implementation of PGAS, but it is only about
3000 lines of code and has introduced only two new
objects (maps and distributed arrays). pMatlab also runs
on any combination of heterogeneous systems that support MATLAB, which includes Windows, Linux, MacOS
X, and SunOS.
3

pMatlab Implementation

This section discusses the implementation details of the
pMatlab library. The library is designed and implemented
at MIT Lincoln Laboratory and builds upon concepts from
the Parallel Vector Library (PVL; Lebak et al. 2005), StarP (Choy and Edelman 2005), and uses MatlabMPI (Kepner and Ahalt 2004) as the communication layer. Figure 7
illustrates the layered architecture of the parallel library.
In the layered architecture, the pMatlab library implements distributed constructs, such as distributed matrices and higher dimensional arrays. In addition, pMatlab
provides parallel implementations of a select number of
functions such as redistribution, fast Fourier transform
(FFT), and matrix multiplication. However, it is usually
simpler for a user to create a parallel implementation of a

function focused on his/her particular data sizes and data
distributions of interests, than to provide generic parallel
implementations of functions which give good performance for all data distributions and data sizes.
The pMatlab library uses the parallelism through polymorphism approach as discussed by Choy and Edelman
(2005). Monomorphic languages require that each variable is of only one type; on the other hand in polymorphic
languages variables can be of different types and polymorphic functions can operate on different types of variables
(Abadi et al. 1991). The concept of polymorphism is inherent in the MATLAB language ‚Äì variable types do not have
to be defined, variable types can change during the execution of the program, and many functions operate on a
variety of data types such as double, single, complex,
etc.
In pMatlab, as in Star-P, this concept is taken one step
further. The polymorphism is exploited by introducing
the map object. Map objects belong to a pMatlab class
map and are created by specifying the grid description,
distribution description, and the processor list as discussed in Section 2.1 (see Figure 3). The map object can
then be passed to a MATLAB constructor, such as rand,
zeros, or ones. The constructors are overloaded and
when a map object is passed into a constructor, the library
creates a variable of type dmat, or a distributed array. A
PITFALLS structure (see Section 3.3) is created when
each dmat object is constructed. A PITFALLS is a mathematical representation of the data distribution information. pMatlab supports numerical arrays of up to four
dimensions of different numerical data types and allows
creation of distributed sparse matrices.

PMATLAB PARALLEL MATLAB LIBRARY

343

Fig. 7 Layered architecture. The pMatlab library implements distributed constructs, such as vectors, matrices, and
multi-dimensional arrays and parallel algorithms that operate on those constructs, such as redistribution, fast Fourier transform (FFT), and matrix multiplication.

As discussed previously, a subset of functions, such as
plus, minus, fft, mtimes, and all element-wise operations are overloaded to operate on dmat objects. When
using a pure PGAS programming model and an overloaded function, the dmat object can be treated as a regular array. Functions that operate only on the local part of
the dmat structure (element-wise operations) simply perform the operations requested on the dmat.local array,
which is a standard MATLAB numerical type specified at
array creation. Functions that require communication,
such as redistribution (or subsasgn in MATLAB syntax)
use MatlabMPI as the communication layer. The set of
functions that we support is highly influenced by the
needs of our users. New functionality is added to the
library based on what is the highest priority for the users
and their projects.
Let us return to the pMatlab FFT code in Figure 4.
Lines 3 and 4 define two pMatlab map objects: Xmap
and Zmap. The user defines maps to specify how and
where the numerical arrays in the program are mapped.
In this example all available processors are used (numbered sequentially from 0 to Np ‚Äì 1). Distributed arrays
are created using the standard MATLAB array constructors: zeros(), rand(), and ones(). The outputs of
the overloaded constructors are dmats, or distributed
arrays. Lines 5 and 6 in Figure 4 create two distributed
complex matrices split up among Np processors. Xmap
indicates that the matrix should be distributed row-wise
with P/Np rows per processor, whereas Zmap defines a
column-wise distribution with Q/Np columns per processor. If a dimension is not evenly divisible by Np, pMatlab
figures this out and shorts the last processor. Line 7 calls
the overloaded FFT function on the distributed array X
and returns the result into an array with same map as the

344

COMPUTING APPLICATIONS

input. Line 9 uses the overloaded subsasgn (subscripted
assignment) operator, which performs an all-to-all communication, which results in Z having the same data as
X, while distributing this data according to the distribution defined in Zmap.
Since all functions supported in pMatlab are implemented in pure MATLAB, the pMatlab library maintains
the portability of MatlabMPI. pMatlab can run anywhere
MATLAB runs, given that there exists a common file system, a constraint imposed on pMatlab by MatlabMPI. A
further benefit of the layered architecture of pMatlab is
that any other communication library could be substituted for MatlabMPI (Section 3.4).
3.1

pMatlab Execution

All pMatlab code resides within a generic code framework
(see Figure 8) for initializing pMatlab (pMatlab_
Init), determining the number of processors the program is being run on (pMATLAB.comm_size), determining the rank of the local processor (pMATLAB.my_
rank), and finalizing the pMatlab library when the computation is complete (pMatlab_Finalize). pMatlab
uses the single program multiple data (SPMD) execution
model. The user runs a pMatlab program by calling the
MatlabMPI MPI_Run command to launch and initialize
the multiple instances of MATLAB required to run in parallel. Figure 8 shows an example RUN.m script using
MPI_Run to launch four copies of the pFFT.m script.
3.2

Maps and Distributions

The concept of using maps to describe array distributions has a long history. The ideas for pMatlab maps are

Fig. 8 pMatlab execution framework. A pMatlab program (pFFT.m) is launched using the MPI_Run command shown
in the RUN.m file, which sets the number of processors and the precise machines to run on. MPI_Run starts Np
instances of MATLAB each with a different rank. Within the pMatlab program the pMatlab environment is initialized
and the number of processors and local rank can be obtained. The program is completed with the
pMatlab_Finalize command.

principally drawn from the High Performance Fortran
(HPF) community (Loveman 1993; Zosel 1993), MIT Lincoln Laboratory Space-Time Adaptive Processing Library
(STAPL; DeLuca et al. 1997), and Parallel Vector Library
(PVL; Lebak et al. 2005). A map for a numerical array
defines how and where the array is distributed (Figure 3).
PVL also supports task parallelism with explicit maps for
modules of computation. pMatlab explicitly only supports data parallelism; however, implicit task parallelism
can be implemented through careful mapping of data
arrays.
The pMatlab map construct is defined by three components: (1) grid description, (2) distribution description,
and (3) processor list. The grid description together with
the processor list describes where the data object is distributed, while the distribution describes how the object
is distributed (see Figure 3). pMatlab supports any combination of block-cyclic distributions up to four dimensions. The syntax for defining these distributions is shown
in Figure 9. Note that the addition of maps to the API represents the only major change to the general MATLAB

syntax. The set of distributions supported by pMatlab
was chosen based on our user needs, past experience with
parallel library development (such as PVL), and availability of redistribution algorithms.
Block distribution is the default distribution, which
can be specified explicitly or by simply passing an empty
distribution specification to the map constructor. Cyclic
and block-cyclic distributions require the user to provide
more information. Distributions can be defined for each
dimension and each dimension could potentially have a
different distribution scheme. Additionally, if only a single distribution is specified and the grid indicates that
more than one dimension is distributed, that distribution
is applied to each dimension.
Some applications, particularly image processing, require
data overlap, or replicating rows or columns of data on
neighboring processors. This capability is also supported
through the map interface. If overlap is necessary, it is
specified as an additional fourth argument. In Figure 9,
the fourth argument indicates that there is zero overlap
between rows and one column overlap between columns.

PMATLAB PARALLEL MATLAB LIBRARY

345

Fig. 9 Supported distributions. Block distribution divides the object evenly among available processors. Cyclic distribution places a single element on each available processor and then repeats. Block-cyclic distribution places the
specified number of elements on each available processor and then repeats.

Overlap can be defined for any dimension and does not
have to be the same across dimensions.
While maps introduce a new construct and potentially
reduce the ease of programming, they have significant
advantages over both message passing approaches and
predefined limited distribution approaches. Specifically,
pMatlab maps are scalable, allow optimal distributions for
different algorithms, and support pipelining.
Maps are scalable in both the size of the data and the
number of processors. Maps allow the user to separate
the task of mapping the application from the task of writing the application. Different sets of maps do not require
changes to be made to the application code. Specifically,
the distribution of the data and the number of processors
can be changed without making any changes to the algorithm. Separating mapping of the program from ensuring
correctness of the program is an important design approach
in pMatlab (see Section 2.1).
Maps make it easy to specify different distributions to
support different algorithms. Optimal or suggested distributions exist for many specific computations. For example, matrix multiply operations are most efficient on

346

COMPUTING APPLICATIONS

processor grids that are transposes of each other. Column
and row wise FFT operations produce linear speed up if the
dimension along which the array is broken up matches the
dimension on which the FFT is performed (see Figure 4).
Maps also allow the user to set up pipelines in the computation, thus supporting implicit task parallelism. For
example, pipelining is a common approach to hiding the
latency of the all-to-all communication required in parallel FFTs. The following slight change in the maps can be
used to set up a pipeline where the first half of the processors perform the first part of the FFT and the second
half perform the second part:
Xmap = map([Np/2 1],{},[0
:Np/2-1]);
% Row map on 1st set of cpus.
Zmap = map([1 Np/2],{},[Np/2:Np-1]);
% Column map on 2nd set of cpus.

When a processor encounters such a map, it first checks
if it has any data to operate on. If the processor doesn‚Äôt
have any data it proceeds to the next line. In the case of
the FFT with the above mappings, the first half of the

processors [rank 0 to (Np/2) ‚Äì 1] will simply perform the
row FFT, send data to the second set of processors, skip
the column FFT, and proceed to process the next set of
data. Likewise, the second set of processors (ranks Np/2
to Np ‚Äì 1) will skip the row FFT, receive data from the
first set of processors, and perform the column FFT.
3.3 Processor Indexed Tagged FAmiLy of
Line Segments (PITFALLS)
Here we discuss an efficient and general technique for
data redistribution. Such techniques are necessary in
order to support PGAS. We chose to use PITFALLS
(Ramaswamy and Banerjee 1995), which is a mathematical representation of the data distribution. Additionally,
(Ramaswamy and Banerjee 1995) provides an algorithm
for determining which pairs of processors need to communicate when redistribution is required and exactly
what data needs to be sent.
A PITFALLS P is defined by the following tuple:
P = (l, r, s, n, d, p)
where
l
r
s
n

is the starting index,
the ending index,
the stride between successive ls,
the number of equally spaced, equally sized
blocks of elements per processor,
d the spacing between ls of successive processor
FALLS and
p the number of processors.
The PITFALLS intersection algorithm is used to determine the necessary messages for redistribution and is
used by the subsasgn operation. The algorithm can be
applied to each dimension of the array, thus allowing
efficient redistribution of arbitrary dimensional arrays.
The algorithm intersects the PITFALLS structures for
each dimension. If intersections in all of the dimensions
are non-empty, the size and the content of the messages
that need to be sent between each processor are computed. The intersection algorithm reduces the number of
necessary computations by exploiting the periodicity of
block-cyclic distributions. Additionally, the PITFALLS
structure is also used within the referencing operation,
subsref, to determine which processor owns which
piece of the distributed array. For a detailed discussion of
the algorithm and its efficiency see (Ramaswamy and
Banerjee 1995). (Note that the PITFALLS tuple can be
derived in a trivial manner from the map definition.)

Table 3
Selected MPI functions provided by MatlabMPI.
Function name

Function description

MPI_Init

Initializes MPI.

MPI_Comm_size

Gets the number of processors
in a communication.

MPI_Comm_rank

Gets the rank of current
processor within a
communicator.

MPI_Send

Sends a message to
a processor.

MPI_Recv

Receives a message from
a processor.

MPI_Finalize

Finalizes MPI.

3.4

MatlabMPI

MatlabMPI (Kepner and Ahalt 2004) is a pure MATLAB
implementation of the most basic MPI functions. The
functions used by pMatlab are listed in Table 3. The communication is done through file I/O (see Figure 10)
through a common file system. The advantage of this
approach is that the library is very small (~300 lines) and
is highly portable. The price for this portability is that
while MatlabMPI performance is comparable to C+MPI
for large messages, its latency for small messages is much
higher (see Figure 11). pMatlab uses MatlabMPI tags to
achieve synchronization. Specifically, each communication increments a global message tag on all processors in
scope. MatlabMPI also provides effectively an infinite
buffer to pMatlab due to its file I/O communication
scheme. Both the tag management by the pMatlab library
and the infinite buffer prevent deadlocks from occurring.
When designing pMatlab, it was important to ensure
that the overhead incurred by the library did not significantly impact performance. From a library perspective,
this means that the performance of the communication
operations using the overloaded subsasgn operator
should be as close as possible to the equivalent MatlabMPI code. Figure 12 shows the performance of an allto-all operation using MatlabMPI, pMatlab subsasgn
and the pMatlab transpose_grid function (discussed in Section 2.2).
From an application perspective minimizing overhead
means using algorithms that use fewer larger messages
instead of many smaller messages. In Section 4 we will see
that the relative performance of the HPC Challenge benchmarks can essentially be derived from the performance of
the underlying MatlabMPI library. STREAM (no communication) delivers good performance. FFT (all-to-all) and
Top500 (broadcast) fall into the large message regime and

PMATLAB PARALLEL MATLAB LIBRARY

347

Fig. 10 MatlabMPI file I/O based communication. MatlabMPI uses file I/O to implement point-to-point communication. The sender writes variables to a buffer file and then writes a lock file. The receiver waits until it sees the lock
file, it then reads in the buffer file.

Fig. 11 MatlabMPI vs. C+MPI. Bandwidth and latency vs. message size. Bandwidth is given as fraction of the peak
underlying link bandwidth. Latency is given in terms of processor cycles. For large messages the performance is
comparable. For small messages the latency of MatlabMPI is much higher.

deliver reasonable performance. RandomAccess is designed
to stress small messages and the relative performance of
pMatlab is much worse. Fortunately, most real pMatlab
programs tend to involve large messages.
3.5

pMatlab Parallel Support Functions

Every PGAS implementation must provide a set of functions for managing and working with global arrays, which

348

COMPUTING APPLICATIONS

have no serial equivalents. The set of pMatlab parallel support functions is shown in Table 4. These functions allow
the user to aggregate data onto one or many processors,
determine which global indices are local to which processors,
and get/put data from/to the local part of a distributed array.
This set of functions is relatively small and require minimal changes to the user‚Äôs original code. These functions,
in addition to map definitions, represent the core of pMatlab syntax. To support the development process discussed

Fig. 12 MatlabMPI vs. pMatlab. Relative all-to-all performance for a pure MatlabMPI implementation, an A(:,:) = B
implementation and a transpose_grid implementation. The x-axis represents size of each matrix relative to node
memory. The y-axis represents throughput relative to peak bandwidth.

Table 4
pMatlab parallel support functions.
Function name

Function description

synch

Synchronize the data in the distributed matrix (used with overlap distributions)

agg

Aggregates the parts of a distributed matrix on the leader processor

agg_all

Aggregates the parts of a distributed matrix on all processors in the communication scope

global_block_range

Returns the ranges of global indices local to the current processor (used with block distributions)

global_block_ranges

Returns the ranges of global indices for all processors in the map of distributed array D on
all processors in communication scope

global_ind

Returns the global indices local to the current processor

global_inds

Returns the global indices for all processors in the map of distributed array D

global_range

Returns the ranges of global indices local to the current processor

global_ranges

Returns the ranges of global indices for all processors in the map of distributed array D

local

Returns the local part of the distributed array

put_local

Assigns new data to the local part of the distributed array

grid

Returns the processor grid onto which the distributed array is mapped

inmap

Checks if a processor is in the map

in Section 2.1, all of these functions have been overloaded to also work on serial MATLAB arrays so that the
code will still work if the pMatlab maps have been turned
off.

4

HPC Challenge Benchmarks

In this section we focus on benchmark results to determine the limits of pMatlab performance. We chose to

PMATLAB PARALLEL MATLAB LIBRARY

349

Fig. 13 HPC Challenge and the memory hierarchy. HPC Challenge benchmarks have been chosen to cover a range
of memory access patterns and stress different parts of the memory hierarchy. Top500 performance is mostly dominated by local matrix multiply operations. RandomAccess is dominated by all-to-all communications of very small
messages. FFT is also dominated by all-to-all communications, but for very large messages. STREAM requires no
communication, is dominated by local vector operations, and stresses local processor to memory bandwidth.

implement a set of standardized benchmarks in order to
measure pMatlab performance against other libraries.
The benchmarks are not exhaustive tests of all of the
library functions but they do test variety of operations.
We are interested in looking at performance from a number
of viewpoints. First, we are interested in the performance
of pMatlab relative to serial MATLAB since this is what
most users care about. Second, we are interested in the
performance of pMatlab relative C+MPI as way of gauging the quality of the implementation and as a guide to
future performance enhancements. We have chosen to
use the HPC Challenge benchmark suite (Luszczek et al.
2005) for this comparison (see Figure 13). HPC Challenge is designed to look at a range of computations that
focus on different parts of the memory hierarchy. In addition, HPC Challenge computations are sufficiently well
defined so that they can be implemented using a variety
of programming models. We will first present the performance results and then discuss each of the benchmarks in more detail.
The four primary HPC Challenge benchmarks (STREAM,
FFT, Top500 and RandomAccess) were implemented using
pMatlab and run on a commodity cluster system (see
Appendix A for a precise description of the hardware).
Both the pMatlab and C+MPI reference implementation
of the benchmarks were run on 1, 2, 4, 8, 16, 32, 64 and 128
processors. At each processor count the largest problem
size was run that would fit in the main memory. The collected data measures the relative compute performance

350

COMPUTING APPLICATIONS

and memory overhead of pMatlab with respect to C+MPI
(see Figure 14). In addition, we will also look at the relative code sizes of the benchmarks as an approximate measure of the complexity of the implementations. The relative
memory, performance and code sizes are summarized in
Table 5.
In general we see that the pMatlab implementations can
run problems that are typically half the size of C+MPI
implementation problem size. This is mostly because
of the need to create temporary arrays when using
high-level expressions. The pMatlab performance ranges
from being comparable to the C+MPI code (FFT and
STREAM), to somewhat slower (Top500), to a lot slower
(RandomAccess). In contrast, the pMatlab code is typically
three to 40 times smaller than the equivalent C+MPI
code. We chose to compare the benchmark implementations based on lines of code since that is a standard metric of comparison for the HPC Challenge benchmarks.
4.1

STREAM

The STREAM benchmark consists of local operations on
distributed vectors. The operations are copy, scale, add,
and scale with addition defined as
a‚Üêb+Œ±c
where a, b and c are double precision vectors of length m,
with the constraint

Fig. 14 pMatlab and C+MPI HPC Challenge performance. pMatlab can run problems that are typically ¬Ω the size of
C+MPI problem size. pMatlab performance varies from being comparable to the C+MPI code (FFT and STREAM), to
somewhat slower (Top500), to a lot slower (RandomAccess). The figure presents performance relative to the 1-processor C+MPI case. The actual performance for the 1-processor C+MPI case can be found in Appendix A.

Table 5a
Maximum problem size relative to the C+MPI single processor case on 128 processors.
Implementation

STREAM

FFT

RandomAccess

HPL(32)

C+MPI/C serial

63.9

72.7

48

32.6

pMatlab/C serial

42.8

21.3

32

9.3

C+MPI/pMatlab

1.5

3.4

1.5

3.5

Table 5b
Benchmark performance relative to the C+MPI single processor case on 128 processors.
Implementation

STREAM

FFT core*

RandomAccess

C+MPI/C serial

62.4

4.6

7.4 √ó 10

‚Äì2

pMatlab/C serial

63.4

4.3

1.6 √ó 10

‚Äì3

C+MPI/pMatlab

1

1

46

HPL(32)
28.2
6.8
4

PMATLAB PARALLEL MATLAB LIBRARY

351

Table 5c
Code size comparisons. Code size is measured in terms of source lines of code (SLOC). The parallel
code sizes of the HPC Challenge C+MPI reference code are taken from the HPC Challenge FAQ.
Implementation

STREAM

FFT

C+MPI

347

pMatlab

119
3

C+MPI/pMatlab

RandomAccess

HPL

787

938

8800

78*

157

190

10

6

40

*Includes code used to create random waves, does not include code for initial and final all-to-all operations. Combined
these should roughly offset each other.

size(a) + size(b) + size(c)
= 24m bytes > ¬º system memory.
The goal of the benchmark is to measure local main
memory bandwidth, so performance is reported in terms
of bytes/sec
‚Äì9

Gigabytes/sec = 10 24m/time.
The operations are embarrassingly parallel and are implemented entirely with the pMatlab fragmented PGAS
approach (see Figure 6).
The maximum problem size of the pMatlab code is one
and a half times smaller than the C+MPI code because of
the need to create intermediate temporary arrays. The need
for temporaries is a side effect of most high-level programming environments. The performance of the pMatlab code is the same as the C+MPI code. This is because
the MATLAB interpreter recognizes the ‚Äúscale and add‚Äù
statement and replaces it with a call to the appropriate
optimized basic linear algebra subroutine (BLAS). The
pMatlab code is approximately three times smaller than
the C+MPI code as a result of the elimination of various
for loops and the use of built in MATLAB functions.
4.2

FFT

The FFT benchmark performs a complex-to-complex
1-D fast Fourier transform (FFT)
Z ‚Üê FFT(z)
where Z and z are m element double precision complex
vectors; with the constraint
size(z + Z) = 32m bytes > ¬º system memory
z input should be in linear ‚Äútime‚Äù order. Z output should
be in standard frequency order. Any necessary reordering
time should be included. Regardless of how many actual

352

COMPUTING APPLICATIONS

operations are performed the performance in Gigaflops is
reported using the standard radix 2 FFT algorithm operations count:
Gigaflops = 10‚Äì9 5m log2(m)/time.
The standard parallel implementation of a 1-D FFT performs two 2-D FFTs with a corner turn, or an all-to-all
redistribution, between the two FFTs (see Figure 4). In
our pMatlab implementation we deviated from the FFT
specification in two ways. First, the input data is initialized using a random selection of cosine and sine
waves, which does not affect performance, but is a significant aid to debugging the code. Second, our implementation uses an ordering scheme that eliminates initial
and final all-to-all communication steps, which is more
consistent with the use of this function for most real
applications and provides a better predictor of 2-D and 3D FFT performance. We have properly removed the time
resulting from the initial and final all-to-all steps in the
C+MPI code so that a legitimate comparison can be
made. The optimized pMatlab code (Figure 6) uses local
arrays and the transpose_grid function with optimized message ordering previously discussed in Section 2.2.
The maximum problem size of the pMatlab code is three
and a half times smaller than the C+MPI code because of
the need to create intermediate temporary arrays. In addition, MATLAB internally uses a ‚Äúsplit‚Äù representation for
complex data types, while the serial FFTW library being
called uses an ‚Äúinterleaved‚Äù representation. The result is
that the data needs to be transformed between these representations, which takes additional memory. On one processor the MATLAB FFT performance is approximately five
times slower than the C code because of the time overhead
required to perform the conversion between complex data
storage formats. As the problem grows, the FFT time
becomes dominated by the time to perform the all-to-all
communication necessary between computation stages.
Since these are primarily large messages, the performance
of pMatlab becomes the same as the C+MPI code at large

numbers of processors. The pMatlab code is approximately ten times smaller than the C+MPI code because of
the use of built-in local FFT calls and the elimination of
MPI messaging code.
4.3

RandomAccess

The RandomAccess benchmark generates a sequence of
random array indices and uses these to update a large
table. Let T be a table of size 2m and let {ai} be a pseudorandom stream of 64-bit integers of length 2m + 2. Then
for each ai, we update the table as follows
T( AND(ai, m ‚Äì 1) ) = XOR( T( AND(ai, m ‚Äì 1), ai)
with the additional constraints that each processor can
buffer no more that 1024 updates and
size(Table) = 8m bytes > ¬º system memory.
The goal of the benchmark is to measure the rate at which
atomic updates can be performed to global memory

Giga Updates Per Second (GUPS)
‚Äì9

= 10 NUPDATE/time.
RandomAccess requires communication patterns that are
significantly more complicated than STREAM or FFT. In
addition, communication is sufficiently fine grained that
there is significant overhead associated with computing
global to local array indexing mappings every time a
global array is accessed. Thus, RandomAccess uses the
pMatlab constructs to determine the global-to-local index
mappings once, but then subsequently uses fragmented
PGAS with direct message passing to perform the appropriate redistributions (see Figure 15). This methodology
allows us to implicitly exploit the fact that the array
redistributions are static. For example, each processor is
able to compute in advance the optimal send order and
optimal receive order of its messages so as to minimize
contention. RandomAccess is a good illustration of how
PGAS and messaging can work together to reduce the
bookkeeping necessary for a parallel program, while still
allowing a complex messaging scheme that is outside of
the traditional PGAS formalism.

Fig. 15 RandomAccess benchmark code highlights. The first two lines set the various constants required by the
program such as the number of processors and the size of the table. The next two lines create a map and a distributed table to be broken up equally among all the processors. The next two lines get the indices of the boundaries of
the table, which are used in the main loop to compute which indices to send to which processors using direct MatlabMPI messages.

PMATLAB PARALLEL MATLAB LIBRARY

353

The maximum problem size of the pMatlab code is one
and a half times smaller than the C+MPI code because of
the need to create intermediate temporary arrays. On one
processor the pMatlab RandomAccess performance is
comparable to the C+MPI code. However, on larger
number of processors the pMatlab code is 45 times
slower than the C+MPI code. This performance difference is because of the large latency of using file I/O for
communicating small messages (see Section 3.4), which
should be eliminated if pMatlab was built on a more traditional MPI implementation such as that used in DCT.
Additionally, if the underlying communication mechanism allowed for one-sided communication, pMatlab
performance would be improved. The pMatlab code is
six times smaller than the C+MPI code.

4.4

High Performance Linpack (Top500)

The High-Performance Linpack (HPL) benchmark solves
a dense linear system Ax = b using LU factorization with
partial pivoting, where b is an n element vector, and A is
an n √ó n double precision matrix with the constraint
size(A) = 8n2 bytes > 1/2 system memory
The LU factorization is the dominant computation step in this
algorithm and is principally made up of repeated matrix
multiplies. The traditional parallel algorithm uses a sophisticated 2-D block-cyclic distribution for the matrix A. This
algorithm has demonstrated very good performance even
on computers with relatively slow networks. The pMatlab
version uses a simpler, but poorer performing algorithm,
using a 1-D block distribution for A (see Figure 16). This

Fig. 16 HPL/Top500 benchmark code highlights. This algorithm uses the simpler 1-D block distribution. The first
four lines derive information about the parallel environment from the pMATLAB global variable and the input distributed matrix A. The core loop of the program performs a local solve of a rectangular LU, broadcasts the results to the
remaining processors to then apply via a matrix multiply.

354

COMPUTING APPLICATIONS

algorithm and its theoretical performance limits are presented in detail in Appendix B. We chose to use the standard distribution for the C/MPI version of the algorithm to
provide comparison between the faster C/MPI algorithm
and the pMatlab algorithm. Since we chose to use a simpler distribution for coding purposes, we are willing to
accept the performance decrease. The pMatlab code uses
distributed arrays to break up the array and keep track of
the various global indices. A key step in the algorithm
requires broadcasting the results to a subset of the other
processors which is best done with a simple MPI multicast command.
The maximum problem size of the pMatlab code is
three and a half times smaller than the C+MPI code
because of the need to create intermediate temporary
arrays. In particular, the lower and upper triangular matrices are returned as full matrices, whereas in the C+MPI
code these can be merged into a single array. The pMatlab code provides a 10-fold speedup on 32 processors,
which is about four times slower than the C+MPI code.
The analysis in Appendix B shows that pMatlab is
achieving the performance limits of the 1-D block algorithm on the system. Improving the network of this hardware should significantly improve the pMatlab code
performance, relative to the C+MPI code. The pMatlab
code is 40 times smaller than the C+MPI code. About a
10-fold part of this improvement is due to the higherlevel abstractions from pMatlab and about a four-fold
part is due to using the simpler algorithm.
4.5

HPC Challenge Performance Summary

Returning to our initial metrics we see that relative to
serial MATLAB all of the pMatlab codes allow problem sizes
to scale linearly with the number of processors. Likewise,
they all experience significant performance improvements
(with the exception of RandomAccess). RandomAccess
performance is limited by the underlying communication
mechanism. Performance would improve if the underlying communication layer supported one-sided communication. Nothing in pMatlab explicitly limits this capability
in the future. Relative to C+MPI the pMatlab problem
sizes are smaller by a factor of two and the performance
of pMatlab on both the STREAM and FFT is comparable. The time for the additional communication in the
C+MPI code are removed to allow for a fair comparison
in performance (see Section 4.2).
One approach to summarizing the performance of the
HPC Challenge benchmarks is shown in Figure 17. The
speedup and relative SLOC for each implementation were
calculated with respect to a serial C/Fortran implementation. In this plot we see that with the exception of Random Access, the C+MPI implementations all fall into the
upper right quadrant of the graph, indicating that they

Fig. 17 Speedup vs. code size. Speedup (relative to
serial C) vs. code size (relative to serial C). The upper
right quadrant is the traditional HPC regime: more coding is required to give more performance and most of
the C+MPI codes fall here. The lower left quadrant is
the traditional regime of serial high-level languages
that produce much smaller codes, but are slower. RandomAccess lies in the lower right and represents algorithms that are simply a poor match to the underlying
hardware and communication mechanism. The upper
left quadrant is where most of the pMatlab implementations are found and represent smaller codes that are
delivering some speedup.

deliver some level of parallel speedup, while requiring
more SLOC than the serial code. As expected the serial
MATLAB implementations do not deliver any speedup,
but do all require fewer SLOC than the serial C/Fortran
code. The pMatlab implementations (except Random
Access) fall into the upper left quadrant of the graph,
delivering parallel speedup while requiring fewer linesof-code.
5

User Results

The true measure of success for any technology is its
effectiveness for real users. Table 6 highlights several
projects that are using pMatlab on the MIT Lincoln Laboratory interactive LLGrid system (Reuther et al. 2004).
The projects are drawn from approximately 100 current
users and are representative of the user base. Of particular
interest are the columns showing the time to parallelize
and what parallelization enables. The time to parallelize
shows both how quickly MATLAB code can be converted
from serial code to parallel code as well as how quickly
the user is able to get the parallel code running on the
LLGrid compute facility. The applications that paralleli-

PMATLAB PARALLEL MATLAB LIBRARY

355

Table 6
Selected pMatlab applications. The first and last columns provide a brief description of the code
and what the parallel version of the code has enabled. The middle column shows estimated time to
write the original serial code and the additional time to parallelize the code with pMatlab and get it
running well on the LLGrid system.
Code description

Serial/parallel dev time (hours)

Parallelization enables more or faster

Missile & Sensor Simulations

2000/8

Higher fidelity radar

First-principles LADAR

1300/1

Speckle image simulations

Analytic TOM Leakage

40/0.4

Parameter space studies

900/0.75

Monte Carlos

Coherent laser propagation

40/1

Run time

Polynomial coefficient approx.

700/8

Faster training algorithm

Ground motion tracker

600/3

Faster & larger data sets

Automatic target recognition

650/40

Target classes & scenarios

Hyper-spectral Image Analysis

960/6

Larger datasets of images

Hercules Metric TOM

zation enables include scenarios in which larger data sets,
more thorough parameter set exploration, and more complex simulations can be considered. While we usually
provide users with some initial guidance regarding algorithm parallelization, the users quickly feel comfortable
writing new parallel code.
Nearly all of these applications involve embarrassingly
parallel problems most similar to the STREAM type of
problem. Interestingly, because MATLAB is such an array
oriented language users find PGAS a very natural way to
express embarrassingly parallel problems. For these types
of applications the coding overhead is much smaller than
message passing. In addition, PGAS naturally decomposes
problems into their largest natural units, which maximizes
the local performance. In contrast a client/server approach
tends to decompose problems into their smallest functional units and incur a higher overhead. The users of
pMatlab find that the functions pMatlab provides are sufficient to parallelize their applications and gain significant performance increases. The users take advantage of
both distributed arrays, which they often use as container
classes, and distributed indices, in situations where distributed ‚Äúfor‚Äù loops or parameter sweep type operations are
required. A majority of the users use fragmented PGAS
and find fragmented PGAS easy to use and intuitive for
their problems.
6

Conclusions

pMatlab is a unique high performance, high productivity
parallel MATLAB library. It combines the productivity
inherent in the MATLAB programming language with global array semantics, allowing MATLAB users to exploit

356

COMPUTING APPLICATIONS

distributed systems with only minor changes to the code.
The underlying communication layer, MatlabMPI, is comparable in performance to C+MPI for large message sizes.
Introduction of maps for numerical arrays allows for separation of ensuring program correctness from mapping
the program to a parallel architecture. The implementation is small (~3000 lines of code). The implementation
of the HPC Challenge benchmark suite using the pMatlab
library allows for comparison with equivalent C+MPI codes.
These results indicate that pMatlab can achieve comparable performance to C+MPI at usually one tenth the code
size. Finally, implementation data collected from a sample of 10 real pMatlab applications indicate that users are
typically able to go from a serial code to a well-performing pMatlab code in about 3 hours while changing less
than 1% of their code.
Acknowledgments
The authors would like to thank a number of individuals
who have contributed to this work: Bob Bond for his
vision and insight throughout this project; Hahn Kim for
his work on the pMatlab library and benchmarking;
Andy Funk for his benchmarking analysis; Albert Reuther for leading the LLgrid project and providing us the
pMatlab user analysis; Cleve Moler and Ryan Haney for
their assistance with the parallel LU algorithm; and Charlie Rader for his assistance with the parallel FFT algorithm. We would like to thank Ken Senne, Dave Martinez,
John Grosh, and Robert Graybill for supporting this
project. Finally, we would like to thank the anonymous
reviewers for their insightful suggestions and contributions.

Author Biographies
Nadya Travinin Bliss is an technical staff member at
MIT Lincoln Laboratory in the Embedded Digital Systems group. Since she joined the Laboratory in 2002, she
has been one of the principal innovators and developers
of the pMapper automated parallelization system and
pMatlab (Parallel Matlab Toolbox). The pMatlab library
is widely used at the laboratory and has been released to
the open source community. The pMapper system, which
was prototyped in MATLAB, is currently being adapted
for real time embedded systems. Her current research
topics include parallel and distributed computing and
intelligent/cognitive algorithms. Nadya received her
Master and Bachelor Degrees in computer science from
Cornell University (1998‚Äì2002).
Jeremy Kepner received a B.A. with distinction in astrophysics from Pomona College (Claremont, CA). After
receiving a DoE Computational Science Graduate Fellowship in 1994 he obtained his Ph.D. from the Department of Astrophysics at Princeton University in 1998 and
then joined MIT Lincoln Laboratory. His research is
focused on the development of advanced libraries for the
application of massively parallel computing to a variety
of data intensive signal processing problems on which he
has published over a dozen articles. Jeremy is the overall
lead of the DARPA HPCS Productivity Team, the technical
lead of the DUSD HPEC Software Initiative, the technical
chairman of the High Performance Embedded Computing
(HPEC) workshop, and the lead software architect of
pMatlab and MatlabMPI.
Appendix A: Benchmark System
All the performance data collected in this paper were
obtained using the LLGrid system at MIT Lincoln Laboratory (Reuther et al. 2004). The system consists of ~150
nodes connected by Gigabit Ethernet. Each node has two
Gigabit Ethernet interfaces: one gigabit interface to the
Lincoln Laboratory LAN (LLAN), and one gigabit intercluster interface. The network switches are connected
directly to the LLAN backbone via fiber. Furthermore to
enhance the communication of the file I/O based communication system each node mounts the local disk drive of
all the other nodes. Each node is configured as follows:
Processors: Dual 3.2 GHz EM-64T Xeon (P4)
Bus: 800 MHz front-side bus
Memory: 6 Gigabyte RAM
Disk: Two 144 GB SCSI hard drives
Main Network: Two Gig-E Intel interfaces
Management Network: 10/100 Ethernet interface
Operating System: Red Hat Linux ES 3

Table A.1
C+MPI single processor HPC Challenge.
Maximum problem size and performance of
the HPC Challenge benchmarks for C+MPI
implementation on a single processor.

Benchmark
STREAM

Maximum
problem
size (GB)

Performance

4.6

2.79 GB/s

FFT

3

0.43 GFlops

RandomAccess

4

2.3 √ó 10‚Äì3 GUPS

4.3

4.04 GFlops

HPL/Top500

Table A.1 provides the actual benchmark values of the
HPC Challenge benchmark suite on the LLGrid system
for the C+MPI single processor case.
Appendix B: 1-D LU Algorithm
Performance Analysis
This appendix presents an analysis of the theoretical performance achievable for an LU factorization using only a
1-D block distribution. Extensive analysis of LU performance has been completed for optimal 2-D block
cyclic distributions (Dongarra, van de Geijn, and Walker
1994), which shows the excellent scalability of this distribution. However, more recently it has become apparent that there is a complexity/performance trade off
associated with using 2-D block cyclic distributions (i.e.
they are a lot harder to program). Specifically, in pMatlab the index computation for 2-D block cyclic LU
implementation creates a significant overhead both in the
amount of coding and computation. Consequently, it is
worth examining the performance of a 1-D block distribution (such as the one used for the pMatlab implementation of Top500) so that we have a clear understanding of
the performance.
First we define the time for an ideal N √ó N LU factorization on P processors
2 3
ideal
T N ( P ) = --- N t calc ‚ÅÑ P
3
where tcalc is the time for one floating point operation on
one processor. Furthermore let r = N/P, so that in terms
of P and r the ideal performance is
2 2 3
( P ) = --- P r t calc .
3

ideal

Tr

We will further restrict ourselves to weak scaling such
that the problem size grows linearly with the number or
processors. In this case, we have the additional constraint

PMATLAB PARALLEL MATLAB LIBRARY

357

r N = c mem

P

‚àë ( N ‚Äì ( k ‚Äì 1 )r )r t

c mem
--------P

r =

or

comm

k=1

where cmem is the number of 8 byte double precision
numbers that will fit on one processor.
Now let us consider the time to perform a parallel LU
factorization using a 1-D block distribution. The algorithm consists of k = 1, ‚Ä¶, P steps and at each step three
operations must be completed before the next step can
begin.
First, a local LU factorization of a (N ‚Äì (k ‚Äì 1)r) x r
matrix is performed on the kth processor

and B(P) = P
P

‚àë ( P ‚Äì k ) ( N ‚Äì ( k ‚Äì 1 )r )rt
k=1

comm

1 2 2
‚Äì1
= --- P r t comm [ P ‚Äì P ]
3

where we have used the summation identity
P

‚àëk

Ô£´ N ‚Äì r Ô£´ k + 2---Ô£∂ Ô£∂ r 2 t .
Ô£≠
Ô£≠
3Ô£∏ Ô£∏ calc

k=1

Second, the result of this local LU factorization is broadcast from the kth processors to P ‚Äì k processors

1 2 2
‚Äì1
= --- P r t comm [ 1 + P ]
2

2

1 3 1 2 1
= --- P + --- P + --- P .
3
2
6

Finally, for each case, we sum the computation and
network terms and normalize to the ideal time. In the
case of an ideal network, B(P) = 1, the ratio of the 1-D
panel algorithm to the ideal time is

B(P ‚Äì k)(N ‚Äì (k ‚Äì 1)r) r tcomm
where tcomm is the average time to send 8 bytes between
two processors (assuming large messages), and B(P) is
the ‚Äúbroadcast‚Äù parameter which is the penalty associated
with sending the same message to many processors. In an
ideal broadcast B(P) = 1 and a processor can send to many
processors in the same time it takes to send to one. In the
worst case B(P) = P. Most networks are somewhere in
between and B(P) = log2(P) is typical. The third and final
step is to apply the local LU factorization to the local part
of the matrix stored on the processor using a lower triangle update and a matrix-matrix multiply operation
3

2

r tcalc + (N ‚Äì (k ‚Äì 1)r)r tcalc.
Next we sum the above steps over k = 1, ‚Ä¶, P and
reformulate in terms of P and r, which yields the total
time required to compute the LU, lower triangle update,
and the matrix multiply
P

‚àë Ô£´Ô£≠ N ‚Äì r Ô£´Ô£≠ k + --3-Ô£∂Ô£∏ Ô£∂Ô£∏ r t
2

2

+ r t calc + ( N ‚Äì ( k ‚Äì 1 )r )r t calc
3

calc

1 2 3
‚Äì1
= --- P r t calc [ 3 + P ]
3
where we have used the summation identity
P

k=1

1
= --- P ( P + 1 ) .
2

Similarly, we sum up the broadcast term for two cases,
B(P) = 1

358

COMPUTING APPLICATIONS

Interestingly, the first computation term goes to a constant value of 3/2 for large P, which indicates the 1-D
block algorithm is always at least 50% less efficient
because of load imbalance. In addition, the second communication term will grow with the square root of P for
large P. However, for typical values of cmem (~229) on a
high performance system with a fast network (tcomm/tcalc ~
8) the constant in front the communication term is quite
small and the overhead resulting from the broadcast does
not become significant until P > 100,000. Thus, for such
a system the 1-D block distribution will scale well.
In the worst case network, B(P) = P, the ratio to the
ideal algorithm is
1 P
1
‚Äì1
--- [ 3 + P ‚Äì1 ] + --- ---------(t
‚ÅÑ t )[ P ‚Äì P ]
2 c mem comm calc
2

2

k=1

‚àëk

1
‚Äì1
P -(t
--- [ 3 + P ‚Äì1 ] + 3
--- --------‚ÅÑ t )[1 + P ] .
2
4 c mem comm calc

which is nearly the same as the best case formula except
that the broadcast term has an additional factor P which
causes it to become significant for much lower values of
P. The system used to obtain the results in the paper (see
Appendix A) is best approximated by this model with
values cmem ~ 227 and tcomm/tcalc ~ 400.
Notes
1 MATLAB¬Æ is a registered trademark of the MathWorks. Reference to commercial products, trade names, trademarks or
manufacturer does not constitute or imply endorsement.

2 Note that the acronym PGAS is used to refer to parallel global
array semantics, partitioned global array semantics, and parallel global address space. We chose to use the meaning that
was most appropriate to our library.

References
Abadi, M., Cardelli, L., Pierce, B., and Plotkin, G. (1991).
Dynamic typing in a statically typed language, ACM
Transactions on Programming Languages and Systems,
13(2): 237‚Äì268, ACM Press.
Bikshandi, G., Guo, J., Hoeflinger, D., Almasi, G., Fraguela,
B., Garzaran, M., Padua, D., and von Praun, C. (2006).
Programming for parallelism and locality with hierarchically tiled arrays, in Proceedings of the Eleventh ACM
SIGPLAN Symposium on Principles and Practice of Parallel Programming (PPoPP06), New York, March.
Choy, R. (2003). Parallel Matlab survey, http://supertech.
lcs.mit.edu/~cly/survey.html
Choy, R. and Edelman, A. (2005). Parallel MATLAB: doing it
right. Proceedings of the IEEE, 93(2).
CMTM Cornell multitask toolbox for MATLAB, http://
www.cs.cornell.edu/Info/People/lnt/multimatlab.html
Cummings, J. C., Crotinger, J. A., Haney, S. W., Humphrey,
W. F., Karmesin, S. R., Reynders, J. V., Smith, S. A., and
Williams, T. J. (1998). Rapid application development
and enhanced code interoperability using the POOMA
framework, in Proceedings of the SIAM Workshop on
Object-Oriented Methods and Code Interoperability in
Scientific and Engineering Computing (OO98), Yorktown
Heights, NY, October.
Dean, L., Grad-Freilich, S., Kepner, J., and Reuther, A. (2005).
Distributed and parallel computing with MATLAB, tutorial
presented at Supercomputing 2005, November 12, Seattle,
WA.
DeLuca, C. M., Heisey, C. W., Bond, R. A., and Daly, J. M.
(1997). A portable object-based parallel library and layered framework for real-time radar signal processing, in
Proceedings of the First Conference on International
Scientific Computing in Object-Oriented Parallel Environments (ISCOPE ‚Äô97), Marina Del Rey, CA, pp. 241‚Äì
248.
Dongarra, J., van de Geijn, R., and Walker, D. (1994). Scalability issues affecting the design of a dense linear algebra
library, Journal of Parallel and Distributed Computing,
22: 523‚Äì537.
El-Ghazawi, T., Carlson, W., Sterling, T., and Yelick, K.
(2005). UPC: Distributed shared memory programming,
John Wiley and Sons, Hobohen, NJ.
Falcon. Falcon project: Fast array language computation, http://
www.csrd.uiuc.edu/falcon/falcon.html
Funk, A., Kepner, J., Basili, V., and Hochstein, L. (2005). A
relative development time productivity metric for HPC
systems, Proceedings of the High Performance Embedded
Computing Workshop (HPEC2005), Lexington, MA, September 20‚Äì22.

Johnson, P. (ed.) (2004). Proceedings of 26th International Conference on Software Engineering (ICSE 2004), Edinburgh,
Scotland, May 23‚Äì28.
Johnson, P. (ed.) (2005). Proceedings of 27th International Conference on Software Engineering (ICSE 2005), St. Louis,
Missouri, May 15‚Äì21.
Kepner, J. (ed.) (2004). Special issue on HPC productivity,
International Journal of High Performance Computing
Applications, 18(4).
Kepner, J. and Ahalt, S. (2004). MatlabMPI, Journal of Parallel and Distributed Computing, 64(8): 997‚Äì1005.
Koelbel, C. (1994). The high performance Fortran handbook,
Cambridge, MA: MIT Press.
Lebak, J., Kepner, J., Hoffmann, H., and Rutledge, E. (2005).
Parallel VSIPL++: An open standard software library for
high-performance parallel signal processing, Proceedings
of the IEEE, 93(2).
Loveman, D. B. (1993). High performance Fortran, Parallel
and Distributed Technology: Systems and Applications,
IEEE, 1(1).
Luszczek, P., Dongarra, J., Koester, D., Rabenseifner, R.,
Lucas, B., Kepner, J., McCalpin, J., Bailey, D., and Takahashi, D. (2005). Introduction to the HPC Challenge
benchmark suite, April, http://repositories.cdlib.org/lbnl/
LBNL-57493/
MathWorks Inc. (2005). Distributed computing toolbox user‚Äôs
guide, http://www.mathworks.com/access/helpdesk/help/
toolbox/distcomp/
McCalpin, J. (2005). STREAM: Sustainable memory bandwidth
in high performance computers, http://www.cs.virginia.
edu/stream/
Morrow, G. and van de Geijn, R. (1998). A parallel linear algebra server for Matlab-like environments, in Proceedings
of Supercomputing 1998, Orlando, FL, November.
Numrich, R. and Reid, J. (1998). Co-array Fortran for parallel
programming, ACM SIGPLAN Fortran Forum, 17(2): 1‚Äì31.
Nieplocha, J., Harrison, R., Krishnan, M., Palmer, B., and Tipparaju, V. (2002). Combining shared and distributed memory models: Evolution and recent advancements of the
Global Array Toolkit, Proceedings POOHL‚Äô2002 Workshop of ICS-2002, New York City.
Ramaswamy, S. and Banerjee, P. (1995). Automatic generation
of efficient array redistribution routines for distributed
memory multicomputers. Proceedings of the Fifth Symposium on the Frontiers of Massively Parallel Computation
(Frontiers ‚Äô95), McClean, VA, February 6‚Äì9.
Reuther, A., Currie, T., Kepner, J., Kim, H., McCabe, A.,
Moore, M., and Travinin, N. (2004). LLgrid: Enabling ondemand grid computing with gridMatlab and pMatlab, in
Proceedings of High Performance Embedded Computing
Workshop (HPEC2004), Lexington, MA 28‚Äì30 September.
RTExpress. Parallel MATLAB development for high performance computing with RTExpress, http://www.rtexpress.com/
Zosel, M. E. (1993). High performance Fortran: an overview,
Compcon Spring ‚Äô93, Digest of Papers, San Francisco,
CA, February 22‚Äì26.

PMATLAB PARALLEL MATLAB LIBRARY

359

2013 IEEE 27th International Symposium on Parallel & Distributed Processing

P-sync: A Photonically Enabled Architecture for
EfÔ¨Åcient Non-local Data Access
David Whelihan‚àó , Jeffrey J. Hughes‚àó , Scott M. Sawyer‚àó , Eric Robinson‚àó , Michael Wolf‚àó ,
Sanjeev Mohindra‚àó , Julie Mullen‚àó , Anna Klein‚àó , Michelle Beard‚àó , Nadya T. Bliss‚àó ,
Johnnie Chan‚Ä† , Robert Hendry‚Ä† , Keren Bergman‚Ä† and Luca P. Carloni‚Ä°
‚àó Massachusetts

Institute of Technology Lincoln Laboratory, Lexington, MA
of Electrical Engineering, Columbia University, New York, NY
‚Ä° Department of Computer Science, Columbia University, New York, NY

‚Ä† Department

Abstract‚ÄîCommunication in multi- and many-core processors
has long been a bottleneck to performance due to the high
cost of long-distance electrical transmission. This difÔ¨Åculty has
been partially remedied by architectural constructs such as
caches and novel interconnect topologies, albeit at a steep cost
in terms of complexity. Unfortunately, even these measures are
rendered ineffective by certain kinds of communication, most
notably scatter and gather operations that exhibit highly nonlocal data access patterns. Much work has gone into examining
how the increased bandwidth density afforded by chip-scale
silicon photonic interconnect technologies affects computing, but
photonics have additional properties that can be leveraged to
greatly accelerate performance and energy efÔ¨Åciency under such
difÔ¨Åcult loads. This paper describes a novel synchronized global
photonic bus and system architecture called P-sync that uses
photonics‚Äô distance independence to greatly improve performance
on many important applications previously limited by electronic
interconnect. The architecture is evaluated in the context of a
non-local yet common application: the distributed Fast Fourier
Transform. We show that it is possible to achieve high efÔ¨Åciency
by tightly balancing computation and communication latency in
P-sync and achieve upwards of a 6x performance increase on
gather patterns, even when bandwidth is equalized.

occur in a wide variety of applications and are difÔ¨Åcult to
implement efÔ¨Åciently on current CMPs. An example of a
scatter is the distribution of program and data from a shared
memory interface to numerous processing elements on one or
more chips prior to program execution, whereas storing results
in the same shared memory at the end of execution represents
a gather operation. The salient property possessed by both
of these patterns is unsynchronized distributed non-local data
access, wherein spatially or logically (address-wise) separate
data must be efÔ¨Åciently co-located or re-distributed.
Both of these patterns are seen in applications that process multi-dimensional data structures mapped into a onedimensional address space. An example is accessing both rows
and columns of a memory-mapped matrix, which is an application bottleneck in Ô¨Åelds such as astronomy, medical imaging,
and intelligence, surveillance, and reconnaissance (ISR). The
distributed access of data which is logically separate creates
great inefÔ¨Åciencies within the memory hierarchy, especially
when the access granularity is small. This inefÔ¨Åciency is
oftentimes the central concern of a developer for applications
targeting a CMP platform. While many solutions exist to deal
with this problem, most require the addition of signiÔ¨Åcant
specialized hardware, such as in [4]. Unfortunately, this additional hardware not only costs silicon area, but also power.
The latter is problematic in embedded and mobile systems and
increasingly in large data centers.
We take a holistic approach to architecture design that
reconsiders application needs in the context of the intrinsic properties of photonics. This paper introduces two new
modes of operation on a photonic waveguide to mitigate the
problem of unsynchronized distributed non-local data access:
the Synchronous Coalesced Accesses (SCA) and the Inverse
Synchronous Coalesced Accesses (SCA‚àí1 ). Both the SCA,
or scatter operation, and the SCA‚àí1 , or gather operation, are
enabled by the novel Photonic Synchronous Coalesced Access
Network (PSCAN). The PSCAN, implemented on a shared
photonic bus topology, was developed to greatly accelerate
non-local accesses by reorganizing data in-Ô¨Çight on a photonic
waveguide.
Further, the novel P-sync architecture, which utilizes the
PSCAN to provide high performance in real applications is
proposed. P-sync is evaluated analytically and experimentally

I. I NTRODUCTION
As Chip Multi-Processors (CMPs) continue to integrate
increasing numbers of cores on a single chip, processor performance and power efÔ¨Åciency become limited by a system‚Äôs
ability to supply data to the cores. Packet-switched grid architectures have emerged as a solution for on-chip networks due
to limitations in wire length and fanout imposed by electronic
technology [1]. Integrated silicon nano-photonics have been
considered for their bandwidth and energy efÔ¨Åciency [2], [3].
In addition to these beneÔ¨Åts, the distance independence and
relative fanout insensitivity of chip-scale photonics allow for
scalable multi-point shared busses and tight global synchronization. The value of these chip-scale photonic innovations
can be observed by measuring their impact on real-world
applications.
Scatter/Gather communication patterns that distribute information to many receivers, or collect them at a single receiver
‚àó This work is sponsored by Defense Advanced Research Projects Agency
(DARPA) under Air Force contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the author and are not
necessarily endorsed by the United States Government. Distribution Statement
A. Approved for public release; distribution is unlimited.

1530-2075/13 $26.00 ¬© 2013 IEEE
DOI 10.1109/IPDPS.2013.56

189

in the context of a difÔ¨Åcult yet common application kernel: the distributed Fast-Fourier Transform (FFT). The FFT
presents many challenges which make it difÔ¨Åcult to efÔ¨Åciently
implement on general-purpose multi-core computing architectures: large multi-dimensional data distribution/retrieval,
synchronization of compute resources, and non-local data
dependencies.
This paper makes the following contributions:
‚Ä¢ The development of a novel photonic communication
mode that excels at non-local data accesses, and a network to support it called PSCAN
‚Ä¢ The creation of a new computer architecture called Psync that utilizes PSCAN to achieve high efÔ¨Åciency by
tightly interleaving data delivery and computation
‚Ä¢ A generalized quantitative analysis of the P-sync architecture performance vs. a wormhole routed electrical mesh
‚Ä¢ Experimental results showing P-sync‚Äôs efÔ¨Åciency on a
common, but highly non-local application kernel: the
parallel FFT.
The paper is organized as follows: Section II surveys previous work in state-of-the-art chip-scale interconnection networks and introduces the FFT as a highly non-local application
kernel. Section III introduces the PSCAN and its transmission
mode: Synchronous Coalesced Access. Section IV describes
the P-sync computer architecture based upon the PSCAN.
Section V is an analytical explanation of the complexities of
the FFT and a performance comparison of a mesh-style CMP
and a P-sync CMP. Section VI presents high-level application
simulations of the full FFT on both of these architectures.
Sections VII and VIII conclude with a discussion followed by
future work.

1D and 2D FFTs can be found in many applications, large 1D
vector FFTs are typically implemented as 2D matrix FFTs to
improve overall performance [7]. Therefore, the optimization
of the 2D FFT is generalizable to the 1D case. This paper
considers speciÔ¨Åcally the 2D FFT matrix operation as a case
study for the challenges of performing a data transpose. This
two-dimensional data structure must be accessed in both the
row and column dimensions, resulting in highly non-local
access patterns.
This non-locality presents design challenges for CMP networks, in which interconnect limitations are a signiÔ¨Åcant driver
of architecture. In this work, architectures utilizing electronic
interconnect are contrasted with a novel photonically interconnected architecture. As the number of on-chip components
increases due to process scaling, electronic communication
architectures have moved away from point-to-point globally
interconnected networks primarily due to wire delay [8].
Though there has been much research into electronic architectures exhibiting constant delay via delay-optimized repeater
insertion [9], this comes at the cost of increased power and
area. Instead, the trend has been for on-chip communication to
minimize or even abandon global wires in favor of network-onchip architectures. Although there is no clear consensus on the
most appropriate on-chip network topology, mesh networks are
commonly employed due to their short but regular interconnect
allowing ease of design and routing [1], [10], [11].
Recent advances in silicon photonics, compatible with
CMOS [12]‚Äì[14], and 3D integration [15] provide for the
opportunity to replace global on-chip electrical interconnects
with a medium providing greater ability to scale while providing lower-power high-bandwidth communications. Architectures based on fully connected [16]‚Äì[18] photonic networks
and those based on scalable electronic equivalents [19]‚Äì[21]
rely heavily on non-optimal arbitration policies or additional
electronic communication networks and buffers to be practical
at optimizing non-local data access. This is primarily due to
the fact that photonic networks are incapable of buffering innetwork messages without incurring an unreasonable opticalelectrical-optical conversion penalty.
In the following sections, a new communication mode and
encompassing architecture will be presented that overcomes
these limitations to achieve high performance on non-local
access patterns by exploiting the properties of silicon photonics.

II. BACKGROUND
Modern general purpose processors are highly optimized
to deal with locality in one-dimensional data because data is
physically mapped to a linear address space. Unfortunately,
they perform comparatively poorly when dealing with locality
over multiple dimensions, which results in a strided access
pattern. A good example of this problem is accessing Ô¨Årst
the rows, then the columns, of a matrix. In this case, it
is easy for a processor to access successive values on the
major dimension, stored linearly in memory, but is much more
difÔ¨Åcult to access successive values stored along the nonmajor dimension. Common solutions to avoiding non-locality
on modern processors, such as caching, do little to help this
strided access [5], as caches also have a preferred access
dimension and limited size. This problem is important enough
that an entire class of architectures, Graphics Processing Units
(GPUs) [6], include hardware for optimizing this type of data
access. While GPUs are able to more efÔ¨Åciently retrieve data
locally across two dimensions, they do this at the expense of
silicon area and power.
The Fast Fourier Transform, a highly non-local application
kernel, is used in this work to explore the effectiveness of
the proposed P-sync architecture. The FFT is a vital kernel in
many applications, especially signal processing. While both

III. P HOTONIC S YNCHRONOUS C OALESCED ACCESS
N ETWORK (PSCAN)
The patterns described thus far require rapid co-location
of spatially separate data. Wire delay makes this difÔ¨Åcult in
electronically connected systems constraining communication
to a decentralized hop-by-hop model in which it is difÔ¨Åcult
to efÔ¨Åciently schedule data delivery over a scalable array of
processing elements. If wire delay were not an issue one might
build an electronic bus circuit like that shown in Figure 1
to shufÔ¨Çe spatially separate data efÔ¨Åciently to a single data
receiver. In that circuit, four frequency-locked clocks with

190

Fig. 1. Electronic time domain multiplexing bus with four processors, P0 ‚Äì
P3 , clocked by œÜ0 ‚ÄìœÜ3 and containing data bits a‚Äìd, respectively. Processor
outputs feed an electronic shift register with clk frequency locked to œÜi .
Fig. 2. Photonic time domain multiplexing bus in which each processor Pi
has a modulator M sending data to the shift register via a photodiode receiver
circuit.

phase offsets œÜ0 ‚ÄìœÜ3 are used to drive a shared bus which
terminates at a destination node, represented as a buffer and
simple shift register. Assuming that each processing element
P0 ‚ÄìP3 has one bit of data that must be aggregated, the
utilization of the bus will be 100% over the duration of the
four-cycle transaction. However, two problems prevent this
circuit from scaling in size and bandwidth. First, the differently
phased clocks require low-skew distribution or generation,
which is very difÔ¨Åcult over large spatial distances. Second, at
high clock rates, the bus will not scale effectively beyond tens
of nodes because timing in that bus would be highly variable
depending on the location of the driving node relative to the
terminus. These limitations on bus scaling are fundamental to
traditional electronic interconnect technology.
In contrast, chip-scale photonic waveguides are transmission
lines. A simple photonic link is comprised of a laser acting
as a source of light, a modulation device, a transmission
medium such as a silicon waveguide, and a detector such as a
photodiode. The modulator interrupts the incident continuouswave light from the laser to drive data onto the waveguide.
The light travels along the waveguide until it is detected
by the photodiode. The speed with which the signal travels
is determined by the effective index of refraction of the
guiding medium and is independent of the length of the
waveguide. The only signiÔ¨Åcant length-dependent parameter
is the waveguide loss Lw , which determines the maximum
waveguide length before the modulated signal is attenuated
below the photodiode‚Äôs detection threshold. Another important
property of photonics is that the directionality of an incident
signal is usually preserved at the output of most devices (e.g.
modulators).
The photonic equivalent of the circuit described in Figure 1
would be modeled as shown in Figure 2. In the photonic bus,
incident energy is modulated by nodes P0 ‚ÄìP3 as it passes by
and is detected at the receiver, where the photodiode converts it
to an electrical signal. This particular arrangement scales until
the combined attenuation of the waveguide and modulators
decreases the signal strength below the detection threshold.
Synchronization in this model requires the global coordination of each processor to avoid collisions when writing to the
optical bus. This is challenging as the bus requires distribution
of a global clock to a large number of processors with minimal

Fig. 3. Transmission and reception of a photonic message (red) between two
processors P0 and Pn synchronized by a global photonic clock (yellow).

skew. When discussing synchronization in this model, it is
also important to consider signal Ô¨Çight time. Light with a
wavelength of 1550 nm (common in photonic technology) will
travel approximately 7 cm/ns in a silicon waveguide. Therefore, even with perfect global clocking, network throughput
could be reduced because a processor must wait to use the
waveguide until all previously transmitted data bits reach their
destination.
Fortunately the directivity of photonics can be used to
mitigate this problem. Consider the scheme shown in Figure 3,
in which a clock signal is transmitted down a waveguide
and detected at intervals by the processors. Due to Ô¨Çighttime delays, each processor‚Äôs local frame of reference is
unique such that a particular clock cycle will be detected at
different times by each processor. Each processor can have a
Communication Program (CP) that assigns a disjoint set of
clock cycles to each processor.
A second wavelength is then modulated by a Serializer/Deserializer (SerDes) that is clocked by the received photonic clock signal qualiÔ¨Åed by the local CP across all receivers
in PSCAN. There is a common skew between the reception
of the clock and the modulation of the data wavelength. The
modulated data wavelength is therefore synchronized with
the clock at each processing element, albeit with constant
skew. The CPs comprise non-overlapping portions of a global
schedule that is relative to the waveguide clock. Therefore, the
program speciÔ¨Åes when the waveguide is available for any one
processor to modulate light.
In Figure 4, two processors, P0 and P1 , are interleaving
data bits held in their local memory on the waveguide such

191

A. Synchronization
In order for the SCA and SCA‚àí1 operations to be possible in the PSCAN, a synchronization mechanism needs
to be implemented to ensure exact timing of data injection
and extraction. Traditional global synchronization strives to
minimize timing skew by providing a constant-phase signal
throughout a chip. Standard methods use H-tree topologies,
which ensure a uniform transmission line length and number
of repeater traversals for all endpoints. In contrast, the optical
propagation delay of the PSCAN requires an exact amount
of phase skew between nodes to account for timing offsets
induced by the bus topology. In fact, constant phase in PSCAN
would result in data overlap or wait times, lowering network
utilization. The full utilization achieved during the SCA and
SCA‚àí1 operations requires exact temporal alignment of data
elements. This form of packet construction has been previously
demonstrated [22].
The network utilizes open-loop distribution of the clock.
Open-loop distribution forgoes usage of a Phase-Locked Loop
(PLL) or Delay-Lock Loop (DLL) and the clock edge used by
the input/output memory elements is taken directly from the
distributed photonic clock. Traditional distribution networks
implement circuits and designs to minimize phase skew, in
contrast to the circuit in Figure 3, which requires it. In
practice, the clock signal is either propagated along the same
waveguide as the data transmission as shown in Figure 3
or along a separate parallel waveguide that is path-length
matched to the PSCAN data waveguide. Comparatively, the
single-waveguide design requires a more complex Ô¨Åltering and
wavelength selectivity scheme, while the parallel waveguide
design must deal with ensuring waveguide lengths remain
uniform. In either case, the propagation delay provides the
exact clock timing that needs to be utilized for data element
alignment.
Each network node can utilize a dual-clock FIFO circuit to
separate the disparate clock domains of the compute core and
the PSCAN. For the SCA operation, the FIFO input would be
clocked by the processor core, and the FIFO output would be
clocked by the clocking wavelength from the PSCAN. This
is reversed for the SCA‚àí1 , with the network clock on the
FIFO input and core clock on the FIFO output. This separation
of clock domains also encapsulates the timing needs of the
network from the compute cores.

Fig. 4. SCA operation: Each processor location P0 ‚ÄìP2 contains a detector
and modulator. The timing diagram represents the clock and data wavelengths,
Œªc and Œªd respectivly, traveling past the waveguide locations x0 ‚Äìx2 .

that a detector at P2 sees a continuous stream of spliced data
arriving at the maximum data rate supported by the clock.
The two independent wavelengths are Œªc , which carries a
modulated clock, and Œªd , which is modulated by P0 and P1
and detected as data at P2 . The waveforms at three locations
on the waveguide are shown as x0 , x1 and x2 .
At time t0 , a clock edge is detected by P0 on wavelength Œªc .
After a short delay for P0 to sense and respond to the clock, at
t1 the modulator transmits the SerDes output on wavelength
Œªd , completing its transmission of two bits at time t2 . At this
point, P0 ‚Äôs CP dictates that it allow unmodulated energy to
pass by for use by downstream processors. At time t3 , P1
begins modulating that energy in response to the received
clock edge ‚Äú2‚Äù. At time t4 , P0 begins modulating Œªd even
though, based on absolute time, P1 is still modulating it.
This simultaneous modulation is possible because of the nonnegligible delay along the waveguide between P0 and P1 .
After driving data for two clock cycles, P0 ‚Äôs CP dictates that
it let all subsequent energy pass by. The energy P1 passes
is the energy that P0 modulated at t4 . The entire transaction
is complete from the perspective of P0 and P1 at time t5 .
From the perspective of P2 (at physical location x2 ), a single
six-cycle burst transaction was received, as if from a single
source.
This in-Ô¨Çight synthesized transaction is called a Synchronous Coalesced Access (SCA). The inverse process
(SCA‚àí1 ) is also possible, in which a single source sends
a large burst of data that is synchronously distributed to a
number of receivers. The SCA takes data from a number
of spatially separate transmitters, and synthesizes it into a
monolithic transaction. The SCA‚àí1 is a scatter operation in
which one transmitter sends a monolithic transaction that is
broken apart on the Ô¨Çy to deliver pieces of data to a number
of receivers. The network that supports the photonic SCA and
SCA‚àí1 is called the Photonic Synchronous Coalesced Access
Network (PSCAN).

B. Scalability
The PSCAN depends upon a relatively long-distance shared
bus to drive re-organized data in bursts. Since physical memory occupies physical space, the longer the bus, the more nonlocal data that can be re-organized and co-located. The primary
limiting factor for PSCAN is loss in the waveguide, since the
length of the bus does not affect the speed of the signal.
As light moves through the waveguide, its intensity diminishes due to scattering and other factors. If the intensity of
this light drops below a critical threshold, deÔ¨Åned by the
sensitivity of a receiving photodiode, then the signal is no

192

longer detectable. Thus, the scalability of a single PSCAN
segment is deÔ¨Åned by the following equation:
Energy per Bit (pJ/bit)

Pi ‚àí Lw ‚â• Pmin-pd

60

(1)

where Pi is the incident optical power at the start of the
waveguide, Lw is the loss across the waveguide, and Pmin-pd is
the minimum detectable power of the photodiode. Loss in the
waveguide itself can be measured in terms of attenuation (in
dB) per unit length, though this loss is different for straight and
curved paths. Loss also occurs when a waveguide passes a ring
resonator that is not tuned to any frequency on the waveguide.
This analysis assumes that modulators are evenly spaced along
the waveguide. Thus, a segment is deÔ¨Åned to include a ring
resonator and a section of waveguide equivalent in length to
the modulator pitch. The loss in a segment is therefore:
Lws = Lr-off + Dm Lw

Electronic
Photonic

50
40
30
20
10
0

4√ó4

6√ó6

8√ó8

10√ó10

12√ó12

14√ó14

Network Size

Fig. 5.

SCA energy comparison.

Fig. 6.

The P-sync Architecture

(2)

where Lr-off is the attenuation due to light passing near a detuned ring resonator, Dm is the inter-resonator pitch, and Lw
is the loss in the waveguide. The maximum number of PSCAN
segments N , is bound by:
Pi ‚àí Pmin-pd
‚â•N
Lws

(3)

A PSCAN must traverse a chip in a serpentine pattern that
includes a number of waveguide curves. The effect of the
curves is to slightly decrease N . However, for simplicity they
are ignored in this analysis. The primary loss mechanism is
attenuation in the waveguide (Lw ). The number of possible
modulation sites can be quite large, but in practice will depend
upon the size of individual processors and layout constraints.
Thus, an in depth analysis is not presented here. It is important
to note, however, that individual PSCAN segments can be
linked via repeaters to form larger networks.

nodes. The number of link repeater stages is calculated based
on the ORION router model [24], and the router is assumed
to have three-stage delay. The chip size was Ô¨Åxed to 2 cm √ó
2 cm in all simulations. Therefore, the link-repeater stages are
inversely related to the number of network nodes. Figure 5
shows the energy-per-bit plots for both the electronic mesh
and PSCAN. PSCAN achieves at least a 5.2x improvement
for the networks simulated.
IV. P- SYNC

C. Energy

The P-sync architecture, built around PSCAN, was conceived to perform non-local accesses such as scatter/gather
patterns with extreme efÔ¨Åciency. The notion of inter-processor
communication (other than with a data-serving head-node)
is not supported by the particular implementation of the
architecture described in this section. However, this does not
mean that the P-sync architecture precludes communication
between processors. PSCAN does not necessarily obviate the
need for a relatively low-bandwidth photonic or even electrical
network to handle such trafÔ¨Åc. In fact, the PSCAN physical
layer was deliberately designed to be generic, such that it
could be shared with other trafÔ¨Åc besides SCA and SCA‚àí1
transactions. Thus, it is important to understand that PSCAN
presents a communication mode on a multi-purpose physical
channel, and P-sync, as presented here, is an architecture that
is optimized for that mode.
To support the PSCAN communication modes, P-sync is
based on a bus topology, with all processors sharing the same
photonic waveguide as shown in Figure 6. The following are
the important elements of the system:

The network energy efÔ¨Åciency of the PSCAN SCA operation was compared to the logically identical operation on
a mesh using the PhoenixSim simulator [23]. The electronic
mesh and PSCAN simulation models both possess an equivalent 320 Gb/s link to memory. The electronic network is
structured as a standard mesh topology with four memory
interfaces at the corner network nodes each with a 80 Gb/s
link-bandwidth (160 Gb/s for bidirectional.) The electronic
routers of the mesh are composed with a 32-bit bus width,
2.5 GHz network clock, and an input buffer size of 480 bits.
The PSCAN possesses a single 320 Gb/s link, composed of
32 wavelengths each modulated at 10 Gb/s.
Note the dichotomy of the two networks. The electronic
network possesses four separate memory controllers each with
one fourth of the total bandwidth of the single memory
controller in the PSCAN. This allows electronics to properly
leverage the advantage it has in alleviating network-level
contention through communication-path diversity. However, it
has the disadvantage of large pipeline depths between network

193

waveguide in every processor. It is derived from the high-level
operational code in much the same way that the individual
computations required by a multi-processor‚Äôs processing elements are compiled into a list of instructions based on source
code in a higher-level language.
All CPs on a PSCAN are linked together such that adherence to the PSCAN clock results in only one processor driving
the bus and one processor reading the bus at a time. The Head
Node also has a synchronous CP, though it has a different
function: to make requests to memory such that data is
available to scatter on the SCA‚àí1 waveguide. CPs can be quite
small, with the program for FFT being approximately 96-bits.
In the P-sync architecture, all data, including communication
programs and computation programs can be delivered on the
SCA‚àí1 PSCAN. CPs are delivered, along with operational
code to the processor on SCA‚àí1 operations, interleaved with
data delivery. CPs form chains in which one CP loads data,
and the CP for the SCA waveguide driver, followed by a CP
for the next SCA‚àí1 operation. This interspersing of different
kinds of control and data information can result in high levels
of efÔ¨Åciency, as data can be delivered ‚Äújust-in-time‚Äù by the
synchronous waveguide.
In the remainder of this paper, the P-sync architecture will
be evaluated quantitatively for its effectiveness in handling
non-local communication generally, and then speciÔ¨Åcally for
a parallel FFT implementation.

Fig. 7. The processor architecture of a P-sync node enables the SCA and
SCA‚àí1 communication modes.

‚Ä¢
‚Ä¢
‚Ä¢
‚Ä¢

Each processor understands its position on the linear
busses.
Photonic Clock Generators establish the optical time
reference on a waveguide.
Storage/DRAM are a photonically enabled random access
memory.
The Head Node is a processor that understands the memory layout (via its own program) and performs requests to
the memory such that data is streamed out on the SCA‚àí1
waveguide.

A processing element of a machine that could realize this
data pattern is shown in Figure 7. The computation core in
that processor consists of a local Data Memory, an Execution
Unit, and a Computation Instruction Memory. The Execution
Unit contains all of the arithmetic and logical units needed
to support the instruction set. The Computation Instruction
Memory and Data Memory are fed via the Network Interface. The Network Interface coordinates distribution of data
from the Waveguide Interface to the various memories in the
processing element. The Waveguide Interface coordinates inÔ¨Çight data reorganizations based upon a program stored in the
Communication Instruction Memory.
The PSCAN facilitates tight scheduling due to the distance
independent nature of photonics and the resulting ease of
global synchrony. It is therefore possible to fuse computation
with communication to achieve maximum hardware efÔ¨Åciency
and performance. This implies that communication must be
described at a similar level of detail as computation. However,
the tight interaction between computation and communication
does not mean that these functions are carried out in the
same functional hardware unit. If so, it would be difÔ¨Åcult
to parallelize these operations to reduce apparent latency.
Rather, the hardware units responsible for communication and
computation in each processor must operate in tight synchrony.
Modern computer systems are comprised of relatively inÔ¨Çexible hardware that efÔ¨Åciently runs Ô¨Çexible software. The
software generally is quite explicit about the computation
operations, but the method by which data is stored and
retrieved from other processors or memories is implicit, and is
usually handled by hardware. In P-sync however, the communication is quite explicit, and is described by a Communication
Program (CP). The CP is a simple schedule that is loaded
by the hardware unit responsible for communication on the

V. Q UANTITATIVE A NALYSIS
This section begins with generalized equations for the ideal
efÔ¨Åciency of computation based on the overlap of data delivery
and computation. Next, the relationships derived are used to
determine the potential efÔ¨Åciencies of the FFT application
kernel on a mesh-interconnected multiprocessor and a PSCAN
architecture.
A. Generalized Performance Model
The computation model used in this analysis decomposes a
parallel computation into two parts:
1) Time to deliver the data to the processors
2) Time to compute on the data
Two models of data delivery are used in the analysis for
comparison. In Model I a processor must receive all of its
data prior to starting computation. We assume the processors
share a single path to memory and that the memory controller
distributes one processor‚Äôs data at a time sequentially, such
that data delivery is serialized. Figure 8 shows the initial
distribution of data on a four-processor system using this
model.
For Model I, let:
‚Ä¢ Sb be number of data elements per delivery cycle delivered to a single processor
‚Ä¢ Ss be the size of each data element in bits
‚Ä¢ td equal the time to deliver data to a single processor
‚Ä¢ tc equal the time to compute on that data
Given the assumption that the data delivery to all the
processors is serialized, the total delivery time is P td , where

194

‚Ä¢
‚Ä¢

œÅ be the peak theoretical operations per seconds for each
processor
œÜ be the operations required to be performed on each
processor by the algorithm

For distributed processing, maximum theoretical throughput
is P œÅ, but realized operations in both models will be reduced
due to data delivery latency.
Consider Model I (k = 1) shown in Figure 8:
Fig. 8.

Model I: Simple data delivery model.

Œ∑=

œÉr

=

œÉt

=

PœÜ
P td +tc

PœÅ

=

PœÜ
P td + tc
PœÅ

œÜ/œÅ
tc
=
P td + t c
P td + t c

(5)
(6)
(7)

Relating it to architectural model parameters,

Fig. 9.

Model II: Blocked data delivery model.

P is the number of processors. This assumes that data is
distributed uniformly to all processors. If P N is the total
number of elements of data in the parallel computation, each
processor receives N elements each of size Ss bits. Because of
serialization, there are two phases of a computation in which
the processing nodes are not fully utilized. The Ô¨Årst is the time
when data is being loaded, called start-up, and the second is
the time at the end of computation when the computed data
is written back to memory, called wind-down.
Model II (Figure 9) relaxes the constraint that all data
needs to be at a processor prior to computation and provides
opportunities for greater efÔ¨Åciency. In algorithms where this
is a feasible data delivery strategy, it is possible to deliver the
data in k blocks to each processor in turn, repeating the roundrobin data delivery until all P k blocks of data are delivered.
The goal of this scheme is to overlap communication and
computation to reduce algorithm latency.
For Model II, let:
‚Ä¢ Sb and Ss are deÔ¨Åned as in Model I
‚Ä¢ k be the number of blocks of data delivered to a single
processor
‚Ä¢ tdk is the time to deliver a single block of data to a single
processor
‚Ä¢ tck is the time to compute on that data block
As in Model I, each processor receives N elements where
the number of elements in each block of data are Nk . Note that
Model I is the special case where k = 1.
If:
‚Ä¢ œÉr is the realized operations / sec
‚Ä¢ œÉt is the theoretical peak operations / sec
then, the computational efÔ¨Åciency of the multi-processor is
deÔ¨Åned as:
œÉr
Œ∑=
(4)
œÉt
Let:

tc
td

=
=

Œ∑

=

œÜ/œÅ
Œª + Sb Ss /Wp
œÜ/œÅ
P (Œª + Sb Ss /Wp ) + œÜ/œÅ

(8)
(9)
(10)

where Œª is the network latency, and Wp is the network
bandwidth.
To determine the efÔ¨Åciency of Model II, we must account
for the overlapping computation and communication. Denoting
the total time for computation and communication as T ,
T

=

œÉr

=

œÉt

=

=‚áí Œ∑

=

P tdk + (k ‚àí 1) max(tck , P tdk ) + tck
PœÜ
P tdk + (k ‚àí 1) max(tck , P tdk ) + tck
PœÅ
tc
P tdk + (k ‚àí 1) max(tck , P tdk ) + tck

(11)
(12)
(13)
(14)

The efÔ¨Åciency for k > 1 data delivery can be subdivided
into two cases:
Case 1: P tdk ‚â§ tck :
Œ∑=

tc
tc
=
P tdk + ktck
P tdk + tc

(15)

tc
P ktdk + tck

(16)

Case 2: P tdk > tck :
Œ∑=

Based on this analysis, Case 1 is compute bound, while Case 2
is communication bound. EfÔ¨Åciency can only be maximized in
Case 1, when computation and communication are balanced,
i.e. P tdk = tck . The previous equations provide insight into
the optimal relationship between data delivery and computation in a parallel architecture. It will be shown in the next
section that varying latency and network uncertainty makes
it difÔ¨Åcult to achieve this in a mesh-connected processor.
However, using SCA‚àí1 in a PSCAN to tightly interleave data
delivery to multiple processors, these high levels of compute
efÔ¨Åciency are possible.

195

TABLE I
C OMPUTE E FFICIENCY FOR ZERO LATENCY
k

Sb

tck (ns)

tcf (ns)

Wp (Gb/s)

Œ∑ (%)

1
2
4
8
16
32
64

1024
512
256
128
64
32
16

40960
18432
8192
3584
1536
640
256

0
4096
8192
12288
16384
20480
24576

409.6
455.1
512.0
585.1
682.7
819.2
1024.0

50.00
68.97
83.33
91.95
96.39
98.46
99.38

operations which are computed in time tcf .
As was concluded from Eq. 15 and Eq. 16, the following
relationship should be maintained for optimal efÔ¨Åciency:
Fig. 10. The FFT can be broken down into smaller, more local computations.
Dotted lines are shown for k = 2 delivery cycles, each four elements in size.

P =

N
) = 2N log2 k
k

(19)

Table I presents compute efÔ¨Åciency and required bandwidth
for the block-based FFT for several block sizes (Sb = Nk )
Here the peak chip bandwidth (Wp ) is computed as follows:

B. The FFT Application
The data delivery and transpose steps of the FFT are analyzed by computing the ideal efÔ¨Åciency based upon compute
and delivery time. Then the effects on efÔ¨Åciency of a wormhole
routed mesh are analyzed. The remaining discussion centers
on the 2D FFT comprised of the following steps:
‚Ä¢ Deliver P blocks of size N samples to the processor array
‚Ä¢ Perform P row FFTs in parallel
‚Ä¢ Transpose the data into off-chip DRAM
‚Ä¢ Load the reorganized data back into the processor array
‚Ä¢ Perform P column FFTs in parallel
1) EfÔ¨Åciency in Data Delivery: In many operations, especially the FFT, the complexity of long-distance communication
between processors and memory results in the starvation of the
compute hardware. In this section, the limits of efÔ¨Åciency, and
the parameters that inÔ¨Çuence it are analytically described for
the parallel FFT operation.
This analysis assumes a 1024 √ó 1024 sample 2D FFT,
running on a 256 processor system. Because of memory
serialization, the larger the FFT row size, the larger each
processor‚Äôs data delivery phase and the longer the other
processors sit idle. However it is possible to use the data
delivery mode described in Model II to increase efÔ¨Åciency.
In the case of FFT, this is possible because the structure of
a Decimation-in-Time FFT results in increasing non-locality
as the computation progresses. Therefore, the non-locality as
deÔ¨Åned by the span in linear memory between two operands
increases as 2n , where n is the number of butterÔ¨Çy stages
executed. Thus, small portions of the FFT can be executed on
data local to the processor, followed by a pure computation
(no data delivery) phase after all sub-blocks are computed, as
shown in Figure 10.
The number of multiplication operations per delivery cycle
is:
N
2N
log2
(17)
k
k
and the Ô¨Ånal, compute-only phase requires:
2N (log2 N ‚àí log2

tck
.
tdk

Wp =

S b Ss P
tck

(20)

Where: Sb is the block size in samples Ss is the number of
bits per sample
The numbers in Table I assume the following:
‚Ä¢ 1024 point FFTs
‚Ä¢ 256 processors
‚Ä¢ Floating-point multiplies take 2 ns
‚Ä¢ 4 32-bit multiplies per FFT butterÔ¨Çy
‚Ä¢ Ss = 64
‚Ä¢ Only Multiplies are counted
The result of these relationships is that efÔ¨Åciency can
be improved by increasing bandwidth, but not for obvious
reasons. In this case, the reduction of start-up and wind-down
time achieved by decreasing the block size means that the data
must be delivered in less time to avoid stalling the processors,
which increases the required bandwidth. This occurs because
the computational complexity of the FFT is O(N log2 (N )),
whereas the delivery time increases as O(N ). Thus, block
size affects the balance of computation and communication
because computation scales non-linearly. Larger block sizes
result in a longer compute and delivery phase (related to tck
and tdk ), while smaller block sizes result in a longer postdelivery computation phase (tcf ).
2) Data Delivery in an Electronic Mesh: For the electronic
mesh the following is assumed:
‚Ä¢ Square processor array
‚Ä¢ Single channels between processors
‚Ä¢ Flit Size = FFT element size
‚Ä¢ Each processor can hold two Ô¨Çits of data in the case of
a blocked channel
‚Ä¢ Packets are injected into the network serially from the
memory node on the network periphery.
The bandwidth between two neighboring processors is the
same as the bandwidth to memory; thus, the bisection network

(18)

196

TABLE II
E LECTRONIC M ESH C OMPUTE E FFICIENCY WITH L ATENCY
k

Delivery
EfÔ¨Åciency, Œ∑d (%)

Compute
EfÔ¨Åciency, Œ∑ (%)

1
2
4
8
16
32
64

98.46
96.97
94.12
88.89
80.00
66.67
50.01

49.23
66.88
78.43
81.74
77.11
65.64
49.70

shows the maximum efÔ¨Åciency accounting for the latency
of a mesh network. Therefore, the overall efÔ¨Åciency for the
mesh will be the product of those efÔ¨Åciencies, since td is
proportional to the network delivery efÔ¨Åciency. Even under
ideal conditions, compute efÔ¨Åciency peaks at 82% when k = 8
(shown in Table II in boldface). Achieving this value in
the mesh is complicated because network effects make tight
synchronization increasingly difÔ¨Åcult. Higher efÔ¨Åciency in the
electronic mesh is possible only by increasing bandwidth
proportional to the inverse of the delivery efÔ¨Åciency.
3) Data Delivery in PSCAN: Using a PSCAN, the Psync architecture can achieve or come extremely close to
these levels of compute efÔ¨Åciency by enabling monolithic
transactions from memory that are scattered on-the-Ô¨Çy by
receiving processors. The P-sync architecture does this by
utilizing the global synchrony permitted by photonic‚Äôs relative
distance independence to tightly interleave the delivery of
data from a single source (i.e. memory). The FFT compute
efÔ¨Åciency of an electronic mesh-routed architecture and P-sync
is compared in Figure 11.

Compute Efficiency vs. Delivery Cycles (k)
100
90

Compute Efficiency

80
70
60
50
40

C. EfÔ¨Åciency in Transpose

30

The distributed transpose operation begins when a number
of processors have data to write back to memory, such that
elements from one processor will be interleaved in the linear
address space of main memory with elements from other
processors. This is a gather operation in which the target is
the memory.
The following analysis assumes that bandwidth to memory
is held constant in both the P-sync (PSCAN) and the electronic
mesh used for comparison. Let each system have P = 1024
processors and a single memory port. While a single port for
1024 processors may be unrealistic in a general case, the trends
shown here apply to systems with more memory ports.
1) PSCAN: The decisions made for data block sizes assume
a DRAM system with 2048-bit rows. In such a system, 32 64bit complex samples can be bursted at a time before a costly
row-precharge must occur. Therefore, the assumption is that
the trafÔ¨Åc to memory at the periphery of the chip should be
optimally emitted in 32 √ó 64-bit blocks.
After each of the Ô¨Årst FFTs are executed, each processor
writes back one full row, or N Ss bits of data. That data would
be divided into Pt transactions:
N Ss P
(23)
Pt =
Sr
where N is the row size in FFT samples, Ss is the FFT sample
size in bits, P is the number of processors in the array and
Sr is the DRAM row size in bits.
Each transaction can be sent to memory in SSrb cycles, where
Sb is the bus width. Assuming each transaction needs a Sh bit
address header the total transaction time in bus cycles tt is:

20

Electronic Mesh
P‚àísync

10
0

1

2

4

8

16

32

64

128

256

512

k
Fig. 11. Global synchrony and pre-scheduled communication allow P-sync
to achieve near ideal FFT compute efÔ¨Åciency as k increases. Such efÔ¨Åciency
gains in the mesh are limited by the increased overhead of routing smaller
packets.

bandwidth far exceeds bandwidth to memory. Let tr equal the
time (in cycles) to route a wormhole header in any processor
on the way to a packet‚Äôs destination. Assuming data to be
scattered is always available at the memory controller, and tr
is 0, then the total time to scatter the data is simply P F where
P is the number of processors and F is the number of Ô¨Çits
delivered to each processor
In real systems it takes at least a cycle (tr ‚â• 1) for routing
logic in each processor to determine the next step in a packet‚Äôs
traversal of the network. The equation for delivery time in
cycles is then:
‚àö
(21)
P F + P P tr
When F is large, this routing overhead is small, but when
using Method II to increase efÔ¨Åciency the overhead becomes
large. In Table II, the k = 64 case is half as efÔ¨Åcient as the
k = 1 case.
Comparing the time of delivery assuming zero latency and
routing delays to the actual delivery time accounting for these
delays results in the following delivery efÔ¨Åciency:
Œ∑d =

Sb Sc /Wp
Œª + Sb Sc /Wp

S r + Sh
(24)
Sb
Therefore, the transpose can be completed on a PSCAN in
Pt tt bus cycles. Using the following:
tt =

(22)

Table I shows the maximum theoretical efÔ¨Åciency given
start-up and wind-down time without network latency. Table II

197

TABLE III
T RANSPOSE C OMPLETION T IME IN C YCLES
tp

Writeback Time(cycles)

Multiplier (vs PSCAN)

1
4

3526620
6553448

3.26
6.06

gain is principally due to network effects such as congestion,
header overhead, and the impact of transposing in cache with
a processor.
In the next section, a high-level simulation of a full FFT
is presented to show how the effects studied here manifest
themselves in a full application.
VI. A PPLICATION P ERFORMANCE R ESULTS

N = 1024 samples
‚Ä¢ Ss = 64 bits
‚Ä¢ P = 1024 processors
‚Ä¢ Sr = 2048 bits
‚Ä¢ Sb = 64 bits
‚Ä¢ Sh = 64 bits
The 220 sample transpose writeback can be optimally completed in 1,081,344 bus cycles.
2) Wormhole Mesh: Because of the diversity in architectural options in modern many-core inter-communication
networks, designing a one-size-Ô¨Åts-all model is an intractable
task. Therefore, in this analysis, a simple wormhole routed
mesh model written in SystemC/TLM with the following
parameters is used to illustrate the limits of such networks
under the transpose load:
‚Ä¢ N = 1024 processors
‚Ä¢ Minimal adaptive wormhole routed
‚Ä¢ 1-cycle delay to route a packet header in each encountered router
‚Ä¢ 2-Ô¨Çit deep buffers output to inter-processor channels
‚Ä¢ 64-bit Ô¨Çits are sent between adjacent processors or memory in 1 cycle
One of the difÔ¨Åculties in transposing in a mesh network is
the disorder imposed by the routing hardware on the packets of
data. Since the transpose is a data reorganization, the order in
which data arrives at the memory interface is very important.
Since all of the data is spatially separated, in the simplest
case, each element is output independently where it could
either be forwarded directly to the main memory as multiple
writes (extremely inefÔ¨Åcient) or reassembled at the output node
using buffers (preferred). The time to transpose a matrix in this
environment depends on the ordering of the data, as well as
the overhead of sending small packets through the network.
In general, in a fully electrical system, the intercommunication bandwidth available on-chip exceeds the bandwidth
off chip. Therefore, there is an unavoidable bottleneck at
the memory interface in the transpose, as all processors are
attempting to communicate with a single lower bandwidth
interface.
Reordering the data requires multiple cycles to account
for address decode, transport to staging buffers and time for
storage. Further latency is incurred when the data is written
to memory. The parameter tp represents the time spent (in
cycles) reorganizing data at the destination node to enable
efÔ¨Åcient memory writes. For comparison, Table III contrasts
the ideal case where tp = tr = 1 with the case where
tp = 4. The rationale for choosing tp = 4 is based on the
required reordering steps. The PSCAN performs the operation
3.2x and 6x faster for tp = 1 and tp = 4, respectably. This
‚Ä¢

For this paper, we have focused on the Fast Fourier Transform (FFT), a kernel in which performance, both in terms
of time as well as power, is critical to many real-world
applications. In this section, a full FFT Ô¨Çow is simulated at
a high level to illustrate the gains possible with P-sync as
bandwidth and processing power increases.
A. Simulation Environment
For the purposes of simulating the 2D FFT operation,
the Lincoln Laboratory Mapping and Optimization Runtime
Environment (LLMORE) is used. LLMORE is a framework
for optimizing the mapping of parallel data objects in parallel
applications, simulating and optimizing new (and existing)
architectures, generating performance data (runtime, power,
etc.), and generating/executing optimized code on target architectures. LLMORE can be used to improve the performance of parallel applications and as an important tool for
analyzing new hardware architectures. LLMORE takes as
input user code, a model of the system architecture, and a
set of LLMORE speciÔ¨Åc parameters that guide its execution.
LLMORE‚Äôs output consists of one or more of the following
Ô¨Åve items: a complete set of optimized maps (describing the
data distribution for all parallel objects in the user code),
performance data, a set of optimized architectures for the user
code, optimized generated code, and results from a run on
target architectures.
LLMORE is used to generate performance data of the 2D
FFT operation on the two architectures shown in Figure 12:
an electronic mesh and P-sync architecture (using PSCAN).
Both architectures assume fast local memory and four shared
external memory banks located in the corners for the electronic
mesh and at the end of the waveguide for P-scan. When scaling
the number of cores the architectures assume a square topology
(e.g., a 2x2 mesh). The link bandwidths and latencies are
equivalent across architectures in order to achieve a conservative, fair comparison. This results in a bisection bandwidth for
the electronic mesh architecture which is signiÔ¨Åcantly larger
than that of the P-sync.
In this analysis, both the electronic and P-sync architectures
execute two parallel FFT phases interspersed with a transpose.
The P-sync architecture achieves this with an SCA, while the
electronic mesh executes a block-wise transpose. The blockwise transpose loads the data in small pieces to local memory
to perform the reorganization prior to writeback.
B. LLMORE Simulation Results
The simulated performance of the 2D FFT on the electronic
mesh (blue) and P-sync (green) architectures in gigaÔ¨Çops as

198

(a) Electronic Mesh Architecture
Fig. 12.

(b) P-sync Photonic Architecture

Both architectures contain processing (P), memory (M) and memory interface (MI) elements.

Performance of 2D FFT Operation

11

10

Time Spent Reorganizing Data for 2D FFT

100
90

80

Percentage

FLOPS

70

10

10

60

50
40

30
20

Electronic Mesh
Photonic Bus (PSCAN)
Ideal

9

10

4

8

16

32

64

128

256

512

1024

2048

10

0

4096

Number of Cores

Electronic Mesh
Photonic Bus (PSCAN)
4

8

16

32

64

128

256

512

1024

2048

4096

Number of Cores

Fig. 14. Percentage of runtime spent in reorganizing data for the 2D FFT
on the electronic mesh (blue) and P-sync (green) architectures.

Fig. 13. Simulated performance in Ô¨Çoating-point operations per second of
the electronic mesh architecture (blue) and P-synch architecture using PSCAN
(green) for 2D FFT. Ideal performance shown by red curve.

the percentage of total runtime for the SCA operation used by
the P-sync architecture for data reorganization levels off to a
signiÔ¨Åcantly more reasonable percentage.

the number of cores increases from 4 to 4096 (mesh dimension
from 2 to 64) is shown in Figure 13. It is important to note
that the ideal performance for these types of architectures
(shown in red) does not scale linearly with the number of cores
due to limited memory parallelism (4 memory controllers)
and bandwidth. As the number of cores is increased, the
performance of the P-sync architecture converges to ideal
performance. However, the performance of the electronic mesh
architecture peaks around 256 cores and decreases for larger
numbers of cores. The performance for the P-sync architecture
for P > 256 is two to ten times better than the electronic mesh
architecture, which is consistent with the previous analysis
(Table III). It is also important to note that these simulations
use a Model I delivery mode. It is likely that the performance
would improve further under P-sync if a Model II delivery
mode was used.
Figure 14 shows the percentage of total runtime that each
architecture spends in the reorganization of the data between
the two 1D FFT operations. This illustrates the difÔ¨Åculty
that the electronic mesh architecture has with scaling the 2D
FFT up to a large number of cores. The block transpose
operation used for the data reorganization on the electronic
mesh architecture requires an increasingly larger percentage of
the total runtime as the number of cores is increased. However,

VII. C ONCLUSION
Due to the expense of long-distance on-chip communication, many electrically interconnected CMPs have adopted
mesh interconnect that linearizes delay at the expense of
additional hardware and increased energy consumption. To
mitigate the latency of communication across this hop-based
network, architects include hierarchical caches to maximize
data locality near processing cores. These hardware resources
result in redundant storage of data but ultimately increase
aggregate performance. Unfortunately, maintaining cache coherence and re-distributing data during parallel operations
presents a challenge to algorithm developers trying to achieve
high performance.
In this paper, an architecture called P-sync is described. Psync utilizes chip-scale photonics in a novel manner, greatly
accelerating parallel applications and application kernels that
exhibit non-local data accesses, such as scatter/gather patterns.
These patterns are common in linear algebra computations in
the form of the matrix transpose as well as signal and image
processing kernels, such as the Fast Fourier Transform.
It was shown that the very common scatter data pattern can
be optimized to maximize the utilization of compute hard-

199

[5] H. Izumi, K. Sasaki, K. Nakajima, and H. Sato, ‚ÄúAn efÔ¨Åcient technique
for corner-turn in SAR image reconstruction by improving cache access,‚Äù in Parallel and Distributed Processing Symposium., Proceedings
International, IPDPS 2002, Abstracts and CD-ROM, 2002, pp. 3‚Äì8.
[6] J. Owens, M. Houston, D. Luebke, S. Green, J. Stone, and J. Phillips,
‚ÄúGpu computing,‚Äù Proceedings of the IEEE, vol. 96, no. 5, pp. 879 ‚Äì899,
may 2008.
[7] D. H. Bailey, ‚ÄúFFTs in external or hierarchical memory,‚Äù Journal of
Supercomputing, vol. 4, pp. 23‚Äì35, 1990.
[8] W. Dally and B. Towles, ‚ÄúRoute packets, not wires: on-chip interconnection networks,‚Äù in Design Automation Conference, 2001. Proceedings,
2001, pp. 684‚Äì689.
[9] R. Ho, K. Mai, and M. Horowitz, ‚ÄúThe future of wires,‚Äù Proceedings
of the IEEE, vol. 89, no. 4, pp. 490‚Äì504, Apr 2001.
[10] M. Taylor, J. Kim, J. Miller, D. Wentzlaff, F. Ghodrat, B. Greenwald,
H. Hoffman, P. Johnson, J.-W. Lee, W. Lee, A. Ma, A. Saraf, M. Seneski,
N. Shnidman, V. Strumpen, M. Frank, S. Amarasinghe, and A. Agarwal,
‚ÄúThe Raw microprocessor: a computational fabric for software circuits
and general-purpose programs,‚Äù Micro, IEEE, vol. 22, no. 2, pp. 25‚Äì35,
Mar./Apr. 2002.
[11] S. Vangal, J. Howard, G. Ruhl, S. Dighe, H. Wilson, J. Tschanz, D. Finan, A. Singh, T. Jacob, S. Jain, V. Erraguntla, C. Roberts, Y. Hoskote,
N. Borkar, and S. Borkar, ‚ÄúAn 80-tile sub-100-W TeraFLOPS processor
in 65-nm CMOS,‚Äù Solid-State Circuits, IEEE Journal of, vol. 43, no. 1,
pp. 29‚Äì41, Jan. 2008.
[12] C. Gunn, ‚ÄúCMOS photonics‚ÄîSOI learns a new trick,‚Äù in SOI Conference, 2005. Proceedings. 2005 IEEE International, Oct. 2005, pp. 7‚Äì13.
[13] M. Salib, L. Liao, M. Morse, A. Liu, and D. Samara-Rubio, ‚ÄúSilicon
photonics,‚Äù Intel Technology Journal, vol. 08, May 2004.
[14] M. Lipson, ‚ÄúGuiding, modulating, and emitting light on siliconchallenges and opportunities,‚Äù Lightwave Technology, Journal of, vol. 23,
no. 12, pp. 4222‚Äì4238, Dec. 2005.
[15] N. Sherwood-Droz and M. Lipson, ‚ÄúScalable 3D dense integration of
photonics on bulk silicon,‚Äù Opt. Express, vol. 19, no. 18, pp. 17 758‚Äì
17 765, Aug. 2011.
[16] N. Kirman, M. Kirman, R. Dokania, J. Martinez, A. Apsel, M. Watkins,
and D. Albonesi, ‚ÄúLeveraging optical technology in future bus-based
chip multiprocessors,‚Äù in Microarchitecture, 2006. MICRO-39. 39th
Annual IEEE/ACM International Symposium on, Dec. 2006, pp. 492‚Äì
503.
[17] D. Vantrease, R. Schreiber, M. Monchiero, M. McLaren, N. P. Jouppi,
M. Fiorentino, A. Davis, N. Binkert, R. G. Beausoleil, and J. H. Ahn,
‚ÄúCorona: System implications of emerging nanophotonic technology,‚Äù in
Proceedings of the 35th Annual International Symposium on Computer
Architecture, ser. ISCA, 2008, pp. 153‚Äì164.
[18] A. Krishnamoorthy, R. Ho, X. Zheng, H. Schwetman, J. Lexau, P. Koka,
G. Li, I. Shubin, and J. Cunningham, ‚ÄúComputer systems based on
silicon photonic interconnects,‚Äù Proceedings of the IEEE, vol. 97, no. 7,
pp. 1337‚Äì1361, July 2009.
[19] A. Shacham, B. Lee, A. Biberman, K. Bergman, and L. Carloni,
‚ÄúPhotonic noc for dma communications in chip multiprocessors,‚Äù in
High-Performance Interconnects, 2007. HOTI 2007. 15th Annual IEEE
Symposium on, aug. 2007, pp. 29 ‚Äì38.
[20] A. Joshi, C. Batten, Y.-J. Kwon, S. Beamer, I. Shamim, K. Asanovic,
and V. Stojanovic, ‚ÄúSilicon-photonic Clos networks for global onchip communication,‚Äù in Networks-on-Chip, 2009. NoCS 2009. 3rd
ACM/IEEE International Symposium on, May 2009, pp. 124‚Äì133.
[21] G. Hendry, J. Chan, S. Kamil, L. Oliker, J. Shalf, L. Carloni, and
K. Bergman, ‚ÄúSilicon nanophotonic network-on-chip using TDM arbitration,‚Äù in High Performance Interconnects (HOTI), 2010 IEEE 18th
Annual Symposium on, Aug. 2010, pp. 88‚Äì95.
[22] O. Liboiron-Ladouceur, H. Wang, and K. Bergman, ‚ÄúAn all-optical PCIExpress network interface for optical packet switched networks,‚Äù in
OFC/NFOEC 2007, Mar. 2007, pp. 1‚Äì3.
[23] J. Chan, G. Hendry, A. Biberman, K. Bergman, and L. Carloni,
‚ÄúPhoenixsim: A simulator for physical-layer analysis of chip-scale photonic interconnection networks,‚Äù in Design, Automation Test in Europe
Conference Exhibition (DATE), 2010, Mar. 2010, pp. 691‚Äì696.
[24] H.-S. Wang, X. Zhu, L.-S. Peh, and S. Malik, ‚ÄúOrion: a powerperformance simulator for interconnection networks,‚Äù in Microarchitecture, 2002. (MICRO-35). Proceedings. 35th Annual IEEE/ACM International Symposium on, 2002, pp. 294‚Äì305.

ware. Achieving this efÔ¨Åciency in a typical electrical meshconnected network was analytically shown to be extremely
difÔ¨Åcult due to network effects, routing overhead, and the cost
of reordering data at the memory port. In addition, the inverse
gather process, in which distributed data must be accumulated
and reordered at a single location, was explored and shown
to beneÔ¨Åt greatly from in-Ô¨Çight data reorganization in the
proposed photonic interconnect.
The mechanisms that enable these performance gains are
the Synchronous Coalesced Access (SCA) and the network
to support it, the Photonic Synchronous Coalesced Access
Network (PSCAN). The PSCAN allows burst transactions to
a single destination to be synthesized in-Ô¨Çight from multiple
spatially separate contributors for reorganization of distributed
data. This operation is performed without any special buffering
logic and can result in optimal use of channel bandwidth to
off-chip storage. Through simulation, it is shown that a large
number of cores can reorganize data in-Ô¨Çight at the maximum
data rate to memory, achieving upwards of 6x in improved
performance.
This exploration is a signiÔ¨Åcant departure from previous thinking about the uses of chip-scale photonics. In
this work, features enabled by the unique properties of
photonic interconnect‚Äîeasier global synchrony and distance
independence‚Äîare explored as application performance enablers, rather than solely bandwidth density and lower power.
By Ô¨Årst focusing on the application, and then rethinking
the interaction of the processors with the network, we have
shown that it is possible to achieve large gains in performance
and energy efÔ¨Åciency for challenging non-local data access
patterns.
VIII. F UTURE W ORK
The experiments presented in this paper are relatively simple, but they expose the power of the PSCAN interconnect.
There is much work to be done to explore the characteristics, scalability, and utility of the SCA. Areas for further
exploration include: generation of distributed communication
programs from abstract programmer constructs, compatibility
with other transfer modes and architectures, and utility in other
applications and application domains.
R EFERENCES
[1] D. Wentzlaff, P. GrifÔ¨Ån, H. Hoffmann, L. Bao, B. Edwards, C. Ramey,
M. Mattina, C.-C. Miao, J. Brown, and A. Agarwal, ‚ÄúOn-chip interconnection architecture of the Tile processor,‚Äù Micro, IEEE, vol. 27, no. 5,
pp. 15‚Äì31, Sept.‚ÄìOct. 2007.
[2] G. Hendry, E. Robinson, V. Gleyzer, J. Chan, L. Carloni, N. Bliss,
and K. Bergman, ‚ÄúCircuit-switched memory access in photonic interconnection networks for high-performance embedded computing,‚Äù in
Proceedings of the 2010 ACM/IEEE International Conference for High
Performance Computing, Networking, Storage and Analysis, ser. SC ‚Äô10.
Washington, DC, USA: IEEE Computer Society, 2010, pp. 1‚Äì12.
[3] G. Hendry, E. Robinson, V. Gleyzer, J. Chan, L. P. Carloni, N. Bliss, and
K. Bergman, ‚ÄúEnabling high performance embedded computing through
memory access via photonic interconnects,‚Äù in High Performance Embedded Computing (HPEC), Sep 2010.
[4] B. S. Nordquist and S. D. Lew, ‚ÄúApparatus, system, and method for
coalescing parallel memory requests,‚Äù U.S. Patent 7,4923,68B1, Feb.
17, 2009.

200

Anomalous Subgraph Detection in Publication
Networks: Leveraging Truth
Nadya T. Bliss, Senior Member, IEEE B. R. Erick Peirson and Deryc Painter
School of Computing, Informatics
and Decision Systems Engineering
Simon A. Levin Mathematical, Computational
and Modeling Sciences Center
Arizona State University
Tempe, Arizona 85287‚Äì7205
Email: nadya.bliss@asu.edu

School of Life Sciences
Arizona State University
Tempe, Arizona 85287‚Äì4501
Email: bpeirson,deryc.painter@asu.edu

Abstract‚ÄîAnalysis of social networks has the potential to
provide insight into a wide range of applications. As datasets
grow, a key challenge is the lack of existing truth models.
Unlike traditional signal processing, where models of truth and
background data exist and are often well deÔ¨Åned, these models
are commonly lacking for social networks. This paper presents
a transdisciplinary approach of mitigating this challenge by
leveraging research on scientiÔ¨Åc innovation together with a novel
Signal Processing for Graphs (SPG) algorithmic framework. The
results suggest new ways for the study of innovation patterns in
publication networks.

I. I NTRODUCTION
In many applications, from social network analysis to protein interactions, the observed data consist of entities and
relationships between those entities. These data can be represented formally as a graph G = (V, E) with V representing
the entities or vertices and E representing the relationships
between those entities or edges.
Graphs, and correspondingly, the Ô¨Åeld of graph theory, are
not novel - the Ô¨Årst documented graph problem was deÔ¨Åned
in 1736 by Euler [1]. However, starting in the early 2000s [2],
the datasets of interest have become difÔ¨Åcult to handle with
traditional, traversal based algorithms such as those discussed
in [3]. For example, a common dataset studied by graph
theorists in 1970s and 1980s is Zachary‚Äôs Karate Club which
included 34 vertices and 78 edges [4]. By comparison, recent
datasets of interest include hundreds of thousands to millions
and billions of vertices [5]. This explosion in data size of
graphs along with a prevalence of applications has motivated
a new interest in this topic [6], [7].
A key question of interest when considering a particular
graph is whether that graph contains a signiÔ¨Åcantly smaller
graph that is anomalous with regards to the background (or
the rest of the graph). In the context of this discussion, we
focus speciÔ¨Åcally on topological anomalies. This motivates the
creation of a new algorithmic framework allowing detection
of such anomalies in large, noisy background - a signal
processing framework for graph based data. Unlike traditional
signal processing, when working with graphs, we commonly
lack information on both the signal and the noise. For example,

¬ã,(((

Manfred D. Laubichler
School of Life Sciences
Arizona State University
Tempe, Arizona 85287‚Äì4501
Santa Fe Institute
Santa Fe, New Mexico 87501
Marine Biological Laboratory
Woods Hole, MA 02543
Email: manfred.laubichler@asu.edu

the notion of anomalous group in a social network is not
trivial to deÔ¨Åne and is likely highly variable across application
domains. This makes the problem of signal detection in graphs
particularly challenging.
In this paper, we focus on the analysis of publication
networks in the Ô¨Åeld of developmental biology. Leveraging
both a well studied case of scientiÔ¨Åc innovation and the
Signal Processing for Graphs (SPG) algorithmic framework,
previously discussed in [8], [9], [10], we analyze a co-author
network to both provide insights into the evolution of the
Ô¨Åeld of developmental biology and develop a set of Ô¨Ålters to
detect patterns of innovation. Following a brief overview of the
related work, the rest of the paper is organized as follows. In
Section II, we present an overview of the Signal Processing
for Graphs framework. In Section III, we describe the case
of scientiÔ¨Åc innovation in developmental biology which we
leveraged for our analysis. In Section IV, we present the results
of our analysis along with a discussion of the signiÔ¨Åcance of
the results. In Section V, we discuss current directions of our
research. Finally, we conclude the paper in Section VI.
A. Related Work
In our research, we leverage the adjacency matrix representation for graphs. A graph G = (V, E) can be represented
by matrix A where each element of A, Aij is non-zero if
there exists an edge between vertex vi and vertex vj in G.
Working with the matrix representation, we perform spectral
analysis of matrices associated with the graphs under study.
In that analysis, we leverage the fundamental work in spectral
graph theory by [11] . However, unlike many of the derivations
in [11], we focus not on the graph Laplacian, but on the
modularity matrix as deÔ¨Åned in [12], [13].
In the last decade, there has seen an emergence of work on
theoretical detectability bounds of small subgraphs in networks
[14], [15], [16]. Our work differs from these efforts in that we
endeavor to develop a general signal processing framework for
graph based data, not constrained to any particular background
or foreground topology.



$VLORPDU

Finally, a different, but important emerging research area
that is worth mentioning is signal processing on graphs as
in [17], [18]. In this research, the signals reside on a graph;
unlike in our work, where we look to separate signal and noise
given observed graph-based data.
II. A LGORITHMIC F RAMEWORK
In this section, we review the Signal Processing for Graphs
framework as discussed in [8], [9], [10]. The framework is
inspired by the notion of regression analysis on graph-based
data. Akin to linear regression, we look for deviations that
cannot be explained by variance in the data and, in that case,
declare a detection. The formal detection problem that we aim
to solve is as follows - determine whether H0 or H1 is true
where:
H0 :G = GB
H1 :G = GB ‚à™ GS
where GB is a background graph (noise only) and GS is
the signal subgraph.
In the analysis presented here, we are interested in the emergence of scientiÔ¨Åc innovation over time and thus are interested
in detecting signals in dynamic graphs. Therefore, for each
time interval t, we have an adjacency matrix At that deÔ¨Ånes
the graph at that time interval. To make the determination
of whether a signal is present in the data, we perform the
following steps. First, we compute residuals of the observed
graph G from a model. The residual computation is followed
by temporal integration of the residuals at each time slice, t.
We then perform the dimensionality reduction step (leveraging
spectral techniques) to reduce the high-dimensional space to
two dimensions. Then, we perform anomaly detection, making
a determination of whether a signal is present in the observed
data, followed by identiÔ¨Åcation of that signal. The speciÔ¨Åcs of
each of the steps performed in the analysis for the results in
this paper are described below. For our results, since we are
leveraging truth data, we skip the detection step.
A. Residual Computation
In general, the residual computation can be deÔ¨Åned as
follows:
R[A] = A ‚àí E[A]

Since we are working with dynamic graphs, we are interested in residuals integrated over a time window. We consider
two Ô¨Ålters in our analysis: a simple ramp Ô¨Ålter and a Ô¨Ålter
constructed from the known eigenvalues of the truth subgraph
(to be discussed in Section III).
A ramp Ô¨Ålter is constructed by creating Ô¨Ålter coefÔ¨Åcients
that increase over time, centered, and normalized (T here is
the length of the time window of interest):
ht = t ‚àí T /2
h = h /  h 

kkT
(2)
2M
where k is the degree vector (ki is the number of edges
associated with each vertex) of matrix A and M is the total
number of edges in G. For the work presented here, we assume
that the graphs are unweighted and undirected and therefore
A is symmetric.

(3)
(4)

An alternative Ô¨Ålter, based on the maximum eigenvalue of
the known subgraph was also constructed. We compute the
maximum eigenvalue, Œªmax , of the known signal matrix, S
for each time interval, t and normalize it to create the second
Ô¨Ålter, g.
The integration procedure is the same as described in [9]:

RÃÉn =

T
‚àí1


R(n‚àít) ht

(5)

t=0

C. Dimensionality Reduction
In this case, we simply take the eigendecompostion of the
integrated residuals, RÃÉ, and use the top two eigenvectors as the
2-dimensional coordinates for projection of our graph, G. Note
that this step reduces the dimensionality of space from |V | to 2
dimensions, where |V | is the carnality of the vertex set. More
sophisticated techniques for dimensionality reduction can also
be utilized as described in [19].
D. Detection and IdentiÔ¨Åcation
For this analysis, we skip the detection step, as we are
predominately interested in identifying which vertices (authors) have contributed in a detectable manner to an innovation
within a scientiÔ¨Åc Ô¨Åeld (and we already know that the anomaly
is indeed present). We perform a simple thresholding in one
dimension to identify vertices for further investigation.
III. T RUTH : I NNOVATION IN THE F IELD OF
D EVELOPMENTAL B IOLOGY

(1)

where R[A] represents the residuals and E[A] represents
the expected value of A. For the residuals in this analysis, we
use the modularity matrix, as deÔ¨Åned in [12]:
R[A] = A ‚àí

B. Integration

We focus on leveraging well studied truth data to construct
our Ô¨Ålters and determine whether the notion of leveraging truth
is applicable in this context. The case study of gene regulatory
networks as a scientiÔ¨Åc innovation in developmental biology
is brieÔ¨Çy described in this section.
Gene regulatory networks are one of the main explanatory
concepts in today‚Äôs evolutionary and developmental biology
[20], [21], [22]. Furthermore, the history of this idea is also
understood, at least in its broad patterns [23]. This includes
early conceptual ideas, dating back to the beginning of the
20th century, as well as more recent developments that derive
from a clear conceptual formulation by Roy Britten and Eric



For the subgraph data, GS , the graphs for years 1969-1980
were constructed by creating the co-author graphs leveraging
authors only from the papers that cited the BD paper.
The number of unique authors during each year was held
consistently to 294,700 (total number of authors under consideration over the entire time period). The ordering of authors
in the graph, while arbitrary, was preserved (as is necessary)
in each year.
B. Evolution of the ScientiÔ¨Åc Field
Britten and Davidson 1969
Secondary Citations

Fig. 1. Truth citation data. The Britten-Davidson paper is considered to have
signiÔ¨Åcantly altered the Ô¨Åeld of developmental biology. Here, we present both
direct and second order (papers that cite papers that directly cite the BD
paper) citations to that paper. The second order citations indicate not only
the continued inÔ¨Çuence of the work at present time, but also highlight the
resurgence of the signiÔ¨Åcance of the work in recent years. The direct citation
data is used to construct the signal matrix S by creating the co-author network
of the authors of those papers.

In our initial analysis, we applied the residual computation
and dimensionality reduction steps of the SPG framework to
the entire Ô¨Åeld of developmental biology. The 2D projections
of the co-author networks are shown for years 1959, 1969,
1989, and 1999 in Figure 2. The shape of the projection is
clearly evolving over the course of the decades under study.
During the 1960s and 1970s the Ô¨Åeld was still relatively
diverse. In 1979 the majority of genetic work in developmental biology focused on single genes. What we see in
the progression of graphs is the (1) the constancy of genetic
approaches (2) the emergence of some (short-lived) fashions
and (3) the subsequent expansion and diversiÔ¨Åcation of genetic
and genomic approaches. By the 1980s and 1990s, the Ô¨Åeld
coheres around a single theme - the projection of the coauthorship networks are indeed consistent with the historical
study of this scientiÔ¨Åc Ô¨Åeld.
C. Author Detection

Davidson published in Science in 1969 [24]. The BrittenDavidson (BD) model for gene regulation in higher cells is, by
all possible metrics, a case of a scientiÔ¨Åc innovation. Figure 1
shows the direct and second order citations to the BD paper.
Second order citations are citations to papers that cite the BD
paper and are a good approximation for broader impacts of a
scientiÔ¨Åc idea, especially when considered together with direct
citations.
Detailed historical analysis of the reception of the BrittenDavidson model as well as of investigative pathways more
generally have revealed several interesting patterns, including
the observation that scientiÔ¨Åc innovations, judged here by their
subsequent impact on a scientiÔ¨Åc Ô¨Åeld, lead to a re-structuring
of patterns of collaboration within areas of science.
The data leveraged to create Ô¨Ålter g described in Section
II was constructed by creating a co-authorship graph of the
individuals that have cited (direct citations) the BD paper.
IV. R ESULTS AND D ISCUSSION
A. Data
For each year, from 1969-2000, we created a citation graph
Gt . The citation graphs were extracted from the Web of
Science database and covered the entire Ô¨Åeld of developmental
biology. SpeciÔ¨Åcally, the data covers the top 12 journals in
developmental biology plus Science, Nature, and PNAS. For
each year, the resulting matrix At is both unweighted and
undirected.

We applied both the ramp Ô¨Ålter and Ô¨Ålter tracking the
maximum eigenvalue of the foreground (the truth data). The
2D projection of both is shown in Figure 3. Vertices (points)
associated with the truth subgraphs are indicated in red.
It is apparent from the analysis that the ramp Ô¨Ålter does
not lead to any separation of the truth vertices. On the other
hand, the maximum eigenvalue Ô¨Ålter, g, clearly identiÔ¨Åed the
author Monroy, A. Alberto Monroy was a leading Ô¨Ågure in
Italian and European developmental biology. Monroy was one
of the Ô¨Årst prominent developmental biologist, who applied
the new concept of gene regulatory networks in the context
of molecular explanations of developmental processes in the
mid-1970s.
One of the hypothesis that we have considered is that
innovation in science as deÔ¨Åned by the publication networks
has a particular consistent structure. We tested this hypothesis
by applying the maximum eigenvalue Ô¨Ålter constructed from
the truth data covering 1969-1980 to the publication networks
covering 1990-2000. The results of the two dimensional projection are shown in Figure 4. A simple thresholding procedure
(analogous to a one dimensional clustering) produces 4 names:
(1)Voet, M, (2) Sprincl, L, (3) Mewes, HW, and (4) Murphy,
L.
From a history of science perspective, Hans-Werner Mewes
is a signiÔ¨Åcant Ô¨Ågure in this context. Mewes has been an
early proponent of Systems Biology. One of the conceptual
consequences of the notion of gene regulatory networks is
a shift away from a single gene to a genomic network



Eigenvector 2

Eigenvector 2

Eigenvector 1

Eigenvector 2

Eigenvector 2

Eigenvector 1

Eigenvector 1

Eigenvector 1

Fig. 2. SPG Analysis of the Ô¨Åeld of developmental biology. For each year, we performed the residual computation and the dimensionality reduction step.
This analysis does not perform any temporal integration. The Ô¨Ågure presents a 2-dimensional projection of the co-author network with each point on the plot
representing a single vertex (author).

paradigm, so it is not at all surprising that we detect proponents
of Systems Biology within the innovation trajectory of the
Britten-Davidson paper.
V. F UTURE W ORK
Our preliminary results are promising and illustrate beneÔ¨Åts of working across disciplines. The analysis and results
described here provide insight into the study of innovation
in science as a mathematical graph phenomenon. Since our
analysis of developmental biology networks has shown that
the graph degree distribution does not follow a power law
distribution, a potential future direction includes developing a
new residual model better suited to publication data. Additionally, we plan to consider Ô¨Ålter optimization techniques along
with noise suppression techniques as described in [25].
VI. C ONCLUSION
Applying our method of signal detection to study patterns
of innovation within science allowed us to (1) identify these
events within graphs of scientiÔ¨Åc collaboration and (2) validate
our Ô¨Åndings with concrete historical analysis of the detected

signatures. Leveraging expertise from multiple disciplines, we
are able to present a novel approach to studying scientiÔ¨Åc
innovation at scale. A future application of a reÔ¨Åned method
is to observe the innovation dynamics of science closer to real
time, which would have implications for science policy, by
allowing identiÔ¨Åcation of emerging scientiÔ¨Åc areas.
ACKNOWLEDGMENT
The authors would like to thank Benjamin Miller at MIT
Lincoln Laboratory. B. R. E. P. is supported by the NSF
under Grant No. 2011131209, and NSF Doctoral Dissertation
Research Improvement Grant No. 1256752. M. D. L. is
support by the NSF SES Grant No. 1243575.
R EFERENCES
[1] N. Biggs, E. K. Lloyd, and R. J. Wilson, Graph Theory, 1736-1936.
Clarendon Press, 1986.
[2] M. Faloutsos, P. Faloutsos, and C. Faloutsos, ‚ÄúOn power-law relationships of the Internet topology,‚Äù in Proc. SIGCOMM, 1999.
[3] T. H. Cormen, C. E. Leiserson, and R. L. Rivest, Introduction to
Algorithms. MIT Press, 1990.
[4] W. W. Zachary, ‚ÄúAn information Ô¨Çow model for conÔ¨Çict and Ô¨Åssion in
small groups,‚Äù Journal of anthropological research, pp. 452‚Äì473, 1977.



Eigenvector 2

Eigenvector 2

Eigenvector 1

Monroy, A

Eigenvector 1

Eigenvector 2

Fig. 3. Temporal integration leveraging truth. Two Ô¨Ålters were applied: a ramp Ô¨Ålter and a maximum eigenvalue of the truth subgraph Ô¨Ålter. The red colored
points indicated the known truth data (based on the S subgraph). As in Figure 2, each point represents an author in the co-author network. The ramp Ô¨Ålter
does not separate the truth from the background. The maximum eigenvalue Ô¨Ålter does identify a key individual, Monroy A.

Eigenvector 1

Fig. 4. Temporal integration on a new time period. As in Figure 2, each
point represents an author in the co-author network. The maximum eigenvalue
Ô¨Ålter constructed leveraging truth data covering the 1969-1980 time period
is applied to a new time window: 1980-2000. The SPG approach with a
truth-based Ô¨Ålter and a simple thresholding procedure identiÔ¨Åes 4 individuals,
one of which is considered a signiÔ¨Åcant Ô¨Ågure in scientiÔ¨Åc innovation in
developmental biology, Mewes, HW.

[5] B. A. M. et al., ‚ÄúVery large graphs for information extraction (vlg):
Summary of Ô¨Årst-year proof-of-concept study,‚Äù Lincoln Laboratory
Project Report VLG-1, 2013.
[6] M. E. Newman, ‚ÄúThe structure and function of complex networks,‚Äù
SIAM review, vol. 45, no. 2, pp. 167‚Äì256, 2003.
[7] D. Easley and J. Kleinberg, Networks, crowds, and markets: Reasoning
about a highly connected world. Cambridge University Press, 2010.
[8] B. A. Miller, N. T. Bliss, P. J. Wolfe, and M. S. Beard, ‚ÄúA spectral
framework for anomalous subgraph detection,‚Äù arXiv.org:1401.7702,
2014.
[9] B. A. Miller, M. S. Beard, and N. T. Bliss, ‚ÄúMatched Ô¨Åltering for
subgraph detection in dynamic networks,‚Äù in Proc. IEEE Statistical
Signal Process. Workshop, 2011, pp. 509‚Äì512.

[10] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúToward signal processing
theory for graphs and non-Euclidean data,‚Äù in Proc. IEEE Int. Conf.
Acoust., Speech and Signal Process., 2010, pp. 5414‚Äì5417.
[11] F. R. K. Chung, Spectral Graph Theory.
American Mathematical
Society, 1997.
[12] M. E. J. Newman and M. Girvan, ‚ÄúFinding and evaluating community
structure in networks,‚Äù Phys. Rev. E, vol. 69, no. 2, 2004.
[13] M. E. J. Newman, ‚ÄúFinding community structure in networks using the
eigenvectors of matrices,‚Äù Phys. Rev. E, vol. 74, no. 3, 2006.
[14] T. MifÔ¨Çin, ‚ÄúDetection theory on random graphs,‚Äù in Proc. Int. Conf.
Inform. Fusion, 2009, pp. 954‚Äì959.
[15] R. R. Nadakuditi, ‚ÄúOn hard limits of eigen-analysis based planted clique
detection,‚Äù in Proc. IEEE Statistical Signal Process. Workshop, 2012, pp.
129‚Äì132.
[16] E. Arias-Castro and N. Verzelen, ‚ÄúCommunity detection in random
networks,‚Äù 2013, preprint: arXiv.org:1302.7099.
[17] A. Sandryhaila and J. M. F. Moura, ‚ÄúDiscrete signal processing on
graphs,‚Äù IEEE Trans. Signal Process., vol. 61, pp. 1644‚Äì1656, April
2013.
[18] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, ‚ÄúThe emerging Ô¨Åeld of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular
domains,‚Äù IEEE Signal Processing Mag., vol. 30, pp. 83‚Äì98, May 2013.
[19] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúSubgraph detection using
eigenvector L1 norms,‚Äù in Advances in Neural Inform. Process. Syst. 23,
J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel, and A. Culotta,
Eds., 2010, pp. 1633‚Äì1641.
[20] E. H. Davidson, The regulatory genome: gene regulatory networks in
development and evolution. Academic Press, 2010.
[21] D. C. Krakauer, J. P. Collins, D. Erwin, J. C. Flack, W. Fontana, M. D.
Laubichler, S. J. Prohaska, G. B. West, and P. F. Stadler, ‚ÄúThe challenges
and scope of theoretical biology,‚Äù Journal of theoretical biology, vol.
276, no. 1, pp. 269‚Äì276, 2011.
[22] M. D. Laubichler, J. Maienschein, and J. Renn, ‚ÄúComputational perspectives in the history of science: To the memory of peter damerow,‚Äù
Isis, vol. 104, no. 1, pp. 119‚Äì130, 2013.
[23] M. Laubichler and J. Maienschein, ‚ÄúDevelopmental evolution,‚Äù in
The Cambridge Encyclopedia of Darwin and Evolutionary Thought,
M. Ruse, Ed. Cambridge: Cambridge University Press, 2013, pp. 375‚Äì
382.
[24] R. J. Britten and E. H. Davidson, ‚ÄúGene regulation for higher cells: a
theory,‚Äù Science, vol. 165, no. 891, pp. 349‚Äì357, 1969.
[25] B. A. Miller and N. T. Bliss, ‚ÄúToward matched Ô¨Ålter optimization for
subgraph detection in dynamic networks,‚Äù in Proc. IEEE Statistical
Signal Process. Workshop, 2012, pp. 113‚Äì116.



GOODNESS-OF-FIT STATISTICS FOR ANOMALY DETECTION IN CHUNG‚ÄìLU RANDOM
GRAPHS
Benjamin A. Miller1 , Lauren H. Stephens2 and Nadya T. Bliss1
1

MIT Lincoln Laboratory, Lexington, MA, 02420
{bamiller, nt}@ll.mit.edu
2
Massachusetts Institute of Technology, Cambridge, MA, 02139
lhs@mit.edu
ABSTRACT
Anomaly detection in graphs is a relevant problem in numerous applications. When determining whether an observation is anomalous with respect to the model of typical behavior, the notion of
‚Äúgoodness of Ô¨Åt‚Äù is important. This notion, however, is not wellunderstood in the context of graph data. In this paper, we propose three goodness-of-Ô¨Åt statistics for Chung‚ÄìLu random graphs,
and analyze their efÔ¨Åcacy in discriminating graphs generated by the
Chung‚ÄìLu model from those with anomalous topologies. In the results of a Monte Carlo simulation, we see that the most powerful
statistic for anomaly detection depends on the type of anomaly, suggesting that a hybrid statistic would be the most powerful.
Index Terms‚Äî Graph theory, signal detection theory, anomaly
detection, goodness of Ô¨Åt, probabilistic models

1. INTRODUCTION
A graph G = (V, E) is deÔ¨Åned as a set of vertices (V ) that are
interconnected by a set of edges (E). Graphs are useful in many applications in which relationships (edges) between entities (vertices)
are of interest. As they are used in a wide variety of disciplines, the
problem of anomaly detection in graphs has gained signiÔ¨Åcant interest in the past several years (see, e.g., [1]). Detecting anomalies
in a graph‚Äôs topology is relevant to a host of applications, such as
the detection of strange or malicious behavior in computer or social
networks.
Recent work has focused on developing a statistical detection
framework for graphs, akin to that for Euclidean data [2, 3]. The
central tool of this framework is the modularity matrix [4]. The modularity matrix B of an unweighted, undirected graph G is deÔ¨Åned as
B =A‚àí

kkT
.
2|E|

Here A is the adjacency matrix of G, where the entry in the ith row
and jth column of A is 1 if {vi , vj } ‚àà E and is 0 otherwise; and k
is the degree vector of G, with the ith entry in k being the number
of edges adjacent to vertex i. In a random graph in which the probability of an edge between two vertices is proportional to the degrees
of the associated vertices, kkT /(2|E|) is the expected value of A.
This work is sponsored by the Department of the Air Force under Air Force
Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the author and are not necessarily endorsed by the
United States Government.

978-1-4673-0046-9/12/$26.00 ¬©2012 IEEE

3265

Thus, we consider the modularity matrix a graph ‚Äúresiduals‚Äù matrix,
representing the difference between the observed and expected adjacency matrices.
This interpretation broaches an important topic in signal detection: goodness of Ô¨Åt. Given an observation, it is useful to understand
how well the data Ô¨Åt the assumed model of ‚Äúnormal‚Äù behavior. While
this notion is well understood for linear models, it is not at all mature
in the context of graphs. The purpose of this paper is to investigate
the use of goodness-of-Ô¨Åt statistics in a speciÔ¨Åc random graph model
to determine whether or not an observed graph was generated by that
model, i.e., to reject the hypothesis that the graph was generated under the model if the data do not properly Ô¨Åt.
The remainder of this paper is organized as follows. In Section 2 we describe our problem model, including our null model,
the Chung‚ÄìLu random graph; and several anomalous alternatives.
Section 3 introduces the goodness-of-Ô¨Åt statistics we use to test our
observations. Section 4 presents empirical results, demonstrating
detection performance for each statistic paired with each alternative
model. We discuss interesting phenomena observed in the results in
Section 5, and in Section 6 we summarize and outline future work.
2. PROBLEM MODEL
2.1. The Chung‚ÄìLu Model
In this work, we focus on determining whether or not an observed
graph is generated by the Chung‚ÄìLu random graph model [5]. Under
this model, an unweighted, undirected graph G is created according
to the following process. Each vertex v ‚àà V is given an expected
degree dv . The probability that an edge occurs between two vertices
v, u ‚àà V is equal to
d d
P v u ,
x‚ààV dx
that is, it is proportional to the product of the vertices‚Äô expected degrees. (It is easily veriÔ¨Åed that the probabilities of edges adjacent to
v sum to dv .) We allow self-loops (edges from a vertex to itself), as
it simpliÔ¨Åes our analysis, although an extension to the case without
these edges is possible. This model requires that the largest expected
degree of any vertex be at most the square root of the sum of all expected degrees, so that no probability is greater than 1.
In an alternative formulation, we assign each vertex v a weight
wv ‚àà [0, 1]. The two deÔ¨Ånitions are equivalent, with the change in
parameters given by
dv
wv = qP
u‚ààV

and dv = wv
du

X

wu .

u‚ààV

ICASSP 2012

In matrix form, let w ‚àà [0, 1]|V | be a vector of weights. Then the
expected value of the adjacency matrix A of a graph generated by
the Chung‚ÄìLu model with weights w is given by E[A] = P =
wwT . We will refer to P as the probability matrix for the graph,
which is equal to the expected value since each edge is the result of
a Bernoulli trial with a 0-or-1 outcome.
Given this probability structure, certain subgraphs or topologies
are unlikely to occur. We now discuss the alternative models that we
wish to reject as being non-Chung‚ÄìLu.
2.2. Alternative Models
Two alternatives start with a Chung‚ÄìLu background graph, and connect it to an anomalous subgraph, i.e., a subgraph that violates the
assumptions of the Chung‚ÄìLu model. In both cases, the probability
matrix of the graph has the form
!
pin 1Ns 1TNs pout 1Ns 1TNb
P =
.
pout 1Nb 1TNs
wb wbT
Here 1N is a column vector of N ones, Nb is the number of background vertices, Ns is the number of anomalous subgraph vertices,
and wb ‚àà [0, 1]Nb is the vector of weights for the Chung‚ÄìLu background. The values for pin and pout are, respectively, the probability
of an edge between two subgraph vertices and the probability of an
edge between a subgraph vertex and a background vertex. Note that
the expected degree of a vertex in the subgraph is
pin Ns + pout Nb ,

As discussed in [7], while goodness of Ô¨Åt is an important concept
in statistical modeling, this concept is underdeveloped in the context
of graphs. In this section we propose 3 goodness-of-Ô¨Åt statistics for
Chung‚ÄìLu graphs that we will use in our experiments.
3.1. Spectral Norm (SN)
Our Ô¨Årst test statistic measures how far the observed graph is from its
expected value. We use the spectral norm of the difference between
the adjacency matrix and its expected value, i.e.,
A ‚àí E[A] = A ‚àí wwT .
This is the maximum eigenvalue (in terms of absolute value) of the
matrix consisting of the observed minus expected edges in the graph.
If the observed degree vector k is equal to the expected degree vector
d, then this is the same as the maximum eigenvalue of the graph‚Äôs
modularity matrix.
3.2. Least Squares CoefÔ¨Åcient (LSC)
Another metric for the difference between the observed and expected
graph is the least squares coefÔ¨Åcient, that is, the coefÔ¨Åcient that optimally Ô¨Åts the observed graph to the expectation in a least squares
sense. This statistic is expressed as
arg minA ‚àí Œ≥wwT F ,
Œ≥

(1)

with the optimal value given by

and the expected degree of vertex i in the background is
pout Ns + wb (i)wb 1 ,

3. TEST STATISTICS

(2)

where wb (i) is the ith entry in wb .
Manipulating these values, we can create a topology that is unlikely to occur under a Chung‚ÄìLu model. If we let pin be large and
pout be small, then the subgraph will be a tightly-connected cluster
with little connectivity to the background. Conversely, if we let pout
be large and pin be small, then the subgraph will consist of highdegree vertices that are unlikely to be connected to each other. Both
of these phenomena are anomalous under the Chung‚ÄìLu model. Indeed, considering an extreme case in which pin = 1 and pout = 0,
the subgraph vertices will have degree Ns , but will never be connected to any background vertices, regardless of their expected degree. At the opposite extreme, we may set pin = 0 and adjust pout
so that, under the Chung‚ÄìLu model, the probability of an edge between subgraph vertices is arbitrarily close to 1. We will refer to the
high pin , low pout case as a cluster anomaly, and the high pout , low
pin case as a hubs anomaly.
We are also interested in cases where the graph is anomalous
throughout its topology, and for this purpose we use the R-MAT
Kronecker graph [6]. An R-MAT graph is generated by an iterative
procedure where at each iteration, an edge is selected according to a
probability matrix deÔ¨Åned by the n-fold Kronecker product of a 2√ó2
probability matrix. This procedure continues for a Ô¨Åxed number of
iterations or until the graph has a certain number of edges. (Since
we are dealing with unweighted, undirected graphs, we do not increase the weight if an edge is chosen multiple times, and we use the
‚Äúclip-and-Ô¨Çip‚Äù procedure from [6] to undirect the graph.) The probability matrix for this alternative does not have the rank-1 structure of
a Chung‚ÄìLu graph, and this should create a topology unlikely under
the Chung‚ÄìLu model.

3266

Œ≥min =

wT Aw
.
w42

One convenient property of this statistic is that it is relatively easy to
analyze. It is not difÔ¨Åcult to show that, under the Chung‚ÄìLu model,
the expected value of Œ≥min is 1, and its variance is
Var(Œ≥min ) =

2w63 ‚àí 2w84 ‚àí w66 + w88
.
w82

(3)

3.3. Minimum Neighborhood Likelihood (MNL)
The Ô¨Ånal statistic is derived from the probability that a given graph
G would be created by a Chung-Lu model with a weight vector w.
The likelihood of an observed graph under the Chung‚ÄìLu model is
expressed as
|V | |V |
Y
Y

((Aij wi wj ) + (1 ‚àí Aij )(1 ‚àí wi wj ))),

i=1 j=i

where Aij is the entry in the ith row and jth column of the adjacency
matrix. For the types of non-Chung‚ÄìLu behavior we examined, however, the graph likelihood was ineffective at distinguishing between
the Chung-Lu and alternative models. We reÔ¨Åned this statistic in an
attempt to improve its detection power. Rather than the likelihood of
the entire graph, we use the least likely 1-hop vertex neighborhood.
The minimum neighborhood likelihood for an observed graph is
min
i

|V |
Y
j=1

((Aij wi wj ) + (1 ‚àí Aij )(1 ‚àí wi wj )) .

Alternative
R-MAT-1
R-MAT-2
R-MAT-3
Cluster
Hubs

SN
AUC
0.561
0.758
0.998
0.981
0.685

EER
0.538
0.688
0.983
0.943
0.635

True Weights
LSC
AUC
EER
0.555 0.541
0.981 0.931
0.547 0.532
0.509 0.506
0.882 0.799

MNL
AUC
EER
0.483 0.489
0.730 0.668
0.440 0.447
0.496 0.497
0.711 0.656

SN
AUC
0.558
0.716
0.999
0.977
0.656

EER
0.537
0.657
0.985
0.933
0.614

Estimated Weights
LSC
AUC
EER
0.785 0.713
1.000 1.000
0.957 0.886
0.597 0.568
0.999 0.987

MNL
AUC
EER
0.483 0.487
0.724 0.659
0.426 0.448
0.495 0.497
0.717 0.660

Table 1. Equal-error rate and area under the curve performance for the 3 test statistics, using given and estimated weights. For each alternative
model, the most powerful statistic in terms of EER is highlighted.

4. EXPERIMENTAL RESULTS
In each of the following experiments, we ran a 10,000-trial Monte
Carlo simulation under the null model and one of the alternatives,
and computed our test statistics for each graph. For the cluster and
hub alternatives, we Ô¨Årst generate a 1024-vertex R-MAT graph with
average degree 10, and use the degree vector of this graph to compute
the background weights wb , yielding a background with a powerlaw
degree distribution. For the cluster anomaly, we use 8 vertices with
pin = 0.9375 and pout = 2/(8 ¬∑ 1024), resulting in a subgraph
with an average internal degree of 7.5 and 2 edges, in expectation,
between the background and the subgraph. For the hubs anomaly, we
add 5 vertices with pin = 1/25 and pout = 0.065, yielding expected
external degrees of over 66 and 0.6 expected internal edges. In both
cases, we compare the resulting alternative models to a Chung‚ÄìLu
graph whose expected degree vector is computed by equations (1)
and (2). We are, thus, comparing the test statistics of two random
graphs with the same expected degree vector.
For the R-MAT alternatives, we use 3 base probability matrices,
‚Äû
¬´
‚Äû
¬´
0.3
0.238
0.2 0.2
p1 =
, p2 =
,
0.238 0.224
0.2 0.4
‚Äû
¬´
0.35
0.1625
and p3 =
,
0.1625 0.325
and we will refer to the models resulting from these as R-MAT-1,
R-MAT-2 and R-MAT-3, respectively. In each case, we used the 10fold Kronecker product to deÔ¨Åne the edge probability matrix (resulting in a 1024-vertex graph), and ran the algorithm for 10240 iterations. Letting pÃÇij be the proabability of an edge between vertex i and
vertex j being added in a single iteration, the probability of an edge
occurring between these vertices before the algorithm terminates is
pij = 1 ‚àí (1 ‚àí pÃÇij )10240 .
For the Chung‚ÄìLu graphs we compare to the
P R-MAT graphs, we use
the expected degree vector given by di = j pij , again making the
expected degree vectors the same for the null and alternative models.
Since, in an anomaly detection problem, we may not have access
to the true model parameters, we evaluate performance with both
given and estimated weights. For an estimated weight vector, we use
the simple, closed-form estimator
wÃÇ = k/

p
2|E|,

i.e., we substitute the observed degree for the expected degree.
Receiver operating characteristic (ROC) curves for these simulations are shown in Fig. 1, with performance summarized in terms

3267

of equal-error rate (EER) and area under the curve (AUC) in Table 1. Using the R-MAT-1 alternative, which has the most balanced probability matrix, detection performance is little better than
chance except using LSC with estimated weights. For R-MAT-2,
which is much more biased toward one corner of the probability
matrix, all statistics have better-than-chance detection performance,
with LSC yielding near-perfect detection with estimated and given
weights (again, better performance with estimated weights). One
interesting observation is that while MNL is somewhat effective at
discriminating Chung‚ÄìLu from R-MAT graphs in this case, the minimum likelihood is actually higher (i.e., less unlikely) under the alternative (we account for this in Table 1). For MNL and SN, performance with given and estimated weights is similar. For R-MAT-3,
in which the probability matrix is much more concentrated on the
diagonal, the spectral norm is the most powerful statistic. The least
squares coefÔ¨Åcient with estimated weights is a close second, with the
other cases not much better than chance.
For the other two alternatives, we see similar trends. For the
cluster anomaly, like the R-MAT-3 case, we get excellent detection
performance with SN, followed by LSC with estimated weights, and
near-chance for all others. For hubs we see a simimlar behavior
to R-MAT-2, with near-perfect detection using LSC with estimated
weights, good performance for LSC with given weights, and lower,
but still better-than-chance, performance for the other statistics.
5. DISCUSSION
One particularly intriguing phenomenon in the results is the signiÔ¨Åcant increase in anomaly detection performance using the estimated
weights rather than given weights for the LSC statistic. On closer inspection, the distribution of test statistics (under both the null and alternative models) using estimated weights has about the same mean
as when the true weights are used, but the variance is much tighter;
in all cases at least a factor of 7 lower. Using the true weights, we
conÔ¨Årm that the sample variance in the simulation data is very close
to (3). The variance of the LSC statistic with estimated weights is
expressed as
" P P P
!2 #
"P P P
#2
i
j
 Aij ki kj k
i
j
 Aij ki kj k
P P 2 2
P P 2 2
E
‚àíE
.
i
j ki kj
i
j ki kj
Analysis of this quantity is complicated and beyond the scope of this
paper, but an additional Monte Carlo simulation conÔ¨Årms that, for
the degree distributions in these experiments, it is much smaller than
the variance expressed in (3).
Considering the two cases in which SN is the most powerful
statistic, it is notable that R-MAT-3 is the R-MAT with the most clustering (i.e., with the 2 √ó 2 probability matrix that puts most entries in

the lower right or upper left quarters). Due to this probability structure, the resulting alternative graph will be partitioned, like the cluster anomaly, in a way such that connections within the two partitions
are much more likely than those across the partition. Eigenspace
techniques have long been associated with graph partitioning, and
the same underlying phenomena may be at work here.
Finally, considering the MNL statistic under R-MAT-2, we
would like to understand why the lowest neighborhood likelihood is
larger under the alternative than under the null model. Looking at
the probability matrices for the null and alternative models, we see
that, around certain high-degree vertices, the R-MAT model further
biases the graph to have edges with high likelihood under Chung‚Äì
Lu. Since high-degree vertices tend to have the lowest likelihoods
(due to the probability of occurrence for any particular edge being
rather small), this causes higher minimum likelihoods under the
alternative, resulting in the behavior we see in Fig. 1b.
6. SUMMARY
In this paper we investigate the use of goodness-of-Ô¨Åt statistics to
reject the hypothesis that an observed graph was generated by a particular model, speciÔ¨Åcally the Chung‚ÄìLu random graph model. We
propose 3 goodness-of-Ô¨Åt statistics and analyze their power to discriminate between Chung‚ÄìLu graphs and several alternative models.
Simulation results demonstrate that the spectral norm of the graph
residual performs the best when there is a partitioning of the nodes
in which internal connectivity is much more likely than connectivity across the partition, while a least squares Ô¨Åtting coefÔ¨Åcient is the
most powerful statistic in other cases. Future work will include a
deeper theoretical study of the test statistic distributions, as well as
the integration of several statistics to optimize detection power.
7. ACKNOWLEDGMENT
The authors wish to thank Nicholas Arcolano, Karl Ni, Matthew
Schmidt and Patrick Wolfe for many helpful comments and conversations.
8. REFERENCES
[1] C. C. Noble and D. J. Cook, ‚ÄúGraph-based anomaly detection,‚Äù
in Proc. KDD, 2003, pp. 631‚Äì636.
[2] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúToward signal processing theory for graphs and non-Euclidean data,‚Äù in Proc.
ICASSP, 2010, pp. 5414‚Äì5417.
[3] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúSubgraph detection
using eigenvector L1 norms,‚Äù in Advances in Neural Inform.
Process. Syst. 23, J. Lafferty, C. K. I. Williams, J. Shawe-Taylor,
R.S. Zemel, and A. Culotta, Eds., pp. 1633‚Äì1641. 2010.
[4] M. E. J. Newman, ‚ÄúFinding community structure in networks
using the eigenvectors of matrices,‚Äù Phys. Rev. E, vol. 74, no. 3,
2006.
[5] F. Chung, L. Lu, and V. Vu, ‚ÄúThe spectra of random graphs with
given expected degrees,‚Äù PNAS, vol. 100, no. 11, pp. 6313‚Äì
6318, 2003.
[6] D. Chakrabarti, Y. Zhan, and C. Faloutsos, ‚ÄúR-MAT: A recursive model for graph mining,‚Äù in Proc. SIAM Int. Conf. Data
Mining, 2004, vol. 6, pp. 442‚Äì446.
[7] E. D. Kolaczyk, Statistical Analysis of Network Data: Methods
and Models, Springer, 2009.

3268

Fig. 1. ROC curves demonstrating the power of the 3 test statistics
to discriminate between graphs generated by a Chung‚ÄìLu model and
those generated by one of the 5 anomalous alternatives.

DYNAMIC DISTRIBUTED DIMENSIONAL DATA MODEL (D4M) DATABASE AND
COMPUTATION SYSTEM
Jeremy Kepner, William Arcand, William Bergeron, Nadya Bliss, Robert Bond, Chansup Byun, Gary
Condon, Kenneth Gregson, Matthew Hubbell, Jonathan Kurz, Andrew McCabe, Peter Michaleas,
Andrew Prout, Albert Reuther, Antonio Rosa, Charles Yee
MIT Lincoln Laboratory1, Lexington, MA
ABSTRACT
A crucial element of large web companies is their ability to
collect and analyze massive amounts of data. Tuple store
databases are a key enabling technology employed by many
of these companies (e.g., Google Big Table and Amazon
Dynamo). Tuple stores are highly scalable and run on
commodity clusters, but lack interfaces to support efficient
development of mathematically based analytics. D4M
(Dynamic Distributed Dimensional Data Model) has been
developed to provide a mathematically rich interface to
tuple stores (and structured query language "SQL"
databases). D4M allows linear algebra to be readily applied
to databases.
Using D4M, it is possible to create
composable analytics with significantly less effort than
using traditional approaches. This work describes the D4M
technology and its application and performance.
Index Terms‚Äî associative array, database, tuple store,
linear algebra, fuzzy algebra
1. INTRODUCTION
Modern database analysis in the areas of healthcare, internet
search, finance, and network security are outgrowing the
capabilities of current technologies. The increasing size of
the data (doubling every year), the increasing diversity of
the data (e.g., web pages, documents, audio, images, and
video), and the increasing complexity of the operations (e.g.,
ingestion, scanning, link analysis, and importance scoring)
all present tremendous challenges.
The standard approach for handling the increasing size
of data is to increase the storage capacity at the cost of
increasing the time it takes to access any particular data
item.
Solving this problem requires a transparent
mechanism (e.g., distributed arrays) for adding computation
and network bandwidth as storage capacity is increased.
The standard approach for handling the increasing
diversity of data is to increase the number of
databases/tables at the cost of increasing the effort required
to make the data available to users. Solving this problem

requires a general mechanism (e.g., tuple stores) for adding
a wide variety of data into a single database table.
The standard approach for handling the increasing
complexity of operations is to increase the size of the
functions at the cost of increasing the effort required to build
these functions.
Solving this problem requires a
composable mechanism (e.g., multi-dimensional associative
arrays) for creating operations of increasing complexity
without increasing their relative effort.
A final challenge is that the above problems are anticorrelated. Addressing one problem will typically result in
the other issue becoming more difficult. For example,
adding computing and network resources to a database
system increases the difficulty of adding more diverse data
and adding more complex operations. Thus, a viable
solution must address all of the issues simultaneously.
The goal of the Dynamic Distributed Dimensional Data
Model (D4M) is to combine the advantages of distributed
arrays, tuple stores, and multi-dimensional associative
arrays to create a database and computation system that
solves the challenges associated with increasing data size,
data diversity and operation complexity. Our prototype
implementation of D4M has demonstrated simultaneous
improvement in all of these dimensions when compared to
current standard approaches (e.g., Java + SQL).
2. D4M ARCHITECTURE
D4M addresses the challenges presented in the previous
section by using a layered software architecture that
addresses each challenge in its own layer (Fig. 1). The top
layer consists of composable associative arrays that provide
a one-to-one correspondence between database queries and
linear algebra. Associative arrays can be both the input and
output of a wide range of database operations and allow
complex operations to be composed with a small number of
statements. The middle layer consists of several parallel
computation technologies (e.g., pMatlab [1,2], MatlabMPI
[3], and gridMatlab [4]) that allow associative arrays to be

1

This work is sponsored by the Department of the Air Force under Air Force contract FA8721-05-C-0002. Opinions,
interpretations, conclusions and recommendations are those of the author and are not necessarily endorsed by the United
States Government.

978-1-4673-0046-9/12/$26.00 ¬©2012 IEEE

5349

ICASSP 2012

distributed efficiently across a parallel computer.
Furthermore, the pieces of the associative array can be
bound to specific parts of one more databases to optimize
the performance of data insertion and query across a parallel
database system. The bottom layer consists of databases
(e.g., HBase [5] and Accumulo [6]) running on parallel
computation hardware. D4M can use any type of database.
D4M can fully exploit the power of databases that use an
internal sparse tuple representation (e.g., a row/col/val triple
store) to store all data regardless of type. The D4M
approach provides several advantages and improvements
over existing methods, specifically: D4M Represents
complex database operations and queries as composable
algebraic operations on associative arrays; D4M Provides
distributed arrays for parallel database operations; D4M
Transparently handles diverse data types using a tuple store.

A(‚Äôalice : bob ‚Äô,:) rows alice to bob
A(1:2,:)
first two rows
A == 47.0
all entries equal to 47.0

The composability of associative arrays stems from the
ability to define fundamental mathematical operations
whose results are also associative arrays. Given two
associative arrays A and B, the results of all the following
operations will also be associative arrays
A + B

A ‚Äì B

A & B

A|B

A*B

Associative array composability can be further grounded
in the mathematical closure of semirings (i.e., linear
algebraic ‚Äúlike‚Äù operations) on multi-dimensional functions
of infinite strict totally ordered sets (i.e., sorted strings). In
addition, many of the linear algebraic properties of fuzzy
algebra can also be directly applied: linear independence
[7], strong regularity [8], and uniqueness [9].
Finally, associative arrays can represent complex
relationships in either a sparse matrix or a graph form (Fig.
2). Associative arrays are a natural data structure for
performing both matrix and graph algorithms.
Such
algorithms are the foundation of many complex database
operations across a wide range of fields [10]. Measurements
using the D4M prototype indicate that these algorithms can
be implemented with significantly less coding effort when
compared standard approaches [11].

Fig. 1. D4M Matlab prototype architecture. At the top is the user
application consisting of a series of query and analysis steps. In
the middle is the parallel library that hides the parallel mapping of
the operations. On the bottom are the databases (typically tuple
stores) running on parallel computing hardware.
Fig. 2. A sparse associative 2D array and its graph dual.

3. ASSOCIATIVE ARRAYS
Associations between multidimensional entities (tuples)
using number/string keys and number/string values can be
stored in data structures called associative arrays. For
example, in two dimensions an associative array entry might
be
A(‚Äôalice ‚Äô,‚Äôbob ‚Äô) = ‚Äôtalked ‚Äô

or
A(‚Äôalice ‚Äô,‚Äôbob ‚Äô) = 47.0

The above tuples have a one-to-one correspondence with the
triple store representations
(‚Äôalice ‚Äô,‚Äôbob ‚Äô,‚Äôtalked ‚Äô)

and
(‚Äôalice ‚Äô,‚Äôbob ‚Äô,47.0)

Constructing complex composable query operations can
be expressed using simple array indexing of the associative
array keys and values, which themselves return associative
arrays. For example
A(‚Äôalice ‚Äô,:)
A(‚Äôalice bob ‚Äô,:)
A(‚Äôal* ‚Äô,:)

alice row
alice and bob rows
rows beginning with al

5350

4. DISTRIBUTED ARRAYS
Distributed arrays provide a simple mechanism for writing
efficient parallel programs [1]. When an algorithm is run in
parallel, the same algorithm (or code) is run by every
instance of the program (on one or more processors) with a
unique identity number PID = 0‚Ä¶NP-1. This execution
model is referred to as the Single-Program Multiple-Data
(SPMD) computation model.
In distributed array
programming, it is necessary to map the tuples of an array
onto a set of PIDs. This mapping process allows each PID to
know which tuples of a distributed it ‚Äúowns.‚Äù In pMatlab
this process is supported by adding a ‚Äúmap‚Äù object to the
construction of an array. A map contains information on the
method used to assign tuples to PIDs. Thus it is possible to
formally separate the execution of a program from how it is
mapped onto a parallel processor. The primary benefits of
this approach are simplicity (making a serial program a
parallel program can be done with just a few lines of code)
and performance (distributing arrays across processors

6. TECHNOLOGY COMPARISON
D4M provides a database and computation system that
combines composable associative arrays, distributed arrays,
and tuple stores in an integrated manner. Table 1 compares
various other technologies having aspects of D4M. Note
that while there are many distributed array technologies,
none ‚Äì except D4M ‚Äì implement an associative array.
Likewise, D4M is the first implementation of multidimensional numeric associative arrays and the first
implementation of composable associative arrays. Finally,
D4M is the only associative array technology that can take
advantage of the features of a tuple store.

X
X

VSIPL++

UPC

HPF
X
X

X
X

D4M

X

pMatlab

X
X

MPI

Feature
Associative Array
1D
X
2D
String key/value
X
Numeric key/value
Composable query
Composable compute
Tuple Store
Parallel
Distributed array

HBase

Web pages, documents, audio, images, and video all
produce very different kinds of data. Traditional databases
require different tables to handle these data. Tuple stores
handle all of this data by treating them all as key/value pairs.
This greatly simplifies the design of the database and allows
for significant performance improvements.
For example, consider a traditional database table where
each row represents the keywords in the document. Column
names of this table might be ‚Äúkeyword1‚Äù, ‚Äúkeyword2‚Äù, ‚Ä¶
To find a row with a particular keyword entry requires a
complete scan of the table or the construction of an index of
all the entries. In a row/col/val triple store each row
represents a document and the column keys can be the
actual keywords themselves.
The Hadoop [12] distributed file system (HadoopDFS) is
an open source distributed file system modeled on the
Google file system [13]. Hadoop is a replicated block based
distributed filesystem optimized for handling very large
blocks and is well suited for managing large files that are
larger than a typical storage device. HBase [5], Accumulo,
and other ‚ÄúBig Table Like‚Äù databases leverage the
HadoopDFS by modeling Google Big Table [14]. These
databases are designed for data mining applications that do
little read-modify-write and where statistical consistency is
more than adequate. Under these the relaxed restrictions
there is potential to get a sizable increase in performance.
These databases are typically ‚ÄúNoSQL‚Äù databases that have
their own custom interfaces.
D4M associative arrays provide a one-to-one mapping
onto the tables in a tuple store that makes complex
manipulations simple to code. Storing both the table and its
logical transpose in the database allows for all rows and
columns to be searched efficiently without the need to build
specialized indexes. D4M associative arrays can make both
the insertion and retrieval of data from transpose pairs
transparent to the user.

SQL

5. TUPLE STORES

Table 4. Technology Comparison. D4M uniquely supports
composable multi-dimensional associative arrays on parallel
computers and tuple store databases. Key: SQL (System Query
Language), MPI (Message Passing Interface), HPF (High
Performance Fortran), UPC (Universal Parallel C), VSIPL++
(Vector, Signal and Image Processing Library).

Perl

allows each processor to easily work on data that is in its
own memory).

X
X

X
X
X
X
X
X
X
X
X

7. APPLICATION EXAMPLE
To illustrate the use of D4M consider a facet search on the
document keyword table A shown in Fig. 3. In this context
facet search selects the subset of documents containing a set
of keywords and then computes the histogram of all the
keywords in this document subset. Facet search is particular
useful in helping a user build searches by providing
guidance as to the most popular keywords as their search
narrows. Facet search is a highly dynamic query because it
is not possible to compute the histograms of all sets of
keywords in advance.
Fig. 3. Facet Search in D4M.
Table A stores a list of documents
and their keywords.
Selecting
keywords UN and Carl indicate
that the documents c.pdf and
e.ppt contain both. Selecting the
documents c.pdf and e.ppt and
summing the occurrences of their
keywords retrieves the facets DC
and Bob.

Facet search in D4M begins with choosing keywords in
the table
x = ‚ÄôUN ‚Äô
and
y = ‚ÄôCarl ‚Äô
Next, all documents that contain both of those keywords are
found
B = (sum(A(:,[x y]),2) == 2)

Finally, the distribution of keywords in that set of
documents is computed
F = transpose(B) * A(row(B),:)

This complex query can be performed efficiently in just two
lines of D4M code that perform two database queries (one
column query and one row query). If the underlying table is
a transpose table pair, then both of these queries are be

5351

performed efficiently in a manner that is completely
transparent to the user. Implementing a similar query in
Java and SQL takes hundreds of lines of code.
8. PERFORMANCE





Graph500 [15] is derived from the Graph Analysis
benchmark [16] and is reflective of a new class of graph
based computations. For large problems Graph500 becomes
a database benchmark. Specifically, the initialization phase
measures the insert rate into a database. The performance of
D4M is measured by implementing the Graph Analysis
benchmark. Figs. 4 and 5 shows the insert performance of
D4M and D4M+Accumulo. These results are consistent
with the performance of the native Java interface and deliver
near the theoretical performance limits of the hardware.

analytics.
D4M has been developed to provide a
mathematically rich interface to tuple stores (and structured
query language "SQL" databases). Using D4M, it is possible
to create composable analytics with significantly less effort
than using traditional approaches.
10. REFERENCES
[1] J. Kepner, Parallel Matlab for Multicore and Multinode
Computers, SIAM Press, Philadelphia, 2009.
[2] N. Bliss and J. Kepner, ‚ÄúpMatlab Parallel Matlab Library,‚Äù
International Journal of High Performance Computing
Applications: Special Issue on High Level Programming
Languages and Models, J. Kepner and H. Zima (editors), Winter
2006 (November).
[3] J. Kepner and S. Ahalt, ‚ÄúMatlabMPI,‚Äù Journal of Parallel and
Distributed Computing, vol. 64, issue 8, August, 2004
[4] N. Bliss, R. Bond, H. Kim, A. Reuther, and J. Kepner,
‚ÄúInteractive Grid Computing at Lincoln Laboratory,‚Äù Lincoln
Laboratory Journal, vol. 16, no. 1, 2006.







[5] Apache HBase http://hbase.apache.org/
[6] Apache Accumulo http://incubator.apache.org/accumulo/
[7] J. Plavka, ‚ÄúLinear Independences in Bottleneck Algebra and
their Coherences with Matroids,‚Äù Acta Math. Univ. Comenianae,
vol. LXIV, no. 2, pp. 265‚Äì271, 1995.



[8] P. Butkovic, ‚ÄúStrong Regularity of Matrices - a Survey of
Results,‚Äù Discrete Applied Mathematics, vol. 48, pp. 45-68, 1994.

	

 

	





	






Fig. 4. Single process D4M and D4M+AccumuloDB insert rates
(top) versus the number of table entries. Curves are shown for
standalone D4M (in memory) and D4M connected to the
Accumulo database (in storage).

 














[9] M. Gavalec and J. Plavka, ‚ÄúSimple Image Set of Linear
Mappings in a Max‚ÄìMin Algebra,‚Äù Discrete Applied Mathematics,
vol. 155, pp. 611 ‚Äì 622, 2007.
[10] J. Kepner and J. Gilbert (editors), Graph Algorithms in the
Language of Linear Algebra, SIAM Press, Philadelphia, 2011.
[11] B.A. Miller, N. Arcolano, M.S. Beard, N.T. Bliss, J. Kepner,
M.C. Schmidt, and P.J. Wolfe, ‚ÄúA Scalable Signal Processing
Architecture for Massive Graph Analysis,‚Äù submitted.
[12] HadoopDFS http://hadoop.apache.org/hdfs/
[13] K. Shvachko, H. Kuang, S. Radia, and R. Chansler, ‚ÄúThe
Hadoop Distributed File System,‚Äù in 26th IEEE Symposium on
Mass Storage Systems and Technologies, 3-7 May, 2010.




!











[14] F. Chang, J. Dean, S. Ghemawat, W. Hsieh, D. Wallach, M.
Burrows, T. Chandra, A. Fikes, and R. Gruber, ‚ÄúBigtable: A
Distributed Storage System for Structured Data,‚Äù ACM
Transactions on Computer Systems, Volume 26 Issue 2, June 2008.



[15] Graph500 http://www.graph500.org

Fig. 5. Parallel D4M and D4M+AccumuloDB insert rates (top)
versus number of insert processes on a single node system. Curves
are shown for standalone D4M (in memory) and D4M connected
to the Accumulo database (in storage).

9. CONCLUSIONS
Tuple store databases are a key enabling technology for
larges scale data analysis. Tuple stores are highly scalable
and run on commodity clusters, but lack interfaces to
support efficient development of mathematically based

5352

[16] D. Bader, K. Madduri, J. Gilbert, V. Shah, J.y Kepner, T.
Meuse, and A. Krishnamurthy, ‚ÄúDesigning Scalable Synthetic
Compact Applications for Benchmarking High Productivity
Computing Systems,‚Äù CT Watch, Vol 2, Number 4A, November,
2006.

Circuit-Switched Memory Access in
Photonic Interconnection Networks for
High-Performance Embedded Computing
Gilbert Hendry‚àó , Eric Robinson‚Ä† , Vitaliy Gleyzer‚Ä† , Johnnie Chan‚àó ,
Luca P. Carloni‚Ä° , Nadya Bliss‚Ä† and Keren Bergman‚àó
‚àó Lightwave

Research Lab, Department of Electrical Engineering, Columbia University, New York, NY
‚Ä† Lincoln Laboratories, MIT, Lexington, MA
‚Ä° Computer Science Department, Columbia University, New York, NY

Abstract‚Äî As advancements in CMOS technology trend toward ever increasing core counts in chip multiprocessors for
high-performance embedded computing, the discrepancy between
on- and off-chip communication bandwidth continues to widen
due to the power and spatial constraints of electronic off-chip
signaling. Silicon photonics-based communication offers many
advantages over electronics for network-on-chip design, namely
power consumption that is effectively agnostic to distance traveled
at the chip- and board-scale, even across chip boundaries. In this
work we develop a design for a photonic network-on-chip with
integrated DRAM I/O interfaces and compare its performance
to similar electronic solutions using a detailed network-on-chip
simulation. When used in a circuit-switched network, silicon
nanophotonic switches offer higher bandwidth density and low
power transmission, adding up to over 10√ó better performance
and 3-5√ó lower power over the baseline for projective transform,
matrix multiply, and Fast Fourier Transform (FFT), all key
algorithms in embedded real-time signal and image processing.

I. I NTRODUCTION
Many important classes of applications including personal
mobile devices, image processing, avionics, and defense applications such as aerial surveillance require the design of highperformance embedded systems. These systems are characterized by a combination of real-time performance requirements,
the need for fast streaming access to memory, and very
stringent energy constraints [12], [46], [50]. While commodity
general purpose processors offer a cheap and customizable
solution, they typically do not meet the power and performance
requirements for the systems in question. For this reason,
specialized chip multiprocessors (CMPs) are used.
As the number of cores in CMPs scale to provide greater
on-chip computational power, communication becomes an
increasing contributor to power and performance. The gap
between the available off-chip bandwidth and that which is required to appropriately feed the processors continues to widen
under current memory access architectures. For many highperformance embedded computing applications, the bandwidth
available for both on- and off-chip communications can play a
¬ß This work is sponsored by Defense Advanced Research Projects Agency
(DARPA) under Air Force contract FA8721-05-C-0002, DARPA MTO under
grant ARL-W911NF-08-1-0127, the NSF (Award #: 0811012), and the FCRP
Interconnect Focus Center (IFC).. Opinions, interpretations, conclusions and
recommendations are those of the author and are not necessarily endorsed by
the United States Government.

vital role in efÔ¨Åcient execution due to the use of data-parallel
or data-centric algorithms.
Unfortunately, current electronic memory access architectures have the following characteristics that will impede performance scaling and energy efÔ¨Åciency for applications that
require large memory bandwidths:
‚Ä¢ Distance-Dependant. Electronic I/O wires must often be
path-length matched to reduce clock skew. In addition,
there are limitations on the length of these wires which
constrains board layout and scalability [19].
‚Ä¢ Low I/O Density. Electronic I/O wires are predicted to
have pitches on the order of around 80 microns [18]. Increasing the available off-chip communication bandwidth
will become difÔ¨Åcult while staying within manageable pin
counts.
‚Ä¢ Low I/O Frequencies. Driving long I/O wires requires
lower frequencies, currently up to 1600 MT/s with the most
recent DDR3 implementation [32].
Recent advances in silicon nanophotonic devices and integration have made it possible to consider optical transmission
on the chip- and board-scale [7], [28]. Microprocessor I/O
signaling can directly beneÔ¨Åt from photonics in the following
ways:
‚Ä¢ Distance-Independent. Optical transmission of data can
be made agnostic to distance at the chip- and board-scale;
photonic energy dissipation is effectively not a function of
distance.
‚Ä¢ Data-rate Transparent. Most photonic devices, including
switches as well as on- and off-chip waveguides are not
bitrate-dependent, providing a natural bandwidth match
between compute cores and the memory subsystem.
‚Ä¢ High Bandwidth Density. Waveguides crossing the chip
boundary can have a similar pitch to that of electronics
[41], which makes the bandwidth density of nanophotonics
using wavelength division multiplexing (WDM) orders of
magnitude higher than electronic wires.
Though photonics can offer signiÔ¨Åcant physical-layer advantages, constructing a memory access architecture to realize
them requires signiÔ¨Åcant design space exploration. Trade-offs
exist in the selection of speciÔ¨Åc components, architectures,

c
2010
IEEE Personal use of this material is permitted. However, permission to reprint/republish this material for promotional purposes or for creating new
collective works for resale or redistribution to servers or lists, or to reuse any copyrighted component of this work in other works must be obtained from the
IEEE.
SC10 November 2010, New Orleans, Louisiana, USA 978-1- 4244-7558-2/10/$26.00

Buffer Cntrl

Request Bus

tRAS

Xbar Cntrl

Data Path

(a)

Arbiter

Control
Address Bus
Data Bus

tRCD

tCL

RAS

CAS

Row Address

Col Address

Overhead

tPRE

WE

PRE

Burst Length
B
th

Overhead

‚Ä¶
‚Ä¶

Control
Address Bus
Data Bus

(c)

Control
Address Bus
Data Bus

‚Ä¶

(b)

‚Ä¶

VC Mux
Fig. 1.

VC Buffer

Crossbar

Row1

Row1

Row2

Col1

Col2

Row3

Col3

Col1

Fig. 2. Control of a DRAM module (a) a single transaction (b)
amortizing overhead by pipelining transactions (c) amortizing
overhead through increased burst length.

Packet-switching router.

and protocols. Our approach to this problem employs a single circuit-switched photonic network-on-chip (NoC) design,
enabling both core-to-core and core-to-DRAM communication
which are necessary for efÔ¨Åciently implementing programming
models such as PGAS [4].
In this work, we study the problem of designing a NoC
architecture for an embedded computing platform that supports
both on-chip communication and off-chip memory access in a
power-efÔ¨Åcient way. In particular, we propose the adoption of
circuit-switched NoC architectures that rely on a simple mechanism to switch circuit paths off-chip to exchange data with
the DRAM memory modules. While this method is presented
independently of the particular transmission technology, we
show the advantages offered by an implementation based on
photonic communication over an electronic one.
We simulate this memory access architecture on a 256-core
chip with a concentrated 64-node network using detailed traces
of computation kernels widely used in signal and image processing high-performance embedded applications, speciÔ¨Åcally
the projective transformation, matrix multiply, and Fast Fourier
Transform (FFT). This work accomplishes the Ô¨Årst complete
detailed simulation of a nanophotonic NoC with physicallyaccurate photonic device models coupled with cycle-accurate
DRAM device and control models. These simulations are
used to determine the beneÔ¨Åts of circuit-switching and silicon
photonic technology in CMP memory access performance.
II. PACKET-S WITCHED M EMORY ACCESS
Packet-switched NoCs use router buffers to store and forward small packets through the network, where a packet is a
small number of Ô¨Çits (Ô¨Çow control units). Typically, purely
electronic store-and-forward routers use multiple physical
buffers to implement virtual channels, alleviating head-of-line
blocking under congestion. An illustration of a pipelined router
can be seen in Figure 1. If a core-to-DRAM or core-to-core
application-level message is larger than the physical buffers
themselves, or larger than the Ô¨Çow control mechanism can
reasonably sustain without deadlock, these messages must be

broken into several smaller packets.
The structure of a packet-switched NoC has important implications on how memory accesses are performed. Typically,
multiple on-chip memory controllers distributed around the
periphery of a CMP service requests from all the cores. If
a memory controller receives packets from different cores
(different messages), it must then schedule memory transactions with potentially disparate addresses. Indeed, the memory
controller depends on this paradigm to optimize the utilization
of the data and control buses using rank and bank concurrency.
Figure 2(a) shows the basic protocol of a single memory
transaction. The row address is latched into the DRAM chip
with the row address select (RAS) signal for the row access
time (tRAS ) until the decoded row is driven into the sense
amps. After the row-column delay time (tRCD ), the column
address then selects the starting point in the array, using the
column address select (CAS) signal. A write enable (WE)
signal determines whether the I/O circuitry is accepting data
from the bus or pushing data onto it. Data is then read or
written after the column-access latency (tCL ), incrementing
the initial column address in a burst. Once the transaction is
complete, depending on the control policy, the row can be
closed and must be precharged (PRE) for a time tP RE .
Figure 2(b) shows how a contemporary DRAM memory
controller schedules transactions concurrently across banks,
chips, and ranks to maximize performance and hide the
access latency. There exist different control policies to manage
queued transactions for lower latency and higher throughput,
both dynamic in the memory controller (e.g. page mode), and
static at compile-time [29]. The burst length is usually Ô¨Åxed
in this conÔ¨Åguration, matching the on-chip cache-line size.
Allowing a variable burst length would introduce signiÔ¨Åcant
complexity to the scheduling mechanism.
Typical DRAM subsystems implemented this way have
been effective for providing short latencies for small, random
accesses, as required by contemporary cache miss access patterns. However, providing the increasing bandwidth required
by future embedded applications will come at the cost of
power consumption in the on-chip interconnect, due partially

Data Switch

On Chip

Chip Boundary
MAP Switch

To/From
Memory Module 1

Data
plane

Arbiter

Memory Module 2
Memory Module 3

To/From
Network-on-Chip

‚Ä¶

Control
plane

Memory
Control

Off Chip

Control Router
Fig. 3.

Typical circuit-switching router.

to the relationship of the amount of network buffering to
performance.
III. M EMORY ACCESS FOR E MBEDDED C OMPUTING
Embedded processors are devices typically found in mobile
or extreme environments, and their design is commonly driven
by the needs of the application in question. They frequently
require specialized hardware or software, or commonly, both
to efÔ¨Åciently meet their performance, power, and reliablity
requirements. Because of this, a hardware / software co-design
approach is generally taken [31].
Of key consideration to this work are embedded applications
that involve signal and image processing (SIP). These applications typically require the aggregation and processing of many
data points collected from various locations over a period
of time, originating from sensors or other continuous data
streams. A typical example of this is a camera or other sensor
placed on an unmanned air vehicle (UAV). Applications in this
domain require signifcant computing power in the form of high
bandwidth data access and streaming processing capabilities.
In addition, they must achieve this using a low power budget.
In these applications, data is typically placed in contiguous
blocks of an embedded computing system‚Äôs memory space
around a central CMP via direct memory access (DMA) or
a similar mechanism by incoming data streams. The memory
access system outlined in this section proposes to make use of
the fact that these contiguous blocks of data can be accessed
using long burst lengths. The application can exhibit very
dynamic communication patterns between individual cores and
banks of memory, all while making use of efÔ¨Åcient memory
access circuits.
A. Circuit-Switched Memory Access
In a circuit-switched network, a control network provides
a mechanism for setting up and tearing down energy-efÔ¨Åcient
high-bandwidth end-to-end circuit paths. If a network node
wishes to send data to another node, a PATH - SETUP message
is sent to reserve the necessary network resources to allocate
the path. A PATH - BLOCKED message is returned to the node if
some parts of the path is currently reserved by another circuit.

Fig. 4.

Circuit-Switched Memory Access Point.

A PATH - ACK message is returned if the path successfully made
it to the end node. After data is transmitted along the data
plane, a PATH - TEARDOWN message is sent from the sending
node to release network resources for other paths.
This method effectively relaxes the relationship between
router buffer size, a large contributor (> 30%) to NoC power
[21], and performance because router buffers do not become
directly congested as communication demands grow. Figure 3
shows the router architecture for a circuit-switched NoC. The
control network uses smaller buffers and channels to transmit
the small control messages, which reduces the total amount of
buffering (and thus power) in the network. Because the higherbandwidth data plane is circuit switched end-to-end, it suffers
from higher latency due to the circuit-path setup overhead,
which must be amortized through a combination of larger
messages and well-scheduled or time-division multiplexed
communication patterns.
Aside from the power savings advantage, we can also
decrease considerably the complexity of the memory controller
through circuit-switching. We propose to allow a circuitswitched on-chip network to directly access memory modules,
giving a single core exclusive access to a memory module
for the duration of the transaction it requested. Access overhead is amortized using increased burst lengths as shown
in Figure 2(c). The memory controller complexity can be
greatly reduced because a memory module must sustain only
one transaction at a time. The key difference is that each
transaction is an entire message using long burst lengths, as
opposed to small packets that must be properly scheduled.
In addition, variable burst lengths are inherently supported
without introducing additional complexity.
To facilitate switching on-chip circuit paths off chip to
memory modules, we place memory access points (MAPs)
around the periphery of the chip connected to the network.
These MAPs, shown in Figure 4, contain a memory controller
that can service memory transactions and use the NoC to
allow end-to-end communication between cores and DRAM
modules. Figure 5 shows the logic behind this control.
Read transactions are Ô¨Årst sent as small control messages
to the memory controller. If another transaction is currently

no

2
Send circuit-path
BLOCKED msg
to core

yes
Is Write?

no

Queue
transaction

yes
Waiting

Receive new
transaction
request

Is MC
busy?

Send circuit-path
SETUP msg to
core

1

Circuit
setup?

yes

Set up Switch
from MC to
CAMM

Set up Switch
from CAMM to
NoC / core

Send Row/
Col
Access

Send circuit-path
TEARDOWN msg
to core

yes
Start new
no Transaction

Is Read?

no

Set up Switch
from MC to
CAMM

Send circuitpath ACK msg
to core

Send Row/Col
Access

no

no

yes

3

Queue
empty?

Fig. 5.

Flowchart of memory control logic.

in progress at the MAP, this request is then queued up.
Once a read is started, it Ô¨Årst sets up the data switch for
communication from the memory controller to the memory
module (for DRAM commands) and from memory module
back to the core (for returning read data). A circuit-path is
then established back to the core via the NoC path-setup
mechanism. The memory controller can then issue row and
column access commands, allowing the memory module to
freely send data back to the core. The memory controller is
responsible for knowing the access time of the read, so that
it can issue a PATH - TEARDOWN at the correct time (labeled 1
in Figure 5), which completes the transaction.
Writes begin by a core setting up a circuit-path to a MAP.
By virtue of a PATH - SETUP message successfully arriving
to the MAP, the core will have gained exclusive access to
it. Writes that arrive to a MAP that is servicing a read
return to the core as a blocked path (labeled 2) instead of
queuing it, to release network resources for other transactions
(including the potential read setup that is attempting). The
memory controller then sets up the data switch from memory
controller to memory, which allows the transmission of DRAM
row/col access commands. The data switch is then set from
core to memory module, and a PATH - ACK is sent back to
the core, completing the path setup. Upon receiving the
path acknowledgment, the core then begins transmitting write
data directly to the memory module. The memory controller
considers the transaction Ô¨Ånished when it receives a PATH TEARDOWN from the core (labeled 3). In this way, any core
in the network can establish a direct, end-to-end circuit path
with any memory module.
Livelock is avoided by using random backoff for pathsetup requests. However, starvation for a core is possible,
especially for writes in the presence of many reads. We leave
the impact and effectiveness of the memory access mechanism on power and performance to Section V. Addressing
memory access starvation through both network design and
software/programming models remains a topic for future work.
B. Silicon Nanophotonic Technology
Circuit-switching photonic networks can be achieved using
active broad-band ring-resonators whose diameter is manufactured such that its resonant modes directly align with all of the
wavelengths injected into the nearby waveguide. For example,

Multi-wavelength signal

Waveguide

Electronic Control
p-region n-region

Ring resonator

0V

0V

1V

0V

Transmission

yes

Circuit
teardown?

O

Off-resonance profile
On-resonance profile

Injected Wavelengths

Fig. 6. Operation of PSE. Left - PSE in off state. Right - PSE
in on state. Bottom - Resonance proÔ¨Åle of ring resonator, shifts
from off to on.

a 200um will have a wavelength channel spacing of 50 GHz.
The ring resonator can be conÔ¨Ågured to be used as a photonic
switching element (PSE), as shown in Figure 6. By electrically
injecting carriers into the ring, the entire resonant proÔ¨Åle is
shifted, effectively creating a spatial switch between the ports
of the device [27]. This process is analogous to setting the
control signals of an electronic crossbar.
Given the operation of a single PSE, we can then construct
higher order switches, and ultimately entire networks. Using
ring-resonator devices in this way opens the possibility to
explore different network topologies in much the same way as
packet-switched electronic networks [36]. Different numbers
and conÔ¨Ågurations of ring switches yield different amounts
of energy, different path-blocking characteristics, as well as
varying insertion loss.
We assume off-chip photonic signaling is achieved through
lateral coupling [1] [30], where the optically encoded data is
brought in and out of the chip through inverse-taper optical

To/From
Memory Modules

(a)

Chip Boundary

Vdd, Gnd

Local Bus

To/From

Waveguide / Fiber

Transceiver/Control

(b)

Network

Modulators

(c)

From
Clk

Controller
clk

Waveguide
Fig. 7.

DRAM commands

Ring Control

Photonic switch used in a MAP.

mode converters which expand the on-chip optical cross
section to match the cross section of the external guiding
medium. This method is employed due to its lower insertion
loss, compared to vertical coupling [39] [14]. Waveguide pitch
at the chip edge can easily be on the order of 60 Œºm interfacing
to off-chip arrayed waveguides [41] or optical Ô¨Åber. This
photonic I/O pitch remains well below that of current electrical
I/O pitch (e.g. 190 Œºm in the Sun ULTRASparc T2 [43]),
illustrating the potential for vastly higher bandwidth density
that is offered by using photonic waveguides when using
WDM.
As shown in Figure 4, the MAP controls a switch that
establishes circuit paths between individual memory modules
and the network. The photonic version of this switch is
illustrated in Figure 7, which uses broadband ring-resonators
to allow access to multiple memory modules controlled by
the same memory controller. Modulators convert electronic
DRAM commands from the memory controller to the optical
domain. Additional waveguides can be added to incorporate
an arbitrary number of memory modules into one MAP, as
shown in Figure 7 with three bidirectional memory module
connections.
C. Circuit-Accessed Memory Module
Our proposed circuit-switched memory access architecture
requires slightly different usage of DRAM modules. Figure
8(a) shows the Photonic Circuit-Accessed Memory Module
(P-CAMM) design. Individual conventional DRAM chips are
connected via a local electronic bus to a central optical
controller/transceiver, shown in Figure 8(d). The controller
(Figure 8(c)) is responsible for demultiplexing the single
optical channel into the address and data bus much in the same
way as Rambus RDRAM memory technology [38], using the
simple control Ô¨Çowchart shown in Figure 5.
Figure 8(b) shows the anatomy of an Electronic CircuitAccessed Memory Module (E-CAMM), similar to the P-

(d)

To/From
CMP

DLL

Control

Receivers

Cntrl
Data

Modulators

CAMM Transceiver

Fig. 8. Circuit-Accessed Memory Module design (a) Photonic
CAMM (b) Electronic CAMM (c) CAMM control logic (d)
CAMM Transceiver.

CAMM in structure, but still requiring electronic pins as I/O.
This shift from electrical to photonic technology presents
signiÔ¨Åcant advantages for the physical design and implementation of off-chip signaling. One advantage is that the P-CAMM
can be locally clocked, as shown, performing serialization and
deserialization on the I/O bitrate, and synchronizing it to the
DRAM clock rate. Coding or clock transmission can be used
to recover the clock in the transceiver, and matched to the
local DRAM clock after deserialization. Local clocking and
the elimination of long printed circuit board (PCB) traces that
the DRAM chips drove allow the P-CAMM to sustain higher
clock frequencies than contemporary DRAM modules.
Although the P-CAMM shown in Figure 8(a) retains the
contemporary SDRAM DIMM form factor, this is not required
due to the alleviated pinning requirements. The memory
module can then be designed for larger, smaller, or more dense
conÔ¨Ågurations of DRAM chips. Furthermore, the memory
module can be placed arbitrarily distant from the processor
using low-loss optical Ô¨Åber without incurring any additional
power or optical loss. Latency is also minimal, paying 4.9 ns/m
[11]. Additionally, the driver and receiver banks use much less
power for photonics using ring-resonator based modulators and
SiGe detectors than for off-chip electronic I/O wires [7].
IV. E XPERIMENTAL S ETUP
The main goal of this work is to evaluate how silicon photonic technology and circuit-switching affect power efÔ¨Åciency
in transporting data to and from off-chip DRAM. We perform
this analysis by investigating different network conÔ¨Ågurations
using PhoenixSim, a simulation environment for physical-layer
analysis of chip-scale photonic interconnection networks [6].

Processor Core

Network Router

Memory Access Point

Mods

Dets

Dets

Mods

(a) PS-1

(b) PS-2

Fig. 10. Two designs for a 5-port photonic switch for the PmeshCS

Fig. 9. Abstract illustration of 8√ó8 mesh network-on-chip with
peripheral memory access points

A. On-chip Network Architectures
The 2D mesh topology has some attractive characteristics
including a modular design, short interconnecting wires, and
simple X-Y routing. For these reasons, the 2D mesh has been
used in some of the Ô¨Årst industry instantiations of tiled manycore networks-on-chip [16], [44]. The mesh also provides the
simple and effective means of connecting peripheral memory
access points at the ends of rows and columns, utilizing router
ports that would have otherwise gone unused or required
specialized routing.
We consider three different network architectures: Electronic packet-switched (Emesh), Electronic circuit-switched
(EmeshCS), and Photonic circuit-switched (PmeshCS). All
three use an 8√ó8 2D mesh topology to connect the grid of
64 network nodes with DRAM access points on the periphery.
An abstract illustration of this setup is shown in Figure 9.
The Emesh and EmeshCS use the routers shown in Figures
1 and 3, respectively, to construct the on-chip 8√ó8 mesh. They
also use integrated concentration [24] of 4 cores per network
gateway, for a total core count of 256.
Similar to the electronic circuit-switched mesh, we replace
the electronic data plane with nanophotonic waveguides and
switches to achieve a hybrid photonic circuit-switched network. External concentration [24] is used because of the relative difÔ¨Åculty of designing high-radix photonic switches, and
to reduce the number of modulator/detector banks. Designs of
4√ó4 photonic switches in the context of networks have been
explored in [5], but because a mesh router requires 5 ports (4
directions + processor core), we must reconsider the design of
the photonic switches to minimize power and insertion loss.
Figure 10 introduces two new designs for the photonic 5port ring resonator-based broadband data switch used in the
circuit-switching router for the PmeshCS, designated as PS-1
and PS-2. We designed the PS-1 starting with an optimized

4√ó4 switch [5], and adding the modulator and detector banks
between lanes. As a result, the switch has a small number of
rings and low insertion loss, but exhibits blocking when certain
ports are being used (e.g. when the detector bank is being used,
the east-bound port is blocked). We designed the PS-2 switch
from a full ring-matrix crossbar switch, taking out rings to
account for no U-turns being allowed, and routing waveguides
to eliminate terminations. The PS-2 switch uses more rings
and has larger insertion loss, but is fully nonblocking. Because
it is not obvious how the two switch designs will affect the
network as a whole, we will consider separate photonic mesh
instantiations using each switch.
B. Simulation Environment
The PhoenixSim simulation environment allows us to capture physical-layer details, such as physical dimensions and
layout, of both electronic and nanophotonic devices to accurately execute various trafÔ¨Åc models. We describe the relevant
modeling and parameters below.
Photonic Devices. Modeling of optical components is built
on a detailed physical-layer library that has been characterized
and validated through the physical measurement of fabricated
devices. The modeled components are fabricated in silicon
at the nano-scale, and include modulators, photodetectors,
waveguides (straight, bending, crossing), Ô¨Ålters, and PSEs. The
behavior of these devices are characterized and modeled at
runtime by attributes such as insertion loss, crosstalk, delay,
and power dissipation. Tables I and II show some of the most
important optical parameters used.
Photonic Network Physical Layer Analysis. The number
of available wavelengths is obtained through an insertion loss
analysis, a key tool in our simulation environment [6]. Figure
11 shows the relationship between network insertion loss and
the number of wavelengths that can be used. The following
equations specify the constraints that must be met in order to
achieve reliable optical communication:
Ptot < PN T

(1)

Pinj ‚àí Ploss > Pdet

(2)

Parameter

Value

Data rate (per wavelength)
PSE dynamic energy
PSE static (OFF) energy
Modulation switching energy
Modulation static energy
Detector energy
Thermal Tuning energy

2.5 Gb/sec
375 fJ‚àó
400 uJ/sec‚Ä†
25 fJ/bit‚Ä°
30 ŒºW¬ß
50 fJ/bit¬∂
1uW/‚ó¶ K

TABLE II
O PTICAL D EVICE L OSS PARAMETERS

Device

Insertion Loss

Waveguide Propagation
Waveguide Crossing
Waveguide Bend
Passing by Ring (Off)
Insertion into Ring (On)
Optical Power Budget

1.5 dB/cm ‚àó‚àó
0.05‚Ä†‚Ä†
0.005 dB/90 ‚ó¶‚àó‚àó
‚âà0‚Ä°‚Ä°
0.5‚Ä°‚Ä°
35 dB

Equation 1 states that the total injected power at the Ô¨Årst
modulator must be below the threshold at which nonlinear
effects are induced, thus corrupting the data (or introducing
signiÔ¨Åcantly more optical loss). A reasonable value for PN T
is around 10-20 mW [26]. Equation 2 states that the power
received at the detectors must be greater than the detector
sensitivity (usually about -20 dBm) to reliably distinguish
between zeros and ones. To ensure this, every wavelength must
inject at least enough power to overcome the worst-case optical
loss through the network. From these relationships, we can see
that the number of wavelengths that can be used in a network
relies mainly on the worst-case insertion loss through it.
The two photonic switches that we consider here, labeled
PS-1 and PS-2, have different insertion loss characteristics.
We determine the worst case network-level insertion loss
using each of the switches in the photonic mesh, and Ô¨Ånd
that it equates to 13.5 dB and 18.41 dB for the PS-1 and
PS-2, respectively. This means that the Pmesh can safely
use approximately 128 wavelengths for the PS-1, and 45 for
the PS-2. Despite the PS-1 having 2√ó more bandwidth than
the PS-2, its blocking conditions may yield a lower total
bandwidth for the network.
Simulation Parameters. The parameters for all networks
have been chosen for power-efÔ¨Åcient conÔ¨Ågurations, typically
‚àó Dynamic energy calculation based on carrier density, 50-Œºm ring,
320√ó250-nm waveguide, 75% exposure, 1-V bias.
‚Ä† Based on switching energy, including photon lifetime for re-injection.
‚Ä° Same as ‚àó , for a 3 Œºm ring modulator.
¬ß Based on experimental measurements in [49]. Calculated for half a 10
GHz clock cycle, with 50% probability of a 1-bit.
¬∂ Conservative approximation assuming femto-farad class receiverless SiGe
detector with C < 1f F .
 Same value as used in [20]. Average of 20 degrees thermal tuning required.
‚àó‚àó From [51]
‚Ä†‚Ä† Projections based on [13]
‚Ä°‚Ä° From [25]

Optical Power

PNT
Optical Power Budget

TABLE I
O PTICAL D EVICE E NERGY PARAMETERS

= nonlinear effects threshold

Ptot = Œ£ ( Pinj ) = total power

Œª0 Œª1 Œª2 ‚Ä¶ ŒªN

Pinj
Ploss
PRx
Pdet

= injected power
= worse-case optical loss
= received power
= detector sensitivity

Number of wavelengths dictated by insertion loss and
optical power budget.

Fig. 11.

the most important concern for embedded systems. We consider the key limiting factor for our embedded system design to
be ideal I/O bandwidth. For photonics, I/O bandwidth (which
is the same as on-chip bandwidth due to bit-rate transparent
devices) is limited by insertion loss as described above.
The electronic networks, however, are limited by pin count.
Electronic off-chip signaling bandwidth is limited by packaging constraints at a total of 1792 I/O pins (64 pins per MAP),
which is more than 2√ó that of today‚Äôs CMPs (TILE64 [3]).
Note that even though a real chip would require a signiÔ¨Åcant
number of additional I/O ports, we assume that all of these
1792 pins are dedicated to DRAM access. This places the
total number of pins well over 4000, assuming a 50% total
I/O-to-power/ground ratio. According to ITRS [18], attaining
this pin count will require solutions to signiÔ¨Åcant packaging
challenges.
Table III shows the more important simulation parameters
that will be used for simulations in Section V. For each
network, we work backwards from the I/O bandwidth available
across the chip boundary to the on-chip and DRAM parameters. We assume all cores run at 2.5 GHz.
The Emesh uses conventional DRAM bidirectional signalling with 2 DRAM channels for increased access concurrency running at 1.6 GT/s, using a conventional 8 arrays
per chip and 8 chips per DIMM. The Emesh network runs
at 1.6GHz to match this bandwidth. Our router model implements a fully pipelined router which can issue two grant
requests per cycle (for different outputs) and uses dimension
ordered routing for deadlock avoidance, and bubble Ô¨Çow
control [37] for congestion management. One virtual channel
(VC) is used for writes and core-to-core communication,
and a separate VC is used for read responses for reduced
read latency. For power dissipation modeling, the ORION 2.0
electronic router model [21] is integrated into the simulator,
which provides detailed technology node-speciÔ¨Åc modeling
of router components such as buffers, crossbars, arbiters,
clock tree, and wires. The technology point is speciÔ¨Åed as
32 nm, and the VDD and Vth ORION parameters are set
according to frequency (lower voltage, higher threshold for
lower frequencies). The ORION model also calculates the
area of these components, which is used to determine the
lengths of interconnecting wires. Off-chip electronic I/O wires

TABLE III
E LECTRONIC S IMULATION PARAMETERS

Parameter
Physical I/O per MAP
I/O bit rate
Ideal Bandwidth per I/O Link (Gb/s)
Packet switched Clock Freq (GHz)
Data Plane Freq (GHz)
Buffer Size (b)
Virtual Channels
Control Plane VDD
Control Plane Vth
Data Plane VDD
Data Plane Vth
Electronic Channel Width
Bandwidth per On-chip Link (Gb/s)
Base DRAM Frequency (MHz)
Arrays Per Bank
Chips Per DIMM
DIMMs Per MAP
Total Memory Per MAP
Bandwidth per DIMM (Gb/s)

Emesh

EmeshCS

Chip IO Parameters
64
32 (diff pair)
1.6 GT/s
10 Gb/s
102
320
NoC Electronic Parameters
1.6
1.0
2.5
1024
128
2
1
0.8
0.8
Norm
High
1.0
Norm
64
32 (128 for data plane)
102
320
DRAM Parameters
1066
1066
8
32
8
10
2
1
2GB
2GB
128
320

and transceivers are modeled as using 1 pJ/bit, a reasonable
projection based on [18], [33].
The EmeshCS uses high speed (10Gb/s) bidirectional differential pairs for I/O signalling, requiring serialization and
deserialization (SerDes) at the chip edge between the 2.5
GHz data plane. The path-setup electronic control plane runs
at a slower 1.0 GHz to save power. The photonic networks
use the exact same control plane as the EmeshCS, and the
same 2.5 Gb/s bitrate per wavelength to avoid signiÔ¨Åcant
SerDes power consumption at the network gateways. SerDes
power is modeled using ORION Ô¨Çip-Ô¨Çop models as shift
registers running at the higher clock rate, bandwidth matching
both sides with parallel wires. For all three circuit-switched
conÔ¨Ågurations, we increase the number of DRAM arrays per
chip by decreasing the row and column count to be able to
continuously feed the I/O. A bit-rate clock is sent with the data
on a separate channel to lock on to the data at the receiver, and
we allocate 16 clock cycles of overhead for each transmission
for locking.
DRAM Modeling. The cycle-accurate simulation of the
DRAM memory subsystem along with the network on chip for
the Emesh is accomplished by integrating DRAMsim [48] into
our simulator. The Emesh behaves like a typical contemporary
system in that the packetization of messages required by
the packet-switched network yields small memory transaction
sizes, analogous to today‚Äôs cachelines. Therefore, a DRAM
model which is based on typical DDR SDRAM components
and control policies that might be seen in real systems, such
as DRAMsim, is appropriate for this conÔ¨Åguration.
The two circuit-switched networks, however, exhibit different memory access behavior than a packet-switched version,
thus enabling a simpliÔ¨Åcation of the memory control logic. For

PmeshCS (PS1)

PmeshCS (PS2)

2 (w/ 128 Œª)
2.5 Gb/s
320

2 (w/ 45 Œª)
2.5 Gb/s
112

1.0
2.5
128
1
0.8
High
1.0
Norm
32
320

1.0
2.5
128
1
0.8
High
1.0
Norm
32
112

1066
32
10
1
2GB
320

1066
16
8
1
2GB
128

this reason, we use our own model for DRAM components
and control. This model cycle-accurately enforces all timing
constraints of real DRAM chips, including row access time,
row-column delay, column access latency, and precharge time.
Because access to the memory modules is arbitrated by the
on-chip path-setup mechanism, only one transaction must be
sustained by a MAP, which greatly simpliÔ¨Åes the control logic
as previously discussed.
We base our model parameters around a Micron 1-Gb
DDR3 chip [32], with (tRCD - tRP - tCL ) chosen as (12.5
- 12.5 - 12.5) (ns). To normalize the three different network
architectures for experiment, we assign them the same amount
of similarly-conÔ¨Ågured DDR3 DRAM around the periphery.
V. E MBEDDED A PPLICATION S IMULATION
A. Evaluation Framework
We evaluate the proposed network architectures using the
application modeling framework, Mapping and Optimization
Runtime Environment (MORE) to collect traces from the
execution of high-performance embedded signal and image
processing applications.
The MORE system, based on pMapper [45], is designed to
project a user program written in Matlab onto a distributed
or parallel architecture and provide performance results and
analysis. The MORE framework translates application code
into a dependency-based instruction trace, which captures the
individual operations performed as well as their interdependencies. By creating an instruction trace interface for PhoenixSim,
we were able to accurately model the execution of applications
on the proposed architectures.
MORE consists of the following primary components:

The program analysis component is responsible for converting the user program, taken as input, into a parse
graph, a description of the high-level operations and their
dependences on one another.
‚Ä¢ The data mapping component is responsible for distributing the data of each variable speciÔ¨Åed in the user code
across the processors in the architecture.
‚Ä¢ The operations analysis component is responsible for
taking the parse graph and data maps and forming the
dependency graph, a description of the low-level operations and their dependences on one another.
PhoenixSim then reads the dependency graphs produced by
MORE, generating computation and communication events.
Combining PhoenixSim with MORE in this way allows us
to characterize photonic networks on the physical level by
generating trafÔ¨Åc which exactly describes the communication,
memory access, and computation of the given application.
Three applications are considered: projective transform,
matrix multiply, and fast fourier transform (FFT). Results for
power usage, performance (GOPS), and efÔ¨Åciency (GOPS/W)
improvement are provided for each.
Projective Transform. When registering multiple images
taken from various aerial surveillance platforms, it is frequently advantageous to change the perspective of these images so that they are all registered from a common angle and
orientation (typically straight down with north being at the
top of the image). In order to do this, a process known as
projective transform is used [22].
Projective transform takes as input a two-dimensional image
M as well as a transformation matrix t that expresses the
transformational component between the angle and orientation
of the image presented and the desired image. The projective
transform algorithm outputs M  , or the image M after projection through t. To populate a pixel p in M  , its x and
y positions are back-projected through t to get their relative
position in M , p. This position likely does not fall directly
on a pixel in M , but rather somewhere between a set of four
pixels. Using the distance from p to each of its corners as
well as the corner values themselves, the value for p can be
obtained.
MORE allows us to retain identical image and projections
sizes while still inducing data movement in the projection
process as well as investigating various transformation matrices. For this experiment, we consider this application on
various image sizes where the image orientation is rotated by
ninety degrees.
Matrix Multiply Matrix multiplication is a common operation in signal and image processing, where it can be used
in Ô¨Åltering as well as to control hue, saturation and contrast
in an image. It is a natural candidate for consideration on
our architecture, given that multiple data points need to be
accessed and then summed to form a single entry in the result.
While various algorithms for matrix multiplication can be
considered for matrices of any dimension, we shall focus our
analysis on an inner product algorithm over square matrices.
Here, in an N √ó N matrix, each entry is generated by Ô¨Årst
‚Ä¢

Read
Stage

FFT
stage

Linear
stage

Linear
stage

Linear
stage

0

0

0

0

0

1

1

1

1

1

2

2

2

2

2

3

3

3

3

3

4

4

4

4

4

5

5

5

5

5

6

6

6

6

6

7

7

7

7

7

Fig. 12.

Write
stage

FFT computation per the Cooley-Tukey algorithm.

multiplying together two vectors of size N (corresponding to
a row and a column), and then summing the entries in the
resulting vector to form a single entry in the result.
The inner product algorithm requires time proportional to
N 3 . While the best known algorithm for matrix multiply is
O(N 2.376 ), the constants in the algorithm make it infeasible
for all but the largest of matrices. Even Strassen‚Äôs algorithm
[42], with a bound of O(N 2.806 ) is frequently considered too
cumbersome and awkward to implement, especially in a parallel environment. Though more computationally expensive, the
inner product algorithm also lends itself more naturally to a
parallel implementation, making it our algorithm of choice.
Fast Fourier Transform Computing the Fast Fourier Transform (FFT) of a set of data points is an essential algorithm
which underlies many signal processing and scientiÔ¨Åc applications. In addition to the widespread use of the FFT, the inherent
data parallelism that can be exploited in its computation
makes it a good match for measuring the performance of
networks-on-chip. A typical way the FFT is computed in
parallel, and which is employed in our execution model, is
the Cooley-Tukey method [10]. The communication patterns
and computation stages for 8 nodes are shown in Figure 12.
We run the FFT where each core begins with 210 , 212 , 214 ,
216 , and 218 samples, and average the results.
B. Simulation Results
Table IV shows the averaged results for the different
network conÔ¨Ågurations across the 3 applications, showing
network-related power, total system performance (GOPS), and
total system efÔ¨Åciency (GOPS/W) which is normalized to the
Emesh for comparison. In all cases, the circuit switched networks achieve considerable improvements in both performance
and power over the Emesh.
For the Projective Transform and Matrix Multiply, the
EmeshCS consumes some additional power to achieve considerable gains in performance. The photonic networks also
perform signiÔ¨Åcantly better than the Emesh, though at much
lower power than the EmeshCS. The PS-2 generally consumes
less power because it has less modulators (but less bandwidth),
and uses non-blocking switches which reduces path-setup
block and retry on the electronic control plane, and therefore
power. The FFT exhibits different communication and memory
access behavior than the other applications, and gains are not

TABLE IV
R ESULTS FOR P ERFORMANCE , N ETWORK P OWER , AND I MPROVEMENT OVER E LECTRONIC M ESH IN S IGNAL AND I MAGE P ROCESSING A PPLICATIONS

Network
Emesh
EmeshCS
PS-1
PS-2
25

Power (W)

20

Projective Transform
Power
Perf.
Impr.
(Watts) (GOPS) (GOPS/W)
11.2
1.04
1x
19.0
47.3
26.9x
4.37
27.80
68.6x
2.21
17.76
86.7x

Electronic Arbiter
Electronic Buffer
SerDes
PSE

Electronic Clock Tree
Electronic Wire
Modulator

Matrix Multiply
Net. Pow.
Perf.
Impr.
(Watts)
(GOPS) (GOPS/W)
11.1
0.78
1x
15.8
31.82
29.01x
4.35
26.51
87.64x
2.17
13.48
89.33x

Electronic Crossbar
Electronic IO Wire
Detector

15
10

5
0

Emesh

Fig. 13.

EmeshCS

PS-1

PS-2

Projective Transform network power breakdown.

as profound though still an order of magnitude in efÔ¨Åciency
for PS-2.
The breakdown of power consumption for the various
network components is shown in Figure 13 for the Projective
Transform, one of the more network-active applications. We
can see that the Emesh power is comprised mostly of buffer,
crossbar, and clock power. EmeshCS alleviates buffer power
as intended, but at the cost of crossbar and wire power in the
higher-frequency data plane. Finally, the photonic networks
achieve drastically lower power through distance-independent
efÔ¨Åcient modulation and detection.
VI. R ELATED W ORK
Networks-on-chip have entered the computer architecture
arena to enable core-to-core and core-to-DRAM communication on contemporary processors. The Tilera TILE-Gx
processors [44] and Intel Polaris [16] are examples of real
packet-switched NoC implementations with up to 100 and 80
cores, respectively. The Cell BE [9] uses a circuit-switched
network to connect heterogeneous cores and a single memory
controller.
Next-generation NoC designs using silicon nanophotonic
technology have also been proposed. The Corona network
is an example of a network that uses optical arbitration
via a wavelength-routed token ring to reserve access to a
full serpentine crossbar made from redundant waveguides,
modulators, and detectors [47]. Similarly, wavelength-routed
bus-based architectures have been proposed which take advantage of WDM for arbitration [23], [34]. Batten et al. proposed a wavelength-selective routed architecture for off-chip
communications which takes advantage of WDM to dedi-

Power
(Watts)
11.4
11.2
4.28
2.15

FFT
Perf.
(GOPS)
1.75
4.74
4.32
3.12

Impr.
(GOPS/W)
1x
2.82x
6.72x
9.67x

cate wavelengths to different DRAM banks, forming a large
wavelength-tuned ring-resonator matrix as a central crossbar
[2] on which source nodes transmit on the speciÔ¨Åc wavelength
that is received by a single destination. Hadke et al. proposed
OCDIMM, a WDM-based optical interconnect for FBDIMM
memory banks, which uses wavelength-routing to achieve a
memory system that scales while sustaining low latencies [15].
Phastlane was designed for a cache-coherent CMP, enabling
snoop-broadcasts and cacheline transfers in the optical domain
[8]. Finally, on-chip hybrid electronically circuit-switched photonic networks have been proposed by Shacham et al. [40]
and Petracca et al. [35], and further investigated by Hendry
et al. [17] and Chan et al. [5] .
The main contribution of this work over previous work
is to explore circuit-switching as a memory access method
in the context of a nanophotonic-enabled interconnect, using
the same network resources which enable core-to-core communication. Uniquely, our simulation framework incorporates
physically-accurate photonic device models, detailed electronic component models, and cycle-accurate DRAM device
and control models into a full system simulation.

VII. C ONCLUSION
By incorporating cycle-accurate DRAM control and device
models into a network simulator with detailed physicallyaccurate models of both photonic and electronic components,
we are able to investigate circuit-switched memory access in
an embedded high-performance CMP computing node design.
We run three signal and image processing applications on
different network implementations normalized to topology, pin
constraints, total memory, and CMOS technology to characterize the different networks with respect to bandwidth and
latency. Accessing memory using a circuit-switched network
was found to increase performance through long burst lengths
and decrease power by eliminating performance-dependent
buffers. Silicon nanophotonic technology adds to these beneÔ¨Åts
with low-energy transmission and higher bandwidth density
which will enable future scaling. Additional beneÔ¨Åts include
reduced memory controller complexity, dramatically lower
pin counts, and relaxed memory module and compute board
design constraints, all of which are beneÔ¨Åcial to the embedded
computing world.

R EFERENCES
[1] V. R. Almeida, R. R. Panepucci, and M. Lipson. Nanotaper for compact
mode conversion. Optics Letters, 28(15):1302‚Äì1304, August 2003.
[2] C. Batten et al. Building manycore processor-to-DRAM networks with
monolithic silicon photonics. In IEEE Micro Special Issue: Micro‚Äôs Top
Picks from Hot Interconnects 16, 2009.
[3] S. Bell, B. Edwards, J. Amann, et al. TILE64 Processor: A 64-Core
SoC with Mesh Interconnect. In Proceedings of the IEEE International
Solid-State Circuits Conference, pages 88‚Äì89, 598, 2008.
[4] W. Carlson, T. El-Ghazawi, B. Numrich, and K. Yelick. Programming in
the partitioned global address space model. Presented at Supercomputing
2003. Online at http://www.gwu.edu/upc/tutorials.html.
[5] J. Chan, A. Biberman, B. G. Lee, and K. Bergman. Insertion loss
analysis in a photonic interconnection network for on-chip and off-chip
communications. In IEEE Lasers and Electro-Optics Society (LEOS),
Nov. 2008.
[6] J. Chan, G. Hendry, A. Biberman, K. Bergman, and L. P. Carloni.
Phoenixsim: A simulator for physical-layer analysis of chip-scale photonic interconnection networks. In DATE: Design, Automation, and Test
in Europe., Mar. 2010.
[7] L. Chen, K. Preston, S. Manipatruni, and M. Lipson. Integrated GHz
silicon photonic interconnect with micrometer-scale modulators and
detectors. Optics Express, 17(17), August 2009.
[8] M. J. Cianchetti, J. C. Kerekes, and D. H. Albonesi. Phastlane: a
rapid transit optical routing network. SIGARCH Comput. Archit. News,
37(3):441‚Äì450, 2009.
[9] S. Clark, K. Haselhorst, K. Imming, J. Irish, D. Krolak, and T. Ozguner.
Cell broadband engine interconnect and memory interface. In 17th
Annual Hot Chips, Aug 2005.
[10] J. W. Cooley and J. W. Tukey. An algorithm for the machine calculation
of comples fourier series. Mathematics of Computation, 19:297‚Äì301,
1965.
[11] Corning Inc. Datasheet: Corning SMF-28e optical Ô¨Åber product information. Online at http://www.princetel.com/datasheets/SMF28e.pdf.
[12] M. Duranton. The challenges for high performance embedded systems.
In DSD ‚Äô06: Proceedings of the 9th EUROMICRO Conference on
Digital System Design, pages 3‚Äì7, Washington, DC, USA, 2006. IEEE
Computer Society.
[13] T. Fukazawa, T. Hirano, F. Ohno, and T. Baba. Low loss intersection
of Si photonic wire waveguides. Japanese Journal of Applied Physics,
43(2):646‚Äì647, 2004.
[14] C. Gunn. CMOS photonics for high-speed interconnects. IEEE Micro,
26(2):58‚Äì66, March 2006.
[15] A. Hadke et al. OCDIMM: Scaling the DRAM memory wall using
WDM based optical interconnects. In Proceedings of the 16th IEEE
Symposium on High Performance Interconnects, Aug. 2008.
[16] J. Held, J. Bautista, and S. Koehl. From a few cores to many: A
tera-scale computing research overview, 2006. White paper. Online at
http://download.intel.com/research/platform/terascale/.
[17] G. Hendry et al. Analysis of photonic networks for a chip-multiprocessor
using scientiÔ¨Åc applications. In The 3rd ACM/IEEE International
Symposium on Networks-on-Chip, May 2009.
[18] The international technology roadmap for semiconductors (ITRS).
http://www.itrs.net.
[19] B. Jacob, S. W. Ng, and D. T. Wang. Memory Systems: Cache, DRAM,
Disk. Morgan Kaufmann, 2007.
[20] A. Joshi, C. Batten, Y.-J. Kwon, S. Beamer, I. Shamim, K. Asanovic,
and V. Stojanovic. Silicon-photonic Clos networks for global onchip communication. In 3rd ACM/IEEE International Symposium on
Networks-on-Chip, May 2009.
[21] A. B. Kahng, B. Li, L.-S. Peh, and K. Samadi. Orion 2.0: A fast
and accurate NoC power and area model for early-stage design space
exploration. pages 423‚Äì428, April 2009.
[22] H. Kim, E. Rutledge, S. Sacco, S. Mohindra, M. Marzilli, J. Kepner, R. Haney, J. Daly, and N. Bliss. Pvtol: Providing productivity,
performance and portability to dod signal processing applications on
multicore processors. In HPCMP-UGC ‚Äô08: Proceedings of the 2008
DoD HPCMP Users Group Conference, pages 327‚Äì333, Washington,
DC, USA, 2008. IEEE Computer Society.
[23] N. Kirman et al. Leveraging optical technology in future bus-based
chip multiprocessors. In MICRO 39: Proceedings of the 39th Annual
IEEE/ACM International Symposium on Microarchitecture, pages 492‚Äì
503, Washington, DC, USA, 2006. IEEE Computer Society.

[24] P. Kumar, Y. Pan, J. Kim, G. Memik, and A. Choudhary. Exploring
concentration and channel slicing in on-chip network router. In NOCS
‚Äô09: Proceedings of the 2009 3rd ACM/IEEE International Symposium
on Networks-on-Chip, pages 276‚Äì285, Washington, DC, USA, 2009.
IEEE Computer Society.
[25] B. G. Lee, A. Biberman, P. Dong, M. Lipson, and K. Bergman. Alloptical comb switch for multiwavelength message routign in silicon
photonic networks. IEEE Photonics Technology Letters, 20(10):767‚Äì
769, May 2008.
[26] B. G. Lee, X. Chen, A. Biberman, X. Liu, I.-W. Hsieh, C.-Y. Chou,
J. Dadap, R. M. Osgood, and K. Bergman. Ultra-high-bandwidth WDM
signal integrity in silicon-on-insulator nanowire waveguides. IEEE
Photonics Technology Letters, 20(6):398‚Äì400, May 2007.
[27] B. G. Lee et al. High-speed 2 √ó 2 switch for multi-wavelength message
routing in on-chip silicon photonic networks. In European Conference
on Optical Communication (ECOC), Sept. 2008.
[28] B. G. Lee et al.
High-speed 2√ó2 switch for multiwavelength
silicon-photonic networks-on-chip. Journal of Lightwave Technology,
27(14):2900‚Äì2907, July 2009.
[29] D. Lee, S. S. Bhattacharyya, and W. Wolf. High-performance buffer
mapping to exploit DRAM concurrency in multiprocessor DSP systems.
In IEEE/IFIP International Symposium on Rapid System Prototyping,
2009.
[30] S. J. McNab, N. Moll, and Y. A. Vlasov. Ultra-low loss photonic
integrated circuit with membrane-type photonic crystal waveguides.
Optics Express, 11(22):2927‚Äì2938, November 2003.
[31] G. D. Micheli, R. Ernst, and W. Wolf. Readings in hardware/software
co-design, 2001.
[32] Micron
Technology
Inc.
Product
speciÔ¨Åcation.
1
Gb
DDR3
SDRAM
Chip.
Online
at
http://www.micron.com/products/partdetail?part=MT41J256M4JP-125.
[33] D. A. B. Miller. Device requirements for optical interconnects to silicon
chips. In Proc.IEEE Special Issue on Silicon Photonics, pages 1166 ‚Äì
1185, 2009.
[34] Y. Pan et al. FireÔ¨Çy: Illuminating future network-on-chip with nanophotonics. In Proceedings of the International Symposium on Computer
Architecture, 2009.
[35] M. Petracca, B. G. Lee, K. Bergman, and L. Carloni. Design exploration
of optical interconnection networks for chip multiprocessors. In 16th
IEEE Symposium on High Performance Interconnects, Aug 2008.
[36] M. Petracca, B. G. Lee, K. Bergman, and L. P. Carloni. Photonic NoCs:
System-level design exploration. IEEE Micro, 29:74‚Äì85, 2009.
[37] V. Puente et al. Adaptive bubble router: a design to improve performance in torus networks. In Proc. Of International Conf. On Parallel
Processing, pages 58‚Äì67, 1999.
[38] Rambus.
RDRAM
memory
technology.
http://www.rambus.com/us/products/rdram/index.html.
[39] J. Schrauwen, F. V. Laere, D. V. Thourhout, and R. Baets. Focusedion-beam fabrication of slanted grating couplers in silicon-on-insulator
waveguides. IEEE Photonics Technology Letters, 19(11):816‚Äì818, June
2007.
[40] A. Shacham, K. Bergman, and L. P. Carloni. Photonic networks-on-chip
for future generations of chip multiprocessors. IEEE Transactions on
Computers, 57(9):1246‚Äì1260, 2008.
[41] L. Shares et al. Terabus: Terabit/second-class card-level optical interconnect technologies. IEEE Journal of Selected Topics in Quantum
Electronics, 12(5), Sept. 2006.
[42] V. Strassen. Gaussian elimination is not optimal. Numerische Mathematik, 14(3):354‚Äì356, 1969.
[43] Sun Microsystems, Inc. Sun Microsystems Ultrasparc T2 datasheet.
available at http://www.sun.com/.
[44] Tilera Corporation.
TILE-Gx processor family.
Online at
http://www.tilera.com/products/TILE-Gx.php.
[45] N. Travinin, H. Hoffmann, R. Bond, H. Chan, J. Kepner, and
E. Wong. pMapper: Automatic mapping of parallel matlab programs. In
DOD UGC ‚Äô05: Proceedings of the 2005 Users Group Conference on
2005 Users Group Conference, page 254, Washington, DC, USA, 2005.
IEEE Computer Society.
[46] C. H. van Berkel. Multi-core for mobile phones. In DATE, pages 1260‚Äì
1265, 2009.
[47] D. Vantrease et al. Corona: System implications of emerging nanophotonic technology. In Proceedings of 35th International Symposium on
Computer Architecture, Aug 2008.

[48] D. Wang et al. DRAMsim: A memory-system simulator. SIGARCH
Computer Architecture News, 33(4):100‚Äì107, Sept. 2005.
[49] M. R. Watts. Ultralow power silicon microdisk modulators and switches.
In 5th Annual Conference on Group IV Photonics, 2008.
[50] W. Wolf. Embedded computer architectures in the MPSoC age. In
WCAE ‚Äô05: Proceedings of the 2005 workshop on Computer architecture
education, page 1, New York, NY, USA, 2005. ACM.
[51] F. Xia, L. Sekaric, and Y. Vlasov. Ultracompact optical buffers on a
silicon chip. Nature Photonics, 1:65‚Äì71, Jan. 2007.

356

IEEE SIGNAL PROCESSING LETTERS, VOL. 19, NO. 6, JUNE 2012

A Stochastic System for Large Network Growth
Benjamin A. Miller, Member, IEEE, and Nadya T. Bliss, Senior Member, IEEE

Abstract‚ÄîThis letter proposes a new model for preferential attachment in dynamic directed networks. This model consists of a
linear time-invariant system that uses past observations to predict
future attachment rates, and an innovation noise process that induces growth on vertices that previously had no attachments. Analyzing a large citation network in this context, we show that the
proposed model fits the data better than existing preferential attachment models. An analysis of the noise in the dataset reveals
power-law degree distributions often seen in large networks, and
polynomial decay with respect to age in the probability of citing
yet-uncited documents.
Index Terms‚ÄîGraph theory, large network analysis, network
growth, preferential attachment, stochastic models.

I. INTRODUCTION

A

GRAPH
is a pair of sets: a set of vertices
representing entities and a set of edges
denoting
relationships that exist between pairs of entities. This structure
is used in a wide variety of applications where relationships or
connections between entities are of interest, such as cyber security, social network analysis and the physical sciences.
In many relevant applications, the topology of a graph will
vary over time, and thus recent research has considered dynamic networks [1]. One form of dynamic behavior in graphs
is network growth, the most popular model for which is preferential attachment [2]. In this model, vertices join the network
one at a time and attach to existing vertices with probabilities proportional to their current degrees. This creates networks
with power-law degree distributions, which are seen in many
real-world graphs.
In this letter, we propose a new preferential attachment model
for directed networks that allows for a decay in popularity and
influence. Rather than using the accumulated degree of a vertex
to determine the likelihood of creating an edge, this model applies a linear, time-invariant filter to the temporal degree sequence of a vertex to calculate this probability, and also incorporates an innovation noise term to determine attachment rates
to vertices without incoming edges. We show that this model
Manuscript received February 20, 2012; accepted March 22, 2012. Date
of publication April 18, 2012; date of current version April 26, 2012. This
work was supported by the Intelligence Advanced Research Projects Activity
(IARPA) via Air Force Contract FA8721-05-C-0002. The U.S. Government
is authorized to reproduce and distribute reprints for Governmental purposes
notwithstanding any copyright annotation thereon. Disclaimer: The views and
conclusions contained herein are those of the authors and should not be interpreted as necessarily representing the official policies or endorsements, either
expressed or implied, of IARPA or the U.S. Government. The associate editor
coordinating the review of this manuscript and approving it for publication was
Prof. Michael Rabbat.
The authors are with the MIT Lincoln Laboratory, Lexington, MA 02420
USA (e-mail: bamiller@ll.mit.edu; nt@ll.mit.edu).
Color versions of one or more of the figures in this paper are available online
at http://ieeexplore.ieee.org.
Digital Object Identifier 10.1109/LSP.2012.2195312

better fits the growth pattern of a large citation network than existing preferential attachment models.
The remainder of this letter is organized as follows. In
Section II, we review preferential attachment and briefly
discuss its variants. Section III outlines our proposed network growth model. Section IV introduces our dataset of
interest‚Äîa dynamic citation graph based on a large document
database‚Äîand demonstrates the fit of this dataset to the proposed model. Characteristics of the innovation noise are also
presented. In Section V we summarize and outline future work.
II. PREFERENTIAL ATTACHMENT
The notion of preferential attachment, introduced by Price
[3], regained significant attention due to the work of Barab√°si
and Albert [2]. The intuition behind this model is that, when a
vertex joins the network, the probability that it connects to an
existing vertex is proportional to the degree of . Formally,
let
denote a graph at time that grows under
preferential attachment. At time
, a new vertex
joins
the network with new edges. The probability that an edge is
created between
and
is given by

where
is the degree (i.e., the number of adjacent edges)
of vertex . Under this model, nodes with higher degree acquire more edges, and this rich-get-richer behavior results in a
power-law degree distribution [4].
While it is easy to comprehend and has several nice properties, this model has a number of shortcomings. One in particular
is the lack of an aging effect. In many graphs of interest, such
as citation networks, the rate at which connections are made to
a vertex will typically decrease as it gets older. This has driven
recent research, especially in the physics literature, into network
growth models that incorporate aging into attachment probabilities. Indeed, several models assume exponential decay in attachment probability based on a vertex‚Äôs age [5]‚Äì[7], some of
which also model the probability as a sublinear polynomial of
the vertex‚Äôs degree, as in [8]. These models formulate the probability of attachment to as
, where
is
the time that joined the network (sometimes called the node‚Äôs
‚Äúbirthday,‚Äù as in [7]). The function is typically of the form
for some
, and is usually of the form
,
.
While these models take into account the degree and age of a
vertex, they do not consider the number of recent connections.
That is, two nodes of the same age with an equal number of connections are equally likely to gain a new connection, even if the
bulk of attachments to one node are more recent than those to the
other. The model we propose, as in [9], takes into account the recency of the connections, as this may be important information.

1070-9908/$31.00 ¬© 2012 IEEE

MILLER AND BLISS: STOCHASTIC SYSTEM FOR LARGE NETWORK GROWTH

357

for all . Since there is, in general, no closed-form solution to
(3), we use the Newton‚ÄìRaphson iteration method to estimate
the coefficient values.

III. PROPOSED MODEL
Definitions and Notation
denote the graph at time . We consider
Let
directed graphs, so
is a set of ordered pairs of vertices in
. We will denote by
an edge from vertex to vertex
. At time
, a new set of vertices
joins the network.
is the number of new
The th entry in the vector
edges connecting to vertex at time . This requires an arbitrary labeling of vertices, which must be consistent across time
samples. Since vertices join the network in a group, we want to
find the attachment rate‚Äîthe expected number of new connections‚Äîrather than the attachment probability. The attachment
rate for vertex at time is denoted by
.
A. Attachment Rate Generation
For vertices with prior connections, we use a discrete linear
time-invariant (LTI) system to generate current attachment
rates. The rate
is determined by applying a set of coefficients to the incoming edge counts for the previous
samples, and another set of coefficients to the
most recent
attachment rates, expressed as
(1)
i.e., an autoregressive moving average model applied to the
vertex in-degree sequences. This formula generalizes two recent
models for preferential attachment with memory, both outlined
in [9]. For
and
with
,
, we
have the Gradually-vanishing Memory Preferential Attachment
Mechanism (GMPAM), in which the attachment rate of vertex
at time is proportional to
, and letting
and
, we have the simpler Short-term Memory
Preferential Attachment Mechanism (SMPAM), where the attachment rate is proportional to the number of attachments made
at the previous time sample.
Under this model, the number of attachments made to vertex
at time is drawn independently from a Poisson distribution
. We can express the log likelihood of
with rate parameter
the observed number of attachments as

(2)
with the log likelihood for all observations at time given by
. If the LTI system generating the attachment rates is a finite impulse response (FIR) filter, i.e., if
in (1), then the maximum likelihood (ML) estimate of the filter
coefficients is a fairly straightforward calculation based on the
observed attachments. In this case, we want to solve the system
of equations
(3)

B. Innovation Noise
Since the proposed model is for directed graphs, and the attachment rates are based entirely on the number of incoming
edges at previous times, we require an additional mechanism to
account for attachment to vertices with no prior incoming connections, i.e., vertices in the set
. We consider this a noise process, similar to innovation
noise that drives stochastic processes in, for example, speech
modeling. For this purpose, we consider the ages of the vertices
with new incoming attachments. Let be the discrete time step
at which vertex , which has no prior incoming connections,
was added to the vertex set. In the present model, we draw an
attachment rate for according to some probability distribution
based on . We do not propose a specific model for the noise
in this letter, instead investigating its properties in the dataset of
interest in Section IV.
IV. DATA ANALYSIS AND RESULTS
In this section, we analyze the commercially available
Thomson Reuters Web of Science¬Æ (WoS) database [10]. This
dataset is comprised of records, compiled for research purposes, representing scholarly publications of the international
scientific community, published between 1900 and present in
public commercial and open source journals and conference
proceedings. Each record represents an individual document,
and fields include document title and type, journal name, author
names and institutional affiliations (as provided in publication),
cited references, and publication date.
Due to the availability of the data at the time of this writing,
we use the documents spanning the years from 1900 to 1979.
The graphs of interest will be citation graphs, with vertices denoting documents and directed edges
denoting that vertex
cites vertex . This set is comprised of about 8.3 million documents with about 81 million references to over 18 million cited
documents, including documents not in the database. Most documents have a year of publication listed, and in many cases there
is also a month. Since we analyze the data in uniform time steps,
we use one year as our sampling period, as it is the most reliable
resolution. The few documents with no date information are disregarded, since we are specifically interested in the dynamics of
the network.
A. LTI System Identification
We identified the LTI system in three configurations: a 1-tap
autoregressive (AR) model (equivalent to GMPAM), a 1-tap
FIR model (equivalent to SMPAM), and a 5-tap FIR model. Coefficient values for a 5-tap filter when fit year by year (i.e., when
the coefficients are estimated independently for each year in
the dataset) are shown in Fig. 1. There are two distinct shapes
in the filter coefficients, one in which the most recent year is
weighted between 0.3 and 0.45, and one with all weights less
than 0.25. This split occurs between 1944 and 1945, with the
earlier years having smaller coefficients. To identify one set of
coefficients for many years, we split the data into two groups,
where the coefficients for the years prior to 1945 are estimated

358

IEEE SIGNAL PROCESSING LETTERS, VOL. 19, NO. 6, JUNE 2012

Fig. 1. Coefficients of a 5-tap FIR filter for citation rate generation. Each curve
represents the coefficients for a single year, with the bold lines representing a fit
to all years before 1945 and since 1945.

separately from those for the later years. The bold lines in the
plot indicate the overall coefficient estimates for the earlier and
later blocks of years.
To evaluate the fit of our models to the data, we use the reduced chi-squared statistic, calculated at each year, given by

where
is the set of indices of the vertices on which we fit
the model, and is the number of parameters. We compare
this technique to other possible methods of rate generation,
in all cases assuming a Poisson distribution on each vertex.
In addition to the LTI systems mentioned above, we fit the
data to a process following a standard preferential attachment
mechanism,

and one in which the rates are defined by algebraic preferential
attachment with an aging effect:
(4)
As for (1), we use the Newton‚ÄìRaphson iteration method to
compute an ML estimate of the parameter values. When using
(4), we restrict the vertices to those within the database, since
we do not currently have the birthdays of documents not in the
database. Similarly, when using a FIR filter, we restrict the vertices to those that have been cited in the past years to avoid
undefined values in the gradient (3).
Evaluations of parameter estimates when fit to the WoS
dataset are shown in Fig. 2. In all cases, we estimate separate
parameters for the citations before 1945 and those occurring
since, as indicated by the dark vertical lines in the plots. In
Fig. 2(a), we fit and evaluated the preferential attachment
model, the 1-tap AR model, and the 1-tap FIR model to documents that had been cited in the previous year. The value of
is similar for the FIR and AR models for all , and both
are significantly closer to 1 than rates generated by preferential
attachment, especially in the years since 1945. Changing the
FIR filter from 1 tap to 5 taps, and now performing the fit on
documents receiving citations within 5 years,
is reduced
for preferential attachment, as shown in Fig. 2(b), but still not

Fig. 2. Reduced chi-squared statistics when fitting the parameters of the LTI
system, and other preferential attachment models, to the WoS data. (a) Nodes
cited in previous year. (b) Nodes cited in previous five years. (c) Nodes with
birthdays, cited in previous five years.

to the level of the FIR and AR filters. In most cases, the statistic
is closer to 1 for the 5-tap FIR model than the 1-tap AR model,
but only slightly, indicating that, while the additional flexibility
may be helpful, using an exponential decay function to predict
the influence of past citations on current ones does nearly as
good a job.
In Fig. 2(c), we compare fitting statistics for the proposed
model with a 5-tap FIR filter and a model based on algebraic
preferential attachment with aging. In this case, we restricted the
data for fitting to those with citations in the last five years that
also exist in the database (so that their birthdays are known).
While this restricted data yields larger values of , the FIR
model again outperforms the model that does not account for
recency of attachments, especially in the later years.
B. Noise Characteristics
The innovation noise is a complicated process, and a thorough characterization is beyond the scope of this letter. Here we

MILLER AND BLISS: STOCHASTIC SYSTEM FOR LARGE NETWORK GROWTH

359

Fig. 3. Characteristics of the innovation noise in the Web of Science dataset. The probability of an old, uncited document being cited decays roughly polynomially
with its age (left). The likelihood of an uncited document of a given age receiving citations decays similarly with respect to (right). (a) Probability of citation
by age and (b) degree of previously uncited docs, 1975.

describe some of the characteristics of citations to previouslyuncited documents. We restrict the data to documents within the
database, since document age may be a useful variable.
One statistic of interest is the probability that a vertex of a
certain age, with no incoming connections, will be cited at a
given time. Formally, we want to know

For documents published since 1945, the proportion of uncited
documents that are cited after a given number of years is shown
in Fig. 3(a). Our intuition suggests that documents that have
not been cited for many years are unlikely to be cited, and the
evidence confirms this. Each curve in the figure corresponds to
a year between 1945 and 1979, and in each case the 1-year-old
documents have the largest proportion that are cited, with
the probability of citing older documents roughly following a
Zipf‚Äôs-law-like polynomial decay.
In addition to the probability of attachment, it is useful to
know how many attachments a previously-uncited document receives in a given year. Thus, we are also interested in

for
. In Fig. 3(b), we plot the in-degree distribution for
uncited documents in 1975 (we see similar behavior in other
years). Each curve corresponds to a different vertex age, from
0‚Äì6 years old, which accounts for 95.4% of the newly-cited
documents and 96.9% of the citations to yet-uncited material
in 1975. For documents of a given age, we see the kind of
power-law degree distribution in the innovation noise that has
been observed in numerous real networks (see, e.g., [11]).
V. SUMMARY
This letter provides a signal processing perspective on preferential attachment, treating the network growth mechanism as
a stochastic system. This system generates attachment rates for
existing vertices based on the output of an LTI model applied
to previous observations and an innovation noise process. A
chi-squared fitting statistic indicates that this model better fits

attachment rates in a large citation network than existing preferential attachment models. Analysis of the innovation noise
reveals that the probability of a previously-uncited document
being cited decreases polynomially with its age, and the number
of new attachments to an uncited document follows a similar
decay function.
In future research, it will be useful to derive a solid formula
for the innovation noise, which will likely depend on as-yet-undecided variables in addition to a vertex‚Äôs age and the number
of new attachments. Considering innovation noise that affects
cited documents‚Äîso that it accounts for not only newly-cited
material, but also deviations from the expected attachment rate
for cited works‚Äîmay improve modeling capability. Optimizing
the order of the filters according to, for example, the Akaike information criterion, is another topic for investigation. Also, incorporating this system into a model that uses additional vertex
metadata for temporal link prediction may prove powerful for
modeling and anomaly detection.
REFERENCES
[1] J. Leskovec, J. Kleinberg, and C. Faloutsos, ‚ÄúGraph evolution: Densification and shinking diameters,‚Äù ACM Trans. Knowl. Discovery From
Data, vol. 1, no. 1, 2007.
[2] A. Barab√°si and R. Albert, ‚ÄúEmergence of scaling in random networks,‚Äù Science, vol. 286, no. 5439, pp. 509‚Äì512, 1999.
[3] D. J. de Solla Price, ‚ÄúNetworks of scientific papers,‚Äù Science, vol. 149,
no. 3683, pp. 510‚Äì515, 1965.
[4] E. D. Kolaczyk, Statistical Analysis of Network Data: Methods and
Models. New York: Springer, 2009.
[5] S. N. Dorogovtsev and J. F. F. Mendes, ‚ÄúEvolution of networks with
aging of sites,‚Äù Phys. Rev. E, vol. 62, no. 2, 2000.
[6] H. Zhu, X. Wang, and J.-Y. Zhu, ‚ÄúEffect of aging on network structure,‚Äù Phys. Rev. E, vol. 68, no. 5, 2003.
[7] A. Herdaƒüdelen, E. Ayg√ºn, and H. Bingol, ‚ÄúA formal treatment of generalized preferential attachment and its empirical validation,‚Äù EPL, no.
78, 2007.
[8] Z. Liu, Y.-C. Lai, and N. Ye, ‚ÄúStatistical properties and attack tolerance of growing networks with algebraic preferential attachment,‚Äù
Phys. Rev. E, vol. 66, no. 3, 2002.
[9] M. Wang, G. Yu, and D. Yu, ‚ÄúMeasuring the preferential attachment
mechanism in citation networks,‚Äù Physica A, no. 387, pp. 4692‚Äì4698,
2008.
[10] Thomson Reuters Web of Science [Online]. Available: http://thomsonreuters.com/products_services/science/science_products/a-z/
web_of_science
[11] M. Faloutsos, P. Faloutsos, and C. Faloutsos, ‚ÄúOn power-law relationships of the internet topology,‚Äù in Proc. SIGCOMM, 1999.

Eigenspace Analysis for Threat Detection in Social
Networks
Benjamin A. Miller, Michelle S. Beard and Nadya T. Bliss
Lincoln Laboratory
Massachusetts Institute of Technology
Lexington, MA 02420
Email: {bamiller, michelle.beard, nt}@ll.mit.edu

Abstract‚ÄîThe problem of detecting a small, anomalous subgraph within a large background network is important and
applicable to many fields. The non-Euclidean nature of graph
data, however, complicates the application of classical detection
theory in this context. A recent statistical framework for anomalous subgraph detection uses spectral properties of a graph‚Äôs
modularity matrix to determine the presence of an anomaly. In
this paper, this detection framework and the related algorithms
are applied to data focused on a specific application: detection
of a threat subgraph embedded in a social network. The results
presented use data created to simulate threat activity among
noisy interactions. The detectability of the threat subgraph and
its separability from the noise is analyzed under a variety of
background conditions in both static and dynamic scenarios.

Keywords: Subgraph detection, threat network detection,
network modularity, signal detection theory
I. I NTRODUCTION
Relational data is highly important in many applications,
including the social sciences, biology and physics. Analysis
of the relationships between entities of interest can provide
insight and awareness that is absent when considering the
entities alone. This form of data is commonly represented with
a graph. A graph G = (V, E) is a pair of sets: a set of vertices,
V , which correspond to the entities; and a set of edges, E,
which represent the relationships. One particularly frequent
application of graphs and relational data is in social network
analysis. In this setting, vertices are people and the edges
denote some interpersonal relationship, such as friendship,
electronic communication or monetary exchange. A great deal
of research has gone into this area, including the detection of
communities [1] and influential figures [2]. Since the primary
focus of this area is on relationships, graphs are a very natural
fit.
One application of social network analysis that is highly
relevant to law enforcement and intelligence applications‚Äî
and has a substantial information fusion component‚Äîis the
detection of threat activity within a network [3], [4]. In
this scenario, the interactions between many individuals are
observed, typically using data obtained from several disparate
sources. Most interactions are innocuous, so the threat activity
This work is sponsored by the Department of the Air Force under Air
Force Contract FA8721-05-C-0002. Opinions, interpretations, conclusions and
recommendations are those of the author and are not necessarily endorsed by
the United States Government.

978-0-9824438-2-8/11/$26.00 ¬©2011 IEEE

is buried within a noisy background. Identifying the presence
of threat activity within background noise is a challenging
problem with substantial interest across many communities.
Another important, related area is anomaly detection in
graphs. There are a number of different flavors of graph-based
anomaly detection [5], [6]. This paper focuses on the form of
the problem in which the objective is to determine whether
an observed graph was generated by a given random process
or if there is other behavior that deviates from the model
[7]. Since graphs are non-Euclidean, the traditional detection
theory used in vector spaces cannot be directly applied. A
recent statistical detection framework proposed in [8], [9]
analyzes the eigenvectors of a graph‚Äôs modularity matrix to
determine the presence of an anomaly.
In this paper we investigate the use of this framework within
the application space of detecting threats in social networks.
As a background we use a graph generator designed to exhibit
characteristics of real networks. A synthetic threat subgraph
we embed into the background is designed to simulate real
threat activity. The remainder of this paper is organized
as follows. In Section II we formally define the subgraph
detection problem and describe our background noise model.
The subgraph detection framework of [8], [9] is reviewed
in Section III. The threat foreground network is briefly described in Section IV. Section V presents experimental results
demonstrating the detection of the threat network in a large
background graph under various conditions. In Section VI we
apply a recent temporal integration technique and demonstrate
a significant increase in signal power by exploiting knowledge
of the evolution of the foreground and the background. In
Section VII we conclude with a brief summary and ideas for
future research.
II. S UBGRAPH D ETECTION P ROBLEM
We formulate the subgraph detection problem as one of detecting a signal in noise. This is similar to [7], which presents
a likelihood-ratio test for subgraph detection in ErdoÃàs‚ÄìReÃÅnyi
random graphs [10]. Casting the problem in the context of
traditional signal detection theory, our objective is to resolve
the binary hypothesis test
(
H0 : The observed graph is ‚Äúnoise‚Äù GB
H1 : The observed graph is ‚Äúsignal+noise‚Äù GB ‚à™ GS .

Here the null hypothesis H0 is that the graph GB = (VB , EB )
is generated by some random model. The alternative hypothesis H1 states that the graph has had an additional subgraph
GS = (VS , ES ) embedded into it. The less likely this subgraph
is to occur under the null model, we expect from intuition,
the easier it will be to detect. We define the union of two
unweighted graphs G1 = (V1 , E1 ) and G2 = (V2 , E2 ) as
G1 ‚à™ G2 = (V1 ‚à™ V2 , E1 ‚à™ E2 ). In our simulations, we use
VS ‚äÇ VB so that our foreground is naturally connected to the
background.
There are a number of possible backgrounds to consider.
The simplest background is the ErdoÃàs‚ÄìReÃÅnyi random graph,
in which each of the |V2 | possible edges occurs with a fixed
probability p. In a more general random graph model proposed
by Chung and Lu [11], each vertex vi ‚àà V has a specified
expected degree di , and the probability of an edge occurring
between vertices vi and vj is given by
d i dj
,
pij = P
k dk
P
with the restriction that d2i ‚â§ k dk for all i. In this case,
while the probabilities are no longer identical, the edges still
occur independently of each other.
In our simulations we use the R-MAT Kronecker graph
as a background model [12]. This model exhibits a heavytailed powerlaw degree distribution and inherent clustering of
vertices frequently seen in real network topologies. An R-MAT
graph is generated based on a probability matrix


a c
P =
,
b d
with 0 ‚â§ a, b, c, d ‚â§ 1 and a + b + c + d = 1. At each
iteration, an edge is added with probability dictated by the
N -fold Kronecker product of P (there are, thus, 2N vertices
in the graph). The procedure ends when the graph has a given
number of edges M . This model is used as a background in
[3], along with a foreground similar to the one we discuss in
Section IV. We denote by
G ‚àº RMAT(a, b, c, d, M, N )
that G is generated by the R-MAT algorithm with the given
parameters. Since the R-MAT algorithm generates a directed
graph, we undirect the edges of the resulting graph. This
approximately doubles the number of edges, and is done
implicitly throughout the rest of the paper.
III. M ODULARITY A NALYSIS FOR S UBGRAPH D ETECTION
The recent subgraph detection framework of [8], [9] is based
on the analysis of modularity. Modularity is commonly used
to determine how well the vertices of a graph separate into
communities [13]. The modularity matrix B of an unweighted,
undirected graph G = (V, E) is given by
B =A‚àí

KK T
.
2|E|

(1)

Here A is the adjacency matrix of G, where Aij is 1 if
{vi , vj } ‚àà E and is 0 otherwise; and K is the observed degree

vector, where Ki is the degree of vi . Note that the right-hand
term in (1) is a matrix of edge probabilities under the Chung‚Äì
Lu model mentioned in Section II, with the expected degree
sequence given by the observed degrees. The matrix B can
therefore be considered a residuals matrix, comprised of the
difference between the observed edges A and the ‚Äúexpected‚Äù
edges KK T / (2|E|). The ‚Äúexpected‚Äù term assumes that the
edge probabilities depend only on the degree of the vertices,
so graphs with more clustering will have larger residuals.
The algorithms in [8], [9] analyze the residuals under H0
and H1 and compute test statistics to discriminate between the
two hypotheses. Both algorithms are based on the eigendecomposition of B, given by B = U ŒõU T . Since B is symmetric,
U is an orthonormal matrix of eigenvectors of B and Œõ is a
diagonal matrix of real eigenvalues. Let Œªi denote Œõii , with
Œªi ‚â• Œªi+1 for 1 ‚â§ i < |V |. Then the eigenvector associated
with Œªi is the ith column of U , denoted by ui .
In [8], the two principal components of B are analyzed to
determine the presence of an anomalous subgraph. Letting u1
and u2 comprise coordinate axes, a chi-squared test statistic
is computed based on the number of points that fall into
each quadrant. The test statistic is maximized with respect
to rotation in the plane, and thus is smaller for projections
that are more radially symmetric. This technique is shown to
detect the presence of a small, dense subgraph embedded into
either an R-MAT or ErdoÃàs‚ÄìReÃÅnyi background. Since we only
need to compute the first two eigenvectors of B, we can use
an iterative sparse eigensolver to compute this test statistic in
O(|V | + |E|) time per iteration.
Some subgraph anomalies, however, will be too small to
stand out in the two principal eigenvectors of B. The method
of [9] seeks to detect smaller anomalies, and does so by
analyzing the L1 norms of many more of the eigenvectors.
The intuition behind this technique is that if there is a small
subset of vertices that is much more tightly connected than
expected, there will be an eigenvector closely aligned with
this subgraph (i.e., with large components in entries that
correspond to subgraph vertices). Since the eigenvectors are
unit-normalized in their L2 norms, an eigenvector that is
aligned with a few axes will have a smaller L1 norm than one
that is not. This property is exploited to detect subgraphs in an
R-MAT background that are undetectable in the two principal
eigenvectors: if there is an eigenvector with an exceptionally
small L1 norm, a detection is declared. The complexity of this
method is also dominated by the sparse eigensolver, which
takes O(|E|k + |V |k2 + k 3 ) time per iteration to compute the
k most significant eigenvectors [14].
Both of these methods are designed for uncued detection,
i.e., detection of the anomalous subgraph without a cue to
the vertices that are exhibiting interesting behavior. They rely,
therefore, on the subgraph having a substantially different
topology than the background graph. When detecting threat
activity in social networks, it is unclear how much the topology
of the threat subnetwork will deviate from the norm, and one
purpose of this paper is to analyze the conditions under which
a realistic foreground stands out in a complex background.

15
16

1

4

9

5

11

3

13

8
2

10
18

7
12

14

6

19

20

17

Figure 1.

The CT-SNAIR threat network.

Figure 2.
Scatterplot of the CT-SNAIR scenario embedded into an RMAT background graph. The foreground vertices are inseparable from the
background in this subspace.

IV. T HREAT N ETWORK F OREGROUND
Our foreground of interest comes from the Counter-Terror
Social Network Analysis and Intent Recognition (CT-SNAIR)
effort [3]. The authors of [3] use their simulator to synthesize
transactions in a threat network, mimicking specifically the
planning and execution of the 2004 bombing of the Australian
embassy in Jakarta. The network presented in Figure 1 is
generated by the same simulator. This graph is not tightly
connected: it has 20 vertices and an average degree of 2.5. Half
of the vertices have only one edge. The maximum eigenvalue
of this graph‚Äôs adjacency matrix‚Äîwhich has been a good
measure of signal strength using the spectral methods proposed
here‚Äîis about 3.72. This embedding should be difficult to
detect in background networks with inherent clustering.
V. T HREAT D ETECTION IN S TATIC G RAPHS
In our first set of experiments, we want to determine whether
the observed graph was generated by a random process, or if
it had an additional subgraph embedded. In all cases we use
the R-MAT generator to create a background graph and use
the foreground presented in Section IV.
The focus of our simulations is on cases where the foreground does not stand out in the principal components. In
practice, it will often be the case that the difference between
the topologies of the threat network and the background is not
exceptionally strong, and thus the technique of [8] will not
reliably reveal the subgraph‚Äôs presence. An example is shown
in Figure 2, which presents B projected into its two principal eigenvectors for a graph generated under the alternative
hypothesis. (The background in this example is taken from
the experiments discussed later in this section.) Each point in
the scatterplot correponds to a vertex. The foreground vertices
are buried within the background, preventing detection of the
foreground in this subspace. We therefore use the eigenvector
L1 norm analysis technique of [9] for the experiments in this
section.
In [9], it is shown that detection performance improves as
the embedded subgraph decreases its external degree (i.e., the

number of edges between the subgraph and the rest of the
graph). Selection of vertices for the embedding was biased
by simply choosing the foreground vertices from the set of
background vertices with degree under some threshold. When
embedding the foreground described in Section IV, however,
our subgraph detection performance is not much better than
chance, even when we embed the foreground of interest on a
set of 20 background vertices all with degree 1. We thus take a
different approach to embedding the subgraph as follows. First
we perform an eigendecomposition of the modularity matrix
of the background alone, denoted by BB . Let the eigenvectors
of BB associated with eigenvalues greater than some threshold
b , and uÃÇi be the
(we use 3) comprise the columns of a matrix U
b . We then select the vertices for the embedded
ith row of U
subgraph from the vertex subset
Vb = {vi ‚àà V | kuÃÇi k22 < t},

(2)

where t is a parameter we allow to vary. As we reduce t, our
detection performance should improve. The intuition behind
this procedure is that we choose a set of foreground vertices
roughly orthogonal (as t ‚Üí 0) to the strongest portion of
the background. This suggests that the foreground will not
be overpowered by stronger portions of the background. We
select the foreground vertex set uniformly at random from
all 20-vertex subsets of Vb . As we will show, this embedding
procedure has a significant impact on detection performance,
demonstrating that the detectability of a subgraph using this
method is not a simple matter of internal and external degree.
In this experiment,


1‚àíp 1‚àíp 1‚àíp
GB ‚àº RMAT p,
,
,
, 2048, 10 ,
4
4
2
with p varying from 0.35 to 0.60. Example distributions of
eigenvalues are shown in Figure 3. As p increases, the number
of eigenvalues greater than 3 decreases, but they take on much
larger values. When embedding the threat foreground, t in (2)
is varied from 0.001 to 0.050. We ran a 10,000-trial Monte

e that
the maximum eigenvalue of the principal submatrix of B
corresponds to the subgraph vertices. Let hf g be a filter that
maximizes this quantity and hbg emphasize the background.
Taking both of these into account, we use the filter
h = hf g ‚àí hbg

Figure 3. Average eigenvalues of the R-MAT background‚Äôs modularity matrix
for various values of p.

Carlo simulation for each case (i.e., each value of p and t),
computing the L1 norms of the top 100 eigenvectors of B
under the null and alternative hypotheses.
Results of this experiment are given in Figure 4. As expected, regardless of the value of p, detection performance
improves as t gets smaller, since this means that the foreground
is being embedded into a space less correlated with the
stronger eigenvectors. Also, as p increases, detection performance degrades. This may be because the most significant
b
eigenvalues are larger as p increases, and so the subspace U
is stronger, with greater potential to overpower the embedding.
VI. T HREAT D ETECTION IN DYNAMIC G RAPHS
The framework used in the previous section has recently
been extended to include dynamic graphs, combining temporal
integration gain with the statistical tests to achieve greater detection power [15]. In this setting, we have vertex and edge sets
that vary over time. We will denote by G(n) = (V (n), E(n))
the graph at a discrete time step n, and we let B(n) denote
the modularity matrix of G(n). By integrating the modularity
matrices over time, we can detect much weaker subgraphs
than using the same method with a static graph, as we will
demonstrate in this section using a variety of background and
foreground evolution models.
Our approach to temporal integration is similar to that for
Euclidean data. We use a finite impulse response filter h to
integrate the modularity matrices over a time window of `
samples, computing
e
B(n)
=

`‚àí1
X

B(n ‚àí m)h(m).

m=0

The objective is then to pick filter coefficients that emphasize the foreground while de-emphasizing the background.
We do this‚Äîstill without a cue to the subgraph vertices‚Äî
by leveraging knowledge of the subgraph‚Äôs evolution over
time, as we explain later in this section. Since we are using
spectral techniques, we use as a measure of signal strength

hTfg hbg
khbg k22

(3)

to filter the modularity matrices in our experiments. In (3),
the filters are treated as vectors in R` , with the entries in the
vectors corresponding to the filter weights.
In the dynamic scenario we use two different background
models, both based on the R-MAT generator. One dynamic
background simply generates independent R-MAT graphs at
each time step, i.e.,


1‚àíp 1‚àíp 1‚àíp
GB (n) ‚àº RMAT p,
,
,
, 2048, 10
(4)
4
4
2
for all n. This is a powerlaw-graph-based version of independent, identically distributed noise. In many applications,
however, there will be a strong dependence between consecutive snapshots of the network. To simulate this effect, we take
the following approach. At the first time sample, n = 0, we
generate an R-MAT graph as in (4). For the following samples,
we generate an ‚Äúupdate‚Äù graph


1‚àíp 1‚àíp 1‚àíp
GU (n) ‚àº RMAT p,
,
,
, 16, 10
(5)
4
4
2
and let GB (n) = GB (n ‚àí 1) ‚à™ GU (n). The vertex sets
remain the same throughout the process to maintain the
distributions of edge probabilities. Since the same probability
matrix is used, this has the same effect as capturing the RMAT background at various points in the graph generation
process (i.e., GB (n ‚àí 1) is GB (n) with the procedure stopped
early).
For the evolving foreground graph, we use the same graph
as in Section IV but add or remove edges over the course of
the time window. We simulate 3 behaviors in the experiments.
The first and simplest is linear growth in the size of the edge
set until all 25 edges are present at the last time step, i.e.,
having the number of edges follow


25
|ES (n)| =
n .
`‚àí1
Another dynamic foreground model linearly adds edges until
a certain point in the window, at which point the edges are
linearly removed. This could simulate a process in which
transactions (e.g, meetings or communication) between actors
increase as an event is planned, but then rapidly decrease to
avoid detection. In this case, the size of the edge set increases
linearly until n = 3`/4 ‚àí 1, when |E(n)| = 25. For the
remainder of the window edges are removed linearly until
|E(n)| = 0 at n = ` ‚àí 1. Our third subgraph evolution pattern
follows sinusoidal growth and decay, with


 
2œÄn
25
|ES (n)| =
1 ‚àí cos
.
2
`

(a)

(b)

(c)

(d)

(e)

(f)

Figure 4.

Threat detection performance using eigenvector L1 norm analysis.

In all cases, the order that edges are added is selected uniformly at random from the 25! permutations of edges. When
edges are removed, their removal occurs in the same order as
their addition.
Our experiments consist of several 10,000-trial Monte Carlo
simulations. For each simulation we choose one of our two
dynamic background models and one of our three foreground
patterns. Under both the null and alternative hypotheses, we
filter the graph with the appropriate coefficients for the chosen
background and foreground over a time window with ` = 32.
To determine the filter coefficients, we use the maximum
eigenvalue of BB (n ‚àí m) for hbg (m) and the maximum
eigenvalue of AS (n ‚àí m) for hf g (m), with AS denoting
the adjacency matrix of the subgraph vertices. (Since the
background and foreground are both random, these values are
averaged over several trials, then fixed for the experiments.)
While we do not claim that these are the optimal filter
coefficients, the results demonstrate a significant increase in
detection power using this method. Unlike in the previous
section, we select the foreground vertex subset from all
background vertices, uniformly at random from among all 20vertex subsets.
Using a background where an independent R-MAT graph is
generated at each sample, we achieve detection performance
as shown in Figure 5. In this case, we use the chi-squared test
e Under the null
statistic in the two principal components of B.
model there is good radial symmetry of the projection using
each of the filters. The embeddings stand out in this subspace,
creating a change in the symmetry of the projection, and we

achieve excellent detection performance, as demonstrated in
the figure. Recall from the example shown in Figure 2 that
this foreground, even when all 25 edges are present, does not
stand out in the principal two eigenvectors of B. By leveraging
our knowledge of the evolution of the subgraph, however,
we can increase the signal power by performing coherent
temporal integration. In all cases, detection performance is
very good when p = 0.5, with equal error rates of 3% or
less. For the densifying and densify-and-disperse models in
Figures 5(a) and 5(b), respectively, there is a slow degradation
in performance as p is increased, since this increases strength
of the background. The sinusoid foreground model, shown in
Figure 5(c), on the other hand, has near-perfect performance
in all cases. On closer inspection, the sinusoidal filter provides
greater integrated signal power than the other filters provide for
their respective signal models, and suppresses the background
about equally.
Using the ‚Äúexpanding‚Äù background (where at each sample
the graph is updated according to (5)), the projection onto
the two principal eigenvectors is not radially symmetric, and
the chi-squared statistic frequently takes on large values, even
under H0 . This results in poor detection performance using
this metric, with equal error rates in all cases of about 50%.
Understanding this phenomenon is a topic for further research.
We can, however, detect the presence of the foreground using
e If our embedding is strong
the maximum eigenvalue of B.
enough, it will increase the maximum eigenvalue of this matrix. (This could be used with the ‚Äúindependent‚Äù background as
well, but the chi-squared statistic yields superior performance

(a)
Figure 5.

(b)

(c)

Dynamic threat detection performance with an R-MAT background generated independently at each time step.

Projection of B into two eigenvectors based on analysis of a cue

Figure 6. Detection performance using a maximum eigenvalue metric to
detect a densifying threat network in an expanding R-MAT background.

Figure 7.
vertex.

in that case.) Performance with the ‚Äúlinear growth‚Äù foreground
is shown in Figure 6. Again, there is an increase in detection
performance as p decreases. Using the other two foreground
models, we achieve perfect detection for the same values of
p. The weaker performance for the densifying foreground
is likely due to the similarity between the evolution of the
foreground and the background. Here, using a different background and a different test statistic, we again see the advantage
of temporal integration gain in subgraph detection.

‚Äúpower‚Äù boosted with some knowledge of how it evolves over
time.
One area for future research is cued anomaly detection.
While our focus here is uncued detection of anomalous subgraphs, modularity analysis can be useful for cued detection
as well. Given a cue vertex vc ‚àà VS , the eigenvectors of B
that have the largest component associated with vc can be
analyzed for other vertices that have large components in those
eigenvectors. An example is shown in Figure 7, taken from
the Monte Carlo simulation in Section V with p = 0.50 and
t = 0.050. Recall from Figure 4(d) that in this case, detection
performance with eigenvector L1 analysis is no better than
chance. However, in the space of u78 and u72 , the axes used
in the figure, a significant portion of the subgraph stands out.
Most of the central subgraph vertices are more correlated with
these eigenvectors than any others, which is the case for only
one of the background vertices. With a cue to a subgraph
vertex, the subspace in the figure may be found by looking
at the most correlated eigenvectors. The use of cues with the
spectral techniques presented here, currently being explored in
[16], may prove to be quite powerful.
Other ongoing work in this area includes optimizing filter
coefficients, further analysis of the ‚Äúexpanding‚Äù background

VII. C ONCLUSION
In this paper we demonstrate the use of a statistical framework
for anomalous subgraph detection with a focus on a specific
problem: detection of threat activity within a social network.
Using a background model designed to model real-world
networks and a foreground synthesized to simulate a threat network, we present several simulations showing the conditions
under which the proposed algorithms can detect the presence
of the foreground without a cue. In a static background, the
foreground is detected when it is not well-correlated with
strong portions of the background. In a dynamic setting, we
demonstrate that even this weak threat subgraph can have its

model and its interaction with the filters, and an analysis of
the subspaces where the subgraph is embedded in Section V
to determine if detection performance tracks other, easier-tocompute graph statistics. Also, the combination of topological
detection methods like those used here with additional vertex
or edge metadata to provide another level of ‚Äúfiltering‚Äù is an
area of significant interest.
ACKNOWLEDGMENT
The authors wish to thank J. Acevedo-Aviles and W. Campbell
for providing the foreground network data, and M. C. Schmidt
and the anonymous reviewers for helpful comments on this
paper.
R EFERENCES
[1] N. Du, B. Wu, X. Pei, B. Wang, and L. Xu, ‚ÄúCommunity detection
in large-scale social networks,‚Äù in Int. Conf. Knowledge Discovery and
Data Mining, pp. 16‚Äì25, 2007.
[2] J. M. Kleinberg, ‚ÄúAuthoritative sources in a hyperlinked environment,‚Äù
Journal of the ACM, vol. 46, pp. 604‚Äì632, September 1999.
[3] C. Weinstein, W. Campbell, B. Delaney, and G. O‚ÄôLeary, ‚ÄúModeling
and detection techniques for counter-terror social network analysis and
intent recognition,‚Äù in Proc. IEEE Aerospace Conf., pp. 1‚Äì16, 2009.
[4] T. L. Mifflin, C. Boner, G. A. Godfrey, and J. Skokan, ‚ÄúA random
graph model for terrorist transactions,‚Äù in Proc. IEEE Aerospace Conf.,
pp. 3258‚Äì3264, 2004.

[5] W. Eberle and L. Holder, ‚ÄúAnomaly detection in data represented as
graphs,‚Äù Intelligent Data Analysis, vol. 11, pp. 663‚Äì689, December
2007.
[6] C. E. Priebe, J. M. Conroy, D. J. Marchette, and Y. Park, ‚ÄúScan statistics
on Enron graphs,‚Äù Computational & Mathematical Organization Theory,
vol. 11, no. 3, pp. 229‚Äì247, 2005.
[7] T. Mifflin, ‚ÄúDetection theory on random graphs,‚Äù in Proc. Int. Conf.
Inform. Fusion, pp. 954‚Äì959, 2009.
[8] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúToward signal processing
theory for graphs and non-Euclidean data,‚Äù in Proc. IEEE Int. Conf.
Acoust., Speech and Signal Process., pp. 5414‚Äì5417, 2010.
[9] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúSubgraph detection using
eigenvector L1 norms,‚Äù in Advances in Neural Information Processing
Systems 23 (J. Lafferty, C. K. I. Williams, J. Shawe-Taylor, R. Zemel,
and A. Culotta, eds.), pp. 1633‚Äì1641, 2010.
[10] P. ErdoÃàs and A. ReÃÅnyi, ‚ÄúOn random graphs,‚Äù Publicationes Mathematicae, vol. 6, pp. 290‚Äì297, 1959.
[11] F. Chung and L. Lu, ‚ÄúConnected components in random graphs with
given expected degree sequences,‚Äù Annals of Combinatorics, vol. 6,
no. 2, pp. 125‚Äì145, 2002.
[12] D. Chakrabarti, Y. Zhan, and C. Faloutsos, ‚ÄúR-MAT: A recursive model
for graph mining,‚Äù in Proc. SIAM Data Mining Conf., pp. 442‚Äì446,
2004.
[13] M. E. J. Newman, ‚ÄúFinding community structure in networks using the
eigenvectors of matrices,‚Äù Phys. Rev. E, vol. 74, no. 3, 2006.
[14] S. White and P. Smyth, ‚ÄúA spectral clustering approach to finding
communities in graphs,‚Äù in Proc. SIAM Data Mining Conf., 2005.
[15] B. A. Miller, M. S. Beard, and N. T. Bliss, ‚ÄúMatched filtering for
subgraph detection in dynamic networks,‚Äù in Proc. IEEE Statistical
Signal Process. Workshop, 2011. To appear.
[16] S. T. Smith, A. Silberfarb, S. Philips, E. K. Kao, and C. Anderson,
‚ÄúNetwork discovery using wide-area surveillance data,‚Äù in Proc. Int.
Conf. Inform. Fusion, 2011. To appear.

2012 Conference on Intelligent Data Understanding

Characterization of Traffic and Structure in the U.S.
Airport Network
Vineet Mehta, Feanil Patel, Yan Glina, Matthew Schmidt, Ben Miller and Nadya Bliss
MIT Lincoln Laboratory, 244 Wood Street, Lexington, MA 02421

Abstract‚ÄîIn this paper we seek to characterize traffic in
the U.S. air transportation system, and to subsequently develop
improved models of traffic demand. We model the air traffic
within the U.S. national airspace system as dynamic weighted
network. We employ techniques advanced by work in complex
networks over the past several years in characterizing the structure and dynamics of the U.S. airport network. We show that the
airport network is more dynamic over successive days than has
been previously reported. The network has some properties that
appear stationary over time, while others exhibit a high degree
of variation. We characterize the network and its dynamics using
structural measures such as degree distributions and clustering
coefficients. We employ spectral analysis to show that dominant
eigenvectors of the network are nearly stationary with time. We
use this observation to suggest how low dimensional models of
traffic demand in the airport network can be fashioned.

I. I NTRODUCTION
In this paper our aim is to characterize traffic in US national
air space in order to gain insight that would subsequently
lead to developing detailed demand models. We model the
US air transportation system as a network to assist with the
characterization of traffic demand. The U.S. airport network
has been the subject of previous study, although not with the
aim of developing traffic demand models. Xu and Harriss [1]
have studied the structure of the U.S. intercity passenger air
transportation network. The cumulative degree distribution of
the network was shown to have a power-law tail. The vertex
strengths (with number of passengers, fare and distance as
incident edge weights) were shown to grow as a power of the
vertex degree. As a result airports with higher degree were
found to handle more traffic than those with smaller degree.
The clustering property of the network was also examined
through clustering coefficients. The clustering coefficient as a
function of degree was found to have a power-law distribution,
with lower clustering coefficients for higher degree vertices.
This behavior is characteristic of a hierarchical network structure.
There have been similar studies of the airport network structure for other national networks and the world wide network.
The evolution of the Chinese airport network over successive
years has been analyzed by Zhang,et.al. [2]. The traffic growth
in this network was found to correlate well with the growth
This work was sponsored by the Assistant Secretary of Defense (Research
and Engineering) under Air Force Contract FA8721-05-C-0002. Opinions,
interpretations, conclusions, and recommendations are those of the author and
are not necessarily endorsed by the United States Government.

in Chinese GDP. The Chinese airport network was shown to
have a degree tail distribution which exhibits a two-regime
power-law. The relationship between the clustering coefficient
and degree showed vertices with smaller degree to have larger
clustering coefficient. The vertex traffic strength was shown to
grow as a power of vertex degree. The topological properties
on the Chinese airport network were seen to remain nearly
fixed over 2002 to 2009, even though airports were added and
removed from the network and the network saw an exponential
growth in passenger and cargo traffic. Bagler [3] has studied
the weighted airport network of India. da Rocha [4] examined
the structure and evolution of the Brazilian airport network
over consecutive years and found similar degree, strength and
clustering characteristics as other examinations of national
airport networks. The worldwide air transportation network
was also shown to exhibit some characteristics similar to other
national airport networks [5].
We examine the temporal dynamics of the network by
considering the behavior over a number of successive days. We
find the network to be highly dynamic over successive days,
and exhibit an unexpected level of spatial and temporal complexity. In the next section (Section II) we present the approach
we have employed in constructing the daily airport networks.
The resulting airport networks and their spatial structure is
shown in Section III. The structural characteristics, as well as
the dynamic behavior of the US airport network is discussed
in Section IV. An analysis of the spectral characteristics of
the airport network using the adjacency matrix is presented in
Section V.
II. N ETWORK C ONSTRUCTION
In order to construct the US airport pair network, we employ
the Aircraft Situation Display to Industry (ASDI) [6] data
feed provided by the Federal Aviation Administration (FAA).
The generation of graphs for the airport network relies on
the use of Flight Management Information (RT) messages.
The RT messages contain information about a flight‚Äôs identity,
origin, destination, departure/arrival times, and filed route. We
construct each graph by either adding new airport pairs (edges)
or updating the flight count on an existing edge for all flights
reported by RT messages occurring within a 24-hour period.
These graphs include flights with origin or destination outside
the continental US territory. The graphs are then truncated to
retain only airports that reside inside the bounding spherical

124

2012 Conference on Intelligent Data Understanding

Ci =

2|{ek }|
, ek = (vi , vj ) 2 E ^ vj 2 Ni
Ô£øi (Ô£øi 1)

(1)

III. U.S. A IRPORT N ETWORK
The US airport network and its dynamic character is depicted in Figure 2. The graphs shown in Figure 2(a-c) capture
the aggregate air traffic between city pairs on a particular date.
The graph in Figure 2(d) is the union-graph GÃÑ[ . This graph is
computed from a set of N daily weighted graphs {Gn }, with
edge sets {En } and vertex sets {Vn }. The edge set EÃÑ[ and
vertex set VÃÑ[ of GÃÑ[ are given as:
EÃÑ[ =
The weight of the k
as:

N[1

En ,

n=0

VÃÑ[ =

N[1

Vn

(2)

n=0

th edge, wÃÑk , in the set EÃÑ[ is computed

wÃÑk =

N 1
1 X
wÃÇk,n
N n=0

(3)

The quantity wÃÇk,n is equal to the weight wk,n for k 2 En ,
and zero for k 2
/ En . The strength sÃÑi of vertex vi 2 VÃÑ[ is the
sum of weights wÃÑl(i) , where l(i) is the index over edges in
the incidence set L(i) of the vertex vi .
The graphs shown in Figure 2(a-c) depict the volume of
traffic carried on links between city pairs. The straight line
edges are meant to depict linkages between city pairs, and
should not be mistaken as travel paths between the cities.
The edges are assigned colors such that blues denote lower
traffic volume, and reds denote higher traffic volume. The
edge colors make it easier to visually discern that daily graphs
are dominated by lower volume edges, and have relatively

fewer higher volume edges. This is verified by examining the
distribution of edge weights in Figure 1.
‚óè
‚óè

103.5

‚óè
‚óè

103
‚óè
‚óè

‚óè
‚óè

102.5

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

Frequency

quadrilateral specified by (latitude, longitude)-coordinates:
(25 , 125 ) and (50 , 65 ).
The outcome of the aforementioned processing are daily
graphs Gn , where n is the daily index. Each graph Gn is
specified by a set of edges En and a set of vertices Vn . The
set Vn is composed of vertices {vi }n . Each vertex vi denotes
an airport with geographic position # = ( , ‚úì), where and
‚úì specify the latitude and longitude respectively. The set En
is composed of vertex two-tuples {(vi , vj )n |vi , vj 2 Vn }. For
the analysis in this paper we consider {Gn } to be undirected
weighted graphs. The weight of each edge ek,n = (vi , vj )n is
denoted by wk,n , with k as the edge index.
Here we define three parameters that are employed in subsequent sections for quantifying graph characteristics: vertex
degree, vertex strength, and local clustering coefficient. The
degree Ô£øi is defined as the number of edges incident upon
the vertex vi . The strength si of vertex vi is defined as the
sum of edge weights for all edges incident upon the vertex.
In order to define the clustering coefficient Ci , we first define
the vertex neighborhood Ni = {vj : ek = (vi , vj ) 2 E}. For a
vertex with Ô£øi neighbors it is possible to define a maximum of
Ô£øi (Ô£øi 1)/2 edges amongst the neighbors. The local clustering
coefficient Ci measures the number of actual edges within the
neighborhood as a fraction of the maximum number possible.

2

10

‚óè
‚óè
‚óè
‚óè

10

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè
‚óè ‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

1.5

101
10

‚óè ‚óè‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

‚óè

‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

0.5

‚óè‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè‚óè
‚óè
‚óè
‚óè

100

‚óè‚óè
‚óè
‚óè

‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

‚óè‚óè
‚óè‚óè
‚óè
‚óè

‚óè‚óè
‚óè ‚óè
‚óè
‚óè‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

0

0.5

10

10

1

10

1.5

10

102

Edge Weight

Fig. 1.

Distribution of edge weights

The variations in spatial edge density and coloring of edges
in these plots illustrate the changes in traffic across the national
airspace over consecutive days. This is visually most apparent
in comparing graph Figure 2(c), which shows the traffic for a
Saturday, with graphs for the other days. The size and color
of the vertices depict their strength, which is the number
of flights that were served by the airport during that day.
The major airports such as Chicago O‚ÄôHare International
Airport (KORD), Hartsfield-Jackson Atlanta Airport (KATL),
Denver International Airport (KDEN), and Dallas/Fort Worth
International (KDFW) stand out as the busiest airports by
traffic volume.
The union-graph GÃÑ[ shown in Figure 2(d) is computed from
a set of 29 daily graphs over the period 26 April 2011
to 24 May 2011. These graphs show expected as well as
some unexpected features. We had anticipated that a set of
major airports would continue to represent the vertices with
dominant strength over consecutive days. However, we did
not expect the amount of variation found in the sub-dominant
vertices and edges in the set of daily graphs examined. A
remarkable contrast is found in comparing
P the average number
of edges in the daily graph |E¬Øn | = N1
|En | to the number
of edges in the expected graph, |EÃÑ|. The quantity |E¬Øn | is
approximately 10K, while |EÃÑ[ | is approximately 80K. This
surprisingly large difference suggests a level of dynamicism
in the daily air traffic over the national airspace that we have
not seen reported elsewhere.
IV. S TRUCTURAL C HARACTERISTICS
In this section we employ basic structural metrics such
as vertex degree, edge weights, and vertex clustering for
characterizing both static and dynamic aspects of the daily
airport network graphs. The tail distributions of degree Ô£ø for
Gn over selected days are shown in Figure 3. The degree tail
distributions Pn (Ô£ø > K) for the consecutive days are found to
be similar. The mean distribution is found to exhibit a power
law decay, P (Ô£ø > K) ‚á† Ô£ø ‚Üµ , with ‚Üµ ‚á° 1.17.

125

2012 Conference on Intelligent Data Understanding

(a) Tuesday, 04/26/2011

(b) Thursday, 04/28/2011

(c) Saturday, 04/30/2011

(d) Union-Graph GÃÑ[

Fig. 2.

‚óè

‚óè

‚óè

‚óè

‚óè ‚óè
‚óè ‚óè
‚óè ‚óè
‚óè‚óè
‚óè

10‚àí1

10‚àí2

10‚àí3

‚óè
‚óè‚óè

‚óè‚óè
‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè
‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

Average Vertex Clustering Coefficient

Tail Distribution P(k > K)

100

(a-c) Gn : Daily Geospatial graphs of the US airport network; (d) GÃÑ: Expected value graph

10‚àí4
1

10

1.5

10

2

10

0.5
‚óè

0.4

‚óè

‚óè
‚óè

‚óè
‚óè

0.3

0.2

‚óè
‚óè

‚óè
‚óè

‚óè‚óè

‚óè
‚óè

‚óè ‚óè‚óè

‚óè

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè ‚óè ‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè‚óè ‚óè
‚óè‚óè ‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè ‚óè

‚óè
‚óè

‚óè
‚óè

‚óè

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

‚óè

‚óè
‚óè

‚óè
‚óè

‚óè

‚óè
‚óè
‚óè ‚óè
‚óè ‚óè

‚óè

‚óè

‚óè

‚óè
‚óè

2.5

10

50

Degree

Fig. 3.

‚óè
‚óè

‚óè

‚óè

100

150

200

Vertex Degree

Degree distribution for daily graphs

Fig. 4.

The variation in the clustering coefficient with vertex degree
Ô£ø is given in Figure 4. The clustering coefficient is found to
exhibit approximately a uniform distribution with degree Ô£ø.
This behavior is in contrast with results reported in other investigations of the airport network. We believe this difference can
be explained by examining the clustering coefficient for the

Clustering coefficient for daily graph

intersection-graph G\ . We define the intersection-graph GÃÑ\
as the intersection of as set of graphs {Gn }. The intersectiongraph has an edge set EÃÑ\ = \En , a vertex set VÃÑ\ = \Vn , and
weights {wÃÑk } that are averaged over the weight sets {wk,n }.
The variation in the clustering coefficient with vertex degree
k for the intersection-graph GÃÑ\ is given in Figure 5. The near

126

2012 Conference on Intelligent Data Understanding

‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè ‚óè
‚óè ‚óè
‚óè
‚óè

0.8

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè
‚óè
‚óè

‚óè‚óè
‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè ‚óè
‚óè

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè‚óè
‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

0.4

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

40000

‚óè
‚óè

80

100

120

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè

‚óè ‚óè
‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè

140

25‚àíApr

02‚àíMay

‚óè
‚óè

09‚àíMay

Degree

‚óè
‚óè

16‚àíMay

Clustering coefficient for the intersection graph

Fig. 7.

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

11500

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

10500

‚óè
‚óè

10000

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè

9000

‚óè

‚óè
‚óè
‚óè
‚óè

0

8500

‚óè
‚óè

‚óè‚óè

‚óè

‚óè

500

‚óè

‚óè
‚óè ‚óè ‚óè

25‚àíApr

‚óè

02‚àíMay

‚óè
‚óè
‚óè‚óè
‚óè ‚óè ‚óè ‚óè‚óè ‚óè
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè ‚óè ‚óè
‚óè
‚óè
‚óè ‚óè
‚óè ‚óè‚óè ‚óè
‚óè ‚óè
‚óè
‚óè‚óè ‚óè‚óè
‚óè‚óè ‚óè‚óè
‚óè
‚óè ‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè ‚óè ‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

100

16‚àíMay

‚óè

‚óè

Fig. 8.

150

Daily Edge Count: 04/26/2011 to 05/24/2011

graphs Gn and Gn

200

1.

Vertex Degree

Fig. 6.

23‚àíMay

‚óè

‚óè

50

09‚àíMay

Date

‚óè

‚óè

‚óè
‚óè

‚óè
‚óè

‚óè

‚óè

‚óè
‚óè

‚óè
‚óè

‚óè

1500

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

9500
‚óè

‚óè
‚óè

‚óè
‚óè

‚óè

2000

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

11000

Edge Count

2500

Daily Flight Count: 04/26/2011 to 05/24/2011

12000

‚óè

1000

23‚àíMay

Date

monotonic decay of the clustering coefficient of the GÃÑ\ is
consistent with behavior reported by others.
The variation of average vertex strength with vertex degree
is given in Figure 6. The growth trend in strength with degree

Average Vertex Strength

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè

38000

34000

‚óè
‚óè
‚óè
‚óè

Fig. 5.

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè‚óè
‚óè

‚óè
‚óè

60

‚óè
‚óè

36000

‚óè
‚óè

0.2

40

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

20

‚óè
‚óè

42000

‚óè
‚óè

0.6

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè ‚óè
‚óè

0

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

44000

‚óè
‚óè

‚óè
‚óè

Flight Count

Average Clustering Coefficient

‚óè

gn,1 = (Gn \ Gn

Variation in the vertex strength with degree

is similar to vertex strength behavior reported in previous
studies. This behavior indicates that airports with larger degree
tend to also on average carry more traffic on their city pair
links.
The temporal dynamics of the US airport network in the
aggregate can be seen by examining the daily variation in flight
count, as shown in Figure 7. We find that flight counts exhibit
a weekly cyclic trend. It is interesting that this periodic trend
is also seen in the edge (Figure 8) and vertex counts over
successive days. These results indicate the growth in traffic
volume during the week is coupled with an increase in the
diversity of cities that host flights.
The results in Figure 8 suggest that there are city pairs
for which flights occur with periodicity less than a day, and
perhaps as infrequently as once in a week, or longer. In fact
there are flights that occured only once over the set of 29 days
examined. In order to quantify the number of new city pairs
visited over successive days, we examine the one lag difference of our graph time series {Gn }. In particular we consider
the complement of the intersection between successive day

1)

C

(4)

The edge count of the complement graphs {gn,1 } for successive lags in the daily time series is given in Figure 9.
This figure shows that the geospatial traffic pattern varies
substantially from day-to-day. Although the range of variation
in the edge count over a week is approximately 3K, the
disparity in city pairs visited between successive days can be
nearly as high as 13K. As a result more than 1/3 of the flights
between two successive days are to a different set of cities. We
have also examined the edge counts for complement graphs
{gn,m }, with m = 7, in an attempt to remove the apparent
seven day sesonality as evident in Figure 8. However, the edge
counts for {gn,7 } were also found to be highly non-stationary
over successive days.
In an attempt to further characterize the dynamics of
the graph time series {Gn } we examine the autocorrelation
function over the edge weights. In order to compute the
autocorrelation we treat GÃÑ[ as a root-graph, for the set of
graphs {Gn }. We then define the set of graphs {GÃÇn } that have
edge and vertex sets identical to GÃÑ[ . The edge weights of Gn
are mapped onto GÃÇn for shared edges, and set to zero for all
other edges. The estimate of the autocorrelation function for

127

2012 Conference on Intelligent Data Understanding

V. S PECTRAL C HARACTERISTICS

‚óè
‚óè

12500

In this section we discuss the spectral characteristics of the
set of adjacency matrices {AÃÇn } associated with the normalized
graphs {GÃÇn }. The eigenvalue magnitude spectrum {| ÀÜ i |} for
a set of consecutive days is given in Figure 11. The structure
of the eigenvalue magnitude spectrum is similar over the
days examined. The eigenvalue spectrum follows a power-law
decay, ÀÜ i ‚á† i , with ‚á° 1.04.

‚óè
‚óè
‚óè
‚óè
‚óè
‚óè

12000

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

Edge Count

‚óè
‚óè

11500

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

11000

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

10500

‚óè
‚óè

‚óè
‚óè

‚óè
‚óè
‚óè
‚óè

‚óè
‚óè

‚óè
‚óè

10000

103

‚óè
‚óè

‚óè
‚óè

25‚àíApr

02‚àíMay

09‚àíMay

16‚àíMay

23‚àíMay

2

10

Fig. 9.

Eigenvalue Magnitude

Date

One Lag Difference Edge Count: 04/26/2011 to 05/19/2011

the weights of each edge is given as:
RwÃÉk ,wÃÉk (n) =

N
X1

1
N

1 m=0

wÃÉk,m wÃÉk,m+n

(5)

The time series wÃÉk,n is obtained by converting wk,n to be
zero-mean and unit variance. The averaged autocorrelation
function over all edges is plotted in Figure 10. The range bars
that bracket the averaged autocorrelation function indicate the
maximum and minimum values of the autocorrelation function
amongst all the edges. The boxes at each lag give the standard
deviation in the autocorrelation values over all edges. The

101
100
10‚àí1
10‚àí2
10‚àí3
10‚àí4

‚óè ‚óè ‚óè‚óè‚óè
‚óè‚óè‚óè‚óè‚óè‚óè‚óè
‚óè‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
Eigenvalue Spectra
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
eval0
‚óè
‚óè
‚óè
‚óè
‚óè
‚óè
eval1
‚óè
‚óè
‚óè
‚óè
‚óè
eval2
‚óè
‚óè
eval3

100

100.5

101

101.5

Fig. 11.

102.5

103

103.5

Eigenvalue Magnitude Spectra For 26 April to 29 April, 2011

The aggregate temporal behavior of the eigenvectors is examined by observing their normalized autocorrelation function
RxÃÇx,xÃÇx (n), which is defined as:

1.0

RxÃÇxj ,xÃÇxj (n) =

N
X1

1
N

1 m=0

0.5

Correlation Variability

102

Index(n)

xTj,mxÃÇ
xj,m+n
xÃÇ

(6)

xj,n is the eigenvector corresponding to the eigenvalue
where xÃÇ
ÀÜ j,n , for the graph GÃÇn . The normalized autocorrelation function of the leading daily eigenvectors is given in Figure 12. The

0.0

‚àí0.5
1.0

‚óè
‚óè
‚óè

5

10

15

20

Normalized auto‚àícorrelation

‚àí1.0
25

lag

Fig. 10.

Averaged autocorrelation function

average and standard deviation of autocorrelation shows the
temporal behavior of flights on the majority of edges in US
airport network to be weakly correlated. The maximum values
of autocorrelation for each lag show the presence of highly
correlated components. These highly correlated components
tend to also carry a high volume of traffic. These results
suggest that standard time series techniques that employ time
series differencing and autocorrelation to construct forecasting
models, are unlikely to yield low dimensional models that
would capture aggregate, as well as fine scale features of traffic
demand. In the next section we examine spectral techniques
that may hold greater promise in development of such low
dimensional models.

‚óè

‚óè

0.8

‚óè

‚óè

‚óè
‚óè
‚óè

‚óè

0.6

‚óè

‚óè

‚óè

0.4
Eigenvectors
eigv0
‚óè
eigv1
eigv2
eigv3

0.2

0.0
0

2

4

6

8

10

12

Lag(n)

Fig. 12. Normalized autocorrelation function of the leading daily eigenvectors

normalized autocorrelation function for the leading eigenvectors shows a temporal character that is quasi-stationary. The
apparent decay in the autocorrelation function with lags is due
to finite sample size effects. The quasi-stationary behavior of
the autocorrelation function suggests that it may be possible

128

2012 Conference on Intelligent Data Understanding

to model the leading order dynamics of the airport graphs by
employing the spectral decomposition of the adjacency matrix.
AÃÇn =

|VÃÇn | 1

X

ÀÜ j,n QÃÇj,n

(7)

j=0

The matrix QÃÇj,n is the spectral projection matrix corresponding to the eigenvalue ÀÜ j,n . Since QÃÇj,n is function of
the eigenvectors, we expect to approximate it‚Äôs behavior as
stationary in modeling the leading order behavior of graphs
GÃÇn . The application of this approximation allows AÃÇn to be
modeled by the matrix MÃÇn
MÃÇn ‚á°

J
X1

ÀÜ j,n QÃÇj,0

(8)

j=0

Residual power as a fraction of graph power

where QÃÇj,0 is the j-th spectral projection matrix of AÃÇ0 ,
ÀÜ j,n is the j-th eigenvalue of AÃÇn , and J is the rank of the
approximation. The solid line in Figure 13 corresponds to
MÃÇ0 , and shows that much of the information in AÃÇ0 can be
approximated by a low-rank model. The dashed line in Figure
1.0
^
M0
^
M2

0.8

0.6

0.4

its edges, is found to vary remarkably from day to day.
The non-stationary components of the network were found
to equalize the distribution of clustering coefficients. We have
also performed a spectral analysis of the U.S. airport network.
The eigenvalue spectrum of the network shows a power law
decay. The structure of the eigenvalue spectrum is similar, even
though the eigenvalues themselves exhibit oscillatory behavior
over successive days. The leading order behavior of the
eigenvectors is found to be quasi-stationary. The observation
has been used to construct low rank models of the the airport
network. This approach can be useful in developing national
airspace wide predictive models for traffic demand.
VII. ACKNOWLEDGMENTS
The authors extend their gratitude for the support provided
by the MIT Lincoln Laboratory Technology Office.
R EFERENCES
[1] Z. Xu and R. Harriss, ‚ÄúExploring the structure of the U.S. intercity passenger air transportation network: a weighted complex network approach,‚Äù
GeoJournal, vol. 73, 2008.
[2] J. Zhang, X. Cao, W. Du and K. Cai, ‚ÄúEvolution of Chinese airport
network,‚Äù Physica A, vol. 389, 2010.
[3] G. Bagler, ‚ÄúAnalysis of the airport network of India as a complex
weighted network,‚Äù Physica A, vol. 387, 2007.
[4] L. da Rocha, ‚ÄúStructural evolution of the brazilian airport network,‚Äù
Journal of Statistical Mechanics: Theory and Experiment, 2009.
[5] R. GuimeraÃÄ, S. Mossa, A. Turtschi and L.A.N. Amaral, ‚ÄúThe worldwide
air transportation network Anomalous centrality community structure, and
cities global roles,‚Äù Proceedings of the National Academy of Sciences,
vol. 102, no. 22, 2005.
[6] Federal Aviation Administration, ‚ÄúTraffic Flow Management System to
Aircraft Situation Display to Industry Interface Control Document for the
Traffic Flow Management Modernization Program,‚Äù 2009.

0.2

0.0
20

40

60

80

100

Model rank

Fig. 13.

Residual power for successively higher rank approximations

13 correspond to MÃÇ2 . Despite using the projection matrix AÃÇ0
to compute the model MÃÇ2 , the model still contains much of the
information of AÃÇ2 . A similar result is found for other days‚Äô
adjacency matrices AÃÇn6=0 . This shows that the eigenvectors
corresponding to the largest eigenvalues of AÃÇn are fairly static.
With a method of modeling ÀÜ j,n we would be able to create a
low-rank approximation of AÃÇn without having to re-calculate
the eigenvectors of AÃÇn . We expect this result to also aid in
developing low dimensional predictive models of the daily
national airspace wide traffic demand.
VI. C ONCLUSIONS AND F UTURE W ORK
In our examination of the U.S. airport network we have attempted to account for all flights that are tracked by the FAA‚Äôs
Traffic Flow Management System. We have found that these
flights have a significant impact on the topological structure of
the airport network. We have also found the temporal dynamics
of the US airport network to be more complex than previously
reported. The structure of the network, as characterized by
129

Parallel Computing 36 (2010) 635‚Äì644

Contents lists available at ScienceDirect

Parallel Computing
journal homepage: www.elsevier.com/locate/parco

Hogs and slackers: Using operations balance in a genetic algorithm
to optimize sparse algebra computation on distributed architectures
Una-May O‚ÄôReilly a,*, Eric Robinson b, Sanjeev Mohindra b, Julie Mullen b, Nadya Bliss b
a
b

Computer Science and ArtiÔ¨Åcial Intelligence Laboratory, Massachuestts Institute of Technology, Cambridge, USA
Lincoln Laboratory, Massachuestts Institute of Technology, Cambridge, USA

a r t i c l e

i n f o

Article history:
Available online 11 August 2010
Keywords:
Sparse matrix
Genetic algorithm
Simulation-based optimization

a b s t r a c t
We present a framework for optimizing the distributed performance of sparse matrix computations. These computations are optimally parallelized by distributing their operations
across processors in a subtly uneven balance. Because the optimal balance point depends
on the non-zero patterns in the data, the algorithm, and the underlying hardware architecture, it is difÔ¨Åcult to determine. The Hogs and Slackers genetic algorithm (GA) identiÔ¨Åes processors with many operations ‚Äì hogs, and processors with few operations ‚Äì slackers. Its
intelligent operation-balancing mutation operator swaps data blocks between hogs and
slackers to explore new balance points. We show that this operator is integral to the performance of the genetic algorithm and use the framework to conduct an architecture study
that varies network speciÔ¨Åcations. The Hogs and Slackers GA is itself a parallel algorithm
with near linear speedup on a large computing cluster.
√ì 2010 Elsevier B.V. All rights reserved.

1. Introduction
High performance embedded computing is transitioning from solely serving the domain of signal processing to additionally serving the domains of knowledge extraction and decision support. In many decision support applications, graphs play a
role as important information elements. For example, graphs are used when examining social networks or determining sensor net coverage and can also represent Bayesian networks that are used to fuse images. For these, general graph algorithms
such as betweenness centrality, Bayesian belief propagation, minimal spanning trees, and single source shortest path are
commonly needed.
Previous work shows it is efÔ¨Åcient to cast graph algorithms as a series of linear algebra operations [16,24]. With this casting, a graph is represented with a two-dimensional adjacency matrix which is indexed by node identiÔ¨Åers and Ô¨Ålled with a
non-zero entry wherever there is a link between the nodes corresponding to the column and row indices. Common operations on graphs can be implemented to take advantage of this representation. For example, in a breadth-Ô¨Årst edge traversal,
each successive time an adjacency matrix is multiplied by itself, a non-zero entry indicates a path between the nodes corresponding to the indices of length equal to the current number of multiplications.
In addition to the rigour and precision of its mathematical formalism, the linear algebra translation also supports a
straightforward means of parallelization through the distribution of blocks of the matrix elements across processors of
the distributed architecture, or mapping. As a Ô¨Årst order approximation, the efÔ¨Åciency of a distributed matrix computation

* Corresponding author. Tel.: +1 6172536437; fax: +1 6172588682.
E-mail addresses: unamay@csail.mit.edu (U.-M. O‚ÄôReilly), erobinson@ll.mit.edu (E. Robinson), smohindra@ll.mit.edu (S. Mohindra), jmullen@ll.mit.edu
(J. Mullen), nt@ll.mit.edu (N. Bliss).
0167-8191/$ - see front matter √ì 2010 Elsevier B.V. All rights reserved.
doi:10.1016/j.parco.2010.08.001

636

U.-M. O‚ÄôReilly et al. / Parallel Computing 36 (2010) 635‚Äì644

depends on how its operands, the matrices, are mapped. Mappings such as those shown in Fig. 1 have been established as
practical and efÔ¨Åcient for dense matrix computations.
However, while signal processing data is typically dense, knowledge extraction and decision support frequently involves
sparse matrices where the number of edges is a small multiple of the graph size. Unfortunately, challenges arise when computing with this data. Fig. 2(a) shows that dense computation is signiÔ¨Åcantly more efÔ¨Åcient than sparse computation. For
computations of a reasonable size, a difference of approximately 103 is observed. Fig. 2(b) shows that sparse parallel performance gains typically cannot be realized beyond 30 processors [28]. Mappings have been established as practical and efÔ¨Åcient for dense matrix computations. Unfortunately, the best sparse performance results typically fall well below
performance targets for many decision support applications.
To achieve performance goals, a number of approaches can be taken. While hardware and hardware-software co-design
approaches are powerful, they require too long a lead time to address existing hardware, commonly with 10‚Äôs to 100‚Äôs of
processing nodes. In the short term, software optimization, which does not have these weaknesses, holds the most promise.
However, software optimization tools should also be versatile and general enough to be used on future hardware models.
This allows for short term gains that can also contribute to long term successes.
In a typical dense computation, an atlas of maps can be stored and loaded at runtime based on the application [27]. This is
not sufÔ¨Åcient for sparse computation, where conventional maps are not sufÔ¨Åcient and mapping is substantially harder [28].
For sparse matrices, the main performance bottleneck is irregular data access. Finding a good map to minimize irregularity
involves taking into account three factors: (1) the location of the non-zero data in a matrix, (2) the algorithm implementing
the matrix operation, dictating data access patterns, and (3) the parameters of the underlying hardware, dictating the cost of
those movements as well as computations.
Studying just one of these factors is difÔ¨Åcult. For example, there are multiple sparse data patterns that commonly arise
such as power law, toroidal, or random. The difÔ¨Åculty of understanding all of their interactions in order to determine an efÔ¨Åcient map is very high despite knowing, in general, that some ideal balance of the computation across the processors is
desired.
In order to automatically evaluate and optimize maps for a speciÔ¨Åc sparse matrix operation, we have designed and implemented the MORE (Mapping and Optimization Runtime Environment) framework. At a high level, this framework translates
user matrix-based algebra code into a parse tree and variably speciÔ¨Åes maps. The parse tree and maps are then used to produce a dependency graph with nodes that correspond to all the memory, network and compute hardware operations and
their execution order dependence on one another. The dependency graph and routing choices are inputs into a simulator
that references a model of a distributed computation architecture. The simulator provides a time the computation would
take to execute on the modeled architecture. This execution time is a performance score for the maps.
An optimization component within MORE consists of the Hogs and Slackers GA. The goal of the genetic algorithm is to
search for maps that result in the best performance of the code. When enabled, the genetic algorithm ‚Äò‚Äòwraps around‚Äù the

Fig. 1. Conventional block mappings for dense matrices.

Fig. 2. Challenges of sparse graph computation.

U.-M. O‚ÄôReilly et al. / Parallel Computing 36 (2010) 635‚Äì644

637

dependency and simulator modules to repeatedly use them to test the performance score of the maps that the genetic algorithm encodes as a genome. Its genome representation is straightforward assignments of processor to block. The output of
the genetic algorithm optimization is the best set of maps along with the associated performance score.
It has long been recognized [11] that practitioners want to bring all available knowledge to bear in solving an optimization problem. In our domain, we understand the source of the performance gap ‚Äì irregular data access. Thus, the Hogs and
Slackers GA incorporates the insight that the search must Ô¨Ånd an ideal parallelization that suitably balances the entire computation across processors. The Hogs and Slackers GA references the computation‚Äôs distribution of memory operations across
processors in order to guide a genetic mutation operator, BALANCINGMU, to intelligently choose processors for block exchange. The BALANCINGMU operator ranks all the processors by either the number or cumulative size of their memory operations. Highly ranked processors are ‚Äò‚Äòhogs‚Äù and lowly ranked ones, ‚Äò‚Äòslackers‚Äù. The mutation operator trades blocks assigned
to hogs with those assigned to slackers. When tested against a naive block mutation operator, the BALANCINGMU operator is
superior. We demonstrate this advantage and then use the Hogs and Slackers GA to study a range of three hardware models
with parameterized network bandwidth. The study shows that, on different architectures, the genetic algorithm provides
consistent and signiÔ¨Åcant optimization over conventional maps.
This paper is divided into the following sections: Section 2 describes the MORE framework for studying and optimizing a
sparse matrix computations. Section 3 provides a complete description of the Hogs and Slackers GA. Section 4 is experimental
results related to the knowledge-based BALANCINGMU operator and a study of the optimized performance of a sparse matrix multiply kernel on a range of architectures. Section 5 covers related work. Section 6 concludes and describes future work.
2. The MORE framework
The MORE (Mapping and Optimization Runtime Environment) framework measures and improves the performance of a
distributed sparse algebra computation. The framework is implemented in pMatlab [2] and is supported by the pMapper
automatic matrix mapping project [27], both developed at Massachusetts Institute of Technology Lincoln Laboratory. The
framework, see Fig. 3, has four principle components: a code analyzer, a mapper, an operations analyzer and a performance
simulator.
2.1. The program analyzer
The program analysis component converts the user code into a parse tree, T, to be analyzed. It uses a lazy evaluation strategy, not sending code for analysis until it is required by the user. This allows for the analysis to take in as much of the context
of the code as possible and can lead to additional optimizations.
2.2. The mapper
The mapping component produces a set of variable maps, M, for the user code. It facilitates better sparse matrix performance by producing Ô¨Åne-grained maps that allow for much smaller blocks of data than for the dense mapping case. It also
allows any processor to be assigned to any block because map regularity can lead to operation imbalance and poor performance. Maps are speciÔ¨Åed using PITFALLS, or Processor Indexed Tagged FAmiLy of Line Segments [22].
2.3. The operations analyzer
The operations analysis component performs Ô¨Åne-grained dependency analysis. It takes a set of variable maps and a parse
tree and constructs a dependency graph, or analysis of hardware operations and their dependence on one another induced by
the algorithm and the data.

Fig. 3. The Ô¨Çow and components of the MORE framework.

638

U.-M. O‚ÄôReilly et al. / Parallel Computing 36 (2010) 635‚Äì644

2.4. Performance Simulator
The performance simulator determines the performance of the program on the intended hardware. It takes a dependency
graph of the program and network routes, R, along with a model of the hardware, H, and computes the time required to run
the operations speciÔ¨Åed by the dependency graph on that hardware. Note that this allows one to easily interchange the hardware model and keep in place the remaining structure, providing easy transition to analysis of future hardware designs.
For the purposes of this paper, we consider a topological simulator. In a topological simulator, the dependency graph is
sorted topologically based on the dependences. This organizes the nodes in the graph into distinct levels. The simulation
time for each level, L, can then be evaluated, where the operations within L may be run concurrently according to H. The
total simulation time is the sum of the simulation time for all L.
3. The Hogs and Slackers GA of the MORE optimization module
MORE uses a genetic algorithm because it is simulation-based and little is known about the nature of the solution space:
whether it is Ô¨Çat in terms of mapping options, rugged or multi-modal. The goal of the genetic algorithm is to return a set of
maps such that the simulated execution time of the related dependency graph is minimized.
Formally, given the parse tree, T, and the hardware model, H, the genetic algorithm identiÔ¨Åes a set of maps, M with corresponding set of routes, R, such that the following objective function, f is satisÔ¨Åed:

argminM;R f √∞T; H; M; R√û

√∞1√û

The function f returns a duration of execution. Evaluation of f is performed by simulating the dependency graph of the code
using the hardware model H. We report optimization results in operations per second (OP/s) by dividing the operations executed by the duration of execution. The minimum size of search space of maps is:
√∞B√û

SM ¬º N P

√∞2√û

where
 NP = number of processing nodes
P M
 B = number of blocks, which is equal to Ni¬º1
Bi where NM is the number of matrices or arrays in T and Bi is the number of
blocks in matrix i.

Fig. 4. Genetic algorithm overview.

U.-M. O‚ÄôReilly et al. / Parallel Computing 36 (2010) 635‚Äì644

639

The genetic algorithm, see Fig. 4(a), runs for a speciÔ¨Åed number of generations. When invoked, it receives the matrix operands of the sparse algebra operator. It starts by creating a random initial population of candidate genomes. Each genome
encodes a map for each operand (see Fig. 4(b)) and can optionally encode a map for the result. Each map is Ô¨Årst set up with
a grid which is determined randomly from pre-speciÔ¨Åed options such as the minimum number of blocks per row or column
or some speciÔ¨Åc set of grids. The grid dimensions are linked to the genome for reference. Then, for each grid, processors are
assigned in approximately equal numbers to blocks.
To obtain the Ô¨Åtness score, the genome is passed to the operations analysis module which generates the Ô¨Åne-grained
dependency graph (abbreviated as DG in Fig. 4(a)) of the computation using the maps and parse tree. The dependency graph
is then input to the simulator, with a greedy route selection policy, which returns the execution time.
After Ô¨Åtness evaluation of the population, the genetic algorithm uses tournament selection, with replacement, to select
parents. A fraction of parents undergo both crossover and mutation. Another fraction undergo only mutation. Elitism propagates a small number of current best solutions, usually one or two, without any genetic variation. Upon completion, the
genetic algorithm returns the maps of the Ô¨Åttest genome and its execution time.
When route selection is to be co-optimized with mapping, an inner genetic algorithm is triggered after the dependency
graph has been generated. All possible routes for the given graph are determined and the inner genetic algorithm does a
smaller scale search of them to return the best routes and execution time of the genome (and dependency graph and route
set). In this contribution, route selection is done greedily. This allows us to focus upon the impact of exploiting problem
knowledge. As a result, the inner genetic algorithm is not invoked. For results on the nested genetic algorithm see [28].
The genetic algorithm can also optimize multiple objectives though, in this contribution, the capability is not used. See
[20] for more details.
3.1. Parallel implementation
Another reason we chose a genetic algorithm is that it is well suited to parallelization. In our parallel implementation,
Ô¨Åtness evaluation of the population is divided among nodes of a cluster each generation. Each ‚Äò‚Äòworker‚Äù node receives a slice
of the population. For each genome, it computes the dependency graph and runs the simulation. It stores the Ô¨Åtness scores of
its sub-population locally. Upon completion of the entire sub-population‚Äôs evaluation, it sends the results to the master node
where they are aggregated to reassemble a population Ô¨Åtness array. This requires minimal code changes using pMatlab ‚Äì
about 20 lines of code or 1% of the code base. Our implementation is executed on the Lincoln Laboratory computing cluster
named LLGrid [23] and achieves near linear speedup. The time to pass each worker‚Äôs Ô¨Åtness results across the network is
signiÔ¨Åcantly lower than the time to evaluate the Ô¨Åtness of one individual.
3.2. Genetic representation and naive operators
Fig. 4(b) shows that the genome is a linear vector of processor identiÔ¨Åers, one per block. When the grid of a map is allowed to vary across genomes in the population, genomes have different numbers of blocks and vary in length.
The BLOCKXO operator is a uniform crossover operator. It aligns the blocks of the parents‚Äô genome and, at each block,
probabilistically determines whether to swap the block between the parents. BLOCKXO is only permitted on maps with
the same number of blocks because of the philosophical intent to exchange alleles of a gene. For more macroscopic recombination, MAPXO probabilistically exchanges the maps of a matrix operand between the parents.
The ‚Äò‚Äònaive‚Äù RANDMU operator probabilistically changes the processor assignment of a block to a random processor.
3.3. Operations balancing mutation: BALANCINGMU
We now discuss the design of knowledge-based BALANCINGMU. Recall the basic hypothesis: the genetic algorithm will
beneÔ¨Åt from explicitly varying the computational balance as a means of exploring new maps. In the MORE framework, the
available surrogate for computational balance is the ‚Äò‚Äòeven-ness‚Äù or ‚Äò‚Äòbalance‚Äù of operations across processors. These operations (in three types ‚Äì CPU, memory or network) are countable from the dependency graph because each of its nodes corresponds to a hardware operation and is annotated with its type, the operation payload and what processor executes the
operation. The operations balance measure expresses a normalized variation in the distribution of operations (of an operation type) across processors. The balance of a set of maps‚Äô memory operations is:

bal:memops ¬º

stdev √∞opcounts:memops√û
mean√∞opscounts:memops√û

√∞3√û

CPU or network operations can be substituted for memory operations to similarly derive CPU or network operations balance.
To test whether the measure is adequately reÔ¨Çective of computational balance, we conÔ¨Årmed there are differences in the
CPU, memory and network operations balances in an optimized map when the pattern of sparsity in the matrices and the
speciÔ¨Åc matrix multiply kernel algorithm are varied. Table 1 shows balances in a random and optimized map when two different matrix types (powerlaw and scrambled powerlaw) and two different matrix multiply implementations ‚Äì hybrid (see
Section 4.1 for details) and inner product [12], are paired for map optimization.

640

U.-M. O‚ÄôReilly et al. / Parallel Computing 36 (2010) 635‚Äì644

Table 1
Balance of CPU, memory and network operations for matrix operands with different sparsity patterns.
Pattern

MatMult

Random (CPU, MEM, NW)

Optimized (CPU, MEM, NW)

Scrambled power law
Scrambled power law
Power law
Power law

Inner
Hybrid
Inner
Hybrid

(0.34, 0.16, 0.17)
(0.38, 0.23, 0.26)
(0.91, 0.42, 0.46)
(0.91, 0.55, 0.62)

(0.09, 0.05, 0.05)
(0.45, 0.19, 0.29)
(0.70, 0.30, 0.28)
(0.99, 0.52, 0.68)

Fig. 5. CPU, memory and network operation counts.

Fig. 5(a) and (b) shows counts of all three operation types for a random (from generation 1) and best of run map set with
scrambled power law matrices and the hybrid algorithm. In the initial generation despite blocks being distributed evenly
across processors, the operations are quite imbalanced because of the matrix sparsity pattern and the matrix multiply algorithm. This imbalance diminishes in the best individual but, as one would also expect, due to the complex interactions
among the network, memory and CPU computations, perfectly even balance is not actually ideal.
Next we conÔ¨Årmed that, despite indirection, operation redistribution can be controlled by changing the processor assigned to an entire block of the matrix. We tested a simpliÔ¨Åed intelligent operations balancing operator, called MICRO_BALANCING_MU that reassigns only a single block between one of the processors with the highest number of memory
operations (the ‚Äò‚Äòhog‚Äù) and one of the processors with the lowest number of memory operations (the ‚Äò‚Äòslacker‚Äù). On maps
sequestered from the late interval of different genetic algorithm runs, this reassignment frequently resulted in a more even
(lower) operations balance. Accurate control was more frequent when consulting memory or CPU operations counts: 95% to
99% of changes changed balance in the appropriate direction versus network operations counts (85%). When done in the
opposite direction (block reassignment from slacker to hog), the operations balances also consistently became less even
(i.e. higher). Concurrently we determined that MICRO_BALANCING_MU frequently generates offspring that are better than
the parent (roughly 70% of the time). These results are better than the naive operators: BLOCKXO, MAPXO and RANDMU
whose success frequency rates lie around 10% to 20%.

Fig. 6. BALANCINGMU inputs.

641

U.-M. O‚ÄôReilly et al. / Parallel Computing 36 (2010) 635‚Äì644

Fig. 7. Pseudocode of BALANCINGMU.

Given these analyses, we design BALANCINGMU, a more general version of MICRO_BALANCING_MU. The operator‚Äôs inputs and pseudocode are shown in Figs. 6 and 7. BALANCINGMU is passed the sorted memory operations counts corresponding to the operation nodes of the dependency graph. It selects as a set of hogs or slackers 1/6 of the total number of
processors from the upper or lower extremes, respectively. These processors are those whose operation counts are greater
or less than approximately one standard deviation from the mean. From among each set, with uniform probability, a group of
their blocks is selected to meet a quota. These groups of blocks are then swapped between hogs and slackers.

4. Experimental results
4.1. Experimental setup
In this contribution, the kernel selected for optimization is matrix multiply. The codelet is shown in Table 2 (left). The
genetic algorithm maps the operands A and B. The result C is assigned the map of A. Both A and B have either a powerlaw
or scrambled powerlaw distribution with a non-zero element density of 8/N where N = 1024 is the number of nodes in
the graph (the dimensions of the matrices). We use an R-MAT power law generator [5] to produce A and B. The implementation of ‚Äò‚ÄòmatrixMult‚Äù is a hybrid inner-outer product algorithm. It alleviates some of the communication load associated
with an outer product algorithm and handles distributed sparse data better than Strassen matrix multiply. Similar to an outer product, the algorithm sends A‚Äôs entries to B‚Äôs column owners as their row position requires. However, rather than producing a local sum, the products generated are sent immediately to C, as with an inner product, and Ô¨Ånal values are summed
once all of the products are gathered.
Because of our need for computational efÔ¨Åciency and fast optimization, we run the genetic algorithm for only 50 generations with a population size of 100 which amounts to searching a very tiny fraction of the search space. The tournament size
is set to 5. Elitism preserves the two best genomes each generation. The genetic algorithm has 9 different grids it can randomly assign to a map when it initializes the population. Thus the number of possible combinations of grids for 2 matrices is
81. Each grid has 256 blocks. See Table 2 (right) for a list of them. To select operator probabilities we explored ranges via
experimental design. We decrease the probability of MAPXO from 0.125 to 0.0 over the Ô¨Årst half of the run to foster large
exploration steps early. We increase the probability of BLOCKXO from 0.25 to 0.75 over the run with the likelihood of a block
crossover decreasing from 10% to 1% to foster Ô¨Åner-grained exploration steps later on fewer blocks. We apply RANDMU with
0.75 probability throughout the run on each genome while the likelihood of a block being reassigned linearly decreases from
1.0 to zero. We apply BALANCINGMU frequently at the beginning of a run (selecting a matrix map with 0.75 probability) then
linearly decrease its application (to 0.25 probability) over the course of the run. Over the same interval, we simultaneously
decrease the quota of blocks swapped from 10% of the maximum blocks in the grid to 1%.

Table 2
Optimized codelet (left), grid dimensions (right).
Possible grids
A = rand (N, N, p);
B = rand (N, N, p);
C = zeros (N, N, p);
C = matrixMult (A, B);
eval (C);

1  256
2  128
4  64
8  32
16  16

256  1
128  2
64  4
32  8

642

U.-M. O‚ÄôReilly et al. / Parallel Computing 36 (2010) 635‚Äì644

Table 3
Comparison of BALANCINGMU to using RANDMU or conventional Anti-Diagonal Block Cyclic (ADBC) maps, then RANDSWAP added.

ADBC
RANDMU
BALANCINGMU
BALANCINGMU + RANDSWAP

Mean (SD) best of run ( OP/s)

Best of runs (OP/s)

Relative to ADBC

3.17E+09
8.06E+09
9.47E+09
9.56E+09

3.71E+09
8.68E+09
9.78E+09
9.92E+09

1.0
2.54X
2.99X
3.01X

(6.39E+08)
(3.23E+08)
(1.70E+08)
(1.45E+08)

Fig. 8. Left: Balance over a run. Right: Baseline hardware model.

4.2. Evaluation of BALANCINGMU
To evaluate BALANCINGMU, we compare it (see Table 3) to using RANDMU or conventional Anti-Diagonal Block Cyclic
(ADBC) maps (which provide the best advantage in dense computation). We execute 50 runs (each with different matrix data
but same kind of non-zero pattern and density) and consider their best maps in terms of OP/s (higher is better). Both mutation operators are teamed with the crossover operators. The mean performance of BALANCINGMU‚Äôs best maps is approximately 3 times better than ADBC. Compared to RANDMU, the mean performance is statistically signiÔ¨Åcantly better (ttest, p = 5.5e-38) with the best of all runs providing a 13% improvement (8.678e9 vs 9.778e9 OP/s). To see whether random
swapping would further improve the smart swapping of BALANCINGMU, we add in a RANDSWAP operator that swaps a set
of blocks chosen at random. We keep the total applications of both swap operators and their blocks quotas equal to values
when BALANCINGMU is used alone. The unpaired t-test conÔ¨Årms there is a statistically signiÔ¨Åcant beneÔ¨Åt to adding the random swap (p = 0.0035) though the best result is, on average, only 1% better.
Using the run which provided the best maps over 50 runs for ADBC, BALANCINGMU, and RANDMU respectively, we show
the memory operations balance of the best individual each generation in Fig. 8 left. While ADBC generates an evenly balanced map in terms of memory operations counts, the ideal distribution of counts (per the optimized map of the BALANCINGMU run) is slightly more uneven. If the balance is too uneven, like the maps of the RANDMU run, performance is not as
good. In general, our Ô¨Ånding is that BALANCINGMU‚Äôs behavior is consistent with the goals of its design which are to significantly boost the efÔ¨Åciency of the genetic algorithm.

4.3. Hardware model parametric study
We next demonstrate how the MORE framework enables comparative investigation of sparse matrix computation on different architectures. In this study we vary network rate. The baseline model (Fig. 8, right) corresponds to a nominal Cray-like
machine with a 4  4  4 torus topology. Like the Cray, it has very high network provisioning. We create two variations of it:
Ô¨Çooding the system even more with 10X bandwidth capacity and restricting the bandwidth by 101X.

Table 4
Comparison of network rate parameterized hardware model.
Model

Mean of runs absolute OP/s

Relative

Best of runs absolute OP/s

Relative

Improvement over ADBC

TenthX
Nominal
TenX

4.998E+09
9.471E+09
9.335E+09

0.53
1
0.99

5.575E+09
9.777E+09
1.023E+10

0.57
1
1.05

2.4X
3.0X
2.7X

U.-M. O‚ÄôReilly et al. / Parallel Computing 36 (2010) 635‚Äì644

643

When the hardware model is constrained (row ‚Äò‚ÄòTenthX‚Äù of Table 4), the best of runs solution reaches 5.575e + 09 OP/s
which is only approximately 0.57 times the rate when the model is nominally conÔ¨Ågured. This is still over 2 times better
than an ADBC map. The lower improvement ratio (2.4:1 vs 3.0:1) versus the nominal model reÔ¨Çects the difference between
the optimized balance point and balance point of the ADBC map (for this sort of matrix, matrix operation, implementation of
the operation). Across the 50 runs, the mean best solution is 0.53 times less efÔ¨Åcient than solutions for the nominal model. As
expected, we do not observe anywhere near the same relative differences between nominal and when the network capacity
is further ‚Äò‚ÄòÔ¨Çooded‚Äù, i.e. ‚Äò‚ÄòTenX‚Äù, because the nominal model already has a relatively high network rate. The differences are in
fact statistically signiÔ¨Åcant but smaller. There is larger variance, but a lower mean, in the outcome of in the TenX runs (mean
best solution = 9.335E+09 OP/s with SD = 4.40e08) while the best of runs is better: 1.02e10 vs 9.78e10 OP/s. Note that in all
of the models the Hogs and Slackers GA provides faster performing maps than the conventional ADBC mapping.
5. Related work
Computational linear algebra has long considered parallel and distributed sparse matrix-dense vector multiplication, as
discussed in [7,18]. More recently sparse matrix-sparse matrix multiplication, such as in [3,4], has been examined and the
old, and previously inefÔ¨Åcient, idea of representing a graph as an adjacency matrix has become plausible. Before, most of the
work using an adjacency matrix representation revolved around formalizing proofs about graph computations as opposed to
performing the computations themselves, e.g. [8,26,30]. In contrast, recent work has focused on using mathematical software and algebraic operations to solve various graph problems, e.g. [21,31,6,10,16]. Our contribution is the Ô¨Årst to optimize
sparse matrix-sparse matrix algebra via a genetic algorithm. The idea of using a genetic algorithm to optimize a distributed
algorithm traces back (at least) to Talbi and Muntean [25] who compared it to hill climbing and simulated annealing for a
mapping problem deÔ¨Åned as ‚Äò‚Äòthe optimal static allocation of communication processes on distributed memory architectures‚Äù. Subsequently other work has widened this deÔ¨Ånition of mapping to include task allocation [15,1,14,17], while also
considering speciÔ¨Åc problems as varied as training set parallelism, [9], and query ordering for sequence analysis, [29]. Lin
[19] uses a GA for distributed sparse matrix Cholesky factoring. The contribution also includes a tree rotate mutation operator that references auxilliary knowledge. A recent example of exploiting domain speciÔ¨Åc intelligent genetic operators (with
complementary genome representation) comes from the single vehicle pickup and delivery problem with time windows in
[13].
6. Conclusions and future work
Our results contribute to the study of genetic algorithms as well as optimization of distributed computations. They demonstrate again that knowledge gives a genetic algorithm an advantage. BALANCINGMU is a means by which the genetic algorithm can consider information on one representation level ‚Äì operations balance, and make changes on another (processor
and block assignment) with an effective improvement in performance.
With respect to the present challenge of optimizing a parallelized computation on increasingly complex architectures
with increasingly complex consideration, the MORE framework serves as a concrete demonstration of a general methodology
that will be more frequently necessary in high performance embedded computing. We plan to expand the optimization beyond that of one kernel to support entire graph algorithms. Our plan is to use its multiobjective genetic algorithm to examine
power-performance trade offs. This current work focuses on distributed memory models, in the future we will also consider
shared memory models.
Acknowledgement
This work is sponsored by the Department of the Air Force under Air Force contract FA8721-05-C-0002. Opinions, interpretations, conclusions and recommendations are those of the authors and are not necessarily endorsed by the United States
Government.
References
[1] A. Biriukov, D. Ulyanov, Simulation of parallel time-critical programs with the dynamo system, in: Proceedings of the Third IEEE Conference on Control
Applications, IEEE Computer Society, 1994, pp. 825‚Äì829.
[2] N. Bliss, J. Kepner, pMatlab Parallel MATLAB Library, Int. J.High Perform. Comput. Appl. (IJHPCA) 21 (3) (2007) 336‚Äì359 (Special Issue on HighProductivity Programming Languages and Models).
[3] A. Bulu√ß, J.R. Gilbert, Challenges and advances in parallel sparse matrix-matrix multiplication, in: The 37th International Conference on Parallel
Processing (ICPP‚Äô08), IEEE Computer Society, 2008, pp. 503‚Äì510.
[4] A. Bulu√ß, J.R. Gilbert, New ideas in sparse matrix-matrix multiplication, in: J. Kepner, J.R. Gilbert (Eds.), Graph Algorithms in the Language of Linear
Algebra, SIAM Press, 2008.
[5] D. Chakrabarti, Y. Zhan, C. Faloutsos, R-mat: a recursive model for graph mining, in: SIAM Data Mining (SDM04), 2004.
[6] P. D‚ÄôAlberto, A. Nicolau, R-kleene: a high-performance divide-and-conquer algorithm for the all-pair shortest path for densely connected networks,
Algorithmica 47 (2) (2007) 203‚Äì213.
[7] S. Filippone, M. Colajanni, Psblas: a library for parallel linear algebra computation on sparse matrices, ACM Trans. Math. Softw. 26 (4) (2000) 527‚Äì550.
[8] R.W. Floyd, Algorithm 97: shortest path, Commun. ACM 5 (6) (1962) 345.

644

U.-M. O‚ÄôReilly et al. / Parallel Computing 36 (2010) 635‚Äì644

[9] S.K. Foo, P. Saratchandran, N. Sundararajan, Genetic algorithm based pattern allocation schemes for training set parallelism in backpropagation neural
networks, in: IEEE International Conference on Evolutionary Computation, IEEE Computer Society, 1995, pp. 545‚Äì550.
[10] J.R. Gilbert, S. Reinhardt, V.B. Shah, A uniÔ¨Åed framework for numerical and combinatorial computing, Comput. Sci. Eng. 10 (2) (2008) 20‚Äì25.
[11] J.J. Grefenstette, Incorporating problem-speciÔ¨Åc knowledge into genetic algorithms, in: L. Davis (Ed.), Genetic Algorithms and Simulated Annealing,
Morgan Kaufmann, 1987, pp. 42‚Äì60 (Chapter 4).
[12] R.A. Horn, C.R. Johnson, Matrix Analysis, Cambridge University Press, Cambridge, UK, 1985.
[13] M.I. Hosny, C.L. Mumford, Single vehicle pickup and delivery with time windows: made to measure genetic encoding and operators, in: Proceedings of
the 2007 Genetic and Evolutionary Computation Conference, GECCO ‚Äô07, New York, NY, USA, ACM, New York, NY, USA, 2007, pp. 2489‚Äì2496.
[14] A. Jose, An approach to mapping parallel programs on hypercube multiprocessors, in: Proceedings of the Seventh Euromicro Workshop on Parallel and
Distributed Processing, PDP ‚Äô99, 1999, pp. 221‚Äì225.
[15] T. Kalinowski, Solving the mapping problem with a genetic algorithm on the maspar-1, in: Proceedings of the First International Conference on
Massively Parallel Computing Systems, IEEE Computer Society, 1994, pp. 370‚Äì374.
[16] J. Kepner, N. Bliss, E. Robinson, Linear algebraic graph algorithms for back end processing, in: Proceedings of Workshop on High Performance
Embedded Computing, HPEC ‚Äô08, 2008.
[17] Y.-K. Kwok, I. Ahmad, Static scheduling algorithms for allocating directed task graphs to multiprocessors, ACM Comput. Surv. 31 (4) (1999) 406‚Äì471.
[18] S. Lee, R. Eigenmann, Adaptive runtime tuning of parallel sparse matrix-vector multiplication on distributed memory systems, in: ICS ‚Äô08: Proceedings
of the 22nd Annual International Conference on Supercomputing, ACM, New York, NY, USA, 2008, pp. 195‚Äì204.
[19] W.-Y. Lin, Parallel sparse matrix ordering: quality improvement using genetic algorithms, in: Proceedings of the 1999 Congress on Evolutionary
Computation, CEC‚Äô99, IEEE Computer Society, 1999, pp. 2295‚Äì2301.
[20] U. O‚ÄôReilly, N. Bliss, S. Mohindra, J. Mullen, E. Robinson, Multi-objective optimization of sparse array computations, in: Proceedings of Workshop on
High Performance Embedded Computing, HPEC ‚Äô09, 2009.
[21] M.O. Rabin, V.V. Vazirani, Maximum matchings in general graphs through randomization, J. Algorithms 10 (4) (1989) 557‚Äì567.
[22] S. Ramaswamy, P. Banerjee, Automatic generation of efÔ¨Åcient array redistribution routines for distributed memory multicomputers, in: Proceedings of
the Fifth Symposium on the Frontiers of Massively Parallel Computation (Frontiers +95), IEEE Computer Society, 1995.
[23] A. Reuther, J. Kepner, A. McCabe, J. Mullen, N. Bliss, H. Kim, Technical challenges of supporting interactive HPC, in: HPCMP Users Group Conference,
IEEE Computer Society, 2007, pp. 403‚Äì409.
[24] E. Robinson, 2008, Array based betweenness centrality, in: SIAM Conference on Parallel Processing for ScientiÔ¨Åc Computing.
[25] E.-G. Talbi, T. Muntean, Hill-climbing, simulated annealing and genetic algorithms: a comparative study and application to the mapping problem, in:
Proceeding of the 26th Hawaii International Conference on System Sciences, IEEE Computer Society, 1993, pp. 565‚Äì573.
[26] R.E. Tarjan, A uniÔ¨Åed approach to path problems, J. ACM 28 (3) (1981) 577‚Äì593.
[27] N. Travinin, H. Hoffman, R. Bond, H. Chan, J. Kepner, E. Wong, pMapper: Automatic mapping of parallel matlab programs, in: HPCMP Users Group
Conference, IEEE Computer Society, 2005, pp. 254‚Äì261.
[28] N. Travinin Bliss, S. Mohindra, U. O‚ÄôReilly, Performance modeling and mapping of sparse computations, in: HPCMP Users Group Conference, IEEE
Computer Society, 2008, pp. 448‚Äì456.
[29] K. Xiong, S. Suh, M. Yang, J. Yang, H. Arabnia, Next generation sequence analysis using genetic algorithms on multi-core technology, in: International
Joint Conference on Bioinformatics, Systems Biology and Intelligent Computing, (IJCBS ‚Äô09), 2009, pp. 190‚Äì191.
[30] A. Yoo, E. Chow, K. Henderson, W. McLendon, B. Hendrickson, U. Catalyurek, A scalable distributed parallel breadth-Ô¨Årst search algorithm on BlueGene/
L, in: Proceedings of the 2005 ACM/IEEE Conference on Supercomputing, SC‚Äô05, IEEE Computer Society, Washington, DC, USA, 2005, p. 25.
[31] R. Yuster, U. Zwick, Detecting short directed cycles using rectangular matrix multiplication and dynamic programming, in: Proceedings of the 15th
Annual ACM-SIAM Symposium on Discrete Algorithms, SODA‚Äô04, Society for Industrial and Applied Mathematics, Philadelphia, PA, USA, 2004, pp. 254‚Äì
260.

Construction and exploitation of a 3D model from 2D image features
Karl Ni1 , Zachary Sun12 ,Nadya Bliss1 , and Noah Snavely3
1 MIT

- Lincoln Laboratory, 244 Wood Street, Lexington, MA, USA;
University, One Silber Way, Boston, MA, USA;
3 Cornell University, Department of Computer Science, Ithaca, NY, USA
2 Boston

ABSTRACT
This paper proposes a trainable computer vision approach for visual object registration relative to a collection of training
images obtained a priori. The algorithm Ô¨Årst identiÔ¨Åes whether or not the image belongs to the scene location, and should
it belong, it will identify objects of interest within the image and geo-register them. To accomplish this task, the processing
chain relies on 3-D structure derived from motion to represent feature locations in a proposed model. Using current stateof-the-art algorithms, detected objects are extracted and their two-dimensional sizes in pixel quantities are converted into
relative 3-D real-world coordinates using scene information, homography, and camera geometry. Locations can then be
given with distance alignment information. The tasks can be accomplished in an efÔ¨Åcient manner. Finally, algorithmic
evaluation is presented with receiver operating characteristics, computational analysis, and registration errors in physical
distances.
Keywords: Structure from Motion, Object Detection, Registration, Bundle Adjustment

1. INTRODUCTION
With increased web usage over a more diverse demographic, online media availability in the United States has risen 40%
in the past year alone. (See the Nielsen Media Wire report, Sept 2, 2009). As part of the media explosion, the quantity
of images has grown and become increasingly accessible due to digital photo posting websites. The accessibility has
promoted opportunities for exploitation due to inherent image information now readily available.
Among the applications that have begun to realize such potential, automated tagging algorithms in photo storage sites
are currently implemented (See Google‚Äôs latest PicasaTM technology). Tagging humans is just one example of the common
computer vision problem of image labeling. It provides the viewer with the answer to one of many questions that he or
she might ask: ‚ÄúWho is in this picture?‚Äù Another, more general, tagging question the user may pose is, ‚ÄúWhat is this a
picture of,‚Äù to which solutions are often ingrained in semantic annotation and image retrieval techniques.5 Such techniques
have grown considerably because they address the experience deÔ¨Åned by online image searching. It is also possible to ask
‚Äúwhere‚Äù an image was taken. Location matching is a concept that 3D modeling techniques13 have focused on by integrating
advances in Structure from Motion (SfM), bundle adjustment, and feature representation.7 The question we ask takes a step
further and involves integration of aforementioned object annotation and SfM technologies, i.e the ‚Äúwhere‚Äù and ‚Äúwhat‚Äù.
While multi- and single-view geometry have only previously been applied to images collectively, we can expand the scope
to items within the image; that is, ‚ÄúWhere are speciÔ¨Åc objects in this image?‚Äù
The problem translates to Ô¨Ånding the 3-D real-world coordinates of an object of interest given a single photograph. With
adequate recognition software (to be discussed), the administration of the proposed algorithm spans a broad spectrum of
problems. ‚ÄúWhere was I in this photo‚Äù; ‚ÄúWhere exactly did I leave my keys‚Äù; ‚ÄúWhere did I park my car‚Äù; etc. Eventually,
real-world coordinates, if aligned absolutely with physical landmarks and terrain a priori, can be used to geo-register
objects on a global scale, a topic immensely useful to military surveillance, where target identiÔ¨Åcation and geo-registration
are valuable assets.
As expected, the ideas presented in this paper are intimately related to object detection and 3D image registration.
Combined, we propose an object registration algorithm. The processing chain in object detection and image registration
are similar in that both require feature identiÔ¨Åcation. For faces, cars, and other objects detectable through a mixture of
This work is sponsored by the Department of the Air Force under Air Force contract FA8721-05-C-0002. Opinions, interpretations,
conclusions, and recommendations are those of the author and are not necessarily endorsed by the United States Government

Computational Imaging VIII, edited by Charles A. Bouman, Ilya Pollak, Patrick J. Wolfe,
Proc. of SPIE-IS&T Electronic Imaging, SPIE Vol. 7533, 75330J ¬∑ ¬© 2010 SPIE-IS&T
CCC code: 0277-786X/10/$18 ¬∑ doi: 10.1117/12.849919
SPIE-IS&T/ Vol. 7533 75330J-1
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

experts and boosting cascades,11 prominent basis functions for features have included Haar Filters. Other object detection
algorithms1 have built features using Gabor functions. Likewise, features in Snavely et al.13 register images collectively
through SfM using scale invariant feature transforms (SIFT), originally proposed by Lowe et al.7
As a learning-based algorithm, we require image collections, or training sets, to generalize to unseen data points. To
register objects we need sets to resolve:
1.
2.
3.

Location
Objects of interest
Objects not of interest

Item 1 involves the same bank of co-located images used in SfM.13 Items 2 and 3 refer to images of the objects that are
and are not of interest, following often used conventions.11 In practice,16 their attributes include the standard 24 √ó 24 pixel
sizes with real-time realizations, which can be found in digital cameras and tracking applications (see US Patent 7315631).
Item 3 may (and in our case does) overlap with Item 1.
The proposed algorithm is described in the remainder of this paper with the following sections. Sec. 2 reviews the
collection of all three training sets and the building of our 3-D model through SfM. As there may be overlap between
sets, we also discuss related issues and computational savings. Next, our major contributions are discussed with image
registration in Sec. 3 as the Ô¨Årst substep and object registration in Sec. 4 as the second substep towards our goal. Finally, a
number of experimental results and conclusions can be found in Sec. 5 and Sec. 6.

2. BACKGROUND AND SETUP
Testing and application of object registration techniques occur in three serial phases: image registration, object detection,
and object registration. As shown in Fig. 1(b), each of the stages feeds from a previous stage. Applications of two of the
phases require training and initialization.
Like most algorithms, the proposed algorithm generalizes with large amounts of data to build point clouds and determine object detection parameters. The two sets can be collected independently, but to improve performance (and some
added supervision), we can overlap training images between the two stages. Training object detection algorithms not only
requires numerous images of the objects of interest but also of background image patches that are not of interest, i.e. the
negative set, and could potentially generate false positives. Should we introduce such redundancy during training, test images matching to the generated 3-D point cloud are more likely to have the same image structures, which naturally implies
fewer false positives. In a sense, we are telling the object detection algorithm exactly what image texture patches to ignore.
Further setup of the proposed algorithm relies heavily on several well-established techniques in the computer vision
community. The overall initialization diagram is shown in Fig. 1(a), which includes the 3-D point cloud generation and
detection parameter training. For the two stages requiring training, the primary techniques are embedded heavily in 3-D
geometry13 and computer vision.11, 16 This section reviews their contributions and how they Ô¨Åt into our framework.

2.1 3-D Scene Representation
The goal of this initialization phase is to create a 3-D point cloud whose individual elements describe the 3-D point coordinates of features from a set of images. It is a collective data Ô¨Åltering process that simultaneously solves for scene structure
while determining camera parameters and projection matrices for each image. The implementation is primarily taken from
Noah Snavely‚Äôs Photo Tourism work.13 The component modules extract features from images, Ô¨Ånd the correspondences
across images, and then run SfM on the matching information. The process is shown in the upper subdiagram of Fig. 1(a)
with the stages consecutively labeled:
1.
2.
3.

SIFT Feature Extraction7
Approximate Nearest Neighbor Matching3
Structure from Motion with Bundle Adjustment

The SIFT keypoint detector7 is used because of its invariance to image transformation and robustness in matching. Not
only does SIFT return a list of keypoint locations within an image, but also gives a n-dimensional descriptor vector that

SPIE-IS&T/ Vol. 7533 75330J-2
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

Test Image

3-D
3-D
Point
PointCloud
Cloud

Register Image

Object Detect
{wi}
FEATURE + WEIGHTS

ROTATION: R
CENTER:
C

Register Object

X1 = (LAT, LONG, ALT)
X2 = (LAT, LONG, ALT)
X3 = (LAT, LONG, ALT)

(a) Overall Training Procedure

(b) Overall Testing Procedure

Figure 1. Sample of Pictures Used

can be used for matching. Lowe demonstrates that using 128-dimension vectors typically gives the best balance between
speed and performance when trying to reliably match images. For typical 8MP images, SIFT returns around 8-10 thousand
features. Depending on the application, other feature types10 could also be used here.
Once all of the features have been extracted, correspondences need to be established. For each pair of images, keypoints
are matched by Ô¨Ånding the nearest neighbor vector in the corresponding image, which is traditionally deÔ¨Åned in Euclidean
L2 space. To speed up matching, Arya and Mount‚Äôs approximate nearest neighbor (ANN) package3 can be exploited. For
image I and J, ANN builds a kd-tree of the features in image J and then queries the tree for the best match of each feature
in image I. Instead of deÔ¨Åning a valid match by thresholding the distance, valid matches are determined using a ratio test:7
Ô¨Ånd the best two nearest neighbors in image I with distances d1 and d2 where d1 < d2 . Accept as a match if dd12 < 0.6.
Photo Tourism13 computes voxel locations of reliably matched features, while estimating the camera parameters.
Tracks of matching features are then built to be triangulated later. Photo Tourism initializes with a reconstruction of
two images by triangulating the tracks of two images with the most number of images and largest baseline. After reconstructing the Ô¨Årst pair it then proceeds to add images that observe a large set of what has already been reconstructed and
adds in new tracks that are observed by already reconstructed images. Like other SfM projects, Photo Tourism also runs a
bundle adjustment on the reconstructed scene after each image is added.14
Each voxel represents a track of features from multiple images. Each feature in a track (because of the matching
criterion) can be considered nearly identical. As a result, each voxel has its own set of descriptor vectors that can be used
to match with the new image. For computational efÔ¨Åciency in image matching and reduction of noise, we take an average
of the feature descriptors for a given voxel into a representative descriptor. (There are other data reduction techniques that
could work.) The descriptors are then stored in the same format as Lowe keypoint Ô¨Åles.
While we no longer have a concept of scale nor an absolute orientation, the storage format can be modiÔ¨Åed to reduce
the four keypoint values (scale, orientation, location) down to just three (location). Given a representative set of feature
descriptors, the Ô¨Ånal kd-tree can be built with representative features using Arya and Mount‚Äôs ANN package.3

2.2 Object Detection
Depending on the application, detection is intimately associated with the type of features chosen. While SIFT features yield
excellent results as a means for discriminating location, one fortuituous consequence is that they are not able to discern
‚Äúinteresting‚Äù foreground objects. For example, in preliminary experiments, 3-D scenes were built with an abundance of
stationary cars and trucks as a signiÔ¨Åcant portion of the images. No or very few meaningful features from any vehicle were
involved in the 3-D point cloud reconstruction.
As it turns out, faces, humans, cars, and a number of other potential foreground objects of interest reconstruct very
poorly in three dimensions on a global scale. Furthermore, should the object move, its features are even less likely to
match with other images. The conclusion is that SIFT features of foreground objects will not be good discriminants of its
location.

SPIE-IS&T/ Vol. 7533 75330J-3
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

This is fortunate because the objects themselves may not remain stationary. (Local motion between images introduces
system noise that image registration is sensitive to.) Therefore, at least for faces and cars, the presence of objects in an
image will not impact the overall image registration performance, and hence can be done independently.
Though it is conceivable to register an arbitrary type of object, the results in Sec. 5 reÔ¨Çect human face registration data.
The features used in the most cited face detection algorithms11 are Gabor functions and Haar Wavelets.8 Additionally,
recent works have suggested that skin tone is relatively robust to changes in luminance. Consequently, for face detection,
the proposed algorithm combines simple skin tone template matching after an openCV real-time implementation of Viola
and Jones.16

3. INCREMENTAL IMAGE MATCHING
After the initialization phases of Fig. 1(a), we are presented with both 3-D information and object of interest (face) detection
information. This section is concerned with the image registration box in Fig. 1(b), where the two inputs are the test image
and the 3-D point cloud setup by Sec. 2.1. The goal is to produce a single rotation matrix and translation vector that
represents where the camera was located and in what direction it was pointing at the time the picture was taken.
Ideally, SfM and bundle adjustment can decide whether or not an image belongs to a set by incorporating it into the set,
recomputing the correspondences, and observing how well it correlates. Yet, to do so is both unnecessary and prohibitively
expensive because SfM13 and related algorithms focus on joint optimization of images within the set. We are, instead,
concerned with incrementally registering and matching a single test image without augmenting the point cloud, foregoing
any online training.
Because the reconstructed point cloud is based on the matched features from the images themselves, this lends itself to
a different way of matching. Rather than matching to images, the image can be matched to the point cloud. That is to say,
we match to the representative feature introduced at the end of Sec. 2.1 instead of the collective set of images.
(T )

Let {fi } be the set of features obtained in the test image T . We search for the smallest and second smallest distances,
d1 and d2 , and test the ratio dd12 < 0.6 to Ô¨Ånd matches deÔ¨Åned by (1):
(T )

‚àí Fj 2

(T )

‚àí Fj 2

Xmatch,i = argminfi
Fj

d1 , d2 = min fi
Fj,1 ,Fj,2

(1)

4. PROJECTIVE OBJECT REGISTRATION
In this section, we address the task of registering an object to absolute and standard metrics, the bottom box in Fig. 1(a).
Object registration requires several inputs, which can be fed directly from both object detection and image registration
modules. From the image registration module (Sec. 3), object registration takes matching image information (in the form
of matched features) and camera information (i.e., camera rotation matrix R and coordinates C). From the object detection
module (Sec. 2.2), object registration takes the pixel locations of an object‚Äôs bounding box. Object registration determines
real-world measurements (3-D coordinates) that describe the location of the object.
The algorithm begins by taking the image plane Pf that contains ranging information from an ‚Äúoptimal‚Äù feature f based
on accuracy and preciseness critera in Sec. 4.1. Once real-world coordinates of f ‚àó can be attained, single-view camera
techniques extract individual object registration information in Sec. 4.2. Finally, geo-registration can occur by aligning
distances to known absolute positions in Sec. 4.3.

4.1 Optimal Feature Selection
Registering objects requires a sense of positioning in the generated 3-D world. An abundance of such information is
presented as matched features, and a single point, the optimal point, can register a detected item relative to the point cloud.
There are numerous factors that we consider in choosing such a point: re-projection error, camera distance, radial angle,
and the number of correspondences with the associated feature. The most accurate and precise point will yield the best
3-D object coordinates.

SPIE-IS&T/ Vol. 7533 75330J-4
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

Accuracy depends on re-projection error as well as the number of feature correspondences per related voxel. The
assumption is that any chosen feature must foremost be both correct (re-projection error) and ‚Äúinteresting‚Äù (in terms of the
number of images in which a feature is present and matches).
Meanwhile, precision depends on camera distance and radial angle from the center of the image. Information could
potentially be lost in resolution as images are traditionally represented on a regularly sampled grid. That is, a pixel
corresponding to a far away voxel represents a larger physical distance than one that corresponds to a closer feature. Thus,
the pixel to information content is higher in the latter as is its resolution. Likewise, pixels on the edge of an image may not
yield as precise results as those closer to the center.
Because 3-D construction is done a priori, obtaining the ‚Äúbest‚Äù feature is simply a discrete minimization over several
number of variables and hence can be solved iteratively. As a pre-processing step, we threshold the re-projection error so
that only handful of points are relevant. Let S be the set of image voxels that have been matched with the image I. The
optimal feature fi‚àó can be determined by:
i‚àó = argminŒª1 Œîxi + Œª2 V ‚àí1 (xi ) + Œª3 ||xi || + Œª4 ||Xi ‚àí C||.
i:xi ‚ààS

(2)

Here, xi denotes the 2-D pixel location of the ith extracted feature, Xi the 3-D matched voxel location, C the camera
location, and V (x) the feature count per voxel. The terms in (2) can be summarized for the ith candidate feature as:
1st term:
2nd term:
3rd term:
4th term:

Œîxi
V ‚àí1 (xi )
||xi ||
||Xi ‚àí C||

Accuracy Metric:
Accuracy Metric:
Precision Metric
Precision Metric

Reprojection Error
Inverse feature per voxel count
2-D Offset from center pixel
3-D Distance from Camera

Determining Œªi values involves understanding the relationships between the metrics. For example, it is not unrealistic
to assume that the positioning of landmarks are generally oriented alongside the principle plane (the plane orthogonal to
the camera pointing direction). With this assumption, should we wish to, we can reduce a degree of freedom for Œª3 with
respect to Œª4 . Let Œ± be a single pixel angle given by:


Ô¨Ålm size
2
√ó arctan
Œ±=
,
(3)
image size
2 √ó focal length
then the distance that pixel subtends, which we wish to minimize, is directly relatable to a features‚Äô distance from the
camera center and its angular offset:
pi = ||Xi ‚àí C|| {sin (Œ±xi ) ‚àí sin (Œ±(xi ‚àí 1))} .

(4)

Substituting the precision terms, the optimal feature can then be written as:
x‚àó = argminŒª1 Œîxi + Œª2 V ‚àí1 (xi ) + Œª3 pi .
xi ‚ààS

(5)

4.2 Relatively Registering the Object
As a geometric-intensive study, our computer vision notation follows that of the well-written and cited source, Hartley and
Zisserman.6 The ideas presented in this subsection determine, in single-view camera geometry, the real-world coordinates
of an object from an image that is known to have been taken of a particular scene. The basic steps that we reference or
derive to meet our goal can summarized as follows:
1.
2.
3.
4.

Take the optimal feature x‚àó from Sec. 4.1 and Ô¨Ånd its distance along the camera point
direction to our camera coordinates C
Find the plane Pf through Xf‚àó that is parallel to the image plane
Back project the detected object‚Äôs coordinates onto Pf
Using similar triangles and assumptions on the object of interests‚Äô size, derive its
location

SPIE-IS&T/ Vol. 7533 75330J-5
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

From meta-data, the camera intrinsics can be found and labeled as K, and from software developed for Sec. 3, the
rotation R and translation t can be applied to produce a general projective camera model P . The forward projection matrix
P = K [R|t] maps world points in X to image points x according to x = P X.
In Steps 1 and 2, once selected, determining the horizontal distance to Xf‚àó , the distance along the camera pointing axis,
is simply the third coordinate of P Xf . Let us call this distance df . The full descriptor of the pointing vector to the feature
plane P 4 is thusly known with normal vector, m3 , determined by
‚é°
‚é° ‚é§T ‚é§
0
‚é¢ R‚àí1 ‚é£ 0 ‚é¶ ‚é•
‚é•
(6)
m3 = ‚é¢
‚é£
‚é¶
1
0


‚àí1
The pseudo-inverse (Moore-Penrose solution) of projective matrix P is P + = P T P P T
. The ray that connects
a world-point and the camera is determined by the join of the camera center C = ‚àíKRt and the feature point P + x. As
it happens, C ‚àà N (P ), the null-space of the forward projection matrix, and P C = 0. Thus, adding a vector P + x to any
scalar multiple of C will always project back onto x since P P + = I. The set of world-points that project onto the image
at x is thus given by:
(7)
X(Œ∑) = P + x + Œ∑C.
Backprojecting into the real-world is an ill-posed solution without knowledge of how far to project. That is, we can
backproject an arbitrary distance unless Œ∑ is known in (7). In Steps 3 and 4, we can use the optimal feature Xf as a
reference point that describes what how large the object would appear were it projected onto the image plane on which Xf
lies. Values of Œ∑ could easily be calculated in (8) by using the dot product of the ray described by (7) and the unit normal
m3 . Given a point xobs , then its projection onto Pf is given by
Œ∑(Pf , xobs ) =

df ‚àí (P + xobs ) ¬∑ m3
+ 1.
C ¬∑ m3

(8)

The features in the experiments are often a few thousand meters away. In face detection, even the smallest detection
box of 24 √ó 24 pixels are typically not descriptive of a human faces (unless their heads are thousands of meters long.) If we
assume a typical or average size of a human face, then the ratio of his/her face size can be applied to the distance projected
onto the Pf .
Let x(T L) and x(BL) be the top-left and bottom-left corners of the detected object returned by the object detector in
(T L)
(BL)
and Xf
, the respective real-world projections. Then, if l is the assumed
Sec. 2.2. We can use (8) and (7) to obtain Xf
length of an object (e.g. someone‚Äôs face), then the ratio can be applied to obtain the object‚Äôs real-world 3-D coordinates
X (T L) = (KR)‚àí1
and
X

(BL)

‚àí1

= (KR)





l
Œ∑
||X(T L) ‚àí X (BL) ||
l
Œ∑
||X(T L) ‚àí X (BL) ||





xT L
0
xBL
0



‚àí C,

(9)

‚àí C.

(10)




4.3 Distance Alignment
To geo-register the object, that is, to gain an absolute sense of where it lies in the real-world, a scaling metric must be
deÔ¨Åned to transfer 3-D coordinates into tangible locations and distances. This can be done in a number of ways. Recently,
we have begun to relate 3-D point clouds with ladar data, an integration work that is still in progress. At present, a scaling
factor has been chosen simply with approximations through Google MapsTM and some pedometer tools. One interesting
cue speciÔ¨Åc to the photo-location aiding in distance alignment involves the nonstandard unit of length ‚ÄúSmoot‚Äù, which has
been incorporated as an optional unit of measure in Google EarthTM . As imprecise as the entire process may seem, the
approximation can be put into perspective. With issues pertaining to object size, pixel resolutions, and 3-D reprojections,
errors due to distance approximation may be mitigated to an extent, but only insofar as the exactness of the parameter
estimations of other factors.

SPIE-IS&T/ Vol. 7533 75330J-6
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

5. RESULTS
There is currently a large collection of images available on the internet and various other sources. SfM reconstructions
such as the ‚ÄúRome in a Day‚Äù project2 are uncooperative in the sense that they draw from images that were not taken with
the sole purpose of 3-D image reconstruction. We utilize a cooperative data set of 1201 images (Table 1) of the Boston
skyline taken from across the Charles River by Kilian Court on MIT campus. Of the 1201 images, we partition the set into
871 images to reconstruct a 3-D point cloud (S) and 330 images for testing and cross-validation (Figure 3(b)). In addition
another 319 images are gathered of a similar New York City skyline from Flickr to be utilized as images that should not be
part of the set (Figure 3(c)). To test across image scales, the image set consists of origimal images and their counterparts
that have been decimated by two and by four, as part of the original 1201 images. The 1201 images are reconstructed and
a representative set of features is generated as our initialization phase. Fig. 2 shows the reconstruction results of the 1201
images.

Figure 2. Point Cloud Reconstruction

(a) Training Image, Boston Skyline

(b) Testing Image, Boston Skyline

(c) Testing Image, New York Skyline

Figure 3. Sample of Pictures Used

There are existing trained cascades for frontal faces and in proÔ¨Åles, but it is unlikely that any of them were trained with
the negative set of Boston in the background. Therefore, the positive frontal face training set is obtained from a mix of
the 4916 hand labeled faces created by Peter Carbonetto,4 each of which are scaled and aligned to 24 √ó 24 resolution, and
some post-processed versions of the UCD database.12 The negative set was hand selected from the 1201 skyline images
used in SfM.

5.1 Image Matching
From the tests we determine probability of detection and false alarms with thresholds ranging from 0 to 5% feature match.
The false alarm rate is peaky around 0% and tends to drop off at a rapid rate as we increase our threshold, resulting in the
sharp slope to the far left of the ROC curve in Fig. 4. Naturally, the attributes pertaining to probability curves are highly
dependent on the types of images used. For example, it is easier to classify an image as not belonging to the reconstructed
3-D Boston skyline scene if it is of an entirely unrelated location (e.g., a forest or indoors), which yields PF A = 0 thus far
in our experiments. In terms of Pmiss , further review of the images reveals that missed images have been taken at extreme
angles or highly obstructed Ô¨Åelds of view. Hence, the number of features of the background is signiÔ¨Åcantly reduced.

SPIE-IS&T/ Vol. 7533 75330J-7
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

Figure 4. Image Detection Receiver Operating Characteristic

5.2 Image and Object Registration Error
Object detection and registration was performed with a variety of 420 test images, a subset of which, was used in object
registration. Our true position for each test point recording was generated by coordinating a combination of Google
Earth GPS coordinates, some markers generated by non-standard units on Harvard bridge (termed ‚ÄúSmoots‚Äù), physical
landmarks, and a large number of physical markers.
In response to the detector‚Äôs insensitivity to small scale and translation, Viola and Jones16 Ô¨Ålters out multiple detections
per single object. A single bounding box is ideal, but the fact that it is necessary is of concern as we derive much of our
registration information from the bounding box size. The consequent approximations have been imprecise as can be seen
in Fig. 5 and Fig. 6 and are a topic of interest for future work.
The overall registration error can be measured in meters as the distance from the estimated position to the actual target
position. As expected, the curve reÔ¨Çects a growing trend with respect to distance from the camera in both Ô¨Ågures. The
primary source of error, we have noticed, occurs from the actual image registration itself, and not object registration. This
is reÔ¨Çected in the Ô¨Årst Ô¨Ågure, where the trend is upwards and near-linear.
Note that our detections span three dimensions, each of which introduces some error. The seemingly randomness at
Table 1. Images Used for Reconstruction

Number of Images
for Reconstruction
(R)
Number of Voxels
Average Number of
Features per Voxel
Test Statistic
H1 Event
H0 Event
Number of H1 Test
Images
Number of H0 Test
Images

1201 at 8MP

83,796
5.3693
Percentage of Features in new image that matched to voxel features
Test Statistic above a threshold
Test Statistic below a threshold
330 Images (110 at 10MP, decimated by 2, decimated by 4)
319 Flickr Images

Table 2. Data Reduction Rate for 1201 Images

Data
Raw Pixels
SIFT
3D SIFT

Number of Points
9,563,111,424 pixels
12,976,125 features
83,796 features

Percent of Original
100%
0.1357%
0.000876%

SPIE-IS&T/ Vol. 7533 75330J-8
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

further distances, especially in the feature distance, is due in most part to pixel resolution coverage. We roughly approximated one of pixels at a distance of 200 meters to be 43 meters on the horizon of the image plane, which would give us the
estimation results in Fig. 6.

Figure 5. Error based on Object Proximity. Fixed feature distance at 756m.

Figure 6. Error based on Background Feature Proximity. Fixed object distance at 8m.

6. CONCLUSIONS AND FUTURE WORK
An algorithm has been proposed for 3-D object registration given an image taken of a particular location. As an integration
application with many parts, the potential for improvements is in accordance with all three phases of Fig. 1(b).
For image registration, particular image attributes for acceptable inclusion into SfM optimization remains an interesting
problem. (Skeletal graph construction papers observing connectivity15 are useful.) On a lower level, we are also currently
investigating the image content that produces the most salient features.
For object detection, classiÔ¨Åers for object detection are often obtained by thresholding continuous functions, which is
theoretically optimal as it is akin to Bayes decision rule. It would be beneÔ¨Åcial for our applications to be as Ô¨Çexible as
possible in terms of what targets to detect and how sensitive we are to detecting them. While adaboost is cost insensitive,
and we can turn to asymmetric boosting techniques.9
In terms of the actual object registration, there is an abundance of data to be fused yet, including ladar and aerial
imagery and videos that we have not begun to incorporate. Once integrated, an improvement in accuracy and precision is
espected to be drastic as the available amount of information will dramatically grow. Such questions are addressable and
open.

SPIE-IS&T/ Vol. 7533 75330J-9
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

7. ACKNOWLEDGEMENTS
We would like to thank Noah Snavely for all the help and advice he has given us in regard to working with his Photo Tourism
SfM project. A copy of his code that we have been working with can be downloaded from the Photo Tourism website
(http://phototour.cs.washington.edu/). In addition we would also like to thank Peter Cho at MIT Lincoln Laboratories for
his insights into applications with the Photo Tourism work.

REFERENCES
1. R. Alterson and M. Spetsakis. Object recognition with adaptive gabor features. Image and Vision Computing, 22(12):1007 ‚Äì 1014,
2004. Proceedings from the 15th International Conference on Vision Interface.
2. S. Argawal, N. Snavely, I. Simon, S. Seitz, and S. Szeleski. Building rome in a day. In International Conference on Computer
Vision, Kyoto, Japan, 2009.
3. S. Arya, D. M. Mount, N. S. Netanyahu, R. Silverman, and A. Y. Wu. An optimal algorithm for approximate nearest neighbor
searching in Ô¨Åxed dimensions. In ACM-SIAM Symposium on Discrete Algorithms, pages 573‚Äì582, 1994.
4. P. Carbonetto. Viola-jones training data. http://www.cs.ubc.ca/\Àúpcarbo.
5. G. Carneiro, A. B. Chan, P. J. Moreno, and N. Vasconcelos. Supervised learning of semantic classes for image annotation and
retrieval. IEEE Transactions on Pattern Analysis and Machine Intelligence, 29(3):394‚Äì410, March 2006.
6. R. Hartley and A. Zisserman. Multiple View Geometry in Computer Vision. Cambridge University Press, 2000.
7. D. G. Lowe. Distinctive image features from scale-invariant keypoints. International Journal of Computer Vision, 60:91‚Äì110,
2004.
8. S. Mallat. A theory for multiresolution signal decomposition: The wavelet representation. IEEE Trans. Pattern Anal. Mach. Intell.,
11(7):674‚Äì693, 1989.
9. H. Masnadi-Shirazi and N. Vasconcelos. Asymmetric boosting. In International Conference on Machine Learning, pages 609‚Äì619,
2007.
10. K. Mikolajczyk and C. Schmid. A performance evaluation of local descriptors. IEEE Trans. Pattern Anal. Mach. Intell.,
27(10):1615‚Äì1630, October 2005.
11. C. Papageorgiou, M. Oren, and T. Poggio. A general framework for object detection. International Conference on Computer
Vision, 1998.
12. P. Sharma. The UCD colour face image database for face detection download page. http://ee.ucd.ie/\Àúprag/.
13. N. Snavely, S. M. Seitz, and R. Szeliski. Photo tourism: Exploring photo collections in 3d. In SIGGRAPH Conference Proceedings,
pages 835‚Äì846, New York, NY, USA, 2006. ACM Press.
14. N. Snavely, S. M. Seitz, and R. Szeliski. Modeling the world from Internet photo collections. International Journal of Computer
Vision, 80(2):189‚Äì210, November 2008.
15. N. Snavely, S. M. Seitz, and R. Szeliski. Skeletal sets for efÔ¨Åcient structure from motion. In Proc. Computer Vision and Pattern
Recognition, 2008.
16. P. Viola and M. Jones. Rapid object detection using a boosted cascade of simple features. Proc. CVPR, 1:511‚Äì518, 2001.

SPIE-IS&T/ Vol. 7533 75330J-10
Downloaded From: http://proceedings.spiedigitallibrary.org/ on 06/22/2017 Terms of Use: http://spiedigitallibrary.org/ss/termsofuse.aspx

A 3D FEATURE MODEL FOR IMAGE MATCHING
Zachary Sun, Nadya Bliss, Karl Ni
M.I.T. Lincoln Laboratories
E-mails: zsun86@ll.mit.edu, nt@ll.mit.edu, karl.ni@ll.mit.edu

ABSTRACT
The proposed algorithm identiÔ¨Åes whether or not a test photo belongs to a set of co-located training images based on its spatial proximity to the training set. We leverage concepts from Lowe‚Äôs SIFT
and Snavely‚Äôs Photo Tourism algorithms, and match an image by
its 2D features to the 3D features representing the training set. To
reduce complexity and increase efÔ¨Åciency, the proposed algorithm
implements a compact representation of the image set by merging
collections similar features. Test images are then matched with the
derived structure. Finally, a decision statistic is determined based on
the percentage of features that match. Receiver operating characteristics, computational analysis, and distributions are included in the
performance analysis.
Index Terms‚Äî SIFT, 3D Matching, Approximate Nearest
Neighbor, Structure from Motion
1. INTRODUCTION
With the advent of social networking websites, smart cameras, and
military surveillance operations that feature autonomous picture labeling and image grouping, it is commonly desired to determine relationship of an image with respect to an organized set of images. The
quality of computer vision techniques that solve this problem can
be assessed by a variety of metrics. Depending on the application,
the metric could include how similar objects are inside the images,
the Ô¨Ådelity and resolution of the image, or in our case, closeness in
spatial proximity.
The image set can be generated in a variety of ways: images
from the internet, videos, photo albums, etc. Organizing and compressing the data for timely exploitation presents a challenge. As
humans, we often store memories by the most noticeable landmarks
of a scene. In computer vision terms, we would like to retain the
most salient features of an image set and form a compact representation that describes a location with the most important visual cues.
Given a set of images known to be spatially co-located, we propose an algorithm that builds such a representation and determines
whether or not a test picture belongs to the image set. In addition, should the picture belong, we provide the capability to register and provide 3-dimensional (3-D) information about where it was
taken. Determining such parameters relies heavily on iterative structure from motion (SfM) algorithms to provide a basis structure and
image matching to make the Ô¨Ånal decision.
SfM is the process of modeling the 3-D structure of a collection
of images from relative motion. SfM [1, 2], bundle adjustment [3],
This work is sponsored by the Department of the Air Force under Air
Force contract FA8721-05-C-0002. Opinions, interpretations, conclusions,
and recommendations are those of the author and are not necessarily endorsed
by the United States Government.

978-1-4244-4296-6/10/$25.00 ¬©2010 IEEE

2194

3-D modeling in general [4], and other related computer vision concepts have recently drawn a considerable amount of attention in a
number of works. We have modiÔ¨Åed a particularly successful development in Noah Snavely‚Äôs Photo Tourism research [1]. Ideally, [1]
can decide whether or not an image belongs to a set by incorporating it into the set, recomputing the correspondences, and observing
how well it correlates. Yet, to do so is unnecessary and prohibitively
expensive because [1] and related algorithms focus on the joint optimization of the images within the set. We are, instead, concerned
with incrementally registering and matching a single test picture to
the entire set without augmenting it, a much simpler problem.
Matching and registering images can be approached from either intensity-based or feature-based perspectives [5]. Because our
location-based registration matches according to tangible objects
themselves, the proposed algorithm utilizes features as the appropriate matching metric. Like [1], we make extensive use of Lowe‚Äôs
commonly cited Scale Invariant Feature Transform (SIFT, [6]) for
image matching. By using extracted SIFT features from the test
photo to compare to the features describing our 3-D model, we can
create a single, one-dimensional test statistic describing our conÔ¨Ådence in how well the image Ô¨Åts into the set. Directly following
from test statistic construction, a threshold can be calculated based
on a speciÔ¨Åed false alarm rate.
Aside from algorithmic developments, we have completed the
framework by modifying [1] for our purposes. As will be discussed
in later sections, we provide a compact and efÔ¨Åcient representation of
the 3-D modeling structure to search through when matching images.
Hence, the objectives and contributions of the proposed algorithm
can be summarized as:
‚Ä¢ Build a 3-D model based on [1] that describes a set of images
in terms of features
‚Ä¢ Accurately accept or reject a test picture into or from a set of
co-located images with a level of conÔ¨Ådence by attempting to
register it with the obtained 3-D model.
The remainder of the work addresses the above two points and is divided as follows. Sec. 2 reviews the key tools that build our environment and their relevance to our particular problem. Sec. 3 discusses
our previously discussed contributions and their associated issues in
detail. Sec. 4 provides results and analysis, and Ô¨Ånally Sec. 5 summarizes and discusses future directions.
2. RELATED WORK
The initialization phase of our work utilizes the work of Noah
Snavely‚Äôs Photo Tourism research [1]. We extract features from
images, Ô¨Ånd the correspondences across images, and then run SfM
on the matching information. From the structure we obtain a point
cloud that can be further exploited. This section will proceed to
describe the overall workings of each stage of this process: Feature

ICASSP 2010

Extraction (Sec. 2.1), Feature Matching (Sec. 2.2), and Structure
From Motion (Sec. 2.3).
2.1. Feature Extraction
We utilize the SIFT keypoint detector [6] because of its invariance
to image transformation and robustness in matching. Not only does
SIFT return a list of keypoint locations within an image, but also
gives a n-dimensional descriptor vector that can be used for matching. Lowe [6] demonstrated that using 128-dimension vectors typically gives the best balance between speed and performance when
trying to reliably match images. For typical 8MP images, SIFT returns around 8-10 thousand features. Depending on the application,
other feature types (see the analysis of [7]) could also be used here.

and time as R could potentially be very large. In an effort to reduce
the cost of subsequent detections, we leverage Photo Tourism to give
us a new way of representing this set: the real world geometry.
We Ô¨Årst extract the SIFT features from R, match them, and then
run SfM on the matches to create a point cloud. Since the reconstructed point cloud is based on the matched features from the images themselves, this lends itself to a different way of matching. Instead of matching to each image, we choose to match the image to
the point cloud. Figure 1 shows our methodology of matching. As
an additional part to our initialization stage we compute the 3D feature for each voxel and store it as a keypoint Ô¨Åle (Sec. 3.1) and then
the new image‚Äôs 2D features are matched to 3D voxel features (Sec.
3.2).

2.2. Feature Matching
Once all of the features have been extracted, correspondences need
to be established. For each pair of images, keypoints are matched
by Ô¨Ånding the nearest neighbor vector in the corresponding image,
which is traditionally deÔ¨Åned in (Eqn. 1).
To speed up the matching we used Arya and Mount‚Äôs approximate nearest neighbor (ANN) package [8]. For image I and J, ANN
builds a kd-tree of the features in image J and then queries the tree
for the best match of each feature in image I. Instead of deÔ¨Åning
a valid match by thresholding the distance, valid matches are determined using Lowe‚Äôs ratio test [6]: Ô¨Ånd the best two nearest neighbors
in image I with distances d1 and d2 where d1 < d2 . Accept as a
match if dd12 < 0.6.
(I)

Let fi be a feature obtained from image I. Then the nearest
(I)
(J)
neighbor match to fi in the set of features {fj } obtained from
image J is deÔ¨Åned as:
(J)

fmatch,i

=

d

=

(I)

argminfi
fj

(I)

minfi
fj

(J)

‚àí fj 2
(J)

‚àí fj 

(1)
(2)

2.3. Structure From Motion
Photo Tourism [1] utilizes (1) and (2) to compute the voxel locations
of reliably matched features, while estimating the camera parameters. Tracks of matching features are then built to be triangulated
later. Photo Tourism initializes with a reconstruction of an initial
two images by triangulating the tracks of two images with the most
number of images and largest baseline. After an initial reconstruction it then proceeds to add images that observe a large set of what
has already been reconstructed and adds in new tracks that are observed by already reconstructed images. Like other SfM projects,
Photo Tourism also runs a bundle adjustment on the reconstructed
scene after each image is added. Snavely in [9] describes this process in greater detail.
3. MATCHING 2D FEATURES TO 3D FEATURES
Given an area (for example, a section of a residential street, a courtyard, a city block, etc.), let S be the set of all images of that area. We
then deÔ¨Åne a subset R ‚àà S where R is a collection of images of the
area. We want to determine whether a new image, T , belongs to set
S. As Lowe demonstrated in [6], one way to go about this would be
to take T and match it to every image in R to decide whether or not T
belongs to S. However this is excessively expensive in both storage

2195

Fig. 1. Flow Chart of 2D to 3D Matching

3.1. 3D Feature Representation
The initialization stage has created a constraint such that each voxel
is a track of features from multiple images. Each feature in these
tracks (because of the matching criterium) can be considered nearly
identical. As a result each voxel has its own set of descriptor vectors
that can be used to match with the new image. We took an average of the feature descriptors for a given voxel into a representative
descriptor (Eqn. 3), however any other data reduction techniques
could work (median, min, max). We then store these descriptors in
the same format as Lowe keypoint Ô¨Åles. While we no longer have a
concept of scale nor an absolute orientation, we modify the storage
format to reduce his four keypoint values (scale, orientation, location) down to just three (location).
Fj =

m‚àí1
1 
fkj , j = 1, ..., n
m

(3)

k=0

Here, Fj is the j th representative descriptor in the point cloud, and
fkj is the kth feature corresponding to the j th voxel.
3.2. 3D Feature Matching
Recall that the problem statement is to determine whether or not a
test picture T is co-located with a set of images S, which we have
represented by a number of keypoint descriptors in the set {Fj }.
Therefore, the framework of our problem necessitates two hypotheses, where the events, labeled H0 and H1, are deÔ¨Åned as

‚éß
H0:
‚é™
‚é®
‚é™
‚é© H1:

Image T is not taken of speciÔ¨Åed area and
hence does not belong to set S
Image T is taken of the speciÔ¨Åed area and
hence belongs to set S

(4)

Given a representative set of feature descriptors, we once again
leverage Arya and Mount‚Äôs ANN package [8], and build a kd-tree
with the representative features. The new image‚Äôs features are extracted using SIFT as well, and then using the aforementioned ratio
test matches are determined.
(T )
Let {fi } be the set of features obtained in the test image T .
We once again search for the smallest and second smallest distances,
d1 and d2 , and test the ratio dd12 < 0.6 to Ô¨Ånd the match deÔ¨Åned by
(Equ. 5)
(T )

‚àí Fj 2

(T )
fi

2

Fmatch,i = argminfi
Fj

d1 , d2 =

min

Fj,1 ,Fj,2

‚àí Fj 

(a) Training Image in S

(b) H1 Testing Image

(5)

(c) H0 Testing Image

Fig. 2. Sample of Pictures Used

Once the number of matches in the new image are determined,
we utilize a percentage as our test statistic in (Eqn. 6).
T est Statistic =

|{Fmatch,i }|
(T )

|{fi

}|

(6)

If the test statistic is above a certain threshold, then the image is
said to match the 3D scene.
4. RESULTS
We utilize a data set of 1531 images (Table 1) of the Boston skyline
taken from across the Charles River by M.I.T. (images in set S). We
then partition the set into 1201 images to reconstruct a 3-D point
cloud (S) and 330 images for testing the event H1 (Figure 2(b)).
In addition another 319 images are gathered of a similar New York
City skyline from Flickr to be utilized as images that should not be
part of the set (H0, Figure 2(c)). To test across image scales, the
image set consists of decimated by two and by four images, as part
of the original 330 images. The 1201 images are reconstructed and a
representative set of features is generated as our initialization phase.
Figure 3 shows the reconstruction results of the 1201 images.
Table 1. Images Used for Reconstruction
Number of Images for 1201 at 8MP
Reconstruction (R)
Number of Voxels
83,796
Average Number of 5.37
Features per Voxel
Test Statistic
Percentage of features in new image
that matched to voxel features
H0 Event
Test Statistic below a threshold
H1 Event
Test Statistic above a threshold
Number of H0 Test Im- 319 Flickr Images
ages
Number of H1 Test Im- 330 Images (110 at 10MP, decimated by
ages
2, decimated by 4)
From the tests we determine probability of detection and false
alarms with thresholds ranging from 0 to 5% feature match. Our

Fig. 3. Point Cloud Reconstruction

false alarm rate tends to drop off at a rapid rate as we increase our
threshold (Figure 4(b)), but as Figure 4(a) shows, there are still a
good number of missed detections for any given threshold.
Further review of the images in H1 that have low detection
scores reveals images that have a Ô¨Åeld of view that was highly obstructed or taken at an extreme angle. Hence, the number of features
of the background is signiÔ¨Åcantly reduced (Figure 5(a)) or are of
parts of the area that did not reconstruct very well (Figure 5(b)).
Our technique not only depicts the ability to efÔ¨Åciently match a
test image to a given data set, but has also greatly further reduced
the storage requirements for said set. Table 2 shows our rate of reduction. We acknowledge that our space saving is dependent on the
success of Photo Tourism‚Äôs reconstruction, as the more points are
reconstructed the more 3D features we store.

Table 2. Data Reduction Rate for 1201 Images
Data Representation
Number of Points
Percent of Original
Raw Pixels
9,563,111,424 pixels
100%
Lowe SIFT Features
12,976,125 features
0.1357%
3D SIFT Features
83,796 features
0.000876%

2196

(a) Histogram of Matches

(b) Probability of Detection and False Alarm

(c) Receiver Operating Charactersitic

Fig. 4. (a) Shows the histogram of images whose percentage of matched features falls within bins of size 0.005. (b) shows the decay of
probability of detection and false alarm as we increase our detection threshold. (c) Our ROC curve for this particular data set.
6. ACKNOWLEDGEMENTS

(a) Image at far away spot

We would like to thank Noah Snavely for all the help and advice
he has given us in regard to working with his Photo Tourism structure from motion project. A copy of his code that we have been
working with can be downloaded from the Photo Tourism website
(http://phototour.cs.washington.edu/).
In addition we would also like to thank Peter Cho at Lincoln
Labs for his insights into applications with the Photo Tourism work.

(b) Image of different angle

Fig. 5. Images of H1 that had low detection levels

7. REFERENCES
[1] Noah Snavely, Steven M. Seitz, and Richard Szeliski, ‚ÄúPhoto
tourism: Exploring photo collections in 3d,‚Äù in SIGGRAPH
Conference Proceedings, New York, NY, USA, 2006, pp. 835‚Äì
846, ACM Press.

5. CONCLUSIONS
We have proposed an approach to store voxel features for the purposes of future matching. With this, we have also signiÔ¨Åcantly reduced the cost of determing if an image is a part of a scene or not.
However, we have noticed that the success rate of a match is significantly dependent on the success of Photo Tourism. An insufÔ¨Åcient
reconstruction will yield to a very poor set of voxel features and thus
a very low probability of detection. While probability of detection
seems to vary, our false alarm rate tends to be very close to zero for
any reasonable thresholds. In addition our detection appears to be
limited by images that are taken at the same resolution or smaller
as the images that were used in the reconstruction. This is due to
SIFT‚Äôs ability to detect smaller features within higher resolution images, and since if features are not detected Photo Tourism will not
reconstruct them, and thus they will not be matched to the new image.
While we have framed this as a detection problem, this has other
applications that could potentially be exploited. Matching 2D images to a 3D point cloud always leads to the question of pose estimation. Right now the detection is based solely on percentage of
matched features, but we recognize that even those incorporate errors. Pose estimation could be utilized to further reÔ¨Åne the matches
and reject some erroneous matches to improve our matching accuracy. Once an image is localized within a point cloud other applications such as object recognition can be further applied. We would
like to see a larger integration of data from not just camera based
images but potentially aerial or even satellite images as well. With
reliable matching to 3D data sets the additional data from these other
sets can be easily passed through to matched images.

2197

[2] E. Arbogast and R. Mohr, ‚Äú3-d structure inference from image
sequences,‚Äù Journal of Pattern Recognition and ArtiÔ¨Åcial Intelligence, vol. 5, pp. 749‚Äì764, 1991.
[3] Bill Triggs, Philip F. Mclauchlan, Richard I. Hartley, and Andrew W. Fitzgibbon, Bundle Adjustment ‚Äì A Modern Synthesis,
vol. 1883, January 2000.
[4] R. I. Hartley and A. Zisserman, Multiple View Geometry
in Computer Vision, Cambridge University Press, ISBN:
0521540518, second edition, 2004.
[5] A. Ardeshir Goshtasby, 2-D and 3-D Image Registration: for
Medical, Remote Sensing, and Industrial Applications, WileyInterscience, 2005.
[6] David G. Lowe, ‚ÄúDistinctive image features from scale-invariant
keypoints,‚Äù International Journal of Computer Vision, vol. 60,
pp. 91‚Äì110, 2004.
[7] Krystian Mikolajczyk and Cordelia Schmid, ‚ÄúA performance
evaluation of local descriptors,‚Äù IEEE Trans. Pattern Anal.
Mach. Intell., vol. 27, no. 10, pp. 1615‚Äì1630, October 2005.
[8] Sunil Arya, David M. Mount, Nathan S. Netanyahu, Ruth Silverman, and Angela Y. Wu, ‚ÄúAn optimal algorithm for approximate nearest neighbor searching in Ô¨Åxed dimensions,‚Äù in ACMSIAM Symposium on Discrete Algorithms, 1994, pp. 573‚Äì582.
[9] N. Snavely, S. M. Seitz, and R. Szeliski, ‚ÄúModeling the world
from Internet photo collections,‚Äù International Journal of Computer Vision, vol. 80, no. 2, pp. 189‚Äì210, November 2008.

S

cie n t i f ic

Programming
Editors: George K. Thiruvathukal, gkt@cs.luc.edu
Konstantin L√§ufer, laufer@cs.luc.edu

High-Productivity Software
Development with pMatlab
By Julie Mullen, Nadya Bliss, Robert Bond, Jeremy Kepner, Hahn Kim,
and Albert Reuther
pMatlab, a parallel Matlab library, lowers the barrier to development of parallel and distributed application
codes by providing an easy-to-use programming environment.

A

lthough high-performance
computing (HPC) has been
around for decades, most
working engineers and scientists don‚Äôt
use parallel or distributed systems
for their research or design because
of the hurdles associated with using
such resources. Matlab, a widely used
programming tool for scientists and
engineers, has the characteristics that
appeal to the scientific community‚Äî
namely, ease of use, rapid prototyping
capability, and an interactive development environment. Bringing this
segment of the scientific community
to HPC requires a similar set of tools
and features that abstract the hardware
and parallel coding details away from
the design of scientific applications.
Recognizing the need for such a library, the MIT Lincoln Laboratory
created an interactive parallel development environment that includes
pMatlab,1 an open source parallel
Matlab library; MatlabMPI, an open
source Matlab-compatible library for
interprocessor communication; and
gridMatlab, a proprietary library that
transparently connects the user‚Äôs desktop Matlab session with N ‚Äì 1 remote
Matlab sessions on a large distributed
cluster. In this installment of Scientific
Programming, we explore the ease of
tackling a communication-intensive
parallel computing task‚Äînamely, the
2D fast Fourier transform (FFT). We
start with a simple serial Matlab code,

January/February 2009	

explore in detail a 1D parallel FFT,
and illustrate how it can be extended
to multidimensional FFTs.

Parallel Development
Environment

Figure 1 shows the system‚Äôs general
structure. The scientific programmer
works at the application layer, while
pMatlab abstracts the hardware architecture and system details. The library level abstracts the (parallel) data
and task distribution through maps.
The kernel layer is composed of math
(such as Matlab or Octave), communication (such as MatlabMPI, bcMPI, or
MPIToolBox), and cluster launch kernels. The cluster launch kernels control interactions between the desktop
and remote Matlab sessions. Although
our cluster launch kernel, gridMatlab,
is proprietary, pMatlab can run interactively on any cluster.
The core data structures in Matlab are arrays‚Äîvectors, matrices,
and higher-order arrays. To maintain
consistent functionality in serial and
parallel environments, the core data
structures in pMatlab are distributed
arrays. pMatlab creates these distributed arrays via maps, which provide
the means of rapidly transforming a
serial program into a parallel program
by specifying how the data is to be
distributed across a set of processors.
Figure 2 illustrates both the syntax for
defining maps as well as the data lay-

Copublished by the IEEE CS and the AIP	

out on the processors for the standard
distributions; block, cyclic, and block
cylic. For simplicity, we show 2D arrays in the figure, but pMatlab supports distributed arrays for up to four
dimensions. Creating the map requires
the user to specify the array dimensions to be distributed, how it‚Äôs to be
distributed, and the set of processors
over which it‚Äôs distributed. Changing
one of these parameters changes the
data distribution. The mappings in
Figure 2 clearly indicate that the map
abstraction provides an easy way to
evaluate a range of distribution strategies for a given application.

Programming in pMatlab

The programming paradigm is best
understood by example. To illustrate
the commonly used pMatlab functions, we explore an example from signal and image processing. A common
procedure in this domain is to read in
data, process it via FFTs, send the results to another stage, or collect them
for graphing or display. For simplicity, we consider a serial Matlab code
that loads the data into a 2D array,
performs an FFT on each row, and
displays the result, as in Figure 3.
Although a desktop is fast enough for
small datasets, the time to completion
becomes significant as the number and
size of FFTs increase. Clearly, we can
expedite the processing by distributing rows of data to multiple processors.

1521-9615/09/$25.00 ¬© 2009 IEEE

75

S cie n t i f ic P r o g r a m m i n g
Application

Input

Analysis

Output
Parallel function

Parallel vector/matrix

Library layer (pMatlab)
Parallel library
Messaging
(MatlabMPI)

Kernel layer
Cluster launch
(gridMatlab)

Math
(Matlab)

User
interface
Hardware
interface

Parallel hardware

Figure 1. Development environment overview. The goal of the library is to
abstract the interprocessor communication and hardware details from the user
application. The library and kernel layers provide a middleware that renders the
user application easy to develop and portable across platforms.

Map
Grid specification
together with
processor list
describe where
data are
distributed

grid: 1x2
dist: block
procs: 0:1

grid: 1x2
dist: cyclic
procs: 0:1

Cluster

Proc
0

Map

Map

grid:
dist:

1x2
blockcyclic
procs: 0:1

Cluster

Proc
1

Proc
0

Cluster

Proc
1

Proc
0

Proc
1

Distribution specification
describes how data are distributed

Figure 2. Map definitions for pMatlab. A change of data distributions requires only
a simple change to the map, making it easy for the programmer to determine the
best mapping strategy for a given application.

% basic code (serial)
% Load data into Z
inData = readInputData;
z = inData;
% Compute the FFT of each row
results = fft(z,[],1);
% display
imagesc(z);

Figure 3. Serial Matlab code. This
code segment computes fast Fourier
transforms along rows.

It‚Äôs less clear how to efficiently gather
the data for display, or even how to approach processing a multidimensional
FFT that requires distributing rows
76

first and then redistributing the data
along columns. In traditional parallel
computing, this requires hand coding
of the distribution and communication
via the use of a message-passing interface (MPI) library, generally the open
source MPICH or an architecturetuned version. For the simple case of
the 1D FFT, this method of coding
requires that the programmer determine and specify where to send each
row of data (which processor), who will
collect the results (leader), and how
each processor will send results to the
leader. In the multidimensional case,
the programmer is required to distribute the data for the row FFTs and then
redistribute the data for the column
FFTs, as well as collect the results. Although it‚Äôs easy to think about the 1D
data distribution, the implementation
details quickly become complicated,

and additional dimensions increase the
complexity. (The left-hand block of
code in Figure 4 illustrates a Matlab¬≠
MPI program to perform a 2D FFT
and provides a sense of the complexity
associated with writing the data distribution and local-global mapping code.)
By contrast, to create a pMatlab
version of the code as in Figure 5, we
map the (independent) rows of data
across processors by calling the map
function as shown in line 7. Once we
create the map, we can initialize the
distributed array, DZ, by passing a map
object to the zeros constructor. This
creates a distributed matrix of zeros.
Notionally, the dmat looks like the
colored matrix in the right-hand side
of Figure 5. Note that each color represents a separate processor.
The next step in the application is
the assignment of the data to the distributed matrix, which we perform
via the subsassign function (line
11). Each processor will perform an
FFT on a subset of the total number
of rows, and all processors execute
the same instructions. The execution
of the function local returns a copy
of the local portion of the distributed
matrix data so that processing can
begin. Through map construction,
each processor implicitly knows its
data partition and the associated indices, but often the programmer needs
these values to perform some other
task that directly references the global
indices. To provide this information,
the pMatlab function global_ind
returns a vector of global indices (for
example, row numbers) for the associated rank (processor identifier). Once
a processor has the local copy of the
data, it executes the Matlab FFT
function on the local data, myRows.
When the processing is complete,
the put_local function returns the
data to the dmat object. Note that at

Computing in Science & Engineering

FFT columns
my_rank=MPI_Comm_rank(comm);
if (my_rank==0)|(my_rank==1)|(my_rank==2)|(my_rank==3)
Xlocal=rand(M,N/4);end
if (my_rank==4)|(my_rank==5)|(my_rank==6)|(my_rank==7)
Zlocal=zeros(M/4,N);end
Xlocal=fft(Xlocal);
tag=0;
if (my_rank==0)|(my_rank==1)|(my_rank==2)|(my_rank==3)
start=1;
len=M/4;
for dest_rank=4:7
last=start+len-1;
MPI_Send(dest_rank,tag,comm,Xlocal(start:last,:));
start=last+1;
end
end
if (my_rank==4)|(my_rank==5)|(my_rank==6)|(my_rank==7)
start=1;
len=N/4;
for recv_rank=0:3
last=start+len-1;
Zlocal(:,start:last)=MPI_Recv(recv_rank,tag,comm);
start=last+1;
end
end
MatlabMPI
Zlocal=fft(Zlocal);

FFT rows

‚Ä¶

1
Corner turn

‚Ä¶

Np ‚Äì 1

1

0

0

Np ‚Äì 1
X

Z
pMatlab

X = fft(X,[],2);
Z(:,:) = X;
Z = fft(Z,[],1);

pMatlab with
transpose_grid
X = fft(X);
Z = transpose_grid(X);
Z = fft(Z);

Figure 4. Parallel two-dimensional fast Fourier transform (FFT) code. A comparison of MatlabMPI, pMatlab, and optimized
pMatlab implementations shows the level of complexity that‚Äôs abstracted away from the programmer through the
pMatlab library.

%Initialize pMatlab
pMatlab_Init;
Ncpus = pMatlab.comm_size;
%read input data
inData = readInputData;
% Create Maps ‚Äì distribute rows
map1 = map([Ncpus 1],{},0:Ncpus-1);
% Create DZ - distributed matrix.
DZ = zeros(n, m, map1);
Assign data to distributed array
DZ(:, :)= inData;
% Get the local portion and local indices
myRowNumbers = global_ind(DZ,1);
myRows = local(DZ);
%perform FFT on rows
myRows = fft(myRows,[],1);

DZ : Distributed matrix
Ncpus = number of processors = 4
Global indices

Local indices

1

1

2

2

3

1

4

2

5

1

6

2

7

1

8

2

% Copy local portion to global
DZ = put_local(DZ, myRows)
%Gather results to leader processor for display
or next processing stage ‚Äì Note: Result is a
plain MATLAB¬Æ matrix.
results = agg(DZ);
pMatlab_Finalize;

Figure 5. pMatlab version. On the left, we see code for the pMatlab fast Fourier transforms, and on the right, the annotated
distributed array.

January/February 2009

77

S cie n t i f ic P r o g r a m m i n g

Relative corner turn throughput

100
10

‚Äì1

MatlabMPI
TransposeGrid
pMatlab

10‚Äì2
10‚Äì3
10‚Äì4
10

‚Äì5

10‚Äì6
10‚Äì7
10‚Äì8

10‚Äì7

10‚Äì6

10‚Äì5

10‚Äì4

10‚Äì3

10‚Äì2

10‚Äì1

100

Relative matrix size

Figure 6. Two-dimensional fast Fourier transform performance. Compare the
results for three different approaches, MatlabMPI, pMatlab, and pMatlab using
the transposegrid function.

% RUN is a generic script for running pMatlab scripts.
% Define number of processors to use
Ncpus = 4;
% Name of the script you want to run
mFile = ‚Äòparam_sweep_parallel‚Äô;
% Define cpus

(0), which collects them in rank order.
The result of the agg command is a
plain Matlab matrix on processor 0
(the leader). By design, processor 0 is
the user‚Äôs local desktop‚Äîthe code has
been running interactively in a Matlab session on the desktop and n ‚Äì 1
processors in a remote system. Having completed the parallel processing,
we close the remote processes using
pMatlab_Finalize, leaving the result in the workspace of processor
0. In this way, we‚Äôve maintained the
interactive Matlab environment, but
the user has achieved significant acceleration of his or her workflow. In
general, such applications approach
linear speedup, although collection in
the agg function is inherently serial
and thus a bottleneck.

% Run on user‚Äôs local machine

pMatlab Code

% cpus = {};
% Specify which machines to run on
cpus = {‚Äònode-1‚Äô, ‚Äònode-2‚Äô, ‚Äònode-3‚Äô, ‚Äònode-4‚Äô};
% Abort left over jobs
MPI_Abort;
pause(2.0);
% Delete left over MPI directory
MatMPI_Delete_all;
pause(2.0);
% Define global variables
global pMATLAB;
% Run the script.
[‚ÄòRunning ‚Äò mFile ‚Äò on ‚Äò num2str(Ncpus) ‚Äò cpus‚Äô]
eval(MPI_Run(mFile, Ncpus, cpus));

Figure 7. pMatlab launch script. For a general distributed cluster, we set the
number of processors on which to run, their names, and the names of the m-file
and the machines on which the job is to be run.

this point, each processor only has
the FFT results of the local data.
The data on other processors needs
to be communicated to this processor if required for a subsequent stage
of computation or display. To aggre78

gate the data for display or the next
stage in a processing chain, the agg
command will gather the data from
all the processors. During this function‚Äôs execution, each processor sends
its local data to the leader processor

This simple code we just described
illustrates the required initialization
and finalization functions, ¬≠pMatlab
_Init and pMatlab_Finalize, as
well as the most commonly used functions, map, global_ind, local, put
_local, agg, and the method of distributed matrix creation.2 We find that
these are often the only functions required for signal- and image-processing applications in which the images
fit in a single processor‚Äôs memory.
However, in many applications, the
data doesn‚Äôt fit into the memory of a
single processor, so distributed tensor
products, matrix inverses, or multi¬≠
dimensional FFTs must be supported.
The multidimensional FFT leads to
the most communication-intensive
operation in parallel computing, the
corner turn or complete exchange
(an all-to-all communication between
processors). Building on our earlier
example, let‚Äôs consider a 2D FFT code
segment. The code, shown in Figure
4, is very similar to that in Figure 5;

Computing in Science & Engineering

the initialization steps are the same,
but now there are two maps, one for
the row-wise FFT and one for the column FFT.
Here, we illustrate three approaches
to creating the parallel Matlab code:
using MatlabMPI, which requires
the programmer to compute the assignment by hand, using pMatlab to
remap the data distribution between
the row and column distributions,
and using a specialized pMatlab function, ¬≠transpose_grid, to optimally
remap the data between the two FFT
stages. We include the MatlabMPI
code to provide some sense of the level
of complexity involved in the corner
turn or remapping. Figure 6 plots the
speedup achieved in each approach.
Clearly, the optimized pMatlab version compares well with the MatlabMPI results for significantly less coding
effort, resulting in a performance and
productivity win.
We can run the pMatlab code on
any distributed cluster that has the
¬≠pMatlab and MatlabMPI libraries.
Figure 7 shows the basic run script,
which requires setting the number of
processors on which to run, the names
of the processors, and the name of the
m-file. The additional commands for
launch control clean up old jobs and
prepare for the new one.

C

urrent work involves adding
Octave into the kernel layer to
produce an open source solution as
well as expanding the suite of Matlabcompatible message-passing libraries.
We‚Äôve also begun work on developing
parallel Web services that have pMatlab at their core. For more information on pMatlab and MatlabMPI, visit
www.ll.mit.edu/pMatlab and www.
ll.mit.edu/MatlabMPI. Note that
MatlabMPI is included in the pMat-

January/February 2009

lab suite of software, along with the
libraries and example code.
Acknowledgments
This work is sponsored by the US Air
Force under Air Force contract FA872105-C-0002. Opinions, interpretations,
conclusions, and recommendations are
those of the authors and are not necessarily endorsed by the US government.

References
1.	 N.T. Bliss et al., ‚ÄúInteractive Grid Computing
at Lincoln Laboratory,‚Äù Lincoln Laboratory J.,
vol. 16, no. 1, 2006, pp 165‚Äì216.
2.	 H. Kim and N. Travinin, pMatlab Function Reference, MIT Lincoln Laboratory, 2005; www.
ll.mit.edu/mission/isr/pmatlab/pMatlab_v0.7
_func_ref.pdf.

Julie Mullen is a technical staff subcontractor at MIT Lincoln Laboratory. Her research
interests include parallel and distributed
computing, and high-productivity software
tools for engineering applications. Mullen
has a PhD in engineering from Brown University. Contact her at jsm@ll.mit.edu.
Nadya Bliss is a technical staff member
at MIT Lincoln Laboratory. Her research
interests include parallel and distributed
computing‚Äîspecifically, program analysis
and optimization, intelligent/cognitive algorithms, and software/hardware co-design
methodology. Bliss has an MS in computer
science from Cornell University. Contact her
at nt@ll.mit.edu.

Robert Bond is the leader of the Embedded
Digital Systems group at MIT Lincoln Laboratory. His research interests include research
and development of high-performance embedded processors, advanced signal proc¬≠
essing, and novel embedded middleware
architectures. Bond has a BS in physics from
Queen‚Äôs University. Contact him at rbond@
ll.mit.edu.
Jeremy Kepner is a senior staff member at
MIT Lincoln Laboratory. His research interests include advanced libraries for the application of massively parallel computing to
a variety of data-intensive signal-processing
problems. Kepner has a PhD in astrophysics
from Princeton University. Contact him at
kepner@ll.mit.edu.
Hahn Kim is an associate staff member at
the MIT Lincoln Laboratory. His research interests are in high-performance embedded
systems for signal processing and high productivity technologies for parallel computing. Kim has an MS in computer science and
engineering from the University of Michigan.
Contact him at hgk@ll.mit.edu.
Albert Reuther is a technical staff member
at MIT Lincoln Laboratory. His research interests include rapid prototyping and signal
processing using high-performance computing and the economics of high-performance
computing. Reuther has a PhD in electrical
and computer engineering from Purdue University. Contact him at reuther@ll.mit.edu.

IEEE Computer Society Members

Save 25%

on all conferences sponsored
by the IEEE Computer Society

www.computer.org/join
79

IN SUBMISSION TO THE IEEE, ‚Äì ‚Äì ‚Äì 2014

1

A Spectral Framework for Anomalous Subgraph
Detection

arXiv:1401.7702v2 [cs.SI] 22 Oct 2014

Benjamin A. Miller, Member, IEEE, Michelle S. Beard, Patrick J. Wolfe, Senior Member, IEEE, and
Nadya T. Bliss Senior Member, IEEE

Abstract‚ÄîA wide variety of application domains are concerned with data consisting of entities and their relationships
or connections, formally represented as graphs. Within these
diverse application areas, a common problem of interest is the
detection of a subset of entities whose connectivity is anomalous
with respect to the rest of the data. While the detection of
such anomalous subgraphs has received a substantial amount of
attention, no application-agnostic framework exists for analysis
of signal detectability in graph-based data. In this paper, we
describe a framework that enables such analysis using the
principal eigenspace of a graph‚Äôs residuals matrix, commonly
called the modularity matrix in community detection. Leveraging
this analytical tool, we show that the framework has a natural
power metric in the spectral norm of the anomalous subgraph‚Äôs
adjacency matrix (signal power) and of the background graph‚Äôs
residuals matrix (noise power). We propose several algorithms
based on spectral properties of the residuals matrix, with more
computationally expensive techniques providing greater detection
power. Detection and identification performance are presented for
a number of signal and noise models, including clusters and bipartite foregrounds embedded into simple random backgrounds
as well as graphs with community structure and realistic degree
distributions. The trends observed verify intuition gleaned from
other signal processing areas, such as greater detection power
when the signal is embedded within a less active portion of
the background. We demonstrate the utility of the proposed
techniques in detecting small, highly anomalous subgraphs in real
graphs derived from Internet traffic and product co-purchases.
Index Terms‚ÄîGraph theory, signal detection theory, spectral
analysis, residuals analysis, principal components analysis

I. I NTRODUCTION

I

N numerous applications, the data of interest consist of
entities and the relationships between them. In social network analysis, for example, the data are connections between
individuals, such as who knows whom personally, who is in the
same organization, or who is connected on a social networking
website. In computer networks, we are often interested in
which computers communicate with one another. In the natural
sciences, we may want to know which chemicals interact in a
This work is sponsored by the Assistant Secretary of Defense for Research
& Engineering under Air Force Contract FA8721-05-C-0002. Opinions,
interpretations, conclusions and recommendations are those of the authors
and are not necessarily endorsed by the United States Government.
B. A. Miller is with Lincoln Laboratory, Massachusetts Institute of Technology, Lexington, MA, 02420 USA (e-mail: bamiller@ll.mit.edu).
M. S. Beard is with Charles Stark Draper Laboratory, Cambridge, MA,
02139 USA (email: mbeard@draper.com).
P. J. Wolfe is with the Department of Statistical Science, University College
London, London WC1E 6BT UK (e-mail: p.wolfe@ucl.ac.uk).
N. T. Bliss is with Arizona State University, Tempe, AZ, 85287 USA (email: nadya.bliss@asu.edu).

reaction. Across these varied domains, data regarding connections, relationships and interactions between discrete entities
enhances situational awareness and diversifies by incorporating
additional contextual information.
When working with relational data, it is common to formally
represent the relationships as a graph. A graph G = (V, E)
is a pair of sets: a set of vertices, V , comprising the entities,
and a set of edges, E, denoting relationships between them.
Graph theory provides an abstract mathematical object that has
been applied in all of the above contexts. Indeed, graphs have
been used to model protein interactions [1] and to represent
communication between computers [2]. Graphs‚Äîcommonly
called networks in practice‚Äîare used extensively in social
network analysis, with many graph algorithms focused on
detection of communities [3], [4] and influential figures [5].
As a data structure, graphs have long been utilized by
signal processing practitioners. Analysis of graphs derived
from radio frequency or image data is common, as a graph
structure can help classify similar measurements (see, e.g.,
[6]). Recent research has also defined traditional digital signal
processing kernels‚Äîsuch as filtering and Fourier transforms‚Äî
for signals that propagate along edges in a graph [7], [8].
When the graph comprises the data itself, rather than a
means of storage, significant complications arise. Graphs are
discrete, combinatorial structures, and, thus, they lack the
convenient mathematical context of Euclidean vector spaces.
The ability to perform linear transformations and the analytical
tractability of working with Gaussian noise are not available
in general when working with relational data. Deriving an
optimal detector for a small signal subgraph buried within a
large network, then, becomes potentially intractable, as it may
require the solution to an NP-hard problem.
Despite these complications, it is desirable to understand
notions of detectability of small subgraphs embedded within
a large background. The ability to detect small signals in
these contexts would be useful in many domains, from the
detection of malicious traffic in a computer network to the
discovery of threatening activity in a social network. Recent
work in this area has considered subgraph detection from a
variety of perspectives. Work has been done on detection of
specific target subgraphs in random backgrounds [9], with
special attention paid in the computer science and statistics
communities to planted cliques [10], [11] and planted clusters
[12], [13]. Other work assumes common substructures over
the graph, and detects anomalies based on deviations from the
‚Äúnormative pattern‚Äù via methods such as minimum description
length [14] or analysis of the graph Laplacian [15]. Techniques

2

IN SUBMISSION TO THE IEEE, ‚Äì ‚Äì ‚Äì 2014

such as threat propagation [16], [17] and vertex nomination
[18] consider a cue vertex as a knowledge prior, giving an
initial indication of which vertices are of interest, the objective
then being to find the remainder of the subgraph. Community
detection in graphs is a widely studied related problem [19],
where the communities in the graph are sometimes cast as
deviations from a null hypothesis in which the graph has no
community structure [20].
The objective of the present contribution is to develop a
broadly applicable detection framework for graph-based data.
To apply in these varied domains, this framework should be
independent of the specific application. We focus specifically
on the uncued anomalous subgraph detection problem, where
the goal is to detect the presence of a subgraph that is a
statistical outlier without a ‚Äútip‚Äù vertex provided as a cue. As
graphs of interest are often extremely large, the framework
should have favorable scaling properties as the number of
vertices and edges grow. To gain insight into properties that
influence subgraph detectability, the framework will ideally
have a natural metric for signal and noise power, to enable
discussion of quantities like signal-to-noise ratio that are
intrinsic to signal processing applications.
In this paper, we present a spectral framework to address
the uncued subgraph detection. This framework is based on a
regression-style analysis of residuals, in which an observed
random graph is compared to its expected value to find
outliers. We analyze the graph in the space of the principal
eigenvectors of its residuals matrix, which offers two advantages: it allows us to use results from spectral graph theory
to elucidate the notion of subgraph detectability, and works
within in a linear algebraic framework with which many signal
processing researchers are familiar. Within this framework, the
spectral norm provides a good metric for signal and noise
power, as we demonstrate analytically and empirically. This
framework also enables the development of algorithms that
work in a low-dimensional space to detect small anomalies,
several of which are discussed in this paper.
The remainder of this paper is organized as follows. In
Section II, we formally define the subgraph detection problem. Section III provides a brief summary of related work
on subgraph detection and graph residuals analysis. Section
IV details our proposed subgraph detection framework. In
Section V, we outline several algorithms for anomaly detection
within the framework. Section VI presents detection results for
several simulated datasets, and in Section VII we demonstrate
these techniques on real datasets. Finally, in Section VIII, we
summarize and discuss open problems and ongoing work.
II. P ROBLEM M ODEL
A. Definitions and Notation
In the subgraph detection problem, the observation is a
graph G = (V, E). We will denote the sizes of the vertex
and edge sets as N = |V | and M = |E|, respectively. A
subgraph GS = (VS , ES ) of G is a graph in which VS ‚äÇ V
and ES ‚äÇ E ‚à© (VS √ó VS ), where the Cartesian product V √ó V
is the set of all possible edges in a graph with vertex set V . In
this paper, we consider graphs whose edges are unweighted

and undirected. We will allow the possibility of self-loops,
meaning an edge may connect vertex to itself. Since edges
have no weight, two graphs will be combined via their union.
The union of two graphs, G1 = (V1 , E1 ) and G2 = (V2 , E2 ),
is defined as G1 ‚à™ G2 = (V1 ‚à™ V2 , E1 ‚à™ E2 ).
Working in a spectral framework, we will make use of
matrix representations for graphs. The adjacency matrix A =
{aij } of G is a binary N √ó N matrix. Each row and column
is associated with a vertex in V . This implies an arbitrary
ordering of the vertices with integers from 1 to N , and we
will denote the ith vertex vi . Then aij is 1 if there is an
edge connecting vi and vj , and is 0 otherwise. Similarly, let
AS = {sij } be the adjacency matrix for the signal subgraph.
Since we consider undirected graphs, A and AS are symmetric.
Matrix norms will also be used in the discussion of signal and
noise power. Unless otherwise noted, the matrix norm will be
the spectral norm, i.e., the induced L2 norm,
kAk = max kAxk2 ,
kxk2 =1

(1)

which is equivalent to the absolute value of the largestmagnitude eigenvalue of the matrix.
Our framework is focused on detection of signals within a
random background. The analysis presented in this paper is
based on the assumption of Bernoulli random graphs, where
the probability of an edge between vi and vj is a Bernoulli
random variable with expected value pij . Note that the edge
probabilities may be different for all pairs of vertices. Since
the presence of each edge is a Bernoulli random variable, the
expected value of A is given by P = {pij }. We refer to P as
the probability matrix of the graph.
Another important notion when dealing with graphs is
degree. A vertex‚Äôs degree is the number of edges adjacent to
the vertex. The observed degree of vertex vi will be denoted
ki , and its P
expected degree is denoted
E [ki ] = di . Note
PN
N
1
that ki =
a
and
d
=
p
. The vectors of
ij
i
ij
j=1
j=1
the observed and expected degrees will be denoted k and d,
respectively. The volume of the graph, Vol(G), is the sum of
the degrees over all vertices.
B. The Subgraph Detection Problem
In some cases, the observed graph G will consist of only
typical background activity. This is the ‚Äúnoise only‚Äù scenario.
In other cases, most of G exhibits typical behavior, but a
small subgraph has an anomalous topology. This is the ‚Äúsignalplus-noise‚Äù scenario. In this case, the noise graph, denoted
GN = (VN , EN ), and the signal subgraph, GS = (VS , ES )
are combined via union.
The objective, given the observation G, is to discriminate
between the two scenarios. Formally, we want to resolve the
following binary hypothesis test:
(
H0 : G = GN
(2)
H1 : G = GN ‚à™ GS .
Thus, we have the classical signal detection problem: under
the null hypothesis H0 , the observation is purely noise, while
1 Using

this convention, a self-loop only increases a vertex‚Äôs degree by 1.

MILLER et al.: A SPECTRAL FRAMEWORK FOR ANOMALOUS SUBGRAPH DETECTION

under the alternative hypothesis H1 , a signal is also present.
Here GN and GS are both random graphs, with GN drawn
from the noise distribution and GS drawn from the signal
distribution. We will only consider cases in which the vertex
set of the signal subgraph is a subset of the vertices in the
background, i.e., VS ‚äÇ VN = V .

3

Theorem 2. For an observed graph G = (V, E), let X be a
subset of V of size NS , and EX ‚äÇ E be the set of all edges
existing between the vertices in X. The likelihood ratio for
resolving the hypothesis test in (4) is given by

 NS
|E |
 ‚àí1 
pÃÇ(1 ‚àí p) X
N
1 ‚àí pÃÇ ( 2 ) X
, (5)
1‚àíp
p(1 ‚àí pÃÇ)
NS
X‚äÇV
|X|=NS

III. R ELATED W ORK
While there are many flavors of subgraph detection research,
not all of them work under the same assumptions as in this
paper. For example, we consider a variety of noise models,
which may not have the ‚Äúnormative pattern‚Äù required to use
techniques based on common subgraphs [14], [15]. Research
into anomaly detection in dynamic graphs by Priebe et al. [21]
uses the history of a node‚Äôs neighborhood to detect anomalous
behavior, but this would not apply in the case of static graphs,
which is the focus of this work. As our interest is in uncued
techniques, we operate in a different context from the work in
[16]‚Äì[18]. These methods are complementary to the techniques
outlined in this paper, as a set of outlier vertices could be used
to seed a cued algorithm and do further exploration.
Previous work has considered optimal detection in the same
context we consider in this paper, though in a restricted
setting. In [9], the authors consider the detection of a specific
foreground embedded (via union) into a large graph in which
each possible edge occurs with equal probability (i.e., the
random graph model of ErdoÃãs and ReÃÅnyi). In this setting, the
likelihood ratio can be written in closed form, as demonstrated
by the following theorem.
Theorem 1 (Mifflin et al. [9]). Let G denote the random
graph where each possible edge occurs with equal probability
p, and let H denote the target graph. The likelihood ratio of
an observed graph J is
ŒõH (J) =

XH (J)
.
E [XH (G)]

(3)

Here XH (¬∑) denotes the number of occurrences of H in the
graph. The applicability of this result, therefore, requires a
tractable way to count all subgraphs of the observation J that
are isomorphic with the target. This is NP-hard in general [22],
although there may be feasible methods to accomplish this for
certain targets within sparse backgrounds.
While the previous example requires a complicated procedure, detection of random subgraphs embedded into random backgrounds may be an even harder problem. Take, for
example, the detection problem where the background and
foreground are both ErdoÃãs‚ÄìReÃÅnyi, i.e., when the null and
alternative hypotheses are given by
Ô£±
H0 : each pair of vertices shares an edge with
Ô£¥
Ô£¥
Ô£¥
Ô£≤
probability p
(4)
Ô£¥
H1 : an NS -vertex subgraph was embedded whose
Ô£¥
Ô£¥
Ô£≥
edges were generated with probability pS .
In this situation, we can derive an optimal detection statistic.

where pÃÇ = p + pS ‚àí ppS .
A proof of Theorem 2 is provided in Appendix A. Even in
this relatively simple scenario, computing the likelihood ratio
in (5) requires, at least, knowing how many NS -vertex induced
subgraphs contain each possible number of edges. In [12], it is
shown that some computable tests asymptotically achieve the
information-theoretic bound for dense backgrounds, but there
are no known polynomial-time algorithms that achieve the
bound in a sparse graph [13]. For more complicated models,
calculating the optimal detection statistic is likely to be even
more difficult.
The subgraph detection framework presented in this paper is
based on graph residuals analysis. The residuals of a random
graph are the difference between the observed graph and
its expected value2 . For a random graph G, we analyze its
residuals matrix
B := A ‚àí E [A] .
(6)
In the area of community detection, a widely used quantity to
evaluate the quality of separation of a graph into communities
is modularity, defined in [20]. The modularity of a partition
C = {C1 , ¬∑ ¬∑ ¬∑ , Cn } is defined as
Q=

n
X

(eii ‚àí a2i ),

(7)

i=1

where Ci are disjoint subsets of V covering the entire set, eii
is the proportion of edges entirely within Ci , and ai is the
proportion of edge connections in Ci , i.e.,
ai =

n
X

eij ,

(8)

j=1

with eij denoting half the number of edges between Ci and
Cj for i 6= j (half to prevent from counting the edge in both
eij and eji ). Note that a2i is the expected proportion of edges
within Ci if the edges were randomly rewired (i.e., the degree
of each vertex is preserved, but edges are cut and reconnected
at random). Indeed, if the edge proportions are the only thing
maintained in the rewiring, the fraction of edges from any
community that connect to a vertex in Ci will be ai . Thus,
the proportion of the total edges from Ci to Cj will be ai aj .
Taken as an analysis of deviations from an expected topology,
modularity is a residuals-based quantity.
In the community detection literature, numerous algorithms
exist to maximize Q for a given number of communities. In
[3], an algorithm is proposed by casting modularity maximization as optimization of a vector with respect to a matrix.
2 This is distinct, it should be noted, from the notion of residual networks
when computing network flow [22].

4

IN SUBMISSION TO THE IEEE, ‚Äì ‚Äì ‚Äì 2014

The modularity matrix B is given as the observed minus the
expected adjacency matrices, i.e., a matrix of the form in (6).
To divide the graph into two partitions in which modularity is
maximized, we can solve


1
T
T
kk
s,
(9)
sÃÇ = arg max s
A‚àí
Vol(G)
s‚àà{‚àí1,1}N
and declare the vertices corresponding to the positive entries of
sÃÇ to be in one community, with the negative entries indicating
the other. This technique will optimize Q for a partition into
2 communities. Since this is a hard problem, it is suggested
that the principal eigenvector of
B =A‚àí

1
kk T
Vol(G)

(10)

is computed‚Äîthereby relaxing the problem into the real
numbers‚Äîwith the same strategy of discriminating based on
the sign of eigenvector components used to divide the graph
into communities.
This is an example of a community detection algorithm
based on spectral properties of a graph, which have inspired a
significant amount of work in the detection of communities [3],
[23]‚Äì[25] and global anomalies [2], [26], [27]. In this paper,
we leverage these same properties within a novel framework
for detection of small subgraphs whose behavior is distinct
from background activity.
IV. D ETECTION F RAMEWORK
A. Framework Overview
The subgraph detection framework we propose is based on
the analysis of graph residuals, as expressed by (6). We may
be given E[A], or it may be estimated from the observed data.
This is similar to analysis of variance in linear regression:
We compare the observed data to its expectation, and if the
deviations from the expected value are not consistent with
variations due to noise, then this may indicate the presence of
a signal (in this case an anomalous subgraph).
To reduce the dimensionality of the problem, this framework
deals with a graph‚Äôs spectral properties. Using the principal
components of the residuals matrix, we can consider a graph in
the linear subspace in which its residuals are largest. For some
established models, there is also theory regarding the eigenvalues and eigenvectors of these matrices [28]. This technique is
used in community detection, and is similar to models in which
each vertex has a position in a latent Euclidean space (see, e.g.,
[29]). The presence of certain anomalous subgraphs will alter
the projection of a graph into this Euclidean residuals space.
Working within this space, we can compute test statistics and,
from these, resolve the hypothesis test (2). While these will
not be optimal detection statistics as in Theorems 1 and 2, this
framework can be applied to a wide variety of random graph
models, is computationally tractable, and, as we demonstrate in
subsequent sections, is quite useful for resolving the subgraph
detection problem in a variety of scenarios.
We use the modularity matrix from (9) as our baseline
residuals model. This has several advantages. First, the ‚Äúgiven
expected degree‚Äù model has been well studied, and we know

properties of its eigenvalues and eigenvectors [30]. Second,
the model‚Äôs expected value term is low-rank, which allows
easy computation of the eigenvectors of B without computing
a dense N √ó N matrix (as noted in [3] and described in
[31]). This makes the model computationally tractable for
large graphs where algorithms more expensive than O(M ) can
be prohibitive. This model also has a simple fitting procedure.
Estimating the expected degree as simply the observed degree
is, in fact, the maximum likelihood estimator for the version
of this model where each possible edge is a Poisson random
variable [32]. For small edge probabilities, this is a good
approximation for Bernoulli random variables. Finally, this
model has demonstrated utility for inter-community behavior;
i.e., the probability of connections between vertices in different
communities seems to follow such a model (the reason that
observed degree was added as a covariate in [33]).
B. Power Metrics
As mentioned previously, one important aspect of a signal
processing framework is a metric for signal and noise power.
This provides a quantity that enables an intuitive assessment
of the detectability of a signal in a given background. Again,
vector signals with Gaussian noise provide an intuitive metric
based on vector norms, while such quantities are less clear in
the context of random graphs.
There are several intuitive quantities that could be used for
signal or noise power in the context of random graphs. One
natural choice would be number of edges, or perhaps average
degree. It seems intuitive that a signal graph with a large
number of edges would be easier to detect, and that greater
variance in the number of edges in the background would make
this more difficult. A related linear algebraic quantity would
be the Frobenius norm of the residuals matrix, i.e., the sum of
the squared residuals over all ordered pairs of vertices. This
would consider each edge probability separately, emphasizing
the presence of less-likely edges.
These metrics, however, have a few shortcomings. In both
cases, the signal power measurement will be exactly the same
for any subgraph with the same number of edges. Consider
two different trees: a path, in which each edge can be traversed
while visiting each vertex exactly once; and a star, where one
vertex is connected to all others. Both will have NS ‚àí1 edges,
and a Frobenius norm of 2(NS ‚àí1). The star, however, is much
more concentrated on one vertex, and this will cause it to stand
out more in the eigenspace (it is also much less likely to occur
by chance if edges are randomly placed). The power metric
we use should provide an indication of a subgraph‚Äôs likelihood
to stand apart from the background in the eigenspace, since
this is the space in which we consider the data.
Working within a spectral framework, the spectral norm defined in (1) provides a natural power metric. Using kA‚àíE[A]k
as a metric for noise power, and kAS k as a metric for signal
power, we can determine the detectability of a subgraph in
principal eigenspace. To see this, we first define a new matrix,
b = {aÃÇij }, which is the adjacency matrix of G
b = (V, ES \E),
A
i.e., the edges of the anomaly that do not appear in the
background. For deterministic foreground graphs, if sij is 1,

MILLER et al.: A SPECTRAL FRAMEWORK FOR ANOMALOUS SUBGRAPH DETECTION

0

10

High Edge Prob.
Low Edge Prob.
BS=0

‚àí2

2

10

1‚àí||us||2

then aÃÇij is a random variable whose value is 1 with probability
1 ‚àí pij and 0 with probability pij . For a random Bernoulli
foreground, if E[sij ] = qij , then aÃÇij is 1 with probability
qij (1 ‚àí pij ). Thus, when the subgraph is embedded within
b ‚âà E[AS ]. For
vertices where the interaction level is low, E[A]
convenience, we will also denote a partition of the residuals
matrix as


BS BSN
B=
,
(11)
T
BSN
BN

5

‚àí4

10

‚àí6

10

‚àí8

10

‚àí10

10

‚àí10

10

‚àí8

10

‚àí6

10

‚àí4

1‚àíŒ¥

10

‚àí2

10

0

10

min

where the rows and columns have been permuted so that the
subgraph vertices are those with the smallest indices. The
submatrix BS is the background residuals within the subgraph
vertices, BSN is the residuals occurring between the subgraph
and the rest of the graph, and BN includes only the residuals
within the complement of the subgraph vertices.
If the spectral norm of the signal subgraph is sufficiently
large with respect to the background power, the subgraph will
dominate the principal eigenvector of the residuals matrix.
This is captured in the following theorem, a proof of which
is provided in Appendix B.
Theorem 3. Let B be the residuals matrix of a graph drawn
b be the
from an arbitrary Bernoulli graph process, and A
adjacency matrix of the subgraph that does not include edges
in the background graph. If u is the unit eigenvector associated
b (the residuals
with the largest positive eigenvalue of B + A
b > kBN k+kBS k,
matrix after embedding), then assuming kAk
the components of u associated with only the signal vertices,
denoted uS , is bounded below as kuS k22 ‚â• 1 ‚àí Œµ where
Ô£∂
Ô£´

b ‚àí kBN k (kBS k + kBSN k) + kBSN k2
k
Ak
Ô£∑
Ô£¨
Œµ = OÔ£≠
Ô£∏.

2
b ‚àí kBN k + kBSN k2
kAk
(12)
Consider the implication of Theorem 3 for a fixed background, when embedding on a fixed subset of vertices. The
theorem states that as the difference between the signal power
and the power of the noise among the non-signal vertices
b ‚àí kBN k) becomes much larger than the noise power
(kAk
involving subgraph vertices (kBS k + kBSN k), the principal
eigenvector will become concentrated on the foreground vertices. A few aspects of this theorem confirm intuition from
other signal processing areas. First, if there is significant noise
b may be signifactivity within the subgraph vertices, then kAk
icantly smaller than kAS k, and kBS k may be relatively large.
This means that a signal placed in strong noise will be difficult
to detect, which is always the case in detection problems.
Also, the bound in the theorem shows that if a relatively
strong subgraph is embedded where there is typically very
little activity, and where there is relatively little interaction
with the remainder of the graph (i.e., small kBS k and kBSN k),
the subgraph will be much easier to detect. Put in traditional
signal processing language, the signal will be much easier to
detect when it is less correlated with the noise. Working within
this framework, we see the same properties of the interaction
between signal and noise that affect detectability in domains
like radar and communications.

Fig. 1. Empirical comparison to bound in Theorem 3. The bound holds
for each case in this scenario with a 4096-vertex random background and
a 15-vertex dense signal subgraph, though it is only tight for cases where
BS = 0.

An empirical example is provided in Fig. 1. In this case, a
4096-vertex ErdoÃãs‚ÄìReÃÅnyi graph (see Section VI-A1) is generated, with a 15-vertex subgraph with 90% edge probability
embedded. The horizontal axis is 1 ‚àí Œ¥min , where Œ¥min is the
expression in (35) in Appendix B. The bound holds for all
cases considered, and the empirical results often are an order
of magnitude below the maximum for both the higher and
lower edge probabilities (p = 4 √ó 10‚àí4 and p = 1 √ó 10‚àí6 ,
respectively). Only when a case is considered where there is
no background connectivity within the subgraph vertices is the
bound approached more closely.
V. D ETECTION A LGORITHMS
For relatively large subgraph anomalies, a simple ‚Äúenergy
detector‚Äù based on the spectral norm of the residuals matrix
will provide good detection performance. It is desirable, however, to be able to detect much smaller subgraphs, which may
not stand out in the principal eigenvector. A few techniques
have been developed within this framework to detect subtler
anomalies [34]‚Äì[37], which we outline in this section.
A. Chi-Squared Statistic in Principal Components
The first algorithm is based on the symmetry of the
projection of B into its 2 principal components. This will
enable the detection of subgraphs that do not stand out in
the first eigenvector. We have empirically observed for several
random graph models that, when projecting the residuals into
their principal two components, the result is rather radially
symmetric. For sparse graphs, the entries in the principal
eigenvectors resemble a Laplace distribution, as shown on
the left in Fig. 2, which is consistent with behavior observed
in sparse ErdoÃãs‚ÄìReÃÅnyi graphs. The righthand plot in Fig. 2
demonstrates the symmetry of the residuals in the top two
eigenvectors.
When an anomaly is embedded within the graph, as previously discussed, the subgraph vertices will stand apart from
the background. Therefore, we compute a statistic that is
based on symmetry in this space to detect the presence of
an anomaly. The detection statistic is a chi-squared statistic
based on a 2 √ó 2 contingency table, where the table contains

IN SUBMISSION TO THE IEEE, ‚Äì ‚Äì ‚Äì 2014

.25

0.3
0.2
0.1
0

‚àí0.1
‚àí0.2

0.125

‚àí0.05

0

0.05

Component Value

0.1

‚àí0.4
‚àí0.4

‚àí0.2

0

0.2

35
30
25

First Eigenvector

the number of vertices projected into each quadrant of the twodimensional space. (That is, the number of rows of [u1 , u2 ],
where u1 and u2 are (column) eigenvectors of B, fall into each
quadrant.) This yields a 2√ó2 matrix O = {oij } of the observed
numbers of points in each section. From the observation, we
compute the expected number of points under the assumption
of independence, OÃÑ = {oÃÑij }, where
(oi1 + oi2 )(o1j + o2j )
.
N
The chi-squared statistic is then calculated as
X X (oij ‚àí oÃÑij )2
œá2 ([u1 u2 ]) =
,
oÃÑij
i
j

With Embedding
¬µ ¬± 3œÉ
20

40

60

80

Eigenvector Index

100

Sample Distribution
Gumbel Distribution

0.05
0.04
0.03
0.02
0.01
0

2

4

6

Test Statistic Value

0.4

Fig. 2.
Distributions of vertex components in principal eigenvectors: a
histogram of components in the first eigenvector (left), with a comparison to
a Laplace distribution, and a scatterplot (right) in the principal 2-dimensional
subspace, demonstrating its radial symmetry.

oÃÑij =

40

20
0

‚àí0.3

0
‚àí0.1

45

Sample Proportion

0.375

0.4

Eigenvector L1 Norm

0.5

Empirical Distribution
Laplace Distribution

Second Eigenvector

Sample Proportion

6

(13)

(14)

and, to favor radial symmetry, we maximize the statistic over
rotation in the plane, computing
!

T
cos Œ∏ ‚àí sin Œ∏
2
2
œámax = max œá
[u1 u2 ] .
(15)
sin Œ∏
cos Œ∏
Œ∏

Fig. 3. An example of using eigenvector L1 norms for subgraph detection.
When a small, dense subgraph is embedded into a background with a skewed
degree distribution, the L1 norm of one of the eigenvectors of the residuals
matrix becomes much smaller than usual, as shown on the left. Under the
null hypothesis, the largest negative deviation from the mean will resemble a
Gumbel distribution, plotted on the right.

than for P
the background alone. The L1 norm of a vector x,
kxk1 = i |xi |, is much smaller when it is concentrated on a
small subset of entries, provided that it is unit-normalized in
an L2 sense. For this reason, the L1 norm serves as a proxy
for sparsity in applications such as compressed sensing [38].
The following algorithm enables detection when an eigenvector is concentrated on the vertices of the subgraph. This
will occur when, for example, a dense subgraph is embedded
on relatively low degree vertices, as discussed in Appendix C.
We compute the eigenvectors corresponding to the m largest
eigenvalues. By measuring cases with no embedding present,
we obtain the mean ¬µi and standard deviation œÉi for the L1
norm of the ith eigenvector. For each of the eigenvectors ui ,
1 ‚â§ i ‚â§ m, we subtract the mean and normalize by the
standard deviation. The smallest (i.e., most negative) value is
then used as a test statistic, since we are interested in cases
where the norm is small. The test statistic is given by
‚àí min

1‚â§i‚â§m

The statistic œá2max is used to detect an anomalous subgraph.
When the spectral norm is a reliable detection statistic,
thresholding along the principal eigenvalue is often an effective method to identify the vertices that are exhibiting
anomalous behavior. Working in multiple dimensions, while it
enables the detection of smaller subgraphs, makes the process
of identification more complicated. In this setting, we use a
method based on k-means clustering to identify the subgraph
vertices. Within the 2-dimensional space, we compute a small
number of clusters, and declare the smallest cluster with at
least a minimum number of vertices to be the signal subgraph.
B. Eigenvector L1 Norms
It is also desirable to detect signal subgraphs that do not
stand out in the principal two components of the residuals
matrix, and extending the algorithm of Section V-A to an
arbitrary number of dimensions may not be feasible. One
method to detect such anomalies relies on the subgraphs being
separable in the space of a single eigenvector. As mentioned
previously, the entries in the eigenvectors of the background
alone resemble numbers drawn from a Laplace distribution.
Thus, if a subgraph were to stand out in a single eigenvector,
that eigenvector will have a substantially smaller L1 norm

kui k1 ‚àí ¬µi
.
œÉi

(16)

An example demonstrating this method is provided in Fig. 3.
The example uses a 4096-vertex graph with a skewed degree
distribution (using the CL model described in Section VI-A2),
with a 15-vertex subgraph with average degree 10.5 randomly
embedded into the background. The analysis is run on the 100
eigenvectors associated with the largest positive eigenvalues.
While the L1 norms of most eigenvectors in the resulting
matrix fall within 3 standard deviations of the mean for their
index, the L1 norm of the 6th eigenvector is over 10 standard
deviations below the mean, which is extremely unlikely to
occur under the null hypothesis. Under the null hypothesis,
the test statistic (16) will resemble a Gumbel distribution
(commonly used to model extreme values), as shown in the
plot on the right. When an embedding occurs that creates a
deviation as large as that in the lefthand plot, it will take
on a value much larger than the maximum under normal
circumstances.
The occurrence of tightly connected subgraphs highly
aligned with eigenvectors was documented independently in
[39], and a similar anomaly detection method using eigenvector kurtosis in [40]. Here, we use this phenomenon to find
subgraphs whose internal connectivity is much larger than the
expectation, given the background model. When an anomaly

MILLER et al.: A SPECTRAL FRAMEWORK FOR ANOMALOUS SUBGRAPH DETECTION

is detected according to (16), the corresponding eigenvector is
thresholded to determine the subgraph vertices.
C. Sparse Principal Component Analysis
While analysis of eigenvector L1 norms enables the detection of some subgraphs that do not separate in the principal
components of the residuals space, this technique has some
shortcomings. In particular, as consecutive eigenvalues get
closer together, the stability of the direction of the eigenvectors
becomes unstable. Therefore, we cannot rely on the test
statistic being sufficiently changed because an eigenvector
points in the direction of the subgraph.
There is, however, a similar technique that enables the
detection of small subgraphs with large residuals. Rather than
first computing the eigenvectors of the residuals matrix and
then finding an eigenvector with a small L1 norm, we can
find a vector that is nearly an eigenvector whose L1 norm
is constrained. This is a technique known as sparse principal
component analysis (sparse PCA) [41]. This method has been
used in the statistics literature to find high variance in the
space of a limited number of variables. We utilize it here for
a similar goal: to find large residuals in the space of a small
number of vertices.
The problem is formulated as follows. The goal is to find a
vector that is projected substantially onto itself by the residuals
matrix, but with few nonzero components. Put formally, the
objective is to solve
xÃÇ = arg max xT Bx

(17)

kxk2 =1

subject to kxk0 ‚â§ NS ,
where k¬∑k0 denotes the L0 quasi-norm (the number of nonzero
components in a vector). This, however, is an integer programming problem, and is NP-hard. We therefore use a relaxation
with an L1 constraint, recast as a penalized optimization:
xÃÇ = arg max xT Bx ‚àí Œªkxk1 .

(18)

kxk2 =1

This problem is still not in an easily solvable form, due to the
quadratic equality constraint. We use an additional relaxation,
following the method of [41], to achieve a semidefinite program that can be solved using well-documented techniques:
b = arg max tr(BX) ‚àí Œª1T |X|1
X

(19)

X‚ààSn

subject to tr(X) = 1,
where tr(¬∑) denotes the matrix trace and Sn is the set of
positive semidefinite matrices in Rn√ón . The principal eigenb denoted xÃÇ, is then returned (and should be
vector of X,
sparse, given the constraints). The subgraph detection statistic
is kxÃÇk1 . If no small subgraph has sufficiently large residuals,
the vector should be relatively diffuse, and have a relatively
large L1 norm. For vertex identification, the sparse principal
component is thresholded, and the vertices corresponding to
the components of the vector greater than the threshold are
declared to be part of the anomalous subgraph.
One drawback of this technique is its computational complexity. As mentioned in the introduction, one goal of this

7

work is to develop techniques that scale to very large graphs.
The algorithms described in Sections V-A and V-B rely on
a partial eigendecomposition. Using the Lanczos method for
computing m eigenvectors and eigenvalues of a matrix, and
leveraging sparseness of the graphs, this requires a running
time of O((M m + N m2 + m3 )h), where h is the number of
restarts in the algorithm [42]. Thus, if the number of eigenvectors to compute is fixed, this algorithm scales linearly in the
number of edges in its per-restart running time. Sparse
‚àö PCA, as
described in [41], has a running time that is O(N 4 log N /),
where  controls accuracy of the solution. This implies that
sparse PCA will not scale to extremely large datasets without
additional optimization, which is a problem for future work.
We present results using this technique to demonstrate the
feasibility of detecting exceptionally small anomalies using
the framework outlined in this paper.
VI. S IMULATION R ESULTS
A. Noise Models
There are many models for random graphs, with varying
degrees of complexity. In this section we outline 3 random
models that will be used for background noise in our experiments.
1) ErdoÃãs‚ÄìReÃÅnyi (ER) Random Graphs: The simplest random graph model was proposed by ErdoÃãs and ReÃÅnyi in [43].
In this model, given a vertex set V and a number p ‚àà (0, 1), an
edge occurs between any two vertices in V with probability p.
In matrix form, pij = p for all i and j. This model is subsumed
by the model for a random graph with a given expected degree
sequence assumed by equation (9), where, in this case, all
vertices have the same expected degree.
2) Chung‚ÄìLu (CL) Random Graphs: The ‚Äúgiven expected
degree‚Äù model has been studied extensively by Chung and
Lu [30]. Similarly to the dynamic preferential attachment
model of [44], in this model, the probability of two nodes
sharing a connection increases with their popularity. Formally,
each vertex vi is given an expected degree di , and the
probability of vertices vi and vj sharing an edge is given by
P|V |
pij = (di dj )/ `=1 d` , yielding a rank-1 probability matrix
1
P = P|V |

ddT .

(20)

i=1 di

Using the observed degree as the expected degree‚Äîshown
to be an approximately asymptotically unbiased estimator in
[45]‚Äîthe standard formulation of the modularity matrix (10)
perfectly fits this model for background behavior.
3) R-MAT Stochastic Kronecker Graphs: To include a
slightly more complicated model, we also consider the Recursive Matrix (R-MAT) stochastic Kronecker graph [46]. In
this model, a base probability matrix


a b
Pb =
(21)
c d
is given, where a, b, c and d are nonnegative values that sum to
1, and edge probabilities are defined byN
the n-fold Kronecker
n
product of Pb , denoted Pb = {pÃÇij } = i=1 Pb . This results
n
in matrices with 2 vertices. The graph is generated by an

8

IN SUBMISSION TO THE IEEE, ‚Äì ‚Äì ‚Äì 2014

Fig. 4. Sparsity patterns for background graphs: an R-MAT graph (left), a
Chung‚ÄìLu graph (center) and an ErdoÃãs‚ÄìReÃÅnyi graph (right).

iterative method where 1 edge is added at each iteration with
probabilities defined by Pb. If the total number of iterations is
t, the edge probabilities are given by
pij = 1 ‚àí (1 ‚àí pÃÇij )t .

(22)

If the base probability matrix has rank 1, this generator will
produce graphs with a similar structure to the CL model. As
shown in [46], however, this model creates graphs with mild
community structure, thereby presenting a more challenging
noisy background for our subgraph detection framework.
These three models represent varying degrees of complexity
for the detection framework. The ER model is overspecified
by the given expected degree model used in the modularity
matrix, the CL model matches the formula exactly, and the
R-MAT model is mismatched due to its mild community
structure. In the simulations in Section VI-C, the R-MAT
graphs are generated using a base probability matrix with
a = 0.5, b = c = 0.125, and d = 0.25, and the algorithm
is run for 12N iterations, resulting in an average degree of
approximately 12. The graph is unweighted, and undirected
via the ‚Äúclip-and-flip‚Äù procedure as in [46], i.e., the edges
below the main diagonal in the adjacency matrix are removed,
and those above the main diagonal are made undirected. For
CL backgrounds, the expected degree sequence is defined
by the P
edge probabilities of the R-MAT background, i.e.,
|V |
di =
j=1 pij where pij is defined as in (22). The ER
backgrounds use an edge probability that yields an average
degree the same as the more complicated models.
Example sparsity patterns of the adjacency matrices, each
with 1024 vertices, are shown in Fig. 4. Note the moderate
community structure in the R-MAT graph. While the CL graph
has vertices of varying degree, it does not have the same
structure of the R-MAT. One particularly visible difference
is the lack of connections between low-degree vertices and
high-degree vertices in the R-MAT graph, seen in the upperright and lower-left corners of the matrix. Both of these graphs
contain more variation that the ER graph, where the uniform
randomness can be seen in its sparsity pattern.
B. Signal Subgraph
Two random graph models are used for the anomalous signal
subgraph. In one case, an ER graph with probability parameter
pS is generated and combined with randomly selected vertices
from the background. Here, the expected adjacency matrix is
an NS √ó NS matrix where every entry is pS , and thus has
spectral norm pS NS . The second subgraph we consider is a

random bipartite graph, where the vertex set is split into two
subsets and no edge can occur between vertices in the same
subset. Letting N1 and N2 be the numbers of vertices in each
subset, there are N1 N2 possible edges between the two vertex
subsets, and, as in the ER subgraph case, each of these possible
edges is generated with equal probability pS . For the bipartite
subgraph, the expected adjacency matrix has the form


0N1 √óN1
pS 1N1 √óN2
E [AS ] =
,
(23)
pS 1N2 √óN1
0N2 √óN2
‚àö
which has spectral norm pS N1 N2 . This subgraph provides
us with a signal where the average degree does not equal
the spectral norm (unless N1 = N2 ), demonstrating that the
spectral norm is a more appropriate power metric.
C. Monte Carlo Simulations
The results in this section detail the outcomes of several
10,000-trial Monte Carlo simulations. In each simulation, a
background graph is generated, and may or may not have a
signal subgraph embedded on a subset of its vertices. The
subgraph may be a 15-vertex cluster or a bipartite graph with
N1 = 12 and N2 = 25. Test statistics outlined in Section V
are computed on the resulting graph, creating several empirical
distributions that can be used to discriminate between H0
and H1 . Residuals matrices are formed using either the exact
expected value3 , or a rank-1 approximation based on the
observed degrees, as in (9). The expected degree sequence
from the R-MAT model is used for CL backgrounds, and ER
backgrounds use the same average degree. For R-MAT and CL
backgrounds, we consider cases where the foreground vertices
are selected uniformly at random from all background vertices,
and cases where they are randomly selected from the set of
vertices with expected degree less than 5.
For 4096-vertex graphs, ER graphs always achieved nearperfect detection performance. Identification and detection
performance for CL and R-MAT backgrounds are summarized
in Fig. 5. A few phenomena in the results confirm our
intuition. First, note that CL backgrounds have extremely
similar performance, whether the expected value term is given
or estimated. This is because the observed degree is a good
estimate for expected degree, and the small embedding has
a minimal effect on the expected value term, as shown in
Appendix D. (The small but noticeable difference when using
a bipartite foreground emphasizes the impact of the number
of subgraph vertices.) The R-MAT backgrounds have much
more substantial performance differences, due to the model
mismatch. In fact, when the true expected value is given,
performance is better than with the CL background. This
is likely due to the lower variance in the noise, caused by
smaller connection probabilities among low-degree vertices.
Detection performance improves going from the spectral norm
statistic to the chi-squared statistic, and improves further when
analyzing the eigenvector L1 norms. Also, when the subgraph
is embedded only on vertices with expected degree at least
5, performance significantly increases for L1 norm analysis,
3 Due to time and memory constraints, a rank-100 approximation for the
R-MAT expected value was used instead of the true probability matrix.

0.2
0.1
10

12

Foreground Average Degree

0.5
0.4
0.3
0.2
0.1
0
6

8

10

12

0.4
0.3
0.2
0.1

Foreground Average Degree

8

10

12

Foreground Average Degree

0.4
0.3
0.2
0.1
8

10

12

0.5
0.4
0.3
0.2
0.1
0
6

14

0.5

0
6

14

Foreground Average Degree

0.8

0
6

Precision

0.8

0.4
0.2

8

10

12

0
6

14

Foreground Average Degree

8

10

12

0
6

14

Foreground Average Degree

0.8

Precision

0.8

Precision

0.8

0
6

0.6
0.4
0.2

8

10

12

14

Foreground Average Degree

0
6

14

8

10

12

14

8

10

12

14

0.2

1

0.2

12

0.4

1

0.4

10

Foreground Average Degree

0.6

1

0.6

8

0.1

0.8

0.4

14

0.2

1

0.6

12

0.3

1

0.6

10

0.4

0
6

14

8

Foreground Average Degree

0.5

1

0.2

Precision

0.5

0
6

14

Equal Error Rate

Equal Error Rate

8

Equal Error Rate

0.3

9

Equal Error Rate

0.4

0
6

Precision

Equal Error Rate

0.5

Precision

Equal Error Rate

MILLER et al.: A SPECTRAL FRAMEWORK FOR ANOMALOUS SUBGRAPH DETECTION

Foreground Average Degree

0.6
0.4
0.2

8

10

12

14

Foreground Average Degree

0
6

Foreground Average Degree

Fig. 5. A summary of detection and identification performance. The equal error rate (EER) for each background and foreground is shown as the average degree
increases from 6 to 15. Results are shown for cluster subgraphs (solid line) and bipartite subgraphs (dashed line), for R-MAT graphs with the true expected
value (), R-MAT graphs with an estimated rank-1 expected value (+), CL graphs with given expected degrees (‚ó¶) and CL graphs using observed degrees
(√ó). Performance improves as the test statistic goes from the spectral norm (left column), to the chi-squared statistic (center column) to the largest deviation
in L1 norm (right column). Detection performance with the L1 norm-based statistic improves when the subgraph is embedded on low-degree vertices (second
row), rather than choosing the vertices uniformly at random (first row). The same performance trends typically hold for the vertex identification algorithms
(uniform random embedding in third row, degree-biased embedding in fourth row), shown here in terms of precision at a 35% recall rate. The non-monotone
behavior using L1 norms is caused by a cluster of larger eigenvalues in the R-MAT background, which, as discussed in Appendix C, makes detection more
difficult with this method.

10

IN SUBMISSION TO THE IEEE, ‚Äì ‚Äì ‚Äì 2014

1.5
1
0.5
0

1

0.8

0.8

0.6
0.4
Spectral Norm
Chi‚àíSquared
L1 Norm
Sparse PCA

0.2
0

10

12

14

16

0

18
Probability of Detection

while it degrades for the other statistics (since it is likely
to be more orthogonal to the principal eigenvectors). Note
also that, for the spectral norm and chi-squared statistics, the
bipartite embedding is more detectable than the cluster with
the same average degree, since the bipartite foreground has
a higher spectral norm. This does not hold for the L1 norm
statistic, since the cluster embedding, while less powerful, is
concentrated on a smaller subset of vertices, making it more
detectable using this statistic.
One interesting aspect of the L1 norm technique is its nonmonotonic behavior when using the estimated rank-1 expected
value. In both detection and identification, performance improves as the subgraphs increase in size up to a certain point,
after which performance degrades and then improves again.
This is due to clustering of eigenvalues caused by the model
mismatch, as shown in Fig. 6. The figure presents a histogram
of eigenvalues for the R-MAT graph minus the estimated rankT
1 expected value matrix, kkk‚àí1
1 kk . (The vertical axis is the
average number of eigenvalues that fell into a given bin over
the 10,000 Monte Carlo trials.) Most of the eigenvalues are
below 12, while there is always 1 that is over 16 and 11
in the cluster that spans approximately 12 to 15. Since, as
discussed in Appendix C, having eigenvalues that are close
together hinders performance with this method, performance
improves when the subgraph can be localized in an eigenvector
as its eigenvalue approaches 12, but it will be more difficult
around 13. Using the true expected value instead of the rank1 approximation does not yield this behavior, since there is
no model mismatch. The mismatch between R-MAT and the
rank-1 expected value also causes the slight degradation in
performance using the chi-squared statistic before it rapidly
improves. This may be because the embedded subgraph actually improves the symmetry of the projection by balancing
out the mismatch, before finally overpowering it.
The identification results on the bottom half of the figure
follow similar trends, with one notable exception. Performance
is shown in terms of precision at a 35% recall rate (precision
is emphasized since the foreground vertex set is much smaller
than the background). While the k-means-based identification
method (center column, using 3 clusters and a subgraph
threshold of 5 vertices) typically improves performance over
thresholding of the principal eigenvector (first column) for

0.4
0.2

‚àí0.2

1

Probability of False Alarm

Eigenvalue
Fig. 6. Histogram of eigenvalues from an R-MAT matrix using an estimated
rank-1 expected value. The two clusters of larger eigenvalues are responsible
for the non-monotonic behavior in the L1 -norm statistic in shown in Fig. 5.

0.5

0.6

0

1.2

1

0.6
0.4
Spectral Norm
Chi‚àíSquared
L1 Norm
Sparse PCA

0.2
0
0

0.5

1

Probability of False Alarm

Top Eigenvector
K‚àíMeans
Lower Eiegenvector
Sparse PC
0

0.5

1

Recall

Top Eigenvector
K‚àíMeans
Lower Eiegenvector
Sparse PC

1

0.8

Precision

2

1

Precision

Probability of Detection

Proportion

2.5

0.8
0.6
0.4
0.2
0

0

0.5

Recall

1

Fig. 7.
Detection and identification results using sparse PCA. In both
an ErdoÃãs‚ÄìReÃÅnyi background (top row) and an R-MAT background (bottom
row), sparse PCA significantly outperforms the other algorithms. Similar
performance gaps are seen in detection performance (left column) and
identification (right column).

cases where precision is relatively low, it actually hinders
performance in cases where precision is high. This shows that
a subgraph that separates well along the first eigenvector will
not necessarily be equally detectable via k-means, possibly
due to spreading in the second dimension.
Since sparse PCA has a much greater computational burden,
we carried out a more limited set of experiments on smaller
graphs. In each trial, a 512-vertex background graph is generated according to either an R-MAT or ER model. The RMAT graphs use the same probability matrix as in the previous
experiment, and the ER graphs have equal expected volume. In
each case, we use an estimated rank-1 expected value, and use
the DSPCA software package [47] to solve (19). Detection and
identification performance are shown in Fig. 7. These results
demonstrate the detection of a 7-vertex, 80% dense subgraph
in the R-MAT background or a 5-vertex, 85% dense subgraph
in an ER background. Sparse PCA yields markedly superior
performance to the three methods used in Fig. 5. By using this
more costly technique, much smaller, subtler anomalies can
be detected, using the same principles as the less expensive
algorithms.
VII. R ESULTS ON A PPLICATION DATA
Two network datasets were downloaded from the Stanford
Network Analysis Project (SNAP) large graph dataset collection (available at http://snap.stanford.edu/data). One dataset
consists of product co-purchase records on amazon.com,
where each of the 548,552 vertices represents a product,
and a directed edge from vertex i to vertex j denotes that
when product i is purchased, product j is frequently also
purchased [48]. The other dataset has 1,696,415 vertices,
representing nodes on the Internet, taken from autonomous

MILLER et al.: A SPECTRAL FRAMEWORK FOR ANOMALOUS SUBGRAPH DETECTION

Autonomous System Network
400

Eigenvector L1 Norm

Eigenvector L1 Norm

Product Co‚àíPurchase Network
100
80

300

60

200

40

100

20
0
0

50
100
Eigenvector Index

150

0
0

50

100

Eigenvector Index

150

Fig. 8. Eigenvector L1 norms in application datasets: an amazon.com product
co-purchase network (left) and an autonomous system network (right).

system traceroutes in 2005 [49]. The edges in this graph are
undirected and represent communication links between nodes.
In both cases, the 150 eigenvectors corresponding to the largest
positive eigenvalues of the residuals matrices were computed,
and subgraphs were analyzed that align with eigenvectors with
small L1 norms.
In the amazon.com co-purchase network, edges are directed, and each vertex has at most 5 outward edges. We
use the symmetrized modularity matrix introduced in [50] as
a residuals matrix. As shown on the left in Fig. 8, many
of the eigenvectors have small L1 norms, due to frequent
co-purchase of small, relatively isolated sets of products.
We consider the 2 smallest L1 norms, corresponding to the
23rd and 135th largest eigenvectors. These eigenvectors are
concentrated, respectively, on a 53-vertex subgraph with all
possible internal edges (265) and a 44-vertex subgraph with
215 of its 220 possible internal edges. Neither subgraph has
any outgoing edges, and both have fewer than 20 incoming
edges. To compare this to the graph as a whole, we took 1
million samples of comparable size by performing random
walks on the graph. Of all 53-vertex samples, only 609 have
average internal degree greater than 4.5, and of those, none has
fewer than 20 external edges. Similarly, among the random
samples with 44 vertices, 108 have average internal degree
greater than 4.4 and fewer than 40 external edges. Each of
these 108 samples, however, is primarily outside of the 150dimensional space spanned by the computed eigenvectors‚Äîan
indicator vector for the sample vertices in each case is nearly
in the null space of the matrix of eigenvectors. Thus, both of
these subgraphs are anomalous with respect to random samples
of similar size, when considering portions of the graph that are
well-represented in the computed subspace.
The eigenvector L1 norms in the autonomous system graph
generally follow a trend, getting larger as the eigenvalues
get smaller (indices increasing). The two vectors highlighted
in the figure‚Äìthe 10th and 94th‚Äìwere considered for further
investigation, since they have the largest local deviations. The
10th eigenvector is aligned with a 70-vertex subgraph with
over 99% of its possible edges, and the 94th eigenvector
is aligned with a 28-vertex subgraph with over 81% of its
possible edges. These subgraphs consisted of primarily highdegree vertices, with average external degrees of about 957
and 577 for the 70- and 28-vertex subgraphs, respectively. We
took 1 million random samples from among the subgraph of
vertices with degree greater than 500, with sizes commensurate
with the number of vertices in the subgraph within the high-

11

degree vertex set (68 of 70 and 17 of 28). Among the three
68-vertex samples with density greater than 80%, all share
at least 55 vertices with the detected subgraph. Of the 17vertex samples, 713 are at least 75% dense and have fewer
than 16,000 external edges (the 17-vertex subset is 93% dense
and has about 12,500 external edges). Of these 713 samples,
all are significantly aligned with eigenvectors 10 and 18, both
of which also have extremely small L1 norms as shown in
the figure. Thus, the only subgraphs among the samples with
similar densities and external degrees are detectable through
analysis of eigenvector L1 norms.
VIII. C ONCLUSION
In this paper, we present a spectral framework for the
uncued detection of small anomalous signals within large,
noisy background graphs. This framework is based on analysis
of graph residuals in their principal eigenspace. We propose
the spectral norm as a power metric, and several algorithms are
outlined, with varying degrees of complexity. In simulation,
we demonstrate the utility of the algorithms for detection
and identification of two foregrounds within three background
models, with the more computationally complex methods
providing better detection performance. In two real networks,
subgraphs detected via one of the algorithms are shown to be
anomalous with respect to random samples of the background.
The framework presented in this paper demonstrates the
utility of considering the anomalous subgraph detection problem in a signal processing context. There are myriad avenues
of investigation from this point. Recent work has focused on
extending this framework to time-varying graphs [51], [52]
and attributed graphs [53]. Non-spectral statistics have also
been of interest, in particular for detecting anomalously sparse
(rather than anomalously dense) subgraphs [54], though this
complicates the analysis since embedding the signal involves
subtracting edges rather than adding them. Another interesting
area is detection using supervised learning based on subgraph
features, as in [55]. Performance bounds in spectral detection
of cliques and communities have recently been studied [11],
[56], as have computational limits of detection [57], [58]. Also,
while the presented framework relies on analysis of residuals,
considering normalized residuals may improve detection for
subgraphs where the edges are extremely unlikely [30], [59].
This analysis, however, may be intractable for more complicated graph models, since it requires normalizing each observed vertex pair and may not allow the computational tricks
mentioned in Section IV-A. As the detection of anomalous
behavior in relational datasets continues to be a problem of
interest, the field of signal processing for graphs will continue
to pose a rich set of challenges for the research community.
A PPENDIX A
P ROOF OF T HEOREM 2
Under H0 , the hypothesis that the observed graph was
generated by an ErdoÃãs‚ÄìReÃÅnyi process, the likelihood of the
observed graph is given by
N
L(G; H0 , p) = p|E| (1 ‚àí p)( 2 )‚àí|E| .

(24)

12

IN SUBMISSION TO THE IEEE, ‚Äì ‚Äì ‚Äì 2014

Under the alternative hypothesis, an NS -vertex subset was selected uniformly at random to serve as the subgraph. Suppose
that VS ‚äÇ V , |VS | = NS , was chosen as the subset. Each
pair of vertices within VS still has probability p of sharing
an edge due to background activity. If there is no edge in the
background, however, an edge will be added with probability
pS . Thus, the probability of an edge occurring between a given
pair of vertices both in VS is
pÃÇ = p + (1 ‚àí p)pS = p + ps ‚àí p ¬∑ pS .

NS
(26)
L(G; H1 , p, VS , pS ) = pÃÇ|ES | (1 ‚àí pÃÇ)( 2 )‚àí|ES |
NS
N
¬∑ p|E|‚àí|ES | (1 ‚àí p)( 2 )‚àí|E|‚àí( 2 )+|ES | .



Note that N2 ‚àí|E|‚àí N2S +|ES | is the number of ‚Äúnon-edges‚Äù
that are not within the subgraph vertices. Since only one vertex
subset is chosen for the signal embedding, the likelihood of
G under the alternative hypothesis is
X
L(G; H1 , p, VS , pS ) Pr [VS is chosen]. (27)
VS ‚äÇV,|VS |=NS


Each of the NNS possible subsets is equally likely, so the
likelihood ratio is

N ‚àí1 P
VS ‚äÇV,|VS |=NS L(G; H1 , p, VS , pS )
NS
(28)
L(G; H0 , p)

X
VS ‚äÇV,|VS |=NS

L(G; H1 , p, VS , pS )
.
L(G; H0 , p)

b + B)u = uTS Au
b S + 2uTS Au
b B + uTB Au
b B
uT (A
+

(29)

The ratio in (29) can be further simplified as

 NS 
|E |
L(G; H1 , p, VS , pS )
1 ‚àí pÃÇ ( 2 ) pÃÇ(1 ‚àí p) S
=
. (30)
L(G; H0 , p)
1‚àíp
p(1 ‚àí pÃÇ)
Replacing the ratio in (29) with the expression in (30), and
moving the non-subgraph-dependent portion outside of the
summation, yields the expression in (5). This completes the
proof.

+

2uTS BuB

+

(32)

uTB BuB .

b B are zero, since A
b is only nonzero
Both terms that include Au
within the subgraph vertices. To get an upper bound for this
quantity, we bound each term in (32), yielding
b + B)u ‚â§ Œ¥kAk
b + Œ¥kBS k
uT (A
(33)
p
+ 2 Œ¥(1 ‚àí Œ¥)kBSN k + (1 ‚àí Œ¥)kBN k.
b + kBS k ‚àí kBN k, Œ≤ =
For convenience, let Œ± = kAk
b Combining the upper
2kBSN k, and Œ≥ = kBN k+kBS k‚àíkAk.
bound in (33) with the lower bound yields
p
Œ±Œ¥ + Œ≤ Œ¥(1 ‚àí Œ¥) + Œ≥ ‚â• 0
(34)
We can verify that, for Œ≤ ‚â• 0 and ‚àíŒ± ‚â§ Œ≥ < 0, the expression
of (34) will achieve equality at the lesser of the two roots of
the parabola obtained by squaring the expression. Therefore,
(34) holds whenever
p
Œ≤ 2 ‚àí 2Œ±Œ≥ ‚àí Œ≤ 4 ‚àí 4Œ≤ 2 Œ≥(Œ± + Œ≥)
Œ¥‚â•
.
(35)
2(Œ±2 + Œ≤ 2 )
Using the triangle inequality to remove the radical in (35) and
substituting the matrix norms back into the equation yields the
bound in (12). This completes the proof.
A PPENDIX C
C ONCENTRATION OF E IGENVECTORS ON S UBGRAPH
V ERTICES
Here we provide an example of an embedding on which a
single eigenvector will be concentrated. Consider a subgraph
b that is regular, i.e., each vertex has the same degree dS .
A
b = dS , and the
Such a subgraph will have a spectral norm kAk
principal eigenvector will be a vector in which all components
are the same. Let x be a unit-normalized indicator vector
‚àö for
the subgraph, i.e., a vector where the ith component is 1/ NS
if i corresponds to a subgraph vertex and is 0 otherwise.
b and xT (B + A)
b 2 x. We have
Further consider xT (B + A)x

A PPENDIX B
P ROOF OF T HEOREM 3

b = dS + xT Bx = dS + X,
xT (B + A)x

Let u1 be the (unit-normalized) principal eigenvector of
b Since u is the eigenvector corresponding to the largest
A.
b we have
eigenvalue of B + A,
b 1 = kAk
b + uT Bu1 ‚â§ uT (B + A)u.
b
uT1 (B + A)u
1

uTS BuS

(25)

All other vertex pairs still have probability p of sharing an
edge. Therefore, we have

or, equivalently
 ‚àí1
N
NS

kuB k22 = 1 ‚àí Œ¥. The largest eigenvalue of the residuals matrix
is then given by

(31)

Since u1 only has nonzero entries in rows corresponding to
b
subgraph vertices, we can bound this quantity below by kAk‚àí
kBS k.
The vector u can be decomposed as u = uS + uB , where
the only nonzero components of uS correspond to the signal
subgraph vertices and uB may only be nonzero in the rows
corresponding to V \ VS . Let Œ¥ = kuS k22 . Since u has unit L2
norm, and uS and uB are orthogonal, we have 0 ‚â§ Œ¥ ‚â§ 1 and

where
X=

1 X
(aij ‚àí pij )
NS

(36)

(37)

i,j‚ààVS

is a random variable whose mean is 0 and variance is less
than N22 E [|E ‚à© (VS √ó VS )|], that is, the expected fraction of
S
possible edges between the subgraph vertices that exist in the
background. If the embedding occurs on vertices where the
expected connectivity is low, then X will likely be very small.
We also have
b 2 x =d2S + 2dS X + xT B 2 x
xT (B + A)
=d2S

+ 2dS X + Y.

(38)

MILLER et al.: A SPECTRAL FRAMEWORK FOR ANOMALOUS SUBGRAPH DETECTION

Note that Y = kBxk22 = xT B 2 x, which can be rewritten as

13

following substitutions:
a=

Ô£π2

Ô£Æ

N
1 XÔ£∞ X
T 2
(aij ‚àí pij )Ô£ª
x B x=
NS i=1

(39)

b

j‚ààVS

=

N
1 X X
(aij aik ‚àí aij pik ‚àí aik pij + pij pik ).
NS i=1

Œµ+
1 =

(40)

<

N
1 XX
pij ,
NS i=1

b
xT (B + A)x

=

N
X

(47)

‚àÜi zi2

(48)

a
PN

i=m+1

‚àÜi zi2

c

.

(49)

‚àí
dS + X = a(Œªm + Œµ+
1 ) + bŒªm + c(Œªm + Œµ1 ).

(50)

Œµ+
2 =
and
Œµ‚àí
2

Pm‚àí1
i=1

(51)

a

PN
=

‚àÜ2i zi2

i=m+1

‚àÜ2i zi2

c

,

(52)

(43) can be rewritten as
+
2
d2S + 2XdS + Y = a(Œª2m + 2Œµ+
1 Œªm + Œµ2 ) + bŒªm

(53)

‚àí
+ c(Œª2m + 2Œµ‚àí
1 Œªm + Œµ2 ).

!2
Œªi zi2

Combining equations (50) and (53) and performing some
algebraic manipulation yields the system of equations

i=1

=(dS + X)2
=d2S

i=m+1
Pm‚àí1
i=1

Similarly, letting

where the upper bound is the average expected degree of the
subgraph vertices before the embedding occurs. Again, if the
subgraph is embedded on vertices with low expected degree,
this quantity is likely to be small.
b be the eigendecomposition of the
Let U ŒõU T = B + A
residuals matrix, with Œªi denoting the ith eigenvector (Œªi ‚â• Œªj
for i < j), and let z = U T x. We have
2

zi2

‚àí
Thus, Œªm + Œµ+
1 and Œªm + Œµ1 are convex combinations of the
eigenvalues greater than Œªm and less than Œªm , respectively.
We can then express (44) as

(41)

j‚ààVS



Œµ‚àí
1 =

(45)
(46)

c=

For j 6= k, the expectation of the summand is 0. Considering
only j = k, we have

j‚ààVS

zi2

i=1
2
=zm
N
X

j,k‚ààVS

N


1 XX
pij (1 ‚àí pij )
E xT B 2 x =
NS i=1

m‚àí1
X

(42)

+ 2dS X + X

2

2

‚àí
Œ¥ = aŒµ+
1 + cŒµ1

(54)

cŒµ‚àí
2

(55)

1 = a + b + c,

(56)

2

Œ¥ +Y ‚àíX =

and

aŒµ+
2

+

which, solving for b, gives us
T

2

b x=
x (B + A)

N
X

Œª2i zi2

=

d2S

+ 2dS X + Y.

(43)

i=1

If the quantities in (42) and (43) were the same, then x would
b Since their difference is very small
be an eigenvector of B+ A.
2
(i.e., assuming Y and X are small, as they are in expectation),
then x may be highly correlated with a single eigenvector. That
is, for some i, zi2 may be quite large, so that x concentrates
most of its magnitude on the ith eigenvector. Let Œªm be the
eigenvalue closest to dS + X, and Œ¥ = dS + X ‚àí Œªm . Then
we have
dS + X =Œªm + Œ¥
=

m‚àí1
X
i=1

2
Œªi zi2 + Œªm zm
+

(44)
N
X

Œªi zi2 .

i=m+1

For i 6= m, let ‚àÜi = Œªi ‚àí Œªm . For convenience, define the

‚àí
+
‚àí
(Œ¥ 2 + Y ‚àí X 2 )(Œµ+
1 ‚àí Œµ1 ) ‚àí Œ¥(Œµ2 ‚àí Œµ2 )
+ ‚àí
‚àí +
Œµ1 Œµ2 ‚àí Œµ1 Œµ2
|Œ¥|
Œ¥2 + Y ‚àí X 2
‚àí
>1 ‚àí
+ ‚àí
‚àí .
min(Œµ2 , Œµ2 )
min(Œµ+
1 , ‚àíŒµ1 )

b =1 ‚àí

(57)

If the eigenvalues around Œªm are spread far apart, then Œµ+
1,
+
‚àí
‚àíŒµ‚àí
,
Œµ
and
Œµ
will
be
relatively
large,
the
fractions
in
1
2
2
(57) will be small, and x will be heavily concentrated on a
single eigenvector. This is supported by the empirical results
in Section VI, where embedding clusters onto vertices with
low expected degree yields separation in a single eigenvector.

A PPENDIX D
C HANGE IN M ODULARITY D UE TO S UBGRAPH
E MBEDDING
When using observed degree to estimate expected degree,
the difference in the expected value terms caused by the
signal is as follows. If no embedding occurs, the estimated
T
expected value is kkk‚àí1
1 kk , where k is the observed degree

14

IN SUBMISSION TO THE IEEE, ‚Äì ‚Äì ‚Äì 2014

vector resulting from the background noise. If an anomalous
subgraph is embedded into the background, the degree vector
b Since A
b consists of only edges
is changed by kÃÇ = A1.
within the subgraph that do not appear due to noise, the
degree vector after embedding is k + kÃÇ, and the volume is
kk + kÃÇk1 = kkk1 + kkÃÇk1 . Thus, the difference between the
modularity matrix with estimated expected degrees under H0
and H1 is

‚àÜK =

kk T
‚àí
kkk1


T
k + kÃÇ k + kÃÇ

kkk1 + kkÃÇk1


T
kkÃÇk1 kk ‚àí kkk1 k kÃÇ T + kÃÇk T + kÃÇ kÃÇ T


.
=
kkk1 kkk1 + kkÃÇk1

kkÃÇk1 kkk22 + 2kkk1 kkk2 kkÃÇk2 + kkk1 kkÃÇk22
. (59)
kkk21

To show that the strength of this quantity will grow more
slowly than the signal strength, given certain conditions, we
b is o(1), i.e., that
will show that k‚àÜKk/kAk


kkÃÇk1 kkk22 + 2kkk1 kkk2 kkÃÇk2 + kkk1 kkÃÇk22



b
kkk21 kAk

‚Üí 0.

(60)

Since NS  N , we will ignore the kkk1 kkÃÇk22 term, as the
other terms will dominate it. Thus, we must bound
kkÃÇk1 kk22 + 2kkk1 kkk2 kkÃÇk2 2kkk2 kkÃÇk2
=
(61)
¬∑
b
b
kkk1 kAk
kkk21 kAk

2
kkk2
kkÃÇk1
+
.
b
kkk1
kAk
In many applications, the graphs of interest have degree
sequences that follow a power law; i.e., the number of vertices
with degree i is approximately Œ±i‚àíŒ≤ for constants Œ±, Œ≤ > 0.
Using this model, we can analyze the ratio of L1 and L2
norms in graphs with a realistic growth pattern. Let kmax be
the largest degree in the graph. Then the squares of the L1
and L2 norms of k can be approximated as
kkk21 ‚âà

kmax
X

!2
i ¬∑ Œ±i‚àíŒ≤

=

i=1

kmax
X

(62)

i=1

and
kkk22 ‚âà

kmax
X
i=1

i2 ¬∑ Œ±i‚àíŒ≤ =

kmax
X

Œ±i2‚àíŒ≤ ,

1
3‚àíŒ≤
‚àí 1]
1 3‚àíŒ≤ [(kmax + 1)
2

Œ±
2‚àíŒ≤
1
2‚àíŒ≤ ]
2‚àíŒ≤ [kmax ‚àí 2

=

(2 ‚àí Œ≤)2
(kmax + 1)3‚àíŒ≤ ‚àí 1
.
4‚àí2Œ≤
Œ±(3 ‚àí Œ≤) kmax ‚àí 2(2kmax )2‚àíŒ≤ + 24‚àí2Œ≤

In practice, Œ≤ is typically greater than 1 and less than 3 (see,
e.g., [60]), so the constant (2 ‚àí Œ≤)2 /(3 ‚àí Œ≤) will be positive.
Œ≤‚àí1
As kmax increases, the ratio on the right will tend to kmax
.
If we let the maximum degree increase, however, Œ± should
be allowed to increase as well, since this controls the number
of vertices with a given degree. Assume kmax is a degree that
will probably not occur in the graph. Specifically,	 for a small,
constant threshold t, let kmax = inf i|Œ±i‚àíŒ≤ < t . Since this
means that
Œ±(kmax ‚àí 1)‚àíŒ≤ ‚â• t,
(65)
we have


1 p
1 Œ≤‚àí1
kmax ‚â§ ( Œ≤ Œ±/t + 1)Œ≤‚àí1 = O Œ±‚àí1/Œ≤ .
Œ±
Œ±

(66)

Using the approximation in (64), the‚àöratio of the L2 and L1
norms of k is approximately O (1/ 2Œ≤ Œ±).
To bound the term dependent on the subgraph, we have
q
p
b 2
T
2
b
p
NS kAk
kkÃÇk2
1 A 1
=
‚â§
= NS .
(67)
b
b
b
kAk
kAk
kAk
This upper bound can be achieved
‚àö if the subgraph is a clique
or a star. Noting that kkÃÇk1 ‚â§ NS kkÃÇk2 , we substitute (66)
and (67) into (61) to obtain
q

‚àö
‚àö
k‚àÜKk
Œ≤
Œ≤
NS / Œ± + NS / Œ±
‚âàO
(68)
b
kAk
‚àö 
=O NS / Œ≤ Œ± ,
Œ≤
b if NS is o( ‚àö
meaning that k‚àÜKk is o(kAk)
Œ±). Using (65) as
b will vanish
a lower bound for Œ±, this implies that k‚àÜKk/kAk
as the graph grows if NS grows more slowly than kmax .
ACKNOWLEDGMENT

!2
Œ±i1‚àíŒ≤

(64)

=
(58)

To bound the strength of ‚àÜK, we will bound the spectral
norm of each summand in the numerator of (58) and ignore
the kkÃÇk1 in the denominator, yielding
k‚àÜKk ‚â§

not exactly equal 1 or 2, as
Pkmax 2‚àíŒ≤

2
Œ±i
kkk2
‚âà P i=1
2
kkk1
kmax
1‚àíŒ≤
i=1 Œ±i
R kmax +1 2‚àíŒ≤
x
dx
1
< R1
2
kmax 1‚àíŒ≤
Œ±
x
dx
2

(63)

i=1

respectively. Their ratio is then approximated, assuming Œ≤ does

The authors would like to thank Dr. B. Johnson and the
Lincoln Laboratory Technology Office for supporting this
work, and R. Bond, Dr. J. Ward, and D. Martinez for their
managerial support. We would also like to thank N. Singh,
for his early work on the method in Section V-C. Finally,
we would like to thank Dr. R. S. Caceres, Dr. R. J. Crouser,
Prof. A. O. Hero III, Dr. S. Kelley, Dr. A. Reuther, Dr. M.
C. Schmidt, Dr. M. M. Wolf, and the anonymous referees for
many helpful comments on this paper.

MILLER et al.: A SPECTRAL FRAMEWORK FOR ANOMALOUS SUBGRAPH DETECTION

R EFERENCES
[1] D. Bu, Y. Zhao, L. Cai, H. Xue, X. Zhu, H. Lu, J. Zhang, S. Sun,
L. Ling, N. Zhang, G. Li, and R. Chen, ‚ÄúTopological structure analysis
of the protein‚Äìprotein interaction network in budding yeast,‚Äù Nucleic
Acids Research, vol. 31, no. 9, pp. 2443‚Äì2450, 2003.
[2] T. IdeÃÅ and H. Kashima, ‚ÄúEigenspace-based anomaly detection in computer systems,‚Äù in Proc. ACM Int. Conf. Knowledge Discovery and Data
Mining, 2004, pp. 440‚Äì449.
[3] M. E. J. Newman, ‚ÄúFinding community structure in networks using the
eigenvectors of matrices,‚Äù Phys. Rev. E, vol. 74, no. 3, 2006.
[4] K. S. Xu and A. O. Hero III, ‚ÄúDynamic stochastic blockmodels for timeevolving social networks,‚Äù IEEE J. Sel. Topics Signal Process., 2014, to
appear, preprint available: arXiv:1403.0921.
[5] J. M. Kleinberg, ‚ÄúAuthoritative sources in a hyperlinked environment,‚Äù
J. ACM, vol. 46, no. 5, pp. 604‚Äì632, September 1999.
[6] K. Chen, C. Huo, Z. Zhou, and H. Lu, ‚ÄúUnsupervised change detection
in SAR image using graph cuts,‚Äù in IEEE Int. Geoscience and Remote
Sensing Symp., vol. 3, July 2008, pp. 1162‚Äì1165.
[7] A. Sandryhaila and J. M. F. Moura, ‚ÄúDiscrete signal processing on
graphs,‚Äù IEEE Trans. Signal Process., vol. 61, pp. 1644‚Äì1656, April
2013.
[8] D. I. Shuman, S. K. Narang, P. Frossard, A. Ortega, and P. Vandergheynst, ‚ÄúThe emerging field of signal processing on graphs: Extending high-dimensional data analysis to networks and other irregular
domains,‚Äù IEEE Signal Processing Mag., vol. 30, pp. 83‚Äì98, May 2013.
[9] T. L. Mifflin, C. Boner, G. A. Godfrey, and J. Skokan, ‚ÄúA random graph
model for terrorist transactions,‚Äù in Proc. IEEE Aerospace Conf., 2004,
pp. 3258‚Äì3264.
[10] N. Alon, M. Krivelevich, and B. Sudakov, ‚ÄúFinding a large hidden clique
in a random graph,‚Äù in Proc. ACM-SIAM Symp. Discrete Algorithms,
1998, pp. 594‚Äì598.
[11] R. R. Nadakuditi, ‚ÄúOn hard limits of eigen-analysis based planted clique
detection,‚Äù in Proc. IEEE Statistical Signal Process. Workshop, 2012, pp.
129‚Äì132.
[12] E. Arias-Castro and N. Verzelen, ‚ÄúCommunity detection in random
networks,‚Äù 2013, preprint: arXiv.org:1302.7099.
[13] N. Verzelen and E. Arias-Castro, ‚ÄúCommunity detection in sparse
random networks,‚Äù 2013, preprint: arXiv:1308.2955.
[14] W. Eberle and L. Holder, ‚ÄúAnomaly detection in data represented as
graphs,‚Äù Intelligent Data Analysis, vol. 11, no. 6, pp. 663‚Äì689, December
2007.
[15] D. B. Skillicorn, ‚ÄúDetecting anomalies in graphs,‚Äù in Proc. IEEE
Intelligence and Security Informatics, 2007, pp. 209‚Äì216.
[16] S. T. Smith, S. Philips, and E. K. Kao, ‚ÄúHarmonic space-time threat
propagation for graph detection,‚Äù in Proc. IEEE Int. Conf. Acoust.,
Speech and Signal Process., 2012, pp. 3933‚Äì3936.
[17] S. T. Smith, K. D. Senne, S. Philips, E. K. Kao, G. Bernstein, and
S. Philips, ‚ÄúBayesian discovery of threat networks,‚Äù IEEE Trans. Signal
Process., 2014, to be published.
[18] G. A. Coppersmith and C. E. Priebe, ‚ÄúVertex nomination via content
and context,‚Äù 2012, preprint: arXiv.org:1201.4118v1.
[19] S. Fortunato, ‚ÄúCommunity detection in graphs,‚Äù Physics Reports, vol.
486, pp. 75‚Äì174, February 2010.
[20] M. E. J. Newman and M. Girvan, ‚ÄúFinding and evaluating community
structure in networks,‚Äù Phys. Rev. E, vol. 69, no. 2, 2004.
[21] C. E. Priebe, J. M. Conroy, D. J. Marchette, and Y. Park, ‚ÄúScan statistics
on Enron graphs,‚Äù Computational & Mathematical Organization Theory,
vol. 11, no. 3, pp. 229‚Äì247, 2005.
[22] T. H. Cormen, C. E. Leiserson, and R. L. Rivest, Introduction to
Algorithms. MIT Press, 1990.
[23] J. Ruan and W. Zhang, ‚ÄúAn efficient spectral algorithm for network
community discovery and its applications to biological and social
networks,‚Äù in Proc. IEEE Int. Conf. Data Mining, 2007, pp. 643‚Äì648.
[24] S. White and P. Smyth, ‚ÄúA spectral clustering approach to finding
communities in graphs,‚Äù in Proc. SIAM Int. Conf. Data Mining, 2005.
[25] D. Fasino and F. Tudisco, ‚ÄúAn algebraic analysis of the graph modularity,‚Äù 2013, preprint: arXiv:1310.3031.
[26] Q. Ding and E. D. Kolaczyk, ‚ÄúA compressed PCA subspace method for
anomaly detection in high-dimensional data,‚Äù IEEE Trans. Inf. Theory,
vol. 59, November 2013.
[27] S. Hirose, K. Yamanishi, T. Nakata, and R. Fujimaki, ‚ÄúNetwork anomaly
detection based on eigen equation compression,‚Äù in Proc. ACM Int. Conf.
Knowledge Discovery and Data Mining, 2009, pp. 1185‚Äì1193.
[28] F. R. K. Chung, Spectral Graph Theory.
American Mathematical
Society, 1997.

15

[29] S. J. Young and E. R. Scheinerman, ‚ÄúRandom dot product graph models
for social networks,‚Äù in Algorithms and Models for the Web-Graph, ser.
LNCS, A. Bonato and F. R. K. Chung, Eds. Springer, 2007, vol. 4863,
pp. 138‚Äì149.
[30] F. Chung, L. Lu, and V. Vu, ‚ÄúThe spectra of random graphs with given
expected degrees,‚Äù Proc. Nat. Acad. Sci. USA, vol. 100, no. 11, pp.
6313‚Äì6318, 2003.
[31] B. A. Miller, N. Arcolano, M. S. Beard, J. Kepner, M. C. Schmidt,
N. T. Bliss, and P. J. Wolfe, ‚ÄúA scalable signal processing architecture
for massive graph analysis,‚Äù in Proc. IEEE Int. Conf. Acoust., Speech
and Signal Process., 2012, pp. 5329‚Äì5332.
[32] P. O. Perry and P. J. Wolfe, ‚ÄúNull models for network data,‚Äù 2012,
preprint: arXiv:1201.5871v1.
[33] D. S. Choi, P. J. Wolfe, and E. M. Airoldi, ‚ÄúStochastic blockmodels with
growing number of classes,‚Äù Biometrika, vol. 99, no. 2, pp. 273‚Äì284,
2012.
[34] B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúToward signal processing
theory for graphs and non-Euclidean data,‚Äù in Proc. IEEE Int. Conf.
Acoust., Speech and Signal Process., 2010, pp. 5414‚Äì5417.
[35] ‚Äî‚Äî, ‚ÄúSubgraph detection using eigenvector L1 norms,‚Äù in Advances in
Neural Inform. Process. Syst. 23, J. Lafferty, C. K. I. Williams, J. ShaweTaylor, R. Zemel, and A. Culotta, Eds., 2010, pp. 1633‚Äì1641.
[36] N. Singh, B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúAnomalous
subgraph detection via sparse principal component analysis,‚Äù in Proc.
IEEE Statistical Signal Process. Workshop, 2011, pp. 485‚Äì488.
[37] B. A. Miller, N. T. Bliss, P. J. Wolfe, and M. S. Beard, ‚ÄúDetection theory
for graphs,‚Äù Lincoln Laboratory J., vol. 20, no. 1, 2013.
[38] D. Donoho, ‚ÄúCompressed sensing,‚Äù IEEE Trans. Inf. Theory, vol. 52,
no. 4, pp. 1289‚Äì1306, 2006.
[39] B. A. Prakash, A. Sridharan, M. Seshadri, S. Machiraju, and C. Faloutsos, ‚ÄúEigenSpokes: Surprising patterns and scalable community chipping
in large graphs,‚Äù in Advances in Knowledge Discovery and Data Mining,
ser. LNCS, M. J. Zaki, J. X. Yu, B. Ravindran, and V. Pudi, Eds.
Springer, 2010, vol. 6119, ch. 14, pp. 435‚Äì448.
[40] L. Wu, X. Wu, A. Lu, and Z.-H. Zhou, ‚ÄúA spectral approach to detecting
subtle anomalies in graphs,‚Äù J. Intelligent Inform. Syst., vol. 41, no. 2,
pp. 313‚Äì337, 2013.
[41] A. d‚ÄôAspremont, L. E. Ghaoui, M. I. Jordan, and G. R. G. Lanckriet,
‚ÄúA direct formulation for sparse PCA using semidefinite programming,‚Äù
SIAM Review, vol. 49, no. 3, pp. 434‚Äì448, 2007.
[42] R. Lehoucq and D. Sorensen, ‚ÄúImplicitly restarted Lanczos method,‚Äù in
Templates for the Solution of Algebraic Eigenvalue Problems: A Practical Guide, Z. Bai, J. Demmel, J. Dongarra, A. Ruhe, and H. van der
Vorst, Eds. Philadelphia: SIAM, 2000, ch. 4.5.
[43] P. ErdoÃãs and A. ReÃÅnyi, ‚ÄúOn random graphs,‚Äù Publicationes Mathematicae Debrecen, vol. 6, pp. 290‚Äì297, 1959.
[44] A. BarabaÃÅsi and R. Albert, ‚ÄúEmergence of scaling in random networks,‚Äù
Science, vol. 286, no. 5439, pp. 509‚Äì512, 1999.
[45] N. Arcolano, K. Ni, B. A. Miller, N. T. Bliss, and P. J. Wolfe, ‚ÄúMoments
of parameter estimates for Chung‚ÄìLu random graph models,‚Äù in Proc.
IEEE Int. Conf. Acoust., Speech and Signal Process., 2012, pp. 3961‚Äì
3964.
[46] D. Chakrabarti, Y. Zhan, and C. Faloutsos, ‚ÄúR-MAT: A recursive model
for graph mining,‚Äù in Proc. SIAM Int. Conf. Data Mining, 2004, pp.
442‚Äì446.
[47] R. Luss, A. d‚ÄôAspremont, and L. E. Ghaoui, ‚ÄúDSPCA: Sparse
PCA using semidefinite programming,‚Äù December 2008, version 0.6.
[Online]. Available: http://www.di.ens.fr/$\sim$aspremon/DSPCA.html
[48] J. Leskovec, L. A. Adamic, and B. A. Huberman, ‚ÄúThe dynamics of
viral marketing,‚Äù ACM Trans. Web, vol. 1, pp. 1‚Äì39, May 2007.
[49] J. Leskovec, J. Kleinberg, and C. Faloutsos, ‚ÄúGraphs over time: Densification laws, shinking diameters and possible explanations,‚Äù in Proc.
Int. Conf. Knowledge Discovery and Data Mining, 2005, pp. 177‚Äì187.
[50] E. A. Leicht and M. E. J. Newman, ‚ÄúCommunity structure in directed
networks,‚Äù Phys. Rev. Lett., vol. 100, pp. 118 703‚Äì(1‚Äì4), Mar 2008.
[51] B. A. Miller, M. S. Beard, and N. T. Bliss, ‚ÄúMatched filtering for
subgraph detection in dynamic networks,‚Äù in Proc. IEEE Statistical
Signal Process. Workshop, 2011, pp. 509‚Äì512.
[52] B. A. Miller and N. T. Bliss, ‚ÄúToward matched filter optimization for
subgraph detection in dynamic networks,‚Äù in Proc. IEEE Statistical
Signal Process. Workshop, 2012, pp. 113‚Äì116.
[53] B. A. Miller, N. Arcolano, and N. T. Bliss, ‚ÄúEfficient anomaly detection
in dynamic, attributed graphs,‚Äù in Proc. IEEE Intelligence and Security
Informatics, 2013, pp. 179‚Äì184.
[54] B. A. Miller, L. H. Stephens, and N. T. Bliss, ‚ÄúGoodness-of-fit statistics
for anomaly detection in Chung‚ÄìLu random graphs,‚Äù in Proc. IEEE Int.
Conf. Acoust., Speech and Signal Process., 2012, pp. 3265‚Äì3268.

16

[55] S. Pan and X. Zhu, ‚ÄúGraph classification with imbalanced class distributions and noise,‚Äù in Proc. Int. Joint Conf. Artificial Intell., 2013, pp.
1586‚Äì1592.
[56] R. R. Nadakuditi and M. E. J. Newman, ‚ÄúGraph spectra and the
detectability of community structure in networks,‚Äù Phys. Rev. Lett., vol.
108, no. 18, pp. 188 701‚Äì1‚Äì5, 2012.
[57] Q. Berthet and P. Rigollet, ‚ÄúComplexity theoretic lower bounds for
sparse principal component detection,‚Äù in Conf. Learning Theory, ser.
JMLR W&CP, S. Shalev-Shwartz and I. Steinwart, Eds., 2013, vol. 30,
pp. 1046‚Äì1066.
[58] Y. Chen and J. Xu, ‚ÄúStatistical-computational tradeoffs in planted
problems and submatrix localization with a growing number of clusters
and submatrices,‚Äù 2014, preprint: arXiv:1402.1267.
[59] R. R. Nadakuditi and M. E. J. Newman, ‚ÄúSpectra of random graphs with
arbitrary expected degrees,‚Äù Phys. Rev. E, vol. 87, no. 1, pp. 012 803‚Äì
1‚Äì12, 2013.
[60] M. Faloutsos, P. Faloutsos, and C. Faloutsos, ‚ÄúOn power-law relationships of the Internet topology,‚Äù in Proc. SIGCOMM, 1999.

IN SUBMISSION TO THE IEEE, ‚Äì ‚Äì ‚Äì 2014

