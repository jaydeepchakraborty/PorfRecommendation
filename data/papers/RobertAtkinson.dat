Computers & Education 49 (2007) 677–690
www.elsevier.com/locate/compedu

Fostering multimedia learning of science: Exploring the role
of an animated agentÕs image
Qi Dunsworth a, Robert K. Atkinson

b,*

a

b

A Center for Teaching & Educational Technologies, Penn State Behrend, 5091 Station Road,
Erie, PA 16563-0101, United States
Division of Psychology in Education, Arizona State University, P.O. Box 870611, Tempe, AZ 85287-0611, United States
Received 11 August 2005; accepted 7 November 2005

Abstract
Research suggests that students learn better when studying a picture coupled with narration rather than
on-screen text in a computer-based multimedia learning environment. Moreover, combining narration with
the visual presence of an animated pedagogical agent may also encourage students to process information
deeper than narration or on-screen text alone. The current study was designed to evaluate three eﬀects among
students learning about the human cardiovascular system: the modality eﬀect (narration vs. on-screen text),
the embodied agent eﬀect (narration + agent vs. on-screen text), and the image eﬀect (narration + agent vs.
narration). The results of this study document large and signiﬁcant embodied agent and image eﬀects on the
posttest (particularly retention items) but surprisingly no modality eﬀect was found. Overall, the results suggest that incorporating an animated pedagogical agent – programmed to coordinate narration with gaze and
pointing – into a science-focused multimedia learning environment can foster learning.
 2005 Elsevier Ltd. All rights reserved.
Keywords: Animated pedagogical agents; Modality eﬀect; Multimedia learning

*

Corresponding author. Tel.: +1 480 965 1832; fax: +1 480 965 7193.
E-mail address: robert.atkinson@asu.edu (R.K. Atkinson).

0360-1315/$ - see front matter  2005 Elsevier Ltd. All rights reserved.
doi:10.1016/j.compedu.2005.11.010

678

Q. Dunsworth, R.K. Atkinson / Computers & Education 49 (2007) 677–690

1. Introduction
As new technologies emerge that enable instructional designers to create powerful multimedia
learning environments for desktop computers, the interest in incorporating animated pedagogical
agents into instruction has increased. According to Dehn and van Mulken (2000), animated pedagogical agents are characters on the computer screen with embodied life-like behaviors such as
speech, emotions, locomotion, gestures, and movements of the head, the eye, or other parts of the
body. Because these behaviors mimic the type of non-verbal communication that usually occur in
human interaction, animated agents capable of these behaviors are considered powerful additions
to multimedia learning environments since they enable these environments to combine verbal and
non-verbal forms of communication (Atkinson, 2002; Atkinson, Mayer, & Merrill, 2005).
Currently, there are a number of animated agents used in a variety of computer-based multimedia
learning environments. For instance, Herman the Bug is a talkative agent that provides students
real time advice in an environment designed to focus on botanical anatomy (Lester & Stone,
1997; Moreno, Mayer, Spires, & Lester, 2001). Peedy the Parrot is an animated agent programmed
to ﬂy across the screen and use gesture or gaze to help the learners associate verbal information (textual or aural) with visual information in an environment involving multi-step proportion word
problems (Atkinson, 2002; Atkinson et al., 2005). In a learning environment focusing on how an
electric motor works, Dr. Phyz uses his voice to answer questions popped on the screen while moving to point on the explanatory visuals (Mayer, Dow, & Mayer, 2003). On the other hand, Amy Baylor explores how multiple animated agents can by deployed in learning environments in ways that
enable learners to develop multiple perspectives by watching several animated agents interact in
association with a single learning task (Baylor, 1999, 2000, 2001, 2002a, 2002b; Baylor & Ryu,
2003).
One theoretical perspective that helps account for the positive impact of animated agents used
in computer-based learning environments is social agency theory (Atkinson et al., 2005; Mayer,
Sobko, & Mautone, 2003; Moreno et al., 2001). Social agency theory contends that bringing verbal (spoken words) and non-verbal social cues (e.g., gestures, gaze, locomotion) into a multimedia
environment can simulate the human-to-human connection, therefore facilitating the studentsÕ
engagement in the learning process (Atkinson et al., 2005; Mayer, Sobko, et al., 2003; Moreno
et al., 2001). Once such a simulated human-to-human connection is established, the social communication between the student and computer is thought to be natural and automatic, following
the rules of human communication. Furthermore, these automatic responses to computers can be
evoked by minimal social cues (Reeves & Nass, 1996). A gesture, gaze, or nod from an animated
pedagogical agent can create an environment in which the computer invokes the social cues typically found in human-to-human communication (Atkinson, 2002), thus encouraging the learner
to behave as if he or she is communicating with another human. However, the learner may not be
consciously aware of the humanlike qualities of the computer in this social communication.
Although social agency theory is one theoretical perspective that justiﬁes why animated agents
should rely on spoken rather than written text in multimedia learning environments, multimedia
learning theory provides additional support for this practice (Mayer, 2001). According to this theory, our cognitive architecture is capable of dual-channel information processing, that is, that we
process visual and aural information through two channels that work independently – the auditory/verbal channel for processing words and the visual/pictorial channel for processing pictures.

Q. Dunsworth, R.K. Atkinson / Computers & Education 49 (2007) 677–690

679

While words can be delivered either aurally (i.e., narration) or visually (i.e., on-screen text), images
can only be delivered in the visual mode. Some researchers contend that when both words and
images are involved in the instruction, verbal information delivered in an auditory rather than a
visual mode decreases the cognitive load and increases the size of working memory because our cognitive architecture can receive information from both auditory and visual channels simultaneously
as opposed to sequentially (Jeung, Chandler, & Sweller, 1997). One outgrowth of this theory is the
modality principle (Mayer, 2001), which suggests that if a multimedia environment contains words
and images, words should be spoken (e.g., narration) rather than written (e.g., on-screen text). Consistent with the modality principle, a number of studies have reported that aurally delivered instruction in combination with pictures is superior to instructions delivered by text (Atkinson, 2002;
Mayer & Moreno, 1998; Moreno & Mayer, 1999; Mousavi, Low, & Sweller, 1995).
The use of an agentÕs non-verbal humanlike features has been investigated. A number of studies
have explored the eﬀect of an agentÕs presence in computer-based multimedia learning environments, including embodied agent eﬀect and image eﬀect. The embodied agent eﬀect compares
the agent-delivered to the text-delivered instructions to assess the impact on learner performance
and attitudes brought by a fully embodied agent that delivers aural instructions and adopts nonverbal cues such as gesture and gaze to direct a learnerÕs attention. Moreno et al. (2000) found that
students had more positive attitudes and better achievement when the lesson was taught by an
agent rather than by on-screen text (referred to as personal-agent eﬀect in their study). Atkinson
(2002) also found that an embodied agent capable of narration was more eﬀective at fostering
learning than a comparable text-based learning environment. The results of his research revealed
that students learned from an animated agent reported problem solving less diﬃculty and produced more conceptually accurate solutions than their peers in the text-only conditions.
The image eﬀect compares two aurally delivered instructions with or without the animated pedagogical agent on the screen and measures the impact of the presence of the agent on learner performance and attitudes. The on-screen agent may use gestures and gaze to direct a learnerÕs
attention. Lester et al. (1997) found that even when the agent was not very expressive, the ‘‘very
presence’’ of the agent could strongly and positively aﬀect student perception of their learning
experience. Students reported that learning was more enjoyable with the pedagogical agent on
the screen. Lester and his colleges referred to this as ‘‘persona eﬀect’’. Atkinson (2002) also found
an image eﬀect when the agent functioned as a visual indicator to bring the learnerÕs attention to
the relevant materials. Therefore the agent can help the learner to dedicate his limited cognitive
resources to the important information and transfer knowledge to problem solving.
Nonetheless, the research ﬁndings on whether an animated pedagogical agent can facilitate
learning are mixed. For example, although Lester et al. (1997) found favorable results of the presence of an animated pedagogical agent even if it is not fully expressive, Moreno et al. (2001) found
that the visual presence of the agent did not aﬀect studentsÕ learning although it did not hurt studentsÕ performance. Along the same line, van Mulken, André, and Müller (1998) conducted an
empirical study which indicated neither a positive nor a negative eﬀect on comprehension and
recall performance from the agentÕs presence. One argument against using an animated pedagogical agent is that it might produce a split-attention eﬀect. Split-attention eﬀect occurs when the
presence of an agent cannot be intellectually integrated with the instruction but serves as additional cognitive load. A possible reason is that the students might focus their visual attention
on the agent instead of the visuals that need to be integrated with the spoken text (Craig, Gholson,

680

Q. Dunsworth, R.K. Atkinson / Computers & Education 49 (2007) 677–690

& Driscoll, 2002). However, in Craig et al.Õs study (2002) that compared learning performance
under agent-only, agent-plus-gesture, and no-agent conditions, no split-attention eﬀect was found
in any of the dependent measures. Detractors of agents also suggest that the agentÕs physical
image on the screen might act as a source of ‘‘seductive detail’’ (Mayer, Dow, et al., 2003). Based
on their research involving an animated pedagogical agent named Dr. Phyz who ﬂies around to
gesture in addition to on-screen arrows, Mayer et al. concluded that the image of the agent might
not be as important as the agentÕs voice in learning improvement.
2. Overview of the present experiment
In order to shed some additional light on these mixed ﬁndings regarding the eﬃcacy of animated agents, the current study was designed to re-examine the modality eﬀect, the embodied
agent eﬀect, and the image eﬀect among college students while learning about the human cardiovascular system from a computer-based multimedia learning environment. Speciﬁcally, it was
designed to address three questions: (a) Is there a modality eﬀect? (b) Is there an embodied agent
eﬀect? (c) Is there an image eﬀect? The modality eﬀect suggests that students who learn with narration will beneﬁt more from the increased working memory and more likely to remember and to
use the instructional content of the lesson than students who learn in a comparable environment
but with on-screen text (Atkinson, 2002; Mayer, 2001; Mousavi et al., 1995). We predicted that
the students assigned to the narration condition would outperform their peers in on-screen text
condition on measures of learning. The embodied agent eﬀect suggests that an agent can facilitate
learning by simulating the student–instructor relationship by using an agent that is animated to
move, gaze, and deliver auditory messages (Atkinson, 2002). We predicted that the participants
assigned to the narration + agent condition would outperform their counterparts in the on-screen
text condition on measures of learning. The image eﬀect compares the learner performance
between the two conditions provided with aural instructions – the narration condition and the
narration + agent condition. We predicted that students in the narration + agent condition would
outperform their peers in the narration condition on measure of learning since adding the agent
would function as a visual indicator to direct student attention on screen.
The major independent variable of the present study was three levels of presentation mode: onscreen text, narration, and narration + agent, where the animated agentÕs actions were coordinated with the narration and were designed to direct the learnerÕs attention to the relevant portion
of the on-screen image being referred to by the narration. The dependent variables included learner performance in solving retention questions, near transfer questions, and far transfer questions,
in order to examine the degree and depth of learning. The dependent measures also included a
drawing of the human heart and an attitudinal survey.
3. Method
3.1. Participants and design
Fifty-one undergraduate students (15 males and 36 females; M = 19.6 years of age, SD = 2.34;
GPA M = 3.29, SD = 0.44) enrolled in several educational technology courses at a large public

Q. Dunsworth, R.K. Atkinson / Computers & Education 49 (2007) 677–690

681

university located in the Southwest of the United States participated in the study. All students
were volunteers in this study and received course credits for participation. The design of the present study was a single independent variable comparison with three levels. The participants were
randomly assigned in equal proportions (N = 17) to one of the three computer-based conditions:
on-screen text, narration, or narration + agent.
3.2. Computer-based leaning environment
The six-lesson computer-based instruction was developed using Macromedia Director MX
(Macromedia, 2003) with instructional content adapted from a study by Chi, Siler, Jeong, Yamauchi, and Hausmann (2001) involving learning from human tutoring in the context of instruction
on the human circulatory system. To make the instructions more self-explanatory in the multimedia environment, the original 86 sentences were re-organized into a sequence of six lessons: Lesson
1 – Circulatory System; Lesson 2 – The Heart Structure; Lesson 3 – Bloodﬂow in the Heart;
Lesson 4 – Oxygen and Oxygenation; Lesson 5 – Vessels; Lesson 6 – Diﬀusion. A number of
the original Chi et al. sentences were modiﬁed in order to keep the lessons concise or to make
the connection between lessons more ﬂuent.
Within each lesson, the visuals on screen included colorful still images and animations. They
were created for this multimedia environment and were relevant to the instructional content on
page. Navigation of the lessons was sequential from Lesson 1 to Lesson 6 and no lesson could
be skipped. Within each lesson, the learners had the choice to advance to the next screen, replay
the current screen (simultaneously with the audio clip if any), or go back to the previous screen.
The learners were aﬀorded as much time as they wanted to process the information on each screen.
The learning environment contained a number of features that did not vary across conditions,
including (1) its size, which was 800 · 600 pixels; (2) a listing of all six lessons, with the current lesson highlighted (see right side of Fig. 1); (3) navigational aid, indicating what lesson the learner was
in (e.g., ‘‘Circulatory System’’), the total number of pages (or screens) in the lesson, and the current
page (e.g., ‘‘Page 3 of 8’’); (4) a control panel containing a back button, a replay button, and a next
button that enabled the learner to move back a single screen, replay the current screen (including
animation and, for those conditions with it, narration), or move to the next screen, respectively (see
bottom middle of Fig. 1); and (5) a glossary, containing 38 words with there deﬁnitions (see top
right of Fig. 1). The text box that appears on Fig. 1 was only visible in the on-screen text condition.
The learning environment was conﬁgurable to run in one of three instructional modes corresponding to the three conditions used in the present experiment. All three versions of the computer-based learning environment contained exactly the same instructional information,
including the same 53 screens. In the on-screen text mode, the learners were presented with all
of the instructional content as text, that appeared just to the right of the colorful visuals (see
Fig. 1). In the narration and narration + agent versions, the same instructional information was
presented through a monologue-style human voice with the same visuals used in the on-screen text
condition. That is, the script used to create the voice ﬁles for the narration and narration + agent
conditions was identical to the text in the on-screen text version. In the narration mode, the students listened to the audio instruction and watched the visuals on the screen that are synchronized
to the voice. In the narration + agent mode, the students listened to the audio instruction presented
by the animated pedagogical agent and watched him gesturing and pointing to the synchronized

682

Q. Dunsworth, R.K. Atkinson / Computers & Education 49 (2007) 677–690

Fig. 1. On-screen text condition.

visuals (see Fig. 2). The agent was developed using Microsoft Agent. The graphic image of the
agent is a male character named ‘‘Dr. Bob’’. He was programmed to move around the screen, using
gaze, gestures and pointing to direct learner attention as he moves and talks.
3.3. Pencil–paper materials
Pencil and paper materials included (a) a 30-item multiple-choice posttest containing three categories of questions, (b) an item that required students to draw a human heart, and (c) a 14-item
attitude survey. The multiple-choice posttest was adapted from the posttest of the Chi et al. (2001)
study. It consisted of retention, near transfer, and far transfer questions that required students to
apply the knowledge attained form the computer-based learning environment. Speciﬁcally, the
three levels of questions were used to examine the degree and depth of student learning, such
as near or far knowledge transfer. The retention and near transfer questions used in the present
study correspond to Chi et al. category one and two posttest items. Chi et al.Õs original category
four (health questions) was adapted to category three (far transfer problems) in the current study.
There were 10 questions in each category. The ﬁrst category involved retention questions where
only recall of information was required. Answers to these questions could be found explicitly
in the instruction. An example of retention question is ‘‘Where is the blood oxygenated?’’ and students were provided with four options: (a) heart, (b) brain, (c) lungs and (d) kidneys.
The second category entailed near transfer questions. Answers of those questions were indicated in the instruction but the students had to make inferences and integrate them across text

Q. Dunsworth, R.K. Atkinson / Computers & Education 49 (2007) 677–690

683

Fig. 2. Narration + agent condition.

sentences. A sample near transfer questions is ‘‘What is it that you are feeling when you take your
pulse?’’ followed by four options: (a) arteries stretching, (b) veins stretching, (c) ventricles contracting and (d) atria contracting.
The third category, far transfer questions, tested studentsÕ ability to apply the knowledge to
explain health-related problems. A sample question of the third category is ‘‘The short-term eﬀect
of exercise is faster heartbeat. What is the eﬀect of long-term exercise?’’ Possible answers provided
with the students were (a) weaker heart muscle and slower heartbeat, resulting in less blood in
circulation; (b) stronger heart muscle and slower heartbeat, resulting in better circulation; (c)
weaker heart muscle and faster heartbeat, resulting in less blood in circulation; (d) stronger heart
muscle and faster heartbeat, resulting in better circulation.
In addition to the multiple-choice questions, the students were asked to draw a diagram of a
human heart and mark the blood ﬂow in the drawing as well as complete an attitudinal survey.
The drawing of a human heart was adapted from the original category three in the Chi et al.Õs
posttest (2001). Students were asked to illustrate the construction of the human heart, add labels
appropriately and use arrows to mark the direction of bloodﬂow in the heart.
The attitude survey assessed student perception of the learning environment. Speciﬁcally, it
consisted of 14 statements, with seven statements for each of the two categories: instructional
value and the general satisfaction. A sample item for instructional value is ‘‘The way the information was presented made me think more deeply about the topic.’’ A sample item for overall likeliness is ‘‘The presentation style was enjoyable and exciting.’’ Learners were required to respond

684

Q. Dunsworth, R.K. Atkinson / Computers & Education 49 (2007) 677–690

to each statement a ﬁve-point Likert-type scale where 5 equals strongly agree and 1 equals
strongly disagree.
3.4. Procedure
The participants came to the experiment lab and were briefed on the procedures for the experiment. Each of them was given a packet of materials with a participation number on the top of the
envelope. The participants ﬁrst signed the consent form, then started a set of activities in the order
of (a) working through the computer-based program of the assigned version (on-screen text, narration, or narration + agent) at their own pace, exit the computer-based instruction when completed; (b) taking the multiple-choice posttest; (c) taking the human heart drawing posttest;
and (d) responding to the attitude survey. The participants were requested to document the time
shown on the computer clock immediately before and after each activity. When the participants
completed all activities, responding to all materials in the packet, the experimenter checked all
materials and thanked the participants for participation.
3.5. Scoring
The multiple-choice questions were scored one point for each question answered correctly. The
highest score for each category of questions was 10 and the total for the posttest was 30. The posttest of drawing of the human heart was scored on a binary scoring system which employs 1 s and
0 s to code student drawing with being present and correct = 1, or absent or wrong = 0. The maximum score of the drawing is 39. Of the 14 items on the attitude survey, seven items loaded on the
instructional value and the remaining seven items loaded on general satisfaction. The attitude survey was measured on a ﬁve-point Likert-type scale where 5 equals strongly agree and 1 equals
strongly disagree. For of the surveys two categories (instructional value and the general satisfaction), an average response was calculated by summing across the items and dividing by seven.
3.6. Results
Analyses of variances (ANOVAs) were used to explore for treatment-related diﬀerences on the
overall posttest as well as each category of the posttest (retention questions, near transfer questions, and far transfer questions). In the event the overall omnibus test was signiﬁcant, followup comparisons were conducted using FisherÕs LSD with familywise alpha controlled at the .05
level. CohenÕs d statistic was used as an eﬀect size index where d values of .2, .5, and .8 correspond
to small, medium, and large values, respectively (Cohen, 1988). Table 1 shows the mean score
(and standard deviation) for each condition on the overall posttest, each of the posttestÕs three
subtests, the heart drawing, attitudinal survey, and learning time.
3.7. Posttest
On the overall posttest, the ANOVA revealed that the presentation mode had a signiﬁcant eﬀect
on student performance, F(2, 48) = 3.09, MSE = 13.98, p < .05. Results of the pairwise comparisons showed that the narration + agent condition (M = 17.65, SD = 3.74) outperformed the

Q. Dunsworth, R.K. Atkinson / Computers & Education 49 (2007) 677–690

685

Table 1
Mean scores and standard deviations by condition on the experimental measures
Condition
On-screen text

Narration

Narration + Agent

M

SD

M

SD

M

SD

Overall multiple-choice posttest
Retention subtest
Near transfer subtest
Far transfer subtest

14.71
4.88
5.41
4.41

3.60
1.90
1.50
1.54

15.12
5.18
5.94
4.00

3.87
1.59
1.89
1.90

17.65a,b
6.47a,b
6.41
4.76

3.74
1.59
2.00
1.92

Heart drawing posttest

13.71

7.67

14.24

7.28

15.24

7.99

3.77
3.55

.40
.40

3.61
3.52

.41
.53

3.50
3.43

.71
.51

28.44

9.80

25.88

4.72

29.53

6.04

Attitudinal survey
Instructional value subtest
General satisfaction subtest
Learning time

Note. n = 17 for each condition.
a
Denotes voice + agent condition scored signiﬁcantly higher than text-only condition at p < .05.
b
Denotes voice + agent condition scored signiﬁcantly higher than voice-only condition at p < .05; time is reported in
minutes.

on-screen text condition (M = 14.71, SD = 3.60). CohenÕs d statistic for these data yields an eﬀect
size estimate of 0.80, which corresponds to a large eﬀect. The narration + agent condition also
performed signiﬁcantly better than the narration condition (M = 15.12, SD = 3.87). CohenÕs d
statistic for these data yields an eﬀect size estimate of 0.66, which corresponds to a medium-tolarge eﬀect. The mean diﬀerence between the narration condition and the on-screen text condition
was not signiﬁcant.
In retention questions, the ANOVA revealed that the presentation mode had a signiﬁcant eﬀect
on student performance, F(2, 48) = 4.21, MSE = 2.89, p < .05. Results of the pairwise comparisons showed that the narration + agent condition (M = 6.47, SD = 1.59) outperformed the onscreen text condition (M = 4.88, SD = 1.90). CohenÕs d statistic for these data yields an eﬀect size
estimate of 0.91, which corresponds to a large eﬀect. The narration + agent condition also performed signiﬁcantly better than the narration condition (M = 5.18, SD = 1.59). CohenÕs d statistic
for these data yields an eﬀect size estimate of 0.81, which corresponds to a large eﬀect. The mean
diﬀerence between the narration condition and the on-screen text condition was not signiﬁcant.
In answering near transfer questions, the presentation mode did not show overall signiﬁcant
diﬀerences among conditions, F(2, 48) = 1.30, MSE = 3.28, p > .05, although the narration +
agent condition achieved the highest mean score (M = 6.41, SD = 2.00). Similarly, analysis of
far transfer problems found no signiﬁcant diﬀerences among the three conditions, F(2, 48) =
.77, MSE = 3.23, p > .05, although again the narration + agent condition achieved the highest
mean score (M = 4.76, SD = 1.92).
3.8. Human heart drawing
Descriptively, the narration + agent condition achieved a higher mean score in the posttest
drawing (M = 15.24, SD = 7.99) than the narration condition (M = 14.24, SD = 7.28) and the

686

Q. Dunsworth, R.K. Atkinson / Computers & Education 49 (2007) 677–690

on-screen text condition (M = 13.71, SD = 7.67). However, there was no statistical signiﬁcance
found among the three treatment conditions, F(2, 48) = .18, MSE = 58.53, p > .05.
3.9. Learner attitudes
A one-way ANOVA was conducted to test the overall signiﬁcance of learner attitudes towards
the instructional program of diﬀerent presentation modes. The results showed no signiﬁcant differences on the two subscales, that is, in perceived instructional value, F(2, 48) = 1.18, MSE = .28,
p > .05, or general satisfaction with the learning environment among the three treatment conditions, F(2, 48) = .28, MSE = .23, p > .05. This suggests that overall there was neither strong preferences nor dislikes whether the instruction was presented by on-screen text, narration, or by
narration + agent mode.
3.10. Learning time
A one-way ANOVA was conducted to explore whether there were diﬀerences across the diﬀerent presentation modes in terms of time spent in the computer-based learning environment. The
analysis showed no signiﬁcant diﬀerences in learning time, F(2, 46) = 1.12, MSE = 51.26, p > .05.
This suggests that overall students spent approximately equal amount of time in the multimedia
learning environment regardless of which presentation mode they were assigned to.

4. Discussion
4.1. Is there a modality eﬀect?
The narration condition did not score signiﬁcantly higher than the on-screen text condition in
the posttest, albeit descriptively the narration condition achieved a higher mean score in answering retention and near transfer questions. This outcome is surprising given the robust modality
eﬀect – that is, narration is superior to on-screen text in multimedia learning environments –
established in previous studies (Atkinson, 2002; Mayer & Moreno, 1998; Moreno & Mayer,
1999; Mousavi et al., 1995).
The absence of a modality eﬀect raises several questions: under what instructional conditions is
the modality eﬀect likely to appear in a multimedia environment? Did the complexity of the
instruction used in the present experiment mitigate the advantages associated with dual-channel
information processing? Speciﬁcally, if the learning constantly requires reference to what has been
presented in the previous screens, is the voice modality still likely to facilitate learning? Compared
with the learning materials used in existing literature, there is a noting diﬀerence in the present
study. In Atkinson (2002) and Mousavi et al. (1995), students dealt with few scientiﬁc terms,
and the instructional materials used in Mayer and Moreno (1998) and Moreno and Mayer
(1999) asked students to focus on explaining cause-and-eﬀect system. In all studies, students solve
problems in a linear fashion. The learning material of the present study was a non-linear process
to build a mental model of the human cardiovascular system. There were 38 scientiﬁc terms
embedded in the instruction, of which 15 referred to characteristics of the human heart that

Q. Dunsworth, R.K. Atkinson / Computers & Education 49 (2007) 677–690

687

pertained to on-screen images (e.g., ventricular valves, right atrium). It seems that students have
to ﬁrst remember the newly introduced terms, match them correctly with the visual information,
then to approach the understanding of the human cardiovascular system. When many new terms
are introduced at once, the learner may not be able to retrieve the words immediately and relate
them to relevant visual information on the screen. However, the ultimate understanding of a
human heartÕs function is largely contingent upon memorizing and mentally integrating the narrative statement with the matching diagrammatic entity. It seems reasonable to postulate that the
quantity of scientiﬁc terms required high mental eﬀorts and visual search during the learning
process.
4.2. Is there an embodied agent eﬀect?
In answering retention questions, the participants assigned to the narration + agent condition
signiﬁcantly outperformed their on-screen text counterparts and yielded a large eﬀect. Consistent
with Atkinson (2002) and Moreno et al.Õs personal-agent eﬀect (2000), this study demonstrates an
embodied agent eﬀect over text-delivered instruction, showing that learning can be fostered when
an animated agent uses gazes and gestures in conjunction with narration. It seems that when an
agent is animated to carry motions such as speech, gazes, gestures, or pointing on the screen, the
computer has changed its role from an information delivery tool to a social character that bears
personal features to induce social responses. As in a real human-to-human conversation, the audience in the multimedia environment would naturally involve in an agent–learner relationship by
being more attentive to what is being said. Accordingly, the embodied agent eﬀect supports social
agency theory that an animated pedagogical agent is able to maintain student attention and keep
the learner engaged in cognitive processing of the instruction.
The embodied agent eﬀect also revealed that the agent in the current study did not distract student away from the instruction and cause split-attention, which would have impeded learning.
Although the agent, Dr. Bob, may ‘‘roller board’’ from one part of the screen to another, the
entertaining element included in the animated agent has been kept minimum to reduce irrelevant
information.
However, the large embodied agent eﬀect for the retention questions did not extend to student
performance on near and far transfer questions. Although the descriptive data showed that the
narration + agent condition achieved higher mean scores than the on-screen text condition across
near and far transfer questions, the eﬀect size was medium in answering near transfer questions
(CohenÕs d = .57) and small-to-medium in answering far transfer problems (CohenÕs d = .40).
As mentioned before, given the number of scientiﬁc terms in the current study, student performance on transfer problems largely depend on whether the new terms have been memorized
and matched on the visual diagrams. Student performance might be impeded by the quantity
of scientiﬁc terms. In addition, the relatively low sample sizes precluded us from establishing anything other than medium-to-large or large eﬀects.
4.3. Is there an image eﬀect?
A large image eﬀect was found in answering retention questions as the narration + agent condition performed signiﬁcantly better than the narration condition. Diﬀerent from what Mayer

688

Q. Dunsworth, R.K. Atkinson / Computers & Education 49 (2007) 677–690

et al. (2003) postulated, the presence of the animated pedagogical agent in the current study did
not seem to serve as a ‘‘seductive detail’’ in the learning process. One possible reason is that in
Mayer et al. (2003), agent Dr. PhyzÕs appearance did not provide any assistance in terms of learning. The existing animations already contained arrows or highlights to guide students processing
information. In contrast, in the present study Dr. Bob was animated to gesture and point on the
screen to focus the learnerÕs attention on the most relevant information. If students already made
a lot of eﬀorts in memorizing the scientiﬁc terms, the animated agent pointing may help reduce
visual searching and matching load on screen. In other words, the presence of the animated pedagogical agent was a visual indicator that helped relate information input from aural and visual
channels.
Interestingly, some insight regarding cognitive load and visual indicator can be gleaned from
Jeung et al. (1997), where the authors conducted a series of experiments that investigated the
modality eﬀect in consideration of high and low search demand required by geometry instruction.
In their study, Jeung et al. used three conditions that were similar to the current study: a visual–
visual condition (comparable to our on-screen text condition), an audio–visual condition (comparable to our narration condition), and an audio-visual-ﬂashing (comparable to our
narration + agent where the student attention was directed by the animated pedagogical agent).
The results showed that the audio–visual–ﬂashing was superior to the other two conditions while
no diﬀerence was found between visual–visual and audio–visual conditions. Jeung et al. explained
that in order to understand the materials, the learner had to hold auditory information while
searching for the relevant visual information on the screen. When visual search demand was high,
the advantage of increased working memory capacity may not be apparent unless an appropriate
visual signal was used to coordinate with the audio information. Consistent with Atkinson (2002),
the agent in the current study contributed to cognitive searching load reduction as a visual indicator during the learning process.
Nevertheless, the image eﬀect did not carry over from retention to near and far transfer questions. In addition to the number of scientiﬁc terms that might hamper knowledge transfer, gestures, gazes, and pointing of the agent in the present study may not be intense enough to serve
as a salient visual indicator.

5. Conclusion
The present study found support for an embodied agent eﬀect and an image eﬀect. Both of the
eﬀects demonstrated a large eﬀect size but they were limited to answering retention questions and
did not carry over to higher level of cognitive learning. Given the quantity of scientiﬁc terms
involved in the instruction, learning process has been complicated into three phases before students understand the human cardiovascular system: memorizing, searching, and matching verbal
information to relevant visual information. Being unfamiliar with the scientiﬁc terms apparently
added diﬃculty to cognitive search and transfer, which might explain why the present study failed
to document a modality eﬀect as well as why the impact of diﬀerent presentation modes was not
apparent on other dependent measures (e.g., transfer). However, the large signiﬁcant embodied
agent eﬀect in retention demonstrated support to social agency theory. It suggests that using
an animated pedagogical agent with verbal and non-verbal features fosters information processing

Q. Dunsworth, R.K. Atkinson / Computers & Education 49 (2007) 677–690

689

because students naturally perceive learning process as social and respond more engagingly. The
large image eﬀect indicates that adding an animated pedagogical agentÕs image on the screen may
foster learning if the agent conducts instructionaly valuable behavior such as being a visual indicator to reduce student cognitive search load. Studies in the future need to improve the intensity
of the visual aid, such as making the agent gesturing more accurately on the visuals and better
synchronized with the voice.
The experiment has a few limitations. First, the amount of instructions was heavy for students
who had little or no prior knowledge of the human circulatory system. Extension of the current
study should minimize scientiﬁc jargons and avoid overloading students with new terms. The
omission of practice items in the computer-based program added diﬃculty to the processing process. It is advisable to add practice problems along with immediate feedback to enhance knowledge transfer in future studies. Second, due to the relatively small sample size under each
treatment, there was not enough power in the design to detect anything other than medium-tolarge or greater eﬀect size diﬀerences among the conditions. Future research should rely on a more
powerful design to replicate and extend the eﬀects reported in this study.
In sum, results of this study suggest that incorporating an animated pedagogical agent – programmed to deliver instructional explanations aurally while simultaneously using gaze and gesture to direct attention – in a computer-based science learning environment fostered learning.
The study also yields a few avenues for further research. For instance, if a visual aid is needed
for high search learning materials, what kind of visual indicator can be used to reduce the cognitive search demand? Does the nature of the instructions (e.g., example-based linear problem solving or knowledge-based non-linear mental model building) interact with the presentation mode?
Further, since the current study employed a monologue-style narration in the voice instructions, it
would be interesting to compare the eﬀectiveness of diﬀerent narration styles, such as a conversational style, and real dialogue between the agent and the learner.
References
Atkinson, R. (2002). Optimizing learning from examples using animated pedagogical agents. Journal of Educational
Psychology, 94, 416–427.
Atkinson, R., Mayer, R., & Merrill, M. (2005). Fostering social agency in multimedia learning: Examining the impact
of an animated agentÕs voice. Contemporary Educational Psychology, 30, 117–139.
Baylor, A. L. (1999). Intelligent agents as cognitive tools. Educational Technology, 39, 36–40.
Baylor, A. L. (2000). Beyond butlers: Intelligent agents as mentors. Journal of Educational Computing Research, 22,
373–382.
Baylor, A. L. (2001). Permutations of control: Cognitive considerations for agent-based learning environments. Journal
of Interactive Learning Research, 12, 403–425.
Baylor, A. L. (2002a). Expanding pre-service teachersÕ metacognitive awareness of instructional planning through
pedagogical agents. Educational Technology Research and Development, 50, 5–22.
Baylor, A. L. (2002b). Agent-based learning environments for investigating teaching and learning. Journal of
Educational Computing Research, 26, 249–270.
Baylor, A. L., & Ryu, J. (2003). Does the presence of image and animation enhance pedagogical agent persona?. Journal
of Educational Computing Research 28, 373–395.
Chi, M. T. H., Siler, S. A., Jeong, H., Yamauchi, T., & Hausmann, R. G. (2001). Learning from human tutoring.
Cognitive Science, 25, 471–533.
Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Hillsdale, NJ: Erlbaum.

690

Q. Dunsworth, R.K. Atkinson / Computers & Education 49 (2007) 677–690

Craig, S., Gholson, B., & Driscoll, D. (2002). Animated pedagogical agents in multimedia educational environments:
Eﬀects of agent properties picture features, and redundancy. Journal of Educational Psychology, 94, 428–434.
Dehn, D. M., & van Mulken, S. (2000). The impact of animated interface agents: A review of empirical research.
International Journal of Human-Computer Studies, 52, 1–22.
Jeung, H., Chandler, P., & Sweller, J. (1997). The role of visual indicators in dual sensory model instruction.
Educational Psychology, 17, 329–433.
Lester, J. C., & Stone, B. A. (1997). Increasing believability in animated pedagogical agents. In Proceedings of the ﬁrst
international conference on autonomous agents (pp. 16–21). New York: ACM Press.
Lester, J. C., Converse, S. A., Kahler, S. E., Barlow, S. T., Stone, B. A., & Bhoga, R. S. (1997). The personal eﬀect:
Aﬀective impact of animated pedagogical agents. In Proceedings of CHIÕ97 (pp. 359–366). New York: ACM Press.
Macromedia (2003). Director MX [Computer Software]. San Francisco: Author.
Mayer, R. E. (2001). Multimedia learning. New York: Cambridge University Press.
Mayer, R., Dow, G., & Mayer, S. (2003). Multimedia learning in an interactive self-explaining environment: What
works in the design of agent-based microworlds? Journal of Educational Psychology, 95, 806–813.
Mayer, R. E., & Moreno, R. (1998). A split-attention eﬀect in multimedia learning: Evidence for dual processing
systems in working memory. Journal of Educational Psychology, 90, 312–320.
Mayer, R., Sobko, K., & Mautone, P. (2003). Social cues in multimedia learning: Roles of speakerÕs voice. Journal of
Educational Psychology, 95, 419–425.
Moreno, R., & Mayer, R. E. (1999). Cognitive principles of multimedia learning: The role of modality and contiguity.
Journal of Educational Psychology, 91, 358–368.
Moreno, R., Mayer R. E., & Lester, J. C. (2000). Life-like pedagogical agents in constructivist multimedia
environments: Cognitive consequences of their interaction. In J. Bourdeau, & R. Heller (Eds.), Proceedings of the
world conference on educational multimedia, hypermedia, and telecommunication (ED-MEDIA 2000) (pp. 741–746).
Moreno, R., Mayer, R. E., Spires, H., & Lester, J. (2001). The case for social agency in computer-based teaching: Do
students learn more deeply when they interact with animated pedagogical agents? Cognition and Instruction, 19,
177–213.
Mousavi, S., Low, R., & Sweller, J. (1995). Reducing cognitive load by mixing auditory and visual presentation modes.
Journal of Educational Psychology, 87, 319–334.
Reeves, B., & Nass, C. (1996). The media equation. New York: Cambridge University Press.
van Mulken, S., André, E., & Müller, J. (1998). The persona eﬀect: How substantial is it? In H. Johnson, L. Nigay, & C.
Roast (Eds.), People and computers XIII: Proceedings of HCT98 (pp. 53–66). Berlin: Springer.

British
British Journal
Journal ofof Educational
Educational Technology
Technology(2015)
doi:10.1111/bjet.12285
doi:10.1111/bjet.12285

Vol 47 No 5 2016

893–905

Technology-enhanced learning in college
mathematics remediation
Cecile M. Foshee, Stephen N. Elliott and Robert K. Atkinson
Cecile M. Foshee is a postdoctoral medical education fellow at the Cleveland Clinic Foundation. She has over a decade
of experience as an educator and currently holds an appointment as assistant clinical professor at Cleveland Clinic
Lerner College of Medicine of Case Western University. Her expertise is in instructional design with a particular
emphasis on leveraging technology to maximize learning and reflective practices. Her research interests include
interprofessional education and methodologies for improving outcome-based curricula across the continuum of
undergraduate, graduate and continuing medical education. Stephen N. Elliott is a Mickelson Foundation professor in
the Sanford School of Social and Family Dynamics at Arizona State University and a professorial fellow in the
Learning Sciences Institute at the Australian Catholic University. His research focuses on scale development and
educational assessment practices with students with disabilities or at risk for educational difficulties. Assessment
tools he has developed include the Social Skills Rating System (SSRS), Social Skills Improvement System (SSiS),
Academic Competence Evaluation Scales (ACES), Vanderbilt Assessment of Leadership in Education and My
Instructional Learning Opportunities Guidance System (MyiLOGS). Elliott is a co-PI of the National Center on
Assessment and Accountability for Special Education, a US Department of Education center. Robert K. Atkinson is
an associate professor holding a joint appointment in the School of Computing, Informatics, and Decision Systems
Engineering at the Ira A. Fulton School of Engineering and the Division of Educational Leadership and Innovation
in the Mary Lou Fulton Teacher’s College at Arizona State University. His research focuses on personalized learning,
social media, learner analytics, mobile learning, cognitive science, usability testing and human–computer interaction. Address for correspondence: Dr Cecile M. Foshee, Education Fellow and Assistant Clinical Professor, Cleveland
Clinic Lerner College of Medicine of Case Western University, 9500 Euclid Avenue NA25, Cleveland, OH 44195,
USA. Email: cmfoshee@gmail.com or fosheec@ccf.org

Abstract
US colleges presently face an academic plight; thousands of high school graduates are
performing below the expected ability for college-level mathematics. This paper describes
an innovative approach intended to improve the mathematics performance of first-year
college students, at a large US university. The innovation involved the integration of
faculty-led instruction with technology-enhanced learning (TEL). In this case, TEL refers
to a sophisticated software program that delivers mathematics education using an adaptive, self-paced, individualized, mastery-based approach. The purpose of this investigation was to examine the extent to which TEL met the educational requirements of college
students in need of remediation and to explore the effects of TEL on students’ beliefs
about their academic ability and academic behaviors (academic competence). The
sample of 2880 included all the students enrolled in a single semester of remedial
mathematics. Results suggested successful remediation, as indicated by the end-ofsemester course completion rate, with 75% of students eligible to enroll in a first-year
sequence mathematics course and an additional 18% on track for eligibility by the
following semester. TEL also appeared to have a positive, statistically significant effect on
students’ learning and academic competence. For these findings, we discuss study limitations and implications for future research.
What contributes to academic success?
Colleges in the USA remediate thousands of high school graduates who perform below the
expected ability for college-level mathematics (Bettinger & Long, 2009). Recent research by The
C 2015 British
V
©
British Educational
EducationalResearch
ResearchAssociation
Association

2894 British
Journal
of Educational
Technology
British
Journal
of Educational
Technology

Vol 47 No 5 2016

Practitioner Notes
What is already known about this topic
• Mastery learning is an effective approach to promote learning.
• Adapting instruction to students’ skill level promotes academic success.
• Self-efficacy beliefs are positively correlated with academic performance.
What this paper adds
• Demonstrates how technology-enhanced learning (TEL) can address some of the limitations of a mastery-based instruction.
• Illustrates the potential benefits of using TEL for remediation.
• Shows how adaptive instruction can support math efficacy when instruction is
adapted to students’ present abilities.
Implications for practice and/or policy
• Provides justification for individualizing instruction based on students’ current academic needs.
• Informs development of future mastery-based remedial instruction.
• Provides a basis for future exploration on the impact of using frequent formative
assessments and feedback to improve the performance of students in need of
remediation.

American College Testing (ACT, 2012) pointed to some dismal results; on the ACT, 45% of all high
school students in the nation met the mathematics benchmark, while only 25% met the benchmarks in all four-subject areas: English, reading, mathematics and science. Math academic
achievement is believed to increase student success in college (Lee, 2012), yet 80% of universities
and 98% of community colleges place 20–50% of first-year students in remedial courses
(Bettinger & Long, 2009; Complete College America, 2012). In 2011, the National Center for
Educational Achievement (NCEA, 2011) identified the highest performing schools from over 300
school districts. They identified two important characteristics: (1) alignment of the curriculum to
the needs of the students to properly introduce, develop and master content, and (2) the assessment of concepts at each grade level as a prerequisite for advancement.
We interpreted these characteristics as a call for (1) adaptive instruction and (2) a mastery-based
approach. We describe our observations as we examine the learning experience of a large group
of first-year college students (n = 2880). These students received mathematics remediation
through a combination of self-paced adaptive instruction and weekly instructor-led sessions at a
large US university. This university secured a partnership with a commercial software developer
to create a self-paced course in alignment with Common Core State Standards (Arizona
Department of Education, 2013) for all first-year, college-level mathematic courses. The adaptive
component of the course was a computer-based program that delivered all the instructional
content. Students’ advancement was dependent on their ability to demonstrate mastery of the
content not on a predetermined period of time (unit or semester). The pedagogical affordances of
the technology employed—individualized, adaptive and mastery based—constitute technologyenhanced learning (TEL) (Goodyear & Retalis, 2010).
The purpose of this study was to examine the extent to which TEL met the educational requirements of college students in need of mathematics remediation and its effects on students’ beliefs
about their academic ability and academic behaviors (academic competence). Our key questions
©
C 2015 British
V
British Educational
EducationalResearch
ResearchAssociation
Association

in college
math
remediation 895
3
TELTEL
in college
math
remediation

were “Does college students’ academic competence change after participating in a remedial
mathematics course using TEL?” and “How does TEL contribute to college students’ mathematics
remediation?”
Theoretical framework
Bandura (1997) held that effective performance is dependent on skills and one’s perceptions
about those skills—perceived self-efficacy. Self-efficacy theory refers to the personal beliefs or
judgments made about the ability or inability to take action or perform specific tasks. Self-efficacy
is context specific; for instance, a person can have low mathematics efficacy and high English
efficacy or vice versa. Generally, students who have a low sense of efficacy about their ability to do
a certain task tend to avoid the task. Past performance is the primary contributor to efficacy
beliefs. The effects on self-efficacy are strongest when students experience success through their
own efforts. Self-regulatory beliefs also contribute to the formation of self-efficacy perceptions.
When students regulate their behavior by focusing on goal attainment, they increase their motivation to engage in the task and overcome low efficacy beliefs. Goals also serve as personal
evidence of accomplishment, particularly when goals attained are progressively difficult (Schunk,
1991). Positive beliefs about how well one will perform in a given activity (expectancy values) are
associated with the motivation to persist in that activity. While expectancy values and self-efficacy
are independent constructs, they are closely related. The difference is that self-efficacy is task
based and expectancy values tend to be value based. Hence, when students experience repeated
failures in a specific domain, their self-efficacy in that domain is low; this in turn may lead to a
value judgment such as I’m dumb, I can’t do this! In contrast, students with strong self-efficacy
beliefs approach difficult tasks as challenges or opportunities (Wigfield & Eccles, 2000).
Self-efficacy is predictive of performance (Bandura, 1997), contributes to beliefs about self-worth
(Wigfield & Eccles, 2000) and is linked to motivation (George, 2010). A review of research on
motivation (an essential component of learning) and engagement found that motivational
factors were more likely to contribute to academic success when students experienced greater
levels of autonomy and had frequent opportunities to demonstrate competence (Toshalis &
Nakkula, 2012). In other studies, self-efficacy had a reciprocal relationship with mastery experiences and acted as a mediator of motivation and learning (van Dinther, Dochy & Segers, 2011;
Pajares & Usher, 2008). When students have the opportunity to work on attainable tasks, they
develop confidence in their abilities; this confidence then becomes a motivator to learn. Hence,
mastery experiences motivate students to engage in activities that they perceive to be attainable,
which in turn boost their self-efficacy. Self-efficacy theory underlies our assumptions that the
adaptive, mastery nature of TEL would influence students’ mathematics performance, while the
autonomy afforded by TEL would further support positive competence beliefs.
A sense of competence is particularly important in mathematics, which is associated with expectancies for success and self-determined behavior (Ryan & Deci, 2000; Wigfield & Eccles, 2000).
Competence beliefs refer to perceptions about academic ability and attitudes toward success. We
conceptualized competence beliefs with academic competence, a construct validated and measured by the Academic Competence Evaluation Scales (ACES College, DiPerna & Elliott, 2000).
ACES describes students’ beliefs about their academic ability (academic skills domain) and beliefs
about their academic conduct (academic enablers domain) by capturing students’ perceptions
about their most basic characteristics, skills, attitudes and behaviors deemed important to be
successful in school.
Related research
Mastery learning
Traditional instruction is time based, requiring learners to move from topic to topic as defined by
a curricular schedule. In contrast, mastery-based instruction allows learners to move through
2015British
British Educational
Educational Research Association
C©2015
V

4896 British
Journal
of Educational
Technology
British
Journal
of Educational
Technology

Vol 47 No 5 2016

topics at their own pace and skill level, where new topics are introduced only after prerequisite
topics are learned and mastery demonstrated. Bloom (1978) believed the distinction between top
performers and low performers was time. He held that given sufficient time and the appropriate
learning conditions, 95% of students could achieve mastery. A recent experimental study compared students’ motivation to learn with two teaching approaches, mastery and traditional
(Changeiywo, Wambugu & Wachanga, 2011). Students in the mastery group had significantly
higher motivation than those in the traditional group. We were not concerned with comparing
approaches in efforts to identify superior methods; rather, we wished to explore methods that help
students learn. In a meta-analysis of 279 studies involving 22,000 students, Anderson (1994)
consistently found that mastery learning was effective in achieving predetermined learning
objectives.
The mastery approach has been widely used, but not without criticism (Slavin, 1990). Some
concerns relate to findings that students in mastery learning do not perform better on standardized tests than those in traditional education. One could argue that this is more reflective of the
limitations of standardized tests than an assertion about the efficacy of the source of training.
Another concern relates to the potential narrowing of content covered. We contend that achieving depth over breadth is not necessarily a limitation. Some also view mastery learning as a
burden on teachers and school systems because of the frequent feedback, specific guidance and
additional time required by this pedagogy. Leveraging technology to adapt instruction and build
on students’ current capabilities to provide just the right balance of challenge can offset some of
the limitations of a mastery approach and personalized instruction (Eyre, 2007; Keller, 1968). TEL
provides such a balance.
TEL – Technology-Enhanced Learning
TEL refers to any learning that incorporates digital technologies to present content and support
learning (JISC, 2009). We used the term TEL to describe the combined set of pedagogical
approaches used by the university under investigation; specifically, a self-paced, mastery
approach with individualized learning, adaptive instruction, feedback and faculty guidance
(Knewton, 2012). Admittedly, the integration of technology in education is not a new concept;
technology integration dates back to instructional television and filmstrips. However, emerging
technologies such as adaptive learning stand a higher chance of changing education in ways that
television could not. Adaptive and individualized instruction are related concepts often used
interchangeably yet discrete. Adaptive instruction relates to the process used to adapt or select
instructional materials from a range of instructional options and resources. Our TEL assessed
students’ prior knowledge (pretests) and responded in real-time by selecting and presenting
appropriate content for their skill level—adaptive instruction. Individualized instruction is the
product, the customized instruction presented to the learners based on their individualized needs.
Because our TEL selected lesson activities based on skill level, each student had a different
sequence of activities to complete—individualized instruction.
When it comes to adapting instruction to match students’ needs, it is important to do so based on
prior knowledge (Lalley & Gentile, 2009). Prior knowledge helps build meaningful connections
and enhances retention (Dochy, Segers & Buehl, 1999). Connecting new information to prior
knowledge is particularly important for novice learners, who tend to organize new knowledge
around explicit or literal pieces of known information. Prior knowledge builds a foundation for
new learning; this alignment promotes success, which in turn enhances competence beliefs. This
is consistent with previous findings on the dependence of performance on achievement motivation and competence beliefs (Hirschfeld, Lawson & Mossholder, 2004).
The literature supports mastery learning and adapting instruction to students’ capabilities to
stimulate efficacy beliefs, motivation, learning and performance. Returning to our subject of
©
C 2015 British
V
British Educational
EducationalResearch
ResearchAssociation
Association

in college
math
remediation 897
5
TELTEL
in college
math
remediation

investigation, the pedagogical affordances of TEL systematically aligned course content to the
students’ capabilities and allowed them to work at their own pace with the proper level of challenge to achieve mastery. Therefore, we inferred that TEL would result in higher levels of competence beliefs and contribute to successful mathematics remediation.
Method
Using a pretest and posttest design, we examined extant data collected as part of a longitudinal
study from consenting college students enrolled in a remedial mathematics course for college
algebra and college mathematics. During the original data collection, because of the nature of the
intervention and the potential benefits to all, it was deemed unethical to use control groups; thus,
all first-year mathematics students received the TEL approach. Our sample represented a small
part of these data (one course in a single semester).
Participants
Our data described 2880 students enrolled in the remedial mathematics course, offered during
the fall 2012 semester. To enroll in a first-year level college mathematics course, students were
required to obtain a passing score (≥40 or ≥30 points, depending on their course of study) on the
Assessment and Learning in Knowledge Spaces (ALEKS) placement test. Students were placed in
the remedial course when their scores fell below the required thresholds. The sample represented
27% of the 2012 first-year university enrollment. Of the 2880 students, 1970 were active
enrollments (56% female). The majority (78%) were between the ages of 18 and 20. The ethnic
distribution was 52% white, 26% Hispanic and 11% African American, and the remaining 11%
comprised various races.
The TEL approach
The adaptive component of the TEL approach presented students with video lectures, demonstrations, worked-out examples, practice activities, specific and explanatory feedback, and assessments (formative and summative). The system adapted instruction by continually assessing
students, using thousands of data points, to determine—in real time—the most appropriate
individualized learning path. These learning paths included topics students were ready to master
relative to their known proficiencies. The assessments measured students’ performance, individual ability, specificity of topic, “guessability” of the question and many other variables
(Knewton, 2012). Instructor-led sessions supplemented the adaptive instruction, where faculty
provided guidance as students applied concepts. Content included ratios and proportions, the
number system, expressions and equations, geometry, statistics and probability, functions and
algebra. Students and faculty could track progress through a dashboard (Figure 1), containing all
information related to the course, including earned scores, viewed and completed lessons, feedback received and suggested lessons to complete. The cost to implement the TEL approach was not
substantially different from traditional courses. The software developer absorbed all initial costs
and collected dues on a per-student basis, which amount to the standard cost associated with the
development and sustainability of traditional educational materials. The number of faculty
members and instructional assistants used for this approach was also not different from the
typical course structure at this university (ratio of 1 faculty or instructional assistant to 25
students).
Procedure
The course started out with all lessons in a locked state. Students chose lessons in any order;
however, to unlock a lesson, they were required to demonstrate mastery of the content (the
mastery threshold set by the university was 70%). Students had the freedom to complete the
course in as little as a few weeks or as long as two full semesters. Students were required to attend
two weekly, 75-minute, faculty-led sessions. These sessions provided students with a venue to
2015British
British Educational
Educational Research Association
C©2015
V

6898 British
Journal
of Educational
Technology
British
Journal
of Educational
Technology

Vol 47 No 5 2016

Figure 1: Top image shows faculty dashboard; bottom image shows student dashboard with resources

©
C 2015 British
V
British Educational
EducationalResearch
ResearchAssociation
Association

in college
math
remediation 899
7
TELTEL
in college
math
remediation

obtain answers or clarification, apply skills and receive feedback. The first session was an “open
computer lab” where students worked through the adaptive component of the TEL; the second
session focused on problem solving. During the problem-solving session, faculty members used
students’ prior performance as a guide to divide students into groups of five to six and present
them with a developmentally appropriate problems. Students received guidance from faculty or
one of three instructional assistants present at these sessions.
When a student completed all the required lessons, the final exam was unlocked and administered; there were no penalties associated with retaking the final exam (up to three attempts).
Passing this exam marked the student status as complete and eligible to enroll in a first-year
mathematics course. We used the final exam scores and course completion as indicators of
successful remediation.
Obtaining mastery indicators
Pretests and posttests served as system indicators to adapt instruction; while only the highest
scores were stored, the system tracked the number of attempts per student, per lesson. If students
demonstrated 100% mastery in a pre-lesson test, the system marked the lesson as placed out and
deemed those students eligible for the next level. Those who scored below the required 70% had
to complete the entire lesson. Once students reached the end of a lesson, they had to demonstrate
mastery with a second posttest. If students did not reach mastery on the second posttest, they
entered focus mode (Figure 2). During focus mode, the software presented foundational knowledge
going as far back as necessary (up to 6th grade math). Upon completing focus mode, students
took a third posttest. Those who did not reach mastery on this posttest remained in focus mode
until able to demonstrate mastery.
Academic competence indicators
We used the ACES (Academic Competence Evaluation Scales, self-report version; DiPerna &
Elliott, 2000) to document students’ perceived academic skills and academic enabling
behaviors. Academic skills assessed beliefs about academic ability with three sub-scales: mathematics and scientific inquiry; reading and writing; and critical thinking skills. Academic enablers assessed beliefs about academic behaviors with four sub-scales: interpersonal skills; study
skills; motivation; and engagement. Academic competence is consistent with competence beliefs
and efficacy in that it is context specific. For example, the math sub-scale rated students’ ability,
in comparison with other students at their same academic level, on activities such as computation, breaking down complex problems and testing hypothesis. As another example, the motivation and study skills sub-scales rated the frequency with which students exhibited behaviors
such as class participation, assuming responsibility for own learning and preparing for exams.
These normed sub-scale ratings placed students along a competence continuum of developing,
competent and advanced.
Statistical treatment
Academic competence
We expected TEL to have a positive impact on academic competence. To offset the use of an intact
group, we utilized students’ prior achievement as covariates (operationalized through Grade
Point Average (GPA), ALEKS and SAT scores). We conducted a repeated measures analysis of
covariance (ANCOVA) using each of the ACES sub-scales scores as levels (pre- and post-TEL:
reading/writing, math/science, critical thinking, motivation, engagement, study skills, impersonal skills). Given the independence of each ACES sub-scale, we conducted follow-up pairwise
comparisons using paired sample t-tests (two levels each) to evaluate which mean differences
were statistically significant. To evaluate the practical implications of the ACES results, we plotted
scores along the ACES competence continuum. This continuum facilitated the construction of
2015British
British Educational
Educational Research Association
C©2015
V

8900 British
Journal
of Educational
Technology
British
Journal
of Educational
Technology

Figure 2: The process of obtaining mastery

©
C 2015 British
V
British Educational
EducationalResearch
ResearchAssociation
Association

Vol 47 No 5 2016

in college
math
remediation 901
9
TELTEL
in college
math
remediation

confidence intervals (CIs) around students’ scores, which provided a range of scores within which
their actual scores were likely to fall—from “developing” to “competent” to “advanced.”
Mathematics remediation
We operationalized successful remediation through course completion and gain scores. Course
completion served as indicators of performance as designated by the final exam score (passing
score, 21). We conceptualized learning gains by computing gain scores from pretests and
posttests obtained throughout the course. To explore how TEL contributed to remediation, we
examined learning gains using the 52 core lessons required to complete the course grouped by
number of posttest attempts. The number of attempts was indicative of student performance. For
example, one posttest indicated that students attained mastery sometime after the lesson introduction but before the end of the lesson. Two attempts indicated that students attained mastery
after completing the full lesson. Three attempts indicated that students attained mastery after
entering focus mode. We evaluated the average gains per lesson using a two-way analysis of
variance (ANOVA) (variables: lesson gain scores and number of posttests taken).
Results
Does college students’ academic competence change after participating in a remedial mathematics
course using TEL?
Because of missing data, the ANCOVA analysis included 270 cases with complete data across all
10 variables (3 prior achievement and 7 ACES sub-scales). We found a statistically significant
change, F(13, 233) = 8.70, p < 0.01, η2 = 0.33, in students’ academic competence.
Paired sample t-test results were statistically significant on all three sub-scales measuring academic skills. Students experienced a statistically significant increase in their judgment about their
ability in mathematics, t(580) = −10.40, p < 0.01, about their reading skills, t(580) = −4.67,
p < 0.01, and their critical thinking, t(580) = −4.54, p < 0.01. There was a significant decrease in
self-reported study skills, t(580) = 4.50, p < 0.01, and motivation, t(580) = 2.45, p = 0.02. The
sub-scale math/science had a moderate effect size, Cohen’s d = 0.40 (Cohen, 1992).
Most of the ACES scales remained at their original range of competence within the ACES competence continuum (Figure 3). The math competence beliefs range shifted from the developing
range (before TEL 90% CI [27.06, 33.60]) to the competent range (after TEL 90% CI [30.06,
36.06]), indicating an observable change in students’ math competence beliefs.
How does TEL contribute to college students’ mathematics remediation?
The median final exam score was 21.33 (n = 1869). Results showed that 75% of the students
who completed the remedial course were eligible to enter a first-year college-level mathematics
course with an additional 18% on track for completion the following semester. These 18%
included students who had completed all the course work, at the time of data analysis, but had
not taken the exam and those who had taken the exam but their grades were not processed.
The mean gain score was 40.61, SD = 11.86, n = 1303. The mean gain score for lessons with two
posttests was 37.49, whereas the mean gain for lessons with one posttest was 9.08. The two-way
ANOVA showed significant results with a large effect size, F(2, 52) = 3.71 p = 0.03, η2 = 0.14,
indicating a relationship between students’ learning gains and the extent to which they completed the lessons.
Discussion
We examined the effects of TEL on college students’ academic competence and their mathematics
remediation, which yielded promising results. There was a marked difference among the competence sub-scales, with a favorable shift in students’ perceptions about their math ability. Mathematics efficacy likely experienced a more measureable gain because TEL afforded students
2015British
British Educational
Educational Research Association
C©2015
V

British
BritishJournal
Journalof ofEducational
EducationalTechnology
Technology

Vol 47 No 5 2016

Figure 3: Academic competence continuum. Confidence intervals (CIs) plotted based on mean scores for each sub-scale

10
902

©
C 2015 British
V
British Educational
EducationalResearch
ResearchAssociation
Association

TEL
collegemath
mathremediation
remediation
TEL
in in
college

11
903

repeated success, which enhanced their perceptions about their mathematics ability. Our findings
are consistent with the contributions of self-efficacy to academic success (Bandura, 1997) and
provide support for the potential benefits of utilizing technology to create conditions that stimulate positive academic beliefs.
Surprisingly, we showed a significant decrease in motivation and study skills. These decreases are
likely attributable to the adaptive system itself. It is possible that students experienced a reduced
need for self-initiated strategies as the system presented learning paths to them; this may also
explain the decrease in motivation. The structure and guidance of the system elicit a lower level
of self-initiated behavior and thus may weaken motivation. These important findings suggest TEL
may be a good fit for remedial students, by allowing the system to compensate for low motivation
or lack of study skills.
Course completion rates provide some evidence that TEL played an important role in successful
remediation. Students’ performance was contingent on lesson viewing. On average, students who
viewed lessons in their entirety appeared to overcome their mathematics skill deficiencies by the
end of the course, as evidenced by the higher gain scores, compared with those who did not view
lessons in full. Consistent with the literature on remediation (Bettinger & Long, 2009) and
mastery (Bloom, 1978), our results show that TEL, when fully embraced, can contribute to
reducing students’ mathematic deficiencies and enhance competence beliefs. Overall, our findings advocate for utilizing technology to enhance learning and promote academic success.
Limitations
We present our findings conditional to a number of limitations. We do not claim any causal
evidence, as we lacked a comparison group. Our convenience sample, although large, may not be
representative of all students in need of mathematics remediation; thus, the generalizability of
the findings is unclear. It is possible that the positive effects experienced resulted from the novel
approach and may be subject bound (mathematics). We also did not evaluate students’ perceptions about the program. Finally, we were not allowed access to data on retention and students’
success in subsequent mathematics courses. However, we know that the TEL remediation course
completion rate was 15% higher than the national average (Complete College America, 2012)
and that this type of course has been continued for 3 years since our study was completed,
suggesting it is considered both cost-effective and an academic sound alternative to traditional
remedial courses. Given these limitations, there is an opportunity to improve upon the present
study with future studies.
Implications for future research
The use of assessment for learning was one of the distinguishing features of TEL. Assessment for
learning refers to the use of formative and summative assessments as opportunities to enhance
learning. This includes the practice of providing frequent, timely feedback describing how learners may improve their performance and/or meet expectations. Using assessment for learning with
remedial students represents a rich area for investigation. Future research could also focus on
evaluating the effects of a mastery approach on the performance of remediated students in
subsequent courses, as one would expect their performance to be indistinguishable from students
who did not need remediation. Another area to explore is the sustainability of efficacy beliefs. The
heightened perceptions of math ability resulting from successful remediation should translate
into enduring changes in efficacy beliefs. While the present investigation provided an explanation
and modest evidence in these areas, longitudinal research is needed.
Conclusions
Given the present academic plight of thousands of US high school graduates underperforming
and the nation’s desire to improve mathematics achievement, our findings provide promising
2015British
British Educational
Educational Research Association
C©2015
V

12
904

British
BritishJournal
Journalof ofEducational
EducationalTechnology
Technology

Vol 47 No 5 2016

evidence and justification for future research exploring the benefits of TEL in remediation. We
must develop learning approaches that build on students’ current ability and support efficacy
beliefs by providing autonomy with the appropriate level of challenge to promote academic
success. Our results illustrate how leveraging technology to support effective pedagogies can
provide individualized learning, enhance competence beliefs and contribute to academic success.
Academic success is as much about student performance as it is about the curriculum and its
pedagogy.
Statements on open data, ethics and conflict of interest
Open data statement
The data utilized for this investigation will be accessible upon request by writing to steve_elliott@
asu.edu. The British Journal of Educational Technology may if desired store the data file in a local
repository; however, data should not be released without prior written consent from Dr. Elliott.
Additionally, we expect that those utilizing these data will share their findings with us and offer
proper attribution to our study.
Ethical guidelines
Data in this study represent a small sub-set of a very large data set collected in compliance with
the institutional review board (IRB) ethical guidelines and pursuant to federal regulations, 45
CFR Part 46.101(b)(1) (2) (4). IRB exempt status: Protocol No. 1108006723; Study Title: Student
Success in Math—Longitudinal Study. Data were de-identified and kept in password-protected files.
Prior to the intervention, all students received information about the program and were informed
that their scores would be used for future studies investigating educational practices and/or
exploring the effects of the intervention. While all students received the TEL intervention, only
those who consented to share their information were included in the study.
Declaration of interest
We, the authors, have no conflicts of interest to report and are solely responsible for the content
and the writing of this paper.
References
ACT. (2012). The condition of college & career readiness (National Report). Retrieved March 4, 2013, from
http://act.org/research-policy/college-career-readiness-report-2012/
Anderson, S. A. (1994). Synthesis of research on mastery learning. Information Analyses, 18, 1–15.
Arizona Department of Education. (2013). Arizona’s college and career ready standards. Description of standards from the Arizona Department of Education. Retrieved December 1, 2013, from http://
www.azed.gov/azccrs/files/2013/10/azccrs_math_generaloverview_10072013.pdf
Bandura, A. (1997). Self-efficacy: the exercise of control. New York, NY: W.H. Freeman and Company.
Bettinger, E. & Long, B. (2009). Addressing the needs of underprepared students in higher education: does
college remediation work? Journal of Human Resources, 44, 3, 736–771. doi: 10.1353/jhr.2009.0033.
Bloom, B. S. (1978). New views of the learner: implications for instruction and curriculum. Educational
Leadership, 35, 7, 563–576.
Changeiywo, J. M., Wambugu, P. W. & Wachanga, S. W. (2011). Investigations of student’s motivation
towards learning secondary school physics through mastery learning approach. International Journal of
Science and Mathematics Education, 9, 1333–1350.
Cohen, J. (1992). A power primer. Psychological Bulletin, 112, 1, 155–159.
Complete College America (2012). Remediation: higher education’s bridge to nowhere (Report). Retrieved
March 4, 2013, from http://completecollege.org/docs/CCA-Remediation-final.pdf
van Dinther, M., Dochy, F. & Segers, M. (2011). Factors affecting student’s self-efficacy in higher education.
Educational Research Review, 6, 2, 95–108. doi: 10.1016/j.edurev.2010.10.003
DiPerna, J. C. & Elliott, S. N. (2000). The Academic Competence Evaluation Scales college manual. San Antonio,
TX: Pearson.
Dochy, F., Segers, M. & Buehl, M. M. (1999). The relation between assessment practices and outcomes of
studies: the case of research on prior knowledge. Review of Educational Research, 69, 2, 145–186.
Eyre, H. (2007). Keller’s personalized system of instruction: was it a fleeting fancy or is there a revival on the
horizon? The Behavior Analyst Today, 8, 3, 317–324.
©
C 2015 British
V
British Educational
EducationalResearch
ResearchAssociation
Association

TEL
collegemath
mathremediation
remediation
TEL
in in
college

13
905

George, M. (2010). Ethics and motivation in remedial mathematics education. Community College Review,
38, 1, 82–92. doi: 10.1177/0091552110373385.
Goodyear, P. & Retalis, S. (2010). Technology-enhanced learning [White paper]. Sense Publishers. Retrieved
February 7, 2014, from https://www.sensepublishers.com/media/1037-technology-enhanced-learning
.pdf
Hirschfeld, R., Lawson, L. & Mossholder, K. (2004). Moderators of the relationship between cognitive ability
and performance: general versus context-specific achievement motivation. Journal of Applied Social Psychology, 34, 11, 2389–2409.
JISC (2009). Effective practice in a digital age: a guide to technology-enhanced learning and teaching [White
paper]. Retrieved February 7, 2014, from http://www.jisc.ac.uk/media/documents/publications/
effectivepracticedigitalage.pdf
Keller, F. (1968). Good-by teacher. Journal of Applied Behavior Analysis, 1, 79–89.
Knewton (2012). Adaptive learning, building the world’s most powerful education recommendation engine [White
paper]. Retrieved January 25, 2013, from http://www.knewton.com/wp-content/uploads/knewtonadaptive-learning-whitepaper.pdf
Lalley, J. P. & Gentile, J. R. (2009). Adapting instruction to individuals based on the evidence, what should it
mean? International Journal of Teaching and Learning in Higher Education, 20, 3, 462–475.
Lee, J. (2012). College for all: gaps between desirable and actual P-12 math achievement trajectories for
college readiness. Educational Researcher, 41, 2, 43–55. doi: 10.3102/0013189X11432746.
NCEA (2011). National Center for Educational Achievement report: the 20 non-negotiable characteristics of higher
performing school systems (Report). Retrieved March 4, 2013, from http://www.act.org/research/
policymakers/pdf/Non-Negotiable-Characteristics.pdf
Pajares, F. & Usher, E. L. (2008). Self-efficacy, motivation, and achievement in school from the perspective of
reciprocal determinism. In M. Maehr, S. Karabenick & T. Urdan (Eds), Social psychology perspectives:
advances in motivation and achievement Vol. 15 (pp. 391–424). Bingley, UK: Emerald Group Publishing.
Ryan, R. M. & Deci, E. L. (2000). Self-determination theory and the facilitation of intrinsic motivation, social
development and well-being. American Psychologist, 55, 1, 8–78. doi: 10.1037//0003-066x.55.1.68.
Schunk, D. H. (1991). Self-efficacy and academic motivation. Educational Psychologist, 26, 3–4, 207–231.
Slavin, R. E. (1990). Mastery learning re-reconsidered. Review of Educational Research, 60, 2, 300–302.
Toshalis, E. & Nakkula, M. J. (2012). Motivation, engagement, and student voice: the students at the center series.
[A Jobs for the Future Project report]. Retrieved May 6, 2014, from http://studentsatthecenter.org/topics/
motivation-engagement-and-student-voice.
Wigfield, A. & Eccles, J. (2000). Expectancy-value theory of achievement motivation. Contemporary Educational Psychology, 25, 1, 68–81. doi: 10.1006/ceps.1999.1015.

2015British
British Educational
Educational Research Association
C©2015
V

w

Fermi National

Accelerator

Laboratory
FERMILAB-TM-2004
Revised

A Virtual Library of Technical Publications

The Technical Publications Working Group
Elizabeth Anderson, Robert Atkinson, Elizabeth Buckley-Geer, Cynthia Crego, Lisa Giacchetti,
Stephen Hanson, David Ritchie, Jean Slisz, Sara Tompson and Stephen Wolbers
Fermi National Accelerator Laboratory
P.O. Box 500, Bataavia,Illinois 60510

October 1997

Presented at the Sixth International World Wide Web Conference, Santa Clara, California
Presented at Infovum ‘97, Oak Ridge, Tennessee, May 6-8,1997

%

Operated by Universities

Research Association

Inc. under Contract NO. DE-AC02-76CH03000

with the United States Department

and

of Energy

Disclaimer
This repoti was prepared as an account of work sponsored by an agency of the United States
Government. Neither the United States Government nor any agency thereof, nor any of their
employees, makes any warranty, express or implied, or assumes any /egga/liability or
responsibility for the accuracy, completeness, or usefulness of any information, apparatus,
product, orprocess disclosed, or represents that its use would not infringe privately owned
rights. Reference herein to any specific commercial product, process, or service by trade name,
trademark, manufacturer, or othenuise, does not necessarily constitute or imp/y ifs endorsement,
recommendation, or favoring by the United States Government or any agency thereof. The views
and opinions of authors expressed herein do not necessarily state or reflect those of the United
States Government or any agency thereof.

Distribution

Approved for public release; further dissemination

unlimited.

A Virtual Library

of Technical Publications

The Fermilah Technical Publications

Fileserver Project

Pilot and Phase I Report

By the Technical

Publications

Working

Group

October 1997

Introduction
Through a collaborative effort, the Fermilab Information Resources Department and Computing Division
have created a “virtual library” of technical publications that provides public access to electronic full-text
documents. This paper will discuss the vision, planning and milestones of the project, as well as the
hardware, software and interdepartmental cooperation components.

Driving Forces
In 1993 the head of the Publications Office and the Library administrator proposed to the head of
Laboratory Services Section (LSS) a project to make Fermilab full-text scientific and technical
publications available on the Internet. The project proposal was in part driven by the DOEKENDI
Electronic Exchange Initiative and the 1993 DOE Electronic Exchange of Scientific and Technical
Information Strategic Plan. (The goal identified in the strategic plan was to make electronic exchange of
full-text DOE scientific and technical information the norm by the year 2000.) However, the major force
driving this project was customer demand. The physics community, which the Publications Office and
the Library serve, wanted faster, more convenient access to scientific information. From the time the
project was proposed, the vision of the working group has been to make the full-text of scientific and
technical information developed at Fermilab fully accessible from every researcher’s desktop.

Reorganization and Cooperation
The head of Laboratory Services Section approved the proposal and also the request to seek project
assistance from the Computing Division. He then submitted the proposal to the Directorate along with a
request for equipment funding. After the proposal was approved by the Directorate and equipment funds
were earmarked, the proposal was then sent to the Computing Division. The deputy head of the
Computing Division arranged two meetings with the head of the Publications Office, the Library
2

administrator and members of the Operating Systems Support Department to discuss the proposal and the
level of support sought from the Computing Division. She agreed to provide personnel to assist LSS with
writing the hardware specifications and completing the requisitioning process for the purchase of a
technical publications fileserver. She also agreed to appoint Computing Division staff to serve with
Laboratory Services Section staff on a Technical Publications Fileserver Working Group.

The appointed working group was comprised of members of the Publications Office, the Library, and the
Computing Division’s Operating Systems Support Department, Databases and Information Department,
and the CDF Computing and Analysis Group. The group members included information specialists,
computing specialists and physicists. Their initial charge was to study the feasibility of full-text
electronic exchange of technical reports and establish support parameters. After conducting a feasibility
study that showed the project to be viable, the working group was then asked to refine the project goals,
identify the overall project strategy, develop an initial project plan and implement a pilot program.

Shortly after the working group was convened, the Library and the Publications Office merged to form
the Information Resources Department. The purpose of the reorganization was to consolidate resources
to enhance information management and facilitate the electronic exchange project.

Over the next several months, the working group laid the groundwork for the fileserver project which
included formalizing with a Memorandum of Understanding (MOU) the partnership between the
Computing Division and Information Resources Department, making hardware recommendations,
securing earmarked equipment funds, and developing an acquisition and implementation plan.

Under the MOU, the Computing Division agreed to provide technical support for the project, which
included setting up the server and peripheral equipment, housing the server, and providing system
backups and upgrades. The Information Resources Department agreed to provide primary system
administration of the server, workflow management, information organization, and customer service and
training.

Project Implementation
The fileserver system hardware was purchased in August 1994 and was in full production by April 1995.
The fileserver machine is a Sun Spare 20/612MP, named fnalpubs.fnal.gov. The Spare station nms the
Solaris operating system and includes 256MB of RAM and two 1.05GB hard drives as well as an 8.4GB
multidisk pack for a total of 1OGB of local disk. The system also uses Andrew File System (AFS) for
part of its disk needs. This configuration was recommended by the Operating Systems Support Group.

3

AFS space is not machine dependent, and allows for easy file access anywhere on the disks from any
privileged AFS machine in the world.

Procedures

Delivery Format
While planning for the pilot program, one of the first decisions the working group had to make was
deciding what delivery format to use. The group chose Postscript after studying trends in electronic
publishing and meeting with members the SLAC Publications Office, who had been involved in a similar
project for two years. (See SLAC trip report). Postscript was already an accepted format for electronic
information exchange within the physics community and it also represented the most affordable delivery
format in terms of labor, software and hardware costs. Other formats considered were Standardized
General Markup Language (SGML), Hypertext Markup Language (HTML) and Portable Document
Format (PDF). These formats were rejected for the initial pilot because SGML and HTML were very
labor-intensive formats and PDF required an investment in software and was not accessible from
machines running VMS and some flavors of UNIX operating systems.

After Postscript was chosen as the delivery format for the pilot program, the workflow procedures were
established. It was decided that reports would be submitted in Postscript format to the technical
publications server by the authors. Upon receiving, the Publications Office’s technical editor would test
the reports using the Ghostview/Ghostscript

suite of products to ensure that they could be viewed and

printed. The technical editor would then create a LaTeX cover sheet that would be saved as Postscript.
The Postscript cover sheet would then be electronically attached to the paper using a script written by
Computing Division staff. Finally the paper would be retested to ensure that it was still viewable and
printable after the cover sheet was attached. (A cover sheet for each report is required by DOE’s Office
of Scientific and Technical Information. The cover sheet must contain the Fermilab report number, the
Department of Energy disclaimer, distribution authorization, and the Universities Research Association,
Incorporated and DOE acknowledgment statement.)

In May 1995, the first technical publications Web pages were published and policies and procedures by
which Fermilab authors would transmit their reports to the server were developed and distributed. In
June the first full-text preprints in Postscript format were made available on the Internet via the World
Wide Web and AFS space. The serving of the first reports successfully ended the pilot program and
Phase I of the technical publications fileserver project began.

4

Postscript

errors

During Phase I of the project, the procedures for submitting documents worked fairly well. Occasionally
files contained Postscript errors that prohibited the technical editor from either attaching the cover sheet,
printing or viewing the final document. A Computing Division staff member wrote a script that solved a
common Postscript problem found on files generated on Macintosh computers using Microsoft Word.
Most of the other problems were unique and had to be handled on a case-by-case basis. During the initial
pilot program, the Publications Office and Computing Division staff made an effort to correct as many
Postscript errors as staff time allowed. The working group found, however, that correcting many of the
errors was too time-consuming and placed too heavy of a burden on the Publications Office, which had
only one staff member working full-time on this project. For Phase I of the project, the working group
decided that the responsibility for providing a good Postscript file had to ultimately rest with the author.
If the Publications Office, with assistance from Computing Division staff, could not easily fix a paper
containing a Postscript error, the author was asked to correct the file and resubmit the document. If the
author could not fix the file, the paper was distributed in paper format only. During the pilot and Phase I
of the project, the Publications Office was able to serve 88% of all submitted reports. (See Figure 1)

Twlnical P”blicstbnr Filesewer PrOjnt

q * Avallabls
,*Plbl,shd

Figure I

i

Orga”izirlg directories
After cover sheets were made and the reports were tested, all technical reports were stored in a consistent
directory structure called /archive. Subdirectories were created for each year (e.g., larchive0996).
Further subdirectories represent the type of report (i.e., conference proceeding, physics note, etc.) and
then the reports were given file names that corresponded to the report numbers assigned to the reports by
the Fermilab Publications Office. For example, a conference report with Fermilab report number
FERMILAB-Conf-96/002

resides on the fnalpubs server at: /archive/l 996/conf/Conf-96.022.html.

The

Uniform Resource Locator (URL) for that report is http://fnalpubs.fnal.gov/archive/l996/conf/Conf-96022.html.

Expanding Formats
In 1996 the working group decided to expand the delivery format to include Portable Document Format
(PDF) and in 1997 compressed Postscript documents were added. PDF became an option at this time
because the Publications Office was able to budget for the software, the technical editor had become
proficient at posting papers and was able to add the additional step to her work processes, and the
Laboratory moved away from supporting the VMS operating system for which there was no PDF reader.
Compressed Postscript was added to enable faster retrieval of online documents and again was possible
because of the speed at which the technical editor could process reports.

The following procedures were added: The technical editor creates the PDF files from the authorsubmitted Postscript documents using Adobe Acrobat Distiller and the compressed Postscript files using
gzip. All three formats are then posted to the Web along with file size information.

Special Services Offered
As the project progressed, additional customer services were added. Using Web-based forms, customers
can request report numbers, order paper copies of reports, and subscribe to an e-mail listserv that
notifies subscribers when new reports are posted. Also, the Library created a weekly list on their Web
site of new technical reports received in the Library, including Fermilab publications. Links to the full
text of the reports are added when available. (See Appendix 1 for related URLs).

The Library’s Role
Fermilab’s technical publications fileserver project dovetailed with the acquisition of a World Wide Web
search module for the Library online catalog. The Library’s catalog is the search engine for the

Laboratory’s technical publications and provides hyperlinks to the full-text documents. Users can search
the catalog by author, title, call number, keyword, and combination searches.

Creating the Virtual Library
In the summer of 1995, the Fermilab Library had the opportunity to beta test Data Research Associate’s
(DRA) new Web-based interface to their online catalog product. The Library owns the DRA package of
integrated library automation products. The new interface module works in tandem with these, layering
HTML pages and the Web server over existing T&et-accessible
system nms under the VMS operating system on a MicroVAX

databases. The Library’s automation
3400 minicomputer.

The Library was very interested in a Web forms-based interface that would rely on industry standards
such as Hypertext Transfer Protocol (HTTP), Hypertext Markup Language (HTML) and 239.50 (the
ANSI/NISO Standard for Information Retrieval, Service and Protocols.). The high-energy-physics
community was using the World Wide Web for the purpose of sharing physics data and analyses. The
Library staff therefore expected that a Web interface for the Library catalog would be quickly embraced
by its users.

Because of the Library’s experience with the World Wide Web and the high level of sophistication of the
online catalog users, the Fermilab Library was able to play an instrumental role in the development of the
DRA Web interface module. Weekly bug reports, suggestions and comments were written by the
systems librarian and sent to the vendor during the beta test period. Some of the suggestions made were
requests for additional formats for record output and ideas for different display parameters.

After a robust test period, the Library received and installed the general release of this software in
October 1995. The Library customized the vendor-supplied HTML pages to meet its needs. The Library
eliminated many of the graphics supplied by the vendor, rewrote help examples so that they were more
appropriate for the Fermilab user community, and tailored the pages to match the Information Resources
Department’s existing Web pages.

The Web module was 239.50 and HTTPlHTML

compliant and made use of Machine Readable

Cataloging (MARC) standards for bibliographic information. The MARC Standard for Electronic
Location and Access makes use of a specific field in the bibliographic record (the 856 field) to denote
Electronic Location and Access for a particular work. The standard called for the subfield 71” of the 856
field to be used for URLs and allowed the subfield “3” to be used for descriptive labels for the URLs.

The Library had been adding URLs to many of the online catalog bibliographic records since 1994. In
1995, the Library began adding Fermilab technical report URLs. At that time users could copy and paste
addresses from the catalog into their graphical Web browsers.

Immediately upon installing the new DRA Web interface, the preprint URLs the Library had been adding
to the catalog records became hypertext links to the full-text documents. These direct links made the
online catalog more interactive than the previous method of copying and pasting URLs into browsers.
Instead of providing only pointers to resources, users could now access the full documents directly from
the online catalog.

The Web interface also allowed the Library to add links in the online catalog to electronic journals,
Internet information resources, and prepublished physics, engineering and technical papers from other
institutions. In this arena, the Library’s traditional need to acquire preprints has been diminished, if not
eliminated, thereby laying the foundation for a tmly virtual library environment.

Changing Role
With the new Web interface, the Information Resources Department saw its roles expanding from that of
information providers to facilitators of information exchange. Part of the new responsibilities included
providing training for customers and making them aware of the tools they need to access technical
reports on the Web. Information Resources staff needed to provide tools by which its customers could
easily view and print the online documents provided in Postscript and PDF formats. The
GhostviewlGhostscript

suite of products and PSTool were recommended for viewing and printing

Postscript and Adobe Acrobat Reader was recommended for viewing and printing PDF. These tools were
installed on the public access catalog machines in the Fermilab Library. Links on the Information
Resources Web pages point to sites where the products can be downloaded.

Easy Access
To enhance patron access to the technical publications online collection, the Information Resources
Department installed four public access workstations in the Library. The machines are Tektronics XP356
x-stations. The x-terminals are configured with limited access logins, and are set to run the Netscape
browser upon startup. The Library welcome page is set as the “home” page. This page includes a large
search button link to the online catalog Web interface. All the machines that comprise the Information
Resources Department’s virtual library system are accessible from one or more of Fermilab’s networks,
and all of them can connect to the Internet. Currently Ethernet is used for most of the network
connections.
8

Rapid Growth
Since the technical publications fileserver project began, server usage has increased dramatically. The
Information resources Department began gathering usage statistics in October 1996. Fnalpubs usage has
grown steadily from 1,062 hits in October 1996 to 66,375 hits in September 1997. (Figure 2).

Figure 2

The Future
Having successfully initiated the technical publications fileserver pilot and Phase I programs, the
Technical Publications Working Group is looking to the future. The group has completed a year long
study that produced a three-year plan and Phase II recommendations for project expansion.

The recommendations from the working group include developing scripts to automate many of the
workflow processes, streamlining the author submission process, a legacy document conversion project,
and continued exploration and evaluation of delivery formats that provide easier access, greater
searchability and device independence.

The collaboration between the Computing Division and the Information Resources Department has been
an excellent cooperative arrangement. The Computing Division provided the technical expertise to make
this project successful while the Information Resources Department provided the workflow management,
information organization and customer service components. Providing for the information needs of the
physics community in the year 2000 and beyond will require the continued cooperation and expertise of
both Computing Division and the Information Resources Department.
9

Appendix 1 - URLs
Fermilab Information Resources Department
http://fnalpubs.fnal.gov/index.html

Library Home Page
http://fnalpubs.fnal.gov/library/welcome.html

Publications Office Home Page
http://fnalpubs.fnal.gov/techpubs/welcome.html

Library On-line Catalog
http://fnlib.fnal.gov/K’IARION

Preprint Announcement Mailing List Registration
http://fnalpubs.fnal.gov/techpubs/maillist.html

Fermilab Preprint Number Request Form
http://fnalpubs.fnal.gov/techpubs/numreq.html

Monthly Lists of Fermilab Preprints
http://fnalpubs.fnal.gov/techpubs/pubs_lists.html

Instructions for Electronic Submission
http://fnalpubs.fnal.gov/techpubs/guidelines.html

Processing a Fermilab Technical Report
http://fnalpubs.fnal.gov/techpubs/guideaddendum.html

Feedback
http://fnalpubs.fnal.gov/techpubs/feedback.html

10

Leveraging Social Media to Support
Collaborative e-Learning
Robert K. Atkinson
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
Tempe, United States
robert.atkinson@asu.edu

EXTENDED ABSTRACT
The advent of social media has spawned a surge of opensource, user-created intelligence available through online
media such as blogs, wikis and social networks. This session
will outline reflections on and work towards harnessing the
power of social media for instructional purposes
by incorporating empirically-derived
instructional
design
principles to scaffold their use. It specifically addresses the
current and potential uses of social media in a variety of formal
and informal learning contexts. The session will include
examples for applying “traditional” best practices for
implementing
instructionally-enhanced
social
media
applications for collaborative e-Learning.

learning needs. Now that social media is being recognized for
it’s potency as a learning resource, questions arise.
Why should we force Twitter and Facebook into the
classroom when it isn’t clear what learning affordances are
associated with them? The processes of human learning
evolved over millennia while technology evolves at a dizzying
pace. Most digital media including most social media was not
designed with learning in mind. Why should educators
constantly chase the latest technology with the blind hope that
we can jury-rig it to enhance learning? We have a relatively
good handle on how people learn. At the very least, we have
theories, models and strategies designed and to facilitate
learning. Many of those models and strategies are empirically
proven. With our understanding of human learning in a variety
of contexts, I argue that we could put that understanding to
good use to redesign social media for learning, rather than
redesign learning for social media.

A. Social Media
The advent of social media has started a surge of opensource intelligence via online media such as blogs and social
networks. Since more and more people are participating in
social media activities, it has generated enormous amounts of
collective wisdom or open-source intelligence. Social media
has allowed the masses not only to contribute and edit
posts/articles through blogs and wikis, but also enrich the
existing content by providing tags or labels; hence turning
former information consumers into the new producers.
Allowing the masses to contribute or edit has also increased
learning potential for people unlike Web 1.0, where the ability
to create content was limited. Social media applications such as
Facebook, Twitter, Blogger, and Youtube, facilitate people of
all walks of life to express their thoughts, voice their opinions,
and connect to each other anytime and anywhere. For instance,
popular content-sharing sites like Del.icio.us, Flickr, and
Howcast allow users to upload, tag and comment different
types of contents (bookmarks, photos, videos). Users registered
at these sites can also become friends, a fan or follower of
others. The prolific and expanded use of social media has
turned online interactions into a vital part of human experience.

Traditional media (e.g, books, video and software) created
especially for learning is severely limited by their particular
constraints when designers attempt to create comprehensive
instruction based on learning theory and instructional models.
Social media, while it has constraints of its own, has many
more affordances that give it the potential to be a
comprehensive instructional tool. While it shares many
beneficial affordances with other digital media, the unique
affordances of social media could allow learners to Flag, Tag,
Share. Rate, Comment, Subscribe, Syndicate, Remix, Publish,
Edit, Connect, Create, Curate, Customize, and Manage.
To date, the possibilities social media has for learning are
far from realized. In fact, traditional media for learning may
currently outpace social media in instructional effectiveness
due to years of research and theory-backed development. For
example, a user-generated instructional video could be
enhanced by a system within a video sharing site that prompted
video producers to add features that alert learners to focus on
important information, state the objectives or provide learning
guidance. In fact, video sharing site Howcast is moving in that
direction
with
their
Easy
Steps
(http://info.howcast.com/easysteps). The video, How to
Survive a Bear Attack is a good example of these elements in

Educators and students alike are using digital content to
enhance their learning experience. From a psychology
professor using YouTube clips as examples of personality traits
for her university students, to a Second Life newbie reading
about custom avatar creation on a Second Life wiki, social
media is currently being used to meet both formal and informal

978-1-4673-1382-7/12/$31.00 ©2012 IEEE

273

action
(http://www.howcast.com/videos/100020-How-ToSurvive-a-Bear-Attack).

students create their own tumblr blogs and each student follows
the course blog and every other student's blog. The students are
directed to post and share course assignments on their blog.
The instructor and each student can review and comment on all
the work posted on the individual blogs through their
dashboard. The instructor uses her dashboard to comment on
assignments, “like” them or reblog student assignments to the
main class tumblr. Once a week the instructor shares the most
challenging, problematic or exceptional assignments. The
instructor sites a variety of benefits including consistent
feedback, proactive study and students writing for a larger
audience than just the instructor [2].

In this presentation, I will explore how social media could
be enhanced for learning by leveraging instructional design
theories and models. I will share examples of how social media
is currently being used in the classroom, social media
affordances, and the learning theories and instructional design
models appropriate for redesigning social media for learning.
B. Designing Instruction for Social Media
Social media provides a source of user-developed learning
content never before available. What we are currently seeing is
only the beginning of the untapped learning potential of social
media. Social media offers a user experience that encourages
students to create and share new content while enabling
communication about content and lessons. Unlike other
content-centric websites, social media-enhanced tools like
Pearson’s Open Class and Khan Academy are the intersection
of best of class content and community connecting people,
content, and ideas for the purpose of learning.

E. Social Media and Learning Theory
I will discuss what learning theories fit with the unique
affordances of social media. For example constructivism which
states that “learning occurs when learners actively create their
own knowledge by trying to make sense out of (their learning
environment)” [3] seems especially in tune with the use of
social media for learning. In addition to constructivism, I will
discuss constructionist learning and situated cognition.

Educators and students alike are using online content to
enhance their learning experience. From an algebra teacher
using YouTube clips with worked examples as a resource for
her students, to a Twitter newbie reading about what is
happening around campus, social media is currently being used
to meet both formal and informal learning needs. Now that
social media is being recognized for its potency as a learning
resource, questions arise. The questions I am asking are:
1.

What are the unique social media affordances that
could enhance learning?

2.

What social media applications show promise as
instructional tools?

3.

How is social media currently being integrated in eLearning?

4.

How should learning theory guide the implementation
of social media?

5.

What instructional design models leverage the unique
affordances of social media?

F. Instructional Design Models for Social Media
Finally, I will end with the implementation of social media
as prescribed by appropriate instructional design models. For
example, John Keller's ARCS model focuses on motivation
when designing learning. The use of social media could satisfy
the four categories of the ARCS model: Attention, Relevance,
Confidence and Satisfaction [4]. I will also discuss how social
media can be implemented with descriptive models like ARCS
as well as more prescriptive models like those from Smith and
Ragan, and Morrison, Ross and Kemp.

Keywords – Social media; e-Learning; collaboation;
learning
ACKNOWLEDGMENT
This work is supported by an Office of Naval Research
Grant (N000141010143) awarded to Robert K. Atkinson.

BIOGRAPHY
ROBERT K. ATKINSON is an Associate Professor with
a joint appointment in the School of Computing, Informatics,
and Decision Systems Engineering in the Ira A. Schools of
Engineering and the Division of Educational Leadership and
Innovation in the Mary Lou Fulton Teacher’s College.

C. Social Media for Learning
I will describe and demonstrate social media tools that have
readily applicable instructional merit. One such tool is the
experimental social search tool from Microsoft Research, So.cl
(pronounced social). The focus of So.cl is on learning with
social search. So.cl combines elements of social networking
with search to enable students to search and share
collaboratively in real time. So.cl allows groups to create and
share posts with a variety of content that result from web
searches. One feature allows a group to create and watch a
playlist of videos simultaneously while they share comments
on the videos [1].

REFERENCES
[1]

[2]

[3]

D. Examples of Learning with Social Media
I will describe best practices from a variety of grade levels
and contexts. One such example is from an undergraduate
writing class in which the instructor uses tumblr.com. The
instructor posts class info and readings to the class tumblr. The

[4]

274

T. Simonite, “Microsoft Mixes Social Networking with Search.
Technology
Review,”
retrieved
from
http://www.technologyreview.com/computing/39353/
A.V. Young, “Follow, heart, reblog, crush: Teaching writing with
tumblr,” retrieved from http://learningthroughdigitalmedia.net/followheart-reblog-crush-teaching-writing-with-tumblr.
R. Mayer, “Designing instruction for constructivist learning,” In
Reigeluth, C. M. (Ed.). Instructional-Design Theories and Models: A
New Paradigm of Instructional Theory (Vol. 2). Hillsdale, N.J:
Lawrence Erlbaum Associates.
J. M. Keller, “Development and use of the ARCS model of instructional
design,” Journal of Instructional Development, 10(3), 2–10.
doi:10.1007/BF02905780

MOOCS & e-Learning

CHI 2015, Crossings, Seoul, Korea

Mining Memories: Designing a Platform to Support
Social Media Based Writing
John Sadauskas
Arizona State University
699 S. Mill Ave.
Tempe, AZ 85281
john.sadauskas@asu.edu

Daragh Byrne
Carnegie Mellon University
5000 Forbes Avenue,
Pittsburgh, PA 15213
daraghb@andrew.cmu.edu

ABSTRACT

Many teens struggle with school writing and particularly in
identifying personally relevant topics, which motivate
writing improvement. However, a wealth of potential topics
is available through their social media data. We explore the
design of Sparkfolio, a prewriting support tool that aims to
help students successfully prepare meaningful writing
topics from their own social media content. This webdelivered tool was evaluated with 46 teen users in the
context of their high school English class. Findings
demonstrate that the use of social media can enhance the
quality of written outcomes, as Sparkfolio users had
significantly greater gains than a control group.
Author Keywords

Social Media; Education; Storytelling; Creative Writing
ACM Classification Keywords

H.5.2 [User Interfaces]: User-centered design; K.3.1
[Computer Uses in Education]: Computer-assisted
instruction
INTRODUCTION

Over the past decade, the social web has enabled new and
profound ways to document and examine our everyday
experiences. Through platforms like Facebook, Flickr,
Instagram and Twitter, we now have large, long-term and
intimate archives of ourselves authored as short, succinct
multimedia-narratives or ‘status updates.’ Teens are among
the most deeply immersed in these platforms [17] and this
creates an exciting opportunity space to enhance the
educational experiences of students by connecting
instructional content to communication technologies.
Several recent studies highlight how social platforms
positively enhance instruction by creating forums for
discourse and engagement, by supporting digital
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. Copyrights for
components of this work owned by others than ACM must be honored.
Abstracting with credit is permitted. To copy otherwise, or republish, to
post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from Permissions@acm.org.
CHI 2015, April 18 - 23, 2015, Seoul, Republic of Korea.
Copyright 2015 ACM 978-1-4503-3145-6/15/04 $15.00.
http://dx.doi.org/10.1145/2702123.2702383

Robert K. Atkinson
Arizona State University
699 S. Mill Ave.
Tempe, AZ 85281
robert.atkinson@asu.edu

collaboration, and by offering mechanisms for formal and
informal mentorship [3]. These classroom interventions
typically either directly co-opt or create simple nontechnical extensions to these platforms (e.g. using Facebook
Groups to coordinate assignments). Rarely are social media
tools adapted; rarer still are purpose-designed solutions for
social participation, engagement or exchange found within
the classroom. Consequently, there are many unrecognized
opportunities to enhance the classroom by leveraging these
social platforms and their content [24].
When employed in curricular contexts, the use of social
content mainly explores the development and sharing of
new content rather than creative re-use of students’ existing
multimedia. The re-examination of past, personal content
could inspire new instructional scenarios for civic and
social education, art making, and in our case, English
instruction. In particular, the efficacy with which teens
regularly narrate their experiences online ought to connect
strongly to literacy and creative writing education.
However, a strong perception remains that using social
media, and especially Facebook, is antithetical to
educational use, particularly as social networking is often
seen as a vehicle for procrastination and distraction from
core curricular content [3].
This raises fundamental questions about introducing social
media re-use within instructional settings: How can social
data be re-used in education scenarios such that it
enhances student outcomes but minimizes the potential for
distraction? To address this, we prepared and evaluated an
online tool, named Sparkfolio, which was designed to
support English personal creative writing in K-12 settings.
Through this prewriting support scenario, we explore how
socially documented experiences can help identify writing
topics, overcome creative blocks, and highlight selfefficacy in writing via previously authored social narratives.
Our goal is to illuminate students’ natural storytelling
abilities, positively affect their writing self-confidence, and
ultimately motivate writing improvement.
We begin this paper by describing prior work in the area
and motivate the need for a social media based approach to
prewriting. Next, we describe two probative studies that led
to design recommendations for our tool. Following an
overview of the implemented online platform, we present

3691

MOOCS & e-Learning

CHI 2015, Crossings, Seoul, Korea

findings from an evaluation with 46 high school students
using the tool over a four-week period. We conclude with a
discussion of the primary contributions of this work: 1)
evidencing the utility of social media in enhancing student
motivation for writing and writing quality; 2) highlighting
the advantages of tool support in mediating social media
based writing and 3) the potential to enhance engagement
and learning through personally relevant social content.
BACKGROUND

As modern communication becomes increasingly digital in
the age of the Internet, adolescents in particular have
embraced this paradigm shift, cited by many as “digital
natives” [21] due to their frequent, enthusiastic use of
technologies such as social media, instant messaging, SMS,
and email [17]. Through these channels, American
teenagers regularly engage in informal writing activities
[15], and as teen cell phone ownership continues to increase
[17], many are able to engage in these writing activities
anytime and anywhere. Despite being deeply engaged in
writing practice through text-based communication via
digital devices, this does not necessarily translate to
increased academic writing proficiency among adolescents,
as in 2011, only 27% of 8th and 12th grade students scored
as “proficient” or higher in writing [26]. One reason for this
disconnect is that students view their digital activity as
“being social” rather than “writing” [15]. Furthermore,
writing involves complex cognitive processes [12] and is
especially daunting for adolescents [14]. This is particularly
because they are self-conscious about their academic
abilities [1] and tend to believe they are born with them
rather than seeing possibilities for acquiring and developing
them [19]. Hence, the act of writing has a strong affective
component [12,19,20]. Self-beliefs impact one’s actions
[19], and so in the case of writing, negative self-views can
result in low quality work which in turn affirms negative
self-beliefs, and the cycle repeats.
With this in mind, many efforts have been made to
intrinsically motivate students to write by allowing them to
choose topics that are personally meaningful [2]. Students
themselves provide support for this approach, reporting
“they are motivated to write when they can select topics
that are relevant to their lives and interests, and report
greater enjoyment of school writing when they have the
opportunity to write creatively” [15].
One particularly effective instructional strategy for
improving adolescent writing involves prewriting—the
gathering, planning, and organization of ideas before
beginning a first draft [11]. Although the practice is
constantly employed by professional writers, school-age
students tend to spend little to no time prewriting prior to
starting first drafts [6], even when prompted by a teacher
[4]. Thus, focusing on structured prewriting strategies is
especially beneficial for adolescents [11], including the use
of visual tools such as graphic organizers to externalize

their thoughts [22]—which can reduce cognitive load while
organizing ideas and drawing connections [13].
However, an additional problem lies in combining
personally relevant topics with prewriting strategies,
particularly with struggling adolescent writers. While
choosing their own writing topics appeals to adolescents
[2,15], actually deciding on one can be challenging and
even intimidating. Given boundless options, they often do
not know where to start [22] and struggle with developing
initial general ideas to workable, specific ones [2]. While
teachers can work with students one-on-one and provide
structured questions for teasing out an idea [2,22], a teacher
cannot conference with every student. Moreover, if the aim
is for the student to select a topic that is personally
meaningful, a teacher cannot make that choice—it must be
the student. One approach to addressing this problem
involves “mining memories” [2], a practice in which
students are encouraged to keep a log of topics that interest
them as a bank of potential ideas for future writing [22],
typically cultivated over time through frequent
brainstorming sessions and freewriting. Yet, this method
requires a significant time commitment from both teachers
and students and is typically paper-based, making months’
worth of work easy to misplace. Hence, while approaches
such as mind maps, one-on-one conferencing and idea logs
are available, for many students, the stumbling block of
finding the spark for a relevant topic remains.
One possible solution to these challenges is the use of
technology to support prewriting, but most existing
computational writing tools instead focus on subsequent
portions of the writing process (drafting and revising) and
the use of natural language processing to evaluate writing
quality [24]. Few systems (if any) approach writing from a
content perspective [24]—i.e. supporting personally
relevant idea generation. Rather than helping students
generate engaging ideas, most frameworks assume a topic
is already chosen and address how to organize it. This is
largely because it is easier to have computers analyze
language patterns than to mimic creative human thought.
However, social media, smartphones, and digital media
have made it very easy to archive one’s life events [7], and
using these technologies, adolescents regularly document
their lives and thoughts via status updates, photos, and
videos [15,16]—essentially “authoring” stories every day—
stories that matter to them, stories they share because they
want to. Furthermore, lifelogs and social media data have
proven effective in seeding other narrative forms [7].
DESIGNING SOCIAL MEDIA BASED WRITING

Thus, within this work, we explore the potential of social
media based writing to empower students to better writing
outcomes. Our hypothesis is that grounding this creative
exercise in personally relevant topics, drawing on past
personal experiences to stimulate recall, and repurposing
previously authored content toward new applications will
lead to increased writing quality and motivation. By

3692

MOOCS & e-Learning

CHI 2015, Crossings, Seoul, Korea

leveraging content authored previously by the writer, this
can potentially highlight that they have valid ideas worth
sharing with peers and can provide a supportive framework
during topic selection. However, using social media in this
context simultaneously yields both opportunities and
challenges; it would provide a vast collection of potential
ideas, but making sense of this large volume of disparate
content could quickly become unbounded and
overwhelming for a struggling teen writer. Hence, in
exploring the use of social media in this context we
consider the following research questions:
RQ1: Does the use of social media data as inspiration for
writing during the prewriting process improve adolescents’
writing quality and writing motivation in comparison to
those who do not?
RQ2: How should social data be leveraged in a
computational tool for students to enhance their personally
relevant written outcomes while minimizing distraction
(overwhelming, procrastination, etc.)?
To balance the (sometimes competing) needs of teachers
and students during prewriting, we conducted two design
studies to explore the utility of social media as a dataset for
prewriting support: first, a probative study assessed teacher
and student needs in prewriting; next, a series of design
workshops with teen writers were carried out. Principle
outcomes are summarized below.
Needs Assessment

One-on-one interviews with six middle/high school English
teachers and focus group interviews with 14 high school
students were first conducted [22]. Discussions with
teachers explored needs, challenges and best writing
instruction practices; use of technology in classrooms, and
social media as a tool for writing. The six English teacher
participants (4 female, 2 male) all work in schools in the
American Southwest, and range in age (31-54 years), years
of teaching experience (4-28), and grades taught (6-12).
Student focus group topics included: students’ experiences
with writing; their social media/technology use; and social
media based writing. The 14 students (4 female, 10 male;
aged 13-18; grades 8-12) participated in three focus groups
(3-5 students per group). Students were recruited from two
high schools in the American Southwest, both of which
emphasize the integration of technology into instruction.
All sessions were audio-recorded, transcribed, iteratively
coded for emerging categories and themes, and
conceptualized using Grounded Theory [8]. Outcomes were
ultimately synthesized into two main categories
(justification for a social media based writing approach, and
design recommendations for such applications) as
summarized below; more details can be found in [22].
Both students and teachers agreed that student selfconfidence is a major challenge in writing instruction. One
teacher shared that many students come to school “saying
‘I’m a bad writer,’ and they don’t have any confidence in

their writing…[they’ve] made a decision that reading is
boring and writing is too hard”. 10 out of the 14 student
participants self-described as “okay” or “bad” writers,
including several students in AP/Honors English. This
implies a disjoint between students’ actual abilities and selfconfidence. Yet, and consistent with [15,16], teachers and
students affirmed that teens are constantly engaged in
social media writing activities, particularly via
smartphones, which they “always” and “constantly” keep
within reach. Teens also reported that they appreciate
having a large audience for their thoughts, and being able to
share with them at any moment through social media,
and—in agreement with [10,23]—that the instant validation
of their posts and photos from friends and family positively
impacts their self-views, encouraging them to continue to
share [22]. Finally, as supported by [2,15], teachers find
success in using personally relevant topics to ease students
into writing, and students feel this makes writing “easier”
and allows them to “play to their strengths” [22].
Overall, students were especially interested in “seeing all of
[their online] activity in one place,” allowing them to
“learn about themselves” and “reflect” while looking for
writing ideas. However, teachers confirmed that students
often do not know where to begin with a writing
assignment. Even given the option of choosing their own
topic, students claim they “don’t have anything interesting
to write.” In agreement with [11,14], teachers cited
“scaffolding” (walking students through the writing
process) as the most effective strategy for addressing this
issue, specifically providing structured questions for
generating personally relevant ideas: “I give them various
questions based on experiences they’ve had in the
past…memories that are funny or things where they have a
lot of emotion.” ;“When they can map out all of their ideas
in some way…it really helps them to see that there’s lots of
ideas to choose.” Once a topic is chosen, teachers typically
continue to aid students in “narrowing [the topic] down” to
a usable idea: “you’ve got this huge monstrosity and you
want to pick just a slice…I don’t want to hear about your
entire day at Disneyland…I want to hear about a moment in
time.” Teachers emphasized the importance of a scaffolded
workflow guiding students from initial idea to a specific
topic and then to a formal, organized plan and highlighted
the value of several popular mechanisms to flesh out and
organize ideas in preparation for a draft including “mind
maps,” and “graphic organizers.”
Design Workshops

Based on design insight gathered from the student/teacher
interviews, a low fidelity prototype of the tool was
prepared. This initial prototype was presented to teenagers
as part of a participatory design session. The wireframe
design included two principle features. First, a constrained
review model only displays five social media items at a
time. This was designed to avoid overwhelming students
and unbounded exploration. Second, it includes a flexible
planning tool intended to integrate familiar graphic

3693

MOOCS & e-Learning

CHI 2015, Crossings, Seoul, Korea

organizer techniques, solicit rich descriptions and scaffold
toward a structured writing plan. Focus groups were
conducted with 21 students (12 female, 9 male) in four
sessions. All participants were eleventh-grade students
recruited from one of the same high schools (see Needs
Assessment). The interactive wireframe was displayed and
students were asked—without any prior explanation—how
they would expect or want to interact with each screen. This
aimed to explore the expectations, affordances and
intuitiveness of the two approaches. Directed feedback, as
well as open-ended discussion, was solicited. Focus group
responses were audio-recorded, transcribed, and iteratively
coded via Grounded Theory, following the same
methodology as described previously [8,22]. Student
responses were positive and enthusiastic, with little (if any)
negative feedback. Findings were organized around the two
principle areas of interest: the constrained approach and the
digital organizer.
Constraint During Ideation: Students were particularly
receptive to the use of constraint. They felt viewing all of
one’s data at once (as in a timeline or exploratory interface)
would be “overwhelming”, “distracting” and run the risk
of bringing “the procrastination level to a new point”
during prewriting. They noted that by only using five items,
it “doesn’t overload you” with too much content. While
constrained review is atypical for social media review,
participants were in favor of it in this context. They
expressed interest in using this method for their own
writing and believed it would effectively aid peers in
choosing personally relevant writing topics while avoiding
distractions.
Scaffolding A Writing Plan. Participants felt the writing
organizer would meet the needs of a wide variety of
students and learning styles. They immediately recognized
the graphic organizer approach and included writing
elements. One student appreciated that it was “like doing a
web [planning diagram]. I love doing those…I’m a very
visual person, so that’s probably why I like it.” At the same
time, students who prefer text-based planning felt it was “a
lot like an outline.” Students believed the workflow would
need to help scaffold their writing planning without being
prescriptive and that it should be flexible to the preferences
of the individual student. For example, some students
favored the preparation of highly detailed writing plans and
wanted to be able to include multiple layers of detail and
specificity. Other students (consistent with [4,6]) preferred
not to construct detailed plans: “I don’t like to prewrite.
I’ve never prewritten.” Another added: “I just write out
[lists of] ideas.” A third student shared that once she has a
basic plan (a list of 1-2 words for each idea to include), “I
just start writing.” Nevertheless, students admitted that
successful writing requires at least some level of planning,
as “you need to have somewhere to begin rather than just
staring at a blank piece of paper.” They also acknowledged
that instructors often require them to write detailed outlines
they do not always feel are valuable to their writing

process: “You don’t want to make [students] fill out extra
details if they’re just ready to start writing.” “Everyone is
different,” another student echoed, “so if you feel confident
enough, you should be able to just start writing.” Finally,
asked when a user ought to be “finished” with a writing
plan, students unanimously answered: “when you want to
be.”
Design Recommendations

Inspiration and Planning: Both students and teachers were
very receptive to the use of personally relevant social media
to support ideation and planning as part of prewriting. The
use of constraint during the presentation and review of
social content was seen as an effective strategy to avoid
overwhelming students.
Flexible planning: Students noted that they used prewriting/
planning methods to varying degrees and had a variety of
preferences for their application and use. While teachers
strongly favored every student using an organizational
method, students strongly believed that they should be able
to proceed directly to writing a draft when they already had
clear topics for their writing and felt confident about their
ideas. This predicates the need for a flexible approach to
writing organization that would facilitate scaffolding
toward a well-developed writing plan while also catering
for a wide range of preferred prewriting strategies.
IMPLEMENTATION AND EVALUATION

The design research and its outcomes were leveraged in the
preparation of an online tool to support student writing via
personally relevant social media, following which a 46student trial was conducted in an 11th grade English class.
Implementation

The application, named Sparkfolio, is realized as a webdelivered application, allowing students flexible and
ubiquitous use from any Internet-enabled device. It offers
the ability to: import and automatically organize social
multimedia from a variety of sources; to revisit and re-find
social content to seed ideas for personal writing; to
assemble social media into a lightweight narrative and to
develop that narrative into a fully-formed writing plan by
adding relevant details. It offers a carefully designed
experience intended to increase writing quality and
motivation. In subsequent sections, we describe this
approach, design considerations, and the functionality
prepared in more detail.
Content Aggregation: Users are able to connect their social
media accounts and import their data from three platforms:
Facebook, Twitter and Instagram. Once connected, the
application performs an initial import of content but
periodically synchronizes with their feeds to include new
content. Users can also manually initiate this update. For
Facebook, the application imports their: friends and
associated metadata; photo albums; location check-ins;
photos; their status updates; and photos they were tagged in.
For Instagram, the tool will parse the full history of the

3694

MOOCS & e-Learning

CHI 2015, Crossings, Seoul, Korea

user’s timeline, importing both videos and photos. Media
liked by the user and the accounts they follow are also
imported. Finally, for Twitter, the user’s personal timeline
is parsed to a maximum of 3200 status updates (imposed by
the API), along with “tweets” the user has been mentioned
in and/or favorited, and their list of followed accounts. In
addition to content authored by the user, content they have
acted on (replies, re-shares, likes) is imported, after which,
content is denoted as one of five media types: text, photo,
video, place or person. Metadata on authorship and social
activity and engagement for each item (number of “likes,”
comments, replies, or shares by other users) is also stored.
Content Re-experience: Constraint is deliberately designed
into the ideation phase for the tool, using the familiar
metaphor of playing cards. It compiles imported social
media into a “deck” of cards, with each “card” representing
a single post (e.g. a tweet, status update, or photo). When
presenting content, a constrained “hand” of five of these
idea cards is dealt (see Figure 1). Given that the user does
not know what will satisfy their information need, it also
avoids user-driven search as the primary means to identify
relevant content within their archive. The card metaphor is
extended to offer five suits of cards, namely text, photo,
video, locations, and people. At each draw, the user can
decide which of the five results they wish to “hold” before
clicking a “redraw” button to replace unwanted cards. Users
are free to redraw as many times as they like until they find
a set of cards to use as a starting point for a writing piece.
Associated with enhancing creativity, the topic of constraint
is of particular interest in this context [18], as it is intended
to encourage students to weigh and evaluate each option for
latent relationships for more considered outcomes.

conceived writing plan (see Figure 2). During the planning
process, students sort and organize their selected cards.
Based on the relevant genre (e.g. personal story,
explanation, persuasive piece), students are encouraged to
add relevant details through iterative research-based
prompts (e.g. [9]). For example, if students are writing a
personal story, they are cued to add story elements such as
an Exposition, Conflict, Climax, and Resolution. Each
element can then be ‘fleshed out’ with questions prompting
additional details specific to the story element (e.g. people
involved, emotional responses to the situation, etc.). This
encourages reflection on the media through established
story development methods toward a complete and wellformed story plan.

Figure 2. Final graphical organizer designed to assist story
planning and organization.

Export for Writing: Once a plan is completed, it is saved
and can be accessed and revised at any later date. The user
can also export it as a richly described PDF. The exported
plan is organized as a nested bullet-point summary of
chosen media, their arrangement, and descriptive
annotations. This enables portability of the developed plan
for subsequent development into a fully-formed narrative.
Evaluation

Figure 1. A constrained layout shows 5 social media cards to
help inspire writing topics from personally relevant content.

Narrative Development: Once students have found their
idea or source material, a new challenge lies in developing
it into a well-formed first draft. A common solution is to
use graphic organizers to scaffold toward a draft by
organizing thoughts and applying rich descriptions
appropriate for the assigned writing genre. The tool
incorporates a variant of these classroom strategies to guide
students from their initial “idea cards” toward a well-

Writing Quality: Sparkfolio was evaluated in a study
involving 46 eleventh-grade English students, each of
whom were enrolled a two sections of a large, block-style
English class (at one of the Needs Assessment schools)
taught by a two-teacher team (60 students per section, 120
total). Student participation was based on their own (and
their parents’) interest, using a standard recruitment script.
The 46 participants were randomly assigned to three
experimental groups: a) the “My Media” group, who used
self-authored social media post data while planning with
Sparkfolio; b) the “Network Media” group, who used only
data from posts authored by their friends while planning
with Sparkfolio (and no self-authored data—similar to the
social media content they would review through a timeline);
and c) a control group who did not use Sparkfolio.
Assignment to the conditions was purely random; students
were automatically assigned as they registered for the tool,

3695

MOOCS & e-Learning

CHI 2015, Crossings, Seoul, Korea

with the first assigned to My Media, the second to Network
Media, the third to Control, the fourth to My Media, etc.
Participants wrote three personal narratives during the
course of the four-week experiment. A first, pretest
personal narrative was written prior to the introduction of
Sparkfolio—all participants wrote this narrative however
they felt most comfortable. Next, Sparkfolio was
introduced, and participants were then asked to write two
additional personal narratives within their assigned
conditions. They were given two 20-30 minute sessions per
each treatment narrative for in-class prewriting (during
which My/Network Media prewrote with Sparkfolio while
Control prewrote by hand). My/Network Media students
were also free to log onto and use Sparkfolio outside of
class, and the Control group was free to prewrite (without
Sparkfolio) outside of class as well. Writing prompts were
selected based on state standards and personal narratives
were scored with a nationally-adopted holistic writing
evaluation rubric [25]. This rubric was chosen because it is
the “officially” adopted writing quality instrument used on
all standardized tests for the state in which the study
occurred. The rubric considers multiple facets of writing
(ideas & content, organization, voice, word choice,
sentence fluency, and conventions) and assigns a single
holistic score from 1 to 6, with scores of 1 and 2 indicating
that the writing “falls far below” grade-level standards, 3
indicating that the writing “approaches” the standards, 4
“meets” the standards, and scores of 5 and 6 “exceed” the
standards [27]. Thus, the outcome variable of interest in
assessing writing quality was the rubric-based change in
holistic writing quality from the first (pretest) narrative to
the third (second treatment) narrative. Student outcomes
were independently assessed using this rubric by a teacher
with 10 years’ experience, who was very familiar with the
instrument, and had used it for several years with her own
students’ writing. She had no knowledge of the study’s
purpose or design, Writing samples were additionally
anonymized, randomly sequenced, and contained no
information on experimental group or the order that papers
were written in. In addition to the writing quality measures,
the Writing Dispositions Scale (WDS) [20] was
administered to all 46 participants as a pre/posttest to
measure the change in writing motivation. The pretest was
administered before Sparkfolio was described or used and
the posttest was completed after submission of the final
personal narrative. The primary dependent variable was the
change in writing motivation—the change in the mean
scaled response for all items of the WDS. Finally, the
System Usability Scale (SUS) [5] was administered,
including open response items to qualify scaled responses.
RESULTS
Perception of the Tool

Students received Sparkfolio well with a mean SUS score
[5] of 2.39 (on a 0-4 scale; SD=0.53), particularly
responding to its ease of use (M=2.53), ability to quickly
learn the system (M=2.73) and their confidence in using it

(M=2.53). They felt it “helped [them] remember events that
happened in the past,” aiding in the identification of
personally relevant topics, and enjoyed “the ideas that
came from bringing up old posts.” Overall the tool was
viewed as supportive as "it helped promote ideas and create
a full story from beginning to end.” There were slight
differences between the two conditions with My Media
having a mean scaled score of 2.50 and Networked Media’s
lower score of 2.27, but this difference was nonsignificant.
Social Media Data

All 46 users imported content into the system (15 My
Media, 15 Network Media, 16 Control.) A mean of
5,514.09 (SD = 4127.42) items were imported per user, of
which 1,986.82 (SD = 1,669.08) were explicitly authored by
the user, 10.51 (SD = 10.58) represented tagged locations,
and 1,089.60 (SD = 789.32) were friends listed in their
social networks. 32 users had Twitter accounts, 36 users
had Facebook accounts and 37 users had Instagram
accounts. Twitter users authored a mean of 1,338.81 items
imported over 1.46 years. 342.76 were replies, indicating
the users were actively engaged in discussions on Twitter
with peers and the content was typically text-based
(1,231.23 items) but a small amount of media content was
also posted (40.5 photos). They had large social networks
with an average of 463.14 friends and regularly engaged
with their content, ‘liking’ 492.06 items and ‘retweeting’
504.20 items. Overall, their posts contained limited location
metadata, with only 9.11 places being tagged. Instagram
had much lower use, with only 127.51 items posted over a
1.12-year period. These tended to be images, but 2.32
videos were also posted. Instagram users liked a mean of
20.92 items and had smaller networks of 51.97 people.
Place metadata was also only sparsely applied, with an
average of 0.86 locations per user. Finally, Facebook users
posted an average of 1162.42 items over a 3.12-year period.
Like Twitter, these were primarily status updates (880.31
text-based items) but users had also uploaded a large
volume of photo content (282.11 items on average). As
with all other platforms, place-based metadata was low
(3.38 locations tagged). The social network size was the
largest among all of the platforms; 858.28 Facebook friends
were found on average. Mean content available for review
was as follows. For My Media: 625.46 text; 257.13 photo,
12.06 place, 1209 people; and 2.73 video items. For
Network Media: 2332.13 text; 604.5 photo, 13.5 place,
929.25 people; and 0.19 video items.
Identifying Writing Topics and Story Composition

My Media users (2,106.4) averaged half the amount of
content as Network Media counterparts (3,880.13 items).
As noted above, this was primarily distributed across text
(MM: 60.1%; NM: 13.8%), photo (MM: 15.6%; NM:
70.7%), and people content (MM: 23.9%; NM: 12.3%).
Users in the My Media condition needed to review half the
amount of content to identify writing topics. My Media
users issued an average of 244.13 redraws, from which they

3696

MOOCS & e-Learning

CHI 2015, Crossings, Seoul, Korea

viewed an average of 1,049.20 items, 552.53 of which were
unique. Network Media users issued a substantially higher
number of redraws (441.73) where they saw an average of
1,890.60 items, of which 1,065.47 were distinct. While
Network Media had half the amount of content to review, it
was presented randomly, giving an equal chance of
identifying prospective content and so the more involved
review phase is in part attributable to the increased volume
of content but also the lowered utility of content (which was
not authored explicitly by the user) in this creative exercise.
Both groups each prepared 26 stories using a total of 130
items of media from their archives. The distribution of
media across ‘suits’ is presented in Table 1. Interestingly,
despite lower relative distribution of photo-based content in
the My Media condition, the group showed a very strong
affinity for this content in their final selections. 70.8% (92
of 130 items) of media selected was photographic. Text and
people content was included to a lesser degree (18 and 16
items from 130 respectively.) However, while photo
content was abundant for Network Media (70.7% of
available archive) it was less preferred (66 of 130 items
selected or 50.8% of selections). In this condition, the use
of text and place based content doubled. This is indicative
of differences in the fluencies and affordances of selfauthored and peer-authored content in story development.
Exploration

Text

Photo

Place People

Video

My Media

13.8%

70.8%

3.0% 12.3%

0.0%

Network Media

29.2%

50.8%

7.7% 12.3%

0.0%

irrelevant to me.” Another said her cards “were all old,
irrelevant pictures of random people.” In contrast, My
Media participants felt Sparkfolio “helped bring ideas and
thoughts to the table” and was “beneficial because I got to
use pictures to help me think about the things in the past.”
The trend of the My Media group viewing Sparkfolio as
more useful than Network Media was extended to the
number of total detail nodes created on planners as well as
the planner total word count. My Media created almost
twice as many (M = 18.80) as Network Media (M = 11.20).
While this difference was nonsignificant, it agrees with the
number of story plans results. Additionally, the My Media
group wrote more than twice as many words on their
planners (M = 210.20) as the Network Media group (M =
95.33). This difference was significant at the .05 level: F(1,
28) = 4.259, p < .05, with a large observed effect (f = .39).
Writing Motivation

The My Media (M = .06; see Table 2) and Control (M =
.03) groups increased and Network Media decreased (M = .07). This Network Media decrease can be partially
supported by qualitative comments, as Network Media
participants tended to report more difficulties finding useful
social data than the My Media group (see above). However,
these between-group differences not only represent less
than .1 of a Likert point, but were also nonsignificant.
Planning Mode

Table 1. Social Media Content Type Used in Stories

M

SD

N

My Media

.06

.37

15

Network Media

-.07

.36

15

Control

.03

.41

16

Total

.01

.38

46

Table 2. Change in Overall Writing Motivation

Story Planner Use

The total number of story plans created by participants did
not widely differ between groups, and consequently this
difference was nonsignificant. The My Media group created
a mean of 1.73 stories (SD = .59) and the Network Media
group created a mean of 1.40 (SD = .83). While participants
were asked to create at least two story plans (one for each
treatment narrative), participants varied in their readiness
and prewriting processes for each assignment. While
strongly encouraged to use all Sparkfolio’s features for both
treatment narratives, some participants simply felt they
were confident and ready to start writing first drafts as soon
as they received writing prompts (and before logging into
Sparkfolio). While encouraged to use both the idea cards
and planner to build upon and structure their initial ideas,
participants were not forced to do so.
Furthermore, open responses on the usability survey and
one-on-one conferences indicated that Network Media users
tended to find their data (authored by others) more difficult
to work with than My Media users (who used self-authored
data). For example, one Network Media user shared, “Most
of the stuff that would come up on the cards was things I
would never want to write about, some even completely

Writing Quality Improvement

An ANOVA with experimental group as a between-subject
factor and pretest writing quality scores as the dependent
variable was nonsignificant, indicating that there were no
significant differences in writing ability between the three
groups as they began the study. Furthermore, as shown in
Table 3, the two treatment groups increased in writing
quality from pretest to posttest and the Control group
decreased. My Media had the greatest increase (M = .27),
followed by Network Media (M = .07). Although the
Control group had the greatest change from pretest to
posttest, it was a decrease (M =-.38). These results pose the
question as to why the Control group decreased in writing
quality. Participants from all three groups commented
during the study that completing three writing assignments
of the same genre during a four-week period was
challenging, and so the Control group’s decrease can be
attributed to the assumed decreased enthusiasm by the third
assignment. However, it is also important to note that both
My Media and Network Media still increased in writing
quality. It can thus be argued that Sparkfolio not only
maintained participants’ interest despite the multiple

3697

MOOCS & e-Learning

CHI 2015, Crossings, Seoul, Korea

assignments, but also actually resulted in improved writing
quality under these circumstances. Furthermore, an a priori
orthogonal contrast between the two treatment groups and
the Control group was significant, F(1, 43) = 5.139, p < .05,
with a medium-to-large observed effect (f = .35).
Planning Mode

M

SD

N

My Media

.27

.59

15

Network Media

.07

.96

15

Control

-.38

.72

16

Total

-.02

.80

46

Table 3. Change in Overall Writing Quality

A multiple regression analysis provided further validation
of these results. Using the forward selection method, the
analysis investigated whether gender, treatment/no
treatment, mean authored social media posts per day, or
total social media activity accurately predicted the change
in writing quality. In the resultant linear regression model,
the treatment variable was the sole predictor included, and
was significantly related to the change in writing quality,
F(1, 44) = 5.198, p < .05. With R = .33 and R2 = .11,
approximately 11% of the variance of the sample’s change
in writing quality can be accounted for by a linear
prediction model with treatment as the sole predictor. These
results imply that the treatment was, in fact, effective; those
who used Sparkfolio had significantly greater gains in
overall writing quality than participants in the Control
group, regardless of their genders and social media usage.
DISCUSSION
Writing Quality and Motivation

Most notable among the results is the validation of
Sparkfolio’s efficacy in enhancing writing quality
outcomes, as those who used it had statistically
significantly greater gains in writing quality compared to
the Control group. This justifies the writing approach
explored within this work, particularly the use of personally
relevant social media data during prewriting. However,
while the difference in writing quality gains between the
My Media and Network Media groups was nonsignificant,
it is worth noting that self-authored data appears to be more
useful and efficient in increasing writing quality compared
to data authored by others. My Media participants had
higher gains in writing quality (four times the Network
Media group’s gain). My Media users also reviewed half as
many idea cards as the Network Media group, suggesting
that using one’s own media was “twice” as efficient as
using data authored by others. This trend was also reflected
in story planner data, as My Media users created nearly
twice as many story detail nodes and wrote more than twice
as many words on their planners as the Network Media
group. With four times the writing quality gain, half the
idea selection effort, and twice the output on story planners,
it was clear that My Media users had a more positive
experience overall than Network Media users.

Complexities of Self-Motivation

While results suggest that utilizing adolescents’ social
media data during prewriting empowers higher quality
writing, there was little impact on writing motivation. We
assumed that engaging students with personally relevant
content as a foundational resource for writing would
provide a framework that affirmed self-efficacy and enticed
students. This effect (or lack thereof) may be reflective of
larger, open challenges for K-12 writing instruction and
education [12,19,20] but may also be a product of the short
time frame of the studies conducted. A four-week
evaluation is unlikely to produce noticeable effects on
students’ long-term motivation. Additionally, students did
not receive feedback or grading on the final products until
after the trial was completed. Rapid (and positive) iterative
feedback may have fostered the gains in motivation and
increased feelings of self-efficacy. Nevertheless, it is
somewhat encouraging that writing motivation largely
remained stable among students during the introduction of
this new tool and process for writing. Broadly, we note a
need to more fully explore and assess the affective
implications of social media based writing through longerterm evaluation.
Inspiring Prewriting: Media Availability

Our intent was to support students in identifying personally
relevant writing topics through social media. This
predicates the need for rich and relevant content to be
available through social platforms, and we know from the
data imported from the 46 participants that certainly longerterm multimodal content (typically 1 to 3.5 years’ worth) is
at the disposal of teens for creative production in this
context. For Facebook, the average user had over 3 years of
content and over 1,000 personally narrated moments
available to them. Additional social sources enriched this
dataset, and users brought in another 1,338.81 items from
Twitter (excluding replies - 996.05) and 127.51 items from
Instagram, over a 1.5- and 1-year period respectively.
Twitter users are much more engaged in regular authoring
(approx. 17.63 per week; 13.12 excluding replies), adding
considerably more content than Facebook users (approx.
7.16 items per week) in a shorter span of time.
Inspiring Prewriting: Media Fluency

While rich content is definitely available to teens via social
data, it begs the question, what content is most useful? The
benefit of personal content is clear from the outcomes
between the two treatment conditions. Students working
with timeline/network content had to perform double the
number of ‘redraws’ to identify a prospective topic. While
some of this content may be relevant, such content is likely
to be sparsely found. We anticipate that it is more
cumbersome to creatively re-use this content, requiring
reframing to view one’s self through the lens of others.
Clearly, not all content is created equal. Not only does
authorship, but also the modality of the media plays a role
in composing writing ideas. We observed that ‘people’ and
‘places’ infrequently populated story outlines. This

3698

MOOCS & e-Learning

CHI 2015, Crossings, Seoul, Korea

questions whether contextual information has as much
value as social content (text and photos) within ideation.
Inherently broader and more thematic, contextual
information may not ground writing ideas in the way that
more specific content can. Additionally, both treatment
conditions strongly favored visual media, despite the
availability of higher volumes of text-based content. We
expect this is because photos allow versatility that text does
not. Photos are rich, expressive and can simultaneously
convey an array of experiential information. They can be
interpreted in a variety of ways and more easily juxtaposed
with other media, allowing students to more readily adapt
meanings to fit within the context of a narrative plan. In
general, text content is more narrowly focused, and thus
more difficult to situate within a nascent topic.
While there is a wealth of social content and context
information available, the resources that will most empower
students to more creative outcomes is a key consideration.
Noting the different fluencies of social media prompts the
question, would displaying only photo content reduce the
potential for inspiration and the quality of outcome? By
tailoring the media presented to those most suited to the
writing context, we anticipate there is the potential to
further enhance the writing inspiration phase.
Use of Constraint

A key design decision in the user interface approach was a
constrained presentation of media explicitly aimed at
reducing distraction in ideation. Given the large volume of
social media available, it avoided overwhelming students
with a vast review of large social media datasets. This
approach shifts the burden away from the user by removing
the opportunity to exhaustively review a ‘stream’ of social
data. Logged data indicates that under this paradigm
students are still reviewing a large volume of their content
in order to identify topics, but they do so by refreshes or
“redraws.” By presenting only five items at a time, users do
not need to look for associations at large within the archive,
but instead do so only for the currently presented items.
This visual arrangement inherently frames content as
possible pieces of a narrative, juxtaposing them to
encourage examination of possible relationships and latent
meaning. This makes the process reflective rather than
interrogative. Further, there were neither reports of an
overwhelming experience nor procrastination from students
who used the tool. We anticipate that this reflective
approach not only contributed to overall increases in
quality, but also to the observed increases in organization
and planning found within the two groups who used the
tool, compared with the Control group.
Computer Mediated Review

Nonsignficant results indicate that regardless of
experimental condition, the two treatment groups had
minor, but equivalent, gains in the organization of their
writing (while the Control group had no gains). This
suggests the tool’s utility in enhancing and planning

organization. Our approach is to heavily mediate this
process and employ “technology to manage technology.”
While a simpler, less sophisticated approach likely has
merit (e.g. simply asking students to manually review social
timelines) this would likely introduce other complexities
into the classroom. Design research highlighted shared
concerns around procrastination, disruption and exhaustive
interrogation. Moreover, a less mediated approach would
place a burden on the instructor to conceive and manage the
process for students, as well as guiding them in repurposing
their social data into a well-formed narrative. Demonstrated
by our process, by working with the stakeholders involved,
well-documented and research-based organizational
strategies can be successfully embedded in digital tools to
offer self-guided and self-paced strategies for student
success. This enables students more opportunities to
develop self-efficacy and become less reliant on the teacher
for support.
The Value of Social Media in the Classroom

We have highlighted that creative reuse of socially
documented experiences can help students identify writing
topics, organize thinking and improve the quality of their
writing through a mediated process. Despite the potential of
such support tools and the possible impact of reusing social
data in education contexts, there are relatively few
explorations in this space. However, social media is a
medium in which students are deeply immersed and
regularly engage. We hope the encouraging outcomes of
this preliminary study will also call attention to this rich
opportunity space. Our outcomes demonstrate the potential
for enhancing and personalizing learning experiences by
increasing the relevance and personal connection to learned
content. We note that successful interventions require
crafted solutions that blend the requirements of multiple
stakeholders (institutional, instructor, and student), that
negotiate the perception of technology and that creatively
intertwine media, reuse and mediated process; this presents
a fertile arena for future design research and humancentered inquiry.
FUTURE WORK

Sparkfolio will be the subject of several future evaluations
in adolescent writing. We will next examine how the tool
impacts student writing quality in longer-term, larger-scale
deployments. Future work will also explore improved
interactive and content-based methods for identifying
prospective story content from social media, e.g. by mining
social metadata, or via improved recommendations.
Additionally, we plan to examine this approach within other
writing genres (e.g. argumentative) and for other candidate
populations (e.g. undergraduates, creative professionals).
CONCLUSION

Despite their constant engagement in digital communication
via social media, adolescents are generally averse to
academic writing. However, it stands to reason that
leveraging their own personally relevant, informally
authored social media content in a writing classroom would

3699

MOOCS & e-Learning

CHI 2015, Crossings, Seoul, Korea

foster intrinsic motivation and deeper engagement,
ultimately improving writing quality. It was with this mind
that Sparkfolio was developed, and through its constrained
ideation and scaffolded planning, it has been validated as a
sound approach for improving writing, as Sparkfolio’s users
had statistically significantly higher gains in writing quality
than a control group in a preliminary evaluation. These
results imply that utilizing one’s own social media data
holds powerful potential in personalizing learning,
particularly in offering opportunities for making personally
relevant connections in learning environments, a design
space that is still relatively unexplored.
ACKNOWLEDGEMENTS

We thank the teachers, students, school administration, and
district technology team for their participation in this work.
REFERENCES

1.

Anderman, E.M. and Maehr, M.L. Motivation and
Schooling in the Middle Grades. Review of
Educational Research 64, 2 (1994), 287–309.
2. Atwell, N. Lessons That Change Writers. Firsthand,
Portsmouth, NH, USA, 2002.
3. Aydin, S. A review of research on Facebook as an
educational environment. Educational Technology
Research and Development 60, 6 (2012), 1093–1106.
4. Berninger, V., Whitaker, D., Feng, Y., Swanson, H.L.,
and Abbott, R.D. Assessment of Planning, Translating,
and Revising in junior High Writers. Journal of School
Psychology 34, 1 (1996), 23–52.
5. Brooke, J. SUS: A “Quick and Dirty” Usability Scale.
In P.W. Jordan, B. Thomas, B.A. Weerdmeester and
I.L. McClelland, eds., Usability Evaluation in Industry.
Taylor & Francis, Bristol, PA, USA, 1996, 189–194.
6. Burtis, P., Bereiter, C., Scardamalia, M., and Tetroe, J.
The Development of Planning in Writing. Wiley,
Chichester, England, 1983.
7. Byrne, D., Kelliher, A., and Jones, G.J.F. Life Editing:
Third-Party Perspectives on Lifelog Content.
Proceedings of the 2011 annual conference on Human
factors in computing systems (CHI ’11), ACM (2011),
1501–1510.
8. Charmaz, K. Grounded Theory. In J.A. Smith, ed.,
Qualitative Psychology: A Practical Guide to Research
Methods. Sage Publications, London, 2003, 81–110.
9. Danoff, B., Harris, K., and Graham, S. Incorporating
strategy instruction within the writing process in the
regular classroom: Effects on the writing of students
with and without learning disabilities. Journal of
Literacy Research 25, 3 (1993), 295–322.
10. Ellison, N.B., Steinfield, C., and Lampe, C. The
Benefits of Facebook “Friends:” Social Capital and
College Students’ Use of Online Social Network Sites.
Journal of Computer-Mediated Communication 12, 4
(2007), 1143–1168.
11. Graham, S. and Perin, D. A meta-analysis of writing
instruction for adolescent students. Journal of
Educational Psychology 99, 3 (2007), 445–476.

12. Hayes, J. A new framework for understanding
cognition and affect in writing. In M. Levy and S.
Ransdell, eds., The science of writing: Theories,
methods, individual differences, and applications.
Erlbaum, Mahwah, NJ, USA, 1996, 1–27.
13. Kellogg, R.T. Effects of topic knowledge on the
allocation of processing time and cognitive effort to
writing processes. Memory & cognition 15, 3 (1987),
256–266.
14. De La Paz, S. and Graham, S. Explicitly teaching
strategies, skills, and knowledge: Writing instruction in
middle school classrooms. Journal of Educational
Psychology 94, 4 (2002), 687–698.
15. Lenhart, A., Arafeh, S., Smith, A., and Macgill, A.R.
Writing, Technology and Teens. Washington, DC,
USA, 2008.
16. Lenhart, A., Purcell, K., and Zickuhr, K. Social Media
& Mobile Internet Use Among Teens and Young
Adults. Washington, DC, USA, 2010.
17. Madden, M., Lenhart, A., Duggan, M., Cortesi, S., and
Gasser, U. Teens and Technology 2013. Washington,
DC, USA, 2013.
18. Moreau, C.P. and Dahl, D.W. Designing the Solution:
The Impact of Constraints on Consumers’ Creativity.
Journal of Consumer Research 32, 1 (2005), 13–22.
19. Pajares, F. Self-Efficacy Beliefs, Motivation, and
Achievement in Writing: A Review of the Literature.
Reading & Writing Quarterly 19, (2003), 139–159.
20. Piazza, C.L. and Siebert, C.F. Development and
Validation of a Writing Dispositions Scale for
Elementary and Middle School Students. The Journal
of Educational Research 101, 5 (2008), 275–286.
21. Prensky, M. Digital Natives, Digital Immigrants Part 1.
On the Horizon 9, 5 (2001), 1–6.
22. Sadauskas, J., Byrne, D., and Atkinson, R.K. Toward
Social Media Based Writing. Proceedings of the 15th
International Conference on Human-Computer
Interaction - DUXU/HCII 2013, Part II, LNCS 8013,
Springer (2013), 276–285.
23. Valkenburg, P.M., Peter, J., and Schouten, A.P. Friend
networking sites and their relationship to adolescents’
well-being and social self-esteem. Cyberpsychology &
behavior: The impact of the Internet, multimedia and
virtual reality on behavior and society 9, 5 (2006),
584–590.
24. Vojak, C., Kline, S., Cope, B., McCarthey, S., and
Kalantzis, M. New Spaces and Old Places: An
Analysis of Writing Assessment Software. Computers
and Composition 28, 2 (2011), 97–111.
25. Official Scoring Guide: Arizona’s Instrument to
Measure Standards - Holistic Rubric Based on 6 Traits
of Writing. Arizona Department of Education, 2010.
26. The Nation’s Report Card: Writing. Washington, DC,
USA, 2011.
27. Assesssment: AIMS Writing. Arizona Department of
Education, 2013. http://www.azed.gov/standardsdevelopment-assessment/aims/aims-writing/.

3700

Lost in the Dark: Emotion Adaption
Ryan Bernays, Jeremy Mone, Patty Yau, Michael Murcia, Javier Gonzalez-Sanchez, Maria
Elena Chavez-Echeagaray, Robert Christopherson, Robert Atkinson, Yoshihiro Kobayashi
Arizona State University
University Drive and Mill Avenue, Tempe, AZ 85281,USA
{rbernays, jeremy.mone, patty.yau, michael.murcia, javiergs,
helenchavez, robert.christopherson, robert.atkinson, ykobaya@asu.edu}@asu.edu
ABSTRACT

Having environments that are able to adjust accordingly
with the user has been sought in the last years particularly
in the area of Human Computer Interfaces. Environments
able to recognize the user emotions and react in
consequence have been of interest on the area of Affective
Computing. This work presents a project – an adaptable 3D
video game, Lost in the Dark: Emotion Adaption, which
uses user’s emotions as input to alter and adjust the gaming
environment. To achieve this, an interface that is capable of
reading brain waves, facial expressions, and head motion
was used, an Emotiv® EPOC headset. For our purposes we
read emotions such as meditation, excitement, and
engagement into the game, altering the lighting, music,
gates, colors, and other elements that would appeal to the
user emotional state. With this, we achieve closing the loop
of using the emotions as inputs, adjusting a system
accordingly as a result, and elicit emotions.
Author Keywords

Emotion recognition; affective states; EEG; 3D videogames
ACM Classification Keywords

H.5.2 [Information interfaces and presentation]: User
Interfaces --- interaction styles, input devices and
strategies.

to take control of his emotions and use them to progress.
Like recent developments in game design in which user
movements are used to make him feel more part of the
game, using a user’s emotions can open up entirely new
levels of gameplay and research, and increase games’
ability to adapt to an input as complicated as human
emotions.
BACKGROUND

This section provides information about the technology
behind the game including hardware and software.
Emotiv® EPOC Headset

Our research uses the Emotiv ® EPOC headset [2]. This
device has non-invasive electrodes that are used to capture
brain-wave signals. It measures voltage fluctuations
resulting from ionic current flows within the neurons of the
brain, which occur differentially in the presence of diverse
emotions. This device uses pattern detection analysis to
infer emotional states and reports affective states such as:
excitement, engagement, meditation, calmness, boredom,
and frustration.
Emotiv SDK

Design, Human Factors

The Emotiv® EPOC headset comes with a Software
Develop Kit (SDK). This SDK in conjunction and based on
the ABE system [3] allows developers to get data from the
device in real time, by introducing its functionality into
game-developing environments.

INTRODUCTION

XNA + Visual Studio

In games, immersion has always been a long sought goal of
the designing process. Feeling more connected with the
game through interactivity, wealth of sensory information,
or an environment that demands your full attention always
makes it more enjoyable. An adaptive environment helps to
enhance immersion. With our game’s unique emotion
controls, we hope to enhance it more to create a fun and
helpful experience for players. Our game uses the
Emotiv® EPOC headset to read brain waves that indicate
user’s current levels of excitement, engagement, and
meditation. Using these as game variables, we have the
game either adapt or adjust to match the player’s current
emotional state, or provide obstacles that require the player

XNA Game Studio 4.0 [5] is a game-developing
environment that allows the use of Visual Studio 2010 [4],
an integrated development environment (IDE) from
Microsoft used to develop console and graphical user
interface (GUI) applications. It supports XNA’s
Framework 4.0, a set of libraries for game development.

General terms

Copyright is held by the author/owner(s).
UIST ’12, October 7–10, 2012, Cambridge, Massachusetts, USA.
ACM 978-1-4503-1582-1/12/10. 	
  

THE GAME

The game offers a 3D maze enhanced with emotional
components as inputs that are mapped with features in the
game, allowing an adaptive maze in terms of light, sound,
play time, and access to different sections and levels.
The 3D Maze

The maze takes place on the inside of a gigantic cylinder,
gravity forces players downward from the center so that
they can walk along the inside of it. The maze walls are
generated to also stretch across the walls, providing the

player with a view of other parts of the maze that they can
look up at to discern where to go next. The walls are
colored (red, yellow, and green) indicating how far or near
is the exit. The player starts at one end of the cylinder, must
unlock and cross through diverse gates, and make his way
to the other side where an elevator takes him into the next
level. There are keyboards’ controls (WASD keys) for
movement, an Xbox 360 controller to control movements
and the camera, and a mouse for looking around. The
players are also provided with a mini-map for navigation.
The Components based on Emotion Inputs

We use three emotions from the ones the Emotiv® EPOC
headset picks up: excitement, engagement, and meditation.
Excitement determines the color of the Light Bot, a floating
sphere that projects all of the light within the maze (see
Figure 1). Light Bot has colors that run along the sides,
matching up to a certain level of excitement. Ranging from
black to red to yellow to green to teal to blue to purple to
white (from calmest to most excited), a bar to the left
demonstrates the spectrum and user’s current level, so that
he knows how to reacts if he needs the Light Bot to change
its color. There are also gates within the maze that range in
the same color spectrum, and they will open and allow the
player to continue if he can match the Light Bot’s color to
that of the gate. This forces players to take control of their
excitement by calming or invigorating themselves.
Engagement controls music. When the player is relaxed
and unconcerned, the music changes volume to a softer
tone to reflect the player’s relaxed state, but when the
player becomes more interested or agitated, the music
increases in volume to match the new manic state. This
helps set the mood for the game according to the player’s
emotional status.
For meditation, we added in the ever-increasing danger of
the player’s sole light source giving out. As time passes in
the game, the Light Bot will lose light, forcing the player to
hurry to the exit before it goes out. However, by focusing
on the meditation, players are able to expand their field of
vision and keep the darkness at bay, and keeping this level
of focus while playing makes the game easier (see Figure
1). This forces players to learn to play with a cool,
concentrated head under stressful situations. As the
meditation level increases, the current level of light
increases, and as it decreases, so does the light.
Implementation

The game’s mazes are generated by reading commaseparated-values (csv) text files that use characters to
represent wall pieces, gates, or elevators. After reading the
file, the game automatically generates the maze. Mazes
could be created or edited with ease by altering the text file.
Graphics were made using the Maya 3D Animation
Software [1]. We created the shapes for the wall pieces,
gates, the cylinder, elevators, and the Light Bot.

Figure 1. A screen shown from gameplay, as the light level is
closing in due to a lack of meditation upon the part of the
player, as the player is attempting to change his excitement
level to match the colors of the gate and ball.

This environment was used to create textures that would fit
over all of the game objects: the floor tiles, wall graphics,
and the lights that make up the gates and Light Bot.
Music for the game was not specifically made, but is
simply the song “Self Esteem Fund” from the popular game
Portal, mainly to create the feeling of lonely suspense.
CONCLUSION AND FUTURE WORK

Adjusting an environment according to emotions is
complex, and it’s difficult to find mechanics for each
emotion that feels natural to be connected to. However, we
consider our game to have done a great job in creating a
complete experience for players to use their emotions as a
tool to play the game, and we consider it provides a world
of opportunities in the future where emotions patterns can
be used for much more integrated controls, and changing of
an environment. Not only is it entertaining, but it could be
transferred to education and learning areas, allowing people
to practice controlling their emotions, and discerning what
elements on an environment trigger what emotions.
ACKNOWLEDGMENTS

The project was developed as a course work of CPI441
(Gaming Capstone) at ASU collaborating with the research
group supported by Office of Naval Research under Grant
N00014-10-1-0143 awarded to Dr. Robert Atkinson.
REFERENCES

1. Autodesk Maya. 3D Animation Software.
http://usa.autodesk.com/maya
2. Emotiv. Brain Computer Interface Technology.
http://www.emotiv.com
3. Gonzalez-Sanchez, J., Chavez-Echeagaray, M.E.,
Atkinson, R. and Burleson, W. 2011, ABE: An AgentBased Software Architecture for a Multimodal Emotion
Recognition Framework. In Proc. of Ninth Working
IEEE/IFIP Conference on Software Architecture, 2011,
187-193.
4. Visual Studios 2010. Microsoft.
http://www.microsoft.com/visualstudio/enus/products/2010-editions
5. XNA Game Studio 4.0. Microsoft.
http://www.microsoft.com/enus/download/details.aspx?id=23714

Computers & Education 56 (2011) 650–658

Contents lists available at ScienceDirect

Computers & Education
journal homepage: www.elsevier.com/locate/compedu

Using animations and visual cueing to support learning of scientiﬁc concepts
and processes
Lijia Lin a, Robert K. Atkinson b, *
a
b

Division of Advanced Studies in Learning, Technology and Psychology in Education, Arizona State University, P.O. Box 870611, Tempe, AZ 85287-0611, USA
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, P.O. Box 878809, Tempe, AZ 85287-8809, USA

a r t i c l e i n f o

a b s t r a c t

Article history:
Received 30 September 2009
Received in revised form
7 October 2010
Accepted 8 October 2010

The purpose of the study is to investigate the potential beneﬁts of using animation, visual cueing, and
their combination in a multimedia environment designed to support learners’ acquisition and retention
of scientiﬁc concepts and processes. Undergraduate participants (N ¼ 119) were randomly assigned to
one of the four experimental conditions in a 2  2 factorial design with visual presentation format
(animated vs. static graphics) and visual cueing (visual cues vs. no cues) as factors. Participants provided
with animations retained signiﬁcantly more concepts than their peers provided with static graphics and
those afforded visual cues learned equally well but in signiﬁcantly less time than their counterparts in
uncued conditions. Moreover, taking into consideration both learning outcomes and learning time, cued
participants displayed more instructional efﬁciency than their uncued peers. Implications and future
directions are discussed.
Published by Elsevier Ltd.

Keywords:
Multimedia/hypermedia systems
Human–computer interface
Interactive learning environments

1. Introduction
As computer technologies advance, the use of graphics in computer-based educational environments has become commonplace and
appears to be gaining increasing popularity. In the past several decades, a large number of studies have been conducted to investigate
various issues concerning the beneﬁts of using static and dynamic graphical representations in multimedia learning environments. Two
important issues are: (a) the relative effectiveness of the presentation format (i.e., animation versus static media) and (b) the potential
instructional beneﬁts of visual cuing. This study investigates how these two factors (i.e., presentation format and visual cues), either
separately or in combination with one another, inﬂuence the retention of science knowledge in a multimedia learning environment.
1.1. Instructional animation
According to Bétrancourt and Tversky (2000, p. 313), animation is the visual representation that “generates a series of frames, so that
each frame appears as an alternation of the previous one”. Therefore, by its nature, animation is able to vividly present events which change
over time, such as motion, processes and procedures. It provides more external support for learners to construct their dynamic internal
representations than static graphics. Some studies in the past decades have shown positive results that favor the use of instructional
animations. For instance, Rieber (1990) provided participants with a computer-based lesson describing Newton’s law of motion, using either
static or animated graphics. The results revealed that participants in the animated graphics condition had a better understanding of theconcepts and rules of Newton’s law than those in the static graphics condition. In another study, participants viewed either animation or
static diagrams of chemical reactions during a lecture (Yang, Andre, & Greenbowe, 2003). The researchers found that participants who
received the instructor-paced animation demonstrated better understanding of chemistry concepts than their counterparts studying static
diagrams. Kriz and Hegarty (2007) conducted a series of experiments using an animated diagram and a static diagram to teach participants
about a ﬂushing system. They found that participants who learned from the animation had signiﬁcantly better comprehension of the system
compared to those studying a static diagram, regardless of whether the animation was interactive or had signaling devices. In the metaanalysis conducted by Höfﬂer and Leutner (2007), an overall-effect of animations was found among dozens of reviewed studies. Other
* Corresponding author. Tel.: þ1 480 965 1832; fax: þ1 480 965 7193.
E-mail address: robert.atkinson@asu.edu (R.K. Atkinson).
0360-1315/$ – see front matter Published by Elsevier Ltd.
doi:10.1016/j.compedu.2010.10.007

L. Lin, R.K. Atkinson / Computers & Education 56 (2011) 650–658

651

studies also support animation’s effectiveness on learning (e.g., Arguel & Jamet, 2009; Ayres, Marcus, Chan, & Qian, 2009; Catrambone &
Seay, 2002; Large, Beheshti, Breuleux, & Renaud, 1996; Münzer, Seufert, & Brünken, 2009; Wong et al., 2009). It is of note that a positive
learning effect was found for animations in a wide range of domains including science (physics and chemistry concepts), engineering
(mechanical systems), and daily life skills (paper folding and knot making).
Cognitive load theory (Paas, Renkl, & Sweller, 2003; Schnotz & Kurschner, 2007; Sweller, van Merrienboer, & Paas, 1998) provides
a theoretical framework to explain the superiority of instructional animations over static graphics. It assumes that a human’s working
memory has limited capacity and considers learning as a process of schema acquisition. There are three subcomponents of cognitive
loaddintrinsic load, extraneous load and germane load. Intrinsic load is determined by element interactivity and cannot be altered on the
condition that the learners’ expertise has not changed and the learning material has been designated (Schnotz & Kurschner, 2007).
Extraneous load is caused by inappropriate instructional format and is irrelevant to learning, whereas appropriate instructional design
fosters learning-related cognitive activities, i.e., fosters germane load. By viewing instructional animations, learners do not exert cognitive
effort to mentally construct dynamic representations. As a result, more cognitive resources are freed up, which could potentially be used
for learning-related activities and deep processing. On the other hand, learning with static graphical representations requires information
integration and inferential reasoning, which may impose considerable mental load on learners. These additional processing requirements
may cause learners to experience cognitive overload, as indicated in some research ﬁndings (Hegarty, 1992; Hegarty & Just, 1993).
Tversky, Morrison and Betrancourt (2002) concluded that the superiority of animation found in some reviewed studies (e.g., Park &
Gittelman, 1992; Thompson & Riding, 1990) should be attributed to the increased amount of information that is conveyed in animation
compared to static graphics, rather than the animation per se. By controlling for the information delivered by different visualizations in
a series of experiments, some researchers (Mayer, Hegarty, Mayer & Campbell, 2005) found that paper-based static media (i.e., illustrations
accompanied with text) was neutral or even better to promote retention and transfer than computer-based system-paced animations
with narrations. A cognitive load approach could provide one possible explanation of the failure of the animations’ effectiveness. Due to the
animation’s transitory nature, learners need to study the current information delivered by the animation while at the same time referring to
the previous learning content. As a result, learners may experience high level of extraneous load, which impedes learning. Mayer and
Chandler (2001) found that learners, who studied with segmented, learner controlled animations, understood the lightning formation
more deeply than their peers who viewed a whole, continuous unit of animation. Therefore, in order to mitigate the transitory nature
of animation, learner control should be available to learners (Ayres & Paas, 2007). Segmentation and interactivity are the speciﬁc techniques
to provide learners control over the learning environment (Mayer & Moreno, 2003).
From the reviewed literature, the results of research designed to compare the effectiveness of animations and static diagrams are
divergent and inconsistent. Some studies revealed the advantage of using instructional animations, while other studies showed the effects
of animations and static graphics were equivalent with regard to learning. A few studies even reported that static visualizations were
superior. Therefore, general comparisons between dynamic and static graphics without taking into account some speciﬁc issues will not
lead to any systematic results and conclusions. As Hegarty (2004, p. 344) indicated, researchers should investigate “what conditions must be
in place for dynamic visualizations to be effective in learning”. Höfﬂer and Leutner (2007) found effect sizes in favor of animations differed
for teaching declarative knowledge, problem-solving knowledge and procedural knowledge; procedural knowledge produced the largest
effect size while problem-solving knowledge showed the smallest. Therefore, the animated-static comparison should take different types
of knowledge into consideration.
1.2. Visual cueing
In a multimedia learning environment, information is presented through visual and/or auditory channels via multiple formats, such as
graphics, on-screen text and narrations. When graphics are presented with narrations, learners may need to search the relevant information
on the visualizations to build connections between what they see and what they hear. In a learning environment where complex visualizations are presented, visually searching relevant information to match narrations held in the working memory may become difﬁcult
for learners, and lead to high extraneous load. Under such conditions, learners may perceive and comprehend information from obvious yet
irrelevant parts of graphical representations, resulting in poor learning and performance (Lowe, 2003). Visual cueing is one of the techniques
to direct learners’ attention in the multimedia environment.
Visual cueing is the addition of non-content information (e.g., arrows, circles, and coloring) to visual representations. Research (de Koning,
Tabbers, Rikers, & Paas, 2009; de Koning, Tabbers, Rikers, & Paas, 2010a) has shown that visual cues are effective to guide learners’ attention to
animations in multimedia environments. As a result, visual cueing has the potential to facilitate the processes of selecting relevant information, which is one of the essential processes for active learning (Mayer, 2005). From the cognitive load perspective, a substantial number of
studies have found that visual cueing is an effective method to reduce extraneous load in multimedia learning environments (for reviews, see
Mayer & Moreno, 2003; Wouters, Paas, & van Merriënboer, 2008) and several studies supported the instructional beneﬁts of visual cueing
(Atkinson, Lin, & Harrison, 2009; de Koning, Tabbers, Rikers, & Paas, 2007, 2010b; Jamet, Gavota, & Quaireau, 2008; Jeung, Chandler, & Sweller,
1997; Kalyuga, Chandler, & Sweller, 1999). For instance, de Koning et al. (2007) conducted a study to investigate the effectiveness of a cued
animated cardiovascular system (using a spotlight effect). The researchers compared learning outcomes for participants who viewed a cued
animation with those who viewed the animation without a visual cue. The results showed that participants in the cued animated condition
had signiﬁcantly higher scores on both comprehension and transfer tests. Jamet et al. (2008) used a coloring technique as visual cues in their
study. They found that participants who studied saliently colored graphics of the human brain performed signiﬁcantly better than the group
that viewed non-salient colored graphics. In terms of efﬁciency (Paas & van Merriënboer, 1993; van Gog & Paas, 2008), Kalyuga et al. (1999)
found that color-coded diagrams promoted more efﬁcient learning than conventional no-color-coding diagrams.
2. Overview of the experiment
One purpose of the current study was to investigate whether animations were more effective than static graphics to promote learning,
i.e., to retain concepts and processes about the rock cycle, an earth science content. The rock cycle is a model that involves formation,

652

L. Lin, R.K. Atkinson / Computers & Education 56 (2011) 650–658

breakdown, and reformation between three main types of rocks on the earthdigneous rock, sedimentary rock and metamorphic rock. The
content requires learners to learn concepts and processes, which comprise the cycle of rock. In order to retain knowledge about concepts
and processes, learners need to accurately build dynamic mental models of how rocks are formed. Animations have the potential to facilitate
knowledge construction with this type of learning content (Höfﬂer & Leutner, 2007; Rieber, 1990; Yang et al., 2003). Therefore,
we hypothesized that animations enhance retention of both concepts and processes. The study also investigated the potential cognitive
beneﬁts of adding visual cues to visualizations to enhance science learning in a multimedia environment. Based on the literature reviewed
in previous section, we hypothesized that visual cueing is effective to enhance learning.
In addition to learning, cognitive load and motivation were also investigated. By providing learner control over animations, the transitory
nature of animations could be overcome. Therefore, we expected that when comparing animations to static graphics, animations would
reduce extraneous load and consequently foster germane load. We also expected visual cueing to reduce extraneous load in multimedia
learning environment, which is in line with Mayer and Moreno (2003) and Wouters et al. (2008). Only a few studies have investigated
learners’ motivation in multimedia learning, e.g., motivation in an agent-based environment (Moreno, Mayer, Spires, & Lester, 2001), in
an online animation-based environment (Rosen, 2009) or motivation with young children with low cognitive interest (Kim, Yoon, Whang,
Tversky, & Morrison, 2007). As motivation impacts learning (Boekaerts, 2007; Husman & Hilpert, 2007), this study explored the potential
effects of animations and visual cueing on learners’ intrinsic motivation in the multimedia environment.
Two independent variables were manipulated in the study: presentation format (animated vs. static graphics) and visual cueing (visual
cues vs. no visual cues). Other variables, such as the instructional content, the level of learner control and the number of presentation
segments were held constant. This study incorporated a number of dependent variables, including participants’ (a) learning outcomes,
(b) subjective cognitive load and (c) intrinsic motivation. As there was no time restriction in the learning phase, learning time was measured
as an en-route variable. Also, learners’ prior knowledge was statistically controlled in the study, as research revealed an interaction between
learners’ level of prior knowledge and the instructional presentation format (ChanLin, 1998, 2001; Kalyuga, 2007, 2008; Kalyuga, Ayres,
Chandler, & Sweller, 2003).
pﬃﬃﬃ
Learning efﬁciency scores were computed using the formula E ¼ ðzperformance  zlearning time Þ= 2, which was adapted from previous
literature (Paas & van Merriënboer, 1993; van Gog & Paas, 2008) and was used by Gerjets, Scheiter, Opfermann, Hesse, and Eysink (2009).
By using this construct, the current study has taken into account both learning outcomes and learning time. The greater the value of the
learning efﬁciency score, the more efﬁcient the instruction.
3. Method
3.1. Participants and design
One hundred and nineteen participants (61 males and 58 females) from a large southwestern university in the US participated in the
study. They were students recruited from the general campus population as well as from educational psychology and introductory computer
courses in the College of Education. They were all over 18 years old and their average age was 25.57 (SD ¼ 8.98). They were paid a small
stipend ($10) for their participation.
This study used a pretest–posttest, 2 (animation vs. static graphics)  2 (visual cues vs. no visual cues) between-subjects design, in which
the participants were randomly assigned to one of the four conditions: (a) static graphics with visual cues, (b) static graphics without visual
cues, (c) animations with visual cues, and (d) animations without visual cues.
Due to technical problems, data of seven participants were not recorded by the computer program. Therefore, they were excluded
for analysis, leaving 112 participants in total (28 for each condition).
3.2. Computer-based learning environment
The computer-based instructional materials were intended to deliver a lesson about the rock cycle. The characteristics of the three types
of rocks (i.e., igneous rock, sedimentary rock and metamorphic rock) were described. Processes, such as volcano eruption, weathering,
erosion and metamorphism, were also explained to show how different types of rocks transform into each other. By retaining concepts
and processes in this domain, learners construct their internal representations of rock cycle.
The learning environment was created using Visual Basic and was embedded with two-dimensional graphics created using Adobe Flash.
In all of the four experimental conditions, participants listened to a female voice without foreign accent, narrating the content and
simultaneously presenting the content-related graphics. The visual presentations differed among the four conditions. In the visually cued
animation condition, participants viewed 20 segments of animations about the characteristics of the three main types of rocks and the
transformation processes between each other. Visual cues (i.e., red arrows) were added to these animations to highlight important information. Speciﬁcally, the arrows were used to highlight concepts (e.g., name of a rock) or processes (e.g., weathering). No other types of visual
cues, such as circles, spotlight or hand pointing, were used. Participants assigned to the uncued animation condition viewed the same
number of animation segments in the same order as the cued condition and with the same narrations but without the addition of any visual
cues. In the cued-static-graphics condition (Fig. 1), 20 key frames taken from the corresponding segments of the cued animated graphics
were presented to participants; whereas 20 key frames taken from the corresponding uncued animation condition were presented to
participants assigned to uncued-static-graphics condition (Fig. 2). To keep the information delivered in the four conditions as equivalent
as possible, the narrations accompanying each of the 20 static graphics were exactly the same as those accompanied the 20 segments
of animations.
Before the lesson, a tutorial screen (Fig. 3) appeared and a brief description of the navigation features and the multimedia environment
was provided. Neither content-related graphics nor narration appeared in the tutorial. The content in computer-based lesson was
segmented into 20 separate screens. In the animation conditions, learners could stop and start the narration and animation as often as
they needed using the two control buttons located at the bottom of the animation on each screen. These buttons controlled both the
animation and the narration in order to maintain visual and audio synchronization. In the two conditions where the graphics were static,

L. Lin, R.K. Atkinson / Computers & Education 56 (2011) 650–658

653

Fig. 1. A sample segment of static graphic with visual cue.

learners were provided with the two control buttons to stop and start the narration. Each screen also included navigation buttons to allow
learners to go back to the previous screen or go forward to the next screen to view visualizations. There was no time limit for learners
to complete the lesson. However, their learning times (in minutes) were recorded by the computer program.
3.3. Measures and instruments
A pretest consisting of 20 multiple choice questions was administered to measure participants’ prior knowledge about the content. Each
test question had four choicesdone correct answer and three distracters. All items in the pretest were automatically scored by the computer
program according to the following rules: 0 points for an incorrect answer or 1 point for a correct answer. Therefore, a maximum total of 20
points could be achieved on the pretest. A posttest was used to measure participants’ comprehension of the material after instruction. The
posttest was almost identical to the pretest except that the order of the questions was different. The order of the pretest and posttest was
determined by using a random number table. Since the content involved learning about concepts and processes in the rock cycle and

Fig. 2. A sample segment of static graphic without visual cue.

654

L. Lin, R.K. Atkinson / Computers & Education 56 (2011) 650–658

Fig. 3. Tutorial screen.

knowledge retention was the learning goal, the test was divided into two sets of questions designed to measure both concept retention and
process retention. Speciﬁcally, there were 10 questions measuring learners’ concept retention and 10 questions measuring learners’ process
retention. A similar way of labeling different forms of tests was also used by Jamet et al. (2008) and Münzer et al. (2009). A sample test
question for concept retention is “What is the name of molten rock under the earth’s surface?” with four choices (A. Magma; B. Lava;
C. Sediments; D. Volcanic rock). A sample test question for process retention is “According to the rock cycle, which of the following is incorrect?”
with four choices (A. Igneous rocks may metamorphose into metamorphic rocks; B. Magma may crystallize to form igneous rocks;
C. Sedimentary rocks may weather to become igneous rocks; D. Metamorphic rocks may melt to become magma).
We assume that learners have the ability to reﬂect on their cognitive processes and provide their responses on numerical scales (Gopher
& Braune, 1984; Paas, Tuovinen, Tabbers, & Van Gerven, 2003). Therefore, self-report measures were used to measure participants’ cognitive
load and intrinsic motivation. Three subjective questions (i.e., task demands, effort and navigational demands, see Table 1) were used to
measure each of the subcomponents of cognitive load (i.e., intrinsic load, extrinsic load and germane load). They were adapted from the
NASA-TLX (Hart & Staveland, 1988) and were described in studies conducted by Gerjets, Scheiter, and Catrambone (2004) and Scheiter,
Gerjets, and Catrambone (2006). According to Scheiter et al. (2006), a mapping was assumed between the theoretical subcomponents
and the items of modiﬁed NASA-TLX. Participants rated each of the three questions on an 8-point Likert scale. For the ﬁrst question, “1” was
labeled as easy in the rating scale and “8” as demanding; for the second question, “1” was labeled as not hard at all and “8” as very hard; for the
third question, “1” was labeled as low effort and “8” as high effort. Participants’ intrinsic motivation was also measured by an 8-point
Likert scale ranging from “1” (not at all true) to “8” (very true). There were a total of 15 statements, adapted from Ryan’s study (Ryan, 1982),
assessing intrinsic motivation with six subscalesdinterest, competence, value, effort, pressure and choice (see Table 2).

3.4. Procedure
The experiment was conducted in a laboratory setting. At the beginning of the experiment, a researcher asked participants to sign
a consent form for participation. Next, they were seated at an individual cubicle, facing a computer, and were briefed by the researcher about
the procedure of the experiment. However, participants were unaware of the different conditions and the research questions included in the
experiment. Then, they started the pretest on the computer with no time limit. After the completion of the pretest, each participant
Table 1
Cognitive load measurement.
Item

Measure

Indication of

1. How much mental and physical activity was required?
That is, was the learning task easy (simple, forgiving) or demanding
(exacting or complex)?
2. How hard did you have to work in your attempt to understand the
contents of the learning environment?
3. How much effort did you have to invest in order to navigate the
learning environment (e.g., for deciding between different hyperlinks,
ﬁnding your way around)?

Task demands

Intrinsic load

Effort

Germane load

Navigational demands

Extraneous load

L. Lin, R.K. Atkinson / Computers & Education 56 (2011) 650–658

655

Table 2
Intrinsic motivation items.
Item

Subscale

1. I thought it was a boring activity.
2. I think I was pretty good at this activity.
3. I think that doing this activity could be useful.
4. I didn’t try very hard to do well at this activity.
5. I did not feel nervous at all while doing this.
6. I believe I had some choice about doing this activity.
7. It was important to me to do well at this task.
8. I believe doing this activity could be beneﬁcial to me.
9. I felt very tense while doing this activity.
10. I did this activity because I had no choice.
11. This activity was fun to do.
12. I put a lot of effort into this.
13. This was an activity that I couldn’t do very well.
14. I believe this activity could be of some value to me.
15. I would describe this activity as very interesting.

Interest
Competence
Value
Effort
Pressure
Choice
Effort
Value
Pressure
Choice
Interest
Effort
Competence
Value
Interest

was provided with a randomly assigned experiment ID number to start the computer-based lesson. Once the participants completed the
lesson, a posttest was administered followed by a questionnaire. Neither activity had a time limit. The questionnaire had two parts:
subjective cognitive load measures and intrinsic motivation measures. Upon completion of the posttest and the questionnaire, the
participants were thanked and paid. The participants needed approximately 30 min to complete the entire study.
4. Results
Table 3 presents the means, adjusted means (if available) and standard deviations for two types of learning outcome measures (i.e.,
concept retention and process retention), subjective cognitive load measures (i.e., task demands, effort and navigational demands), learning
time and learning efﬁciency for the four experimental conditions. All of the means for the learning outcome measures were transformed
from raw scores to percentages. An alpha level of .05 was used for all statistical analysis. Cohen’s f was used as an effect size index.
Accordingly, .02, .15 and .35 were deﬁned as the values for small, medium and large effect sizes (Cohen, 1988).
4.1. Prior knowledge
A one-way analysis of variance (ANOVA) was conducted to evaluate whether participants’ prior knowledge signiﬁcantly differed across
the four experimental conditions. There was no signiﬁcant difference of total pretest percentage scores across the four conditions, F(3,
108) ¼ 1.18, MSE ¼ .03, p ¼ .32, f ¼ .18. In addition, no signiﬁcant difference was found in participants’ knowledge on concepts (F(3, 108) ¼ .86,
MSE ¼ .05, p ¼ .47, f ¼ .15) or processes (F(3, 108) ¼ 1.58, MSE ¼ .04, p ¼ .20, f ¼ .21).
4.2. Learning outcomes
4.2.1. Concept retention
A two-way analysis of covariance (ANCOVA) was planned to evaluate the effects of presentation format (static vs. animated) and visual
cueing (cued vs. uncued) on participants’ concept retention. The percentage correct score on the pretest for concept retention was used as

Table 3
Mean and standard deviations of test scores, cognitive load, time, instructional efﬁciency and six subscales of intrinsic motivation.
Animation

Static graphics

Visual cueing

CR/pretest
PR/pretest
CR/posttest
PR/posttest
Task demands
Effort
Navigational demands
Timea
E
Interest/IM
Competence/IM
Value/IM
Effort/IM
Pressure/IM
Choice/IM

M

SD

50.36
51.07
86.43
80.71
3.29
3.18
3.25
8.43
.28
5.71
6.14
5.67
5.42
2.11
6.96

.22
.19
.12
.17
1.58
1.87
2.38
.81
.71
1.36
1.45
1.44
1.65
1.33
1.14

No visual cueing
Adj. M

88.07
81.45
3.19
3.09
3.15

M

SD

56.01
58.93
86.43
81.43
2.79
3.11
1.93
8.37
.02
5.94
6.91
6.57
5.8
1.48
7.13

.19
.18
.17
.18
1.71
2.03
1.63
1.34
.76
1.74
1.33
1.51
1.9
.82
1.27

Visual cueing
Adj. M

86.19
79.21
2.91
3.22
2.07

M

SD

55.36
47.86
83.21
79.64
3.07
2.89
2.32
7.87
.38
6.05
5.84
6.5
5.31
2.29
6.96

.23
.21
.16
.18
1.7
1.75
1.79
1.26
.84
1.58
1.54
1.57
1.81
1.6
1.4

No visual cueing
Adj. M

83.21
81.59
2.96
2.78
2.19

M

SD

59.64
54.29
81.79
81.07
3.32
3.29
2.14
9.24
.45
5.64
6.32
6.17
5.57
2.23
7.39

.23
.22
.17
.15
1.88
1.86
1.58
2.45
1.08
1.7
1.72
1.31
1.4
1.42
1.06

Adj. M

80.38
80.6
3.41
3.37
2.24

Note. Measures of cognitive load and intrinsic motivation were on 8-point scales. Adj. ¼ adjusted. CR ¼ concept retention. PR ¼ process retention. E ¼ efﬁciency. IM ¼ intrinsic
motivation. Means of CR/pretest, PR/pretest, CR/posttest and PR/posttest and adjusted means of CR/posttest and PR/posttest are percentage scores.
a
The unit of time is minute.

656

L. Lin, R.K. Atkinson / Computers & Education 56 (2011) 650–658

a covariate. A preliminary analysis was conducted to evaluate the homogeneity-of-slope assumption. It showed a non-signiﬁcant interaction
between presentation format and the covariate (F(1, 108) ¼ 3.29, MSE ¼ .02, p ¼ .51, f ¼ .07) as well as a non-signiﬁcant interaction between
visual cueing and the covariate (F(1, 108) ¼ .25, p ¼ .62, f ¼ .04), indicating that the relationship between the covariate and the dependent
variable (i.e., concept retention) did not differ signiﬁcantly as a function of the two independent variables (i.e., presentation format and
visual cueing). Therefore, ANCOVA was conducted. The analysis revealed a signiﬁcant difference between the animation conditions and
static graphics conditions, F(1, 107) ¼ 4.18, MSE ¼ .02, p ¼ .04, f ¼ .20, indicating a medium-to-large effect of the superiority of animations
(M ¼ 86.4%, SD ¼ .14) over static graphics (M ¼ 82.5%, SD ¼ .16). However, there was no signiﬁcant visual cueing main effect between
the cued condition (M ¼ 84.8%, SD ¼ .14) and uncued condition (M ¼ 84.1%, SD ¼ .17) with regard to concept retention, F(1, 107) ¼ .81, p ¼ .37,
f ¼ .09. Neither was there an interaction effect, F(1, 107) ¼ .03, p ¼ .85, f ¼ .02.
4.2.2. Process retention
A two-way ANCOVA was planned to evaluate the effects of presentation format and visual cueing on participants’ process retention. The
percentage correct score on the pretest for process retention was used as a covariate. The homogeneity-of-slope assumption was examined
before ANCOVA was conducted. This preliminary analysis showed a non-signiﬁcant interaction between presentation format and the
covariate (F(1, 108) ¼ .01, MSE ¼ .02, p ¼ .93, f ¼ .01) as well as a non-signiﬁcant interaction between visual cueing and the covariate (F(1,
108) ¼ .66, p ¼ .42, f ¼ .08), indicating that the relationship between the covariate and the dependent variable (i.e., process retention)
did not differ signiﬁcantly as a function of the two independent variables. Therefore, ANCOVA was conducted. No signiﬁcant main effects
were found between the animation condition and static graphics condition, F(1, 107) ¼ .07, MSE ¼ .02, p ¼ .80, f ¼ .03, as well as the visually
cued conditions and uncued conditions, F(1, 107) ¼ .30, p ¼ .59, f ¼ .05. Nor was there a signiﬁcant interaction effect, F(1, 107) ¼ .05, p ¼ .83,
f ¼ .02.
4.3. Subjective cognitive load
Three two-way ANCOVAs were planned to evaluate the effects of presentation format and visual cueing on learners’ task demands, effort,
and navigational demands, which were indications of intrinsic, extraneous and germane cognitive load respectively. The covariate was the
percent correct score on all pretest questions so that participants’ prior knowledge was statistically controlled. Before each ANCOVA was
conducted, the homogeneity-of-slope assumption was examined. This preliminary analyses showed: (a) for task demands, a non-signiﬁcant
interaction between presentation format and the covariate (F(1, 108) ¼ .10, MSE ¼ 2.64, p ¼ .75, f ¼ .03) as well as a non-signiﬁcant interaction between visual cueing and the covariate (F(1, 108) ¼ .01, p ¼ .92, f ¼ .01); (b) for effort, a non-signiﬁcant interaction between
presentation format and the covariate (F(1, 108) ¼ .08, MSE ¼ 3.22, p ¼ .78, f ¼ .03) as well as a non-signiﬁcant interaction between visual
cueing and the covariate (F(1, 108) ¼ .97, p ¼ .33, f ¼ .09); and (c) for navigational demands, a non-signiﬁcant interaction between presentation format and the covariate (F(1, 108) ¼ .30, MSE ¼ 3.17, p ¼ .58, f ¼ .05) as well as a non-signiﬁcant interaction between visual cueing and
the covariate (F(1, 108) ¼ 2.16, p ¼ .15, f ¼ .14). Therefore, planned ANCOVAs were conducted. With regard to task demandsdan indication of
intrinsic load, neither main effects nor interaction was signiﬁcant: for presentation main effect, F(1, 107) ¼ .18, MSE ¼ 2.63, p ¼ .67, f ¼ .04; for
visual cueing main effect, F(1, 107) ¼ .07, p ¼ .80, f ¼ .03; for interaction effect, F(1, 107) ¼ 1.45, p ¼ .23, f ¼ .11. Neither main effects nor
interaction was found signiﬁcant for effortdan indication of germane load: for presentation main effect, F(1, 107) ¼ .06, MSE ¼ 3.24, p ¼ .81,
f ¼ .03; for visual cueing main effect, F(1, 107) ¼ 1.09, p ¼ .30, f ¼ .10; for interaction effect, F(1, 107) ¼ .44, p ¼ .51, f ¼ .06. Moreover, neither
main effects nor interaction was found signiﬁcant for navigational demandsdan indication of extraneous load; for presentation main effect,
F(1, 107) ¼ 1.39, MSE ¼ 3.08, p ¼ .24, f ¼ .11, for visual cueing main effect, F(1, 107) ¼ 2.32, p ¼ .13, f ¼ .15, for interaction effect, F(1, 107) ¼ 2.90,
p ¼ .09, f ¼ .16.
4.4. Learning time
A two-way ANOVA was conducted to explore whether participants in the four experimental conditions had spent signiﬁcantly different
times in studying the visualizations. A signiﬁcant difference was found between the visually cued conditions (M ¼ 8.15 min, SD ¼ .21 min)
and the uncued conditions (M ¼ 8.81 min, SD ¼ .21 min), F(1, 108) ¼ 4.83, MSE ¼ 2.52, p ¼ .03, indicating that participants in the uncued
conditions spent more time on learning than those in cued conditions. The effect size (f ¼ .21) showed a medium effect of visual cueing.
No signiﬁcant difference was found between the animation conditions (M ¼ 8.40 min, SD ¼ 1.10 min) and static graphics conditions
(M ¼ 8.56 min, SD ¼ 2.05 min), F(1, 108) ¼ .29, p ¼ .59, f ¼ .05. In addition, there was a signiﬁcant interaction, F(1, 108) ¼ 5.60, p ¼ .02, f ¼ .23.
Therefore, analysis of simple main effects was conducted. To control for type I error, Bonferroni approach was used and alpha was set
at .025 (.05/2). It was found that participants spent signiﬁcantly more time learning with uncued static graphics than those learning
with cued static graphics, F(1, 108) ¼ 10.41, MSE ¼ 2.52, p ¼ .002, with a medium-to-large effect size (f ¼ .31). However, the times that
participants spent studying cued and uncued animations were not signiﬁcantly different, F(1, 108) ¼ .01, p ¼ .91, f ¼ .01. No other signiﬁcant
results were found concerning learning times.
4.5. Learning efﬁciency
According to Paas and van Merriënboer (1993) and van Gog and Paas (2008), raw scores (performance and time) should be transformed
to z scores to compute the efﬁciency. In order to take into account participants’ prior knowledge, gain scores (posttest–pretest) were
pﬃﬃﬃ
computed and standardized. Therefore, learning efﬁciency scores were computed by using the formula E ¼ ðzperformance  zlearning time Þ= 2.
A two-way ANOVA was conducted to investigate whether participants’ learning efﬁciency differed signiﬁcantly among the four
conditions. The presentation main effect was non-signiﬁcant for the animation conditions (M ¼ .15, SD ¼ .74) and the static graphics
conditions (M ¼ .03, SD ¼ 1.04), F(1, 108) ¼ 1.20, MSE ¼ .74, p ¼ .28, f ¼ .11; whereas a signiﬁcant visual cueing main effect was found
between the visually cued conditions (M ¼ .33, SD ¼ .78) and the uncued conditions (M ¼ .21, SD ¼ .95), F(1, 108) ¼ 11.24, p ¼ .001, f ¼ .32.
No interaction effect was found, F(1, 108) ¼ 3.04, p ¼ .08, f ¼ .17.

L. Lin, R.K. Atkinson / Computers & Education 56 (2011) 650–658

657

4.6. Intrinsic motivation
A two-way multivariate analysis of variance (MANOVA) was conducted to explore the effects of presentation formats and visual cueing
on the six subscales of intrinsic motivationdinterest, competence, value, effort, pressure and choice. Means on each of the six subscales
were computed and were used as dependent variables. No signiﬁcant difference was found on the six subscales for the presentation format
main effect, Wilks’ lambda ¼ .92, F(6, 103) ¼ 1.51, p ¼ .18, f ¼ .30, nor the visual cueing main effect, Wilks’ lambda ¼ .90, F(6, 103) ¼ 1.85,
p ¼ .10, f ¼ .33. Neither was there a signiﬁcant interaction, Wilks’ lambda ¼ .93, F(6, 103) ¼ 1.30, p ¼ .26, f ¼ .28.
5. Discussion
One purpose of the study was to investigate the superiority of animations over static graphics in a multimedia learning environment. No
previous research has investigated this issue in learning knowledge about rock cycle, which is one of the signiﬁcant contributions of
the current study. We hypothesized that instructional animations should promote the retention of concept and process knowledge in the
domain of rock cycle. This hypothesis was partially supporteddanimations promote learning concepts. In order to learn concepts about
the rock cycle, learners need to construct internal representations of the rock cycle. From the results, we conclude that animations facilitate
this knowledge construction. One possible explanation is that the changes over time that the animations showed corresponded to the
nature of the rock cycle concepts. For instance, learners may beneﬁt from the animations showing that magma is the molten rock under
the earth’s surface while lava is the molten rock coming out to the earth’s surface. According to Tversky et al. (2002), this correspondence is
the condition for successful use of animations. The current ﬁnding is consistent with Höfﬂer and Leutner’s (2007) results that revealed
a medium positive effect for learning declarative knowledge with animations. As we have controlled the degree of interactivity, the number
of presentation segments and accompanying narrations to be identical across all of the four experimental conditions, we can conclude
that the animation effect in the study is not due to any of the above-mentioned three factors. In addition, it is worthwhile to indicate that
no positive result of animations was found for the intrinsic motivation scale in the study. Therefore, we should not attribute the animation
effect to motivation. It is the animation per se that facilitates concept retention in the domain.
The study revealed a non-signiﬁcant pattern that on average participants who studied animations scored higher on process retention
test questions than those who studied static graphics. As retaining information from animations greatly depends on the perception of
animations (Lowe, 2003), it is likely that the animations presenting rock cycle processes are not salient enough to be perceived by the
learners in both cued and uncued conditions. This implies that more visual cueing devices are needed for the visualizations. An eye tracking
technique, tracking learners’ eye movements on animations, could be considered in future research to identify speciﬁc diagrammatic
elements that need to be visually cued. With regard to cognitive load, we did not ﬁnd an animation effect for any of the three items intended
to measure intrinsic, germane and extraneous load. This may be due to the fact that the subjective cognitive load measures were not
administered during instruction. As a result, no distinction between concept retention and process retention can be made concerning
cognitive load. In future research, we could consider modifying the cognitive load measures by specifying task demands, effort and navigational demands in learning concepts or processes of the domain content and administering them multiple times during the learning
phase. By doing so, the limitation in the current study could be overcome. In addition, we also admit the possible measurement errors in
measuring cognitive load, as currently the subjective rating scales make it difﬁcult to distinguish each subcomponent of cognitive load
(Schnotz & Kurschner, 2007).
In this study, we also investigated the effect of visual cueing. In the past decade, literatures (de Koning et al., 2007, 2010b; Jamet et al.,
2008; Jeung et al., 1997; Kalyuga et al., 1999; Mayer & Moreno, 2003; Wouters et al., 2008) revealed ﬁndings that supported the instructional
beneﬁts of visual cueing. Although in the current study no signiﬁcant visual cueing effect was found for learning outcome measures,
signiﬁcant differences were found in learning time and efﬁciency that favored visual cueing in an environment that does not impose time
constraint on learning. Speciﬁcally, when studying visually cued graphics, learners spent less time and learned more efﬁciently than their
peers studying uncued instructional materials. Furthermore, learners in cued-static condition spent less time than their peers in uncuedstatic condition. Therefore, the results of the current study partially conﬁrmed our hypothesis that from the perspective of learning time and
efﬁciency visual cueing enhances learning. Visual cues may reduce learners’ search activity on the graphics, leading to the reduced learning
time and the enhanced efﬁciency in the learning environment that does not impose a time limit. Therefore, learners ﬁnally reached the same
level of knowledge with different learning times due to the visual cueing effect. However, no cueing effect on cognitive load has been found
in the current study. This is consistent with a few previous studies (de Koning et al., 2007, 2010a, 2010b). Learners’ low ratings on the three
cognitive load measures and fairly high learning outcomes shed some light on the possible explanations. It is possible that the instructional
content was not difﬁcult for learners after they studied it for a certain amount of time. Consequently, this resulted in low self-report ratings
on those subjective cognitive load measures. In future studies, more measures and techniques may be used to determine learners’
perceptions of difﬁculty. For instance, we can consider adding more subjective questions to ask learners about their perceived difﬁculty of
the instruction and their frustration level. We may also consider using physiological measures in the future. Speciﬁcally, physiological
sensors can be used to measure a learner’s pressure on a mouse and the movements on a chair to reﬂect his/her frustration and other
emotional states (D’Mello, Picard, & Graesser, 2007). Also, electroencephalography (EEG) methodology can be used to assess variations of
cognitive load (Antonenko, Paas, Grabner, & Van Gog, in press).
Some empirical studies (e.g., Atkinson et al., 2009; Boucheix & Lowe, 2010; de Koning et al., 2007, 2010a, 2010b; Mautone & Mayer, 2001)
only investigated the visual cueing effect in learning with animations. The results of the current study revealed that there was a visual cueing
effect on learning time when comparing the two static graphics conditions, which is also one of the signiﬁcant contributions to the literature. The uncued static graphics prevented learning so that learners had to invest more time to compensate for disadvantaged learning. Our
explanation is that the reduced time in learning cued static graphics may be attributed to the reduced visual search activity. This shows that
adding visual cueing devices to static graphics, a simpler presentation format than animations, has instructional beneﬁts. However, the
cueing effect disappeared when the instructional visualizations were animated (i.e., the interaction effect) suggesting that the type of
visualizations is a potential moderator that inﬂuences the visual cueing effect in multimedia learning. Therefore, we recommend
researchers take the type of visualizations into consideration when conducting research on visual cueing in the future.

658

L. Lin, R.K. Atkinson / Computers & Education 56 (2011) 650–658

References
Antonenko, P., Paas, F., Grabner, R., Van Gog, T. Using electroencephalography to measure cognitive load. Educational Psychology Review, in press.
Arguel, A., & Jamet, E. (2009). Using video and static pictures to improve learning of procedural contents. Computers in Human Behavior, 25(2), 354–359.
Atkinson, R. K., Lin, L., & Harrison, C. (2009). Comparing the efﬁcacy of different signaling techniques. In Proceedings of world conference on educational multimedia, hypermedia
and telecommunications 2009 (pp. 954–962). Chesapeake, VA: AACE.
Ayres, P., Marcus, N., Chan, C., & Qian, N. (2009). Learning hand manipulative tasks: when instructional animations are superior to equivalent static representations.
Computers in Human Behavior, 25(2), 348–353.
Ayres, P., & Paas, F. (2007). Making instructional animations more effective: a cognitive load approach. Applied Cognitive Psychology, 21(6), 695–700.
Bétrancourt, M., & Tversky, B. (2000). Effect of computer animation on users’ performance: a review. Le Travail Humain, 63(4), 311–329.
Boekaerts, M. (2007). What have we learned about the link between motivation and learning/performance? Zeitschrift für Pädagogische Psychologie, 21, 263–269.
Boucheix, J., & Lowe, R. K. (2010). An eye tracking comparison of external pointing cues and internal continuous cues in learning with complex animations. Learning and
Instruction, 20(2), 123–135.
Catrambone, R., & Seay, A. F. (2002). Using animation to help students learn computer algorithms. Human Factors, 44(3), 495–511.
ChanLin, L. (1998). Animation to teach students of different knowledge levels. Journal of Instructional Psychology, 25(3), 166–175.
ChanLin, L. (2001). Formats and prior knowledge on learning in a computer-based lesson. Journal of Computer Assisted Learning, 17(4), 409–419.
Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Hillsdale, NJ: L. Erlbaum Associates.
de Koning, B. B., Tabbers, H., Rikers, R. M. J. P., & Paas, F. (2007). Attention cueing as a means to enhance learning from an animation. Applied Cognitive Psychology, 21(6), 731–746.
de Koning, B. B., Tabbers, H., Rikers, R. M. J. P., & Paas, F. (2009). Towards a framework for attention cueing in instructional animations: guidelines for research and design.
Educational Psychology Review, 21(2), 113–140.
de Koning, B. B., Tabbers, H. K., Rikers, R. M. J. P., & Paas, F. (2010a). Attention guidance in learning from a complex animation: seeing is understanding? Learning and
Instruction, 20(2), 111–122.
de Koning, B. B., Tabbers, H. K., Rikers, R. M. J. P., & Paas, F. (2010b). Learning by generating vs. receiving instructional explanations: two approaches to enhance attention
cueing in animations. Computers & Education, 55(2), 681–691.
D’Mello, S., Picard, R., & Graesser, A. (2007). Toward an affect-sensitive autotutor. IEEE Intelligent Systems, 22(4), 53–61.
Gerjets, P., Scheiter, K., & Catrambone, R. (2004). Designing instructional examples to reduce intrinsic cognitive load: molar versus modular presentation of solution
procedures. Instructional Science, 32(1–2), 33–58.
Gerjets, P., Scheiter, K., Opfermann, M., Hesse, F. W., & Eysink, T. H. S. (2009). Learning with hypermedia: the inﬂuence of representational formats and different levels of
learner control on performance and learning behavior. Computers in Human Behavior, 25(2), 360–370.
Gopher, D., & Braune, R. (1984). On the psychophysics of workload: why bother with subjective measures? Human Factors, 26, 519–532.
Hart, S. G., & Staveland, L. E. (1988). Development of NASA-TLX (task load index): results of experimental and theoretical research. In P. A. Hancock, & N. Meshkati (Eds.),
Human mental workload (pp. 139–183). Amsterdam: North-Holland.
Hegarty, M. (1992). Mental animation: inferring motion from static displays of mechanical systems. Journal of Experimental Psychology: Learning, Memory, and Cognition, 18(5),
1084–1102.
Hegarty, M. (2004). Dynamic visualizations and learning: getting to the difﬁcult questions. Learning and Instruction, 14(3), 343–351.
Hegarty, M., & Just, M. A. (1993). Constructing mental models of machines from text and diagrams. Journal of Memory and Language, 32(6), 717–742.
Höfﬂer, T. N., & Leutner, D. (2007). Instructional animation versus static pictures: a meta-analysis. Learning and Instruction, 17(6), 722–738.
Husman, J., & Hilpert, J. (2007). The intersection of students’ perceptions of instrumentality, self-efﬁcacy, and goal orientations in an online mathematics course. Zeitschrift für
Pädagogische Psychologie, 21, 229–239.
Jamet, E., Gavota, M., & Quaireau, C. (2008). Attention guiding in multimedia learning. Learning and Instruction, 18(2), 135–145.
Jeung, H., Chandler, P., & Sweller, J. (1997). The role of visual indicators in dual sensory mode instruction. Educational Psychology, 17(3), 329–343.
Kalyuga, S. (2007). Expertise reversal effect and its implications for learner-tailored instruction. Educational Psychology Review, 19(4), 509–539.
Kalyuga, S. (2008). Relative effectiveness of animated and static diagrams: an effect of learner prior knowledge. Computers in Human Behavior, 24(3), 852–861.
Kalyuga, S., Ayres, P., Chandler, P., & Sweller, J. (2003). The expertise reversal effect. Educational Psychologist, 38(1), 23–31.
Kalyuga, S., Chandler, P., & Sweller, J. (1999). Managing split-attention and redundancy in multimedia instruction. Applied Cognitive Psychology, 13(4), 351–371.
Kim, S., Yoon, M., Whang, S. M., Tversky, B., & Morrison, J. B. (2007). The effect of animation on comprehension and interest. Journal of Computer Assisted Learning, 23(3), 260–270.
Kriz, S., & Hegarty, M. (2007). Top-down and bottom-up inﬂuences on learning from animations. International Journal of Human–Computer Studies, 65(11), 911–930.
Large, A., Beheshti, J., Breuleux, A., & Renaud, A. (1996). Effect of animation in enhancing descriptive and procedural texts in a multimedia learning environment. Journal of the
American Society for Information Science, 47(6), 437–448.
Lowe, R. K. (2003). Animation and learning: selective processing of information in dynamic graphics. Learning and Instruction, 13(2), 157–176.
Mautone, P. D., & Mayer, R. E. (2001). Signaling as a cognitive guide in multimedia learning. Journal of Educational Psychology, 93(2), 377–389.
Mayer, R. E. (2005). Cognitive theory of multimedia learning. In R. E. Mayer (Ed.), The Cambridge handbook of multimedia learning (pp. 31–48). New York, NY, USA: Cambridge
University Press.
Mayer, R. E., & Chandler, P. (2001). When learning is just a click away: does simple user interaction foster deeper understanding of multimedia messages? Journal of
Educational Psychology, 93(2), 390–397.
Mayer, R. E., Hegarty, M., Mayer, S., & Campbell, J. (2005). When static media promote active learning: annotated illustrations versus narrated animations in multimedia
instruction. Journal of Experimental Psychology: Applied, 11(4), 256–265.
Mayer, R. E., & Moreno, R. (2003). Nine ways to reduce cognitive load in multimedia learning. Educational Psychologist, 38(1), 43–52.
Moreno, R., Mayer, R. E., Spires, H. A., & Lester, J. C. (2001). The case for social agency in computer-based teaching: do students learn more deeply when they interact with
animated pedagogical agents? Cognition and Instruction, 19(2), 177–213.
Münzer, S., Seufert, T., & Brünken, R. (2009). Learning from multimedia presentations: facilitation function of animations and spatial abilities. Learning and Individual
Differences, 19(4), 481–485.
Paas, F., Renkl, A., & Sweller, J. (2003). Cognitive load theory and instructional design: recent developments. Educational Psychologist, 38(1), 1–4.
Paas, F., Tuovinen, J. E., Tabbers, H., & Van Gerven, P. W. M. (2003). Cognitive load measurement as a means to advance cognitive load theory. Educational Psychologist, 38(1), 63–71.
Paas, F., & van Merriënboer, J. J. G. (1993). The efﬁciency of instructional conditions: an approach to combine mental effort and performance measures. Human Factors, 35(4),
737–743.
Park, O.-C., & Gittelman, S. S. (1992). Selective use of animation and feedback in computer-based instruction. Educational Technology, Research, and Development, 40, 27–38.
Rieber, L. P. (1990). Using computer animated graphics with science instruction with children. Journal of Educational Psychology, 82(1), 135–140.
Rosen, Y. (2009). The effects of an animation-based on-line learning environment on transfer of knowledge and on motivation for science and technology learning. Journal of
Educational Computing Research, 40(4), 451–467.
Ryan, R. M. (1982). Control and information in the intrapersonal sphere: an extension of cognitive evaluation theory. Journal of Personality and Social Psychology, 43, 450–461.
Scheiter, K., Gerjets, P., & Catrambone, R. (2006). Making the abstract concrete: visualizing mathematical solution procedures. Computers in Human Behavior, 22(1), 9–25.
Schnotz, W., & Kurschner, C. (2007). A reconsideration of cognitive load theory. Educational Psychology Review, 19(4), 469–508.
Sweller, J., van Merrienboer, J. J. G., & Paas, F. (1998). Cognitive architecture and instructional design. Educational Psychology Review, 10(3), 251–296.
Thompson, S. V., & Riding, R. J. (1990). The effect of animated diagrams on the understanding of a mathematical demonstration in 11- to 14-year-old pupils. British Journal of
Educational Psychology, 60, 93–98.
Tversky, B., Morrison, J. B., & Betrancourt, M. (2002). Animation: can it facilitate? International Journal of Human–Computer Studies, 57(4), 247–262.
van Gog, T., & Paas, F. (2008). Instructional efﬁciency: revisiting the original construct in educational research. Educational Psychologist, 43(1), 16–26.
Wong, A., Marcus, N., Ayres, P., Smith, L., Cooper, G. A., Paas, F., et al. (2009). Instructional animations can be superior to statics when learning human motor skills. Computers
in Human Behavior, 25(2), 339–347.
Wouters, P., Paas, F. G. W. C., & van Merriënboer, J. J. G. (2008). How to optimize learning from animated models: a review of guidelines based on cognitive load. Review of
Educational Research, 78(3), 645–675.
Yang, E., Andre, T., & Greenbowe, T. J. (2003). Spatial ability and the impact of visualization/animation on learning electrochemistry. International Journal of Science Education,
25(3), 329.

2013 IEEE 13th International Conference on Advanced Learning Technologies

Affect Recognition in Learning Scenarios: Matching Facial- and BCI-Based Values
Javier Gonzalez-Sanchez1, Maria Elena Chavez-Echeagaray1, Lijia Lin2, Mustafa Baydogan1,
Robert Christopherson1, David Gibson3, Robert Atkinson1, Winslow Burleson1
1

Arizona State University, AZ, USA, 2East China Normal University, Shanghai, China, 3Curtin University, Perth, Australia
javiergs@asu.edu, helenchavez@asu.edu, ljlin@psy.ecnu.edu.cn, mbaydoga@asu.edu,
robert.christopherson@asu.edu, david.gibson@curveshift.com, robert.atkinson@asu.edu, winslow.burleson@asu.edu

Abstract— The ability of a learning system to infer a student’s
affects has become highly relevant to be able to adjust its
pedagogical strategies. Several methods have been used to infer
affects. One of the most recognized for its reliability is facebased affect recognition. Another emerging one involves the
use of brain-computer interfaces. In this paper we compare
those strategies and explore if, to a great extent, it is possible to
infer the values of one source from the other source.

III.

To explore the relationships between face-based and
BCI-based approaches, we used data collected from
participants engaged in two studies designed to stimulate
distinct affects. The stimuli, protocol, and participant details
for these studies are presented below.
Study one. The Guitar Hero® video game [7] was used
for this study to generate both deep engagement and, at
times, frustration [6]. The goal of the game is to press one or
more colored buttons at the same time as moving target
lights of the same color cross a line on the screen. The study
consisted of a one-hour session, with the first 15 minutes
allocated to practice, followed by a 45-minute session in
which participants played four songs of their choice, one of
each level: easy, medium, hard, and expert. Data was
collected from six participants.
Study two. Text from an educational psychology
textbook was used for this study. This text was presented in
two ways: one with off-task images and captions (nonessential content) and the other containing only essential
content. Having these two presentations allowed the
evaluation of how the presence or absence of off-task images
could impact the engagement of the reader and therefore the
understanding of the reading. The study consisted of a onehour session in which participants were presented with 10
pages and asked to read for understanding. Each participant
was asked to complete a pre- and post-test. Data was
collected from 27 participants.

Keywords-brain computer interfaces; affect recognition;
random forest

I.

INTRODUCTION

Several approaches have been used for affect recognition
and researchers have explored how these approaches
complement or supplement each other in learning scenarios
[1]. Affect recognition using facial expression as input has
been regarded as the most accurate measurement [2].
However, brain-computer interfaces (BCI) have not yet been
incorporated. In this paper we describe our results correlating
face-based and BCI-based values toward the verification of
the interchangeability of these approaches in learning
contexts. We have explored if, to a great extent, it is possible
to infer the values of one source from the other source.
II.

BACKGROUND

The face-based approach uses as input sequences of head
and facial images. It performs best when the user stays
within the camera’s viewing angle and avoids abrupt
movements (e.g., reading on the screen or selecting with the
mouse). In cases where the user is not quietly sitting (e.g.,
playing an active video game or participating in an active
environment), the loss of data becomes problematic. Our
research uses MindReader, face-based affect recognition
software [3], which provides measurements for agreement,
disagreement, concentrating, interest, thinking, and
unsureness.
The BCI-based approach uses non-invasive electrodes to
capture brain-wave signals and uses pattern detection
analysis of these signals to infer affective states [4]. The BCI
allows the user to move freely and is able to provide accurate
inferences; however, it is susceptible to electromagnetic
interference and its setup encounters some difficulties (e.g.,
moisturizing the electrodes and maintaining their contact)
that can cause missing or noisy data. Our research uses the
Emotiv® EPOC headset [5], which reports measurements for
excitement, engagement, meditation, and frustration.
978-0-7695-5009-1/13 $26.00 © 2013 IEEE
DOI 10.1109/ICALT.2013.26

CASE STUDIES

IV.

MATCHING VALUES AND DISCUSSION

The values collected from both approaches are in the
range of 0 to 1 and represent the level of each affect. The
sampling rate from the face-based approach is 10Hz while
the sampling rate from the BCI-based approach is 8Hz;
hence, the data needed to be synchronized. The data
synchronization was done using a state-machine technique,
in which it is assumed that an input value is “alive” until a
new one arrives. The resulting dataset was composed of 10
rows per second (the higher sampling rate) and each row has
the attributes of both approaches (ten values).
Random Forest (RF) [8] was used to model the relations
between both approaches. RF uses a random selection of
features and provides a ranking of the feature’s importance
as a predictor. This makes RF a good choice for our
multidimensional exploratory factor analysis. We built two
RF models, the first one to predict each face-based inferred
70

affect using the BCI-based inferred affect and the second one
to do the opposite, predicting each BCI-based inferred affect
using the face-based inferred affect. The performance
measure considered in our study is the correlation of the
predicted values with the actual values. The obtained
correlation values are as given in Table 1.

engagement, meditation, and frustration (factor>0.60). High
values of frustration increase the assumption of interest;
however, a detailed interpretation of the other variables is
part of the ongoing work.
5) Thinking is inferred with excitement and meditation
as the variables with most importance (factor>0.90)
followed by engagement and frustration (factor>0.75).
Higher values of engagement and low values of frustration
and excitement increase the assumption of thinking.
6) Unsure is inferred with engagement and excitement
as the variable with most importance (factor>0.90) followed
by meditation and frustration (factor>0.75). High values of
engagement and meditation and low excitement and
frustration are related to unsureness.

TABLE I. CORRELATION BETWEEN BCI-BASED AND FACE-BASED VALUES
USING RF MODELS
BCI-based values
Excitement
Engagement
Meditation
Frustration
Face-based values
Agreement
Concentrating
Dissagreement
Interested
Thinking
Unsure

Correlation with predicted
values using face-based values
0.284
0.282
0.188
0.275
Correlation with predicted
values using BCI-based values
0.760
0.765
0.794
0.774
0.780
0.828

V.

CONCLUSIONS

According to our results, it is fairly reliable to infer affect
measurements obtained from a face-based affect recognition
system using a BCI. In the inverse case, inferring the BCI
values using the values from the face-based affect
recognition system generates models with low correlation;
therefore, they are not reliable.

The correlation values are not good for the models in
which BCI-based inferred affects are predicted by the facebased inferred affects. But the reverse task, predicting facebased infered affect by BCI-based infered affect, provides
reasonable correlational values. We expect this lack of
symmetry because face-based detections are dependent
outcomes of a person in some mental state, while the reverse
is not true. We do not expect facial expressions to
significantly drive the internal mental state of the person
being studied; therefore, low correlations do not surprise us.
A low correlation implies a poor model; analysis over that
model is not valuable and may mislead. Consequently, we
realize an analysis in terms of interpretability only for the
models built for prediction of face-based inferred affects
using BCI values as follows:
1) Agreement is inferred with excitement as the variable
with most importance (factor>0.90) followed by
engagement, frustration, and meditation (factor>0.75).
Medium or high values of excitement increase the
assumption of agreement, and meditation levels appear to
proportionally affect the agreement level.
2) Concentrating is inferred with excitement and
meditation as the variables with most importance
(factor>0.90) followed by engagement and frustration
(factor>0.80). High values of engagement and low
excitement are related to concentration. Frustation and
meditation do not show consistant dependence.
3) Disagreement is inferred with excitement as the
variable with most importance (factor>0.90) followed by
meditation, engagement, and frustration (factor>0.80). Low
values of excitement and frustration but high engagement
and meditation are related to disagreement.
4) Interested is inferred with excitement as the variable
with most importance (factor>0.90) followed by

ACKNOWLEDGMENT
This research was supported by the Office of Naval
Research under Grant N00014-10-1-0143 awarded to Dr.
Robert Atkinson.
REFERENCES
[1] I. Arroyo, D. G. Cooper, W. Burleson, B. P. Woolf, K.
Muldner, and R. M. Christopherson, “Emotion Sensors Go To
School,” presented at the Proceedings of the 2009 conference
on Artificial Intelligence in Education: Building Learning
Systems that Care: From Knowledge Representation to
Affective Modeling, Amsterdam, The Netherlands, The
Netherlands, 2009, pp. 17–24.
[2] D. G. Cooper, I. Arroyo, B. P. Woolf, K. Muldner, W.
Burleson, and R. M. Christopherson, “Sensors Model Student
Self Concept in the Classroom,” presented at the Proceedings
of the 17th International Conference on User Modeling,
Adaptation, and Personalization, Berlin, Heidelberg, 2009, pp.
30–41.
[3] P. Michel and R. El Kaliouby, “Real time facial expression
recognition in video using support vector machines,”
presented at the Proceedings of the 5th international
conference on Multimodal interfaces, 2003, pp. 258–264.
[4] D. S. Tan and A. Nijholt, Brain-Computer Interfaces:
Applying Our Minds to Human-Computer Interaction.
Springer London, Limited, 2010.
[5] Emotiv EPOC Headset. [Online]. Available:
http://www.emotiv.com. [Accessed: 10-Apr-2013].
[6] P. G. Schrader and M. McCreery, “The Acquisition of Skill
and Expertise in Massively Multiplayer Online Games,”
Educational Technology Research and Development, vol. 56,
no. 5, pp. 557–574, 2007.
[7] Guitar Hero. [Online]. Available:
http://www.guitarhero.com. [Accessed: 10-Apr-2013].
[8] A. Liaw and M. Wiener, “Classification and Regression by
randomForest,” R NEWS, vol. 2, no. 3, pp. 18–22, 2002.

71

The 2014 International Conference on Collaboration Technologies and Systems (CTS 2014)
May 19 - 23, 2014, Minneapolis, Minnesota, USA

CTS 2014 TUTORIALS

TUTORIAL I

Introduction to Social Media Network Analysis with NODEXL
Marc A. Smith
Social Media Research Foundation, California, USA
Email: marc@smrfoundation.org
TUTORIAL DESCRIPTION
This tutorial provides an overview of social network analysis (SNA) and demonstrates through theory and
practical case studies its application to research on Weblogs and Social Media. This topic has grown in
importance with the increasing popularity of social networking websites in particular (e.g. Twitter, YouTube,
Facebook, LinkedIn etc.) and social computing in general. As people increasingly participate in online
communities for social, commercial, and civic interaction, new methods are needed to study these phenomena.
SNA is a valuable contribution to social media research providing a language and measures for sets of complex
relationships created from patterns of online communication.
Social network theory conceptualizes networks as a group of actors who are connected by a set of relationships. Actors
are often people, but can also be nations, organizations, objects, etc. Social network analysis focuses on patterns of
relations among actors that include humans. It seeks to describe networks of relations as fully as possible. This includes
identifying prominent patterns in networks, tracing the flow of information through them, and discovering what effects
these relations and networks have on people and organizations. It can therefore be used to study network patterns of
organizations, ideas, and people that are connected via various means in an online environment.
The tutorial provides a guide to research into collaborative systems by representing all collaboration as a stream
of connections or edges. The NodeXL system simplifies the process of collecting, storing, analyzing, visualizing,
and publishing insights into connected structures.

TUTORIAL OUTLINE
In this tutorial, we cover the following topics:
•
•
•
•
•

Introduction into the components and characteristics of social networks
Information about relational data that is used for SNA along with the different ways that the data can be
presented
Presentation of different measurements of network characteristics within SNA
Discussion of different approaches towards SNA
Practical applications of SNA to “live” datasets:
•
Demonstration and discussion of popular SNA tools including the free and open NodeXL and Gephi
•
Different forms of data analysis with SNA

•
•
•

The application of SNA to large hyperlink data analysis
SNA as a method to study different CMC and CSCW settings
SNA in combinations with other research methods

REFERENCES
01. E. M. Rodrigues, N. Milic-Frayling, M. Smith, B. Shneiderman, D. Hansen. Group-in-a-box Layout for
Multi-faceted Analysis of Communities. IEEE Third International Conference on Social Computing, October
9-11, 2011.
02. D. Hansen, M. Smith, B. Shneiderman. EventGraphs: charting collections of conference connections. FortyForth Annual Hawaii International Conference on System Sciences (HICSS). January 4-7, 2011. Kauai, Hawaii.
03. H. T. Welser, E. Gleave, D. Fisher, M. Smith. Visualizing the Signatures of Social Roles in Online Discussion
Groups. Journal of Social Structure, vol. 8, 2007.
04. I. Himelboim, E. Gleave, M. Smith. Discussion catalysts in online political discussions: Content importers
and conversation starters. Journal of Computer-Mediated Communication, vol. 14, 2009
05. Smith, M., Shneiderman, B., Milic-Frayling, N., Rodrigues, E.M., Barash, V., Dunne, C., Capone, T., Perer,
A. & Gleave, E. Analyzing (Social Media) Networks with NodeXL. Proceedings of the Fourth International
Conference on Communities and Technologies, 2009.
06. H. Welser, E. Gleave, M. Smith, V. Barash, J. Meckes. Whither the experts: Social affordances and the
cultivation of experts in community Q&A systems. Proceedings of the International Symposium on Social
Intelligence and Networking, 2009.
07. E. M. Bonsignore, C. Dunne, D. Rotman, M. Smith, T. Capone, D. L. Hansen, B. Shneiderman. First steps to
NetViz Nirvana: evaluating social network analysis with NodeXL. Proceedings of the International
Symposium on Social Intelligence and Networking, 2009.
08. D. Hansen, D. Rotman, E. Bonsignore, N. Milic-Frayling, E. Rodrigues, M. Smith, B. Shneiderman. Do You
Know the Way to SNA?: A Process Model for Analyzing and Visualizing Social Media Data. University of
Maryland Tech Report: HCIL-2009-17, 2009, http://hcil.cs.umd.edu/trs/2009-17/2009-17.pdf
09. Group-in-a-box: http://www.connectedaction.net/2011/10/10/october-9-11-2011-ieee-2011-social-computingboston-nodexl-paper-on-group-in-a-box-layouts/.
10. EventGraphs: http://www.cs.umd.edu/localphp/hcil/tech-reports-search.php?number=2010-13.
11. Visualizing Signatures: http://www.cmu.edu/joss/content/articles/volume8/Welser/
12. Discussion catalysts: http://jcmc.indiana.edu/ at http://ping.fm/7NF5T
13. Analyzing (social media) networks: http://www.connectedaction.net/wp-content/uploads/2009/08/2009-CTNodeXL-and-Social-Queries-a-social-media-network-analysis-toolkit.pdf.
14. Whither the Experts: http://www.connectedaction.net/wp-content/uploads/2009/08/2009-Social-ComputingWhither-the-Experts.pdf.
15. NetViz Nirvana: http://www.cs.umd.edu/~cdunne/pubs/Bonsignore09Firststepsto.pdf.
16. The way to SNA: http://hcil.cs.umd.edu/trs/2009-17/2009-17.pdf.
17. http://www.pewinternet.org/2014/02/20/mapping-twitter-topic-networks-from-polarized-crowds-tocommunity-clusters/.

REQUIREMENTS AND TARGET AUDIENCE
We welcome practitioners and academics interested in computer-mediated communication, universal design, and
social software. No background knowledge about Social Network Analysis or statistics is required.

TUTORIAL DURATION
The total duration of the course will be 6-7 hours. It consists of one theoretical unit that will introduce the attendees
to the method and characteristics of SNA, and one practical unit that will demonstrate tools and address possible
application areas in presenting case studies. It is advised that participants take both course blocks unless they have a
strong background in social network concepts and seek details only on the particulars of the NodeXL application.

A/V AND EQUIPMENT
Projector & screen. hands-on sessions and discussion will be held in order to encourage interaction. As SNA is a
flexible method that can be applied in different ways, we will create space in the course for discussing and
elaborating on the possibilities of SNA to the research or working areas of the participants.

INSTRUCTOR BIOGRAPHY
Marc Smith is a sociologist specializing in the social organization of online communities
and computer mediated interaction. Smith received a B.S. in International Area
Studies from Drexel University in Philadelphia in 1988, an M.Phil. in social
theory from Cambridge University in 1990, and a Ph.D. in Sociology from UCLA in 2001.
He is an adjunct lecturer at the College of Information Studies at the University of
Maryland. Smith is also a Distinguished Visiting Scholar at the Media-X
Program at Stanford University. Smith leads the Connected Action consulting group and
lives and works in Silicon Valley, California. Smith co-founded the Social Media Research
Foundation (http://www.smrfoundation.org/), a non-profit devoted to open tools, data, and scholarship related to
social media research. Smith is the co-editor of Communities in Cyberspace (Routledge), a collection of essays
exploring the ways identity; interaction and social order develop in online groups. Along with Derek Hansen and
Ben Shneiderman, he is the co-author and editor of Analyzing Social Media Networks with NodeXL: Insights
from a connected world, from Morgan-Kaufmann which is a guide to mapping connections created through
computer-mediated interactions.

The 2014 International Conference on Collaboration Technologies and Systems (CTS 2014)
May 19 - 23, 2014, Minneapolis, Minnesota, USA

TUTORIAL II

Cloud Based Federated Infrastructure for
Big Data e-Science and Collaboration
Yuri Demchenko
University of Amsterdam, The Netherlands
Email: y.demchenko@uva.nl

TUTORIAL DESCRIPTION
The tutorial will start with an overview of modern e-Science use cases that deal with Big Data and require special
Scientific Data Infrastructure (SDI) to support both managing large amount of scientific data and support effective
collaboration of distributed groups of researches worldwide. The analysis of use cases of large scientific projects
such LHC experiment, Human Genome research, or astronomical research will help formulating requirements to
collaborative infrastructure and how they can be implemented using Cloud Computing technologies.
The tutorial will outline the general cloud aware services design principles and how they can be implemented with
major cloud platforms such as Amazon AWS and Microsoft Azure.
Special attention will be given to emerging Intercloud federation models to address such issues as federated
collaborative cloud resources usage, federated access control, and identity management.

TUTORIAL OUTLINE
The tutorial will cover the following topics:
•
•
•
•
•
•
•

Big Data challenges and use cases in modern data driven e-Science, involving collaboration of large
research communities;
Cloud based Scientific Data Infrastructure architecture;
Cloud aware and cloud enabled infrastructure services design with Amazon AWS and Microsoft Azure;
Intercloud Federation models and design patterns;
Federated collaborative cloud resources usage, including data federation;
Federated access control and identity management for collaborative groups;
Experience of federated infrastructure management by EGI (European Grid Initiative) and GEANT
research communities in Europe.

REFERENCES
1. Grey, J., The Fourth Paradigm: Data-Intensive Scientific Discovery, edited by Tony Hey, Stewart Tansley,
and Kristin Tolle.
Microsoft Corporation, October 2009. ISBN 978-0-9825442-0-4 [online]
http://research.microsoft.com/en-us/collaboration/fourthparadigm/.

2. Riding the wave: How Europe can gain from the rising tide of scientific data. Final report of the High Level
Expert Group on Scientific Data. October 2010. [online] Available at http://cordis.europa.eu/fp7/ict/einfrastructure/docs/hlg-sdi-report.pdf.
3. Demchenko, Yuri, Peter Membrey, Cees de Laat, Defining Architecture Components of the Big Data
Ecosystem, Second International Symposium on Big Data and Data Analytics in Collaboration (BDDAC
2014). Proc. The 2014 International Conference on Collaboration Technologies and Systems (CTS 2014),
May 19-23, 2014, Minneapolis, USA. (This proceedings)
4. Demchenko, Y., C. Lee, C.Ngo, C. de Laat, Federated Access Control in Heterogeneous Intercloud
Environment: Basic Models and Architecture Patterns. IEEE Third International Workshop on Cloud
Computing Interclouds, Multiclouds, Federations, and Interoperability (Intercloud 2014), In Proc IEEE
International Conference on Cloud Engineering (IC2E), March 11, 2014, Boston, USA.
5. Intercloud Architecture Framework for Interoperability and Integration, Release 2, Draft Version 0.8. SNE
Technical Report 2012-03-02, 17 April 2014. [online] http://www.uazone.org/demch/worksinprogress/sne2012techreport-12-05-intercloud-architecture-draft08.pdf.

REQUIREMENTS AND TARGET AUDIENCE
No specific preconditions are requested. The expected target audience is wide but primarily oriented on
researchers and scientific applications developers that consider moving their services to clouds. There is no
specific prerequisite knowledge on Cloud Computing required.

TUTORIAL DURATION
The tutorial material will be presented in a 2 to 3-hour session.

A/V AND EQUIPMENT
No specific request, beyond a projector and, possibly, an internet connection.

INSTRUCTOR BIOGRAPHY AND PHOTO
Yuri Demchenko is a Senior Researcher at the System and Network Engineering of the
University of Amsterdam, The Netherlands. He graduated from the National Technical
University of Ukraine “Kiev Polytechnic Institute” where he also received his PhD (Cand.
of Science) degree. His main research areas include Big Data Intensive Science
Technologies and Infrastructure, Cloud and Intercloud Architecture, general security
architectures and distributed access control infrastructure for cloud based services and data
centric applications. He is currently involved in the European projects GN3plus and
EUBrazil where he conducts research and developments on the cloud federation
infrastructure and cloud based scientific infrastructures.
He is actively contributing to the standardisation activity at RDA, OGF, IETF, NIST on defining Big Data
Architecture Framework, Intercloud architecture for complex infrastructure services provisioning in clouds.

The 2014 International Conference on Collaboration Technologies and Systems (CTS 2014)
May 19 - 23, 2014, Minneapolis, Minnesota, USA

TUTORIAL III

Multimodal Detection of Affective States During Collaboration:
Overview of Methods and Applications
Robert K. Atkinson
Arizona State University, Arizona, USA
Email: Robert.Atkinson@asu.edu
TUTORIAL DESCRIPTION
One important way for systems to adapt to their users is related to their ability to show empathy. Being
empathetic implies that the computer is able to recognize users’ affective states and understand the implication of
those states. Detection of affective states is a step forward to provide machines with the necessary intelligence to
appropriately interact with humans. This course provides a description and demonstration of tools and
methodologies for automatically detecting affective states during collaboration with a multimodal approach.

TUTORIAL OUTLINE
We cover the following topics:
•
•
•
•
•
•

Description of inexpensive, easy to install, and widely available sensing devices for affect detection
Discussion of the latest developments in mobile eye tracking
A description of off-the-shelf software for integrating sensor data (e.g., time synching data from different
sensors)
Discussion of techniques and methodologies used to filter and integrate information from the different
sources
Quantitative approaches for data analysis
An example application with a commercial game

REFERENCES
•
•
•
•

Emotiv | EEG System. http://www.emotiv.com.
Gonzalez-Sanchez, J., Chavez-Echeagaray, M.E., Atkinson, R. and Burleson, W. ABE: An Agent-Based
Software Architecture for a Multimodal Emotion Recognition Framework. In Proc. WICSA 2011, IEEE
Press (2011) 187-193.
Hall, M., Frank, E., Holmes, G., Pfahringer, B., Reutemann, P. and Witten, I.H. The WEKA Data Mining
Software: An Update. In Proc. SIGKDD Explorations, (2009) vol. 11, Issue 1. 10-18.
Tobii Technology. http://www.tobii.com.

REQUIREMENTS AND TARGET AUDIENCE
No prerequisites.

TUTORIAL DURATION
The tutorial material will be presented in a 2 to 3-hour session.

A/V AND EQUIPMENT
LCD projector.

INSTRUCTOR BIOGRAPHY AND PHOTO
Robert Atkinson, an Associate Professor with a joint appointment in Division of
Educational Leadership and Innovation in the Mary Lou Fulton Teacher’s College at ASU
and CIDSE in the Ira A. Schools of Engineering, will serve as the project PI. He earned in
Applied Cognitive Science PhD degree from University of Wisconsin - Madison with a
minor in statistics and research design. He has obtained—both independently and
collaboratively—over $25 million dollars in grant support from a variety of sources
including the National Science Foundation, Office of Naval Research (ONR), Pearson
PLC, and the Intel Corporation. His research appears in a variety of highly respected
academic journals including Journal of Educational Psychology, Applied Cognitive
Psychology, Learning and Instruction, Review of Educational Research and Educational
Psychologist. He directs ASU’s Advancing Next Generation Learning Environments
(ANGLE) laboratory. He is currently the PI on ONR project that involves multimodal detection of affective states
titled “Incorporating affective-based dynamic difficulty adjustment into personalized digital environments: A
generalizable framework for closed-loop systems.”

The 2014 International Conference on Collaboration Technologies and Systems (CTS 2014)
May 19 - 23, 2014, Minneapolis, Minnesota, USA

TUTORIAL IV

Big Data Processing with Less Work and Less Code
Parsing Semi-Structured and Free Form Text
Richard Taylor
Chief Trainer, HPCC Systems
LexisNexis, Georgia, USA
Email: richard.taylor@lexisnexis.com

TUTORIAL DESCRIPTION
Attendees will experience hands-on use of ECL’s natural language parsing techniques on an HPCC Systems
cluster to extract entities from both semi-structured and free-form text data. This represents a combined text
extraction task with human intervention. Participants will leave with all the data and code used in the class along
with the latest HPCC Client Tools installation, HPCC documentation, and HPCC’s VMware installation. Raffle
prize of free training is included.

TUTORIAL OUTLINE
We cover the following topics:
• HPCC Systems Platform Overview
• Parsing Semi-Structured Data
• Parsing Unstructured Data
• Post-Processing Parse Results

REFERENCES
HPCC Systems site, http://hpccsystems.com;
Similar tutorial presented at O’Reilly Strata Conference February 28, 2012.

REQUIREMENTS AND TARGET AUDIENCE
This tutorial is geared towards developers. Anyone who is tasked with or has an interest in entity resolution as it
pertains to big data is encouraged to attend. Participants are welcome to bring their own laptops to take away the
code and examples from the class.

TUTORIAL DURATION
The tutorial material will be presented in a 2 to 3-hour session.

A/V AND EQUIPMENT
Projector, PA, and Internet access (wired or WiFi).

INSTRUCTOR BIOGRAPHY AND PHOTO
Richard Taylor has worked with the HPCC technology platform and the ECL programming
language since its inception. He developed all the ECL programming courses and has taught
ECL from its beginning. He is the author of the ECL programming documentation: Language
Reference, Programmer’s Guide, and Standard Library Reference.

The 2014 International Conference on Collaboration Technologies and Systems (CTS 2014)
May 19 - 23, 2014, Minneapolis, Minnesota, USA

TUTORIAL V

Design Synthesis – A Process for Crafting Meaningful
Products, Systems, & Services
Matt Franks
Austin Center for Design
Texas, USA
Email: mfranks@austincenterfordesign.com

TUTORIAL DESCRIPTION
Synthesis is an abductive sense-making process; it’s an often collaborative, external approach to making sense of
seemingly unrelated data points. The methods of synthesis are highly leveraged in the user centered design
process as a means to “find solutions” to ill-defined or wicked problems.
This tutorial introduces participants to the primary methods of synthesis, and their application within the context
of the design process. Participants will discover a means to understand the world around them and craft
meaningful product, system, or service directions (often referred to as Design Thinking).
Participants will utilize a set of unorganized data points from research conducted with two and four year college
students. They will utilize affinity diagramming to make sense of these data points and craft meaningful insights
into the academic journey.
Participants will then use insight combination to produce product, system, and service ideas that support college
students throughout their academic journey.
Finally, the group will review an example of how the same research was used to create the product direction and
strategic roadmap for myEdu – an ed-tech startup that was recently acquired by Blackboard.

TUTORIAL OUTLINE
We cover the following topics:
•
•
•
•
•

The “Design Thinking” Process;
Synthesis: What it is and how to do it;
Activity – Affinity Diagramming;
Activity - Insight Combination;
Example – The Academic Journey.

REQUIREMENTS AND TARGET AUDIENCE
Participants can come from a wide range of background, including managers, system designers, usability and
human factors engineers, technical communications specialists and programmers. There are no prerequisites for
participation.
This tutorial is targeted toward individuals interested in the user centered design process (Design Thinking),
individuals who’s roles and responsibilities fall within the scope of addressing complex problems, or those
looking to shift the mindset of their organization away from short-term thinking or “incremental innovation”.

TUTORIAL DURATION
The tutorial material will be presented in a 3-hour session. The tutorial will include 2 – 50 minute activities.
Participants will be split into groups of 4 – 5 participants.

A/V AND EQUIPMENT
Presenter: Projector and screen. For each Group: round tables for group work, 8.5 x 11 Paper, Sharpie Markers
(Black), Packs of post-its (Yellow, Pink, & Green), 1 pair of scissors.

INSTRUCTOR BIOGRAPHY AND PHOTO
Matt Franks is the lead interaction designer at Blackboard & a professor at the Austin
Center for Design, Texas. Prior to working at blackboard, he was the lead interaction
designer @ myEdu (a startup acquired by blackboard), a Senior Interaction Designer at frog
design and a designer for Target Corporation. His work ranges from mobile systems for
both handsets and tablets, physical products like Target’s ClearRx prescription bottle, and
entertainment experiences for TV, web, and video. In the past four years, he has released
over 400 products and services into the market while maintaining an active role as an
educator. Matt spends his free time exploring the intersection of digital and physical objects. An avid believer in
the MAKE movement, he is continually experimenting with intelligent objects that challenge people’s
understanding of technology. Matt earned his BFA in Industrial Design, with a focus on Interaction Design, from
the Savannah College of Art and Design.

Computers & Education 67 (2013) 36–50

Contents lists available at SciVerse ScienceDirect

Computers & Education
journal homepage: www.elsevier.com/locate/compedu

Investigating the impact of pedagogical agent gender matching and
learner choice on learning outcomes and perceptions
Gamze Ozogul a, Amy M. Johnson a, Robert K. Atkinson b, *, Martin Reisslein a
a
b

School of Electrical, Computer, and Energy Engineering, Arizona State University, Tempe, AZ 85287, USA
School of Computing, Informatics, and Decision Systems Engineering and Division of Educational Leadership, Arizona State University, Tempe, AZ 85287, USA

a r t i c l e i n f o

a b s t r a c t

Article history:
Received 29 August 2012
Received in revised form
7 December 2012
Accepted 16 February 2013

The similarity attraction hypothesis posits that humans are drawn toward others who behave and appear
similar to themselves. Two experiments examined this hypothesis with middle-school students learning
electrical circuit analysis in a computer-based environment with an Animated Pedagogical Agent (APA).
Experiment 1 was designed to determine whether matching the gender of the APA to the student has a
positive impact on learning outcomes or student perceptions. One hundred ninety-seven middle-school
students learned with the computer-based environment using an APA that matched their gender or one
which was opposite in gender. Female students reported higher program ratings when the APA matched
their gender. Male students, on the other hand, reported higher program ratings than females when the
APA did not match their gender. Experiment 2 systematically tested the impact of providing learners the
choice among four APAs on learning outcomes and student perceptions. Three hundred thirty-four
middle-school students received either a pre-assigned random APA or were free to choose from four
APA options: young male agent, older male agent, young female agent, or older female agent. Learners
had higher far transfer scores when provided a choice of animated agent, but student perceptions were
not impacted by having the ability to make this choice. We suggest that offering students learner control
positively impacts student motivation and learning by increasing student perceptions of autonomy, responsibility for the success of the instructional materials, and global satisfaction with the design of
materials.
Ó 2013 Elsevier Ltd. All rights reserved.

Keywords:
Gender studies
Human-computer interface
Interactive learning environments
Multimedia/hypermedia systems

1. Introduction
Multimedia learning environments are well-known to promote student learning (Mayer, 2005; Mayer, 2008). In such environments,
verbal descriptions are presented either through narration or written text and are combined with visual depictions such as diagrams, tables,
graphs, animations, or videos. A well-established line of research demonstrates that students learn better from words and graphics than from
words alone (Mayer, 1989; Mayer, 2008; Moreno & Mayer, 1999). Such multimedia environments sometimes employ animated pedagogical
agents to facilitate learning from multiple representations (e.g., text, diagrams, and equations) of information (Atkinson, 2002; Craig,
Gholson, & Driscoll, 2002; Moreno, Mayer, Spires, & Lester, 2001; Moreno, Reisslein, & Ozogul, 2010; Ozogul, Reisslein, & Johnson, 2011).
Animated pedagogical agents (APAs) are humanlike or cartoon animated characters which are displayed within a computer-based
learning environment to provide learners with pedagogical assistance (Bradshaw, 1997; Choi & Clark, 2006; Woo, 2009). APAs have the
potential to increase learner engagement and the instructional methods they employ can increase learning (Baylor, 2009; Choi & Clark,
2006; Moreno, 2005). Moreno (2005) proposed that APAs have both internal and external properties which inﬂuence student learning.
The internal properties of APAs concern the instructional methods used by the agent in facilitating learning. Instructional methods applied
through APAs include directing learner attention through gestures (Moreno, 2004; Moreno et al., 2010) and delivering feedback messages,
verbal guidance, and modeling (Azevedo et al., 2009; Graesser et al., 2004; Moreno et al., 2001). External properties of APAs relate to the
image and voice of the agent, and include such agent characteristics as gender, age, ethnicity, and tone of voice. In the current investigation,

* Corresponding author. Arizona State University, School of Computing, Informatics, and Decision Systems Engineering, P.O. Box 878809, Tempe, AZ 85287-8809, USA. Tel.:
þ1 480 727 7765.
E-mail address: robert.atkinson@asu.edu (R.K. Atkinson).
0360-1315/$ – see front matter Ó 2013 Elsevier Ltd. All rights reserved.
http://dx.doi.org/10.1016/j.compedu.2013.02.006

G. Ozogul et al. / Computers & Education 67 (2013) 36–50

37

the multimedia environment is the setting in which the animated pedagogical agent is used to facilitate instruction via narrated instruction
and through signaling of relevant visual information using hand gestures. These internal characteristics are invariant across the versions of
multimedia instruction. The reported experiments were conducted to examine the potential effects of external properties of the animated
agent (i.e., age and gender) on learning and learner perceptions.
Although it may seem that external characteristics, such as agent gender or age, would have trivial consequences for learning or affect,
some research suggests that these properties can play important roles (Ozogul et al., 2011; van Vugt, Bailenson, Hoorn, & Konijn, 2010).
According to the similarity attraction hypothesis, humans are more attracted to others who appear and behave similarly to themselves
(Byrne & Nelson, 1965). It has been suggested that this similarity attraction hypothesis may be applicable in computer-based learning
environments, since computer users attribute social presence to computers (Moreno & Flowerday, 2006; Reeves & Nass, 1996). The following
sections describe the use of APAs in multimedia environments and present relevant empirical background on APAs in multimedia.
1.1. APAs in multimedia
APAs are used in computer-based learning environments to provide learners with pedagogical assistance using one or more instructional
methods, such as directing attention to relevant information, providing feedback messages, or delivering direct instruction (Dehn & van
Mulken, 2000; Heidig & Clarebot, 2011; Moreno, 2005). Such instructional methods are intended to keep students focused on essential
information and to provide context-speciﬁc learning strategies (Clark & Choi, 2005). Apart from the didactic objectives of APAs, they are also
assumed to play motivational roles. By establishing a social interaction between learner and agent, APAs may maintain learners’ engagement in a learning task, ultimately promoting learning outcomes (Baylor, 2011; Kim & Baylor, 2006; Moreno et al., 2001; Ryu & Baylor, 2005).
According to the persona hypothesis, the visual presence of an APA in computer-based learning environments can increase learning outcomes and positively affect learners’ perceptions of the learning experience (Cassell, Sullivan, Prevost, & Churchill, 2000; Lester et al., 1997;
Mitrovic & Suraweera, 2000). The following section describes results from empirical work aimed at testing the persona hypothesis.
1.1.1. Persona hypothesis
Lester et al. (1997) presented learners with ﬁve different versions of a microworld centered on botany, each with a visually represented
pedagogical agent “Herman the Bug”. The different versions of the environment varied in the communicative behaviors used by the
pedagogical agent. The authors found that all conditions led to higher scores at posttest, compared to pretest, and concluded that their
ﬁndings supported a persona effect; that is, the visual presence of the animated agent led to increased student motivation and learning
outcomes. This study has been criticized for not including a control group without the visual presence of the agent (Dehn & van Mulken,
2000; Heidig & Clarebot, 2011). In fact, few experimental studies have compared an APA condition to one using identical instruction
without the visual presence of an agent. Heidig and Clarebot (2011) conducted a review of literature on APAs and found 15 experimental
investigations which included an appropriate control condition. Nine of the 15 studies found no signiﬁcant difference in learning between
an APA condition and control. However, Atkinson (2002, experiment 2) found better learning outcomes from an APA condition, compared to
text only or voice only conditions. Also, Moreno et al. (2010) found that an APA providing visual signaling within multiple representations
led to better posttest scores and program ratings than arrow signaling or a control condition without such signaling.
In summary, results are not conclusive and debate continues concerning the assumption that the visual presence of an agent increases
motivation or facilitates learning. Heidig and Clarebot (2011) suggest that a more appropriate research goal would be to determine under
what conditions an APA can be helpful. The following section reviews research exploring the effect of agent gender on learners’ perceptions
or learning outcomes.
1.1.2. Agent gender studies
Arroyo, Woolf, Royer, and Tai (2009) explored the effect of different gendered learning companions on students’ attitudes about math,
students’ emotions during learning, and learning outcomes. The authors showed that female high school and undergraduate students had
better learning outcomes and more positive attitudes about math after learning with the male learning companion, compared to the female
learning companion. Learners’ open-ended responses did not suggest that the female students liked the male agent better. The authors
suggest that gender stereotypes about mathematics transfer to the computer environment and the female students thus regard the male
companion’s information as more credible. Similar ﬁndings were obtained with undergraduate students learning about blood pressure
(Moreno, Klettke, Nibbaragandla, & Graesser, 2002); learning outcomes were higher with male agents than female agents, and stereotyping
scales provided some evidence that participants applied gender stereotypes to the animated agents.
Baylor and Kim (2003) investigated the effect of student gender, student ethnicity, agent gender, and agent ethnicity on learning and
learner perceptions of pre-service teachers learning about instructional design. Their results indicated that, overall, learners rated male
agents as more extroverted. Although learners reported greater satisfaction in their performance and more use of self-regulation after
learning with a male agent, learning did not differ between male and female agent conditions. Kim, Baylor, and Shen (2007) had mixed
results from two experiments with undergraduate students learning about instructional design. The ﬁrst experiment used computer literacy
students and demonstrated better recall from a male agent than a female agent; the second experiment did not replicate this ﬁnding with
pre-service teachers.
Plant, Baylor, Doerr, and Rosenberg-Kima (2009) found that a female agent led to better math posttest scores, higher ratings of engineering utility, interest and self-efﬁcacy than a control (no agent) condition, whereas the male agent only led to increased self-efﬁcacy
compared to the control condition. Furthermore, math scores were higher in the female agent condition compared to the male agent
condition. The authors suggest that these middle-school students have many experiences with female teachers, and thus view them as
credible sources of information. Their interpretation may explain why this study stands out from other work demonstrating better outcomes
with male agents.
To summarize, the results from several previous studies have shown that male agents often lead to better learning outcomes (Arroyo
et al., 2009; Kim et al., 2007; exp. 1; Moreno et al., 2002) and more positive evaluations of the learning experience (Arroyo et al., 2009;
Baylor & Kim, 2003; Moreno et al., 2002) in math and technical domains than female agents; conversely, Plant et al. (2009) found better

38

G. Ozogul et al. / Computers & Education 67 (2013) 36–50

learning outcomes when using female agents for middle-school students. A related research question is whether matching the gender of the
agent to the learner can result in better learning or learner perceptions. The following section presents empirical research aimed at
investigating the effect of agent similarity to learner, including gender matching.
1.1.3. Agent similarity hypothesis
Because humans often treat computers as social entities (Reeves & Nass, 1996), social accounts of interaction such as the similarity
attraction hypothesis may be relevant to computer-based environments. The similarity attraction hypothesis in the context of learning with
animated pedagogical agents would predict increased learning and more positive perceptions the greater the similarity between the learner
and the agent. Previous research has explored agent similarity effects with regard to agent gender (Baylor & Kim, 2003; Behrend &
Thompson, 2011; Lee, Liao, & Ryu, 2007; Moreno & Flowerday, 2006; Plant et al., 2009; Rosenberg-Kima, Plant, Doerr, & Baylor, 2010;
Van der Meij, Van der Meij, & Harmsen, 2012), age (Rosenberg-Kima, Baylor, Plant, & Doerr, 2008) ethnicity (Baylor & Kim, 2003; Behrend &
Thompson, 2011; Moreno & Flowerday, 2006; Pratt, Hauser, Ugray, & Patterson, 2007; Rosenberg-Kima et al., 2010), personality (Isbister &
Nass, 2000; Moon & Nass, 1998; Nass & Lee, 2001), physical appearance (Rosenberg-Kima et al., 2008; van Vugt et al., 2010), and feedback
style (Behrend & Thompson, 2011).
Moreno and Flowerday (2006) randomly assigned learners to a choice condition, in which learners selected an agent from 10 options,
differing in gender and ethnicity, or a non-choice condition, in which learners were assigned to an agent. Results ﬁrst indicated that overall
learners did not select an agent that matched their gender or ethnicity more often, but students of color were more likely to select an agent
with the same ethnicity than their Caucasian counterparts. Next, the results did not indicate positive effects of gender similarity or ethnicity
similarity on retention, transfer, or program ratings. Furthermore, the students who were able to choose had lower transfer scores, lower
retention scores, and lower program ratings when the agent matched their ethnicity.
Behrend and Thompson (2011) did not ﬁnd positive effects of gender similarity and surprisingly found a negative effect of ethnic
similarity on utility ratings of the agent. However, these two effects were shown to be additive for engagement; the highest engagement
ratings were obtained in the group where both gender and ethnicity was matched to the learner. Learning outcomes were not signiﬁcantly
inﬂuenced by gender or ethnicity similarity. Baylor and Kim (2003) found that Caucasian students rated Caucasian agents as more engaging
and affable, whereas African American students rated these characteristics higher for African American agents. The researchers did not ﬁnd
better learning, self-reported self-regulation or self-reported satisfaction for agents who matched the learners in gender or ethnicity.
Rosenberg-Kima et al. (2008; Experiment 2) explored participant perceptions of engineering (self-efﬁcacy, interest, stereotypes, and
utility) after learning with one of eight agents differing on three factors (age, gender, and ‘coolness’). The authors expected that participant
perceptions would be most impacted after viewing an agent they considered similar or aspired to (i.e., young and ‘cool’). Results supported
this hypothesis; the two conditions (male and female) with young and ‘cool’ agents led to higher self-efﬁcacy and interest than the
remaining six conditions. Lee et al. (2007) explored gender similarity using a computerized voice only. The authors showed that male
participants rated a male agent’s voice more likeable than a female agent, whereas no difference in voice likeability was found for female
participants. A similar pattern was found in participants’ ratings of voice credibility, content quality, and self-conﬁdence in the topic discussed (e.g., skin care and makeup or dinosaurs). Learning outcomes were not measured by Rosenberg-Kima et al. (2008) or Lee et al. (2007).
The results from these studies do not provide evidence for a positive impact of gender matching on learning outcomes. However, there is
some support for the similarity attraction hypothesis on perceptions of the computer programs. Little prior investigation has been conducted on the agent similarity attraction hypothesis using younger, middle-school aged students (cf. Lee et al., 2007). Experiment 1 in this
study was conducted to investigate the effect of matching gender to middle-school students on learning outcomes and learner perceptions.
The next section presents empirical research aimed at investigating the effect of providing choice of APA to learners.
1.1.4. Agent choice
Providing learners with choice may elevate feelings of autonomy, leading to greater motivation and self-efﬁcacy in the task (Bandura,
2001; Ryan & Deci, 2000). However, there is little empirical evidence on the effect of agent (or APA) choice on learning or learning perceptions. Moreno and Flowerday (2006) did not ﬁnd a beneﬁcial effect of agent choice on learning, and in fact, when provided a choice of
APA, learners who chose ethnically similar agents had lower learning outcomes than those who chose ethnically dissimilar agents. The
authors conclude that the students who chose ethnically similar agents focused attention on the agent, rather than the instructional materials, diverting cognitive resources to the APA (Moreno & Flowerday, 2006). Kim and Wei (2011) also did not ﬁnd any positive impact of
agent choice on learning. Their results indicated that male students had more positive attitudes toward mathematics and higher feelings of
self-efﬁcacy after learning with an agent of their choice, whereas female students had more positive attitudes and higher self-efﬁcacy after
being assigned an agent randomly. Behrend and Thompson (2012) found increased self-efﬁcacy with learner choice of the agent appearance.
These prior investigations did not provide evidence for a positive impact of learner choice of APA on immediate learning outcomes from a
computer-based learning environment. Further, prior results are mixed concerning the effect of choice on learner perceptions of the
learning environment and domain. Experiment 2 was conducted to investigate the effect of providing choice of APA on learning outcomes
and learner perceptions with middle-school students. The next section describes a preliminary study conducted to investigate students’
self-reported preferences for APAs and characteristics of APAs.
2. Preliminary study
2.1. Method
A preliminary study was conducted to determine which image of an animated agent appeals to middle-school students and what
external and internal properties of agents the students prefer. Participants were 77 middle-school students (54.5% female) at a public middle
school in the Southwestern U.S. with the mean age of 12.83 years (SD ¼ 0.84). The students completed a survey with pictures of three agents:
an old male, dressed in clothing that resembled a teacher’s, and a young female and young male, both approximately the same age as the
participants and dressed in casual attire similar to the middle schoolers’ (see Fig. 1). The survey posed several questions about agent

G. Ozogul et al. / Computers & Education 67 (2013) 36–50

39

preferences. First, students selected which agent they would prefer to learn about engineering from (agent choice, “Which of the below
would you want to teach you about electric circuits in the computer?”). Second, the survey asked the students to list three reasons for their
agent selection (agent choice rationale). Third, the survey included six forced choice items requiring students to indicate their preferences
for an animated engineering tutor on six dimensions: gender preference (girl or boy), age preference (young or old), personality preference
(fun or serious), speech rate preference (talks fast or talks slow), clothing preference (dresses serious or dresses cool), and realism preference
(cartoon human or real human). Each of the forced choice items also included an open-ended question asking students to explain their
choices in detail. Finally, the survey asked students to indicate their own gender. Students were given as much time as needed to complete
the survey.
Quantitative and qualitative data analysis techniques were used to analyze students’ responses to the survey items. Frequencies were
obtained for agent choice and each agent characteristic dimension preference and analyzed quantitatively for signiﬁcant differences.
Students’ open-ended responses for agent choice rationale were coded by two independent researchers. The researchers identiﬁed agent
characteristics noted by the students within their open-ended responses. Any characteristic that was noted only once which did not ﬁt into
any already existing category was coded as “other”. For any characteristic noted by two or more participants, a category was established.
From this coding procedure, seven superordinate categories emerged: Age, Gender, Appearance, Personality, Speech, Teaching, and Other.
The Age superordinate category was comprised of two subordinate categories: Young and Old. Gender included three subordinate categories: Male, Female, and Opposite. Appearance included four subordinate categories: Dress, Pretty, Profession, and Realistic. Personality
included 10 subordinate categories: Comfortable, Cool, Fun, Good, Interesting, Interested, Nice, Relatable, Smart, and Trustworthy. Speech
included three subordinate categories: Boring, Clear, and Slow. Teaching included seven subordinate categories: Comprehensive, Effective,
Examples, Friend, Gesturing, Patient, and Understands. Table 1 displays the seven superordinate categories and their corresponding subordinates, with example statements from the students.
2.2. Results
2.2.1. Agent choice
Students were more likely to choose either the young female or young male agent as an engineering tutor, c2(2, N ¼ 77) ¼ 10.62, p ¼ .005.
Twenty-eight (36%) of the students chose a young male agent to be their engineering tutor. Thirty-six (47%) of the students preferred a
young female agent. Thirteen (17%) of the students preferred an old male agent for engineering tutor.
2.2.2. Agent choice rationale
Table 2 displays the number of participants who noted one of the 30 agent choice rationale categories for each of the three agents.
Students who chose the young male agent frequently noted Teaching-Effective (13 students), Personality-Cool (11), Age-Young (9), and
Appearance-Dress (9) as reasons for their choice. Students who chose the young female agent frequently noted Gender-Female (16 students), Teaching-Effective (14), Appearance-Real (13), and Personality-Smart (11) as reasons for their choice. Because fewer students chose
the old male agent to learn with, the frequencies of rationales were lower: Teaching-Effective (13 students), Personality-Smart (8), and
Appearance-Professional (4).
2.2.3. Characteristics preferences
2.2.3.1. Gender preference. There was not an overall signiﬁcant difference in the gender preferred by participants, c2 (1, N ¼ 77) ¼ 0.12,
p ¼ .73. Forty participants (52%) preferred a female engineering agent, and 37 participants (48%) preferred a male engineering agent.
However, male and female students demonstrated a signiﬁcant preference toward an agent that matched their own gender, c2(1) ¼ 21.75,
p < .001. Seventy-six percent of female students reported that they preferred a female agent and 77% of males preferred a male agent.

Fig. 1. Agents. Left to right – old male, young female, young male, old female; old female agent was not provided as a choice in the preliminary study.

40

G. Ozogul et al. / Computers & Education 67 (2013) 36–50
Table 1
Preliminary study: Agent preference superordinate and subordinate Categories, with Example statements.
Superordinate category

Subordinate category

Example statements

Age

Young
Old

Gender

Male
Female
Opposite
Dress
Pretty
Professional
Realistic
Comfortable
Cool
Fun
Good
Interesting
Interested
Nice
Relatable
Smart
Trustworthy
Boring
Clear
Slow
Comprehensive
Effective
Examples
Friend
Gesturing
Patient
Understands

He looks younger (Young)
He is older he may know more (Old)
She seems more of my age (Young)
She is a girl (Female)
He is the opposite sex (Opposite)
Women know a lot of thing (Female)
She has cool shoes (Dress)
He dresses like us (Dress)
He knows how to dress (Dress)

Appearance

Personality

Speech

Teaching

He looks like someone to trust (Trustworthy)
She looks like she is interested (Interested)
Because he looks like someone I would get along (Relatable)

It looks he doesn’t talk fast (Slow)
Talk clear (Clear)
He is smarter as a teacher (Effective)
He looks like a person who explains things to you (Effective)
He might teach me a lot (Effective)

Other

Feels more better
Because I don’t know her, and I would like to know
what she likes
The others do not inﬂuence me

Table 2
Preliminary study: frequency of participants noting agent characteristics, by agent chosen.
Superordinate category

Subordinate category

Age

Young
Old
Male
Female
Opposite
Dress
Pretty
Professional
Realistic
Comfortable
Cool
Fun
Good
Interesting
Interested
Nice
Relatable
Smart
Trustworthy
Boring
Clear
Slow
Comprehensive
Effective
Examples
Friend
Gesturing
Patient
Understands

Gender

Appearance

Personality

Speech

Teaching

Other

Chosen agent
Young male (n ¼ 28)

Young female (n ¼ 36)

Old male (n ¼ 13)

0
9
3
0
1
9
0
0
6
0
11
3
1
0
2
4
3
7
2
1
0
1
2
13
0
0
1
0
0
5

0
4
0
16
0
5
2
1
13
1
3
1
0
2
1
2
0
11
2
1
5
0
0
14
1
2
1
1
1
6

2
0
0
0
0
0
0
4
0
0
2
2
1
0
0
1
0
8
0
0
1
1
1
13
3
0
0
0
0
3

G. Ozogul et al. / Computers & Education 67 (2013) 36–50

41

Example student rationales for preferring matching gender are “I am a girl too”, “boys are better than girls”, “I am a boy too”, and “they
[boys] would be cooler.”
2.2.3.2. Age preference. Overall, students preferred a young agent over an old agent for their learning interactions, c2(1, N ¼ 77) ¼ 17.78,
p < .001. Seventy-four percent of all students reported preference for a young agent. Eighty-six percent of the females chose a young agent,
whereas 60.0% of the male students chose a young agent. The preference for a young agent among female students was signiﬁcant, c2(1,
N ¼ 42) ¼ 21.43, p < .001, while there was not a signiﬁcant preference among male learners. Example student rationales for preferring a
young agent are “[young] up to date”, “I can relate to them”, “they don’t need to stop and think”, “he understands us because he is young,”, “it
would be like a friend teaching me,” and “old people don’t get my attention.”
2.2.3.3. Personality preference. Overall, learners preferred a ‘fun’ personality, compared to a more ‘serious’ personality, c2(1, N ¼ 77) ¼ 12.48,
p < .001. Seventy percent of all learners reported preference for an agent with a fun personality. Eighty-one percent of females preferred a
‘fun’ agent, whereas 57% of male students preferred a ‘fun’ agent. The difference in number of males preferring a fun agent over a serious
agent was not signiﬁcant, c2(1, N ¼ 35) ¼ 0.71, p ¼ .40. However, female learners did demonstrate a signiﬁcant inclination toward a ‘fun’
pedagogical agent, c2(1, N ¼ 42) ¼ 16.10, p < .001. Example student rationales for choosing a fun personality agent are “serious is boring”,
“fun is good”, “I learn more”, “make you laugh”, “make subject fun”, “to make the learning process fun”, and “it will make learning easier.”
2.2.3.4. Speech rate preference. Overall, learners preferred an agent with slow speech rate for their engineering domain learning interactions
rather than a fast speech rate, c2(1, N ¼ 77) ¼ 31.18, p < .001. Eighty-two percent of the learners reported preference for an agent with slow
speech rate. Example rationales for preferring a slower speech rate are “so I could understand it”, “that is good that he talks slow”, “slow is
better”, “so I can hear everything they are saying”, “explains more clearly”, “so he explains it step-by-step”, and “it lets me memorize.”
2.2.3.5. Clothing preference. There was a marginally signiﬁcant preference, across all participants, for animated agents with dress described
as ‘cool’, compared to agents with ‘serious’ dress, c2(1) ¼ 3.75, p ¼ .053. Sixty-one percent of all learners reported a preference for an agent
with ‘cool’ wardrobe, whereas 39% of the students preferred an agent with a “serious” wardrobe. Example rationales for preferring an agent
with cool clothing are “I dress cool”, “she looks great”, “makes me want to pay attention”, “class would go easy”, “more fashion the better”,
“they look pretty”, and “so you could learn fast.”
2.2.3.6. Realism preference. Overall no signiﬁcant differences were found for the choices for a cartoon or real human image, c2
(1, N ¼ 77) ¼ 0.12, p ¼ .73. Fifty-two percent of the students preferred a cartoon-like image for the engineering animated agent. Example
rationales for preferring a cartoon-like image are “cartoon humans grab my attention”, “it would be fun and educational”, “funny” and
“engineer teachers look like a cartoon”, “I would focus more on the problems”, and “it’s cool and funny”. Forty-eight percent of the students
preferred a real-humanlike image for the engineering tutor, and example rationales for this preference are “serious”, “helps us understand
more”, “so I can ask questions back at her”, “they explain better”, “to explain easier and no distraction”, “it would look better”, and “it would
be more realistic.”
2.2.4. Summary of ﬁndings
Results from this preliminary study indicated that when provided static images of animated agents, middle-school students choose
young agents which match their gender. The most common reason for selecting either the young male agent or the old male agent was
the perception that they would be effective teachers, whereas the most common reason for selecting the young female was because she
was female. However, this should not be taken as evidence that the participants did not see the young female agent as an effective
teacher, since the same number of students indicated that she would be an effective teacher (n ¼ 14). Closer inspection of those who
noted female gender as the reason for choosing the young female agent revealed that 87% of those students were female. In addition to
the students’ tendency toward young agents of their own gender, they also all preferred an agent with cool clothing and a slow rate of
speech. Additionally, female students preferred young agents with fun personality, whereas male students did not have these
inclinations.
3. Experiment 1
The results from the preliminary study indicated that middle school-students would prefer to learn from a young agent that matches
their gender. However, the inclination toward agents of the same gender does not necessarily imply that students will learn better from or
have more positive perceptions of learning from a same-gendered agent. Experiment 1 was designed to determine whether gender
matching has a positive impact on learning outcomes or student perceptions. The experiment set out to answer three primary research
questions: 1) Does matching the gender of the APA to the learner impact learning outcomes?; 2) Does matching the gender of the APA to the
learner impact learners’ subjective perceptions of the computer program?; 3) Does gender matching of an APA have a differential impact on
male and female learners?
3.1. Method
3.1.1. Participants and design
The participants were a total of 197 6th, 7th, and 8th grade students in a public middle school in the Southwestern U.S., 109 females and
88 males. The mean age of the participants was 12.1 years (SD ¼ 1.01 years). Eighty-two (41.6%) of the students reported that they were
Hispanic American, 36 (18.3%) students reported they were Caucasian, 29 students (14.7%) reported they were African American, 23 students (11.7%) reported being of other ethnicities, 20 (10.2%) reported they were Asian American, and seven (3.6%) reported their ethnicity as
Native American. The students had no school instruction on electrical circuits prior to participating in this study.

42

G. Ozogul et al. / Computers & Education 67 (2013) 36–50

To determine the effect of matching APA gender to participant gender, we manipulated whether students received an APA which
matched their gender or one which was opposite in gender (Match, Opposite). Dependent variables included performance on the posttest
and student ratings of perceived difﬁculty and attitudes toward the instructional module. All participants were randomly assigned to one of
the two experimental conditions. There were 96 students in the Match (M) condition and 101 students in the Opposite (O) condition. Within
the Match condition, there were 54 females and 42 males. Within the Opposite condition, there were 55 females and 46 males.
3.1.2. Materials and apparatus
3.1.2.1. Computerized materials. For each participant, the computerized materials consisted of an interactive program that included: (1) a
demographic questionnaire asking participants to report their gender, age, ethnicity, and interest in engineering, electric circuits, mathematics, and learning from a computer; (2) an introduction to the objectives of the instructional program; (3) an instructional session
providing a brief conceptual overview of a single-resistor electrical circuit; (4) a simulation session; and (5) a program rating questionnaire.
The topic domain of electrical engineering was chosen since engineering instruction is becoming increasingly important for K-12 students
(Brophy, Klein, Portsmore, & Rogers, 2008; Carr, Bennet, & Strobel, 2012; Reisslein, Johnson, Bishop, Harvey, & Reisslein, 2013) as well as the
general population (Ozogul, Johnson, Moreno, & Reisslein, 2012; Pearson & Young, 2002). Also, teachers often have reservations about
teaching engineering; thus computer-based education is an important avenue for K-12 engineering instruction.
Both conditions contained an identical introduction to the objectives presented by the appropriate APA (step 2). Also, both conditions
had identical narrated explanations and calculations using Ohm’s Law equation as well as identical depictive representations, including the
circuit diagram and the Cartesian graph of voltage as a function of current in the instructional session (step 3) and the simulation session
(step 4). As illustrated in Fig. 2, the presentation screen in the simulation session contained a circuit diagram depicting the considered circuit
and a Cartesian graph that depicted a plot of the voltage as a function of the current in the considered circuit. The circuit diagram contained
the equations specifying the given resistance and current values. In addition, the sequence of equation calculation steps for evaluating the
voltage using Ohm’s Law was presented to the left of the voltage source symbol of the circuit diagram. In summary, the simulation session
employed multiple representations, namely narration and mathematical equations, i.e., descriptive representations, as well as a schematic
circuit diagram and a plot relating system quantities, i.e., depictive representations.
The simulation session ﬁrst presented an electrical circuit with given default resistance and current values and explained how to obtain
the voltage value by using Ohm’s Law equation or the Cartesian graph of voltage as a function of current. Then, students were given three
opportunities to select different current or voltage values and observe the outcome of their selection. For each of the selected current or
voltage values, the simulation session explained how to use the corresponding Ohm’s Law equation and Cartesian graph and how to obtain
the missing voltage or current value using both Ohm’s Law equation and the Cartesian graph. More speciﬁcally, for a given circuit example,
the simulation session ﬁrst introduced the given circuit and then calculated the missing circuit quantity using Ohm’s Law equation. Subsequently, the simulation session explained how to obtain the missing circuit quantity using the Cartesian graph, and ﬁnally related the
result found in the Cartesian graph back to the result found with Ohm’s Law equation and the given circuit. During the simulation session,
the APA appeared on the screen to dynamically point to the visual element of the multiple representations in the display screen that
corresponds to the present passage in the narrated explanation. The APA pointed to the visual element through deictic gestures, e.g.,
pointing with arms and ﬁngers, as illustrated in Fig. 2. More speciﬁcally, the agent provides visual attention guidance by signaling relevant
information on graphs and electric circuit diagrams in synchrony with the narrated message. The primary pedagogical functions of the

Fig. 2. Sample screen shot of multi-representation display screen with Ohm’s Law equation calculations, a circuit diagram, and a Cartesian graph of voltage as a function of current
used in the simulation session.

G. Ozogul et al. / Computers & Education 67 (2013) 36–50

43

animated agent are to 1) deliver verbal instruction on the fundamentals of electric circuits and Ohm’s Law and 2) assist learners in identifying relevant visual information which corresponds to the verbal message (i.e., narration).
The instructional program was presented using one of two animated pedagogical agents: a young male agent or a young female agent.
Both animated agents were approximately of the same age as the student participants, and had casual attire similar to the students, see
Fig. 1. The design of the APAs was inspired by several similar avatars found in games that are popular among precollege students. More
speciﬁcally, the APAs were 3D computer agents created with Autodesk 3D Studio Max 5, a software for building, animating, and rendering
3D models and characters. The narration voice ﬁles were applied to the APAs using the Ventriloquist program, which uses a collection of
twelve phonemes to animate the agent’s mouth and facial expressions in correlation to recorded speech. Additional facial expressions of
eyebrow motions, eye movements, and head nods as well as animated body and hand movement were added. All of these animated
movements were cued within 3D Studio Max to the speech of the agents. Completed APA animations were rendered by 3D Studio Max as
video ﬁles which were imported into Adobe After Effects CS2 to be layered onto the static image of the multiple representation screen. The
experimental conditions differed only in which animated agent was used: In the Match (M) condition, the APA matched the gender of the
participant, in the Opposite (O) condition, the APA’s gender was opposite to the participant.
The last section in the computer program was an 18-item Likert instrument, which included 10 items asking participants to rate their
learning perceptions concerning the program (e.g., “I would recommend this program to other students”) and 8 items related to cognitive
load. All items were on a 5-point scale ranging from 0–strongly disagree to 4–strongly agree. The learning perceptions questionnaire was a
revised version of a 16-item survey that the authors had developed in collaboration with experts in computer-based engineering education
(Moreno, Reisslein, & Ozogul, 2009; Reisslein, Moreno, & Ozogul, 2010). The construct validity of the revised survey was assessed with the
judgment of subject matter experts in electrical engineering instruction.
To examine the reliability of the program rating instrument in the present study, we conducted a factor analysis using principal axis
estimation, with all 18 items from the program rating instrument. Results demonstrated that three factors accounted for 61.7% of the
variance for student ratings. Extraction of three factors was based on a threshold of one eigenvalue. The three identiﬁed factors related to 1)
evaluations of the program or content matter (eight items, such as “I would recommend this program to other students” and “I would like to
learn more about electrical circuits”, with factor loadings ranging from 0.47 to 0.77), 2) evaluations of the graphics used in the program (four
items, such as “The graphics made the lesson easier to understand” and “The graphics in the program helped me to learn”, with factor
loadings ranging from 0.55 to 0.75), and 3) difﬁculty ratings (six items, such as "The lesson was difﬁcult" and "The topics that were covered
in the lesson were difﬁcult", with factor loadings ranging from 0.41 to 0.90). As measured by Cronbach’s alpha (Allen, Reed-Rhoads, Terry,
Murphy, & Stone, 2008), the internal reliability of the program rating scale was 0.91, internal reliability of the graphics rating scale was 0.86,
and internal reliability of the difﬁculty ratings was 0.89. A program ratings score, a graphics ratings score, and a perceived difﬁculty score
were computed by averaging the ratings from the respective questions which loaded on these factors.
The program rating questionnaire also included four open-ended questions to capture what students liked best and least about the
computer-based instructional module and about the animated agent used in the program. Two researchers independently examined
participants’ responses from these four open-ended questions. The characteristics noted by the students were identiﬁed and from this initial
inspection, 14 coding categories for program characteristics emerged (including an ‘Other’ category) and 17 coding categories for agent
characteristics (including ‘Other’). All of the open-ended responses were coded following this coding scheme.1 A subset of this data (30%)
was coded by both researchers to determine interrater reliability; there was agreement on 74.1% of responses, an acceptable percentage
agreement for open-ended qualitative coding (Stemler, 2004).
The computer-based learning module used in the study was developed using Adobe Flash CS4 software, an authoring tool for creating
web-based and standalone multimedia programs. The module provided log ﬁles, including participant responses to the demographic and
program rating questionnaires and interaction data (e.g., time on task). The equipment consisted of a set of laptop computer systems, each
with a screen size of 1680  1050 pixels, and headphones.
3.1.2.2. Paper and pencil materials. The paper and pencil materials consisted of a pretest and a posttest on electric circuit analysis. The
pretest was a 12-item multiple-choice test on students’ domain-speciﬁc prior knowledge (with internal reliability of a ¼ .56), and the
posttest included 13 novel single-resistor electrical circuit problems to be solved both with the symbolic approach using Ohm’s Law
equation and with the graphical approach using the Cartesian graph (internal reliability: a ¼ .89). A posttest problem was presented as a
circuit diagram of a single-resistor circuit with given voltage of V ¼ 20 V and resistance R ¼ 5 U and asked to ﬁnd the current in the circuit a)
using Ohm’s Law equation, and b) using the provided Cartesian graph of voltage as a function of current. Eleven near transfer items required
learners to use a provided Cartesian graph or Ohm’s Law equation to determine or calculate voltage (or current) for given resistance and
current (or voltage) values following the same solution procedure as taught in the computer module. Two far transfer items required the
students to identify different correspondences between the Cartesian graphs and given single resistor problems than directly taught in the
computer module, such as mapping a given single-resistor problem to a Cartesian graph, or reasoning about the behaviors of current when
the resistance value changes in a circuit with a given source voltage. Correct solution of the far transfer problems required a deep understanding of the relationships between current, voltage, and resistance in a single-resistor circuit. Both pretest and posttest were designed
and printed using the same color and layout scheme as the computer program. Two independent scorers who were blind to the conditions
of the participants scored the pretest and posttest (interrater reliability 98.5%).
3.1.3. Procedure
Each participant was provided with a laptop, headphones, and two closed envelopes, which contained the paper-based pretest and
posttest. The subject identiﬁcation number and the letter representing the condition of the student were written on the envelope. The
envelopes were randomly distributed to the students. First, the researcher instructed students to start working on the pretest envelope. Once

1
The same coding scheme was used in Experiment 2, except an additional category was added because several participants noted liking the ability to choose their
animated agent in the choice condition.

44

G. Ozogul et al. / Computers & Education 67 (2013) 36–50

they were done with the pretest and returned the pretest back to the envelope, the researcher had the students start the computer-based
module by entering the combination of identiﬁcation number and condition letter on the envelopes. They were then instructed to put on
their headphones and work independently on all sections of the module. Once the computer-based learning session was over, participants
were instructed to open the posttest envelope, and complete the posttest. After completing the posttest, the students returned the posttest to
the envelope, and closed it. The researcher then collected all the laptops and the pretest and posttest envelopes for scoring and data analysis.
3.2. Results
An initial 2 (Condition: Match and Opposite)  2 (Participant gender: Male and Female) ANOVA was conducted on pretest scores. The
main effect for condition was not signiﬁcant, F(1, 193) ¼ 1.15, p ¼ .29. There was also no signiﬁcant difference in pretest scores between male
and female students, F(1,193) ¼ 2.09, p ¼ .15, nor a signiﬁcant interaction between condition and participant gender (F < 1). The participants
spent on average 6.6 min (SD ¼ 1.6 min) on demographic questionnaire, introduction, and instructional session (steps 1–3) and on average
10.2 min (SD ¼ 0.8 min) on the simulation session (step 4). A t-test on the total time spent on the computer-based module (steps 1–4)
indicated no signiﬁcant differences between conditions, t(195) ¼ 0.83, p ¼ .41.
Table 3 displays the means and standard deviations for total posttest scores, near and far transfer scores, program ratings, graphics
ratings, and difﬁculty ratings by experimental condition and participant gender. Analyses of variance (ANOVAs) were conducted on students’ posttest scores, program ratings, graphics ratings, and difﬁculty ratings using experimental condition and participant gender as
between-subject factors. A series of 2 (Condition: Match and Opposite)  2 (Participant gender: Male and Female) univariate analyses of
variance were conducted to determine whether there was a main effect of experimental condition, a main effect of participant gender, or an
interaction between experimental condition and participant gender on each of the dependent variables. The respective ANOVAs on total
posttest scores, near transfer scores, and far transfer scores indicated no signiﬁcant main effect for experimental condition, no signiﬁcant
main effect for participant gender, nor a signiﬁcant interaction between condition and participant gender (All p’s > 0.10).
The ANOVA on program ratings indicated no signiﬁcant main effect for experimental condition (F < 1). A signiﬁcant main effect of
participant gender was indicated, F(1,193) ¼ 5.89, MSE ¼ 0.71, p < .05, h2P ¼ 0.03. Male participants rated the program signiﬁcantly higher
than females. Also, results indicated a signiﬁcant interaction between experimental condition and participant gender, F(1,93) ¼ 6.58, p < .05,
h2P ¼ 0.03. Follow-up analyses indicated that female participants rated the Match condition signiﬁcantly higher than the Opposite condition,
t (107) ¼ 2.15, p < .05. Although the mean program rating of male participants was actually higher for the Opposite condition compared to
the Match condition, this difference was not statistically signiﬁcant, t(86) ¼ 1.52, p ¼ .13. Within the Opposite condition, males had higher
program ratings than females, t(99) ¼ 3.61, p < .001; no signiﬁcant difference was found between males and females in the Match condition,
t(94) ¼ 0.10, p ¼ .92. No signiﬁcant main effects or interactions were revealed for participants’ graphics ratings or difﬁculty ratings.
Tables 4 and 5 display the number of participants who noted various categories as best and least liked about the program and about the
agent, respectively. The most common characteristics of the computer program noted as liked best were Graphics (n ¼ 58 participants),
Topic (n ¼ 43), Agent (n ¼ 39), and Difﬁculty level (n ¼ 28). The most common characteristics of the program noted as liked least were Boring
(n ¼ 45), Difﬁculty level (n ¼ 38), Agent (n ¼ 13), Graphics (n ¼ 13), and Questions (n ¼ 13). The most common characteristics noted as liked
best about the agent were Pointing (n ¼ 41), Helpful (n ¼ 37), Examples/Explanations (n ¼ 31), and Agent speech (n ¼ 18). The most common
characteristics noted as least liked about the agent were Agent speech (n ¼ 31), Pace (n ¼ 29), Image (n ¼ 22), and Movements (n ¼ 18).
To examine whether matching the agent gender to the student impacted the characteristics favored and disliked by the students, 2
(match or opposite)  2 (noted or not noted) chi-square analyses were conducted on each category. The results of these chi-square analyses
are reported in Tables 4 and 5. Results suggested that the match condition did not signiﬁcantly impact students’ most liked characteristics of
the program as a whole. However, more students in the opposite condition noted the difﬁculty level as the least liked characteristic of the
program, compared to the match condition. Concerning agent characteristics, results indicated that signiﬁcantly more match condition
participants noted the agent personality as their favorite characteristic, compared to the opposite condition. More students in the opposite
condition noted the examples or explanations as their least favorite aspect of the agent, compared to the match condition, whereas more of
the match participants noted realism as their least favorite aspect.
3.3. Summary of ﬁndings
As previously noted, Experiment 1 was designed to address three research questions. While this experiment did not produce evidence
that matching the gender of the APA to the learner impacted learning outcomes, it did reveal that female students reported higher program

Table 3
Experiment 1: Descriptive statistics for posttest scores and ratings, by experimental condition and participant gender.
Experimental
condition

Participant
gender

Total posttest
(max ¼ 13)
EMM

SE

EMM

SE

EMM

Match
(n ¼ 96)

Female (n ¼ 54)
Male (n ¼ 42)
Total
Female (n ¼ 55)
Male (n ¼ 46)
Total
Female (n ¼ 109)
Male (n ¼ 88)

10.58
10.43
10.51
9.94
10.22
10.08
10.26
10.32

0.46
0.52
0.35
0.46
0.50
0.34
0.32
0.36

9.32
8.98
9.15
8.52
8.72
8.62
8.92
8.85

0.42
0.48
0.32
0.42
0.46
0.31
0.30
0.33

1.26
1.45
1.36
1.42
1.50
1.46
1.34
1.48

Opposite
(n ¼ 101)
Totals
a
b

Near transfer
(max ¼ 11)

Signiﬁcantly (p < .05) higher than rating for Opposite condition.
Signiﬁcantly (p < .05) higher than females.

Far transfer
(max ¼ 2)

Program rating

Graphics rating

Difﬁculty
rating

SE

EMM

SE

EMM

SE

EMM

SE

0.09
0.11
0.07
0.09
0.10
0.07
0.07
0.07

2.68a
2.66
2.67
2.34
2.95b
2.64
2.51
2.81b

0.12
0.13
0.09
0.11
0.13
0.08
0.08
0.09

3.07
2.83
2.95
2.79
2.98
2.89
2.93
2.91

0.12
0.14
0.09
0.12
0.13
0.09
0.09
0.10

1.69
1.60
1.65
1.80
1.70
1.75
1.75
1.65

0.14
0.15
0.10
0.13
0.15
0.10
0.10
0.11

G. Ozogul et al. / Computers & Education 67 (2013) 36–50

45

Table 4
Experiment 1: frequencies of participants who noted program characteristics best and least liked, by condition.

c2(1), p

Liked best

Agent
Agent speech
Computer-based
Difﬁculty Level
Examples/Explanations
Formulas
Fun/boring
Graphics
Interactivity
Modality
Questions
Pace
Topic
Other

Opposite (n ¼ 101)

Match (n ¼ 96)

18
2
2
14
9
–
3
29
5
2
–
1
24
2

21
1
1
14
8
1
4
29
6
1
–
–
19
2

0.51,
0.29,
0.29,
0.02,
0.02,
1.06,
0.21,
0.05,
0.16,
0.29,
N/A
0.96,
0.46,
0.00,

0.48
0.59
0.59
0.89
0.89
0.30
0.65
0.82
0.69
0.59
0.33
0.50
0.96

c2(1), p

Liked least
Opposite (n ¼ 101)

Match (n ¼ 96)

8
5
1
25
2
2
18
5
1
1
8
5
6
9

5
7
–
13
–
3
27
8
1
1
5
2
3
2

0.59, 0.44
0.47, 0.49
0.96, 0.33
3.97, <0.05
1.92, 0.17
0.26, 0.61
2.96, 0.09
0.91, 0.34
0.00, 0.98
0.00, 0.98
0.59, 0.44
1.19, 0.28
0.90, 0.34
4.35, 0.04

Bold typeface indicates signiﬁcant difference at the .05 level.

ratings when the APA matched their gender. On the other hand, when the APA did not match their gender, male students reported higher
program ratings than females. Qualitative analysis of participants’ feelings toward the program and agents revealed that: 1) when the APA
did not match student gender, participants more frequently noted difﬁculty level as least favorite aspect of the program and examples/
explanations as least favorite aspect of the agent; and 2) when the APA matched student gender, participants more often noted agent
personality as favorite aspect of the agent and agent realism as least favorite aspect of the agent.
4. Experiment 2
In Experiment 1, students were not given a choice over whether the APA matched their gender or not, which is an example of system or
program control approach to instructional design (Clark & Mayer, 2011). In contrast, a learner control approach to instruction provides
learners with a varying range and degree of control over the instruction. This can include selecting the sequencing, pacing, content, and
appearance of the instructional environment. For certain learners, allowing some degree of control over the learning process elevates their
feelings of autonomy, leads to greater motivation and self-efﬁcacy in the task, and better learning outcomes (Bandura, 2001; Clark & Mayer,
2011; Ryan & Deci, 2000). To address this issue, Experiment 2 was designed to systematically test the impact of providing learners the choice
among four APAs. Speciﬁcally, Experiment 2 was conducted to investigate three primary research questions: 1) Does providing learners a
choice of animated agent inﬂuence learning outcomes or learner perceptions?; 2) Does the age of the animated agent have an impact on
learning outcomes or learner perceptions?; and 3) Do the inﬂuences of choice, agent age, and gender matching interact to inﬂuence learning
outcomes or learner perceptions?
4.1. Method
4.1.1. Participants and design
The participants were a total of 334 6th, 7th, and 8th grade students in a public middle school in the Southwestern U.S., 161 females and
173 males. The mean age of the participants was 12.3 years (SD ¼ 0.86 years). One hundred ninety-three (57.8%) of the students reported that
they were Hispanic American, 66 (19.8%) students reported they were Caucasian, 38 students (11.4%) reported they were African American,
Table 5
Experiment 1: frequencies of participants who noted agent characteristics best and least liked, by condition.

c2(1), p

Liked best
Opposite (n ¼ 101)
Agent is female
Agent is male
Helpful/unhelpful
Distracting
Examples/explanations
Fun/boring
Image
Movements
Pace
Personality
Pointing
Realism
Relatable
Repetitive
Smart
Agent speech
Other

1
–
19
–
15
2
5
1
2
21
2
1
1
2
9
9

Bold typeface indicates signiﬁcant difference at the .05 level.

Match (n ¼ 96)
1
–
18
–
16
5
10
1
6
4
20
3
4
2
–
9
7

0.00, 0.98
N/A
0
N/A
0.12, 0.73
1.50, 0.22
2.09, 0.15
0.00, 0.98
2.30, 0.13
4.30, 0.04
0
0.26, 0.61
2.01, 0.16
0.39, 0.53
1.92, 0.17
0.01, 0.91
0.17, 0.68

c2(1), p

Liked least
Opposite (n ¼ 101)

Match (n ¼ 96)

–

–
–

2
10
1
7
4
13
13
14
–
–
–
5
–
18
4

4
3
1
5
9
5
15
–
1
6
–
6
–
13
5

N/A
1.92, 0.17
2.45, 0.12
1.13, 0.29
4.38, 0.04
0.18, 0.66
0.61, 0.44
3.48, 0.06
0.12, 0.73
N/A
1.06, 0.30
6.51, 0.01
N/A
0.16, 0.69
N/A
0.68, 0.41
0.18, 0.67

46

G. Ozogul et al. / Computers & Education 67 (2013) 36–50

20 students (6.0%) reported being of other ethnicities, nine (2.7%) reported they were Asian American, and eight (2.4%) reported their
ethnicity as Native American. The students had no school instruction on electrical circuits prior to participating in this study.
To determine the effect of allowing learners the choice of APA, we manipulated whether students received a pre-assigned random APA or
whether they were free to choose from four APA options based on the agents’ static image. The dependent variables included performance on
the posttest, student ratings of perceived difﬁculty and attitudes toward the instructional module. All participants were randomly assigned to
one of the two experimental conditions. There were 170 students in the Choice (C) condition and 164 students in the No Choice (NC) condition.
Within the Choice condition, there were 90 females and 80 males. Within the No Choice condition, there were 71 females and 93 males.
4.1.2. Materials and apparatus
4.1.2.1. Computerized materials. The computerized materials in Experiment 2 were identical to those used in Experiment 1, except for the
addition of the pretest to the computer program. Thus, the program for Experiment 2 consisted of: (1) demographic questionnaire; (2)
pretest; (3) introduction to the objectives; (4) brief conceptual overview of simple circuit; (5) simulation session; and (6) program rating
questionnaire. As in the ﬁrst experiment, the program rating questionnaire included four open-ended questions to capture what students
liked best and least about the computer program and the agents. A subset of this data (30%) was independently coded by both researchers to
establish interrater reliability; the coders agreed on 77.5% of responses. The computerized pretest used in the computer program was
identical to the paper and pencil pretest used in Experiment 1.
The instructional program was presented using one of four animated pedagogical agents: a young male agent, older male agent, young
female agent, or an older female agent, see Fig. 1. The two young agents were of approximately the same age as the participants, and wore
casual attire. The two older agents wore clothing that resembled that of a teacher’s. The voices of the older agents were identical to those
used by the young agents. The experimental conditions differed only in whether a choice was provided in animated agent: In the Choice (C)
condition, immediately before the introduction (step 3), a screen displayed the still images of the four agents and participants were able to
choose the APA from these options. In the No Choice (NC) condition, there was no choice screen presented; the APA was randomly selected
from the four alternatives for them.
4.1.2.2. Paper and pencil materials. The paper and pencil materials consisted of the posttest on electric circuit analysis. The posttest included
9 novel single-resistor electrical circuit problems to be solved both with the symbolic approach using Ohm’s Law equation and with the
graphical approach using the Cartesian graph (internal reliability: a ¼ .74). These nine items included six near transfer items and three far
transfer items (see explanation of near and far transfer items in Section 3.1.2.2.). The posttest was printed using the same color and layout
scheme as the computer program. Two independent scorers blind to the conditions of the participants scored the pretest and posttest
(interrater reliability 98.5%).
4.1.3. Procedure
The procedure for Experiment 2 was identical to Experiment 1 except the pretest was completed within the computer-based module.
4.2. Results
An initial set of analyses was conducted to determine the pattern of choices made by participants within the Choice condition. Table 6
shows the frequency of male and female participants who chose each type of animated agent. Participants overwhelmingly chose to learn
with a young animated agent that matched their gender. A c2 test of independence indicated that the proportion of students who selected a
young agent (85%) was signiﬁcantly higher than for the old agent, c2 (1, N ¼ 170) ¼ 81.9, p < .001. Results also demonstrated that the
proportion of students who selected a matched-gender agent (89%) was signiﬁcantly higher than for opposite gender, c2 (1,
N ¼ 170) ¼ 102.5, p < .001.
An initial 2 (Condition: Choice and No Choice)  2 (Participant gender: Male and Female) ANOVA was conducted on pretest score. There
were no signiﬁcant differences between conditions (F < 1). There was also not a signiﬁcant difference in pretest scores between young agent
and old agent, F(1,326) ¼ 3.48, p ¼ .45, or between gender-matched agents and non-gender-matched agents, F(1,326) ¼ 1.82, p ¼ .58. The
analysis further indicated that none of the interaction terms were signiﬁcant (all F’s < 1). The participants spent on average 10.8 min
(SD ¼ 2.6 min) on demographic questionnaire, pretest, introduction, and instructional session (steps 1–4) and on average 10.1 min
(SD ¼ 1.0 min) on the simulation session (step 5). A t-test on the total time spent on the computer-based module (steps 1–5) indicated no
signiﬁcant differences between conditions, t(332) ¼ 0.15, p ¼ .88.
Table 7 displays the means and standard deviations for total posttest scores, near and far transfer scores, program ratings, graphics
ratings, and difﬁculty ratings by experimental condition, agent age, and agent-participant gender match (match/opposite). Analyses of
variance (ANOVAs) were conducted on students’ posttest scores, program ratings, graphics ratings, and difﬁculty ratings using experimental
condition, agent age, and agent-participant gender match as between-subject factors. A series of 2 (Condition: Choice and Non-choice)  2
(Agent age: Young and Old)  2 (Gender match: Match and Opposite) univariate analyses of variance were conducted for each of the
dependent variables. The ANOVA on overall posttest scores indicated no signiﬁcant main effects for the three factors and no signiﬁcant
interactions among the factors (all p’s > 0.10). The ANOVA on near transfer scores also indicated no signiﬁcant main effects or interactions
(all p’s > 0.1).
Table 6
Experiment 2: frequencies of APA selections of choice-condition participants, by participant gender.

Male (n ¼ 80)
Female (n ¼ 90)

Young male

Old male

Young female

Old female

58
6

11
2

10
70

1
12

G. Ozogul et al. / Computers & Education 67 (2013) 36–50

47

Table 7
Experiment 2: Descriptive statistics for posttest scores and ratings, by experimental condition, agent Age, and gender Match.
Total posttest
(max ¼ 9)

Near transfer
(max ¼ 6)

Far transfer
(max ¼ 3)

Program
rating

Graphics
rating

Difﬁculty
rating

Experimental
condition

Agent age

Agent-participant
gender match

EMM

SE

EMM

SE

EMM

SE

EMM

SE

EMM

SE

EMM

SE

No choice
(n ¼ 164)

Young (n ¼ 81)

Opposite (n ¼ 40)
Match (n ¼ 41)
Total
Opposite (n ¼ 39)
Match (n ¼ 44)
Total

6.01
5.74
5.88
6.53
6.42
6.47
6.18

0.34
0.33
0.24
0.34
0.32
0.24
0.17

4.61
4.62
4.62
5.14
4.99
5.07
4.84

0.25
0.25
0.18
0.25
0.24
0.17
0.12

1.40
1.12
1.26
1.39
1.43
1.43
1.34

0.15
0.15
0.11
0.15
0.14
0.13
0.07

2.56
2.73
2.65
2.55
2.51
2.53
2.59

0.12
0.12
0.09
0.12
0.12
0.09
0.06

2.75
3.03
2.89
2.78
2.61
2.70
2.79

0.14
0.14
0.10
0.14
0.13
0.10
0.07

1.67
1.95
1.81
1.57
1.75
1.66
1.73

0.16
0.15
0.11
0.16
0.15
0.11
0.08

Opposite (n ¼ 16)
Match (n ¼ 128)
Total
Opposite (n ¼ 3)
Match (n ¼ 23)
Total

7.13
5.90
6.51
6.67
6.59
6.63
6.57

0.53
0.19
0.28
1.23
0.45
0.66
0.36

5.44
4.73
5.08
4.33
4.98
4.66
4.87

0.40
0.14
0.21
0.92
0.33
0.49
0.27

1.69c
1.17
1.43
2.33c
1.61
1.97b
1.70a

0.24
0.08
0.13
0.55
0.20
0.29
0.16

2.67
2.63
2.65
2.50
2.70
2.60
2.63

0.19
0.07
0.10
0.45
0.16
0.24
0.13

2.75
2.79
2.77
2.25
2.86
2.55
2.66

0.22
0.08
0.11
0.50
0.18
0.27
0.14

1.96
1.96
1.96
1.67
1.68
1.67
1.82

0.25
0.09
0.13
0.57
0.21
0.30
0.16

Old (n ¼ 83)

Total
Choice
(n ¼ 170)

Young (n ¼ 144)

Old (n ¼ 26)

Total
a
b
c

Signiﬁcantly (p < .05) higher than score for No choice condition.
Signiﬁcantly (p < .05) higher than score for Young agents (within Choice condition).
Signiﬁcantly (p < .05) higher than score for Matched-gendered agents (within Choice condition).

The ANOVA on far transfer scores indicated a signiﬁcant main effect for Choice, F(1,326) ¼ 4.33, p < .05, h2P ¼ 0.013. Students in the Choice
condition scored signiﬁcantly higher on far transfer items than those in the No Choice condition. There was also a marginal main effect of
Agent age, F(1,326) ¼ 3.84, p ¼ .051, h2P ¼ 0.012. Students who learned with an old agent scored signiﬁcantly higher on far transfer items than
did those who learned with a young agent. There was a signiﬁcant main effect for Gender match, F(1,326) ¼ 4.38, p < .05, h2P ¼ 0.013.
Students who learned with an agent of an opposite gender scored signiﬁcantly higher on far transfer items than did those who learned with
a same-gendered agent. Although there was not a signiﬁcant interaction between experimental condition and Agent age (p ¼ .26) or Gender
match (p ¼ .15), we suspected that these factors inﬂuenced students in the Choice condition more than their counterparts in the Non-choice
condition. Therefore, we conducted follow-up tests comparing far transfer scores between Young and Old agents and between Oppositegendered and Matched-gendered agents within the two conditions. There was not a signiﬁcant difference between young and old
agents in far transfer performance within the Non-choice condition, F(1,162) ¼ 1.05, p ¼ .31. However, within the Choice condition, students
who chose an old agent scored signiﬁcantly higher than those who chose a young agent, F(1,168) ¼ 4.99, p < .03, h2P ¼ 0.029. Also, there was
not a signiﬁcant difference between Opposite-gendered and Matched-gendered agents on far transfer performance within the Non-choice
condition (F < 1). Within the Choice condition, students who chose an opposite-gendered agent scored signiﬁcantly higher than those who
chose a matched-gendered agent, F(1,168) ¼ 5.43, p < .05, h2P ¼ 0.031. To further elucidate potential differences between the students who
chose old agents and those who chose opposite-gendered agents for the learning session, we compared their individual difference measures
(i.e., age and prior knowledge). Students who chose to learn with an old agent were signiﬁcantly older (M ¼ 12.6, SD ¼ 0.81) than those who
chose a young agent (M ¼ 12.2, SD ¼ 0.82), t(168) ¼ 2.03, p < .05. These students also had signiﬁcantly higher pretest scores (M ¼ 4.08,
SD ¼ 1.97) than those who chose a young agent (M ¼ 3.08, SD ¼ 1.95), t(168) ¼ 2.41, p < .05. Furthermore, the students who chose an
opposite-gendered agent were signiﬁcantly older (M ¼ 12.7, SD ¼ 0.56) than those who chose a matched-gendered agent (M ¼ 12.2,
SD ¼ 0.84), t (168) ¼ 2.61, p < .01. There was not a signiﬁcant difference in pretest scores between students who chose opposite or matchedgendered agents, t(168) ¼ 0.04, p ¼ .97.
The ANOVA on program ratings, graphics ratings, and difﬁculty ratings indicated no signiﬁcant main effects or interactions for the three
factors (All p’s > 0.10).
Tables 8 and 9 display the number of participants who noted various categories as best and least liked about the program and about the
agent, respectively. The most common characteristics of the computer program noted as liked best were Graphics (n ¼ 76 participants),
Agent (n ¼ 62), Topic (n ¼ 44), and Formulas (n ¼ 37). The most common characteristics of the program noted as liked least were Pace
(n ¼ 47), Difﬁculty level (n ¼ 33), Graphics (n ¼ 24), and Formulas (n ¼ 19). The most common characteristics noted as liked best about the
agent were Helpful (n ¼ 79), Examples/Explanations (n ¼ 71), and Agent speech (n ¼ 37). The most common characteristics noted as least
liked about the agent were Agent speech (n ¼ 73), Pace (n ¼ 26), Movements (n ¼ 22), and Unhelpful (n ¼ 20).
To examine whether providing choice of animated agent impacted characteristics favored or disliked by the students, 2 (choice or no
choice)  2 (noted or not noted) chi-square analyses were conducted on each category. The results of these chi-square analyses are reported
in Tables 8 and 9. Results suggested that providing choice did not signiﬁcantly impact students’ least liked characteristics of the program
generally. More students in the choice condition reported that the pace of the instruction was their favorite characteristic, compared to the
no choice condition. Concerning agent characteristics, results showed that signiﬁcantly more choice condition participants noted agent
personality as their favorite characteristic, whereas the no choice participants noted agent movements as their favorite characteristic. More
of the choice participants indicated that the examples or explanations provided by the agent were their least liked characteristic of the
agent, compared to the no choice condition.
4.3. Summary of ﬁndings
Findings from the second experiment showed, similar to the preliminary study, that when given a choice of animated agents, young
students will select a young agent that matches their gender. Analyses did not indicate signiﬁcant effects of providing choice of agent on
near transfer, but far transfer scores were higher for the choice students than the non-choice students. Furthermore, when provided a

48

G. Ozogul et al. / Computers & Education 67 (2013) 36–50

Table 8
Experiment 2: frequencies of participants who noted program characteristics best and least liked, by condition.
Liked best

Choice
Agent
Agent speech
Computer-based
Difﬁculty Level
Examples/Explanations
Formulas
Fun/Boring
Graphics
Interactivity
Modality
Questions
Pace
Topic
Other

No choice (n ¼ 164)

Choice (n ¼ 170)

–
28
2
3
7
16
17
6
42
4
4
2
1
20
–

5
34
3
4
9
9
20
4
34
2
3
3
8
24
5

c2(1), p

Liked least

N/A
0.47, 0.49
0.17, 0.68
0.11, 0.74
0.19, 0.66
2.40, 0.12
0.17, 0.68
0.49, 0.48
1.49,0.22
0.75, 0.39
0.11, 0.74
0.17, 0.68
5.34, 0.02
0.27, 0.60
4.90, 0.03

–

No choice (n ¼ 164)
9
12
–
17
6
7
6
13
–
5
8
23
2
8

c2(1), p
Choice (n ¼ 170)
1
5
5
–
16
7
12
8
11
2
1
4
24
3
9

0.97,
1.35,
3.31,
N/A
0.09,
0.05,
1.21,
0.23,
0.27,
1.94,
2.87,
1.54,
0.00,
0.17,
0.03,

0.33
0.25
0.07
0.77
0.83
0.27
0.63
0.61
0.16
0.09
0.22
0.97
0.68
0.86

Bold typeface indicates signiﬁcant difference at the .05 level.

choice, students who selected an older agent scored higher on far transfer items than those who selected a young agent. Also, students who
selected an opposite-gendered agent scored higher on far transfer than those who selected a same-gendered agent. An analysis of individual
differences indicated that students who selected older agents and opposite-gendered agents were older than their counterparts who chose
young, same-gendered agents. Program ratings, graphic ratings, and difﬁculty ratings did not differ among the experimental conditions.
Qualitative analysis of participants’ feelings toward the program and agents revealed that: 1) students in the choice condition more often
noted instructional pace as the favorite aspect of the program, agent personality as favorite aspect of the agent, and examples/explanations
as least favorite aspect of the agent; and 2) students in the no choice condition more often noted agent movements as favorite aspect of the
agent.
5. Discussion
The results from the preliminary study and Experiment 2 show that middle-school learners, when given a choice in animated pedagogical agent, will select a young agent that matches their gender. These ﬁndings support the similarity attraction hypothesis in learners’
preferences for animated agents in computer-based learning environments (Byrne & Nelson, 1965). There is increasing evidence that
younger students have a greater likelihood of selecting agents of the same gender. Eighty-nine percent of the middle-school participants
from experiment 2 selected agents in this manner. The proportion of high school students from Kim and Wei’s experiment (2011) who chose
a gender-matched agent was lower (79% of all participants). In Moreno and Flowerday (2006), which used college-aged students, the
difference in the proportion of participants who selected a gender-matched agent and those who selected an opposite-gendered agent was
not signiﬁcant. This variation in preferences for students of different developmental levels may signify that the optimal agent to be used in
computer-based learning environments depends on the age of the student. More research is needed to determine how student age interacts
with the external and internal properties of an animated agent.
Although our results show students will select agents with similar characteristics to them, analysis of posttest scores do not support the
hypothesis that matched-gender agents lead to better immediate learning outcomes. In Experiment 1, posttest scores were not signiﬁcantly
different between matched- and opposite-gendered experimental conditions. Furthermore, non-choice learners in Experiment 2 did not
Table 9
Experiment 2: frequencies of participants who noted agent characteristics best and least liked, by condition.

c2(1), p

Liked best
No choice (n ¼ 164)
Agent is female
Agent is male
Helpful/unhelpful
Distracting
Examples/explanations
Fun/Boring
Image
Movements
Pace
Personality
Pointing
Realism
Relatable
Repetitive
Smart
Agent speech
Other

Choice (n ¼ 170)

1

2

–
33
–
34
2
8
7
3
1
14
4
1
1
–
22
19

–
46
–
37
6
12
–
7
8
10
1
6
3
3
15
11

Bold typeface indicates signiﬁcant difference at the .05 level.

0.30, 0.58
N/A
2.22, 0.14
N/A
0.05, 0.82
1.91, 0.17
0.71, 0.40
7.41, 0.006
1.51, 0.22
5.34, 0.02
0.88, 0.35
1.94, 0.16
3.47, 0.06
0.94, 0.33
2.92, 0.09
1.79, 0.18
2.67, 0.10

c2(1), p

Liked least
No choice (n ¼ 164)

Choice (n ¼ 170)

2
–
9
1
2
7
11
10
14
1
2
1
–
2
–
34
21

–
–
11
–
9
3
7
12
12
1
3
3
–
5
–
39
15

2.09, 0.15
N/A
0.14, 0.71
1.04, 0.31
4.35, 0.04
1.80, 0.18
1.10, 0.29
0.13, 0.72
0.25, 0.61
0.00, 0.97
0.17, 0.68
0.94, 0.33
N/A
1.21, 0.27
N/A
0.24, 0.62
1.38, 0.24

G. Ozogul et al. / Computers & Education 67 (2013) 36–50

49

learn more from the matched-gendered agents. These results reﬂect similar ﬁndings from earlier experiments; studies thus far have not
found signiﬁcant learning beneﬁts of gender matching (Baylor & Kim, 2003; Behrend & Thompson, 2011; Moreno & Flowerday, 2006).
Although immediate learning beneﬁts of matching the gender of the agent to the learner have not been demonstrated, to this point, the
effect of gender matching has not been tested in settings in which learners can use learning environment(s) repeatedly. Since young
students show a strong preference to matched-gender agents, they may be more likely to re-engage with a learning environment that fulﬁlls
this preference.
Results from the ﬁrst experiment indicate that, in general, male students had signiﬁcantly higher positive perceptions of the program
than female students. However, an important ﬁnding from Experiment 1 is that female students’ perception of instruction can be positively
impacted when provided a same-gendered agent; females had higher program ratings when the APA matched their gender than when a
male agent was presented. Female students may perceive the program using a same-gendered agent more favorably than when using
opposite-gendered agents because of gender stereotypes concerning women in engineering (Byrne, 1993; Capobianco, Diefes-Dux, Mena, &
Weller, 2011; Johnson, Ozogul, Moreno, & Reisslein, 2013; Knight & Cunningham, 2004). The use of a peer-model to explain engineering
problem solving to female students may increase the female learners’ feelings of self-efﬁcacy toward engineering. This ﬁnding is important
because, although gender matching has not been shown to have a signiﬁcant impact on learning outcomes, if this manipulation impacts
female learners’ subjective perceptions of the learning environment, they may be more likely to persist within the environment or, more
generally, persist in studying engineering or mathematics in the future.
Experiment 2 showed a beneﬁcial effect of agent choice on learning outcomes. Learners in the choice condition had signiﬁcantly higher
far transfer scores than the learners in the no choice condition, but no signiﬁcant differences were found for near transfer. Examination of
the descriptive statistics for total near transfer scores suggests a ceiling effect; for this set of participants, performance was very high on near
transfer and there was little variability in the scores.
We conclude that providing choice of animated agent in educational technology can positively impact student motivation and learning in
three ways. First, allowing students to select an animated agent for instruction represents learner control, such that choice condition
participants experience greater feelings of autonomy, leading to higher motivation and self-efﬁcacy during the task and ultimately better
learning outcomes (Bandura, 2001; Behrend & Thompson, 2012; Clark & Mayer, 2011; Ryan & Deci, 2000). Second, participants in the choice
condition may feel some amount of responsibility for the success of a learning environment which they perceive as partially ‘developed’ by
themselves, even to a small degree (i.e., selecting the agent used in instruction). This perceived responsibility for the success of the learning
materials can further promote motivation and learning outcomes. Third, learner control in instruction may simply make learners more
satisﬁed with the task, also improving motivation and learning. An interesting future direction for research on agent choice is to offer
students an array of choices related to multiple internal and external properties of APA (e.g., gender, age, personality, speech rate, and
clothing). Findings from the preliminary study indicated that middle-school students had strong preferences in these categories.
Analyses from Experiment 2 ﬁrst seemed to show that older agents and opposite-gendered agents led to better learning than younger
agents or matched-gendered agents. However, further inspection of the data showed that older agents and opposite-gendered agents
resulted in higher far transfer scores only for the choice condition participants. We noted that choice condition participants who selected
older or opposite-gendered agents were signiﬁcantly older than their counterparts. This suggests that these students may be more mature
and thus more interested in the opposite gender and in learning from ‘instructors’ that appear older and more knowledgeable about the
content. This same maturity level may also be associated with better student focus on instruction and better learning outcomes.
5.1. Conclusions
In sum, this investigation provides additional support for the similarity attraction hypothesis in terms of learners’ perceptions of the
instructional experience while suggesting a caveat. Consistent with this hypothesis, female students in Experiment 1 perceived the
instructional experience more positively when the APA matched their gender. However, the hypothesis was not supported by the ﬁndings
that male students actually had descriptively higher ratings when the APA did not match their gender, than when the APA matched. This
suggests that the hypothesis may need to take into consideration a students’ gender.
The results from Experiment 2 support the assumption that providing a greater degree of learner control in learning technologies can
improve student motivation and learning by increasing perceived autonomy. We further suggest that providing learner choice may elicit a
feeling of responsibility for the success of the learning materials and an enhanced feeling of satisfaction toward the learning task; both of
these factors have potential to increase student motivation and learning. Practically, ﬁndings from Experiment 2 provide further evidence
that computer-based learning environments should include features which increase learner control (Clark & Mayer, 2011). More speciﬁcally,
these results suggest that when such environments involve animated pedagogical agents, students should be offered choice of animated
agent to be used in instruction.
References
Allen, K., Reed-Rhoads, T., Terry, R. A., Murphy, T. J., & Stone, A. D. (2008). Coefﬁcient alpha: an engineer’s interpretation of test reliability. Journal of Engineering Education,
97(1), 87–94.
Arroyo, I., Woolf, B. P., Royer, J. M., & Tai, M. (2009). Affective gendered learning companion. In International conference on artiﬁcial intelligence and education). Brighton,
England: IOS Press.
Atkinson, R. K. (2002). Optimizing learning from examples using animated pedagogical agents. Journal of Educational Psychology, 94(2), 416–427.
Azevedo, R., Witherspoon, A., Graesser, A., McNamara, D., Chauncey, A., Siler, E., et al. (2009). MetaTutor: analyzing self-regulated learning in a tutoring system for biology. In
V. Dimitrova, R. Mizoguchi, B. Du Boulay, & A. C. Graesser (Eds.), Artiﬁcial intelligence in education: Building learning systems that care: From knowledge representation to
affective modeling (pp. 635–637). Amsterdam: IOS Press.
Bandura, A. (2001). Social cognitive theory: an agentic perspective. Annual Review of Psychology, 52, 1–26.
Baylor, A. L. (2009). Promoting motivation with virtual agents and avatars: role of visual presence and appearance. Philosophical Transactions of the Royal Society B, 364,
3559–3565.
Baylor, A. L. (2011). The design of motivational agents and avatars. Educational Technology Research and Development, 59(2), 291–300.
Baylor, A. L., & Kim, Y. (2003). The role of gender and ethnicity in pedagogical agent perception. In G. Richards (Ed.), Proceedings of world conference on e-learning in corporate,
government, healthcare, and higher education 2003 (pp. 1503–1506). Chesapeake, VA: AACE.

50

G. Ozogul et al. / Computers & Education 67 (2013) 36–50

Behrend, T. S., & Thompson, L. F. (2011). Similarity effects in online training: effects with computerized trainer agents. Computers in Human Behavior, 27, 1201–1206.
Behrend, T. S., & Thompson, L. F. (2012). Using animated agents in learner-controlled training: the effects of design control. International Journal of Training and Development,
16, 263–283.
Bradshaw, J. M. (1997). Software agents. Cambridge, MA: MIT Press.
Brophy, S., Klein, S., Portsmore, M., & Rogers, C. (2008). Advancing engineering education in P-12 classrooms. Journal of Engineering Education, 97(3), 369–387.
Byrne, E. M. (1993). Women and science: The Snark Syndrome. London: Falmer Press.
Byrne, D., & Nelson, D. (1965). Attraction as a linear function of proportion of positive reinforcements. Journal of Personality and Social Psychology Bulletin, 4, 240–243.
Capobianco, B. M., Diefes-Dux, H. A., Mena, I., & Weller, J. (2011). What is an engineer? Implications of elementary school student conceptions for engineering education.
Journal of Engineering Education, 100(2), 304–328.
Carr, R. L., Bennet, L. D., & Strobel, J. (2012). Engineering in the K-12 STEM standards of the 50 U.S. States: an analysis of presence and extent. Journal of Engineering Education,
101(3), 1–26.
Cassell, J., Sullivan, J., Prevost, S., & Churchill, E. (Eds.). (2000). Embodied conversational agents. Cambridge, MA: MIT Press.
Choi, S., & Clark, R. E. (2006). Cognitive and affective beneﬁts of an animated pedagogical agent for learning English as a second language. Journal of Educational Computing
Research, 34(4), 441–466.
Clark, R. E., & Choi, S. (2005). Five design principles for experiments on the effects of animated pedagogical agents. Journal of Educational Computing Research, 32(3), 209–225.
Clark, R. C., & Mayer, R. E. (2011). ELearning and the science of instruction; Proven guidelines for consumers and designers of multimedia learning (3rd ed.). San Francisco: Pfeiffer.
Craig, S. D., Gholson, B., & Driscoll, D. M. (2002). Animated pedagogical agents in multimedia educational environments: effects of agent properties, picture features, and
redundancy. Journal of Educational Psychology, 94(2), 428–434.
Dehn, D. M., & van Mulken, S. (2000). The impact of animated interface agents: a review of empirical research. International Journal of Human-Computer Studies, 52, 1–22.
Graesser, A. C., Lu, S., Jackson, G. T., Mitchell, H. H., Ventura, M., Olney, A., et al. (2004). AutoTutor: a tutor with dialogue in natural language. Behavioral Research Methods,
Instruments, and Computers, 36, 180–193.
Heidig, S., & Clarebot, G. (2011). Do pedagogical agents make a difference to student motivation and learning? Educational Research Review, 6, 27–54.
Isbister, K., & Nass, C. (2000). Consistency of personality in interactive characters: verbal cues, non-verbal cues, and user characteristics. International Journal of HumanComputer Studies, 53(2), 251–267.
Johnson, A. M., Ozogul, G., Moreno, R., & Reisslein, M. (2013). Pedagogical agent signaling of multiple visual engineering representations: the case of the young female agent.
Journal of Engineering Education, 102(2).
Kim, Y., & Baylor, A. L. (2006). A social-cognitive framework for pedagogical agents as learning companions. Educational Technology Research &Development, 54(6), 569–596.
Kim, Y., Baylor, A. L., & Shen, E. (2007). Pedagogical agents as learning companions: the impact of agent emotion and gender. Journal of Computer Assisted Learning, 23, 220–234.
Kim, Y., & Wei, Q. (2011). The impact of learner attributes and learner choice in an agent-based environment. Computers & Education, 56, 505–514.
Knight, M., & Cunningham, C. (2004). Draw an Engineer Test (DAET): development of a tool to investigate students’ ideas about engineers and engineering. ASEE Annual
Conference Proceedings, pp. 4079–4089.
Lee, K. M., Liao, K., & Ryu, S. (2007). Children’s responses to computer-synthesized speech in educational media: gender consistency and gender similarity effects. Human
Communication Research, 33, 310–329.
Lester, J. C., Converse, S. A., Kahler, S. E., Barlow, S. T., Stone, B. A., & Bhogal, R. S. (1997). The persona effect: affective impact of animated pedagogical agents. In Proceedings of
the SIGCHI Conference on Human Factors in Computing Systems (pp. 359–366). New York, NY: ACM.
Mayer, R. E. (1989). Systematic thinking fostered by illustrations in scientiﬁc text. Journal of Educational Psychology, 81(2), 240–246.
Mayer, R. E. (2005). Cognitive theory of multimedia learning. In R. E. Mayer (Ed.), The Cambridge handbook of multimedia learning (pp. 31–48). New York: Cambridge University
Press.
Mayer, R. E. (2008). Applying the science of learning: evidence-based principles for the design of multimedia instruction. American Psychologist, 63(8), 760–769.
Mitrovic, A., & Suraweera, P. (2000). Evaluating an animated pedagogical agent. Lecture Notes in Computer Science, 1839, 73–82.
Moon, Y., & Nass, C. (1998). Are computers scapegoats? Attributions of responsibility in human–computer interaction. International Journal of Human-Computer Interaction,
49(1), 79–94.
Moreno, R. (2004). Decreasing cognitive load for novice students: effects of explanatory versus corrective feedback in discovery-based multimedia. Instructional Science, 32,
99–113.
Moreno, R. (2005). Multimedia learning with animated pedagogical agents. In R. Mayer (Ed.), The Cambridge handbook of multimedia learning (pp. 507–524). New York:
Cambridge University Press.
Moreno, R., Reisslein, M., & Ozogul, G. (2009). Optimizing worked-example instruction in electrical engineering: the role of fading and feedback during problem-solving
practice. Journal of Engineering Education, 98(1), 83–92.
Moreno, R., Reisslein, M., & Ozogul, G. (2010). Using virtual peers to guide visual attention during learning: a test of the persona hypothesis. Journal of Media Psychology:
Theories, Methods, and Applications, 22(2), 52–60.
Moreno, R., & Flowerday, T. (2006). Students’ choice of animated pedagogical agents in science learning: a test of the similarity attraction hypothesis on gender and ethnicity.
Contemporary Educational Psychology, 31, 186–207.
Moreno, K. N., Klettke, B., Nibbaragandla, K., & Graesser, A. C. (2002). Perceived characteristics and pedagogical efﬁcacy of animated conversational agents. Lecture Notes in
Computer Science: Intelligent Tutoring Systems, 2363, 963–971.
Moreno, R., & Mayer, R. E. (1999). Cognitive principles of multimedia learning: the role of modality and contiguity. Journal of Educational Psychology, 91(2), 358–368.
Moreno, R., Mayer, R. E., Spires, A. H., & Lester, J. C. (2001). The case of social agency in computer-based teaching: do students learn more deeply when they interact with
animated pedagogical agents? Cognition and Instruction, 19, 177–213.
Nass, C., & Lee, K. M. (2001). Does computer-synthesized speech manifest personality? Experimental tests of recognition, similarity-attraction, and consistency-attraction.
Journal of Experimental Psychology: Applied, 7(3), 171–181.
Ozogul, G., Johnson, A. M., Moreno, R., & Reisslein, M. (2012). Technological literacy learning with cumulative and stepwise integration of equations into electrical circuit
diagrams. IEEE Transactions on Education, 55, 480–487.
Ozogul, G., Reisslein, M., & Johnson, A. M. (2011). Effects of visual signaling on pre-college students’ engineering learning performance and attitudes: peer versus adult
pedagogical agents versus arrow signaling. In Proceedings of the 118th annual conference and exposition of the American Society for Engineering Education.
Pearson, G., & Young, A. T. (2002). Technically speaking: Why all Americans need to know more about technology. Washington, DC: Nat. Acad. Press.
Plant, E. A., Baylor, A. L., Doerr, C. E., & Rosenberg-Kima, R. B. (2009). Changing middle-school students’ attitudes and performance regarding engineering with computerbased social models. Computers & Education, 53, 209–215.
Pratt, J. A., Hauser, K., Ugray, Z., & Patterson, O. (2007). Looking at human-computer interface design: effects of ethnicity in computer agents. Interacting with Computers, 19(4),
512–523.
Reeves, B., & Nass, C. (1996). The media equation: How people treat computers, television, and new media like real people and places. Cambridge, MA: Cambridge University Press.
Reisslein, J., Johnson, A. M., Bishop, K. L., Harvey, J., & Reisslein, M. (2013). Circuits Kit K-12 Outreach: impact of circuit element representation and student gender. IEEE
Transactions on Education, 56.
Reisslein, M., Moreno, R., & Ozogul, G. (2010). Pre-college electrical engineering instruction: the impact of abstract vs. contextualized representation and practice on learning.
Journal of Engineering Education, 99(3), 225–235.
Rosenberg-Kima, R. B., Baylor, A. L., Plant, E. A., & Doerr, C. E. (2008). Interface agents as social models for female students: the effects of agent visual presence and appearance
on female students’ attitudes and beliefs. Computers in Human Behavior, 24(6), 2741–2756.
Rosenberg-Kima, R. B., Plant, A., Doerr, C. E., & Baylor, A. L. (2010). The inﬂuence of computer-based model’s race and gender on female students’ attitudes and beliefs towards
engineering. Journal of Engineering Education, 99(1), 35–44.
Ryan, R. M., & Deci, E. L. (2000). Self-determination theory and the facilitation of intrinsic motivation, social development, and well-being. American Psychologist, 55(1), 68–78.
Ryu, J., & Baylor, A. L. (2005). The psychometric structure of pedagogical agent persona. Technology, Instruction, Cognition and Learning, 2, 291–314.
Stemler, S. E. (2004). A comparison of consensus, consistency, and measurement approaches to estimating interrater reliability. Practical Assessment, Research & Evaluation, 9(4).
Van der Meij, H., Van der Meij, J., & Harmsen, R. (2012). Animated Pedagogical Agents: Do they advance student motivation and learning in an inquiry learning environment?.
Technical Report TR-CTIT-12-02 Enschede: Centre for Telematics and Information Technology University of Twente.
van Vugt, H. C., Bailenson, J. N., Hoorn, J. F., & Konijn, E. A. (2010). Effects of facial similarity on user responses to embodied agents. ACM Transactions on Computer–Human
Interaction, 17(2), 1–27.
Woo, H. L. (2009). Designing multimedia learning environments using animated pedagogical agents: factors and issues. Journal of Computer Assisted Learning, 25.

Development of the Science Technology Engineering and Mathematics – Active
Listening Skills Assessment (STEM-ALSA)
Kerrie G. Wilkins, Bianca L. Bernstein
Counseling and Counseling Psychology Program
Arizona State University
Tempe, AZ USA

Caroline J. Harrison
CareerWISE Project Manager/Post-Doctoral Researcher
Arizona State University
Tempe, AZ, USA

Jennifer M. Bekki
Department of Engineering
Arizona State University
Mesa, AZ USA

Robert K. Atkinson
School of Computing, Informatics, and Decision Systems
Engineering
Arizona State University
Tempe, AZ USA

Abstract —The purpose of this investigation was to develop the
STEM Active Listening Skills Assessment (STEM-ALSA), a
conceptually grounded instrument designed to measure four
components of active listening, a key element of communication
in an academic setting. The STEM-ALSA is comprised of three
unique scales that measure a person’s knowledge (12 items),
ability to apply (25 items), and self-efficacy (5 items) with respect
to active listening. Two pilot studies were conducted with N = 99
upper level undergraduate students enrolled in STEM disciplines
to develop and evaluate the instrument. Results of an exploratory
factor analysis identified both a unidimensional factor structure
for each of the three scales and total scores with adequate
internal consistency reliability estimates. The STEM-ALSA
provides a mechanism for measuring active listening skills among
students in STEM.
Keywords-Assessment; communication; graduate students in STEM

I.

INTRODUCTION

Effective communication skills are an essential commodity
in today’s workplace. In engineering, technology, and the
sciences, communicating disciplinary knowledge to the public
is considered critical. Interpersonal communication skills
(ICS) are also recognized as key transdisciplinary capabilities
necessary for career success [1]. For example, an estimated
50-75% of scientists’ work involves communicating with
others individually, in small groups, and in teams [2]. The
Engineer of 2020 report [3] and other researchers [4], [5], [6]
have also highlighted the fact that communication skills will
become increasingly important as engineers are required to
communicate in globally diverse and interdisciplinary teams.
However, despite the expressed importance of these
communication skills, numerous research studies, reports from
industrial recruiters, and anecdotal evidence by educators have
indicated that graduates of science, technology, engineering,
and mathematics (STEM) disciplines are inadequately
equipped to communicate effectively in the workplace [7].
This leads organizations such as The Society for
Manufacturing Engineers to list “lack of communication
skills” among the top “competency gaps” in engineers’
education [8].
In addition to plaguing recent graduates of STEM
disciplines who work in industry, inadequate training in ICS at
This work was supported by the National Science Foundation, Grants
0634519 and 0910384. Any opinions, findings and conclusions and
recommendations expressed in this material are those of the authors and do
not necessarily reflect the views of the National Science Foundation.
978-1-4673-1352-0/12/$31.00 ©2012 IEEE

the undergraduate level has implications for graduate studies
in STEM. It has been stated that “science these
days…increasingly draws on skills in written and oral
communication with scientists and non-scientists alike” [9].
For the most part, however, the oral communications that are
central in students’ daily practices are conversational and
informal. Additionally, students who are able to communicate
their needs to their graduate program are more likely to
complete their degrees [10]. In fact, recent research suggests
that graduate students who take a more active role in
developing ICS are more successful in graduate school [11],
[12].
An ICS identified as a particularly important element for
communicating across disciplines is “active listening” [13].
Active, or empathic, listening can be traced to Carl Rogers
[14] and is a cornerstone of his humanistic psychology [15].
Since its introduction, active listening skills have been found
to play an important role in effective communication and have
now become a mainstay of communication training programs
across a variety of fields [16].
In addition to being widely recognized as an important
interpersonal communication skill, active listening has also
been shown to be a teachable skill. For example, research has
shown that the listening skills of counseling students [17] and
helpline volunteers [18] improved with active listening
training. Moreover, the effects of such training continued for
at least nine months in the case of the helpline volunteers [19].
Yet, despite the importance of the skill and the fact that it has
been shown to be something that can be learned, there are few
empirically validated instruments available to measure active
listening skills.
This paper discusses the development of the STEM
Communication Skills Assessment (STEM-ALSA), which is
comprised of three unique measures: knowledge of, self
efficacy in the domain of, and skill in the application of active
listening. The context of all the items in the instrument is the
advisor-advisee relationship. Advising is at the heart of the
institutional and interpersonal structures that make up graduate
education [20]. Consequently, it is imperative that the advisoradvisee relationship takes on a supportive stance. This is
especially the case for female doctoral students, for whom the
graduate program milieu is often described as a “chilly

climate” [21], [22]. The STEM-ALSA was developed for a
specific study within the CareerWISE research program. In
the following sections, we provide a brief description of
CareerWISE to help situate the motivation for the instrument
development, outline the process of instrument development,
and describe the scales within the instrument in more detail.
II.

CareerWISE RESEARCH PROGRAM

The CareerWISE research program is a large, NSFfunded, multidisciplinary research program housed at Arizona
State University. The program strives to both understand the
reasons that women in STEM doctoral programs drop out and
to develop and disseminate a resource to strengthen key
personal and interpersonal skills so that women will be better
equipped to persist in their doctoral degree programs. Built on
an extensive foundation of theory and research, the
CareerWISE resource (http://careerwise.asu.edu) is an online
resilience training program designed to address the personal
and interpersonal challenges of women in science and
engineering fields by strengthening their personal assets and
supports [23]. Key objectives of the resilience training
program are to enhance the communication skills of doctoral
women and to improve interpersonal problem solving skills.
The CareerWISE resource is unique in that it is an
individualized program that pairs empirically based
pedagogical materials with an interactive simulation
environment designed to hone users’ ICS skills. It is the first
program of its kind to provide systematic training in ICS
customized for female students in STEM. Consequently,
instruction in active listening skills is an essential building
block in the ICS within the CareerWISE program.
III.

OVERVIEW OF THE THE INSTRUMENT

The development of the STEM-ALSA was underscored by
the following definition of active listening: “active listening
requires that the listener try to understand the speaker's own
understanding of an experience without the listener's own
interpretive structures intruding on his or her understanding of
the other person” [13, p. 35]. The goal in active listening is to
develop a clear understanding of the speaker’s concern and
also to clearly communicate the listener’s interest in the
speaker’s message [18]. The CareerWISE team specified the
following four sub skills for active listening and defined them
as indicated in Table I: asking open ended questions, listening
for critical information, communicating via nonverbal cues,
and perception checking. Items in the instrument were
specifically included to assess each of these four elements.
The STEM-ALSA is comprised of three unique scales. The
Knowledge Assessment scale measures the respondent’s selfreported current knowledge of active listening. The following
are two sample items from this scale: “I know how to restate a
speaker’s message to verify my understanding” and “I know
how to convey nonverbally that I am interested in what the
other person is saying.” Response options for this scale are
arrayed on a 5-point Likert scale ranging from 1 (strongly
disagree) to 5 (strongly agree).
The Self Efficacy Assessment scale is designed to measure
an individual’s confidence in her own ability to use active
listening skills in an academic setting. Two sample items from

this scale are: “I can detect the important messages in a
conversation with professors,” and “I can ensure that I have
understood the speaker’s message.” Response options for this
scale are also arrayed on a 5-point Likert scale, ranging from 1
(strongly disagree) to 5 (strongly agree).
TABLE I. DESCRIPTION OF ACTIVE LISTENING SUB-SKILLS
Asking open-ended
questions
Attending to nonverbal
cues

Questions that are broadly framed to encourage
elaboration and allow for responses other than
yes or no.
Observing the speaker’s nonverbal cues (e.g.,
facial expressions, hand gestures, posture, etc.)
for information about the meaning of the
speaker’s message.

Listening for critical
information

Identifying the main points of a speaker’s
message.

Perception checking

Ensuring that the listener understands the
speaker's message by paraphrasing the listener’s
interpretation of the speaker’s feelings and
message content.

The third scale, Skills Assessment, measures the
participant’s ability to actually apply her active listening skills.
Skills are often measured using an observation rating approach
[24]. However, actual observation can be costly and time
intensive. To get around this issue, the STEM-ALSA uses a
self-report format to measure skill application. In this section
of the instrument, a series of scenarios is presented, modeling
situations that could realistically occur for a female doctoral
student in STEM. Each scenario includes a stated goal for the
student. Following each scenario are four responses, each
corresponding to a particular course of action that could assist
or hinder the student in achieving her desired outcome.
Participants are asked to rate the likelihood of achieving the
desired goal for each action on a five-point Likert scale with
response options ranging from 1 (very unlikely) to 5 (very
likely). Figure I gives a sample scenario and its corresponding
items.
A month ago, Dr. Simpson asked Sarah to organize an informal bi-weekly
meeting at which graduate students from the department would present their
research to each other. Today is the first of those meetings, and Sarah is
presenting her own research to kick things off. During her presentation, she
notices that Dr. Simpson appears to be falling asleep and is not paying
attention to what she is saying. For each of the following actions, indicate
how likely it is to assist Sarah in getting his attention.
1.
2.
3.
4.

Pause for a moment to convey nonverbally that she is waiting for
his attention.
Talk as usual while ignoring his lack of interest.
Catch Dr. Simpson’s gaze when he looks up.
Speak with more animation and address him by name.
Figure I: Sample item from the Skill Assessment scale

IV.

INSTRUMENT DEVELOPMENT METHOD AND RESULTS

The STEM-ALSA instrument was developed in three
phases. The first phase consisted of initial item development
and expert feedback, the second phase involved piloting and
then modifying the instrument based on expert feedback (pilot
study #1), and the third phase included a second pilot and
associated modifications (pilot study #2).

A. Phase 1 – Initial Item Development
Initially, 16 items were written for the Knowledge
Assessment scale, 32 items for the Skills Assessment scale,
and 8 items for the Self Efficacy Assessment scale. Items were
developed using the literature on active listening and were
examined by an interdisciplinary research team consisting of
students and faculty from the disciplines of counseling
psychology, engineering, and educational technology. To
assess content validity, three experts in psychological
measurement and interpersonal communication provided
open-ended comments on the original items in each of the
three scales. The experts rated each item on content
appropriateness and clarity/readability using a 5-point scale
that ranged from 1 (not at all appropriate or clear) to 5 (very
appropriate or clear). Based on their feedback, several items
were revised for clarification. Two of the original content
experts then re-evaluated the three revised measures for
appropriateness and clarity.
B. Phase 2 – Pilot Study #1 Methods
The purpose of pilot study #1 was to examine the initial
factor structures of the items in each of the three scales in the
STEM-ALSA instrument. During the pilot, undergraduate
women majoring in mathematics, sciences, and engineering at
a large southwestern public university were recruited through
university organizations and contacts within the departments.
Participants in the study were given a short online introduction
to communication skills and then were presented with the
instrument so that their active listening skills could be
assessed. Demographic information was also collected from
participants.
A total of 72 participants (primarily juniors and seniors
with an average age of 21.6) completed the anonymous online
pilot study. The majority of participants reported that they
were US citizens with English as their primary language, and
over 90% of those who responded stated they had an advisor
with whom they interacted. Participants who completed the
study were given a $25 gift card redeemable at the university
bookstore.
Using the participant responses, a principal-components
analysis was performed on the items in the STEM-ALSA
instrument. A separate analysis was performed for each of the
three scales. Of note is that the responses in the Skills Section
were scored in a unique way. First, three experts were asked to
score each item for how likely it would be to produce the
desired outcome. The mean of those values was recorded as
the final “best answer.” Participant scores, then, were recoded
based on this best answer. Participant responses that were in
the same direction as the best answer (i.e., above or below the
score of “3”, the midpoint of the scale) were scored as “1,”
and those that were not in the direction of the best answer
were scored as “0.” For example, if the best answer based on
for an item based on expert scoring was 4.33, and a
participant’s score was 5 (also above a score of “3”), then the
participant’s score was recoded to be a “1.” On the other hand,
if the participant’s score was “2”, which is in opposite
direction of the best answer, the participant score was recoded
to be a “0.”

C. Phase 2 – Pilot Study #1 Results
Analyses of each of the three scales in the STEM-ALSA
indicated that a one-factor solution was most interpretable. In
the Knowledge Assessment scale, the factor structure
accounted for 33.31% of the variance. All 16 items loaded at
or above 0.40, and the internal consistency of the scale was
good, with a Cronbach’s alpha [25], of 0.84. In the SelfEfficacy scale, three of the eight items were deleted, as the
factor only accounted for a small amount of their unique
variance. After excluding these three variables, the structure of
this scale was reanalyzed. The revised structure accounted for
62% of the total variance and was comprised of five items that
loaded above 0.70. Cronbach’s alpha [25] for the total score of
the one factor solution in the Self Efficacy scale was found to
be 0.85. The results were not quite as good for the Skills
Assessment scale. The one-factor solution in that scale, while
most interpretable, only accounted for 14 % of the total
variance. The Cronbach’s alpha coefficient [25] for the total
score of the one factor solution in the Skills Assessment scale
was also only 0.38.
D. Phase 3 – Pilot Study #2 Methods
After completing pilot study #1, the three experts in the
areas of interpersonal communications and psychometrics
were again consulted to examine the items of the three scales,
paying particular attention to the Skills Assessment scale.
Based on their analysis, items in the Skills Assessment scale
for which the experts disagreed on the general direction of the
best response were dropped (e.g., when two of the three
experts thought it was generally a good course of action, and
one didn’t).
Following the expert input, a second pilot study was
conducted to further examine the factor structure of the
revised STEM–CSA. For this pilot, undergraduate students in
an introductory computer informatics course at a large
southwestern public university were recruited. As in the first
pilot study, participants in this pilot were given a short online
introduction to communication skills and then were presented
with the instrument so that their active listening skills could be
assessed. Demographic information was also collected.
Students’ majors included liberal arts and communication, as
well as computing. A total of 27 participants (primarily
sophomores and juniors, with an average age of 21.3)
completed the second pilot study. The majority of participants
reported that they were US citizens with English as their
primary language, and over 76% of those who responded
stated they had an advisor with whom they interacted.
Participants who completed the study received course credit
for their participation.
E. Phase 2 – Pilot Study #2 Results
Analysis of the Knowledge Assessment scale in pilot study
#2 supported the one-factor solution. However, the factor
solution accounted for less that 50% of the individual variance
for four of the 16 items. Consequently, these four items were
excluded from the instrument. The revised structure accounted
for 56% of the total variance and included 12 items that loaded
above 0.50 (See Table II.). Cronbach’s alpha for the total
score of the one factor solution was found to be 0.93.

A one-factor solution in the second analysis of the Self
Efficacy Assessment was also robust and conceptually sound.
The measure accounted for 69% of the total variance, and
included 5 items that loaded above 0.70 (See Table III.). The
alpha coefficient for the total score of the one factor solution
was found to be 0.88.
Based on expert suggestion and the statistical results of
pilot study #1, a number of items were deleted and/or revised
in the skills-assessment section. Analysis of the revised, 25
item STEM- Skills Assessment indicated that a one-factor
solution again yielded the most interpretable solution. This
factor structure accounted for 16% of the variance. The alpha
coefficient for the total score of the one factor solution was
found to be 0.52. Factor loadings ranged from 0.04 to 0.72.
Therefore no support was found for this scale. As such, further
revision is necessary.
V.

CONCLUSION

The development and initial validation of the STEMALSA is an important preliminary step in filling the gap of
empirically-validated instruments for measuring active
listening. The scales, Knowledge Assessment, Self Efficacy
Assessment, and Skills Assessment were designed to measure
a respondent’s perceived knowledge, self efficacy, and ability
to apply active listening skills respectively.
The results indicated that two of the three scales in the
instrument, Knowledge and Self Efficacy, demonstrated high
internal consistency and fit a unidimensional factor solution.
STEM-ALSA also contained a unique section for measuring
the application of active listening skills. In this section, a
scenario based approach was used, representing a novel
approach for measuring the ability to apply active listening
skills. However, the analyses suggest that this section of the
instrument needs further validation and examination of the
factor structure. In future work, we plan to further assess the
scenario based approach of the Skills Assessment scale and
examine the construct validity of the STEM-ALSA scales.
Finally, of note is that although the STEM-ALSA was
developed for use in the context of a specific study, further
studies can evaluate the usefulness of the instrument for
measuring academic communication skills in other
populations (e.g., STEM undergraduate students) as well as
within other professions.
TABLE II. STEM-KNOWLEDGE ASSESSMENT ITEMS, FACTOR LOADINGS,
MEANS, AND STANDARD DEVIATIONS FOR STUDY 2

1.
2.
3.
4.

Items
I know how to quiet my own
thoughts in order to listen
carefully.
I know how to listen and watch
for the main points of a speaker's
message.
I know how to identify the
overarching message even when
other topics come up.
I know how to ask a question that
doesn’t give away the answer I’m
hoping to receive.

5.

I know how to ask questions that
will encourage the other person to
elaborate.
6. I know how to find out more
about the other person’s
perspective.
7. I know how to ensure that I have
understood someone’s point of
view.
8. I know how to restate a speaker’s
message to verify my
understanding.
9. I know how to check whether
what I heard is what the speaker
meant.
10. I know how to convey
nonverbally that I am interested in
what the other person is saying.
11. I know how to recognize when
someone I am conversing with is
distracted.
12. I know how to be consistent in
what I’m saying and how I’m
saying it.

0.76

3.85

1.20

0.75

4.19

.79

0.78

3.96

1.09

0.82

4.19

.92

0.87

3.81

1.04

0.73

4.11

1.05

0.72

4.26

.984

0.71

3.70

1.10

Note.. Scores on individual items in the STEM- CSA Knowledge Assessment scale ranged from
1-5. Total scores ranged from 5-60.

TABLE III. SELF EFFICACY ASSESSMENT ITEMS, FACTOR LOADINGS, MEANS,
AND STANDARD DEVIATIONS FOR STUDY 1

1.
2.

3.
4.
5.

Items
I can detect the important
messages in a conversation with
professors.
I can identify the intended
meaning of a verbal message
even when it is phrased
ambiguously.
I can behave in a manner that is
consistent with how I am feeling.
I can verify my perceptions of
what the other person is telling
me.
I can ensure that I have
understood the speakers’
message.

Factor 1
loadings

M

SD

0.86

3.93

0.92

0.88

4.04

0.85

0.72

3.78

0.97

0.90

3.81

0.88

0.77

3.78

0.85

Note .Scores on individual items in the STEM- CSA Self Efficacy Assessment scale ranged from
1-5. Total scores ranged from 5-25.

ACKNOWLEDGMENT
We would like to thank the experts who reviewed the STEMALSA instrument for their valuable input and suggestions.

Factor 1
loadings

M

SD

0.57

3.93

.96

0.71

4.37

.69

[2]

0.81

3.63

1.08

[3]

0.70

3.96

.90

[4]

REFERENCES
[1]

B. L. Bernstein, et al. (in press). The continuing evolution of the
research doctorate. In M. Nerad & B. Evans (Eds.). Preparing PhDs for a
Global Future: Forces and Form in Doctoral Education Worldwide.
Rotterdam, Netherlands: Sense Publishers.
A. L. Darling, and D. P. Dannels, “Practicing engineers talk about the
importance of talk: A report on the role of oral communication in the
workplace.” Communication Education, vol. 52, pp.1-16, 2003.
The Engineer of 2020: Visions of Engineering in the New Century.
National Academy of Engineering, 2004. National Acadmies Press,
Washington DC.
J. P. Trevelyan, “Reconstructing engineering from practice. Engineering
Studies,” vol. 2, no.3, pp. 175-195, 2010.

[5]

[6]

[7]

[8]

[9]

[10]

[11]

[12]

[13]

[14]

N. Spinks, N. L. J. Silburn, and D. W. Birchall, “Educating Engineers
for the 21st Century: The Industry View.” Henley, England: Henley
Management College, 2006.
N. Spinks, N. L. J. Silburn, and D. W. Birchall, “Making it all work: the
engineering graduate of the future, a UK perspective.” European Journal
of Engineering Education, vol. 32, no. 3, pp. 325-335, 2007.
D.Vest, M. Long, and T. Anderson, “Electrical Engineers’ perceptions
of communication training and their recommendations for curricular
change: Results of a national survey,” IEEE Transactions on
Professional Communication, vol. 39, no. 1, March 1996, pp. 38–42.
E.L. Allen, A.J. Muscat, and E.D.H. Green, “Interdisciplinary Team
Learning in a Semiconductor Processing Course,” Proceedings, 1996
Frontiers in Education Conference, ASEE/IEEE, 1996.
A. Moore, “What you don’t learn at the bench: Conclusions from the
EMBO/ELSF-organized meeting on career prospects in the life
sciences.” EMBO Rep. 3, pp. 1018–1020, 2002.
B. E. Lovitts, Leaving the Ivory Tower: The causes and consequences of
departure from doctoral study. Lanham, MD: Rowman & Littlefield
Publishers, 2001.
T. L. Raoul Tan, and D. Potocnik, “Are you experienced? Junior
scientists should make the most of opportunities to develop skills outside
the laboratory.” EMBO Rep. 7, pp. 961–964, 2006.
E. M. Tomazou & G. T. Powell, “Look who’s talking too: Graduates
developing skills through communication.” Nature Reviews Genetics,
Vol. 8, September 2007.
H.Weger, G. R. Castle, & M. C. Emmett. “Active listening in peer
interviews: The influence of message paraphrasing on perceptions of
listening skill.” The International Journal of Listening, vol. 24, pp. 3449, 2010.
C. R. Rogers, (1951). Client-centered therapy. Boston: HoughtonMifflin.

[15] A. B. Orlov, “Carl Rogers and contemporary humanism.” Journal of
Russian and East European Psychology, 30, pp. 36-41, 1992.
[16] L. O’Shea, R. Algozzine, D. Hammittee, D. O’Shea, Families and
Teachers of Iindividuals with Disabilities: Collaborative Orientations
and Responsive Practices. Boston: Allyn & Bacon, 2000.
[17] D. McNaughton, D. Hamlin, J. McCarthy, D. Head-Reeves, and M.
Schreiner, “Learning to listen, teaching an active learning strategy to
preservice education professionals,” Topics in Early Childhood Special
Education, vol. 27, pp. 223-231, winter 2007.
[18] D. H. Levitt, “Active listening and counselor self-efficacy: Emphasis on
one micro-skill in beginning counselor training.” The Clinical
Supervisor, vol 20, pp. 101-l 15, 2001.
[19] A. Paukert, B. Stagner, and K. Hope, “The assessment of active listening
skills in helpline volunteers.” Stress, Trauma, and Crisis, vol. 7, pp. 6176, 2004.
[20] V. Chapman and T. Sork, “Confessing regulation or telling secrets?
Opening up the conversation on graduate supervision,” Adult Education
Quarterly, vol 51, pp. 94-107, 2001.
[21] S. Prentice, “The conceptual politics of chilly climate controversies”,
Gender and Education, vol. 12, pp. 195-207, 2000.
[22] R. Hall, and B. Sandler, “The classroom climate: A chilly one for
women” Project on the Status and Education of Women, Association of
American Colleges, Washington, DC, 1982.
[23] B. L. Bernstein, “Managing barriers and building supports in science and
engineering doctoral programs: Conceptual underpinnings for a new
online training program for women.” Journal of Women and Minorities
in Science and Engineering, vol. 17, pp. 29-50, 2011.
[24] N. Mishima, H. Kubot, S. Nagata. “The development of a questionnaire
to assess the attitude of active listening.” Journal of Occupational
Health, vol 42, pp. 111-118, 2000.
L.J. Cronbach, “Coefficient alpha and the internal structure of tests.”
Psychometrika,vol. 16, pp. 297-334, 1951.

Computer-Aided Instruction for
Introductory Linear Circuit Analysis
B. J. Skromme*, P. J. Rayes*, C. D. Whitlatch*,
Q. Wang*, A. Barrus**, J. M. Quick**,
R. K. Atkinson**
*School of Electrical, Computer, and Energy Engrg.
**School of Computing, Informatics, and Decision
Systems Engrg.
Arizona State University, Tempe, AZ 85287-5706, USA
skromme@asu.edu
Abstract—A step-based tutoring system for linear circuit
analysis is being developed with the capabilities to automatically
generate circuit problems with specified characteristics,
including randomly generated topologies and element values.
The system further generates fully-worked, error-free solutions
using the methods typically taught in such classes, and accepts a
rich variety of student input such as equations, matrix equations,
numerical and multiple-choice answers, re-drawn circuit
diagrams, and sketches of waveforms. A randomized, controlled
study was conducted using paid student volunteers to compare
the effectiveness of two of our tutorials in comparison to working
conventional textbook-based problems. The average learning
gain was only 3/100 points for the textbook users, but 29/100
points, about 10 times higher, for the tutorial users. The effect
size on the post-test scores was 1.21 pooled standard deviations
(Cohen d-value) and was statistically significant. A motivational
survey administered to these students yielded a 0.53 point higher
rating for the software than for the textbook (on a 1-5 scale). The
system is being used in Spring 2013 by over 340 students in EEE
202 at Arizona State and two community colleges. About 99% of
these students rated the system as “very helpful” or “somewhat
helpful.”
Keywords—linear circuit analysis; computer-aided instruction;
step-based tutoring

I. INTRODUCTION
Linear circuit analysis is a foundational topic for electrical
engineers, but is also widely studied by other engineering
majors. For example, this course is taught to over 600 students
a year at ASU in 11 sections, about 90% of whom are nonmajors. Students frequently struggle with this material, due to
such factors as delayed or inaccurate feedback on their
homework, inadequate use of active and cooperative learning
strategies, an insufficient (from the student’s perspective)
supply of worked examples of carefully graded difficulty, and
failure on the part of instructors to recognize persistent
misconceptions about basic electricity that students frequently
bring into these classes [1-3]. Traditional lecture-based
instruction offers only a single pace to all students, regardless
of prior knowledge and ability, and inevitably leaves some
students bored and others unable to keep up. One method to
This work was supported by the National Science Foundation through the
Transforming Undergraduate Education in Science, Technology, Engineering
and Mathematics Program under Grant No. DUE-1044497.
U.S. Government work not protected by U.S. copyright

T. S. Frank
Engineering Program
South Mountain Community College
Phoenix, AZ 85042, USA

address these issues is to supplement conventional approaches
with computer-aided instruction, which has been employed in a
number of prior studies [4-35]. While useful, many of these
studies have developed only incomplete or partial prototypes,
or have not carried out rigorous evaluations to determine their
effectiveness in increasing student learning. There has been
little sustained, widespread usage of previously developed
systems, and most have not had the ability to generate new
circuit problems, which can be a tedious and error-prone task
when done by hand.
Publisher-based web sites as supplements to textbooks have
been more widely used in recent years. Whereas these sites
provide additional resources to students and can be used to
automate grading, they mainly provide algorithmic problems,
in which some of the element values are varied in a given
problem. They are typically answer-based tutors, which
provide rapid feedback on the correctness of an answer, but do
not accept a sufficient amount or variety of student input to
diagnose the reason for a wrong answer (they lack sufficient
“bandwidth” in the language of intelligent tutoring systems). A
recent meta-analysis by VanLehn concludes that the typical
effect size (Cohen’s d-value) of answer-based tutoring systems
is around 0.31, compared to 0.76 for step-based tutors and 0.79
for expert (expensive) human tutors [36]. Thus, we have
undertaken to develop a potentially more effective step-based
system that can do a better job of analyzing a wide variety of
student inputs [37, 38]. Further, we automate the process of
generating problems and solutions, eliminating both human
error that can result in great frustration to students and the
temptation to misuse solution manuals that are widely available
on the Internet. This type of system can provide rapid, accurate
feedback and an unlimited supply of problems, examples, and
solutions of any desired difficulty and complexity.
II. SOFTWARE DESIGN AND FEATURES
A. Circuit Generation
The algorithms we use to generate circuit problems and
solutions have been described elsewhere in detail [37, 38]. We
generate circuit layouts rather than netlists to ensure that our
circuits are planar so that mesh analysis can be applied. Briefly,

the circuit generation process involves a series of three steps, to
avoid the prohibitive combinatorial issues involved in trying to
place circuit elements in a purely random way. We first
generate a “topology” with the desired number of meshes,
consisting of only shorts or opens placed on a square grid with
the desired number of rows and columns. The generation
algorithm ensures that it is fully connected and has no
“dangling” shorts. It is checked to ensure that it is not
“hinged” (i.e., it cannot be drawn so that two parts of the circuit
are connected by only a single wire, and therefore constitutes
effectively separate circuit problems). In the second step, the
desired number of shorts are replaced by generic circuit
elements, leaving the others as shorts and leaving all opens
permanently as opens. In doing so we place at least two
elements on every mesh including the “outer” mesh around the
circuit periphery, to ensure that no element is shorted and that
there are no meshes of shorts (which would reduce the true
number of meshes below the desired value). We then check
that this “populated topology” has not become hinged as a
result of element placement (which includes shorted elements.
If the result is not acceptable, the process is restarted.
In the third step, the generic elements are replaced with
actual circuit elements of the desired types. To avoid insoluble
(inconsistent) problems, we first find all or many trees of the
populated topology. Voltage sources and inductors are placed
exclusively on the twigs of a randomly selected tree, and
current sources and capacitors are placed exclusively on the
links of that tree, thereby avoiding insoluble problems
involving loops of only voltage sources or stars consisting only
of current sources (resistors can be placed anywhere). The
placement algorithm also allows us to limit the number of
voltage sources in series and the number of current sources in
parallel, and can optionally prohibit passive elements of the
same type in series or parallel with each other. Further, it
optionally avoids creating problems where a given circuit
element is “redundant” and has no significant impact on the
rest of the circuit (such as any element in series with a current
source or in parallel with a voltage source). The algorithm also
has the ability to create a desired (feasible) number of floating
supernodes (i.e., supernodes that do not include the reference
node), through a combination of source repositioning and/or
choice of specific reference nodes. Circuits can also be
automatically selected having a desired number of
supermeshes. The user also has the option to reject circuits
whose node voltages are all controlled directly by voltage
sources, or whose mesh currents are controlled directly by
current sources, since such problems are relatively trivial to
solve. Control variables for dependent sources are randomly
selected in accordance with rules described elsewhere [37].
Element values including dependent source gains are then
randomly generated within specified ranges, and the circuit is
checked and modified as necessary to ensure that it is soluble.
To specify the type of circuit to be generated, the user
specifies the number of squares in the grid in both x and ydirections, the number of nodes, the number of meshes, and the
number of each type of circuit element to be used. However,
any one of the last three quantities is determined by the other
two, so that the user has the choice of which two to specify
[37].

Once a circuit has been generated, the program randomly
selects a user-specified number and type of “sought quantities,”
or values of unknowns for which the student is asked to solve.
These may be branch voltages, branch currents, non-branch
voltages (i.e., voltages that do not appear directly across any
one circuit element), or branch powers. We avoid specifying
quantities that are trivial to determine (such as a branch current
for an element in series with an independent current source).
Alternatively, the student can be required to solve for all node
voltages or all mesh currents.
B. Solution Generation
The system currently generates and displays complete sets
of node or mesh equations and solves them using matrix
methods. Equations are automatically adjusted based on a
user-selected choice of reference node. There is an optional
“pre-simplification” step prior to this solution, in which
independent voltage sources in series and independent current
sources in parallel are combined into single sources of each
type, and in which any passive elements of the same type that
are in series or parallel with each other are combined. The
circuit diagram can be automatically re-drawn after each such
simplification step to illustrate the process to the student. Work
is currently in progress to implement a number of other
common solution methods, such as use of voltage and current
division, superposition, source transformation, and use of
Thévenin or Norton equivalent circuits. We also plan to extend
the system from its current coverage of DC resistive circuits to
cover steady-state AC phasor analysis and transients using both
differential equation and Laplace transform approaches. A
sample automatically-generated circuit and its solution by mesh
analysis are shown in Fig. 1.
C. User Input Modules
Students input node or mesh equations using a specially
designed template interface as shown in Fig. 2, in which they
are offered a palette of properly formed terms from which to
choose [depending on the type of equation, such as a voltage
constraint equation, Kirchoff’s current law (KCL) equation,
dependent source control variable equation, etc.]. They can
drag these terms into the equation area as needed, and easily
reposition or delete them. In Fig. 2, for example, the user has
dragged the various terms needed for a KVL equation for mesh
3 from the upper palette into the equation entry area below, and
is about to check the equation for correctness. Once the user
selects the appropriate templates, they complete each term by
filling in the numerical values and subscripts. Students appear
to find this interface to be highly intuitive and easily adapt to
using it with little or no instruction. The program then
indicates immediately if the equation is correct, and optionally
displays the correct solution if it was not. In the latter case, the
student then has to work an additional problem of the same
type to proceed.
Simplified forms of the algebraic equations are entered on a
special form designed for that purpose, as is the matrix form of
the equations. They are provided with feedback at each stage,
so they do not waste time and become frustrated by proceeding
to the next stage when they have already made a mistake (as

Some problems of a more qualitative nature are also being
developed, to help dispel typical student misconceptions. An
example is one in which a circuit diagram is displayed and
students are asked to input lists of which elements are in series
or in parallel in the circuit (or which form wye or delta
connections). Students typically have difficulty in doing this at
first, particularly for the parallel case, which can lead to many
other errors in quantitative analyses.

Fig. 1. Example of an automatically generated problem and solution using
mesh analysis.

they would do in typical answer-based tutors). Numerical
answers are accepted in a tabular format, and multiple choice
responses can also be processed.

D. Pedagogical Features
We have included a number of items to help students
understand and visualize the methods by which circuits can be
analyzed. For example, different nodes can optionally be color
coded, to help visualize the nodes. The circuit elements can
even be temporarily blanked out, leaving only the wires, to help
students understand the definition of nodes. We also illustrate
sets of elements in series or parallel (or forming wye or delta
connections) by selectively highlighting each set in red in turn.
Nodes and mesh currents can be automatically labeled and
numbered. There are also features to help students understand
the origin of node or mesh equations. For example, the
currents leaving a selected node or supernode can be
automatically labeled with color-coded arrows, in which case
the terms of the KCL equation are correspondingly color-coded
to match. Similarly, the path around which a KVL equation is
written for a supermesh can be labeled automatically, and
voltage drops around any selected mesh or supermesh can be
labeled with color-coded +/– signs, with corresponding colorcoding of the terms in the KVL equation. In Fig. 1, for
example, the first KVL equation is the one that has been
selected for highlighting, so the supermesh path and +/− signs
appear around meshes 1 and 3, which form a supermesh. (In
this view, the unknown voltage Vo is not shown.)
E. Tutorial Modules
To date we have implemented three software tutorial
sequences on the topics of identifying elements in series and

Some types of circuit solution (such as superposition, or
simplification by combining elements) require students to redraw the given circuit diagram prior to writing any equations.
We have therefore constructed a graphical circuit editor
function in which students can modify a given circuit in any
required way (or even create a new circuit from scratch). By
checking if this step has been completed correctly, we will
again be able to avoid letting the student waste time by writing
equations for an incorrect diagram.
We are also currently constructing a web-based graphical
waveform sketching tool that students can use to draw
waveforms as a function of time. This module will be useful
when students are given the current through a capacitor and
asked to sketch its charge or voltage as a function of time, for
example (or similar problems involving inductors). The tool is
similar to a vector graphics drawing tool, but will include
various scientific functions such as exponentials, sinusoids,
piecewise linear functions, and so on. The student input will be
checked automatically against a correct solution, without
requiring that problems be of a multiple choice format.
Fig. 2. Equation entry interface.

parallel and on writing node and mesh equations. The
series/parallel tutorial includes a sequences of illustrations and
examples with associated interactive questions, followed by the
option to view examples and complete exercises at four
different levels of difficulty. The node and mesh equation
tutorials present a brief set of instructions, followed by the
opportunity to view worked examples and do exercises at five
different levels. Student activity in each tutorial is logged to a
server to record their progress, allowing them to stop or re-start
at any time. The results can be viewed and downloaded by the
instructor. Students are required to complete the highest level
of difficulty in each tutorial before receiving full credit. At the
completion of each tutorial, a one-question survey is
administered and the student can enter comments to provide
feedback.
We are also developing scripts for future tutorial sequences,
which will be implemented later in software using a planned
tutorial scripting and execution interface. A total of 14 scripts
have been generated to date, including 5 on node analysis, 5 on
mesh analysis, and 4 on basic electrical concepts such as
current, voltage, power, KCL, KVL, etc. The scripts outline
sequences of instructional material together with interactive
questions and exercises that will be posed to the students.
III. LABORATORY-BASED STUDY
To test the effectiveness of our software in improving
student learning, a laboratory-based study was conducted in
December 2012 using 33 paid student volunteers, all of whom
were either enrolled in the relevant course (EEE 202) or had
completed it within the last year. Students were given a pretest and a post-test using two different test forms that were
randomly assigned and similar in difficulty. The two topics
were identification of series and parallel circuit elements and
writing node equations for DC resistive circuits. The students
were randomly assigned either to use two of the corresponding
software tutorials for a total of one hour (for 25 min. and 35
min., respectively), or to work on similar textbook problems, as
they would normally do in the course. All students were given
a copy of the course textbook [39] and advised that they could
consult any relevant material in it as needed. After completing
the post-test, both groups of students were asked to complete
the Instructional Materials Motivation Survey (IMMS)
developed by Keller to assess the effects of the different
instructional approaches on student motivation [40].
The pre-test and post-test data for both groups are
summarized in Table I. Both groups had very similar pre-test
scores (averages of 59% and 58%), as would be expected from
the randomized experimental design. The textbook users had
only a small average increase in average score up to 62%,
about 3 points higher.
The software users saw an
approximately 10× larger improvement, to an average of 86%
on the post-test. The effect size (Cohen d-value) of the
experimental condition on the post-test scores was found to be
1.21 in units of the pooled standard deviation of those scores.
The difference was statistically significant assuming
independent samples (but not equal variances, as they are
substantially different) at the 95% confidence interval, t(19.7)
= 3.303, p < 0.05. We therefore conclude that the tutorials are
much more effective than conventional homework assignments

TABLE I. LEARNING GAINS IN LABORATORY STUDY
Exptl.
Pre-Test Post-Test Gain
Group
Score
Score
Average
Textbook*
58.6
61.6
2.9
Median
Textbook
60.5
67.0
1.5
Std. Dev.
Textbook
25.3
28.0
14.1
Average
Software**
57.8
86.4
28.6
Median
Software
57.0
85.0
30.0
Std. Dev.
Software
22.1
11.5
14.9
Std. Dev.
Pooled
23.0
20.5
14.1
*16 users. **17 users.
in terms of immediate learning gains. We attribute the
difference to the step-based nature of the tutoring process, the
rapid feedback provided to the student, their ability to practice
or view examples as many times as needed (within the
available study time), and the pedagogical features that help
them learn to understand the formation and structure of the
equations.
The results were better for both the qualitative (seriesparallel identification) and quantitative (node equation) topic,
suggesting that this approach may be effective for a wide
variety of course topics when fully developed. The mean posttest scores for the series-parallel identification were 68% and
91% for textbook and computer users, respectively, and were
57% and 83% for the node equation topic for the two
corresponding groups. In particular, the easier node analysis
problem (having only independent current sources and
resistors, with four nodes) on the post-test yielded a remarkable
average score of 98% for the computer users (and only 70% for
textbook users), suggesting near-perfect mastery of that level of
the topic after only 35 minutes of automated instruction. It is
unlikely that most students were able to complete the tutorials
during the allotted time, so results on more difficult problems
could be even better given more time.
The results of the IMMS are shown in Table II. Overall
scores were determined (where 1=least favorable and 5=most
favorable response) as well as scores on four subscales
corresponding
to
the
attention-relevance-confidencesatisfaction (ARCS) factors proposed by Keller to model the
effects of instructional materials on student motivation [40].
The mean score was higher for the software on every scale,
though the difference was statistically significant only for the
total scores and attention and satisfaction subscales. The
relevance factor showed the least difference, which makes
sense as both the textbook and tutorials addressed the same
topics and the students seem to be making appropriate
connections between the practice problems and real-world
engineering applications in both cases. The degree to which
the software engaged students’ attention and especially their
overall satisfaction were the biggest differences (the latter
being nearly a full point higher for the software users, with a
large effect size of 1.27 pooled standard deviations). We
believe that it was the ability to repeat exercises and examples
without limitation for practice, and the rapid feedback provided
on student errors, that led to the higher satisfaction rating. The
Cronbach alpha coefficients we measured for the total score,

TABLE II. RESULTS OF INSTRUCTIONAL MATERIALS MOTIVATION SURVEY (SCALE = 1-5, 5=BEST)
Group
Statistic
Total Attention Relevance Confidence Satisfaction
Software Users Means
3.54
3.44
3.22
3.94
3.62
Std. Dev.
0.40
0.49
0.60
0.52
0.66
Medians
3.57
3.54
3.11
3.83
3.75
Textbook Users Means
3.01
2.84
2.99
3.51
2.65
Std. Dev.
0.77
0.80
0.83
0.99
0.91
Medians
3.01
2.88
3.00
3.72
2.33
Comparisons
Diff. of Means
0.53*
0.60*
0.23
0.44
0.97*
Pooled Std. Dev. 0.58
0.64
0.70
0.75
0.76
Cohen d-value
0.91*
0.94*
0.33
0.58
1.27*
*Statistically significant difference with p < 0.05.
attention, relevance, confidence, and satisfaction scales were
0.932, 0.851, 0.756, 0.855, and 0.838, suggesting good
reliability and consistency of the survey instrument, though it is
marginal on the relevance scale.
IV. CLASSROOM TRIALS
Preliminary trials were conducted on a voluntary basis in
Summer 2012 and Fall 2012 in our course EEE 202, but
sample sizes (in either the control group or the experimental
group, depending on the class section) were not large enough
to make statistically significant evaluations of the effects of the
software (though favorable effects were suggested by the data).
A much larger trial with strongly encouraged or mandatory
participation is being conducted in Spring 2013 with over 340
students in five sections with five different instructors.
Assessment of the impact of the software on student learning is
in progress, but of course it is challenging to isolate the effects
from variations in instruction and students from semester to
semester. However, student satisfaction with the tutorials
appears to be quite high. At the completion of each tutorial,
TABLE III. SAMPLE STUDENT COMMENTS ON TUTORIALS
Good job on the game! It was actually fun going through it and trying to do
a good job! Thanks for making this.
Worked as intended, didn't take too long, kind of fun, and I feel like it
helped!
I HAVEN'T EVEN LEARNED IT YET BUT IT WAS REALLY EASY
TO GRASP USING THIS! YAY
I really thought it was awesome; it was very helpful. I understood the
concepts, but this helped me develop a thought process on it.
I like how you are not marked off for getting on wrong, you just get to try
again. You only really fail if you give up, and that is reassuring.
These modules honestly do help me learn circuit analysis. I feel that it is
extremely helpful to have a good amount of practice problems, and a system
that provides instant feedback. This helps me learn the correct techniques
and master
I AM A PRO AT THIS. Major self-confidence booster. Really though, I
feel like I'm talented at this node analysis!
It definitely helped me understand supernodes, I think this was more usefull
than book work
This exercise helped me understand loop analysis very well. The
assignment was great.
I would prefer to have a statistics page showing # of correct and incorrect
attempts and possibly even a ladder [leader?] board showing how well
different students did as opposed to everyone getting a congratulatory gold
medal for doing thier hw
Wow is all i can say... This is the best, better than any hw I have done so far

students were asked to rate the tutorial as “very helpful,”
“somewhat helpful,” “not very helpful,” or “a waste of time,”
and were given the opportunity to enter comments. About
98.5% of students gave favorable ratings of “very helpful” or
“somewhat helpful,” and 74% rated them as “very helpful.”
Some sample student comments (verbatim) are shown in Table
III. Most negative comments (relatively uncommon) asked for
better instructions on the user interface (which we will add), or
complained about platform compatibility, as the program
currently runs only on Windows with Microsoft PowerPoint
installed. A future web-based version is planned to address that
issue. Some students requested more detailed feedback on their
errors, which could be added in a later version.
Some limitations are that the topical coverage is not yet
large enough to have a major impact on the overall class
(though additional development is in progress), and retention of
material from the time of the tutorial completion until the time
of exams may be an issue. Refinement of the tutorial to require
regular “refresher” exercises is planned to try to address
retention. The DIRECT concept inventory [1] is being used as
a pre- and post-test, but post-test results for Spring 2013 are not
yet available. Additional analysis of the effect on student
grades is planned, as is additional usage in future semesters.
Ultimately we hope to assess if the students can transfer the
knowledge gained using this software to work in subsequent
courses and to real-world engineering applications.
V. CONCLUSIONS
Our progress in developing a system to automate the
generation of problems and solutions for linear circuit analysis
has been described, as well as initial incorporation of these
modules into a tutorial system that can be used for homework
assignments in this type of course. The rapid feedback
characteristic of a step-based tutor, pedagogical features, and
unlimited practice opportunities appear to be very popular with
students and increase learning in laboratory-based trials
compared to conventional textbook exercises by a factor of
about 10. Additional work is needed to expand the scope of
material covered by the system, and to refine the user interface
and platform compatibility. The modular system is designed to
be very flexible, so that it could be used, for example, to
generate homework problems and solution manuals for a
conventional textbook, to automate the generation and grading
of individually customized homework assignments, to generate
problems for examinations and quizzes, and to create problems

for interactive in-class exercises.
system may be extendable to a
engineering education as well
problems of well-defined types,
statics, and so forth.

The overall approach of this
number of other domains in
where students must solve
such as logic circuit design,

ACKNOWLEDGMENTS
We thank Daniel Sayre of John Wiley & Sons, Inc. for
providing the textbook copies used in our laboratory study and
for his support. We thank Professors R. Gorur, G. Karady, B.
Matar, C. Tepedelenlioglu, T. Thornton, Chao Wang, Hongbin
Yu, and Hongyu Yu for using our software in their EEE 202
sections and for their interest in this project. We also thank P.
Selveraj and S. Ramalingam for their assistance.
REFERENCES
[1] P. V. Engelhardt and R. J. Beichner, “Students' understanding of direct
current resistive electrical circuits,” Am. J. Phys., vol. 72, pp. 98-115, 2004.
[2] M. Caillot, “Learning Electricity and Electronics with Advanced
Educational Technology,” in NATO ASI Series F: Computer and Systems
Sciences, vol. 115. Berlin: Springer-Verlag, 1993, pp. 329.
[3] R. Duit, W. Jung, and C. von Rhoneck, “Aspects of Understanding
Electricity--Proceedings of an International Workshop.” Kiel, Germany:
Verlag, Schmidt, & Klaunig, 1984.
[4] D. L. Millard, “Interactive learning modules for electrical engineering
education,” in Electron. Compon. and Technol. Conf., 2000, pp. 1042-1047.
[5] D. Millard and G. Burnham, “Interactive educational materials and
technologies,” in Internat. Conf. Engrg. Educat., Manchester, U.K., 2002, pp.
445-1-445-6.
[6] D. Millard and G. Burnham, “Increasing interactivity in electrical
engineering,” in 33rd ASEE/IEEE Frontiers in Educat., Boulder, CO, 2003,
pp. F3F8-F3F12.
[7] M. Nahvi, “Teaching introductory courses in electrical engineering to
engineering majors, new tools and context,” in Proc. 1988 Frontiers in
Education Conf., 1988, pp. 76-80.
[8] M. Nahvi, “A computer based intelligent synthetic tutor-tester for
electrical engineering,” in Proc. IEEE Internat. Conf. on Systems, Man, &
Cybernetics, 1990, pp. 742-744.
[9] H. E. Hanrahan and S. S. Caetano, “A knowledge-based aid for dc
circuit analysis,” IEEE Trans. Educ., vol. 32, pp. 448-453, 1989.
[10] J. S. Demetry, B. Black, D. Voltmer, M. Nahvi, and J. Jones,
“Computer-assisted interactive instruction: Results from a developmental
effort,” in Proc. 1992 Frontiers in Education Conf., 1992, pp. 662-667.
[11] A. Yoshikawa, M. Shintani, and Y. Ohba, “Intelligent tutoring system
for electric circuit exercising,” IEEE Trans. Educ., vol. 35, pp. 222-225, 1992.
[12] F. de Coulon, E. Forte, and J. M. Rivera, “KIRCHHOFF: An
educational software for learning the basic principles and methodology in
electrical circuits modeling,” IEEE Trans. Educ., vol. 1993, pp. 19-22, 1993.
[13] B. Oakley II, “Use of the Internet in an introductory circuit analysis
course,” in Proc. 1993 Frontiers in Education Conf., 1993, pp. 602-606.
[14] B. Oakley II and R. E. Roper, “Implementation of a virtual classroom
for an introductory circuit analysis course,” in Proc. 1994 Frontiers in
Education Conf., 1994, pp. 279-283.
[15] G. F. Shannon, “Multi-media computer based teaching--A case study,”
in Proc. IEEE 1st Internat. Conf. on Multi-Media Engineering Education,
1994, pp. 398-402.
[16] J. R. Jones and D. A. Conner, “The development of interactive tutorials
for introductory circuits,” in Proc. IEEE 1st Internat. Conf. on Multi-media
Engineering Education, 1994, pp. 108-114.
[17] J. Teng, J. Fidler, and Y. Sun, “Symbolic circuit analysis using
Mathematica,” Int. J. Electr. Eng. Educ., vol. 31, pp. 324, 1994.
[18] E. R. Doering, “CircuitViz: A new method for visualizing the dynamic
behavior of electric circuits,” IEEE Trans. Educ., vol. 39, pp. 297-303, 1996.

[19] L. P. Huelsman, “Symbolic analysis--A tool for teaching undergraduate
circuit theory,” IEEE Trans. Educ., vol. 39, pp. 243-250, 1996.
[20] B. Oakley II, “A virtual classroom approach to teaching circuit
analysis,” IEEE Trans. Educ., vol. 39, pp. 287-296, 1996.
[21] E. C. Shaffer and F. J. Mabry, “A student designed, Web-based learning
program for circuit analysis,” in Proc. 2000 Frontiers in Education Conf.,
2000, pp. T2D-17-T2D-22.
[22] A. Luchetta, S. Manetti, and A. Reatti, “SAPWIN-A symbolic simulator
as a support in electrical engineering education,” IEEE Trans. Educ., vol. 44,
pp. CDROM, 2001.
[23] L. Palma, R. F. Morrison, P. N. Enjeti, and J. W. Howze, “Use of Webbased materials to teach electric circuit theory,” IEEE Trans. Educ., vol. 48,
pp. 729-734, 2005.
[24] B. P. Butz, M. Duarte, and S. M. Miller, “An intelligent tutoring system
for circuit analysis,” IEEE Trans. Educ., vol. 49, pp. 216-223, 2006.
[25] B. P. Butz and S. M. Miller, “Evaluation of IMITS for the National
Science Foundation,” www.temple.edu/imits/shock/Evaluation.pdf.
[26] L. Weyten, P. Rombouts, and J. De Maeyer, “Web-based trainer for
electrical circuit analysis,” IEEE Trans. Educ., vol. 52, pp. 185-189, 2009.
[27] A. M. Rushdi, “Development of modified nodal analysis into a
pedagogical tool,” IEEE Trans. Educ., vol. 28, pp. 17-25, 1985.
[28] G. E. Chatzarakis, “Nodal analysis optimization based on the use of
virtual current sources: A powerful new pedagogical method,” IEEE Trans.
Educ., vol. 52, pp. 144-150, 2009.
[29] L. R. J. Costa, M. Honkala, and A. Lehtovuori, “Applying the problembased learning approach to teach elementary circuit analysis,” IEEE Trans.
Educ., vol. 50, pp. 41-48, 2007.
[30] A. Sterian, B. Adamczyk, and M. M. A. Rahman, “A project-based
approach to teaching introductory circuit analysis,” in Proc. 2008 Frontiers in
Educat. Conf., 2008, pp. S1F-3-S1F-8.
[31] Y. Tsividis, “Teaching circuits and electronics to first-year students,” in
Proc. IEEE Internat. Sympos. on Circuits & Systems, vol. 1, 1998, pp. 424427.
[32] T. Grabowiecki, “Expert system for teaching of problem solving in
circuit theory,” in Europ. Conf. on Circuit Theory & Design, Stresa, Italy,
1999, pp. 1283-1286.
[33] P. D. Cristea, R. I. Tuduce, and A. R. Tuduce, “Knowledge assessment
in intelligent e-learning environments,” in Internat. Conf. on Signals &
Electronic Systems, Poznan, Poland, 2004, pp. 573-576.
[34] P. Cristea and R. Tuduce, “Automatic generation of exercises for selftesting in adaptive e-learning systems: Exercises on AC circuits,” in 3rd
Workshop on Adaptive and Adaptable Educational Hypermedia at the
AIED’05 conference (A3EH), Amsterdam, 2005, pp. 28-35.
[35] S. Watanabe, J. Miyamichi, and I. R. Katz, “Teaching circuit analysis:
A mixed-initiative intelligent tutoring system and its evaluation,” in Proc.
IFIP TC3 Internat. Conf. on Advanced Research on Computers in Education
(ARCE'90), Tokyo, 1991, pp. 19-25.
[36] K. VanLehn, “The relative effectiveness of human tutoring, intelligent
tutoring systems, and other tutoring systems,” Educat. Psychologist, vol. 46,
pp. 197-221, 2011.
[37] C. D. Whitlatch, Q. Wang, and B. J. Skromme, “Automated problem and
solution generation software for computer-aided instruction in elementary
linear circuit analysis,” in Proceedings of the 2012 American Society for
Engineering Education Annual Conference & Exposition. Washington, D.C.:
Amer. Soc. Engrg. Educat., 2012, Session M356.
[38] B. J. Skromme, C. D. Whitlatch, Q. Wang, P. M. Rayes, A. Barrus, J. M.
Quick, R. K. Atkinson, and T. Frank, “Teaching linear circuit analysis
techniques with computers,” in Proceedings of the 2013 American Society for
Engineering Education Annual Conference & Exposition. Washington, D.C.:
Amer. Soc. Engrg. Educat., 2013, submitted.
[39] J. D. Irwin and R. M. Nelms, Basic Engineering Circuit Analysis, 10th
ed. Hoboken, NJ: Wiley, 2010.
[40] J. M. Keller, Motivational Design for Learning and Performance: The
ARCS Model Approach. New York: Springer, 2010.

2011 11th IEEE International Conference on Advanced Learning Technologies

How to Do Multimodal Detection of Affective States?
1

Javier Gonzalez-Sanchez1, Robert M. Christopherson1, Maria Elena Chavez-Echeagaray1,
David C. Gibson2, Robert Atkinson1, Winslow Burleson1

School of Computing, Informatics, and Decision Systems Engineering, 2School of Social Transformation
Arizona State University
Tempe, AZ USA
{javiergs, robert.christopherson, helenchavez, david.c.gibson, robert.atkinson, winslow.burleson}@asu.edu
Whether it is an empathetic attentive machine or trained
human tutor, any additional insight into how an individual is
feeling through the use of sensing devices can then be used
to improve their interactions with learners.

Abstract— The human-element is crucial for designing and
implementing interactive intelligent systems, and therefore on
instructional design. This tutorial provides a description and
hands-on demonstration for detection of affective states and a
description of devices, methodologies and tools necessary for
automatic detection of affective states. Automatic detection of
affective states requires that the computer sense information
that is complex and diverse, it can range from brain-waves
signals, and biofeedback readings to face-based and gesture
emotion recognition to posture and pressure sensing.
Obtaining, processing and understanding that information, to
create systems that improve learning, requires the use of
several sensing devices (and their perceiving algorithms) and
the application of software tools.

II.

This tutorial will provide its attendees with the
knowledge and clear understanding of some of the available
technologies, and how to use them to provide the computer
with the ability to sense and perceive affective states. It will
also provide information about data processing tools that can
be applied to work with that data. To conclude, some case
examples of the use of these technologies will be shown,
e.g., for data collection in research studies and for the
creation of affective tutoring systems.

Keywords- affective state; sensors; physiological activity;
student affect inference; empathetic systems; multimodal

I.

A. Sensing Devices
During the first part of the tutorial a hands-on
demonstration using inexpensive, easy to install, and widely
available devices will be demonstrate how to work with
different categories of sensing sources:

INTRODUCTION

One important way for systems to adapt to their
individual users is related with their ability to show empathy.
Being empathetic implies that the computer is able to
recognize user’s mental states and understand the implication
of those states. Empathy is related to and is influenced by
affective states. Detection of affective states is a step forward
to provide machines with the necessary intelligence to
identify and understand human emotions and then
appropriately interact with humans. This does not mean that
it is necessary to make machines feel these emotions, but
rather that machines can be equipped with hardware and
software to enable them to perceive users’ affective states
and then use this understanding to create more harmonic
interactions between humans and machines [1].
Within the context of educational environments, there is
a great deal of research showing that learning is more than
just a cognitive process [2]. In particular, the act of learning
is as much a motivational and affective task as it is a
demonstration of mental ability. Often the goal of a human
tutor is to not only help the learner gain knowledge, but to
help the learner identify how they feel and to continue
learning when frustrated [3]. Many computer-based tutors or
intelligent tutoring systems (ITS) have adopted models that
include both cognitive and affective capabilities. Until
recently much of the affective data gathered by ITS have
relied heavily on learner’s self-report of their affective state,
observation, or software data logs [4], but now many ITS
have started to include data from the physical manifestations
of affective states through the use of sensing devices [5].
978-0-7695-4346-8/11 $26.00 © 2011 IEEE
DOI 10.1109/ICALT.2011.206

TUTORIAL CONTENT

1) Physiological sensors: These are instruments that
provide information on the activity of physiological
functions. Arousal detection is shown as an example, using
a wireless and wearable device created by MIT and ASU
researchers.
2) Brain-computer interfaces: These are a particular
type of a physiological instrument that uses brainwaves as
information sources (electrical activity along the scalp
produced by the firing of neurons within the brain).
Emotiv© EPOC headset [6] device will be used to show
how to collect and work with this kind of data.
3) Face-based emotion recognition systems: These
systems infer affective states by capturing images of the
user’s facial expressions and head movements. The
capabilities of face-based emotion recognition systems are
showed using a simple 30 fps USB webcam and
MindReader [7], a MIT Media Lab software.
4) Eye-tracking systems: These are instruments that
measure eye positions and eye movement in order to detect
zones in which the user has particular interest in a specific
time and moment. Examples using Tobii© Eye Tracking
System [8] will be shown.
Using the information provided by these devices and
systems as input it is possible to measure in a quantitative
way the user experience.
654

data, the tutorial will describe the basis of a multimodal
approach that attendees can use to launch their own research
efforts.

B. Gathering Data: Filtering and Integration
The second part of the tutorial is a practical presentation
and discussion about software, techniques and
methodologies that are used to combine and integrate
information from the different categories of sensing sources.
This part provides an overview about: (a) approaches to filter
and integrate data (such as sampling rate unification); (b)
using ABE emotion recognition framework [9] to do filtering
and data integration; (c) integrating ABE framework to
support third-party systems becoming empathetic.

ACKNOWLEDGMENT
This research was supported by the Office of Naval
Research under Grant N00014-10-1-0143 awarded to Dr.
Robert Atkinson and by the National Science Foundation,
Award 0705554, IIS/HCC Affective Learning Companions:
Modeling and supporting emotion during teaching Dr.
Beverly Woolf and Dr. Winslow Burleson.

C. Analyzing Data
The third part of the tutorial is about data analysis, which
involves a variety of quantitative approaches, including
automated reverse engineering of dynamic systems,
clustering and classification. For reverse engineering
searches of the data, the Eureqa tool [10] is used to discover
mathematical expressions of the structural relationships in
the data records. For example, if a record holds information
about the physical and emotional behavior of an individual
who was engaged in a single experimental setting, Eureqa
could take all the available sources of data and reveal both
how the measure of engagement is calculated from specific
data streams as well as how other sensors may influence the
proposed emotional construct. For clustering and
classification approaches Weka [11], a tool that implements a
collection of machine learning algorithms for data mining
tasks, is used to explore the data composition and
relationships and derive useful knowledge from data records.

REFERENCES
[1]
[2]

R. W. Picard, Affective Computing, MIT Press, 1997.
B. du Boulay, “Towards a Motivationally-Intelligent Pedagogy: How
should an intelligent tutor respond to the unmotivated or the
demotivated?,” Proc. New Perspectives on Affect and Learning
Technologies, R. A. Calvo & S. D'Mello (Eds.), Springer-Verlag, in
press.
[3] M. R. Lepper, L.G. Aspinwall, D.L. Mumme and R.W. Chabay, “Self
perception and social-perception processes in tutoring: Subtle social
control strategies of expert tutors,” Proc. Self-Inference Processes:
The Ontario Symposium, J. M. Olson & M. P. Zanna (Eds.),
Lawrence Erlbaum Associates, 1990, Vol. 6, pp. 217-237.
[4] R.S.J. Baker, M.M.T Rodrigo and U.E. Xolocotzin, “The Dynamics
of Affective Transitions in Simulation Problem-solving
Environments,” Proc. Affective Computing and Intelligent
Interaction: Second International Conference (ACII ’07), A. Paiva, R.
Prada & R. W. Picard (Eds.), Springer-Verlag, Vol. Lecture Notes in
Computer Science 4738, pp. 666-677.
[5] I. Arroyo, D. G. Cooper, W. Burleson, F. P. Woolf, K. Muldner, and
R. Christopherson, “Emotion Sensors Go to School,” Proc. Artificial
Intelligence in Education: Building Learning Systems that Care: from
Knowledge Representation to Affective Modelling, (AIED 09), V.
Dimitrova, R. Mizoguchi, B. du Boulay & A. Grasser (Eds.), IOS
Press, July 2009, vol. Frontiers in Artificial Intelligence and
Applications 200, pp. 17-24.
[6] Emotiv - Brain Computer Interface Technology. Retrieved April 26,
2011, from http://www.emotiv.com.
[7] R. E. Kaliouby and P. Robinson, “Real-Time Inference of Complex
Mental States from Facial Expressions and Head Gestures,” Proc.
Conference on Computer Vision and Pattern Recognition Workshop
(CVPRW ‘04), IEEE Computer Society, June 2004, Volume 10, p.
154.
[8] Tobii Technology - Eye Tracking and Eye Control. Retrieved April
26, 2011, from http://www.tobii.com.
[9] J. Gonzalez-Sanchez, M.E. Chavez-Echeagaray, R. Atkinson, and W.
Burleson, “ABE: An Agent-Based Software Architecture for a
Multimodal Emotion Recognition Framework,” Proc. of 9th Working
IEEE/IFIP Conference on Software Architecture (WICSA ‘11), June
2011, in press.
[10] M. Schmidt, and H. Lipson, "Distilling Free-Form Natural Laws from
Experimental Data," Science, Vol. 324, no. 5923, pp. 81 - 85.
[11] M. Hall, E. Frank, G. Holmes, B. Pfahringer, P. Reutemann, and I.H.
Witten, “The WEKA Data Mining Software: An Update,” Proc.
SIGKDD Explorations, 2009, Volume 11, Issue 1.
[12] K. Avramides, and B. du Boulay, “Motivational Diagnosis in ITSs:
Collaborative, Reflective Self-Report,” Proc. Artificial Intelligence in
Education. Building Learning Systems that Care: From Knowledge
Representation to Affective Modelling, V. Dimitrova, R. Nizoguchi,
B. du Boulay & A. Graesser (Eds.), IOS Press, 2009, Vol. Frontiers in
Artificial Intelligence and Applications 200, pp. 587-589.

D. Usage Examples
The fourth and last part of the tutorial describes examples
of using affective state information to provide real-time and
archival data for both machine and human tutors as well as
their students. While most of the current use of emotion
recognition is within ITS development, there are also many
possible applications outside of computer-driven instruction.
By providing the psychophysiological data to an instructor,
they can improve their understanding of their students and
subsequently adapt instruction or learning materials. Recent
research has suggested that providing learners with access to
the same physiological data may help them monitor and/or
change how they manage their own learning [12].
III.

AUDIENCE

This tutorial is open to researchers, practitioners, and
educators interested in incorporating affective computing as
part of their adaptive and personalized technology toolbox.
IV.

CONCLUSIONS

The goal of this tutorial is to provide the attendees with
enough information and examples to enable them to start
their own investigations of the cognitive-affective elements
of learning [2]. While the tutorial will not present an
exhaustive list of all the methods available for collecting,
manipulating, analyzing and interpreting affective sensor

655

2013 Humaine Association Conference on Affective Computing and Intelligent Interaction

Multimodal Affect Recognition in Virtual Worlds:
Avatars Mirroring Users’ Affect
Javier Gonzalez-Sanchez1, Maria Elena Chavez-Echeagaray1, David Gibson2, Robert Atkinson1
1

Arizona State University, AZ, USA, 2Curtin University, Perth, Australia
javiergs@asu.edu, helenchavez@asu.edu, david.c.gibson@curtin.edu.au, robert.atkinson@asu.edu
bracelet (for arousal detection) [5], a chair with posture
sensing [6], and a mouse with pressure sensing [7]. Each
sensor is handled by an inference mechanism that reports
the following affects: the EEG headset infers excitement,
engagement/boredom, meditation, and frustration; facebased
recognition
infers
agreement/disagreement,
concentration, interest, thinking, and unsureness; the skin
conductance bracelet infers arousal; the chair sensor infers
interest; and mouse sensors infer frustration. Inference
mechanisms report their outputs to the hub, which: (1)
synchronizes them; (2) maps them in a continuous
emotional 3D space, where the axes are Pleasure, Arousal,
and Dominance (PAD) [8]; and (3) combines them in one
affect vector. The foundation of the affect recognition hub is
described in previous work [9], [10]. The PAD model is a
formalism to represent and interpret affect; it has been
previously used in real-time interactive systems because it
supports the representation of diverse and continuous
affective states over time [11].

Abstract— Virtual worlds enable users’ interactions through
avatars. Avatars embody individual characteristics from their
owners and exhibit those characteristics outward to the
community. Motivated by the role of avatars in interpersonal
communication, we integrated a generic real-time multimodal
affect recognition hub as an input within an online virtual
world to make an avatar mirror its owner’s affect. Affect
vectors (determined by pleasure, arousal, and dominance
coordinates) in a continuous affective space are applied to
characterize the user’s affective state in real time.
Keywords- affect recognition; affect mirroring; virtual
worlds; second life; multimodal; framework

I.

PROBLEM

Virtual worlds enable users’ interactions through avatars.
Users operate their avatars to explore the world, meet others,
socialize, participate in individual and group activities, and
even trade virtual goods and services with one another. Users
have a tendency to customize their avatars according to their
own appearance, which influences online interpersonal
communication, allowing more intimate engagement and
conversations with stronger interpersonal bonds [1].
However, as important as the avatar’s customization is to
reflect the owner’s appearance, it is more important to reflect
the owner’s affect (emotions, feeling, and moods). Mirroring
the owner’s affective expressions makes the avatar more
believable, likeable, trustable, and enjoyable, which creates
long-lasting social relationships and provides a suitable
environment where human users feel more comfortable [2].
This work shows the use of a generic, multimodal affect
recognition hub based on the state-of-the-art analysis of user
affective reactions and their interface with an online virtual
world. This demo serves as a test bed for our multimodal
affect recognition hub and allows data collection for
analyzing user experience in virtual worlds as well as the
later exploration of approaches to improve virtual
interactions.
II.

2) Online virtual world (white boxes). We chose to use
Second Life (SL), an online virtual world launched in 2003,
which has an active user base of about 600,000 people. SL
allows avatars to explore and interact. Exploration is similar
to traveling in the real world; avatars either search for places
of interest and then go there or decide to go to a random
place. Individuals can explore alone, but often small groups
explore together. Some exploration is spontaneous, but there
are also organized tours and even travel agents. Interaction

TECHNICAL CONTENT

The system to be demonstrated is composed of three
independent parts: the affect recognition hub, the virtual
world, and an interface between them. The structure of the
system is shown in Fig. 1, from right to left, as follows:
1) Affect recognition hub (gray boxes). Signals of a
user’s affect changes are gathered by diverse sensors. The
sensors included are an EEG headset [3], a web camera (for
face-based affect recognition) [4], a skin conductance
978-0-7695-5048-0/13 $26.00 © 2013 IEEE
DOI 10.1109/ACII.2013.133

Fig. 1. The user interacts with the virtual world by manipulating the avatar
with the keyboard and mouse; the multimodal affect recognition hub gathers
signals of the user’s affect changes, combines them, and projects the
inferred affect on the avatar by means of an interface.

724

occurs via text chat, text instant messaging, or voice, not
unlike the popular Internet applications, except that full 3D
visuals are usually included. Interaction is also behavioral,
such as when friends and acquaintances gather at a club to
dance or play while listening to recorded or live music [12].
Mouse and keyboard controls are used to move the avatar in
the virtual world. The mouse controls the camera and view
perspectives of the avatar, while the keyboard is used to
control the avatar’s movements and gestures.
3) Interface (dash pattern box). An interface module
connects the multimodal affect recognition hub and the
virtual world. The interface triggers native system input
events that change the avatar according to the affect vector
value. To simplify the demonstration, continuous 3D
affective vectors are discretized in four categories, not
including neutral, as shown in Fig. 2: frustrated (low
pleasure, high arousal, and low dominance), engaged (high
pleasure, high arousal, and high dominance), bored (low
pleasure, low arousal, and low dominance), and concentrated
(high pleasure, low arousal, and high dominance).
III.

Fig. 2. Affective vectors are discretized in four categories, besides neutral:
frustrated, engaged, bored, and concentrated. The avatar mirrors those
affects as showed.

EXPERIMENT

REFERENCES

Our exploratory study with avatars mirroring a user’s
affect while exploring and interacting in a virtual world aims
to answer three main questions: (1) does the user agree with
the emotion reflected by the avatar; (2) during a long session,
how does the user feel about having his or her feelings
uncovered; and (3) do the users agree that having their
avatars mirroring their affects increases the naturalness of the
interactions compared with traditional scenarios? In this
study, users will explore the virtual world by looking around
and interacting under two conditions: using an avatar with
affect connection and using an avatar without affect
connection. Users will fill out a post survey about the overall
experience. This research looks forward to potentially
improving human-computer interaction in virtual worlds by
exploring whether the moment-by-moment reflection of
affect is (1) feasible with a reasonable level of accuracy and
(2) usable in embedded, automated feedback to aid, guide, or
simply better understand the user.
IV.

[1]

A. Vasalou, A. N. Joinson, and J. Pitt, “Constructing my online self:
avatars that increase self-focused attention,” CHI 2007, vol. 1, pp.
445–448, Feb. 2007.
[2] T. W. Bickmore and R. W. Picard, “Establishing and maintaining
long-term human-computer relationships,” ACM Transactions on
Computer-Human Interaction (TOCHI), vol. 12, no. 2, pp. 293–327,
2005.
[3] Emotiv EPOC Headset. [Online]. Available: http://www.emotiv.com.
[Accessed: 14-Jul-2013].
[4] R. el Kaliouby and P. Robinson, “Generalization of a vision-based
computational model of mind-reading,” Affective Computing and
Intelligent Interaction, pp. 582–589, 2005.
[5] M. Strauss, C. Reynolds, S. Hughes, K. Park, G. McDarby, and R. W.
Picard, “The handwave bluetooth skin conductance sensor,” presented
at the ACII'05: Proceedings of the First international conference on
Affective Computing and Intelligent Interaction, 2005, pp. 699–706.
[6] S. Mota and R. W. Picard, “Automated Posture Analysis for
Detecting Learner's Interest Level,” Computer Vision and Pattern
Recognition Workshop, 2003. CVPRW'03., vol. 5, pp. 49–49, 2003.
[7] Y. Qi and R. W. Picard, “Context-Sensitive Bayesian Classifiers and
Application to Mouse Pressure Pattern Classification,” presented at
the Proceedings 16th International Conference on Pattern
Recognition, 2012, 2002, vol. 3.
[8] J. A. Russell, “A Circumplex Model of Affect,” Journal of Personality
and Social Psychhology, vol. 39, pp. 1161–1178, 1980.
[9] J. Gonzalez-Sanchez, M. E. Chavez-Echeagaray, R. K. Atkinson, and
W. Burleson, “Towards a Pattern Language for Affective Systems,”
Proceedings of the 19th Conference on Pattern Languages of
Programs, pp. 1–23, 2012.
[10] J. Gonzalez-Sanchez, M. E. Chavez-Echeagaray, R. Atkinson, and W.
Burleson, “Abe: An agent-based software architecture for a
multimodal emotion recognition framework,” pp. 187–193, 2011.
[11] S. W. Gilroy, M. Cavazza, and M. Benayoun, “Using affective
trajectories to describe states of flow in interactive art,” presented at
the Proceedings of the International Conference on Advances in
Computer Enterntainment Technology, New York, New York, USA,
2009, pp. 165–172.
[12] Second Life Official Site. [Online]. Available: http://secondlife.com.
[Accessed: 14-Jul-2013].

FUTURE WORK

This approach has potential implications for the broad
integration of affect recognition as a generic capability in
systems. We intended to make a first step forward to show
how our multimodal affect recognition hub could be adjusted
for diverse projects in a feasible and cost-effective way with
beneficial quality trade-offs.
ACKNOWLEDGMENT
This research was supported by the Office of Naval
Research through Grant N000141310438 awarded to Dr.
Robert Atkinson.

725

Computers in Human Behavior 29 (2013) 1833–1840

Contents lists available at SciVerse ScienceDirect

Computers in Human Behavior
journal homepage: www.elsevier.com/locate/comphumbeh

Searching for the two sigma advantage: Evaluating algebra intelligent
tutors q
Kent E. Sabo a, Robert K. Atkinson b,⇑, Angela L. Barrus a, Stacey S. Joseph a, Ray S. Perez c
a

Division of Educational Leadership and Innovation, Arizona State University, United States
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, United States
c
Warﬁghter Performance Department, Ofﬁce of Naval Research, United States
b

a r t i c l e

i n f o

Article history:
Available online 9 April 2013
Keywords:
Intelligent tutoring systems
Evaluation of CAL systems
Applications in subject areas
Evaluation methodologies
Secondary education

a b s t r a c t
This study evaluated 2 off-the-shelf, computer-based, mathematics intelligent-tutoring systems that provide instruction in algebra during a remedial mathematics summer program. The majority of the enrolled
high school students failed to pass algebra in the previous semester. Students were randomly assigned in
approximately equal proportions to work with the Carnegie Learning Algebra Cognitive Tutor or the ALEKS Algebra Course. Using the tutoring system exclusively, the students completed a 4-h-a-day, 14-day
summer school high school algebra class for credit. The results revealed that both tutoring systems produced statistically and practically meaningful learning gains on measures of arithmetic and algebra
knowledge.
Ó 2013 Elsevier Ltd. All rights reserved.

1. Introduction
Now more than ever, math skills are fundamental to successful
job performance. Although scientiﬁc work has always required a
high level of mathematical ability, an increasing number of lower
level jobs require math skills to operate high-tech equipment
(Agondi et al., 2009). In response to a math achievement gap and
the need for math skills in a competitive job market, the No Child
Left Behind Act (2001) required schools to make adequate yearly
progress in math with the goal that all students meet or exceed
proﬁciency by 2014. Schools across the country are falling far behind this goal. The 2007 National Assessment of Educational Progress showed that many students demonstrate only basic
mathematics mastery (Lee, Grigg, & Dion, 2007). Not surprisingly,
many curricular approaches are implemented in school math classes, but little rigorous research exists to prove their effectiveness
(Slavin & Lake, 2008).
One curricular approach is the use of intelligent tutoring systems in order to leverage advances in artiﬁcial intelligence and
cognitive science as well as the evolving power of the Internet. Several features differentiate intelligent tutors from more traditional
computer-based instruction, including the power to contextually
track a student’s performance and carefully adjust the teaching

q
This study was made possible by funding from the Ofﬁce of Naval Research
(ONR), grant number N000141010143.
⇑ Corresponding author. Address: School of Computing, Informatics, and Decision
Systems Engineering, Arizona State University, PO Box 870611, Tempe, AZ 852870611, United States. Tel.: +1 480 727 5692.
E-mail address: robert.atkinson@asu.edu (R.K. Atkinson).

0747-5632/$ - see front matter Ó 2013 Elsevier Ltd. All rights reserved.
http://dx.doi.org/10.1016/j.chb.2013.03.001

approach based on a student’s learning needs (Woolf, 2009). Intelligent tutors attempt to replicate human one-on-one tutoring
which, according to Bloom (1984), enjoys a two sigma advantage
over traditional classroom instruction. Numerous studies have
concluded that computer-based systems (including intelligent
tutoring systems) designed to deliver math instruction and assessments provide positive learning effects, they do not come close to
reaching the two sigma advantage that human one-on-one
tutoring enjoys (Beal, Arroyo, Cohen, & Woolf, 2010; Kulik, 1994;
Murphy et al., 2002). Studies of CBI have reported an average effect
size of d = 0.31 while intelligent tutor studies have reported
average effect sizes of d = .76 (VanLehn, 2011).
Some researchers have discovered design ﬂaws in many computer-based instruction studies. Few studies employed a randomized, experimental design; many were only descriptive studies,
and many lacked relevant data and speciﬁcity (Waxman, Lin, &
Michko, 2003). Educators and researchers should focus on the
effectiveness of algebra curriculum because algebra is a prerequisite for higher-level math and algebra proﬁciency is correlated
with students’ success in college and in obtaining jobs (Adelman,
1999; Carnevale & Desrochers, 2003). However, few independent
studies exist that evaluate the effectiveness of algebra intelligent
tutoring systems. Many of the algebra intelligent tutoring system
studies are conducted by the system developer, which increases
the potential for experimenter bias.
2. Algebra intelligent tutor studies
Hannaﬁn and Foshay (2006) evaluated a PLATO Learning’s computer-based algebra product as part of a new high school remedial

1834

K.E. Sabo et al. / Computers in Human Behavior 29 (2013) 1833–1840

math program. The goal of the larger remedial program was to increase scores on the math portion of the high-stakes state test. The
treatment group included 87 students, while 39 students were in
the control. The treatment group was scheduled to work with
the computer-based system for four of the ﬁve instructional days
per week. Both the treatment and control made signiﬁcant gains
on the state exam. The mean score for the control was signiﬁcantly
higher than the mean score for the treatment group. However, the
treatment group gain scores were signiﬁcantly higher than the
control group’s gain scores (Hannaﬁn & Foshay, 2006). This may
be a statistical artifact that indicates regression to the mean as
the treatment group’s scores on the ﬁrst state exam were signiﬁcantly lower than the control group’s scores.
In an early intelligent tutor study, Koedinger, Anderson, Hadley,
and Mark (1997) evaluated a new algebra curriculum called PUMP
and an intelligent tutoring system called PAT. Their experimental
group included 470 students in 20 algebra classes that worked
with the new algebra curriculum and the intelligent tutor. The control group included 120 students in ﬁve algebra classes who received a traditional curriculum and did not use the intelligent
tutor. The experimental group worked with the intelligent tutor
approximately 25 out of the 180 class meetings, which lasted
44 min apiece. The researchers reported a signiﬁcant difference between the two groups on two standardized tests and two researcher-created tests, with the experimental group performing better on
all the tests. The sigma effect on both standardized test was 0.3 and
was 0.7 and 1.2 on each researcher-created test. Percent correct on
each of the four tests for the experimental group ranged from 52%
to 32% (Koedinger et al., 1997).
As part of a large-scale study on reading and mathematics software, Campuzano, Dynarski, Agodini, & Rall (2009) evaluated the
computer based instructional program Larson Algebra I and the
intelligent tutoring system Cognitive Tutor Algebra I. The Larson
program was a supplement to traditional instruction while Cognitive Tutor was used as the core algebra I curriculum. Larson students were logged onto the system for an average of 313 min per
year with the usage occurring over 6 weeks. Cognitive Tutor students were logged into the system an average of 2149 min a year
with usage occurring over 24 weeks. Researchers reported no signiﬁcant difference between treatment and control groups using
computer-based and intelligent tutoring, algebra software products. Overall, the scores for the treatment students on Educational
Testing Service’s End-of-Course Algebra Assessment was 37.3%
correct (Campuzano et al., 2009).
It is important to study intelligent tutors as they attempt to
simulate human tutoring and through that simulation, to produce
similar learning gains. The reported effect size for human tutoring
when compared to classroom teaching is d = 2.0 (Bloom, 1984),
while the effect size of standard computer-based instruction is
d = 0.31 (VanLehn, 2011). If intelligent tutors demonstrate that
they produce learning gains close to human tutoring, the implications for education are signiﬁcant. Our study seeks to add value to
the literature investigating the effectiveness of algebra intelligent
tutoring systems through well-designed, independent research.
Speciﬁcally, we were interested in the effects of algebra intelligent
tutoring systems on student learning and attitudes in the intensiﬁed and concentrated time period allocated to a summer school
session.

3. Conceptual and theoretical frameworks
3.1. Human tutoring
Intelligent tutor developers have attempted to replicate human,
one-on-one tutoring because it is believed to be the most effective

method of instruction (VanLehn, 2011). In his review of intelligent
tutoring systems, VanLehn (2011) outlines eight theories of why
human, one-on-one tutoring may outperform other types of
instruction including traditional computer-based instruction and
intelligent tutoring systems. Of the eight theories he reviews, only
two are viable explanations: feedback and scaffolding (VanLehn,
2011). Tutors can identify errors in reasoning and help students
ﬁx the error in their knowledge during the smallest of steps. Frequent and immediate feedback helps students ﬁnd their reasoning
ﬂaws and ﬁx their knowledge. Scaffolding involves the tutor guiding the student only when the student is working on skills just beyond his or her capability (VanLehn, 2011). Early in learning new
skills, the student will require more scaffolding intervention from
the tutor, but this guidance should be gradually removed until
the student performs a skill independently (Lipscomb, Swanson,
& West, 2004).
3.2. Computer and intelligent tutoring
VanLehn (2011) identiﬁed two types of computer tutors: Computer-Based Instruction (CBI) and Intelligent Tutoring Systems
(ITS). CBI provides feedback and hints on student answers. In CBI,
a student must solve a problem. He works this out in his head or
on paper and then enters the answer. The CBI provides feedback
and/or hints based on the student’s answer (VanLehn, 2011). The
CBI is unaware of any of the student’s reasoning or thought processes and thus can be referred to as an answer-based tutor. An
ITS provides the student an interface in which the student enters
information for each step of the problem-solving process just as
they might if they were solving the problem on paper. The ITS is
then able to provide the student feedback and hints based on the
analysis of the responses to each step (VanLehn, 2011). This type
of ITS is a step-based tutor. A substep-based ITS features a ﬁnergrained level of interactivity by providing feedback and scaffolding
on substeps. Substeps are not completed explicitly by students in
writing the steps involved in solving a problem but may ‘‘correspond to mental inferences’’ (VanLehn, 2011, p. 12).
Intelligent tutoring systems are architected using information
technology systems and student learning models based on advances in cognitive science and artiﬁcial intelligence. They have
evolved in their ability to customize the learning experience to a
student’s ability and simulate the efﬁciencies of human tutoring
since the early 1970s (Self, 1998). Currently, there are several commercial software companies and educational research institutions
developing intelligent tutoring (Woolf, 2009). A wide-spread adoption of these intelligent tutors in school systems, the military, and
other training venues has been slow in the past due to cost and
complexity. However, the advances of the Internet now allow
intelligent tutoring vendors to provide access to powerful and useable programs via web-based clients anytime, anywhere, at a much
lower cost. ‘‘The implication is that personalized tutoring can be
made widely and inexpensively available just as printed materials
and books were made widely and inexpensively available by the
invention of the printing press’’ (Woolf, 2009, p. 20).
3.3. Carnegie learning’s adaptive control of thought–rational
Adaptive control of thought–rational (ACT-R) is a cognitive
architecture and theory, the purpose of which is to model the processes of human cognition (Anderson, 1992). The Carnegie Learning Cognitive Tutor is based on ACT-R. In ACT-R theory,
procedural knowledge called production rules control human cognition. These production rules take the form of if-then statements.
Learning in this theory is comprised of three learning processes.
First, experiences are coded into declarative knowledge called
chunks. Second, the chunks are converted into the form of a

K.E. Sabo et al. / Computers in Human Behavior 29 (2013) 1833–1840

production rule. Third, the production rules and chunks are
strengthened through active repetition (Anderson, 1992).
It is the interaction of the procedural and declarative knowledge
that gives rise to complex cognition. A large database of chunks
and production rules forms human cognition. Appropriate chunks
and rules are selected based on processes that are contextually
aware of the environment. Effective human cognition is dependent
on the volume of chunks and production rules and the accurate
deployment of that knowledge (Anderson, 1996).
In acquiring knowledge, ACT-R assumes that objects in the environment are synthesized and are then available as chunks in the
working memory. The theory also assumes that the production
rules, which specify the use of the chunks, are encoded by observing the use of the chunks in the environment (Anderson, 1996). For
example, students learn to solve math problems by following
examples of worked solutions. As deﬁned by the theory, learning
is the gradual process of acquiring all the necessary chunks and
production rules bit by bit. A process called rational analysis is
employed when knowledge needs to be used (Anderson, 1996).
This process identiﬁes chunks and production rules that are likely
to be needed in a particular context. The contextually appropriate chunks then deﬁne the performance in the environment
(Anderson, 1996).
ACT-R cognitive tutors are based on the analysis of subject area
(e.g., algebra) production rules. By studying the learning rate of
these production rules, ACT-R cognitive tutor developers and
researchers came to the conclusion that intelligence is explained
by the acquisition and tuning of many knowledge units (Anderson,
1996).

3.4. ALEKS’ knowledge space theory
Knowledge space theory (KST) attempts to mimic the ability of
an expert teacher to assess a student’s knowledge state (Doignon &
Falmagne, 1985). ALEKS’ intelligent tutoring systems are based on
KST. KST is not a theory of human cognition; rather, it is a theory
that informed the creation of a computer-based assessment procedure that provides an accurate and continuously updated assessment of student knowledge. KST theorists deﬁne a knowledge
state as a ‘‘particular subset of questions or problems that the subject is capable of solving’’ (Falmagne, Doignon, Koppen, Villano, &
Johannesen, 1990, p. 201). A knowledge space for a certain topic
is made up of all the knowledge states speciﬁc to that topic.
KST posits that given a student response to a problem, whether
correct or not, in a certain topic area, inferences can be made about
what other questions in the topic area the student could answer.
Constructing particular knowledge spaces and their subordinate
knowledge states is a labor-intensive undertaking as even a knowledge space deﬁned by tens of questions could result in thousands
of possible knowledge states (Doignon & Falmagne, 1985). For
example, Falmagne et al. (1990) asked an expert teacher to evaluate 24 algebra problems. The process involved the teacher evaluating whether given a student fails to answer problem X, would the
student also fail to answer problem Y. The teacher responded yes
or no to this scenario 196 times to establish all the knowledge sets
of the 24-problem knowledge space. To validate the knowledge
space deﬁned by the procedure, the knowledge space model is statistically compared to empirical student data (Falmagne et al.,
1990). An assessment system based on a validated knowledge
space is then able to assess a student in an efﬁcient manner by
using previous student responses to provide future problems. The
outcome of a KST assessment is not a numerical score but rather
a list of what a student is able to do, represented by the most difﬁcult problem types, and a list of problem types that a student is
prepared to learn (Falmagne, Cosyn, Doignon, & Thiery, 2006).

1835

3.5. Field-based evaluation
Evaluation and research share so many similarities that some
researchers and evaluators do not make a distinction between
the two. However, there are researchers that have delineated characteristics of evaluations that make them distinct from research.
Evaluations focus on the effectiveness of particular interventions
and describe the impact of those interventions. Evaluation is a
decidedly practical method and is designed to provide potential
solutions that allow decisions to be made in practical classroom
contexts (Scott & Usher, 2011).
Evaluations are typically conducted in the ﬁeld, thus providing
additional beneﬁts over laboratory-based research. As Cohen,
Manion, Morrison, and Morrison (2007) wrote, ‘‘Schools and classrooms are not the antiseptic, reductionist, analyzed-out or analyzable-out world of the laboratory’’ (p. 278). Since the laboratory is a
contrived context, generalizing results from the laboratory to the
classroom is inadvisable (Cohen et al., 2007). It was believed that
research conducted by science educators had little impact on
classroom practice in the 1980s. One explanation why this occurred was that the research being conducted had such a narrow
scope that the results had little application to a real classroom
(Brickhouse, 1989). Field-based evaluations also provide opportunities for building connections between schools and universities.
The connections can foster valuable collegial networks. Teachers
can gain access to curriculum resources previously unavailable
due to cost and the students often obtain the beneﬁts of participating in the intervention (Javorsky, Kline, & Zentall, 2000).
A speciﬁc type of evaluation known as an impact evaluation is
used to ‘‘assess the effects of a settled program’’ (Owen, 2006, p.
47) and focuses on what happens to participants as a result of
the intervention or program as they typically try to make causal
connections between an evaluand and an outcome (Russ-Eft & Preskill, 2009). In order to conduct an impact evaluation of an intelligent tutoring system, an evaluation framework must be described
to outline how the processes and impact will be determined (RussEft & Preskill, 2009). Educational technology ﬁeld evaluations seek
to show that learning outcomes have been achieved and that the
software or tool works appropriately, but they usually go beyond
laboratory or developer evaluations to demonstrate that the intervention is effective in real classrooms with real students (Woolf,
2009). In order to effectively conduct summative impact evaluations that measure how well intelligent tutoring systems are meeting this goal, a ﬁeld evaluation framework adapted from Woolf
(2009) was used to measure student experiences and achievement
using intelligent tutoring systems. The framework included the following processes:







Establish the goals of the tutor(s).
Identify the goals of the evaluation.
Develop an evaluation design to test research questions.
Instantiate the evaluation design.
Present results.
Discuss the evaluation results.

Using these six stages to design, implement, and document an
intelligent tutoring evaluation study increases the validity and rigor of the study and approximates the process of experimental
study designs but allows for the ﬂuidity and at times unpredictability of a ﬁeld-based environment (Woolf, 2009).

4. Overview of study
This study was part of larger multi-year research project
designed to evaluate the effectiveness of off-the-shelf, algebra

1836

K.E. Sabo et al. / Computers in Human Behavior 29 (2013) 1833–1840

tutoring systems. The results of this study were used to inform a
series of subsequent studies within the project tasked with determining the most effective intelligent tutoring system for algebra
remediation in high school instructional settings and in the military. The two mathematics intelligent tutors selected for evaluation were the Carnegie Learning Algebra Cognitive Tutor
(Carnegie) and the ALEKS Algebra Course (ALEKS). The goal of this
ﬁeld evaluation was to compare the effectiveness of the two intelligent tutors, examine for trends in learning gains, and measure
students’ experiences with the systems.
5. Method
5.1. Participants and design
Participants were 31 remedial high school algebra students (15
male and 16 female, with an average age of 14.7) who chose to enroll in a district-organized 14-day summer school high school algebra class. The majority of these students failed to pass algebra in
previous semesters. A total of 41 students enrolled in summer
school, with 31 completing the program. English was the ﬁrst language learned for 77% of the students, and for 97% of the students,
English was their preferred language. Students were not enrolled
in any other summer courses and did not receive any other formal
math instruction other than that provided by the summer school
course. Each class meeting was 4 h and started at 8 AM. Tuition
was $175 per session per one-half credit. Three high school math
teachers were assigned by the district to supervise the summer
school sessions.
The study’s design involved a single independent variable with
two levels, coupled with pre- and post-tests. The two levels reﬂected the two tutoring systems used in the evaluation. The participants were randomly assigned in approximately equal numbers to
the two tutoring conditions. Three students in the ALEKS condition
did not complete the summer school program, resulting in 14 ALEKS students completing the program and 17 Carnegie students
completing the program.
5.2. Intelligent tutors
This study evaluated two off-the-shelf, intelligent tutoring systems: the ALEKS Algebra Course (ALEKS) and the Carnegie Learning
Algebra Cognitive Tutor (Carnegie). The two systems are comparable intelligent tutors that include similar features. The most important feature, for the purposes of this study, is the ability for both
systems to adapt and respond intelligently to student needs based
on input from the student. While both systems qualify as intelligent tutors, they do differ in dominant question types, content
delivery structure and theoretical bases. Carnegie’s questions are
word problems based on real-world scenarios, while ALEKS’ questions are equation-based. Carnegie’s content delivery is linear,
while ALEKS allows students to work on any section of the content
that the system deems the student is ready to learn. Finally, Carnegie is based on a cognitive architecture and theory that aims to
model the processes of human cognition (Anderson, 1992), while
ALEKS is based on an assessment theory that attempts to mimic
the ability of an expert teacher to assess student knowledge
(Doignon & Falmagne, 1985).
5.2.1. ALEKS algebra course
ALEKS assesses students continuously by using artiﬁcial intelligence with questions that require a free response. ALEKS structures
questions and requests input from the student such that it makes
solving a problem similar to what a student would do on paper
(ALEKS, 2011). As the ﬁrst activity for a new student, ALEKS

administers a 20–30 question assessment. ALEKS makes a decision
on each question delivered based on the student responses to the
previous questions. At the completion of the initial assessment,
ALEKS determines the topics the student has and has not mastered.
ALEKS represents this topic mastery information with a pie chart of
topics available in the course.
This pie chart displays the topics ALEKS determines the student
is ready to learn based on the initial assessment. Students have the
freedom to choose the topic they wish to practice. Once a student
chooses a topic, he or she begins working on practice problems.
When a student consistently answers the practice problems from
a certain topic correctly, ALEKS determines that the student has
mastered the topic. ALEKS updates the student’s pie chart and
the student can choose the next topic to practice. While practicing,
students have access to explanations of the problems within ALEKS. ALEKS administers assessments periodically throughout the
course to test previously mastered topics. If a student no longer
demonstrates mastery of a topic, that topic returns to the list of
available topics in the student’s pie chart.
5.2.2. Carnegie Learning Algebra Cognitive Tutor
Carnegie allows instructors to build a custom curriculum by
choosing and excluding topics to meet student, school, and district
needs and requirements. Carnegie administers an assessment before and after every unit. Instructors can conﬁgure the pre-test
so that the results of the pre-test are used to set pacing for the unit.
Students work through the unit in a linear fashion. Carnegie also
assesses students continuously by analyzing student responses to
problems that require a free response (Carnegie Learning, 2011).
Like ALEKS, Carnegie attempts to structure questions and provide
response options that mimic the problem-solving steps a student
would engage in on paper.
Problems are presented with multiple representations and are
based on real-world situations. For example a word problem can
be displayed along with a graph and a data table. Students can access interactive examples and hints for problems. An interactive
example displays the steps required to solve a certain type of problem. Hints provide information to help the student proceed
through the steps of a problem. Carnegie requires mastery of a topic within a unit before a student is allowed to progress to the following topic. At the completions of a unit, students are assessed
and are allowed to move onto the next unit in the sequence prepared by the instructor.
5.3. Instruments
5.3.1. Accuplacer
The Accuplacer is a computer-adaptive placement test featuring
10 modules, three of which are designed to measure mathematics
skills and knowledge. Choosing an appropriate assessment was
crucial to evaluating these intelligent tutors. Previous studies have
used state high stakes tests (Hannaﬁn & Foshay, 2006), commercially available standardized tests (Campuzano et al., 2009) and a
combination of standardized tests and research-created tests
(Koedinger et al., 1997). Accuplacer has characteristics that make
it particularly well suited to assess learning gains in an intelligent-tutor context. Primarily, the Accuplacer modules we chose
were generally aligned with the content of the two tutoring systems. We say generally aligned because as each student interacts
with the systems, the systems deliver content based on the students’ needs. Even within the same tutoring system, no student received exactly the same content or sequence of content. Within a
context where students are potentially interacting with and learning different content at different times, a static assessment may
not be effective. Thus, Accuplacer’s adaptability was a secondary
reason for its selection. The adaptability of the assessment may

K.E. Sabo et al. / Computers in Human Behavior 29 (2013) 1833–1840

also reduce potential practice effects from our repeated administrations. Finally, although research-created assessments obtain larger effect sizes (Koedinger et al., 1997), professionally-designed
assessments likely provide stronger psychometric properties.
Two modules were used for this study: arithmetic and elementary algebra. Both the arithmetic and elementary algebra modules
featured multiple-choice questions. Fundamental arithmetic concepts were measured in the 17-item arithmetic module. Students
were tested on basic arithmetic operations (e.g., ‘‘Solve the problem. Use the paper you were given for scratch work. A soccer team
played 160 games and won 65 percent of them. How many games
did it win? A. 94, B. 104, C. 114, D. 124’’ (College Board, 2011).
Questions were divided into three types: (a) ‘‘operations with
whole numbers and fractions: topics included were addition, subtraction, multiplication, division, recognizing equivalent fractions
and mixed numbers, and estimating; (b) operations with decimals
and percents: topics included addition, subtraction, multiplication,
and division with decimals (percent problems, recognition of decimals, fraction and percent equivalencies, and problems involving
estimation were also given); and (c) applications and problem
solving: topics included rate, percent, and measurement problems,
simple geometry problems, and distribution of a quantity into its
fractional parts’’ (College Board, 2011).
The 12-item elementary algebra module measured students’
ability to perform basic algebra operations (e.g., ‘‘Solve the problem. Use the paper you were given for scratch work. If
2x 3(x + 4) = 5, then x = A. 7, B. 7, C. 17, D. 17’’ (College
Board, 2011). The module contained three types of questions. The
ﬁrst type of question was on operations with integers and rational
numbers, absolute values, and ordering. The second type of question was on operations with algebraic expressions, and monomial
and polynomial addition and subtraction. Questions required multiplication and division of monomials and polynomials, positive rational root and exponent evaluation, simpliﬁcation of algebraic
fractions, and factoring. The third type of question required solving
equations, inequalities, word problems and linear equations and
inequalities. This type also required quadratic equations by factoring, algebraically contextual verbal problems, and translating written descriptions into algebraic expressions (College Board, 2011).
5.3.2. User experience questionnaire
Upon completion of the summer school algebra program, students completed a user background and experience questionnaire
designed to gather demographic information (e.g., What language
do you prefer to communicate in?), comfort level with computers
(e.g., Rate your comfort level for locating and opening programs),
and their experiences with the intelligent tutors (e.g., My software
program helped me understand the math content) and summer
school (e.g., I learned a lot from the summer school math class).
Students responded to the comfort level questions by selecting
from a Likert-type scale with ﬁve options ranging from very
uncomfortable to very comfortable. They responded to the experience questions by selecting from a Likert-type scale with seven options ranging from not true at all to very true.
5.4. Procedure
Students completed the 14-day summer school session in two
computer labs within the same high school, equipped with PC
workstations connected to the Internet. The software programs
used in this evaluation were delivered through the Internet using
subscription, web-based software clients that do not require individual installations on the computer workstations. Participant and
parental consent was obtained through signed assent and consent
forms from both the participant and the participant’s parents,
respectively.

1837

On the ﬁrst day of the summer school session, each student was
randomly assigned to either the ALEKS or Carnegie intelligent
tutoring system and given a study booklet that outlined the purpose of the study, the student’s assigned software and directions
for its use, and a detailed study protocol. The booklets also included a demographic questionnaire. Students were then given a
brief verbal overview of the summer school course and the evaluation study. Students also completed the Accuplacer pre-test modules on Day 1. On Day 2, students began working in their assigned
intelligent tutoring system and practiced algebra skills and concepts using their tutoring system for the next 10 days for four
hours per day. On a typical day, students would arrive at 8 AM,
log into their assigned tutor and begin working on the algebra content. Along with the system’s help and hint functions, the supervising math teachers were available to the students to answer their
algebra questions. Students were given regular breaks based on
the district’s standard summer school schedule. On Day 7, students
completed the adaptive arithmetic reasoning and elementary algebra Accuplacer math tests as a repeated measure assessment intended to capture iterative learning gains. The ﬁnal 2 days of the
14-day program consisted of the students completing the Accuplacer module post tests and a demographic and experience questionnaire, as well as completing the district ﬁnal exam that
determined each student’s readiness to progress to the next math
course in the high school sequence. By the conclusion of the 14-day
summer school course, each student had practiced and studied
using his or her assigned intelligent tutoring system for an average
of 35.5 h. Students completed periodic assessments provided and
used by the intelligent tutoring systems to regularly adapt the
practice, pace, and content to the individual student’s learning
needs. The summer school teachers used these periodic tutor
assessments as well as student progress in the tutor curriculum
to assign students their grades for the course.

6. Results
This study utilized a mixed within- and between-subjects design to evaluate the effect of the intelligent tutors on algebra and
arithmetic test scores. The between-subjects factor was algebra
tutoring groups with two levels (CL and AK) and the withinsubjects was Accuplacer administration time with three levels
(Day 1, Day 7 and Day 13). Two separate, two-way mixed ANOVAs
were conducted to evaluate the effect of the two math programs on
the Accuplacer tests.
To evaluate the effect of the tutors on the Accuplacer algebra
test, the between-subjects factor was algebra tutoring with two
levels (CL and AK) and the within-subjects was Accuplacer algebra
administration time with three levels (Pre-test on Day 1, Day 7,
and Post-test on Day 13). For the dependent measure Accuplacer
algebra score, results indicated a signiﬁcant effect for time, Wilks’
K = .52, F(2, 28) = 12.66, p < .0001, multivariate g2 = .48. The
strength of the relationship between the intelligent tutor treatment and the Accuplacer algebra score was strong, with the intelligent tutor factor accounting for 48% of the variance in Accuplacer
algebra scores. A nonsigniﬁcant time  tutor interaction effect was
found, Wilks’ K = .99, F(2, 28) = .15, p = .96, multivariate g2 = .003.
This result suggests that there were no statistically signiﬁcant differences between the tutors on Accuplacer algebra scores over
time. Follow-up analyses indicated that at each point in time the
Accuplacer algebra scores differed signiﬁcantly from one another
and increased overtime. Students made signiﬁcant gains on the
Accuplacer algebra subtest from Day 1 to Day 13 (see Table 1).
Independent samples t-tests were conducted to evaluate the
difference in means on the algebra pre-test and post-test between
the tutor groups. There was a signiﬁcant a priori difference in the

K.E. Sabo et al. / Computers in Human Behavior 29 (2013) 1833–1840

60
55
50

Score

algebra pre-test scores between the two conditions, t(29) = 3.01,
p = .005. There was no signiﬁcant difference in the algebra posttest scores between the two conditions, t(29) = 1.92, p = .065 (see
Fig. 1). ALEKS students saw an algebra mean gain of 15.29 while
Carnegie students saw an algebra mean gain of 16.59. Combined
algebra scores showed an effect size of d = .95 while ALEKS alone
showed an effect size of d = .95 and Carnegie showed and effect
size of d = 1.18.
To evaluate the effect of the software programs on the Accuplacer arithmetic reasoning test, the between-subjects factor was
algebra tutoring condition with two levels (CL and AK) and the
within-subjects was Accuplacer arithmetic administration time
with three levels (Pre-test on Day 1, Day 7, and Post-test on Day
13). For the dependent measure, Accuplacer arithmetic score, results indicated a signiﬁcant effect for time, Wilks’ K = .45,
F(2, 28) = 17.26, p < .0001, multivariate g2 = .55. The strength of
the relationship between the intelligent tutor treatment and the
Accuplacer arithmetic score was strong, with the intelligent tutor
factor accounting for 55% of the variance in Accuplacer arithmetic
scores. A nonsigniﬁcant time  tutor interaction effect was found,
Wilks’ K = .98, F(2, 28) = .24, p = .79, multivariate g2 = .02. This result suggests that there were no statistically signiﬁcant differences
between the tutors on Accuplacer arithmetic scores over time. Follow-up analyses indicated that at each point in time the Accuplacer
arithmetic scores differed signiﬁcantly from one another and increased overtime. Students made signiﬁcant gains on the Accuplacer arithmetic reasoning subtest from Day 1 to Day 13 (see Table 2).
Independent samples t-tests were conducted to evaluate the
difference in means on the arithmetic pre-test and post-test between the tutor groups. There was a signiﬁcant a priori difference
in the arithmetic pre-test scores between the two conditions,
t(29) = 2.40, p = .023. There was no signiﬁcant difference in the
arithmetic post-test scores between the two conditions,
t(29) = 1.48, p = .15 (see Fig. 2). ALEKS students saw an arithmetic
mean gain of 18.92 while Carnegie students saw an arithmetic
mean gain of 23.88. Combined arithmetic scores showed an effect
size of d = 1.10 while ALEKS alone showed an effect size of d = 1.12
and Carnegie showed an effect size of d = 1.25.
An independent t-test was conducted to evaluate the differences in composite mean scores on the experience questionnaire
between the tutor groups. Students rated statements from 1 (Not
true at all) to 7 (Very true). There was no signiﬁcant difference between ALEKS (M = 4.86, SD = 1.09) and Carnegie (M = 4.63,
SD = .89), t(25) = .62, p = .54. Because there was no signiﬁcant difference between the groups, the descriptive statistics from the
experience questionnaire are reported in aggregate. Just over half
of students (51.6%) responded true to very true that their tutor
helped them understand the math content. A majority of the students (61.3%) responded true to very true that they learned a lot

45
40
35

ALEKS

30

Carnegie

25
Time 1

Time 2

Fig. 1. Mean algebra repeated measure scores for ALEKS and Carnegie.

Table 2
Means and standard deviations for arithmetic repeated measure.

a
b
c

Tutor

M

SD

Time 1

AKa
CLb
Totalc

47.64
29.94
37.94

28.18
10.70
22.03

Time 2

AK
CL
Total

57.86
43.41
49.94

25.33
22.26
24.41

Time 3

AK
CL
Total

66.57
53.82
59.58

24.25
23.56
24.34

n = 15.
n = 17.
n = 31.

Table 1
Means and standard deviations for algebra repeated measure.

a
b
c

ALEKS

Tutor

M

SD

AKa
CLb
Totalc

42.43
27.06
34.00

18.09
9.86
15.94

Time 2

AK
CL
Total

51.00
37.94
43.84

26.36
16.70
22.21

Time 3

AK
CL
Total

57.71
43.65
50.00

25.12
15.31
21.19

Time 1

n = 15.
n = 17.
n = 31.

Time 3

Score

1838

Carnegie

Time 1

Time 2

Time 3

Fig. 2. Mean arithmetic repeated measure scores for ALEKS and Carnegie.

in summer school. Only a quarter (25.8%) of students responded
true to very true that they enjoyed the activity. Over half of students (54.9%) responded true to very true that they felt competent
after working on the activity for a while. Only 38.7% responded
true to very true that they were satisﬁed with their performance.
Nearly three quarters (74.2%) of the students responded true to

K.E. Sabo et al. / Computers in Human Behavior 29 (2013) 1833–1840

very true that it was important for them to do well on this activity.
A majority (61.3%) of students responded true to very true that this
activity could be beneﬁcial to them. However, only 51.6% responded true to very true that they would recommend this math
class to their classmates.
7. Discussion
This study demonstrates that classroom teachers can implement intelligent tutoring systems to provide an effective learning
environment that produces signiﬁcant learning gains. Both Accuplacer arithmetic and algebra test scores increased signiﬁcantly
over time for students in the two tutoring conditions. In addition,
the pre-test to post-test effect sizes for the two tutoring conditions
were very large. Interestingly, there was no signiﬁcant difference
for the interaction between tutor conditions across the administration times. These results suggest that both ALEKS and Carnegie are
very effective curriculum replacements when implemented in an
intensive, short-duration summer school program.
In his meta-analysis, VanLehn (2011) found that intelligent
tutor research did not study implementations in which intelligent
tutors were used as full curriculum replacement. Since there are
many instructional activities that can have an effect on student
learning, these studies make it more difﬁcult to attribute the
learning gains to only the intelligent tutor (VanLehn, 2011). Our
summer school implementation replaced completely the algebra
curriculum. Although this study did not compare intelligent tutors
to classroom instruction, much of the positive changes in learning
over time can be attributed to the intelligent tutors.
7.1. The two sigma advantage
According to Bloom (1984), and believed by many intelligent
tutor developers and researchers, human tutoring has a two sigma
advantage over traditional classroom instruction. Because intelligent tutors are developed to simulate human tutoring, we took
our ﬁrst steps in the search for the same exceptional gains reportedly provided by human tutoring. While the effect sizes we observed were impressive on their own, they did not come close to
our a priori expectations. Considering that our study compared
pre-test scores to post-test scores rather than comparing differences in scores from a traditional classroom condition to an intelligent tutoring condition, the two sigma gain appears to be an
unrealistic goal. Indeed, previous studies evaluating both traditional computer-based instruction and intelligent tutoring systems
have failed to ﬁnd this elusive two sigma gain. VanLehn (2011)
suggests that Bloom’s assertion inspired the research and development of modern intelligent tutoring systems. However, only two
studies of one-on-one tutoring found a two sigma advantage over
classroom instruction and the results of these studies can be called
into question (VanLehn, 2011). In the Bloom (1984) study there
were three conditions: traditional classroom instruction, classroom mastery learning and one-on-one tutoring with mastery
learning. The mastery learning students performed better than
the traditional classroom students (d = 1.00) and the tutoring students performed better than the mastery learning students
(d = 2.0). However, the mastery level required for the mastery
classroom students and the tutoring mastery students were different; 80% mastery versus 90% mastery. The higher mastery threshold for the tutoring students could explain the two sigma
advantage (VanLehn, 2011). Evens and Michael (2006) saw a similar two sigma difference in an experiment with a small sample
size (N = 17), but failed to see as large an advantage (d = .52) in a
follow-up experiment with a larger sample size (N = 53). The effect
size of their initial experiment was likely due to sampling error.

1839

While the instructional effectiveness of the tutors is of primary
importance, student attitudes about their instructional experience
are also critical. The results from our student experience questionnaire showed that there was no signiﬁcant difference between student responses from the two tutor groups. As neither tutor
demonstrated a statistically signiﬁcant advantage in instructional
effectiveness, similarly, neither showed a signiﬁcant advantage in
student preference. Results from the experience questionnaire suggest that student attitudes ranged from negative to ambivalent to
positive. Unsurprisingly, students did not enjoy summer school but
had more positive attitudes about the tutoring systems and the
beneﬁts of summer school. Students were ambivalent about their
respective tutoring systems and their competence after working
in the tutors for a while. Students were equally ambivalent about
recommending the summer school program to their classmates.
However, many students responded that it was important for them
to do well and that learning algebra was beneﬁcial. This suggests
students had a desire to learn and understood the relevance of
the content. The effect that this desire and understanding may
have had on test scores should not be underestimated.

8. Conclusion
Although Bloom’s two-sigma gain may be unattainable by current intelligent tutoring systems, we intend to continue our investigations in different school contexts with additional technologies
and instruments. Indeed, future studies are needed. Despite the
raw number of computer-based math instruction studies and to
a lesser extent, math intelligent-tutor studies: (1) few are peer reviewed, (2) many exhibit design deﬁciencies, (3) most focus on elementary curricula, and (4) recent technologies are not studied
(Murphy et al., 2002; Waxman et al., 2003). In our future research,
we intend to further address these issues in the computer-based
math instruction ﬁeld, especially as the issues pertain to intelligent
tutors.
It is important, as an unbiased third party, to continue our ﬁeldbased research that studies the learning outcomes that result from
use of intelligent tutors. Many intelligent tutoring studies are conducted by the developers of the systems and the companies that
market the systems. As such, few of these studies are scrutinized
by the peer-review process. The value of peer-reviewed studies
produced by those outside the companies should not be underestimated. As academic third-party researchers, we will continue to
submit our studies to journals that engage in the rigorous peerreview process. Further, intelligent tutoring studies are often
performed in a laboratory or a classroom laboratory-type environment. Our current and future studies have and will continue to
take place in real schools and classrooms, with real teachers and
students who are engaged in the business of teaching and learning.
We believe that studying intelligent tutoring systems in the ﬁeld as
they would be implemented by schools provides additional external validity to our results. Within this context, we will continue to
focus on high school algebra content, as success in algebra is the
gateway to higher-level mathematics topics. Algebra proﬁciency
also has a strong relationship with college success and in securing
a job.
The goal of intelligent tutoring is to produce learning outcomes
that far surpass the learning outcomes of traditional instruction.
Accordingly, intelligent tutoring research should seek to compare
learning outcomes produced by engaging in traditional or business-as-usual instruction and intelligent tutoring. We recognize
that the lack of this comparison in our current study is a limitation.
Although our study reported large and signiﬁcant learning gains
from a pre-test to a post-test, we are not able to report that the
gains were any better than gains from traditional instruction. A

1840

K.E. Sabo et al. / Computers in Human Behavior 29 (2013) 1833–1840

future study will feature a traditional algebra classroom condition
that will serve as a control group. In addition to the control group,
we will add another answer-based tutor to compare to the tutors
we evaluated in this study. The purpose for adding another tutor
to our future evaluation is to provide evidence that allows administrators and educators to make informed decisions based on both
dollar value and learning outcomes. With this quasi-experimental
design, a larger sample size, and the tutoring systems implemented
as full curriculum replacement, our results will present a clear picture of the effectiveness of intelligent tutoring systems.
Aside from the summer school context, we plan to examine
intelligent tutoring system implementations within constraints of
a standard school-year schedule. We intend to study the systems’
use in regular math classrooms as a once-a-week supplement to
the traditional curriculum during the standard school day. We will
compare classrooms that supplement with intelligent tutors with
classrooms that do not. In a third implementation, we will investigate the use of the tutors in an after-school program.
Beyond learning outcomes, we intend to measure student motivation produced by using the tutoring systems as well as students’
metacognitive strategies. Although we’ve found that the tutoring
systems can produce large and signiﬁcant learning gains, it is also
important to investigate student strategies and whether they are
motivated to use the systems. We hypothesize that students who
exhibit higher motivation while using the tutoring systems will
perform better on the mathematics and algebra assessments. We
also believe that students with more sophisticated metacognitive
strategies and self-regulation will perform better on the assessments. Our current and future studies will make for a strong body
of work that will produce robust results and provide researchers
and educators important information on the effectiveness of intelligent tutors in a variety of classroom contexts, both as curriculum
replacement or as supplemental curriculum.

References
Adelman, C. (1999). Answers in the toolbox: Academic intensity, attendance patterns,
and bachelor’s degree attainment (ED 1.302:D 36/3). Washington, DC: U.S.
Department of Education.
Agondi, R., Harris, B., Atkins-Burnett, S., Heaviside, S., Novak, T., & Murphy, R.
(2009). Achievement effects of four early elementary school math curricula:
Findings from ﬁrst graders in 39 Schools (NCEE 2009-4052). Washington, DC:
National Center for Education Evaluation and Regional Assistance, Institute of
Education Sciences, U.S. Department of Education.
ALEKS. (2011). ALEKS. <http://www.aleks.com/>. Retrieved 27.10.11.
Anderson, J. R. (1992). Automaticity and the ACT theory. The American Journal of
Psychology, 105(2), 165–180. http://dx.doi.org/10.2307/1423026.
Anderson, J. R. (1996). ACT: A simple theory of complex cognition. American
Psychologist, 51(4), 355–365. http://dx.doi.org/10.1037/0003-066X.51.4.355.
Beal, C. R., Arroyo, I. M., Cohen, P. R., & Woolf, B. P. (2010). Evaluation of
AnimalWatch: An intelligent tutoring system for arithmetic and fractions.
Journal of Interactive Online Learning, 9(1).
Bloom, B. (1984). The 2 sigma problem: The search for methods of group instruction
as effective as one-to-one tutoring. Educational Researcher, 13(6), 4–16.
Brickhouse, N. W. (1989). Ethics in ﬁeld-based research: Contractual and relational
responsibilities. Presented at the National Association for Research in Science
Teaching, San Francisco, CA. Retrieved from <http://www.eric.ed.gov/
ERICWebPortal/contentdelivery/servlet/ERICServlet?accno=ED307152>.

Campuzano, L., Dynarski, M., Agodini, R., & Rall, K. (2009). Effectiveness of reading
and mathematics software products: Findings from two student cohorts (NCEE
2009-4041). Washington, DC: National Center for Education Evaluation and
Regional Assistance, Institute of Education Sciences, U.S. Department of
Education.
Carnegie Learning (2011). Carnegie Learning. <http://www.carnegielearning.com/>.
Retrieved 27.11.11.
Carnevale, A. P., & Desrochers, D. M. (2003). Standards for what? The economic roots
of K-16 reform. Washington, DC: Educational Testing Service.
Cohen, L., Manion, L., Morrison, K., & Morrison, K. R. B. (2007). Research methods in
education. Psychology Press.
College Board, The (2011). ACCUPLACER. An accurate student placement test delivered
over the Internet. <http://professionals.collegeboard.com/higher-ed/placement/
accuplacer>. Retrieved 12.03.11.
Doignon, J., & Falmagne, J. (1985). Spaces for the assessment of knowledge.
International Journal of Man–Machine Studies, 23(2), 175–196. http://dx.doi.org/
10.1016/S0020-7373(85)80031-6.
Evens, M., & Michael, J. (2006). One-on-one tutoring by humans and machines.
Mahwah, NJ: Erlbaum.
Falmagne, J., Cosyn, E., Doignon, J., & Thiery, N. (2006). The assessment of
knowledge, in theory and in practice. Lecture Notes in Computer Science, 3874,
61–79.
Falmagne, J., Doignon, J., Koppen, M., Villano, M., & Johannesen, L. (1990).
Introduction to knowledge spaces: How to build, test, and search them.
Psychological Review, 97(2), 201–224.
Hannaﬁn, R. D., & Foshay, W. R. (2006). Computer-based instruction’s (CBI)
rediscovered role in K-12: An evaluation case study of one high school’s use
of CBI to improve pass rates on high-stakes tests. Educational Technology
Research and Development, 56(2), 147–160. http://dx.doi.org/10.1007/s11423006-9007-4.
Javorsky, J. M., Kline, C. E., & Zentall, S. S. (2000). Field-based research practices:
Bridging the gap between university and classroom. Journal of Special Education
Leadership, 13(2), 27–37.
Koedinger, K. R., Anderson, J. R., Hadley, W. H., & Mark, M. A. (1997). Intelligent
tutoring goes to school in the big city. International Journal of Artiﬁcial
Intelligence in Education, 8(1), 30–43.
Kulik, J. (1994). Meta-analytic studies of ﬁndings on computer-based instruction. In
E. L. Baker & H. F. O’Neil, Jr. (Eds.), Technology Assessment in Education and
Training (pp. 9–33). Hillsdale, NJ: Lawrence Erlbaum.
Lee, J., Grigg, W. & Dion, G. (2007). The nation’s report card: Mathematics 2007 (NCES
2007–494). Washington, D.C: U.S. Department of Education, National Center for
Education Statistics, Institute of Education Sciences.
Lipscomb, L., Swanson, J., West, A. (2004). Scaffolding. In M. Orey (Ed.), Emerging
perspectives on learning, teaching, and technology. <http://projects.coe.uga.edu/
epltt/>. Retrieved 01.11.11.
Murphy, R., Penuel, W., Means, B., Korbak, C., Whaley, A., & Allen, J. (2002). E-DESK:
A review of recent evidence on the effectiveness of discrete educational software.
Menlo Park, CA: SRI International.
No Child Left Behind (NCLB) Act of 2001, Pub. L. No. 107-110, § 115, Stat. 1425
(2002).
Owen, J. M. (2006). Program evaluation: Forms and approaches. Guilford Press.
Russ-Eft, D. F., & Preskill, H. S. (2009). Evaluation in organizations: a systematic
approach to enhancing learning (2nd ed.). Basic Books.
Scott, D., & Usher, R. (2011). Researching education: Data, methods and theory in
educational enquiry. Continuum research methods (2nd ed.). London; New York:
Continuum International.
Self, J. (1998). The deﬁning characteristics of intelligent tutoring systems research:
ITSs care, precisely. International Journal of Artiﬁcial Intelligence in Education, 10,
350–364.
Slavin, R., & Lake, C. (2008). Effective programs in mathematics: A best-evidence
synthesis. Review of Educational Research, 78(3), 427–515. http://dx.doi.org/
10.3102/0034654308317473.
VanLehn, K. (2011). The relative effectiveness of human tutoring, intelligent
tutoring systems, and other tutoring systems. Educational Psychologist, 46(4),
197–221. http://dx.doi.org/10.1080/00461520.2011.611369.
Waxman, H. C., Lin, M., & Michko, G. M. (2003). A meta-analysis of the effectiveness of
teaching and learning with technology on student outcomes. Naperville, IL:
Learning Point Associates.
Woolf, B. P. (2009). Building intelligent interactive tutors. New York: Morgan
Kaufman.

Computers & Education 67 (2013) 239–249

Contents lists available at SciVerse ScienceDirect

Computers & Education
journal homepage: www.elsevier.com/locate/compedu

Animated agents and learning: Does the type of verbal feedback they
provide matter?
Lijia Lin a, *, Robert K. Atkinson b, Robert M. Christopherson c, Stacey S. Joseph c, Caroline J. Harrison d
a

School of Psychology and Cognitive Science, East China Normal University, 3663 N. ZhongShan Rd., Shanghai 200062, China
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, USA
c
Mary Lou Fulton Teachers College, Arizona State University, USA
d
School of Letters and Sciences, Arizona State University, USA
b

a r t i c l e i n f o

a b s t r a c t

Article history:
Received 22 December 2012
Received in revised form
14 April 2013
Accepted 15 April 2013

The current study was conducted to investigate the effects of an animated agent’s presence and different
types of feedback on learning, motivation and cognitive load in a multimedia-learning environment
designed to teach science content. Participants were 135 college students randomly assigned to one of
four experimental conditions formed by a 2  2 factorial design with agent presence as one factor (agent
vs. no-agent) and type of verbal feedback it provided as the other factor (simple feedback vs. elaborate
feedback). Results revealed that participants who learned with the animated agent that delivered
elaborate feedback had signiﬁcantly higher scores on a learning measure compared to participants who
learned with an agent that provided simple feedback. The results are interpreted from both social agency
and cognitive load theoretical perspectives.
Ó 2013 Elsevier Ltd. All rights reserved.

Keywords:
Multimedia/hypermedia systems
Human–computer interface
Interactive learning environments
Teaching/learning strategies
Architectures for educational technology
system

1. Introduction
As researchers continue to investigate methods and guidelines to increase the effectiveness of learning environments, attention is being
focused on how motivation, social interaction and cognitive processes impact learning in multimedia environments (Mayer, Sobko, &
Mautone, 2003; Moreno, 2007; Moreno & Mayer, 2007). Multimedia environments provide an interface that incorporates words and
pictures in ways that can potentially capitalize on these factors and enhance learning (Mayer, 2005). For example, researchers have
explored using animated pedagogical agents to enhance social interaction between the computer and the learner and promote learning
processes (Atkinson, 2002; Craig, Gholson, & Driscoll, 2002; Dunsworth & Atkinson, 2007). An animated pedagogical agent is a lifelike
character that provides instructional information through verbal and nonverbal forms of communication. An agent incorporates some or
all of the following features: (a) a human-like look, (b) locomotion, (c) goal-directed gestures, (d) facial expression, (e) gaze, (f) a human
voice, (g) personalized speech, and (h) interactive behavior by reacting to a learner’s actions (e.g., providing verbal feedback). This study
investigated the impact of an animated agent and the type of corrective feedback on learning, motivation and cognition in a multimedia
environment.
1.1. Social agency theory perspective
Social agency theory (Atkinson, Mayer, & Merrill, 2005; Mayer, Sobko, et al., 2003) is one of the theoretical frameworks that researchers
use to investigate the effectiveness of animated pedagogical agents in multimedia learning environments. According to this theory, an
animated agent that appears on a computer screen and provides learners with verbal and/or non-verbal learning cues has the potential to
prime their social-interaction schema and involve the learner in social interaction. As a result, learners may be triggered to interact with

* Corresponding author. Tel.: þ86 21 6223 3280; fax: þ86 21 6223 3352.
E-mail addresses: ljlin@psy.ecnu.edu.cn, lijia.lin615@gmail.com (L. Lin).
0360-1315/$ – see front matter Ó 2013 Elsevier Ltd. All rights reserved.
http://dx.doi.org/10.1016/j.compedu.2013.04.017

240

L. Lin et al. / Computers & Education 67 (2013) 239–249

the agent in a computer-based multimedia learning environment in much the same way they would interact with their peer, mentor or
teacher in a classroom. Once learners perceive a computer-based instructional episode as a social event, they apply social rulesdthe
conventions for human-to-human communicationdwhen they are interacting with the computer (Reeves & Nass, 1996; Van der Meij,
2013). There are a number of social norms primed by the human–computer interactiondone of which is the cooperation principle
(Grice, 1975). Grice proposed that a person who is listening to someone talk in a human-to-human communication scenario will assume
that the speaker is making a concerted effort to clearly communicate by being informative, accurate, relevant, and concise. Therefore, the
learner is potentially motivated in this situation to make sense of what is being presented to him/her and will be more likely to process the
information deeply and achieve meaningful learning. In effect, they will be more motivated to select relevant information and integrate it
with prior knowledge.
There is modest empirical evidence in the educational research literature supporting social agency theory as several studies have
revealed positive learning effects of presenting an animated pedagogical agent in a multimedia environment. For instance, Atkinson
(2002) conducted a study in which an animated parrot (Peedy) was used in a multimedia program to deliver worked-example instruction about proportion-word problems. He found that participants studying content with the agent that narrated the instruction
performed signiﬁcantly better on learning outcome measures than their counterparts studying the same content with narrated instruction but no agent. This ﬁnding indicated that the presence of the agent enhanced the learning effectiveness of the multimedia
environment (i.e., image effect). Other studies (e.g., Dunsworth & Atkinson, 2007; Lester et al., 1997; Lusk & Atkinson, 2007; Moreno,
Mayer, & Lester, 2000; Moreno, Mayer, Spires, & Lester, 2001; Yilmaz & Kılıç-Çakmak, 2012) also showed that the presence of an
agent fostered learning in a multimedia environment. Kim and Ryu (2003) reviewed 28 studies and found a strong positive learning effect
for visually presented agents that are utilized to deliver instruction. In addition, past research revealed the positive impact of agents’
voices (e.g., personalized speech) and affective behaviors (e.g., facial expressions) on learners’ affective states (e.g., motivation and interest) in multimedia environments (Atkinson et al., 2005; Baylor & Kim, 2005, 2009; Kim & Baylor, 2006; Kim, Baylor, & Shen, 2007).
These ﬁndings provide further evidence of social-motivational aspects of agents. Additionally, Atkinson et al. (2005) found that learners
who studied worked examples that were narrated by an agent with a human voice rated the agent’s speech more positively and had
better performance on transfer test questions than their peers who studied examples accompanied by the same agent with a computer
voice. Therefore, learning, motivation and cognition should all be considered and investigated in multimedia environments, as these
three factors are inﬂuenced by different instructional methods and media (Brünken, Plass, & Moreno, 2010; Moreno, 2010; Moreno &
Mayer, 2007).
1.2. Cognitive load theory perspective
Cognitive load theory (CLT; Paas, Renkl, & Sweller, 2003; Schnotz & Kurschner, 2007; Sweller, 1994; Sweller, Ayres, & Kalyuga, 2011;
Sweller, van Merrienboer, & Paas, 1998) provides another theoretical framework for researchers to explain their ﬁndings in agent-based
learning environments. CLT is built around a multicomponent working memory model (Baddeley, 2007) that assumes humans process
information via dual sensory channelsdaudio/verbal channel and visual/pictorial channel and consequently have a limited working
memory capacity. During the learning process, learners must select relevant information from the two channels, organize it in working
memory and integrate it with their prior knowledge. This process is essential for learning, as it facilitates schema construction and the
transfer of information to long-term memory (Sweller, 2005). Learners experience cognitive load when their working memory capacity has
been exceeded.
There are three sources of cognitive loaddintrinsic load, extraneous load and germane load. Intrinsic load is due to the natural
complexity of the learning content that results from the number of interacting elements (element interactivity) necessary to process the task
(Sweller, 2005). More interactive elements increase the intrinsic load, the working memory load (Sweller, 2010) and the difﬁculty level of
the task. Extraneous load is caused by ineffective instructional design and should be reduced to promote learning. Finally, germane load is
caused by the necessary effortful processing that is required to facilitate schema acquisition. Regardless of the source, the underlying cause
of cognitive load that taxes limited working memory resources is proposed to be element interactivity (Sweller, 2010). Sweller suggested
that this notion may make it difﬁcult to assess how much load is caused by the different sources but that overall cognitive load can be still be
determined and there is “.no reason why the currently commonly used subjective ratings of task difﬁculty.cannot be used to determine
changes in overall cognitive load” (p. 128).
The design of instruction, or the instructional format, has the potential to impact how learners interact with a learning environment
and experience cognitive load. For example, it could be argued that a multimedia learning program designed with an animated agent has
no effect or even negative effect on learning. According to Harp and Mayer (1998), an animated agent that displays gestures, gaze, facial
expressions or locomotion may provide learners too many seductive details and cause learners to split their attention from relevant
information and consequently experience extraneous load (or additional element interactivity) in the learning environment. Results
revealed from several studies (Chen, 2012; Choi & Clark, 2006; Craig et al., 2002; Mayer, Dow, & Mayer, 2003) support this claim. For
instance, in Choi and Clark’s study (2006), either an animated agent or an arrow was used in a multimedia program to teach an English
language topic about relative clauses. However, the study failed to reveal any learning beneﬁts for those who learned from the animated
pedagogical agent. This ﬁnding is consistent with Mayer’s (Mayer, Dow, et al., 2003) results, who found that participants who studied
with an animated agent did not signiﬁcantly improve on the transfer test compared to their peers who learned without an agent.
Irrespective of theoretical orientation, the current education research literature on the effectiveness of animated agents is rich
with diverse research hypotheses and varied empirical outcomes (for review, see Heidig & Clarebout, 2011). In fact, some researchers
have concluded that no generalization can be made about whether it is advantageous to embed an agent in a learning environment.
Instead, research should investigate the speciﬁc conditions under which an agent enhances learning by taking into account a series of
potential moderators, such as learner characteristics, the agent’s functions, the agent’s design, learning environments, and the type of
knowledge (Atkinson et al., 2009; Johnson, DiDonato, & Reisslein, 2013; Kim & Wei, 2011; Ozogul, Johnson, Atkinson, & Reisslein,
2013; for review, see Dehn & van Mulken, 2000; Heidig & Clarebout, 2011). Therefore, they recommended that empirical research
should address the effect of a speciﬁc type of agent in a speciﬁc domain. In order to shed light on the mixed and inconclusive

L. Lin et al. / Computers & Education 67 (2013) 239–249

241

empirical results on animated pedagogical agents, the current study was designed to investigate the learning and motivational
beneﬁts of an animated agent that functioned to provide verbal feedback in a multimedia environment designed to deliver science
instruction.
1.3. Types of verbal feedback
Shute (2008) deﬁned feedback as “information communicated to the learner that is intended to modify his or her thinking or
behavior for the purpose of improving learning” (p. 154). Instructional designers considered feedback as one of the important elements of effective instruction (Sullivan & Higgins, 1983), as it has the potential to assist learners monitor their own learning (Butler &
Winne, 1995). In the past several decades, researchers have investigated the role of feedback in learning and instruction from multiple
perspectives, e.g., the timing of feedback (immediate feedback vs. delayed feedback, Schroth, 1992), the source of feedback (selfgenerated feedback vs. externally provided feedback, Andre & Thieman, 1988) and the degree of elaboration of feedback (simple
feedback vs. elaborate feedback, Moreno, 2004). To help researchers and practitioners better understand the effectiveness of feedback, a couple of models of feedback were proposed from review articles (Bangert-Drowns, Kulik, Kulic & Morgan, 1991; Butler &
Winne, 1995; Hattie & Timperley, 2007). The commonality of these models is that the effectiveness of feedback is related to a
range of factors internal (e.g., meta-cognition) and external (e.g., task level) to learners. This is supported by the results of a metaanalysis conducted by Azevedo and Bernard (1995), which revealed that the effect of a particular type of feedback was inconsistent in
the literature.
One category distinguishes feedback into simple and elaborate feedback based on the amount of information contained in the feedback
(Bangert-Drowns et al., 1991). Feedback can be as simple as a conﬁrmation of whether a learner’s response is correct or not (simple
feedback) or it can provide an explanation for correct and incorrect responses (elaborate feedback). In a review of 40 research studies
utilizing either computerized or non-computerized environments, Bangert-Drowns et al. (1991) found that results from studies that used
elaborate feedback produced larger effect sizes compared to results from studies that used simple feedback. Additionally, studies that
utilized computer-based learning environments also revealed results that showed the effectiveness of elaborate feedback (e.g., Narciss &
Huth, 2006; Pridemore & Klein, 1991). For instance, Pridemore and Klein (1991) found that participants who received elaborate feedback
outperformed their counterparts who received veriﬁcation feedback (i.e., simple feedback), regardless of whether learner control was
provided. One interpretation of this effect is that elaborate feedback cues the learners into a cognitive elaboration process, which enhances
deep understanding (Anderson & Reder, 1979).
One of the affordances of an animated pedagogical agent is its ability to serve as a source of verbal social cues (e.g., feedback)
when learners are interacting with the multimedia environment. Considering that the feedback is most effective when it fosters
cognitive processes (Azevedo & Bernard, 1995; Bangert-Drowns et al., 1991), it is possible and plausible to predict that providing
verbal feedback that is external to learners, facilitates positive learning outcomes in the agent-based environments. For instance,
participants in two studies (Moreno, 2004; Moreno & Mayer, 2005) completed an activity designing plants for various weather
conditions in a discovery game-like learning environment augmented with an animated agent (called Herman the Bug). The results
of both studies revealed that spoken explanatory feedback (i.e., elaborate feedback) provided by the agent promoted learning and
reduced perceived cognitive load more effectively than when the same agent provided simple feedback. However, as the review of
past literature revealed a wide range of variables that inﬂuence the effectiveness of feedback, it is worthwhile continuing to
investigate the interplay between the agent and feedback by extending previous studies by Moreno and her colleagues (Moreno,
2004; Moreno & Mayer, 2005) by using a non-gaming environment and incorporating a no agent control group to deliver
different types of feedback.
2. Overview of experiment
The purpose of the current study was to investigate the effects of an animated pedagogical agent that provided verbal feedback in a
multimedia learning environment. Speciﬁcally, the study was designed to test the social agency theory and the cognitive load theory by
exploring the effect of the agent (agent or no-agent) and type of feedback (simple or elaborate), as well as the potential interaction between
the agent and the type of feedback, on a learning outcome measure and perceived motivation and cognitive load ratings. The study
addressed three research questions within a multimedia learning environment: (a) How does the presence of an animated agent that
narrates instructional content impact learning, motivation and cognitive load? (b) How does the type of instructional feedback affect
learning, motivation and cognitive load? and, (c) How does the presence of the agent interact with the type of feedback with respect to
learning, motivation and cognitive load?
Two independent variables were manipulated in the studydthe presence of an agent (agent or no agent) and the type of verbal feedback
(simple or elaborate). The dependent variables were participants’ (a) learning outcomes, (b) subjective ratings of cognitive load, and (c)
subject ratings of motivation. Learning time was also included as an en-route variable.
Because of the mixed results in the literature (e.g., Dehn & van Mulken, 2000; Heidig & Clarebout, 2011) regarding the effect of animated
agents on learning, we tested hypotheses according to the social agency theory perspective as well as the cognitive load theory perspective.
We hypothesized that participants in the agent conditions would perform better on a learning outcome measure and report higher levels of
intrinsic motivation. The social agency theory supports this hypothesis and the assertion that agents have the capacity to evoke a learner’s
social schema and promote motivation to select and process relevant learning stimuli. However, from the cognitive load theory perspective,
we hypothesized that the presence of the agent woulddat a minimumdhave no impact on learning compared to the no agent conditions
and might even contribute to higher levels of perceived cognitive load. Thus, we hypothesized that the participants provided with an agent
would report higher levels of extraneous cognitive load.
In terms of feedback, we had several hypotheses. We hypothesized that participants who learned with elaborate feedback would perform
better on a learning outcome measure given that elaborate feedback provides instructional explanation while simple feedback does not.
Finally, we hypothesized from a social agency theory perspective that participants who learned with an agent providing elaborate feedback

242

L. Lin et al. / Computers & Education 67 (2013) 239–249

would outperform their peers learning from an agent providing simple feedback on the learning outcome measure. This hypothesis also
takes into account the impact of potential moderating variables (Azevedo & Bernard, 1995; Dehn & van Mulken, 2000; Heidig & Clarebout,
2011).
3. Method
3.1. Participants and design
The participants consisted of 135 undergraduate and graduate students from a southwestern university in the US. They were recruited
from a participant pool, as well as from ﬂyers and emails that were distributed throughout campus. A wide range of disciplines (Education,
Engineering, Music, Business, Journalism, etc), representing the general student population, participated in the study. Participants were
either paid a small stipend ($20) or received class credits for participation. The sample was comprised of 55 (41%) males and 80 (59%)
females. The average age of the participants was 26.01 (SD ¼ 9.21).
This study used a pretest–posttest, 2  2 factorial design; the ﬁrst factor was the agent presence (animated agent with narration vs.
narration only) and the second factor was the type of verbal feedback (simple feedback vs. elaborate feedback). Participants were randomly
assigned to one of the four conditions: (a) Agent/Simple (an animated agent narrating the content and providing simple feedback), (b) Agent/
Elaborate (an animated agent narrating the content and providing elaborate feedback), (c) No-agent/Simple (content and simple feedback
were presented with narration alone), and (d) No-agent/Elaborate (content and elaborate feedback were presented with narration alone).
3.2. Computer-based multimedia learning environment
The computer-based materials were composed of a multimedia learning environment that contained three lessons about thermodynamicsdLesson 1: Introduction to Thermal Energy Transfer, Lesson 2: Thermal Energy Transfer by Conduction, and Lesson 3: Thermal
Energy Transfer by Convection. The learning environment consisted of 23 screens and was created in Visual Basic and embedded with
animated movies created in Adobe Flash. In each of the four experimental conditions, participants viewed the same number of screens (i.e.,
23) embedded with animations about thermodynamics. There were no time constraints imposed on learners and thus the degree of learner
control was consistent across all four conditions. There were a total of 12 multiple-choice practice questions (four in Lesson 1, three in Lesson
2 and ﬁve in Lesson 3) dispersed among the content screens. Practice was included in the design of the learning environment as it is an
essential component of effective instruction, its inclusion helps to emulate an authentic learning environment and it was the activity for
which feedback was provided.
Each condition differed with regard to the presence of the agent and the type of verbal feedback that learners received when they
responded to the practice questions. In the Agent/Simple condition, a female human agent (head and shoulders shot) appeared on each
screen, narrated the instructional content (see Fig. 1) and provided simple feedback verifying right or wrong each time after the participant
responded to a multiple-choice practice question (see Fig. 2). The participant would receive the statement “Yes, that’s correct” when
correctly answering the practice question, or receive the statement “No, that’s wrong” when incorrectly answering the practice question. In
Agent/Elaborate condition, the same agent, positioned at the same location on the screen as the agent in the Agent/Simple condition,
delivered auditory instruction and provided elaborate feedback on each of the participant’s responses to the practice questions. The extent
of the agent’s non-verbal social cues was controlled for in the two agent conditions. Speciﬁcally, the agent’s facial expression, voice, clothing

Fig. 1. Agent narrating the content.

L. Lin et al. / Computers & Education 67 (2013) 239–249

243

Fig. 2. Agent providing the feedback.

and the degree and timing of the head movement were exactly the same in Agent/Simple condition and Agent/Elaborate condition. The
elaborate feedback in Agent/Elaborate condition not only included veriﬁcation of right or wrong, but also included information as to why the
answer was right or wrong. An example of this type of feedback was “Your answer is wrong because temperature is a measure of the average
kinetic energy of the particles in a substance, not a process of energy transfer.” Within the computer-based learning environment, the
interface for the No-agent/Simple condition and No-agent/Elaborate condition were almost identical to the conditions with the agent
present, except that the participants could not view the agent and only heard the narrated learning content (see Fig. 3) and feedback during
the practice activity (see Fig. 4). Although the presence of the agent differed across the four conditions (agent and no agent), the audio
narrations were the exactly the same.
A tutorial screen (Fig. 5) was launched by the computer program before the instruction started. The purpose of providing participants
with a tutorial screen was to explain the controls of navigation and animation of the learning environment. No content-related graphics,
narration or agent appeared on the screen.

Fig. 3. No agent (narration only) delivering the content.

244

L. Lin et al. / Computers & Education 67 (2013) 239–249

Fig. 4. No agent (narration only) providing the feedback.

3.3. Measures and instruments
Participants’ prior knowledge about the learning contentdthermodynamicsdwas measured by a pretest of 20 multiple-choice questions. Each test question had four choicesdone correct answer and three distracters. A participant received zero points for each incorrect
answer and one point for each correct answer. Therefore, a maximum of 20 points could be achieved. Participants’ performance on the
pretest was automatically scored by the computer-based program. A posttest was administered immediately after the instruction to
measure learning outcomes. The posttest was identical to the pretest but the items were reordered. It had the same format, the same
number of test questions and the same scoring method as the pretest. In addition to the pretest and posttest, 12 multiple-choice practice
questions, which had the same format and scoring method, were interspersed between screens of the content. Each practice question was
related to the content of the screen or screens immediately preceding it and practice opportunities were similar for each content area
included on the posttest.

Fig. 5. Tutorial screen.

L. Lin et al. / Computers & Education 67 (2013) 239–249

245

Table 1
Cognitive load measurement.
Item

1.
2.
3.
4.

How
How
How
How

difﬁcult was the lessons?
much mental effort did it take to learn the lessons?
hard was it to navigate through the lessons?
frustrated were you during the lessons?

Four subjective questions were used to measure cognitive load (see Table 1). They were adapted from the NASA-TLX (Hart & Staveland,
1988) and were described by Gerjets, Scheiter and Catrambone (Gerjets, Scheiter, & Catrambone, 2004; Scheiter, Gerjets, & Catrambone,
2006). Each cognitive load question contained a Likert-type rating scale from 1 (very low cognitive load) to 8 (very high cognitive load).
The focus of the questions was to measure participants’ perception of task difﬁculty as this is the aspect of cognitive load that reﬂects
element interactivity and is an indicator of overall cognitive load (Sweller, 2010). An item speciﬁcally assessing perception of task difﬁculty
was used along with three additional items related to cognitive load: perception of effort, ease of task, and frustration. The use of the
additional items was exploratory and a factor analysis was conducted to determine the factor structure for each of the items as they related
to the construct of cognitive load.
There were 15 statements used to measure participants’ intrinsic motivation, which was adapted from McAuley, Duncan, and Tammen
(1989) and Ryan (1982). This intrinsic motivation scale included six subscalesdinterest, competence, value, effort, pressure and choice (see
Table 2). Participants ranked each item on an 8-point Likert scale ranging from 1 (“not at all true”) to 8 (“very true”). Negatively worded
items were reverse-scored such that higher scores reﬂect more positive motivation.
3.4. Procedure
The experiment was conducted in a controlled multimedia laboratory setting. First, participants signed a consent form to participate and
were then seated at an individual cubicle, facing a computer monitor. Next, a researcher informed participants about the goal and procedure
of the experiment. However, participants were left unaware of the experimental conditions and the research questions. Then, they
completed the pretest on the computer with no time limit and were randomly assigned to a condition with an experiment ID number.
Experiment ID numbers were used to preserve the anonymity of each participant. When the participants completed the lesson, a posttest
and a questionnaire were administered, both of which did not have a time limit. When these items were completed, participants were
thanked and either paid or provided course credit. The study was approximately 60 min in duration.
4. Results
All participants’ data were included in the analysis for two reasons: (a) there were no missing cases; and (b) the results of preliminary
data screening showed no outliers. Table 3 presents the means and standard deviations (in parentheses) of participants’ (a) total pretest
scores, (b) total posttest scores, and (c) adjusted total posttest scores, where appropriate. Family-wise alpha was set at the .05 level. Cohen’s f
was used as an effect size measure with .10, .25 and .40 deﬁned as small, medium and large effect sizes, respectively (Cohen, 1988).
4.1. Prior knowledge
An analysis of variance (ANOVA) was conducted to evaluate whether participants’ prior knowledge signiﬁcantly differed across the four
experimental conditions (i.e., Agent/Simple, Agent/Elaborate, No-agent/Simple and No-agent/Elaborate). There was no signiﬁcant main
effect for the agent presence or the type of verbal feedback (both Fs < 1.00 and both ps > .50), nor was there a signiﬁcant interaction effect,
F(1, 131) ¼ 2.18, p ¼ .14.
Table 2
Intrinsic motivation items.
Item

1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.
12.
13.
14.
15.

Subscale

I thought it was a boring activity.
I think I was pretty good at this activity.
I think that doing this activity could be useful.
I didn’t try very hard to do well at this activity.
I did not feel nervous at all while doing this.
I believe I had some choice about doing this activity.
It was important to me to do well at this task.
I believe doing this activity could be beneﬁcial to me.
I felt very tense while doing this activity.
I did this activity because I had no choice.
This activity was fun to do.
I put a lot of effort into this.
This was an activity that I couldn’t do very well.
I believe this activity could be of some value to me.
I would describe this activity as very interesting.

Interest
Competence
Value
Effort
Pressure
Choice
Effort
Value
Pressure
Choice
Interest
Effort
Competence
Value
Interest

246

L. Lin et al. / Computers & Education 67 (2013) 239–249

Table 3
Mean and standard deviations of test scores.
Agent

Pretest
Posttest
Difﬁculty
Mental effort
Navigation
Frustration
Overall CLa
Interest/IMb
Competence/IM
Value/IM
Effort/IM
Pressure/IM
Choice/IM
Timec

No Agent

Simple feedback (n ¼ 32)

Elaborate feedback (n ¼ 34)

M

SD

M

SD

11.16
14.87
3.97
3.91
1.69
2.41
2.99
5.94
5.67
4.88
5.55
4.05
3.78
14.18

4.45
3.84
2.06
1.91
1.49
1.62
1.35
1.16
1.33
.95
1.23
1.40
.90
3.26

9.94
15.50
3.76
4.12
1.62
2.12
2.90
6.09
6.01
5.17
5.54
4.22
3.97
14.90

3.10
3.20
1.72
1.74
1.13
1.23
1.07
1.25
1.27
1.05
1.22
1.36
.97
2.72

Adj. M
14.52

Adj. M
16.05

Simple feedback (n ¼ 35)
M

SD

10.51
15.00
4.17
4.31
1.63
2.37
3.12
6.10
5.83
4.95
5.57
3.87
3.76
14.42

3.70
3.68
1.81
2.13
1.06
1.77
1.27
1.59
1.25
1.35
1.13
1.17
1.09
2.83

Adj. M
15.12

Elaborate feedback (n ¼ 34)
M

SD

11.15
15.35
3.53
3.53
1.65
2.93
2.68
6.34
6.01
4.83
5.91
3.97
3.93
14.72

3.19
3.19
1.76
1.83
.081
1.22
1.13
1.20
1.41
1.10
1.17
1.11
.75
2.54

Adj. M
15.01

Note. The maximum scores of both pretest and posttest were 20.
a
Overall CL ¼ Overall Cognitive Load.
b
IM ¼ Intrinsic Motivation.
c
The unit of time is minute.

4.2. Learning outcomes
A two-way analysis of covariance (ANCOVA) was conducted. The ﬁrst factor was agent presence with two levels, agent and no-agent; the
second factor was type of feedback with two levels, elaborate and simple. The dependent measure was the score on the posttest and the
covariate was the score on the pretest. An a priori analysis of the homogeneity-of-slopes assumption indicated that the relationship between
the pretest score and the posttest score did not differ signiﬁcantly as a function of the agent presence or the type of feedback (both Fs < 1.00).
The ANCOVA showed a signiﬁcant interaction, F(1, 130) ¼ 4.60, MSE ¼ 4.88, p ¼ .03, f ¼ .19 (see Fig. 6). However, the main effect for agent
presence was not signiﬁcant, F(1, 130) ¼ .34, MSE ¼ 4.88, p ¼ .56, f ¼ .05, and neither was the main effect for type of feedback, F(1, 130) ¼ 3.43,
MSE ¼ 4.88, p ¼ .07, f ¼ .16.
To clarify the signiﬁcant interaction effect, we conducted follow-up analyses of simple main effects controlling for the effect of pretest
scores. We found that participants in the Agent/Elaborate condition (Mean ¼ 15.50, SD ¼ 3.20, adjusted M ¼ 16.05) achieved higher posttest
scores than their counterparts in the Agent/Simple condition (Mean ¼ 14.87, SD ¼ 3.84, adjusted M ¼ 14.52), F(1, 130) ¼ 7.77, p ¼ .006, f ¼ .24.
All of the remaining analyses were non-signiﬁcant (all Fs < 1.20 and all ps > .28).
4.3. Cognitive load
Four separate ANOVAs were conducted to evaluate the effects of agent and type of feedback on learners’ perceived difﬁculty, mental
effort, navigation of the environment and frustration respectively. There were no signiﬁcant main effects or interaction effects (all Fs < 2.30
and all ps > .13).
In addition, a conﬁrmatory factor analysis was conducted on the four cognitive load measures to determine whether the four measures
assess a single underlying factor of cognitive load. The ﬁt of the hypothesized single-factor model was assessed by the robust maximum

Adjusted Posttest Scores of the Four Conditions

Fig. 6. Interaction effect on adjusted posttest scores.

L. Lin et al. / Computers & Education 67 (2013) 239–249

247

likelihood estimation in Mplus 6.1. The results showed that the hypothesized model had an acceptable ﬁt, c2(2) ¼ 6.14, p ¼ .047, CFI ¼ .97,
SRMR ¼ .05, RMSEA ¼ .12, 90% CI [.01–.23]. Based on this empirical evidence, the mean of the four cognitive load measures was computed
for each participant to represent the overall cognitive load. A two-way ANOVA was then conducted to assess the potential effect of agent and
feedback on the overall cognitive load. However, there were no signiﬁcant main effects or interaction (all Fs < 1.60 and all ps > .21).
4.4. Intrinsic motivation
Means of participants’ ratings on each of the six subscalesdinterest, competence, value, effort, pressure and choicedof intrinsic
motivation measures were computed. A two-way multivariate analysis of variance (MANOVA) was conducted to determine the effects of the
agent presence and the type of verbal feedback on these six subscales. No signiﬁcant differences were found on the six subscales for the
agent presence main effect, Wilks’ L ¼ .98, F(5, 127) ¼ .48, p ¼ .79, f ¼ .14, or for the type of verbal feedback main effect, Wilks’ L ¼ .97, F(5,
127) ¼ .85, p ¼ .52, f ¼ .18. The interaction effect was also non-signiﬁcant, Wilks’ L ¼ .93, F(5, 127) ¼ .96, p ¼ .41, f ¼ .20.
4.5. Learning time
A two-way ANOVA was conducted to evaluate the effects of agent and the type of verbal feedback on the learning time (in minutes) spent
by the participants in the multimedia environment. There was no main effect for agent, no main effect for the type of verbal feedback, and no
interaction effect (all Fs < 1.09 and all ps > .30).
5. Discussion
The ﬁndings in the educational research literature regarding the effects of animated pedagogical agents (image effect) are varied and
inconclusive. Results from some studies support the agent’s image effect (e.g., Atkinson, 2002; Dunsworth & Atkinson, 2007; Lester et al.,
1997) while others do not (e.g., Moreno et al., 2001). This study was designed to explore Dehn and van Mulken’s (2000) recommendation to
study a speciﬁc type of agent in a speciﬁc domain and attempt to disentangle the complex design issues involved in creating and delivering
instruction via a pedagogical agent in a multimedia learning environment for college students. We investigated the learning, motivational,
and cognitive beneﬁts of utilizing an animated agent to provide two different types of verbal feedback during an instructional multimedia
science module about thermodynamics.
5.1. How does the presence of an animated agent that narrates instructional content impact learning, motivation and cognitive load?
There were no signiﬁcant main effects for the agent factor on learning outcome measures or for perceived motivation. Thus, there was no
support for our hypothesis based on social agency theory that the presence of an agent would support learning and motivation. Instead, the
ﬁnding that the participants who learned without the presence of the agent performed equivalent to participants provided with an agent on
the learning measure provides partial support for our hypothesis based on cognitive load theory, i.e., the agent’s presence would not foster
learning. On the other hand, since cognitive load theory also suggests that the presence of an animated agent could be a source of cognitive
load, it is also reasonable to anticipate that learners in the agent conditions would report higher levels of perceived cognitive load. However,
this was not the case as participants experienced the agent and no-agent conditions similarly with regard to self-report measures of
motivation and cognitive load as there were no signiﬁcant mean differences among conditions. This ﬁnding provides evidence that participants did not necessarily experience the agent conditions with an increased level of cognitive load as indicated in the previous studies
(Choi & Clark, 2006; Craig et al., 2002; Mayer, Dow, et al., 2003). We can postulate that the agent’s presence in the learning environment did
not appear to impose an additional amount of element interactivity that could be detected by the self-report measure. Participants in the
agent conditions did not report signiﬁcantly higher levels of perceived cognitive load despite the fact that our animated agent was programmed with head movement, gaze, and lip-synced narration. An alternate explanation is that our cognitive load measurement tool may
not have been sufﬁciently sensitive to changes in perceived levels of cognitive load. Further research is needed to determine how best to
measure cognitive load in multimedia environments.
5.2. How does the type of instructional feedback affect learning, motivation and cognitive load?
Likewise, there were no signiﬁcant main effects for the type of feedback on the learning outcome measure, and consequently, we did
not ﬁnd support for our hypothesis that participants would perform better when provided with elaborate rather than simple feedback.
While we did not ﬁnd a statistically signiﬁcant difference, the descriptive statistics suggested an advantage for the elaborate feedback
condition given the estimated effect size (f ¼ .16) and that the difference approached signiﬁcance (p ¼ .07). While we cannot deﬁnitively
state that the elaborate feedback fostered learning to a greater degree than simple feedback, our descriptive statistics are consistent with
previous ﬁndings documenting the advantage of elaborate feedback. As mentioned previously, we did not ﬁnd any signiﬁcant main effect for
the feedback factor on the measures of intrinsic motivation instrument or cognitive load.
There are various perspectives to distinguish different feedback (Bangert-Drowns et al., 1991). In this study, we investigated the type of
feedback in an agent-based learning environment, in which an animated agent provided verbal feedback with different amount of information,
i.e., simple feedback and elaborate feedback. Other types of feedback, such as the feedback modality (text vs. audio), may also impact learning
outcomes, motivation and cognitive load in the agent-based environment. Future research should further investigate these interesting issues.
5.3. How does the presence of the agent interact with the type of feedback with respect to learning, motivation and cognitive load?
We did ﬁnd a signiﬁcant interaction between the presence of an agent and the type of feedback it provided during learning. Our results
indicated that learners beneﬁted the most from the animated agent that provided elaborate verbal feedback relative to an agent that

248

L. Lin et al. / Computers & Education 67 (2013) 239–249

provided simple feedback. This ﬁnding is consistent with social agency theory as there is an indication that the agent giving elaborate
feedback potentially enriched the learning environment by providing a higher degree of social cues (Atkinson et al., 2005; Mayer, Sobko,
et al., 2003). In other words, the animated pedagogical agent providing elaborate feedback is better able to evoke learners’ social schema
and thus facilitate deep processing and meaningful learning compared to the same agent providing simple feedback. This result, in combination with the ﬁndings that there were no main effects for agent or for type of feedback, contributes to the current literature by directly
addressing the question “When is a pedagogical agent effective?” The result supports the notion that the effectiveness of an agent in a
multimedia environment depends on the type of feedback a pedagogical agent delivers, one of the functions that an agent executes (Heidig
& Clarebout, 2011). However, it is unclear if the learning effect is a result of including an instructional manipulation, such as the provision of
elaborate feedback, or a result of the agent “behaving” in a manner that was more consistent with learners’ expectations of a mentor or
teacher. Perhaps learners experienced the agent that offered elaborate feedback as a more authentic learning companion and therefore it
was more effective at evoking learners’ social schema and promoting them to adopt the cooperation principle in their interaction with the
computer environment and learning process.
The ﬁndings in this study only partially support the agent’s image effect in the multimedia environment on condition that the animated
agent presents narrated elaborate feedback to learners’ responses on the multiple-choice practice questions. This implicates that the
learning beneﬁts of incorporating an agent in the multimedia environment may not be applied to the general instructional settings, but to
some speciﬁc settings where a human-like agent is designed to provide maximum verbal social cues, such as elaborate feedback provided by
a visually presented agent. Instructional design and development of the agent-based learning environments should not only take into
account the technological aspects of the animated agent, but also the cognitive aspects of the agent or the computer-based learning
environment. In future research, we will further investigate the moderating effect of the type of feedback in an agent-based learning
environment. For instance, we can consider manipulating the presence of an animated agent either in the content delivery phase or in the
practice activity providing different types of feedback so that we can clearly investigate the potential moderating effect of different types of
feedback.
The current study provides evidence that learning is fostered when an animated agent provides learners instructional explanations (i.e.,
elaborate feedback) via human narration after they respond to practice questions. It is possible learners are prompted by the agent’s social
cues, especially the elaborate verbal feedback, to self-explain what they have learned and, in turn, this possible self-explanation process
leads to better learning and understanding (Chi, de Leeuw, Chiu, & LaVancher, 1994). However, the current study did not collect any selfexplanation data, which limits our ability to attribute the learning effects to this type of process. In future research, we will consider using methods to collect data of learners’ self-explanation, such as think aloud method or written self-explanations. By doing so, we could
have a clearer idea of learners’ inner mechanism in a multimedia environment augmented with an animated agent.
We need to note that the animated agent used in the current study is a human female character coupled with human female voice
recordings. The study conducted by Lattner, Meyer, and Friederici (2005) indicated that learners preferred a female voice to a male voice,
whereas Harrison and Atkinson (2009) found no impact of the agent’s gender on learning. Thus, it is possible that an animated pedagogical
agent’s gender may have differential impacts on learners. Moreover, an agent’s degree of anthropomorphism may impact its effectiveness in
supporting learning. In future research, we intended to investigate whether we see the same interaction effect between agent and feedback
when a human male character or a non-human cartoon character provides elaborate verbal feedback.
We also found that learners spent equivalent amount of time learning the content in the environment, regardless of the type of
feedback provided or the presence/absence of the agent. Taking into account that there was no substantial difference of learning times
across the four conditions, we can conclude that it is not the amount of instructional explanations included in the feedback but the
function of an agent that provides feedback that impact on learning in the multimedia environment. The current ﬁndings suggest that
elaborate verbal feedback has the potential to promote learning in the agent-based environment when social cues are maximally provideddthe presence of an animated agent plus verbal narration. Additionally, it is worth noting that participants presented with an agent
that provided simple narration performed descriptively lower on the learning measure than their counterparts in the two non-agent
conditions (no-agent plus simple feedback, no-agent plus elaborate feedback). This suggests that when the agent in our study provided simple feedback it was a detriment to learning. If we consider this ﬁnding in light of both the social agency theory and cognitive
load theory, perhaps when an agent does not behave as a learner expects (offering useful explanations to promote learning), learners
experience are less likely to adopt the cooperation principle, thereby limiting the degree to which they immerse themselves into the
computer-based environment and learning process.
6. Conclusion
The results of the study indicate that an animated agent’s ability to foster learning when deployed in a computer-based multimedia
learning environment is moderated by instructional components, speciﬁcally the type of verbal feedback that an agent delivers. This study
supports the idea that different types of verbal feedback may moderate the effect of the presence of an animated agent (image effect). It also
suggests that when a computer-based multimedia learning environment is complemented by an animated agent for college students to
obtain knowledge or skills in a certain domain, instructional designers should consider incorporating an agent that can optimally provide
verbal social cues, such as elaborate feedback.
References
Anderson, J. R., & Reder, L. M. (1979). An elaborative processing explanation of depth of processing. In L. S. Cermak, & F. I. M. Craik (Eds.), Levels of processing in human
memory). Hillsdale, NJ: Erlbaum.
Andre, T., & Thieman, A. (1988). Level of adjunct question, type of feedback, and learning concepts by reading. Contemporary Educational Psychology, 13, 296–307.
Atkinson, R. K. (2002). Optimizing learning from examples using animated pedagogical agents. Journal of Educational Psychology, 94(2), 416–427.
Atkinson, R. K., Foshee, C., Harrison, C., Lin, L., Joseph, S., & Christopherson, R. (2009). Does the type and degree of animation present in a visual representation accompanying
narration in a multimedia environment impact learning?. In Proceedings of world conference on educational multimedia, hypermedia and telecommunications 2009 (pp.
726–734) Chesapeake, VA: AACE.

L. Lin et al. / Computers & Education 67 (2013) 239–249

249

Atkinson, R. K., Mayer, R. E., & Merrill, M. M. (2005). Fostering social agency in multimedia learning: examining the impact of an animated agent’s voice. Contemporary
Educational Psychology, 30(1), 117–139.
Azevedo, R., & Bernard, R. M. (1995). A meta-analysis of the effects of feedback in computer-based instruction. Journal of Educational Computing Research, 13(2), 111–127.
Baddeley, A. (2007). Working memory, thought, and action. New York, NY: Oxford University Press.
Bangert-Drowns, R. L., Kulik, C. C., Kulik, J. A., & Morgan, M. (1991). The instructional effect of feedback in test-like events. Review of Educational Research, 61(2), 213–238.
Baylor, A. L., & Kim, Y. (2005). Simulating instructional roles through pedagogical agents. International Journal of Artiﬁcial Intelligence in Education, 15, 95–115.
Baylor, A. L., & Kim, Y. (2009). Designing nonverbal communication for pedagogical agents: when less is more. Computers in Human Behavior, 25(2), 450–457.
Brünken, R., Plass, J. L., & Moreno, R. (2010). Current issues and open questions in cognitive load research. In J. L. Plass, R. Moreno, & R. Brünken (Eds.), Cognitive load theory
(pp. 253–272). New York, NY, US: Cambridge University Press.
Butler, D. L., & Winne, P. H. (1995). Feedback and self-regulated learning: a theoretical synthesis. Review of Educational Research, 65(3), 245–281.
Chen, Z. H. (2012). We care about you: incorporating pet characteristics with educational agents through reciprocal caring approach. Computers & Education, 59, 1081–1088.
Chi, M. T. H., de Leeuw, N., Chiu, M., & LaVancher, C. (1994). Eliciting self-explanations improves understanding. Cognitive Science: A Multidisciplinary Journal, 18(3), 439–477.
Choi, S., & Clark, R. E. (2006). Cognitive and affective beneﬁts of an animated pedagogical agent for learning English as a second language. Journal of Educational Computing
Research, 34(4), 441–466.
Cohen, J. (1988). Statistical power analysis for the behavioral sciences (2nd ed.). Hillsdale, N.J.: L. Erlbaum Associates.
Craig, S. D., Gholson, B., & Driscoll, D. M. (2002). Animated pedagogical agents in multimedia educational environments: effects of agent properties, picture features and
redundancy. Journal of Educational Psychology, 94(2), 428–434.
Dehn, D. M., & van Mulken, S. (2000). The impact of animated interface agents: a review of empirical research. International Journal of Human-Computer Studies, 52(1), 1–22.
Dunsworth, Q., & Atkinson, R. K. (2007). Fostering multimedia learning of science: exploring the role of an animated agent’s image. Computers & Education, 49(3), 677–690.
Gerjets, P., Scheiter, K., & Catrambone, R. (2004). Designing instructional examples to reduce intrinsic cognitive load: molar versus modular presentation of solution procedures. Instructional Science, 32(1–2), 33–58.
Grice, H. P. (1975). Logic and conversation. In P. Cole, & J. Morgan (Eds.). Syntax and semantics, Vol. 3, (pp. 41–58). New York: Academic Press.
Harp, S. F., & Mayer, R. E. (1998). How seductive details do their damage: a theory of cognitive interest in science learning. Journal of Educational Psychology, 90(3), 414–434.
Harrison, C., & Atkinson, R. K. (2009). Narration in multimedia learning environments: exploring the impact of voice origin, gender, and presentation mode. In G. Siemens, &
C. Fulford (Eds.), Proceedings of world Conference on educational multimedia, hypermedia and telecommunications 2009 (pp. 980–985). Chesapeake, VA: AACE.
Hart, S. G., & Staveland, L. E. (1988). Development of NASA-TLX (Task Load Index): results of experimental and theoretical research. In P. A. Hancock, & N. Meshkati (Eds.),
Human mental workload (pp. 139–183). Amsterdam: North-Holland.
Hattie, J., & Timperley, H. (2007). The power of feedback. Review of Educational Research, 77, 81–112.
Heidig, S., & Clarebout, H. (2011). Do pedagogical agents make a difference to student motivation and learning? Educational Research Review, 6, 27–54.
Johnson, A. M., DiDonato, M. D., & Reisslein, M. (2013). Animated agents in K-12 engineering outreach: preferred agent characteristics across age levels. Computers in Human
Behavior, 29, 1807–1815.
Kim, Y., & Baylor, A. L. (2006). A social-cognitive framework for pedagogical agents as learning companions. Educational Technology Research and Development, 54, 569–590.
Kim, Y., Baylor, A. L., & Shen, E. (2007). Pedagogical agents as learning companions: the impact of agent emotion and gender. Journal of Computer Assisted Learning, 23,
220–234.
Kim, M., & Ryu, J. (2003). Meta-analysis of the effectiveness of pedagogical agent. In D. Lassner, & C. McNaught (Eds.), Proceedings of world conference on educational
multimedia, hypermedia and telecommunications 2003 (pp. 479–486). Chesapeake, VA: AACE.
Kim, Y., & Wei, Q. (2011). The impact of learner attributes and learner choice in an agent-based environment. Computers & Education, 56, 505–514.
Lattner, S., Meyer, M. E., & Friederici, A. D. (2005). Voice perception: sex, pitch, and the right hemisphere. Human Brain Mapping, 24(1), 11–20.
Lester, J. C., Converse, S. A., Kahler, S. E., Barlow, S. T., Stone, B. A., & Bhoga, R. S. (1997). The personal effect: affective impact of animated pedagogical agents. In Proceedings of
CHI_97 (pp. 359–366). New York: ACM Press.
Lusk, M. M., & Atkinson, R. K. (2007). Animated pedagogical agents: does their degree of embodiment impact learning from static or animated work examples? Applied
Cognitive Psychology, 21(6), 747–764.
McAuley, E., Duncan, T., & Tammen, V. V. (1989). Psychometric properties of the Intrinsic Motivation Inventory in a competitive sport setting: a conﬁrmatory factor analysis.
Research Quarterly for Exercise and Sport, 60, 48–58.
Mayer, R. E. (2005). Cognitive theory of multimedia learning. In R. E. Mayer (Ed.), The Cambridge handbook of multimedia learning (pp. 31–48). New York, NY, US: Cambridge
University Press.
Mayer, R. E., Dow, G. T., & Mayer, S. (2003). Multimedia learning in an interactive self-explaining environment: what works in the design of agent-based microworlds? Journal
of Educational Psychology, 95(4), 806–812.
Mayer, R. E., Sobko, K., & Mautone, P. D. (2003). Social cues in multimedia learning: role of speaker’s voice. Journal of Educational Psychology, 95(2), 419–425.
Moreno, R. (2004). Decreasing cognitive load for novice students: effects of explanatory versus corrective feedback in discovery-based multimedia. Instructional Science, 32(1),
99–113.
Moreno, R. (2007). Optimizing learning from animations by minimizing cognitive load: cognitive and affective consequences of signaling and segmentation methods. Applied
Cognitive Psychology, 21, 1–17.
Moreno, R. (2010). Cognitive load theory: more food for thought. Instructional Science, 38(2), 135–141.
Moreno, R., & Mayer, R. E. (2005). Role of guidance, reﬂection, and interactivity in an agent-based multimedia game. Journal of Educational Psychology, 97(1), 117–128.
Moreno, R., & Mayer, R. (2007). Interactive multimodal learning environments. Educational Psychology Review, 19(3), 309–326.
Moreno, R., Mayer, R., & Lester, J. (2000). Life-like pedagogical agents in constructivist multimedia environments: cognitive consequences of their interaction. In J. Bourdeau, &
R. Heller (Eds.), Proceedings of world conference on educational multimedia, hypermedia and telecommunications 2000 (pp. 776–781). Chesapeake, VA: AACE.
Moreno, R., Mayer, R. E., Spires, H. A., & Lester, J. C. (2001). The case for social agency in computer-based teaching: do students learn more deeply when they interact with
animated pedagogical agents? Cognition and Instruction, 19(2), 177–213.
Narciss, S., & Huth, K. (2006). Fostering achievement and motivation with bug-related tutoring feedback in a computer-based training for written subtraction. Learning and
Instruction, 16(4), 310–322.
Ozogul, G., Johnson, A. M., Atkinson, R. K., & Reisslein, M. (2013). Investigating the impact of pedagogical agent gender matching and learner choice on learning outcomes and
perceptions. Computers & Education, 67, 36–50.
Paas, F., Renkl, A., & Sweller, J. (2003). Cognitive load theory and instructional design: recent developments. Educational Psychologist, 38(1), 1–4.
Pridemore, D. R., & Klein, J. D. (1991). Control of feedback in computer-assisted instruction. Educational Technology Research and Development, 39(4), 27–33.
Reeves, B., & Nass, C. (1996). The media equation. New York: Cambridge University Press.
Ryan, R. M. (1982). Control and information in the intrapersonal sphere: an extension of cognitive evaluation theory. Journal of Personality and Social Psychology, 43, 450–461.
Scheiter, K., Gerjets, P., & Catrambone, R. (2006). Making the abstract concrete: visualizing mathematical solution procedures. Computers in Human Behavior, 22, 9–25.
Schnotz, W., & Kurschner, C. (2007). A reconsideration of cognitive load theory. Educational Psychology Review, 19(4), 469–508.
Schroth, M. L. (1992). The effects of delay of feedback on a delayed concept formation transfer task. Contemporary Educational Psychology, 17, 78–82.
Shute, V. J. (2008). Focus on formative feedback. Review of Educational Research, 78(1), 153–189.
Sullivan, H., & Higgins, N. (1983). Teaching for competence. New York, US: Teachers College Press.
Sweller, J. (1994). Cognitive load theory, learning difﬁculty, and instructional design. Learning and Instruction, 4(4), 295–312.
Sweller, J. (2005). Implications for cognitive load in multimedia learning. In R. E. Mayer (Ed.), The Cambridge handbook of multimedia learning (pp. 19–30). New York, NY:
Cambridge University Press.
Sweller, J. (2010). Element interactivity and intrinsic, extraneous, and germane cognitive load. Educational Psychology Review, 22, 123–138.
Sweller, J., Ayres, P., & Kalyuga, S. (2011). Cognitive load theory. New York: Springer.
Sweller, J., van Merrienboer, J. J. G., & Paas, F. G. W. C. (1998). Cognitive architecture and instructional design. Educational Psychology Review, 10(3), 251–296.
Van der Meij, H. (2013). Motivating agents in software tutorials. Computers in Human Behavior, 29, 845–857.
Yilmaz, R., & Kılıç-Çakmak, E. (2012). Educational interface agents as social models to inﬂuence learner achievement, attitude and retention of learning. Computers & Education, 59, 828–838.

Affective	
  Computing	
  Meets	
  Design	
  Patterns:	
  A	
  Pattern-­‐Based	
  
Model	
  for	
  a	
  Multimodal	
  Emotion	
  Recognition	
  Framework	
   	
  
JAVIER	
  GONZALEZ-­‐SANCHEZ,	
  MARIA-­‐ELENA	
  CHAVEZ-­‐ECHEAGARAY,	
  	
  
ROBERT	
  ATKINSON,	
  AND	
  WINSLOW	
  BURLESON	
  
Arizona	
  State	
  University	
  	
  
There	
   is	
   a	
   growing	
   interest	
   in	
   how	
   to	
   leverage	
   information	
   about	
   users’	
   emotions	
   as	
   a	
   mean	
   of	
   personalizing	
   the	
   response	
   of	
   computer	
  
systems.	
   This	
   is	
   particularly	
   useful	
   for	
   computer-­‐aided	
   learning,	
   health,	
   and	
   entertainment	
   systems.	
   However,	
   there	
   are	
   few	
   architectures,	
  
frameworks,	
   libraries,	
   or	
   software	
   tools	
   that	
   allow	
   developers	
   to	
   easily	
   integrate	
   emotion	
   recognition	
   into	
   their	
   software	
   projects.	
   The	
  
work	
   reported	
   in	
   this	
   paper	
   offers	
   a	
   way	
   to	
   address	
   this	
   shortcoming	
   in	
   models	
   by	
   proposing	
   the	
   use	
   of	
   software	
   design	
   patterns	
   for	
  
modeling	
   a	
   multimodal	
   emotion	
   recognition	
   framework.	
   The	
   framework	
   is	
   designed	
   to:	
   (1)	
   integrate	
   existing	
   sensing	
   devices	
   and	
   SDK	
  
platforms,	
   (2)	
   include	
   diverse	
   inference	
   algorithms,	
   and	
   (3)	
   correlate	
   measurements	
   from	
   diverse	
   sources.	
   We	
   describe	
   our	
   experience	
  
using	
   this	
   model	
   and	
   its	
   impact	
   on	
   facets,	
   such	
   as	
   creating	
   a	
   common	
   language	
   among	
   stakeholders,	
   supporting	
   an	
   incremental	
  
development,	
  and	
  adjusting	
  to	
  a	
  highly	
  shifting	
  development	
  team,	
  as	
  well	
  as	
  the	
  qualities	
  achieved	
  and	
  trade-­‐offs	
  made.	
  
Categories	
   and	
   Subject	
   Descriptors:	
   D.2.10	
   [Software	
   Engineering]:	
   Design;	
   D.2.11	
   [Software	
   Engineering]:	
   Software	
   Architecture;	
  
H.1.2	
  [User/Machine	
  Systems]:	
  Human	
  Information	
  Processing;	
  
General	
  Terms:	
  Design,	
  Human	
  Factors	
  
Additional	
  Key	
  Words	
  and	
  Phrases:	
  Design	
  patterns,	
  emotion	
  recognition,	
  multimodal,	
  framework	
  
ACM	
  Reference	
  Format:	
  	
  
Gonzalez-­‐Sanchez,	
   J.,	
   Chavez-­‐Echeagaray,	
   M.E.,	
   Atkinson,	
   R.,	
   and	
   Burleson,	
   W.	
   2011.	
   Affective	
   Computing	
   Meets	
   Design	
   Patterns:	
   A	
  
Patterns-­‐Based	
  Model	
  of	
  a	
  Multimodal	
  Emotion	
  Recognition	
  Framework.	
  In	
  Proceedings	
  of	
  the	
  2011	
  Conference	
  on	
  Pattern	
  Languages	
  of	
  
Programming.	
  ACM	
  Press,	
  2011.	
  Article	
  27.	
  11	
  pages.	
  	
  

1. INTRODUCTION	
  
The	
  goal	
  of	
  affective	
  computing	
  is	
  to	
  enable	
  computer	
  systems	
  to	
  be	
  empathetic,	
  which	
  refers	
  to	
  the	
  ability	
  to	
  
accurately	
  recognize,	
  understand,	
  and	
  respond	
  to	
  human	
  emotions	
  (Picard	
  1997).	
  Enabling	
  computer	
  systems	
  
to	
   be	
   empathetic	
   involves	
   the	
   convergence	
   of	
   hardware	
   (e.g.	
   sensing	
   devices)	
   and	
   the	
   application	
   of	
   novel	
  
machine	
  learning	
  algorithms	
  to	
  deal	
  with	
  the	
  vast	
  amount	
  of	
  data	
  generated	
  by	
  the	
  sensing	
  devices.	
  There	
  are	
  
several	
  examples	
  of	
  research	
  conducted	
  on	
  creating	
  empathetic	
  computer	
  systems	
  to	
  support	
  learning	
  (Arroyo	
  
et	
  al.	
  2009,	
  Woolf	
  et	
  al.	
  2007,	
  D’Mello	
  et	
  al.	
  2007),	
  monitor	
  patient	
  in	
  health	
  care	
  (Chao	
  and	
  Zhiyong	
  2008),	
  and	
  
enhance	
   videogames	
   (Gilleade	
   et	
   al.	
   2005).	
   However,	
   the	
   majority	
   of	
   this	
   research	
   does	
   not	
   focus	
   on	
   the	
  
creation	
  of	
  reusable	
  software,	
  software	
  frameworks,	
  or	
  the	
  best	
  methodological	
  practices	
  for	
  those	
  purposes.	
  
Instead,	
   these	
   approaches	
   are	
   focused	
   on	
   creating	
   proof-­‐of-­‐concept	
   systems	
   to	
   collect	
   data	
   and	
   validate	
  
technology	
  approaches.	
  	
  
Software	
   design	
   patterns	
   are	
   used	
   as	
   a	
   general	
   reusable	
   solution	
   to	
   a	
   commonly	
   occurring	
   problem	
   in	
  
software	
   design	
   to	
   show	
   relationships	
   and	
   interactions	
   between	
   components	
   and	
   provide	
   a	
   skeleton	
   for	
   the	
  
implementation	
  (Gamma	
  et	
  al.	
  1995).	
  The	
  concept	
  of	
  patterns	
  has	
  received	
  relatively	
  little	
  attention	
  in	
  the	
  field	
  
of	
  Affective	
  Computing,	
  different	
  systems	
  and	
  models	
  of	
  emotion	
  recognition	
  systems	
  show	
  diverse	
  solutions	
  
for	
   design	
   problems	
   that	
   are	
   common	
   to	
   all	
   of	
   them;	
   even	
   when	
   a	
   closer	
   look	
   into	
   that	
   solutions	
   and	
   their	
  
comparison	
   often	
   shows	
   that	
   different	
   solutions	
   and	
   the	
   contexts	
   in	
   which	
   they	
   are	
   applied	
   have	
   much	
   in	
  
Author's	
  address:	
  Javier	
  Gonzalez-­‐Sanchez,	
  University	
  Drive	
  and	
  Mill	
  Avenue,	
  Tempe	
  AZ	
  85287;	
  email:	
  javiergs@asu.edu;	
  Author’s	
  address:	
  
Maria-­‐Elena	
   Chavez-­‐Echeagaray,	
   University	
   Drive	
   and	
   Mill	
   Avenue,	
   Tempe	
   AZ	
   85287;	
   email:	
   helenchavez@asu.edu;	
   Author's	
   address:	
  
Robert	
   Atkinson,	
   University	
   Drive	
   and	
   Mill	
   Avenue,	
   Tempe	
   AZ	
   85287;	
   email:	
   robert.atkinson@asu.edu;	
   Author’s	
   address:	
   Winslow	
  
Burleson,	
  University	
  Drive	
  and	
  Mill	
  Avenue,	
  Tempe	
  AZ	
  85287;	
  email:	
  winslow.burleson@asu.edu	
  	
  
	
  
Permission	
  to	
  make	
  digital	
  or	
  hard	
  copies	
  of	
  all	
  or	
  part	
  of	
  this	
  work	
  for	
  personal	
  or	
  classroom	
  use	
  is	
  granted	
  without	
  fee	
  provided	
  that	
  
copies	
  are	
  not	
  made	
  or	
  distributed	
  for	
  profit	
  or	
  commercial	
  advantage	
  and	
  that	
  copies	
  bear	
  this	
  notice	
  and	
  the	
  full	
  citation	
  on	
  the	
  first	
  page.	
  
To	
  copy	
  otherwise,	
  to	
  republish,	
  to	
  post	
  on	
  servers	
  or	
  to	
  redistribute	
  to	
  lists,	
  requires	
  prior	
  specific	
  permission	
  and/or	
  a	
  fee.	
  
	
  
EuroPLoP	
  '11,	
  July	
  13-­‐17,	
  2011,	
  Irsee	
  Monastery,	
  Bavaria,	
  Germany	
  
	
  
Copyright	
  ©	
  2012	
  ACM	
  978-­‐1-­‐4503-­‐1302-­‐5/11/07...	
  $15.00	
  
	
  

common.	
   In	
   that	
   context,	
   we	
   decided	
   to	
   standardize	
   an	
   object-­‐oriented	
   model	
   for	
   a	
   multimodal	
   emotion	
  
recognition	
  framework	
  using	
  design	
  patterns.	
  This	
  model	
  drives	
  the	
  way	
  in	
  which	
  emotion	
  recognition	
  could	
  
be	
   integrated	
   into	
   software	
   projects.	
   Our	
   choice	
   to	
   use	
   design	
   patterns	
   was	
   driven	
   by	
   our	
   interest	
   in:	
   (1)	
  
obtaining	
  a	
  common	
  vocabulary	
  among	
  diverse	
  stakeholders	
  in	
  order	
  to	
  improve	
  the	
  communication	
  process;	
  
(2)	
   sharing	
   of	
   constructions	
   between	
   developers	
   in	
   a	
   way	
   that	
   no	
   matter	
   what	
   is	
   being	
   built	
   or	
   what	
   others	
  
built,	
   everyone	
   is	
   aware	
   of	
   the	
   relationships	
   (connections)	
   among	
   different	
   constructions;	
   (3)	
  creating	
  
components	
  and	
  supporting	
   the	
  creation	
  of	
  families	
  of	
  products	
  for	
  use	
  in	
  prototype	
  and	
  usability	
  testing;	
   and	
  
(4)	
  	
   providing	
  a	
  “controlled”	
  freedom	
  to	
  the	
  programmers	
  because	
  they	
  can	
  develop	
  functionality	
  in	
  their	
  own	
  
creative	
  way,	
  while	
  still	
  following	
  and	
  preserving	
  the	
  guidelines	
  of	
  a	
  defined	
  design.	
  	
  
This	
   work	
   corresponds	
   to	
   the	
   second	
   of	
   three	
   stages	
   in	
   our	
   research	
   on	
   modeling	
   and	
   developing	
   a	
  
multimodal	
  emotion	
  recognition	
  framework:	
  (1)	
  in	
  the	
  first	
  stage,	
  we	
  worked	
  on	
  the	
  definition	
  of	
  a	
  software	
  
architecture	
  model;	
  (2)	
  in	
  the	
  second	
  stage,	
  described	
  in	
  this	
  paper,	
   we	
  shifted	
  from	
  that	
  software	
  architecture	
  
model	
  to	
  a	
  design	
  model	
  using	
  design	
  patterns;	
  and	
  (3)	
  in	
  the	
  third	
  stage,	
  we	
  will	
  be	
  focused	
  on	
  discovering	
  
patterns	
   (for	
   affective-­‐aware	
   systems)	
   either	
   composed	
   or	
   new	
   ones,	
   as	
   occurred	
   with	
   Customer	
   Iteration	
  
Patterns,	
   Pedagogical	
   Patterns,	
   and	
   Security	
   Patterns.	
   With	
   this	
   approach	
   we	
   seek	
   to	
   contribute	
   to	
   the	
   HCI	
  
community	
  by	
  moving	
  the	
  construction	
  of	
  systems	
  that	
  require	
  multimodal	
  emotion	
  recognition	
  from	
  software	
  
development	
   as	
   a	
   one-­‐of-­‐a-­‐kind	
   endeavor	
   to	
   software	
   development	
   as	
   a	
   system	
   of	
   components	
   that	
   are	
   widely	
  
used	
  and	
  highly	
  adaptable	
  (Jacobson	
  1997).	
  
This	
   paper	
   is	
   organized	
   as	
   follows:	
   Section	
   2	
   provides	
   background	
   on	
   the	
   system	
   and	
   software	
   architecture	
  
for	
  multimodal	
  emotion	
  recognition	
  systems;	
  Section	
  3	
  presents	
  a	
  pattern-­‐based	
  model,	
  specifying	
  the	
  design	
  
process	
  and	
  design	
  patterns	
  that	
  were	
  used;	
  Section	
  4	
  elaborates	
  on	
  our	
  experience	
  report,	
  as	
  well	
  as	
  on	
  the	
  
use	
   and	
   evaluation	
   of	
   the	
   framework;	
   finally,	
   Section	
   5	
   concludes	
   the	
   paper.	
   We	
   expect	
   practitioners	
   and	
  
developers	
  with	
  an	
  interest	
  in	
  integrating	
  emotion	
  recognition	
  into	
  their	
  software	
  projects	
  to	
  find	
  this	
  paper	
  
useful	
  as	
  a	
  reference	
  and	
  as	
  an	
  example	
  of	
  the	
  use	
  and	
  advantages	
  of	
  design	
  patterns.	
  For	
  the	
  software	
  design	
  
community,	
   this	
   is	
   an	
   experience	
   report	
   generated	
   by	
   a	
   research	
   group	
   using	
   design	
   patterns	
   to	
   improve	
   its	
  
software	
  process.	
  
	
  
2. BACKGROUND	
  
This	
   section	
   provides	
   background	
   on	
   multimodal	
   emotion	
   recognition	
   systems	
   by	
   describing	
   the	
   subjacent	
  
system	
  architecture,	
  the	
  software	
  architecture,	
  and	
  the	
  quality	
  attributes	
  required	
  from	
  them.	
  
	
  
2.1

System	
  Architecture	
  

The	
   functionality	
   of	
   a	
   multimodal	
   emotion	
   recognition	
   system	
   can	
   be	
   summarized	
   in	
   four	
   statements	
   listed	
  
below,	
   which	
   identify	
   the	
   involved	
   elements	
   and	
   relationships	
   between	
   elements.	
   In	
   the	
   list,	
   components’	
  
names	
   are	
   marked	
   in	
   bold	
   and	
   relationships	
   between	
   components	
   are	
   explained.	
   Figure	
   1	
   shows	
   these	
  
elements	
  and	
  their	
  relationships.	
  The	
  list	
  of	
  statements	
  is	
  as	
  follows:	
  
	
  
(1) Sensing	
  devices	
  obtain	
  data	
  from	
  the	
  user’s	
  physiological	
  responses	
  and	
  body	
  reactions.	
  Sensing	
  devices	
  
are	
   hardware	
   devices	
   that	
   collect	
   quantitative	
   data	
   as	
   measures	
   of	
   physiological	
   signals	
   of	
   emotional	
  
change.	
   We	
   call	
   the	
   measures	
   provided	
   by	
   the	
   sensing	
   devices	
   raw	
   data.	
   Our	
   approach	
   includes	
   the	
   use	
   of:	
  
brain-­‐computer	
  interfaces	
  (Emotiv	
  2011),	
  eye	
  tracking	
  systems	
  (Tobii	
  2011),	
  biofeedback	
  sensors	
   (Strauss	
  
et	
   al.	
   2005,	
   Mota	
   and	
   Picard	
   2003,	
   Qi	
   and	
   Picard	
   2002),	
   and	
   face-­‐based	
   emotion	
   recognition	
   systems	
   (El	
  
Kaliouby	
   and	
   Robinson	
   2005).	
   The	
   use	
   of	
   several	
   sensing	
   devices,	
   either	
   to	
   recognize	
   a	
   broad	
   range	
   of	
  
emotions	
  or	
  to	
  improve	
  the	
  accuracy	
  for	
  recognizing	
  one	
  emotion,	
  is	
  referred	
  to	
  as	
  a	
  multimodal	
  approach.	
  
(2) System	
   elements	
   called	
   data	
   sources	
   read	
   raw	
   data	
   from	
   the	
   sensing	
   devices.	
   The	
   raw	
   data	
   —after	
  
being	
   parsed,	
   filtered,	
   labeled,	
   and	
   time-­‐stamped—	
   becomes	
   sensed	
   values.	
   Table	
   1	
   provides	
   a	
   short	
  
description	
   of	
   each	
   sensing	
   device,	
   related	
   legacy	
   software,	
   related	
   inputs	
   (raw	
   data),	
   and	
   related	
   outputs	
  
(sensed	
  values).	
  
(3) Sensed	
   values	
   are	
   processed	
   by	
   system	
   elements,	
   called	
   specialists,	
   that	
   implement	
   perception	
  
mechanisms.	
   Perception	
   mechanisms	
   are	
   algorithms	
   that	
   infer	
   emotions	
   using	
   sensed	
   values	
   as	
   input	
  
and	
  that	
  provide	
  beliefs	
  about	
  their	
  particular	
  understanding	
  of	
  the	
  user’s	
  emotional	
  change	
  as	
  output.	
  The	
  

perception	
   mechanism	
   could	
   be	
   as	
   general	
   as	
   passing	
   a	
   threshold	
   or	
   as	
   complex	
   as	
   a	
   machine	
   learning	
  
data	
  processing	
  algorithm.	
  	
  
(4) 	
  Each	
   specialist	
   communicates	
   its	
   beliefs	
   to	
   a	
   common	
   control	
   unit,	
   called	
   Centre.	
   Centre	
   creates	
  
emotional	
  states’	
  reports	
  integrating	
  the	
  received	
  beliefs.	
  Emotional	
  states	
  represent	
  the	
  user’s	
  emotion	
  
in	
  a	
  time	
  t.	
  Centre	
  acts	
  as	
  the	
  representative	
  and	
  spokesman	
  of	
  the	
  multimodal	
  emotion	
  recognition	
  system	
  
with	
   third-­‐party	
   systems.	
   These	
   third-­‐party	
   systems	
   could	
   provide	
   functionality	
   to	
   the	
   sensed	
   user	
   (for	
  
example,	
   while	
   using	
   a	
   gaming	
   environment	
   or	
   a	
   tutoring	
   system)	
   or	
   share	
   information	
   with	
   social	
  
networks	
  or	
  other	
  collaborative	
  environments.	
  	
  
	
  

	
  

Fig.	
  1.	
  A	
  multimodal	
  emotion	
  recognition	
  system	
  involves:	
  sensing	
  devices	
  gathering	
  raw	
  data,	
  Data	
  Sources	
  processing	
  raw	
  data	
  into	
  
sensed	
  values,	
  Specialists	
  using	
  sensed	
  values	
  to	
  infer	
  emotions	
  and	
  reporting	
  their	
  beliefs	
  to	
  Centre.	
  
	
  
Table	
  1.	
  The	
  sensing	
  devices	
  and	
  legacy	
  software	
  integrated	
  into	
  the	
  multimodal	
  emotion	
  recognition	
  framework	
  include	
  devices	
  for:	
  brain	
  
waves,	
  eye	
  tracking,	
  physiological	
  signal	
  sensing,	
  and	
  facial	
  expression	
  analysis	
  
Sensing	
  device	
  
(rate	
  in	
  Hz)	
  

Legacy	
  
Software	
  

Emotiv©	
  EEG	
  
headset	
  
	
  (128	
  Hz)	
  

Emotiv©	
  SDK	
  

Sensing	
  
(input	
  or	
  
raw	
  data)	
  
Brain	
  waves	
  
	
  

Physiological	
  responses	
  and/or	
  Emotion	
  reported	
  	
  
(output	
  or	
  sensed	
  values)	
  
EEG	
  activity.	
  Reported	
  in	
  14	
  channels	
  (Sharbrough	
  et	
  al.	
  1991),	
  labeled:	
  AF3,	
  
F7,	
  F3,	
  FC5,	
  T7,	
  P7,	
  O1,	
  O2,	
  P8,	
  T8,	
  FC6,	
  F4,	
  F8,	
  and	
  AF4.	
  	
  
Face	
   activity.	
   Blink,	
   wink	
   (left	
   and	
   right),	
   look	
   (left	
   and	
   right),	
   raise	
   brow,	
  
furrow	
  brow,	
  smile,	
  clench,	
  smirk	
  (left	
  and	
  right),	
  and	
  laugh.	
  
Emotions.	
  Excitement,	
  engagement,	
  boredom,	
  meditation,	
  and	
  frustration.	
  

Standard	
  Webcam	
  
	
  (10	
  Hz)	
  

MIT	
  Media	
  Lab	
  
MindReader	
  
System	
  

Facial	
  
expressions	
  

Emotions.	
   Agreeing,	
   concentrating,	
   disagreeing,	
   interested,	
   thinking,	
   and	
  
unsure.	
  

MIT	
  skin	
  
conductance	
  sensor	
  
	
  (2	
  Hz)	
  
MIT	
  pressure	
  sensor	
  
	
  (6	
  Hz)	
  
Tobii©	
  Eye	
  tracking	
  
	
  (60	
  Hz)	
  
MIT	
  posture	
  sensor	
  
	
  (6	
  Hz)	
  

USB	
  driver	
  

Skin	
  
conductivity	
  

Arousal.	
  

USB	
  driver	
  

Pressure	
  

Tobii©	
  SDK	
  

Eye	
  tracking	
  

USB	
  driver	
  

Pressure	
  

One	
  pressure	
  value	
  per	
  sensor	
  allocated	
  into	
  the	
  input/control	
  device.	
  
Gaze	
  point	
  (x,	
  y).	
  
Pressure	
  values	
  in	
  the	
  back	
  and	
  the	
  seat	
  (in	
  the	
  right,	
  middle,	
  and	
  left	
  zones)	
  
of	
  a	
  cushion	
  chair.	
  

2.2 Software	
  Architecture	
  
The	
  software	
  architecture	
  behind	
  the	
  multimodal	
  emotion	
  recognition	
  system	
  described	
  above	
  was	
  created	
  in	
  
the	
  previous	
  stage	
  (stage	
  1)	
  of	
  this	
  project	
  and	
  is	
  described	
  in	
  a	
  previously	
  published	
  paper	
  (Gonzalez-­‐Sanchez	
  
et	
   al.	
   2011).	
   Due	
   to	
   space	
   limitations	
   only	
   a	
   subset	
   of	
   the	
   original	
   software	
   architecture	
   is	
   modeled	
   in	
   this	
  
paper.	
  As	
  a	
  reference,	
  that	
  subset	
  is	
  summarized	
  in	
  this	
  section.	
  
The	
   software	
   architecture,	
   as	
   shown	
   in	
   Figure	
   2,	
   follows	
   a	
   distributed	
   approach:	
   each	
   sensing	
   device	
   is	
  
connected	
   to	
   a	
   client	
   piece	
   of	
   software	
   (an	
   agent)	
   implemented	
   by	
   two	
   main	
   components	
   DataSource	
   and	
  
Specialist,	
  which	
  communicates	
  with	
  a	
  centralized	
  control	
  unit,	
  implemented	
  by	
  Centre	
  component.	
  
	
  

	
  

Fig.	
  2.	
  DataSource,	
  Specialist,	
  and	
  Centre	
  are	
  the	
  main	
  components	
  in	
  the	
  software	
  architecture	
  of	
  the	
  framework	
  

Data	
   Source.	
   DataSource	
   encapsulates	
   sensing	
   devices	
   or	
   legacy	
   software.	
   One	
   DataSource	
   component	
  
exists	
   for	
   each	
   sensing	
   device	
   and	
   legacy	
   software	
   listed	
   in	
   Table	
   1.	
   DataSource	
   creates	
   SensedValue	
   objects	
  
containing	
  the	
  information	
  gathered	
  from	
  the	
  sensing	
  devices,	
  including:	
  a	
  header,	
  composed	
  by	
  a	
  timestamp	
  
(in	
  milliseconds)	
  and	
  a	
  sensor	
  ID;	
  and	
  a	
  body,	
  composed	
  by	
  an	
  array	
  of	
  rational	
  numbers.	
  
Specialist.	
   Specialist	
   reads	
   SensedValue	
   objects	
   from	
   a	
   DataSource,	
   and	
   uses	
   them	
   to	
   build	
   Belief	
  
objects.	
  Specialist	
  has	
  two	
  responsibilities:	
  (1)	
  encapsulating	
  the	
  perception	
  mechanism	
  used	
  to	
  create	
  Belief	
  
objects;	
   and	
   (2)	
   establishing	
   the	
   communication	
   channel	
   with	
   Centre	
   to	
   send	
   Belief	
   objects	
   and	
   receive	
  
configuration	
   instructions.	
   The	
   communication	
   channel	
   is	
   a	
   TCP/IP	
   connection,	
   therefore	
   Specialist	
   and	
  
Centre	
  can	
  run	
  all	
  together	
  in	
  the	
  same	
  computer	
  or	
  distributed	
  among	
  several	
  computers.	
  
Centre.	
   Centre	
   acts	
   as	
   the	
   head	
   of	
   the	
   system	
   and	
   it	
   has	
   three	
   responsibilities:	
   (1)	
   establishing	
   the	
  
communication	
   channel	
   with	
  Specialist	
  to	
  receive	
   Belief	
  objects	
   and	
   to	
   send	
   configuration	
   instructions;	
   as	
  
said	
   before,	
   the	
   communication	
   channel	
   is	
   a	
   TCP/IP	
   connection;	
   (2)	
   conjugating,	
   in	
   one	
   record,	
   the	
   user’s	
  
emotional	
  state,	
  synchronizing	
  the	
  data	
  provided	
  by	
   Specialist	
  components;	
  (3)	
  implementing	
  the	
  service-­‐
oriented	
  behavior	
  that	
  allow	
  third-­‐party	
  systems	
  (such	
  as	
  loggers,	
  games,	
  and	
  tutoring	
  systems)	
  to	
  be	
  able	
  to	
  
contact	
   Centre	
  and	
  obtain	
  information	
  about	
  emotional	
  changes	
  in	
  the	
  user	
  in	
  a	
  publish-­‐subscribe	
  style.	
  For	
  
third-­‐party	
  systems	
  Centre	
  acts	
  as	
  a	
  facade	
  that	
  hides	
  the	
  internal	
  complexity	
  of	
  the	
  framework.	
  	
  
	
  
2.3

Software	
  Qualities	
  

Before	
  describing	
  the	
  migration	
  from	
  the	
  software	
  architecture	
  to	
  a	
  design	
  for	
  implementation,	
  it	
  is	
  important	
  
to	
  describe	
  the	
  software	
  qualities	
  that	
  drive	
  its	
  design	
  and	
  implementation:	
  	
  
	
  
(1) Reusability.	
  Framework	
  components	
  must	
  be	
  usable	
  for	
  the	
  implementation	
  of	
  other	
  products	
  with	
  slight	
  
or	
  no	
  modification.	
  	
  

(2) Extensibility.	
   Components	
   must	
   be	
   able	
   to	
   extend	
   the	
   framework’s	
   built-­‐in	
   functionality	
   incorporating	
  
new	
   functionalities.	
   The	
   framework	
   must	
   support	
   the	
   incorporation	
   of	
   new	
   sensing	
   devices	
   and	
   the	
  
integration	
  of	
  new	
  inference	
  and	
  data-­‐merging	
  algorithms.	
  	
  
(3) Flexibility.	
   Components	
   must	
   be	
   able	
   to	
   modify	
   existing	
   functionalities	
   and	
   adapt	
   the	
   framework	
   to	
   be	
  
used	
  in	
  applications	
  other	
  than	
  those	
  for	
  which	
  it	
  was	
  specifically	
  designed.	
  
(4) Low	
   latency.	
   The	
   faster	
   the	
   system	
   provides	
   information	
   about	
   user’s	
   emotional	
   state,	
   the	
   better	
   the	
  
provided	
   response	
   or	
   adjustment	
   of	
   the	
   third-­‐party	
   system	
   will	
   be.	
   The	
   user	
   experience	
   highly	
   depends	
   on	
  
allowing	
  unnoticeable	
  delays	
  between	
  reading	
  and	
  processing	
  the	
  sensed	
  data	
  and	
  reporting	
  information	
  
about	
  user’s	
  emotional	
  state.	
  
(5) Performance.	
  The	
  intention	
  is	
  to	
  create	
  the	
  framework	
  as	
  light	
  as	
  possible	
  in	
  order	
  to	
  be	
  able	
  to	
  run	
  in	
  the	
  
background	
   of	
   existing	
   systems.	
   Therefore,	
   the	
   goal	
   is	
   to	
   accomplish	
   functionality	
   constraining	
   memory	
  
and	
  CPU	
  usage.	
  	
  
	
  
Each	
  term	
  above	
  is	
  used	
  according	
  with	
  its	
  definition	
  in	
  the	
  IEEE	
  Standard	
  Glossary	
  of	
  Software	
  Engineering	
  
Terminology	
  (IEEE	
  1999).	
  	
  
The	
   use	
   of	
   patterns	
   becomes	
   the	
   keystone	
   to	
   satisfy	
   the	
   first	
   three	
   qualities	
   enumerated	
   above.	
   Design	
  
patterns	
   aim	
   to	
   take	
   previous	
   experiences	
   to	
   implement	
   non-­‐functional	
   requirements	
   and	
   to	
   avoid,	
   when	
  
properly	
  used,	
  accidental	
  complexity.	
  Satisfaction	
  of	
  the	
  last	
  two	
  requirements	
  (low	
  latency	
  and	
  performance)	
  
is	
  related	
  to	
  the	
  implementation	
  of	
  the	
  model	
  and	
  not	
  with	
  the	
  model	
  per	
  se.	
  	
  
3. PATTERN-­‐BASED	
  MODEL	
  
Finding	
   the	
   appropriate	
   pattern	
   to	
   be	
   applied	
   was	
   a	
   process	
   based	
   on	
   experience	
   and	
   literature	
   research	
  
(Buschmann	
  et	
  al.	
  1996,	
  Gamma	
  et	
  al.	
  1995,	
  Serial	
  2011,	
  Sinha	
  1996).	
  There	
  is	
  no	
  set	
  of	
  rules	
  on	
  how	
  to	
  choose	
  
a	
  pattern;	
  instead,	
  a	
  firm	
  knowledge	
  of	
  existing	
  patterns	
  as	
  well	
  as	
  the	
  problems	
  they	
  solve	
  is	
  required	
  in	
  order	
  
to	
   effectively	
   use	
   patterns	
   to	
   describe	
   what	
   happens	
   within	
   a	
   given	
   system.	
   Our	
   approach	
   consists	
   of	
   using	
   the	
  
pattern	
   that	
   most	
   closely	
   matches	
   the	
   semantic	
   description	
   of	
   the	
   requirement	
   or	
   group	
   of	
   requirements.	
  	
  
DataSource,	
  Specialist,	
  and	
  Centre	
  were	
  modeled	
  as	
  a	
  combination	
  of	
  design	
  patterns;	
  we	
  identified	
  the	
  parts	
  
that	
  would	
  be	
  variable	
  and	
  defined	
  if	
  they	
  were	
  related	
  with	
  the	
  structure	
  of	
  the	
  component,	
  the	
  behavior	
  or	
  
the	
  functionality	
  that	
  the	
  component	
  would	
  provide,	
  or	
  established	
  how	
  new	
  objects	
  would	
  be	
  created.	
  Table	
  2	
  
summarizes	
  the	
  patterns	
  we	
  used	
  to	
  implement	
  the	
  responsibilities	
  of	
  each	
  component,	
  as	
  described	
  in	
  section	
  
2.2.	
  	
  
	
  
Table	
  2.	
  Design	
  patterns	
  used	
  in	
  each	
  component	
  
Component	
  
DataSource	
  

Specialist	
  

Centre	
  

Responsibilities	
  
1.	
  Wrapping	
  legacy	
  software	
  
	
  
2.	
  Reading	
  sensing	
  devices	
  
	
  
1.	
  Encapsulating	
  perception	
  mechanism	
  
	
  
2.	
  Communicating	
  information	
  to	
  Centre	
  
	
  
1.	
  Receiving	
  information	
  from	
  Specialist	
  

Used	
  Design	
  Patterns	
  
ADAPTER	
  
	
  

SERIAL-PORT, DELEGATE
	
  

STRATEGY, ABSTRACT FACTORY
	
  

CLIENT-SERVER
	
  

CLIENT-SERVER
	
  

2.	
  Conjugating,	
  in	
  one	
  record,	
  the	
  user’s	
  emotional	
  state	
  
synchronizing	
  the	
  data	
  provided	
  by	
  the	
  Specialist.	
  
	
  
3.	
  Implementing	
  service-­‐oriented	
  behavior.	
  
	
  

BLACKBOARD, STRATEGY
	
  

PUBLISH-SUBSCRIBE, FACADE	
  
	
  

	
  
In	
   the	
   following	
   sections,	
   UML	
   class	
   diagrams	
   are	
  used	
   to	
   show	
   the	
   design	
   of	
   the	
   model.	
   For	
   clarity	
   and	
   due	
  
to	
   space	
   limitations,	
   the	
   diagrams	
   in	
   Figures	
   3,	
   4,	
   and	
   5	
   only	
   show	
   attributes	
   and	
   operations	
   related	
   with	
  
design	
   patterns	
   templates.	
   Attached	
   to	
   each	
   class	
   is	
   a	
   comment	
   (in	
   UML	
   notation)	
   that	
   indicates	
   the	
   role	
   of	
   the	
  
class	
   in	
   its	
   corresponding	
   design	
   pattern,	
   as	
   recommended	
   in	
   (Jing	
   et	
   al.	
   2007).	
   The	
   comment	
   shows	
   the	
   name	
  

of	
  the	
  design	
  pattern	
  and	
  the	
  equivalent	
  name	
  of	
  the	
  class	
  in	
  the	
  design	
  pattern	
  template	
  using	
  the	
  symbol	
  “::”	
  
as	
  delimiter.	
  
	
  
3.1

Data	
  Source	
  Component	
  

DataSource	
   component	
   was	
   compartmentalized	
   in	
   two	
   categories	
   defining	
   DataSource	
   as	
   interface	
   and	
  
specialize	
   DataSourceWrapper	
   and	
   DataSourceDriver	
   from	
   that	
   interface.	
   Figure	
   3	
   shows	
   the	
   UML	
   class	
  
diagram	
  for	
  DataSource	
  component.	
  

	
  

	
  

	
  
Fig.	
  3.	
  UML	
  class	
  diagram	
  for	
  DataSource	
  component.	
  DataSource	
  interface	
  is	
  compartmentalized	
  in	
  two	
  categories:	
  Wrappers	
  and	
  
Drivers.	
  Wrappers	
  use	
  ADAPTER	
  pattern	
  to	
  connect	
  with	
  SDK	
  platforms,	
  and	
  Drivers	
  use	
  SERIAL-PORT	
  and	
  DELEGATE	
  patterns	
  to	
  read	
  
raw	
  data	
  and	
  parse	
  it	
  into	
  sensed	
  values.	
  

DataSourceWrapper	
   class	
   encapsulates	
   legacy	
   software	
   that	
   provides	
   an	
   SDK	
   platform.	
   This	
   is	
   the	
   case	
   for	
  
Emotiv©	
   SDK,	
   Tobii©	
   SDK,	
   and	
   MIT	
   Media	
   Lab	
   MindReader	
   System.	
   To	
   integrate	
   those	
   SDK	
   libraries,	
   we	
  
incorporated	
   the	
   ADAPTER	
   pattern	
   (Gamma	
   et	
   al.	
   1995)	
   into	
   DataSourceWrapper.	
   This	
   creates	
   an	
  
intermediary	
   abstraction	
   that	
   translates	
   the	
   external	
   SDK	
   platform	
   (adaptee)	
   to	
   a	
   DataSourceWrapper	
  
(client).	
   Adapter	
   class	
   does	
   the	
   corresponding	
   calls	
   to	
   the	
   SDK	
   interfaces	
   to	
   obtain	
   sensed	
   values.	
  
Incorporating	
  a	
  new	
  SDK	
  requires	
  creating	
  a	
  new	
  Adapter	
  class	
  and	
  changes	
  into	
  the	
  SDK	
  (like	
  new	
  versions	
  of	
  
it)	
   implies	
   changes	
   only	
   on	
   its	
   corresponding	
   Adapter	
   class.	
   Adapter	
   classes	
   were	
   developed	
   for	
   Emotiv©	
  
SDK,	
  Tobii©	
  SDK,	
  and	
  MIT	
  Media	
  Lab	
  MindReader	
  System.	
  
DataSourceDriver	
   class	
   encapsulates	
   sensing	
   devices	
   that	
   use	
   serial	
   port	
   communication	
   with	
   the	
  
computer	
   (this	
   is	
   the	
   case	
   of	
   pressure,	
   posture,	
   and	
   skin	
   conductance	
   sensors).	
   To	
   handle	
   the	
   sensors’	
  
hardware	
   interface	
   and	
   to	
   read	
   raw	
   data	
   from	
   the	
   serial	
   port,	
   we	
   incorporated	
   the	
   SERIAL-PORT	
   pattern	
  
(Serial	
   2011)	
   into	
   the	
   DataSourceDriver	
   class.	
   Data	
   coming	
   from	
   the	
   serial	
   port	
   is	
   processed	
   by	
   a	
   sensing	
  
engine,	
   which	
   parses	
   the	
   raw	
   data	
   into	
   sensed	
   values.	
   The	
   DELEGATE	
   pattern	
   (Deugo	
   1998)	
   handles	
   the	
  
separation	
   of	
   the	
   perception	
   mechanism	
   from	
   the	
   rest	
   of	
   the	
   component.	
   We	
   developed	
   SensingEngine	
  
classes	
  for	
  the	
  pressure,	
  posture,	
  and	
  skin	
  conductance	
  sensors.	
  

3.2

Specialist	
  Component	
  

Specialist	
  component	
  implements	
  the	
  intelligence	
  of	
  the	
  system,	
  the	
  perception	
  mechanisms	
  that	
  converts	
  
SensedValue	
   objects	
   into	
   inferred	
   Belief	
   objects,	
   and	
   reports	
   them	
   to	
   the	
   central	
   unit,	
   Centre.	
   	
   Figure	
   4	
  
shows	
  the	
  UML	
  class	
  diagram	
  for	
  Specialist	
  component.	
  

	
  

Fig.	
  4.	
  UML	
  class	
  diagram	
  for	
  Specialist	
  component.	
  DataSource	
  component	
  provides	
  SensedValue	
  objects	
  to	
  Modeler	
  class.	
  
Modeler	
  class	
  uses	
  STRATEGY	
  and	
  ABSTRACT	
  FACTORY	
  patterns	
  to	
  create	
  Belief	
  objects.	
  The	
  Communicator	
  class	
  use	
  CLIENTSERVER	
  pattern	
  to	
  communicate	
  this	
  component	
  with	
  Centre	
  component.	
  

	
  

Modeler	
  class	
  is	
  the	
  part	
  of	
  the	
   Specialist	
  component	
  that	
  connects	
  it	
  with	
  the	
   DataSource	
  component	
  
to	
   read	
   SensedValues	
   objects.	
   Modeler	
   class	
   declares	
   a	
   DataSource	
   object.	
   Modeler	
   class	
   uses	
   the	
   class	
  
BeliefFactory	
  that	
  is	
  part	
  of	
  the	
   ABSTRACT	
   FACTORY	
  pattern	
  (Gamma	
  et	
  al.	
  1995)	
  to	
  create	
   Belief	
  objects	
  
from	
   SensedValues	
   objects.	
   The	
   factory	
   implements	
   perception	
   mechanisms	
   as	
   strategies,	
   thus	
   the	
  
PerceptionMechanism	
  class	
  implements	
  the	
  STRATEGY	
  pattern	
  (Gamma	
  et	
  al.	
  1995).	
  
Communicator	
  class	
  implements	
  what	
  is	
  needed	
  to	
  send	
  data	
  to	
   Centre,	
  realizing	
  the	
  client	
  part	
  of	
  client-­‐
server	
  model	
  (Sinha	
  1996)	
  between	
  Specialist	
  and	
  Centre	
  components.	
  	
  

	
  
3.3

Centre	
  Component	
  

Centre	
  component	
  provides	
  a	
  concurrent	
  control	
  mechanism	
  for	
  data	
  collection:	
  a	
  concurrent	
  data	
  structure	
  
in	
   which	
   each	
   Specialist	
   component	
   adds	
   information.	
   In	
   order	
   to	
   provide	
   its	
   functionality,	
   Centre	
  
implements	
   BLACKBOARD	
   pattern	
   (Buschmann	
   et	
   al.	
   1996)	
   under	
   its	
   variant	
   of	
   concurrent	
   access	
   repository.	
  
BLACKBOARD	
  pattern	
  provides	
  the	
  concurrent	
  data	
  structure	
  and	
  defines	
  two	
  elements	
  that	
  have	
  access	
  to	
  it:	
  
SpecialistAdvocate	
   class	
   and	
   Supervisor	
   class.	
   	
   Figure	
   5	
   shows	
   the	
   UML	
   class	
   diagram	
   for	
   Centre	
  
component.	
  	
  
SpecialistAdvocate	
  class	
  assumes	
  the	
  role	
  of	
  a	
  knowledge	
  source	
  in	
  the	
   BLACKBOARD	
  pattern	
  receiving	
  
Belief	
   objects	
   from	
   Specialist	
   components.	
   SpecialistAdvocate	
   class	
   uses	
   an	
   instance	
   of	
  
CommunicatorServer	
  class,	
  which	
  tackles	
  the	
  server	
  part	
  of	
  the	
  client-­‐server	
  model	
  (Sinha	
  1996).	
  

	
  

Fig.	
  5.	
  UML	
  class	
  diagram	
  for	
  Centre	
  component.	
  Centre	
  implements	
  BLACKBOARD	
  pattern	
  to	
  provide	
  a	
  concurrent	
  data	
  repository.	
  
STRATEGY	
  pattern	
  supports	
  the	
  process	
  to	
  convert	
  Belief	
  objects	
  into	
  EmotionalState	
  objects	
  and	
  FACADE	
  pattern	
  hides	
  the	
  
complexity	
  of	
  Centre.	
  PUBLISH-SUBSCRIBE	
  pattern	
  is	
  used	
  to	
  offer	
  a	
  publish-­‐subscribe	
  style	
  service	
  to	
  deliver	
  emotional	
  state	
  
information	
  to	
  third-­‐party	
  systems.	
  
	
  

Supervisor	
   class	
   takes,	
   from	
   the	
   BLACKBOARD,	
   the	
   Belief	
   objects	
   provided	
   by	
   all	
   SpecialistAdvocates	
  
and	
  integrates	
  them	
  into	
   EmotionalState	
  objects.	
   Supervisor	
  class	
  implements	
   STRATEGY	
  pattern	
  (Gamma	
  
et	
  al.	
  1995)	
  to	
  conjugate	
  individual	
   Belief	
  objects	
  in	
   EmotionalState	
  objects.	
   Editor	
  class	
  implements	
  the	
  
concrete	
  strategy;	
  strategies	
  deal	
  with	
  the	
  fact	
  that	
  high	
  rate	
  hardware	
  has	
  high	
  rate	
  of	
  changing	
  values.	
  The	
  
last	
  provided	
  values	
  are	
  used	
  until	
  new	
  ones	
  arrive.	
  	
  
Publisher	
   class	
   implements	
   the	
   publisher	
   part	
   of	
   a	
   PUBLISH-SUBSCRIBE	
   pattern	
   (Buschmann	
   et	
   al.	
   1996)	
  
to	
  provide	
  information	
  to	
  third-­‐party	
  systems	
  that	
  express	
  interest	
  in	
  this	
  information.	
  The	
  pattern	
  works	
  as	
  
an	
   OBSERVER	
  pattern	
  (Gamma	
  et	
  al.	
  1995)	
  but	
  in	
  a	
  distributed	
  environment	
  (publisher	
  and	
  subscriber	
  are	
  like	
  
subject	
   and	
   observers	
   allocated	
   in	
   different	
   places).	
   Publisher	
   class	
   also	
   acts	
   as	
   the	
   FACADE	
   (Gamma	
   et	
   al.	
  
1995)	
  that	
  hides	
  the	
  complexity	
  of	
  Centre	
  to	
  third-­‐party	
  systems.	
  
	
  

4. DISCUSSION	
  
In	
   our	
   modest-­‐sized	
   project,	
   we	
   leveraged	
   the	
   use	
   of	
   design	
   patterns	
   in	
   every	
   aspect	
   of	
   the	
   system	
   without	
  
forcing	
   their	
   use,	
   but	
   incorporated	
   them	
   whenever	
   it	
   seemed	
   prudent.	
   With	
   patterns,	
   we	
   addressed	
   the	
  
creation	
  of	
  the	
  framework	
  and	
  sought	
  to	
  incorporate	
  on	
  it	
  software	
  quality	
  factors	
  that	
  patterns	
  have	
  been	
  said	
  
to	
  have	
  positive	
  impact	
  on	
  (Gamma	
  et	
  al.	
  1995,	
  Khomh	
  and	
  Gueheneuc	
  2008).	
  The	
   particular	
   quality	
  attributes	
  
we	
  were	
  interested	
  in	
  are:	
  reusability,	
  extensibility,	
  and	
  flexibility,	
  as	
  well	
  as	
  understandability. These	
  qualities	
  
were	
   chosen	
   because	
   they	
   help	
   in	
   addressing	
   the	
   contextual	
   elements	
   mentioned	
   above:	
   incremental	
  
requirements,	
  changing	
  requirements,	
  and	
  a	
  shifting	
  development	
  team.	
  
Reusability.	
   Given	
   that	
   our	
   work	
   is	
   a	
   research	
   effort,	
   a	
   key	
   goal	
   in	
   our	
   group	
   was	
   to	
   create	
   reusable	
  
artifacts	
  to	
  use	
  them	
  in	
  future	
  not	
  well-­‐defined	
  projects.	
  Framework	
  components	
  have	
  been	
  used	
  with	
  slight	
  or	
  

no	
  modification	
  in	
  two	
  different	
  kinds	
  of	
  applications:	
  a	
  gaming	
  environment	
  study	
  and	
  an	
  intelligent	
  tutoring	
  
system	
  development	
  project.	
  For	
  the	
  gaming	
  environment	
  study,	
  students	
  were	
  asked	
  to	
  play	
  the	
  Guitar	
  Hero©	
  
videogame	
   (as	
   a	
   learning	
   experience)	
   while	
   their	
   emotional	
   status	
   was	
   measured.	
   The	
   framework	
   was	
   used	
   as	
  
an	
  independent	
  tool	
  where	
  publish-­‐subscribe	
  functionality	
  in	
   Centre	
  was	
  replaced	
  with	
  a	
   Logger	
  component	
  
(developed	
   for	
   this	
   experiment)	
   which	
   responsibility	
   was	
   to	
   store	
   the	
   received	
   information	
   in	
   an	
   online	
  
database	
   for	
   their	
   posterior	
   analysis.	
   This	
   implied	
   replacing	
   Publisher	
   class	
   with	
   a	
   Logger	
   class.	
   For	
   the	
  
intelligent	
   tutoring	
   system	
   development	
   project,	
   we	
   are	
   working	
   on	
   developing	
   an	
   affective	
   meta-­‐tutoring	
  
system,	
   integrating	
   our	
   framework	
   to	
   support	
   its	
   affective	
   part.	
   While	
   the	
   student	
   is	
   working	
   on	
   tasks,	
   the	
  
tutoring	
  system	
  collects	
  emotional	
  state	
  information	
  from	
   Centre	
  with	
  the	
  intention	
  of	
  being	
  able	
  to	
  generate	
  
better	
  and	
  more	
  accurate	
  hints	
  and	
  feedback,	
  as	
  well	
  as	
  affective	
  support	
  for	
  the	
  student,	
  in	
  order	
  to	
  reduce	
  the	
  
frustration	
  and	
  avoid	
  student	
  desertion.	
  	
  
Extensibility.	
   Framework components must be able to extend the framework built-­‐in	
   functionality	
  
incorporating	
   new	
   functionalities.	
   In	
   particular	
   ABSTRACT-FACTORY,	
   ADAPTER,	
   and	
   STRATEGY	
   patterns	
   were	
  
key	
  for	
  this	
  purpose	
  in	
  two	
  ways:	
  (1)	
  to	
  facilitate	
  doing	
  several	
  iterations	
  extending	
  functionality,	
  changing	
  or	
  
complementing	
  the	
  inference	
  or	
  data-­‐merging	
  algorithms	
  and	
  data	
  storage	
  strategies;	
  and	
  (2)	
  to	
  support	
  the	
  
incorporation	
   of	
   new	
   sensing	
   devices	
   and	
   the	
   incorporation	
   of	
   new	
   legacy	
   software.	
   For	
   example,	
   we	
   are	
  
considering	
   adding	
   the	
   BodyMedia	
   FIT	
   armband	
   (BodyMedia	
   2011)	
   to	
   measure	
   physical	
   activity	
   and	
   burned	
  
calories.	
  
Flexibility.	
   Framework	
   components	
   must	
   be	
   able	
   to	
   modify	
   existing	
   functionalities	
   and	
   adapt	
   the	
  
framework	
  to	
  be	
  used	
  in	
  applications	
  other	
  than	
  those	
  for	
  which	
  it	
  was	
  specifically	
  designed.	
  For	
  example,	
  in	
  
the	
   case	
   of	
   the	
   intelligent	
   tutoring	
   system	
   development	
   project,	
   described	
   above,	
   DataSources	
   and	
  
Specialist	
   components	
   were	
   used	
   as	
   they	
   are,	
   while	
   the	
   strategy	
   in	
   the	
   Centre	
   was	
   aimed	
   to	
   be	
  
implemented	
   ad-­‐hoc	
   through	
   a	
   Bayesian	
   Network.	
   Doing	
   so	
   requires	
   a	
   new	
   class	
   implementing	
  
StrategyEditor,	
  which	
  will	
  embed	
  an	
  open-­‐source	
  suite	
  for	
  Bayesian	
  Networks.	
  Another	
  example	
  is	
  a	
  mobile	
  
learning	
  project,	
  where	
  the	
  approach	
  is	
  to	
  replace	
   Centre	
  with	
  a	
  cloud-­‐based	
  solution	
  and	
  modify	
  the	
  agents	
  
(Specialist and DataSource)	
  to	
  become	
  a	
  mobile	
  application.	
  
Understandability.	
   The	
   use	
   of	
   patterns	
   increased	
   understandability	
   for	
   both	
   the	
   development	
   team	
   and	
  
stakeholders.	
   First,	
   for	
   the	
   development	
   team,	
   even	
   though	
   it	
   had	
   a	
   high	
   rotation	
   of	
   members,	
   we	
   were	
   able	
   to	
  
split	
   the	
   work	
   between	
   developers;	
   for	
   example:	
   a	
   given	
   developer	
   was	
   able	
   to	
   focus	
   on	
   implementing	
   one	
  
algorithm	
   even	
   without	
   the	
   knowledge	
   of	
   how	
   the	
   inputs	
   were	
   to	
   be	
   obtained	
   or	
   where	
   the	
   results	
   were	
   going	
  
to	
  be	
  used;	
  the	
  developer	
  then	
  implemented	
  and	
  tested	
  the	
  algorithm	
  into	
  the	
  system	
  by	
  implementing	
  them	
  
from	
  the	
  corresponding	
   ADAPTER	
  interface.	
  Second,	
   stakeholders	
  (researchers)	
  were	
  able	
  to	
  better	
  understand	
  
the	
   system.	
   Stakeholders’	
   concerns	
   were	
   less	
   about	
   programming	
   and	
   more	
   on	
   meeting	
   deadlines	
   for	
   each	
  
successive	
  version.	
  This	
  included	
  an	
  interest	
  in	
  knowing	
  how	
  the	
  work	
  was	
  delegated	
  and	
  how	
  the	
  resources	
  
(programmers	
   and	
   time)	
   would	
   be	
   used.	
   Even	
   when	
   stakeholders	
   were	
   not	
   familiar	
   with	
   design	
   patterns	
   or	
  
patterns	
  in	
  general,	
  the	
  terms	
  and	
  analogies	
  about	
   BLACKBOARD,	
  FACTORY,	
  STRATEGY	
  and	
  PUBLISH-SUBSCRIBE	
  
became	
  a	
  common-­‐ground	
  language	
  for	
  both	
  the	
  development	
  team	
  and	
  the	
  stakeholders.	
  Patterns	
  helped	
  us	
  
to	
  avoid	
  reinventing	
  the	
  wheel	
  on	
  how	
  to	
  modularize	
  our	
  framework.	
  
However,	
  any	
  given	
  dynamic	
  system	
  has	
  tradeoffs,	
  improving	
  one	
  quality	
  may	
  degrade	
  the	
  quality	
  in	
  other	
  
areas.	
   There are studies that note that patterns do not always improve qualities and the risk about using a lot of them
in one system (Khomh	
  and	
  Gueheneuc	
  2008,	
  and	
  Wendorff	
  2001).	
  However,	
  we	
  recognize	
  that	
  using	
  patterns	
  to	
  
model	
   a	
   software	
   design	
   with	
   a	
   well-­‐defined	
   software	
   architecture	
   helps	
   to	
   promote	
   the	
   desired	
   software	
  
qualities	
  with	
  only	
  a	
  modicum	
  of	
  negative	
  tradeoffs	
  in	
  latency	
  and	
  performance.	
  In	
  our	
  case,	
  the	
  decoupling	
  of	
  
components	
   in	
   agents	
   and	
   the	
   communication	
   model	
   between	
   Specialist	
   and	
   Centre,	
   slightly	
   increased	
  
latency	
  and	
  reduced	
  performance.	
  However,	
  they	
  stayed	
  at	
  levels	
  that	
  were	
  acceptable	
  for	
  the	
  purposes	
  of	
  the	
  
projects.	
  
Performance.	
   A	
   first	
   iteration	
   of	
   this	
   pattern-­‐based	
   model	
   was	
   implemented	
   in	
   Java	
   SE	
   6.	
   Five	
  
implementations	
   of	
   Specialist	
   components	
   (one	
   per	
   sensing	
   device)	
   and	
   the	
   Centre	
   component	
   were	
  
implemented	
  and	
  are	
  fully	
  functional.	
  Memory	
  load	
  and	
  CPU	
  usage	
  were	
  measured	
  and	
  used	
  as	
  performance	
  
indicators.	
   Table	
   3	
   shows	
   the	
   result	
   of	
   memory	
   load	
   and	
   processor	
   usage	
   running	
   each	
   of	
   the	
   Specialists	
   in	
  
a	
   system	
   with	
   the	
   following	
   characteristics:	
   Intel	
   Xeon	
   CPU	
   W3520	
   at	
   2.67	
   GHz	
   with	
   4	
   cores,	
   and	
   3.50	
   GB	
   of	
  
RAM	
   at	
   2.67GHz.	
   The	
   system	
   was	
   running	
   Windows	
   XP	
   Professional	
   with	
   Service	
   Pack	
   3.	
   It	
   is	
   important	
   to	
  

mention	
   that	
   for	
   Specialist	
   component	
   for	
   Emotiv©	
   SDK,	
   Tobii©	
   SDK,	
   and	
   MIT	
   Media	
   Lab	
   MindReader	
  
System	
  the	
  values	
  reported	
  in	
  Table	
  3	
  include	
  the	
  load	
  caused	
  by	
  the	
   Specialist	
  and	
  by	
  the	
  underlying	
  SDK	
  
system.	
  	
  
	
  
Table	
  3.	
  System	
  Performance	
  Test	
  Results	
  
Specialist	
  component	
  
Skin	
  

%	
  CPU	
  
8	
  -­‐	
  15	
  

Performance	
  
Memory	
  (Kb)	
  
14,100	
  -­‐	
  15,200	
  

Face	
  

34	
  -­‐	
  43	
  

60,000	
  -­‐	
  110,000	
  

Brain	
  (for	
  emotion)	
  

9	
  -­‐	
  16	
  

8,260	
  -­‐	
  8,500	
  

Brain	
  	
  (for	
  physiological	
  responses)	
  

6	
  -­‐	
  15	
  

7,200	
  -­‐	
  7,800	
  

Pressure	
  

8	
  -­‐	
  14	
  

15,900	
  -­‐	
  16,200	
  

Eye	
  

15	
  -­‐	
  25	
  

169,	
  500	
  -­‐	
  170,000	
  

	
  
Numbers	
  in	
  Table	
  3	
  indicate	
  acceptable	
  operation	
  levels	
  in	
  lab	
  and	
  classroom	
  computers.	
  	
  
An	
   additional	
   dispute	
   emerged	
   with	
   stakeholders	
   regarding	
   size.	
   Stakeholders	
   point	
   to	
   the	
   increase	
   in	
   lines	
  
of	
  code	
  while	
  using	
  patterns	
  as	
  an	
  issue.	
  While	
  using	
  patterns	
  generates	
  more	
  code	
  in	
  our	
  project,	
  this	
  is	
  not	
  
only	
  due	
  to	
  patterns	
  (interfaces	
  and	
  abstract	
  classes	
  declarations),	
  but	
  also	
  because	
  we	
  decided	
  to	
  maintain	
  the	
  
cyclomatic	
   complexity	
   (McCabe	
   1976)	
   for	
   every	
   method	
   under	
   10,	
   which	
   means	
   applying	
   a	
   “divide	
   and	
  
conquer”	
  strategy	
  that	
  generates	
  more	
  methods	
  in	
  the	
  system.	
  
	
  
5. CONCLUSIONS	
  AND	
  ONGOING	
  WORK	
  
This	
  paper	
  is	
  an	
  attempt	
  to	
  increase	
  the	
  affective	
  computing	
  community's	
  awareness	
  of	
  the	
  benefits	
  of	
  using	
  
design	
   patterns	
   for	
   modeling	
   emotional-­‐aware	
   software	
   and	
   developing	
   empathetic	
   systems.	
   This	
   work	
  
represents	
  a	
  significant	
  step	
  forward	
  addressing	
  the	
  lack	
  of	
  models,	
  libraries,	
  and	
  tools	
  to	
  develop	
  multimodal	
  
emotion	
  recognition	
  systems	
  and	
  alleviate	
  some	
  of	
  the	
  complexity	
  in	
  the	
  process.	
  
The	
  proposed	
  framework,	
  designed	
  as	
  a	
  collection	
  of	
  design	
  patterns,	
  offers	
  an	
  option	
  to	
  achieve	
  large-­‐scale	
  
implementation	
   and	
   integration	
   of	
   emotional-­‐aware	
   support	
   for	
   computers	
   systems.	
   It	
   is	
   focused	
   on	
   achieving	
  
the	
  creation	
  of	
  reusable,	
  flexible,	
  and	
  extensible	
  components,	
  although,	
  there	
  are	
  trade-­‐offs	
  between	
  qualities	
  
achieved	
  and	
  the	
  performance,	
  latency,	
  and	
  size	
  of	
  the	
  system.	
  
This	
  experience	
  report	
  demonstrates	
  how	
  design	
  patterns	
  can	
  help	
  to	
  improve	
  understandability	
  between	
  
stakeholders	
   and	
   developers	
   and	
   also	
   the	
   management	
   of	
   work	
   assignment	
   in	
   research-­‐focused	
   projects,	
  
particularly	
   those	
   with	
   the	
   potential	
   for	
   high	
   turnover	
   in	
   developers.	
   Our	
   future	
   efforts	
   will	
   focus	
   on	
   (1)	
  
discovering	
  affective	
  computing	
  design	
  patterns	
  and	
  documenting	
  them	
  in	
  a	
  pattern	
  language	
  for	
  the	
  domain	
  of	
  
affective	
   computing,	
   (2)	
   describing	
   good	
   design	
   models	
   and	
   practices	
   within	
   this	
   field	
   to	
   help	
   designers	
  
building	
   empathetic	
   systems	
   in	
   the	
   future,	
   and	
   (3)	
   supporting	
   the	
   integration	
   of	
   emotional-­‐awareness	
  
capabilities	
  in	
  modern	
  software	
  systems.	
  
	
  
6. ACKNOWLEDGEMENTS	
  
We	
   are	
   grateful	
   to	
   James	
   Siddle	
   for	
   his	
   support	
   during	
   the	
   writing	
   process	
   of	
   this	
   paper.	
   This	
   research	
   was	
  
supported	
  by	
  Office	
  of	
  Naval	
  Research	
  under	
  Grant	
  N00014-­‐10-­‐1-­‐0143	
  awarded	
  to	
  Dr.	
  Robert	
  Atkinson.	
  
	
  
	
  
	
  
	
  

REFERENCES	
  
	
  
ALEXANDER,	
  C.,	
  Ishikawa,	
  S.,	
  Silverstein,	
  M.,	
  et	
  al.	
  1997.	
  A	
  Pattern	
  Language.	
  Oxford	
  University	
  Press.	
  
ARROYO,	
   I.,	
   Cooper,	
   D.	
   G.,	
   Burleson,	
   W.,	
   Woolf,	
   B.	
   P.,	
   Muldner,	
   K.,	
   and	
   Christopherson,	
   R.	
   2009.	
   Emotion	
   Sensors	
   Go	
   to	
   School.	
   In	
   V.	
  
Dimitrova,	
   R.	
   Mizoguchi,	
   B.	
   du	
   Boulay	
   &	
   A.	
   Grasser	
   (Eds.),	
   Artificial	
   Intelligence	
   in	
   Education.	
   Building	
   Learning	
   Systems	
   that	
   Care:	
   from	
  
Knowledge	
  Representation	
  to	
  Affective	
  Modelling	
  (Vol.	
  Frontiers	
  in	
  Artificial	
  Intelligence	
  and	
  Applications	
  200),	
  IOS	
  Press,	
  17—24.	
  
BODYMEDIA	
  FIT.	
  2011.	
  http://www.bodymedia.com/	
  
BUSCHMANN,	
  F.,	
  Meunier,	
  R.,	
  Rohnert,	
  H.,	
  Sommerlad,	
  P.,	
  and	
  Stal,	
  M.	
  1996.	
  A	
  system	
  of	
  patterns:	
  Pattern-­‐oriented	
  software	
  architecture.	
  
Wiley.	
  
CHAO,	
   X.	
   and	
   Zhiyong,	
   F.	
   2008.	
   A	
   Trusted	
   Affective	
   Model	
   Approach	
   to	
   Proactive	
   Health	
   Monitoring	
   System.	
   In	
   Proceedings	
   of	
   the	
   2008	
  
International	
  Seminar	
  on	
  Future	
  BioMedical	
  Information	
  Engineering.	
  FBIE	
  '08,	
  IEEE	
  Computer	
  Society.	
  429—432.	
  
DEUGO,	
  D.	
  1998.	
  Foundation	
  Patterns.	
  In	
  Proceedings	
  of	
  Fifth	
  Pattern	
  Languages	
  of	
  Programs	
  Conference.	
  Allerton	
  Park,	
  Illinois.	
  
D'MELLO,	
  S.,	
  Picard,	
  R.	
  W.,	
  and	
  Graesser,	
  A.	
  2007.	
  Toward	
  an	
  Affect-­‐Sensitive	
  AutoTutor.	
  In	
  IEEE	
  Intelligent	
  Systems,	
  (Vol.	
  22	
  no.	
  4),	
  53—
61.	
  
EL	
   KALIOUBY,	
   R.	
   and	
   Robinson,	
   P.	
   2005.	
   Generalization	
   of	
   a	
   vision-­‐based	
   computational	
   model	
   of	
   mind-­‐reading.	
   In	
   Proceedings	
   of	
   First	
  
International	
  Conference	
  on	
  Affective	
  Computing	
  and	
  Intelligent	
  Interaction.	
  ACII’05,	
  Springer-­‐Verlang,	
  582—589.	
  	
  
EMOTIV	
  -­‐	
  Brain	
  Computer	
  Interface	
  Technology.	
  2011.	
  http://www.emotiv.com.	
  
GAMMA,	
  E.,	
  Helm,	
  R.,	
  Johnson,	
  R.,	
  and	
  Vlissides,	
  J.	
  1995.	
  Design	
  Patterns:	
  Elements	
  of	
  Reusable	
  Object-­‐Oriented	
  Software.	
  Addison-­‐Wesley	
  
Longman	
  Publishing	
  Co.,	
  Inc.,	
  Boston,	
  MA,	
  USA.	
  
GILLEADE,	
  K.,	
  Dix,	
  A.,	
  and	
  Allanson,	
  J.	
  2005.	
  Affective	
  Videogames	
  and	
  Modes	
  of	
  Affective	
  Gaming:	
  Assist	
  Me,	
  Challenge	
  Me,	
  Emote	
  Me.	
  In	
  
Proceedings	
  of	
  Digital	
  Games	
  Research	
  Association.	
  DIGRA'05,	
  16—20.	
  	
  
GONZALEZ-­‐SANCHEZ,	
   J.,	
   Chavez-­‐Echeagaray,	
   M.E.,	
   Atkinson,	
   R.,	
   and	
   Burleson,	
   W.	
   2001.	
   An	
   Agent-­‐Based	
   Software	
   Architecture	
   for	
   a	
  
Multimodal	
  Emotion	
  Recognition	
  Framework.	
  In	
   Proceedings	
  of	
  2011	
  Ninth	
  Working	
  IEEE/IFIP	
  Conference	
  on	
  Software	
  Architecture.	
  
WICSA'11,	
  IEEE	
  Computer	
  Society.	
  187—193.	
  
IEEE:	
  Standard	
  Glossary	
  of	
  Software	
  Engineering	
  Terminology.	
  1999.	
  610.12-­‐1990,	
  (Vol.1).	
  IEEE	
  Press.	
  
JING,	
  D.,	
  Sheng,	
  Y.,	
  and	
  Kang,	
  Z.	
  2007.	
  Visualizing	
  design	
  patterns	
  in	
  their	
  applications	
  and	
  compositions.	
  IEEE	
  Transactions	
  on	
  Software	
  
Engineering,	
  33	
  (7),	
  433—453.	
  
JOHNSON,	
  R.E.	
  1997.	
  Components,	
  frameworks,	
  patterns.	
  Harandi,	
  M.	
  (Ed.).	
  In	
  Proceedings	
  of	
  the	
  1997	
  symposium	
  on	
  Software	
  reusability.	
  
SSR	
  '97,	
  ACM,	
  10—17.	
  
KHOMH,	
   F.,	
   and	
   Gueheneuc,	
   Y.	
   G.	
   2008.	
   Do	
   Design	
   Patterns	
   Impact	
   Software	
   Quality	
   Positively?	
   In	
   Proceedings	
   of	
   12th	
   European	
  
Conference	
  on	
  Software	
  Maintenance	
  and	
  Reengineering.	
  CSMR’08,	
  274—278.	
  
MOTA,	
  S.,	
  and	
  Picard,	
  R.	
  W.	
  2003.	
  Automated	
  Posture	
  Analysis	
  for	
  Detecting	
  Learners	
  Interest	
  Level.	
  In	
   Proceedings	
  of	
  Computer	
  Vision	
  
and	
  Pattern	
  Recognition	
  Workshop.	
  CVPRW‘03,	
  IEEE	
  Press.	
  (Vol.	
  5),	
  49.	
  
PICARD,	
  R.	
  W.	
  1997.	
  Affective	
  Computing,	
  MIT	
  Press.	
  
QI,	
   Y.,	
   and	
   Picard,	
   R.	
   W.	
   2002.	
   Context-­‐Sensitive	
   Bayesian	
   Classifiers	
   and	
   Application	
   to	
   Mouse	
   Pressure	
   Pattern	
   Classification.	
   In	
  
Proceedings	
  of	
  International	
  Conference	
  on	
  Pattern	
  Recognition.	
  ICPR’02,	
  (Vol.	
  3),	
  30448.	
  
SERIAL	
  Port	
  Design	
  Pattern.	
  2011.	
  http://www.eventhelix.com/RealtimeMantra/PatternCatalog/serial_port_design_pattern.htm.	
  
SHARBROUGH	
  F,	
  Chatrian	
  G-­‐E,	
  Lesser	
  RP,	
  Lüders	
  H,	
  Nuwer	
  M,	
  and	
  Picton	
  TW.	
  1991.	
  American	
  Electroencephalographic	
  Society	
  Guidelines	
  
for	
  Standard	
  Electrode	
  Position	
  Nomenclature.	
  In	
  J.	
  Clin.	
  Neurophysiol,	
  (Vol.	
  8),	
  200—202.	
  
SINHA,	
  A.	
  1992.	
  Client-­‐server	
  computing.	
  In	
  Communications	
  of	
  the	
  ACM.	
  (Vol.	
  35),	
  77—98.	
  
STRAUSS,	
   M.,	
   Reynolds,	
   C.,	
   Hughes,	
   S.,	
   Park,	
   K.,	
   McDarby,	
   G.,	
   and	
   Picard,	
   R.W.	
   2005.	
   The	
   HandWave	
   Bluetooth	
   Skin	
   Conductance	
   Sensor.	
   In	
  
Proceedings	
  of	
  First	
  International	
  Conference	
  on	
  Affective	
  Computing	
  and	
  Intelligent	
  Interaction.	
  ACII’05.	
  Springer-­‐Verlang.	
  699—706.	
  	
  
TOBII	
  Technology	
  -­‐	
  Eye	
  Tracking	
  and	
  Eye	
  Control.	
  2011.	
  http://www.tobii.com.	
  
WENDORFF,	
  P.:	
  Assessment	
  of	
  design	
  patterns	
  during	
  software	
  reengineering:	
  Lessons	
  learned	
  from	
  a	
  large	
  commercial	
  project.	
  Sousa,	
  P.	
  
and	
   Ebert,	
   J.	
   (Eds.),	
   In	
   Proceedings	
   of	
   5th	
   Conference	
   on	
   Software	
   Maintenance	
   and	
   Reengineering.	
   IEEE	
   Computer	
   Society	
   Press,	
   77–84	
  
(2001)	
  
WOOLF,	
   B.,	
   Burelson,	
   W.,	
   and	
   Arroyo,	
   I.	
   2007.	
   Emotional	
   Intelligence	
   for	
   Computer	
   Tutors.	
   In	
   Supplementary	
   Proceedings	
   of	
   the	
   13th	
  
International	
  Conference	
  on	
  Artificial	
  Intelligence	
  in	
  Education.	
  AIED	
  ‘07,	
  6—15.	
   	
  

Copyright	
  ©	
  2012	
  ACM	
  978-­‐1-­‐4503-­‐1302-­‐5/11/07...	
  $15.00	
  
	
  

ISWC '14 ADJUNCT, SEPTEMBER 13 - 17, 2014, SEATTLE, WA, USA

Including Affect-Driven Adaptation to
the Pac-Man Video Game
Abstract
Ahbiya Harris

Alyza Villa

Arizona State University

Arizona State University

University Drive and Mill Avenue

University Drive and Mill Avenue

Tempe, AZ 85281, USA
ahharris@asu.edu

Tempe, AZ 85281, USA
agvilla3@asu.edu

Andrew Hoch

Maria Elena Chavez-Echeagaray

Arizona State University

Arizona State University

University Drive and Mill Avenue

University Drive and Mill Avenue

Tempe, AZ 85281, USA

Tempe, AZ 85281, USA

ahoch@asu.edu

helenchavez@asu.edu

Ryan Kral

Javier Gonzalez-Sanchez

Arizona State University

Arizona State University

University Drive and Mill Avenue

University Drive and Mill Avenue

Tempe, AZ 85281, USA

Tempe, AZ 85281, USA

rdkral@asu.edu

javiergs@asu.edu

Michael Teposte

Robert K. Atkinson

Arizona State University

Arizona State University

University Drive and Mill Avenue

University Drive and Mill Avenue

Tempe, AZ 85281, USA

Tempe, AZ 85281, USA

mteposte@asu.edu

Robert.atkinson@asu.edu

Permission to make digital or hard copies of part or all of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that
copies bear this notice and the full citation on the first page. Copyrights
for third-party components of this work must be honored. For all other
uses, contact the Owner/Author.
Copyright is held by the owner/author(s).
ISWC'14 Adjunct, September 13 – 17, 2014, Seattle, WA, USA
ACM 978-1-4503-3048-0/14/09.
http://dx.doi.org/10.1145/2641248.2641360

1

Building affect-driven adaptive environments is a task
geared toward creating environments able to change
based on the affective state of a target user. In our
project, the environment is the well-known game, PacMan. To provide affect-driven adaptive capabilities,
diverse sensors were utilized to gather a user’s
physiological data and an emotion recognition
framework was used to fuse the sensed data and infer
affective states. The game changes driven by those
affective states aim to improve the user experience by
keeping or increasing player’s engagement.

Author Keywords
Affective states, affect recognition, affect-driven
adaptation, video games

ACM Classification Keywords
H.5.2 [Information interfaces and presentation]: User
Interfaces --- interaction styles, input devices and
strategies.

Introduction
Building affect-driven adaptive environments comprises
reading a user’s physiological information through
sensors, inferring the user’s affect, and then using this
information to create a feedback loop: the software
change driven by the affect and its changes aims to
alter the user’s affect. Our project is geared toward

ISWC '14 ADJUNCT, SEPTEMBER 13 - 17, 2014, SEATTLE, WA, USA

altering the well-known game, Pac-Man [1], in order to
elicit affective responses from users. A user is able to
play Pac-Man while wearing various sensors that
communicate with a server application, which then
sends information to the Pac-Man client application.
The server application synchronizes and fuses the data
collected by the sensors and infers the affective state
[2]. The affective state is represented as a pleasure,
arousal, and dominance (PAD) vector. The Pac-Man
client utilizes the PAD vector at regular intervals to
alter the game and ultimately attempts to push the
user into an engaged state.
Figure 1. When the player is meditating,
the speed of ghosts and the music tempo
are increased; the special features (e.g.,
power pellets) are disabled; and when
the game ends, a slightly more difficult
map is loaded.

measured. A frustrated user would press on the
directional keys in an aggressive manner rather than in
a gentle or calm manner.
EEG sensor
Electroencephalographically (EEG) sensors use
brainwaves as an information source; they measure the
electrical activity along the scalp, produced by the firing
of neurons within the brain over a period of time, which
is gathered from multiple electrodes placed on the
scalp. They are able to infer diverse affective constructs
such as engagement, excitement, boredom, meditation,
and frustration.

Technology Background
The technologies, both hardware and software, utilized
for this project are described in [3] and summarized in
the follow paragraphs.
Posture sensor
The project utilizes a chair posture sensor. This device
produces raw data values based on how the user is
positioned on a chair. Values are related with the user’s
interest level: if the user is engaged, he/she would be
more likely to be sitting forward rather than relaxed
and leaning back against the chair; for an engaged
user, the values in the sensor would measure low or
lack of pressure on the back of the chair and high
pressure on the front of the seat.

The affect recognition module
The server application is an affect recognition
framework that we use off-the-shelf [2]. It is a server
application that receives sensor data, synchronizes and
fuses the data, determines an affective state, expresses
the affective state as a PAD vector, and communicates
it with other applications, named client applications.
We feed the server application with the raw values that
posture, pressure, and EEG sensors gather. Then, our
client application (the game) uses the PAD vector,
provided by the server, as an input and adjusts itself
according to the values on the PAD vector.

The Game

Figure 2. While the player is engaged,
the settings of the game stay the same.

Pressure sensor
This device is based on the mouse pressure sensor
described in [3]; however, since Pac-Man uses the
directional keys on the keyboard, the mouse pressure
sensor was modified to put the sensor pads of the
mouse into the fingertips of a glove so that the
pressure sensitivity the user has on the keys can be

2

The game is a highly modified version of an open
source version of the video game Pac-Man [1]. We
chose this game due to its simplicity, which allows a
considerable amount of modifications. The goal of the
game of Pac-Man is to eat all the small pellets in a
certain maze to win the game by avoiding being eaten
by the ghosts. In our game, affective components,

SESSION: DEMOS

expressed in PAD vectors, are taken as inputs to allow
the game to change as discussed further below.

Figure 3. When the player is bored, the
speed of ghosts and the music tempo are
increased; the special features (e.g.,
power pellets) are disabled; and when
the game ends, a very difficult map is
loaded.

Affective state changes
Four affective states were focused on for this game:
meditation, engagement, boredom, and frustration. As
the goal of the affect-driven adaptive version of the
game is to keep or bring the player into the engaged
state, the settings are therefore maintained once the
player reaches this state. Table 1 shows the changes
that the game will undergo when the user reaches each
of the affective states. The features that are adapted
accordingly with the affective state include: the color of
Pac-Man (blue, green, gray, and red); the speed of
Pac-Man, which can increase (+) or decrease (-); the
number of ghosts, which can increase (+) or decrease
(-); the speed of the ghost, which can increase (+) or
decrease (-); the music tempo, which can be faster or
upbeat (+) or slower or ballad (-); the special features
(such as fruits, power pellets, and 1-up component),
which can be enabled (E) or disabled (DE); and the
difficulty level of the next maze, which can increase (+)
or decrease (-).

Implementation

Figure 4. When the player is frustrated, the
speed of ghosts and the music tempo are
decreased; the special features (e.g., power
pellets) are enabled; and when the game
ends, an easier map will be loaded.

The project was developed in Java using Eclipse as IDE.
The development involved the modification of an open
source version of the Pac-Man video game and its
connection to the emotion recognition framework, using
a client/server approach. Documentation, user-guides,
and a configuration file were created. Documentation
and user-guides are available for both developers and
non-developers to easily maneuver through the code
and make changes to the game. The configuration file
allows researchers to set up diverse starting variables
in the game, such as: the base speed of Pac-Man and

3

the ghosts, the poll time of readings (in seconds), and
the affective states to be considered to adjust the
game.
Once the user has begun the game, the PAD vector
readings are polled at the frequency defined by the
researcher in the configuration file and used to
determine the appropriate adjustments. The software
stores all the information about the affective state and
status of all the game features in the log file. The
information in the log file is used to determine what
happens as the user plays the game and serves as key
data for researchers to answer empirical questions
about personalizing game environments based on
affect.

d

Feature \State
Color
Pac-Man speed
Ghosts #
Ghosts speed
Music tempo
Fruits
Power pellets
1-up
Next Level Diff

M

E

B

F

Blue
=
=
+
+
DE
DE
DE
+

Green
=
=
=
=
=
=
=
=

Gray
+
+
+
DE
DE
DE
+

Red
+
E
E
E
-

Table 1. Changes made in the game when the user reaches
each of the affective states: Meditation (M), Engagement (E),
Boredom (B) and Frustration (F). Features can increase (+),
decrease (-), be enabled (E), be disabled (DE), or stay in the
same status (=).

Discussion
An affect-driven adaptive environment was successfully
developed (using an open source version of Pac-Man)

ISWC '14 ADJUNCT, SEPTEMBER 13 - 17, 2014, SEATTLE, WA, USA

to work with the sensor suite developed in a prior
project of the sponsoring research laboratory. The
original open source Pac-Man game allowed us to add,
in a short period of time, extra features, which allowed
the game to adapt based on the user’s affect.
When designing the game adaptation strategy, it was
necessary to figure out the affective states on which to
focus. After a first brief analysis, we ascertained that
changing the speed of Pac Man and the ghost entities
were not enough to manipulate the user’s affective
state, so additional features needed to be manipulated.
Additional testing led to the manipulation of the music,
colors, and special elements (such as fruits and 1-up).
The strategy for keeping the user engaged by
manipulating the described features is defined as
follows: (a) we did not want the player to be bored,
thus we made the game extremely difficult when the
player reached that state by decreasing the speed of
Pac-Man and increasing the number and speed of the
ghosts and the music tempo; (b) we did not want the
user to be frustrated, thus we made the game very
easy in this state by increasing the speed of Pac-Man
and decreasing the number and speed of the ghosts
and the music tempo as well as providing special
features such as power pellets and fruits; (c) finally, we
did not want the player to be in meditation, thus we
made the game slightly more difficult to give the player
a push toward the engaged state by increasing the
speed of Pac-Man and the music tempo.

4

Enabling the Pac-Man game with affective-driven
adaptive settings resulted in a game that became
easier to win, which was more enjoyable to users.
Additional research and testing should be done to
further hone the types of adaptations that guide users
to an optimal state and to explore and test the impacts
on other user outcomes including learning and
performance management.

Acknowledgments
The project was developed as coursework of
CSE423/424 (Systems Capstone) at ASU collaborating
with the Advancing New Generation Learning
Environments Lab and supported by Office of Naval
Research under Grant N000141310438.

References
[1] Open source Pac-Man software.
http://sourceforge.net/projects/javaipacman/
[2] Gonzalez-Sanchez, J., Chavez-Echeagaray, M. E.,
Atkinson, R., and Burleson, W. ABE: an Agent-Based
Software Architecture for a Multimodal Emotion
Recognition Framework. In Proc. Software Architecture
(WICSA), 2011 Ninth Working IEEE/IFIP Conference on,
IEEE (2011), 187–193.
[3] Gonzalez-Sanchez, J., Christopherson, R. M.,
Chavez-Echeagaray, M. E., Gibson, D. C., Atkinson, R.,
and Burleson, W. How to Do Multimodal Detection of
Affective States? In Proc. Advanced Learning
Technologies (ICALT), 2011 11th IEEE International
Conference on, IEEE (2011), 654–655.

2011 2011
NinthNinth
Working
Working
IEEE/IFIP
Conference
Conference
on Software
on Software
Architecture
Architecture

ABE: An Agent-Based Software Architecture
for A Multimodal Emotion Recognition Framework

Javier Gonzalez-Sanchez, Maria Elena Chavez-Echeagaray, Robert Atkinson, Winslow Burleson
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University
Tempe, Arizona, US
e-mail:{javiergs, helenchavez, robert.atkinson, winslow.burleson}@asu.edu
software architecture is not described and the definition of a
framework to enable others to integrate emotion recognition
into their systems was not a goal.
To the best of our knowledge there are no architectures,
frameworks, libraries or generic software tools, that allow
software engineers to easily integrate true multimodal
emotion recognition into their software projects, such as the
ones that exist for computer vision [10], web platforms [11]
or database management [12].
Accordingly, the work reported here offers a first step
toward filling the gap in the lack of frameworks and models,
addressing: (a) the modeling of an agent-driven componentbased architecture for multimodal emotion recognition,
called ABE (for Agent-Based Environment), and (b) the use
of ABE to implement a multimodal emotion recognition
framework, under the paradigm of “highly reusable software
components”, which can be integrated into third-party
systems and provide them with the ability to become an
empathetic system, as needed.
It is important to clarify that the primary contribution of
this work is related to software architecture and it is not
about new algorithms for emotion recognition or new
hardware devices for sensing human signals indicating
emotional changes.
This paper is structured as follows. Section II reviews the
related background about sensing devices and some related
terminology. Section III presents the architecture and
framework. Section IV exemplifies the use of ABE
framework by describing how it was integrated in two demo
systems. Section V presents an evaluation of the framework
in terms of performance while used in the demo systems.
Section VI presents conclusions and ongoing work.

Abstract—The computer's ability to recognize human
emotional states given physiological signals is gaining in
popularity to create empathetic systems such as learning
environments, health care systems and videogames. Despite
that, there are few frameworks, libraries, architectures, or
software tools, which allow systems developers to easily
integrate emotion recognition into their software projects. The
work reported here offers a first step to fill this gap in the lack
of frameworks and models, addressing: (a) the modeling of an
agent-driven component-based architecture for multimodal
emotion recognition, called ABE, and (b) the use of ABE to
implement a multimodal emotion recognition framework to
support third-party systems becoming empathetic systems.
Keywords - affective computing; architecture; framework;
agent-based; multimodal; emotion recognition; empathetic
systems

I.

INTRODUCTION

A key concept in this work is empathy, i.e., to enable a
system to recognize and understand human emotions and
react
appropriately
in
consequence
with
those
understandings. Enabling computers to be empathetic has
implied the convergence of affordable wireless sensors and
the application of novel machine learning and data mining
algorithms to deal with the vast amounts of data generated by
the sensors [1].
On the one hand, there are several examples of research
conducted on creating empathetic systems to support
learning [2][3][4], health care [5] and videogames [6]. But
the majority of the research does not focus on the creation of
reusable software, software frameworks or the best
methodological practices for those purposes [7]. Each
attempt either develops its own system, or uses a legacy
system. They are focused on creating a proof-of-concept
system to collect data and validate technology approaches.
On the other hand, some of the best-known existing
architectures and libraries that provide support for emotion
recognition use monomodal or bimodal approaches. For
example the project described in [8] is an open-source
implementation that combines speech emotion recognition
with facial expressions analysis and head movement
tracking. The multimodal proposal in [9] could be considered
an antecedent of this work due the use of multiple sensors
and their integration in a client-server structure although
978-0-7695-4351-2/11 $26.00 © 2011 IEEE
DOI 10.1109/WICSA.2011.32

II.

BACKGROUND

Because this work is related to emotions and to the
intention of enabling computers with the ability to recognize
them, this section provides background information to clarify
some terminology used within this paper and describes the
sensing devices and perception mechanisms used in this
work.
A. Definitions
In the rest of the paper we use these definitions for the
related concepts.

187

Our multimodal approach includes: brain-computer
interfaces, eye tracking systems, face-based emotion
recognition systems, and sensors to measure other
physiological signals (skin conductivity, posture, and finger
pressure). Fig. 1 shows the elements related to our approach
of a multimodal emotion recognition system. The
description of each element is as follows:

1) Sensing device. These are hardware devices that
collect quantitative data as measures of physiological
signals of emotional change.
2) Raw data. We call the measures provided by the
sensing devices “raw data”. These are data packages that
sensing devices send to the computer.
3) Sensed value. This refers to the raw data after being
parsed into a software data structure. Sensed values are
useful by themselves and also help to infer emotions. For
example, face sensed values are “actions units”; action units
are standard values used to categorize facial expressions and
they have proven being useful to recognition of basic
emotions [13].
4) Perception mechanism. These are algorithms that
infer emotions using sensed values as input. An example of
this is the act of inferring emotions from facial expressions
sensed by a video camera.
5) Belief. Perception mechanisms provide “beliefs”
about their understanding of the user’s emotional change.
6) Multimodal. It refers to combining several sensing
devices (i.e., perception mechanisms), either to recognize a
broad range of emotions or to improve the accuracy of a
process. The multimodal strategy provides more than one
way to recognize an emotion.
7) Emotional state. Represents the user emotion in a
time !"#This is developed from the integration of beliefs in
that time !.

1) Brainwaves. We incorporate the Emotiv© EPOC
headset, an inexpensive wireless hardware device, which
uses EEG technology to sense electroencephalography
activities [14].
2) Eye movement. We incorporate the Tobii© Eye
Tracking System. An eye tracker provides data about a
user’s focus of attention and focus time while the user
performs a task on the computer [15].
3) Facial expressions. We incorporate MindReader, an
inference system developed at MIT Media Lab, which
infers, in real-time, emotions from facial expressions and
head movements [16].
4) Skin conductivity. This sensor measures the electrical
conductance of the skin, which varies with its moisture level
that depends on the sweat glands, which are controlled by
the sympathetic and parasympathetic nervous systems. Skin
conductance is an indicator of psychological or
physiological arousal. We use a wireless Bluetooth skin
conductance device developed at MIT Media Lab [17].

B. Sensing and Perceiving
Several existing systems are able to detect a single
emotion or a reduced set of emotions. One of our goals was
to make significant advances to emotion recognition by
integrating and encapsulating pre-existing software
components into our implementation to sense and recognize
a wide range and diverse set of emotions.

TABLE I.
Sensing
Device
Emotiv©
headset
Webcam and
MindReader
software

Figure 1. Multimodal emotion recognition includes: sensing brainwaves,
eye tracking, facial expression analysis and physiological signals (skin
conductivity, posture, and finger pressure). Beliefs are inferred from raw
data and emotional states correspond to the integration of beliefs from
several sources in a time !.

188

INFERRED EMOTIONS AND SENSED VALUES
Sampling
Rate (in ms)
125
100

Emotiv©
headset

7

Emotiv©
headset

125

Skin
conductance
sensor
Pressure
sensor

500

Tobii©
Eye Tracking
Posture
sensor

16

150

500

Inferred emotions
Excitement, engagement, boredom,
meditation and frustration.
Agreeing, concentrating,
disagreeing, interested, thinking and
unsure.
Sensed values
EEG activity. Reported in 14
channels, labeled: AF3, F7, F3,
FC5, T7, P7, O1, O2, P8, T8, FC6,
F4, F8, and AF4 [18].
Blink, wink (left and right), look
(left and right), raise brow, furrow
brow, smile, clench, smirk (left and
right), and laugh.
Arousal.
One pressure value per sensor
allocated into the input/control
devices.
Gaze point (x, y).
Pressure values in the back pad and
the seat cushion (in the right, middle
and left zones).

5) Posture. We use a low-cost, low-resolution pressure
sensitive seat cushion and back pad with an incorporated
accelerometer to measure elements of users’ posture and
activity, developed at ASU based on experience using a
more expensive high resolution unit from the MIT Media
Lab. The use of this sensing device to recognizing naturally
occurring postures and associated emotional states is
describe in [19].
6) Finger pressure. Based on pressure sensors we detect
the increasing amount of pressure that the user puts on a
mouse, or any other controller (such as a game controller).
These measures are correlated with levels of frustration.
Mouse implementation is described in [20].

head of the federation. Specialist agents are in permanent
communication with Centre, contributing with data. Centre
implements the integration algorithms that convert, in realtime, the beliefs reported by the Specialist agents into
emotional states. For third-party systems Centre acts as a
facade that hides the internal complexity of the federation.
As shown in Fig. 2, third-party systems are able to contact
Centre and subscribe to receive information about the
emotional state of the user.
3) Internal communication. The communication
structure between Specialist agents and Centre is made by
data flows moving messages from one point to another.
Specialist agents encapsulate information in packages with:
(a) a header composed of timestamp, in milliseconds, and an
agent ID, and (b) a body composed of an array of rational
numbers.
4) External communication. The federation, through
Centre, offers a service-oriented behavior using a publishsubscribe style. The underlying concept is that the agent
federation provides a service and other systems can
subscribe to the service and customize which information
they are interested in receiving.

Sensed values, inferred emotions and sampling rates are
listed in Table I. Given this collection of sensing devices
with proven functionality when used independently, the
research question we posed was how to achieve a framework
where all of them work together. We sought to create a
framework that can be used and reused without a
cumbersome installation or adaptation (re-programming)
process.
C. Agents and Agent Federation
Agents are autonomous pieces of software that allow us
to encapsulate sensing devices and their perception
mechanisms into independent, individual and intelligent
components [21] [22].
When several agents need to interact, it is important to
establish an organizational strategy for them, which defines
authority relationships, data flows and coordination
protocols. To articulate the organizational strategy we define
an agent federation. An agent federation is conformed by a
group of agents that cede some amount of autonomy to a
single delegate, which represents the group. The delegate is a
“distinguished” agent that acts as an intermediary between
all the agents in the group and the outside world [23].
III.

Both internal and external communications are achieved
using TCP/IP connections, so agents can run within one
computer or be distributed among several computers.
The elements in Fig. 2 constitute a federation unit. One
federation unit can take care of one user. This means that,
during runtime, there is one agent of each kind related to
each sensing device for each user. Several federation units
can be instantiated and communicated with to establish the
emotional state of a group of users. The following sections
provide a detailed description of the architecture, framework
and data management.

IMPLEMENTATION

We propose an agent-based model as the most
appropriate solution because we need to deal with several
different sources of data flows, where each flow proceeds
along different time intervals, and furthermore, these
multiple highly varying inputs can be used as a whole or as a
subset. Our model takes into account that it may be
necessary, in the future, to add even more sources of sensed
values.
ABE follows the federation organizational strategy as
shown in Fig. 2. The structure is composed of:
1) Specialist agents. These are responsible for: (a)
collecting raw data, (b) parsing it into sensed values and
inferring beliefs, and (c) communicating their beliefs with
the head of their federation.
2) Centre agent or simply Centre. This is the name of
our delegate or “distinguished” agent to which the rest
ceded some amount of autonomy, and which acts as the

Figure 2. ABE follows the organizational strategy of federation. The
federation assigns one Specialist agent to collect raw data from each
sensing device. Specialist agent implements the perception mechanism for
its assigned sensing device to map raw data into beliefs. Beliefs are
reported to Centre, which integrate them into one emotional state report.
Third-party systems are able to obtain emotional state reports from Centre
in a publish-subscribe style.

189

Figure 3. Architecture. Specialist agents (brain, eye, face, skin, pressure and posture) deal with sensing devices and encapsulate parsing, inference, and
communication functionalities. They work as a federation, which implies that they send resources (beliefs) and subordinate their behavior to the head of the
federation. Centre acts as the head of the federation. Centre’s responsibilities include integrating beliefs to create emotional state reports that can be shared
with third-party systems in a publish-subscribe style

serial port communication
corresponding sensors.

A. The Architecture
Fig. 3 shows the macro-level [24] view for the ABE
agent-based multilayer distributed architecture, as it is
currently designed, where all the agents are represented
along with their components and communication channels.
Each component is described below.

to

read

data

from

the

b) Model. Model obtains sensed values from a Data
Source and uses a perception mechanism to infer beliefs.
Model is also responsible for filtering, labeling, time
stamping, packaging beliefs, and sending packages to
Centre.

1) Specialist agent. One Specialist agent is included for
each sensing device listed in Table I, they are labeled as
brain-agent, eye-agent, face-agent, skin-agent, pressureagent, and posture-agent. A Specialist agent is divided into
four components: (a) Data Source, (b) Model, (c) Controller,
and (d) Communicator. Each component implements one
agent responsibility.
a) Data Source. Data Sources access raw data from the
sensing devices, parse the raw data into sensed values, and
provide those sensed values to the component in the next
layer. Two types of Data Sources are defined: Wrappers and
Drivers. On the one hand, Wrappers encapsulate legacy
systems making calls to their SDK methods; Data Sources
for brain-agent, eye-agent and face-agent wrap Emotiv©
SDK, Tobii© SDK and MIT MindReader System
respectively. On the other hand, Drivers communicate with
sensing devices through the computer bus or
communication subsystems (such as serial or USB ports);
skin-agent, pressure-agent, and posture-agent implement

c) Controller. Controller implements a configuration
mechanism. It receives requests from Centre and modifies
the behavior of the Specialist agent according to the
received requested. Configuration parameters include
change sampling rate and change filtering strategy.
d) Communicator. Communicator provides networking
capabilities to send beliefs to Centre.
2) Centre Agent. Centre is divided into four
components, each of them implements one agent
responsibility: (a) Communicator, (b) Supervisor, (c)
Concurrent Data Repository, and (d) Publisher.
a) Communicator. It connects with the communicator
component of the Specialist agent to receive beliefs.
b) Supervisor. It is the component that takes beliefs as
input for its integration algorithms and generates emotional
states. Since sensing devices and their associated Specialist
agents have different sampling rates, part of the

190

responsibility of Supervisor is to standardize the sampling
rates. Thus, Centre is able to report emotional states with
the requested sampling rate.
c) Concurrent Data Repository. The Data repository is
explained in the Data Management section below.
d) Publisher. Publisher is the component that acts as
the facade of the agent federation to the outside world
implementing the publish-subscribe functionality. A thirdparty system can express interest in receiving reports of the
emotional state of the user by sending a “subscribe”
message. The subscription includes the specification of the
periodicity that is desired (starting in 1 millisecond) and the
composition of the data of interest (e.g., all sensors,
emotions and eye tracking, posture and facial expressions,
only positive emotions, only negative emotions). From that
point forward Publisher will be sending reports about the
emotional state of the user to that third-party system.

Figure 4. The Concurrent Data Repository, located in Centre Agent, has
one data-item mapped to each agent.

C. Data Management
Centre provides a concurrent control mechanism for data
collection: a concurrent data repository in which each dataitem is mapped to one of the Specialist agents and stores the
set of sensed values provided by that Specialist agent. Each
data-item stores the most recent information provided by its
Specialist agent without interfering with the information
provided by the others Specialist agents. In this way we
solve the problem of different sampling rates across diverse
sensing devices, Fig 4.
Reading the data repository content in a time t provides
us with the user’s emotional state for t. Data items store the
last provided value until a new one arrives. Table II shows
how this is handled in the data repository across time. Each
row corresponds to the data repository in a time t, and
represents the integration of the beliefs reported by the
Specialist agents into one emotional state. As can be
observed skin conductance remains the same from t1 to t4
since the sensor device related with that information has the
slowest sampling rate (500 ms).

B. The Framework
The definition of the micro-level architecture [24] uses a
pattern-based approach [25]. The use of software design
patterns populated our framework and SDK. The process of
moving from macro-level architecture to the micro-level
architecture is described in detail in [26]. In order to build
upon ABE framework, developers are required to do the
following:
1) Create new Specialist agent. To create new types of
Specialist agents developers are responsible for:
• Implementing the interface Agent in a new class.
• Creating the associated Data Source implementing a
class from $%!%&'()*+,)%--+) interface (to
encapsulate a legacy SDK) or a class from
$%!%&'()*+$)./+) interface (to access a new
sensing device).
• Completing the implementation of an ADAPTER
pattern, for $%!%&'()*+,)%--+), or completing the
implementation of a DELEGATE pattern, for
$%!%&'()*+$)./+).
• Programming the Model component of the agent (i.e.
complete a STRATEGY pattern implementation).
• Using the communicator facilities to send
information to Centre (i.e. implement 0'11(2.*%!')
interface).

IV.

USAGE EXAMPLES

We have evaluated the suitability of ABE to our needs
both at the architectural and runtime levels. The six
Specialist agents and Centre agent have been implemented
and ABE has been tested in two different kinds of scenarios,
in a gaming environment study and in an intelligent tutoring
system development project.
A.

Gaming Environment Study
The purpose of this study was to measure the correlation
of the engagement/boredom of the player (depending on
his/her level of expertise) while playing different difficulty
levels in Guitar Hero© video game.

2) Extend Centre capabilities. Centre’s capabilities can
be extend in order to handle new types of beliefs (coming
from new types of Specialist agents) by:
• Completing the implementation of a STRATEGY
pattern to define new integration algorithms.
• Implementing a new knowledge source in a
BLACKBOARD pattern.
• Using communicator facilities to receive information
from a new Specialist agent.

TABLE II.
Time
t1
t2
t3
t4
t5

191

INTEGRATION OF EMOTIONAL STATES IN TIMES TN

Brain

Eye

Face

Skin

Pressure

Posture

v1
v1
v2
v2
v3

v1
v2
v3
v4
v5

v1
v2
v3
v4
v5

v1
v1
v1
v1
v2

v1
v1
v2
v2
v3

v1
v1
v2
v2
v3

Guitar Hero© is a game in which players use a guitarshaped game controller to simulate playing lead, bass guitar,
and rhythm guitar across numerous rock music songs.
Players match notes that scroll on-screen to colored fret
buttons on the controller, strumming the controller in time to
the music in order to score points, and keep the virtual
audience excited [27].
ABE was applied in a lab setup where 21 students were
asked to play Guitar Hero© (as a learning experience, learn
to play). The scenario involved the user wearing the skin
conductance bracelet and the Emotive© EPOC headset while
being tracked by the MIT MindReader System and by
Tobii© Eye Tracking System. The game controller was
modified by adding a pressure sensor to each of the colored
fret buttons, on the arm of the guitar. Users were asked to
play their choice of a single song at each of four different
levels (easy, medium, hard and expert), from a pre-selected
list of songs. ABE provided access to the sensed data and
provided mixing functionality. Centre sends data to a logger
component (developed for this experiment). Two data
streams were provided to create two dataset files, one with
1/128 s rate and other with 1s rate. Each dataset was
requested for a different data mining approach.
Fig. 5 shows one of the results of this study where Centre
provided integrated data from eye tracking and brainwaves.
The figure shows the Guitar Hero© screen; the superimposed
dots (in black) indicate that at one or more times, throughout
the session, while the user was looking at that location on the
screen frustration was detected by ABE. The Guitar Hero©
screen is composed of an image that resembles the arm (or
frets) of a guitar, in the middle of the image; overlaid on this
image, musical notes scroll down toward the bottom of the
screen. Each note is mapped according to the color of a
physical button that the user should actuate before the note
moves off the screen. The scene is decorated with stage
items, instruments and performers.

The frustration points were the 34#5 coordinates of the
user gaze while feeling frustrated. A circle represents each
frustration point and its size is equivalent to the time spent
by the user staring at that location. Since the subject was a
novice user of this videogame, this data shows how the user
feel frustration looking into the notes while they scroll onscreen.
B. Intelligent Tutoring System
In a second case, we are currently working on the
development of an intelligent tutoring system applying ABE
to generate better and more accurate hints and feedback to
the student with the intention of creating a more empathetic
learning process and environment, reducing student
frustration and avoiding student quitting.
During a pilot test in July 2010, the intelligent tutoring
system [28] was enriched with the sensing devices and
agents related to: (a) skin conductance bracelet, (b) Emotiv©
EPOC headset, (c) the MIT MindReader System, and (d)
pressure sensors on the mouse. Students were asked to
perform a task (about system dynamics modeling) using the
intelligent tutoring system. While the student was working
on this task, the intelligent tutoring system collected data
using ABE. During this study the data was collected for post
analysis and review what information the student reported.
The next step in the project will consist on using this
information to create a student model and adjust the behavior
of the intelligent tutoring system in response to the emotional
state of the student.
V.

PERFORMANCE

ABE framework components were created to be as
lightweight as possible to run in the background of an
existing system. The goal was to avoid creating a dominant,
“star player” system, but rather to provide a platform that
could operate behind the scenes, to improves and
complements other systems. Table III shows the result of
memory load and processor usage running each of the agents
in a system with the following characteristics: Intel Xenon
CPU W3520 at 2.67 GHz with 4 cores, 3.50 GB of RAM,
and running Windows XP professional with Service Pack 3.
It is important to mention that for face, brain and eye
agents the values reported in Table III include the load
caused by the agent and by the underlying system: MIT
MindReader System, Emotiv© EPOC SDK and Tobii© Eye
Tracking SDK, respectively.
TABLE III.

PERFORMANCE OF SPECIALIST AGENTS

Agent

Figure 5. Mixed data from eye tracking system and frustration
measurement for a single novice user playing Guitar Hero©. Black dots
correspond to places where the player was looking while feeling frustrated.
The image was converted to gray scale, the contrast incremented and the
tones inverted to facilitate its print.

192

% CPU

Performance
Memory (Kb)

Skin

8 - 15

14,100 - 15,200

Face

34 - 43

60,000 - 110,000

Brain (for emotion)

9 - 16

8,260 - 8,500

Brain
(For physiological signals)

6 - 15

7,200 - 7,800

Pressure

8 - 14

15,900 - 16,200

Eye

15 - 25

169, 500 - 170,000

VI.

CONCLUSIONS AND ONGOING WORK

[8]

In this paper we have presented ABE as our
architectonical proposal for a multimodal emotion
recognition framework that supports the creation of
empathetic systems. This work is rooted in an agent-based
approach under a multilayer-distributed architecture oriented
to create highly reusable, flexible and extensible software
components. We have achieved the integration of both novel
and well-known sensing devices into ABE including brain
computer interfaces, eye tracking systems, computer vision
systems and physiological sensors. We illustrated the use of
ABE in practice, building software for two different
scenarios: one was a gaming study in which we sensed
emotional status of students while playing a video game, and
a second one into an affective tutoring system development
project. In both scenarios it was seen that the integration of
ABE was a reasonably easy experience with good
performance results. We are excited and encouraged to
continue with the next step in this process and deploy ABE
externally in order to have others research and development
groups using and testing ABE by themselves without support
from our engineering team. The next steps for ABE are
focused on: (a) refactoring components (b) deploy API’s
documentation, (c) adding support agents such as loggers
and visualizers to conform a dashboard interface. Beside
that, looking into test-case scenarios for reactive systems, has
become more relevant to maintain latency in a useful level
for real-time interaction, therefore integration of parallel and
multicore computing models is also in our list of next steps.

[9]

[10]
[11]
[12]
/%+0
/%-0
[15]
[16]

[17]

[18]

[19]

ACKNOWLEDGMENT
This research was supported by Office of Naval Research
under Grant N00014-10-1-0143 awarded to Dr. Robert
Atkinson.

[20]

REFERENCES
[1]
[2]

[3]

[4]
[5]

[6]

[7]

[21]

R. W. Picard, Affective Computing, MIT Press, 1997.
I. Arroyo, D. G. Cooper, W. Burleson, F. P. Woolf, K. Muldner, and
R. Christopherson, “Emotion Sensors Go to School,” Proc. Artificial
Intelligence in Education: Building Learning Systems that Care: from
Knowledge Representation to Affective Modelling, (AIED 09), V.
Dimitrova, R. Mizoguchi, B. du Boulay & A. Grasser (Eds.), IOS
Press, July 2009, vol. Frontiers in Artificial Intelligence and
Applications 200, pp. 17-24, doi: 10.3233/978-1-60750-028-5-17.
B. Woolf, W. Burelson, and I. Arroyo, “Emotional Intelligence for
Computer Tutors,” Supplementary Proc. 13th International
Conference on Artificial Intelligence in Education (AIED 07), July
2007, pp. 6-15.
S. D'Mello, R. W. Picard, and A. Graesser, "Toward an AffectSensitive AutoTutor," IEEE Intelligent Systems, July/August 2007,
vol. 22 no. 4, pp. 53-61, doi:10.1109/MIS.2007.79.
X. Chao, and F. Zhiyong, “A Trusted Affective Model Approach to
Proactive Health Monitoring System,” Proc. International Seminar
on Future BioMedical Information Engineering (FBIE 08). IEEE
Computer Society, 2008, pp. 429-432, doi:10.1109/FBIE.2008.52.
K. Gilleade, A. Dix and J. Allanson, “Affective Videogames and
Modes of Affective Gaming: Assist Me, Challenge Me, Emote Me,”
Proc. Digital Games Research Association, Changing Views –
Worlds in Play (DiGRA 05), June 2005.
R. E. Johnson, “Components, frameworks, patterns,” Proc.
Symposium on Software Reusability (SSR 97), Medhi Harandi (Ed.).
ACM, Feb. 1997, pp. 10-17, doi:10.1145/258366.258378.

[22]

[23]

[24]
[25]

[26]

[27]
[28]

193

M. Schroder, “The SEMAINE API: towards a standards-based
framework for building emotion-oriented systems,” Advances in
Human-Computer Interaction, vol. 2010, 2010,
!"#$%&'%%(()*&%&)+%,-&..
W. Burleson, R. Picard, K. Perlin, and J. Lippincott, “A platform for
affective agent research”, Workshop on Empathetic Agents,
International Conference on Autonomous Agents and Multiagent
Systems, Columbia University, New York, NY, (2004),
ReacTIVision - a toolkit for tangible multi-touch surfaces. Retrieved
April 4, 2011, from http://reactivision.sourceforge.net.
Apache Http Server Project. Retrieved April 4, 2011, from
http://httpd.apache.org.
MySQL – open source database. Retrieved April 4, 2011, from
http://www.mysql.com.
1'2 345672 67!2 8'2 9:#;<;7=2 >96?#6@2 A?B#"72 C"!#7D2 EF<B;5$2 A2
G;?H7#IJ;2K":2BH;2L;6<J:;5;7B2"K296?#6@2L"M;5;7B=N2C"7<J@B#7D2
Psychologists Press, 1978. 2
Emotiv - Brain Computer Interface Technology. Retrieved April 4,
2011, from http://www.emotiv.com.2
Tobii Technology - Eye Tracking and Eye Control. Retrieved April
4, 2011, from http://www.tobii.com.
R. El Kaliouby, and P. Robinson, “Generalization of a vision-based
computational model of mind-reading,” Proc. First International
Conference on Affective Computing and Intelligent Interaction
(ACII 05), Springer-Verlag, Oct. 2005, pp. 582-589,
doi:10.1007/11573548_75.
M. Strauss, C. Reynolds, S. Hughes, K. Park, G. McDarby, and R.
W. Picard, “The HandWave Bluetooth Skin Conductance Sensor,”
Proc. First International Conference on Affective Computing and
Intelligent Interaction (ACII 05), Springer-Verlang, Oct. 2005, pp.
699-706, doi:10.1007/11573548_90.
F. Sharbrough, G. E. Chatrian, R. P. Lesser, H. Luders, M. Nuwer,
and T. W. Picton,
"American electroencephalographic society
guidelines for standard electrode position nomenclature," J. Clin.
Neurophysiol., vol. 8, 1991, pp. 200-202.
S. Mota, and R. W. Picard, "Automated Posture Analysis for
Detecting Learners Interest Level," Proc. Computer Vision and
Pattern Recognition Workshop (CVPRW 03), IEEE Press, June
2003, vol. 5, pp. 49, doi:10.1109/CVPRW.2003.10047.
Y. Qi, and R. W. Picard, "Context-Sensitive Bayesian Classifiers and
Application to Mouse Pressure Pattern Classification," Proc.
International Conference on Pattern Recognition (ICPR 02), Aug.
2002, vol 3, pp. 30448, doi:10.1109/ICPR.2002.1047973.
F. Tuijnman, and H. Afsarmanesh, "Distributed objects in a
federation of autonomous cooperating agents," Proc. International
Conference on Intelligent and Cooperative Information Systems,
May 1993, pp. 256-265, doi:10.1109/ICICIS.1993.291763.
M. Wood, and S. DeLoach, “An overview of the multiagent systems
engineering methodology,” Agent-Oriented Software Engineering,
(AOSE 2000), Springer-Verlag, 2001, pp. 207-221, doi:10.1007/3540-44564-1_14.
B. Horling, and V. Lesser, “A survey of multi-agent organizational
paradigms,” The Knowledge Engineering Review, Cambridge
University Press, 2005, vol. 19, pp. 281-316, doi:
10.1017/S0269888905000317.
J.E. Hollingsworth and B.W. Weide, “Micro-Architecture vs. MacroArchitecture”, Proceedings of the Seventh Annual Workshop on
Software Reuse, (1995),
E. Gamma, R. Helm, R. Johnson, and J. Vlissides. “Design patterns:
Abstraction and reuse of object-oriented design,” Proc. 7th European
Conference on Object-Oriented Programming (ECOOP 93),
Springer-Verlag, July 1993, vol. 707, pp. 406–431.
J. Gonzalez-Sanchez, M.E. Chavez-Echeagaray, R. Atkinson, and W.
Burleson,“ Affective computing meets design patterns: a patternbased modeling of a multimodal emotion recognition framework,” in
submission.
Guitar Hero. Retrieved April 4, 2011, from
http://www.guitarhero.com.
Affective Meta Tutor – Arizona State University. Retrieved April 4,
2011, from http://amt.asu.edu.

Course Overview

CHI 2014, One of a CHInd, Toronto, ON, Canada

Multimodal Detection of Affective
States: A Roadmap Through
Diverse Technologies
Javier Gonzalez-Sanchez

Winslow Burleson

Arizona State University

Arizona State University

699 S Mill Ave.

699 S Mill Ave.

Tempe, AZ 85281 USA

Tempe, AZ 85281 USA

javiergs@asu.edu

winslow.burleson@asu.edu

Maria E. Chavez-Echeagaray

Robert K. Atkinson

Arizona State University

Arizona State University

699 S Mill Ave.

699 S Mill Ave.

Tempe, AZ 85281 USA

Tempe, AZ 85281 USA

helenchavez@asu.edu

robert.atkinson@asu.edu

Abstract
One important way for systems to adapt to their
individual users is related to their ability to show
empathy. Being empathetic implies that the computer is
able to recognize a user’s affective states and
understand the implication of those states. Detection of
affective states is a step forward to provide machines
with the necessary intelligence to appropriately interact
with humans. This course provides a description and
demonstration of tools and methodologies for
automatically detecting affective states with a
multimodal approach.

Author Keywords
emotion recognition; affective states; sensors;
multimodal; affect-driven adaptation

ACM Classification Keywords
Permission to make digital or hard copies of part or all of this work for personal
or classroom use is granted without fee provided that copies are not made or
distributed for profit or commercial advantage and that copies bear this notice
and the full citation on the first page. Copyrights for third-party components of
this work must be honored. For all other uses, contact the Owner/Author.
Copyright is held by the owner/author(s).
CHI 2014, Apr 26 - May 01, 2014, Toronto, ON, Canada.
ACM 978-1-4503-2474-8/14/04.
http://dx.doi.org/10.1145/2559206.2567820

H.5.m [Information interfaces and presentation (e.g.,
HCI)]: Miscellaneous

Course Content
A multimodal approach for automatic detection of
affective states is a three-step process: (1) gather data
that is complex and diverse, (2) filter and integrate
data from several sources, and (3) apply software

1023

Course Overview

Objectives
Describe the sensing devices
used to detect affective
states including braincomputer interfaces, facebased emotion recognition
systems, eye-tracking
systems, and physiological
sensors. Compare the pros
and cons of the sensing
devices used to detect
affective states.
Describe the data that is
gathered from each sensing
device and its characteristics.
Examine what it takes to
gather, filter, and integrate
affective data.
Present approaches and
algorithms used to analyze
affective data and how it
could be used to drive
computer functionality or
behavior.
Audience
Open to researchers,
practitioners, and educators
interested in incorporating
detection of affective states
as part of their technology
toolbox.

CHI 2014, One of a CHInd, Toronto, ON, Canada

algorithms and data processing tools to understand the
data.
Sensing Devices
The first part of the course is a demonstration using
inexpensive, easy to install, and widely available
sensing devices, described in [3], and summarized in
the follow paragraph. For brain-computer interfaces,
the Emotiv© EPOC headset [1] device is used. A simple
30 fps USB webcam and a software system
implementing a supervised learning model are used to
show the face-based emotion recognition approach.
Examples for eye-tracking systems using Tobii© Eye
Tracking System [5] datasets are shown. Three
examples for physiological sensors are shown: arousal
detection using a wireless skin conductance sensor,
posture detection using a posture chair sensor, and
increasing compression detection (correlated with levels
of frustration) using a pressure sensor on a mouse.

data composition and relationships and extract useful
knowledge from data records. Finally, examples using
collected datasets in research studies are shown.

Conclusions
This course aims to provide the attendees with a
thought-provoking presentation about tools and dataset
exploration. While the course do not present an
exhaustive list of all the methods available for
gathering, processing, analyzing, and interpreting
affective sensor data, the course describes the basis of
a multimodal approach that attendees can use to
launch their own research efforts.

Acknowledgements
This research was supported by the Office of Naval
Research under Grant N00014-10-1-0143 awarded to
Dr. Robert Atkinson.

References
Data Filtering and Integration
The second part of the course is a presentation and
discussion about techniques and methodologies used to
filter and integrate information from the different
sources. This part provides an overview of diverse
approaches, such as sliding window, sparse, and statemachine unification.
Analyzing Data
The third part of the course presents quantitative
approaches for data analysis, including automated
reverse engineering, clustering, and classification. For
reverse engineering searches, the Eureqa tool [2] is
used to discover mathematical models of the structural
relationships in the data records. For clustering and
classification, the Weka tool [4] is used to explore the

[1]

Emotiv | EEG System.
http://www.emotiv.com

[2]

Eureka Desktop.
http://www.nutonian.com/products/eureqa

[3]

Gonzalez-Sanchez, J., Chavez-Echeagaray, M.E.,
Atkinson, R. and Burleson, W. ABE: An AgentBased Software Architecture for a Multimodal
Emotion Recognition Framework. In Proc. WICSA
2011, IEEE Press (2011) 187-193.

[4]

Hall, M., Frank, E., Holmes, G., Pfahringer, B.,
Reutemann, P. and Witten, I.H. The WEKA Data
Mining Software: An Update. In Proc. SIGKDD
Explorations, (2009) vol. 11, Issue 1. 10-18.

[5]

Tobii Technology.
http://www.tobii.com

1024

IEEE TRANSACTIONS ON EDUCATION, VOL. 48, NO. 3, AUGUST 2005

531

Investigating the Presentation and Format of
Instructional Prompts in an Electrical
Circuit Analysis Computer-Based
Learning Environment
Jana Reisslein, Robert K. Atkinson, Patrick Seeling, Student Member, IEEE, and
Martin Reisslein, Senior Member, IEEE

Abstract—Research has shown that providing instructional
prompts in computer-based learning environments designed to
support example-based learning fosters learning. In computerbased environments, where learners interact only with a computer
and do not have access to direct support from a teacher, learners
need to be provided with instructional prompts or just-in-time
help intended to encourage more active example processing
during learning. This study investigated whether it was more
beneficial to provide the learners access to on-demand (self-regulated) help after they committed an error in problem solving
or for the learning environment to regulate the presentation of
instructional help externally. Furthermore, two different presentational formats—textual and pictorial—of instructional prompts
were examined. This study was conducted with a computer-based
learning environment that introduced high school students without
any prior content-specific knowledge to the principles of parallel
and series circuit analysis. Textual prompts facilitated practice
problem solving notably better than pictorial prompts. Overall,
textual-based prompts produced a large effect on near transfer. A
significant format of prompts by academic ability interaction was
discovered on near transfer. In particular, lower-ability learners
scored significantly better when given textual prompts; whereas,
their higher-ability counterparts performed equally well with both
formats. Moreover, learners provided with externally regulated
prompts reported significantly more positive attitudes toward the
prompts in general compared to learners in the self-regulated conditions. Finally, continuous motivation was significantly stronger
in learners who viewed textual prompts than in their counterparts
in the pictorial prompt groups.
Index Terms—Backward fading, computer-based learning environment, electrical circuit analysis, external control, high school,
instructional prompts, learner control, pictorial format, textual
format.

I. INTRODUCTION AND RELATED WORK

T

HE computer-based instruction of electrical circuit analysis techniques has received a significant amount of interest over the last decade (see [1]–[7]). This literature contains

Manuscript received July 13, 2004; revised May 3, 2005.
J. Reisslein and R. K. Atkinson are with the Division of Psychology in Education, Arizona State University, MC 0611, Tempe, AZ 85287-0611 USA (e-mail:
jana.reisslein@asu.edu; Robert.Atkinson@asu.edu).
P. Seeling and M. Reisslein are with the Department of Electrical Engineering, Arizona State University, MC 5706, Tempe, AZ 85287-5706 USA
(e-mail: patrick.seeling@asu.edu; reisslein@asu.edu).
Digital Object Identifier 10.1109/TE.2005.852602

a wide variety of computer-based instruction and tutoring systems with the aim to teach circuit-analysis techniques and to
provide opportunities for practicing circuit analysis. Many of
the developed systems interact with the learner to aid in imparting the knowledge of the circuit analysis techniques and to
provide feedback on learner input to practice problems. In the
case of incorrect solutions the feedback is often accompanied
by instructional prompts (help). These learner–program interactions are in the form of text and/or graphics and are controlled
(presented) by the learner or the system. To the best of the authors’ knowledge the impact of both the format and the control
(presentation) of the instructional prompts in circuit analysis tutoring systems have not been previously examined in detail. This
study extends the existing literature on computer-based instruction of electrical circuit analysis in that it examines the impact
of the presentation and the format of the instructional prompts
in electrical circuit tutoring systems.
This study is conducted in the context of a computer-based
instructional module that introduces learners without any prior
content-specific knowledge to the basic principles of parallel
and series circuit analysis. The module is well suited for exposing and introducing high school students to electrical circuit
analysis. The module’s pedagogical features (i.e., the presentation and format of instructional prompts) are evaluated with high
schools students. This evaluation is motivated by the increasing
need to expose high school students [8] and home schooled students [9] to engineering in an effort to attract students to engineering programs at universities and colleges.
Following recent research [10], [11] on the structure of computer-based instructional modules, a backward fading structure
is employed, which has been demonstrated to have a positive effect on learning. With the backward fading structure, the learner
is initially presented with a fully worked-out example and in
the next example all but the last of the problem sub-goals (solution steps) are worked out, and the learner is required to solve
(anticipate) independently the solution of the missing problem
sub-goal. In the subsequent example all but the last two problem
sub-goals are worked out, and the learner is required to anticipate the solutions to the two missing problem subgoals, and so
on, until the learner is required to anticipate the solutions for all
problem sub-goals (independent problem solving). Recent research has also found that instructional prompts in computer-

0018-9359/$20.00 © 2005 IEEE

532

IEEE TRANSACTIONS ON EDUCATION, VOL. 48, NO. 3, AUGUST 2005

based instructional modules foster learning (see [12], [13]) and
that instructional prompts in conjunction with backward fading
are beneficial for learning [14].
The issue of learner versus external control has so far been
primarily investigated in the context of navigating hypermedia
learning environments (see [15], [16]). The authors are not
aware of a study on the impact of learner versus external control of the provisioning of instructional prompts within a given
practice problem, which is the focus of this study. The pictorial
and textual presentation formats of instructional content and
the implications for the cognitive load have been extensively
studied (see [17]–[19]). The impact of pictorial or textual
instructional prompts in interactive learning environments with
fading, however, has not yet been studied in detail.
II. STUDY METHODOLOGY
The present study manipulates two independent variables,
namely the presentation (external versus self-regulated) and
format (pictorial versus textual) of instructional prompts. The
study addresses the following research questions:
•
What is the effect of different presentation and format
of instructional prompts on the learner’s performance?
•
Do the different presentation and format of instructional prompts have a differential effect on the performance of higher and lower ability learners?
•
What are the attitudes of learners toward the different
types of presentation and format of the instructional
prompts?
A. Participants and Design
The participants of this study were 51 students from a
small charter high school in the Southwest. The experimental
sample consisted of 26 females and 25 males. The participants ranged from eighth- to twelfth-graders (2 eighth-graders,
8 ninth-graders, 15 tenth-graders, 18 eleventh-graders, and
, standard deviation
8 twelfth-graders; mean
). The participants had an average grade point
and had not been exposed to
average of
formal instruction on electrical circuit analysis techniques
before participating in this study. They were randomly assigned
to one of the four experimental conditions (cells) as defined
by a 2 2 factorial design with presentation (external versus
self-regulated) and format (pictorial versus textual) of instructional prompts as factors. According to [20], the resulting per
cell sample size was sufficient to detect a large effect (Cohen’s
), which was deemed to be substantively significant in
the present study, based on a conventional alpha level of
(two-tailed) and statistical power of
.
B. Pencil–Paper Materials
The participants were administered a set of pencil–paper materials consisting of a demographic questionnaire, a pretest, an
overview of parallel and series electrical circuits, a post-test, and
an attitudinal survey.
1) Demographic Questionnaire: The questionnaire collected basic demographic data (grade level, gender, ethnicity),

and the participants’ GPA and standardized test scores (Arizona’s Instrument to Measure Standards (AIMS) or Stanford
9 math and reading scores). The questionnaire also asked the
participants whether they had ever learned about electrical
circuit analysis.
2) Pretest: The pretest was designed to assess the participants’ prior knowledge in the area of electrical circuit analysis.
It was composed of six multiple-choice questions relating to the
basic physical meaning of electrical current, voltage, and resistance and elementary properties of electrical circuits.
3) Introductory Overview: The four-page overview of parallel and series electrical circuits introduced the participants
to 1) the physical meaning and units of electrical current and
voltage; 2) electrical circuit elements and their graphical representations, such as light bulbs and batteries, and the way circuit elements are connected with wires in the two main forms
of electrical circuits, namely parallel and series circuits; 3) the
physical meaning and units of resistance and Ohm’s Law; 4)
the calculation of the resistance of a parallel circuit; and 5) the
calculation of the resistance in a series circuit. These last two
sections on calculating the resistance of series and parallel circuits were not focused on deriving the formulas for calculating
the total resistance of the circuit from the resistance values of
for
the individual circuit elements (i.e.,
for parallel
series circuit and
circuit).
The instructional goal was not to teach the participants to use
these formulas. Instead the participants were taught to calculate the total resistance from basic principles, namely Ohm’s
Law and the properties of current and voltage in the electrical
circuits.1 In particular, for the parallel circuit, the participants
were presented with the voltage provided by the battery and resistance values of the individual resistors. For the calculation of
the total resistance of the parallel circuit, the participants were
instructed to proceed through the following three steps. First,
the participants observed that the voltage is the same over each
individual resistor and were presented with the calculation of
the value of the current flowing through each individual resistor
using Ohm’s Law. Second, an example showed the calculation
of the total current flowing in the circuit by summing up the currents flowing through the individual resistors. Third, the total
resistance of the parallel circuit was calculated by dividing the
voltage provided by the battery by the sum of the currents determined in step 2.
The instruction for evaluating the resistance of the series circuit was analogous.
4) Post-Test: The post-test contained eight complex problems, more specifically, four problems (two for each type of
the electrical circuits, parallel and series) to measure the performance on near transfer and four problems (two for each type
of the electrical circuits, parallel and series) to assess the far
transfer learning.
The near transfer problems had the same underlying structure but different surface characteristics from the practice
problems encountered during the learning (computer) phase.
1These properties are described by Kirchhoff’s Current Law and Kirchhoff’s
Voltage Law.

REISSLEIN et al.: INVESTIGATING THE PRESENTATION AND FORMAT OF INSTRUCTIONAL PROMPTS

533

Fig. 1. Screen shot of computer-based learning environment showing statement of a parallel circuit problem, completed first solution step, and pictorial format
of instructional prompt for second solution step.

They required the participants to perform the same tasks (e.g.,
calculating the individual voltage or current, respectively;
determining the total voltage or current, respectively; and
finally computing the total resistance) as they had learned in
the computer-based module.
The far transfer problems had different underlying structure
and surface features as compared to the computer-based practice
problems. In particular, in the far transfer parallel circuit problems the participants were provided with the resistance values
of the individual resistors and the current flow through one of
the resistors and were asked to calculate the total current in the
parallel circuit (battery current). To solve this problem, the participants first had to apply Ohm’s Law to the resistor for which
the current was provided to determine the voltage over the resistor. The participants then had to observe that the voltage is the
same over all resistors and had to calculate the currents through
the other resistors by applying Ohm’s Law to each individual
resistor. Finally, the participants had to sum up the individual
currents to determine the battery current. The far transfer problems for the series circuit were structured analogously. In summary, the far transfer problems required the participants to apply
the same basic principles (Ohm’s Law, basic properties of voltages and currents in parallel and series circuits) as in the practice
problems, but the sequence in which these principles were applied and the circuit element to which Ohm’s Law was applied
differed from the practice problems (and the solution steps presented in the introductory overview).
5) Attitudinal Survey: A 14-item attitudinal survey was
used to collect data on participant attitudes and motivation.
The survey pertained to the overall effectiveness of the computer-based program, the format of the instructional prompts,
and participant continuous motivation. The individual items
were five-choice Likert-type questions. The response choices

were assigned ratings of strongly agree, agree, neither agree nor
disagree, disagree, and strongly disagree. The participant attitude on the overall effectiveness was evaluated with six items
such as: “I learned a lot from this instructional unit” or “The
information was presented effectively.” The participant attitude
on the format of the instructional prompts was evaluated with
four items, such as: “The instructional explanations (hints)
helped me to learn.” The participant attitude toward continuous
motivation was assessed with three items, such as: “I would
like to learn more about electrical circuits.”
C. Computer-Based Learning Environment
The module was developed using the Director MX [21] software, which is an authoring tool for creating rich multimedia
programs. The module was programmed to operate in one of
four modes that corresponded to the four experimental conditions of the current study.
The goal of the computer-based learning environment was to
deliver instruction on the principles of calculating resistance in
parallel and series electrical circuits. The aim of the program
was to present worked-out (solved) examples to the participants and to scaffold their learning by progressively reducing
the number of worked-out solution steps and increasing the
amount of independent problem solving by the participants.
The environment presented two sets (parallel and series) with
four problems each, constituting a total of eight problems. The
cover story from one of the instructional examples that were
shown to the participants on the computer screen during the
learning phase is shown in the top box in Fig. 1.
Each problem had exactly three solution steps. Each step was
clearly labeled and visually distinguished from the other steps.
The computer module revealed one step at a time after the participants clicked the “Next” button, thus allowing the participants

534

Fig. 2.

IEEE TRANSACTIONS ON EDUCATION, VOL. 48, NO. 3, AUGUST 2005

Excerpt of screen shot showing textual format of instructional prompt for second solution step in parallel circuit problem.

to control the pace of their learning. The participants proceeded
through the module by clicking on the “Next Problem” buttons
after inspecting all three steps in each problem. The navigation
was linear, and the participants could not return to previous steps
and problems once they finalized their answers.
The first problem in each of the two sets of four problems
was fully solved (worked-out), whereas in the subsequent problems the worked-out steps were backward faded, and the participants had to anticipate the correct solution to the missing steps.
Specifically, the participants had to solve independently one step
(the last one) in the second problem of each set, two steps (the
last two) in the third problem of each set, and were responsible
for independently solving all three steps in the last problem of
each set.
In the case of incorrect anticipation, the computer-based
learning environment offered an instructional prompt that was
either externally regulated or requested by the participant,
depending on the treatment condition. Participants in the
externally regulated groups were always presented with the
instructional prompt if they made a mistake while solving
the individual steps. On the other hand, the decision to view
the instructional prompts was solely at the discretion of the
participants in the self-regulated conditions. They were offered
the option to receive the instructional prompt but could refuse
the help.
The instructional prompts were presented in two different formats, depending on the treatment condition. In the textual-based
prompt groups, the prompts were verbal reminders of Ohm’s
Law and the properties of currents and voltages in series and
parallel circuits. These reminders were tailored to the individual
problem steps, as illustrated for the second step in a parallel circuit problem in Fig. 2.
The pictorial-based prompts were presented as drawings illustrating the current flow and voltages in series and parallel circuits as well as Ohm’s Law tailored to the individual problem
step, as illustrated in Fig. 1 for the second step in a parallel circuit problem.
Once the request for the instructional prompt was detected,
the prompt appeared on the screen next to the solution step that
needed to be solved. The participants were given two attempts
at solving each missing step. The correct solution was then displayed on the screen. The solved steps remained visible on the
screen after the final answer was presented, allowing the participants to study the entire solution (worked example).
D. Procedure
Groups of 8–15 participants attended one of the five scheduled experimental sessions. The average duration of each
session was approximately 60 min. The participants took part

in the study in a computer lab in their high school. Each participant was seated in front of a Windows-based desktop computer
and instructed to work independently of his/her peers. The
participants first filled in the demographic questionnaire. Next,
they answered the pretest. The participants proceeded to study
the introductory overview on electrical circuits. After studying
the introductory instructional text, the participants worked
through the problems in the computer-based learning environment. During this phase the experimental variation took place.
Immediately after completing the computer-based instructional
program, the participants were administered the post-test.
Finally, they indicated their responses on the attitudinal survey.
III. RESULTS
This section presents the scoring protocol and the results
for achievement (post-test performance), en route practice,
instructional time, and participant attitudes. Cohen’s statistic
values of
,
was used as an effect size index where
, and
correspond to small, medium, and large values,
respectively [20].
A. Scoring
The participants’ performance on the pretest, practice
problem solving during the computer-based instruction, and the
post-test (near and far transfer problems) and their responses
to the attitudinal survey were scored. The computer-based
learning module automatically recorded the en route practice
(accuracy of solving the missing steps) and instructional time
on computer. The maximum pretest score was 6, one point for
each correctly answered multiple-choice question. There were
a total of 12 unsolved steps in the computer-based learning
environment. The participants were given two attempts at
solving each of the 12 unsolved steps. For each correctly solved
step, one point was awarded, thus producing a maximum score
of 12 for each of the solving attempts, i.e., the first and second
anticipations. (A score of zero was assigned for the second
anticipation if the first anticipation was correct.) The individual
scores for each of the anticipations were summed and divided
by 12 in order to obtain the proportion of problem steps that
were correctly solved on the first/second anticipation. The
values of the proportions for the first and second anticipations
ranged from 0 to 1. The eight post-test problems had three
distinctive solution steps each, thus resulting in a maximum
score of three points for each problem, equaling a maximum
total score of 24 (12 points each were associated with the performance on the near and far transfer problems, respectively).
On the attitudinal survey, a rating of strongly agree received a
score of 5, agree a score of 4, neither agree or disagree a score
of 3, disagree a score of 2, and strongly disagree a score of 1.

REISSLEIN et al.: INVESTIGATING THE PRESENTATION AND FORMAT OF INSTRUCTIONAL PROMPTS

535

TABLE I
POST-TEST SCORES BY FORMAT AND PRESENTATION OF PROMPTS. THE TEXTUAL PROMPT FORMAT RESULTED IN SIGNIFICANTLY HIGHER NEAR TRANSFER AND
TOTAL POST-TEST SCORES. BOTH SELF- AND EXTERNAL PROMPT PRESENTATIONS WERE EQUALLY CONDUCIVE TO LEARNING

TABLE II
POST-TEST SCORES BY FORMATS OF PROMPTS AND ABILITY LEVEL. HIGHER-ABILITY LEARNERS PERFORMED EQUALLY WELL WITH BOTH FORMATS OF
PROMPTS. THE LOWER-ABILITY LEARNERS ACHIEVED SIGNIFICANTLY HIGHER NEAR TRANSFER AND TOTAL POST-TEST SCORES WITH THE TEXTUAL PROMPTS

B. Achievement
The overall post-test data (near transfer and far transfer
combined) were analyzed with 2 (textual or pictorial format of
prompts) 2 (self- or external presentation of prompts) analysis of covariance (ANCOVA), using the pretest as a covariate
. The mean scores
and standard deviations
for each treatment group on the near and far transfer post-test
problems and the overall post-test are shown in Table I. The ANCOVA revealed that there was no significant difference between
the two levels of the presentation factor (self versus external regulation) on the overall post-test total, ratio
,
, significance level
.
mean square error
The ANCOVA also showed that there was a statistically significant difference when comparing the two different formats of
prompts (pictorial versus textual). Specifically, on the overall
post-test, participants presented with text-based instructional
prompts scored significantly higher than their counterparts provided with pictorial-based prompts,
. Cohen’s statistic for these data yields an effect
size estimate of
for the total post-test, which approaches a
large effect. Further analysis revealed that on the near transfer
post-test problems, participants in the two textual prompt groups
, 76% mastery level) significantly outperformed
(
participants in the two pictorial prompt groups (
,
.
57% mastery level);
Cohen’s statistic for these data yields an effect size estimate
for the near transfer post-test problems, which corof
responds to a large effect. This advantage did not, however,
extend to the performance on the far transfer items. Finally,
there was no significant interaction between the two factors.
In order to determine which format of prompts was the most
beneficial to learners based on their academic ability a 2 (academic ability level: high or low) 2 (format of prompts: textual or pictorial) analysis of covariance (ANCOVA), using the
, was performed. Participant
pretest as a covariate
GPA and standardized test scores (AIMS math plus reading or

Stanford 9 math plus reading) were converted into scores and
combined to create a general indicator of academic ability. The
participants were blocked by their overall scores and equally
distributed into higher-ability and lower-ability groups.
Table II presents the mean scores and standard deviations
on the post-test by academic ability and format of prompts.
There was a significant format of prompts by academic ability
interaction for the post-test total,
and for the near transfer post-test problems,
. This latter interaction
is shown in Fig. 3. The format of prompts by academic ability
interaction effect was analyzed using a simple main effect analysis. The format of prompts influenced the performance on the
near transfer items for the lower-ability participants,
. However, the format of prompts did not influence the performance on the near transfer items for higherability participants,
.
C. Practice
The program automatically recorded the accuracy of practice
problem solving within the computer-based learning environment. The performance on en route practice problems was analyzed using 2 (textual or pictorial format of prompts) 2 (selfor external presentation of prompts) analysis of covariance (ANCOVA), with the pretest as a covariate
. The scores
for the first and second attempt at solving the practice problems are presented in Table III. The ANCOVA revealed a significant main effect on first anticipation for format of prompts,
, where participants
presented with text-based prompts outperformed their counterparts presented with pictorial prompts. Cohen’s statistic for
these data yields an effect size of
for accuracy of anticipation for solving the practice problems on the first trial, which
corresponds to a large effect. The differences on accuracy of anticipations on the first anticipation between the self and external
approach to presentation of prompts were nonsignificant as was

536

IEEE TRANSACTIONS ON EDUCATION, VOL. 48, NO. 3, AUGUST 2005

Fig. 3. Format of prompts by academic ability interaction on near transfer post-test problems. Higher ability learners performed equally well with both formats
of prompts while lower ability learners performed significantly better with textual prompts.
TABLE III
ACCURACY OF PRACTICE PROBLEM SOLVING BY FORMAT AND PRESENTATION OF PROMPTS

TABLE IV
INSTRUCTIONAL TIME (IN MINUTES) BY FORMAT AND PRESENTATION OF PROMPTS. THE INSTRUCTIONAL TIMES ARE NOT STATISTICALLY SIGNIFICANTLY
DIFFERENT AND THUS DO NOT ACCOUNT FOR THE DIFFERENCES IN POST-TEST SCORES AND ACCURACY OF ANTICIPATION

the interaction between presentation and format of instructional
prompts.
There was a significant main effect for format of prompts
on the second trial of solving practice problems. In particular,
the participants who received pictorial-based prompts had a significantly higher probability of accurately solving the practice
problems on the second trial as compared to participants who
were assigned to the text-based prompts groups,
. Cohen’s statistic yields an
effect size of 0.56, which corresponds to a large effect. However, there was no significant difference between the textual and
pictorial format of prompts in the accuracy of the second anticipation given that the first anticipation was incorrect,
. Moreover, there was no significant main effect for the presentation of prompts or interaction
between the two factors.

D. Instructional Time
The overall average time spent on initial knowledge acquisition during the paper-based introductory training was
, and 24.69 min
in the
12.51 min
computer-based learning module across all participants. To test
if the amount of time participants spent on acquiring initial
knowledge during the paper-based training and learning in the
computer-based learning environment influenced their performance on the post-test, an analysis of variance (ANOVA) was
conducted on the instructional time. The analysis indicates that
the advantage of the textual format of instructional prompts can
not be attributed to the amount of instructional time. Table IV
presents the average instructional time in paper-based training
and in computer-based learning by format and presentation
of prompts. Analyses of the training time participants spent
studying the paper-based introductory training packet and

REISSLEIN et al.: INVESTIGATING THE PRESENTATION AND FORMAT OF INSTRUCTIONAL PROMPTS

537

TABLE V
PARTICIPANT CATEGORIZED ATTITUDE SCORES BY FORMAT AND PRESENTATION OF PROMPTS. THE LEARNER ATTITUDES TOWARD THE PROMPTS WERE
SIGNIFICANTLY MORE POSITIVE WITH EXTERNAL REGULATION. THE CONTINUOUS MOTIVATION WAS SIGNIFICANTLY HIGHER WITH TEXTUAL PROMPTS

the computer-based learning time revealed that there was no
significant main effect for format of prompts or presentation of
prompts (see Table IV).
E. Attitudes
The overall mean score across all the 14 attitudinal survey
, a favorable
items for all participants was
rating suggesting the participants generally agreed with the positive statements about the computer-based instructional module
and its components. The attitudinal items were grouped into
three categories, namely instructional effectiveness, role of instructional prompts, and continuous motivation. The mean attitude scores by format and presentation of prompts for participant responses on the three main categories of attitudinal items
on the five-point Likert-type attitudinal survey are presented in
Table V. An analysis of variance (ANOVA) of the attitudinal
category related to the role of instructional prompts revealed
that there was a significant main effect for the presentation of
. Cohen’s
prompts,
statistic for these data yields an effect size of
, which corresponds to medium to large effect. Specifically, the participants
who were assigned to the groups where the presentation of the
instructional prompts was externally regulated had significantly
than their
more positive attitudes
counterparts in the self-regulated groups
. In addition, a significant main effect was discovered on
the continuous motivation attitudinal survey items for the format
of prompts,
. Cohen’s
for these data yields an effect size of
, which corresponds
to medium to large effect. In particular, the participants who
were exposed to the textual format of prompts expressed significantly stronger interest
to continue
learning about the content area and engineering in general than
their counterparts in the groups with pictorial format of prompts
. The differences for the format and
presentation of prompts on the attitudinal survey items relating
to the instructional effectiveness were nonsignificant, indicating
that all examined formats and presentations of prompts were
perceived as equally effective.
IV. DISCUSSION
The two main research questions addressed in the present
study focused on the impact of the format (textual or pictorial) and presentation (self- or externally regulated) of instructional prompts on the learners’ performance and attitudes. Significant differences were revealed for the accuracy of anticipa-

tions on practice problems during the learning phase in the computer-based environment. In particular, the learners assigned to
the textual-based prompt groups were significantly more successful in correctly solving the individual solution steps at the
first problem-solving attempt they were required to solve than
their counterparts in the pictorial-based prompt groups. This
finding corresponds to a large effect and is, therefore, of practical significance. The learners who were assigned to the treatment conditions with pictorial prompts, on the other hand, had a
significantly higher success rate at the second anticipation compared to learners in the text-based prompt conditions. One way
to account for the higher success rate of the learners with the pictorial prompts in the second attempt is that these learners had
a significantly higher probability of advancing to the prompt
and second trial because of their higher error rates at the first
anticipation. In particular, for 18% of the solution steps the
learners in the pictorial-based prompt groups advanced to the
prompt and second trial, compared to 7% for the learners in
the text-based prompt groups. However, there were no significant differences in the conditional probability of correct second
anticipation given that the first anticipation was incorrect, indicating that both prompt formats are equally conducive to correct
anticipation at the second trial given that the learner is incorrect
at the first trial.
The textual prompt format led to significantly higher near
transfer post-test performance compared to the pictorial
prompt format. The advantage of textual prompts over pictorial
prompts on the near transfer learning yielded a large effect,
which indicates this is also of practical relevance. The study
revealed that the textual prompts were especially beneficial
to lower-ability learners. On the other hand, higher-ability
learners were able to perform equally well with both formats
of instructional prompts. The significantly better performance
of the lower-ability learners with the textual prompts indicates
that the textual representation of the electrical analysis techniques is more suitable for novice learners. The lower-ability
learners seemed to have difficulties relating the more abstract
pictorial representations to the problems at hand. In contrast,
the higher-ability learners were able to interpret the pictorial
depictions as effectively as the verbal descriptions. This finding
seems to indicate that higher-ability learners are capable of
comprehending the more expert-like illustrations better than
their lower-ability counterparts. The results suggest that all the
learners, regardless of the treatment condition, encountered
difficulties when attempting to solve the far transfer problems.
Essentially, a floor effect was encountered on this measure.
Therefore, one might consider testing the hypotheses with

538

learners who already possess some general engineering knowledge that would enable them to make the transition to the far
transfer learning.
The results from the attitudinal survey indicate that the
learners in the text-based group expressed significantly more
positive attitudes toward the statements relating to the continuous motivation. This more positive attitude is consistent
with the higher post-test scores of the learners in the text-based
prompt groups. Indeed, the higher post-test scores suggest that
these learners had acquired a better mastery of the instructional
material, were more confident about their newly acquired skills,
and had higher motivation for further study in the content
area of electrical circuits. This difference in attitudes toward
learning more about electrical circuits corresponds to a large
effect, indicating that this difference has practical relevance.
This practical relevance is especially significant in light of the
ongoing efforts to attract high school students to engineering in
general and electrical engineering in particular [8], [9]. While
the studied learning environment was effective in fostering an
interest in further study of electrical circuits in the high school
students, the version with text-based prompts was significantly
more effective in this regard. This finding suggests that instructional designers would be wise to consider using text-based
prompts in learning environments developed for a high school
audience.
The results for the learner attitudes toward the statements relating to the role of the instructional prompts indicate that the
external regulation of the prompts is perceived as significantly
more appealing than the self-regulation of the prompts. This result is interesting considering that both external and self-regulation of the prompts resulted in equal performance on the posttest and equal instructional time. Nevertheless, the learners in
the group with the external control of the prompts expressed significantly more positive attitudes toward the role of the prompts.
This difference, which had a medium to large effect, may be a
result of the low level of prior knowledge of the learners about
electrical circuit analysis. The learners with the low level of
prior knowledge may have appreciated the system automatically presenting them with the helping prompts instead of being
forced to decide for themselves whether or not they should view
the helping prompts. Overall, the results of the attitudinal survey
suggest employing text-based prompts and having the presentation of the prompts under the control of the instructional module
when designing an instructional module on electrical circuit
analysis for high school students without any prior exposure to
this knowledge domain.
Several interesting avenues may be pursued in future research
on computer-based interactive learning modules with instructional prompts. One avenue is to investigate the impact of text
versus pictorial prompts on learners with a higher level of prior
knowledge of general engineering analysis techniques, such as
engineering college freshmen or sophomore students. These students are accustomed to abstract graphical representations of
engineering problems and may, therefore, benefit more from the
pictorial prompts. Another avenue is to study the impact of more
complex and elaborate pictorial prompts that are designed to
foster the acquisition of the more expert-like graphical repre-

IEEE TRANSACTIONS ON EDUCATION, VOL. 48, NO. 3, AUGUST 2005

sentation common in electrical circuit analysis. Moreover, exploring the impact of animated pictorial prompts illustrating the
flow of the electrical particles in the circuits would be worthwhile. These animated pictorial prompts would help the learners
to visualize the circuit behavior, therefore facilitating performance on far transfer problems.
REFERENCES
[1] N. Al-Holou and M. Abdallah, “Teaching introduction to electric circuits using computer-based instruction for manufacturing engineering
students,” in Proc. IEEE Frontiers in Education Conf., Salt Lake City,
UT, Nov. 1996, pp. 23–26.
[2] F. de Coulon, E. Forte, and J. M. Rivera, “Kirchhoff: An educational
software for learning the basic principles and methodology in electrical
circuits modeling,” IEEE Trans. Educ., vol. 36, no. 1, pp. 19–22, Feb.
1993.
[3] E. D. Doering, “Circuit viz: A new method for visualizing the dynamic
behavior of electric circuits,” IEEE Trans. Educ., vol. 39, no. 3, pp.
297–303, Aug. 1996.
[4] H. E. Hanrahan and S. S. Caetano, “A knowledge-based aid for dc
circuit analysis,” IEEE Trans. Educ., vol. 32, no. 4, pp. 448–453,
Nov. 1989.
[5] J. R. Jones and D. A. Conner, “The development of interactive tutorials
for introductory circuits,” in Proc. IEEE 1st Int. Conf. Multi-Media Engineering Education, Melbourne, Australia, July 1994, pp. 108–114.
[6] B. Oakley II, “A virtual classroom approach to teaching circuit analysis,”
IEEE Trans. Educ., vol. 39, no. 3, pp. 287–296, Aug. 1996.
[7] A. Yoshikawa, M. Shintani, and Y. Ohba, “Intelligent tutoring system
for electric circuit exercising,” IEEE Trans. Educ., vol. 35, no. 3, pp.
222–225, Aug. 1992.
[8] G. C. Orsak, “Guest editorial K-12: Engineering’s new frontier,” IEEE
Trans. Educ., vol. 46, no. 2, pp. 209–210, May 2003.
[9] D. W. Callahan and L. B. Callahan, “Looking for engineering students? Go home,” IEEE Trans. Educ., vol. 47, no. 4, pp. 500–501,
Nov. 2004.
[10] R. K. Atkinson, S. J. Derry, A. Renkl, and D. W. Wortham, “Learning
from examples: Instructional principles from the worked examples research,” Rev. Educ. Res., vol. 70, no. 2, pp. 181–214, Summer 2000.
[11] A. Renkl, R. K. Atkinson, and C. S. Grosse, “How fading solution steps
work—A cognitive load perspective,” Instruct. Sci., vol. 32, no. 1/2, pp.
59–82, Jan./Mar. 2004.
[12] M. T. H. Chi, “Self-explaining expository texts: The dual process of
generating inferences and repairing mental models,” in Advances in Instructional Psychology: Educational Design and Cognitive Science, R.
Glaser, Ed. Mahwah, NJ, Erlbaum, 2000, pp. 161–238.
[13] A. Renkl, “Worked-out examples: Instructional explanations support
learning by self-explanations,” Learn. Instruct., vol. 12, no. 5, pp.
529–556, Oct. 2002.
[14] R. K. Atkinson, A. Renkl, and M. M. Merril, “Transitioning from
studying examples to solving problems: Effects of self-explanation
prompts and fading worked-out steps,” J. Educ. Psychol., vol. 95, no.
4, pp. 774–783, Dec. 2003.
[15] S. Y. Chen and R. D. Macredie, “Cognitive styles and hypermedia
navigation: Development of a learning model,” J. Amer. Soc. Inform.
Sci.Technol., vol. 53, no. 1, pp. 3–15, Jan. 2002.
[16] E. C. Shin, D. L. Schallert, and W. C. Savenye, “Effects of learner control, advisement, and prior knowlede on young students’ learning in a
hypertext environment,” Educ. Technol. Res. Dev., vol. 42, no. 1, pp.
33–46, First Quarter 1994.
[17] P. Chandler and J. Sweller, “Cognitive load theory and the format of
instruction,” Cogn. Instruct., vol. 8, no. 4, pp. 293–332, Fourth Quarter
1991.
[18] R. E. Mayer, Multimedia Learning. New York: Cambridge University
Press, 2001.
[19] I. Vekiri, “What is the value of graphical displays in learning?,” Educ.
Psychol. Rev., vol. 14, no. 3, pp. 261–312, Sep. 2002.
[20] J. Cohen, Statistical Power Analysis for the Behavioral Sciences, 2nd
ed. Hillsdale, NJ, Erlbaum, 1988.
[21] Director MX, Macromedia, Inc., San Francisco, CA, 2002.

REISSLEIN et al.: INVESTIGATING THE PRESENTATION AND FORMAT OF INSTRUCTIONAL PROMPTS

Jana Reisslein received the Master’s degree in psychology from Palacky University, Olomouc, Czech Republic, in 2000 and is currently pursuing the Ph.D.
degree in the Educational Technology Program, Division of Psychology in Education, Arizona State University, Tempe.
Her research interests are in the area of engineering education and multimedia
learning.

Robert K. Atkinson received the Ph.D. degree in cognitive science applied to
education from the University of Wisconsin-Madison in 1999.
He is an Assistant Professor in the Division of Psychology in Education
at Arizona State University, Tempe. His research explores the intersection of
cognitive science, instructional design, and educational technology, including
1) analogical reasoning, such as how learners use worked-out examples to
solve problems in semantically rich domains such as mathematics, engineering,
and physics; and 2) the design of instructional material, including software and
multimedia environments, based on principles of cognition.

539

Patrick Seeling (S’03) received the Dipl.-Ing. degree in industrial engineering
and management (specializing in electrical engineering) from the Technical
University of Berlin (TUB), Germany, in 2002. Since 2003, he has been
pursuing the Ph.D. degree in the Department of Electrical Engineering at
Arizona State University, Tempe.
His research interests are in the area of video communications in wired and
wireless networks and distance education.
Mr. Seeling is a Student Member of the ACM.

Martin Reisslein (A’96–S’97–M’98–SM’03) received the Dipl.-Ing. (FH) degree from the Fachhochschule Dieburg, Germany, in 1994 and the M.S.E. degree from the University of Pennsylvania, Philadelphia, in 1996, both in electrical engineering. He received the Ph.D. degree in systems engineering from
the University of Pennsylvania, University Park, in 1998.
Currently, he is an Associate Professor in the Department of Electrical
Engineering at Arizona State University, Tempe. During the academic year
1994–1995, he visited the University of Pennsylvania as a Fulbright Scholar.
From July 1998 through October 2000, he was a Scientist with the German
National Research Center for Information Technology (GMD FOKUS), Berlin
and Lecturer at the Technical University Berlin. He is Editor-in-Chief of the
IEEE Communications Surveys and Tutorials. He maintains an extensive
library of video traces for network performance evaluation, including frame
size traces of MPEG-4 and H.263 encoded video, at http://trace.eas.asu.edu
Dr. Reisslein is a Member of the ASEE.

