An Architecture-Based Approach for Synthesizing and Integrating Adapters for
Legacy Software*
Gerald C . Gannodi Sudhakiran V. Mudiam, and Timothy E. Lindquist
Department of Computer Science & Engineering
Arizona State University
Box 875406
Tempe, A 2 85287-5406
E-mail: {gannod,kiranmvs, tim}@asu. edu

Abstract

require migration of legacy software towards new environments that use modern technology. One technique that has
been suggested for facilitating the migration of existing
legacy assets to new platforms is via the use of the adapter
design pattern [Z], also known as component wrapping.

In sofmare organizations there is a very real possibility
that a commitment to existing assets will require migration of legacy software towards new environments that use
modern technology. One technique that has been suggested
for facilitating the migration of existing legacy assets to
new plaqorms is via the use of the adapter design pattern,
also known as component wrapping. In this papec we describe an approachf o r facilitating the integration of legacy
software into new applications using component wrapping.
That is, we demonstrate the use of a software architecture description language as a means f o r specifying various properties that can be used to assist in the construction of wrappers. In addition, we show how these wrapped
components can be used within a distributed object infrastructure as services that are d)v”cally integrated at runtime.

The ever increasing use of network computing has
made the ability to utilize components running in heterogeneous environments paramount. Technologies such as
CORBA [3] and DCOM [4] have made distributed computing realizable. However, the interaction between components in each of these schemes relies upon a centralized control broker. The h i Connection Technology [ 5 ] ,
in contrast, provides a mechanism whereby several components, or services, can be connected in a way that removes the need for centralized control, thereby expanding the ability to create federations of cooperating components.
In this paper, we describe an approach for facilitating
the integration of legacy software into new applications using component wrapping. That is, we demonstrate the use
of a software architecture description language as a means
for specifying various properties that can be used to assist
in the construction of wrappers. In addition, we show how
these wrapped components can be used within a distributed
object infrastructure as services that are dynamically integrated at run-time.

1 Introduction
A software product line is a collection of systems that
share a managed set of properties that are derived from
a common set of software assets [l]. A product line approach to software development is attractive to most organizations due to the focus on reuse of both intellectual effort and existing tangible artifacts. The systems or “derivatives” in a software product line (e.g., software based on
the product line) usually share a common architecture. For
a product line that leverages existing systems, an architecture may already be in place with organizational commitment to its continued use. In many cases, there is a very
real possibility that the commitment to existing assets will

The remainder of this paper is organized as follows.
Section 2 describes background material in the areas of
reengineering, software architecture, and distributed components. Our proposed approach for facilitating legacy
software wrapping as well as the mechanism for integrating the wrapped components into client applications is presented in Section 3. Section 4 discusses the details of an
automated support tool that generates wrappers for legacy
code, and Section 5 presents an example that demonstrates
the approach. Related work is described in Section 6, and

‘This research supported in part by NASA Langley Research Grant
NAG 1-224 I .
t Contact author.

1095-1350/00 $10.00 0 2000 IEEE

128

Section 7 draws conclusions and suggests further investigations.

and by the constraints imposed on the ports of the component, where component types are intended to capture architectural properties. Ports are the interaction points through
which a component exchanges resources with its environment. Port specifications specify the signatures, and optionally, the behaviors of the resource.
Connectors encapsulate the ways that components interact. A connector is specified by the type of the connector,
the roles defined by the connector type, and the constraints
imposed on the roles of the connector. A connector defines
a set of roles for the participants of the interaction specified
by the connector. Connector types are intended to capture
recurring component interaction styles.
Components are connected by configuring their ports to
the roles of connectors. Each role has a domain that defines
a set of port types and only the ports whose types are in the
domain can be configured to the role.
Another important concept in the area of software architectures is the concept of an architectural style. An architectural style defines patterns and semantic constraints on
a configuration of components and connectors. As such, a
style can define a set or family of systems that share common architectural semantics [8]. For instance, a pipe and
filter style refers to a pipelined set of components whereas
a layered style refers to a set of components that communicate via hierarchies of interfaces.

2 Background
This section describes background material in the areas
of reengineering, software architecture, and ComponentBased Software Engineering.

2.1 Reengineering via Adapters
Reverse engineering is defined as the analysis of software components and their interrelationships in order to
obtain a description of the software at a high level of abstraction [6]. This term is contrasted with reengineering,
which is the process of examination, understanding, and
alteration of a system with the intent of implementing the
system in a new form [6]. Since the functionality of the
existing software has been achieved over a period of time,
it must be preserved for many reasons, including providing
continuity to current users of the software.
One approach to reengineering is to use the adapterpattern [ 2 ] whereby a legacy interface is converted into a form
that a client application can utilize. As such, the adapter
pattern allows components that otherwise could not work
together because of incompatible interfaces to be combined
to form a new software system. In this paper, we describe
an adapter approach for automating the reengineering of
legacy command-line software. Specifically, in terms of
the Gamma et al. adapter pattern, we use the concept of the
object adapter in the manner shown in Figure 1.
Client

-

Target
Request0

Adapter
Request0

2.3

Component-Based Software Engineering (CBSE) is a
form of software development that is based on the construction of applications by integrating existing software
components. CBSE is much more than use of object request brokers, reuse of a library of code, or modular development. It can involve activities such as building, acquiring, assembling, and evolving systems. Emerging concerns
include the use of multiple suppliers of components that
provide the same functionality, along with multiple versions and configurations of the components.
Currently, CBSE addresses issues and technologies
such as Commercial-Off-The-Shelf (COTS) components,
in-built components, and application frameworks. The
emerging technologies for component integration include
Enterprise Java Beans [9], CORBA [3], Jini [ 5 ] ,Microsoft
DNA [lo], and IBM San Francisco [ 1I]. All of these technologies provide a component model where a pre-defined
infrastructure acts as "plumbing" that facilitates communication between components. Tools and environments supporting all these technologies are being widely used in the
industry and continue to provide many benefits.

Adaptee
Specific-Request0

-

---___

<-r-

Component-Based Software Engineering

adaptee-Specific-Request0

Figure 1. Object Adapter [2]

2.2 Software Architecture
A software architecture describes the overall organization of a software system in terms of its constituent elements, including computational units and their interrelationships [7]. In general, an architecture is defined as a
conjiguratiori of components and connectors. A component is an encapsulation of a computational unit and has an
interface that specifies the capabilities that the component
can provide. In addition, the interface specifies the ways
that the component delivers its capabilities. The interface
of a component is specified by the the type of the component, by one or more ports supported by the component,

3 Approach
In this paper we discuss two aspects of a software
migration strategy. First, we introduce an architecture-

129

information is needed in order to automatically construct
the adapter.
As an initial investigation into the automated synthesis of the adapters, we selected command-line applications
as our source of reusable legacy software. Selection of
this class of legacy applications addresses the modification
concern of requirement RI since source code is not available. As such, we are required to provide an interface that
is based solely on the knowledge of how the application is
used rather than how it works. In addition, the selection of
this class of applications has the consequence of enforcing
the use of a particular architectural style, as determined by
the nature of the legacy application. In this context, we
defined several properties that would be needed to appropriately specify the behavior provided by a command-line
application including: Signature, Command, Pre, Post, and
Path. We identified these properties by examining the type
of behaviors, inputs, and outputs generally associated with
command-line applications.
The Command property identifies the command used to
invoke the application while the Pre and Post properties
identify commands that are contained within the adapter
code that will establish preconditions and postconditions
on the execution of the legacy component, respectively.
Path indicates the path to the given command-line application and the Signature property defines the types and names
of the expected input and output of that application. Here,
since we are dealing with command-line applications, the
input types are expected to be strings. Accordingly, a certain degree of semantic information must be used in the
name of the input. Fortunately, for command-line applications, these names are typically limited to filenames and
command-line options.
The requirement R2 (i.e., the decoupling of a specification from a target implementation language) is based on the
desire to apply the synthesis approach to a variety of target
languages and implementations. In addition, this requirement facilitates enforcement of requirement R 1 by ensuring that new source code is not artificially embedded in the
specification. While satisfying this requirement is ideal,
we found in our strategy that a certain amount of implementation dependence was necessary due to the integration
strategy that we chose (see Section 3.2).
When a component has been wrapped using our technique, an interface is defined that facilitates the use of the
source legacy software as part of a new application. However, as indicated by requirement R3, it is also desirable to
be able to use the specification of the adapted component
within a more general architectural context. That is, it is
advantageous to be able to use the specification as part of
the software architecture specification for new systems. In
using a content-rich specification, where interfaces are defined explicitly, the added benefit of providing information

based approach for specifying and subsequently synthesizing wrappers for a certain class of legacy software. Second,
we discuss the issue of integrating the wrapped components with client applications by utilizing the service mediation capabilities of Jini [SI.

3.1 Specification and Synthesis
The concept of using an adapter for wrapping legacy
software is not a new one [2, 12, 13, 141. As a migration strategy, component wrapping has many benefits in
terms of reengineering including a reduction in the amount
of new code that must be created and a reduction in the
amount of existing code that must be rewritten. One of our
primary goals in this research is to develop an environment
whereby a potentially large number of existing legacy software components can be adapted for use within a software
development and integration framework. In particular, unlike CBSE strategies that assume the existence of reusable
components, we are interested in developing an approach
that facilitates the automated wrapping of existing legacy
software into forms that can be easily integrated into a target application.
In regards to wrapping components, our approach uses
two steps. First, a specification of the legacy software as
an architectural component is created. These specifications
provide vital information that is required to define the interface to the legacy software. Second, the appropriate
adapter source code is synthesized based on the specification.
3.1.1

Specification Requirements

To aid in the development of an appropriate scheme for
the wrapping activity, we defined the following requirements upon specifications of the interface to legacy software.
( R l ) A sufficient amount of information should be captured
in the interface specification in order to minimize the
amount of source code that must be manually constructed.

(R2) A specification of the interface of the adapted component should be as loosely coupled as possible from
the target implementation language.
(R3) The specification of the adapted component should be
usable within a more general architectural context.
The requirement RI addresses the fact that we are interested in gaining a benefit from reusing legacy software.
As a consequence, we must avoid modifying the source
code of the legacy software. At the same time, we must
provide an interface that is sufficient for use by a target application. To provide that interface, a sufficient amount of

130

constraint, we make the assumption that any client applications utilizing the wrapped components have a certain amount of knowledge regarding the interface of that
wrapped component. We find this assumption to be reasonable due to the nature of legacy software migration
where legacy applications have an organizational history
with well-known usage profiles.

that can be integrated into an architectural specification of
a target application is gained.
In order to realize the requirements placed upon desired
interface specifications for legacy software wrappers, we
used the ACME [ 151 Architecture Description Language
(ADL). Specifically, we used the properties section of the
ACME ADL to specify the interface features described
earlier (e.g, Signature, Command, Pre, Post, and Path).
ACME is an ADL that has been used for high-level architectural specification and interchange [ 151. ACME contains constructs for embedding specifications written in a
wide variety of existing ADLs, making it extensible to both
existing and future specification languages. ACME is supported by an architectural specification tool, ACMEStudio [ 161, that facilitates graphical construction and manipulation of software architectures.
Figure 2 shows a screen capture of an ACMEStudio session in which a component has been specified as an aggregation of two wrapped applications expressed as ports. In
the figure, the component labeled RCS is the aggregate
component and consists of two ports, Chkln and ChkOut.
These ports are used as wrappers for the Revision Control
System (RCS) programs c i and C O ,respectively. The bottom right portion of the figure contains a list of the properties used to derive the wrapper along with their corresponding values. The amount of knowledge that is required in order to write the wrapper specification is limited to a working knowledge of how a legacy application is used. Often
this includes knowledge of the command-line parameters
as well as other bits of information that can be retrieved
from manual pages (if they exist) and current users.
Several benefits arise from the use of the ACME ADL to
specify the legacy application wrappers. First, if the specification of an adapted component is realized as a port, then
several such components can be aggregated into a single
component. As a consequence, the aggregated component
can offer each of the behaviors as a service through a port
interface. For example, as demonstrated in Section 5, the
RCS suite of applications can be aggregated into a single component that offers services such as check-in and
check-out via ports of the aggregate component. Second,
via the use of ACMEStudio, the specification of an adapted
component can be integrated into the specification of target
applications. Consequently, software architecture analysis
techniques that support the use of ACME can be applied
to target applications that utilize the wrapped legacy software. Finally, since library support for ACME consists of
a set of standard parsers and other manipulation facilities,
construction of support tools is convenient (see Section 4).

We chose the Java programming language and environment as our target migration platform for a number of reasons. First, Java continues to enjoy increases in popularity,
and thus any opportunity to integrate legacy systems into
Java applications has obvious benefits. Second, the objectoriented nature of Java facilitates straightforward construction of components consisting of an aggregation, or federation, of wrapped legacy components. Finally, and most importantly, we found that the services provided by the Jini
Connection Technology [ 5 ] for smart devices could also
be applied to software components. As a result, software
can be packaged as services on a Jini network and integrated dynamically into distributed applications. Accordingly, our approach for synthesizing wrappers for legacy
components is based on implementing the standard discover and join protocol that is required for Jini devices.
In our approach, the information that is needed to generate wrappers corresponds exactly to the properties associated with the ports shown in Figure 3 and previously
described in Section 3.1, with some minor modifications.
In addition to the Signature, Command (shown as Cmd in
Figure 3), Pre, Post, and Path properties, we added the I n t e r f a c e , and Return fields. These fields define the Jini
service name of a port and the value returned after interaction with the port, respectively. In the synthesis process,
ACME specifications are combined with a standard template that implements the setup routines that are required
to register a component on a Jini network. In addition to
synthesizing the appropriate wrapper, the support tool that
we have constructed to automate this process generates the
appropriate source code for facilitating interaction between
a potential client and the wrapped component. At present,
this is an automated tool that generates fully executable
code for the wrapped application and does not require the
user to modify or write any new code.
While the investigations that are described here are limited to our efforts to adapt command-line applications for
use within a Jini-based software integration environment,
we are pursuing investigations into broadening the context
of this approach to other legacy software components including CUI-based applications. In addition, due to the
ability of Java and consequently Jini to run anywhere, we
are investigating approaches for automatically wrapping
applications that exist within heterogeneous operating system environments.

3.1.2 Synthesis
As stated earlier, the class of legacy systems that we
are considering are command-line applications. Given this

131

I

Figure 2. ACMEStudio Session

3.2 Integration

tion technique is that it facilitates construction of applications “on-the-fly’’ whereby components can be used on an
as-needed basis. Another advantage arises from the fact
that services must implement a fairly general set of routines in order to participate in a Jini network. As a result,
the synthesis of wrappers is fairly straightforward. One
of the disadvantages of this approach stems from the fact
that the interface to the services must be well-defined or
must be negotiated between the client and the device upon
connection. That is, the client must have some knowledge
about how to use the service. As stated earlier, this assumption is mitigated by the fact that the class of software
applications that we are wrapping typically have a history
within an organization.

The primary enabling feature of the work described in
this paper is the existence of the Jini technology for the
delivery and management of services. In a typical Jini network, services are provided by devices that are connected
to the network. Typically these devices consist of a variety of products ranging from cell phones, desktop devices, printers, fax machines, and Personal Digital Assistants (PDAs).
Figure 4 shows the layered architecture of the Jini
Connection Technology where the Jini Technology layer
provides distributed system services for activities such
as discovery, lookup, remote event management, transaction management, service registration, and service leasing.
When a “device” is plugged into a Jini network, it becomes
registered as a member (e.g., service) of the network by the
Jini lookup service. Once registered, other network members can discover the availability of the device through the
lookup service. When a client application finds an appropriate device, the lookup service facilitates the connection
but then is no longer involved in subsequent interactions
between the client and device. In our approach to component integration, we use the Jini technology to provide
a standard method for registering and connecting a client
to corresponding software components that are acting as
services.
One of the advantages of using this Jini-based integra-

Once the connection between a client application and a
service has been established, the interaction between the
components will appear to be similar to the architecture
shown in Figure 5. In this figure, the Service component
provides two services to the ClientApplication component
via the interface provided by the Adapter connector. While
the diagram in Figure 5 shows only one service being utilized by the client application, it is possible for several services to be connected to the client. In addition, the interaction between the services is not limited to a call-return
style where services return values to the client but can also
include interaction between services using a wide variety
of architectural styles.

132

Component RCS =
Properties {

{

1;
Port ChkIn = (
Properties {
Signature : string =
"String filename, \
String params, \
String f iledata " ;
Return : string = "Boolean result";
Cmd : string = "ci + filename + params";
Pre : string =
"WriteFileData(fi1edata. filename)";
Post : string = " " ;
Interface : string = "VersionManagement";
Path : string = "C:\\RKTOOLS\\BIN\\CI.EXE " ;
1;

1;
Port ChkOut = {
Properties {
Signature : string =
"String filename, String params";
Pre : string =
Return : string = "String filedata";
Cmd : string = " C O + filename + params";
Interface : string = "VersionManagement";
Post : string =
"filedata = ReadFileData( filename ) " ;
Path : string = "C:\\RKTOOLS\\BIN\\CO.EXE" ;

Figure 5. Conceptual Architecture
face Generator component uses the set of ports to generate the interface or connector to the service. The Function
Generator component uses the same port information to
generate functions that implement the service. The Service
Generator component uses these functions along with the
ServiceTemplate to generate the final Java source code for
the service.

"'I;

);

1;
1;

Figure 3. ACME Specification of RCS Services as ports

Java
Technology

Java
Technology
Interface
Generator

Function
Generator

Figure 4. Jini Architecture
Template

4 Implementation

(T)
[F]
Interface

To support our technique for constructing wrappers for
legacy software, we have created a Java support tool called
ServiceTool. Figure 6 shows the detailed architecture of
ServiceTool which takes an ACME specification and produces a wrapper configured for a Jini network. In the diagram, the rectangles with the square corners represent software components while the rectangles with the rounded
corners represent files. The ArchParser component reads
in an ACME specification similar to the one shown in
Figure 3 and builds an internal model of the architecture.
The Component Inspector component uses the output of
the ArchParser to access the interface specification of the
wrapper component and produces a set of ports. The Inter-

Figure 6. Service Tool Architecture
The ArchParser uses the ACMEParser from the
ACMELib toolkit [ 171 to parse ACME specifications.
ACMELib is a library that facilitates the construction of
architectural tools in Java that read, write and manipulate software architectures specified in the ACME ADL.
The ACMELib framework is designed to support the rapid
development of two classes of applications (1) tools that

133

translate between "native" ADLs (such as Rapide [ 181 and
Wright [ 191) and (2) native ACME-based architectural design and analysis tools.
The Service Generator component is implemented as
an awk script that replaces tags in the ServiceTemplute file
with functions generated by the Function Generator component and the names of services.
Figure 7 contains a portion of the ServiceTenzplate file
which contains all of the application and service independent source code and provides the routines necessary to
integrate the legacy code into a Jini network. Specifically, the ServiceTemplate contains functions that implement the discover and join protocol for registering a service
with the lookup service. The ServiceTemplate also contains tags that are place-holders for the automatically generated functions. For instance, in Figure 7 the tag <putServerName> is a place-holder for the final name of the
adapter component.

void WriteFileData(Strin9 filedata, String filename)
/ / the generic WriteFileData..

(

try(
File f = new File(fi1ename);
System.out.println("IsFile0

:" +

f.isFile0);

Printwriter out =
new Printwriter( new FileOutputStream(f), true);
out.print(filedata);
out.close ( ) ;

I
catch (Exception e)
(

System.out.println
("Server: Error writing file:

"

+ e);

)

I
String ReadFileData(Strin9 filename) (
try(
BufferedReader in =
new BufferedReader(new FileReader(fi1ename));
String s;
StringBuffer sb = new StringBuffer 0 ;

public class <put-ServerName>
extends UnicastRemoteObject
implements
<put-InterfaceName>,
ServiceIDListener,
Serializable

while( ( s = i n . r e a d L i n e O ) ! = null

)

(

sb. append ( s ) ;
sb.append ( " \ n ")

;

)

(

in.close ( ;
/ / returning the whole file as a string..
return (sb.toString0 ) ;

public <put-ServerName> 0 throws RemoteException
(

super

1
catch (Exception e)

() ;

(

System.out.println
("Server: Error writing file: " + e);
return("Server: Error writing file: " + e ) ;

<put-Functions>

1

Figure 8. Sample Library Routines

Figure 7. Excerpt of the ServiceTemplate

5.1 Specification

In addition to the ServiceTemplate, there is also a
reusable set of functions that can be utilized in an interface specification and consequently in the generated wrappers. For instance, the ReadFileData ( ) and WriteFileData ( ) routines (shown in Figure s), are available
as functions for use within the Java code to provide standard read-from and write-to file support, respectively.

5

In order to provide version management services, we
constructed an adapter specification using the ACMEStudio tool, as shown in Figure 2. The final result of
the ACMEStudio session yielded the ACME specification
shown in Figure 3, where the wrapped components are
specified as ports that offer interfaces to the c i (check
in) and C O (check out) RCS programs. The properties for
ChkIn state that the port has an input signature consisting of three Strings: filename, params, and f i l e data. This signature corresponds to the formal parameter
list that will be used as a call to a ChkIn method. Similarly, the Return property within the ACME specification
states that for the ChkIn port, the returned output will be
a Boolean variable called result.
One of the important properties contained within the
ACME specification is the Interface property which
facilitates the registration of the wrapped component as a

Example

To demonstrate the use of the component wrapping and
integration technique described in this paper, we developed
a simple editing application. The intent of this application
is to support text file editing with version control. For the
version control portion of the application, a version management service was utilized while the remainder of the
application consists of source code written specifically for
the editor.

134

service on a Jini network. In this case, the interface is specified as VersionManagement.
Two other fields of interest are the Cmd and Path
which specify how the adapter interface will invoke the
wrapped component and the path to the actual commandline application, respectively.
The last part of the ChkIn port specification is the
Pre field, which identifies the command that is needed to
setup the interaction between the client and service component. In this example, the Pre field states that the routine WriteFileData should be invoked with parameters
filedata and filename. This routine will write to
the file named filename the data contained in the String
f iledata.
A similar specification of the ChkOut port is also given
in Figure 3 and differs primarily in the post-processing
property. Once the ACME specification is completed, the
ServiceTool application can be used to generate a pair of
files named RCSServer . java and VersionManagement . java. These files correspond to the component
that implements the wrapping of the legacy software and
the connector that provides an interface to the wrapper.

Figure 9. Editor and RCSServer interaction
interface required to edit files. It also contains an editing panel that allows the user of the Editor to access the
CheckIn and Checkout Functions available from the RCSServer component. Figure 11 shows the resulting editor
applet.

5.2 Client Application
Client applications in the Jini framework are implemented as Jini services. In the case of our example, the
Editor component requires the use of a Version Management Service. In the source code for the Editor component, the appropriate functions must be written to implement both the graphical user interface for the application
as well as the protocols for discovering and joining a Jini
network. Client applications can also provide services to
others components, if they have any to offer. In our example, however, the Editor does not provide any other services and just makes use of a Version Management service.

5.3 Integration and Execution
In our example scenario, the Editor component joins the
Jini network by discovering the lookup service and registering with lookup. As shown in Figure 9, the Editor uses
the lookup service to find a version management service
that is registered on the Jini network. Plausible version
management services that could be registered include RCS,
SCCS and CVS components.
Once the Editor finds that an RCSServer service (or
other version management service) is available, it requests
that the lookup service return the proxy object that was
registered by the RCSServer. The proxy object acts as a
connector to the VersionManagement service, e.g., an interface that knows how to interact with the RCSServer and
acts as an intermediary between the client application and
RCSServer components. At this point, as shown in Figure 10, the RCSServerApplet becomes an integral part of
the Editor. In this example, the Editor provides the user

Figure 10. Editor and RCSServer Interaction
The power of Jini comes from the fact that proxy objects
contain code that become an integral part of a client application. Another key to Jini Technology is also its spontaneous networking that allows clients and services to become aware of one another and integrate seamlessly.
Getting the Editor to talk to RCSServer through the RCSServer proxy adapter requires that a number of Jini services exist (e.g., are running) prior to integration. First, the
Jini network must be up and running along with an associated lookup service. Since Jini is built on top of the Java
RMI, an RMI daemon must to be running prior to execut-

135

programs in order to integrate them into more current systems via a CORBA interface.
Cimitile et al. [ 131 describe an approach to wrapping
that involves the use of data flow analysis in order to discover various properties of source code. The approach focuses on the static analysis of code in order to determine
appropriate decompositions of different program components as well as discovery of the formal parameters for the
interfaces to these programs. At this time our approach
does not involve analysis of source code but rather takes the
perspective of a continuous engineering effort where reuse
of existing host applications with minimal code rewriting
is desired [20].
Jacobsen and Kramer [ 141 describe an approach for synthesizing wrappers based on the specification of a modified
CORBA IDL description. In their approach, they address
the problem of object synchronization within the context of
the CORBA standard and define a technique based on the
application of the adapter design pattern. In many ways,
Jacobsen and Kramer's approach is similar to the one described in this paper. We are currently interested in software at a higher level of granularity than the ones often
provided via ORB-based interfaces, thus our approach has
some potential for full automation. However, as we increase our scope to include a more general class of component, we must address similar concerns.
Sullivan et al. look at systematic reuse of largescale software components via static component integration [21]. That is, they use an OLE-based approach for
component integration. To demonstrate the use of their
scheme, they developed a safety analysis tool that integrates application components such as Visio and Word. In
our approach we use a dynamic approach for component
integration and thus, can utilize a wide variety of components whose interfaces are discovered at run-time.
CyberDesk [22]is a component-based framework written in Java that supports automatic integration of desktop
and network services. This framework is flexible and can
be easily customized and extended. The components in this
framework treat all data uniformly regardless of whether
the data came from a locally running service or the World
Wide Web. The goal of this framework is to provide ubiquitous access to services. This approach is similar to our
proposed approach in that they use a dynamic mapping facility to support run-time discovery of interfaces.

Figure 11. Editor Session
ing the Jini Lookup service. Finally, a web server must
be running in order to enable code delivery across the Jini
network.
The following is a summary of the order of execution
for the aforementioned services along with the example application components:
1. Web server (httpd)

2. An RMI daemon (rmid)
3. The Jini Lookup service (reggie)

4. RCSServer
5 . Editor

The Editor can also be up and running on the network
before any of the Version Management services become
available. In that case, the Editor can request that the
Lookup service notify it once any service that matches its
required specification becomes available. Once a matching service registers with the Lookup service, the Editor is
notified and subsequently connects with the service.

6 Related Work
Several approaches for component wrapping have been
suggested in the reengineering literature. From the practical experience perspective, Sneed [ 121 described efforts
to encapsulate legacy systems at several different levels.
These levels, corresponding to a COBOL environment, include encapsulation at the job, transaction,program, module, and procedure level. In addition, Sneed demonstrated
the use of wrapping techniques to wrap running assembler

7 Conclusions and Future Investigations
In the context of software product lines, where the reuse
of existing assets is desired, reengineering plays an important role in the realization of the recovery of those assets
from previously existing products. The ability to quickly
adapt those existing assets for use in new products becomes
paramount, especially in an industry dominated by time-to-

136

market issues.
One of the primary issues in reuse or component-based
software engineering techniques is the notion of architectural mismatches between integrated components. The approach described in this paper makes the assumption that
interactions between clients and services registered on the
Jini network interact in a well-defined manner. Currently,
we are investigating techniques for embedding high-level
interaction semantics (e.g., architectural style) within the
ACME specifications of both wrapped components and target client application code.
Our future investigations will focus upon the development of a framework for supporting the construction and
integration of software, where the reusable components
are utilized as services in a dynamic integration environment. In addition, we are investigating techniques for the
construction of the interface ACME specifications via reverse engineering whereby the properties needed to specify a component wrapper are automatically derived from
source code.

[13] A Cimitile, U DeCarlini, and A DeLucia. Incremental migration strategies: Data flow analysis for wrapping. In
Working Conference on Reverse Engineering, pages 5968, Hawaii, USA, Oct 1998. IEEE Computer Society, IEEE
Computer Society Press.
[I41 H.-Arno Jacobsen and Bernd J. Kramer. A design pattern based approach to generating synchronization adaptors
from annotated idl. In Proceedings of the Automated Software Engineering Conference, pages 63-72, Hawaii, USA,
Oct 1998. IEEE Computer Society, IEEE Computer Society
Press.
[15] David Garlan, Robert T. Monroe, and David Wile. Acme:
An Architecture Description Interchange Language. In Proceedings of CASCON’97, pages 169-1 83, Toronto, Ontario,
November 1997.
[I61 Acmestudio: A graphical design environment for acme.
http://www.cs.cmu.edu/-acme/
Acmestudio/ AcmeStudio.htm1.
[17] The acme tool developer’s library (acmelib).
http://www.cs.cmu.edu/-acme/acme~downloads.
html.
[18] D. Luckham and J. Vera. An Event-Based Architecture Definition Language. IEEE Transactions on Software Engineering, 21(9):717-734, 1995.

References

[19] Robert Allen and David Garlan. A Formal Basis for Architectural Connection. ACM Transactions on Sofrware Engineering and Methodology, July 1997.

Len Bass, Paul Clements, and Rick Kazman. Software Architecture in Practice. Addison Wesley, 1998.
Erich Gamma, Richard Helm, Ralph Johnson, and John
Vlissides. Design Patterns: Elements of Reusable ObjectOriented Sofmare. Addison Wesley Longman, 1995.
Object Manangement Group. CORBA: Architecture and
Spec$cation V2.0, forma1/97-02-25 edition, July 1996.
G. Eddon and H. Eddon. Inside Distributed COM. Microsoft Press, 1998.
W. Keith Richards. Core Jini. Prentice-Hall, 1999.
Elliot J. Chikofsky and James H. Cross. Reverse Engineering and Design Recovery: A Taxonomy. IEEE Sofmare,
7(1):13-17, January 1990.
M. Shaw and D. Garlan. Software Architectures: Perspectives on an Emerging Discipline. Prentice Hall, 1996.
N. Medvidovic and R. N. Taylor. Exploiting architectural
style to develop a family of applications. IEE Proc. in Sojiware Engineering, 144(5-6):237-248, Oct-Dec 1997.
Anne Thomas. Enterprisejava beans technology. Technical
report, Patricia Seybold Group, 1998.
Mary Kirtland. Designing Component-Based Applications:
Build Enterprise Solutions with Microsoji Windows DNA.
Microsoft Press, 1999.
Paul Monday, James Carey, and Mary Dangler. SanFrancisco Component Framework: An Introduction. AddisonWesley, 1999.
Harry M. Sneed. Encapsulating legacy software for use in
clientkerver systems. In Working Conference on Reverse
Engineering, pages 104-1 19, Monterey, USA, Oct 1996.
IEEE Computer Society, IEEE Computer Society Press.

[20] Kostas Kontogiannis. Distributed objects and software application wrappers: A vehicle for software re-engineering.
In Working Conference on Reverse Engineering, page 254,
Hawaii, USA, Oct 1998. IEEE Computer Society, IEEE
Computer Society Press.
[21] K. J. Sullivan, J. Cockrell, S. Zhang, and D. Coppit. Package oriented programming of engineering tools. In Proceedings of the International Conference on Software Engineering, pages 616-617, 1997.
[22] Anind K. Dey, Gregory Abowd, Mike Pinkerton, and Andrew Wood. Cyberdesk: A framework for providing selfintegrating ubiquitous software services. Technical Report
GIT-GVU-97-10, Georgia Institute of Technology, 1997.

137

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

FRAMEWORKS FOR SECURING LIMITED-DEVICE APPLICATIONS
Timothy Lindquist
Aarthi Ramamurthy
Ramon Anguamea
Timothy.Lindquist@asu.edu Aarthi.Ramamurthy@asu.edu Ramon.Anguamea@asu.edu
Division of Computing Studies
Arizona State University
Abstract

In this paper, we compare the features
available for developing secure distributed
applications for limited devices, such as smart
phones. We limit our scope to examine
frameworks for Java. This work is part of a
continuing project which is considering
capabilities and performance for application
development on these platforms. The paper
considers performance as it relates to various
approaches to securing applications.
The paper addresses two separate concerns.
First is protecting access to resources by an
executing application. The facilities for defining,
limiting and controlling applications during
their development, installation and execution are
described. Second, we discuss approaches
available for securing communication among
application components running on servers or
limited devices.

1. Introduction
In this paper we consider limited devices
that are connected to the Internet and other
communication media, for example, handheld
devices such as intelligent cell phones and PDAs
with Internet connections or platforms which
combine these functionalities. These devices
have limited memory, limited processing power,
no hard disk storage, small display screens, and
limited human input capability. We consider
only those having communication facilities
(WiFi or EV-DO)..
The operating environments for these
devices are comprised of three base components:
local operating system, network operating
system and language runtime environment. The
leading operating systems for these devices are
Symbian, Palm and Windows Mobile 6.
Connected limited-device configuration and
mobile
information
device
profile
(CLDC1.1/MIDP2) are the Java frameworks
designed for resource constrained devices, such
as phones and PDA’s. CLDC1.1/MIDP2 as

realized by the IBM J9 runtime and SUN
Wireless toolkit pre-defined classes, is the
security environment we evaluated for this paper.
Various development environments are available
depending upon platform and language. For
Microsoft Windows Mobile 6 the application
development environment for the .NET
languages, such as C#, is the .NET Framework
together with Visual Studio 2005 with the
Compact Framework 2.0. Several alternatives are
available for configurations utilizing Java, in part
depending on the Java runtime environment
being used. Sun’s CLDC HotSpot and IBM’s J9
are two popular Java runtime environment
choices. Add-on packages and various
configurations are available to support different
security approaches, device capabilities and
networking needs.

2. Background
The connectivity of computing devices to
the Internet, has enabled malicious attacks. The
motivation for attacks varies from willful
espionage to experimentation. Equally important
to protection from attack is the ability to prevent
harm from mistakes in coding, configuration or
user operations. Protection and detection are
difficult in handheld devices because of limited
capability.
Trust
is
confidence
in
expected
functionality. When running an application the
user must trust that it produces valid information
and that privacy, integrity, or confidentiality will
not be compromised. There are several security
policies, protocols and mechanisms that are of
particular interest to the limited devices.
Languages such as Java, C#, and other scripting
languages are widely used in distributed
applications and provide varying degrees
security support. Java and C# both permit
examination of compiled intermediate code for
unsafe actions. Both the Java CLDC/MIDP and
Mobile 6 execution environments support an
array of cryptographic functions that can be used

1530-1605/08 $25.00 © 2008 IEEE

1

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

by communication protocols and by the system
elements to aid in access control at the device
and application level.

3. Java CLDC/MIDP
Three different limited device configurations
exist for the Java 2 Micro Edition (J2ME). For
more capable devices such as set-top boxes and
high-end wireless devices the Connected Device
Configuration (CDC) defines an API whose
functionality is close to J2SE, but is reduced as
appropriate for the limited hardware and
applications. At the lowest level of functionality
is the JavaCard API (for Smart-cards/Sim-cards).
JavaCard as can include functionality for
asynchronous security operations, such as
encryption, decryption, digital signature,
verification and others for limited devices whose
computing capacity is unable to perform such
operations without disrupting user-functionality.
The Connected Limited Device Configuration
(CLDC) is defined for PDA and wireless phone
devices. A device such as a PDA or smart-phone
running Java applications would include a virtual
software stack with the following components:
• Mobile Information Device Profile
(MIDP2) that supports the application
life-time model, persistent storage,
network resources and the user-interface.
• CLDC1.1 that supports the core Java
language, IO and networking classes,
security features and internationalization
facilities.
• The selected Java runtime environment
• The device operating system and related
services

3.1 Application Security Model
The J2SE model for securing the operations in
an executing virtual machine changed
dramatically as Java evolved. Java originally,
used the sandbox model for application security.
Initial versions of Java provided full trust to
classes loaded locally and prohibited all sensitive
operations from any code obtained dynamically.
Java1.2 introduced support for a continuum of
access control. Access to system resources (such
as files, sockets, runtime, properties, security
permissions, serializable, reflection, and window
toolkit) is granted based on domains. A domain
is defined to include:
• a set of permissions (resources),

•
•
•

a set of operations associated with
each permission,
codebase indicating the code origin,
a digital signature of the code which
allows identification of the signer
and verification that the code has not
been modified.

The codebase indicates the file or URL from
which the code is loaded. If signed, the alias of
the public key can also be used to define a
domain. Each class loaded into a Java virtual
machine has an associated protection domain,
which defines the access it has to resources.
When execution encounters an operation that
requires a system resource, all classes
representing the currently executing methods
(contents of the runtime stack) are checked to
assure all have access to the resource. The Figure
below is taken from the On-line Java Tutorial
and shows how an execution can include a range
of protection domains ranging from no access to
resources to full access.

Figure 1.

Controlling Access to Java Resources

In J2SE (Java2), security domains are defined
by a policy file granting permissions to the
domain. For example, suppose the company
GrowthStocksExpress publishes an applet on
their (hypothetical) web site at the URL:
http://GSE.com/applets
Assuming the applet needs connections to one
or more hosts having a domain address ending
with GSE.com on ports beginning at 2575, a
policy for clients who access the applet may be:
grant signedBy "GrowthStockExpress", codeBase
"http://GSE.com/applets"
{
permission java.net.SocketPermission

2

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

"*.GSE.com:2575-", "accept, connect, listen, resolve";
};

A policy may consist of one or more grants each
defining different domains. Each domain may
have one or more associated permissions.
CLDC/MIDP2 security. The application
security model for CLDC/MIDP2 draws on the
model for J2SE, in that it includes domains and
signed code. CLDC/MIDP2, however has a
simpler model, in part because of the constraints
imposed by the configuration and profile. The
following CLDC/MIDP2 constraints are most
significant
• Java Native Interface (JNI). JNI provides
J2SE applications access to native code
running on the platform. CLDC provides
similar capabilities in Kilo Native
Interface (KNI), but prohibits dynamically
loading and calling arbitrary native
functions.
• No reflection, remote method invocation
or serialization. In J2SE, an RMI server or
client can cause remote code to be
automatically downloaded and executed
to satisfy argument or return (sub)classes.
When a serializable RMI parameter is
provided an argument of an extended
type, the RMI system will attempt to load
(if necessary from an http codebase) the
needed class.
• No user-defined class loaders. Related to
the constraint above, the developer cannot
define a class loader in CLDC. The classloader in CLDC cannot be extended or
replaced
by
the
developer.
A
CLDC/MIDP2 application can only load
classes from its own (signed) Java
archive. As a result, the developer cannot
extend or modify any classes in the CLDC
configuration, MIDP2 profile, or which
are provided by the runtime environment
vendor.
• CLDC supports multi-threading, but it
does not provide facilities to build
daemons or thread-groups.
MIDP2 security protects access to
sensitive API’s by permissions. Protecting
resources includes the concept of a domain,
which is conceptually similar to J2SE. The
full scope of protection includes the following
elements:
• Protection domains (4) that are
statically defined in a policy file (by

•

•

the vendor) and associated to
resource permissions; for example,
socket, http, https, PushRegistry. The
protection domains are Minimum,
Maximum
(or
Trusted)
and
Untrusted.
Certificate and archive signature.
The jar file containing application
class files and other resources can be
digitally signed.
Level of access – either Allowed or
User.

Unlike J2SE, the 4 protection domains are
device-specific and defined by the runtime
vendor. They can be modified only as
provided by the vendor. Each of the four
domains is associated with a set of
permissions together with a level of access.
The 4 protection domains are defined by the
runtime vendor.
• Minimum. None of the permissions
are allowed.
• Maximum. All of the permissions are
allowed.
• Trusted. All of the permissions are
allowed.
• Untrusted. To be allowed, the user
must provide consent.
The permissions defined by the MIDP2
specification include: http, socket, https, ssl,
datagram, serversocket, datagramreceiver, and
PushRegistry (invoke other applications). These
permissions may be grouped together by the
vendor into meaningful subsets and assigned to
domains based on the subsets; for example,
NetworkAccess.
Within a domain, the level of access may be
different for different permission (sets). The
accesses are:
• Allowed. The permission (set) is
allowed without involving the device
user.
• User level. The application’s access
to the permission(set) depends on
explicit authorization from the
device user.
With user level of access, a dialog box is
presented to the user indicating information
about the permission and asking the user

3

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

whether access should be granted. User level
access can be specified on one of 3 modes:
• Oneshot. The user must be prompted
for each operation on the protected
resource.
• Session. When the user grants
access, it applies to all operations on
the resource during a single
execution of the MIDlet.
• Blanket. When the user grants
access, it applies to all operations on
the resource during any execution of
the MIDlet.
CLDC/MIDP2 provides MIDlet access to the
Record Management System (RMS), which
provides persistent storage for application data
via a record store. MIDP2 provides shared access
to the record store of other MIDlet suites, and
provides that access should be provided as readwrite or read-only.
Low-level security is provided by the J2ME
Java virtual machine. A virtual machine
supporting CLDC must reject invalid class files.
This is accomplished by a two-step process. At
development time, classes are pre-verified by a
tool which adds special attributes to class files to
facilitate runtime class verification on the device.
Much of the verification process can be handled
statically by the pre-verifier. At runtime, the
virtual machine rejects classes that have not been
pre-verified.

3.2 Security and Trust API
MIDP2 provides HTTPS and SSL for secure
communications with other devices. But, runtime
environment
providers
are
increasingly
providing additional options. For example,
IBM’s J9 version 5.7 provides web service
security package which allows web method calls
using encrypted SOAP envelopes or digitally
signed method calls for authentication and
information integrity.
Security and Trust Services API (SATSA)
provides access to more comprehensive hash
code,
digital
signature/verification,
key/certificate management, as well as
encryption and decryption. SATSA is designed
as 4 optional components. The primary purpose
is to provide access to a SmartCard Java device,
which provides security functionality in an
asynchronous manner that does not disrupt
applications supporting the device user.

SmartCard includes the Java Card Protection
Profile. The protection profile supports both
open and closed cards. Open cards provide the
end-user with the ability to install or activate
new applications on the card. Closed cards have
applications set by the vendor at the time the
card is personalized for the end-user. A good
example of a closed card may be a banking card
that supports personal electronic purchases and
bank account functions. Open cards that allow
new applications to be downloaded and installed
on the card present special security risk that
would exclude open cards that include banking
applications. Nevertheless, applications for open
cards that support other aspects of security may
become increasingly important. An example may
be securely communicating information outside
of direct e-commerce applications. Data integrity
and authentication are becoming increasingly
important
as
electronic
communication
proliferates.
The Java Card Protection Profile defines
four different configurations for a Java Card
based on open and closed cards. The minimum
configuration corresponds to a closed card in
which no applications can be installed on the
card after it’s been issued to an end-user. The
three
remaining
configurations
provide
additional functionality that’s available through
the evolution of the Java Card specification, such
as RMI (a limited version), logical channels,
applet deletion, object deletion, external
memory, biometry, and contactless interface.
The Java virtual machine for the device includes
an API (RTE API) that may contain classes for
performing security operations on information
and for certificate and key management.
SATSA runs on the limited-device, not on
the Java Card. SATSA provides an interface to
card security functionality, or when there is no
associated smart card, provides security
operations for the limited-device. SATSA has
four optional packages.
• SATSA-APDU provides low-level
stream/socket-based protocol for communicating
between the limited-device and the card.
• SATSA-JCRMI provides an RMI
interface that allows an application running on
the limited-device to call methods running in
applications on the Card. This interface would be
used to instead of SATSA-APDU to avoid the
overhead of programming with a low-level
socket data protocol.
• SATSA-PKI allows limited device
applications to use the smart card to digitally
sign information or to verify digital signatures.

4

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

PKI also provides for key and certificate
management.
• SATSA-CRYPTO. When a Java Card is
not available, the CRYPTO API is used to
compute security operations directly on the
limited-device.
Use of APDU, JCRMI, and/or PKI is
accomplished using threading on the limiteddevice. Threading allows security operations to
take place on the card while other applications
continue to run on the device supporting the enduser. In this scenario, SATSA is appropriate for
limited-devices with constrained processing
power. Independent of processing capability,
using a Java Card may be necessary to provide
assurance level that is appropriate to the
application. The open-device nature of cell
phones and PDA’s make it difficult to certify
trustworthiness of applications on the device.
Instead, we can isolate all high-risk user-specific
information and computations to a certified
secure Java Card.

HTML page, it returns an XML message in
Simple Object Application Protocol (SOAP)
format.
The service description - specified in Web
Services Description Language (WSDL) - this
description defines the web methods (functions)
that a service will accept - the inputs that go into
these methods, and the format of the output that
can be expected in return. This is used in
generating a web service client proxy class for
the limited device.
The web service registry - is a directory of
web services. The directory is optional because a
web service need not be listed in a registry to be
used. The registry provides a catalogue of
available services - similar to Java Naming and
Directory Service (JNDI).
The web service client proxy – The proxy
negotiates the communication between a limited
device client and the web service. It marshals
arguments, signs or encrypts as appropriate,
posts the message and interprets the result.

4. Secure Web Services

4.1 Types of Security Services

Web services provide an XML-based
service protocol for communicating among
components of a distributed application. Web
services differ from prior similar technologies,
such as Microsoft DCOM, Object Management
Group CORBA and Java Remote Method
Invocation through reliance on http protocol and
XML.

The following are the security services that
may be required by a distributed limited device
application.
Authentication: Ensures that the sender and
receiver are who they claim to be. Mechanisms
such as username/password, smart cards, and
Public Key Infrastructure (PKI) can be used to
assure authentication.
Authorization or Access Control: Ensures that
an authenticated entity can access only those
services they are allowed to access. Access
control lists are used to implement this.
Confidentiality: This assures that information
in storage and in-transit are accessible only for
reading by authorized parties. Encryption is used
to
assure
message
confidentiality.
Integrity: Ensures that information, either in
storage or in-transit cannot be modified
intentionally unintentionally. Digital signatures
are used to assure message integrity.
Non-repudiation: Requires that neither the
sender nor the receiver of a message be able to
legitimately claim they didn't send/receive the
message.

Figure 2. Web Services Architecture [9]
In Figure 2, the service, is performed by a web
server acting as a container for executing the
service code. This is generally just a web like
page that gets posted similar to the way other
http web requests are done. Instead of returning a

4.2 Transport Level Security
The most popular security scheme for web
services is SSL (Secure Socket Layer), which is
typically used with http, and is supported by

5

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

CLDC/MIDP2.
However using SSL for
securing web services has a number of
limitations. The inadequacy of SSL can be easily
explained by a simple example.
Consider a Web Service that can be provided
indirectly to a user. A user accesses a website
which indirectly invokes another remote web
service. In this case, we have two security
contexts:
1. Between the user and the website
2. Between the user and the web service
The second security context requires the
security of SOAP request/reply message
(between the web site and the web service) to be
assured over more than one client-server
connection. SSL is inadequate to provide this
type of security mainly because of the fact that
while it encrypts the data stream, it does not
support end-to-end confidentiality.
The shortcomings of SSL (https) should be
considered when being used for a distributed
application to reside on a limited-device.
SSL is designed to provide point-to-point
security. Often, Web services require end-to-end
security, where multiple intermediary nodes
could exist between the two endpoints. In a
typical Web services environment XML-based
business documents route through multiple
intermediary nodes.
Https in its current form does not support
non-repudiation well. Non-repudiation is critical
for business Web services and, for that matter,
any business transaction.
Finally, SSL does not provide element-wise
signing and encryption. For example, if there is a
large purchase order XML document, yet only a
single element, say, a credit card element needs
to be encrypted. Signing or encrypting a single
element is difficult with transport level security.

4.3 XML Signature
XML based security schemes, provide unified
and comprehensive security functionalities for
Web Services. The important ones being, XML
Signature, XML Encryption, WS-Security (Web
ServicesSecurity).
The W3C (World Wide Web Consortium)
and the IETF (Internet Engineering Task Force)
jointly coordinated to generate the XML digital
signature technology. The XML digital signature
specification [10] defines XML syntax for
representing digital signatures over any data
type. It also specifies the procedures for
computing and verifying such signatures.

Another important area that XML digital
signature addresses is the canonicalization of
XML documents. Canonicalization enables the
generation of the identical message digest and
thus identical digital signatures for XML
documents that are syntactically equivalent but
different in appearance due to, for example, a
different number of white spaces present in the
documents.
The advantages of using XML digital
signature can be summarized as below.
•
•

•

•
•

It accounts for and takes advantages of two
existing and popular technologies, viz., the
Internet and XML.
XML digital signature provides a flexible
means of signing. For example, individual
item or multiple items of an XML document
can be signed. This becomes extremely
useful in a scenario where each person in a
workflow is responsible ONLY for certain
work.
It supports diverse sets of Internet
transaction models. For instance, the
document signed can be local or even a
remote object, as long as those objects can
be referenced through a URI (Uniform
Resource Identifier). A signature can be
either enveloped or enveloping, which
means the signature can be either embedded
in a document being signed or reside outside
the document.
It provides important security features like
authentication, data integrity (tamperproofing), and non-repudiation.
XML digital signature also allows multiple
signing levels for the same content, thus
allowing flexible signing semantics. For
example, the same content can be
semantically signed, cosigned, witnessed,
and notarized by different people.

Figure 3. WSE – Input/Output filters [1]

6

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

Web Service Deployment Descriptor
(WSDD) and Handlers play the pivotal role in
the implementation of digital signature in Java.
A deployment descriptor specifies aspects such
as handlers and communication protocol. The
Handler is a java class (implementing the Input
and output filters of Figure 3) that provides a
MessageContext through which access is
provided to the input/output stream of XML \In
the Apache AXIS framework, MessageContext
is a structure, that contains: 1) a request message,
2) a response message, and 3) a number of
properties.
All the SOAP message manipulation is done
within the handler class.

6. Conclusions and Issues
The authors have continuing efforts in this
area which include obtaining devices, software
development environments, and simulators /
emulators related to securing limited-devices.
Our approach considers the operating
environment on the device as well as their
applications.
We are in the midst of consolidation of
small hand-held devices to provide integrated
functionality. Common applications including
personal organizers, cell-phones, and multimedia players can effectively be placed on a
single platform. While users who desire more
than one of these functionalities are exploring
integrated solutions, the industry is pushing
separation (partly for financial reasons.)
Consolidated functionality brings a higher
diversity of applications onto limited devices, as
does special purpose applications (for example
autonomous vehicle control). Either way,
security concerns increase.
The use of smart cards in the United States
is just beginning after lagging behind use in
some other regions. The integration of smart
cards (SIM-Cards) on cell phones is an
indication of this trend. Enabling high-risk
applications, such as banking and purchasing,
by leveraging smart cards integrated with other
devices presents an attractive alternative. Of
course the concern for security places new
demands on platforms in which security has not
historically been a high priority.
Performance has been the primary
impediment to the use of more strongly objectoriented languages such as Java for limited
device applications. Securing a distributed

application complicates the issue. Important
considerations include:
• Underlying
architecture
processor
performance,
ancillary
processing
capability such as SmartCard,
• Frameworks supporting securing the
application, as well as communications,
• Use of security mechanisms appropriate
to application needs (authentication,
integrity, confidentiality),
• Proper use of available frameworks
including proper handling of passwords,
certificates, keys, digital signatures, and
encrypted information.
Frameworks discussed in the paper are an
important enabler to developing more secure
distributed limited-device applications. Further
usage reports and benchmarking for security
mechanisms would better support developers.

References
[1.] Tim Ewald, “Programming with Web
Services Enhancements 1.0 for
Microsoft.NET”, Available, see:
http://msdn.microsoft.com/webservices/buil
ding/wse/default.aspx?pull=/library/enus/dnwse/html/progwse.asp
[2.] Sun Microsystems Java Security and
Crypto Implementation,
http://www.cs.wustl.edu/~luther/Classes/Cs
502/WHITE-PAPERS/jcsi.html
[3.] Knudsen, Jonathan; Understanding MIDP
2.0’s Security Architecture.
http://developers.sun.com/techtopics/mobilit
y/midp/articles/permissions/
[4.] WebSphere Everyplace Micro Environment
v5.7; MIDP Installation guide for J9 Palm
runtime environment. Available online from
IBM.
[5.] Mourad Debbabi, Mohamed Saleh,
Chmseddine Talhi and Sami Zhioua:
“Security Evaluation of J2ME CLDC
Embedded Java Platform”, in Journal of
Object Technology, (5,2) Mar-Apr 2006. pp.
125-54.
[6.] Security and Trust Service APIs for Java
Platform Micro Edition Developers Guide.
Available from http://www.java.sun.com/
[7.] Pannu, K.; Lindquist, TE; Whitehouse, RO;
and Li, YH; “Java Performance on Limited
Devices”; Proc The 2005 International
Conference on Embedded Systems and
Applications, CSREA Press, Las Vegas,
June, 2005.

7

Proceedings of the 41st Hawaii International Conference on System Sciences - 2008

[8.] Lindquist, TE, Diarra, M, and Millard, BR;
“A Java Cryptography Service Provider
Implementing One-Time Pad”; Proc. 37th
Annual Hawaii Int'l Conf on Systems
Sciences, ACM, IEEE Computer Society,
January 2004.
[9.] Online Documentation on Web Services,

Available from: http://www.servicearchitecture.com/webservices/articles/web_services_explained.ht
ml
[10.] XML Digital Signature Specification, W3C
Recommendations, Available from:
http://www.w3.org/TR/xmldsig-core

8

Test-Case Generation
with lOGen
Timothy E. Lindquist, Arizona State University
Joyce R. Jenkins, US Air Force Academy

This analysis tool
produces l/O paim that
represent pmgmm
execution paths. You
can use these paim as
humlles for pmgram
testing and intehce
validation to overcome.

da, through its richness and
intended application domain,
presents an increasingly complex
set of software-reliability problems. Work
exploring interactive and integrated testing tools for the language is needed. Integration with other tools - such as target
simulators, developmental tools, and support tools - is also necessary.
As implementations of Ada address
more complex paradigms, we must consider the reliability of the software constructed. For example, you can visualize
an Ada tasked program that is distributed,
possibly at the task level, across several
heterogeneous processors. Reliability of
such a system becomes a problem that
must be addressed at each life-cycle
activity.
The IOGen static-analysis tool for a
subset of Ada addresses part of this verification problem. It uses a technique based

72

0740-7459/88/0100/0072/$01 .OO 01988 IEEE

on symbolic execution and produces a set
of I/O pairs that represent execution paths
through a program. This article presents
IOGen's design and then demonstrates
how to use it to test programs and validate
Ada software interfaces. Although IOGen
is specific to an Ada subset, the same technique can be applied to any language
whose grammar uses a left-to-right scan
with one look-ahead token producing a
leftmost derivation (an LL(1) grammar).
Symbolic execution as described by
Hantler and King' can verify program
correctness and has been applied to program
TO prove a program correct, symbolic execution uses assertions
representing specifications and compares
them to the source code. An input assertion, Assume, is placed at the beginning of
a routine to specify constraints on inputs.
An output assertion, Prove, is placed
immediately before the return from a routine to specify results.
IEEE Software

Assume and Prove statements, as well received by output variables as functions
as invariants placed appropriately in the of the symbolic input. In a proof, the comcode, are predicates over the routine’s puted state at the completion of each path
identifiers, which provide a representation must satisfy the Prove to show that the
of specifications. A routine is symbolically routine is correct.
executed by selecting arbitrary symbols to
represent input values. Symbolic values, Design
Functionally, IOGen symbolically exewhich are assumed t o initially satisfy the
Assume condition, are assigned t o all cutes an Ada program unit and produces
inputs and pushed through computations. a set of 1/0 pairs representing the possible
A symbolic execution tree is formed to execution paths through the unit. One 1/0
verify the Prove statements at the end of pair is created for each execution path represented in the symbolic execution tree.
the routine.
The execution tree is formed by symbol- The input part of the pair is a predicate
ically executing the code. Symbolic execu- characterizing the initial constraints on
tion trees are built recursively from the inputs that cause the path to be executed.
root to the leaves. Two actions are central
to forming the tree:
First, when an assignment statement is
encountered, a new node is appended to
IOGen has a scanner to
the tree. A single subtree emanates from
recognize tokens, a
the node representing the symbolic execuparser to analyze tokens
tion of the rest of the program (beginning
with the continuation of the assignment
and build execution
statement). T o modify the assigned varitrees, and an 1/0 pair
able, the current symbolic values of the
generator.
identifiers in the expression are substituted
into the expression. The resulting formula
becomes the symbolic value of the variable
and is attached to the node.
Second, decision constructs a r e The output part of the pair is a predicate
executed symbolically. Each execution characterizing the values produced for
path through the decision point is repre- objects that are outputs of the routine.
sented by a separate subtree emanating
Components. IOGen has three compofrom the decision’s node. Branches are
formed when symbolic execution reaches nents: a scanner to recognize tokens, a
a decision point. When the execution tree parser to syntactically analyze tokens and
branches, predicates (called path condi- build execution trees, and an 1 / 0 pair
tions) are formed for each subtree. The generator. The scanner and parser perpath conditions are formed from the sym- form a one-pass, left-to-right, top-down
bolic value of the decision predicate and parse of the program. IOGen scans the
input program from beginning to end
the path condition before the decision.
Once the symbolic execution tree is word by word, analyzing its syntactic
formed, each path through it corresponds structure and building the execution trees.
to an execution path through the routine. The trees are then analyzed to get 1 / 0
The path condition closest t o the end of a pairs.
path describes the input constraints causing the path to be executed. Computations
Scanner. The IOGen input is a file conalong a path provide the symbolic values taining a package body assumed to be syn-

January 1988

tactically correct Ada source code. The
scanner reduces this long stream of characters into a sequence of tokens (words) that
are more suited to manipulation. The
scanner, called G e t a t o k e n , accepts identifiers, keywords such as Package and
Loop, and symbols such as < ,/ = ,and
(. All identifiers, keywords, and symbols
recognized by Ada are valid tokens to
IOGen.
In addition to its basic function, the
scanner also removes comments, extraneous blanks, and type information. The
Ada reference manual defines the term
“elaboration” for a declarative section to
allow runtime object initialization. Types
are not needed to generate the symbolic
execution tree, but initializations from the
declarative section must be accounted for
in the trees.
Purser. RRIPLL, a parser-generator
tool developed at Arizona State University, builds the IOGen parser. When written in a specific form, the grammar
describing a language is called LL( 1) (leftto-right scan with one look-ahead token
producing a leftmost d e r i ~ a t i o n ) .The
~
grammar provided to RRIPLL is augmented to include actions (procedure calls) that
build the symbolic execution tree. The
actions themselves are written in Ada.
The parser accepts tokens produced by
G e t a t o k e n and invokes the appropriate
Ada action routines. These action routines
build node sthctures, which contain information on the token and statement type.
During parsing, these nodes are inserted
into their proper locations in the symbolic
execution tree. IOGen builds trees like the
one shown in Figure 1.
Each node in the execution tree
represents an action taken when a particular Ada construct is encountered. The
action is represented by left arrows (< -).
As an example, the action temp < - 1 in
node 3 in Figure 1 represents the result of
the assignment statement temp := 1 ;.
Each path condition describes the initial

73

state causing the path from the root to that
condition to be executed. For example, the
path condition !(TI in Vowel), as seen in the
right subtree of node 2 in Figure 1,
describes a path in which a zero will be
returned. The path condition indicates
that char (the object for which TI is the symbolic value) is not a vowel.
Symbolic execution trees are represented by IOGen as a linked structure of
nodes. Actions taken along a particular
path are represented in one of two information fields (the Condition field and the
Sym-Value field), as shown in Figure 2.
The Condition field of a node refers to the
path condition along the edge emanating
from the parent to the node. An action
taken at a node is referenced by the
Sym-Value field. The Sibling and Children fields maintain the tree’s structure.
The leftmost child of a node is the first element in a linked list of all the node’s children. The list is maintained through the
Sibling field. The Children field maintains
a linked list of all leftmost descendants.

Creating IIOpairs. The parser creates
the symbolic execution tree, which is used
to devise a set of 1 / 0 pairs of the form P,
= (I!, 0,)for each procedure. One 1 / 0
pair is created for each path traversed in
the tree.
Examining the nodes in the tree determines whether the node represents an
action or a change in path condition. If the
node represents an action, the information
contained in the node is appended to the
output portion of the pair. The output
component, 0,,is obtained from actions
representing modifications to nonlocal
variables and output parameters. If the
node represents a condition, the stored
information is inserted into the input portion of the pair. Z,,the input portion of
the pair, is produced by examining the
path condition at the end of each execution
path. I, characterizes the initial states
leading to execution of the path.
Once the entire symbolic execution tree
has been traversed, all 1 / 0 pairs generated
are written to an output file designated by

Initial :

temp <- 1

return 1

11: (char in vowel)
01: return 1

Figure 1. Symbolic execution tree: Vowels.
74

12: not (char in vowel)
02: return 0

the user. The bottom of Figure 1 summarizes the two pairs for the function Vowels
in Figure 3.

Ada example
Ada itself poses no real problem in
creating a symbolic execution tree
representing a program. However, IOGen
does not consider Ada constructs such as
tasking and exceptions. To illustrate our
implementation, we examine assignments,
if-then-else, and procedure calls below.
IOGen treats iterations and recursive
procedures differently than other systems
do. For these constructs, we require that
the user attach invariants such as those for
an inductive proof. IOGen now addresses
assignment, procedure/function call,
return, if-then-else, case, exit, and loop
statement types.
Assignment. IOGen accepts any legal
Ada assignment statement with variables
and operators defined by Ada. Assignment statements are symbolically executed
by replacing the variables in the right-side
expression by their corresponding current
symbolic value. Because the right-side
expression is an algebraic expression, it is
left unchanged (except that it may be simplified). The algebraic expression becomes
the new value for the left-side variable.
IOGen maintains nonlocal and parameter variable lists as a rudimentary symbol
table. When it encounters variables in an
assignment’s expression, IOGen searches
tables to determine symbolic values.
IOGen maintains symbolic values (expressions) as lists so they can be easily manipulated and replaced. Once it evaluates the
entire expression and makes all necessary
substitutions, IOGen finds the object in
the appropriate list and updates its value.

If-Then-Else. Symbolic execution of IfThen-Else statements is similar to the normal execution for the conditional branching statement under evaluation. First, all
variables in the Boolean expression are
replaced by their corresponding symbolic
value. Two separate expressions are
formed with the new Boolean expression.
One expression represents a true Boolean
condition, the second expression
represents the negated condition. An IfThen-Else causes a fork in the execution
IEEE Software

tree. One subtree represents the true
Boolean expression (the continuation
begun by the Then). The other subtree
represents the false Boolean condition (the
continuation begun by the Else).
The path condition for the Then subtree
is formed by Anding the current path condition with the true Boolean expression.
The path condition for the Else subtree is
formed by Anding the current path condition with the negated Boolean expression.
Execution for both paths continues with
the instructions following the If-Then-Else
statement. Figures I and 5 demonstrate
symbolic execution of assignment and IfThen-Else statements found in Figures 3
and 4,respectively.
The function Vowels in Figure 3 presents a function that returns an integer
value (0 or 1) indicating whether the input
character is a vowel. The symbolic execution tree for the corresponding function is
shown in Figure 1. The symbolic value TI
represents the variable char. Figure 5
presents part of the symbolic execution
tree and 1/0 pairs for Count-Vowels, the
program in Figure 4.Count-Vowels contains an iteration that must be handled
with an inductive assertion. T o simplify
the example, Figure 5 contains only the
execution tree and pairs reflecting zero and
one iteration through the While.

Condition

Sym-Value

Pointer to path condition

Pointer to symbolic expression

Sibling

Next sibling

Children

Leftmost offspring

function VOWELS (CHAR : in character) return integer is
type VOWEL-CHAR is (’A’,’E’,’I’,’O’,’U’);
TEMP : integer;
VOWEL : VOWEL-CHAR

[l] begin
if (CHAR in VOWEL) then
[2]
[3]
TEMP : = 1;
[4]
else
[5]
TEMP : = 0;
[6]
end if;
[7]
return TEMP;
[8] end;

Procedure calls. Symbolic execution has Figure 3. Function Vowels.
two approaches to the control transfer
caused by a procedure or function call
when generating the symbolic execution
tree.
type LETTERS is ’A’ .. ’Z’;
One approach is to continue execution
type CHAR-STRING is array < > of LETTERS;
as if the procedure was contained in the
INDEX : integer;
main routine. This involves a macro-like
expansion of the procedure’s code into the
function COUNT-VOWELS (LENGTH:integer; 1NSTR:char-string)
symbolic execution tree. You must be carereturn integer is
ful not to confuse local variables with the
VOWEL-COUNT, TEMP : integer : = 0;
variables of the calling routine. In fact, this
method requires a start-from-scratch
[ 11 begin
approach because it causes the procedure
while INDEX < = LENGTH loop
[2]
code to be retested with every invocation.
TEMP : = VOWELS (INSTR(1NDEX));
[3]
The approach designed by Facemire5
VOWEL-COUNT : = VOWEL-COUNT + TEMP;
[4]
and implemented by IOGen suggests that
INDEX : = INDEX + 1; end loop;
PI
the symbolic execution tree be built
[6] return VOWEL-COUNT;
bottom-up to take advantage, as others
have, of abstraction. The 1/0 pairs for the
end COUNT-VOWELS;
called procedure are generated before symbolically executing the calling procedure.
The pairs generated are expressed in terms Figure 4. Function Count-Vowels.
January 1988

75

Initial:

return 0

(index< =I)and
! (n (index) in vowel)

(index< =I)and
( n (index) in vowel)

paths once” cannot be reliable. The 1 / 0
pairs generated by IOGen, however, can
be used to identify better criteria for certain types of errors. Consider the example
where an operation along a path causes a
divide by zero only in limited executions.
The 1 / 0 pairs characterizing that path
would in fact indicate the problem.
One method to determine this is to
examine the initial states characterized by
a predicate made up of the conjunction of
the input part of a pair with the output part
and by a predicate characterizing the
divide by zero. The resulting predicate
indicates the states where the divide by
zero occurs. For example:
Input part: X ’ < 25
Output part: Y = Z ‘ / X ‘

11:
(index < = length) and
(instr (index) in vowel)

12:
(index < = length) and
! (instr (index) in vowel)

01:
return 1

02:

13:
! (index < = length)
03:
return

o

return 0

Figure 5. Partial t r e e a n d pairs for Count-Vowels.

of formal parameters and any other inputs
or outputs.
If there areNpairs corresponding to the
called procedure, an N-way branch in the
execution tree is generated for the call. The
current path condition of the calling procedure is joined with the input portion of
each pair by an And operation to generate
a path condition for each subtree. To
express a path condition for a branch in
terms of the symbolic values corresponding to the actual parameters, the formal
parameters are substituted with their corresponding symbolic value.
Figure 5 presents the symbolic execution
tree and 1 / 0 pairs for the function
Count-Vowels. The two 1 / 0 pairs generated for the function Vowels cause a twoway fork in the execution tree in Figure 5
at node 3.
Symbolic execution of source code containing procedure and function invocations requires as input all pairs for the
called routines. This is accomplished by
generating a separate symbolic execution
tree for each subprogram. For each subprogram it encounters, IOGen creates an

76

object, called Proc-Node. IOGen maintains a linked list of these objects and subsequently refers to them as it encounters
subroutines in the main body of the package. Because only one pass is made over
the input, information such as formal
parameters and local variables must be
preserved for later retrieval. Although
local variables are not very important, you
must still consider their effects on formal
parameters and nonlocal variables.

Applications
1/0 pairs are completely independent of
specifications. With the exception of assertions placed in the code for iterations and
recursion, pairs are generated based solely
on the code. They describe what the code
does rather than what it should do. Thus,
using 1/0 pairs to directly drive test cases
(and expected results) would be like testing to see if the code does what it does.
There are, however, situations where pairs
can be used productively in testing.

Testing. Other research has established
that the test-selection criterion “execute all

indicates that the error occurs along the
path when the initial value of X is zero.
You can apply this method to other forms
of numeric error and to other Ada-defined
exceptions.
A general symbolic evaluator like
IOGen lets you apply different levels of
testing and verification to different program parts and characteristics. In some
applications, it may be necessary to formally verify (or prove) parts of the program, while testing would suffice for the
remainder. For example, sections of code
dealing with critical inputs or performing
critical computations could be verified,
and other sections could be tested. Because
IOGen is based on a proof method,’ these
critical sections could be verified.
A general symbolic evaluator lets you
both choose which method is appropriate
and use the same technique whatever your
choice. Along the critical paths, specifications can be used to verify code with an
interactive adaptation of IOGen for verification. On the less critical paths, you can
select 1 / 0 pairs and run tests based on
those paths. You can make a three-way
check of correct operation, including
specified results, symbolic results, and
actual results.
Validation. An Ada package defines a
software interface between the package
users or between the package itself and its
user. The interface syntax is defined
through the procedures and data defined
in the package specification. The interface

IEEE Software

I
semantics is defined by the package body.
Anna6 lets the semantics be expressed as a
part of the package specification rather
than be embedded in its implementation.
The general validation problem that
IOGen addresses is the construction of a
suite of test programs that can be used to
validate consistent implementations of a
package body. A suite of tests is needed
because several package bodies may implement the same package, specification
differently, according to the needs of a
particular system. When several
implementations with the same interface
must be validated, the validation programs
must operateindependently of the implementation details. Package bodies may be
operating-system-dependent,architecturedependent, or algorithm-dependent.
However, each implementation must display the same syntax and semantics. Validation programs must examine the
semantics without referring to i m p l d e n tation dependencies.
T o use IOGen to develop a validation
suite for an interface, we assume that at
least oneimplementation has been shown
to be semantically correct through proof,
testing, or acceptance by use. This assumption may at first appear to be wrong, but,
if you consider the problem in the context
of complex software interfaces, you can
see that any approach must have as its
basis an accepted operational version. For
example, you might suggest that you automatically identify validation tests from a
formal definition of the interface. But for
anything more than trivial software, the
the formal definition must be operationally tested to be accepted.

Using IOGen
IOGen output can be used to construct
validation test programs. For example,
consider the construction of a validation
suite for the kernel interface being developed for Ada support-environment tools.
The Ada Joint Program Office has
adopted the concept of a common set of
kernel facilities supporting automated
tools. Having implementations of the
interface on various machines would
greatly increase the transportability of
tools.
To formulate the requirements for the
January 1988

I

CAIS operational
definition source

I

IK
\

IOGen

1

I

Interactive
input
.........................

I/O

pairs

generator

Validation
test programs

I

Figure 6. Validation suite generation

kernel interfaces, the program office
formed the Kernel APSE Interface Team
and the Kernel APSE Interface Team from
Industry and Academia. The KIT/KITIA
teams have designed an initial set of interfaces, called the Common APSE Interface
Set.' CAIS is a set of 23 package specifications with descriptions defining the
interface semantics.
The CAIS was recently adopted as a
government standard. The APSE Evaluation and Validation Team is responsible
for initiating the development of a CAIS
Implementation Validation Capability.
The CIVC will be a set of implementationindependent tests to determine whether an
implementation of the CAIS adheres to
the specifications. CIVC is much the same
as the set of tests in the Ada Compiler Validation Capability developed for Ada compilers.
Facemire and Lindquist5 have described a specification and validation
technique to construct validation tests for
CAIS. Lindquist is developing an operational definition of CAIS, an implementation written nearly entirely in Ada. The
C A E operational definition is the sourcecode input to the method for constructing
validation tests. In addition to IOGen, a
test generator helps the user construct

tests. Like IOGen, the test generator interacts with the user. It produces Ada validation test programs from templates. Figure
6 shows the system's organization.
As an example of the test generator's
functionality, consider the construction of
a validation suite for an Ada interface,
such as CAIS. The method can be characterized by considering a single procedure
in an Ada package, say P. P has as inputs
(in parameters) the vector X = xl,x2,
. . . x, and as outputs the vector Y =
Yl,Y2, . . . Y R .
X and Y need not be distinct; a single
parameter may be specified as mode In
Out. P may also modify a nonlocal environment through nonlocal references to a
vector, say N , of objects defined in either
the package containing P or some other
visible package. P may also modify the
nonlocal environment through the set of
procedures it calls. We write this set as

c = c,,c2, . . . cy.

Symbolic execution of C produces sets
of 1 / 0 pairs, IOc. These are used in the
symbolic execution of P to produce IOr,
the set of 1 / 0 pairs for the procedure. We
need not refer explicitly to the outputs
of Csince these will all be incorporated in
IOp. We assume that the user selects elements from IOp to create validation test
77

programs. From each pair selected, the
generator will construct an Ada program
to execute the corresponding path. The test
program Withs the appropriate packages
of CAIS and performs its function in three
phases:
1. Initialization: Create the appropriate
CAIS context.
2. Test: Call the target procedure, P ,
with predetermined parameters.
3 . Examination: Determine if the call
exhibits the intended effect.
A problem particular to validation is
that the initialization and examination
phases must be performed with X,Y , and
routines defined in the interface. However, the 1 / 0 pairs may contain
implementation-dependent free variables
that may not be visible to the user. The test
must, however, be formulated in terms of
P’s parameters and other visible routines.
A stack Push is a good example of this.
1 / 0 pairs for Push would contain variables
particular to the implementation structure
(perhaps an array). One pair may describe
a condition in that the array is full.
Because the data structure is not directly
accessible by the test program, this condition must be initialized with other operations defined in the stack package.
The input condition of the 1/0 pair for
a test determines the initialization and test
phases. The problem of initialization is to
create the input condition using only elements visible to the interface. The input
conditions in IOpinclude as free variables
those from X and N together with those
free variables in the sets IOc. Initialization must be done using only those free
variables from X.
The test generator performs two functions to help the user construct the initialization code. The code is incrementally
developed by repeating the following
steps.
1. Let Init be a set of variables initialized to contain the free variables in the
input condition.
2. Identify to the user the visible procedures whose 1 / 0 pairs have outputs that
modify the variables in Init.
3. Symbolically execute the initialization section of the test program. Place the
free variables in the output condition
generated by Init.

78

4. Repeat steps 2-4 until the output condition generated in step 3 logically implies
the input condition being initialized to test
P.
These steps assume that the user interacts with the test generator to build the test
program. The test generator helps the user
by providing potential procedures (and
visible objects) to perform the initialization at step 2. At this point, the appropriate procedure calls are placed in the
template for the test program. Initialization is complete when the symbolic output
of the initialization provides the desired
input.
Following the initialization section, the
test generator places a call to the procedure
being tested, P , in the test program. The

Although necessary for
validation, dependent
routines remove the
ability to use test
programs as
development tests.

test generator cannot offer extensive help
in completing the code for the examination
phase. The test generator returns to the
user all the visible objects and procedures
that have free variables in their input or
output conditions that match free variables in the output condition for P . The
system knows nothing more about the routines needed to examine the test results
than that these routines must access the
modified objects.
The test programs, to be independent of
the implementation details, must depend
on other interface procedures. In removing implementation dependencies, we use
dependent routines in the interface.
Although this is necessary for validation,
doing so makes it impossible to use the test
programs as development tests. Because of
the dependencies among interface procedures, a failure to execute a test program
correctly does not imply a fault in the
procedure being tested: The fault may just
as easily be in the procedures used for initialization or examination.

OGen is incomplete in that it does not
recognize and generate a symbolic execution tree for all Ada constructs,
especially exception mechanisms and tasking constructs. Tasking, and concurrent
constructs in general, is an area in which
testing research must continue. Ada exceptions can be generated explicitly with a
Raise statement or implicitly (for systemdefined exceptions) by the runtime system.
Symbolic execution of explicitly raised
exceptions can be implemented straightforwardly, but implicitly raised exceptions
are troublesome. With implicit exceptions,
every expression evaluation performed
may result in a branch in the symbolic execution tree. Thus, when symbolically
evaluating an assignment statement, a
branch would exist for each possible
exception raised by the statement.
We are now converting IOGen to accept
a larger subset of Ada. We are particularly
interested in using explicit and implicit
exception mechanisms to detail all separate conditions that cause a single path to
be executed. We are converting IOGen to
execute a bottom-up parse of the program
using an LALR parser generator written in
Ada
Ada guarantees substantial gains in
reliability by many attributes in the language itself. However, certain constructs,
such as tasking and exceptions, present
reliability challenges to the software
engineer. These challenges can now be
dealt with at an appropriately high level,
where before they were isolated in the
operating system. We hope that tools like
IOGen will continue to address these problems using approaches with a formal
>
-0basis

Acknowledgments
This work was supported in part by the Ada
Joint Program Office through the Institute for
Defense Analyses, Washington, DC, under the
direction of Clyde Roby.

References
1. S.L. Hantler and J.C. King, “An Introduc-

tion to Proving the Correctness of Programs,” ACMComputing Surveys, Sept.
1976, pp. 331-353.
2. W.E. Howden, “Symbolic Testing and the
Dissect Symbolic Evaluation System,”

IEEE Software

> REPLACE THIS LINE WITH YOUR PAPER IDENTIFICATION NUMBER (DOUBLE-CLICK HERE TO EDIT) <

1

Web Applications and Web Services: Supporting
Technologies (Panel Introduction)
Timothy E. Lindquist
Arizona State Univeristy East, Department of Electronics and Computer Engineering Technology
7001 East Williams Field Rd, Mesa, Arizona 85212, USA
Tim@asu.edu

Abstract—Different technologies are emerging for developing
multi-tier enterprise Web-based applications. Among them are
Java 2 Enterprise Edition (J2EE) and the .NET Framework. Both
support XML-based Web Services and offer related features. This
panel explores the future of these technologies and identifies the
issues and strategies underlying their use and evolution.

O

NE of the most rapid areas of change for software
technology today is in frameworks to support Web
applications. As industry increases focus on commercial
utilization of the Web, framework providers vie for developer
buy-in through promise of improved support. Today a vast
majority of Web-centric applications are hosted on Java
solutions. But, technology advances in the recent release of
Visual Studio .NET and its underlying framework promise to
significantly impact market balance.
Suppose you are in the situation today of selecting the
platform for a new project aimed at providing Web presence to
augment your company’s retail offerings. The server-side
options available to you are broad and represent significantly
different approaches. The original approach of scripting
through common gateway interface has largely been replaced
by more efficient technologies represented by Java Server
Pages and Servlets. Recent development platforms, however,
present options where a good decision may depend upon
characteristics of your application, as well as aspects of the
technology framework. What application characteristics must
you consider and how can you compare alternative
frameworks?
Web Services is an emerging technology that utilizes HTTP
and XML to provide network-wide access to an Object’s
methods. The vision includes exposing business related
services to potential consumers thus facilitating business-tobusiness cooperation. The model equally supports business to
consumer communications, and is amenable to multiple cost
models, including pay-per-use, subscription and freely
available services. Interoperability among Web Service
platforms promises platform, language and infrastructure

Proceedings of the 2003 Symposium on Applications and the Internet (SAINT’03)
0-7695-1872-9/03 $17.00 © 2003 IEEE

independence. Web Services are based on the XML SOAP
standard, Simple Object Access Protocol. As realized in the
.NET Framework, Web Service Proxies, which allow a client
to use services, are automatically built upon discovery of a
Web Service. Evolving XML standards such as UDDI provide
network-wide registration systems for advertising and
discovering Web Services.
The .NET Framework supports alternative approaches to
realizing Web applications, including XML centric database
access and .NET Remoting that provides an object rich,
platform dependent solution for network available objects. The
latest version of J2EE provides support for Java Web Services
to augment the server-side session and database entity
approach evolving from Enterprise Java Beans. This panel
explores the issues underlying these various key technologies
for multi-tier enterprise Web applications. For example:
-

-

-

-

How is security of a Web-application best realized?
What security issues exist for infrastructure that
underlies multi-tier applications? What are the security
issues that arise in systems that combine related
technologies?
What is the vision of Web Services? What applications
are best suited to Web Services? What performance
issues underly the use of XML SOAP and related Web
Service standards, such as WSDL, UDDI or ebXML?
Interoperability for Web Service applications. For
example, what issues arise when trying to integrate an
ASP.NET Web Application (.ASPX) to consume Web
Services that are provided by J2EE server-side Web
Services? Do existing XML standards practically
provide interoperability?
Are there existing benchmarks that can help
technology developers select among provider
offerings? What techniques can be used to

This panel features distinguished researchers and
technologists from industry and academia. Included are
representatives of SUN and Microsoft. After they present
positions on the issues, we will have an open dialogue with the
audience.

ACM SIGSOFT

Software Engineering Notes vol 26 no 6

allows for sound group coordination. The paper describes a mail
based workflow engine that uses these aspects to enhance coordination. A 'distributed software development' (DSD) clerk is used
to communicate information between members of the team. Reviews of modifications to the system are done anonymously but
kept in a database. The paper also discusses some practical experiences and some problems that arose in implementation. A
stand-alone application, DSD Client, was implemented but required installation on every member's machine. A solution to use
Jini is provided to allow different kinds of programs to connect
automatically.

Preliminary Results of an Industrial EPG Evaluation by L.
Scott, R. Jeffery, U. Becker-Komstaedt
This paper presented a case study on the usefulness of electronic
process guides (EPGs). As companies define and/or modify their
processes, it is important that the processes and their changes be
communicated to all involved in the process. The usual and often
ineffective method of communication is a paper-based memo.
The Internet, however, provides an effective method of communication regarding processes within an organization. The web already provides distributed software teams with much assistance
and these are discussed briefly by the authors. The focus, however, is on an evaluation of an electronic process guide. The paper looks at what the EPG is used for and how it is used, what the
outcomes were, benefits and possible improvements. An EPG
consists of a project navigation tree, a main page and an individual page for every artifact, activity, role and tool. A survey was
taken to evaluate the outcome of the EPG and the results were
encouraging from all sides but improvements were suggested.
One primary use of the guide was to provide access to template
documents for process steps. Evaluations of the presentation of
the guide concerned the process rather than the actual presentation of the guide.

Process Model Integration in Interact-based Virtual Software
Corporations by Quentin Mair & Zsolt Haag
The Internet and increasing sizes of software development teams
have changed the environments in which software is being developed, and virtual software companies (VSC) are a prime example.
Tool support is required for process management and the desire to
integrate and interoperate process models has defined the need for
a VSC component process. Identifying the atomic processes performed within one organization helps to integrate varying processes between VSC members. A Process Specification Language
defines a neutral language for process specification and assists in
the exchange of process information. Finally, the Resource Description Framework provides interoperability between tools that
exchange information over the Internet. The paper looks at four
advantages in using RDF to define PSL and discusses five parts of
the PSL declaration. A mapping of PSL/RDF to enactable process
models is described and deficiencies such as homogeneity and
lack of an industrial case study are discussed.
References
[1] First Workshop on "SotIware Engineering over the Interact". Web proceedings
available at http://sern.ucalgary.ea/,--maurer/ICSE98WS/ICSE98WS.html
[2] Second Workshop on "Software Engineering over the Internet". Web proceedings
available at http://sern.ucalgary.ea/--maurerflCSE99WS/ICSE99WS.html
[3] Third Workshop on "Software Engineering over the lnternet". Web proceedings
available at http://sern.ucalgary.ea/--,maurer/icse2000ws/ICSE2000WS.htm
[4] Forth Workshop on "Software Engineering over the Internet". Web proceedings
available at http://sern.ucalgary.ca/--~maurer/icse2001 ws/ICSE2001WS.html

Process Support for Distributed Team-based
Software Development Workshop
Pierre F. Tiako
Inria-Irisa Lab, France
Pierre.Tiako@irisa.fr

Approaching Negotiation Automation for Software
Development Companies by Boris KOtting, Frank Maurer
In any company, better performance can be achieved by distributing tasks within the company efficiently. The same is true of
globally operating software development teams. A problem arises
though, when a manager cannot assign tasks according to the
employee's skill set as the required skills are not available in her
local team. In terms of software development, one must make
many decisions regarding varying topics, thus negotiating tasks is
complex. The paper discusses automating negotiation for agents
in a distributed company. A virtual marketplace is created to enable negotiation between different companies. The paper provides
four different strategies, a Yes/No offer, a sealed bid, an English
auction and finally negotiation. To support automatic negotiation
a utility function is used with a lower and an upper threshold.
Values are determined based on user settings. The agent first
checks the negotiation type, then determines the proper course of
action. The paper notes a key point that the details of the utility
function should remain hidden from the user. Finally, the possibility of multi-bids is discussed briefly.

November 2001 Page 31

Tim Lindquist
Arizona State University East, USA
Tim@asu.edu

Volker Gruhn
University of Dortmund, Germany
Gruhn@ls 10.cs.uni-dortmund.de'
Abstract
This reportsummarizes the 2nd International Workshop on
Process Support for Distributed Team-based Software
Development held at the Sheraton World Resort of Orlando,
Florida, on July 25, 2000 in conjunction with the Information
Systems, Analysis and Synthesis (ISAS2000) International
Conference. An overall twenty people attended the workshop
consisting of seven technical presentations in two plenary
sessions. In the following, we outline the presentations and
subsequent discussions, which included modeling and distributing
process component, evolution and change, web-based framework,
consistency management, and reuse and interoperability.

ACM SIGSOFT

Software Engineering Notes vol 26 no 6

The hardcopy of the papers selected for PDTSD'00 are published
by the International Institute of Informatics and Systemics as a
part of the "'Industrial Systems" volume of ISAS 2000, ISBN 98007-6695-2.

Workshop Goal
Due to globalization, it becomes necessary for big companies to
organize their production in autonomous and geographically distributed environments. Resulting problems for the companies,
which develop the software on Process-centered Software Engineering Environments (PSEEs) were in the scope of this forum.
The workshop sought participants to cover practical and theoretical issues of distributed team-based software development addressing the following key questions.
•

•

•
•
•

How can we model and distribute process components, their
internal and external structure and behavior, the services they
can provide in a network?
How can we manage evolution and change as part of the collaboration and the coordination of teams during the development?
How to use the Web to organize the geographically distributed software development ?
How can we manage the consistency, detect, prevent and resolve conflicts of multiples teams working in parallel?
What kind of infrastructure can enable reuse, share and exchange the objects among geographically distributed teams?

Reflecting these goals, PDTSD'00 provided a forum where
emerging approaches were presented, compared and discussed. It
addressed basic issues, especially multiple facets of distributed
software development. The workshop was organized in two technical sessions. In each session, four or five papers were presented.
Each presentation was followed by a short plenary discussion.

Modeling and distributing process component
Oquendo [4] presented the p-SPACE architecture description language and its architecture-driven approach for designing and
generating component-based software process models, p-SPACE
addresses support for compositionally of architectural elements
and the incremental synthesis of process model components. The
presentation by Lindquist [10l highlights a system of cooperating
distributed user interfaces that jointly manipulate the process
components. He reports on applying JINI and JavaSpaces to
achieve distributed interoperable process components. He also
presents the architecture and realization of a prototype system
providing plug-in discovery of available process services.
Maheshwari [12] presented a distributed software project management information system, the DSPMtool. It consists of the
tools and techniques used to gather, analyze, integrate, and disseminate the outputs of various software processes in order to
facilitate teamwork.

November 2001 Page 32

opment. He is based on the fact that software development is
distributed activity involving different people in diverse geo.
graphical locations. These people have different roles, back.
grounds, and tastes. Collaboration and coordination are needec
for team-based software development. He proposed a case stud)
based on an integrated software development environment thai
includes change management tools. He supported the distributior
of change related information by simple tools. The p-SPACE pre.
sented by Oquendo [4] supports the evolution of software proc.
esses through regeneration of components according to changes it
the process architecture in which they operate. Thereby, generatec
components remain compliant as their architecture evolves
dorgensen [8] proposed a framework with four activities, proces.,
model improvement, reuse, enactment and harvesting to generat¢
a set of requirements for process model evolution.

Web-Based Framework
De Man [5] described the evolutionary development of a process.
centered project support environment. He recalled the main moti.
vators and how they influenced implementation decisions, in particular the choice of a lightweight approach using standard wet
technology. He presented an early experience and gave sugges.
tions on how to increase and accelerate the acceptance of it.,
propositions in a globally distributed organization. The paper b)
Maidantchik [11] presents a software process management model
to a worldwide collaboration. It proposes applying a unique model
of integrated processes, which is used to support their continuo,,
improvement. The process model proposed is specialized according to each software group and instantiated for each project. ,~
Web tool supports the enactment of process instances. Belkhat#
[13] reported their experience in developing a temporal process,
for web multimedia applications over the Web. Systems, whicl~
run such applications can be centralized or distributed. He identi.
fled the principal strengths and weaknesses of both multimedia
application and process sensitive systems as software systems and
workflow management systems. He makes some proposals to im.
prove web multimedia applications. The web was chosen because
of its generalization, low-cost and easy to use principle.

Consistency Management
Finkelstein and Smolko [6] gave an account of architecture fol
management of consistency relations between distributed documents. They proposed a collaborative framework of interactin8
software agents, capable of concurrent execution of consistency
checks and architectural performance evaluation and scalability.
The proposition demonstrates the advantages of using of this system in a domain of software engineering documents. To prevent
consistency, Hoen [7] defended the idea for coordinating the efforts of multiple teams working in parallel on a model. A major
part of this effort is to resolve conflicts, which are only detected
when the work of the separate teams is integrated. He presented
how a model can be cut into distinct packages where in parallel
each of these packages is locally modified by just one of the
teams. Integration of the modified packages is straightforward as
Evolution and Change
he only allow local changes to a package, i.e. changes that do not
Ajila [2] presented an approach to manage the change as part of propagate beyond the package and that do not cause conflicts durthe collaboration and the coordination of teams during the
ing integration. Additionally, he showed how the package strucdevelopment. He is based on the fact that software development is

ACM SIGSOFT

Software Engineering Notes vol 26 no 6

November 2001 Page 33

ture of a model and the teams working on the packages could be [4] Chaudet, C., K. Megzari, F. Oquendo. A Formal Architecture-Drivan Approach
for Designing and Generating Component-Based Software Process Models, pp. 700(temporarily) adapted to manage the need for non-local changes.
706. In [1].

Reuse and Interoperability
Belkhatir [3] has suggested an adaptation of the observer pattern
structure by Gang of four to support management and prevent to
possible deviation of a team. He showed how we adapt the observer pattern to support monitoring of processes. His use of the
observer pattern results in an architecture that consists of several
components. He considered an observer as a management tool
which reports back to process administrators with a clear picture
of the overall process state. When applied, the Observers keep
multiple views of the process state in synchronization with the
process state server. Observers can be deployed during the enactment of the process and can operate as a back end tool weakly
coupled to the process engine enacting the process. In order to
coordinate and integrate the process observation driven and the
event-driven process execution driven, he proposed an event
server, which captures occurrences of events that arise during
process execution.
Models of software development processes are used for assessing
and improving work practice, and for supporting the enactment of
work in software projects. Integrating process enactment and improvement would enable a down-to-earth approach to knowledge
management and learning anchored in real practice. Jorgensen
[8] presented a reference model for such integrated process
knowledge management. The framework has activities, which
generate a set of requirements for process model reuse. He evaluated the mechanisms for process model inheritance based on these
requirements, illustrating the applicability of the framework. He
defended the idea that inheritance must be combined with approaches like components, projection, parameterization and patterns to fulfil all requirements. Tiako [9] argued that tools are not
always compatible with the different objects that must be shared
or exchanged among geographically distributed teams involved in
a large project. It is still important to provide an infrastructure
that enables to share or exchange object among teams. He described the basic concepts of such an infrastructure.

Keywords: architecture-driven software process engineering, architecture description
language, architecture-based synthesis of process models, process components.
[5] De Man, J. A lightweight process-cantered project support environment: motivation, implementation and experience, pp. 708-714. In [1].
Keywords: process-centered support environment, web technology, capability maturity model, process model, process tailoring.
[6] Finkelstein, A., D. Smolko. SoRware Agent Architecture for Consistency Management in Distributed Documents, pp. 715-719. In [1].
Keywords: consistency management, XML, software agents, co-operation, mobility,
distributed architecture.
[7] 't Hoen, P.J., and M.H. ter Beck. A Conflict-Free Strategy for Team-Based Model
Development, pp. 720-725. In [1].
Keywords:conflict strategy, package change, team automata.
[8] Jorgensen, H. D. Software Process Model Reuse and Learning, pp. 726-731. In

[11.

Keywords:process model reuse, process model inheritance, knowledge management,
process model evolution.
[9] Tiako, P. F., and G. Kouamou. Tool Integration in Distributed Software Devdopment Environments, pp.732-737. In [1 ].
Keywords:interoperability, CORBA, heterogeneity and distribution, software tool.
[10] Lindqui~ T., K. Gary. Experience with Distributed Process Components on Jini
and JavaSpaces, pp. 738-743. In [1].
Keywords: process component, software process, distributed applications.
[11] Maidantchik, C., A. R. C. Rocha and G. B. Xex6o. Process Management in a
Worldwide Collaboration, pp.744-748. In [1].
Keywords: software process, maturity model, worldwide software development,
process management and improvement.
[12] Maheshwari, P., and R. Surjaputra. DSPMtool: A Tool for Distributed Project
Management, pp. 749-754. In [1].
Keywords: project management, client-server, software engineering, configuration
management.
[13] Villanova, M., N. Belkhatir, N., H. Martin. A Temporal Process Model for Web
Multimedia Applications, pp.755-760. In [1].
Keywords:multimedia, process sensitive system, CO framework, web based systems,
temporal relationships.

4th ICSE Workshop on Component-Based
Software Engineering:
Component Certification and System Prediction
Ivica Crnkovic 1, Heinz Schmidt2, Judith Stafford 3, Kurt Wallnau3
~MiilardalenUniversity, Department of Computer Engineering, Sweden,

Conclusion
ivica.cmkovic@mdh.se
2Monash University, Australia, Heinz.Schmidt@csse.monash.edu.au
PDTSD'00 contained an abundance of insightful and meaningful
3Software Engineering Institute, Carnegie Mellon University, USA, {jas,
communication amongst the researchers in the field. Attendees
kcw}@sei.cmu.edu
came and they talked. In this sense, the workshop completely
Abstract
fulfilled its goals.
This paper gives a short overview of the 4th ICSE Workshop on
Component-based Software Engineering. The workshop brought
Paper References
together
researchers and practitioners from three communities:
[1] Proceeding oft.he International Conference on Information System, Analysis and
Synthesis - ISAS 2000 - "Industrial Systems" volume, Orlando, FL, July 23-26, 2000, component technology, software architecture, and software certi760 Pages. Ed. Sanchez, B., R. Hammel, M. Soriano and P. Tiako.
fication. The goal of the workshop was to find a common under[2] Ajila, S. Change Management in a Team-based Distributed Software Development standing and to discuss the topics related to the component
Environment: A Case Study of Basic Supports using WWW, pp. 688-694, In [1].
composition. The workshop was divided in eight sessions held in
Keywords: change management, distributed team-based development, WWW, and
sequence, starting with invited talks and ended with a final dissoftware evolution.
cussion.
A model problem, to bc used for further research and
[3] Belkhatir, N., N. Mihoubi. Adapting the Observer Pattern for Process Managework in future workshops, was discussed and later selected. The
ment, pp. 694-699. In [1].
Keywords: process model, process sensitive systems, CO framework, logging, process paper gives a comprehensive summary of the sessions and plans
management, observer pattern, d~igu pattern, process state, container, events, triggers.
for future work.

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

A Java Cryptography Service Provider Implementing One-Time Pad
Timothy E. Lindquist, Mohamed Diarra, and Bruce R. Millard
Electronics and Computer Engineering Technology
Arizona State University East
http://www.east.asu.edu/ctas/ecet
mailto:Tim@asu.edu

Abstract
Security is a challenging aspect of communications
today that touches many areas including memory space,
processing speed, code development and maintenance
issues. When it comes to dealing with lightweight computing devices, each of these problems is amplified. In
an attempt to address some of these problems, SUN’s
Java 2 Standard Edition version 1.4 includes the Java
Cryptography Architecture (JCA). The JCA provides a
single encryption API for application developers within
a framework where multiple service providers may
implement different algorithms. To the extent possible
application developers have available multiple encryption technologies through a framework of common
classes, interfaces and methods.
The One Time Pad encryption method is a simple and
reliable cryptographic algorithm whose characteristics
make it attractive for communication with limited computing devices. The major difficulty of the One-Time pad
is key distribution.In this paper, we present an implementation of One-Time Pad as a JCA service provider,
and demonstrate its usefulness on Palm devices.

1. Problem
Dependence on the communications infrastructure
continues to grow as the size of computing devices
decreases. The growing dependence on Internet accessibility to services that do not reside in a local machine
brings with it the need for secure communications. The
target of this work are relatively small devices and their
related systems, such as Windows CE, PalmTE, Handspring and cell phones used to access Internet services.
While several large computer service organizations have
spent millions of dollars recovering from cyber attacks,
the potential economic impact of insecure e-commerce
communications on limited devices is huge[1], [3].

Java continues to enjoy dominance in server-side
technologies, however, a small but growing number of
limited device applications are developed in Java. Nevertheless, Sun Microsystems Inc., added Java Cryptography Extension (JCE) and JCA (to the Java T M 2
Development Kit Standard Edition v1.4 (J2SDK), and
has created a substantial market for applications running
on J2ME (Java 2 Micro Edition). Other vendors are
offering Java runtimes for limited devices. These versions bring Java to client application developers [9],
[11], and raise the issue of appropriate Java-based security mechanisms.
J2ME does not include JCE and JCA, however The
Legion Of The Bouncy Castle has developed a lightweight Cryptography API and a Provider for JCE and
JCA [14]. Neither provider offers implementation of the
One-Time Pad cryptography service [14].
The simplicity of the One-Time Pad method and the
fact that it does not require high processor speed, make
it ideal for lightweight computing devices.

1.1

Context

This paper focuses on integrating the JCA cryptography service provider, starting by defining the engine
classes and then implementing the One-Time Pad
method. We include simple evaluation programs to test
the provider. The problem of pad distribution is one of
the tasks taken-on in order to have successful deployment.
Implementations of the one-time pad encryption0.9.4 are readily available. For example, one product is
available for Windows command line launching. The
source code written in ANSI-C and DOS executable are
available for download at http://www.vidwest.com/otp/
[1].
The Security documentation provided with J2SDK
includes detailed information on the implementation of

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

1

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

the Provider for the JCA [12]. The documentation for “a
Provider for JCE and JCA” of The Legion Of The
Bouncy Castle is also available [14].
The possibility of using the One-Time pad for data
encryption and decryption for security purposes on
lightweight computing devices was covered at the 35th
Hawaii International Conference on System Science
2002 [2].

Sender

Message

2. One Time Pad
The one-time pad algorithm is among the simplest in
the world of cryptography and is considered by some to
be unbreakable. It is nothing more than an exclusive OR
between the message (to be encrypted) and the pad (a
random key - sequence of bits). The principles that govern the encryption technique are not that simple to
apply. First, the key must be random, which by itself is a
big challenge. Second, parts of the key that have already
been used to do encryption must not be available for
other encryption. The key (Pad) must be a sequence of
random bits as long as the message to be encrypted. The
sender exclusive-OR's the message with the pad and
sends the result through a communication channel. The
one time requirement that makes it unbreakable and difficult at the same time is that after use, the sender must
get rid of part of the pad, and not use it again. At the
other end of the communication, the receiver must have
an identical copy of the pad. The receiver decrypts the
cipher text to obtain the original message by doing an
exclusive-OR of the incoming cipher with its copy of
the pad [1], [3], [4]. The receiver should also destroy the
pad after use. See Figure 1.

2.1

Advantages of the One Time Pad

If the pad is actually random and has been distributed
securely to the receiver, then no third party can decrypt
the message. Even guessing part of the key will not
allow a third party to determine the remainder. This is
why some people claim that one-time pad is unbreakable. While there are a number of very good pseudo-random number generators, so far any attempt to generate a
truly random key with computers appears to generate
the same sequence after a certain point. Several
approaches avoid this problem by personalizing the key.
Another advantage of this technique resides in the simplicity of its algorithm. It does not involve complex
operations that challenge the computational speed of
some relatively small processors.

XOR

Encrypted
message
transmitted
normally

OTP
Receiver

Encrypted
message
transmitted
normally

XOR

Original
Message

OTP

Figure 1. One-time pad (OTP) cryptography.

2.2

Disadvantages of the One Time Pad

The key must be as large as the message being
encrypted; this fact is sometimes inconvenient especially in the case of large messages. The principle of the
one-time pad is to have a unique key for each communication, which makes the generation and management of
keys problematic as the number of recipients and frequency of use escalates. The last challenging aspect of
the One Time Pad is the Key distribution. In fact the key
should remain undisclosed (secret) to any other party
besides the communicating parties. Extensions to the
One-Time Pad provider discussed in this paper center on
portable memory devices [15][16] that are frequently
synchronized with more capable machine (laptops or
desktops) for key exchange. However, our implementation is aimed at handheld devices in general. General
purpose (non-proprietary) portable memory interfaces
for handheld devices don’t exist yet, so another
approach is necessary. Some possibilities are discussed
in the Key Management section below. Each application
must deal with this issue in its own effective way(s).

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

2

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

3. Java Cryptography Architecture
The Java security model has been evolving to adjust
to new security issues. The JCA is a framework providing cryptography functionality development capabilities
for a Java platform. It was introduced early in Java’s
evolution as an add-on package. The first release of the
Security API was an extension of JCA including API’s
for encryption, key exchange, and coding message
authentication. Prior to J2SDK 1.4, JCE was optional, in
part due to export restrictions. The “Java Secure Socket
Extension” (JSSE) and “Java Authentication and Authorization Service” (JAAS) security features have also
been integrated into the J2SDK, version 1.4. Two new
security features have been introduced: “Java GSS-API”
(Java Generic Security Services Application Program
Interface) that can be used for securely exchanging messages between communicating applications using the
Kerberos V5 mechanism and “Java Certification Path
API” that includes classes and methods in the java.security.cert package. These classes allow the developer to
build and validate certificate chains.
The java cryptography architecture includes a provider architecture [2], [5], [6], [7], [10], [12]. The notion
of Cryptography Service Provider (CSP), or just provider, has been introduced in JCA. The provider archit e ct u r e a l l o w s f o r m u l t i p l e an d i n t er o p er ab l e
cryptography implementations. An application developer can create or specify his/her own cryptography service provider. The service provider interface (SPI)
presents a single interface for implementors. Classes,
methods and properties are accessible to applications
through the JCA application program interface (API).
The SPI allows a cryptography service provider to plugin implementations for java applications. A provider can
be used to implement any security service. Several providers can be available and they may or may not provide
similar cryptography services and algorithms. Figure 2
depicts the layers of Java Cryptography Architecture,
and is taken from Sun Java documentation [6].
A given installation of J2SDK may have several
cryptography service providers installed, which may
provide implementations of different algorithms and/or
may provide multiple implementations of a single cryptography algorithm. Each provider has a name that is
used by application programmers to specify the desired
provider. It is also possible to specify the order of preference of providers. The default provider that comes
with the J2SDK is the Sun provider, which includes a

wide variety of cryptographic algorithms and tools [2],
[5], [6], [7], [10], [12].

Figure 2. Java cryptography architecture

3.1

Java Cryptographic Service Providers

The Java Cryptography Architecture, which
includes the provider(s), has two main design principles,
First, is independence from implementation and interoperability: This derives from using the services without
knowing their implementation details [12]. Second, is
algorithm independence and extensibility; meaning that
new service providers and/or algorithms can be added
without effecting existing providers. Together these provide a modular architecture that allows for encryption to
be done by an implementation of a specific algorithm
and subsequent decryption to be done by another implementation.

3.2

Provider Implementing One Time Pad

The primary question when building such a provider
is: does the nature of the one-time pad allow it to be
implemented in a pluggable architecture? Figure 1
shows the interoperability between Sender and Receiver
as independent systems in communication. The element
they are required to have in common is the key. The
implementation of the algorithm does not matter. As far
as extensibility is concerned, it is up to the provider programmer to remain independent of other cryptographic
services.
A Cryptographic Service provider is a package or set
of packages providing concrete implementations of a
subset of the cryptography portion of the Java security
SPI. The Java Security Guide in J2SDK, v1.4 documentation lists a series of nine steps to follow for implementing a Provider. This paper follows those steps as
guidelines for its development.
Two aspects in the structures of cryptographic service were needed to write the implementation code: the
Engine Class and the Service Provider Interface (SPI).

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

3

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

3.3

Engine Class

An Engine Class is an abstraction of a cryptographic
service. It defines the service without a concrete implementation of the particular associated algorithms. Applications access instances of the engine class through the
API, to carry out available operations. Every engine
class has a corresponding service provider interface,
which provides abstract classes accessing the engine
class features. The service provider interface indicates
all the methods that the actual cryptographic service
provider should implement for a particular cryptographic algorithm or type. A service provider interface
is named with its engine class name followed by “Spi”
[12].
For each service that a provider implements, we must
define the engine class, and then write its service provider interface. For the One Time Pad technique, the
service provider interface’s abstract class is called OneTimePadSpi. The engine class for OneTimePadSpi in
compliance to the nomenclature of JCA is called OneTimePad. The engine class is a concrete subclass of the
service provider interface, implementing all the abstract
methods.
The provider class is a final subclass of java.security.provider. Our provider is named ASUEcetProvider.
The provider name is used by applications to access our
one-time pad service [12].

3.4

Provider’s Information

The provider class provides access to various properties of the service, including the version, and other information about the service(s) it provides such as
algorithm, type, and techniques [2], [12]. The value provided for this argument in this project is: “ASUEcetProvider v1.0, implementing One Time Pad (OTP)
cryptographic technique, Arizona State University East,
Electronic and Computer Engineering Technology. May
2003”

3.5

Install and Configure the Provider

The provider needs to be correctly installed and configured for the application program to utilize its cryptographic service(s). There are two different ways to
install a provider. The first method consists of creating a
JAR file (Java Archive File) or ZIP file containing all
the class files belonging to the Cryptography Service
Provider. The JAR file is added to the CLASSPATH
environment variable. The exact steps of doing this last
action, depends upon the local operating system [2],
[12]. The second approach deploys the provider’s JAR
file of classes as an extension (optional package) to

Java. The file can be bundled with a particular application (with a manifest indicating relative URLs), or it can
be installed in the Java Runtime Environment to be
shared by all running applications.

3.6

Registering the Service

Configuring the service provider enables client
access to the service(s) by registering the provider and
defining default preferences where more than one provider is registered for the same service algorithm.
Static Registration consists of editing the java.security file (located in “lib\security” subdirectory of the
Java Runtime Environment) to add the provider name to
the list of approved providers. For each available provider for a given algorithm, there is a corresponding line
in the java.security file with the form:
security.provider.<n>=<providerClassName>
Where “n” is the preference number for the provider.
For example the line:
security.provider.2=asue.provider.ASUEcetProvider
registers our provider with an order of preference 2.
Dynamic Registration can be done by a client application upon requesting service(s) from a provider. The
client application calls a class method, such as:
Security.addProvider (Provider providerName).

3.7

Test Programs and Documentation

Several test programs were written to exercise three
aspects of the service provider. For client applications to
be able to request service(s) provided by a specific provider, the provider should be successfully registered
with the security API. A simple test program can verify
registration by creating an instance of the provider and
accessing its name, version, and info (getName (), getVersion (), and getInfo () methods.
After making sure that the provider is accessible
from the security API, we need to retrieve the provided
service(s) by calling its “getAlgorithm ()” method.
Finally, we check the functionality provided by the
service by writing sender and receiver applications that
use the service.
In addition to providing sample service test programs, our implementation provides documentation.
Our documentation is generated by the Javadoc tool
from the source code and it targets application programmers.

4. Key Management
The generation and distribution of the random keys
for this method is of primary concern. Since the size of
the key must match the size of the message being

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

4

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

encrypted, we limit our application to transmission of
relatively small messages. The key can be any random
array of bits, the key type used in JCA has not been
used.
The OneTimePad class has been designed, so that
when a new random key is generated, it is stored in an
external file that could be later sent to the receivers.
There are many approaches to providing random keys
for the One-Time Pad. The primary problem with using
the One-Time Pad is the amount of random bits needed.
Every message sent needs a key of the same length. If
every message is sent encrypted, the required key space
becomes large very quickly. The alternative of only
encrypting sensitive data suffers from forcing the application to make these choices, and only delays the issue
of when and how to replenish the key once all the bits
have been used on earlier messages.
Handheld devices, the target for this work, generally
only send small messages requiring encryption. This
allows for many messages using a small key of say 1
MB. A nightly synchronization using a recharging cradle can be used to also replenish the One-Time Pad key.
For devices that don’t have a connection to a host while
charging, another approach is portable memory devices
[15][16]. With a 2 GB pad, the replenishment cycle
would be much longer. Generally long enough to add a
few jpeg or gif pictures a day. The primary problem
with portable memory and handheld devices is that of
interfacing requirements. Currently, no general-purpose
interfaces (e.g., USB) are available for handhelds. Portable memory devices, to be useful, must come with general purpose interfaces, such as USB. An alternative to
the cradle and portable memory approach is to update or
replenish the pad on-line. In this approach, a One-Time
Pad is used until almost exhausted. Then the remaining
pad is used to exchange a block-cipher key for a more
computationally complex cryptography algorithm, such
as RC4 or AES. The One-Time Pad can then be generated by the server and sent to the handheld using the
complex cipher. The encryption process would then
return to the One-Time Pad methodology. This
approach is expensive on both network and CPU utilization. It may only be acceptable on larger handhelds such
as Windows CE (Pocket PC) machines. A final alternative would be to use PRNG (pseudo random number
generator) that is cryptographically appropriate. An
example PRNG is ISAAC [17]. ISAAC is a relatively
fast random number generator with a very long cycle
(i.e., guaranteed 240 with an average 28295). Generating
random pads then requires knowing the seed. Using the
prior approach of a trailing seed from the last pad as a
seed for the new pad would permit, possibly, near realtime generation of One-Time Pads.

5. Running on Limited Devices
The fact that JCA and JCE are not part of J2ME, limits applicability to our intended application space. One
approach is to configure limited client applications by
embedding the provider directly in the deployed J2ME
application.
Another approach is to use the lightweight cryptography API defined by The Legion Of The Bouncy Castle to
develop a provider based on their design principle of A
provider for the JCE and JCA [14]. This solution results
with a provider not fitting exactly the Java Cryptography Architecture, but which is usable on J2ME devices,
such as PalmOS [14].

6. Conclusions and Enhancements
As long as electronic communication continues to
expand, security will remain an issue. Cryptography is
one of the most effective tools available to address these
issues. One-time pad is among the most powerful existing cryptographic techniques, providing it is used within
the constraints of its applicability. Primarily the constraints are key management, compute limited devices
and encryption tasks that do not require encrypting large
files. The Java Cryptography Architecture offers the
potential of a single interface for applications that
allows plug-in of any number of participating service
providers. The approach allows evolution of security
approaches with the promise of minimal impact on
applications.
Security remains an open field on every computing
platform. As platforms continue to evolve to smaller and
better connected devices, they will meet the information
needs of a broader range of consumers. Its importance to
provide frameworks for developing secure distributed
and web-based applications on such mobile devices.

7. References
[1.]

d@vidwest.net (2000, November 17). “One Time Pad
Encryption v0.9.4” Retrieved January 25, 2002 from
the World Wide Web: http://www.vidwest.com/otp/

[2.]

Gong, L. (1998). “JavaTM 2 Platform Security Architecture Version 1.1”. Sun Microsystems, Inc. Retrieved
February 20, 2003 from the World Wide Web: http://java.sun.com/j2se/1.4/docs/guide/security/spec/securityspec.doc.html

[3.]

Jenkin M. & Dymond P. (2002) “Secure communication between lightweight computing devices over the
Internet”. HICSS 35 January 2002.

[4.]

Kahn D. (1967) The Codebreakers, New York, NY,
MacMillan.

[5.]

Knudsen J. (1998) Java Cryptography, Sebastopol, CA,

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

5

Proceedings of the 37th Hawaii International Conference on System Sciences - 2004

O’Reilly.
[6.]

[7.]

[8.]

[9.]

Lindquist T. (2003, January 14). “Cet427/598 Distributed Object Systems”. Retrieved February 02, 2003
from the World Wide Web:
http://pooh.east.asu.edu/Cet427/ClassNotes/Security/
cnSecurity.html
McGraw, G. and Felten, E. (1996) Java Security: Hostile Applets, Holes, and Antidotes. New York, NY.
John Wiley & Sons.
McGraw, G. (1998) “Testing for security during development: why we should scrap penetrate and patch”.
IEEE Aerospace and Electronic Systems, 13(4):13-15,
April 1998.
Muchow J. (2001) Core J2METM Technology and
MIDP, Upper Saddle River, NJ, Prentice Hall.

[10.]

Oaks, S. (1998) Java Security, Sebastopol, CA, O'Reilly & Associates.

[11.]

Rubin, A, Geer, D. and Ranum, M. (1997) The Web Security Sourcebook. New York, NY, John Wiley &
Sons.

[12.]

Sun Microsystems, Inc (2002). “Java 2 Platform, Standard Edition, v 1.4.0 API Specification”. Retried January 12, 2002 from World Wide Web: http://
java.sun.com/docs/

[13.]

Sundsted T. (2001). “Java, J2ME, and Cryptography”
Retrieved March 20, 2003 from the World Wide Web:
http://www.itworld.com/nl/java_sec/10262001/

[14.]

The Legion Of The Bouncy Castle (2000). “The Bouncy Castle Crypto APIs” Retrieved March 20, 2003 from
the World Wide Web: http://www.bouncycastle.org/index.html

[15.]

Zbar, Jeff, “Portable Memory Gets Small,” retrieved
Sept. 2003; http://www.beststuff.com/article.php3?story_id=4395.

[16.]

Lexar Media, “Samsung Sampling 2Gb NAND Flash
Memory Devices to Lexar Media,” retrieved Sept.
2003; http://www.digitalfilm.com/newsroom/press/
press_02_25_02a.html.

[17.]

Jenkins, Bob, “ISAAC: a fast cryptographic random
number generator,” retrieved Sept. 2003; http://
www.burtleburtle.net/bob/rand/isaacafa.html.

0-7695-2056-1/04 $17.00 (C) 2004 IEEE

6

Tracking Personal Processes in Group Projects
Ly Danielle Sauer,
Sandia National Laboratories
Idsauer@sandia.gov

Timothy E. Lindquist,
Arizona State University
tim@asu.edu

systems. Tool vendors and the research community have
responded with a variety of approaches.
A review of the tool market place shows many groupware, process, and workflow tools whose fhctionality
ranges from graphical modeling or simple enactment to
full support for defining, executing, analyzing, measuring,
and tracking software processes. The Plethora of tools,
most of which have not been widely adopted, combines
together with the increasingly distributed nature of
software development today to form one of the challenges
addressed by this paper. That is, the need to have
interoperability among a heterogeneous set of process
tools (which execute on distributed heterogeneous
platforms).
The efforts of the Workflow Management Coalition
(WfMC) [16] and the Object Management Group (OMG)
are aimed at this challenge. Both organizations are
identifying common interfaces that vendors can use for
interoperability among their products. Other middleware
efforts (Portable Common Interface Set [5],[11] and Open
Process Components [8]) are addressing integration of
process components with other middleware components
(i.e. version, configuration management, etc.). In this
paper, we build on these efforts to show how processes
can be distributed compositions of personal process
components.
Considerable research has addressed automating the
software process [I]. Some are addressing formalisms
(Petri nets, rule-based, process programming languages,
event-based, and object-oriented). Other research includes
comprehensive environments centered on processes, such
as ISTAR, in which all activities are modeled using a
contractual model. In a process-centered environment,
nearly all activity takes place within a defined process.
Christie has elaborated several problems in the
adoption of process automation [3]. Process-centered
environments are typically all-or-nothing and difficult to
adopt in steps. Management is justifiably reluctant to
invest in dramatic change without a gradual migration
path or concrete evidence of value-added. Benefits of
enactment support or track the required time consuming
front-end resources for process definition. Some systems
require definitions of activities that do not have relevance
to tracking and improvement. Adoption also places other
stresses on an organization ranging from an engineer’s

Abstract
Software engineering continues to develop methods for
process improvement and quality. The Personal Software
Process is one way to introduce software engineers to
aspects of process tracking, assessment and improvement.
In this paper, we describe the software tools that we’ve
constructed to support the planning and postmortem of
software activities. We describe an approach that allows
the personal software process to be used in group
projects, while still allowing the individual engineer to
employ personal process quality and improvement
techniques in their own activities. The tools supporting
planning and postmortem are used in the context of a
workjlow system developed at Arizona State University
(ASU), called Open Process Components, whose aim is to
componentize
software
services
and
provide
interoperability among various approaches. These tools
and approaches explore software development in the
increasingly distributed environment in which the
software engineer is responsiblefor their own assessment
and improvement.

1. Introduction
Measuring, guiding, and refining an organization’s
software process improves effectiveness of development
resources and provides a level of control for software
quality. The development of the SEI Capability Maturity
Model [14] has raised awareness of the need for better
software processes. Software processes are often
discussed at the project management level, and its not
uncommon for an organization to employ the services of a
process engineer with the intent of wide-scale process
improvement.
Software processes describe the interaction among
people and artifacts in carrying out the work involved in
the software life-cycle. A software process encompasses
the work that will be done (activities), what it will use and
produce (input and output products), who will do it
(agents), as well as, when and how it will be done
(behavior). The past decade has seen increased demand
for more powerful and robust automated software process

364
0-7695-0368-3/99
$10.000 1999 EEE

Jeremy Cairney
Arizona State University
cairney@asu.edu

coding standards, size measurements, and a Process
Improvement Proposal (PIP).

perception of excessive intrusion to the need for
additional personnel who specialize in process engineering.
In this paper, we present a process framework that
shifts its approach towards composable process
components. Project processes are created by brokering
among the building blocks of an engineer's defined
personal processes. Software engineers are responsible for
tracking measuring and analyzing their own processes
distinct from organizational concerns.

I

I

2. Overview of Personal Software Process
Current software professionals utilize private
techniques and practices that were learned from peers or
through personal experiences. Few software engineers are
aware of, or consistently use methods that lend themselves
to personal process improvement. A personal software
development process is a concept introduced to address
improvement needs of an individual.
Watts Humphrey of the Software Engineering Institute
has formalized a personal software development process
called Personal Software Process (PSP) [9],[ 101. Today,
there are various realizations, of PSP to aid software
engineers in applying the process. The realizations range
from case tools and web-based repository browsers to
formal training classes.

Figure 1. PSP Process Evolution
Personal Planning Process (PSPl , PSPl . l ) adds planning to the baseline. Here, the software engineer prepares
the basis for project tracking, which include software
estimates and development plans. The goal is to learn the
relationship between program size and resources, as well
as how to make realistic schedules. PSPl enhances PSPO
and PSPO.l to include size and resource estimation and a
test report using Proxy Based Estimation (PROBE) as a
method to estimate sizes and development times.
Personal Quality Management (PSP2, PSP2.1)
provides defect management by tracking the relationship
between time spent in reviews and the phases during
which defects are injected and removed. Prior project
defect data are used to realize review checklists and selfassessments. PSP2.1 extends PSP2 with design specifications and accompanying analyses. The goal is to provide the criteria for design completion.
Cycle Personal Process (PSP3) introduces techniques
for developing large-scale projects. The approach calls
for sub-dividing into personal processes. Development is
done in incremental steps starting with a base module.

2.1. Personal Software Process
PSP is designed to assist software engineers in
controlling, managing, and improving their predictability,
productivity, and quality. PSP consists of a family of
seven personal processes that progressively introduce data
and analysis techniques (Figure 1) [lo]. Engineers use
these data and analysis outcomes to determine their
performance and to measure the effectiveness of their
methods. Humphrey's initial result (applied to 50 students
and three industrial software organizations) indicates an
average test defects improvement of over ten times and
productivity improvements of better than 25% [9].
Figure 1 shows the PSP progression in which each PSP
step includes all the elements of prior steps together with
additions. The PSP process steps are Baseline Personal
Process (PSPO, PSPO. l), Personal Planning Process
(PSP1, PSPl .l), Personal Quality Management (PSP2,
PSP2.1), and Cyclic Personal Process (PSP3). Starting in
The Baseline Personal Process, the software engineer
creates the foundations for measurement and
improvement. PSPO is the software engineers current
software development process extended to provide
measurements (time and defect trackings). PSPO covers
three phases: planning, development (design, code,
compile, and test), and postmortem. PSPO. 1 includes

2.2. Personal Software Process Studio
Personal Software Process Studio (PSP Studio or
PSPS) .[6]is a case tool developed at East Tennessee State
University to assist in using the Personal Software Process. PSPS does so by automating the planning and
postmortem artifacts. In particular, PSPS provides the
following features: Data Measurement, Historical Database, Convenient Access to Tables, Statistical Calculations, and Guidance through the Process. The Data
Measurement feature allows developers to accurately
(similar to a stopwatch) measure development times, track
defects, and measure program sizes. The Historical

365

Database feature allows developers to store all of the
developers historical PSP data in a reliable and secure
database. Convenient Access to Tables provides a window
with tab access to the forms. The Statistical Calculations
feature automatically maintains totals and performs the
statistical calculations. The Guidance through the Process
feature provides on-line directions for using the PSP.
PSP Studio groups all of the automated paper work,
forms, and calculations into two categories: Process
Tables and Project Tables. Process Tables guide or
improve the individual software engineer process with an
online process outline, access to standard tables for
defects, LOC and coding standards, and access to a
process improvement proposal.
Project Tables are automated forms, such as Logs,
Summaries, and Templates. The log tables support
tracking time, defects, and issues. The Project Summary
table records the estimated and actual totals for the project
and for all projects to date. The Cycle Summary table
supports the project summaries by capturing the planned
and actual size, time, and defects for each cycle.

brother is watching over me” complex that is common to
organizationally imposed quality and improvement efforts.
This section discusses our approach to providing well
integrated organizational and personal process
improvement.
At ASU, we have been developing software to support
the use of planning and postmortem phases of the PSP and
to support their application to various life-cycle activities.
For example, in an organizational setting, an individual
may be assigned to testing. The test engineer would
develop their own test process that includes planning and
postmortem. The resulting personal test process becomes
part of an organizational or project process. The
“integratable” personal processes (personal test process)
collect product measures, use defect analysis, and
consider resource usage as a means of improving that
process segment. The artifacts and the automation we
have developed are discussed in Section 3.2.

3.1. Process Components
The Open Process Component Toolset (OPC)
[8],[12],[13] is a set of tools developed at ASU to support
process definitions and enactment. OPC’s basic premise is
that a process is a process component and may consist of
one or more process components. Process components
may be compositions of subcomponents whose underlying
representations may differ. For example, a Process
Weaver component, called create-design, may be
composed with a TeamWare Flow component called
review-design. Thus, the Integrated Process is defined as
a process component consisting of three process
components: the Planning Process Component, the
Personal Software Activities Component, and the
Postmortem Component. The Software Activities
component may be any process component such as,
testing, design, coding, or review.
Discussion of these components and the support we
have implemented for Planning and Postmortem artifacts
can be found in Sections 3.1.1,3.1.2, and 3.1.3).

2.3. ECEN 4553 Database Browser
ECEN 4553 PSP Database (PSP Database Browser)
[2] is a web-based database that also automates many of
the PSP forms, scripts, calculations, and reports. The database is organized to capture a set of related data for an
individual software engineer. The core of the database is
the concept of a job, which is a software engineer’s
activity. Once the job is defined, the software engineer
can log time against the job, log defects against the job,
and specify a detailed project plan for the job.
Although the PSP Database Browser does not strictly
adhere to all of Watts Humphrey’s Personal Software
Process data, it collects planned and actual data for each
job. ECEN 4553 PSP Database Browser automates a
subset of Watts Humphrey’s Personal Software Process
with a web-based user interface.

3. Applying PSP to Group Projects

3.1.1. The Planning Process. The Planning Process
Component defines the individual engineer’s plans for the
software activity. The process is assigned to the project
planner, takes as inputs the customer requirements
(written or oral) and produces as output an initial version
of the planning artifacts, a requirements specification, a
cost estimate report, and a size estimate report.
Additionally, an engineering notebook for the project is
created and initialized based on the activity schedule. At
this phase, the project activity schedule and the project
plan summary f o m only contain planning information
such as estimated total size, the project development
duration, and defects injected and removed. Further

IS0 9000 [4] and the Capability Maturity Model
(CMM) [ 141 assist organizations in improving their
processes. Personal Software Process, on the other hand,
provides an improvement technique for software
engineers in the context of individually developed
software. Seamless integration of the PSP within a
software organization cannot be achieved, since individuals rarely cycle through all phases of development on a
software project. Engineers can, however, apply PSP
analysis techniques to their individual activities on group
projects. The resulting metrics can be the basis for
personal process improvement, without having the “big

366

injected,and phase removed. PSP uses yield (percentage
of defects removed before compiling), appraisal cost of
quality and failure cost of quality as the primary input for
quality management and process improvement. These are
good starting points for the practicing software engineer,
however, one must define product measures and defects in
a manner appropriate to the activity. For example, a test
engineer may use test cases generated as the primary
product measure. For example, test cases may be defined
to be triples (input condition, action, expected result)
independent of how the test case is realized in performing
tests. Defect types for a test engineer may include:
unsatisfied test requirement and resulting software defects
for which there existed a test case.

descriptions of the project activity schedule, the project
plan summary, and the engineering notebook are
discussed in Section 3.2.
The Planning Process Component is composed of its
children process components: Identify Requirement,
Perform Size Estimation, Perform Cost Estimation, and
Construct Plan. Each child process component is defined
to perform a specific task to help planning the software
activity and laying the groundwork for analysis. For
instance, the IdentifL Requirement Process Component
generates the requirement specification (SRS) given the
customer requirements.
The OPC definition tool uses graphical depiction to
model the Planning Process [12]. The model includes
nodes for Processes (process components that have subcomponents), Activities (process components without
subs), Roles and Products. Directed edges depict relationships (i.e. is-input-to, has-output and has-sub). For
example, the Requirement Specification (Product)
is-input-to Perform Cost Estimation (Activity), and the
Identify Requirement (Activity) has-output which is the
Requirement Specification.

3.1.3. The Postmortem Process. The Postmortem
Process Component defines a process for analyzing the
performance (postmortem analysis) of a completed
project. Postmortem analysis gathers product measures,
performs actual resource usage analysis, performs actual
defect analysis, and performs summary quality analysis.
We have used OPC to depict Postmortem. The process
is assigned to the process engineer and accomplishes its
objective of producing the project plan summary by using
the initialized project activity schedule, the project defect
log, and the initialized project plan summary as input
products. Unlike the Planning Process Component,
Postmortem does not use children process components to
accomplish its goal.

3.1.2. Personal Software Activities. In the PSP, the
planning and postmortem activities d5pend on a personal
software process that includes the phases: planning,
design, code, code review, compile, test, and postmortem.
In our application of the PSP to group projects, we
provide the capability to replace design, code, code
review. compile, and test with other activities. Our
approach is to provide the background for the planning
and postmortem phases as applied to any software related
activities. In a group project, an individual engineer may
not be involved in coding, compiling, and testing, but may
instead work on design and design reviews, or may instead
be a test engineer whose involvement does not go beyond
planning, developing, executing and reporting on tests.
Our assumption is that the analysis techniques that
consider resources (labor, primarily), product measures
and quality assessment all equally apply to any other
software related activities, whether directly developing
code or not. Process improvement should be a center of
focus for all participants in a software process.
At ASU, we have been using this approach to Integrating Personal Processes for group software projects in a
classroom setting and for group independent study
projects. Thus far, uses are for small applications in which
most project members get involved with all of the lifecycle activities.
The primary challenge in generalizing the approach to
large group efforts has to do with product and quality
measures. PSP relies on Source Lines of Code as the basis
for product measures. Software defect management is the
basis for quality, planning, and process improvement.
Engineers using PSP, record defects by type, phase

3.2. Automated Support for Process Artifacts
The Integrated Personal Process uses four artifacts: the
Engineering Notebook, the Project Activity Schedule &
Log, the Project Defect Log, and the Project Plan Summary. We have implemented each artifact as a stand-alone
application. When using the worklist handler of OPC,
enacting one of the Integrated Personal Process Planning
or Postmortem activities may cause the invocation of one
or all of these applications according to the process input
and output specifications.
All four artifacts use a single repository interface to
store and manipulate data. The interface is implemented in
Java, using synchronization to support multiple concurrent
accesses. Highlights of these artifacts are detailed in the
following sections.
3.2.1. The Engineering Notebook. The Engineering
Notebook is an application, which implements some
concept of the Personal Software Process Engineering
Notebook. The Engineering Notebook objective is to
create an engineering notebook that tracks the software
engineers daily time usage. More specifically, as shown in
Figure 3, the Engineering Notebook allows a software

367

repository of process components to determine the
projects that best fit the specified criteria.
For each activity, the project planner estimates a begin
and end time, the development duration, the project size,
and the output products. These are estimated values, thus,
the project planner can use experience to determine the
values, some tools, or historical project data. OPC is
designed to allow add-on functions. For example, the user
can add the tools for estimation or a LOC Counter. The
add-on tools are specified using MIME types.

engineer to define and record, for a given project, its
activities, the time spent on the activities, and product lists
of the activities.
An activity is a unit of work which takes an engineers
time (e.g. interruptions, coding, breaks, lunch, designing,
etc.); it is any work performed by a software engineer.
The time spent on each activity is recorded in an
increment of hours; for instance, a job that takes 15 minutes could be recorded as 0.25 hours, but our usage generally limits granularity to one tenth of an hour (6
minutes). The product list entry allows the engineer to list
products produced by the activity. The initial engineering
notebook is derived from information in the Project
Activity Schedule 8c Log (Section 3.2.2).

3.2.3. Defect Log. The Defect Log (DL) is an
application, which automates Defect Recording to aid in
tracking defects injected and removed. The defect data are
stored in the defect log, which are used as input to
generating the plan summary (Section 3.2.4) in the
postmortem analysis. The Defect Log is realized as a
tabular application where the rows represent the defects
and their information and the columns are classifications
of the defects. As shown in Figure 4, the DL allows its
user to specify the date, the defect type, the injected
phase, the removed phase, the fixed time, and a
description.

-1

Figure 2. Engineering Notebook Main Window
Modification to the times in the engineering notebook
causes the transfer of the times spent and the product list
to the Project Activity Schedule & Log.
3.2.2. Activity Schedule & Log. The Activity Schedule
& Log (ASL) is an application, which aids in developing
project plans. The ASL application allows the project
planner to identify the activities, phases, agents, and times
for the activity. When the user completes the activity
specifications, ASL places the schedule in a persistent
repository.

Figure 3. Project Defect Log Main Window
The date that the defect was discovered and the defect
description can be anything that the user enters. The DL
default defect types are: Documentation, Syntax, Build
Package, Assignment, Interface, Checking, Data, Function, System, and Environment. These can be modified to
allow application of planning and postmortem to any software activity. Additionally, the DL also provides default
phases including: Planning, Design, Code, Review, Compile, Test, and Postmortem. Analogous to the defect types,
these can be changed to accommodate the activity.
Finally, the time required to correct the defect is recorded
in hours and tenths.

An activity is a task of the project. A project may have
a set of activities (process components) representing the
work of all group members assigned activities on the
project. Similarly, all engineers working on a project will
have their own activity schedules, which reflect the lowerlevel activities necessary to complete their input to the
group. The lower-level activities are subject to analysis
and improvements as defined above.
A project planner may also have an ASL to coordinate
the activities and products of a group of engineers. The
Process Broker [15] can be used to determine the kind of
activities that the project may need and to check for the
availability of those process components. To do this, the
project planner first specifies the characteristics of the
current project to the Process Broker. The Process Broker
uses its locating and matching semantic engine and its

3.2.4. Project Plan Summary. The Plan Summary (PS)
is an application, to aid in planning and tracking a
software activity. The plan summary is initialized in the
planning activity and is completed in postmortem. In our
implementation, information in the plan summary is
derived from the ASL. The plan summary can be saved

368

Standardization”, in Proc. of the PCTE’94 Conf. PCTE
Technical Journal No.2, PIMB Assn, November 1994.
[6] East Tennessee State University, Personal Software Process
Studio, (East Tennessee State University, www-

and named so that an engineer who participates in several
software activities (reviews, testing, and coding, for
example) can track data specific to the activity.

cs.etsu.edu/softeng/psp/dlpsps.html).

4. Current and Future Work

[7] K. Gary, “Process Interoperability with Open Process
Components”, Arizona State University, Ph.D. Dissertation,
January 1999.
[8] K. Gary, T. Lindquist, L. Sauer, and H. Koehnemann,
“Automated Process Support for Organizational and
Personal Processes”, in Proc. of the Intl. Conf. on
Supporting Group Work, Phoenix, AZ, 16-19 Nov 1997.
[9] W.S. Humphrey, Introduction to the Personal Sofware
Process (Reading, MA Addison-Wesley, 1997).
101 W.S. Humphrey, A Discipline for Software Engineering
(Reading, MA: Addison-Wesley, 1995).
113 The US-France Technology Research and Development
Project, PUS2 Architecture Specification Version 1.0,
(Lindquist, TE editor) SPAWAR Systems Command, San
Diego CA, January 1998.
121 T. Lindquist, “A Toolset Supporting Distributed Process
Components”, Arizona State University, Technical Report,
TR-97-034, 1997.
[131 T. Lindquist, and J.C. Demiame, “Towards Distributed and
Composable Process Components”, in Proc. of the
European Workshop on Software Process Technology,
June 1997.
[14] M.C. Paulk, B. Curtis, and M.B. Chrissis, “Capability
Maturity Model, Version 1.1”, (IEEE Software, July 1993,
pp. 18-27).
[151 L. Sauer, “Brokering of Process Components”, Arizona
State University, Ph.D. Dissertation, February 1999.
[161 The Workflow Management Coalition. The Reference
Model. WMC Document Number TC00-1003, January
1995.

OPC provides an initial set of tools for defining and
enacting process components. The underlying implementation of OPC provides the framework for wrapping
various process tools for interoperability. We have
achieved initial wrappings of two products, and hope to
soon demonstrate interoperability between these products
in the near future. Thus, a process component can be
defined in terms of sub components each under the
direction of a different enactment engine.
We have used our integrated personal software process
approach in classroom projects and in group independent
studies. The tools described in this paper will be
introduced to these projects. Enactment using OPC is
controlled by a worklist handler tool, which connects to a
repository of process components. Process components
are all represented using Java objects. Until the tool
wrappers are fully functional, enactment involves
launching an application associated with the input and
output products as specified with MIME types.
A few important distinctions differentiate our approach
to integrating personal processes. Engineers are not asked
to cany out a defined process that they themselves have
not developed. Engineers are motivated to use process
improvement techniques, since they directly and solely
apply to their own activities. Product and defect measures
are defined by the engineer and thus problems of
consistency do not arise. Engineer define their own
personal process for the software activities they perform.
These may defined or applied from definitions they obtain
from other engineers.

5. References
[l] P. Armenise, et al., “A Survey and Assessment of Software

Process Representation Formalisms”, Intl. Joumal of
Software Engineering and Knowledge Engineering, vol. 3,
no. 3, pp. 401-426. 1993.
[2] L. Carter, ECEN 4553 PSP Database Browser, University of
Colorado
at
Boulder,
ecewww.colorado.edu/-ecen4553/Reference/psp/examples.html
[3] A. Christie, et al. “A Study into the Current Usage of
Software Process Automation”, in Proc. of the NSF
Workshop on Workflow and Process Automation in
Information Systems, Athens, GA, May 1996.
[4] F. Coallier, “HOWIS0 9001 Fits into the Software World”,
(IEEE Software, January 1994, pp. 98-100).
[SI J.C. Derniame, et al. “Life-Cycle Process Support in PCIS,
Or It Is Time to Think about Software Process Formalisms

369

Towards Target-Level Testing and Debugging Tools for Embedded Software
Harry Koehnemann, Arizona State University
Dr. Timothy Lindquist, Arizona State University

Abstract

The current process for testing and debugging
embedded sojware is ine~ective at revealing errors. There
are currently huge costs associated with the validation of
embedded applications. Despite the huge costs, the most
dl~cult errors to reveal and locate are found extremely late
in the testing process, making them even more costly to
repm”r. This paper first presents a discussion of embedded
testing research andpractice. This discussion raises a need
to improve the existing process and tools for embe&@i
testing as well as enable better processes and tools for the
jWure. To fmilitate
this improvement, architectural and
software capabilities which support testing and &bugging
with minimal intrusion on the executing system must be
developed. Execution visibility and control must come
@om the underlying system, which should ofJer interjbces
to testing and debugging tools in the same numner it offers
them to a compiler. Finally we propose txtenswns to the
underlying system, which consists of adiiitions to both the
architecture and run-time system that will help reulize
target-level tools.
1. Introduction
Software validation involves many activities that
take place throughout
the lifecycle
of soft w are
development.
A substantial
portion
of the validation
process is software testing, which is the development
of
test procedures and the generation and execution of test
eases.
Notice we are not only concerned
with the
generation of a test case, but are also concerned with how
that test is executed. Therefore, a test case is not simply
composed of inputs to a system, but rdso includes any
environmental
factors.
Other research has examined the
issues behind test case selection, but few are addressing the
problems that surround the execution of those test cases.
The goal of this paper is to identify
the problems
associated with test case execution for embedded systems
and to propose solutions for making embedded testing more
effective at revealing errors.
1.1

Testing
and Debugging
Process
Many of the activities, tools, and methods used
during software testing are shared by software debugging.
Software testing is concerned with executing a piece of
software in order to reveal errors, while software debugging
is concerned with locating and correcting the cause of an
error once it has been revealed.
Though
these two
activities are often referenced separately, their ac$ivi$ies are
tightly coupled and share many common features.
Permission to copy without
fee all or part of this material is granted
provided that the copies are not made or distributed
for direct commercial
advantage, the ACM copyright notice and the title of the publication
and its
date appear, and notice is given that copying
is by permission
of the
Association
for Computing
Machinery.
To copy otherwise
or republish,
requires a fee and/or specific permiss~m.

During debugging, a developer must recreate the
exact execution scenario that revealed the fault during
testing.
Not only must the code execute
the same
instruction sequences, but all environmental
variants must
be accounted for during the debugging session. In addition,
the tools assisting
in the debugging
process
must
providing a &veloper
with a certain degree of execution
visibility
and control while not impacting
the execution
behavior of the program.
1.2

Embedded
Systems
The testing and debugging
process is greatly
restricted by embedded systems. Embedded applications are
among the most
complex
software
systems
being
developed today. Such software is often constrained by
● Concaumnt designs
● RcaI-time
constraints
● l%lbedded
target Imvilrmments
● Distributed
hardware ambitectures
● Device control dependencies
Each of these properties of embedded software severely
restrict
execution
visibility
and
control,
which
conseqmdy
restricts the testing and debugging process.
Our current methods and tools for software testing and
debugging require a great deal of computing
resources.
Such resources are not available on the target environment.
Therefore, a large gap exists between the methods and tools
used during evaluation on the host and those used on the
target. Unfortunately,
mauy errors are only revealed during
testing in the target environment.
Because of the above issues, concerns are raised
over the effectiveness of software validation for embedded
systems.
Embedded
applications
are responsible
for
controlling physical devices and their correct execution is
critical in avoiding and/or recovetig
from device failure.
Often these physical devices control life-critical
processes,
making the embedded software a key element of a lifecritical system. Software failure can lead to system failure,
which in turn could lead to loss of life.
In addition, designers are increasing their use of
embedded software to conuol the physical elements of large
systems. This rate of increase is likely to increase as the
cost for embedded controllers becomes cheaper and more
attractive when compared with other mechanical techniques.
Computer
networks
are fast replacing
point-to-point
wiring,
due to the networks
light
weight,
easy
cotilgurability
and expansibility,
and lower
design
complexity.
The advancement
in the complexity
of
problems
addressed
by software
in these types of

01993

ACM

0-89791-621

-2/93/0009--0288

1.50

applications
may soon be limited
satisfy reliability needs and concerns.

by our inability

to

2. Software
Testing
The software testing phase is concerned with
executing a software program in order to reveal errors.
Software testing for embedded systems takes place in four
basic stages:
1) Module Level Testing
2) Integration Testing
3) System Testing
4) Hardware/Software
Integmtion Testing
The first three testing stages are typical of any software
product.
Testing begins with exercising each soft ware
module and concludes when the software is shown to meet
system specifications
by passing some rigorous
set of
system tests. The fourth phase is unique to embedded
systems. The software must not only be correct, but must
also interface properly with the devices it is controlling.
Testing
literature
contains
countless
methodologies,
techniques,
and tools that support the
software
testing process.
They range from software
verification
and program proving
to random test case
selection.
All testing
methods
indirectly
apply
to
embedded systems, as they do all software.
Of particular
interest to this paper are those techniques that address the
problems identified for embedded software - concurrency,
real-time
constraints,
embedded
environment,
etc.
Unfortunately,
there exists little research into the unique
with testing
embedded
software.
problems SSSOCilltd

techniques, must examine a large set of statw and therefore
must constrain itself to small, simple programs.
Research in dynamic testing of concurrent Ada
programs has largely focused on the detection of deadlocks
~emb85],
the saving of event
histories
KeDo85,
Maug85],
and other tec.huiques that passively watch a
program execute then allow the execution sequences to be
replayed after a failure has been detected.
Hanson Eans78] was among the first to discuss
run-time
control of concment
programs.
In order to
regulate
the sequences of events, he assigned each
concurrent event in the test program a unique time value.
He then introduced a test clock that regulated the system
during execution. A given event could only execute if it’s
time was greater than that of the clock.
Tai ~ai86, Tai91] extended Hanson’s work to the
Ada programnn “ng language.
His method takes an Ada
program P and a rendezvous ordering R and produces a new
Ada program P’ such that the intertask communication
in
P’ is always R. A similar approach was used in Koeh89]
to apply these techniques to testing and debugging tools.
This work addressed the facl that in order to,
testa specific
program state, values in a program
may need to be
modified during run-time.
Modification
o~f the program
state is a capability provided by any debugging tool and is
a required property of a tool debugging tasked programs. It
is important to note that both techniques explicitly perform
rendezvous scheduling, removing those decisions from the
run-time system and placing control in the hands of the
tool.

2.1

2.2

Testing
Concurrent
Systems
Concurrency increases the difficulty
of software
testing,
Given a concurrent program and some set of
input, there exists an unmanageably
large set of legal
execution sequences the program will take. Furthermore,
subsequent execution
with the same input may yield
different,
yet correct results due to differences
in the
operating environment.
This is all complicated
by Ada’s
nondetermins tic select construct. Therefore, when testing
concurrent software, we are not only concerned with a valid
resuk but must also be concerned with how the program
arrived at that result.
Since multiple executions of a concurrent program
may yield different results, it is not enough to ensure that
the system produces the correct output for a given input.
One must also ensure that the system always produces an
acceptable output for each execution sequence that is legal
under the language definition.
Without sufficient control
over program execution,
there is no way of ensuring a
given test is exerasing the code it was intended to test.
Taylor and Osterweil
~ay180]
examined static
analysis of concument programs,
However, this research
considered processes in isolation and does not consider
interprocess cmrummication.
Taylor later extended this
work to Ada and a subset of the Ada rendezvous mechanism
~ay183]. Through this static aualysis technique, one could
determine aIl parallel actions and states that could block a
task from executing.
This method, as with most static

Non-intrusive
testing
Intrusion plays a significant role in the testing and
debugging of embedded software. Any technique used to
raise execution visibility
or provide for program control
must not interfere with the behavior of the teat program.
Embedded applications have strict timing requirements and
any intrusion on a test execution will likely make that test
void.
Intrusion
is typical for host-based
testing, but
becomes a large problem
for target-level
testing and
debugging activities.
The above approaches
address the need for
visibility,
control, and predictability
for testing concurrent
software.
However,
they are all intrusive
and use
instrumentation
(inserting probes into a usem program and
rewriting
certain constructs before submission
to the
compiler) to gather run-time information
and to control
~gram
exmtion.
After the probes are added, the user’s
object code is linked with the rest of the debugging system
and then executed under test. This additional
code has a
serious impact on the execution behavior of the program.
Instrumentation
is not appropriate
for testing real-time,
embedded applications.
A non-intrusive
debugger for Ada is proposed in
[Gil188]. A separate processor executes the testing system
and communicates with the target processor through some
special purpose hardware.
Lyttle and Ford ~ytt90]
have
also implemented a non-intrusive
embedded debugger for
Ada. Their tool provides monitoring,
breakpoints,
and

289

of hardware and the run-time system is called upon to
bridge the impending gap. No argument is made as to the
rate of increase identified
by the line slopes; nor is an
argument ma& that these increases are even linear.

display facilities
for executing embedded applications.
While these efforts provide an excellent start towards targetlevel tools, they do have severe limitations.
These
implementation
do not deal with high level activities such
as task interactions and are only concerned with items that
can be translated from monitoring
system bus activity. As
discussed later in Chapter 5, techniques dependent on bus
activity will likely fail for future architecture designs. In
addition,
many of the error detwted
in the target
environment are indeed concerned with high-level activities
(process scheduling
and interactions,
fault handling,
interrupt response, etc.).
Other real-time,
embedded
tools have been
proposed for crossdevelopment
environments.
They can
typically
be classified into one of the following
three
categories: 1) ROM monitors, 2) Emulators, and 3) Bus
monitors.
These types of tools will be further discussed
later in this paper.

c
o
m
P
1
e
x

i~
t

Y

constructs

Hardware

Time
Figure 2.1

2.3 Impact

of the Underlying
System
One of the large problems with testing concurrent
systems
is dealing
with
abstraction.
The Ada
programming
language
abstracts concurrent
activities
through task objects ~D83].
Tasks allow a developer to
abstract the concepts of concurrency
and interprocess
cxmmmnication
and discuss them at a high level. The
burden of implementation
is then placed on the compiler,
and typically the run-time system.
While abstraction
is a powerful
design tool, it
leads to significant
prthlems
during the testing phase of
software development.
Implementation
details become
buried in the underlying system. At the development level,
this high degree of abstraction is appropriate.
However,
abstraction complicates the testing process. Not only are
we concerned with implementation
details, but we must
aIso control them to demonstrate that certain properties
about a program will hold for every legal
execution
scenario.
Without
sufficient
control
over program
execution, there is no way of ensuring that a spedc
test is
exerasing the code it was intended to evaluate. In addition,
cmmxt operation
in one environment
(host) does not
necessarily imply comet operation in another (target) due
to implementation
difference in the underlying system.
The underlying
system is composed of two parts,
the features of the hardware architecture and the operations
provided by the run-time system. As language constructs
become more abstract, compilers are required to generate
more code to implement them. There is no longer a trivial
mapping from language construct to machine instruction.
Rather, the compiler must provide an algorithmic
solution
in order to implement these high level constructs. Those
solutions
exist as operations
in the run-time
system.
Rather than generate code for these constructs, the compiler
generates a call to a run-time system operation or servim.
As the constmcts
become
more
abstract,
compilers
develop
an increasing
dependency
on the
underlying
system. This increase in shown in figure 2.1.
As new constructs
are introduced
to programming
languages, their increase in abstraction is greater than that

L=

Language

Embedded
systems raise many problems
for
software testing and debugging.
Such software typically
must deal with concurrency,
real-time
constraints,
au
embedded
target environment,
distributed
hardware
architectures,
and a great deal of hardware-software
interfaces for controlling
externaI devices. These issues
tdone do not provide a complete view of the problems
SyStetUS are
extcotttttered by embedded testing. bbedded
typically developed on custom hardware configurations
m@ng
that each system introduces
it’s own unique
problems. Tools and techniques that apply to one are not
generally applicable on another, which leads to ad hoc
approaches to integration and system testing of embedded
software. The program is executed for some length of time
and continual y bombarded with inputs in an attempt to
show it adheres to some speeifkation,
3.1

Current
state of embedded
testing
As described
earlier, the testing
process for
embedded systems consists of 4 phases that conclude with
Hardware/Software
(H/S) Integration
During
H/S
integration
testing, device and timing related errors are
reveakd. These errors eneompass problems such as:
● incorrect
handling of interrupts
● distributed
communication problems
● incorrect
ordering of concumen t events
● resource contention
● incorrect
use of device protocols and timing
“ incomect response to failures or transients
These errors are often extremely difficult
problems to fix
and often require significant modifications
to the software
system.
In addition,
software is for~d
to conform
to
custom hardware that may itself have errors. As stated
above, H/S integration
is the last phase of testing for an
embedded system. Since errors are much cheaper to fix the
earlier they are revealed,. why would one wait until the last
phase of product development to reveal the most diftlcult to

290

locate, costly errors to fix? Our goal should be to reveal
these errors as early as possible.
Unfortunately,
target
level testing tools have yet to become a reality.
The target processor of au embedded computer is
typically minimal in function and size. It is only a small
portion of a larger system, whose goals are to minimize
cost and space. Therefore, target hardware of au embedded
systems will not support software development
nor any
development tools. To resolve this problem, the source is
developed on a larger host platform and cross axnpilers and
linkers are used to generate code and download it to the
target processor.
Consequently,
two environments
exist in our
development process, the host environment
and the target
environment,
each having
completely
different
functionality
and interface to a user. Tools that run on the
host provide a high level interface and give users detailed
information
on and control over their program execution.
However, little is provided on the target. Typically,
the
best information
obtainable is a low-level execution trace
provided by an in-circuit emulator.

3.2

Current
Solutions
Approaches to dealing with the above problems
can be divided
into hardware
solution
and software
solutions. The hardware solutions are attempts at gaining
execution visibility
and program control and include the
bus monitors, ROM monitors, and in-circuit emulators. A
bus monitor gains visibility
of an executing program by
observing
data and instructions
transferred
across the
system bus. With a ROM monitor,
debugger code is
placed into ROM on the target board. When a break point
is encountered,
control is transfered to the debug code
which can accept commands from the user to examine and
change the program’s state. Finally, an in-circuit emulator
connects with a host system across an ethernet connection.
At the other end, a probe replaces the processor on the
target board. The emulator then simulates the behavior of
the processor in (ideally)
real-time,
which allows the
emulator to tell the outaide world what it’s doing while it’s
doing it.
The
hardware
solutions
have
mini
m al
effectiveness for software development.
They can only
gather information
based on low-level machine data. The
developer must then create the mapping between low-level
system eventa and the entities defiied in the program. That
-ping
is the implementation
strategy chosen by a given
compilation
system and becomes severely complicated for
abstractions such as tasks Maintaining
an understanding of
the mapping is extremely difficult and cumbersome.
The software solutions cart be viewed as attempts
to reduce the tremendous costs of testing on the target.
Several factors determine how a pitxe of software is tested
1) Level of criticality of software module
Each software module is assigned a different level of
criticality
based on it’s importance
to the overall
operation of the system.
2) Test platform availability

Typically,
there will exist several test environments
available to test a piece of soft ware, each providing
a
closer approximation
to the actual target environment:
● Host-baaed sours
level debugger
● Host-based
instruction set simulator
● Target emulator
● Integrated
validation faality
3) Test Classification
The tests to be performed
can be categorized
to
determine what they are attempting
to demonstrate.
The goal of a test plays a large role in determining
the
platform on which it will execute. Some examples are
shown below.
● Algorithmic
● Inter-module
● Intra-module
● Performance
● HE
integration
● InttX-cabinet
Each of these factors play a role in assigning program
modules to the various test platforms
based on some
criteria that might contain the following:
● Type of software
● Hardware requirements
● Test chssifkation
● Platform
availability
● Coverage requirements
● Test support software
availability (drivers, stubs)
● Certification
Requirements
● Level of effort required for test
This criteria takes into account the 3 factors discussed
above as well as additional ones.
The software solutions are an attempt to minimize
the time spent testing
in the target
environment.
Validation facilities are expensive to build and time utilimd
for testing is expensive. This is due to the f;act that target
level testing occurs extremely late in the development
lifecycle and only a small window is allocated for HAS
integration
testing.
However,
the target is the only
location that can reveal tin
errors. It is ironic that our
current solutions attempt to reduce the anmunt of target
testing, but will likely lead to extensive modifications
and
thercfom extensive retesting.

4.

Problems with Embedded Irestinq
The solutions proposedaboveare not effective at

revealing errors. Effective implies that a technique reveals
a high percentage of the errors and that it does so in a costef!iaent manner. Instead, what the above tools provide is a
minimal, low-level view of the execution of a program and
those tools become available
at a very late stage in
development.
Below is a list of problems associated with
current approaches to embedded testing
4.1

Expense
of Testing
Process
Target
testing
requires
expensive,
custom
validation facilities.
The expense of these target facilities
is incurred for every project, since little
reuse across
projects is ever realized. The effort required to build these
validation
facilities
means that every test execution is
expensive, making retests extremely costly. Yet, hardware
often arrives late and full of errors, forcing software to be

291

modified
and subsequently retested.
This late arrival of
hardware also impacts the cost of an error, since certain
errom are only revealed during I-IN integrations testing.
Perhaps the largest factor associated with the high
costs of testing will be the questions and concerns that
certification processes are beginning to raise about sofhvare
tools. Typically, development tools have not been required
to meet any validation
criteria and certairdy not the strict
criteria imposed on the development system. This luxury
may soon disappear
as the role tools play in the
development
process comes under tighter scrutiny.
The
huge expense of validation
facilities
will
increase
dmmaticauy.

4.2 Level

of Functionality
on Target
The level of functionality
found on a target
machine is minimal and does not support tools. This lack
of functionality
greatly limits the effectiveness of testing,
since more time and effort is required to locate an error.
While a host system provides a high-level
interface and
discussed software in terms of the high-level language, the
target typically deals in machine instructions and physical
addresses. Translating these low-level entities requires time
and a great deal of tedious, error-prone activities.

4.3

Errora
revealed
late
in
development
lifecycle
Embedded
system designs
often incorporate
custom ASIC parta that are typicaIly not available until
very late in the development
process,
delaying
the
availability
of any target validation facility.
In addition,
errors designed into the ASICa are extremely expensive to
fix, requiring
new masks be created and complete
refabrication.
InsteaA errors in ASICs and other hardware
problems are resolved by modifying
the software.
As
stated before, this greatly delays the time which errors are
revealed, which in turn increasing the cost of software
testing.
4.4

Poor
teat selection
criteria
AIl to often, tool availability
diclates the quality
of a testing process.
Tests cases and scenarios
are
determined by what will work on available platforms and
which test are achedulable rather than being determined by
some theoretical
test cxitcria.
A prime example is the
FAA’s requirements ~AAS51 that 1) all testing be done in
the target environment
and 2) testing include statement
coverage.
Of course, test coverage is not currently
measured on the target.
Unfortunately,
it is cheaper for a company
to
spend it’s resources preparing an argument to obtain some
form of ‘waiver” than to actually perform a test. In time,
the argument approach will no longer be accepted and the
solutions
for embedded
testing must be in place to
accommodate
this
change.
It will
only take one
implementation
that performs statement coverage on the
target to force every embedded, real-time software developer
to perform statement coverage on the target to meet such a
certification requirement.

4.S Potential
use in advancing
architectures
Perhaps the largest problem
facing embedded
testing is that the current solutions cannot be applied to
future h~dware
architectures.
Future architectures
are
Proposing
● wider addreas Spaces
● higher -Sor
speeds
● huge numbers of pins
● internal
pipes
● multiple
execution units
● large internal
caches
● multi-dip
modules
Such complexities
cast a dark shadow over the hardware
solutions previously discussed. With internal caching and
parallel activity being done on the chip, one will no longer
be able to gain processor state information
from simply
monitoring
the system bus. And as on-chip functions
become more complex, emulator vendors will no longer be
able to see into the chip through the pins making them
obsolete as well.
In [Chi191] an even stronger claim is made that
the debugging capabilities provided by the chip will need to
become more sophisticated. In future architectures, perhaps
the only possibility
to view and control the execution of
hardware is to gain that information
from the hardware
itself.

The previous
sections raised issues about the
effectiveness of our testing process and claimed that tcating
is currently being limited by tool functionality.
l%e goal
of this paper is to identify shortcomings in the embedded
testing proccas and propose a solution to those problems.
The view taken by the authors is that tool support for
embedded systems is lacking. Further, those approaches
currently used for gaining execution visibility
and control
will soon be obsolete for future architectures.
We propose
adding facilities to the underlying system to better support
testing and debugging tools for embedded software.
As stated previously,
the underlying
system is
composed of the hardware architecture and the run-time
system (RTS). Both are composed of data structures and
operations that implement
common system abstractions
such as processes, semaphores,
ports, timers, memory
heaps, and faultdexceptions.
It should be noted that there
is no distinct line between features of hardware and features
of the RTS. In fact, as these features and abstractions
become more standardized,
newer architectures
are
attempting to incorporate
them into their instruction
sets
~Nl%92].
In addition,
the implementation,of
a feature
may span parts of the architecture,
RTS, and compiler
generated code (i.e. fauhdexceptions).
5.1

Model
Debugging
System
Below is an illustration
of a debugging
system
(Figure 5.1). The data path from the debugging/testing
tool represents symbol table information
that allows the
tool to map machine level information
to source level

292

Test/Debug
Compiler
Generated
Code

\

Tool

1

A
e
x
t
e

3

Ada
Compilation

“s-

Figure 5.1
constmcts.
The
ASIS
toolkit
provides
easy
required for this physical connection.
The
implementation
for this facility.
ASIS is a proposed
sw-tions describe ‘tie architecture
additions
standard interface between an Ada library and any tool
interfaces in more detail.
requiring compilation information.
Of more interest is the communication
path
between the target processor and the testing tool. A tool
sits external to the rest of the embedded system, while the
RTS resides internally on the target board. At frost glance,
this conceptual
path seems rather difficult
to realize.
However, the implementation
becomes easier if thought
about as a typical host debugging system. Any debugging
system has a least two processes executing, one running
the test program and one running the &bugga.
These two
p=
Sa common physical machine, which allows
one process to gain information
about the other.
The
debugger procem simple requires data and computation
time, which it shares with the test program.
This same scenario is rcqnired
for embedded
debugging, except that the debugger process is split. Part
of the debugger process runs on the target machine and part
runs on the host. The goal is to minimize the portion that
must be run on the target so that it does not intrude on
execution
of the test program.
To realize this nonintrusive
execution
of the debug software,
the target
1) Execute debug code only at a break poinL
2) Run the debugger as a separate process, or
3) Provide a separate execution unit to execute the
debugger.
The details of these options are explored in depth later in
this paper.
The problem now lies with the interfa=
between
the embedded part of the debugger (intermddebuggcr)
and
the portion that lies on the host (external-debugger).
The
solution requires hardware additions that will be discussed
later in this paper. A high level view is given in figure
5.2. In this figure, the tool makes logical calls to services
provided by the RTS. These calls are actually implemented
by the debugging system through data passed between the
internal and external debuggers.
Hardware additions arc

1

I

next two
and RTS

I

Figure 5.2

-=

The past decade has seen hug{: advances in
microprocessor
designs.
Several of these advancements
were listed previously and include pipelining
and separate
functional
units.
The concept
of partitioning
a
microprocessor in order to perform parallel activities is of
great interest to this work.
It was noted
earlier
that these parallel
amputations
severely restrict current methods for testing
and debugging embedded systems, since on{e must simulate
a great amount of computations.
However,
debugging
tools can also use architectural
parallelism
to their
advantage. If a hardware design is partitioned successfully
to allow certain activities to occur concurrently,
then the
testing and debugging methodologies
might wish to add

293

their own computational
requirements to the list of parallel
activities.
This section will explore additions to hardware
architectures.
No claim is made as to the costs associated
with these features.
They assured y will require space
(transistors) and possibly even add to the execution cycles
required to implement certain instructions.
6.1 Hardware
Partitioning
of Memory
One primary concern for industry is reducing the
huge volume of retests associated with development.
The
current testing process ensures that errors are revealed late,
which forces retesting
large portions
of the system.
Despite correcting
these problems, industry will still be
faced with software that is constantly changing. Software
is deceivingly
easy to change and often the element of a
system assigned to unknown or “risky” aspects during
design.
Changing software is extremely expensive late in
the development
for critical
systems.
Such systems
typically
have requirement
that an error raised in one
portion of the system won’t interfere
with the correct
operation
of the rest of the system.
Current software
certification
agencies ~AA85]
have several software
restricdons including
● Any modikation
made to a software module forces the
retesting of all other modules operating on that same
physical device.
. All software on a device must be developed under the
highest level of criticality of any module that will
execute on the same device.
Without
the ability
of hardware to guarantee software
boundaries, such requirements must be enforced. However,
these requirements
add a great deal of costs to software
development.
Consequently,
software is often physically
partitioned
based on critical level, rather than design
factors.
Partitioning
software modules based on critical
levels greatly interferes
with the design process.
One
would rather partition
modules based on factors such as
processor utilization
and inter-module
exmnmnication
requirements.
In fact, load balancing and p17XXsSmigration
are techniques
that would not be usable by embedded
system developers unless all software is developed at the
highest critical level.
The solution
to these issues is hardware
partitioning.
Each process should have it’s own protected
address space that is not accessible by any other process.
In addition, sets of processes may wish to share memory.
The processor should tdso provide the capability to restrict
access to segments of memory based on some criteria.
6.2 Computational
Facilities
for
Debugger.
The debugging
system is partitioned
into an
internal debugger and au external debugger. The internal
debugger must physically
exist on the target board and
communicate
with the external debugger through some
dedicated medium. The internal debugger will also require
execution from the target without
interfering
with the

operation of the application
program.
There are two
possible scenarios
● The internal
debugger runs as a regular process on the
-Or
The architecture provides separate facilities to execute
the internal debugger code
In either case, control is transfered to the debugger when a
breakpoint is encountered
In the fiit
scemuio, the debugger is executed by
the processor as any other process.
If the debugger
executes as a low-level process, it would not interfere with
the operation of the rest of the system. However, this is
not a feasible approach.
Most intern-sting errors occur
during peak system loads, which would mean that the
debugger could only execute when the probability
of an
error occurring was low. Another approach wouId be to
execute the internal debugger as a periodic process of high
priority and design the entire system to take this process
into account when determining issues such as scheduling.
The second scenario requires the target processor
to provide some form of computational
facilities.
This
extra execution will certainly
require some amount of
utilization
of architecture
resources such as internal
registers and bus accesses.
The simplest
example of
architecture facilities would be a machine that contirtuaU y
dumps some representation of the instruction it is currently
executing.
This would require a dedicated bus to the
external world (proposed later in this section) and that
additional circuitry be attached to the computation units to
gain access to the current instruction.
The problem
with fis
approach
is that the
processor is not aware of what data is required by the tools
at the other end. Therefore, it must dump everything.
At
high processor speeds, the amount of information
being
sent could become overwhelming.
However, the data could
be faltered and then captured so that a tool could parse it
later and recreate an execution history of the program. The
hardware required for filtering
is not trivial and requires
great speed and storage capaaty to maintain pace with the
target processor.
lle next step is to allow software to dictate the
information
sent by the processor. The functional
unit of
the hardware sed.ing messagea could be implemented
as a
state machine, emitting
different
messages based on its
current state. The default state would be all processor
transactions. Basically, in this eontiguration,
the processor
is performing
the filtering
rather than the external
debugger.
This addition
should
not add much in
complexity to the hardware architecture and would greatly
reduee the wmplexity
of the external debugging hardware.
The final step is to take the (now
stateful)
functional
unit and make it programmable.
Instead of a
state machine, it now becomes a complete functional
unit
within the processor itself.
The internal debugger code
would then be loaded into this portion of the prowssor at
boot time and reside there for the entire execution,
transmitting
and receiving
messages to and from the
external debugger.
●

294

6.3 Hardware
Break
Points
Software break points are intrusive and require
instructions be inserted into the code of the test program.
Conditioned
break points present a more significant
problem, since they require a computation every time they
are encountered to determine if the proper conditions are
met to halt execution.
Such breakpoints are unacceptable
for d-time
programs.
To resolve this issue, architectures need to provide
the capability
to set breakpointa in hardware.
A set of
registers would be classified
as BreakPoint
Registers
(BPR), which the processor would check against the
operands for each instruction.
Two types of breakpoints
are required, data and instruction.
Each data BPRs inside
the processor would be compared with the address of every
data operand for each instruction.
Instruction BPRs would
be compared with instruction
addresses or type. When a
match occurs, a breakpoint
fault would be raised and
control trsnafered to the internal debugger
Upon returning
from a break, the processor is
required
to restart execution
precisely
where it had
terminated.
The state of the processor consists of all it’s
internal registers, including
any pipeline information
and
cache memory. These values must be saved automatically
when a break is encountered.
Another issues is that of conditional breakpoints.
Such breakpoints
require computations
by the processor
that run in the background behind the program under test.
The evaluation of the conditional expression must begin far
enough in advance so that it may complete before the
processor has passed the breakpoint
location.
This
evaluation will require memory accesses, raising additional
problems. The current value of operands in the expressions
must be available to the processor, which might involve
accessing it from memory or cache. Any accesses to
memory must be scheduled in such a manner that they do
not block any resources required by the program under test.
F@dly, the value used must be valid and not in danger of
-g

before the breakpoint.
While the problems raised above seem difficult,
they are not insurmountable.
The extend
debugger must
compile the conditional expression and download the code.
At that point it can determine the scheduktbili~
of this
evaluation by comparing the @e for the conditional to the
other code that will occur in parallel. The user could then
be notified of problems with their additional
breakpoint.
The hardware is responsible for detecting any collisions in
parallel activity
and must not assume the debugger is
always accurate. Any debugger activity intruding
on the
behavior of the test program is important information
and
must be flagged by the processor.
The primary additions required for hardware break
points are additional registers from the architecture and the
logic necessary to compare them with the operands of the
current instruction.
To support conditional
breakpoints,
the processor must provide background
computational
support.
This support could come from a portion of the
processor dedicated to conditional breakpoints, or the code

could be downloaded to the internal debugger, given the
internal debugger support described previously.
6.4 Architectural
Support
for Abstractions
As common programming
paradigms
become
more refined, architectures will begin to inax-porate them
into their instruction
sets. It would be unlikely
that the
only abstractions supported by architectures would remain
simple data types (integer,
real) and their associated
operations (add, subtract, convert). Other abstractions such
as processes,
semaphores,
ports,
timers,
memory
management,
and faults
that are found
in typical
applications
should be supported as well, along with
associated operations on those abstractions.
M&ing
hardware to another level of abstraction
provides huge advantages for testing tools.
As stated
earlier, the architecture must be the basis for emulation
capabilities
and providing
execution visibility.
As the
hardware becomes more aware of programming elements, it
gains the abdity to send more meaningful messages to the
external world. A context switch between processes could
be sent with a single message, rather than the hundreds of
machine instructions it takes to implement the switch.
As the processor becom-es the single point of
visibility,
awareness of the progrdng
environment
becomes important.
A processor with a high-level
understanding
of program ,entities can emit fewer, more
meaningful
- messag-es than a processor
that only
comprehends low-level instructions.
6.5 Dedicated
Bus
Embedded
testing and debugging
require
an
interface that aliows the processor to communicate with the
external world without interfering with the behavior of the
system under test. This physical connection should reside
on the target and interface
extemall y tlhrough
some
detachable
mechanism.
The separation
technique
is
important,
since the external debugging system will be
detached from this connection once the system is placed
into operation.
The execution behavior alf the program
should be independent of whether or not any external tool
is attached.
Assuming an adequate physical connection,
the
next detmminah “on is the protocol across it. ‘Ile following
issues must be addressed
1) At what rate will messrwes need to be sent?
‘Processor speed raises i~teresting problems, since future
speeds might be too quick for external
processing
techniques.
A solution to this problem was discussed
previously where the processor became aware of highlevel program elements.
The goal is to decrease the
number of messages required relative to the number of
machine cycles. 2) How much data is associated with a message?
If an architecture is required to emit large volumes of data
for messages, there may be instanms where the processor
must be suspended to allow the internal
debugging
hardware to catch up to the current processor state.
Higher level messages may compound
the problem,
since more maningful
messages might require more

295

This paper does not address the question of how
these interfaces should be UtdiZSd. Such SllSWerS should be
given by methodologies
and techniques for detecting and
locating
errors in embedded,
real-time
systems.
As
discussed earlier, the lack of these methods has led to
difficulties
for determining
adequate RTS services for
testing and debugging tools, which has forced a different
approach to determine the required operations.
Since the
RTS is in essence offering au implementation
of high-level
abstractions,
services that provide
visibility
into the
implementation
of RTS abstractions should adequately
fidfdl the needs of most testing and debugging techniques.
A standard currently exists for implementing
these
abstractions
in the MRTSI
[ARTE89]
and CIFO
[ARTE91].
In addition, most of the needs for testing and
debugging can be fulfiiled
by these standards. This is not
surprising, since our solution is based on implementation
visibility,
and the MRTSI and CEO are providing
an
implementation
interface. However, it is important to note
that this approach also indicates that implementations
that
support these staudards should require minimal additions to
and debugging tools as prOpOSed
by
akw SUppCWt testing
this paper. Below is a small discussion surrounding each
of these abstractions
and a list of shortcomings
in the
MRTSI and CIFO for testing and debugging.

information.
There is likely a tradeoff between message
level and data volume.
3) Is the connection bidirectional?
Visibility
concerns dictate that state information
travel
However,
methods
requiring
out of the processor.
control
of the executing
program require that state
information
travel the other direction.
Protoczds must be
in place to handle contention across the bus and those
must be extremely well defined, due to the extreme data
rate that could will be emmuntered across the bus.
4) Who is the active element in sending message9?
Either the processor or the RTS must determine the
information
sent from the processor.
The processor
cannot provide all the state information needed, while the
RTS will likely not be able to maintain adequate speeds
for sending messages.
These questions play a role in determining
the interface
between the internal and external debuggers.
A likely
solution would be a master-slave relation, where either the
internal or external debugger regulated the other. This
scenario does not seem likely, since each has such critical
processing concerns. Therefore, each will likely execute
independently,
while communication
is handled via some
bus and protocol.
There does exist a master-slave relationship
in
respect to the bus, howevex.
During program executiw,
the internal
debugger
must ‘ownn the bus, since it’s
processing concerns are the greatest.
It must meet the
message sending deadlines without altering computations
in other parts of the system.
There are points dting
execution where the extend
debugger must aeiz cmtrol.
If the internal debugger cannot allocate the bus to meet the
demands of the extcmal debuggm, the user must be notifkd
that their requested operation
cannot be accomplished
during a real-time execution.
The final determination
is that of the active
element
within
the processor.
There are two basic
approaches to detemnining control of the internal debugging
activities.
In the fiit
the processor is active and becomes
responsible for sending messages to the extend debugger.
‘fhesecondappmachuse
aaspecialdebugge
rportionofthe
RTS to emit messagea, which is loaded into a dedicated
functional
unit within
the architecture.
Tools require
information
maintained
by both the architecture and the
RTS. Perhaps the solution lies between the two where
both the RTS and architecture have the ability to dump
messages, depending on the cmrcnt mquiremcnts dictated by
the external tool.

7.1

Processes
Concurrency
is a common abstraction
used in
embedded systems. A design can be decomposed without
concern for computational
resources, which can then be
determined
by a scheduler
during
run-time.
A&
irqplements concurren cy through tasks and task types. The
CIFO and MRTSI
provide
extensive
tasking
support
includlng
identifieation,
creation
and activation,
communication
through rendezvous, concurmat access to
shared entities,
and support for scheduling
control.
Elements of interest that are not provided by the CIFO or
MRTSI include
● Task State - A developer
must have the ability to query
and modify the task state for each task in their system.
However, a modification
could leave the RTS in an
inanaistent
state. For example, changing a task’s
state from “delaying” to %unning” without removing
it from the &lay queue would place the RTS into a
state that could not be achieved
through
normal
execution. However, the same modification
ability is
available on typical debugging systems and should be
offered by em beddcddebwrgera as Wd.
● Commm&ation
and Synchronization
- A developer
must have the ability tb view and modify eaeh entry
queue to determine the concurrent state of the system.
Again, modifications
could leave the RTS in an
unobtainable state.
● Scheduling
Control
- In addition
to the extensive
operations provided by the CIFO for concurrency
control, a developer must have awess to the dispatch
port (or ports for muhiprqxssor
systems).

7. Run-Time
Svs tern Additions
The RTS requirements
deseribe an interface
between a tool and the underlying
system.
This is a
logical interface requiring substantial hardware support as
outlined
above.
An obvious goal is to minimize
the
required data and computational requhements of the internal
debugger as well as the required communications
between
the internal and external debuggers.

7.2

296

Interrupt

Management

One of our criticism of the current approach to
embedded testing is that timing errors are revealed late in
Interrupts are very related to
the development
process.
timing issues and their correctness is an important element
in embedded testing. Therefore, support for interrupts is
extremely
important
to target testing and debugging.
Faalities
provided through the CIFO and MRTSI would
allow developers
to bind various
interrupt
handling
routines, enable and disable certain interrupts, mask and
unmask interrupts,
and generate software interrupts
all
controlled dynamically duting program a program test.
7.3

Time
Management
As stated earlier,
important” to target testing
target tools require sfilaertt
time. Tools must be allowed
(although
such modifications
results) and the delay Iist of
by the RTS.

timing
issues are extremely
and debugging.
Therefore,
control over issues relating to
to view and modify the clock
might produce undefined
waiting processes maintained

7.4 Memory

Management
Dynamic
memory
is not typically
used by
due to diffldtk%
in dcmonstradng
embedded ti@iC4itiOliS
reliability.
However,
future
systems
will
likely
incorporate
algorithms
that requite dynamic storage. In
addition, memory ~agemcnt
for dynamic allocations is
part of a RTS and should therefore be included in RTS
visibility
and control discussions.
A tool will likely
require that ability to demonstrate an application programs
behavior when memory is exhausted.
The MRTSI would need to be extended to provide
operations that mim
a collection
making it smaller to
show execution behavior when memory is exhausted or
larger to demonstrate correct execution should a collection
be expanded by the developer.
Resizing is not cheap and
could require a gnat deal of computation and data transfers,
depending on an implementation.
7.5

Exception/Fault
Handling
Proper handling of exceptional events is evaluated
during hardwaresoftware
integration
testing. Therefore,
tools require a great deal of cattrol over exceptions and
One must be able to raise an
recovery mechanisms.
exception
or fault during program execution and also
modify handler binding during execution.
Another question of interest might be to locate the
handler for a given fault or exception at a given program
location.
Such information
is not easily gained from the
underlying
system.
The compiler
is responsible
for
handling exception propagation [ARTE89], so &k
“ “ g
the handler from only RTS information
might be an
impossibility
and is at best resolved uniquely for each
compilation
system.

architectural and RTS additions. The architectural additions
will certainly be costly in both time and space, requiring
space (transistors)
on the chip and access to internal
registers and busses that could cause contention and slow
the execution
of other instructions
provided
by the
architecture.
However, the RTS additions
are minimal.
We defined
the needs of testing
as making
the
implementation
details of common system abstractions
visible and then determined the functionality
required to
view and control them.
The ARTEWG’S
MRTSI
and
CIFO provided an outstanding basis for this approach.
The RTS additions are admittedly
weak.
Our
initial goal was to have the methodologies
and techniques
used for testing embedded, real-time
systems drive the
operations
required
by the RTS.
Unfort.tmatcl y, such
methods do not yet exist. As stated earlier, testing and
debugging of embedded, real-time software remains a black
art, with ad hoc methods and techniques. While there has
been much research into the concurrency and distribution
issues, none has examined real-time constraints, embedded
environments,
and other issues relating
to embedded
systems Perhaps the MRTSI and CIFO are sufficient
for
implementing
target level testing and debugging
tools.
However, this question cannot fully be resolved until more
formal methods exist.
Our next step is to evaluate the additions
and
determine their feasibility.
Questions relating the cost of
these additions to au architecture and RTS in terms of time
and space must be answered.
Also, a more complete
mapping should exist between the added feattues and the
impact they have on the desired features.
One can then
make a valid comparison between a feature and the costs
associated with it.
‘l’he embedded contmllermark~
is currently huge,
but has only begun to require the computational
powers
~SOCiKltti With lUiCrOpKXXWOrS. Embedded i@k.i3tiOttS
have traditional
been event driven rather than computation
dependent. Due to their light weigh~ easy con@mbility
and expansibility,
and lower design complexity,
computers
are quickly being chosen over mechanical techniques for
controlling
devices. As this transition continues, the size
and complexity
of embedded
programs
will
grow.
Controllers will not only have strict timing requirements,
but also have significant
computational
needs as well.
This combination
requires new approaches to our current
testing process for embedded systems and therefore, more
effective tools to aid in testing and debugging embedded
applications.
References

[ARTES9]

Ada Run-time
Environment
Working
Oroup,
“A Model
Run-Time
System
Interface for A&m Ada Letters, January,
1989.

[ARTE91]

Ada Run-time
Environment
Working
Group, “Catslogue of Interface Features

s

co nclusions
The goal of this paper is two fold.

The first goal
is to identify defkienaes
in embedded system testing and
raise questions about the future of current tools.
The
second is to propose a solution to these problems through

297

and Options
for the Ada Runtime
Environment,”
Special Edition of Ada
Letters, Fall 1991 (fI).

lyxn6J

Tai, K.C., “&producing
Testing of Ada
Tasking Programs,” IEEB Transactions
on Software Engineering,
1986.

[CHIL91]

Child,
Jeffrey,
“32-bit
Emulators
Struggle with Processor Complexities,n
Computer Design, May 1,1991.

~A191]

DD83]

Department
of Defense,
Reference
Manual
for the Ada Programming
Language,
ANSI/MIL-STD1815a,
United States DoD, 1983.

Tai, K.C., Carver, R.H., and Obaid,
E.E., “Debugging
Concurrent
Ada
Programs by Deterministic
Execution,”
IEEE
Transactions
on
Software
Engineering, January, 1991.

~AYL80]

Taylor,
R.N.
and Osterweil,
L. J.,
“Anomaly
Detection
in Concurrent
Software by Static Data Flow Analysis,”
IEEE
Transactions
on
Software
Engineering, May, 1980.

~AYIJ33]

Taylor,
R.N., “A General
Purpose
Algorithm
for Analyzing
Concurrent
Programs,”
Communications
of the
ACM, ~y,
1983.

Federal Aviation
Association,
Software
Consideration
in Airlx)me
Systems and
Equipment
Certification,
RTCA/DO178A, 1985.
[GILL88]

Gilles, Jeff aud Ford, Ray, “A Guided
Tour Through
a Window
Oriented
Debugging Environment
for Embedded
Real Time
Ada
Systems,”
IEEE
Transactions
on Software Engineering,
1988.

&IAm78]

Hansen, B., ~eproduable
Testing of
Monitors,”
Software-practice
and
Experience, Volume 8,1978.
Hembold,
D. and Luckham,
D.,
“Debugging
Ada Tasking
Rograms,”
IEEE software, March, 1985.

Iw’fJ=l

Intel Corporation,
i960
Architecture
programmer’s
Manual, 1993.

KOEH91]

Koehnemann,
H.E. and LindquisL
T.E.,
“Runtime Control of Ada Rendezvous for
Testing and ~U@llg,”
Procedm - gs of
the 24th Hawaii International Conference
on System Sciences, Volume II, 1991.

Extended
Reference

LeDoux, C, and Parker, D.S., “Saving
Traces for Ada Debugging,”
Ada in Use
Proceedings
of the Paris Conference,
1985.
Km]

Lyttle, D. and Ford, R., “A Symbolic
Debugger for Red-Time Embedded Ada
Software,”
Software
- Practice
and
Experience, May 1990.

@fAUG851

Mauger, C. and Pammett K., “An EventDriven Debugger for Ati”
Ada in Use:
Proceedings
of the Paris Conference,
1985.

298

A Project Spine for Software Engineering Curricular Design
Kevin Gary, Timothy Lindquist, Srividya Bansal, Arbi Ghazarian
Arizona State University
Mesa, AZ 85212
USA
{kgary*, tim, srividya.bansal, arbi.ghazarian} @ asu.edu
Abstract
Software engineering education is a technologically challenging, rapidly evolving
discipline. Like all STEM educators, software engineering educators are bombarded with
a constant stream of new tools and techniques (MOOCs! Active learning! Inverted
classrooms!) while under national pressure to produce outstanding STEM graduates.
Software engineering educators are also pressured on the discipline side; a constant
evolution of technology coupled with a still emerging engineering discipline. As a handson engineering discipline, where engineers not only design but also construct the
technology, guidance on the adoption of project-centric curricula is needed. This paper
focuses on vertical integration of project experiences in undergraduate software
engineering degree programs or course sequences. The Software Enterprise, now in its
9 th year, has grown from an upper-division course sequence to a vertical integration
program feature. The Software Enterprise is presented as an implementation of a project
spine curricular pattern, and a plan for maturing this model is given.

1. Introduction
Hands-on, or applied learning is expected to produce learners more engaged than those
in traditional lecture oriented classes [1]. Significant efforts [2][3][4] in software
engineering and computer science education focus on content taxonomies and bodies of
knowledge. This is not a bad thing, but taken in isolation may lead educators to believe
content coverage is more important than applied learning experiences. There is literature
on project-based learning within computing as a means to learn soft skills and complex
technical competencies. However, project experiences tend to be disjoint [5]; there may
be a freshman project or a capstone project or a semester project assigned by an
individual instructor. Yearlong capstone projects are offered at most institutions as a
synthesis activity, but to steal a line from Agile methods, if synthesis is good, why not do
it all the time?
Project experiences, while pervasive in computing programs, are not a central
integrating feature. Sheppard et al. [6] suggests that engineering curricular design should
move away from a linear, deductive model and move instead toward a networked model:
“The ideal learning trajectory is a spiral, with all components revisited at increasing
levels of sophistication and interconnection” ([6] p. 191). The general engineering degree
program at Arizona State University (ASU) was designed from its inception in 2005 [7]
to be a flexible, project-centric curriculum that embodied such integration (even before
[6]). Likewise, the Software Enterprise was started at ASU in 2004 as an upper-division
course sequence to integrate contextualized project experiences with software engineering
fundamental concepts. The computing and engineering programs at ASU’s Polytechnic
campus merged under the Department of Engineering in 2008, and in 2009 the Arizona

299
c 2013 IEEE
978-1-4673-5140-9/13/$31.00 

CSEE&T 2013, San Francisco, CA, USA

Board of Regents (ABOR) approved a new Bachelor’s degree in software engineering
(BS SE). The first graduating class will be in Spring 2014 and ASU plans to undergo
accreditation review shortly thereafter.
At the course level the Software Enterprise defines a delivery structure integrating
established learning techniques around a project-based contextualized learning
experience. At the degree program level, the Enterprise weaves project experiences
throughout the BS SE degree program, integrating program outcomes at each year of the
major. There are several publications on the manner in which the Software Enterprise is
conducted within a project course (for example, [8][9]]), and we summarize this in-course
integration pedagogy in section 2. The intent of this work-in-progress paper is to describe
extending the Enterprise as a spiral curricular design feature we refer to as the project
spine, and present our plans moving forward to mature and validate this approach.

2. The Software Enterprise Delivery Model
The Software Enterprise is an innovative pedagogical model for accelerating a
student’s competencies from understanding to comprehension to applied knowledge by
co-locating preparation, discussion, practice, reflection, and contextualized learning
activities in time. In this model, learners prepare for a module by doing readings,
tutorials, or research before a class meeting time. The class discusses the module’s
concepts, in a lecture or seminar-style setting. The students then practice with a tool or
technique that reinforces the concepts in the next class meeting. At this point students
reflect to internalize the concepts and elicit student expectations, or hypotheses, for the
utility of the concept. Then, students apply the concept in the context of a team-oriented,
scalable project, and finally reflect again to (in)validate their earlier hypotheses. These
activities take place in a single three-week sprint, resulting is a highly iterative
methodology for rapidly evolving student competencies (Fig 1).

Figure 1. Software Enterprise Delivery (left) Kolb Learning Cycle (right)
The Software Enterprise represents an innovation derived from existing scholarship in
that it assembles best practices such as preparation, reflection, practice (labs), and
project-centered learning in a rapid integration model that accelerates applied learning.
Readers may recognize the similarity of the model (Figure 1 left) to the Learning Cycle
[10] (Figure 1 right). Our work for the past 9 years in the Enterprise has focused on
maturing the delivery process, creating new or packaging existing learning materials to fit
the delivery model, and to explore ways to assess project-centered learning.

300

3. The Software Enterprise Project Spine
An innovation in the new BS in Software
Engineering at ASU has been the vertical adoption of
the Software Enterprise. Enterprise courses are now
required from the sophomore to senior years. This
innovation represents what [6] calls a professional
spine, as the Enterprise serves as an integrator of
learning outcomes for a given year in the major. We
refer to our project-centered realization as a project
spine, where foundational concepts are tied to project
work throughout the undergraduate program. There is
significant
computing
literature
on
projects
(embedded, mobile, gaming, etc.) to achieve learning
or retention outcomes. However, computing lacks a
framework for integrating concepts in a project spine.
The Enterprise is an implementation that moves
students from basic comprehension to applied
Figure 2. ASU Project Spine
knowledge to critical analysis outcomes. In the BS SE
at ASU, program outcomes are described at 4 levels: describe, apply, select, and
internalize. Students must achieve level 3 (select between alternatives) in at least 1
outcome and achieve level 2 (apply) in all others. The program outcomes for the BS SE
include Design, Computing Practice, Critical Thinking, Professionalism, Perspective,
Problem Solving, Communication, and Technical Competence. An example leveled
outcome description for Perspective is given in Table 1. The Enterprise accelerates level
3 outcomes by providing contextualized integrated experiences fostering decision-making
in the presence of soft (communication, teamwork, etc.) and hard (technical) outcomes.
Table 1. Perspective learning outcome for the BS SE at ASU
Perspective. An understanding of the role and impact of engineering and computing technology in
business, global, economic, environmental, and societal contexts.
Level 1. Understands technological change and development have both positive & negative effects.
Level 2. Identifies and evaluates the assumptions made by others in their description of the role and
impact of engineering and computing on the world.
Level 3. Selects from different scenarios for the future and appropriately adapts them to match current
technical, social, economic and political concerns.
Level 4. Has formed a constructive model for the future of our society, and makes life and
career decisions that are influenced by the model.

The project spine (center of Figure 2 in dark hexagons) integrates technical
competencies by assigning projects inclusive of the technical material covered in the
regular computing courses. So for example, junior projects (Software Enterprise III and
IV) emphasize technical complexities in Networks, Distributed Computing, and
Databases, while senior projects emphasize technical complexities in Web and Mobile
computing. The technical “focus area” courses are chosen more based on faculty expertise
and recruitment goals than software engineering outcomes; one can envision many
different areas represented by upper division courses here. These do help address the
concern that an accredited software engineering degree has an application area. A risk we
have not yet addressed is if the technical area impacts the software engineering process,
such as with a soon-to-be-introduced embedded systems focus area.

301

There are 2 additional aspects of integration to the project spine. As summarized in
section 2, the Enterprise integrates software engineering concepts throughout the project
experiences. Students in the sophomore year learn the Personal Software Process [11] as a
means to build individual understanding of time management, defect management, and
estimation skills. They then focus on Quality, including but not limited to testing. In the
junior year Enterprise students focus on Design (human-centered and system design
principles) followed by best practices in software construction, taken primarily from
eXtreme Programming. In the senior year students focus on Requirements Engineering
then Process and Project Management. The final aspect of integration is with soft-skill
outcomes such as Communication, Teamwork, and Professionalism (see Table 1).
Throughout the spine the project experiences are crafted to ensure variations on pedagogy
to address these outcomes. For example, in the freshman year students receive explicit
instruction in teamwork. In the senior year the emphasis is on formal documentation as a
means of communication. In the junior year, students work on service learning projects of
high social impact to address communication with sensitive populations in a humancentered context.

4. Literature Review and Next Steps
The newness of software engineering degree programs means there are few studies of
program adoption. There are examples of program design and lessons learned [5][12][13],
or reflections and recommendations on the software engineering education landscape
[14][15][16][17][18]. These are worthwhile guides but do not offer examples on
evaluation instruments for program adoption. The SE2004 report [4] contains a chapter on
“Program Implementation and Assessment” which discusses a number of key factors in
program adoption, but is geared toward accreditation and not evaluation instruments. A
survey instrument is presented in [19] but is designed for comparison of a large number
of programs and not for adoption challenges. Bagert & Chenoweth [22] survey graduate
programs in software engineering but more as an aggregate counting exercise in
knowledge areas than as an adoption evaluation approach. The 2009 Graduate Software
Engineering project conducted a survey of graduate degree programs [20] and then
produced a comparison report [21] of graduate programs to the GSwE2009 reference
model, which includes data on program characteristics and in-depth profiles from 3
institutions. A recent study is Conry’s [23] survey of accredited software engineering
degree programs. Conry summarizes institutional, administrative, and curricular
(knowledge area) aspects in describing the 19 accredited programs as of October 2009.
Certainly program adoption measures from other engineering programs are also relevant,
though software engineering programs are unique due to the forces discussed in section 1.
Our next steps for the Enterprise-as-project-spine involve defining measures for
adoption impact, and determining how this concept fits with established patterns for
curricular maps in software engineering programs. We plan to use quantitative and
qualitative instruments to evaluate adoption. Quantitative data, such as program size,
institution type, faculty and student backgrounds, can be collected via available resources
(departmental archives or online) and direct surveys. Qualitative data can be collected
through survey instruments and interviews of all stakeholders (faculty participants,
administrators, and advisors). Different instruments may be used at different times to
evaluate “in-stream” attitudes versus post-adoption reflections. Defining and validating
these instruments is a significant area of work going forward.
The SEEK, SWEBOK, and CS2013 guides define taxonomies of knowledge in
software engineering. Taxonomies are useful and the sign of an emerging discipline. We

302

have done initial mappings of Enterprise modules to SEEK and CS2013 content areas,
and plan to elaborate on these mappings. Specifically, we intend to produce CS2013
course exemplars. Further, the SE2004 report includes a section on program curricular
patterns, and we will propose new patterns based on the project spine concept, which we
hope will be timely in light of the in-process revisions to the SE2004 [24].

References
[1] National Academy of Engineering. Educating the Engineer of 2020: Adapting Engineering Education to the
New Century. The National Academies Press, Washington D.C., 2005.
[2] Institute for Electrical and Electronic Engineers Computer Society, Guide to the Software Engineering Body of
Knowledge (SWEBOK). Los Alamitos, CA, 2004.
[3] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society
Joint Task Force (2012). Computer Science Curricula 2013 (Ironman draft). Available at
http://ai.stanford.edu/users/sahami/CS2013, last accessed January 9, 2013.
[4] Association for Computing Machinery & Institute for Electrical and Electronic Engineers Computer Society.
Software Engineering 2004 Curriculum Guidelines for Undergraduate Degree Programs in Software
Engineering. Joint Task Force on Computing Curricula, 2004.
[5] Shepard, T. “An Efficient Set of Software Degree Programs for One Domain.” In Proceedings of the
International Conference on Software Engineering (ICSE) 2001.
[6] Sheppard, S.D., Macatangay, K., Colby, A., and Sullivan. W.M. Educating Engineers: Designing for the
Future of the Field, Jossey-Bass, San Francisco, 2008.
[7] Morrell, D., Roberts, C., Grondin, B. Kuo, C.Y., Hinks, R., Danielson, S., and Henderson, M. “A Flexible
Curriculum for a Multi-disciplinary Undergraduate Engineering Degree.” Proceedings of the Frontiers in
Education Conference 2005.
[8] Gary, K. “The Software Enterprise: Practicing Best Practices in Software Engineering Education”, The
International Journal of Engineering Education Special Issue on Trends in Software Engineering Education,
Volume 24, Number 4, July 2008, pp. 705-716.
[9] Gary, K., “The Software Enterprise: Preparing Industry-ready Software Engineers” Software Engineering:
Effective Teaching and Learning Approaches, Ellis, H., Demurjian, S., and Naveda, J.F., (eds.), Idea Group
Publishing. October 2008.
[10] Kolb Experiential Learning: Experience as the Source of Learning and Development, Prentice-Hall, N.J. 1984.
[11] Humphrey, W.S. Introduction to the Personal Software Process, Addison-Wesley, Boston, 1997.
[12] Lutz, M. and Naveda, J.F. “The Road Less Traveled: A Baccalaureate Degree in Software Engineering.”
Proceedings of the ACM Conference Special Interest Group on Computer Science Education (SIGCSE), 1997.
[13] Frezza, S.T., Tang, M., and Brinkman, B.J. Creating an Accreditable Software Engineering Bachelor’s Program.
IEEE Software November/December 2006.
[14] Hilburn, T.B., Hislop, G., Bagert, D.J., Lutz, M., Mengel, S. and McCracken, M. “Guidance for the
development of software engineering education programs.” The Journal of Systems and Software,
49(1999):163-169. 1999.
[15] Ghezzi, C. and Mandrioli. “The Challenges of Software Engineering Education.” In Proceedings of the
International Conference on Software Engineering (ICSE) 2006.
[16] Lethbridge, T., Diaz-Herrera, J., LeBlanc, R.J., and Thompson, J.B. “Improving software practice through
education: Challenges and future trends.” Proceedings of the Future of Software Engineering Conference, 2007.
[17] Shaw, M. (2000). Software Engineering Education: A Roadmap. Proceedings of the 2007 Conference on the
Future of Software Engineering, Limerick Ireland, 2000.
[18] Mead, N. (2009). Software Engineering Education: How far We’ve Come and How far We Have to Go.
Proceedings of the 21st Conference on Software Engineering Education and Training (CSEET 2009). 2009.
[19] Modesitt, K., Bagert, D.J., and Werth, L. “Academic Software Engineering: What is it and What Could it be?
Results of the First International Survey for SE Programs.” Proceedings of the International Conference on
Software Engineering (ICSE) 2001.
[20] Pyster, A., Turner, R., Devanandham, H., Lasfer, K., Bernstein, L. "The Current State of Software Engineering
Masters Degree Programs", The Conference on Software Engineering Education & Training (CSEET), 2008.
[21] Frailey, D., Ardis, M., Hutchison, N. (eds). Comparisons of GSwE2009 to Current Master’s Programs in
Software Engineering, v1.0. Available at http://gswe2009.org, last accessed January 24, 2013.
[22] Bagert, D.J. & Chenoweth, S.V. “Future Growth of Software Engineering Baccalaureate Programs in the United
States”, Proceedings of the American Society for Engineering Education Conference. Portland, OR, 2005.
[23] Conry, S. Software Engineering: Where do curricula stand today? Proceedings of the National Conference of
the American Society for Engineering Education, Louisville, KY, 2010.
[24] Hislop, G., Ardis, M., Budgen, D., Sebern, M.J., Offut, J., & Visser, W. (2013). “Revision of the SE2004
Curriculum Model.” Panel at the ACM Conference of the Special Interest Group on Computer Science
Education (SIGCSE), Denver, CO, 2013.

303

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

Security Considerations for Distributed Web-Based e-commerce
Applications in Java
Timothy E. Lindquist
Electronics and Computer Engineering Technology
Arizona State University East
http://www.east.asu.edu/ctas/ecet
Tim@asu.edu
Abstract
Today’s distributed e-commerce applications typically
rely upon various technologies in their realization, including the web, scripting languages, server-side processing
and an underlying database. The combination of these
technologies creates a system that requires attention to the
security issues of each component and the system as a
whole. In considering the overall system, issues arise from
the interactions of security frameworks available for each
component. In this paper, we consider the approach and
related issues for distributed e-commerce applications
developed with Java. The flexible nature of Java allows
migration of objects (compiled code with state) through
features such as RMI and Applets. Security for distributed
applications developed in Java has issues and lessons
applicable to systems of components built on different technologies.

1. Problem
Web-based e-commerce and distributed applications are
changing the way we buy goods, access information and
learn. Use of email and other related technology increasingly facilitates collaboration and is more commonly being
used for official communications. Official communications
via the internet are too often done in insecure mode.
Web-based e-commerce applications commonly employ
multiple tiers (3-tier client server architecture) and a combination of technologies such as HTML, XML, JavaScript,
Java (JSP, Servlets), ASP, dynamic html, CGI, and relational databases, as shown in Figure 1. Each of these technologies have separate and in some cases incompatible
approaches to protection against intrusion.
For web-based applications, the communication
between clients and the middle-tier is via web protocol
http. Clients may employ any number of technologies such
as applets, html, xml, and scripts. The middle-tier business

DBMS

legacy appl

view objects/
clients
IGURE 1.

business logic/
middle-tier/
server objects

3-Tier Client-Server Architecture

logic often employs any of a number of CGI work-arounds
such as Netscape’s NSAPI, Microsoft’s ISAPI, WebObjects, ASPs, Java J2EE, servlets and JSP. The combination
of different technologies at each tier, presents special challenges to security of the overall application.
Development time and cost pressures often short-change
security concerns. Problems range from software design
constraints that prevent adequate security to insufficient
testing to exercise common attacks. Often performance
concerns limit the extent to which assurance can be implemented in a web-based application.
Emerging technologies and applications are also presenting new challenges to secure applications. Distributed
Object technologies have been maturing for the past several
years and are being increasingly utilized in web-based
developments. Although some researchers have supported
an approach where an object-web replaces the largely datacentered web of today, this has not materialized. Nonetheless, object technologies such as CORBA, DCOM and Java
RMI enjoy increased usage in distributed web-based applications. Additional frameworks such as JINI, JavaSpaces,
JNDI, and EJB support distributed Java Objects.

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35’02)
0-7695-1435-9/02 $17.00 © 2002 IEEE

transaction
monitor

1

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

Distributed objects are network aware objects. Applications running on remote machines may communicate with
each other at the object-level using this technology. With a
remote reference to a Java RMI object, methods can be
requested of the object (messages) in the same manner as if
the object were local to the executing application. The SUN
implementation of Java (sdk/jdk1.2 and up) includes a flexible and feature rich approach to security providing the
basis for distributed Java applications.

private key

message

algorithm

signature

message

public key

algorithm

signature

verify

2. Security Concerns
Sender
The platform independence of Java has lead to easy
movement of (compiled) code across the internet. While
the approaches of OMG CORBA (see: http://
www.omg.org) and Microsoft do provide multi-language
solutions, they do not provide the same code migration
capabilities as is available with Java. Interacting remote
Java objects may easily be written in a manner that requires
dynamic movement and execution of code (class files)
across the internet.
Security concerns include authentication, integrity and
encryption/decryption. These may all come into play whenever information (code or data) is moved (across a network
or within a single machine).
1. Authenticity allows the receiver of information to
know with certainty the identity of the sender.
2. Integrity allows the receiver to know with certainty
that information transmitted by the sender has not been
modified or tampered with enroute.
3. Encryption is the process of taking data (called clear
text) and a short key and producing cipher data that is
meaningless to anyone who does not know the key.
Decryption is the process of taking cipher data and a
short key to produce the corresponding clear text.
Each of these basic security concerns come into play
with distributed applications, for controlling executing
applications as well as access to information. Figure 2
shows how authentication and integrity can be provided
using digital signatures.
The sender uses his own private key (which must be
kept protected utilizing access control) and together with a
message to generate a digital signature, which is unique to
the message and private key. The message and signature
are transferred to the receiver. The receiver must have a
public key (usually received separately) corresponding to
the sender’s private key. The public key can be used to verify a signature, but cannot be used to generate a signature.
Upon receipt, the message and signature are verified assuring both authenticity and integrity of the exchanged message.

IGURE 2.

3. Digital Signatures
Keys are generated in pairs. The private key is used to
generate the signature and is kept confidential to whoever
is doing the signing. The public key is used by the receiver
to verify authenticity of the message. The signer should
distribute the public key to anyone who will receive signed
information.
The issue as to whether the public key actually corresponds to the sender is resolved with certificates. A certificate represents a chain of trust leading from the sender to
the receiver, indicating that the public key belongs to whom
you want to believe it belongs. If the sender and receiver
both trust the same certificating agency then the chain may
be of length one. Each link in the chain is a certifying
agency (such as VeriSign or Entrust) which certifies that
the entity prior to it in the chain (the owner of the private
key or another certifying agency) is who they say they are.
Users should understand how certificates are signed and
managed. Current web browsers display information about
the certificate and who signed it, but few users ever look
beyond the lock icon on their web browsers. This provides
some opportunity for anyone with a signed certificate to
use a man-in-the-middle attack. Simple possession of a certificate says nothing of integrity, quality or functionality of
code or other information conveyed by the certificate
holder.
Another complication of digital signatures is management of a certificate revocation list. Once a key is known to
be compromised, there must be some way to inform users
that it should no longer be trusted.
The SUN implementation of Java comes with a primitive set of tools for manipulating keys, certificates and digital signatures. It also includes the framework classes (in the
package java.security) for program creation and verification of digital signatures.
Figure 3 includes a sample of Java that may exist for the
sender. The example generates a public and private key pair

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35’02)
0-7695-1435-9/02 $17.00 © 2002 IEEE

Receiver

3-Tier Client-Server Architecture

2

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

//generate the private and matching public key
KeyPairGenerator keyGen=KeyPairGenerator.getInstance("DSA",
"SUN");
SecureRandom random = SecureRandom.getInstance(
"SHA1PRNG", "SUN");
keyGen.initialize(1024, random);
KeyPair pair = keyGen.generateKeyPair();
PrivateKey priv = pair.getPrivate();
PublicKey pub = pair.getPublic();
//create the signature object
Signature dsa = Signature.getInstance("SHA1withDSA", "SUN");
dsa.initSign(priv);
//read the datafile;
FileInputStream fis = new FileInputStream(args[0]);
BufferedInputStream bufin = new BufferedInputStream(fis);
byte[] buffer = new byte[1024];
int len;
while (bufin.available() != 0) {
len = bufin.read(buffer);
dsa.update(buffer, 0, len);
}
bufin.close();
//generate the signature
byte[] realSig = dsa.sign();
//save the signed data in a file
FileOutputStream sigfos = new FileOutputStream("sigOf"+args[0]);
sigfos.write(realSig);
sigfos.close();

FIGURE 3.

Snipet of Java to sign a file

and uses the private key to generate a digital signature for a
data file. The signature is saved to file. This code represents
simplistically what must be done by the sender. The public
key, the signature file and the data file are all transmitted to
the receiver, where a similar program verifies the signature
using the data and public key. The primary vulnerability of
this approach rests in communicating the public key. An
attack may replace the data, signature file and public key if
they are all three transmitted together. Certificates are the
most common mechanism used to assure the public key
authentically identifies the sender. Good practice dictates
that the public key be transmitted separately in an assureable manner. The public key (certificate) is stored by the
receiver for later use to authenticate multiple subsequent
transmissions.
This mechanism can be used to verify the authenticity
and integrity of either data or program code that is transferred in distributed e-commerce applications. The
approach verifies that information came from the purported
sender and was not modified in transmission. Encryption is
necessary to protect information from reading by others
during transmission, as discussed below. Security within an
executing Java application is based upon authenticity and
integrity using digital signatures.

4. Securing Java Applications
Many aspects of Java’s design lend well to distributed
applications. One such example is serialization. The ability
to externalize objects from one executing Java program
(virtual machine) and to read them into another is a process
Java calls serialization. Serializable objects may be transmitted through the internet without loss of object properties, including methods.
To accomplish object externalization, it is often necessary to move the compiled code along with object data.
Several mechanisms exist within Java to do this either
implicitly or explicitly under programmer control. Applets
and Remote Method Invocation (RMI) are two such mechanisms. Applets are small Java programs communicated
from a web-server and executed by a virtual machine running in the browser. RMI, provides the programmer with an
object view of internet objects so that method calls, for
example can formulated as though the object were in the
same virtual machine. RMI capabilities are similar to
Microsoft DCOM and the Object Management Group’s
Common Object Request Broker Architecture (CORBA).
Java’s platform independence is critical to realizing
these dynamic capabilities. Compiled java code (class files)
can move to a variety of platforms and be executed without
loss of meaning. This powerful capability, which has not
been realized to the same level and extent by any other language efforts, was first made generally available by Java
implementations.
Java’s reflection capabilities, allow a program to discover and access the properties available in an object. This
allows internet available objects to be manipulated in a
manner not necessarily know by the program at the time it
was compiled. In addition to facilitating distribution, Serialization, RMI, and Reflection are leading to a view of
internet enabled software service objects. These provide
the critical infrastructure for e-commerce services, such as
financial, investment, and retail purchase.
The current e-commerce solutions utilize the web and
represent a composition of diverse technologies:
1. User interface through html, xml, Java, JavaScript,
Flash and so on,
2. Server functions through dynamic html, JSP, ASP,
J2EE, CGI or servlets,
3. Legacy data through RMI, ODBC or JDBC connectivity to a relational database.
The challenge is to formulate a secure impenetrable
application in light of the combination of a variety of technologies and capabilities.
The Java model for securing the operations in an executing virtual machine has progressed significantly since the

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35’02)
0-7695-1435-9/02 $17.00 © 2002 IEEE

3

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

initial introduction of the sandbox model for Java applications. Initial versions of Java provided full trust to classes
loaded locally and prohibited all sensitive operations from
any code obtained dynamically.
Java now supports a continuum of access control.
Access to system resources (such as files, sockets, runtime,
properties, security permissions, serializable, reflection,
and window toolkit) is granted based on domains. A
domain includes a set of permissions together with a codebase and an indication of who signed the code. The codebase indicates the file or URL from which the code is
loaded. If signed, the alias of the public key can also be
used to define a domain. Each class loaded into a Java virtual machine has an associated protection domain, which
defines the access it has to resources.

{

permission java.net.SocketPermission
"*.GSE.com:2575-",
"accept, connect, listen, resolve";

};

A policy may consist of one or more grants each defining different domains. Each domain may have one or more
associated permissions.
When a protected operation is attempted, the virtual
machine’s security manager performs a security check. It
looks at the classes of all methods currently on the runtime
call stack. Each associated protection domain is queried to
determine whether the operation is allowed. An operation
is performed only if all methods on the runtime stack have
the appropriate permission.
Signing executable is an important application of
authentication and integrity technology. As the number of
distributed applications grows and those applications
increasingly rely upon migration of code, we need assurance that we are granting permissions to trustworthy code.
Today, code signing is largely platform dependent. For
example, applets executed with the Netscape or Internet
Explorer Java virtual machines require use of Netscape or
Microsoft tools to sign the code. Applets designed to run
with the SUN plug-in virtual machine must be signed with
the SUN tools. This lack of consistency only accentuates
the problems arising from utilizing multiple technologies to
realize an e-commerce application.

5. Cryptography
FIGURE 4.

Controlling Access to Java Resources

Figure 4, is taken from the On-line Java Tutorial,
http://java.sun.com/docs/books/tutorial/
and shows the interaction between the security domains
defined in Java2 and the original sandbox model. In the
Java 2 SDK version 1.4, the standard platform has been
further augmented to integrate the Java authentication and
authorization service (JAAS). Doing so takes a step closer
to integrating user login services with authentication mechanisms. See:
http://java.sun.com/products/jaas/
In Java 2, security domains are defined by a policy
granting permissions to the domain. For example, suppose
the company GrowthStocksExpress publishes an applet
on their hypothetical web site at the URL:
http://GSE.com/applets
Assuming the applet needs connections to one or more
hosts having a domain address ending with GSE.com on
ports beginning at 2575, a policy for clients who access the
applet may be:
grant

signedBy "GrowthStockExpress",
codeBase "http://GSE.com/applets"

The SUN implementation of the Java Cryptography
Architecture (Java Crypto Extensions) is freely available
for developing applications that rely on encryption. Similarly, if the application requires an encrypting web server,
Apache-SSL is one of the freely available web servers
based on OpenSSL. It can be freely obtained and used
commercially. See:
http://www.apache-ssl.org/
In addition to authentication and integrity, distributed ecommerce applications require cryptographic services.
Authentication and integrity assure the identity of the
sender and that information was not changed in transmission, but they do not protect against reading during transmission. Encryption is a concern for financial transactions
or other communication where personal identification
information must be transmitted. To guard against this type
of intrusion, many encryption / decryption algorithms and
implementations exist.
Encryption is the process of taking clear text and converting it into cypher data that is unreadable to anyone who
does not know the key. Decryption reconstructs clear text
from the cypher data, using the key. The sender performs
encryption before transmission and the receiver decrypts to

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35’02)
0-7695-1435-9/02 $17.00 © 2002 IEEE

4

Proceedings of the 35th Hawaii International Conference on System Sciences - 2002

reconstruct the information. Several implementations of
various strength exist. Primary issues relate to strength and
performance - time and space to encrypt and decrypt.
Figure 5 is taken from the Java Tutorial and shows the
service provider architecture that is used in the security
frameworks provided by SUN with Java. The API (application program interface) provides a common interface for
developing e-commerce or other distributed applications.
To the extent possible, various alternative approaches to
security are cast into a single interface. Engine classes
abstractly define cryptographic services.
Providers (implementing security services) write to the
lower level SPI (service providers interface). For an example implementation see JCSI [5]. SUN also provides a
default implementation which is distributed with the downloadable extensions. Where multiple implementations
exist, initialization methods select the appropriate implementation based on parameters. This same approach is
used, for example in Java’s database connectivity, JDBC.
Where multiple drivers exist, selection is wired-into the
API through initialization methods.
Although this architecture is a powerful approach that
adds considerable value to the Java framework, in practice
it is very difficult to achieve a single common interface that
works equally well for all implementations.

Authenticity and integrity are just that and no more.
Signed Java can be relied upon regarding who signed it and
that it has not been disturbed in transmission. The fact that
a digital signature has been verified tells the user nothing
about the goodness of the code or the security of the system
that is delivered in signed form. These are elements of trust
in the individual or company that signed the code.
To further the problem, security problems do and will
continue to result from problems in the infrastructure upon
which the Java implementation is built. For example, denial
of service attacks, file access, host system intrusion and
underlying problems with TCP/IP all arise to the applications built on these technologies.
Nevertheless, e-commerce applications must be secure
and the best way to build in security is to use best software
practices and processes for their development. Specification and design of a secure distributed Java application
should include security risks, requirements and underlying
constraints. Development should proceed with a security
risk assessment, followed by design and reviews from risk
perspective. Security testing, which is necessarily different
from specification testing, should consider likely avenues
of problems and exercise documented successful attacks on
similar systems.
For further reading on security problems with Java and
related technology, see:
http://www.cigital.com/javasecurity/articles-1.html
http://www.w3.org/pub/Conferences/WWW4/Papers/
197/40.html
and the Java security website:
http://www.rstcorp.com/java-security.html
For further reading in Security and Encryption, see Peter
Guttmann’s web site [4], which contains references to various research publications as well as software and other
internet resources related to security and encryption.

7. References
IGURE 5.

Service Provider Architecture

6. Closing Remarks
For a language that has developed and whose use has
spread so rapidly, Java’s features are remarkably complete
and consistent. Nevertheless, security in Java applications
is a difficult task. Java security mechanisms are complex
and as such are likely to be inappropriately used by developers. The Java security model, together with the Java
cryptography architecture are powerful tools that are integrated well into the language both in terms of controlling
applications and in terms of defining security frameworks
that are amenable to realization by multiple implementations.

[1.] McGraw, Gary and Felton, Ed; Securing Java, John
Wiley and Sons Inc., 1999, see:
http://www.securingjava.com/
[2.] Griscom, Daniel; Code Signing for Java Applets; see:
http://www.suitable.com/Doc_CodeSigning.shtml
[3.] Campione, Mary, et al., The Java Tutorial, SUN, Addison Wesley, December 2000,
http://java.sun.com/docs/books/tutorial/
[4.] Guttmann, Peter; Security and Encryption-Related
Resources and Links,
http://www.cs.auckland.ac.nz/~pgut001/links.html
[5.] Sun Microsystems Java Security and Crypto Implementation,
http://www.cs.wustl.edu/~luther/Classes/Cs502/
WHITE-PAPERS/jcsi.html

0-7695-1435-9/02 $17.00 (c) 2002 IEEE

Proceedings of the 35th Annual Hawaii International Conference on System Sciences (HICSS-35’02)
0-7695-1435-9/02 $17.00 © 2002 IEEE

5

Automated Process Support for Organizational and Personal
Processes
Kevin Gary, Tim Lindquist, Harry Koehnemann. Ly Sauer
Arizona State University
Computer Science Department
Mail Stop 5406
Tempe, AZ 85287-5406
yfppg@asu.edu
units. Current technology is growing at a rate that can be
difficult to track. Industry investments in desktop tools and
groupware must be leveraged against growth in local and
wide-area networks (LANs and WANs). In particular, the
growth of the Internet makes it possible to envision computer support of global, decentralized, business processes.

ABSTRACT
We propose two views on process: an organizational view
and a personal process view. Information technology applies
Automated Worktiow technology to define, execute, and
track an organization’s automated business processes. Calendaring tools provide a form of personal process view
through scheduled work items. However, the personal, or
individual, view of the process space has largely been
ignored. We maintain that as organizations become increasingly decentralized, a single organization’s process space is
becoming difficult to recognize. Individuals of the organization are asked to do work that spans organizational, functional, and even geographic boundaries. An integrated view
of organizational worktiows and personal processes is
needed to address these new demands. In this paper we
argue for the need to integrate organizational and personal
processes. We then propose a component-based process
modeling approach and supporting process architecture that
integrates these process spaces. Finally, we describe our
recent efforts at developing Java prototype process tools that
realize the proposed modeling technique and supporting
architecture.

These changes in industry and technology escalate the pressures put on information technology research. Providing
computer support for widely distributed organizations using
new technologies such as the Internet, Groupware, Calendar
Management, and Automated Workflow is at least an IT systems analyst’s headache. Determining the best way to integrate these tools to ensure maximum productivity is at best
an IT manager’s nightmare. In our view, the ability to
define, execute, and track business processes is central to the
ability to integrate these technologies in a widely distributed
setting and make their use productive. Therefore in our
research we focus on automated process support. In the
business domain, automating business processes is known
as Automated Workflow.
Workilow is the study of modeling and enacting business
processes by human and computer agents. Automated
Workflow adds an emphasis on applying current computer
and information technology in a workflow environment,
with the desire of automating parts of worktlows, or supporting entire workflows. ’

Keywords: Workilow, Personal Process, Components

1.0 Introduction
Recent changes in industry and technology are imposing
more demanding technical requirements on information
technology. In industry, organizations are downsizing and
becoming increasingly decentralized, often causing projects
to be managed across multiple organizations or functional

Automated Workflow has traditionally focused on defining
and automating business processes from the organization’s
standpoint. Little regard is given to managing overlapping
workflows in an individual workspace, or for even considering the personal processes of an individual when considering the productivity of the organization. The current solution
is to drop a set of personal productivity tools, such as calendaring tools, in the lap of the individual and let her/him
work it out.

Permission to mnke digitalhard
copies ol’nll or part of this material for
personnl or clnwoom
use is granted without fee provided that the copies
nre no1 mnde ar distributed for profit or commercial advantage, the copyright nolice, the title ofthe publication and its date appear, and notice is
given thl copyright is by pennissiou of the ACM, Inc. To copy othenvise,
10 republish, to post on servers or to redistribute
to lists, requires specific
permission nnd/or fee.
GROUP
Copyright

97 Phoenix Arizona
1997 ACM

049791-897-5/97/l

US.4
l...S3.50

221

tectural elements of workflow systems and the interactions
between those elements. The Process Interchange Format
(PIF) Working Group was formed to explore the potentinl to
provide automatic translations between process representation formalisms[l5]. Finally, Microsoft is pushing their
Messaging API (MAPI) as a defacto standard for implementing workflow systems. Microsoft has recently teamed
with Wang to develop the MAPI-WF specification[l7], an
extension of MAP1 for supporting workflow-specific services.

In order to achieve greater productivity from both workflow
and personal productivity tools, a more integrated view of
organizational and personal processes must be considered.
An integrated view allows individuals the ability to develop
their own productive work practices in support of an organization’s processes, and allows for a more natural handling
of processes spanning multiple organizations and individuals. We are in the beginning stages of our research into the
utility of providing such an integrated view. In this paper we
propose an open architecture for integrating organizational
workflows and personal productivity processes. We motivate the need for an integrated approach, and present a component-based approach to process modeling that provides
the interoperability required to achieve the integration. We
also present a suite of tools being developed at Arizona
State University that realize this architecture.

The WfMC is presently the most significant of the efforts
attempting to standardize workflow systems. The WfMC
Reference Model (Figure 1) identifies the basic architectural
components of a workflow environment. At the center of the
model is a Workflow Enactment Service (WES), comprised
of one or more Workflow Engines. A WES provides services through the WAPIs to workflow-related tools. These
include Process Definition Tools for defining processes,
Workflow Client Applications for handling user requests for
work, Third-party Applications that need to communicate
data and operations to the WES, other WESs for providing
interoperability between enactment services, and Administration and Monitoring Tools for data gathering for process
improvement activities.

The rest of this paper is organized as follows. Section 2.0
discusses relevant issues in current workflow and calendaring technology. Section 3.0 argues for an integrated view of
organizational and personal process spaces, presents a component-based approach to process modeling, and proposes a
general process support architecture. Section 4.0 presents
tool prototypes realizing this architecture that were recently
developed at Arizona State University. We conclude in
Section 5.0 with a summary and discuss future avenues for
our research.

The WfMC Reference Model identifies common workflow
system components and interfaces. The WAPI interface
specifications define a set of low-level protocols for synchronously and asynchronously exchanging workflow data
between the tools and the WES. Our basic problem with this
approach is that these protocols are too low-level: they
imply a restrictive workflow model. Workflow representations that cannot easily convert their process data to conform with this underlying model cannot obtain conformance
with the model. This is one of the issues our research
addresses.

2.0 Background
Approaches to developing workflow systems have both
commercial and academic origins. Commercial systems
have evolved from work on forms-based image processing
systems and groupware[l4]. The line between workflow and
other types of systems is often blurred, with groupware,
scheduling, database, and email tools providing some workflow functionality. In addition, several commercial products
that advertise workllow capabilities fall far short of providing full-fledged support for defining and enacting business
processes. Academic research has focused mainly on process modeling and database transaction issues[9]. Process
modeling research has led to the development of workflow
representations based on a variety of formalisms. Database
transaction research focuses on extending traditional,transaction semantics to support long duration[2][9] and/or
cooperative transaction[5][10] models. The result is aproliferation of approaches and issues relating to workHow.

Recent standardization efforts also address the area of calendaring protocols. One popular calendaring protocol
(adopted by Netscape’s Calendar Server[l8]) is the vCalendar protocol[l2]. In the vcalendar protocol, calendaring and
scheduling entities, called events, are transported between
applications that can understand the protocol. This approach
is similar to the effort of the WfMC protocols in that it
defines a low-level data interchange format that tools must
understand to conform to the protocol. Other, more industry-wide standardization efforts are being sponsored by the
Internet Engineering Task Force (IETF) based in part on the
vcalendar specification. The IETF has recently sponsored
the development of three separate calendaring protocols, the
Calendaring Interoperability Protocol (CIP), the Core
Object Specification (COS), and the Internet Calendar

Current efforts are attempting to get researchers and vendors to converge on a common foundation for workflow,
The Workflow Management Coalition (WfMC) was formed
in August 1993 to promote workflow technology. The
WfMC has proposed a reference mode1[22] and a set of
interfaces,
Called WAPIS’ based on that
mode1[24][25][26][27] as an attempt at standardizing archi-

1. For Workflow API and Interchange

222

3.1 Organizationvs. Personal ProcessSpace
Automated Worktlow is the specification and execution of a
business process of an organization[9]. Workflows are modeled as a collection of process steps, or tasks, assigned to
individuals taking on particular roles. Many modem workflow systems work in this way; the process is considered
from a single organization’s viewpoint. This viewpoint is
illustrated in Figure 2.
Organization A

FIGURE 1. WfMC Reference Model ([23])
Access Protocol (ICAP). These protocols specify interface
and other requirements on calendaring systems exchanging
calendaring data.
The standardization efforts in both workflow and calendaring focus on low-level data interchange and protocols for
exchanging such data in a client-server environment. While
this is a widely accepted standardization approach, we fear
that a stable data format is difficult to obtain due to the
maturing of the underlying models in each domain. This is
especially true in worktlow. In the calendaring domain, a
problematic issue is that calendaring formats and tools support only rudimentary dependencies between tasks. These
issues are compounded when integrating workllow and calendaring systems. Workflow systems can write events to
calendar tools, but are not aware of the personal views of the
process of the participating individuals. Likewise, calendaring systems provide a personalized view of work, but do not
possess sophisticated enough models to negotiate with
workllow systems over the ability to do assigned work.

Organization B
FIGURE 2. Organizational Process Perspective

Figure2 shows the process space of two organizations,
generically labeled A and B. These organizations share two
workflows: Workflow 3 and Workflow 4. Interoperability of
the underlying process models and process support architecture is required to allow these organizations to share these
workflows.
The workllow systems we have experienced or seen in the
literature take this organization-centered approach to automating business processes. For example, Action Workllow
from Action Technologies[l] operates on a cyclical model
where workflow units interoperate to produce customer satisfaction. Different participants are viewed as customers,
performers, or observers at each workflow stage of the
cycle. While Action Workflow provides client-side fimctionality to obtain task lists for individuals, it does not provide a structured way for individuals to define personal
processes and integrate them into the scope of organizational processes. Another example is the application of
groupware-oriented tools such as Lotus Notes to workflow[20]. Notes provides much of the needed infrastructure
for managing data and transactions within a worktlow.
However, again there is no structured way to define personal
processes and integrate them into organizational processes.
Instead, the approach is again organization-centered, where
workflows are defined at the organizational scope, and per-

3.0 Integrated Process Support
We advocate an integrated view of an organization’s process
space and the personal process spaces of its individual
workers, In this view, the organization’s workflows are integrated with individual personal process spaces. Section 3.1
discusses this idea in more detail. To support this integrated
view, we advocate a component-based approach to process
modeling that avoids a reliance on low-level data interchange formats. This approach is called Open Process Components, and is described in Section 3.2. Finally, we propose
a generic architecture in Section 3.3 that derives from our
integrated view of process. In Section 4.0 we present some
Java prototype tools based on our ideas.

223

works for Organization B. Jill participates in Organization
A’s workflow 1 and 3. Bob participates in Organization B’s
workflows 3 and 5. In order to accomplish tasks in workflow
1, Jill employs her Personal Process 1. Likewise, Bob
employs his Personal Process 3 in carrying out tasks relevant to Workflow 5. In addition, Bob employs Personal Process 3 to carry out similar tasks in the shared Workflow 3,
Jill does not have a relevant personal process defined for her
assigned tasks in Workflow 3. Finally, each individual may
have personal processes defined that are outside the scope of
an explicit workllow for either organization. These may be
processes defined solely by the individual’s personal productivity initiative.
OrganizationA’s Space
Jill
t PersonalSpace
/
Y
ersonal
1
Process 1
1 Personal
.,
1%

sonal tasks derived from the workflow model. Still other
workflow platforms, such as InConcert[l6], emphasize collaborative aspects of workllow execution. Collaborative
work is closer in spirit to the idea of integrated process
spaces, but differs in that the emphasis is on mechanisms
supporting shared access to data. Users still act on tasks delegated to them by the organizational workflow model.
To keep pace with industry trends and technology impacts,
this organization-centered viewpoint will have to change in
at least the following ways:
l

Interoperability between worktlows developed across
business functional units and/or organizations must be
supported.

l

The potential for wide-area-distributed participation
must be supported.

l

Individuals must have the ability to define, execute, and
track the personal processes they perform to be productive within the context of &I organization’s business processes and goals.

LJ

@z!g&yI
o&f&v3--------

------

The work of the Workflow Management Coalition as well as
research efforts such as our Open Process Components
Framework (see Section 3.2) address the first two issues
directly. However, there has not been a lot of consideration
for the last issue. At best, current workflow systems notify
individuals of new work items through email or custom client applications. Some even have the ability to write to personal calendaring software through interfaces such as
Microsoft and Wang’s MAPI-WF[l7]. But the viewpoint
still originates with the organizational process. An agentcentered viewpoint, showing the distribution of workflows
an individual participates in, and the set of personal prdcesses an individual employs, is not considered.

I

,% ’P
Worktlow 5

Personal
Process 4

Personal

OrganizationB’s Space

Bob

PersonalSpace

FIGURE 3. Personal Process Perspective
There are several reasons for arguing for an integrated view
of organizational and personal prodesses. Figure 3 shows
the overlap of the personal and organizational process
space. Defining and executing business processes is motivated in part by the need to ensure business goals are
achieved. Workilows are largely assumed to be static, repetitive processes that involve rote decision-making in support
of well-defined business goals[9]‘. To expand the scope of
processes automated workflow systems can support, more
dynamic workflows that include personal processes should
be considered. Another motivating reason comes from the
diverse set of relationships in which both organizations and
individuals participate. Individual workers, particularly at
highly skilled levels, perform in a wide variety of diverse
business functions. Downsizing and decentralization of

The need for supporting the personal process view is just
beginning to be recognized in more dynamic process areas
such as Software Engineering[ll]. In the software process
domain, the work of the software developer is considered
dynamic in the sense that the developer must be creative in
seeking the solutions to design, implementation, and maintenance dilemmas[4]. As workfloi extends to more complex and skilled tasks, automated workflow systems will be
required to encompass more than just the straightforward
document-routing capabilities of image processing systems.
Future demands will include the ability to support more of
the skilled, or knowledge work, that people perform in the
organization. In order to do this, workflow systems must
relax the prescriptive constraints it places on performers of
the worktlow, and allow these workers to perform their own
./
personal processes to carry out the work
1..
Figure 3 shows an agent-centered viewpoint of the process
space. Jill is an agent working for Organization A, Bob

1. We refer to Georgakopoulos, Homick, and Sheth’s[9]
trade press characterization of administrative andproduction workflows. Our research is closer to ad hoc
workflows, though our point is they can be better understood through an integrated view of the process space.

224

organizations coupled with increasing outsourcing of work
makes it unrealistic to take the single organization approach.
The business processes of multiple organizations must be
integrated with the personal processes of the participants.
In order to accomplish this integration, we propose a corn- ’
ponent-based approach to process modeling and an open
architecture for supporting personal and organizational process spaces.

meaning management must decide how to map organizational roles to process-specific roles. This mapping is the
relationship between Roles and Agents. The me&model
described briefly here is adopted from the PCIS LCPS metamodel[7]. However, the concepts are similar in a variety of
general descriptions’ of process in the literature[5][9][15][22]. In the OPC Framework, this set of process entities and relationships form the basis for meaningful
component interactions.
The second important aspect of the OPC Framework is a
state-based encapsulation of execution interfaces. By this
we mean each component in a process model possesses a
process state, and this state is manipulable by a set of interfaces to the component that are available during various
stages of executing the process model. Example interfaces
include start, suspend, resume, abort, completeWithFailure
and completewithsuccess. Each component maintains an
explicit, independent state during execution of the process
model, and the state of process execution at any point in
time is the combination of states of the components
involved in the process.

3.2 Component-based Process Modeling
Organizations developing standards in workflow and calendaring focus on low-level data interchange protocols to be
applied in a client-server environment. The development of
such protocols, particularly the protocols related to workflow definition interchange’, are too restrictive to ensure
widespread adoption. Instead, we propose an object-oriented component-based approach to process modeling and
execution. In our research we are developing a componentbased framework for process modeling called the Open Process Components (OPC) Framework. It is not the focus of
this paper to delve into the details of the OPC Framework,
but we do provide a brief discussion relevant to the process
support architecture presented in Section 3.3. Further details
may be found in [8].

The final salient feature of the OPC Framework is a threetiered object-oriented class hierarchy for defining components. An object-oriented methodology provides several
advantages: encapsulation of heterogeneous process representations, an economy of representation through inheritance, and the ability to specialize component definitions
through subclassing. From a process modeling perspective,
one major advantage of the hierarchy is its ability to be
extended. New component definitions and abstractions can
be added within the framework without modifying preexisting definitions. A second important advantage is that
specialized component definitions allow heterogeneous process modeling formalisms to interoperate with one another.
For example, a Petri-net based process model fragment can ‘
interoperate with a process model fragment developed in a
scripting language by encapsulating each as a component
under the framework. This is especially beneficial in the
organizational/personal context of processes we consider in
this paper since it should not be assumed that homogeneous
process models are generated across these contexts.

There is a need for a unifying framework for representing
and manipulating worktlow abstractions. We take an objectoriented approach we call Open Process Components. Entities of the workflow domain are represented as objects, with
manipulations of those objects defined as object behaviors.
The approach is component-based, from the perspective that
interfaces are well-defined so that components interact in
meaningful ways. The OPC Framework provides a foundation for constructing component-based process models in an
extendable fashion.
~
There are three important aspects to the OPC Framework
that allow it to support component-based process modeling.
The first is a me&model that identifies basic process entities and relationships between entities. Basic process entities include Process, Activity, Product, Role, and Agent. A
Process is a decomposable entity into subprocesses and subactivities. This allows development of process models in a
top-down fashion. An Activity is an executable fragment of
a process model; it represents a refinement of a portion of a
process model down to an executable state. A Prodcrct is a
work artifact that is either consumed as input by an Activity
or produced as output. A Role is a process-specific definition of the skill set required to perform an Activity. A Role
is process-specific as opposed to organization-specific,

As a brief example, consider the ad hoc workilow depicted
in Figure4, taken from [9]. This workflow represents a
paper review process. In a component-based process model,
each task in the workilow is represented as an activity component. Interactions between the components is governed
by the set of interfaces each component supports. The benefit is that the implementation of each component is separated from these interfaces. Different process modeling and
enactment services can be used to define and execute the
details of each task. This differs from existing systems
where homogeneous models and supporting services are
employed.

1. More specifically, the Workflow Process Definition Language proposed in WAPI 1[24].

225

port for the personal process space. Figure 6 shows the
proposed general architecture.

The workflow in Figure 4 is a relevant example of the utility
of integrated organizational and personal process spaces,
Consider for example the “Review” tasks in the workflow.
These are assigned to separate persons fulfilling the role of
Reviewer. However, there is not sufficient detail in this definition to automate the support of review activities for each
reviewer. Furthermore, it is not appropriate to believe that
this organizational workflow should provide such detail.
Instead, it is more natural that each reviewer perform a personalized review process that meets the requirements of the
organizational workflow. Therefore, if Jill and Bob were
Reviewers in this workflow, each would carry outthe review
according to her/his own personal process for reviewing
papers, employing familiar tools and methods for producing
the needed results.

FIGURE6. ProcessSupportArchitecture
The architecture in Figure 6 integrates organizations’ workflow servers and personal process servers with calendaring
technology to produce a time-oriented view of work for the
end-user. Arcs indicate the bidirectional flow of components
over the architecture. This architecture extends traditional
workflow architectures, such as the Workflow Management
Coalition’s Reference Model[22], to include the end user’s
personal process space. The components of this architecture
are:
Process Definition Tools

FIGURE 4. Exam le Workfiow

Process Definition Tools are used to create componentbased process models. These tools may query Personal
Process and Workflow servers in order to reuse existing
process component definitions.

(taken from ::9)

Component-based process modeling is at the heart of our
research and relevant to the topics discussed in the rest of
this paper. However, the elements of organizational versus
personal process spaces and process architecture we discuss
do not necessarily rely on a component-based approach.
One can readily envision modifications to existing tools
such as Action Workflow or Lotus 3Jotes discussed earlier
that would address process space integration. We encourage
the reader to consider process modeling approaches and
process space integration issues as independently as possible.

WorkflowServers

One or more servers create the organizational process
space(s). These servers manage component-based workflow models created for organizational units by Process
Definition Tools.
Personal Process Servers

Similar to a Workflow Server, a Personal Process Server
manages process definitions for individuals, created
from components by Process Definition Tools.

3.3 A Process Support Architecture

Calendar Manager

The Calendar Manager is the organizer of an individual’s
process space. The Calendar Manager manages
instances of process models from the individual’s perspective.

To support the integration of the organizational and personal
process spaces, we propose an architecture that extends traditional worktlow client-server architectures to include sup-

226

l

process models. Component-based process modeling allows
for easier integration of organizational and personal process
spaces in the Process Definition Tools and Calendar Managers. Without components, there would be a push on each
tool to support low-level protocols allowing for heterogeneous process models to be integrated. This is just the type
of interoperability that is deficient in cm-rent worktlow systems, and a major motivating force behind the componentbased approach to process modeling described in
Section 3.2.

WorklistHandler/Calendar Tool

This is a client-side tool that presents the individual with
her/his work to do. This may be in the form of a task list,
or may be a time-oriented view depending on process
constraints and personal scheduling preferences.
This general architecture clearly shows the separation and
integration of organizational and personal process spaces.
The distinct servers manage personal and organization processes, This distinction is a logical one; in practice a single
implemented server may include the functionality to manage both process spaces. Integration of the spaces comes
from the Process Definition Tools and the Calendar Manager. A Process Definition Tool creates component-based
process models. By accessing the process definitions on
both servers, the tool is able to create and reuse organizational process that utilizes process specifications of relevant
individuals. The Calendar Manager integrates instances of
organizational and personal processes from the individual’s
perspective. The Calendar Manager has the ability to accept
or decline work requests from process servers, or manage
changes to the individual’s process space when forced to do
so. This tool is the focal point of the individual’s process
space, Finally, the Worklist Handler/Calendar Tool is a
combination of a workflow client and a personal calendaring tool. This client-side tool has the ability to host process
components and support the enactment of such components
in order to carry out the actual work.

We have developed a set of Java tools realizing the proposed
architecture. In the next section we present our progress
with this project.

4.0 The Current Prototype
The YFPPG Research Group at Arizona State University
has sponsored a series of Master’s projects during the
Spring 1997 semester for developing a toolset in Java for
component-based process modeling and enactment. This
toolset conforms closely to the general architecture presented in Section 3.3. The specific architecture is shown in
Figure 7.

Process

Component
Repository

The architecture we propose is an integration of current
worktlow architectures such as the WfMC’s Reference
Model[22] and calendaring environments such as
Netscape’s Calendar Server[l8]. However, current architectures do not take such an integrated view. We know of no
tool that allows for process models to be created that integrate a workflow model and a personal process model. The
proposed process definition tool allows for this integration.
We know of no environment that provides a componentized
personal view of process like the proposed Calendar Manager. One can envision workflow servers writing to an individual’s calendar through an interface such as Microsoft and
Wang’s MAPI-WF interface[l7]. However, this requires
that the workflow server have explicit knowledge and access
rights to individuals’ calendars, The proposed Calendar
Manager explicitly manages an individual’s workspace,
negotiating between servers and individual preferences to
present the personal process space to the end user. The
existence of such a tool enables a component-based architecture that does not require Personal Process and Worktlow
Servers to communicate directly to negotiate over rights to
assign work to an individual.

FIGURE 7. OPC Support Architecture
The components of this architecture are:
l

Process Component Repository

This is implemented as a Java RMI[22] server that uses
Java Serialization facilities to distribute process objects
to client tools. The repository stores component-based
process definitions and distributed components for
enactment. Multiple named repositories, each storing
multiple process models, can be managed by a single
server.

The proposed architecture is process model independent. It
does not favor any particular representation of process.
However, we again advocate the use of component-based

l

227

Calendar Manager Server

Java RMI and CORBA’ versions of this server exist.
This server stores time-oriented appointments as well as
task lists for individuals.
j/Connected to plglet

Repository Browser

~

The Repository Browser is a process administration and
management tool that allows users to browse through the
current objects in a repository. This is implemented as a
Java RMI client.

32 Submlt Request Enactable

Components Editor

rcl

The Components Editor is another Java RMI client. It
allows users to graphically create component-based process models through component creation and reuse.
Figure 8 shows the Components Editor GUI with our
example process definition from Figure 4.

Working Items:

,I

FIGURE 9. Worklist Handler
At this point in the development of our toolset we have yet
to implement the full envisioned functionality of the Calendar Manager Server. The overlap of the organizational and
personal process space occurs in the Calendar Client, which
is responsible for providing the integrated view of the two
spaces. The next step is to implement the full negotiation
between the two servers, as we discuss in the next section,

WorklistHandler

This client-side tool obtains work items for a user from a
repository. The work items are actually Java objects that
are serialized and obtained through Java RMI calls.
Once the Worklist Handler obtains these objects, it can
execute them, changing the state of the process model
and invoking tools on work products. Figure 9 shows a
Worklist Handler for Bob.

We have already learned several lessons during the development and use of this toolset. On the plus side, these tools
successfully demonstrate the integration of organizational
and personal process spaces. These tools are also demonstrations of forward-looking component distribution technologies such as Java RMI[22] and CORBA[19]. Finally,
these tools demonstrate the ntility of component-based process modeling. There have been some hiccups however.
Managing migrating components in a distributed environment is a difficult configuration management problem. It
has proven troublesome to track distributed process components’ states and synchronize updates to process models
stored in the repository. Despite these problems, we are
excited by the possibilities of distributed, component-based
process modeling, and are initiating a new set of projects to
update the current environment. Readers interested in
obtaining the prototypes or tracking progress of this project
may visit the .YPPPG website at http:I&vww.eas.asu.edt~

Calendar Client

The Calendar Client obtains the appointments and task
lists for an’individual from a Calendar Manager Server.
In addition, the Calendar Client can bring up a Worklist
Handler to access the Process Component Repository.
Java RMI and CORBA versions of this tool exist.

stome!Ser\rlce Request

-Y&P&

5.0 Summary and Future Work
In this paper we have advocated an integrated view of organizational workflows and personal process spaces. In this
view, both the perspective of the organization and the perspective of the individual are considered when integrating
process spaces. This view allows organizational goals to be
pursued while allowing individual workers the flexibility to
define how to sccomplish such goals. Such flexibility will
be required in the not-too-distant future due to the increasing demands on current workflow systems and the current
pace of technology.

FIGURE 8. Components @itor

1. Iona Technologies’ OrbiiWeb[l3] was used to implement the CORBA-enabled calendar server and client.

228

Athens, GA, May, 1996.

In this paper we proposed a generic architecture for process
support that logically integrates functionality needed for
both perspectives. We suggest a component-based process
modeling approach to further reduce the dependencies
between workflow and calendaring systems by avoiding the
need for low-level, brittle data interchange protocols.
Finally, we described a set of prototype tools based on component-based process modeling that realizes the generic
architecture. Despite the success or failure of our efforts, we
hope that the argument for integrated organizational and
personal process spaces will have an effect on future considerations in the converging areas of workflow and groupware
research.

P.1 Conradi, R., Liu, C., and Hagaseth, M. Planning Support for Cooperating Transactions in EPOS. Information Sciences, vol. 20, no. 4, pp. 317’336. 1995.

F-1 Curtis, B., Keller, M., and Over, J. Process Model-

ing. Communications of the ACM, vol. 35, no. 9, pp.
75-90, September, 1992.

l7.1

Demiame, J.C. Life Cycle Process Support in PCIS.
Proceedings of the PCTE ‘94 Conference. 1994.

P-1 Gary, K., Lindquist, T., and Koehnemann, H. Compo-

nent-based Process Modeling. Technical Report TR97-022, Computer Science Department, Arizona
State University. May, 1997.

Given the relatively early stage of this research, there are
several avenues we intend to pursue in this area. First, further research is needed to fully understand the nature of the
negotiation between organizational and personal process
spaces that takes place in the Calendar Manager. We are
pursuing research in this area under the topic Process Component Brokering, where such negotiation is carried out by
having the Calendar Manager provide a brokering service
that identifies personal process components that meet organizational process requirements. Second, we are looking at
ways to integrate automated planning and scheduling techniques for workflow and personal processes. The result will
be enhanced Calendar Managers that negotiate with organizations Workflow Servers to optimize the overlap between
organizational and personal process execution. Finally, we
plan to validate the proposed architecture by employing our
tools in real workflow settings, and extending our work into
more dynamic process areas. Specifically, we are looking at
ways to support Personal Software Processes and Distributed Learning processes between mentors and students.

Georgakopoulos, D., Hornick, M., and Sheth, A. An
Overview of Workflow Management: From Process
Modeling to Workflow Automation Infrastructure.
Distributed and Parallel Databases, vol. 3, pp. 119153.1995.
Godart, C., Canals, G., Charoy, F., and Molli, P. An
Introduction to Cooperative Software Development
in COO. International Conference on System Integration, 1994.
Humphrey, W. The Personal Process in Software Engineering. Proceedings of. the Third International
Conference on the Software Process (ICSP-3). IEEE
Press. October, 1994.
12.1 Internet Mail Consortium. vcalendar V1.0 Specification. Available at http://ivww.imc.org/pdi/pdiproddev.html

13.1 Iona Technologies. OrbixWeb 2.0 Programming
Guide. November 1995.
[14.] Khoshafian, S., and Buckiewicz, M. Introduction to
Groupware, Workflow, and Workgroup Computing.

6.0 References
[I.]

Action Technologies, Inc. Coordination Software:
Enabling the Horizontal Corporation. Action Technologies, Inc. White Paper. July, 1994.

[2,]

Alonso, G. and Schek, H. Research Issues in Large
Workflow Management Systems. Proceedings of the
NSF Workshop on Workflow and Process Automation in Information Systems. May, 1996.

[3.]

[4.]

J. Wiley and Sons, New York. 1995.
[ 15.1 . Lee, J. Gnmiger, M., Jin, Y., Malone, T., and Yost, G.
The PlF Process Interchange Format and Framework.
Available at http:/3vwwaiai.edac.u~pi@uZex.html.
May 24,1996.
[16.] McCarthy, D. and Sarin, S. Workflow and Transactions in InConcert. IEEE Bulletin of the Technical
y;pee
on Data Engineering, vol. 16 no. 2. June

Armitage, J. and Kellner, M. A Conceptual Schema
for Process Definitions and Models. Proceedings of
the Third International Conference on the Software
Process (ICSP3), pp. 153-165, Reston, VA. October,
1994.

[17.] Microsoft Corporation and Wang Laboratories, Inc.
Microsoft MAP1 Workflow Framework Concepts and
Facilities . (White Paper). Available at http://
www.wang.com/sb&vP602210.htmn. February 21,
1996.

Christie, A., Levine, L., Morris, E., Zubrow, D., Belton, T., Proctor, L., Cordelle, D., Ferotin, J. A Study
into the Current Usage of Software Process Automation. Proceedings of the NSF Workshop on Workflow
and Process Automation in Information Systems,

[18.] Netscape Communications Corporation. Netscape
Calendar Server 1.0 and 2.0. Available at http://
home.netscape.com/comprooYserver-central/prod-

229

uct/calendar/calenakr2_datahtml.

[lg.]

January 1995.
[24.] The Workflow Management Coalition. Interface 1:
Process Definition Interchange. WfMC Document
Number TC-1016, Version 1.0 Beta. May 29,1996.
L
[25.] The Workflow Management Coalition. Interface 2
Specification. WfMC Document Number TC-1009,
Version 1.0. November 20, 1995.

Object Management Group. Corba 2.0 Specification.
~;g~lable at http:/..www,omg.org/corbaskhtm. July
.

l

[20.] Reinwald, B and Mohan, C. Structured Workflow
Management with Lotus Notes release 4. Proceedings
of the 41st IEEE CompCon digest of papers, pp.451457, Santa Clara, CA. February, 1996.

[26.] The Workflow Management Coalition. Interoperability Abstract Specification. WfMC Document Number
TC-1012, Version l.O..October 20,1996.

[21.] Riddle, W. E. Fundamental Process Modeling Concepts. Proceedings of the NSF Workshop on Workflow and Process Automation in Information
Systems. May, 1996.

[27.] The Workflow Management Coalition. Draft Audit
Specification. WfMC Document Number TC-1015.
August 14,1996.

[22.] Sun Microsystems, Inc. Remote Method Invocation
Specification.
Available
at
http://www.javasof.com:80/product~jdWI.l/docs/guidekmiLspec/
rmiTOC.doc.html

[23.] The Workflow Management Coalition. The Reference Model. WfMC Document Number TCOO-1003,

230

The Journal of Systems and Software 74 (2005) 65–71
www.elsevier.com/locate/jss

Automated support for service-based software development
and integration
Gerald C. Gannod
b

a,*

, Sudhakiran V. Mudiam a, Timothy E. Lindquist

b

a
Department of Computer Science and Engineering, Arizona State University––Main, P.O. Box 875406, Tempe, AZ 85287-5406, USA
Department of Electronics and Computer Engineering Technology, Arizona State University––East 7001 E, Williams Field Road, Building 50,
Mesa, AZ 85212, USA

Received 16 October 2002; received in revised form 1 February 2003; accepted 2 May 2003
Available online 21 January 2004

Abstract
A service-based development paradigm is one in which components are viewed as services. In this model, services interact and can
be providers or consumers of data and behavior. Applications in this paradigm dynamically integrate services at runtime-based on
available resources. This paper describes an architecture-based approach for the creation of services and their subsequent integration with service-requesting client applications.
Ó 2003 Elsevier Inc. All rights reserved.

1. Introduction
A service-based development paradigm, or services
model (Fremantle et al., 2002) is one in which components are viewed as services. In this model, services can
interact with one another and be providers or consumers
of data and behavior. Some of the deﬁning characteristics of service-based technologies include modularity,
availability, description, implementation-independence,
and publication (Fremantle et al., 2002). In the servicebased development paradigm, a primary focus is upon
the deﬁnition of the interface needed to access a service
(description) while hiding the details of its implementation (implementation-independence). Since the client
and service are decoupled, other concerns such as side
eﬀects become non-factors (modularity). One of the
potential beneﬁts of using a service-based approach for
developing software is that at any given time, a wide
variety of alternatives may be available that meet the
needs of a given client (availability). As a result, any or
all of the services may be integrated with a client at runtime (published).

This paper describes an architecture-based approach
for the creation of services and their subsequent integration with service-requesting client applications. The
technique utilizes an architecture description language
to describe services and achieves run-time integration
using current middleware technology. The approach itself is based on a proxy model (Gamma et al., 1995) and
involves the automatic generation of ‘‘glue’’ code for
both services and applications. The Jini interconnection
technology (Edwards, 1999) is used as a broker for
facilitating service registration, lookup, and integration
at runtime.
The remainder of this paper is organized as follows.
Section 2 describes background material in the areas
of software architecture and the middleware technology we are using to enable dynamic integration (i.e.
Jini). The proposed approach for constructing services and developing service-based applications is presented in Section 3. Section 4 discusses related work,
and Section 5 draws conclusions and suggests further
investigations.

2. Background
*

Corresponding author. Tel.: +1-480-727-4475; fax: +1-480-9652751.
E-mail address: gannod@asu.edu (G.C. Gannod).
0164-1212/$ - see front matter Ó 2003 Elsevier Inc. All rights reserved.
doi:10.1016/j.jss.2003.05.002

This section describes background material on software architecture and Jini.

66

G.C. Gannod et al. / The Journal of Systems and Software 74 (2005) 65–71

2.1. Software architecture

3. Approach

A software architecture describes the overall organization of a software system in terms of its constituent
elements, including computational units and their
interrelationships (Shaw and Garlan, 1996). In general,
an architecture is deﬁned as a conﬁguration of components and connectors. A component is an encapsulation
of a computational unit and has an interface (e.g. port)
that speciﬁes the capabilities that the component can
provide.
Connectors encapsulate the ways that components
interact. A connector is speciﬁed by the type of the
connector, the roles deﬁned by the connector type, and
the constraints imposed on the roles of the connector. A
connector deﬁnes a set of roles for the participants of the
interaction speciﬁed by the connector. Components are
connected by attaching their ports to the roles of connectors.
Another important concept is an architectural style.
An architectural style deﬁnes patterns and semantic
constraints on a conﬁguration of components and connectors. As such, a style can deﬁne a set or family of
systems that share common architectural semantics
(Medvidovic and Taylor, 1997).

This section describes the service-based development
approach including the techniques used for deﬁning
services, specifying client applications, realizing integration, and generating glue code.

2.2. Jini
The primary enabling feature of the work described
in this paper is the existence of Jini (Edwards, 1999) for
the delivery and management of services. In a typical
Jini network, services are provided by devices that are
connected to the network. A Jini technology layer provides distributed system services for activities such as
discovery, lookup, remote event management, transaction
management, service registration, and service leasing.
When a service is plugged into a Jini network, it becomes registered as a member (e.g. service) of the network by the Jini lookup service. When a service is
registered, a proxy (Gamma et al., 1995) is stored by the
lookup service. The proxy can later be transported to
the clients of the service. Other network members can
discover the availability of the service via the lookup
service. When a client application ﬁnds an appropriate
device, the lookup service sets up the connection. In our
approach to component integration, we use Jini to
provide a standard method for registering and connecting a client to corresponding software components
that are acting as services.
One of the advantages of using this Jini-based integration technique is that it facilitates construction of
applications ‘‘on-the-ﬂy’’ whereby components can be
used on an as-needed basis. One of the disadvantages is
that clients of services must have some prior knowledge
about how to use each respective service.

3.1. Example
Fig. 1 shows a network monitoring system that provides a network administrator with a constant update on
the health of systems in a network. This application
utilizes a network sniﬀer service and a port monitoring
service. The network sniﬀer service gives an administrator information about traﬃc on the network. The
port monitoring service provides information about the
open ports on the various machines on a network. Together, these services facilitate determining whether
certain kinds of attacks (such as ping storms) are being
directed to a machine or machines. The client application supports analysis of several networks, each of
which is accessed using the buttons shown on the top
portion of the GUI. From the standpoint of distribution, this application demonstrates the use of services
that utilize diﬀerent models of execution (strict call return and data streams). The remainder of this section
refers to architectural speciﬁcations that were used in the
construction of this example.
3.2. Overview
The methodology that we have developed follows
closely the model suggested by Stal (2002) for web ser-

Fig. 1. Running example.

G.C. Gannod et al. / The Journal of Systems and Software 74 (2005) 65–71

vices, although the technology that we are using to
realize our approach is Jini. The approach itself focuses
on two concerns with respect to software reuse. That is,
it addresses both for reuse and with reuse concerns. With
respect to for reuse, the approach involves the construction of services via the use of adapter and proxy
synthesis. Speciﬁcally, the methodology involves two
steps for creating services as follows: (1) speciﬁcation of
components as services, and (2) generation of services
using proxies via the construction of appropriate
adapters and glue code. These services are consequently
registered and made available on a network.
With respect to with reuse concerns, the approach
involves the construction of applications using services
as follows: (1) speciﬁcation of a client to make use of
services from a repository or network, (2) generation of
the client (both manual construction of client application speciﬁc code and automated generation of glue
code), and (3) execution of the client, including integration of the speciﬁed services at runtime.
Within our approach, a user (e.g. developer) is
responsible for writing the source code for the client
application along with the speciﬁcation of the architecture for a client. Among other things, the client speciﬁcation contains a description of the basic services that
the client application will need in order to be a complete
system. All other source code, including code necessary
to realize the connections between the client and employed services, is generated based on the speciﬁcations
describing clients, services, and connectors.
3.3. Service generation
In this section we describe some of the issues related
to automating the creation of service wrappers. To
support these activities, we have developed an automated tool that takes as input a software architecture
and produces glue code. A primary source of reusable
components that we employ in our approach are legacy
command-line applications (Gannod et al., 2000). In
order to generate services from legacy components, we
take the approach of wrapping the components by utilizing the interface provided by the component. Since
command-line applications have a well-deﬁned input
and output interface, the interface of the application as a
service can be based entirely upon the knowledge of
what the application intends to provide.
3.3.1. Speciﬁcation and synthesis
The concept of using an adapter for wrapping legacy
software is not a new one (Gamma et al., 1995). As a
migration strategy, component wrapping has many
beneﬁts in terms of re-engineering including a reduction
in the amount of new code that must be created and a
reduction in the amount of existing code that must be
rewritten.

67

In regards to wrapping components, our approach
uses two steps. First, a speciﬁcation of the legacy software as an architectural component is created. These
speciﬁcations provide vital information that is required
to deﬁne the interface to the legacy software. Second,
the appropriate adapter source code is synthesized based
on the speciﬁcation.
3.3.2. Speciﬁcation requirements
To aid in the development of an appropriate scheme
for the wrapping activity, we deﬁned the following
requirements upon speciﬁcations. These requirements
are as follows: (S1) a suﬃcient amount of information
should be captured in the interface speciﬁcation in order
to minimize the amount of source code that must be
manually constructed, (S2) a speciﬁcation of the interface of the adapted component should be as loosely
coupled as possible from the target implementation
language, and (S3) the speciﬁcation of the adapted
component should be usable within a more general
architectural context.
The requirement S1 addresses the fact that we are
interested in gaining a beneﬁt from reusing legacy software. As a consequence, we must avoid modifying the
source code of the legacy software. At the same time, we
must provide an interface that is suﬃcient for use by a
target application. To provide that interface, a suﬃcient
amount of information is needed in order to automatically construct the adapter.
Our selection of command-line applications addresses
the modiﬁcation concern of requirement S1 since source
code is not available. As such, we are required to provide an interface that is based solely on the knowledge of
how the application is used rather than how it works.
Table 1 shows the properties used in the speciﬁcation
of services, clients and connectors. A service component speciﬁcation consists of two parts: properties and
ports. The properties section describes style of the service, while the ports section describes functions provided
by the service. In addition, the service speciﬁcations
indicate style-based information as well as conditions
or commands that need to be true or executed, respectively, in order to establish an environment necessary to
use the service. Finally, a key in terms of a ‘‘service
type’’ (e.g. interface property) is used to support a service lookup, which is later utilized during application
integration.
The requirement S2 (i.e. the decoupling of a speciﬁcation from a target implementation language) is based
on the desire to apply the synthesis approach to a variety
of target languages and implementations. In addition,
this requirement facilitates enforcement of requirement
S1 by ensuring that new source code is not artiﬁcially
embedded in the speciﬁcation. While satisfying this
requirement is ideal, we found in our strategy that a
certain amount of implementation dependence was

68

G.C. Gannod et al. / The Journal of Systems and Software 74 (2005) 65–71

Table 1
Properties
Group

Attribute

Description

Service properties

Component-Type

Architectural style this component adheres to

Service port properties

Signature
Return
Cmd
Pre
Post
Interface
Path
Port-Type
Shared-GUI

The port’s signature
The port’s return type
The command-line program being wrapped
Pre-processing command
Post-processing command
The generic interface implemented by this port
Path to the wrapped command-line program
The port’s type based on the Component-Type
Boolean indicating shared (true) or exclusive (false) GUI

Client properties

Part-of-client
GUI-CodeFile
Component-Type
Shared-GUI

Identiﬁes inclusion in client application
The ﬁlename for client’s GUI code
Architectural style this component adheres to
Boolean indicating shared (true) or exclusive (false) GUI

Client port properties

Port-Type
Interface

The port’s type based on the Component-Type
The generic interface that this port can bind with

Connector properties
Connector role

Connector-Type
Prop-type

Architectural style this connector adheres to
The connectors role based on the Connector-Type

necessary due to the fact that our implementation would
make use of Jini.
When a component has been wrapped using our
technique, an interface is deﬁned that facilitates the use
of the source legacy software as part of a new application. However, as indicated by requirement S3, it is also
desirable to be able to use the speciﬁcation of the
adapted component within a more general architectural
context. That is, it is advantageous to be able to use the
speciﬁcation as part of the software architecture speciﬁcation for new systems. In using a content-rich speciﬁcation, where interfaces are deﬁned explicitly, the
added beneﬁt of providing information that can be
integrated into an architectural speciﬁcation of a target
application is gained.
In order to realize the requirements placed upon
desired interface speciﬁcations for legacy software
wrappers, we used the ACME (Garlan et al., 1997)
architecture description language (ADL). Speciﬁcally,
we used the properties section of the ACME ADL to
specify the interface features described earlier (e.g. Signature, Command, Pre, Post, and Path). ACME is an
ADL that has been used for high-level architectural
speciﬁcation and interchange (Garlan et al., 1997).
3.3.3. Synthesis
As stated earlier, the class of legacy systems that we
are considering are command-line applications (Gannod
et al., 2000). Given this constraint, we make the
assumption that any client applications utilizing the
wrapped components have a certain amount of knowledge regarding the interface of that wrapped component. We ﬁnd this assumption to be reasonable due to
the nature of legacy software migration where legacy

applications have an organizational history with wellknown usage proﬁles.
In our approach, the speciﬁcation that is needed to
generate wrappers contains properties associated with
the ports as shown in Fig. 2. These properties include
Signature, Command, Pre, Post, Path, Interface, and
Return. In this case, the speciﬁcation describes the
NetworkSniﬃng and PortMonitor services, which are
services created by wrapping tcpdump, and nmap,
respectively. In the synthesis process, ACME speciﬁca-

Fig. 2. ACME services section.

G.C. Gannod et al. / The Journal of Systems and Software 74 (2005) 65–71

tions are combined with a standard template that
implements the setup routines that are required to register a service on a Jini network. In addition to synthesizing the appropriate wrapper, the support tool that we
have constructed to automate this process generates the
appropriate source code for facilitating interaction between a potential client and the wrapped component. At
present, this is an automated tool that generates fully
executable code for the wrapped application and does
not require the user to modify or write any new code
outside of option GUI code.
Both the service and client synthesis steps utilize a
template-based approach to synthesize code. That is, a
standard ﬁle has been created that has stubs containing
place holders that must be instantiated with either service or client speciﬁc parameters. Fig. 3 contains a
portion of the ServiceTemplate ﬁle which contains all of
the application and service independent source code and
provides the routines necessary to integrate the legacy
code into a Jini network. Speciﬁcally, the ServiceTemplate contains functions that implement the discover and
join protocol for registering a service with the lookup
service. The ServiceTemplate also contains tags that are
place-holders for the automatically generated functions.
For instance, in Fig. 3 the tag <put-ServerName> is
a place-holder for the ﬁnal name of the adapter component.
In addition to the ServiceTemplate, there is also a
reusable set of functions that can be utilized in an
interface speciﬁcation and consequently in the generated
wrappers. For instance, the getOutputStream( )
routine (shown in Fig. 4) is available as a function for
use within the Java code to provide standard stream
input support.

Fig. 3. Excerpt of the service template.

Fig. 4. Sample library routines.

69

The amount of automation that has been achieved
through the approach described above is dependent on
the degree of graphical user interface (GUI) support
that is desired. For a service, the code synthesis step can
be fully automated if no GUI support is desired.
Otherwise, the amount of manual code construction is
limited to GUI support.
3.4. Client generation
Once the services are generated and stored in a
repository, a client application can be architected. First
we need to specify the client application taking into
account the architectural style of each of the services.
Once a client is speciﬁed, it can be veriﬁed and generated. In this subsection we look at the requirements for
specifying the client and then describe synthesis of the
client.
3.4.1. Speciﬁcation
Refer again to Table 1 which, in addition to the
properties for service speciﬁcations, contains the properties of client application components and connectors.
When dealing with integration at the component level,
two issues arise (among others) that are of interest. First,
the problem of architectural style mismatch (Shaw and
Garlan, 1996) occurs when the underlying assumptions
made by components conﬂict. Second, most modern
applications provide a graphical user interface (GUI). As
a result, integration of oﬀ-the-shelf components can
leverage these user interfaces in order to take advantage
of previously built technology. To cope with these issues
we impose two requirements on the speciﬁcation of client
applications as follows: (C1) the speciﬁcation of the
components should capture the notion of architectural
style so that the high-level interaction between clients
and services can be veriﬁed, and (C2) the speciﬁcation
must facilitate the use of shared and exclusive GUI
components.
The requirement C1 addresses the fact that a component must provide a notion of architectural style. A
component’s style plays a very important role when it
interacts with other components by imposing interaction
constraints. Using a basic style attribute (by name)
architectural mismatches can be determined by simple
keyword matching.
Requirement C2 addresses the fact that a service may
provide a GUI that allows a user to access and control
the service. In this context, there may be GUI components provided by services that are either sharable by
other services or exclusive to the service. A sharable
GUI component can be used by both the client as well as
other integrated services while an exclusive GUI component can only be used by the service that provides the
interface.

70

G.C. Gannod et al. / The Journal of Systems and Software 74 (2005) 65–71

3.4.2. Synthesis
The second stage of our approach involves the synthesis of application code. Fig. 5 shows a sample speciﬁcation of a client. The information contained within
client speciﬁcations are used to support the synthesis of
client code. This synthesis step utilizes two features; ﬁrst,
the information regarding connectors and attachments,
such as those shown in Fig. 5 are used to determine the
relationships between client applications and desired
services. Second, information regarding GUIs provided
by services is used to determine how to realize the GUI
in a client application.
In our framework, the wrappers for the various services can implement a common interface that allows the
client to get a handle on the shared and exclusive components of a GUI. Shared components are potentially
used across multiple services and are identiﬁed using a
name taken from a standard GUI vocabulary (for
example ‘‘ResultsWindow’’). The name is then used to
identify which GUI components can be shared across
services. Such shared components facilitate the integration of the GUI components by allowing reuse of widgets that provide the same functionality. An exclusive
component is independent and cannot be shared between services. The exclusive GUI components of the
wrappers are used as is but may interact with one or
more of the shared components. For both shared and
exclusive components, the interaction with the client
GUI and application is seamless since the wrappers

handle direct interaction with the services while the client need only interact with the wrappers.
3.5. Discussion
As stated in Section 1, the service-oriented domain
are characterized by modularity, availability, description, implementation-independence, and publication. As
a result, services and service-based approaches are more
coarse-grained and more loosely coupled than components used in traditional component composition techniques. The approach described in this paper utilizes a
software architecture to specify applications that operate under these characteristics. As such, a software
architecture in this context deﬁnes components, their
interfaces, and the mechanisms by which services (as
components) can be joined in order to fulﬁll needed
software behavior. Consequently, services enable the use
of a software architecture as an integration vehicle in
which the architecture facilitates generation of glue
code. It is the very fact that services adhere to the
characteristics described above that the integration and
code generation become possible at this level. However,
the approach does lack in its ability to address needs
that are more speciﬁc than what individual services
provide. To cope with this, we are developing an approach that allows for the creation of federated services,
where services are combined to meet some higher-level
objective.

4. Related work

Fig. 5. Portion of ACME client speciﬁcation.

Recently, the use of web services has gained attention
with vendors releasing webservices toolkits that allow
for building and using webservices. Webservices and
.NET (Meyer, 2001) are based on the SOAP and XML
(Seely and Sharkey, 2001) protocols. The Jini approach
to service integration goes beyond what the webservices
paradigm provides by deﬁning how services can be used
within a larger application context and providing support for code transportation.
FIELD (Reiss, 1990) is one of the classical approaches to tool integration built using a central server
that distributed messages to other tools that were
interested in them. It is a message-based broadcast system that sends message strings between the tools selectively (selective broadcasting). In this sense, this
approach is a precursor to service-based development.
Urnes and Graham (1999) describe an approach to
facilitate the use of groupware in a distributed environment by using architectural annotations. In this approach, they achieve distribution by partitioning the
component space across a network. In our approach,
services are potentially developed by diﬀerent organizations and thus the choice of what to distribute is not

G.C. Gannod et al. / The Journal of Systems and Software 74 (2005) 65–71

available. The component model being addressed by
Urnes and Graham, as such, is ﬁner-grained and violates implementation-independence, a tenet of servicebased development.
Grundy et al. (2000) discuss issues and experiences in
constructing component-based software engineering
environments. They created a variety of useful software engineering tools using their tool set (JViews,
JComposer, etc.). They use ‘‘plug and play’’ and an
event-based composition approach to achieve component integration. In this framework, components are
more tightly coupled and their granularity is ﬁnegrained. In contrast, our approach is based on dynamic
integration of coarse-grained services that are loosely
coupled.
Mezini et al. (2000) proposed pluggable composite adapters for expressing component integration and
component gluing. This creates a clean separation of
customization code from application and framework implementations and thus results in better modularity, extensibility and maintainability. This work
provides a potential strategy for dealing with component mismatches, which is currently ignored in our
approach.

5. Conclusions
The web-based services paradigm has gained attention
recently with the development of technologies such as
SOAP (Seely and Sharkey, 2001). The beneﬁts of such
technologies has obvious advantages such as application sharing, reuse, and inter-operability between organizations. Services extend these beneﬁts by providing
facilities for on-the-ﬂy integration and component introspection. In this paper, we described an approach for
addressing component integration via the use of services
in the context of Jini interconnection technology. Speciﬁcally, the approach utilizes synthesis to generate code
necessary to realize component integration. To facilitate
integration, the ACME ADL is used to specify both
services and target applications, and is used a medium
for performing service compatibility checking.
We are currently developing an environment that will
assist in the creation of applications within the servicebased paradigm and will support service browsing to
facilitate application design. In addition, we are investigating approaches for allowing services to collaborate
beyond the scope of a client application in order to
create federated groups of services. Furthermore, we are
developing technologies similar to the ones described in
this paper in order to support service-based application
within the .NET and web service frameworks.

71

Acknowledgement
G. Gannod is supported in part by NSF CAREER
grant CCR-0133956.
References
Edwards, W.K., 1999. Core Jini. Prentice-Hall.
Fremantle, P., Weerawarana, S., Khalaf, R., 2002. Enterprise services.
Commun. ACM 45 (10), 77–80.
Gamma, E., Helm, R., Johnson, R., Vlissides, J., 1995. Design
Patterns: Elements of Reusable Object-Oriented Software.
Addison-Wesley Longman.
Gannod, G.C., Mudiam, S.V., Lindquist, T.E., 2000. An architecturebased approach for synthesizing and integrating adapters for legacy
software. In: Proc. 7th Working Conf. Reverse Eng., IEEE, pp.
128–137.
Garlan, D., Monroe, R.T., Wile, D., 1997. Acme: an architecture
description interchange language. In: Proc. CASCON’97, pp. 69–
183.
Grundy, J., Mugridge, W., Hosking, J., 2000. Constructing component-based software engineering environments: issues and experiences. Inform. Software Tech. 42 (2).
Medvidovic, N., Taylor, R.N., 1997. Exploiting architectural style to
develop a family of applications. IEE Proc. Software Eng. 144 (5–
6), 237–248.
Meyer, B., 2001. .NET is coming. IEEE Comput. 34 (8), 92–97.
Mezini, M., Seiter, L., Lieberherr, K., 2000. Component integration
with pluggable composite adapters. Software Archit. Comp.
Technol.
Reiss, S.P., 1990. Connecting tools using message passing in ﬁeld
environment. IEEE Software 7 (7), 57–66.
Seely, S., Sharkey, K., 2001. SOAP: Cross Platform Web Services
Development Using XML. Prentice-Hall.
Shaw, M., Garlan, D., 1996. Software Architectures: Perspectives on
an Emerging Discipline. Prentice-Hall.
Stal, M., 2002. Web services: beyond component-based computing.
Commun. ACM 45 (10), 71–76.
Urnes, T., Graham, T., 1999. Flexibly mapping synchronous groupware architectures to distributed implementations. In: Proc. of
Design, Speciﬁcation and Veriﬁcation of Interactive Systems.
Gerald C. Gannod is an Assistant Professor in the Department of
Computer Science and Engineering at Arizona State University and is
a recipient of a 2002 NSF CAREER Award. He received the M.S.
(1994) and Ph.D. (1998) degrees in Computer Science from Michigan
State University. His research interests include software product lines,
software reverse engineering, formal methods for software development, software architecture, and software for embedded systems.
Sudhakiran V. Mudiam received the Ph.D. degree (2003) from Arizona
State University and is a software architect with Aligo, Inc. He received an M.S. (1997) from the Indian Institute of Technology, Madras
(Chennai), India. His research interests include software engineering,
distributed and object-oriented systems, software design, software
architecture, service-oriented software engineering, and Wireless
Application platforms.
Timothy E. Lindquist is Professor and Chair in the Department of
Electronics and Computer Engineering Technology at Arizona State
University East Campus in Mesa, Arizona. He received the Ph.D.
(1979) degree from Iowa State University. His research interests include software engineering, automated support for processes, distributed web-based applications, and distributed object computing.

Assessing the Usability
of Human-Computer
Interfaces
Timothy E. Lindquist, Virginia Tech

Engineers may be able to
design a better interface if they
take into account the control
structures underlying the
interface syntax.

T here are different criteria to evaluate the human-to-system interface. We use two in this article, "effort-to-learn" and "effort-to-use."
Attention has already been devoted to
evaluating the "effort-to-learn" and
"effort-to-use" the dialogue syntax.
Examples include work on help facilities' and the Dialogue Management
System.2 Another approach, such as
that of Ledgard, 3 has been to consider
naming conventions in command languages. These studies focus on the
form, content, and devices used in
human-computer communication.
We think there is an additional aspect
that is equally important.
In this article, we want to examine
the dialogue structure underneath the
syntax whose logic must be traversed
each time human-computer interaction takes place. This structure is revealed if we characterize the humancomputer interface in the same way we
characterize software-to-software interfaces. Specifying software-to-software interfaces involves specifying
each of its components.4 Software interface components are the routines
involved and the data elements shared
at the interface. Accordingly, in a
human-computer interface, the components are

(1) the commands invoked by the
user,
(2) the manual actions performed by
the user,
(3) the information manipulated by
commands or the user, and
(4) the decisions made by the user to
determine the dialogue sequence.
74

0740-7459/85/0001/0074$01.00 © 1985 IEEE

The specification of each interface
component must also include its syntax, its semantics, and its protocols
with other interface components.
For a human-computer interface,
the syntax includes the command entry
format, for example, menu, parametric, or fill-in-the-blanks; the device,
such as keyboard, mouse, and voice;
and the responsiveness of the system,
for example, immediate command
completion.
The semantics provide the meaning
of individual interactions by detailing
the functionality of interface components. Command semantics describe, for example, what a command
does and what a response means
relative to the use of the command.
Protocols with other interface components are rules that govern how
using one interface component affects
using another. For example, a compiler on a time-sharing system is not invoked until a source file has been
created. To use a debugger, the source
program must first be compiled with
special options before interactively
executing statements, examining data,
or changing data. Protocols make explicit any control or data dependencies
among interface components. While a
control dependency might specify that
one action must be performed before
another, a data dependency indicates
that some information must be available in a specific state before an action
takes place.
In this article, we present this structure underlying the dialogue between a
human and computer,5 and, using a
procedural language, we encode the
semantics and protocols. Included in
IEEE SOFTWARE

the representation is the sequence in
which the user performs actions, along
with the decisions determining the sequence. Called a dialogue structure,
the representation operationally conveys interface semantics and protocols
without addressing syntax. We also
present, with examples, the criteria
that could be used to evaluate the
dialogue structures.

Coding the dialogue
structure

The semantics and protocols of the
interactions that take place between a
user and the computer can be depicted
in an algorithmic form very similar to a
high-level language program. The algorithms characterized in this section
describe the human-computer interface to a system by detailing what the
user must do in order to employ the
computer to solve a specific problem.
The step-by-step actions performed by
the user, whether carried out by the
computer or manually by the user, are
coded in the dialogue structure.
The language used to express dialogue structures has a syntax that is
limited to- definitions and control
statements. Definitions appear at the
top of the algorithm and provide a
means to associate identifiers with real
world entities. The body follows the
definitions and describes actions taken
by the user in performing the task.
DIALOGUE STRUCTURE title(parameters) I'
DECLARE

name-and_actiondefinitions

BEGIN
body
END

Declaration section. Identifiers are
used to name both data manipulated
and actions invoked by the dialogue
structure.

Name definitions. When describing
the human-computer interface to an
editor, the following definitions would
occur.

current : line;

search.Ada: file OF line;
Here, the names current and search.
Ada are being defined. Current names
a line in the text file search. Ada, which
January 1985

is to be manipulated during the editing
session. The definitions include the
typesfile and line to show how they are
related. That is, files are ordered
groups of lines that can be accessed by
storing and retrieving information line
by line. It is appropriate that we omit a
more detailed definition of line since
lines are atomic units of manipulation
for the editor being described.
When constructing dialogue structures, data names should be defined
for each entity that the user must remember. For example, suppose that
an editor requires that the user find a
line number and remember it for all actions to be performed on the line. Then
it would be appropriate for the name
current to be defined. In fact, we might
have the following sequence of actions
in the dialogue structure.
current:= findLnext(current,

misspell);

change(current,misspell,correct_spell);

In an editor that -doesn't require the
user to remember line numbers, the
same actions may be performed without explicitly including the name current, as follows.

findcLnext(misspell);

change(misspell,correct .spell);

Action definitions. Definitions
should exist for each operation that the
user must perform while completing a
task. Actions are activities performed
by the user, or those performed by the
computer at the user's request. For example, suppose a user must sort a list
of items by hand and then request one
or more computerized operations for
each element on the list. Sorting is an
action that, in this scenario, must be
carried out by the user. Thus, this action would be defined as a human
operation in the following manner.
HUMAN-OPERATION sortiist
-sort a list of requests by key

The definition, with appropriate commentary, would indicate that the human will manually sort incoming
requests to produce an ordered, list of
requests.

Suppose the user is presented with a
list that is a group of requests to an inventory system. One request might be
for temporary storage of an incomplete part. Computerized operations
that carry out such a request could involve a search for adequate storage
followed by a move of the material to a
selected location. Name definitions for
such actions are
COMPUTER-OPERATION space;
COMPUTEILOPERATION store;
Although actions are usually invoked wvith arguments, an action's declaration does not specify parameters.
A single action might have different
numbers of arguments and types of
arguments each time it is invoked.
Allowing default values for parameters may cause differing numbers of

An actions declaration does
not specify parameters.
arguments. An editing command may
not require that the user enter a line
number, and if none were specified
then the current line would be assumed. Another example, indicating
how different types of arguments may
be used, is the range specified in some
editor commands. This argument
might be either a single line number, a
relative line number, a range of (relative) line numbers, or a range delimited
by search strings.
When constructing dialogue structures, we must consider the detail addressed by the operations (that is, the
level of abstraction to which the dialogue structure should be written). The
level used affects software analysis.
Two distinct dialogue structures with
different levels of abstraction could
correctly specify a human-computer
interface. Each would result in a different complexity when using software
analysis. For example, the human operation sort-list, discussed above,
could be further decomposed into the
motor and mental discriminations
needed to perform the sort. If the
dialogue structure were expressed at
75

this lower level, its complexity would
be greater (when measured by software metrics) than if written using the
sort-list action.
Although no rule exists to assure
that all dialogue structures for a single
interface define the same actions,
guidelines can be used to assure a similar level of abstraction. For computer
operations, each action should be at a
high enough level to generate at least
one user-viewable response (for example, an error message or normal response), and each action should be at a
low enough level to correspond to a
single unit of user input (for example,
a command line). The level of abstraction for human operations depends on
that used for computer operations.
Human operations should be defined
at the highest level possible that allows
interaction with computer operations.
The inputs-to and outputs-from computer operations should be generated
by, or used in, human operations.
These guidelines define an acceptable
range for the level of abstraction. For
computer operations, the range is
bounded by units of input and output,
and, for human operations, the range
is determined by data jointly manipulated by the user and computer. When
comparing different interfaces for the
same task, this bounded range on the
level of abstraction assures adequate
consistency for comparison.

Instructions in a dialogue structure.
The instructions describe the use of an
interface by detailing the sequence inwhich actions are to be performed.
The names defined in the dialogue
structure are manipulated using selective and iterative program statements
to indicate decisions, and specify the
order in which human-performed and
computer-performed actions take
place.
The basic statement of the dialogue
structure's body is the invocation of an
action, which is denoted by parenthetically including the arguments together
with the action's name. Although we
gave examples of invocation above,
two further examples distinguish functional from procedural use of actions.
Suppose that the following name def76

initions exist in a dialogue structure
indicating that the user's interface includes a computer operation to rename a file and a human operation to
select the next file in a directory. The
definitions for these names might be
COMPUTER-OPERATION rename-file;
HUMAN-OPERATION select-next;

The action rename-file takes two inputs, old and new, and although it has
no explicit outputs, its procedural effect is to associate a new name with an
existing file. The action select-next
takes as input a directory of file-names
and functionally returns as output a
single (selected) file-name. The body
of a dialogue structure defining these
names might contain the following unrelated invocations, which show how
actions may be invoked either functionally or procedurally.
next-file: = select-next(my-directory);

rename-file(search.pas,search.Ada);

metric input, or voice input. In each
case the dialogue structure would be
the same. The dialogue structure allows a syntax-independent evaluation
of the usability of an interface.

When invoking a computer action,
an implicit wait is assumed to occur
until the action has completed. This
implies that the human is inactive
while the computer is busy carrying
out an action. Although this is a reasonable assumption, in many systems
an asynchronous mode of activity exists in which the computer and the
human continue to act independently.
An asynchronous system is often characterized by computer commands to
which there are multiple acknowledgments. An example of this asynchronous mode of operation is the Genie
system.6 Genie's user acts as an air
traffic controller by issuing commands
to vehicles being displayed on a graphics screen. After typing a command
line
AIRPLANE 203 TURN LEFT
HEADING 270

The basic statement of the
dialogue structure's body is the
the user continues other activities until
invocation of an action.
a few seconds later when it can be deIn the first invocation, the assignment
operator, :=, is used to indicate that
next-file obtains the file-name returned by select-next. In the second
case, no value is returned by the procedure rename-file, but it does take
two arguments, search.pas and search
.Ada.
The computer operation to rename
a file might not have the same syntax
for entry as implied by the dialogue
structure's body. By coding invocations using the name of the action,
followed by parenthetical arguments,
details of the syntax (format, device,
and interaction level) used in interacting with the computer are hidden from
the dialogue structure. As mentioned
above, an aspect of usability that is independent of the syntactic appearance
of the interface is considered in this article. Entry of the rename command
could be by fill-in-the-blanks, para-

termined on the display that airplane
203 has actually turned to a heading of
270. Although Genie acknowledges
that the command has been entered,
the computer and human continue activity simultaneously. We use a limited
set of control structures that do not
allow for asynchronous operation of
the computer with the human. Thus,
one action is assumed to be completed
prior to the initiation of another.
Two statement types are necessary
to express decision-based and iterative
structures. It is often necessary for a
user to select from a group of actions
based on some condition of the task
being performed. Two constructs are
used for this purpose, the case and the
if-then-else statements. An example of
the case statement (the if-then-else is
used in a later example) can be seen in
an inventory system, in which a single
user is handed requests for inventory
transactions. Depending on the request, the user may take different acIEEE SOFTWARE

tions in communicating with the

sys-

tem.

CASE current-request IS
WHEN store = >

Similarity

avail:=

Syntax

retrieve(part,quantity,to-location);

Semantics

space(part,quantity,workstation);
store-part(part,quantity,avail);
WHEN retrieve-part= >

END CASE;

Retention

Complexity

If the user's task consists of manualProtocols
ly sorting a number of requests based
on some criteria, and then carrying out
each request, then the dialogue structure representing this would be itera- Figurel.Matiixviewof "effort-to4learn" factors and interface components.
tive. Requests are serviced individually
sorted-list:= sort(request-list);
FOR each-request IN sorted-list LOOP
CASE type(each-request) IS
WHEN store = >
avail:= space(part,quantity,near);
store-part(part,quantity, avail);
WHEN retrieve-part = >

retrieve(part,quantity,to-location);

END CASE;
END LOOP;

individually until the entire sequence
has been exhausted.
Others have used formal language
specification techniques to describe the
human-computer interface. Reisner,
for example, has used grammars to
specify both the physical and mental
actions of human-computer interaction.7 Jacob's work on using formal
specifications for human-computer interfaces8 has contributed to the ideas
we present here. Dialogue structures
address a high-level method of describing human-computer interfaces without including their syntax, which is a
need recognized by Jacob. Although
we have presented the meta-language
for expressing dialogue structures informally, their correspondence to formally described programming languages is clear.

"Effort-to-learn" and "effort-touse" both contribute to the usability of
the human-computer interface. We
define the effort to learn an interface
as the effort needed to become a proficient user. The effort to use an interface is the efficiency with which the
interface can be used once learned.

The syntactic, semantic,
and protocol aspects of
the interface each have
their own "complexity."

Viewing these criteria as contributing
to the effectiveness of an interface is
consistent with other work in interface
evaluation. For example, Good describes an evaluation of the ease of use
of a document-processing system by
considering its ease of learning, its ease
of use once learned, the anxiety it
creates, and user attitudes. 9
We can classify factors contributing
to the effort required to learn a human-computer interface into properUsability of an interface
ties associated with the individual user
The categorization of human-com- and properties independent of the
puter interfaces into syntactic, seman- user. Three such factors are
tic, and protocol aspects provides us
(1) similarity of the learned interwith an opportunity for further analyface
to other known interfaces;
sis. By identifying these aspects, and
(2)
retention of similar interfaces;
by demonstrating how they may be
and
separately specified, avenues become
(3) complexity of the interface.
available for studies aimed at isolating
the key factors of interface usability. Similarity and retention depend on the

January 1985

but complexity is a property of
the interface.
Exposure to other interfaces can be
a positive or negative factor in learning
a new interface. Previous use of commands with similar functionality can
aid in learning a new interface; however, previous use of similar commands with a different functionality
can hinder learning. For each interface
component, there is a positive or negative learning influence determined by
the similarity or difference of the interface components that the user has
learned. The degree of retention measures the effect that past exposure to
other interfaces has on learning the
user,

new

interface.

The syntactic, semantic, and protocol aspects of the interface each have
their own "complexity." Determining
and validating a useful measure of the
effort to learn an interface must include the factor of its complexity. It is
not enough to assess an interface by its

suitability to a particular application,
especially when designing an interface

for users with diverse backgrounds
and experience levels. To determine
the learnability of an interface we need
to objectively measure its complexity.
This complexity factor offers the
greatest potential for developing predictive measures of usability, as Reisner suggests .7
Figure I is a matrix view in which
one dimension is the interface component and the other is the aspect of
"effort-to-learn." According to this
view, the "effort-to-learn" factors

77

DIALOGUE STRUCTURE program_entry_edit_compile IS
DECLARE
entry-list: lisLofteditor-commands;
current: a-single-command;
filename: file_containing_source;
options: list_of_compiler_options;
design: program-design;
HUMAN OPERATION identify-error, correction-scenario,
empty, more-errors, next, construct,

repositioning_necessary;

COMPUTER OPERATION invoke-editor, enter_input_mode,

enter-statement, leave_input-mode,
write_and_exit, invoke-compiler,
position_within_source;

BEGIN

entry-list = construct(design);
invoke-editor; enterJinput_mode;

WHILE (NOT empty(entry_list)) LOOP

enter-statement(next(entry-Jist)); END LOOP;
leave-Jnput.mode; write_and_exit(filename);
invoke_compiler(filename, options);
WHILE (more-errors) LOOP
FOR ALL errors LOOP

identity-error;
correction_scenario(entry_list);

END LOOP;
invoke_editor(filename);
WHILE (NOT emply(entry_list)) LOOP;
current := next(entry_list);
IF (repositioning-necessary(current))

THEN position_within_source(current); END IF;

enter_statement(current);
END LOOP;
write_and_exit(filename);
invoke_compiler(file, options);
END LOOP;
END program_entry_edit_compile;

Figure 2. Program entry and syntactic analysis: text editor and compiler.

such as similarity, retention, and complexity are orthogonal to the syntactic,
semantic, and protocol aspects of the
interface. Predictive assessment techniques need to be developed and validated for the syntactic, semantic, and
protocol aspects of interfaces. In the
previous section, we demonstrated
how the semantics and protocols can
be encoded into a dialogue structure.
In the remainder of this article, we
discuss the relevance of assessing the
complexity of dialogue structures by
using software analysis techniques that
currently exist.
78

Complexity of a dialogue structure.
For a dialogue structure, learning effort is the effort needed to understand
the commands, inputs, responses,
human actions, and decisions of the
dialogue structure. Thus, the components of the dialogue structure
(distinct from the interface syntax) are
learned. The user has to learn what
these components do and how to put
the components together to use the interface. These "effort-to-learn" aspects can be obtained from the semantics and protocols expressed in the
dialogue structure.

The semantic complexity of an individual component of a dialogue
structure excludes interactions and interdependencies with other components. Each component, for which there
is an action defined in the dialogue
structure, has a semantic complexity.
Components are learned by inputs,
outputs, and functionality. Editor
commands, for example, are learned
by matching arguments with the results obtained. Learning the functionality of a command is enhanced by
knowing how different inputs produce
various outputs. The functionality of a
change command can be understood
on the surface, but learning how
specific inputs, such as range options
and pattern-matched search strings,
cause different results contributes to
the complexity.
Learning protocol complexity involves learning the logic for combining
interface components. The semantic
complexity of the interface includes,
for example, knowing what decisions
need to be made and how to make
them. Protocol complexity, however,
includes when decisions are made
(with respect to other dialogue structure actions), where information
needed in making the decision is obtained, and how the result of a decision
is used in determining the next action:
In learning to use an interactive debugger, the user must learn what conditions make "single-step-execution" of
instructions appropriate and what
conditions make "execute-to-nextbreakpoint" appropriate. Further, the
user must learn that it is necessary to
set breakpoints before using "executeto-next-breakpoint." Learning control dependency, which involves learning the order that actions take place,
further exemplifies the protocol complexity represented by the dialogue
structure.

Programming task

To demonstrate the utility of the
analysis technique, we compare the
human interface for programmers by
examining syntax-specific editors and
text editors. These tools represent vastly different modes of program entry
IEEE SOFTWARE

and translation. Typically in an interactive system, the user invokes a
text editor to enter the program into
the file system, and then invokes the
compiler to translate it into an executable form. Another mode becoming more widely used is the syntaxspecific editor. Here the user's program is syntactically analyzed line by
line as it is entered. While the text
editor and compiler interact with each
other at the level of entire files, the
syntax-specific editor contains a translator that allows interaction at the level
of source language statements.
Although only a few syntax-specific
editors exist, users advocate their
benefits over typical interactive-system
tools. Systems such as the Cornell
Program Synthesizer '° have been favorably accepted by user communities.
The use of highly interactive environments originated with program construction environments for dynamic
languages, such as Lisp on Interlisp. "
In addition to providing entry facilities, these environments also provide
support for 'controlled execution of
programs in much the same way as is
done in machine language debuggers.
Few studies have been performed that
support the use of these systems, in
part because the technology for
building them for compiled and blockstructured languages is still developing. In response to this need, one
Pascal environment, Peep, 12 was
developed for the purpose of conducting experiments on programmers
using highly interactive environments.
While experimentation is needed, the
use of software metrics to assess dialogue structures is now used to compare a sample structure for syntaxspecific editors with a typical text
editor and compiler.

Dialogue structures for program entry. The human-computer interface to
text editors and compilers is typical
and conveniently characterized independent of the command format. The
interface to syntax-specific editors,
however, is not as easy to characterize,
and our treatment is limited to program entry to avoid the vast differences in the way that environments
January 1985

DIALOGUE STRUCTURE program_entry_syntax_specific IS
DECLARE
entry-list, new-entries: list_of_commands;
filename: file-containing_source;
current: a-single_command;
design: program-design;
HUMAN OPERATION empty, syntax-error, join-list, construct,
next, correction-scenario, identify-error;
COMPUTER OPERATION invoke_syntaxcspecific-editor,

enter-statement, write-file;

BEGIN
entry-list = construct(design);

invoke-syntax_specific-editor(filename);
WHILE (NOT empty(entry_list)) LOOP
current := next(entry-list);
enter_statement(current);

IF (syntax_error)
THEN identify-error;

new-entries = correction-scenario;
entry-list = join_list(new_entries, entry-list);

END IF;
END LOOP;
write_file(filename);

END program-entry_syntax_specific;
Figure 3. Program entry with a syntax-specific editor.

and providing options. After compilaallow control of execution.
The dialogue structure for an inter- tion, analysis and correction of errors
active text editor and a compiler is bro- begins.
For each error encountered during
ken into three subtasks. The user first
manually constructs the source of the the compilation, two user actions are
performed. The first is to identify the
program in a form that can be directly
entered. Next, the user invokes a text error. The user must decide what the
editor to create the source file. Third, error is and what has caused the error.
he uses the compiler and editor to cor- Identification is distinct from conrect any syntax errors. The dialogue structing a correction for the error.
structure showing the interface for a After recognizing the error, the user
writes out a set of steps necessary for
typical system is given in Figure 2.
The user first constructs, generally correcting the error using the text
off line, the program to be entered. Al- editor available. Corrections are made
though the program may not be in the by attaching changes to the list, ensyntax of the language, the output of try_ list. Once a scenario for correcting
this construction is a sequence of errors has been assembled into the enstatements, called entry-list, that can try list, the editor is invoked and each
be directly entered via the editor. error is corrected using the computer
While the list is actually made up of procedures position_within_source
source program statements, these and enter-statement. Often several
statements are assumed to be in the compilers are needed to detect and corformat of an editor input command. rect all syntax errors, and this is shown
The next subtask is the entry of the by repeating the compile and editing
sessions while errors continue.
program source onto a file. Once the
The dialogue structure for the syneditor has been invoked and the complete list of statements entered, the tax-specific editor contains two subtasks as shown in Figure 3. After conuser writes the file and exits the editor.
At this point, the user requests that the structing an initial list of program
file be compiled, by naming the file statements to be entered, the user
79

McCabe's metric

McCabe's software metric, 13 called Cyclomatic Complexity, addresses
the control complexity of a program. Using a graphical representation of
the program, the metric assesses the program's decision structure by
counting the number of basic decision paths. Many researchers have related these basic paths to testability and maintainability. The relationship
to testability is supported since the metric counts the number of paths corresponding to linearly independent circuits in the program's graph. Basic
paths are commonly used for devising test cases. The relationship to maintainability relies on the observation that a software change can be instrumented only after the decision structure of the affected module is
understood. Thus, the more complicated the decision structure of the program, the more difficult the program is to maintain.
McCabe's metric divides the program into blocks of statements that will
be executed to completion once begun. These statement blocks and the
program's decisions become nodes that are connected using directed
edges. The cyclomatic number of the resulting graph is the number of
linearly independent circuits in the graph. In practice, Cyclomatic Complexity can be calculated (excepting McCabe's suggestion to count all relational conditions once) by counting the number of decision predicates in
the program and adding one. McCabe's measure has received considerable attention in software literature, and Myers has published a notable extension to the technique. 16

Halstead measures

One method to evaluate a dialogue structure's semantic complexity is
Halstead's Measures.14 Halstead viewed algorithms as having measurable interrelated characteristics. To measure these characteristics, Halstead applied laws, similar to physical laws, to four program counts. Two
counts are the total number of operators and the number of unique
operators in an algorithm. The other two counts are the total number of
operands and the number of unique operands in an algorithm.
Halstead's Volume, Level, and Effort measures are of interest in assessing dialogue structure semantic complexity. Halstead defines the volume
of an algorithm to be the total number of uses of operators and operands
times the number of bits needed to represent the program. There are two interpretations of this volume: first, the volume is the number of bits needed
to represent the program; and, second, the volume is the number of mental
comparisons needed to generate the program.
Halstead also defines a program's level of abstraction to be the ratio of
its potential Volume to actual Volume. The level of abstraction increases
with the number of distinct operands and decreases as the number of
distinct operators increases. At the highest level of abstraction
(value = 1), the program consists of a single operation (possibly with an
assignment). Lower levels of abstraction (values less than 1) indicate that
more detail exists in the program itself.
Halstead's Effort measure is an assessment of the effort required to
create and understand a program. The measure, in mental discriminations, is the ratio of the Volume to the Level of abstraction.

80

interactively enters and corrects them.
Constructing program statements is
done in the same manner as with the
text editor. Once the list has been constructed, the syntax-specific editor is
initiated, and an iterative enter-andcorrect loop is begun. The action enter-statement may take a form distinct
from that used in Figure 3, as may any
of the procedures. For the syntax-specific editor, a separate command, or
group of commands, may need to be
entered with each statement.
The syntax-specific editor responds
to each statement entered indicating,
or forcing, proper syntax. Within the
enter-and-correct loop, a human-performed test is made determining
whether the editor has found an error.
Each time that an error is found, the
user responds in much the same way as
detailed for the text editor. First, the
error is recognized, and then a correction for the error is devised. In Figure
3, the correction is named new_entries
to indicate that a single error might require several corrections. Join-list is a
human function that merges the corrections with the remaining entry list.
The loop terminates when the list has
been exhausted, indicating that all errors have been removed, and all statements have been entered. At this point,
the file may be written, or debugging
may begin.
Applying metrics. Three existing
evaluation techniques can be made to
assess dialogue structure semantic and
protocol complexity. These include
assessments of the decision structure,
magnitude, and information flow
through the dialogue structure. As examples, we present McCabe's metric, 13
as an estimate of the complexity of decisions made when using an interface,
and Halstead's Software Science, 14 as
an estimate of the magnitude of the
dialogue structure. McCabe's metric
(decision structure) addresses the protocol complexity of a dialogue structure, and Halstead's Software Science
(magnitude) addresses the semantic
complextity of the structure. Additionally, a metric such as Henry's Information Flow15 could be modified
to estimate the intricacy of interactions
IEEE SOFTWARE

among

actions used in the dialogue

structure. While this would be another
assessment of protocol complexity,
such an example is not constructed in

Semantic
complexity

this article.

Decision complexity. The dialogue

structures constructed in the preceding
section have a decision structure based
totally on the user. The decisions in
these algorithms either select from al-

ternative blocks or guard the actions of
an iterated block. The user makes all
decisions by taking the information
provided by human or computer operations as input. The user evaluates the
predicates of the algorithm and takes
the appropriate action. A prerequisite
to using a specific human-computer
interface is understanding the underlying decision (or control) structure.
Each time the interface is used to solve
a problem, this decision structure is
traversed.
McCabe's measure reveals the size
of the decision structure that the user
must traverse when using the interface.
The algorithm for Figure 2 (the text
editor combined with compiler) has a
complexity of six, and the complexity
for the syntax-specific editor is three.
While using McCabe's measure indicates that the text editor has a more
complex decision structure, other
structures, for example, those having
highly nested decisions, in which each
decision depends on those surrounding
it, have a higher complexity than revealed by McCabe's measure. A further examination beyond the scope of
McCabe's metric can reveal more information about nested decisions and
their relationships. To determine
whether a decision depends on those
surrounding it requires an analysis of
the content of the decisions.

Semantic complexity. Halstead's
Volume, Level, and Effort measures
must be carefully interpreted as they
are applied to dialogue structures. For
the algorithms of Figures 2 and 3, the
Halstead Volumes are 176 and 90, respectively. In dialogue structures, the
Volume augments McCabe's decisioncomplexity measure by indicating the
size of the structure's components that
January 1985

Level

Effort

Decision

176

.04

4400

6

90

.14

643

Volume

Editcompile

Syntaxspecific

1

I__

Protocol
complexity

_I

3

Figure 4. Semantic and protocol complexity assessments.

are executed each time the interface is
used. Halstead's Volume is an example
technique for assessing the semantic
complexity of dialogue structures. Volume accounts for the number of semantic elements that must be learned.
It indicates the complexity of each action, however, only through its explicit
arguments. Thus, the semantic complexity contributed by implicit arguments, and variations in complexity
among different actions, must also be
taken into account.
We would expect, and indeed it is
the case, that the Halstead Level of abstraction of the edit-compile structure
is lower than that of the syntax-specific
editor (0.04 versus 0.14). One reason
for this is that operators are used several times in the edit-compile structure
while this is not as true in the syntaxspecific structure. The compile commands and find command are good
examples. Although the compile command only occurs twice in the algorithm, one occurrence is within an iteration. The find command (positionwithin-source) occurs with a nested iteration as well.
Halstead's Effort for the editorcompiler is higher since its Volume is
higher and its Level of abstraction
lower than those of the syntax-specific
editor (E for Figure 2 is 4400, and E for
Figure 3 is 643). Halstead's Effort indicates that the higher the Volume and
the lower the Level of abstraction of a
program, then the more difficult it is to
understand. In dialogue structures, the
Level of abstraction varies predomi-

nately through the operands. The highflow of information through the
structure, the lower its Level of
abstraction. The operations used are
mostly unique action names. Similar
to Volume, Effort does not account
for the varying semantic complexity
of each unique dialogue-structure
action and argument, but it does indicate a semantic magnitude, assuming all actions and arguments require the same effort to learn.
The McCabe and Halstead measures assess the protocol and semantic
complexities of dialogue structures in
the same manner as they assess software. The matrix of Figure 4 summarizes the application of these measures to the edit-compile interface of
Figure 2 and the syntax-specific interface of Figure 3. While each of these
measures addresses a property of
software, they do not individually, or
even together, provide a complete
assessment tool. Further, results of
applying software metrics to humancomputer interfaces must be interpreted taking into account the distinctions between software and dialogue
structures. Software contains instructions carried out completely by the
computer, but dialogue structures
contain both actions and decisions
that are performed by the user and actions that are performed by the comer the

puter.

Efficiency of use. To show how
other software analysis techniques
may further apply to assessing usabil81

ity, the following question may be addressed. Once an interface has been
learned, what is the measure of its use
efficiency? If this question was asked
of the interface's syntax, for example,
such measures as number of keystrokes, thought time, and response
time might be appropriate indicators.
To measure the time required to traverse the dialogue structure of the interface, we would have to analyze how
long the user takes to perform actions
and make decisions. We could then
estimate use efficiency by analyzing
the dialogue structure's bounds on
iterations. Although it is doubtful that
the values we obtained for the time a
user takes to perform actions or make
decisions would be significant, by
assessing the computational complexity of the dialogue structures we might
find factors that could be used to compare different structures.

B y viewing a human-computer in-

Bterface as having syntax, seman-

tics, and protocols, we have a more
complete view of human-computer interactions. In this article, we have presented a method for representing the
semantics and protocols by coding
them into a dialogue structure. This
coding combines the user's manual actions with the computer's automated
actions. The dialogue structure details
the sequence in which actions take
place and the order in which the users
must make decisions. In the article, we
have separated the effort to learn an
interface's dialogue structure into
user-independent and user-dependent
terms. We have presented dialogue
structure complexity as the user-independent component of the effort to
learn, and we have indicated how software metrics such as those of Halstead
and McCabe can be applied to reveal
this complexity. And, finally, we have
demonstrated this assessment of usability by describing dialogue structures
for two types of program-entry systems, text editors with compilers and
r
syntax-specific editors.
82

Acknowledgments

This research was supported by the Office of Naval Research under Contract
Number N00014-81-K-0143, and Work
Unit Number NR SRO-101. The research
was supported by the Engineering
Psychology Programs, Office of Naval
Research, under the technical direction of
John J. O'Hare.

References

1. A. M. Cohill and R. C. Williges,
"Computer Augmented Retrieval of
HELP Information for Novice Users,"
Proc. Human Factors Society Meeting, Oct. 1982.
2. R. W. Ehrich, "DMS-A System for
Defining and Managing HumanComputer Interfaces," Automatica,
Vol. 19, No. 6, 1983, pp. 655-662.
3. H. Ledgard et al., "The Natural
Language of Interactive Systems,"
Comm. ACM, Vol. 23, No. 10, Oct.
1980, pp. 556-563.
4. T. E. Lindquist, J. L. Facemire, and
D. G. Kafura, "A Specification Technique for the Common APSE Interface Set," J. Pascal, Ada, and
Modula-2, Vol. 3, No. 5, Sept/Oct
1984, pp. 25-31.
5. T. E. Lindquist, "The Application of
Software Metrics to the Human-Computer Interface," Proc. Compcon,
Sept. 1983, pp 239-244.
6. T. E. Lindquist, R. G. Fainter, and
M.T. Hakkinen, "GENIE: A Modifiable Computer-Based Task for Experiments in Human-Computer Interaction," to be published in Int'l J.
Man-Machine Studies.
7. P. Reisner, "Formal Grammar as a
Tool for Analyzing Ease of Use: Some
Fundamental Concepts," Human
Factors in Computer Systems, Ablex.
8. J. K. Jacob, "Using Formal Specification in the Design of a Human-Computer Interface," Comm. ACM, Vol.
26, No. 4, Apr. 1983, pp. 259-264.
9. M. Good, "An Ease of Use Evaluation of an Integrated Document Processing System," Proc. Human Factors Computer Systems, Mar. 1982,
pp. 142-147.
10. T. Teitelbaum and T. Reps, "The
Cornell Program Synthesizer: A
Syntax-Directed Programming Environment," Comm. ACM, Vol. 24,
No. 9, Sept. 1981, pp. 563-573.

11. W. Teitelman, INTERLISP reference
manual, Xerox PARC, Palo Alto,
Calif., 1975.
12. C. S. Ku, "The Design and Implementation of a Language Environment for
Evaluating the Programmning Task,"
Masters thesis, Virginia Tech, Va.,
1982.
13. T. J. McCabe, "A Complexity
Measure," IEEE Trans. Soft. Eng.,
Vol. 2, No. 4, Dec. 1976, pp. 308-320.
14. M. H. Halstead, Elements ofSoftware
Science, Elsevier North-Holland, New
York, 1977.
15. S. Henry and D. G. Kafura, "Software Metrics Based on Information
Flow," IEEE Trans. Soft. Eng., Vol.
7, No. 5, Sept. 1981, pp. 510-518.
16. G. J. Myers, "An Extension to the Cyclomatic Measure of Program Complexity," SIGPLAN Notices, Oct.
1977.

Timothy E. Lindquist is an assistant professor of computer science at Virginia Tech
in Blacksburg, Virginia. He is interested in
programming languages and environments,
human-computer interactions, and verification/validation. For the past three years,
he has been actively involved in work on
Ada environments.
Lindquist received his BS from Purdue
University and his MS and PhD in computer science from Iowa State University.
He is a member of IEEE and ACM.

Questions about this article may be addressed to the author at the Department of

Computer Science, Virginia Tech, 562
McBryde Hall, Blacksburg, VA 24061.

IEEE SOFTWARE

Cooperating Process Components
Kevin A. Gary
Electrical Engineering and Computer Science Department
The Catholic University of America
Washington, D.C. 20064
garyk@cua.edu
Timothy E. Lindquist
Computer Science and Engineering Department
Arizona State University
Tempe, AZ 85287-5406
lindquist@ asu.edu

Abstract

cuting process models, analyzing them, and evolving
them. The proposed framework does not solve all of these
problems in and of itself, but instead provides a platform
by which automated process support issues may be
addressed in further depth. This research is part of a larger
vision of how processes are constructed, executed, monitored, and analyzed. This is just the first step, providing an
infrastructure for realizing this vision by considering
trends in software engineering that will effect process
technology in the foreseeable future.
The recent boom in technologies such as the Internet,
Java, Visual Basic, and CORBA has given rise to new
buzzphrases like “reusable software component”, “distributed object middleware”, “n-tier architecture”, and “ubiquitous client”. The implications (once you get past the
hype) these technologies hold for the future of software
development and deployment includes “plug and play”
software, “rent per use” software and “open marketplaces
for distributed components”. Current approaches to automated process support do not lend themselves to these
trends. Automated process support systems are large,
monolithic creatures that often require extensive knowledge of the users, tools, and data types in their environment. The systems are usually tied to some underlying
representation of process, and in that representation exist
dependencies that make componentization of the process
models themselves difficult. The research community has
begun to recognize and propose solutions to these problems. It is sufficient to state that current automated process
support systems are not the open systems that characterize
prevailing trends in software development.
Open Process Components (OPC)is a first step
toward realizing a component-based environment for auto-

Modern automated process support systems can be
considered monolithic in three ways. First, they model
processes top-down, usually from a single perspective,
such as the organizational perspective. Second, they are
process-centered, in that they often require extensive
knowledge of the users, data types, and applications in
their environment. Third, they tie process implementation
to a specific representation, making reuse and interoperability between process models difficult to achieve. This
paper describes the application of component-based techniques to process modeling across overlapping process
spaces. This approach, the Open Process Components’
approach, encapsulates process fragments as interoperable
and reusable process components. This paper motivates a
vision of cooperating components for automated process
support, presents an overview of the Open Process Components approach, and shows the application of this technique to Humphrey’s Personal Software Process.

1. Introduction
Automated process support is the application of computer technology to assist and automate work on behalf of
users. This includes applications of automated workflow[ 151 and automated software process support[9]. This
paper proposes a component-based framework for supporting process activities: building process models, exe-

1. This work was supported in part by the Office of Naval Research
under award number N00014-97-1-0872.

218
0-7695-0368-3/99 $10.00 0 1999 EEE

mated process support. It is an exercise in modularization
and object-orientation applied in the extreme to process.
OPC modularizes process models in order to define
boundaries between process fragments. It enumerates
objects in the process domain in order to encapsulate
information and behaviors in the process space. It “cornponentizes” processes by capturing collections of objects
that act together as a cohesive entity, and interact with
other such collections in well-defined ways. By componentizing process, the environment in which the process
acts is shielded from the complexity of specific process
representations. Dependencies between processes are not
eliminated, but moved to the boundaries between components where they are easily recognized and managed. In a
component-based environment, there is hope ideas such as
“plug and play processes”, “rent per use processes”, and
“open marketplaces for distributed process components”
can be realized. The Open Process Components vision is
of an environment of distributed process components,
where components can be located, adapted, and optimized
on demand with respect to a set of process requirements.
This paper motivates the need for reusable, interoperable process support that spans multiple process spaces,
and suggests a solution based on component-based process
support. Section 2 and Section 3 provide background and
motivation for a new approach to process automation. Section 3 overviews the component-based approach, suggesting that this technique is useful for realizing the vision
discussed in Section 3. Section 4 demonstrates this
approach on the Personal Software Process, and Section 5
provides some conclusions.

OPC represents a fundamental departure from traditional approaches in distributed, interoperable process
support. OPC promotes a fundamental shift from monolithic process-centered environments, to flexible processenabled environments. OPC is centered around the building blocks of enactable processes: work, people, and artifacts. Participants use their own tool sets and services to
do the work. Process participants accept work items in the
form of components, manipulate these components, and
pass them along to the next participant or activate subsequent components. Strict adherence to prescriptive process
definitions and tight integration of tools is not required.
From the user’s perspective, work items, or process components, are received from any number of sources and
integrated into the user’s personal work space.

3. Cooperating Components
In order to understand component-based process support, one must understand what it means to be a process
component, and how the traditional activities of process
modeling and enactment are performed in a componentbased environment. OPC offers the following definitions:
Process Component - a process component is an encapsulation of process information and behaviors at a
given level of granularity.
Component-based Process Model - a component-based
process model is a collection of process components
that interact in meaningful ways. A component-based
process model is itself a process component.
Component-based Process Enactment - componentbased process enactment refers to the creation of a
component, the dynamic evolution of the component,
its interaction with other process components, and its
interaction with users and an environment in which the
component is situated.
These definitions are elaborated on in this section.

2. Background
Research in providing distributed, interoperable process support focuses on (1) describing low-level infrastructure support for distribution, such as distributed
architectures[1][8][ 13][141 and distributed transactions[13,
or (2) distributed process control integration protocols[l][l1][13][17], or (3) distributed process data interchange protocols, usually in the form of a common
language for translation[7][161. In the context of current
research, OPC contributes new ways of performing control and data integration through component-based design.
The OPC approach is to support top-down process decomposition and bottom up synthesis via interacting components. Control integration is achieved by viewing a
process model as a collection of interacting components.
Data dependencies are reduced by encapsulating components behind well-defined interfaces, eliminating the need
for translating between process languages.

3.1 Process Components
A process component is an encapsulation of process
information and behaviors at a given level of granularity.
The process information is expressed in whatever formalism or language is used to represent the semantics of the
component. OPC separates this information three ways:
process schema, process states and transitions, and process implementation (see Figure 1). The process schema
defines a vocabulary for the component, identifying entities and relationships that have meaning for a particular
component. OPC defines a minimal, extendable, common
schema for components. Process state and transitions

219

sponding views. Additional meta-roles and views can be
defined on a per component basis.

3.2 Component-based Process Models

FIGURE 1. A process component
between states are represented as a finite-state machine.
The set of process states and valid transitions are defined
per component, with a common set of meta-states identified within OPC. The process implementation is the
underlying representation of the component and the
machinery which interprets this representation. This may
be a petri net, a process programming language, or whatever other means are used to represent process semantics.
It may also be a process tool that is wrapped to provide the
process implementation. The implementation of a component is encapsulated in the component, so that components
interact without requiring homogeneous representations.
A process component provides two additional
abstractions. One is a process type, which is a means of
categorizing processes. The schema, states and transitions,
and implementation define the object implementation of a
component, but they do not determine the type of process
a particular component represents. Two components with
the same exact schema, states and transitions, and implementation may represent two distinct processes, for example, a peer review process and a software development
process. Likewise, two components with different schemas, states and transitions, and underlying implementations may both be software inspection processes. In OPC,
a process type is defined for each component that indicates
what type of process the component represents.
The second additional abstraction is meta-views.
Meta-views define the allowable ways in which meta-roles
expect to interact with process components. A meta-role
identifies how the component is currently being used; for
definition, enactment, ownership, or observation. A view
is provided for each of the meta-roles that defines the way
a given meta-role interacts with the component. OPC
defines the four meta-roles just listed and their corre-

220

A component-based process model is a collection of
process components that interact in meaningful ways.
OPC provides mechanisms for component interaction
through well-defined interfaces, process events, and process messages. These mechanisms define a common con-?
tract by which all components abide. However, in order to
prevent from prescribing a contract that is too rigid, these
mechanisms are extendable and open.
A process model realized as a set of cooperating components is a step toward an environment of interoperable,
reusable process fragments. Components are combined in
an aggregation tree to support various levels of process
granularity (see Figure 2). A component-based process
model is itself a process component. OPC supports both
top-down modeling and bottom-up reuse. A top-down process tree is constructed by decomposing process components at coarser levels to finer-grained process parts.
Bottom-up reuse is supported by realizing a relation
between two or more process parts as components. A subprocess component serves as part of the implementation of
its parent process component. In terms of process granularity, a process component captures an abstraction of the
process space at a particular level. The fact that the component may be composed of subprocess components is not
exposed to the outside world.

3.3 Component-based Enactment
Component-based process enactment refers to component creation, dynamic component evolution, component interactions with other process components and with
its environment (including end users}. A process component in OPC is an active, ‘‘living’’ entity. It encapsulates its
own process schema, state, and implementation, and
allows dynamic modification of itself based on its own
implementation. For example, a process component may
extend its schema instance during enactment, or a compo-

FIGURE 2. A Process Component Tree

component defines the boundaries of process information,
while meta-views define ways in which components interact with the external world of users, tools, and middleware
platforms. In the sense that components interact with each
other in meaningful ways that define a contract between
components, meta-roles and meta-views define a contract
between process components and the outside world. A
component-supporting architecture provides the services
components require but cannot provide for themselves.
For example, components require support for registering
interest in, and receiving notification of, categories of
events. Components also require a repository for locating,
naming, and storing process components. While these can
largely be considered implementation issues, they do force
one to reconsider the traditional view of process support
architectures like the Workflow Reference Model [15] in
the context of component-based process support. The OPC
framework provides these types of services in support of
distributed process components.
This section has provided a brief overview of the general approach of OPC. Details of the framework support
for these concepts is beyond the scope of this paper. Further information may be found in [3].

FIGURE 3. Components and Contexts
nent may dynamically change its set of process states and
transitions. Dynamic component modifications are managed by the component itself, reducing reliance on other
components or global process information.
In OPC’s component-based approach, there is not a
strict separation between process model and process
instance. In that a component defines its own schema,
states and transitions, and implementation, it defines its
own process fragment. Process components evolve as they
are enacted, activating subprocess components and instantiating itself as binding information becomes available.
Hence each process component runs the gamut from creation to instantiation[2] during its active lifetime.
Process components interact with other components,
users, and an environment during enactment. Interactions
between components are provided through well-defined
interfaces, events, and messages. Components also define
interactions based on their relationship in the process tree.
For example, in OPC a subprocess component notifies its
parent process component when it changes state through a
process event. Parent processes request subprocesses to
perform actions through process messages.
Process components interact with users and the external environment through meta-views. As shown in Figure
3, a user interacts with a component by assuming a metarole. The user is part of an environment that defines a context in which the component is activated. The environment
provides the component with situation-specific information the component requires when interacted with under a
given meta-view. For example, tool invocation may be
required during process component enactment. The process component may determine what type of tool to invoke
and when to invoke it, but asks the environment for a specific tool to bind to at the given time.
Meta-views and environments are the bridge between
abstract process components and an architecture for distributing and interacting with real components. A process

4. Example: The Personal Software Process
The Personal Software Process (PSP)[4][6], developed by Watts Humphrey at the Software Engineering
Institute (SEI), is the application of a disciplined software
process at the individual level in software engineering.
The PSP is an entire discipline that structures the way
individuals develop software, monitor their work, and
improve their methods of performing tasks. The PSP
includes defined processes for guiding how developers
perform the PSP. This section introduces the PSP, and
applies the component-based approach to produce an
implementation of the PSP in OPC.

4.1 PSP Specification
Although the PSP discusses general process improvement principles for any individual process domain, it is the
Personal Sofrware Process, and as such recommends practices specific to software process development. These
practices include a process script for guiding the sequence
of individual activities.
The PSP introduces a fair amount of overhead into the
daily activities of software developers by adding estimating and recording activities. “Overhead” does not imply
these activities are not worth doing; they are precisely the
activities which provide the value-added benefits of the
PSP. However, they are activities that software developers
do not usually perform on a daily basis. Humphrey

221

implementation of the PSP supports these products as part
of the schema of the components in Figure 4.
A component-supporting environment for OPC has
been implemented in Java. This environment includes
mechanisms for component creation, modification, extension, enactment, monitoring, reuse, and interoperability.
The components in Figure FIGURE 5. are defined within
this environment to provide PSP support. The PSP process
tree in the OPC environment is shown in Figure 5.

acknowledges the extra time requirements and tediousness
of these activities, but argues that in the end performing
these activities actually saves time and increases quality.

4.2 Automating the PSP
The time consuming and tedious tasks the PSP adds to
a personal software development process are ideal candidates for process automation. Humphrey realized as much
and created a specification for automating the PSP [ 5 ] .
The specification is quite large and detailed, specifying a
dataflow between forms presented in [6]. The implementation described here is based on the process scripts specified in [4]. Process automation can reduce the time spent
on these tasks by automatically recording the various
kinds of tracking data the PSP requires. More importantly,
automated process support can ensure that the individual
consistently and faithfully follows the PSP. This section
presents the implementation of the PSP as a set of process
components based on the process script presented in [4].
The PSP is modeled as a set of cooperating components in
OPC that form a process tree as shown in Figure 4.
The PSP specifies a number of forms for collecting
process-related data during process enactment. These
forms are associated with PSP activities tracking time and
defects, and provide input for the PSP Project Plan Summary. PSP enactment also involves the usual products of a
software development process such as requirements documents, design documents, and source code. The OPC

FIGURE 5. PSP process tree in OPC

pl
(ordered)

1

I

planning
Phase
(parallel,
enactable)

I

I

Design
Phase

Coding
Phase

(ordered)

(ordered)

I

I
'0 e
R~v~ew
Phase
(ordered)

I

I

Compile
Phase
(ordered)

Testing
Phase
(ordered)

\.Fix..,
FIGURE 4. PSP process tree

222

1

Postmortem
Phase
(parallel,
non-enactj

ified March 5 1996.

5. Summary
Open Process Components provides a framework for
component-based support for automated software processes. Component-based process support addresses some
of the restrictive characteristics of existing process environments. It allows processes to be viewed as dynamically
interacting components that can be reused across process
models. These components are also interoperable in the
sense they allow process fragments created in heterogeneous process tools to interact with each other without
requiring a translation from one process representation to
another. The result is an open, process-enabled environment as opposed to a closed, monolithic one.
This paper has presented the vision Open Process
Components, provided an overview of the foundation of
the approach, and described an implementation of components for the Personal Software Process. OPC has been
implemented in Java, providing capabilities for creating
dynamic, reusable process components. Our plan to use
this environment to implement repositories of components
for a variety of software processes. For example, in addition to the PSP, the ISPW-6 Software Process Scenario has
also been implemented in OPC [3].
The Open Process Components framework is the first
step toward a vision of distributed, dynamic process components. Powerful new functionality, such as dynamic
lookup of components (brokering) and dynamic scheduling of components can be investigated using this framework. Research is already under way on applying
brokering capabilities for process components [ 121.

16.1

Humphrey, W. A Discipline for Sofnvare Engineering.
Addison-Wesley,Reading MA. 1995.

17.1

Lee, J., Gruniger, M., Jin, Y., Malone, T., Tate, A., Yost,
G., and the PIF Working Group, The PIF Process Interchange Format v.1.2. Available at http://ccs.mit.edu/pif,
December 8 1997. Also published in The Knowledge‘Engineering Review, wol13(1), pp. 91-120. CambridgeUniversity Press. March 1998.

v.1

Miller, J.A., Sheth, A., Kochut, K.J., and Wang, X.Corbabased Run-timeArchitectures for Workflow Management
Systems. Journal of Database Management,Vol. 7, pp.
16-27. 1996.

19.1

Osterweil, L. Software Processes Are SoftwareToo. Proceedings of the 9th International Conferenceon Software
Engineering (ICSE 9). IEEE Computer Society Press.
Monterey, CA. 1987.

6. References

113.1 Swenson, K. SimpleWorkflow Access Protocol (SWAP).
Internet Draft Standard of the Internet Engineering Task
Force (IETF). Available at http://www.aiim.org/wfmc.
August7 1998.

11.1

12.1

111.1 PCIS2Working Group. PCIS2Architecture Specification,
Version 1.0.Lindquist, T. (4.). Available at http://
pcis2.nosc.mil. January 1998.
112.1 Sauer, L. Brokering Process Components.Ph.D. Dissertation Proposal, Department of ComputerScience, Arizona
State University. December, 1997.

Ben-Shaul, I. and Kaiser, G. An Architecture for Federation of Process-Centered Environments.Journal of Automated Software Engineering, vol. 5 , no. 1, pp. 97-132.
January 1998.

114.1 Wallnau, K., Long, F., and Earl, A. Toward a Distributed,
Mediated Architecture for Workflow Management. Proceedings of the NSF Workshop on Workflow and Process
Automation in Information Systems. May 1996.

Conradi, R. Femstrom, C., Fuggetta, A. and Snowdon,R.
Towards a Reference Framework for Process Concepts.
Proc. of the 2nd European Workshopon SoftwareProcess
Technology (EWSPT’92),pp. 3-17, Trondheim, Norway.
September 1992.

13.1

Gary, K. Open Process Components.Ph.D. dissertation,
Department of Computer Science, Arizona State University. January 7, 1999.

14.1

Humphrey, W. Introduction to the Personal Sofhvare
Process. Addison-Wesley,Reading MA. 1997

15.1

110.1 Osterweil, L. Software Processes Are Software Too, Revisited An Invited Talk on the Most Influential Paper of
ICSE 9. Proceedings of the 19th International Cnoference
on SoftwareEngineering. pp. 540-548. Boston, MA. May
1997.

115.1 The Workflow Management Coalition. The Reference
Model. WfMC Document Number TC00-1003, January
1995.
116.1 The Workflow Management Coalition. Interface 1: Process Definition InterchangeProcess Model. WfMC Document Number TC-1016-P, Version 7.05 Beta. August 5
1998.
~ 7 . 1The Workflow Management Coalition. Interoperability
Abstract Specification. WfMC Document Number TC1012,Version 1.0. October 20 1996.

Humphrey,W. A Specificationfor Automated Supportfor
the PSP. Available at http://www.sei.cmu.edu. Last mod-

223

Component-based Software Process Support

Kevin Gary, Tim Lindquist, and Harry Koehnemann
Computer Science and Engineering Department
Arizona State University
Tempe, Arizona 85287-5406
Jean-Claude Derniame
Laboratoire lorrain de Recherche en Informatique et Applications
LORIA : Bd des Aiguillettes
BP 239 54 506 Vandoeuvre Cedex
Abstract
Only recently has the research community started to consider how to make software process models interoperable
and reusable. The task is difficult. Software processes are
inherently creative and dynamic, difficult to define and
repeat at an enactable level of detail. Additionally,
interoperability and reusability have not been considered
important issues. Recent interoperability and reusability
solutions advocate the development of standard process
model representations based on common concepts or
generic schemas, which are used as a basis for translating
between heterogeneous process representations. In this
paper we propose an alternative approach through the
development of process-based components. We present the
Open Process Components Framework, a componentbased framework for software process modeling. In this
approach, process models are constructed as sets of components which interact in meaningful ways. Interoperability and reuse are obtained through encapsulation of
process representations, an explicit representation of process state, and an extendable set of class relationships.

1.0 Introduction
Since Osterweil’s proposal[12] for automating the software process a decade ago, there has been significant
debate about how to best define, execute, and support software processes.The Software Process community has proposed various modeling formalisms including Petri nets
[9], rule-based formalisms [1,8,13], process programming
languages [15], event-based representations [3,6,10],
object-oriented approaches [6,10] and hybrids. Despite
This work was supported in part by the Office of Naval Research
under Award number N00014-97-1-0872

their benefits, systems based on these formalisms create
enactable process models which are not interoperable nor
reusable with one another. The prevailing solution is to
advocate an intermediary standard process representation
and provide translations for interoperability and reuse. We
do not believe this approach is scalable and defeats the
purpose of using heterogeneous process representations.
We advocate an object-oriented, component-based philosophy for providing software process interoperability and
reuse. This paper presents Open Process Components, a
component-based framework for software process definition and enactment. In this framework, components are
well-encapsulated representations of process entities that
interact in meaningful ways. The framework is solidly
founded on mature concepts in the software process field,
and yet is extendable so that process models may be customized in a particular domain. A componentized view of
process representations results in easier process definition,
modularized process enactment, natural interoperability,
and greater potential for reuse.

2.0 A Component-based Process Philosophy
Enactable software processes are typically not repeatable.
Instances vary according to constantly changing demands
of specific projects. Fully elaborating a software process
model to an enactable level of granularity is often too
tedious, time-consuming, and costly[4].
Motivated by the need for interoperability and reuse, we
advocate applying component-based techniques to software process modeling. Constructing software process
models as sets of interacting components allows interoperability by encapsulating the semantics of existing representations. Reuse is achieved by brokering for predefined
components.

A component-based approach:

2.1.2 Process Component States

•
•
•
•

The elements of the meta-model appear in most process
models. Each model requires a different enactment service
to interpret the representation and execute the process.
Regardless of the formalism employed and the interpreter
used, all models define actions on the entities within the
process domain, which effect the states of those entities.
The result of these actions can be modeled using a traditional finite state machine. The OPC framework includes
finite state machines as part of its basic abstractions.
Finite state machines are represented through a state transition graph. Using state transition graphs explicitly for
process modeling is not unique[7,11]. The OPC framework
defines a basic set of states and transitions for Process and
Activity components. These include states such as executing, suspended, and aborted, with corresponding transitions between states defined by actions such as
startProcess, suspendProcess, and abortProcess.
Each component may dynamically modify its state transition graph or even construct a new one. A new state transition graph reflects the object’s unique behavior when
interacting with other components within the framework.
The current class definition for state transition graphs
include operations to add and remove states and allowable
transitions between states, making a component’s state and
behaviors affecting state explicit and manipulable.

avoids deep integration of semantic models
handles the natural complexity of software processes,
responds to dynamic software processes, and
facilitates reuse, minimizing one-shot process models.
Component-based process modeling requires a framework
for developing components. The framework must identify
process entities, define meaningful interactions between
entities, and be able to incorporate new representations.

2.1 The Open Process Components Framework
The Open Process Components (OPC) framework provides
a foundation for developing, integrating, maintaining, and
reusing a variety of process representations. The framework defines basic abstractions of the problem space that
can be specialized. Yet, the framework must make some
commitments regarding the representation and manipulation of processes. The OPC framework is constructed upon
three categories of abstractions, 1) a meta-model that identifies fundamental process entities and relationships, 2) a
per component representation of process states and transitions, and 3) a set of class relationships layered in a structured fashion to produce type definitions for components.

2.1.1 The Basic Meta-model
The OPC Framework is derived from a set of basic abstractions contained in a meta-model. Instead of using this
meta-model as a basis for translation between process
models, we use it as a foundation for identifying elements
of the process space for componentization, and for defining
meaningful ways in which process components interact.
has_sub

role

activity
assigned_to

can_perform

has_input

product

has_output
consists_of

has_version
has_variant

agent

process

has_sub

FIGURE 1. The Open Process Components Framework
Meta-Model

The meta-model is centered around process entities representing work (Process and Activity), people (Role and
Agent), and artifacts (Product). The meta-model defines
the “rules of engagement” for components. It identifies
what component types interact with what other component
types under what relationships. These relationships are not
static; process components and component relationships
are highly dynamic during the course of the component’s
life cycle.

2.1.3 Component Class and Interface Definitions
The OPC framework identifies classes and interfaces in a
layered, three-tier software architecture (Figure 2). The
Framework Layer defines classes and interfaces modeling
process entities derived from the OPC meta-model. The
Representation Layer classes encapsulate process representations behind well-defined interfaces so that heterogeneous representations can interoperate. The Component
Layer extends representations to particular domains. It is
from this layer that actual Process component objects are
instantiated. A process model in the OPC framework is a
set of components, realized as objects of Component Layer
classes, and a set of relations between those components,
created under the constraints of the Framework Layer,
implemented using Representation Layer semantics.
Figure 2 shows example classes at each layer of abstraction
for the meta-model element Process. The Framework
Layer contains the class Process, derived from the metamodel Process entity of Figure 1. The Representation
Layer is comprised of class definitions for specific process
representations, such as event-based processes (ECAProcess), sequencing processes (OrderedProcess), Petri Nets
(PetriNetProcess), rule-based representations (RuleProcess), process definition languages (PDLProcess), and any
other representations we wish to encapsulate. The Compo-

Process

Framework
Layer

Process
Representation
Layer

PDLProcess

ECAProcess

OrderedProcess
PetriNetProcess

Component
Layer
Bug Fix

RuleProcess

Code Module
Code Module

Integration Test

Design

Peer Review
Stress Test

FIGURE 2. Object-oriented class diagram for Process
components

nent Layer contains type definitions for actual process
types. The dashed lines between layers in Figure 2 denotes
that the Representation and Component Layers in fact can
have many levels. This allows for multiple ways in which
to extend and specialize the framework.
The first step identifies a base set of classes and interfaces
at the Framework Layer. The next step is to construct
encapsulations of process representations in the Representation Layer. The semantics of implementing Processes are
encapsulated behind the interfaces inherited from the
Framework Layer. For example, the implementation may
come from a COTS process tool. Finally, components
defined at the Component Layer delegate their implementation to Representation Layer objects. Delegation is used
since inheritance would tie the component’s type to its
implementation. Component Layer objects are configurable. Component Layer classes represent generic process
models. Actual components, or instances, represent customized process models. A component who has its relationships to other components (Process, Activity, Product,
Role, or Agent) fully specified and bound is part of an
instantiated process model[5].
Extending the OPC framework is straightforward. Subclassing the Framework Layer provides specialized implementations according to the semantics of given
representations. Delegating Representation Layer classes
provides implementations for the Component Layer. Create components from Component Layer classes to customize process models. Defining relationships between
components instantiates process models. This methodology is a straightforward object-oriented approach for providing Conradi’s levels of process specialization[5].

3.0 PCIS2 Process Services
The Open Process Components framework is currently
used as a basis for the PCIS2 process services specification. PCIS2 is an architecture for supporting wide-area

software development support. PCIS2 services include
Configuration Management, Traceability, and Process Support. Services are integrated and distributed via CORBA.
The process support services in PCIS2 are based on the
OPC specification and the Workflow Management Coalition (WfMC) sponsored Workflow Facility specification,
known as jFlow[11], submitted to the OMG. The jFlow
specification is largely an “object-ization” of existing
WfMC interfaces[16]. This is not a drawback, but one of
the strengths of the OMG’s approach to adopting and
adapting existing technology. The jFlow specification
improves upon the original WAPI specifications by defining appropriate interactions between objects to gain
interoperability and maintainability of workflow systems.
The PCIS2 specification is object-oriented from the ground
up, but has borrowed some of the jFlow concepts in order
to maintain compliance with emerging standards.
PCIS2 and the jFlow specification differ in three areas.
First, PCIS2 supports dynamic processes through ad hoc
process support, and the ability to modify process definitions during enactment. Second, PCIS2 supports shared
and circulating products, providing a mechanism for reasoning about data artifacts of the process. Finally, PCIS2
incorporates support for the metaprocess, by defining
views on its services for controlling, defining, performing,
and monitoring processes. jFlow only defines interfaces for
performing (enacting) and monitoring workflows.
It should also be pointed out that jFlow identifies concepts
not provided in PCIS2, such as the ability to assign arbitrary resources (including human and computer performers) to a process. Despite the differences in these
specifications, they are largely complementary and both
provide important contributions to process standardization.

4.0 Related Work
The component philosophy espoused in this paper is similar in motivation to the recent work presented by Avrilionis, Belkhatir, and Cunin[2] on the Pynode project. The
authors propose the construction of software process components for producing process artifacts. A “software process component” is essentially a process model fragment
written in some Process Modeling Language (PML). Components are dynamically combined to construct complete
process models through interface types and their respective
“connectors ports”. The authors correctly motivate the
need to eliminate monolithic process systems and instead
provide reuse and integration capabilities for process representations. However, the approach lacks adherence to
foundational concepts, such as those used in OPC (see
Section 2.1). The three-tier layering of the OPC framework
provides a structured, extendable way to develop interoperable and reusable process-oriented components. Despite
their differences, the Pynode component approach is simi-

lar in philosophy and motivation to the OPC framework,
and appears to be at roughly the same level of maturity.
Results of these two experiments will be very useful to the
software process modeling community.
A more methodological object-oriented approach to process modeling is taken by Shams-Aliee and Warboys[14].
The authors view the object space and the process space at
different levels. The object space is data-oriented, whereas
the process space emphasizes process as a set of state transformations. The authors go on to argue for a layer that
brings together the object level and the process level
together. Shams-Aliee and Warboys[14] also advocate
modeling a process as a collection of objects or components. However, we find the distinction between the object
level and the process level unnecessary. In particular, we
do not agree that the object level is a data-oriented model.
In the OPC framework, we view process entities themselves as objects, and are concerned with the behaviors of
these objects as defined by their interfaces. OPC merges
objects and processes into components through an explicit
representation of process state contained in the component.
We propose a full object-oriented framework that includes
class definitions, inheritance, and rules for component
interaction. This merging of objects and processes into a
complete component-based model allows OPC the full
potential to achieve interoperability and reuse by being
independent of any process modeling formalism.

Software Process (ICSP4). December, 1996.
[3.]

Ben-Shaul, I. and Kaiser, G. An Interoperability Model for
Process-Centered Software Engineering Environments
and its Implementation in Oz. Technical Report CUCS034-95, Computer Science Department, Columbia University. 1995.

[4.]

Christie, A., Levine, L., Morris, E., Zubrow, D., Belton, T.,
Proctor, L., Cordelle, D., Ferotin, J. A Study into the Current Usage of Software Process Automation. Proc. of the
NSF Workshop on Workflow and Process Automation in
Information Systems, Athens, GA, May, 1996.

[5.]

Conradi, R. Fernstrom, C., Fuggetta, A. and Snowdon, R.
Towards a Reference Framework for Process Concepts.
Proc. of EWSPT’92, pp. 3-17, Trondheim, Norway. September 1992.

[6.]

Conradi, R., et. al. Object-Oriented and Cooperative Process Modelling in EPOS. In Software Process Modeling
and Technology, A. Finklestein, J. Kramer, and
B.Nuseibeh (Eds.), pp. 33-70. John Wiley. 1994.

[7.]

Derniame, J.C. Life Cycle Process Support in PCIS. Proc.
of the PCTE ‘94 Conference. 1994.

[8.]

Derniame, J.C., and Gruhn, V. Development of ProcessCentered IPSEs in the ALF Project. Journal of Systems Integration, vol. 4, pp. 127-150. 1994.

[9.]

Emmerich, W. and Gruhn, V. FUNSOFT nets: A Petri-net
Based Software Process Modeling Language. Proc. of the
6th International Workshop on Software Specification and
Design, Como, Italy. September 1991.

[10.]

Melo, W.L. and Belkhatir, N. TEMPO: A Support for the
Modeling of Objects with Dynamic Behavior. In A. Verbraeck, H.G. Sol, and P.W.G. Bots (Eds) Dynamic Modeling and Information Systems. Elsevier Science Publishers,
North Holland. 1994.

[11.]

Object Management Group. jFlow Joint Submission.
OMG Document Number bom/98-06-07. July 4, 1998.

[12.]

Osterweil, L. Software Processes are Software Too. Proc.
of the 9th International Conference on Software Engineering, Monterey, CA. IEEE Computer Society Press. 1987.

[13.]

Peuschel, B. and Schafer, W. Concepts and Implementation of a Rule-based Process Engine. Proceedings of the
14th International Conference on Software Engineering,
pp. 262-279. May, 1992.

[14.]

Shams-Aliee F., and Warboys, B. Applying Object-Oriented Modelling to Support Process Technology. Proceedings
of the First World Conference on Design and Process
Technology (IDPT-Vol.1). Ertag, A. et al (ed.). Society for
Design and Process Science, Austin, TX. December 1995.

[15.]

Sutton, S., Heimbigner, D., and Osterweil, L. Language
Constructs for Managing Change in Process-centered Environments. Proc. of the 4th ACM SIGSOFT Symposium
on Software Development Environments, Irvine, CA. December 1990.

[16.]

Workflow Management Coalition. The Reference Model.
WfMC Document Number TC00-1003, January 1995.

5.0 Summary and Future Work
In this paper we have presented a component-based framework for constructing interoperable and reusable software
processes. This framework identifies common concepts in
the research community and defines an object-oriented
framework for applying these concepts. This framework is
currently employed in the construction of a software architecture for support distributed software development.
This approach, together with related efforts in the field of
workflow, makes the important contribution that the software process automation field is maturing to the point that
efforts such as the one described herein can be attempted.
Despite whether the reader agrees with the design of this
framework, providing interoperability and reusability will
overcome one of the serious hurdles preventing wide scale
deployment of software process automation technology.

6.0 References
[1.]

Arbaoui, S., Mouline, S., Oquendo, F., and Tassart, G.
PEACE: Describing and Managing Evolving Knowledge
in Software Process. Proc. of EWSPT ‘92, Trondheim,
Norway. September, 1992.

[2.]

Avrilionis, D., Belkhatir, N., and Cunin, P. A Unified
Framework for Software Process Enactment and Improvement. Proc. of the 4th International Conference on the

