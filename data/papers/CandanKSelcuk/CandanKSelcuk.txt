LR-PPR: Locality-Sensitive, Re-use Promoting, Approximate Personalized PageRank Computation
Jung Hyun Kim
Arizona State University Tempe, AZ 85287, USA



K. Selçuk Candan
Arizona State University Tempe, AZ 85287, USA

Maria Luisa Sapino
University of Torino I-10149 Torino, Italy

jkim294@asu.edu ABSTRACT

candan@asu.edu

mlsapino@di.unito.it
G
v1 d c a b v2

Personalized PageRank (PPR) based measures of node proximity have been shown to be highly effective in many prediction and recommendation applications. The use of personalized PageRank for large graphs, however, is difficult due to its high computation cost. In this paper, we propose a Locality-sensitive, Re-use promoting, approximate personalized PageRank (LR-PPR) algorithm for efficiently computing the PPR values relying on the localities of the given seed nodes on the graph: (a) The LR-PPR algorithm is locality sensitive in the sense that it reduces the computational cost of the PPR computation process by focusing on the local neighborhoods of the seed nodes. (b) LR-PPR is re-use promoting in that instead of performing a monolithic computation for the given seed node set using the entire graph, LR-PPR divides the work into localities of the seeds and caches the intermediary results obtained during the computation. These cached results are then reused for future queries sharing seed nodes. Experiment results for different data sets and under different scenarios show that LR-PPR algorithm is highly-efficient and accurate.

v3

Figure 1: Key questions: Given a graph, G, and a seed set of nodes S = {v1 , v2 , v3 } in G, can we rank the remaining nodes in the graph regarding their relationships to the set S ? Which of the nodes a through d is the most interesting given the seed set of nodes v1 through v3 ? node relatedness, on the other hand, also take into account the density of the edges: unlike in path-based definitions, random walkbased definitions of relatedness also consider how tightly connected two nodes are and argue that nodes that have many paths between them can be considered more related. Random-walk based techniques encode the structure of the network in the form a transition matrix of a stochastic process from which the node relationships can be inferred. When it exists, the convergence probability of a node n gives the ratio of the time spent at that node in a sufficiently long random walk and, therefore, neatly captures the connectivity of the node n in the graph. Therefore, many web search and recommendation algorithms, such as PageRank [5], rely on random-walks to identify significant nodes in the graph: let us consider a weighted, directed graph G(V, E ), where the weight of the edge ej  E is denoted as wj ( 0) and where = 1.0. The PageRank score of the node ej outedge(vi ) wj vi  V is the stationary distribution of a random walk on G, where at each step with probability 1- , the random walk moves along an outgoing edge of the current node with a probability proportional to the edge weights and with probability  , the walk jumps to a random node in V . In other words, if we denote all the PageRank scores of the nodes in V with a vector  , then  = (1 -  )TG ×  +  j, where TG denotes the transition matrix corresponding to the graph G (and the underlying edge weights) and j is a teleportation vector 1 . where all entries are V

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Miscellaneous

Keywords
Personalized PageRank; Locality-Sensitivity; Reuse-Promotion

1.

INTRODUCTION

Node distance/proximity measures are commonly used for quantifying how nearby or otherwise related to two or more nodes on a graph are. Path-length based definitions [17] are useful when the relatedness can be captured solely based on the properties of the nodes and edges on the shortest path (based on some definition of path-length). Random-walk based definitions, such as hitting distance [16] and personalized page rank (PPR) score [4, 13, 21] of
 This work is supported by NSF Grant 1043583 "MiNC: NSDL Middleware for Network- and Context-aware Recommendations".

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM'13, Oct. 27­Nov. 1, 2013, San Francisco, CA, USA. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2263-8/13/10 ...$15.00. http://dx.doi.org/10.1145/2505515.2505651.

1.1 Proximity and PageRank
An early attempt to contextualize the PageRank scores is the topic sensitive PageRank [12] approach which adjusts the PageRank scores of the nodes by assigning the teleportation probabilities in vector j in a way that reflects the graph nodes' degrees of

G
G1
v1 v2

G2

G1

incoming bnd. node of G1

G
G2

a shared node
v1
v3

v2

G3

Figure 2: Locality-sensitivity: Computation of PPR should focus on the neighborhoods (localities) of the seeds
G
G1
v1 v2

outgoing bnd. node of G1

G
G1
v1 v7

G2

G7

Figure 4: Incoming and outgoing boundary nodes/edges and a node shared between two localities · locality sensitive in the sense that it reduces the computational cost of the PPR computation process and improve accuracy by focusing on the neighborhoods of the seed nodes (Figure 2); and · re-use promoting in that it enables caching and re-use of significant portions of the intermediary work for the individual seed nodes in future queries (Figure 3). In the following section, we first formally introduce the problem and then present our solution for locality-sensitive, re-use promoting, approximate personalized PageRank computations. We evaluate LR-PPR for different data sets and under different scenarios in Section 3. We conclude in Section 4.

v3

v6

v9

G3

G6 G9

(a) PPR query 1

(b) PPR query 2

Figure 3: Re-use promotion: Two PPR queries sharing a seed node (v1 ) should also share relevant work match to the search topic. [6, 7] were among the first works which recognized that random-walks can also be used for measuring the degree of association, relatedness, or proximity of the graph nodes to a given seed node set, S  V (Figure 1). An alternative to this approach is to modify (as in topic sensitive PageRank [12]) the teleportation vector, j : instead of jumping to a random node in V with probability  , the random walk jumps to one of the nodes in the seed set, S , given by the user. More specifically, if we denote the personalized PageRank scores of the nodes in V with a vector , then  = (1 -  )TG ×  + s,
1 where s is a re-seeding vector, such that if vi  S , then s[i] = S and s[i] = 0, otherwise. One key advantage of this approach over modifying the transition matrix as in [6] is that the term  can be used to directly control the degree of seeding (or personalization) of the PPR score. However, the use of personalized PageRank for large graphs is difficult due to the high cost of solving for the vector , given  , transition matrix TG , and the seeding vector s. One way to obtain  is to solve the above equation for  mathematically. Alternatively, PowerIteration methods [14] simulate the dissemination of probability mass by repeatedly applying the transition process to an initial distribution 0 until a convergence criterion is satisfied. For large data sets, both of these processes are prohibitively expensive. Recent advances on personalized PageRank includes top-k and approximate personalized PageRank algorithms [1, 3, 8, 10, 11, 20, 22] and parallelized implementations on MapReduce or Pregel based batch data processing systems [2, 15]. The FastRWR algorithm presented in [22] for example partitions the graph into subgraphs and indexes partial intermediary solutions. Unfortunately, for large data sets, FastRWR requires large number of partitions to ensure that the intermediary metadata (which requires dense matrix representation) fits into the available memory and this negatively impacts execution time and accuracy.

2. PROPOSED APPROACH
Let G = (V, E ) be a directed graph. For the simplicity of the discussion, without any loss of generality, let us assume that G is unweighted1 . Let us be given a set S  V of seed nodes (Figure 1) and a personalization parameter,  . Let GS = {Gh (Vh , Eh ) | 1  h  K } be K = S subgraphs of G, such that · for each vi  S , there exists a corresponding Gi  GS such that vi  Vi and · for all Gh  GS , Gh G . We first formalize the locality-sensitivity goal (Figure 2): Desideratum 1: Locality-Sensitivity. Our goal is to compute an approximate PPR vector, apx , using GS instead of G, such that apx  , where  represents the true PPR scores of the nodes in V relative to S : i.e., apx   = (1 -  )TG ×  + s, where TG is the transition matrix corresponding to G and s is the re-seeding vector corresponding to the seed nodes in S . We next formalize the re-use promotion goal (Figure 3): Desideratum 2: Reuse-Promotion. Let S1 and S2 be two sets of seed nodes and let vi be a node such that vi  S1  S2 . Let also the approximate PPR vector, apx,1 corresponding to S1 have already been computed using GS1 and let us assume that the approximate PPR vector, apx,2 corresponding to S2 is being requested. The part of the work performed when processing Gi  GS1 (corresponding to vi ) should not need to be re-performed when processing Gi  GS2 , when computing apx,2 using GS2 .

1.2 Contributions of this Paper
In this paper, we argue that we can improve both scalability and accuracy through a Locality-sensitive, Re-use promoting, approximate personalized PageRank (LR-PPR) algorithm: LR-PPR is

2.1
1

Combined Locality and its Boundary

Unlike existing approximate PPR algorithms [1, 3, 8, 10, 11, 20, 22], LR-PPR is location sensitive. Therefore, given the set, S , of Extending the proposed algorithms to weighted graphs is trivial.

G1
v1 v2

G2
v3

G3

GK
vK



A node shared by multiple seed locality graphs

 0 V2 × V1  ...  0 VK × V1 01× V1

M1

0 0

V1 × V2

M2 ...

01×

VK × V2 V2

. . . 0 V1 × VK . . . 0 V2 × VK ... ... ... MK ... 01× VK

0 0

V1 ×1 V2 ×1

   , 

0 VK ×1 MK +1

...

Figure 5: An equivalence set consists of the copies of a node shared across multiple seed locality graphs seed nodes and the corresponding localities, GS , the computation focuses on the combined locality G+ (V + , E + )  G, where V+ =
1lK

Vl and E + =
1lK

El .

Given a combined locality, G+ , we can also define its external graph, G- (V - , E - ), as the set of nodes and edges of G that are outside of G+ and boundary nodes and edges. As shown in Figure 4, we refer to vi  Vl as an outgoing boundary node of Gl if there is an outgoing edge ei,j = [vi  vj ]  E , where vj  / Vl ; the edge ej is also referred to as an outgoing boundary edge of Gl . The set of all outgoing boundary nodes of Gl is denoted as Voutbound,l and the set of all outgoing boundary edges of Gl is denoted as Eoutbound,l. Note that Voutbound,l  Vl , whereas Eoutbound,l  El = . We also define incoming boundary nodes (Vinbound,l ) and incoming boundary edges (Einbound,l ) similarly to the outgoing boundary nodes and edges of Gl , but considering inbound edges to these subgraphs. More specifically, Einbound,l consists of edges of the form [vi  vj ]  E , where vj  Vl and vi  / Vl .

where MK +1 is equal to the 1 × 1 matrix 01×1 . Intuitively, Mbd combines the K subgraphs into one transition matrix, without considering common nodes/edges or incoming/outgoing boundary edges and ignoring all outgoing and incoming edges. All the external nodes in G- are accounted by a single node represented by the 1 × 1 matrix MK +1 . A key advantage of Mbd is that it is block-diagonal and, hence, there are efficient ways to process it. However, this block-diagonal matrix, Mbd , cannot accurately represent the graph G as it ignores potential overlaps among the individual localities and ignores all the nodes and edges outside of G+ . We therefore need a compensation matrix to · make sure that nodes and edges shared between the localities are not double counted during PPR computation and · take into account the topology of the graph external to both localities G1 through GK .

2.2.3 Compensation Matrix, M 0
Let t be ( V1 + V2 + . . . + VK + 1). The compensation matrix, M0 , is a t × t matrix accounting for the boundary edges of the seed localities as well as the nodes/edges in G- . M0 also ensures that the common nodes in V1 through VK are not double counted during PPR calculations. M0 is constructed as follows: Row/column indexing: Let vl,i be a vertex in Vl . We introduce a row/column indexing function, ind(), defined as follows:   ind(l, i) = 
1h<l

2.2 Localized Transition Matrix
Since LR-PPR focuses on the combined locality, G+ , the next step is to combine the transition matrices of the individual localities into a combined transition matrix. To produce accurate approximations, this localized transition matrix, however, should nevertheless take the external graph, G- , and the boundaries between G- and G+ , into account.

Vh  + i

2.2.1 Transition Matrices of Individual Localities
Let v(l,i) (1  l  K ) denote a re-indexing of vertices in Vl . If v(l,i)  Vl and vc  V s.t. v(l,i) = vc , we say that v(l,i) is a member of an equivalence set, Vc (Figure 5). Intuitively, the equivalence sets capture the common parts across the localities of the individual seed nodes. Given Gl (Vl , El )  G and an appropriate re-indexing, we define the corresponding local transition matrix, Ml , as a Vl × Vl matrix, where · ei,j = [v(l,i)  v(l,j ) ]  El  Ml [j, i] = 0 and
1 , out(v(l,i) )

Intuitively the indexing function, ind(), maps the relevant nodes in the graph to their positions in the M0 matrix. Compensation for the common nodes: Let el,i,j be an edge [v(l,i)  v(l,j ) ]  El and let v(l,j ) be a member of the equivalence set Vc for some vc  V . Then, if Vc > 1 -1 1 × out(G,v and · M0 [ind(l, j ), ind(l, i)] = - Vc Vc l,i ) · v(h,k)  Vc s.t. v(h,k) = v(l,j ) , we have M0 [ind(h, k), ind(l, i)] = - 1 1 × , Vc out(G, vl,i )

· ei,j = [v(l,i)  v(l,j ) ]  El  Ml [j, i] =

where out(v(l,i) ) is the number of outgoing edges of vi .

2.2.2 Localization of the Transition Matrix
Given the local transition matrices, M1 through MK , we localize the transition matrix of G by approximating it as Mapx = Mbd + M0 , where Mbd is a block-diagonal matrix of the form

where out(G, v ) is the outdegree of node v in G. Intuitively, the compensation matrix re-routes a portion of the transitions going towards a shared node in a given locality Vl to the copies in other seed localities. This prevents the transitions to and from the shared node from being mis-counted. Compensation for outgoing boundary edges: The compensation matrix needs to account also for outgoing boundary edges that are not accounted for by the neighborhood transition matrices M1 through MK : · Accounting for boundary edges from nodes in Vl to nodes in Vh : [v(l,i)  v(h,j ) ]  Eoutbound,l ­ M0 [ind(h, j ), ind(l, i)] =
1 out(v(l,i) )

· Accounting for boundary edges from nodes in Vl to graph nodes that are in V - : if [v(l,i)  v ]  Eoutbound,l s.t. v  V -

­ M0 [t, ind(l, i)] =

the number of edges of the form [v(l,i)  v ]  Eoutbound,l where v  V - else M0 [t, ind(l, i)] = 0 The compensation matrix records all outgoing edges, whether they cross into another locality or they are into external nodes in G- . If a node has more than one outgoing edge into the nodes in G- , all such edges are captured using one single compensation edge which aggregates all the corresponding transition probabilities. Compensation for incoming boundary edges (from G- ): Similarly to the outgoing boundary edges, the compensation matrix needs also to account for incoming boundary edges that are not accounted for by the neighborhood transition matrices M1 through MK . Since incoming edges from other localities have been accounted for in the previous step, here we only need to consider incoming boundary edges (from G- ). Following the formulation in [23], we account for incoming edges where the source is external to G+ and the destination is a vertex v(l,i) in Vl by inserting an edge from the dummy node to v(l,i) with a weight that considers the outdegrees of all external source nodes; i.e., v(l,i) s.t. [vk  v(l,i) ]  Einbound,l where vk  V - and v(l,i) is in the equivalence set Vc for a vc  V , M0 [ind(l, i), t] is equal to 1 Vc
([vk v(l,i) ]Einbound,l )(vk V
-)

bnd(v(l,i) ) , out(v(l,i) )

where bnd(v(l,i) ) is

the nodes in V + . In particular, we rely on the following result due to [22], which itself relies on the Sherman-Morisson lemma [18]: Let C = A + USV. Let also (I - cA)-1 = Q-1 . Then, the equation r = (1 - c)(I - cA)-1 e has the solution r = (1 - c)(Q-1 e + cQ-1 UVQ-1 e), where  = (S-1 - cVQ-1 U)-1 . If A is a block diagonal matrix consisting of k blocks, A1 through Ak , then Q-1 is also a block diagonal matrix con1 -1 sisting of k corresponding blocks, Q- 1 through Qk , where -1 -1 Qi = (I - cAi ) . We use the above observation to efficiently obtain PPR scores by setting c = (1 -  ), C = Mapx , A = Mbd , and USV = M0 . In particular, we divide the PPR computation into two steps: a locality-sensitive and re-usable step involving the computation of the Q-1 term using the local transition matrices and a run-time computation step involving the compensation matrix.
1 2.4.1 Locality-sensitive and Re-usable Q - bd

1 out(G,vk )

V-

,

where out(G, v ) is the outdegree of node v in G. Compensation for the edges in G- : We account for edges that are entirely in G- by creating a self-loop that represents the sum of outdegree flow between all external nodes averaged by the number of external nodes; i.e., M0 [t, t] =
v V - out(G- ,v ) out(G,v ) V-

,

where out(G- , v ) and out(G, v ) are the outdegrees of node v in G- and G, respectively. Completion: For any matrix position p, q not considered above, no compensation is necessary; i.e., M0 [p, q ] = 0.

Local transition matrices, M1 through MK corresponding to the seeds v1 through vK are constant (unless the graph itself evolves -1 1 is computed over time). Therefore, if Q- h = (I - (1 -  )Mh ) 1 and cached once, it can be reused for obtaining Q- bd , which is a 1 -1 block diagonal matrix consisting of Q- 1 through QK +1 (as before, -1 the last block, QK +1 , is simply equal to 11×1 ):   1 Q- 0 V1 × V2 . . . 0 V1 × VK 0 V1 ×1 1 - 1 0 V × V Q2 . . . 0 V2 × VK 0 V2 ×1  2 1    . . . . . . . . . . . . ...  ,  -1  0 V × V1 0 . . . Q 0 V × V V × 1 K 2 K K K -1 01× V1 01× V2 ... 01× VK QK +1

2.4.2 Computation of the LR-PPR Scores
In order to be able to use the above formulation for obtaining the PPR scores of the nodes in V + , in the query time, we need to decompose the compensation matrix, M0 , into U0 S0 V0 . While obtaining a precise decomposition in run-time would be prohibitively expensive, since M0 is sparse and since we are looking for an approximation of the PPR scores, we can obtain a fairly accurate lowrank approximation of M0 efficiently [22]: M0 ~ 0S ~0V ~ 0. U

2.3 L-PPR: Locality Sensitive PPR
Once the block-diagonal local transition matrix, Mbd , and the compensation matrix, M0 , are obtained, the next step is to obtain the PPR scores of the nodes in V + . This can be performed using any fast PPR computation algorithm discussed in Section 1.1. Note that the overall transition matrix Mapx = Mbd + M0 is approximate in the sense that all the nodes external to G+ are clustered into a single node, represented by the last row and column of the matrix. Otherwise, the combined matrix Mapx accurately represents the nodes and edges in the "merged localities graph" combining the seed localities, G1 through GK . As we see in Section 3, this leads to highly accurate PPR scores with better scalability than existing techniques.

Given this decomposition, the result vector apx , which contains the (approximate) PPR scores of the nodes in V + , is computed as
1 -1 ~ -1 ~ apx =  Q- bd s + (1 -  )Qbd U0 V0 Qbd s ,

where
1 -1 ~ ~- ~ = S 0 - (1 -  )V0 Qbd U0 -1

2.4 LR-PPR: Locality Sensitive and Reuse Promoting PPR
Our goal is not only to leverage locality-sensitivity as in L-PPR, but also to boost sub-result re-use. Remember that, as discussed above, the localized transition matrix Mapx is equal to Mbd + M0 where (by construction) Mbd is a block-diagonal matrix, whereas M0 (which accounts for shared, boundary, and external nodes) is relatively sparse. We next use these two properties of the decomposition of Mapx to efficiently compute approximate PPR scores of

.

Note that the compensation matrix M0 is query specific and, thus, the work done for the last step cannot be reused across queries. However, as we experimentally verify in Section 3, the last step is relatively cheap and the earlier(costlier) steps involve re-usable work. Thus, caching and re-use through LR-PPR enables significant savings in execution time. We discuss the overall complexity and the opportunities for re-use next.

2.5 Complexity and Re-use
Analysis of LR-PPR points to the following advantages: First of all, computation is done using only local nodes and edges. Secondly, most of the results of the expensive sub-tasks can be cached and re-used. Moreover, costly matrix inversions are limited to the smaller matrices representing localities and small matrices of size r × r . Various subtasks have complexity proportional to V + 2 , where V + = 1lK Vl . While in theory the locality Vl can be arbitrarily large, in practice we select localities with a bounded number of nodes; i.e., 1lK , Vl  L for some L V . As described above LR-PPR algorithm supports caching and reuse of some of the intermediary work. The process results in local transition matrices, each of which can be cached in O( El ) space (where El is the number edges in the locality) assuming a sparse representation. The algorithm also involves a matrix inversion, which results in a dense matrix; as a result, caching the inverted matrix takes O( Vl 2 ) space (where Vl is the number of vertices in the locality). If the locality is size-constrained, this leads to constant space usage of O(L2 ), where L is the maximum number of nodes in the locality. If the inverted matrix of a locality is cached, then the local transition matrix does not need to be maintained further. For cache replacement, any frequency-based or predictive cache-replacement policy can be used.
" 
&%

 1 "    
 ( )!
%.. %.. %.' %.%., %-, %.* %.* %,( %*' %'. %-%-+ %-' %,*

%*

%%

*

&% 

'*  1   

*%

,*

Figure 6: Accuracies of L-PPR, LR-PPR, and FastRWR against the Global PPR for different numbers of target nodes Accuracy: For different algorithm pairs, we report the Spearman's rank correlation
i (x i i (x i

-x ¯)(yi - y ¯)
i (yi

-x ¯ )2

-y ¯)2

,

3.

EXPERIMENTAL EVALUATION

In this section, we present results of experiments assessing the efficiency and effectiveness of the Locality-Sensitive, Re-use Promoting Approximate Personalized PageRank (LR-PPR) algorithm. Table 1 provides overviews of the three data sets (from http : //snap.stanford.edu/data/ ) considered in the experiments. We considered graphs with different sizes and edge densities. We also varied numbers of seeds and the distances between the seeds (thereby varying the overlaps among seed localities). We also considered seed neighborhoods (or localities) of different sizes. Experiments were carried out using a 4-core Intel Core i5-2400, 3.10GHz, machine with 8GB memory and 64-bit Windows 7 Enterprise. Codes were executed using Matlab 7.11.0(2010b). All experiments were run 10 times and averages are reported.

which measures the agreement between two rankings (nodes with the same score are assigned the average of their positions in the ranking). Here, x and y are rankings by two algorithms and x ¯ and y ¯ are average ranks. To compute the rank coefficient, a portion of the highest ranked nodes in the merged graph according to x are considered. As default, we considered 10% highest ranked nodes; but we also varied the target percentage (5%, 10%, 25%, 50%, 75%) to observe how the accuracy varies with result size. Memory: We also report the amount of data read from the cache.

3.3 Results and Discussions
Table 2 presents experimental results for FastRWR, L-PPR, and LR-PPR. First of all, all three algorithms are much faster than Global PPR. As expected, in small data sets (Epinions and Slashdot) FastRWR works faster than L-PPR and LR-PPR, though in many cases, it requires more memory. In large data sets, however, L-PPR and LR-PPR significantly outperform FastRWR in terms of query processing efficiency and run-time memory requirement. In terms of accuracy, the proposed locality sensitive techniques, L-PPR and LR-PPR, constantly outperform FastRWR. This is because, FastRWR tries to approximate the whole graph, whereas the proposed algorithms focus on the relevant localities. FastRWR requires large number of partitions to ensure that the intermediary metadata (which requires dense matrix representation) fits into memory and this negatively impacts accuracy. Our localitysensitive algorithms, L-PPR and LR-PPR, avoid this and provide high accuracy with low memory consumption, especially in large graphs, like WikiTalk. Figure 6 confirms that the accuracies of L-PPR and LR-PPR both stay high as we consider larger numbers of top ranked network nodes for accuracy assessment, whereas the accuracy of FastRWR suffers significantly when we consider larger portions of the merged locality graph. Figure 7 studies the execution time behavior for L-PPR, LRPPR, and FastRWR for different number of seed nodes. As the figure shows, the time cost increases for both L-PPR and LR-PPR algorithms as the number of seeds increases. But, the cost of LRPPR (which leverages re-use) increases much slower than the cost of L-PPR and both remain significantly cheaper than FastRWR.

3.1 Alternative Approaches
Global PPR: This is the default approach where the entire graph is used for PPR computation. We compute the PPR scores by solving the equation presented in Section 1.1. FastRWR: This is an approximation algorithm, referred to as NB_LIN in [22]. The algorithm reduces query execution times by partitioning the graph into subgraphs and preprocessing each partition. The pre-computed files are stored on disk and loaded to the memory during the query stage. To be fair to FastRWR, we selected the number of partitions in a way that minimizes its execution time and memory and maximizes its quality. L-PPR: This is our locality sensitive algorithm, where instead of using the whole graph, we use the localized graph created by combining the locality nodes and edges as described in Section 2.2. Once the localized transition matrix is created, the PPR scores are computed by solving the equation presented in Section 1.1. LR-PPR: This is the locality sensitive and re-use promoting algorithm proposed described in detail in Section 2.4. The restart probability,  , is set to 0.15 for all approaches.

3.2 Evaluation Measures
Efficiency: This is the amount of time taken to load the relevant (cached) data from the disk plus the time needed to carry out the operations to obtain the PPR scores.

Table 1: Data sets
Data Set Epinions SlashDot WikiTalk Overall Graph Characteristics # nodes # edges 76K 500K 82K 870K 2.4M 5M Seeds Data set Epinions 76K nodes 500K edges SlashDot 82K nodes 870K edges WikiTalk 2.4M nodes 5M edges # seeds 2 2 3 3 2 2 3 3 2 2 3 3 Dist (#hops) 3 4 3 4 3 4 3 4 3 4 3 4 Locality Graph Characteristics # nodes per neighborhood # edges per neighborhood from 200 to 2000 from 10K to 75K from 700 to 5000 from 10K to 75K from 700 to 6000 from 10K to 75K Execution Time (sec.) Global PPR 27.81 27.58 27.30 27.90 21.79 21.85 21.74 22.93 681.08 693.44 701.34 706.26 Fast RWR 0.21 0.22 0.21 0.22 0.35 0.35 0.36 0.38 16.28 16.22 16.32 16.34 LPPR 0.37 0.51 0.58 0.76 0.70 0.78 1.12 1.39 0.75 0.73 0.75 0.78 LRPPR 0.14 0.20 0.26 0.36 0.53 0.42 0.95 0.83 0.37 0.37 0.37 0.36 # seeds 2-3 2-3 2-8 Seeds seed distances (hops) 3-4 3-4 3-4

Table 2: Summary of the results for different configurations (in all scenarios, individual seed localities have 75K edges)
Merged Network Avg # nodes 2.2K 3.0K 2.7K 3.5K 5.9K 5.7K 7.1K 7.2K 5.7K 5.8K 6.3K 6.7K Avg # edges 90K 99K 108K 120K 117K 125K 141K 159K 102K 100K 101K 103K Top-10% Spearman's Correl. (vs. Global PPR) Fast LLRRWR PPR PPR 0.963 0.997 0.990 0.960 0.998 0.990 0.967 0.998 0.990 0.967 0.997 0.991 0.955 0.973 0.990 0.943 0.965 0.983 0.957 0.971 0.990 0.958 0.976 0.986 0.868 0.958 0.944 0.870 0.930 0.909 0.877 0.937 0.902 0.869 0.976 0.967 Memory usage(MB) Fast RWR 178.3 LPPR 2.9 3.1 4.6 4.7 5.0 4.9 7.6 7.2 15.5 16.2 24.0 28.7 LRPPR 36.3 55.2 57.6 77.7 228.1 172.8 325.9 256.0 114.5 120.7 211.6 197.5

302.1

1429.0

     #  
   "* +( (* 
!
',) ',) ',*


',+





&- &*
  


(&

)& &

*& '( ',


#  
 



Figure 7: Execution times of L-PPR, LR-PPR, and FastRWR for different numbers of seed nodes

4.

CONCLUSIONS

In this paper, we presented a Locality-sensitive, Re-use promoting, approximate Personalized PageRank (LR-PPR) algorithm for efficiently computing the PPR values relying on the localities of the seed nodes on the graph. Instead of performing a monolithic computation for the given seed node set using the entire graph, LRPPR divides the work into localities of the seeds and caches the intermediary results obtained during the computation. These cached results can then be reused for future queries sharing seed nodes. Experiments showed that the proposed LR-PPR approach provides significant gains in execution time relative to existing approximate PPR computation techniques, where the PPR scores are computed from scratch using the whole network. LR-PPR also outperforms L-PPR, where the PPR scores are computed in a locality-sensitive manner, but without significant re-use.

5.

REFERENCES
[1] K. Avrachenkov, N. Litvak, D. Nemirovsky, E. Smirnova, and M. Sokol. Quick Detection of Top-k Personalized PageRank Lists. WAW'11, 2011. [2] B.Bahmani, K.Chakrabarti, and D. Xin. Fast personalized PageRank on MapReduce. In SIGMOD'11. 973-984. 2011. [3] B.Bahmani, A.Chowdhury, and A.Goel. Fast incremental and personalized PageRank. PVLDB. 4, 3, 173-184, 2010. [4] A. Balmin, V. Hristidis, and Y.Papakonstantinou. ObjectRank: Authority-based keyword search in databases. VLDB, 2004.

[5] S. Brin and L. Page. "The anatomy of a large-scale hypertextual Web search engine". Computer Networks and ISDN Systems 30: 107-117, 1998. [6] K. S. Candan and W.-S. Li. Using random walks for mining web document associations. In PAKDD, pp. 294-305, 2000. [7] K. S. Candan and W.-S. Li. Reasoning for Web document associations and its applications in site map construction. Data Knowl. Eng. 43(2), 2002. [8] K. Csalogany, D.Fogaras, B. Racz, and T. Sarlos. Towards Scaling Fully Personalized PageRank: Algorithms, Lower Bounds, and Experiments Internet Math. 2,3, 333-358, 2005. [9] F. Fouss, A. Pirotte, J. Renders, and M. Saerens. Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. TKDE, 2007. [10] Y. Fujiwara, M. Nakatsuji, M. Onizuka, and M. Kitsuregawa. Fast and exact top-k search for random walk with restart. PVLDB. 5, 5, 442-453. 2012. [11] M. Gupta, A. Pathak, and S. Chakrabarti. Fast algorithms for Top-k Personalized PageRank Queries. In WWW'08. 1225-1226. 2008. [12] T.H. Haveliwala. Topic-sensitive PageRank. WWW'02. 517-526. 2002. [13] G. Jeh and J. Widom. Scaling personalized web search. Stanford University Technical Report. 2002. [14] S.D. Kamvar, T.H. Haveliwala, C.D. Manning, and G.H. Golub. Extrapolation methods for accelerating PageRank computations. In WWW'03 261-270. 2003. [15] G. Malewicz, et al. Pregel: a system for large-scale graph processing. SIGMOD'10, 2010. [16] Q. Mei, D. Zhou, and K. Church. Query suggestion using hitting time, CIKM'08, 2008. [17] C. Palmer, P. Gibbons, and C. Faloutsos. Anf: a fast and scalable tool for data mining in massive graphs. KDD'02, 2002. [18] W. Piegorsch and G. E. Casella. Inverting a sum of matrices. In SIAM Review, 1990. [19] P. Sarkar, A.W. Moore, and A. Prakash. Fast incremental proximity search in large graphs. ICML'08, 2008. [20] H. H. Song, et al. Scalable proximity estimation and link prediction in online social networks. In Internet Measurement Conference, pp. 322­335. 2009. [21] H. Tong, C. Faloutsos, and Y. Koren. Fast direction-aware proximity for graph mining. KDD, pp. 747­756, 2007. [22] H. Tong, C. Faloutsos, and J.-Y. Pan. Fast Random Walk with Restart and Its Applications. In ICDM'06. 613-622. 2006. [23] Y. Wu and L. Raschid, ApproxRank: Estimating Rank for a Subgraph, ICDE'09, 54-65, 2009.

    !

PageRank Revisited: On the Relationship between Node Degrees and Node Significances in Different Applications
Jung Hyun Kim
Arizona State University Tempe, AZ 85287, USA



K. Selçuk Candan
Arizona State University Tempe, AZ 85287-8809

Maria Luisa Sapino
University of Torino I-10149 Torino, Italy

jkim294@asu.edu ABSTRACT

candan@asu.edu

marialuisa.sapino@unito.it

Random-walk based techniques, such as PageRank, encode the structure of the graph in the form of a transition matrix of a stochastic process from which significances of the graph nodes can be inferred. Recommendation systems leverage such node significance measures to rank the objects in the database. Context-aware recommendation techniques complement the data graph with additional data that provide the recommendation context. However, despite their wide-spread use in many graph-based knowledge discovery and recommendation applications, conventional PageRank-based measures have various shortcomings. As we experimentally show in this paper, one such shortcoming is that PageRank scores are tightly coupled with the degrees of the graph nodes, whereas in many applications the relationship between the significance of the node and its degree in the underlying network may not be as implied by PageRank-based measures. In fact, as we also show in the paper, in certain applications, the significance of the node may be negatively correlated with the node degree and in such applications a naive application of PageRank may return poor results. To address these challenges, in this paper, we propose degree decoupled PageRank (D2PR) techniques to improve the effectiveness of PageRank based knowledge discovery and recommendation systems. These suitably penalize or (if needed) boost the transition strength based on the degree of a given node to adapt the node significances based on the network and application characteristics.

dation tasks [1, 7, 9, 12, 26]. The significance of a node in a given graph often needs to reflect the topology of the graph. Measures like the betweenness measure [27] and the centrality/cohesion [5], help quantify how significant any node is on a given graph based on the underlying graph topology. The betweenness measure [27], for example, quantifies whether deleting the node would disconnect or disrupt the graph. Centrality/cohesion [5] measures quantify how close to a clique the given node and its neighbors are. Other authority, prestige, and prominence measures [1, 5, 6] quantify the significance of the node through eigen-analysis or random walks, which help measure how reachable a node is in the graph.

1.1 PageRank as a Measure of Significance
Since enumerating all paths among the graph nodes would require time exponential in the size of the graph, random-walk based techniques encode the structure of the network in the form of a transition matrix of a stochastic process from which the node significance can be inferred.PageRank [6] is one of the most widely-used random-walk based methods for measuring node significance and has been used in a variety of application domains, including web search, biology, and social networks. The basic thesis of PageRank is that a node is important if it is pointed to by other important nodes ­ it takes into account the connectivity of nodes in the graph by defining the score of the node vi  V as the amount of time spent on vi in a sufficiently long random walk on the graph. More specifically, given a graph G(V, E ), the PageRank scores are represented as r, where r = TG r + (1 - )t where TG is a transition matrix corresponding to the graph G, t is 1 a teleportation vector (such that t[i] = V ), and  is the residual probability (or equivalently, (1 - ) is the so-called teleportation probability). Unless the graph is weighted, the transition matrix, TG , is constructed such that for a node v with k (outgoing) neighbors, the transition probability from v to each of its (outgoing) neighbors will be 1/k. If the graph is weighted, then the transition probabilities are adjusted in a way to account for the relative weights of the (outgoing) edges.

1.

INTRODUCTION

In recent years, there has been considerable interest in measuring the significance of a node in a graph and relatedness between two nodes in the graph, as if measured accurately, these can be used for supporting many knowledge discovery, search, and recommen This work is supported by NSF Grants 1339835 "E-SDMS: Energy Simulation Data Management System Software", 1318788 "Data Management for Real-Time Data Driven Epidemic Spread Simulations", 1518939 "RAPID: Understanding the Evolution Patterns of the Ebola Outbreak in West-Africa and Supporting Real-Time Decision Making and Hypothesis Testing through Large Scale Simulations", and 1430144 "Fraud Detection via Visual Analytics: An Infrastructure to Support Complex Financial Patterns (CFP) based Real-Time Services Delivery".

1.2 Tight Coupling of PageRank Scores of Nodes and their Degrees
Let us consider an undirected graph G(V, E ). There are two factors that contribute to the PageRank of a given node, v  V : · Factor 1: Significance of Neighbors: The more significant the neighbors of a node are, the higher its likelihood to be also significant. · Factor 2: Number of Neighbors (Degree of the Node) : Even if the neighbors are not all significant, a large number of

c 2016, Copyright is with the authors. Published in the Workshop Proceedings of the EDBT/ICDT 2016 Joint Conference (March 15, 2016, Bordeaux, France) on CEUR-WS.org (ISSN 1613-0073). Distribution of this paper is permitted under the terms of the Creative Commons license CCby-nc-nd 4.0

Data Set Correlation between PageRank and Degree

Listener Graph (Friendship edges, Last.fm) 0.988

Article Graph (co-author edges, DBLP) 0.997

Movie Graph (co-contributor edges, DBLP) 0.848

Table 1: Spearman's rank correlation between the node degree ranks and the node ranks' based on PageRank scores for various data graphs (see Section 4 for details of the data sets) neighbors would imply that the node, v , is well-connected and, thus, likely to be structurally important. In theory, these two factors should complement each other. In practice, however, the PageRank formulation described above implies that there is a very tight coupling between the degrees of the nodes in the graph and their PageRank scores (see Table 1).

of PageRank based knowledge discovery and recommendation systems. These techniques suitably penalize or (if needed) boost1 the transition strength based on the degree of a given node to adapt the node significances based on the network and application characteristics. This paper is organized as follows: Next, we discuss the related literature. In Sections 3, we introduce the proposed degreedecoupled PageRank techniques. We evaluate the proposed techniques in Section 4 and conclude in Section 5.

2. RELATED WORKS 2.1 Context-Sensitive PageRank
Path-length based definitions of node relatedness, such as those proposed by [4, 24] help capture the relatedness of a pair of nodes solely based on the properties of the nodes and edges on the shortest path between the pair. Random-walk based definitions, such as hitting distance [10,21] and personalized page rank (PPR) score [1, 9, 16], of node relatedness further take into account the density of the edges: as in path-length based definitions, random-walk based definitions also recognize that a node is more related to another node if there are short paths between them; however, random walkbased definitions of relatedness also consider how well the given pair of nodes are connected. In [7], authors construct a transition matrix, TS , where edges leading away from the seed nodes are weighted less than those edges leading towards the seed nodes. An alternative approach for contextualizing PageRank scores is to use the PPR techniques [1,9] discussed in the introduction. One key advantage of this teleportation vector modification based approach over modifying the transition matrix, as in [7], is that the term  can be used to directly control the degree of seeding (or personalization) of the PPR score. [10, 21] rely on a random walk hitting time based approach, where the hitting time is defined as the expected number of steps a random walk from the source vertex to the destination vertex will take. [17] leveraged these properties of PPR to develop localitysensitive algorithms to rank nodes of graphs which are relative to a given set of seed nodes efficiently.

1.2.1 Problem I: When a Large Node Degree Does Not Indicate High Node Significance
In this paper, we highlight (and experimentally show) that, in many applications, node degree and node significance are in fact inversely related and that the tight-coupling between node degrees and PageRank scores might be counter-productive in generating accurate recommendations. E XAMPLE 1. Consider, for example, a recommendation application where a movie graph, consisting of movie and actor nodes, is used for generating movie recommendations. In this application, the first factor (significance of neighbors) clearly has a positive contribution: a movie with good actors is likely to be a good movie and an actress playing in good movies is likely to be a good actress. On the other hand, the second factor (number of neighbors) may in fact be a negative contributor to node significance: the fact that an actor has played in a large number of movies may be a sign that he is a non-discriminating ('B movie') actor, whereas an actress with relatively fewer movies may be a more discriminating ('A movie') actress. As we see in Section 4, this observation turns out to be true in many applications, where (a) acquiring additional edges has a cost that is correlated with the significance of the neighbor (e.g. the effort one needs to invest to a high quality movie) and (b) each node has a limited budget (e.g. total effort an actor/actress can invest in his/her work).

2.2 Improvements to the PageRank Function
Due to the obvious relationship between ranking and monetary rewards (e.g. through selling of advertisements on web search applications), there has been considerable effort in engineering (or manipulating) graphs in a way to maximize ranking scores of particular nodes. This is commonly referred to as PageRank optimization. One way to achieve this goal is carefully adding or removing certain links: If, for example, one or more colluding webmasters can add or remove edges, PageRank scores of target web pages or domains can be increased [23]. [20] established several bounds indicating to what extent the rank of the pages of a website can be changed and the authors derived an optimal referencing strategy to boost PageRank scores. A related, but opposite, problem is to protect the PageRank scores against negative links (which may indicate, for example, negative influence or distrust in a social network), artificial manipulation, and spam. [3], for example, focused on identifying spam pages and link farms and showed that better PageRank scores can be obtained after filtering spam pages and links. In [14], authors show that PPR algorithms that do not differentiate among the seed nodes may not properly rank nodes and present robust personalized PageRank (RPR) strategies, which are insensitive to noise in the set of seed nodes. In this context, de-coupled does not necessarily imply decorrelated. In fact, D2PR can boost correlation between node degree and PageRank if that is required by the application.
1

1.2.2 Problem II: When PageRank Does Not Sufficiently Account for Contributions of Degrees
The mismatch between PageRank and node significance is not limited to the cases where node degrees are inversely related to the node significance. As we see in Section 4, there are other scenarios where PageRank may, in fact, fail to sufficiently account for the contribution of the node degrees to their significances.

1.3 PageRank Revisited: De-coupling Node Significance from Node Degrees
As we discussed above, one key shortcoming of the conventional PageRank scores is that they are often tightly coupled with the degrees of the graph nodes and in many applications the relationship between the significance of the node and its degree in the underlying network may not be as implied by PageRank-based measure: in certain applications, the significance of the node may be negatively correlated with the node degree, whereas in others PageRank may not be sufficient in accounting for degree contributions. Naturally, in such applications a naive application of PageRank in generating recommendations may return poor results. To address these challenges, in this paper, we propose degree decoupled PageRank (D2PR) techniques to improve the effectiveness

There are some efforts to change the impact of degrees on the PageRank computation. [2] proposed a way to boost the power of low-degree nodes in a network. The impact from nodes which are important but are not hubs is relatively small compared to other nodes which are less important with high degrees. To boost the low-degree important nodes for equal opportunity, the teleportation vector is modified with being proportional to the degrees of nodes. [11] boosted the degrees of nodes to reduce the expected cover time of the entire graph by the biassed random-walk.

B

E

Dest. vj B C D

deg. (vj ) 2 3 1

A

C
D

F

Transition probability from A to its neighbors vj p=0 2 -2 0.33 0.18 0.29 0.33 0.08 0.64 0.33 0.74 0.07

(a) A sample graph

(b) Transition probabilities from A

3.

DEGREE DE-COUPLED PAGERANK

The key difficulty of de-coupling node degrees from the PageRank scores is that the definition of the PageRank, based on random walk transitions, is inherently dependent on the number of transitions available from one node to the other. As we mentioned above, the more ways there are to reach into a node, the higher will be its PageRank score.

Figure 1: In conventional PageRank (p = 0), the transition probabilities from node vi = A to all its neighbors vj are the same. In degree de-coupled PageRank (D2PR), the value of p can be used to penalize (p > 0) or boost (p < 0) transition probabilities based on the degree of the destination
node id 53608 351 ... 79538 79917 node degree 883 739 ... 1 1 Ranks of the graph nodes for different de-coupling weights (p) -4 -2 0 2 4 1 1 69 5549 6793 2 12 425 1992 1935 ... ... ... ... ... 7661 7545 4149 195 182 7793 7790 7522 2443 2043

3.1 Desideratum
Therefore, to de-couple the PageRank score from node degrees, we need to modify the transition matrix. In particular, for each node vi in the graph, we would like to be able to control the transition process with a single parameter (p), such that · if p  -1, transitions from node vi are  100% towards the neighbor with the highest degree, · if p = -1, transition probabilities from node vi are proportional to the degrees of its neighbors, · if p = 0, the transition probabilities mirror the standard PageRank probabilities (assuming undifferentiated neighbors), · if p = 1, transition probabilities from node vi are inversely proportional to the degrees of its neighbors, · if p  1, transitions from node vi are  100% towards the neighbor with the lowest degree. In other words, the transition function should de-couple the transition process from node-degrees and penalize or boost the contributions of node degrees in the transition process, as needed.

Table 2: Ranks of graph nodes of different degrees on a sample graph for different de-coupling weights, p: as we see in this figure, when p > 0, high degree nodes are pushed down in the rankings (reducing the correlation between degree and rank), while when p < 0, they are pulled up (improving the correlation between degree and rank) · p  R is a degree de-coupling weight. Intuitively, the numerator term, deg (vj )-p , ensures that the edge incoming to vj is weighted by its degree: if p > 0, then its degree negatively impacts (reduces) transition probabilities into vj , if p < 0 then its degree positively impacts (boosts2 ) transition probabilities into vj , and if p = 0, we obtain the standard PageRank formulation without degree de-coupling. In other words, the transition function satisfies our desideratum of de-coupling the transition process from node-degrees and penalizing or boosting the contributions of node degrees on-demand. Note that, since all transitions from the node vi are degree de-coupled individually based on the degrees of their destinations, the denominator term, -p , ensures that the transition probabilvk neighbor (vi ) deg (vk ) ities from node vi add up to 1.0. Note also that when there is no edge between node vi and vj , TD (j, i) = 0 and, consequently, the term TD (j, i) is not affected by the degree de-coupling process. E XAMPLE 2. Figure 1 shows how the random walk probabilities are differentiated in a degree de-coupled transition matrix on a sample graph where a node A has three neighbors, B (with degree 2), C (with degree 3), and D (with degree 1). In conventional PageRank, the transition probabilities from node A to all its neighbor nodes are equal to 0.33. In degree de-coupled PageRank (D2PR), however, the value of p is used for explicitly accounting for the impact of node degree on the transition probabilities: When p = 2, the transition probabilities from A to its neighbors are 0.18, 0.08, and 0.74, which penalizes nodes which have larger degrees, whereas when p = -2, D2PR boosts the transition probabilities to large degree nodes leading to transition probabilities 0.29, 0.64, and 0.07, respectively.  This example shows that, in degree de-coupled PageRank (D2PR), as we also see in Table 2, the value of p can be used to penalize (p > 0) or boost (p < 0) transition probabilities based on the degree of the destination, vj .
2 In fact, a similar function was used in [11] to quickly locate nodes with higher degrees in a given graph.

3.2 Degree De-coupling Transition Matrix
In this subsection, we will consider degree de-coupling of the transition matrix as implied by the above desideratum.

3.2.1 Undirected Unweighted Graphs
Let G = (V, E ) be an undirected and unweighted graph. Let  also be a given residual probability parameter, and deg (v ) be a function which returns the number of edges on the node v . We represent degree de-coupled PageRank (D2PR) scores in the form of a vector d = TD d + (1 - )t, where t is the teleportation vector, such that t[i] = and TD is a degree de-coupled transition matrix, TD (j, i) =
1 V

for all i

deg (vj )-p , -p vk neighbor (vi ) deg (vk )

(1)

where · TD (j, i) denotes the degree de-coupled transition probability from node vi to node vj over an edge eij = [vi  vj ] when there exists at least one edge between two nodes, · neighbor (vi ) is the set of all neighbors of the source node, vi , and

3.2.2 Directed Unweighted Graphs
The semantics of degree de-coupling is slightly different in directed graphs. In particular, edges incoming to vi often do not require a particular effort from vi to establish and hence are often out of the control of vi , but indicate a certain degree of interestingness, usefulness, or authority as perceived by others. The same is not true for edges outgoing from vi ; in particular, a vertex with a large number of outgoing edges may either indicate a potential hub or simply indicate a non-discerning connection maker. The distinction between these two situations gains importance especially in applications where establishing a new connection has a non-negligible cost to the source node and, thus, a large number of outgoing edges may indicate either (a) a very strong participant to the network or (b) a very poor participant with a large number of weak linkages. Let G = (V, E ) be a directed graph and for the simplicity of the discussion, without any loss of generality, let us assume that G is unweighted. Let us also be given a residual probability parameter,  and let outdeg (v ) be a function which returns the number of outgoing edges from the node v . The degree de-coupled PageRank (D2PR) scores can be represented in the form of a vector d, d = TD d + (1 - )t, where t is the teleportation vector, such that 1 t[i] = V for all i and TD (j, i) = outdeg (vj )-p , -p [vi vk ]out_edges(vi ) outdeg (vk )

accounts for the connection strength (as in the conventional PageRank) whereas TD is a degree de-coupled transition matrix, TD (j, i) = (vj )-p
[vi vk ]out_edges(vi )

(vk )-p

,

such that, TD (j, i) denotes the degree de-coupled transition probability from node vi to node vj over an edge eij = [vi  vj ], p  R is a degree de-coupling weight, and (v ) =
[v vh ]out_edges(v )

w(v  vh ).

Note that, above,  controls whether accounting for the connection strength or degree de-coupling is more critical in a given application. In Section 4, we will study the impact of degree de-coupling in weighted graphs for different scenarios.

4. CASE STUDIES
In this section, we present case studies assessing the effectiveness of the degree de-coupling process and the relationship between the degree de-coupling weight p and recommendation accuracy for different data graphs.

4.1 Setup
For all experiments, the degree de-coupling weight, p, is varied between -4 and 4 with increments of 0.5. The residual probability, , is varied between 0.5 and 0.9, with default value chosen as 0.85. We also varied the  parameter, which controls whether accounting for the connection strength or degree de-coupling is more critical in a given application, between 0.0 and 1.0, with the default value set to 0 (indicating full decoupling).

where TD (j, i) denotes the degree de-coupled transition probability from node vi to node vj over an edge eij = [vi  vj ], out_edges(vi ) is the set of out-going edges from the source node, vi , and p  R is a degree de-coupling weight. E XAMPLE 3. Figure 2 (a) in Section 4 provides an example illustrating the correlations between the degree de-coupled PageRank (D2PR) scores and external evidence for different values of p for some application: here, the higher the correlation, the better resulting ranking reflects the application semantics. As we see in this example, which we will investigate in greater detail in Section 4, the optimal de-coupling weight is not always p = 0 as implied by the conventional PageRank measure. In this particular case, for example, the correlation between D2PR and external evidence of significance is maximized when the de-coupling weight, p, is equal to 0.5, implying that in this application a moderate degree of penalization based on the node degrees is needed to align PageRank scores and application semantics. 

4.1.1 Datasets
Four real data sets are used for the experiments. Each data set is used to create two distinct data graphs and corresponding ratings data. Table 3 provides further details about the various graphs created using these four data sets. These recommendation tasks based on these data graphs are detailed below: · For the IMDB [15] data set, we created (a) a movie-movie graph, where movie nodes are connected by an edge if they share common contributors, such as actors, directors, writers, composers, editors, cosmetic designers, and producers and (b) an actor-actor graph based on whether two actors played in the same movie. Applications: For this data set, we consider applications where movies are rated by the users: thus, we merged the IMDB data with the MovieLens 10M [22] data (based on movie names) to identify user ratings (between 1 and 5) for the movies in the graph. We consider the (a) average user rating as the significance of the movies in the movie-movie graph and (b) average user rating of the movies played in as the significance of the actors in the actor-actor graph. · For the DBLP [26] data set, we constructed (a) an article-article graph where scientific articles were connected to each other if they shared a co-author and (b) an author-author graph based on coauthorship. Applications: (a) In the article-article graph, the number of citations to an article is used to indicate its significance. Similarly, (b) in the author-author graph, average number of citations to an author's papers is used as his/her significance. · For the Last.fm [18], we constructed (a) a listener-listener graph, where the nodes are Last.FM listeners and undirected edges reflect friendship information among these listeners. We also constructed (b) an artist-artist graph based on shared listeners. Applications: (a) In the listener-listener graph, we considered the total listening

3.2.3 Weighted Graphs
Once again, the semantics of degree de-coupling need to be reconsidered for weighted graphs. Let G = (V, E, w) be a directed, weighted graph, where w(e) is a function which returns the weight of the edge associated with edge e. It is important to note that, in such a graph, the weight of an edge can 1) indicate the strength of the connection between two nodes (thus positively contributing to the significance of the destination node); and at the same time and 2) contribute to the degree of a node as a multiplier (thus positively or negatively contributing to the node significance depending on the degree-sensitivity of the application). In other words, given an edge eij = [vi  vj ], from node vi to node vj , the transition probability from vi to vj can be written as T(j, i) =  Tconn_strength (j, i) + (1 -  )TD (j, i), where Tconn_strength (j, i) =
[vi vh ]out_edges(vi )

w(vi  vj ) , w(vi  vh )

Data IMDB DBLP Last.fm Epinions

Graph movie-movie actor-actor article-article author-author listener-listener artist-artist commenter-commenter product-product

# of nodes 191,602 32,208 8,808 47,252 1,892 17,626 6,703 13,384

# of edge 4,465,272 2,493,574 951,798 310,250 25,434 2,640,150 2,395,176 2,355,460

Average node degree 23.30 77.42 108.06 6.57 13.44 149.79 425.05 175.99

Standard deviation of node degrees 51.86 67.15 171.25 8.89 17.31 299.66 438.97 224.12

Median standard deviation of neighbors' node degrees 2.89 114.41 309.92 6.39 22.37 998.53 609.39 202.78

Table 3: Data sets and data graphs activity of a given listener as his/her significance. (b) In the artistartist graph, the number of times an artist has been listened is considered as his/her significance. · For the Epinions [25]: We constructed (a) a commentercommenter graph based on the products on which two individuals both commented and (b) a product-product graph based on shared commenters. Applications: (a) For the nodes on the commentercommenter graph, the number of trusts the commenter received from others is used as his/her commenter significance. (b) For each product in the product-product graph, its average rating by the commenters is used as its node significance. when the degrees are over-penalized (i.e., when p  0.5). The Epinions product-product graph (based on common commenters, Figure 2(c)) also provides the highest correlations with p > 0, but behaves somewhat differently from the other two cases: the correlations stabilize and do not deteriorate significantly when degrees are over-penalized, indicating that the need for degree penalization is especially critical in this case: this is due to the fact that, the larger the number of comments a product has, the more likely it is that the comments are negative (Figure 5). In fact, we see that, among the three graphs, this is the only graph where the traditional PageRank (with p = 0) leads to negative correlations between node ranks and node significances. These results indicate that actors who have had many co-actors, commenters who commented on products also commented by many others, or products which received comments from individuals who also commented on many other products are not good candidates for transition during random walk. This aligns with our expectation that, in applications where each new movie role or comment requires additional effort, high degree may indicate lower per-movie or per-comment effort and, hence, lower significance.

4.2 Measures
In this section, our goal is to observe the impact of different D2PR degree de-coupling weights on the relationship between D2PR rankings and application specific significance measures for the above data sets3 . We also aim to verify whether de-coupling weights can also be used to improve recommendation accuracies. In order to measure the relationship between the degree decoupled PageRank (D2PR) scores and the application-specific node significance, we used Spearman's rank correlation,
i (xi i (xi

-x ¯)(yi - y ¯)
i (yi

-x ¯)2

-y ¯)2

,

4.3.2 Application Group B: When Conventional PageRank is Ideal
Figure 3 shows that, for movie-movie (based on common actors) and author-author (based on common articles) graphs, the peak correlation is at p = 0 indicating that the conventional PageRank which gives positive weight to node degree, is appropriate. This perhaps indicates that movies with a lot of actors tend to be big-budget products and that authors with a large number of coauthors tend to be experts with whom others want to collaborate. Note that, in these applications, additional boosting, with p < 0, negatively affects the correlation, indicating that the relationship between node degree and significance is not very strong (Figure 5). The quick change when p < 0 is because, as we see in Table 3, median standard deviations of neighbors' degrees are low; i.e., degrees of neighbors of a node are comparable: there is no dominant contributor to TD (j, i) in Equation 1 (Section 3) and, thus, the transition probabilities are sensitive to changes in p, when p < 0.

which measures the agreement between the D2PR ranks of the nodes in the graph and their application-specific significances. Here, x are rankings by D2PR and y are significances for an application and x ¯ and y ¯ are averages of two values.

4.3 Impact of De-Coupling in Different Applications (Unweighted Graphs)
In this subsection, we present results that aim to assess D2PR under the settings described above. For these experiments, the residual probability, , and the parameter,  , are set to the default values, 0.85 and 0, respectively. In these experiments, we consider only unweighted graphs (we will study the weighted graphs and the impact of parameter  later in Section 4.5). Figures 2 through 4 include charts showing the Spearman's correlations between the D2PR ranks and application specific node significances for different values of p and for different data graphs. These figures clearly illustrate that different data graphs require different degrees of de-coupling4 to best match the application specific node significance criterion.

4.3.3 Application Group C: When Degree Boosting Helps
Figure 4 shows that there are scenarios where additional boosting based node degrees provides some benefits. The article-article (based on common authors), listener-listener (based on common artists), and artist-artist (based on common listeners) graphs reach their peaks around p  -1, indicating that these also benefit from large node degrees though improvements over p = 0 are slight. A significant difference between applications in Group B and Group C is that, for p < 0, the correlation curve is more or less stable. This is because, as we see in Table 3, in these graphs median standard deviations of neighbors' degrees are high: in other words, for each node, there is a dominant neighbor with a high degree and this neighbor has the highest contribution to TD (j, i); thus, the rankings are not very sensitive to p, when p < 0.

4.3.1 Application Group A: When Degree Penalization Helps
The actor-actor (based on common movies) and commentercommenter (based on common products) graphs have highest correlation at p = 0.5, with the correlations dropping significantly
3 In this paper, we are not proposing a new PageRank computation mechanism. Because of this (and since the focus is not improving scalability of PR), we do not report execution times and compare our results with other PageRank computation mechanisms. 4 Degree penalization or degree-based boosting

  )

   
 % ! &

# $

   '

   
 
 # ! 
" 
   $

   '

   
 
 # ! 
"
   $



' 



  %  

  %  

























 "
  #$

 "
  #$

























 $ ! %&  $   %*(&  "
   #(&$  "




  #(&$

(a) IMDB (actor-actor) (b) Epinions (commenter-commenter) (c) Epinions (product-product) Figure 2: Application Group A: p > 0 is optimal (i.e., node degrees need to be penalized)

  ( 
 
  
 $   
% " 
#


  )

   
 % ! &

#  $ 



& 



' 

























 $ ! %&

 #  $%



 #


 $)'%

 $

  %*(&

(a) DBLP (author-author) (b) IMDB (movie-movie) Figure 3: Application Group B: p = 0 is optimal

  + 
 
  
 %   
& " 
$


   )

   
 
 $ "! #   %

   )

   
 
 $ "! #    %

)#*)- )#*),


' 
   &  

("))+ ("))*
  &  

)#*)*

("*(,) ("*(++

 #
  $%

 $  %&

 #
  $%

 $


 %.)&

 #


  $,(%

 #


  $-(%

(a) DBLP (article-article) (b) Last.fm (listener-listener) (c) Last.fm (artist-artist) Figure 4: Application Group C: p < 0 is optimal (i.e., node degrees need to be boosted)
p<0
 
  
          
  

p=0

p>0

Figure 5: Correlations between node degrees and application specific significances for different data graphs (each color group is a distinct pattern in Figures 2 through 4).

4.3.4 Summary: Correlations between Node Degrees and Application Specific Significances
The experiments reported above show that degree de-coupling is important as different applications, even on the same data set, may associate different semantics to node degrees and the conventional PageRank scores are too tightly coupled with node degrees to be effective in all scenarios. Figure 5, which plots correlations between node degrees and application specific significances for different data graphs, re-confirms that the ideal value of the p is related to the usefulness of the node degree in capturing the application specific definition of node significance.

4.4 Relationship between  and p
In Figures 6 through 8, we investigate the relationship between the value  and the degree de-coupling parameter p for different application types. Here we use the default value, 0, for the parameter  and present the results for unweighted graphs (the results for

the weighted graphs are similar). First thing to notice in these figures is that the grouping of the applications (into those where, respectively, p > 0, p = 0, or p < 0 is useful) is preserved when different values of  are considered. Figure 6 studies the impact of the value of  in application group A, where degree penalization helps (p > 0). As we see here, for the IMDB actor-actor (Figure 6(a)) and Epinions commentercommenter (Figure 6(b)) graphs, having a lower value of  (i.e., lower probability of forward movement during the random walk) provides the highest possible correlations between D2PR ranks and node significance (with the optimal value of p being  0.5 independent of the value of ). This indicates that in these graphs, it is not necessary to traverse far during the random walk. Interestingly, though, when degrees are over-penalized (i.e., p  0), smaller values of  start leading to worse correlations, indicating that (while not being optimal) severe penalization of node degrees helps make random traversals more useful than random jumps. As we have already observed in Figure 2(c), the Epinions productproduct graph (Figure 6(c)) behaves somewhat differently from the other two cases where degree penalization (p > 0) leads to larger correlations: in this case, unlike the other two graphs, the highest possible correlations between D2PR ranks and node significance are obtained for large values of , indicating that this application benefits from longer random walks (though the differences among the correlations for different  values are very small). Figure 7 shows that the pattern is different for application group B, where conventional PageRank is ideal (p = 0): in this case, having a larger value of  (i.e., larger probability of forward movement during the random walk) provides the highest correlations between ranks and significance. Interestingly, in these applications, when

     
 


  (

   
   $ !  % ' " )


   +    
 
   "   & # 
%
    '
  '  

   *      !   % " 
$
   &



* 






  (  


































 













 '  " () $2,&. $2,&/ $2,&0. $2,&1 "0*$,

 %
  &'

 $
  %&


"0*$-

"0*$.,

"0*$/

!/)#+

!/)#,

!/)#-+

!/)#.

(a) IMDB (actor-actor) (b) Epinions (commenter-commenter) (c) Epinions (product-product) Figure 6: Relationship between p and , for application group A, where p > 0 is optimal (i.e., degrees need to be penalized)

  , '


) 



 
  
   # 
 $ 
&
 ! 
(

* 

  (

   
   $ !  % !'! " )

























 '  " ()

  

 & ! '(

#1+%-

#1+%.

#1+%/-

#1+%0

$2,&.

$2,&/

$2,&0.

$2,&1

(a) DBLP (author-author) (b) IMDB (movie-movie) Figure 7: Relationship between p and , for application group B, where p = 0 is optimal

  , ' 
 
  
   # 
 $ 
&
 ! 
(
  (  

   +    
 
   "   & $# %    '

   +    
 
   "   & $# %     '



) 


  (  

 %
  &'

 & ! '(

 %
  &'

#1+%-

#1+%.

#1+%/-

#1+%0

"0*$,

"0*$-

"0*$.,

"0*$/

"0*$,

"0*$-

"0*$.,

"0*$/

(a) DBLP (article-article) (b) Last.fm (listener-listener) (c) Last.fm (artist-artist) Figure 8: Relationship between p and , for application group C, where p < 0 is optimal (i.e., node degrees need to be boosted) p  0 or p  0, higher probabilities of random walk traversal (i.e., larger ) stop being beneficial and lower values of  lead to larger correlations. This re-confirms that, for these applications, p  0 leverages the random walk traversal the best. As we see in Figure 8, in application group C, where degree boosting helps (p < 0), it is also the case that larger values of  (i.e., larger probabilities of forward transitions during the random walk) provides the highest correlations between node ranks and significance. On the other hand, in these applications, p  0.5 serves as a balance point where the value of  stops being relevant; in fact, for p > 0.5 the higher values of  stops being beneficial and lower values of  lead to larger correlations. This re-confirms that smaller values of p (which provides degree boosting) help leverage the random walk traversal the best. the greater  is), the larger is the optimal value of p. Figure 10 shows that, for applications in group B, where p  0 is ideal, when the connection strength is given significantly more weight than degree de-coupling (i.e.,   0), we observe high ranksignificance correlations. Interestingly however, for the moviemovie graph (where the edge weights denote common actors) the highest correlations are obtained not with p = 0, but with p = 0.5 and  = 0.75, indicating that degree penalization is actually beneficial in this case: movies that share large numbers of actors with other movies are likely to be B -movies, which are not good candidates for transitions during the random walk. Figure 11 shows that in application group C, where degree boosting (p < 0) helps, giving more weight to connection strength (i.e.,   1.0) is a good, but not necessarily the best strategy. In fact, in these graphs, the highest overall correlations are obtained with  = 0 or  = 0.25, indicating that degree de-coupling is beneficial also in these cases. Interestingly, (unlike the case with the unweighted listener-listener graph, where the best correlation was obtained when p < 0) for the weighted version of the listenerlistener graph (where edge weights denote the number of shared friends), when  = 0 through 0.5, p = 0 provides the highest correlation and when  = 0.75, p = 0.5 provides the highest correlation ­ these indicate that listeners who have large numbers of shared friends with others are good candidates for random walk. Note that a key observation from the above results is that the conventional PageRank, based on connection strength (i.e.,  = 1.0), is not always the best strategy for the applications considered.

4.5 Relationship between Weighted Graphs



and

p

in

Finally, in Figures 9 through 11, we investigate the relationship between the value  (which controls whether accounting for the connection strength or degree de-coupling is more critical in a given application) and the degree de-coupling parameter p for different application types. Here we use the default value, 0.85, for the parameter  and present the results for weighted graphs: Figure 9 depicts the impact of the value of the parameter  in application group A, where degree penalization helps (p > 0). As we see here, for all three weighted graphs, performing degree penalization (i.e.,  < 1.0) provides better rank-significance correlation than relying solely on the connection strength (i.e.,  = 1.0). Note that the value of  impacts the optimal value of degree penalization parameter p: the more weight is given to connection strength (i.e.,

5. CONCLUSIONS
In this paper, we noted that in many applications the relation-

)

- 

  1    
   $ !  % ( " * +("& .   !,
  +  

   /    
 
   "   ' # 
&
   ( )& $ ,    
*

   .      !   & " 
%
   ' (%# +    
)

 
























  *  



 (  " )*



 &
  '( $40 "2"2-%/0 "2-%0 "2-%10 "2. !1,

 %
  &' !1,$./ !1,$/ !1,$0/ !1-

$4/

$4/'12

$4/'2

$4/'32

(a) IMDB (actor-actor) (b) Epinions (commenter-commenter) (c) Epinions (product-product) Figure 9: Relationship between p and  , for application group A, where p > 0 is optimal (i.e., node degrees need to be penalized)

  0 
 
  
   # 
 ( $ 
'
 ! 
) *'!% -  
+

-  

, 


)

  1    
   $ !  % !(! " * +("& .   ,

























 (  " )*

  

 ' ! ()

#3.

#3.&01

#3.&1

#3.&21

#3/

$4/

$4/'12

$4/'2

$4/'32

$40

(a) DBLP (author-author) (b) IMDB (movie-movie) Figure 10: Relationship between p and  , for application group B, where p = 0 is optimal

  0 
 
  
   # 
 ( $ 
'
 ! 
) *'!% -  
+
  +  

   /    
 
   "   ' %# &   ( )& $ ,    *
  +  

   /    
 
   "   ' %# &    ( )& $ ,    *



, 


 &
  '(

 ' ! ()

 &
  '(

#3.

#3.&01

#3.&1

#3.&21

#3/

"2-

"2-%/0

"2-%0

"2-%10

"2.

"2-

"2-%/0

"2-%0

"2-%10

"2.

(a) DBLP (article-article) (b) Last.fm (listener-listener) (c) Last.fm (artist-artist) Figure 11: Relationship between p and  , for application group C, where p < 0 is optimal (i.e., node degrees need to be boosted) ship between the significance of the node and its degree in the underlying network may not be as strong (or as weak) as implied by PageRank-based measures. We proposed degree de-coupled PageRank (D2PR) to improve the effectiveness of PageRank based knowledge discovery and recommendation tasks. Evaluations on different data graphs and recommendation tasks have confirmed that degree de-coupling would be an effective way to match application specific node significances and improve recommendation accuracies using PageRank based approaches.
[11] C. Cooper, et al. A fast algorithm to find all high degree vertices in graphs with a power law degree sequence. In WAW'12, 2012. [12] O. Fercoq. PageRank optimization applied to spam detection. arXiv:1203.1457, 2012. [13] T. H. Haveliwala. Topic-sensitive PageRank. WWW, 2002. [14] S. Huang, X. Li, K.S Candan, M.L Sapino. "Can you really trust that seed?": Reducing the Impact of Seed Noise in Personalized PageRank. ASONAM'14, 2014. [15] IMDB website: http://www.imdb.com/ [16] G. Jeh and J. Widom. Scaling personalized web search. Stanford Univ. Tech. Report. 2002. [17] J.H Kim, K.S Candan, M.L Sapino. Locality-sensitive and Re-use Promoting Personalized PageRank Computations. Knowledge and Information Systems. 10.1007/s10115-015-0843-6, 2015 [18] http://ir.ii.uam.es/hetrec2011/datasets.html. [19] A.N. Nikolakopoulos and J. Garofalakis, NCDawareRank: A Novel Ranking Method that Exploits the Decomposable Structure of the Web. WSDM, 2013. [20] F. Mathieu and L. Viennot, Local aspects of the global ranking of web pages. In I2CS'06, pp. 1­10, 2006. [21] Q. Mei, D. Zhou, and K. Church. Query suggestion using hitting time. CIKM'08, 2008. [22] http://grouplens.org/datasets/movielens [23] M. Olsen. Maximizing PageRank with New Backlinks. CIAC, 2010. [24] C. Palmer, P. Gibbons, and C. Faloutsos. Anf: a fast and scalable tool for data mining in massive graphs. KDD, 2002. [25] J. Tang, H. Gao, and H. Liu. mTrust: Discerning multi-faceted trust in a connected world. WSDM, 2012. [26] J. Tang, et al. ArnetMiner: Extraction and Mining of Academic Social Networks. In SIGKDD'08, pp 990­998, 2008 [27] D.R. White, et al. Betweenness centrality measures for directed graphs. Social Networks, 16, 335-346,1994.

[1] A. Balmin, V. Hristidis, and Y. Papakonstantinou. ObjectRank: Authority-based keyword search in databases. In VLDB'04. [2] D. Banky, G. Ivan, V. Gromusz. Equal Opportunity for Low-Degree Network Nodes: A PageRank-Based Method for Protein Target Identification in Metabolic Graphs. PLoS One 8(1): e542-4, 2013. [3] L. Becchetti et al. Using Rank Propagation and Probabilistic Counting for Link-Based Spam Detection. WebKDD, 2006. [4] P. Boldi et al. HyperANF: Approximating the neighbourhood function of very large graphs on a budget. WWW, 2011. [5] M.G.Borgatti, et al. Network measures of social capital. Connections 21(2):27-36, 1998. [6] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems, 30, 1998. [7] K.S. Candan and W.D. Li. Using random walks for mining web document associations. PAKDD'00, pp. 294-305, 2000. [8] M. Chen., et al. Clustering via random walk hitting time on directed graphs. AAAI'08. [9] S. Chakrabarti. Dynamic personalized pagerank in entity-relation graphs. In WWW'07, pp 571­580, 2007. [10] M. Chen, J. Liu, and X. Tang. Clustering via random walk hitting time on directed graphs. AAAI'08, pp. 616­621, 2008.

6.

REFERENCES

LR-PPR: Locality-Sensitive, Re-use Promoting, Approximate Personalized PageRank Computation
Jung Hyun Kim
Arizona State University Tempe, AZ 85287, USA



K. Selçuk Candan
Arizona State University Tempe, AZ 85287, USA

Maria Luisa Sapino
University of Torino I-10149 Torino, Italy

jkim294@asu.edu ABSTRACT

candan@asu.edu

mlsapino@di.unito.it
G
v1 d c a b v2

Personalized PageRank (PPR) based measures of node proximity have been shown to be highly effective in many prediction and recommendation applications. The use of personalized PageRank for large graphs, however, is difficult due to its high computation cost. In this paper, we propose a Locality-sensitive, Re-use promoting, approximate personalized PageRank (LR-PPR) algorithm for efficiently computing the PPR values relying on the localities of the given seed nodes on the graph: (a) The LR-PPR algorithm is locality sensitive in the sense that it reduces the computational cost of the PPR computation process by focusing on the local neighborhoods of the seed nodes. (b) LR-PPR is re-use promoting in that instead of performing a monolithic computation for the given seed node set using the entire graph, LR-PPR divides the work into localities of the seeds and caches the intermediary results obtained during the computation. These cached results are then reused for future queries sharing seed nodes. Experiment results for different data sets and under different scenarios show that LR-PPR algorithm is highly-efficient and accurate.

v3

Figure 1: Key questions: Given a graph, G, and a seed set of nodes S = {v1 , v2 , v3 } in G, can we rank the remaining nodes in the graph regarding their relationships to the set S ? Which of the nodes a through d is the most interesting given the seed set of nodes v1 through v3 ? node relatedness, on the other hand, also take into account the density of the edges: unlike in path-based definitions, random walkbased definitions of relatedness also consider how tightly connected two nodes are and argue that nodes that have many paths between them can be considered more related. Random-walk based techniques encode the structure of the network in the form a transition matrix of a stochastic process from which the node relationships can be inferred. When it exists, the convergence probability of a node n gives the ratio of the time spent at that node in a sufficiently long random walk and, therefore, neatly captures the connectivity of the node n in the graph. Therefore, many web search and recommendation algorithms, such as PageRank [5], rely on random-walks to identify significant nodes in the graph: let us consider a weighted, directed graph G(V, E ), where the weight of the edge ej  E is denoted as wj ( 0) and where = 1.0. The PageRank score of the node ej outedge(vi ) wj vi  V is the stationary distribution of a random walk on G, where at each step with probability 1- , the random walk moves along an outgoing edge of the current node with a probability proportional to the edge weights and with probability  , the walk jumps to a random node in V . In other words, if we denote all the PageRank scores of the nodes in V with a vector  , then  = (1 -  )TG ×  +  j, where TG denotes the transition matrix corresponding to the graph G (and the underlying edge weights) and j is a teleportation vector 1 . where all entries are V

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Miscellaneous

Keywords
Personalized PageRank; Locality-Sensitivity; Reuse-Promotion

1.

INTRODUCTION

Node distance/proximity measures are commonly used for quantifying how nearby or otherwise related to two or more nodes on a graph are. Path-length based definitions [17] are useful when the relatedness can be captured solely based on the properties of the nodes and edges on the shortest path (based on some definition of path-length). Random-walk based definitions, such as hitting distance [16] and personalized page rank (PPR) score [4, 13, 21] of
 This work is supported by NSF Grant 1043583 "MiNC: NSDL Middleware for Network- and Context-aware Recommendations".

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than the author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. CIKM'13, Oct. 27­Nov. 1, 2013, San Francisco, CA, USA. Copyright is held by the owner/author(s). Publication rights licensed to ACM. ACM 978-1-4503-2263-8/13/10 ...$15.00. http://dx.doi.org/10.1145/2505515.2505651.

1.1 Proximity and PageRank
An early attempt to contextualize the PageRank scores is the topic sensitive PageRank [12] approach which adjusts the PageRank scores of the nodes by assigning the teleportation probabilities in vector j in a way that reflects the graph nodes' degrees of

G
G1
v1 v2

G2

G1

incoming bnd. node of G1

G
G2

a shared node
v1
v3

v2

G3

Figure 2: Locality-sensitivity: Computation of PPR should focus on the neighborhoods (localities) of the seeds
G
G1
v1 v2

outgoing bnd. node of G1

G
G1
v1 v7

G2

G7

Figure 4: Incoming and outgoing boundary nodes/edges and a node shared between two localities · locality sensitive in the sense that it reduces the computational cost of the PPR computation process and improve accuracy by focusing on the neighborhoods of the seed nodes (Figure 2); and · re-use promoting in that it enables caching and re-use of significant portions of the intermediary work for the individual seed nodes in future queries (Figure 3). In the following section, we first formally introduce the problem and then present our solution for locality-sensitive, re-use promoting, approximate personalized PageRank computations. We evaluate LR-PPR for different data sets and under different scenarios in Section 3. We conclude in Section 4.

v3

v6

v9

G3

G6 G9

(a) PPR query 1

(b) PPR query 2

Figure 3: Re-use promotion: Two PPR queries sharing a seed node (v1 ) should also share relevant work match to the search topic. [6, 7] were among the first works which recognized that random-walks can also be used for measuring the degree of association, relatedness, or proximity of the graph nodes to a given seed node set, S  V (Figure 1). An alternative to this approach is to modify (as in topic sensitive PageRank [12]) the teleportation vector, j : instead of jumping to a random node in V with probability  , the random walk jumps to one of the nodes in the seed set, S , given by the user. More specifically, if we denote the personalized PageRank scores of the nodes in V with a vector , then  = (1 -  )TG ×  + s,
1 where s is a re-seeding vector, such that if vi  S , then s[i] = S and s[i] = 0, otherwise. One key advantage of this approach over modifying the transition matrix as in [6] is that the term  can be used to directly control the degree of seeding (or personalization) of the PPR score. However, the use of personalized PageRank for large graphs is difficult due to the high cost of solving for the vector , given  , transition matrix TG , and the seeding vector s. One way to obtain  is to solve the above equation for  mathematically. Alternatively, PowerIteration methods [14] simulate the dissemination of probability mass by repeatedly applying the transition process to an initial distribution 0 until a convergence criterion is satisfied. For large data sets, both of these processes are prohibitively expensive. Recent advances on personalized PageRank includes top-k and approximate personalized PageRank algorithms [1, 3, 8, 10, 11, 20, 22] and parallelized implementations on MapReduce or Pregel based batch data processing systems [2, 15]. The FastRWR algorithm presented in [22] for example partitions the graph into subgraphs and indexes partial intermediary solutions. Unfortunately, for large data sets, FastRWR requires large number of partitions to ensure that the intermediary metadata (which requires dense matrix representation) fits into the available memory and this negatively impacts execution time and accuracy.

2. PROPOSED APPROACH
Let G = (V, E ) be a directed graph. For the simplicity of the discussion, without any loss of generality, let us assume that G is unweighted1 . Let us be given a set S  V of seed nodes (Figure 1) and a personalization parameter,  . Let GS = {Gh (Vh , Eh ) | 1  h  K } be K = S subgraphs of G, such that · for each vi  S , there exists a corresponding Gi  GS such that vi  Vi and · for all Gh  GS , Gh G . We first formalize the locality-sensitivity goal (Figure 2): Desideratum 1: Locality-Sensitivity. Our goal is to compute an approximate PPR vector, apx , using GS instead of G, such that apx  , where  represents the true PPR scores of the nodes in V relative to S : i.e., apx   = (1 -  )TG ×  + s, where TG is the transition matrix corresponding to G and s is the re-seeding vector corresponding to the seed nodes in S . We next formalize the re-use promotion goal (Figure 3): Desideratum 2: Reuse-Promotion. Let S1 and S2 be two sets of seed nodes and let vi be a node such that vi  S1  S2 . Let also the approximate PPR vector, apx,1 corresponding to S1 have already been computed using GS1 and let us assume that the approximate PPR vector, apx,2 corresponding to S2 is being requested. The part of the work performed when processing Gi  GS1 (corresponding to vi ) should not need to be re-performed when processing Gi  GS2 , when computing apx,2 using GS2 .

1.2 Contributions of this Paper
In this paper, we argue that we can improve both scalability and accuracy through a Locality-sensitive, Re-use promoting, approximate personalized PageRank (LR-PPR) algorithm: LR-PPR is

2.1
1

Combined Locality and its Boundary

Unlike existing approximate PPR algorithms [1, 3, 8, 10, 11, 20, 22], LR-PPR is location sensitive. Therefore, given the set, S , of Extending the proposed algorithms to weighted graphs is trivial.

G1
v1 v2

G2
v3

G3

GK
vK



A node shared by multiple seed locality graphs

 0 V2 × V1  ...  0 VK × V1 01× V1

M1

0 0

V1 × V2

M2 ...

01×

VK × V2 V2

. . . 0 V1 × VK . . . 0 V2 × VK ... ... ... MK ... 01× VK

0 0

V1 ×1 V2 ×1

   , 

0 VK ×1 MK +1

...

Figure 5: An equivalence set consists of the copies of a node shared across multiple seed locality graphs seed nodes and the corresponding localities, GS , the computation focuses on the combined locality G+ (V + , E + )  G, where V+ =
1lK

Vl and E + =
1lK

El .

Given a combined locality, G+ , we can also define its external graph, G- (V - , E - ), as the set of nodes and edges of G that are outside of G+ and boundary nodes and edges. As shown in Figure 4, we refer to vi  Vl as an outgoing boundary node of Gl if there is an outgoing edge ei,j = [vi  vj ]  E , where vj  / Vl ; the edge ej is also referred to as an outgoing boundary edge of Gl . The set of all outgoing boundary nodes of Gl is denoted as Voutbound,l and the set of all outgoing boundary edges of Gl is denoted as Eoutbound,l. Note that Voutbound,l  Vl , whereas Eoutbound,l  El = . We also define incoming boundary nodes (Vinbound,l ) and incoming boundary edges (Einbound,l ) similarly to the outgoing boundary nodes and edges of Gl , but considering inbound edges to these subgraphs. More specifically, Einbound,l consists of edges of the form [vi  vj ]  E , where vj  Vl and vi  / Vl .

where MK +1 is equal to the 1 × 1 matrix 01×1 . Intuitively, Mbd combines the K subgraphs into one transition matrix, without considering common nodes/edges or incoming/outgoing boundary edges and ignoring all outgoing and incoming edges. All the external nodes in G- are accounted by a single node represented by the 1 × 1 matrix MK +1 . A key advantage of Mbd is that it is block-diagonal and, hence, there are efficient ways to process it. However, this block-diagonal matrix, Mbd , cannot accurately represent the graph G as it ignores potential overlaps among the individual localities and ignores all the nodes and edges outside of G+ . We therefore need a compensation matrix to · make sure that nodes and edges shared between the localities are not double counted during PPR computation and · take into account the topology of the graph external to both localities G1 through GK .

2.2.3 Compensation Matrix, M 0
Let t be ( V1 + V2 + . . . + VK + 1). The compensation matrix, M0 , is a t × t matrix accounting for the boundary edges of the seed localities as well as the nodes/edges in G- . M0 also ensures that the common nodes in V1 through VK are not double counted during PPR calculations. M0 is constructed as follows: Row/column indexing: Let vl,i be a vertex in Vl . We introduce a row/column indexing function, ind(), defined as follows:   ind(l, i) = 
1h<l

2.2 Localized Transition Matrix
Since LR-PPR focuses on the combined locality, G+ , the next step is to combine the transition matrices of the individual localities into a combined transition matrix. To produce accurate approximations, this localized transition matrix, however, should nevertheless take the external graph, G- , and the boundaries between G- and G+ , into account.

Vh  + i

2.2.1 Transition Matrices of Individual Localities
Let v(l,i) (1  l  K ) denote a re-indexing of vertices in Vl . If v(l,i)  Vl and vc  V s.t. v(l,i) = vc , we say that v(l,i) is a member of an equivalence set, Vc (Figure 5). Intuitively, the equivalence sets capture the common parts across the localities of the individual seed nodes. Given Gl (Vl , El )  G and an appropriate re-indexing, we define the corresponding local transition matrix, Ml , as a Vl × Vl matrix, where · ei,j = [v(l,i)  v(l,j ) ]  El  Ml [j, i] = 0 and
1 , out(v(l,i) )

Intuitively the indexing function, ind(), maps the relevant nodes in the graph to their positions in the M0 matrix. Compensation for the common nodes: Let el,i,j be an edge [v(l,i)  v(l,j ) ]  El and let v(l,j ) be a member of the equivalence set Vc for some vc  V . Then, if Vc > 1 -1 1 × out(G,v and · M0 [ind(l, j ), ind(l, i)] = - Vc Vc l,i ) · v(h,k)  Vc s.t. v(h,k) = v(l,j ) , we have M0 [ind(h, k), ind(l, i)] = - 1 1 × , Vc out(G, vl,i )

· ei,j = [v(l,i)  v(l,j ) ]  El  Ml [j, i] =

where out(v(l,i) ) is the number of outgoing edges of vi .

2.2.2 Localization of the Transition Matrix
Given the local transition matrices, M1 through MK , we localize the transition matrix of G by approximating it as Mapx = Mbd + M0 , where Mbd is a block-diagonal matrix of the form

where out(G, v ) is the outdegree of node v in G. Intuitively, the compensation matrix re-routes a portion of the transitions going towards a shared node in a given locality Vl to the copies in other seed localities. This prevents the transitions to and from the shared node from being mis-counted. Compensation for outgoing boundary edges: The compensation matrix needs to account also for outgoing boundary edges that are not accounted for by the neighborhood transition matrices M1 through MK : · Accounting for boundary edges from nodes in Vl to nodes in Vh : [v(l,i)  v(h,j ) ]  Eoutbound,l ­ M0 [ind(h, j ), ind(l, i)] =
1 out(v(l,i) )

· Accounting for boundary edges from nodes in Vl to graph nodes that are in V - : if [v(l,i)  v ]  Eoutbound,l s.t. v  V -

­ M0 [t, ind(l, i)] =

the number of edges of the form [v(l,i)  v ]  Eoutbound,l where v  V - else M0 [t, ind(l, i)] = 0 The compensation matrix records all outgoing edges, whether they cross into another locality or they are into external nodes in G- . If a node has more than one outgoing edge into the nodes in G- , all such edges are captured using one single compensation edge which aggregates all the corresponding transition probabilities. Compensation for incoming boundary edges (from G- ): Similarly to the outgoing boundary edges, the compensation matrix needs also to account for incoming boundary edges that are not accounted for by the neighborhood transition matrices M1 through MK . Since incoming edges from other localities have been accounted for in the previous step, here we only need to consider incoming boundary edges (from G- ). Following the formulation in [23], we account for incoming edges where the source is external to G+ and the destination is a vertex v(l,i) in Vl by inserting an edge from the dummy node to v(l,i) with a weight that considers the outdegrees of all external source nodes; i.e., v(l,i) s.t. [vk  v(l,i) ]  Einbound,l where vk  V - and v(l,i) is in the equivalence set Vc for a vc  V , M0 [ind(l, i), t] is equal to 1 Vc
([vk v(l,i) ]Einbound,l )(vk V
-)

bnd(v(l,i) ) , out(v(l,i) )

where bnd(v(l,i) ) is

the nodes in V + . In particular, we rely on the following result due to [22], which itself relies on the Sherman-Morisson lemma [18]: Let C = A + USV. Let also (I - cA)-1 = Q-1 . Then, the equation r = (1 - c)(I - cA)-1 e has the solution r = (1 - c)(Q-1 e + cQ-1 UVQ-1 e), where  = (S-1 - cVQ-1 U)-1 . If A is a block diagonal matrix consisting of k blocks, A1 through Ak , then Q-1 is also a block diagonal matrix con1 -1 sisting of k corresponding blocks, Q- 1 through Qk , where -1 -1 Qi = (I - cAi ) . We use the above observation to efficiently obtain PPR scores by setting c = (1 -  ), C = Mapx , A = Mbd , and USV = M0 . In particular, we divide the PPR computation into two steps: a locality-sensitive and re-usable step involving the computation of the Q-1 term using the local transition matrices and a run-time computation step involving the compensation matrix.
1 2.4.1 Locality-sensitive and Re-usable Q - bd

1 out(G,vk )

V-

,

where out(G, v ) is the outdegree of node v in G. Compensation for the edges in G- : We account for edges that are entirely in G- by creating a self-loop that represents the sum of outdegree flow between all external nodes averaged by the number of external nodes; i.e., M0 [t, t] =
v V - out(G- ,v ) out(G,v ) V-

,

where out(G- , v ) and out(G, v ) are the outdegrees of node v in G- and G, respectively. Completion: For any matrix position p, q not considered above, no compensation is necessary; i.e., M0 [p, q ] = 0.

Local transition matrices, M1 through MK corresponding to the seeds v1 through vK are constant (unless the graph itself evolves -1 1 is computed over time). Therefore, if Q- h = (I - (1 -  )Mh ) 1 and cached once, it can be reused for obtaining Q- bd , which is a 1 -1 block diagonal matrix consisting of Q- 1 through QK +1 (as before, -1 the last block, QK +1 , is simply equal to 11×1 ):   1 Q- 0 V1 × V2 . . . 0 V1 × VK 0 V1 ×1 1 - 1 0 V × V Q2 . . . 0 V2 × VK 0 V2 ×1  2 1    . . . . . . . . . . . . ...  ,  -1  0 V × V1 0 . . . Q 0 V × V V × 1 K 2 K K K -1 01× V1 01× V2 ... 01× VK QK +1

2.4.2 Computation of the LR-PPR Scores
In order to be able to use the above formulation for obtaining the PPR scores of the nodes in V + , in the query time, we need to decompose the compensation matrix, M0 , into U0 S0 V0 . While obtaining a precise decomposition in run-time would be prohibitively expensive, since M0 is sparse and since we are looking for an approximation of the PPR scores, we can obtain a fairly accurate lowrank approximation of M0 efficiently [22]: M0 ~ 0S ~0V ~ 0. U

2.3 L-PPR: Locality Sensitive PPR
Once the block-diagonal local transition matrix, Mbd , and the compensation matrix, M0 , are obtained, the next step is to obtain the PPR scores of the nodes in V + . This can be performed using any fast PPR computation algorithm discussed in Section 1.1. Note that the overall transition matrix Mapx = Mbd + M0 is approximate in the sense that all the nodes external to G+ are clustered into a single node, represented by the last row and column of the matrix. Otherwise, the combined matrix Mapx accurately represents the nodes and edges in the "merged localities graph" combining the seed localities, G1 through GK . As we see in Section 3, this leads to highly accurate PPR scores with better scalability than existing techniques.

Given this decomposition, the result vector apx , which contains the (approximate) PPR scores of the nodes in V + , is computed as
1 -1 ~ -1 ~ apx =  Q- bd s + (1 -  )Qbd U0 V0 Qbd s ,

where
1 -1 ~ ~- ~ = S 0 - (1 -  )V0 Qbd U0 -1

2.4 LR-PPR: Locality Sensitive and Reuse Promoting PPR
Our goal is not only to leverage locality-sensitivity as in L-PPR, but also to boost sub-result re-use. Remember that, as discussed above, the localized transition matrix Mapx is equal to Mbd + M0 where (by construction) Mbd is a block-diagonal matrix, whereas M0 (which accounts for shared, boundary, and external nodes) is relatively sparse. We next use these two properties of the decomposition of Mapx to efficiently compute approximate PPR scores of

.

Note that the compensation matrix M0 is query specific and, thus, the work done for the last step cannot be reused across queries. However, as we experimentally verify in Section 3, the last step is relatively cheap and the earlier(costlier) steps involve re-usable work. Thus, caching and re-use through LR-PPR enables significant savings in execution time. We discuss the overall complexity and the opportunities for re-use next.

2.5 Complexity and Re-use
Analysis of LR-PPR points to the following advantages: First of all, computation is done using only local nodes and edges. Secondly, most of the results of the expensive sub-tasks can be cached and re-used. Moreover, costly matrix inversions are limited to the smaller matrices representing localities and small matrices of size r × r . Various subtasks have complexity proportional to V + 2 , where V + = 1lK Vl . While in theory the locality Vl can be arbitrarily large, in practice we select localities with a bounded number of nodes; i.e., 1lK , Vl  L for some L V . As described above LR-PPR algorithm supports caching and reuse of some of the intermediary work. The process results in local transition matrices, each of which can be cached in O( El ) space (where El is the number edges in the locality) assuming a sparse representation. The algorithm also involves a matrix inversion, which results in a dense matrix; as a result, caching the inverted matrix takes O( Vl 2 ) space (where Vl is the number of vertices in the locality). If the locality is size-constrained, this leads to constant space usage of O(L2 ), where L is the maximum number of nodes in the locality. If the inverted matrix of a locality is cached, then the local transition matrix does not need to be maintained further. For cache replacement, any frequency-based or predictive cache-replacement policy can be used.
" 
&%

 1 "    
 ( )!
%.. %.. %.' %.%., %-, %.* %.* %,( %*' %'. %-%-+ %-' %,*

%*

%%

*

&% 

'*  1   

*%

,*

Figure 6: Accuracies of L-PPR, LR-PPR, and FastRWR against the Global PPR for different numbers of target nodes Accuracy: For different algorithm pairs, we report the Spearman's rank correlation
i (x i i (x i

-x ¯)(yi - y ¯)
i (yi

-x ¯ )2

-y ¯)2

,

3.

EXPERIMENTAL EVALUATION

In this section, we present results of experiments assessing the efficiency and effectiveness of the Locality-Sensitive, Re-use Promoting Approximate Personalized PageRank (LR-PPR) algorithm. Table 1 provides overviews of the three data sets (from http : //snap.stanford.edu/data/ ) considered in the experiments. We considered graphs with different sizes and edge densities. We also varied numbers of seeds and the distances between the seeds (thereby varying the overlaps among seed localities). We also considered seed neighborhoods (or localities) of different sizes. Experiments were carried out using a 4-core Intel Core i5-2400, 3.10GHz, machine with 8GB memory and 64-bit Windows 7 Enterprise. Codes were executed using Matlab 7.11.0(2010b). All experiments were run 10 times and averages are reported.

which measures the agreement between two rankings (nodes with the same score are assigned the average of their positions in the ranking). Here, x and y are rankings by two algorithms and x ¯ and y ¯ are average ranks. To compute the rank coefficient, a portion of the highest ranked nodes in the merged graph according to x are considered. As default, we considered 10% highest ranked nodes; but we also varied the target percentage (5%, 10%, 25%, 50%, 75%) to observe how the accuracy varies with result size. Memory: We also report the amount of data read from the cache.

3.3 Results and Discussions
Table 2 presents experimental results for FastRWR, L-PPR, and LR-PPR. First of all, all three algorithms are much faster than Global PPR. As expected, in small data sets (Epinions and Slashdot) FastRWR works faster than L-PPR and LR-PPR, though in many cases, it requires more memory. In large data sets, however, L-PPR and LR-PPR significantly outperform FastRWR in terms of query processing efficiency and run-time memory requirement. In terms of accuracy, the proposed locality sensitive techniques, L-PPR and LR-PPR, constantly outperform FastRWR. This is because, FastRWR tries to approximate the whole graph, whereas the proposed algorithms focus on the relevant localities. FastRWR requires large number of partitions to ensure that the intermediary metadata (which requires dense matrix representation) fits into memory and this negatively impacts accuracy. Our localitysensitive algorithms, L-PPR and LR-PPR, avoid this and provide high accuracy with low memory consumption, especially in large graphs, like WikiTalk. Figure 6 confirms that the accuracies of L-PPR and LR-PPR both stay high as we consider larger numbers of top ranked network nodes for accuracy assessment, whereas the accuracy of FastRWR suffers significantly when we consider larger portions of the merged locality graph. Figure 7 studies the execution time behavior for L-PPR, LRPPR, and FastRWR for different number of seed nodes. As the figure shows, the time cost increases for both L-PPR and LR-PPR algorithms as the number of seeds increases. But, the cost of LRPPR (which leverages re-use) increases much slower than the cost of L-PPR and both remain significantly cheaper than FastRWR.

3.1 Alternative Approaches
Global PPR: This is the default approach where the entire graph is used for PPR computation. We compute the PPR scores by solving the equation presented in Section 1.1. FastRWR: This is an approximation algorithm, referred to as NB_LIN in [22]. The algorithm reduces query execution times by partitioning the graph into subgraphs and preprocessing each partition. The pre-computed files are stored on disk and loaded to the memory during the query stage. To be fair to FastRWR, we selected the number of partitions in a way that minimizes its execution time and memory and maximizes its quality. L-PPR: This is our locality sensitive algorithm, where instead of using the whole graph, we use the localized graph created by combining the locality nodes and edges as described in Section 2.2. Once the localized transition matrix is created, the PPR scores are computed by solving the equation presented in Section 1.1. LR-PPR: This is the locality sensitive and re-use promoting algorithm proposed described in detail in Section 2.4. The restart probability,  , is set to 0.15 for all approaches.

3.2 Evaluation Measures
Efficiency: This is the amount of time taken to load the relevant (cached) data from the disk plus the time needed to carry out the operations to obtain the PPR scores.

Table 1: Data sets
Data Set Epinions SlashDot WikiTalk Overall Graph Characteristics # nodes # edges 76K 500K 82K 870K 2.4M 5M Seeds Data set Epinions 76K nodes 500K edges SlashDot 82K nodes 870K edges WikiTalk 2.4M nodes 5M edges # seeds 2 2 3 3 2 2 3 3 2 2 3 3 Dist (#hops) 3 4 3 4 3 4 3 4 3 4 3 4 Locality Graph Characteristics # nodes per neighborhood # edges per neighborhood from 200 to 2000 from 10K to 75K from 700 to 5000 from 10K to 75K from 700 to 6000 from 10K to 75K Execution Time (sec.) Global PPR 27.81 27.58 27.30 27.90 21.79 21.85 21.74 22.93 681.08 693.44 701.34 706.26 Fast RWR 0.21 0.22 0.21 0.22 0.35 0.35 0.36 0.38 16.28 16.22 16.32 16.34 LPPR 0.37 0.51 0.58 0.76 0.70 0.78 1.12 1.39 0.75 0.73 0.75 0.78 LRPPR 0.14 0.20 0.26 0.36 0.53 0.42 0.95 0.83 0.37 0.37 0.37 0.36 # seeds 2-3 2-3 2-8 Seeds seed distances (hops) 3-4 3-4 3-4

Table 2: Summary of the results for different configurations (in all scenarios, individual seed localities have 75K edges)
Merged Network Avg # nodes 2.2K 3.0K 2.7K 3.5K 5.9K 5.7K 7.1K 7.2K 5.7K 5.8K 6.3K 6.7K Avg # edges 90K 99K 108K 120K 117K 125K 141K 159K 102K 100K 101K 103K Top-10% Spearman's Correl. (vs. Global PPR) Fast LLRRWR PPR PPR 0.963 0.997 0.990 0.960 0.998 0.990 0.967 0.998 0.990 0.967 0.997 0.991 0.955 0.973 0.990 0.943 0.965 0.983 0.957 0.971 0.990 0.958 0.976 0.986 0.868 0.958 0.944 0.870 0.930 0.909 0.877 0.937 0.902 0.869 0.976 0.967 Memory usage(MB) Fast RWR 178.3 LPPR 2.9 3.1 4.6 4.7 5.0 4.9 7.6 7.2 15.5 16.2 24.0 28.7 LRPPR 36.3 55.2 57.6 77.7 228.1 172.8 325.9 256.0 114.5 120.7 211.6 197.5

302.1

1429.0

     #  
   "* +( (* 
!
',) ',) ',*


',+





&- &*
  


(&

)& &

*& '( ',


#  
 



Figure 7: Execution times of L-PPR, LR-PPR, and FastRWR for different numbers of seed nodes

4.

CONCLUSIONS

In this paper, we presented a Locality-sensitive, Re-use promoting, approximate Personalized PageRank (LR-PPR) algorithm for efficiently computing the PPR values relying on the localities of the seed nodes on the graph. Instead of performing a monolithic computation for the given seed node set using the entire graph, LRPPR divides the work into localities of the seeds and caches the intermediary results obtained during the computation. These cached results can then be reused for future queries sharing seed nodes. Experiments showed that the proposed LR-PPR approach provides significant gains in execution time relative to existing approximate PPR computation techniques, where the PPR scores are computed from scratch using the whole network. LR-PPR also outperforms L-PPR, where the PPR scores are computed in a locality-sensitive manner, but without significant re-use.

5.

REFERENCES
[1] K. Avrachenkov, N. Litvak, D. Nemirovsky, E. Smirnova, and M. Sokol. Quick Detection of Top-k Personalized PageRank Lists. WAW'11, 2011. [2] B.Bahmani, K.Chakrabarti, and D. Xin. Fast personalized PageRank on MapReduce. In SIGMOD'11. 973-984. 2011. [3] B.Bahmani, A.Chowdhury, and A.Goel. Fast incremental and personalized PageRank. PVLDB. 4, 3, 173-184, 2010. [4] A. Balmin, V. Hristidis, and Y.Papakonstantinou. ObjectRank: Authority-based keyword search in databases. VLDB, 2004.

[5] S. Brin and L. Page. "The anatomy of a large-scale hypertextual Web search engine". Computer Networks and ISDN Systems 30: 107-117, 1998. [6] K. S. Candan and W.-S. Li. Using random walks for mining web document associations. In PAKDD, pp. 294-305, 2000. [7] K. S. Candan and W.-S. Li. Reasoning for Web document associations and its applications in site map construction. Data Knowl. Eng. 43(2), 2002. [8] K. Csalogany, D.Fogaras, B. Racz, and T. Sarlos. Towards Scaling Fully Personalized PageRank: Algorithms, Lower Bounds, and Experiments Internet Math. 2,3, 333-358, 2005. [9] F. Fouss, A. Pirotte, J. Renders, and M. Saerens. Random-walk computation of similarities between nodes of a graph with application to collaborative recommendation. TKDE, 2007. [10] Y. Fujiwara, M. Nakatsuji, M. Onizuka, and M. Kitsuregawa. Fast and exact top-k search for random walk with restart. PVLDB. 5, 5, 442-453. 2012. [11] M. Gupta, A. Pathak, and S. Chakrabarti. Fast algorithms for Top-k Personalized PageRank Queries. In WWW'08. 1225-1226. 2008. [12] T.H. Haveliwala. Topic-sensitive PageRank. WWW'02. 517-526. 2002. [13] G. Jeh and J. Widom. Scaling personalized web search. Stanford University Technical Report. 2002. [14] S.D. Kamvar, T.H. Haveliwala, C.D. Manning, and G.H. Golub. Extrapolation methods for accelerating PageRank computations. In WWW'03 261-270. 2003. [15] G. Malewicz, et al. Pregel: a system for large-scale graph processing. SIGMOD'10, 2010. [16] Q. Mei, D. Zhou, and K. Church. Query suggestion using hitting time, CIKM'08, 2008. [17] C. Palmer, P. Gibbons, and C. Faloutsos. Anf: a fast and scalable tool for data mining in massive graphs. KDD'02, 2002. [18] W. Piegorsch and G. E. Casella. Inverting a sum of matrices. In SIAM Review, 1990. [19] P. Sarkar, A.W. Moore, and A. Prakash. Fast incremental proximity search in large graphs. ICML'08, 2008. [20] H. H. Song, et al. Scalable proximity estimation and link prediction in online social networks. In Internet Measurement Conference, pp. 322­335. 2009. [21] H. Tong, C. Faloutsos, and Y. Koren. Fast direction-aware proximity for graph mining. KDD, pp. 747­756, 2007. [22] H. Tong, C. Faloutsos, and J.-Y. Pan. Fast Random Walk with Restart and Its Applications. In ICDM'06. 613-622. 2006. [23] Y. Wu and L. Raschid, ApproxRank: Estimating Rank for a Subgraph, ICDE'09, 54-65, 2009.

    !

PageRank Revisited: On the Relationship between Node Degrees and Node Significances in Different Applications
Jung Hyun Kim
Arizona State University Tempe, AZ 85287, USA



K. Selçuk Candan
Arizona State University Tempe, AZ 85287-8809

Maria Luisa Sapino
University of Torino I-10149 Torino, Italy

jkim294@asu.edu ABSTRACT

candan@asu.edu

marialuisa.sapino@unito.it

Random-walk based techniques, such as PageRank, encode the structure of the graph in the form of a transition matrix of a stochastic process from which significances of the graph nodes can be inferred. Recommendation systems leverage such node significance measures to rank the objects in the database. Context-aware recommendation techniques complement the data graph with additional data that provide the recommendation context. However, despite their wide-spread use in many graph-based knowledge discovery and recommendation applications, conventional PageRank-based measures have various shortcomings. As we experimentally show in this paper, one such shortcoming is that PageRank scores are tightly coupled with the degrees of the graph nodes, whereas in many applications the relationship between the significance of the node and its degree in the underlying network may not be as implied by PageRank-based measures. In fact, as we also show in the paper, in certain applications, the significance of the node may be negatively correlated with the node degree and in such applications a naive application of PageRank may return poor results. To address these challenges, in this paper, we propose degree decoupled PageRank (D2PR) techniques to improve the effectiveness of PageRank based knowledge discovery and recommendation systems. These suitably penalize or (if needed) boost the transition strength based on the degree of a given node to adapt the node significances based on the network and application characteristics.

dation tasks [1, 7, 9, 12, 26]. The significance of a node in a given graph often needs to reflect the topology of the graph. Measures like the betweenness measure [27] and the centrality/cohesion [5], help quantify how significant any node is on a given graph based on the underlying graph topology. The betweenness measure [27], for example, quantifies whether deleting the node would disconnect or disrupt the graph. Centrality/cohesion [5] measures quantify how close to a clique the given node and its neighbors are. Other authority, prestige, and prominence measures [1, 5, 6] quantify the significance of the node through eigen-analysis or random walks, which help measure how reachable a node is in the graph.

1.1 PageRank as a Measure of Significance
Since enumerating all paths among the graph nodes would require time exponential in the size of the graph, random-walk based techniques encode the structure of the network in the form of a transition matrix of a stochastic process from which the node significance can be inferred.PageRank [6] is one of the most widely-used random-walk based methods for measuring node significance and has been used in a variety of application domains, including web search, biology, and social networks. The basic thesis of PageRank is that a node is important if it is pointed to by other important nodes ­ it takes into account the connectivity of nodes in the graph by defining the score of the node vi  V as the amount of time spent on vi in a sufficiently long random walk on the graph. More specifically, given a graph G(V, E ), the PageRank scores are represented as r, where r = TG r + (1 - )t where TG is a transition matrix corresponding to the graph G, t is 1 a teleportation vector (such that t[i] = V ), and  is the residual probability (or equivalently, (1 - ) is the so-called teleportation probability). Unless the graph is weighted, the transition matrix, TG , is constructed such that for a node v with k (outgoing) neighbors, the transition probability from v to each of its (outgoing) neighbors will be 1/k. If the graph is weighted, then the transition probabilities are adjusted in a way to account for the relative weights of the (outgoing) edges.

1.

INTRODUCTION

In recent years, there has been considerable interest in measuring the significance of a node in a graph and relatedness between two nodes in the graph, as if measured accurately, these can be used for supporting many knowledge discovery, search, and recommen This work is supported by NSF Grants 1339835 "E-SDMS: Energy Simulation Data Management System Software", 1318788 "Data Management for Real-Time Data Driven Epidemic Spread Simulations", 1518939 "RAPID: Understanding the Evolution Patterns of the Ebola Outbreak in West-Africa and Supporting Real-Time Decision Making and Hypothesis Testing through Large Scale Simulations", and 1430144 "Fraud Detection via Visual Analytics: An Infrastructure to Support Complex Financial Patterns (CFP) based Real-Time Services Delivery".

1.2 Tight Coupling of PageRank Scores of Nodes and their Degrees
Let us consider an undirected graph G(V, E ). There are two factors that contribute to the PageRank of a given node, v  V : · Factor 1: Significance of Neighbors: The more significant the neighbors of a node are, the higher its likelihood to be also significant. · Factor 2: Number of Neighbors (Degree of the Node) : Even if the neighbors are not all significant, a large number of

c 2016, Copyright is with the authors. Published in the Workshop Proceedings of the EDBT/ICDT 2016 Joint Conference (March 15, 2016, Bordeaux, France) on CEUR-WS.org (ISSN 1613-0073). Distribution of this paper is permitted under the terms of the Creative Commons license CCby-nc-nd 4.0

Data Set Correlation between PageRank and Degree

Listener Graph (Friendship edges, Last.fm) 0.988

Article Graph (co-author edges, DBLP) 0.997

Movie Graph (co-contributor edges, DBLP) 0.848

Table 1: Spearman's rank correlation between the node degree ranks and the node ranks' based on PageRank scores for various data graphs (see Section 4 for details of the data sets) neighbors would imply that the node, v , is well-connected and, thus, likely to be structurally important. In theory, these two factors should complement each other. In practice, however, the PageRank formulation described above implies that there is a very tight coupling between the degrees of the nodes in the graph and their PageRank scores (see Table 1).

of PageRank based knowledge discovery and recommendation systems. These techniques suitably penalize or (if needed) boost1 the transition strength based on the degree of a given node to adapt the node significances based on the network and application characteristics. This paper is organized as follows: Next, we discuss the related literature. In Sections 3, we introduce the proposed degreedecoupled PageRank techniques. We evaluate the proposed techniques in Section 4 and conclude in Section 5.

2. RELATED WORKS 2.1 Context-Sensitive PageRank
Path-length based definitions of node relatedness, such as those proposed by [4, 24] help capture the relatedness of a pair of nodes solely based on the properties of the nodes and edges on the shortest path between the pair. Random-walk based definitions, such as hitting distance [10,21] and personalized page rank (PPR) score [1, 9, 16], of node relatedness further take into account the density of the edges: as in path-length based definitions, random-walk based definitions also recognize that a node is more related to another node if there are short paths between them; however, random walkbased definitions of relatedness also consider how well the given pair of nodes are connected. In [7], authors construct a transition matrix, TS , where edges leading away from the seed nodes are weighted less than those edges leading towards the seed nodes. An alternative approach for contextualizing PageRank scores is to use the PPR techniques [1,9] discussed in the introduction. One key advantage of this teleportation vector modification based approach over modifying the transition matrix, as in [7], is that the term  can be used to directly control the degree of seeding (or personalization) of the PPR score. [10, 21] rely on a random walk hitting time based approach, where the hitting time is defined as the expected number of steps a random walk from the source vertex to the destination vertex will take. [17] leveraged these properties of PPR to develop localitysensitive algorithms to rank nodes of graphs which are relative to a given set of seed nodes efficiently.

1.2.1 Problem I: When a Large Node Degree Does Not Indicate High Node Significance
In this paper, we highlight (and experimentally show) that, in many applications, node degree and node significance are in fact inversely related and that the tight-coupling between node degrees and PageRank scores might be counter-productive in generating accurate recommendations. E XAMPLE 1. Consider, for example, a recommendation application where a movie graph, consisting of movie and actor nodes, is used for generating movie recommendations. In this application, the first factor (significance of neighbors) clearly has a positive contribution: a movie with good actors is likely to be a good movie and an actress playing in good movies is likely to be a good actress. On the other hand, the second factor (number of neighbors) may in fact be a negative contributor to node significance: the fact that an actor has played in a large number of movies may be a sign that he is a non-discriminating ('B movie') actor, whereas an actress with relatively fewer movies may be a more discriminating ('A movie') actress. As we see in Section 4, this observation turns out to be true in many applications, where (a) acquiring additional edges has a cost that is correlated with the significance of the neighbor (e.g. the effort one needs to invest to a high quality movie) and (b) each node has a limited budget (e.g. total effort an actor/actress can invest in his/her work).

2.2 Improvements to the PageRank Function
Due to the obvious relationship between ranking and monetary rewards (e.g. through selling of advertisements on web search applications), there has been considerable effort in engineering (or manipulating) graphs in a way to maximize ranking scores of particular nodes. This is commonly referred to as PageRank optimization. One way to achieve this goal is carefully adding or removing certain links: If, for example, one or more colluding webmasters can add or remove edges, PageRank scores of target web pages or domains can be increased [23]. [20] established several bounds indicating to what extent the rank of the pages of a website can be changed and the authors derived an optimal referencing strategy to boost PageRank scores. A related, but opposite, problem is to protect the PageRank scores against negative links (which may indicate, for example, negative influence or distrust in a social network), artificial manipulation, and spam. [3], for example, focused on identifying spam pages and link farms and showed that better PageRank scores can be obtained after filtering spam pages and links. In [14], authors show that PPR algorithms that do not differentiate among the seed nodes may not properly rank nodes and present robust personalized PageRank (RPR) strategies, which are insensitive to noise in the set of seed nodes. In this context, de-coupled does not necessarily imply decorrelated. In fact, D2PR can boost correlation between node degree and PageRank if that is required by the application.
1

1.2.2 Problem II: When PageRank Does Not Sufficiently Account for Contributions of Degrees
The mismatch between PageRank and node significance is not limited to the cases where node degrees are inversely related to the node significance. As we see in Section 4, there are other scenarios where PageRank may, in fact, fail to sufficiently account for the contribution of the node degrees to their significances.

1.3 PageRank Revisited: De-coupling Node Significance from Node Degrees
As we discussed above, one key shortcoming of the conventional PageRank scores is that they are often tightly coupled with the degrees of the graph nodes and in many applications the relationship between the significance of the node and its degree in the underlying network may not be as implied by PageRank-based measure: in certain applications, the significance of the node may be negatively correlated with the node degree, whereas in others PageRank may not be sufficient in accounting for degree contributions. Naturally, in such applications a naive application of PageRank in generating recommendations may return poor results. To address these challenges, in this paper, we propose degree decoupled PageRank (D2PR) techniques to improve the effectiveness

There are some efforts to change the impact of degrees on the PageRank computation. [2] proposed a way to boost the power of low-degree nodes in a network. The impact from nodes which are important but are not hubs is relatively small compared to other nodes which are less important with high degrees. To boost the low-degree important nodes for equal opportunity, the teleportation vector is modified with being proportional to the degrees of nodes. [11] boosted the degrees of nodes to reduce the expected cover time of the entire graph by the biassed random-walk.

B

E

Dest. vj B C D

deg. (vj ) 2 3 1

A

C
D

F

Transition probability from A to its neighbors vj p=0 2 -2 0.33 0.18 0.29 0.33 0.08 0.64 0.33 0.74 0.07

(a) A sample graph

(b) Transition probabilities from A

3.

DEGREE DE-COUPLED PAGERANK

The key difficulty of de-coupling node degrees from the PageRank scores is that the definition of the PageRank, based on random walk transitions, is inherently dependent on the number of transitions available from one node to the other. As we mentioned above, the more ways there are to reach into a node, the higher will be its PageRank score.

Figure 1: In conventional PageRank (p = 0), the transition probabilities from node vi = A to all its neighbors vj are the same. In degree de-coupled PageRank (D2PR), the value of p can be used to penalize (p > 0) or boost (p < 0) transition probabilities based on the degree of the destination
node id 53608 351 ... 79538 79917 node degree 883 739 ... 1 1 Ranks of the graph nodes for different de-coupling weights (p) -4 -2 0 2 4 1 1 69 5549 6793 2 12 425 1992 1935 ... ... ... ... ... 7661 7545 4149 195 182 7793 7790 7522 2443 2043

3.1 Desideratum
Therefore, to de-couple the PageRank score from node degrees, we need to modify the transition matrix. In particular, for each node vi in the graph, we would like to be able to control the transition process with a single parameter (p), such that · if p  -1, transitions from node vi are  100% towards the neighbor with the highest degree, · if p = -1, transition probabilities from node vi are proportional to the degrees of its neighbors, · if p = 0, the transition probabilities mirror the standard PageRank probabilities (assuming undifferentiated neighbors), · if p = 1, transition probabilities from node vi are inversely proportional to the degrees of its neighbors, · if p  1, transitions from node vi are  100% towards the neighbor with the lowest degree. In other words, the transition function should de-couple the transition process from node-degrees and penalize or boost the contributions of node degrees in the transition process, as needed.

Table 2: Ranks of graph nodes of different degrees on a sample graph for different de-coupling weights, p: as we see in this figure, when p > 0, high degree nodes are pushed down in the rankings (reducing the correlation between degree and rank), while when p < 0, they are pulled up (improving the correlation between degree and rank) · p  R is a degree de-coupling weight. Intuitively, the numerator term, deg (vj )-p , ensures that the edge incoming to vj is weighted by its degree: if p > 0, then its degree negatively impacts (reduces) transition probabilities into vj , if p < 0 then its degree positively impacts (boosts2 ) transition probabilities into vj , and if p = 0, we obtain the standard PageRank formulation without degree de-coupling. In other words, the transition function satisfies our desideratum of de-coupling the transition process from node-degrees and penalizing or boosting the contributions of node degrees on-demand. Note that, since all transitions from the node vi are degree de-coupled individually based on the degrees of their destinations, the denominator term, -p , ensures that the transition probabilvk neighbor (vi ) deg (vk ) ities from node vi add up to 1.0. Note also that when there is no edge between node vi and vj , TD (j, i) = 0 and, consequently, the term TD (j, i) is not affected by the degree de-coupling process. E XAMPLE 2. Figure 1 shows how the random walk probabilities are differentiated in a degree de-coupled transition matrix on a sample graph where a node A has three neighbors, B (with degree 2), C (with degree 3), and D (with degree 1). In conventional PageRank, the transition probabilities from node A to all its neighbor nodes are equal to 0.33. In degree de-coupled PageRank (D2PR), however, the value of p is used for explicitly accounting for the impact of node degree on the transition probabilities: When p = 2, the transition probabilities from A to its neighbors are 0.18, 0.08, and 0.74, which penalizes nodes which have larger degrees, whereas when p = -2, D2PR boosts the transition probabilities to large degree nodes leading to transition probabilities 0.29, 0.64, and 0.07, respectively.  This example shows that, in degree de-coupled PageRank (D2PR), as we also see in Table 2, the value of p can be used to penalize (p > 0) or boost (p < 0) transition probabilities based on the degree of the destination, vj .
2 In fact, a similar function was used in [11] to quickly locate nodes with higher degrees in a given graph.

3.2 Degree De-coupling Transition Matrix
In this subsection, we will consider degree de-coupling of the transition matrix as implied by the above desideratum.

3.2.1 Undirected Unweighted Graphs
Let G = (V, E ) be an undirected and unweighted graph. Let  also be a given residual probability parameter, and deg (v ) be a function which returns the number of edges on the node v . We represent degree de-coupled PageRank (D2PR) scores in the form of a vector d = TD d + (1 - )t, where t is the teleportation vector, such that t[i] = and TD is a degree de-coupled transition matrix, TD (j, i) =
1 V

for all i

deg (vj )-p , -p vk neighbor (vi ) deg (vk )

(1)

where · TD (j, i) denotes the degree de-coupled transition probability from node vi to node vj over an edge eij = [vi  vj ] when there exists at least one edge between two nodes, · neighbor (vi ) is the set of all neighbors of the source node, vi , and

3.2.2 Directed Unweighted Graphs
The semantics of degree de-coupling is slightly different in directed graphs. In particular, edges incoming to vi often do not require a particular effort from vi to establish and hence are often out of the control of vi , but indicate a certain degree of interestingness, usefulness, or authority as perceived by others. The same is not true for edges outgoing from vi ; in particular, a vertex with a large number of outgoing edges may either indicate a potential hub or simply indicate a non-discerning connection maker. The distinction between these two situations gains importance especially in applications where establishing a new connection has a non-negligible cost to the source node and, thus, a large number of outgoing edges may indicate either (a) a very strong participant to the network or (b) a very poor participant with a large number of weak linkages. Let G = (V, E ) be a directed graph and for the simplicity of the discussion, without any loss of generality, let us assume that G is unweighted. Let us also be given a residual probability parameter,  and let outdeg (v ) be a function which returns the number of outgoing edges from the node v . The degree de-coupled PageRank (D2PR) scores can be represented in the form of a vector d, d = TD d + (1 - )t, where t is the teleportation vector, such that 1 t[i] = V for all i and TD (j, i) = outdeg (vj )-p , -p [vi vk ]out_edges(vi ) outdeg (vk )

accounts for the connection strength (as in the conventional PageRank) whereas TD is a degree de-coupled transition matrix, TD (j, i) = (vj )-p
[vi vk ]out_edges(vi )

(vk )-p

,

such that, TD (j, i) denotes the degree de-coupled transition probability from node vi to node vj over an edge eij = [vi  vj ], p  R is a degree de-coupling weight, and (v ) =
[v vh ]out_edges(v )

w(v  vh ).

Note that, above,  controls whether accounting for the connection strength or degree de-coupling is more critical in a given application. In Section 4, we will study the impact of degree de-coupling in weighted graphs for different scenarios.

4. CASE STUDIES
In this section, we present case studies assessing the effectiveness of the degree de-coupling process and the relationship between the degree de-coupling weight p and recommendation accuracy for different data graphs.

4.1 Setup
For all experiments, the degree de-coupling weight, p, is varied between -4 and 4 with increments of 0.5. The residual probability, , is varied between 0.5 and 0.9, with default value chosen as 0.85. We also varied the  parameter, which controls whether accounting for the connection strength or degree de-coupling is more critical in a given application, between 0.0 and 1.0, with the default value set to 0 (indicating full decoupling).

where TD (j, i) denotes the degree de-coupled transition probability from node vi to node vj over an edge eij = [vi  vj ], out_edges(vi ) is the set of out-going edges from the source node, vi , and p  R is a degree de-coupling weight. E XAMPLE 3. Figure 2 (a) in Section 4 provides an example illustrating the correlations between the degree de-coupled PageRank (D2PR) scores and external evidence for different values of p for some application: here, the higher the correlation, the better resulting ranking reflects the application semantics. As we see in this example, which we will investigate in greater detail in Section 4, the optimal de-coupling weight is not always p = 0 as implied by the conventional PageRank measure. In this particular case, for example, the correlation between D2PR and external evidence of significance is maximized when the de-coupling weight, p, is equal to 0.5, implying that in this application a moderate degree of penalization based on the node degrees is needed to align PageRank scores and application semantics. 

4.1.1 Datasets
Four real data sets are used for the experiments. Each data set is used to create two distinct data graphs and corresponding ratings data. Table 3 provides further details about the various graphs created using these four data sets. These recommendation tasks based on these data graphs are detailed below: · For the IMDB [15] data set, we created (a) a movie-movie graph, where movie nodes are connected by an edge if they share common contributors, such as actors, directors, writers, composers, editors, cosmetic designers, and producers and (b) an actor-actor graph based on whether two actors played in the same movie. Applications: For this data set, we consider applications where movies are rated by the users: thus, we merged the IMDB data with the MovieLens 10M [22] data (based on movie names) to identify user ratings (between 1 and 5) for the movies in the graph. We consider the (a) average user rating as the significance of the movies in the movie-movie graph and (b) average user rating of the movies played in as the significance of the actors in the actor-actor graph. · For the DBLP [26] data set, we constructed (a) an article-article graph where scientific articles were connected to each other if they shared a co-author and (b) an author-author graph based on coauthorship. Applications: (a) In the article-article graph, the number of citations to an article is used to indicate its significance. Similarly, (b) in the author-author graph, average number of citations to an author's papers is used as his/her significance. · For the Last.fm [18], we constructed (a) a listener-listener graph, where the nodes are Last.FM listeners and undirected edges reflect friendship information among these listeners. We also constructed (b) an artist-artist graph based on shared listeners. Applications: (a) In the listener-listener graph, we considered the total listening

3.2.3 Weighted Graphs
Once again, the semantics of degree de-coupling need to be reconsidered for weighted graphs. Let G = (V, E, w) be a directed, weighted graph, where w(e) is a function which returns the weight of the edge associated with edge e. It is important to note that, in such a graph, the weight of an edge can 1) indicate the strength of the connection between two nodes (thus positively contributing to the significance of the destination node); and at the same time and 2) contribute to the degree of a node as a multiplier (thus positively or negatively contributing to the node significance depending on the degree-sensitivity of the application). In other words, given an edge eij = [vi  vj ], from node vi to node vj , the transition probability from vi to vj can be written as T(j, i) =  Tconn_strength (j, i) + (1 -  )TD (j, i), where Tconn_strength (j, i) =
[vi vh ]out_edges(vi )

w(vi  vj ) , w(vi  vh )

Data IMDB DBLP Last.fm Epinions

Graph movie-movie actor-actor article-article author-author listener-listener artist-artist commenter-commenter product-product

# of nodes 191,602 32,208 8,808 47,252 1,892 17,626 6,703 13,384

# of edge 4,465,272 2,493,574 951,798 310,250 25,434 2,640,150 2,395,176 2,355,460

Average node degree 23.30 77.42 108.06 6.57 13.44 149.79 425.05 175.99

Standard deviation of node degrees 51.86 67.15 171.25 8.89 17.31 299.66 438.97 224.12

Median standard deviation of neighbors' node degrees 2.89 114.41 309.92 6.39 22.37 998.53 609.39 202.78

Table 3: Data sets and data graphs activity of a given listener as his/her significance. (b) In the artistartist graph, the number of times an artist has been listened is considered as his/her significance. · For the Epinions [25]: We constructed (a) a commentercommenter graph based on the products on which two individuals both commented and (b) a product-product graph based on shared commenters. Applications: (a) For the nodes on the commentercommenter graph, the number of trusts the commenter received from others is used as his/her commenter significance. (b) For each product in the product-product graph, its average rating by the commenters is used as its node significance. when the degrees are over-penalized (i.e., when p  0.5). The Epinions product-product graph (based on common commenters, Figure 2(c)) also provides the highest correlations with p > 0, but behaves somewhat differently from the other two cases: the correlations stabilize and do not deteriorate significantly when degrees are over-penalized, indicating that the need for degree penalization is especially critical in this case: this is due to the fact that, the larger the number of comments a product has, the more likely it is that the comments are negative (Figure 5). In fact, we see that, among the three graphs, this is the only graph where the traditional PageRank (with p = 0) leads to negative correlations between node ranks and node significances. These results indicate that actors who have had many co-actors, commenters who commented on products also commented by many others, or products which received comments from individuals who also commented on many other products are not good candidates for transition during random walk. This aligns with our expectation that, in applications where each new movie role or comment requires additional effort, high degree may indicate lower per-movie or per-comment effort and, hence, lower significance.

4.2 Measures
In this section, our goal is to observe the impact of different D2PR degree de-coupling weights on the relationship between D2PR rankings and application specific significance measures for the above data sets3 . We also aim to verify whether de-coupling weights can also be used to improve recommendation accuracies. In order to measure the relationship between the degree decoupled PageRank (D2PR) scores and the application-specific node significance, we used Spearman's rank correlation,
i (xi i (xi

-x ¯)(yi - y ¯)
i (yi

-x ¯)2

-y ¯)2

,

4.3.2 Application Group B: When Conventional PageRank is Ideal
Figure 3 shows that, for movie-movie (based on common actors) and author-author (based on common articles) graphs, the peak correlation is at p = 0 indicating that the conventional PageRank which gives positive weight to node degree, is appropriate. This perhaps indicates that movies with a lot of actors tend to be big-budget products and that authors with a large number of coauthors tend to be experts with whom others want to collaborate. Note that, in these applications, additional boosting, with p < 0, negatively affects the correlation, indicating that the relationship between node degree and significance is not very strong (Figure 5). The quick change when p < 0 is because, as we see in Table 3, median standard deviations of neighbors' degrees are low; i.e., degrees of neighbors of a node are comparable: there is no dominant contributor to TD (j, i) in Equation 1 (Section 3) and, thus, the transition probabilities are sensitive to changes in p, when p < 0.

which measures the agreement between the D2PR ranks of the nodes in the graph and their application-specific significances. Here, x are rankings by D2PR and y are significances for an application and x ¯ and y ¯ are averages of two values.

4.3 Impact of De-Coupling in Different Applications (Unweighted Graphs)
In this subsection, we present results that aim to assess D2PR under the settings described above. For these experiments, the residual probability, , and the parameter,  , are set to the default values, 0.85 and 0, respectively. In these experiments, we consider only unweighted graphs (we will study the weighted graphs and the impact of parameter  later in Section 4.5). Figures 2 through 4 include charts showing the Spearman's correlations between the D2PR ranks and application specific node significances for different values of p and for different data graphs. These figures clearly illustrate that different data graphs require different degrees of de-coupling4 to best match the application specific node significance criterion.

4.3.3 Application Group C: When Degree Boosting Helps
Figure 4 shows that there are scenarios where additional boosting based node degrees provides some benefits. The article-article (based on common authors), listener-listener (based on common artists), and artist-artist (based on common listeners) graphs reach their peaks around p  -1, indicating that these also benefit from large node degrees though improvements over p = 0 are slight. A significant difference between applications in Group B and Group C is that, for p < 0, the correlation curve is more or less stable. This is because, as we see in Table 3, in these graphs median standard deviations of neighbors' degrees are high: in other words, for each node, there is a dominant neighbor with a high degree and this neighbor has the highest contribution to TD (j, i); thus, the rankings are not very sensitive to p, when p < 0.

4.3.1 Application Group A: When Degree Penalization Helps
The actor-actor (based on common movies) and commentercommenter (based on common products) graphs have highest correlation at p = 0.5, with the correlations dropping significantly
3 In this paper, we are not proposing a new PageRank computation mechanism. Because of this (and since the focus is not improving scalability of PR), we do not report execution times and compare our results with other PageRank computation mechanisms. 4 Degree penalization or degree-based boosting

  )

   
 % ! &

# $

   '

   
 
 # ! 
" 
   $

   '

   
 
 # ! 
"
   $



' 



  %  

  %  

























 "
  #$

 "
  #$

























 $ ! %&  $   %*(&  "
   #(&$  "




  #(&$

(a) IMDB (actor-actor) (b) Epinions (commenter-commenter) (c) Epinions (product-product) Figure 2: Application Group A: p > 0 is optimal (i.e., node degrees need to be penalized)

  ( 
 
  
 $   
% " 
#


  )

   
 % ! &

#  $ 



& 



' 

























 $ ! %&

 #  $%



 #


 $)'%

 $

  %*(&

(a) DBLP (author-author) (b) IMDB (movie-movie) Figure 3: Application Group B: p = 0 is optimal

  + 
 
  
 %   
& " 
$


   )

   
 
 $ "! #   %

   )

   
 
 $ "! #    %

)#*)- )#*),


' 
   &  

("))+ ("))*
  &  

)#*)*

("*(,) ("*(++

 #
  $%

 $  %&

 #
  $%

 $


 %.)&

 #


  $,(%

 #


  $-(%

(a) DBLP (article-article) (b) Last.fm (listener-listener) (c) Last.fm (artist-artist) Figure 4: Application Group C: p < 0 is optimal (i.e., node degrees need to be boosted)
p<0
 
  
          
  

p=0

p>0

Figure 5: Correlations between node degrees and application specific significances for different data graphs (each color group is a distinct pattern in Figures 2 through 4).

4.3.4 Summary: Correlations between Node Degrees and Application Specific Significances
The experiments reported above show that degree de-coupling is important as different applications, even on the same data set, may associate different semantics to node degrees and the conventional PageRank scores are too tightly coupled with node degrees to be effective in all scenarios. Figure 5, which plots correlations between node degrees and application specific significances for different data graphs, re-confirms that the ideal value of the p is related to the usefulness of the node degree in capturing the application specific definition of node significance.

4.4 Relationship between  and p
In Figures 6 through 8, we investigate the relationship between the value  and the degree de-coupling parameter p for different application types. Here we use the default value, 0, for the parameter  and present the results for unweighted graphs (the results for

the weighted graphs are similar). First thing to notice in these figures is that the grouping of the applications (into those where, respectively, p > 0, p = 0, or p < 0 is useful) is preserved when different values of  are considered. Figure 6 studies the impact of the value of  in application group A, where degree penalization helps (p > 0). As we see here, for the IMDB actor-actor (Figure 6(a)) and Epinions commentercommenter (Figure 6(b)) graphs, having a lower value of  (i.e., lower probability of forward movement during the random walk) provides the highest possible correlations between D2PR ranks and node significance (with the optimal value of p being  0.5 independent of the value of ). This indicates that in these graphs, it is not necessary to traverse far during the random walk. Interestingly, though, when degrees are over-penalized (i.e., p  0), smaller values of  start leading to worse correlations, indicating that (while not being optimal) severe penalization of node degrees helps make random traversals more useful than random jumps. As we have already observed in Figure 2(c), the Epinions productproduct graph (Figure 6(c)) behaves somewhat differently from the other two cases where degree penalization (p > 0) leads to larger correlations: in this case, unlike the other two graphs, the highest possible correlations between D2PR ranks and node significance are obtained for large values of , indicating that this application benefits from longer random walks (though the differences among the correlations for different  values are very small). Figure 7 shows that the pattern is different for application group B, where conventional PageRank is ideal (p = 0): in this case, having a larger value of  (i.e., larger probability of forward movement during the random walk) provides the highest correlations between ranks and significance. Interestingly, in these applications, when

     
 


  (

   
   $ !  % ' " )


   +    
 
   "   & # 
%
    '
  '  

   *      !   % " 
$
   &



* 






  (  


































 













 '  " () $2,&. $2,&/ $2,&0. $2,&1 "0*$,

 %
  &'

 $
  %&


"0*$-

"0*$.,

"0*$/

!/)#+

!/)#,

!/)#-+

!/)#.

(a) IMDB (actor-actor) (b) Epinions (commenter-commenter) (c) Epinions (product-product) Figure 6: Relationship between p and , for application group A, where p > 0 is optimal (i.e., degrees need to be penalized)

  , '


) 



 
  
   # 
 $ 
&
 ! 
(

* 

  (

   
   $ !  % !'! " )

























 '  " ()

  

 & ! '(

#1+%-

#1+%.

#1+%/-

#1+%0

$2,&.

$2,&/

$2,&0.

$2,&1

(a) DBLP (author-author) (b) IMDB (movie-movie) Figure 7: Relationship between p and , for application group B, where p = 0 is optimal

  , ' 
 
  
   # 
 $ 
&
 ! 
(
  (  

   +    
 
   "   & $# %    '

   +    
 
   "   & $# %     '



) 


  (  

 %
  &'

 & ! '(

 %
  &'

#1+%-

#1+%.

#1+%/-

#1+%0

"0*$,

"0*$-

"0*$.,

"0*$/

"0*$,

"0*$-

"0*$.,

"0*$/

(a) DBLP (article-article) (b) Last.fm (listener-listener) (c) Last.fm (artist-artist) Figure 8: Relationship between p and , for application group C, where p < 0 is optimal (i.e., node degrees need to be boosted) p  0 or p  0, higher probabilities of random walk traversal (i.e., larger ) stop being beneficial and lower values of  lead to larger correlations. This re-confirms that, for these applications, p  0 leverages the random walk traversal the best. As we see in Figure 8, in application group C, where degree boosting helps (p < 0), it is also the case that larger values of  (i.e., larger probabilities of forward transitions during the random walk) provides the highest correlations between node ranks and significance. On the other hand, in these applications, p  0.5 serves as a balance point where the value of  stops being relevant; in fact, for p > 0.5 the higher values of  stops being beneficial and lower values of  lead to larger correlations. This re-confirms that smaller values of p (which provides degree boosting) help leverage the random walk traversal the best. the greater  is), the larger is the optimal value of p. Figure 10 shows that, for applications in group B, where p  0 is ideal, when the connection strength is given significantly more weight than degree de-coupling (i.e.,   0), we observe high ranksignificance correlations. Interestingly however, for the moviemovie graph (where the edge weights denote common actors) the highest correlations are obtained not with p = 0, but with p = 0.5 and  = 0.75, indicating that degree penalization is actually beneficial in this case: movies that share large numbers of actors with other movies are likely to be B -movies, which are not good candidates for transitions during the random walk. Figure 11 shows that in application group C, where degree boosting (p < 0) helps, giving more weight to connection strength (i.e.,   1.0) is a good, but not necessarily the best strategy. In fact, in these graphs, the highest overall correlations are obtained with  = 0 or  = 0.25, indicating that degree de-coupling is beneficial also in these cases. Interestingly, (unlike the case with the unweighted listener-listener graph, where the best correlation was obtained when p < 0) for the weighted version of the listenerlistener graph (where edge weights denote the number of shared friends), when  = 0 through 0.5, p = 0 provides the highest correlation and when  = 0.75, p = 0.5 provides the highest correlation ­ these indicate that listeners who have large numbers of shared friends with others are good candidates for random walk. Note that a key observation from the above results is that the conventional PageRank, based on connection strength (i.e.,  = 1.0), is not always the best strategy for the applications considered.

4.5 Relationship between Weighted Graphs



and

p

in

Finally, in Figures 9 through 11, we investigate the relationship between the value  (which controls whether accounting for the connection strength or degree de-coupling is more critical in a given application) and the degree de-coupling parameter p for different application types. Here we use the default value, 0.85, for the parameter  and present the results for weighted graphs: Figure 9 depicts the impact of the value of the parameter  in application group A, where degree penalization helps (p > 0). As we see here, for all three weighted graphs, performing degree penalization (i.e.,  < 1.0) provides better rank-significance correlation than relying solely on the connection strength (i.e.,  = 1.0). Note that the value of  impacts the optimal value of degree penalization parameter p: the more weight is given to connection strength (i.e.,

5. CONCLUSIONS
In this paper, we noted that in many applications the relation-

)

- 

  1    
   $ !  % ( " * +("& .   !,
  +  

   /    
 
   "   ' # 
&
   ( )& $ ,    
*

   .      !   & " 
%
   ' (%# +    
)

 
























  *  



 (  " )*



 &
  '( $40 "2"2-%/0 "2-%0 "2-%10 "2. !1,

 %
  &' !1,$./ !1,$/ !1,$0/ !1-

$4/

$4/'12

$4/'2

$4/'32

(a) IMDB (actor-actor) (b) Epinions (commenter-commenter) (c) Epinions (product-product) Figure 9: Relationship between p and  , for application group A, where p > 0 is optimal (i.e., node degrees need to be penalized)

  0 
 
  
   # 
 ( $ 
'
 ! 
) *'!% -  
+

-  

, 


)

  1    
   $ !  % !(! " * +("& .   ,

























 (  " )*

  

 ' ! ()

#3.

#3.&01

#3.&1

#3.&21

#3/

$4/

$4/'12

$4/'2

$4/'32

$40

(a) DBLP (author-author) (b) IMDB (movie-movie) Figure 10: Relationship between p and  , for application group B, where p = 0 is optimal

  0 
 
  
   # 
 ( $ 
'
 ! 
) *'!% -  
+
  +  

   /    
 
   "   ' %# &   ( )& $ ,    *
  +  

   /    
 
   "   ' %# &    ( )& $ ,    *



, 


 &
  '(

 ' ! ()

 &
  '(

#3.

#3.&01

#3.&1

#3.&21

#3/

"2-

"2-%/0

"2-%0

"2-%10

"2.

"2-

"2-%/0

"2-%0

"2-%10

"2.

(a) DBLP (article-article) (b) Last.fm (listener-listener) (c) Last.fm (artist-artist) Figure 11: Relationship between p and  , for application group C, where p < 0 is optimal (i.e., node degrees need to be boosted) ship between the significance of the node and its degree in the underlying network may not be as strong (or as weak) as implied by PageRank-based measures. We proposed degree de-coupled PageRank (D2PR) to improve the effectiveness of PageRank based knowledge discovery and recommendation tasks. Evaluations on different data graphs and recommendation tasks have confirmed that degree de-coupling would be an effective way to match application specific node significances and improve recommendation accuracies using PageRank based approaches.
[11] C. Cooper, et al. A fast algorithm to find all high degree vertices in graphs with a power law degree sequence. In WAW'12, 2012. [12] O. Fercoq. PageRank optimization applied to spam detection. arXiv:1203.1457, 2012. [13] T. H. Haveliwala. Topic-sensitive PageRank. WWW, 2002. [14] S. Huang, X. Li, K.S Candan, M.L Sapino. "Can you really trust that seed?": Reducing the Impact of Seed Noise in Personalized PageRank. ASONAM'14, 2014. [15] IMDB website: http://www.imdb.com/ [16] G. Jeh and J. Widom. Scaling personalized web search. Stanford Univ. Tech. Report. 2002. [17] J.H Kim, K.S Candan, M.L Sapino. Locality-sensitive and Re-use Promoting Personalized PageRank Computations. Knowledge and Information Systems. 10.1007/s10115-015-0843-6, 2015 [18] http://ir.ii.uam.es/hetrec2011/datasets.html. [19] A.N. Nikolakopoulos and J. Garofalakis, NCDawareRank: A Novel Ranking Method that Exploits the Decomposable Structure of the Web. WSDM, 2013. [20] F. Mathieu and L. Viennot, Local aspects of the global ranking of web pages. In I2CS'06, pp. 1­10, 2006. [21] Q. Mei, D. Zhou, and K. Church. Query suggestion using hitting time. CIKM'08, 2008. [22] http://grouplens.org/datasets/movielens [23] M. Olsen. Maximizing PageRank with New Backlinks. CIAC, 2010. [24] C. Palmer, P. Gibbons, and C. Faloutsos. Anf: a fast and scalable tool for data mining in massive graphs. KDD, 2002. [25] J. Tang, H. Gao, and H. Liu. mTrust: Discerning multi-faceted trust in a connected world. WSDM, 2012. [26] J. Tang, et al. ArnetMiner: Extraction and Mining of Academic Social Networks. In SIGKDD'08, pp 990­998, 2008 [27] D.R. White, et al. Betweenness centrality measures for directed graphs. Social Networks, 16, 335-346,1994.

[1] A. Balmin, V. Hristidis, and Y. Papakonstantinou. ObjectRank: Authority-based keyword search in databases. In VLDB'04. [2] D. Banky, G. Ivan, V. Gromusz. Equal Opportunity for Low-Degree Network Nodes: A PageRank-Based Method for Protein Target Identification in Metabolic Graphs. PLoS One 8(1): e542-4, 2013. [3] L. Becchetti et al. Using Rank Propagation and Probabilistic Counting for Link-Based Spam Detection. WebKDD, 2006. [4] P. Boldi et al. HyperANF: Approximating the neighbourhood function of very large graphs on a budget. WWW, 2011. [5] M.G.Borgatti, et al. Network measures of social capital. Connections 21(2):27-36, 1998. [6] S. Brin and L. Page. The anatomy of a large-scale hypertextual web search engine. Computer Networks and ISDN Systems, 30, 1998. [7] K.S. Candan and W.D. Li. Using random walks for mining web document associations. PAKDD'00, pp. 294-305, 2000. [8] M. Chen., et al. Clustering via random walk hitting time on directed graphs. AAAI'08. [9] S. Chakrabarti. Dynamic personalized pagerank in entity-relation graphs. In WWW'07, pp 571­580, 2007. [10] M. Chen, J. Liu, and X. Tang. Clustering via random walk hitting time on directed graphs. AAAI'08, pp. 616­621, 2008.

6.

REFERENCES

KSGM: Keynode-driven Scalable Graph Matching
Xilun Chen, K. SelÃ§uk Candan

Maria Luisa Sapino

Paulo Shakarian

Arizona State University
Tempe, AZ, USA
{xilun.chen, candan}@asu.edu

University of Torino
Torino, Italy
marialuisa.sapino@unito.it

Arizona State University
Tempe, AZ, USA
shak@asu.edu

ABSTRACT
Understanding how a given pair of graphs align with each other
(also known as the graph matching problem) is a critical task in
many search, classification, and analysis applications. Unfortunately, the problem of maximum common subgraph isomorphism
between two graphs is a well known NP-hard problem, rendering
it impractical to search for exact graph alignments. While there
are several heuristics, most of these analyze and encode global and
local structural information for every node of the graph and then
rank pairs of nodes across the two graphs based on their structural
similarities. Moreover, many algorithms involve a post-processing
(or refinement) step which aims to improve the initial matching
accuracy. In this paper 1 we note that the expensive refinement
phase of graph matching algorithms is not practical in any application where scalability is critical. It is also impractical to seek
structural similarity between all pairs of nodes. We argue that a
more practical and scalable solution is to seek structural keynodes
of the input graphs that can be used to limit the amount of time
needed to search for alignments. Naturally, these keynodes need to
be selected carefully to prevent any degradations in accuracy during the alignment process. Given this motivation, in this paper,
we first present a structural keynode extraction (SKE) algorithm and
then use structural keynodes obtained during off-line processing
for keynode-driven scalable graph matching (KSGM). Experiments
show that the proposed keynode-driven scalable graph matching algorithms produce alignments that are as accurate as (or better than)
the state-of-the-art algorithms, with significantly faster online executions.

1.

!"#$%&'&

!"#$%&(&

Figure 1: Graph matching/alignment problem seeks a maximum common subgraph isomorphism between two input graphs
they represent the pairwise relationships between the nodes of the
graph. Edges can be directed or undirected, meaning that the relationship can be non-symmetric or symmetric, respectively. Nodes
and edges of the graph can also be labeled or non-labeled. The
label of an edge, for example, may denote the name of the relationship between the corresponding pair of nodes or may represent
other meta-data, such as the certainty of the relationship or the cost
of leveraging that relationship within an application.
Due to the success of the graph model as a powerful and flexible data representation, graph analysis and search tasks are also
increasingly critical in many application domains. In particular,
understanding how a given set of graphs align with each other (also
known as the graph matching/alignment problem, Figure 1) forms
the core task in many search, classification, and analysis applications. Unfortunately, the problem of maximum common subgraph
isomorphism between two graphs is a well known NP-hard problem [24], making it impractical to search for exact or maximal
graph alignments. As a result, while there are some attempts to
improve the performance of exact maximum common subgraph
matching solutions [23], most of the recent efforts in the area have
focused on seeking approximate/inexact graph alignments [3, 18,
22, 19, 29].
While these algorithms differ in their specific techniques, most
of them rely on a four phase process:

INTRODUCTION

Graphs have been used to represent a large variety of complex
data, from multimedia objects, social networks, hypertext/Web,
knowledge graphs (RDF), mobility graphs,to protein interactions.
Let D be a set of entities of interest, a graph, G(V, E), defined over
V = D describes the relationships between pairs of objects in D.
The elements in the set V are referred to as the nodes or vertices of
the graph. The elements of the set E are referred to as the edges and
1
This work is supported by NSF Grants #1339835 and #1318788.
This work is also supported in part by NSF grant #0856090.

1. First, the matching algorithm analyzes and encodes the
global structural information (for example a spectral signature [23]) corresponding to the nodes of the graph.
2. Secondly, the algorithm analyzes and encodes the local structural information (such as neighborhood degree distribution [29]) for the nodes of the graph.
3. Once these global and local signatures are encoded, the
matching algorithm compares the signatures of pairs of
nodes across the given graphs to rank these pairs of nodes
(for example using a stable matching algorithm, like the Hungarian algorithm [16]) based on their overall structural similarities.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from Permissions@acm.org.
CIKMâ15, October 19â23, 2015, Melbourne, Australia.
c 2015 ACM. ISBN 978-1-4503-3794-6/15/10 ...$15.00.

DOI: http://dx.doi.org/10.1145/2806416.2806577.

1101

)&

)&
)&

)&

)&
)&

*&

)&
)&

!"#$%&'&

)&

)&
)&

)&

!"#$%&(&

Figure 2: Keynode selection problem for scalable graph matching: the nodes marked with "*" are keynodes of the input
graphs that can be used to reduce the amount of time needed to
search for alignments
4. Finally, a post-processing, or refinement, step (involving, for
example, a vertex cover operation) is used to improve the
accuracy of the initial matching [29].

2.

BACKGROUND AND RELATED WORK

In this section, we review key concepts related to the graph
matching problem and discuss the existing algorithms.
Graph Isomorphism: Given two graphs G and H, G is isomorphic to H if there exists a bijective mapping from the nodes of G
to the nodes H that preserves the edge structure [13]: for any two
vertices that are adjacent on G, the vertices they are mapped to are
also adjacent on H, and vice versa.
Subgraph Isomorphism: Subgraph isomorphism seeks a bijective
function, f , such that there is a subgraph G0 of G and a subgraph
H 0 of H, such that G0 is isomorphic to H 0 , with respect to f .
Maximum Common Subgraph Isomorphism: Maximum common subgraph isomorphism seeks the largest subgraph of G isomorphic to a subgraph of H [24]. Intuitively, the larger the maximum common subgraph of two graphs is, the more similar the
graphs are to each other.
One of the first exact graph matching algorithms was proposed
by Ullman [24]. An alternative way to search for a matching between two graphs is to rely on graph edit distance algorithms:
given two graphs the corresponding graph edit distance is the least
cost sequence of edit operations that transforms G1 into G2 . Commonly used graph edit operations include substitution, deletion, and
insertion of graph nodes and edges. Unfortunately, the graph edit
distance problem is also known to be NP-complete [24]. In fact,
even approximating graph-edit distance is very costly; the edit distance problem is known to be APX-hard [8]. [8] shows that graph
isomorphism, subgraph isomorphism, and maximum common subgraph problem are special instances of the graph edit distance computation problem. Many subgraph isomorphism search algorithms
have been developed, such as [15, 29, 14].
Approximate Graph Matching: In order to be applicable to large
graphs, many heuristic and approximate graph matching algorithms
have been proposed.
While, as we discussed above, graph matching through edit distance computation is an expensive task, there are various heuristics
that have been developed to perform this operation more efficiently.
GraphGrep [14] is one such technique, relying on a path-based
representation of graphs. GraphGrep takes an undirected, nodelabeled graph and for each node in the graph, it finds all paths that
start at this node and have length up to a given, small upper bound,
lp . Given a path in the graph, the corresponding id-path is the list
of the ids of the nodes on the path. The corresponding label-path is
the list of the labels of the nodes on the path. The fingerprint of the
graph, then, is a hash table, where each row contains the hash of the
label-path and the corresponding number of id-paths in the graph.
Irrelevant graphs are filtered out by comparing the numbers of idpaths for each matching hash key and by discarding those graphs
which have at least one value in its fingerprint less than the corresponding value in the fingerprint of the query. Matching sub-graphs
are found by focusing on the parts of the graph which correspond
to the label-paths in the query. After, the relevant id-path sets are

Unfortunately, many of these steps result in significant scalability
challenges in terms of the matching time needed to compare the
pairs of nodes:
â¢ In particular, the expensive refinement phase of graph matching algorithms is not practical in applications where scalability of the graph matching operation is critical.
â¢ Moreover, especially in very large graphs, it is also impractical to seek pairwise structural similarities for all node pairs
during the graph matching process.

1.1

Organization of this Paper

The paper is organized as follows: in the next section, we first
introduce basic concepts and review existing graph matching algorithms. In Section 3, we provide overviews of the general graph
matching process as well as the proposed keynode-driven scalable
graph matching (KSGM) algorithm. Then, in Section 4, we present
our structural keynode extraction (SKE) algorithm. In Sections 5
and 6, we discuss how to use these structural keynodes for obtaining graph alignments. We discuss the complexity of the proposed algorithms and parallelization opportunities in Section 7. We
present experimental evaluations with various real and synthetic
data sets in Section 8. These confirm that the proposed approximate graph matching algorithm is highly effective and efficient.
Finally, we conclude the paper in Section 9.

)&

)&
)&
)&

1.2

)&

Contributions of this Paper

Based on these observations, in this paper, we argue that a more
practical and scalable solution would be to seek structural keynodes of the input graphs that can be used to reduce the amount of
time needed to search for alignments (Figure 2). Of course, these
keynodes must be selected carefully to prevent any degradations in
accuracy during the alignment process, especially because, as mentioned above, refinement post-processes are detrimental to scalability of matching algorithms.
Given this motivation, in this paper, we first present a highly efficient and effective structural keynode extraction (SKE) algorithm.
The SKE algorithm, which is executed off-line, relies on a 3-step
process:
1. In the first step, a PageRank algorithm [7] is ran to associate
a structural score to each node in the graph.
2. In the second step, a scale-space (based on a difference-ofGaussians (DoG) function defined over different scales of the
graph) is constructed.
3. In the third step, keynode candidates are extracted by analyzing the resulting scale-space for extrema of the DoG function and a subset of these candidates are selected as structural
keynodes.
We then propose a graph matching algorithm that uses these structural keynodes (obtained during off-line processing) for keynodedriven scalable graph matching (KSGM). In particular, KSGM extracts only local signatures and relies on the structural keynodes for
fast node-to-node similarity searching. In addition, we also show
that this keynode-driven approach not only reduces the number of
comparisons that need to be performed online, but it also enables
effective matching, even without having to rely on an expensive assignment algorithm, like the Hungarian algorithm (with O(|V |3 )
complexity). Experiment results show that the proposed structural
keynode extraction and keynode-driven scalable graph matching
algorithms produce alignments that are as accurate as (or better
than) the state-of-the-art algorithms, while requiring significantly
less online execution time without refinement.

1102

Algorithm 1 Overview of keynodes based graph matching
Input:
A set G = {G1 , G2 , ...Gg } of graphs
A query graph Gq â G.
Output:
Rank Gi â G in terms of matching quality
Offline process:
1: for all Gi â G (including Gq ) do
2:
Perform structural keynode extraction (SKE) for Gi
3:
Extract local-signatures for all nodes in Gi
4:
(Optional) Extract global-signatures for all nodes in Gi
5: end for
Online process:
6: for all Gi â G do
7:
Compute local similarities for keynode pairs from Gi and
Gq .
8:
(Optional) Compute global similarities for keynode pairs
from Gi and Gq and combine these with local similarities.
9:
Select anchors to obtain a base matching
10:
Expand the base matching to obtain Mq,i
11:
Compute matching quality, quality(Mq,i )
12: end for
13: Rank Gi â G in terms of quality(Mq,i )

selected and overlapping id-paths are found and concatenated to
build matching sub-graphs.
A common method to obtain an approximate graph matching
is to use the eigenvectors derived from the adjacency matrix of
the graph [23]: intuitively, two similar graphs should have similar eigenvectors; moreover, if we construct a |V | Ã |V | matrix (for
example the Laplacian of the graph or a matrix encoding node distances) and decompose it into three matrices of |V | Ã c, c Ã c,
and c Ã |V | elements using an eigen-decomposition technique like
SVD, the c-length vector corresponding the node v â V can be
used as a global-signature corresponding to node v. Once node-tonode similarities are computed, an assignment is usually found using an assignment algorithm, such as the Hungarian algorithm [16],
which uses a primal-dual strategy to solve this problem in O(|V |3 )
time. This simple observation, led to several works leveraging different global-signatures for identifying node matches across different graphs [3, 18, 22, 19, 29]. [28] formulates the labeled weighted
graph matching problem in the form of a convex-concave program,
which searches for appropriate permutation matrices by solving a
least-square problem. In addition, feature selection techniques are
used for more accurate calculation [11, 12, 20]. In order to improve
matching accuracy, [29] proposes to enrich the global-signatures
associated to the graph nodes with local-signatures, encoding the
properties of the immediate neighborhood of each node.

3.

OVERVIEW OF KEYNODE-DRIVEN
GRAPH MATCHING

combines (by multiplying) the global and local similarities
of each pair of nodes into a single value, thereby quantifying
the overall similarity of the pair.
4. Once the overall similarities for |Vq | Ã |Vi | pairs of nodes
are computed, [29] drops node pairs with small degrees and,
then, expands the remaining set of anchor pairs by adding,
in an iterative manner, immediate good nearby pairs to this
anchor set.
5. When no more pairs can be added to the anchor set, [29] uses
the Hungarian algorithm to identify an initial node matching
in O(max{|Vq |, |Vi |}3 ) time.
6. Finally, as a post-processing step, [29] applies a vertex cover
based refinement, which explores different subsets of the
nodes and searches for better alignments than the one initially identified. In particular, the algorithm seeks small vertex covers, which are likely to give the mismatched nodes additional chances to be refined. Note that since the minimum
vertex cover problem is known to be NP-hard, the algorithm
searches for minimal vertex covers in O(mÃn3 ) time, where
m = min{|Eq |, |Ei |} and n = max{|Vq |, |Vi |}.

Given a set G = {G1 , G2 , ..., Gg } of graphs and a query graph
Gq â G, in this paper, we seek the maximum graph matching between Gq and all Gi â G (i 6= q). Note that the exact solution for
this problem is NP-hard [24]. Since we treat scalability as a key
constraint, we consider inexact solutions and rely on the matching quality measure proposed in [29] to evaluate the accuracies of
the resulting alignments: Let Gq (Vq , Eq ) be a query graph and let
Gi (Vi , Ei ) be a graph in G. Let Mq,i (Vq,i , Eq,i ) be a subgraph of
both Gq and Gi , returned by an inexact subgraph search algorithm.
[29] defines the matching quality function as follows:
quality(Mq,i ) =

|Eq,i |
,
min(|Eq |, |Ei |)

Intuitively, the quality function describes how similar the given
query graph Gq and known graph Gi are by using the ratio of
matched edges and the maximum number of edges that can be possibly matched, which is equal to the minimum number of edges between two graphs. In other words, the larger the number of edges
in the graph Mq,i , the better is the quality of the matching (or the
more similar the two graphs are).

3.1

This process includes a number of very expensive steps: The first
two steps, involving global and local analysis are expensive, but can
be performed off-line and indexed for later reuse assuming that the
graphs are available ahead of time. The last four steps, however,
need to be performed on-line, yet they consist of operations that
are quadratic or higher. In particular, the last refinement step, with
O(m Ã n3 ) time cost is impractical for most large data graphs.
In this paper, we note that Step 3 can be significantly sped up
if the similarity computations are limited to only a small subset of
the vertices in Vq and Vi (which we refer to as keynodes of Vq and
Vi ). However, the use of keynodes for node similarity computation
is not sufficient to reduce the overall complexity as, once the keynodes are identified and the keynode pairs set is expanded, solving
the assignment problem needed to return the matching would still
take O(max{|Vq |, |Vi |}3 ) time, if we were to apply the Hungarian
algorithm on the extracted keynodes. Therefore, we also need to reduce the time complexity of this step significantly. It is especially
important that the initial keynode based similarity computation is
accurate as we cannot afford a cubic algorithm like Hungarian algorithm to return a high-quality matching.

Challenges

Given a query graph Gq , our goal is to rank the graphs in G according to their matching similarities against Gq (and eventually
return the top few matches to the user). [29] solves this problem by
relying on a 6-step process, common to many graph search algorithms:
1. [29], first, analyzes the global structure of each graph
through eigen-decomposition of the graph Laplacian matrix
and encodes this in the form of a c-length vector associated
to each node in the graph.
2. Secondly, [29] encodes the structural information local to
each node, vj , in the form of an sj -length degree distribution vector, where sj is the number of nodes in the kneighborhood of the node.
3. Given the global and local signatures of all nodes in Gq and
Gi , [29] then computes the global and local similarities for
each pair of nodes from the two graphs, in O(|Vq | Ã |Vi | Ã c)
and O(|Vq | Ã |Vi | Ã maxvj (sj )) time, respectively. It then

1103

3.2

Outline of KSGM
Algorithm 1 illustrates an overview of the keynode-driven scalable graph matching (KSGM) process. In the rest of the paper, we
study each step in detail. First, in the next two sections, we focus on
the offline steps of KSGM, which involve identifying keynodes and
extracting local-signatures. The online steps of the KSGM algorithm
are discussed in Section 6.
4.

4.2

We note that the above alternative has a number of disadvantages:
â¢ First of all, many of the structural significance measures,
such as PageRank, are not entirely robust against modifications in the graph. The PageRank score of a node, for example, can jump significantly, if a new edge connects the
sub-graph in which the node is contained to a high PageRank node in the graph.
â¢ Secondly, common structural significance measures, like
PageRank, capture the significance of a node in the whole
graph and favor nodes that are overall central. However, this
may be disadvantageous as there is a possibility that smaller
scale, but distinct (and, therefore, useful for matching) structural features of the graph may be missed.
We therefore argue that we need a better alternative, which is both
robust and multi-scale. We build the proposed SKE algorithm based
on three key insights:
â¢ Robustness: Even when the PageRank scores of the nodes
themselves vary due to graph transformations, such as edge
insertions and removals, a given nodeâs PageRank score relative to the scores of the nodes in its neighborhood is likely
to be stable.
â¢ Structural distinctiveness: A node is structurally distinctive
in its neighborhood, if "the relationship between its PageRank score to the PageRank scores of its neighbors" is different from the "relationships between the nodeâs neighborsâ PageRank scores and the PageRank scores of their own
neighbors".
â¢ Multi-scale: Since we do not know the scale of the structurally distinctive features of the graph, we need to search
for features of potentially different sizes.
It is important to note that similar requirements also exist in
other application domains. For example, algorithms for extracting such robust, local features have been developed for 2D images
(SIFT [21]), uni-variate time series [9], and multi-variate time series [25]. In this paper, we argue that a similar process can be used
to identify keynodes (corresponding to robust, multi-scale structural features) of a graph, if the nodes are annotated with PageRank
scores ahead of the time. Let G(V, E, p) be a PageRank-labeled
graph, where p() is a mapping from the nodes to the corresponding PageRank scores. What makes the problem of extracting local features from PageRank-labeled graphs challenging is that the
concepts of neighborhood, gradient, and smoothing are not welldefined for graphs.
Therefore, before we describe the keynode extraction process,
we describe how to smooth a PageRank-labeled graph. Intuitively,
smoothing the graph with respect to the scores associated to the
graph nodes creates versions of the given graph at different resolutions and, thus, helps identify features with different amounts of
details.

STRUCTURAL KEYNODE EXTRACTION

In this section, we propose an off-line structural keynode extraction (SKE) algorithm which identifies Î% (where Î is a user
provided parameter) of the nodes in V as the keynode set, K, of
a given graph, G(V, E) to support scalable graph matching. The
proposed SKE algorithm has a number of advantages: (a) First of
all, the identified keynodes are robust against noise, such as random edge insertion/removal; and (b) the identified nodes represent
structural features of the graph of different sizes and complexities
(i.e., correspond to neighborhoods of different sizes).

4.1

Proposed Solution - Robust Keynode Extraction through Scale Space Analysis

Naive Solution - Selecting Structural
Keynodes based on Node Significance

As described above, the keynodes of the graph need to represent
the structural properties of the graph well (i.e., extracted keynodes
need to be structurally significant in the graph) to support effective
matching. Therefore, the first alternative is to rely on traditional
node significance measures.
Measures like betweenness [26] and the centrality/cohesion [5],
help quantify how significant any node is on a given graph based on
the underlying graph topology. The betweenness measure [26], for
example, quantifies the number of shortest paths that pass through
a given node. The centrality/cohesion [5] measures quantify how
close to a clique the given node and its neighbors are. Other authority, prestige, and prominence measures [4, 7, 5] quantify the
significance of the node in the graph through eigen-analysis or random walks, which help measure how reachable a node is in the
graph. PageRank [7] is one of the most widely-used random-walk
based methods for measuring node significance and has been used
in a variety of application domains, including web search, biology,
and social networks. The basic thesis of PageRank is that a node
is important if it is pointed to by other important nodes â it takes
into account the connectivity of nodes in the graph by defining the
score of the node vi â V as the amount of time spent on vi in a sufficiently long random walk on the graph. Given a graph G(V, E),
the PageRank scores are represented as ~r, where
~r = Î±TG~r + (1 â Î±)~t
where TG is a transition matrix corresponding to the graph G, ~t is
a teleportation vector (such that ~t[i] = |V1 | ), and Î± if the residual
probability (or equivalently, (1 â Î±) is the so-called teleportation
probability). Unless the graph is weighted, the transition matrix,
TG , is constructed such that for a node v with k (outgoing) neighbors, the transition probability from v to each of its (outgoing)
neighbors will be 1/k. If the graph is weighted, then the transition probabilities are adjusted in a way to account for the relative
weights of the edges.
Therefore, as the first alternative, we consider a PageRank based
keynode selection scheme: in this scheme, given a graph G(V, E),
we would (a) first identify the PageRank scores, p(vi ), of all vi â
V , then (b) we would rank the nodes in non-increasing order of
PageRank scores, and finally, (c) we would return the top Î% of
the nodes in V as the keynode set, K.

4.2.1

Gaussian Smoothing of a PageRank-Labeled
Graph

1D or 2D data are commonly smoothed by applying a convolution operation with a Gaussian window. For example, if Y =
ht0 , t1 , . . . , tl i is a time series data and Ï is a smoothing parameter,
its smoothed version, YÌ (t, Ï), is obtained through G(t, Ï) â Y (t)
where â is the convolution operation in t and G(t, Ï) is the Gaussian function
ât2
1
G(t, Ï) = â
e 2Ï2 .
2ÏÏ
Essentially, the Gaussian smoothing process takes a weighted
average of values of the points in the vicinity of a given point, t.

1104

The closer a point to t, the higher is the weight. Therefore, in order
to implement a similar Gaussian smoothing of the given graph, we
first need to define a distance function to measure how close different nodes are to each other. Common applicable definitions of
node distance include the hop distance (determined by the shortest
edge distance between the nodes on the given graph) or hitting distance [10]. In this paper, we use hop distance to measure how far
nodes are to each other:

!"#$%"&'()"*+$,(,-../0$1(23/0(4#$%"&'4!"((

6!"77(6.8(9$)$*(!"
!"#$%"&'()"*+$,(,-../0$1(23/0(4!75(

â¢ Nj , for j â¥ 0, is an n Ã n 0, 1-valued matrix, where for a
given node vi in the graph, G, the ith row in the matrix, Nj
is 1 only for nodes that have node distance exactly j from the
node vi , and

!#

!"""""#

D EFINITION 1 (N ODE D ISTANCE M ATRIX ). Let us be given
a graph G(V, E) with n nodes. The ordering among the nodes is
described through a set of node distance matrices, Nj , where

6!75(77(6.8(9$)$*(;!75<(

6:(77(6.8(9$)$*(:(

!"#$%"&'()"*+$,(,-../0$1(23/0(45(

65(77(6.8(9$)$*(5(
!"#$%"&'()"*+$,(,-../0$1(23/0(4-3&(

â¢ Nj , for j â¤ 0, is an n Ã n 0, 1-valued matrix, where for a
given node vi , the ith column in the matrix, N(j, G) = 1
only for nodes that have distance exactly j on the inverted
graph, where all edges are inverted.


Figure 3: Computing the Difference-of-Gaussian (DoG) series
of the PageRank values of a graph

Intuitively, the cell Nj [v1 , v2 ] = 1 if the node v2 is exactly j hops
from v1 . When j is positive the hop-distance is measured following
outgoing edges, whereas when j is negative, incoming edges are
followed. Given this, we construct multiple scales of the given
graph G by using a Gaussian graph smoothing function defined as
follows.

Intuitively, the smoothing function S applies Gaussian smoothing
on the X values (associated with the nodes, vi â V ) based on the
hop-distances between nodes and returns a vector
SG,Ï (X) = hxÌ(v1 ), xÌ(v2 ), . . . , xÌ(vn )i

D EFINITION 2 (G AUSSIAN G RAPH S MOOTHING F UNCTION ).
Let us be given a labeled graph G(V, E, x) and let
â¢ Ï be a smoothing parameter.
â¢ X = hx(v1 ), x(v2 ), ..., x(vn )i be a vector encoding the labels associated with the nodes, vi â V .
Then, if G is a directed graph, the non-normalized Gaussian graph

smoothing function, SG,Ï
() is defined as
n
X

G(j, Ï)Nj X
SG,Ï (X) = G(0, Ï)IX +

encoding the smoothed X values associated with the graph nodes.
Note that, since at a given hop distance there may be more than
one node, all the nodes at the same distance have the same degree
of contribution and the degree of contribution gets progressively
smaller as we get further away from the node for which the smoothing is performed.
Therefore, given a PageRank-labeled graph, G(V, E, p), and a
corresponding PageRank vector, P = hp(v1 ), p(v2 ), ..., p(vn )i,
encoding PageRank scores associated with the nodes, vi â V , the
vector
SG,Ï (P ) = hpÌ(v1 ), pÌ(v2 ), . . . , pÌ(vn )i,

j=1

+

n
X

encodes the Ï-smoothing of the PageRank-annotated graph,
G(V, E, p). We also say that SG,Ï (P ) encodes the PageRank
scores of G at scale Ï.
We next describe how to construct a scale-space for the given
graph through an iterative smoothing process leveraging the PageRank vector and the structure of the graph.

G(j, Ï)Nj X,

j=1

where G(0, Ï) is a Gaussian function with zero mean and Ï standard deviation. If, on the other hand, G is an undirected graph, then
the non-normalized Gaussian graph smoothing function is

SG,Ï
(X)

= G(0, Ï)IX +

n
X

4.2.2

Graph Scale-Space Construction

The first step in identifying robust graph features is to generate a
scale-space representing versions of the given graph with different
amounts of details. In particular, building on the observation that
features are often located where the differences between neighboring regions (also in different scales) are large, we seek structural
features of the given graph at the extrema of the scale space defined
by the difference-of-the-Gaussian (DoG) series. More specifically,
given
â¢ a PageRank-labeled graph, G(V, E, p),
â¢ the corresponding vector, P = hp(v1 ), p(v2 ), ..., p(vn )i encoding the scores associated with the nodes, vi â V ,
â¢ a minimum smoothing scale, Ïmin ,
â¢ a maximum smoothing scale, Ïmax ,
â¢ the number, l, of levels of the scale space,
then, we compute a difference-of-Gaussians (DoG) series,
D(G, P, Ïmin , Ïmax , l) = {D1 , D2 , ..., Dl }, where each Di encodes the differences of two nearby scales separated by a multiplicative factor k:

2G(j, Ï)Nj X.

j=1

Intuitively, S  applies Gaussian-based weighted averaging to the
entries of vector X based on the hop-distances2 . However, unlike
the basic Gaussian smoothing, during (non-normalized) relationship smoothing, there may be more than one node at the same distance and all such nodes have the same degree of contribution. As
a consequence, the sum of all contributions may exceed 1.0. Therefore, the normalized Gaussian graph smoothing function, S(G, Ï),
discounts weights based on the number of nodes at a given distance:

 



SG,Ï (X) = SG,Ï
X Ã· SG,Ï
1(n) ,
where 1(n) is an n-vector such that all values are 1 and âÃ·â is a
pairwise division operation.

2
In practice, since the Gaussian function drops fast as we move
away from the mean, we need to consider only a small window, w,
of hops

Di = SG,ki Ïmin (P ) â SG,kiâ1 Ïmin (P ),

1105

56'
-2'

-3'

â¢ DoG-neighbors numbered #2 and #7 correspond to the
DoG values of the same node at the previous and next levels of the scale space. Therefore, we have

!"#$%&'(),+'

-4'

Nhvi ,Ïj i [2] = Diâ1 [j] and Nhvi ,Ïj i [7] = Di+1 [j].

!"#$%&'()'

-0'

7)869'

-+'

-.'

-1'
!"#$%&'()*+'

â¢ In contrast, DoG-neighbors #3, #5, and #8 correspond to
the (average) DoG values of the forward neighbors of the
node vj , at the previous, current, and next levels of the scale
space, respectively. Therefore, we have

-/'

Nhvi ,Ïj i [3] = (FDiâ1 ) [j],

Figure 4: Extrema detection

Nhvi ,Ïj i [5] = (FDi ) [j], Nhvi ,Ïj i [8] = (FDi+1 ) [j],
where k =

q
l

Ïmax
.
Ïmin

Figure 3 visualizes the process:

where, F is a row-normalized adjacency matrix accumulating
the (averaged) contributions of the nodes to their neighbors
along the forward edges.

â¢ On the left hand side of the figure, we have the incrementally smoothed versions of the PageRank vector, P . Here,
the lowest level, SG,Ïmin (P ), corresponds to the most detailed version of the graph (with the least amount of smoothing), whereas SG,Ïmax (P ) corresponds to the least detailed
(most smoothed) version of the graph. In other words, Ïmin
determines the sizes of the smallest structural features we
can locate and Ïmax = kl Ïmin determines the sizes of the
largest structural features we can identify. In particular, since
under Gaussian smoothing, a diameter of 6Ï would cover
â¼ 99.73% of the weights, the diameter of the smallest structural feature that can be identified using SKE is â¼ 6Ïmin
hops, whereas the diameter of the largest feature would be
â¼ 6Ïmax hops.

â¢ Similarly, DoG-neighbors #1, #4, and #6 correspond to
the (average) DoG values of the backward neighbors at the
previous, current, and next levels of the scale space. Therefore, we have
Nhvi ,Ïj i [1] = (BDiâ1 ) [j],
Nhvi ,Ïj i [4] = (BDi ) [j], Nhvi ,Ïj i [6] = (BDi+1 ) [j],
where, B is a row-normalized backward-adjacency matrix
(where all edges are reversed) accumulating the (averaged)
contributions of the nodes to their neighbors along the backward direction of the edges.
Given these, the pair hvj , Ïi i is an extremum (i.e., vj is a keynode
candidate at scale Ïi ), iff Di [j] is a local maximum

The number of levels, l, denotes the number of detail levels
(or scales) we explore between Ïmin and Ïmax . Intuitively,
each of these levels corresponds to a different target size for
the structural features of the graph.

Di [j] â¥ M AX Nhvj ,Ïi i [h]
1â¤hâ¤8

â¢ On the right hand side of the figure, we have the resulting
Difference-of-Gaussian (DoG) series, consisting of vectors,
D1 through Dl .

or it is a local minimum
Di [j] â¤ M IN Nhvj ,Ïi i [h].
1â¤hâ¤8

Note that, intuitively, Di [j] measures how different the PageRank
values of the neighborhood around vj at scale Ïiâ1 (= kiâ1 Ïmin )
are from the PageRank values of the neighborhood around vj at
scale Ïi (= ki Ïmin ).
Therefore, a large Di [j] value would indicate a major structural
change when neighborhoods of different size around vj are considered (e.g., a node with a high PageRank score is included when
considering a neighborhood of larger scale). In contrast, a small
Di [j] indicates that there is minimal structural change when considering neighborhoods of different scales.

4.2.3

Intuitively, since Di [j] measures how different the PageRank values of the neighborhood around vj at scale Ïi are from the PageRank values of the neighborhood around vj at scale Ïiâ1 , a local
maximum corresponds to a highly scale-sensitive region (amidst
relatively scale-insensitive regions), whereas a local minimum corresponds to a scale-insentive region (amidst more scale-sensitive
regions), of the graph.

4.2.4

Selecting the Best Keynodes

In the final step, we need to rank the keynode candidates and
Î
return the top 100
Ã |V | of them, where Î is a user provided parameter, as the keynode set, K. We propose Extremum Ranking to
select the best keynodes.
Since keynodes are located at the local extrema of the DoG series, we can rank the keynode candidates based on their extremum
score defined as follows: Let the pair hvj , Ïi i be a local extremum.
The corresponding extremum score, Î¾(hvj , Ïi i), is defined as


ï£±
ï£´
if hvj , Ïi i is max.
ï£´ Di [j] â M AX Nhvj ,Ïi i [h]
ï£´
ï£²
1â¤hâ¤8


ï£´
ï£´
ï£´
ï£³ M IN Nhv ,Ï i [h] â Di [j] if hvj , Ïi i is min.
j i

Identifying Keynode Candidates

As we mentioned earlier, our intuition is that a graph node is
structurally distinctive in its neighborhood, if "the relationship between its PageRank score to the PageRank scores of its neighbors"
is different from the "relationships between the nodeâs neighborsâ
PageRank scores and the PageRank scores of their own neighbors,
at multiple scales". Therefore, to locate the keynode candidates,
we focus on the local extrema of the difference-of-Gaussian (DoG)
series D. More specifically, we identify hvj , Ïi i pairs where the
DoG value for node vj at scale, Ïi = ki Ïmin , is an extremium
(maximum and/or minimum) with respect to the neighbors of vj in
the same scale as well as neighbors in the previous and next levels
of the scale space.
In order to verify if the pair hvj , Ïi i is an extremium or not,
we compare Di [j] with the values corresponding to eight DoGneighbors in the scale-space, as visualized in Figure 4:

1â¤hâ¤8

Intuitively, the higher the extremum score is, the better local extremum (and, thus, a better keynode) is hvj , Ïi i.

1106

5.

LOCAL NODE SIGNATURES

nbhd_sim(vi , vj ) proposed by [29] accounts for the alignment between the degree distributions in these neighborhood graphs3 :

The next step in the process is to extract the local signatures (to
be used to compute local node similarities) for the nodes in the
graph. Note that this process is also offline.
While there are different local signatures proposed in the literature, in our work we build on the k-neighborhood degree distribution based local signature proposed in [29] (both because it is
simple and effective and also because this helps us compare our
keynode-driven approach to the approach proposed in [29] more
directly). Briefly, for each node vj â V and for a user provided k,
[29] first identifies the set, Nk (vj ) â V , of nodes that are at most
k hops from vj and extracts a subgraph, Gk (vj ) â G, induced by
vj and its k-hop neighbors. Then the degree sequence,

nbhd_sim(vi , vj ) =
where

dmin = min{degree(vi ), degree(vj )}
nmin = min{Î½i , Î½j }
P min â1
min{di,h , dj,h }
dmin + n
h=1
.
D(vi , vj ) =
2

Node Pair Ranking with Extended Similarity.

Îºj = [dj,1 , dj,2 , . . . , dj,|Nk (vj )| ]

While the local neighborhood similarity computation we use is
similar to the one proposed in [29], we rank pairs of nodes differently. Let vi and vj be two nodes (from two different graphs). In
particular, [29] ranks the pair hvi , vj i of nodes based on their neighborhood similarities, nbhd_sim(vi , vj ). We, however, argue that
neighborhood similarity is not sufficient for accounting for how effective the node pair is in supporting expansion. More specifically,
we observe that a pair, hvi , vj i, is likely to be a better anchor for
expansion than the pair hva , vb i if not only (a) the neighborhoods
of vi and vj are more similar to each other than the pair, va and
vb , but also (b) if vi and vj have degrees that are more aligned with
each other than va and vb . Based on this observation, instead of applying a degree threshold, we propose that the pair hvi , vj i should
be ranked based on the ranking function

consisting of the degrees of nodes in Gk (vj ) (excluding vj ), sorted
in non-increasing order, along with the degree of the node vj and
the numbers of vertices and edges in its k-hop neighborhood, form
the local signature of node vj :
local_signature(vj ) = hÎºj , degree(vj ), Î½j , Îµj i,
where Î½j = |Nk (vj ) âª {vj }| is the number of nodes in Gk (vj ) and
Îµj = |Ek (vj )| is the number of edges.
Note that, while we use a local signature similar to that proposed
in [29], we extend the node pair ranking function to better account
for the node degrees as discussed later in Section 6.1.1. As we see
in Section 8, this extension provides a significant boost in accuracy.

6.

(KEYNODE-BASED) GRAPH MATCHING

Ï(vi , vj ) = nbhd_sim(vi , vj )Ã

As discussed in Section 3 and outlined in Algorithm 1, once the
keynodes are extracted and local signatures are computed offline,
the next steps of the algorithm are to

6.1.2

We now describe how these steps are implemented in the keynodedriven scalable graph matching (KSGM) algorithm.

Keynode pair Selection

[29] uses the Hungarian algorithm to identify an initial node
matching in O(n3 ) time, where n = max{|V1 |, |V2 |}. To reduce
the execution time, [29] prunes those node pairs for which the similarity is â¤ 0.5. Since, instead of considering the node pairs in
V1 Ã V2 , we only need to consider pairs of nodes in K1 Ã K2 , and
since |K1 |  |V1 | and |K2 |  |V2 |, keynode-driven processing
is likely to be faster even without using the threshold. However,
the cubic time of the Hungarian algorithm is still prohibitive and
impractical for scalable graph matching. Therefore, we propose a
greedy anchor selection algorithm, which (as we see in Section 8)
performs very well when used along with keynodes selected in Section 4 and the proposed ranking function, Ï(). In particular, we first
include all keynode pairs in K1 Ã K2 into a queue in the order of
their ranks based on Ï(), then, until the queue is empty, we remove
and consider the keynode pair, hv, ui at the head of the queue. If
neither v nor u has been marked anchored, we include hv, ui as
an anchor and we mark v and u as anchored, otherwise, we drop
the pair, hv, ui.
Note that this process has O((|K1 | Ã |K2 |) Ã log(|K1 | Ã |K2 |))
time cost (instead of the cubic cost of the Hungarian algorithm) and,

Anchor Set Selection

Let G1 (V1 , E1 ) and G2 (V2 , E2 ) be two graphs and let K1 â V1
and K2 â V2 be the corresponding keynodes identified by the SKE
algorithm proposed in Section 4. The next step is to select a subset,
A, of the pairs of nodes in K1 Ã K2 as the anchor set of alignments
based on a ranking function (a) evaluating how structurally similar
a pair of nodes are and (b) how likely they are to lead to an effective
expansion process to discover other alignments.

6.1.1

min{degree(vi ), degree(vj )}
.
max{degree(vi ), degree(vj )}

Note that [29] simply drops node pairs where the minumum of the
two node degrees is smaller than the larger average degree of the
two input graphs. We, however, argue that such node pairs may
be useful, especially if the degrees in the graph are not uniformly
distributed and the maximum matching occurs at the sparse portions of the graph. Therefore, we keep such pairs as long as they
rank highly based on Ï(). We evaluate this ranking function in Section 8.

â¢ compare the signatures of pairs of nodes across the given
graphs to rank these pairs of nodes,
â¢ select a set of pairs of keynodes (we refer it as anchor set)
that serve as the base matching, and
â¢ expand this base matching to obtain Mq,i .

6.1

nmin + D(vi , vj )
,
(Î½i + Îµi )(Î½j + Îµj )

Node Similarity Matching and Node Pair
Ranking

As we discussed in Section 5, KSGM uses a local node signature similar to the one proposed by [29]: hÎºj , degree(vj ), Î½j , Îµj i,
where Î½j is the number of nodes in the neighborhood of
vj , Îµj is the number of neighborhood edges, and Îºj =
[dj,1 , dj,2 , . . . , dj,|Nk (vj )| ] consists of the degrees of nodes in the
k-neighborhood of vj (excluding vj ), sorted in non-increasing order.

3
In addition to using local similarities, [29] also extracts global
signatures along with the local-signatures to compute node similarities. As we see in Section 8, the proposed keynode-driven graph
matching algorithm achieves good results without having to rely on
such global-signatures.

Local Neighborhood Similarity.
Let vi and vj be two nodes (from two different graphs)
and let Gk (vi ) and G0k (vj ) be the corresponding induced
kâneighborhood graphs.
Then, local similarity function

1107

as we see in Section 8, performs very well in practice. Furthermore,
the nature of Hungarian algorithm, which forces to pair all possible
nodes to produce the optimal bipartite matching for the given two
sets of nodes, is not guaranteed to provide a better matching in this
case. Since the extracted keynodes are not all necessarily perfectly
paired with each other, some keynodes can be a unique feature of
the given graph, which does not align with other graphs, by forcing
them to pair with other keynodes, it in fact introduces a bad initial
base matching, and thus expand into an even worse matching. The
proposed greedy matching algorithm, however, only consider the
highly aligned keynodes, which in practice provides better results
than the optimal bipartite matching.

6.2

more efficient randomized algorithms exist [27]. Once the node
distances have been computed, we construct the scale-space in
O(l Ã |V | Ã max_w_nbhd_size), as for each of the l scales, the
score of each node needs to be smoothed considering the scores
of the vertices in its w-hop neighborhood (w is the Gaussian window size and max_w_nbhd_size is the size of the largest w-hop
neighborhood in G).
Once the scale-space is constructed, next, we identify the keynode candidates. This involves O(l Ã |V |) time, because for each
of the l scales, each node needs to be compared with a constant
number (8) of DoG-neighbors in the scale-space.
Finally, we rank the keynode candidates to select the top K =
Î
V many as the keynodes to bootstrap the online matching pro100
cess. Let there be C many keynode candidates. Computing the
ranking scores for these takes O(C) time, because each keynode
candidate needs to be compared with a constant number of DoGneighbors and obtaining the top K takes O(C Ã log(K)) time.

Matching List Expansion

Because keynodes are inherently sparsely localized, the anchor
set, A, is not necessarily a good final matching for graphs G1 and
G2 . We therefore need to expand this anchor list. Here, we follow [29]âs recommendation and expand the list incrementally by
considering the neighbors (and their neighbors) until no effective
expansion is possible (but we use the ranking function Ï() instead
of the node similarity function):

7.1.2

1. we first include all node pairs in A into a ranked queue (i.e.,
max-heap) in the order of their ranks based on the ranking
function, Ï(),
2. then, for each node pair hv, ui â A, we also include the node
pairs in neighbors(u) Ã neighbors(v) in the same ranked
queue

7.2

Online Time Complexity

Let G1 (V1 , E1 ) and G2 (V2 , E2 ) be two graphs. The online process includes the following operations.

3. then, until the ranked queue is empty, we remove and consider the node pair, hv, ui at the head of the ranked queue

7.2.1

Local Similarity Computation for Keynodes

This process has O(|K1 | Ã |K2 | Ã compare_length) complexity, where
compare_length = min{max_k_nbhd_size1 , max_k_nbhd_size2 })
since signatures (of length are compared for each pair of nodes in
the keynode sets K1 and K2 .

(a) if either v or u has not yet been marked matched, then
i. we include the pair, hv, ui, in the expanded matching list, L,
ii. we mark both v and u as matched, and
iii. then, the pairs in neighbors(u) Ã neighbors(v)
are included in the ranked queue
(b) otherwise, we drop the pair, hv, ui

7.2.2

Anchor Set Selection

This greedy process has O((|K1 | Ã |K2 |) Ã log(|K1 | Ã |K2 |))
time cost as each pair off nodes among the keynode sets need to be
considered only once in ranked order.

Once the anchor list is expanded, [29] relies on a post-process,
with time complexity, O(m Ã n3 ), where m = min{|E1 |, |E2 |}
and n = max{|V1 |, |V2 |}. This step is not scalable due to its prohibitive time complexity. Therefore, the proposed keynode-driven
scalable graph matching (KSGM) algorithm omits this refinement
post-process, due to its high time complexity4 . Instead, the set,
L, of node pairs remaining after the expansion process is directly
returned as the aligned nodes of the matching, M1,2 , for the input
graphs, G1 and G2 .

7.2.3

Anchor Set Expansion

This has O((|V1 | Ã |V2 |) Ã log(|V1 | Ã |V2 |)) worst case time
cost, as in the worst case, all pairs of vertices across the two graphs
may need to be considered for expansion in ranked order.

8.

EXPERIMENTS

In this section, we present experimental evaluations of the proposed keynode-driven scalable graph matching (KSGM) algorithm.
In particular, we compare KSGM to the graph matching algorithm
presented in [29] in terms of efficiency and accuracy.

7. TIME COMPLEXITY ANALYSIS
7.1 Offline Time Complexity

8.1

Let G(V, E) be a graph to be indexed in the database.

7.1.1

Local Signature Extraction

Since the local signature extraction process needs to extract the
k-hop neighborhoods around the nodes, the complexity of this step
is O(|V | Ã max_k_nbhd_size), where max_k_nbhd_size is the
size of the largest k-hop neighborhood in G. Note that this step
can also leverage the node distance matrix constructed during the
offline keynode extraction process.

8.1.1

Structural Keynode Extraction

Data Sets
Facebook Data Graph

The first data set we used is the Facebook social circles data
graph obtained from the Stanford Large Network Dataset Collection [2]. This is a connected graph with 4039 nodes and 88234
edges. The graph has a diameter of 8 and a 90-percentile effective
diameter of 4.7. For the experiments, we constructed 10 subgraphs
by uniformly sampling connected subsets, containing 60 â 70% of
the original graph nodes. Once the subgraphs are obtained, each
of the subgraphs is used as a query against the rest. We report the
averages of execution time and accuracy.

The first step in structural keynode extraction is to obtain the
PageRank scores for the nodes of the two graphs. While, this is an
expensive operation (involving a matrix inversion with O(|V |2.373 )
complexity for a graph with |V | nodes), there are many efficient,
approximate implementations of PageRank, including sublinear approximations [6].
The second step is the creation of an l-layer scale-space for
G. To construct the scale space, we first construct a node distance matrix, which requires an all-pairs shortest path computation, with complexity O(|V |3 ) for a graph with |V | nodes, but

8.1.2

4

Synthetic Graph Data Sets

In addition to the Facebook graph, we also used synthetic graphs,
where we controlled the topology, size, and node degree to explore

Though, in cases where scalability is not critical, this refinement
can be implemented without any change in the rest of the algorithm.

1108

Table 1: Synthetic graph topologies and configurations
Graph topology
Erdos-Renyi (ER)
Power law (PL)

Number of nodes
5000, 7500 (plus 1 to 10%)
5000, 7500 (plus 1 to 10%)

the advantages and disadvantages of the algorithms under different
scenarios.
We generated the synthetic graphs using the well known random
graph generating tool, NetworkX [1]. We consider two common
graph topologies: the Erdos-Renyi (ER) model and the power law
topology (PL) under the Barabasi-Albert model. Table 1 lists the
number of nodes and average degree settings that we used for assessing our algorithms. For each configuration, we generated 10
graphs. Note that, in addition to the base sizes (of 5000 and 7500),
we randomly created an additional 1 to 10% more nodes to ensure
that the different graphs in the data set have slightly different numbers of nodes. As before, once the 10 graphs are obtained for each
configuration, each of the subgraphs is used as query against the
rest. We report the averages of execution time and accuracy.

8.2

Table 2: Experiment results for the Facebook Graph (default
parameters)

Average degree
4, 8, 16
4, 8, 16

KSGM
6.4
38.0%

PR
7.25
34.12%

Random
6.0
15.4%

Extraction time (offline, sec.)

11.5

110.4

0.39

-

Matching time (online, sec.)
Accuracy

2%
6.5
37.6%

3%
6.4
38.0%

4%
6.3
38.7%

6%
6.4
36.4%

8%
7.2
33.2%

Table 4: Impact of the node-pair ranking function, Ï(), for the
Facebook Graph
Matching time (online sec.)
Accuracy

Ï()
6.4
38.0%

w/o Degree
6.0
31.8%

with Global
4.0
22.9%

Table 5: Impact of the local-signature neighborhood size, k, for
the Facebook Graph

Evaluation Criteria

Matching time (online, sec.)
Accuracy

2 hops (default)
6.4
38.0%

3 hops
5.5
33.9%

4 hops
4.3
23.3%

processing, its online matching time is 3Ã faster than that of [29].
Moreover, the matching accuracy of KSGM is 1.2Ã better than that
of the competitor, through it does not use global signatures, nor it
relies on the optimal Hungarian algorithm for anchor selection.
The table also lists the performance of KSGM when using top
PageRank (PR) scored nodes instead of those returned by the SKE
algorithm. As we see here, while the offline process is faster when
using PageRank scoring nodes, the runtime performance (both in
terms of execution time and accuracy) is worse when using SKE
keynodes. In addition, to see whether it is possible to achieve a
competitive accuracy if we were to select a similar percentage of
node randomly, in Table 2, we also include results where random
keynodes are used in the matching online phase. As we can see, the
accuracy drops significantly when we use random keynodes instead
of using robust structural keynodes extracted by the proposed SKE
algorithm6 . These indicate that SKE is indeed effective in extracting
structurally distinct and useful keynodes.

Execution Time

We report both offline and online execution times. As shown in
Algorithm 1 in Section 3, for KSGM, the offline execution includes
structural keynode and local-signature extraction steps. Online execution includes similarity computation, anchor selection, and expansion steps. [29] does not perform structural keynode extraction;
instead, offline execution includes eigen-decomposition for global
signatures.
For both KSGM and [29], we omit the refinement step as its complexity is prohibitive for scalable graph matching. For instance, for
the Facebook graph for which KSGM takes â¼ 6 seconds for online
processing for a pair of graphs, refinement takes â¼ 30 minutes â
i.e., it causes a â¼ 260Ã slowdown5 .

8.3

[29]
19.2
35.4%

Table 3: Impact of the keynode percentage, Î, for the Facebook
Graph

All experiments were conducted using a 4-core Intel Core i52400, 3.10GHz, machine with 8GB memory, running 64-bit Windows 7 Enterprise. The codes were executed using Matlab 2013b
and Visual Studio 2012. To evaluate accuracy, we use the matching
quality defined in Section 3.

8.2.1

Matching time (online, sec.)
Accuracy

Experiment Parameters

The default parameters for the structural keynode extraction
(SKE) algorithm are as follows:

8.4.2

Impact of the Keynode Percentage

Table 3 studies the impact of the percentage, Î, of the nodes used
as keynodes. As we see, up to a point, the more keynodes we use,
the more accurate and faster the matching becomes. Beyond that
point, however, additional keynodes become disadvantageous. This
indicates that top keynodes are the most effective in serving as good
starting points and, as expected, below a certain rank they loose
distinctiveness, resulting in increased cost and loss in accuracy.

â¢ PageRank teleportation probability (1âÎ±) = 0.15 (as is commonly assumed),
â¢ least smoothing factor (Ïmin ) = 0.275, corresponding to â¼
2-hop neighborhoods,
â¢ maximum smoothing factor (Ïmax ) = 0.777, corresponding
to â¼ 5-hop neighborhoods, and
â¢ number (l) of smoothing levels = 6.

8.4.3

In addition, for KSGM, the default percentage (Î) of keynodes selected from the graph was set to 3%. Also, for all algorithms, local
signatures were extracted from 2-hop neighborhoods (i.e., k = 2),
as recommended by the authors of [29].

Impact of the Node-Pair Ranking Function

Table 2 lists the online and offline processing times and accuracy for the Facebook graph under the default parameter settings.
As we see here, while KSGM spends more time in one-time, offline

Table 4 studies the impact of the node-pair ranking function,
Ï(). In particular, we compare the performance of the ranking
function proposed in Section 6.1.1, to the ranking function without
degree extension and ranking function including additional globalsignature similarity as proposed in [29]. As we see here, the proposed node-pair ranking function provides the best expansion opportunities (and thus provides the highest accuracy, with slight expansion time overhead). Also, the "with Global" optional provides
a much worse matching. Thus, while the algorithm allows, we encourage the users not to use "with Global" option.

5
For this data configuration, when using expensive refinement postprocessing, KSGM and [29]âs accuracies are 0.72 and 0.696, respectively.

6
The slight time gain when using random keynodes is due to the
fact that random keynodes are not good starting points for expansion and, thus, the expansion process ends earlier.

8.4
8.4.1

Results for the Facebook Graph
Default Configuration

1109

gorithm works faster than the state-of-the-art algorithms without
refinement, yet produces alignments that are as good or better.

Table 6: Experiment results for the synthetic data sets (avg.
degree=4, varying models and number of nodes)
KSGM Online time (sec.)
[29] Online time (sec.)
KSGM Accuracy
[29] Accuracy

PL(5000)
6.9
99.3
45.3%
42.9%

PL(7500)
13.9
223.7
45.1%
42.8%

ER(5000)
6.5
746.7
45.7%
46.3%

ER(7500)
14.5
745.1
51.2%
53.0%

KSGM Offline time (sec.)
[29] Offline time (sec.)

130.0
23.8

284.7
82.8

134.8
24.9

270.8
84.5

Acknowledgments
We thank the authors of [29] for sharing their source code and data.

10.

Table 7: Impact of the average node degree (number of
nodes=5000, power law model)
KSGM Online time (sec.)
[29] Online time (sec.)
KSGM Accuracy
[29] Accuracy

degree=4
6.9
99.3
45.3%
42.9%

degree=8
7.5
116.7
25.2%
25.1%

degree=16
9.3
141.2
13.9%
14.1%

KSGM Offline time (sec.)
[29] Offline time (sec.)

130.0
23.8

199.1
27.9

396.7
53.2

8.4.4

Impact of the Neighborhood Size, k, for LocalSignatures

Table 5 studies the impact of the neighborhood size, k, for localsignature extraction. As we see in the table, the highest accuracy is
at 2 hops7 , increasing the neighborhood size negatively affects the
accuracy, indicating that unless locally meaningful signatures are
used, the resulting node-pair ranking is not effective for expansion.
This shows the keynode matching process is more accurate when
keynodes are easy to localize and this requires them to be distinct
and locally representative. Large neighborhoods potentially violate
both. Note that this is in line with the observation in Table 4.

8.5

Results for the Synthetic Data Sets

In this subsection, we consider the impacts of graph topology,
size, and node degree using ER and PL topologies. We omit discussions of the impacts of the other parameters, as they mirror those
presented in Tables 3 through 5.

8.5.1

Default Configurations

Table 6 lists the performances of KSGM and [29] for synthetic
graphs for different topologies and numbers of nodes under the default parameter settings. As we see here, the online execution time
of KSGM is significantly (10Ã to 115Ã) faster than that of [29], especially for the ER topology. Moreover, on both Erdos-Renyi (ER)
and power law (PL) topologies, the accuracy is highly competitive, with KSGM providing non-negligible accuracy gains for the PL
model (where it is relatively easier to identify effective keynodes).

8.5.2

Impact of Average Node Degree

Table 6 studies the impact of average node degree on matching
accuracies for the power law graph. As we see in the table, both
algorithms see a drop in the matching accuracy with larger node
degrees. However, KSGM stays competitive in terms of accuracy,
whereas it provides more gains in terms of online execution time.

9.

REFERENCES

[1] http://networkx.github.io/
[2] http://snap.stanford.edu/index.html
[3] X. Bai, H. Yu, and E. R. Hancock. Graph matching using
spectrament. ICPR 2004.
[4] A. Balmin, et al. ObjectRank: Authority-based keyword search in
databases. VLDB, 2004.
[5] M.G. Borgatti, et al. Network measures of social capital.
Connections 21(2):27-36, 1998.
[6] C. Borgs, M. Brautbar, J. T. Chayes, S.-H. Teng. Multiscale
Matrix Sampling and Sublinear-Time PageRank Computation.
Internet Mathematics 10(1-2): 20-48, 2014.
[7] S. Brin, et al. The anatomy of a large-scale hypertextual Web
search engine. Computer Networks and ISDN Systems 30:
107-117, 1998.
[8] H. Bunke. Error correcting graph matching: On the influence of
the underlying cost function. IEEE TPAMI, 21(9):917â922, 1999.
[9] K. S. Candan, R. Rossini, M. L. Sapino, X. Wang. sDTW:
Computing DTW Distances using Locally Relevant Constraints
based on Salient Feature Alignments. PVLDB, 1519-1530, 2012.
[10] M. Chen, J. Liu, and X. Tang. Clustering via random walk hitting
time on directed graphs. AAAI 2008.
[11] Xilun Chen, K. Selcuk Candan. LWI-SVD: Low-rank,
Windowed, Incremental Singular Value Decompositions on
Time-Evolving Data Sets. KDD 2014.
[12] Xilun Chen, K. Selcuk Candan. GI-NMF: Group Incremental
Non-Negative Matrix Factorization on Data Streams. CIKM 2014.
[13] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.
Introduction to Algorithms. 2001.
[14] R. Giugno and D. Shasha. Graphgrep: A fast and universal
method for querying graphs. ICPR, pp. 112-115, 2002.
[15] W. S. Han, J. Lee, and J. H. Lee. TurboISO: Towards ultrafast and
robust subgraph isomorphism search in large graph databases.
SIGMOD 2013
[16] R. Jonker and T. Volgenant.Improving the Hungarian assignment
algorithm. Oper. Res. 171-175. 1986.
[17] G. Karypis and V. Kumar "A fast and high quality multilevel
scheme for partitioning irregular graphs". SIAM Journal on
Scientific Computing 20 (1), 1999.
[18] D. Knossow, A. Sharma, D. Mateus, and R. Horaud. Inexact
matching of large and sparse graphs using laplacian eigenvectors.
GbRPR, 2009.
[19] W.-J. Lee and R. P. W. Duin. An inexact graph comparison
approach in joint eigenspace. In SSPR/SPR, 35-44, 2008.
[20] Jundong Li, Xia Hu, Jiliang Tang, Huan Liu. Unsupervised
Streaming Feature Selection in Social Media. CIKM 2015
[21] D. G. Lowe. Distinctive Image Features from Scale-Invariant
Keypoints. Int. Journal of Computer Vision, 60, 2, 2004.
[22] K. Riesen, X. Jiang, and H. Bunke. Exact and inexact graph
matching: Methodology and applications. Managing and Mining
Graph Data, pages 217-247, 2010.
[23] S. Umeyama. An eigen decomposition approach to weighted
graph matching problems. IEEE TPAMI, 10(5):695-703, 1988.
[24] J. R. Ullman. An algorithm for subgraph isomorphism, JACM
Vol. 23, No. 1, pp. 31-42. 1976
[25] X. Wang, K. S. Candan, M. L. Sapino: Leveraging metadata for
identifying local, robust multi-variate temporal (RMT) features.
ICDE, 2014.
[26] White D.R., et al. Betweenness centrality measures for directed
graphs. Social Networks, 16, 335-346,1994.
[27] R. Williams. Faster all-pairs shortest paths via circuit complexity.
STOC, 664-673. 2014.
[28] M. Zaslavskiy, F. R. Bach, and J.-P. Vert. A path following
algorithm for the graph matching problem. IEEE Trans. Pattern
Anal. Mach. Intell., 31(12):2227-2242, 2009.
[29] Y. Zhu, L. Qin, J. X. Yu, et al. High Efficiency and Quality: Large
Graphs Matching. CIKM, pp. 1755-1764. 2011.

CONCLUSIONS

Noticing that existing solutions to the graph matching problem
face major scalability challenges, we argue that it is impractical to
seek alignment among all pairs of nodes. Given these observations,
in this paper, we first presented an offline structural keynode extraction (SKE) algorithm and then discussed how to use these structural keynodes in a novel keynode-driven scalable graph matching
(KSGM) algorithm. Keynodes are selected carefully especially because a post refinement step is not feasible due to scalability requirements. Experiment results show that the proposed KSGM al7
Coincidentally, this also is the scale at which the SKE algorithm
located an overwhelming majority of the keynodes for this graph.

1110

BibTeX:

Combining a Gauss-Markov model and Gaussian process for traffic prediction in Dublin city center
François Schnitzler
Technion Fishbach Building 32000 Haifa, Israel

Thomas Liebig
TU Dortmund University Artificial Intelligence Group Dortmund, Germany

francois@ee.technion.ac.il Shie Mannor
Technion Fishbach Building 32000 Haifa, Israel

thomas.liebig@tudortmund.de Katharina Morik
TU Dortmund University Artificial Intelligence Group Dortmund, Germany

shie@ee.technion.ac.il ABSTRACT
We consider a city where induction-based vehicle count sensors are installed at some, but not all street junctions. Each sensor regularly outputs a count and a saturation value. We first use a discrete time Gauss-Markov model based on historical data to predict the evolution of these saturation values, and then a Gaussian Process derived from the street graph to extend these predictions to all junctions. We construct this model based on real data collected in Dublin city.

katharina.morik@tudortmund.de
observed measurement, at current time predicted measurement for a future time step predicted value for an unobserved junction GaussMarkov Section 2 current time Gaussian Process Section 3 future time steps Figure 1: Future measurements are estimated by a Gauss-Markov process (Section 2). Estimates for junctions without sensors, are provided by a Gaussian Process (Section 3).

Categories and Subject Descriptors
G.3 [Probability and Statistics]: Markov processes, multivariate statistics, stochastic processes, time series analysis; I.2.6 [Artificial Intelligence]: Learning--parameter learning ; J.7 [Computer in Other Systems]: Real time

Keywords
traffic prediction, Gaussian Process, Gauss-Markov, autoregressive, smart cities, time series, spatio-temporal

1.

INTRODUCTION

In the Greater Dublin Area, 750 (4%) junctions are covered by one or several SCATS (Sydney Co-ordinated Adaptive Traffic System) vehicle count sensors. Our goal is to provide estimates of the saturation at each junction, for the current and future times, whereas our previous work [1] only did so for each junction at the current time. High traffic saturation (cars/km) co-occurs with low traffic flux (cars/hour) and is an indicator for congestions [3].

Our work can be used for online signaling and trip planning. The urban street network is a graph (V, E ), where the vertices V are the junctions and the edges E the street segments. Let u be the set of unobserved junctions, with no SCATS sensor, and -u = V \u the junctions with sensors. The saturation of a junction vi at a time t is a continuous random variable yi,t . Furthermore, yu,t  {yi,t }i:vi u . We combine two components to obtain an estimate of the saturation of all junctions at future time steps, yV,t+t , conditioned on the current observations, y-u,t (t  N0 ). The first one, P (y-u,t+t |y-u,t ), models historical measurements. It can estimate future measurements y ^-u,t+t , based on the current observations y ^-u,t : y ^-u,t+t = E (y-u,t+t |y ^-u,t ) . (1)

(c) 2014, Copyright is with the authors. Published in the Workshop Proceedings of the EDBT/ICDT 2014 Joint Conference (March 28, 2014, Athens, Greece) on CEUR-WS.org (ISSN 1613-0073). Distribution of this paper is permitted under the terms of the Creative Commons license CCby-nc-nd 4.0.

The second is a Gaussian Process (GP) based on the street network and defining a multivariate Gaussian distribution P (yV,t ) over the saturations at all junctions. Conditioning this distribution on y-u provides P (yu,t+t |y-u,t+t ) and allows to estimate saturations at junctions without sensors: P (yu,t+t |y-u,t )  P (yu,t+t |y ^-u,t+t ) . Figure 1 illustrates the resulting prediction procedure. (2)

2.

GAUSS-MARKOV

A linear dynamical system models the evolution of a set of state variables y  Rp , where we omit the subscript -u: yt+1 = At yt + wt ¯ t , wt ) . wt  N (w (3) (4)

x1  N (¯ y0 , 0 ), a multivariate Gaussian distribution of ¯ 0 and covariance matrix 0 . The Kalman filter can mean y ^ t+t ) recursively. compute P (yt+t |yt ) = N (y ^t+t ,  Sensor measurements were collected from 2013-01-01 to 2013-05-141 by 512 (470 non trivial ones) vehicle count sensors located in central Dublin. We average all measurements received on non-overlapping 4 minutes intervals, because of missing values, and model the resulting averages from 5am ¯ t , wt change for every time to 12am. The parameters At , w ¯ 0 and 0 . step but are identical for every day. So are y Following the methodology of [6], each matrix At is learned using (averaged) measurements for t  {t - t , . . . , t + t }, weighted by a Gaussian kernel: exp(-(t - t )2 /t ). We arbitrarily use t = 3. For each matrix At , each row ri,t is estimated using an elastic net [7] and ten-fold cross-validation. 0 and each wt are diagonal covariance matrices estimated by maximum likelihood. Alternatively, penalized estimation algorithms such as the graphical lasso [2] could be used.

A similar approach was proposed to provide dynamic cost predictions for a trip planner in the same workshop [4]. Instead of a linear dynamical system (LDS), a spatio-temporal Markov random field (STMRF) is used. It models discretized saturation values only, and inference is approximated by belief propagation whereas it is computationally tractable and performed exactly in LDS. Our model also has a finer temporal resolution. Therefore, it can be used for signaling or online adaptation of the route in addition to offline trip planning. Comparing these two models in terms of precision and speed would be interesting. The Gauss Markov model assumes the dynamics are linear, first-order Markov and perturbed by Gaussian noise. More refined models could be considered and might lead to better estimations.In particular, we could assume the measurements are noisy observations of a hidden process. Other information could also be leveraged. For example, the street network could be used to derive a prior on the coefficient of the transition matrix, influencing the model only. Irregular, pointwise traffic estimation (for example based on mobile phones or GPS) could be integrated into the Gaussian Process to produce finer saturation estimates. Finally, different dynamics could be estimated and used in the presence or the absence of rain, modifying both the model and the estimation process.

3.

GAUSSIAN PROCESS

5.

ACKNOWLEDGMENTS

P (yu,t+t |y-u,t+t ) is derived from a GP regression framework modeling traffic saturation values of all junctions at a given time, similar to [5]. Multiple sensors at a junction are averaged. For each vertex vi , we introduce a latent variable fi , the true traffic saturation at vi : yi = f i +
i i 2

This work was supported by the European FP7 project INSIGHT under grant 318225.

6.

REFERENCES

(5) (6)

 N (0,  ) .

We assume that the random vector of all latent variables follows a GP: any finite set f = {fi }i=1,...,M has a multivariate Gaussian distribution. Therefore, the vector of observed traffic saturations (y-u ) and unobserved traffic saturations (du ) follows a Gaussian distribution y-u N du 0, K-u,-u +  2 I Ku,-u K-u,u Ku,u , (7)

where I is an identity matrix, K the so-called kernel and Ku,-u , K-u,-u , Ku,u , and K-u,u the corresponding entries of K . Conditioning on y produces P (yu,t+t |y-u,t+t ). We use the common regularized Laplacian kernel function K =  (L + I/2 )
-1

,

(8)

where  and  are hyperparameters. L denotes the combinatorial Laplacian, L = D - G. G denotes the adjacency matrix of the graph G and D a diagonal matrix with entries di,i = j Gi,j . Variables adjacent in G are highly correlated.

4.

DISCUSSION

We have described a combination of two models able to respectively predict future traffic saturations at junctions with sensors and to extend these predictions to junctions without sensors, in a city. To the best of our knowledge, no similar model has been proposed before.
1

[1] A. Artikis, M. Weidlich, F. Schnitzler, I. Boutsis, T. Liebig, N. Piatkowski, C. Bockermann, K. Morik, V. Kalogeraki, J. Marecek, A. Gal, S. Mannor, D. Gunopulos, and D. Kinane. Heterogeneous stream processing and crowdsourcing for urban traffic management. In Proceedings of the 17th International Conference on Extending Database Technology, page (to appear), 2014. [2] J. Friedman, T. Hastie, and R. Tibshirani. Sparse inverse covariance estimation with the graphical lasso. Biostatistics, 9(3):432­441, 2008. [3] W. Leutzbach. Introduction to the Theory of Traffic Flow. Springer, 1988. [4] T. Liebig, N. Piatkowski, C. Bokermann, and K. Morik. Predictive trip planning ­ smart routing in smart cities. In Workshop Proceedings of the EDBT/ICDT 2014 Joint Conference, 2014. [5] T. Liebig, Z. Xu, M. May, and S. Wrobel. Pedestrian quantity estimation with trajectory patterns. In Machine Learning and Knowledge Discovery in Databases, volume 7524 of Lecture Notes in Computer Science, pages 629­643. Springer Berlin Heidelberg, 2012. [6] L. Song, M. Kolar, and E. P. Xing. Time-varying dynamic bayesian networks. Advances in Neural Information Processing Systems, 22:1732­1740, 2009. [7] H. Zou and T. Hastie. Regularization and variable selection via the elastic net. Journal of the Royal Statistical Society: Series B (Statistical Methodology), 67(2):301­320, 2005.

http://dublinked.ie/datastore/datasets/dataset-305.php

HCS: Hierarchical Cut Selection for Efficiently Processing
Queries on Data Columns using Hierarchical Bitmap
Indices â
Parth Nagarkar

K. SelÃ§uk Candan

School of Computing, Informatics, and Decision
Systems Engineering
Arizona State University
Tempe, AZ 85287-8809, USA

School of Computing, Informatics, and Decision
Systems Engineering
Arizona State University
Tempe, AZ 85287-8809, USA

nagarkar@asu.edu

candan@asu.edu

ABSTRACT

data [1]. When data are large and the query processing workloads
consist of such data selection and aggregation operations, columnoriented data stores are generally the preferred choice of data organization, especially because they enable effective data compression, leading to significantly reduced IO [2].
Recently, many databases have leveraged bitmap-indices, which
themselves can be compressed, for efficiently answering queries
[3], [4]. When column-domains (e.g., geographical data, categorical data, biological taxonomies, organizational data) are hierarchical in nature [5], it is often more advantageous to create hierarchical bitmap indices to efficiently answer queries over different
sub-ranges of the domain. [5] for example proposes a hierarchically organized bitmap index (HOBI) for answering OLAP queries
over data with hierarchical domains.
In this paper, we also focus on hierarchically organized bitmap
indices for answering queries over column-oriented data and
present efficient algorithms for selecting the subset of bitmap indices to answer queries efficiently over compressed data columns.
Before we detail the contributions of this paper in Section 1.2, we
first provide an overview of the related work in the area.

When data are large and query processing workloads consist of data
selection and aggregation operations (as in online analytical processing), column-oriented data stores are generally the preferred
choice of data organization, because they enable effective data
compression, leading to significantly reduced IO. Most columnstore architectures leverage bitmap indices, which themselves can
be compressed, for answering queries over data columns. Columndomains (e.g., geographical data, categorical data, biological taxonomies, organizational data) are hierarchical in nature, and it may
be more advantageous to create hierarchical bitmap indices, that
can help answer queries over different sub-ranges of the domain.
However, given a query workload, it is critical to choose the appropriate subset of bitmap indices from the given hierarchy. Thus,
in this paper, we introduce the cut-selection problem, which aims
to help identify a subset (cut) of the nodes of the domain hierarchy, with the appropriate bitmap indices. We discuss inclusive, exclusive, and hybrid strategies for cut-selection and show that the
hybrid strategy can be efficiently computed and returns optimal (in
terms of IO) results in cases where there are no memory constraints.
We also show that when there is a memory availability constraint,
the cut-selection problem becomes difficult and, thus, present efficient cut-selection strategies that return close to optimal results, especially in situations where the memory limitations are very strict
(i.e., the data and the hierarchy are much larger than the available
memory). Experiment results confirm the efficiency and effectiveness of the proposed cut-selection algorithms.

1.1

Related Work

Range and Aggregation Queries over Data Columns. As mentioned above, column-oriented data stores are generally the preferred choice of data organization for aggregation queries over single attribute columns, because they enable effective data compression, leading to significantly reduced IO [2]. Range queries are
used in data warehouse environments to perform aggregations over
a specified range for analysis. Research has been done on creation of specific data structures that can better the performance of
range queries. In [6], the authors propose an update to an existing data structure to store the aggregate values of the leaf nodes
in the internal nodes. This reduces the lookup at the leaf nodes
if all the leaf nodes fall under a range of an internal node in the
query. Their drawback is that the proposed approach stores aggregation values even for queries that do not require aggregation,
thus degrading their performance. In [7], the authors build upon
the work described in [6], and create a more generalized data structure that leverages upper levels to help aggregate queries but does
not store the aggregate values for queries that do not require any
aggregation. In [1], the authors present algorithms to solve range
queries for two types of aggregation operations, sum and max, by
using precomputed max over balanced hierarchical tree structures.
Caching results of query data has also been looked into, particularly for column store environments. In [8], the authorsâ main focus is to develop a system that can cache aggregate results as well

1. INTRODUCTION
Range selection queries are frequent in many applications, including online analytical processing (OLAP) scenarios, where an
aggregation operation needs to be applied over a certain range of
âThis work is supported by NSF grant #1116394 "RanKloud:
Data Partitioning and Resource Allocation Strategies for Scalable
Multimedia and Social Media Analysis"

(c) 2014, Copyright is with the authors. Published in Proc. 17th International Conference on Extending Database Technology (EDBT), March
24-28, 2014, Athens, Greece: ISBN 978-3-89318065-3, on OpenProceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0

271

10.5441/002/edbt.2014.26

as be able to handle transactional and analytical workloads in one
system. In [9], the authors present a hierarchical structure to efficiently execute range-sum queries. Their focus is on reducing number of cell accesses per query and improving update performance
specifically in a data cube. In this paper, we present algorithms that
choose specific bitmap indices to be cached in the memory to speed
up a given query workload.

hybrid strategy can be efficiently computed for a single query or
a workload of multiple queries and also that it returns optimal (in
terms of IO) results in cases where there are no memory constraints
(Section 4.2).
However, in cases where the memory is constrained, the cutselection problem becomes difficult to solve. To deal with these
cases, in Section 2.3.4, we present efficient cut-selection strategies
that return close to optimal results, especially in situations where
the memory limitations are very strict (i.e., the data and the hierarchy are much larger than the available memory).
Experiment results presented in Section 4 confirm the efficiency
and effectiveness of the proposed cut-selection algorithms.

Bitmap Indices. Bitmap indices have been used in OLAP queries
and data warehouses for their benefit of compression. There has
been significant amount of work to improve the performance of
bitmap indices as well as keeping the compression rates high [10],
[11], [12]. Most of the newer compression algorithms use runlength encoding for compression: it provides a good compression
ratio and one can do bitwise operations directly on decompressed
bitmaps without actually decompressing them [10]. Recently, researchers have shown that bitmap indices perform well even on
high-cardinality attributes [13].

2.

PROBLEM SPECIFICATION

In this section, we first introduce the relevant concepts and notations, provide a cost model, and introduce the cut-selection problem for identifying a subset of the nodes of the domain hierarchy,
containing the nodes with the bitmap indices to efficiently answer
a given query or a query workload.

Multi-level Indices. There has also been considerable research
done in the area of multi-level indices [14], [15], [16]. In data warehouse applications, bitmap indices are also shown to perform better
than traditional database index structures like the B-tree [10], [17].
Our work focuses on which bitmaps to read and cache in the memory from a given hierarchy. We introduce novel algorithms that
choose these bitmaps efficiently. As far as we know, most of the
existing approaches use what we term as an inclusive strategy:
they first identify upper level bitmaps for retrieving the data that
fully satisfies the given query and the lower level bitmaps to satisfy
the boundary bitmaps in the hierarchy that the upper level bitmaps
could not be used to give an exact answer [11]. As discussed in
the next subsection, however, we generalize the problem into a cutselection problem and introduce exclusive and hybrid strategies that
complement inclusive result construction. The works [5] and [18]
focus on building hierarchies on dimensions, specifically in a data
warehouse environment, to efficiently execute range queries. [19]
deals with the similar problem of choosing the appropriate set of
bitmap join indices of one or more attributes using data mining
techniques; we on the other hand focus on choosing the appropriate set of bitmap indices for a given domain hierarchy.

2.1

Key Concepts, Parameters, and Notations

We first provide an overview of the concepts and parameters necessary to formulate the problem described in this paper and introduce the relevant notations.

2.1.1 Columns and Domain Hierarchies
A database consists of relations, R = {R1 , . . . , Rmaxr }.
Each relation, Rr , consists of a set of attributes,
Ar
=
{Ar,1 , . . . , Ar,maxar }, with domains Dr
=
{Dr,1 , . . . , Dr,maxar } In this paper, without loss of generality, we associate to each attribute, Ar,a , a corresponding hierarchy, Hr,a , which consists of a set of nodes,
Nr,a = {Nr,a,1 , . . . , Nr,a,maxnr,a }. Also, since our goal is
to efficiently answer queries over a single data column, unless
necessary, we omit explicit references to relation Rr and attribute
Ar,a ; hence, when we do not need to refer to a specific relation
and attribute, we simply omit the relation and attribute subscripts;
e.g., we refer to H instead of Hr,a .
In this paper, when talking about the nodes of a domain hierarchy
H, we use the following notations:

1.2 Contributions of this Paper

â¢ Parent of a node: For all Nâ , parent(Nâ ) denotes the parent of Nâ in the corresponding hierarchy; if Nâ is the root,
then parent(Nâ ) = â¥.

Since IO is often the main bottleneck in processing OLAP workloads over large data sets, given a query or a workload consisting
of multiple queries, the main challenge in leveraging hierarchically
organized bitmap indices is to choose the appropriate subset of
bitmap indices from the given hierarchy to process the query. [5],
for example, proposes a (what we term as an âinclusiveâ) strategy which leverages bitmap indices associated to the internal nodes
along with the bitmap indices associated to the data leaves to bring
together the data elements needed to answer the query.
In this paper, we note that such inclusive strategies can be suboptimal. In fact, [5] shows that the inclusive strategy is effective
mainly for small query ranges. Therefore, in this paper, we introduce a more general cut-selection problem, which aims to help
identify a subset (referred to as a cut) of the nodes of the domain
hierarchy, which contain the operations nodes with the appropriate bitmap indices to efficiently answer queries. In particular, we
discuss inclusive, exclusive, and hybrid strategies for cut-selection
(Section 3.1) and experimentally show that the so-called exclusive
strategy provides gains when the query ranges are large and that
the hybrid strategy provides best solutions across all query range
sizes, improving over the inclusive strategy even when the ranges
of interest are relatively small (Section 4.1). We also show that the

â¢ Descendants of a Node: The set of descendants of node n
in the corresponding hierarchy is denoted as desc(n).
â¢ Leaves: LH denotes the set of leaf nodes of the hierarchy
H. Any other node in H that is not a leaf node is called an
internal node. The set of internal nodes of H is denoted by
IH . We assume that only the leaves of a hierarchy occur in
the database.
â¢ Leaf Descendants of a Node: Leaf descendants of a node
are the set of nodes such that they are leaf nodes as well as descendants of the given node; i.e., for a node n, leaf Desc(n)
returns a set of nodes such that
âbâleaf Desc(n) b â LH â§ b â desc(n).

2.1.2 Query Workload
In this paper, we focus on query workloads with range queries
on an attribute (i.e., column) of the database relations:

272

â¢ Range Specification: Given an attribute Aa and the start
and end points, i and j, we denote the corresponding range
specification as, rsa,i,j .

WAH Library vs. Our Cost Model
Bitmap File Size on Disk (in mb)

35

Given two range specifications, rsa,i,j and rsa,k,l ,
â if k > j, then these two range specifications are disjoint,
â if (i < k, l) â§ (j > k) â§ (j < l), then the two range
specifications are intersecting, and

30
25
20
WAH Library

15

Our Model

10
5
0
0.001

0.01

0.1

0.5

Bitmap Density (log scale)

â if (i < k, l) â§ (j > k, l), then the two range specifications are overlapping.
Figure 1: Comparison of our Cost Model and WAH Library
Model. Dx1 = 0.01, Dx2 = 0.015, Dx3 = 0.03, and a =
1043, b = 0.5895, on a 500 GB SATA Hard Drive with 7200
RPM, and 16 MB Buffer Size.

â¢ Range Queries: Each query q involves fetching one or more
sets of column values, such that each set of values belongs to
a continuous range over the domain hierarchy of the attribute.
A query, q, can have multiple range specifications. The set of
range specifications for a query q is denoted as RSq . Without
loss of generality, we assume that all range specifications in
RSq are disjoint. If a query has two intersecting or overlapping range specifications, rsa,i,j and rsa,k,l , then we partition the query into two subqueries, q1 and q2 , such that
range specification for q1 is rsa,i,j and specification for q2
is rsa,k,l . In Sections 3.2 and 3.3, we discuss algorithms for
handling multiple queries.

are performed on compressed versions of the bitmap indices, further boosting the query performance [10]. In general, the time taken
to read the bitmaps from secondary storage into the memory dominates the overall bitwise manipulation time [11], [20]. The cost of
this process is proportional to the size of the bitmap file on the secondary storage; the larger the size of a bitmap file on a secondary
storage, the longer it takes to bring the bitmap into the physical
memory.

â¢ Range Nodes: Given a range specification, rsa,i,j , the set
of leaf nodes that fall in this range is denoted as, RNa,i,j .
These nodes are also referred to as range nodes.

2.2.1 Read Cost of Compressed Bitmap Indices
Therefore, in this paper, we model the cost of a bitmap operation as proportional to the size of the corresponding (compressed)
bitmap file, which in turn determines the time taken to read a
bitmap into the memory. Note that in general the query performance of a bitmap index with density greater than 0.5 is equivalent
to the performance of a bitmap with density complement to the
original [21]. For example, performance of a bitmap with density
0.7 is often equivalent to the performance of a bitmap with density
0.3. This is because a bitmap with density 0.7 can be negated and
stored as a bitmap with density 0.3. We also include this behavior
in our cost model, readCost(Bn ), of reading a bitmap index, Bn ,
as follows:
ï£±
0
if DBn = 0 â¨ DBn = 1
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´aDBn + b if (0 < DBn â¤ Dx1 ) â¨ (1 â Dx1 â¤ DBn < 1)
ï£´
ï£´
ï£´
ï£´
if (Dx1 < DBn â¤ Dx2 )â¨
ï£´
ï£²k1
(1 â Dx2 â¤ DBn < 1 â Dx1 )
ï£´
ï£´
ï£´
k2
if (Dx2 < DBn â¤ Dx3 )â¨
ï£´
ï£´
ï£´
ï£´
ï£´
(1
â Dx3 â¤ DBn < 1 â Dx2 )
ï£´
ï£´
ï£´
ï£³
k3
otherwise

Given a query, q, and a node n, Gq,n â {0, 1} denotes
whether the node n is a range node for query q. More specifically, if node n is a range node for any range specification
in RSq , then Gq,n = 1 and otherwise, Gq,n = 0.
The set of all range nodes for any range specification of query
q is denoted as RNq . If RNq is empty, the query returns
null, whereas if RNq has the exact same nodes as LH , then
the query returns the entire database content for the attribute
on which H is defined.

2.1.3 Hierarchically Organized Bitmap Indices
As described above, the query workload includes queries that
fetch ranges of values from columns of relations in the database,
before performing further operations on these ranges. When bitmap
indices are available, these operations are implemented in terms of
bitmap manipulations [11]: for example, intersection of two range
queries can be performed as bitwise-AND of two bitmap indices
representing the database values in the two ranges. This ensures
that those data objects that will be pruned as a result of the query
processing are never fetched into memory. In this paper, we assume
that indices are organized hierarchically; i.e., every node n in H has
a corresponding bitmap Bn denoting which of the leaf nodes of n
occur in attribute A of the database.

Here DBn is the bit density, 0 < Dx1 < Dx2 < Dx3 < 0.5 are
three bit density thresholds, and a, b, k1 , k2 , and k3 are constants.
Intuitively, when the bit density of a bitmap is 0 or 1, the size
of the bitmap on the disk is very negligible due to the high-level of
compression. Hence, we assume the size of these bitmaps as nonexistant on the secondary storage. The bit density thresholds, Dx1 ,
Dx2 , and Dx3 , and the constant values, a, b, k1 , k2 , and k3 , are
specific to the implementation of the bitmap library.
Figure 1 shows alignment of our cost model with the read cost
of the WAH library for different bit densities.

â¢ Bitmap Density: Each bitmap Bn has a bit density, 0 â¤
DBn â¤ 1, denoting ratio of bits set to 1 to the size in the
bitmap.
Note that bitmap Bn may or may not have been materialized in the
form of a bitmap index in the database.

2.2 Cost Model and Query Plans

2.2.2 Inclusive, Exclusive, and Hybrid Query Plans

Especially when the data sets are large, the bitmaps are often
stored in a compressed manner and the various bit-wise operations

For a query plan for q, we define the set of nodes that are required

273

2.3.1 Query Processing with Cuts

to execute q as its operation nodes. Naturally, a given query can be
executed in various different ways, each with a different set, ONq ,
of operation nodes. In particular, in this paper, we consider two
distinct types of query plans: inclusive and exclusive plans.

We define a cut, c, as a subset of internal nodes (including the
root node) in a hierarchy, H, satisfying the following two conditions:
â¢ validity: there is exactly one node on any root-to-leaf branch
in a given cut (note that, by this definition, the set containing
only the root node of the hierarchy by itself is a cut); and

U.S.

CA

AZ

â¢ completeness: the nodes in c collectively cover every possible root-to-leaf branch in the given hierarchy, H.
SFO

L.A.

S.D.

PHX Tempe Tucson

If a set of internal nodes of H only satisfies the first condition, then
we refer to the cut as an incomplete cut.
The challenge of course is to select the appropriate cut c of the hierarchy H that will minimize the query processing cost, but will not
add significant memory overhead (if the memory is a constraint).
We discuss the alternative formulations of the cut-selection problem, next.

Consider the 3-level location hierarchy, H, shown above. Here,
the leaf nodes (cities in U.S.) are the actual values in the database.
The node U.S. is the root node of the hierarchy. Let us consider a
query q that has a set of range nodes (shaded nodes in the figure)
RNq =[SF O, L.A., S.D., P HX]. Assume that we have bitmap
indices for all the nodes of H. There are at least two different plans
of executing q:

2.3.2 Cut Selection Case 1: Single Query without
Memory Constraints

â¢ Inclusive query plans: The first plan is to combine a subset
of the bitmaps of H. In the above example, one inclusive
way to do this would be to combine the bitmaps of RNq .

The simplest scenario is identifying the cut necessary to execute
a single range query. As we explained earlier, the cost for executing
a query is proportional to the size of the bitmaps that are read into
the memory from the secondary storage. Thus, given a query q and
cut c on H, problem
ï£±
ï£¼
ï£² â
ï£½
cost(c, q) =
M IN
readCost(Bn )
(1)
ï£¾
ONq â(câªLH ) ï£³

Another inclusive plan would be to combine the bitmaps of
CA and P HX (i.e. CA OR P HX). Note that this strategy
is similar to what was reported in the literature [5].
â¢ Exclusive query plans: Alternatively, we can remove the
bitmaps of the non-range nodes of q from the relevant internal nodes of H. For instance, in this example, we
can achieve this by first performing a bitwise-OR operation
on the bitmaps of T empe and T ucson and then doing a
bitwise-ANDNOT operation between the bitmap of U.S and
the resultant bitmap from the OR operation (i.e. U.S ANDNOT (T empe OR T ucson)).

nâONq

denotes the best execution cost for query q given the bitmaps for
the leaves, and the cut c.
The cut-selection problem for a given query q on hierarchy H
can be formulated as finding a cut c such that cost(c, q) is the
smallest among all cuts of the hierarchy H.

Another exclusive plan would be to do the following:
CA OR (AZ ANDNOT (T empe OR T ucson).

2.3.3 Cut Selection Case 2: Multiple Queries without Memory Constraints

It is easy to see that all four plans would return the same result;
however, these plans have different operation nodes: for the first
inclusive query plan, the operation nodes are ONq = [SF O,
L.A., S.D., P HX], whereas for the second inclusive query plan,
ONq = [CA, P HX]. Similarly, for the first exclusive query plan
ONq = [U.S., T empe, T ucson], and for the second exclusive
query plan ONq = [CA, AZ, T empe, T ucson]. As a result, each
execution plan also requires different amount of data being read.
In this paper, we consider inclusive and exclusive strategies
for answering range queries using hierarchical bitmaps. We also
consider hybrid strategies, which combine inclusive and exclusive
strategies (that may make inclusive or exclusive decisions at different nodes of the hierarchy) for better performance.

In general, we are not given a single range query, but a set of
range queries that need to be executed on the same data set. Therefore, we need to generalize the above formulation to scenarios with
multiple range queries. If we are given a set, Q, of queries on hierarchy H, then one way to formulate the cut-selection problem is to
search for a cut c such that cost(c, Q), defined as
cost(c, Q) =

â

cost(c, q)

(2)

qâQ

is the smallest among all cuts of the hierarchy H.
Note, however, that this formulation treats each query independently and implicitly assumes that each query plan accesses the
bitmaps of its operation nodes from the secondary storage; i.e. it
pays the cost of reading a bitmap from the secondary storage every
time the node is needed for query processing. This will obviously
be redundant when the different queries can be processed using the
same operation nodes: in such a case, it would be best to bring the
bitmap for the operation nodes to the memory and keep it to process
all the relevant queries.
This, however, changes the problem formulation significantly; in
particular, we now need to search for a cut c such that costâ² (c, Q),

2.3 Cut Selection Problem
As described above, any range query, q, on hierarchy H, can
be answered (through inclusive, exclusive, and hybrid strategies)
using bitmap indices for the leaves of the hierarchy. We note however that, if we are also given the bitmap indices for a subset of
the internal nodes of the hierarchy, we may be able to reduce the
overall cost of the query significantly by also leveraging the bitmap
indices for these internal nodes. We refer to these subsets as cuts of
the hierarchy.

274

defined as
(
) ï£«
â
readCost(Bn ) + ï£­
nâc

â

Algorithm 1 Inclusive Cut Selection Algorithm
1: Input: Hierarchy H, Set of internal nodes IH , Query q
2: Output: Set of nodes c
3: Initialize: Node n = root, c
4: procedure FIND N ODE I NCLUSIVE C UT(n)
5:
Set children = f indChildren(n, IH );
6:
if children is empty then
7:
add n to c;
8:
return nodeInclCost(n, q);
9:
else
10:
costChildren = 0;
11:
for each child m of n do
12:
costChild = f indN odeInclusiveCut(m);
13:
if costChild Ì¸= â then
14:
costChildren = costChildren + costChild;
15:
end if
16:
end for
17:
if costChildren = 0 then
18:
costChildren = â;
19:
end if
20:
costCurrN ode = nodeInclCost(n, q);
21:
if costCurrN ode â¤ costChildren then
22:
remove all descendants of n from c;
23:
add n to c;
24:
end if
25:
return min(costCurrN ode, costChildren)
26:
end if
27: end procedure

ï£¶
readCost(Bn ))ï£¸

nâ(âªqâQ ONq )/c

(3)
is the smallest among all cuts of the hierarchy H. Intuitively, the
cut is read into the memory once and for each query in Q the remaining operation nodes are brought to the memory as needed. The
first term in equation 3 is the cost of reading the bitmaps of the
nodes in c from the secondary storage into the memory. Once these
bitmaps have been read into the memory, we reuse them for further
query processing, i.e. the bitmaps of the cuts need to be read into
the memory only once. The second term denotes the cost of reading remaining bitmaps from the secondary storage every time it is
needed to execute a query. These remaining bitmaps are also read
only once and cached subsequently for further re-use for queries in
the workload.

2.3.4 Cut Selection Case 3: Multiple Queries with
Memory Constraints
The above formulations do not have any memory availability
constraints; i.e., as many bitmaps as needed can be read and cached
in memory for the given workload. In general, however, there may
be constraints on the amount of data we can cache in memory.
Therefore, we next consider scenarios where we have a constraint
on the amount of memory that can be used during query processing. Let us assume that we have a memory availability constraint
Stotal . Every bitmap has a size associated to it, SBn , denoting
the memory requirement of the bitmap file of node n in the main
memory. Given a query workload Q and Stotal , we want to find a
(potentially incomplete) cut c that minimizes the following cost:
ï£¶
(
) ï£«
â â
â
readCost(Bm )ï£¸
readCost(Bn ) + ï£­
nâc

3.1

As described in Section 2.2.2, queries can be processed using
inclusive, exclusive, and hybrid strategies. In this subsection,
we first provide three algorithms, corresponding to different
strategies, for the basic scenario with a single query without
memory constraint.

3.1.1 Inclusive Cut Selection (I-CS) Algorithm
The inclusive cut selection (I-CS) algorithm associates an inclusive cost to all nodes of the hierarchy and selects the cut using these inclusive costs. Given node v of the hierarchy H, let
l(v) = {mâ¥(m â leaf Desc(v)) â§ (Gq,m = 1)}. Formally, given
a query q and a node n on hierarchy H, we define the inclusive
cost, nodeInclCost(n, q), of the node in the cut as follows:
ï£±
â
if âmâleaf Desc(n) Gq,m = 0
ï£´
ï£´
ï£´
ï£´
ï£²readCost(Bn )
if âmâleaf Desc(n) Gq,m = 1
â
ï£´
readCost(Bm ) otherwise
ï£´
ï£´
ï£´
ï£³mâleaf Desc(n)

qâQ mâONq /c

(4)
subject to

â

SBn â¤ Stotal

Case 1: Single Query without Memory
Constraints

(5)

nâc

Note that the bitmaps for c are read into the memory once and for
each query in Q the remaining operation nodes are brought to the
memory as needed. The major difference from before is that due
to the constraint on the size of the nodes that can be maintained
in memory, c may be an incomplete cut. Moreover, the operation
nodes that are notâ
in the cut cannot be cached in memory for reuse
(unless Stotal > nâc SBn ).

â§Gq,m =1

Note that the inclusive cost is only applicable for the internal
nodes of a hierarchy; it is undefined for a leaf node.
In Alg. 1, we present the outline of the proposed algorithm which
uses the above definition of inclusive cost to find a cut c that gives
the optimal cost to execute a single range query q. Note that since
a valid cut does not include any leaf nodes, the algorithm considers
only the set of internal nodes, IH , of hierarchy H.
The inclusive cut selection algorithm presented in Alg. 1 is a
dynamic programming solution that traverses the nodes in the hierarchy in a bottom-up manner:

3. CUT SELECTION ALGORITHMS
As described in the previous section, query execution times can
be reduced if we are also given the bitmap indices for a subset of the
nodes in the domain hierarchy of the column. A key challenge is
to select the appropriate subset (or cut) of the hierarchy H to minimize the query processing cost, without adding significant memory
overhead. In this section, we present algorithms that search for a
cut, c, given a query q or a workflow of queries Q. It is important
to note that these algorithms do not directly return the operation
nodes required to execute q; instead they aim to find a cut, c, such
that there exists a set of operation nodes ONq â (c âª LH ) with a
small cost. Once a good cut of hierarchy is found, the necessary operation nodes ONq are identified in post-processing by searching
within the cut c.

â¢ In line 5 of the pseudo-code, the set children is empty for
a node on the second-to-last level of the hierarchy H, since
the input to the function f indChildren is the set of internal
nodes IH . Whenever the set children is empty, we add the
current node to the cut c, and return the inclusive cost of the
current node.

275

â¢ The condition on line 13 makes sure that the cost of children
of n does not include the cost when a child m has the cost â.
This will happen when none of the nodes in leaf Desc(m)
is a range node, i.e. q does not want the contents of m to be
included in the result of the query.

Given these node exclusive costs (which can again be computed
in O(1) time per node using a bottom-up algorithm), an optimal
exclusive cut can be find using a linear time algorithm similar to
the node inclusive cut algorithm presented in Alg. 1; the main difference being that each internal node in the hierarchy is associated with an exclusive cost, instead of an inclusive cost. In this
case, the results would be a cut c such that reading every node in
ONq â (c âª N Sq ), we can execute the query q optimally using the
exclusive strategy. If the output cut c is the root node of the hierarchy, then every node in N Sq has to be removed, i.e. an ANDNOT
operation has to be done between the root node and the nodes in
N Sq .

â¢ The condition on line 17 will be true if for every child m of
n, nodeInclCost(m) = â. This also means that no node
in leaf Desc(n) is a range node. In such a case, we want the
total cost of all the children of n to be equal to â.
â¢ The algorithm then compares the inclusive cost of the parent with the inclusive cost of the set of its children. If the
inclusive cost of the parent is cheaper than the combined inclusive cost of its children, then we remove the descendants
of n from c and add n to c. Otherwise, we keep the cut as it
is, since using the children of n is cheaper than using n.

3.1.3 Hybrid Cut Selection (H-CS) Algorithm
So far, we have considered inclusive and exclusive strategies independently from each other. However, we could consider both inclusive and exclusive strategies for each node in the hierarchy and
associate the better strategy to that node. In other words, we could
modify the linear-time, bottom-up algorithm presented in Alg. 1
using the following cost function for each internal node of the hierarchy, H:

If the resulting c contains only the root node of the hierarchy, then
it means that using the leaves is the cheapest option.
Note that the algorithm is very efficient: each internal node in
the hierarchy is considered only once and for each node only its
immediate children need to be considered; moreover, the function nodeInclCost(), which is called for each node, itself has a
bottom-up implementation with O(1) cost per node assuming that
node densities for each internal node has been computed ahead of
time Consequently, the cost of this algorithm is linear in the size of
the hierarchy, H.

nodeHybridCost(n, q) = min(

Unlike when searching for the inclusive or exclusive cuts of the
hierarchy, during the traversal, we also need to mark each node
as an inclusive-preferred or exclusive-preferred node based on the
contributor to the hybrid cost.
Naturally, in this case the resulting cut, c, can be partitioned into
two: an inclusive cut, ci (whose nodes are considered in an inclusive way), and an exclusive cut, ce (whose nodes are considered under the exclusive strategy). Those nodes that have a lower inclusive
cost are included in ci , whereas those that have a lower exclusive
cost are included in ce .

3.1.2 Exclusive Cut Selection (E-CS) Algorithm
Above, we considered the inclusive strategy which uses bitwise
OR operations among the selected bitmaps to execute the query q.
As we see in Section 4.1, this option may be costly when the query
ranges are large. Alternatively, we can identify query results using
an exclusive strategy: For a given query q, consider a leaf node
m such that Gq,m = 0. That means that this node is not a range
node. We call the leaf nodes (like m), which are outside of the
query range, the non-range nodes and denote them as N Sq . The
values of these leaf nodes are part of the actual data that q does not
want to be displayed in the result. The exclusive strategy, initially
introduced in Section 2.2.2, would first identify the non-range leaf
nodes and then use the rest to identify the query results.
Like the inclusive cost, we associate an exclusive cost to all internal nodes of the hierarchy. Consider an internal node n of the hierarchy. If every node in leaf Desc(n) is a range node, that means
that the q wants the content of n to be included in the result of the
query, i.e. leaf Desc(n) does not contain any non-range node. In
this case, we do not need to remove any node from n, and thus,
the exclusive cost of n is the read cost of the node n. Note, that in
the same scenario, the inclusive cost of n is also the read cost of n.
If, in contrast, none of the leaf descendants of n is a range node,
then the query results will not include n and in this case, the node
exclusive cost of n can be said to be â. The main difference is the
scenario when only some of leaf Desc(n) are non-range nodes.
In this case, the exclusive strategy removes the non-range nodes
from n, and thus, the exclusive cost of n is the read cost of reading all the non-range nodes under n, in addition to the read cost
of n. Based on these, we can formulate the node exclusive cost,
nodeExclCost(n, q) as follows:
ï£±
â
ï£´
ï£´
ï£´
ï£´
ï£²readCost(Bn )
ï£´
readCost(Bn )+
ï£´
ï£´
ï£´
ï£³

nodeInclCost(n, q),
nodeExclCost(n, q)).

â¢ If no leaf Desc(n) is in the range, then we call n, an empty
node. An empty node is not used in any query processing
and is ignored.
â¢ If all of leaf Desc(n) are in the range, then we call n, a
complete node. A complete node indicates that all the leaf
descendants of the node are needed for query processing.
Hence, both the inclusive and the exclusive costs of a complete node are same.
â¢ If only some of the leaf Desc(n) are part of the range, then
we call n, a partial node. Note that the only time n will have
potentially different inclusive and exclusive costs is when n
is a partial node. If a node is a partial node, we find both the
inclusive and exclusive costs, and choose the minimum of the
two costs. Subsequently, whichever cost is chosen, we label
the node accordingly as part of the inclusive or the exclusive
cut. This helps us in efficiently finding the operation nodes
as described further.
As we mentioned earlier, the algorithms described in this section return a cut c, but not the specific operation nodes that are
required to optimally execute the query q. Given a cut c, we need
an additional step in order to find the necessary operation nodes.
Alg. 2 provides the pseudo-code for finding the operation nodes
following execution of the H-CS algorithm. Here, the functions
nodeInclusiveCut(n, q) and nodeExclusiveCut(n, q) return
the set of operation nodes required to execute the relevant part of

if âmâleaf Desc(n) Gq,m = 0
if âmâleaf Desc(n) Gq,m = 1
â
mâleaf Desc(n)â§Gq,m =0 readCost(Bm )
otherwise

276

Algorithm 2 Finding the Operation Nodes
1: Input: Set of nodes c, Query q
2: Output: Set of operation nodes ONq
3: Initialize: ONq
4: procedure FIND O PERATION N ODES(c, q)
5:
for each node n in c do
6:
if n is a complete node then
7:
add n to ONq ;
8:
else if n is a partial node then
9:
inclusiveCost = nodeInclCost(n, q);
10:
exclusiveCost = nodeExclCost(n, q);
11:
if inclusiveCost â¤ exclusiveCost then
12:
add every node from nodeInclusiveCut(n, q) to ONq ;
13:
else
14:
add every node from nodeExclusiveCut(n, q) to ONq ;
15:
end if
16:
end if
17:
end for
18:
return ONq
19: end procedure

Algorithm 3 Hybrid Cut Multiple Query Algorithm
1: Input: Hierarchy H, Set of internal nodes IH , Query Workload Q
2: Output: Set of nodes c
3: Initialize: Node n = root, c
4: procedure FIND H YBRID C UT(n)
5:
Set children = f indChildren(n, IH );
6:
if children is empty then
7:
add n to c;
8:
return N CN odeCost(n, Q);
9:
else
10:
costChildren = 0;
11:
for each child m of n do
12:
costChildren = costChildren + costChild;
13:
end for
14:
costCurrN ode = N CN odeCost(n, Q);
15:
if costCurrN ode â¤ costChildren then
16:
remove all descendants of n from c;
17:
add n to c;
18:
end if
19:
end if
20:
return min(costCurrN ode, costChildren)
21: end procedure

the query q at an internal node n based on inclusive or exclusive
strategies, respectively and the algorithm follows the minimal cost
strategy to identify the operation nodes for the hybrid execution.
We explained our marking strategy earlier in this section. Based on
the marking of each node in the cut, we call the respective function
to get the corresponding inclusive or exclusive operation nodes.
Note that if the cut, c, includes the root of the hierarchy, then either
reading the nodes as part of the query range, or removing the nonrange nodes from the root is the cheapest option. This decision is
again made based on whether the root node was labeled as part of
the inclusive or exclusive cut. We do not need to recompute the two
individual costs to make that decision.

queryâs corresponding SNn,q are read:
N CN odeCost(n, Q) =
ï£«
(readCost(Bn )) + ï£­

â

ï£¶
readCost(Bm )ï£¸ .

mâ(âªqâQ SNn,q )/n

Intuitively, this cost tells us how important a particular node, n,
is relative to the query workload Q: If there are two nodes, na and
nb , such that na appears in SNna ,q for more than one query q â Q
and nb does not appear in any SNnb ,q for any q â Q, then the
N CN odeCost(na , Q) will be lower than N CN odeCost(nb , Q).
Consequently, we can say that a node that is included in the SNn,q
is more important (caching it would impact more queries) and such
important nodes have small N CN odeCost values. We use this
as the basis of our algorithm, shown in Alg. 3, to find the relevant hybrid cut given multiple queries. This bottom-up traversing algorithm is similar to the Hybrid Cut Algorithm explained in
the previous section. The main difference is that we use the cost
N CN odeCost(n, Q) for each node, which is derived using the
hybrid logic as explained in the previous section.

3.2 Case 2: Multiple Queries without Memory Constraint
In this previous section, we have shown that the simple case
where there is a single query to be executed can be handled in linear
time in the size of the hierarchy. In general, however, we may be
given a set of range queries and need to identify a cut of the hierarchy to help process this set of queries efficiently. In this subsection,
we present an algorithm to find a cut for multiple queries without
any memory constraints. We consider the more realistic case with
memory constraints in the next subsection.
Assume we are given a query workload Q that contains more
than one query (each with its corresponding range). Since we do
not have memory constraints, if a bitmap node in the hierarchy has
been read into the memory, it can also be cached to be reused by
other queries, without incurring any further read costs.
Remember that in Section 3.1.3 we have discussed how to find
a hybrid cut and the corresponding operation nodes given a single
query. Let us first assume that we use the algorithms discussed
in Section 3.1.3 to find the hybrid costs and the appropriate labeling for each query in the workload, Q, separately. In order to see
how important a particular node n is relative to a particular query
workload. Let us consider, Sub-Operation Nodes, SNn,q , which
denote the operation nodes required to execute the part of q (in
Q) that is under n. Hence, SNn,q will contain nodes that are in
n âª leaf Desc(n). In order to decide which nodes to choose in
the set n âª leaf Desc(n) given q, we use the same hybrid logic as
explained in Algorithm 2.
We associate to each node, n, in the hierarchy a new cost, called
no constraint node cost (N CN odeCost(n, Q)), defined as the cost
to perform the query workload such that (a) first the node is read
and cached into the memory and (b) the remaining nodes in each

3.3

Case 3: Multiple Queries with Memory
Constraint

In the previous subsection, we introduced a node cost (based on
the cost model as described in 2.3.3.) to capture the importance of a
node in a multiple query scenario without a memory constraint. In
this section, we relax the assumption of unlimited memory availability and consider the more general situation where we have a
memory constraint, limiting how many bitmaps we can keep in
memory at a time. More specifically, in this section, we present
two algorithms, namely 1-Cut Selection Algorithm and k-Cut Selection Algorithm, that find a cut given a query workload and a
memory constraint. Note that, as discussed in Section 2.3.4, due to
the memory constraint, the resulting cuts may be incomplete.
Let us consider a set of nodes for each query and each n,
called Constraint Operation Nodes, denoted by CONn,q . Here,
CONn,q â n âª LH . CONn,q chooses the set of nodes from
n âª LH that are required to execute q in the cheapest possible manner given n and the set of leaf nodes.
CONn,q consists of two sets of nodes. The first set is the set of
nodes that includes n and its leaf descendants. We have to decide
which nodes to choose in the set nâªleaf Desc(n) given q. In order
to make this decision, we use the same hybrid logic as explained in

277

Algorithm 4 1-Cut Selection Algorithm
1: Input: Hierarchy H, Set of internal nodes IH , Query Workload Q, Savailable
2: Output: Set of nodes c
3: Initialize: Savailable = Stotal
4: procedure FIND C UT C ONSTRAINT(DH , Savailable )
5:
while IH is not empty OR there exists a node n such that SBn â¤

Algorithm 5 k-Cut Selection Algorithm
1: Input: Hierarchy H, Set of internal nodes IH , Query Workload Q, Savailable ,
cutList
Output: Set of nodes c
Initialize: âcâcutList Sci ,available = Stotal .
procedure FINDK C UT C ONSTRAINT(H)
while each node n in IH is seen OR there exists a node n such that SBn â¤
Sci ,available for i â¤ k do
6:
choose node n such that n has the lowest CN odeCost(n, Q) among
nodes in H;
7:
mark n as seen;
8:
for each cut c in cutList do
9:
if SBn â¤ Sci ,available then
10:
if there is no conflict in c for node n then
11:
if n has not been added to any empty cut then
12:
add n to c;
13:
update Sci ,available = Sci ,available â SBn ;
14:
end if
15:
else
16:
copy each node in c to the next available empty cut;
17:
replace the conflicting node with node n;
18:
end if
19:
end if
20:
end for
21:
Sort the cutList based on the lowest cost for each cut;
22:
end while
23:
return the cut c in cutList that has the lowest total cost;
24: end procedure

2:
3:
4:
5:

Savailable do
choose node n such that n has the lowest CN odeCost(n, Q) among
nodes in IH & SBn â¤ Savailable ;
7:
add n to c;
8:
remove n from IH ;
9:
remove ancestors and descendants of n from IH ;
10:
update Savailable = Savailable â SBn ;
11:
end while
12:
return c
13: end procedure

6:

Algorithm 2. The second set of nodes, consists of the set of leaf
nodes that are not descendants of n , i.e. LH â© leaf Desc(n). In
order to execute q, all the query range nodes in this set have to be
read, and hence we include them in CONn,q .
As we have done in Case 2 (without memory constraints), we introduce a node cost to capture the importance of each internal node
in the hierarchy relative to query workload Q. This cost, called
constrained node cost (CN odeCost(n, Q)), reflects the cost of
performing the query in such a way that (a) only nodes with low
cost, and that can fit into the memory within the given constraint,
are read and cached into the memory and (b) the remaining nodes
in each queryâs CONn,q are read from the secondary storage as
needed.
CN odeCost(n, Q) =
ï£¶
ï£«
â
â
readCost(Bm )ï£¸
(readCost(Bn )) + ï£­

if for every q in Q, Pn,q does not include n, then the node is an
unused node.
It is important to note that the above algorithm does not necessarily return a cut that has the optimal cost. As we see in Section 4.3,
the sub-optimality of the algorithm is most apparent in situations
where we have plenty (yet still insufficient amount of) memory
and, consequently, the cost-sensitive greedy algorithm over-prunes
the solution space (though it still provides cuts that are significantly
more efficient than a naÃ¯ve execution plan). In situations where the
memory constraints are tight, however, the algorithm returns very
close to optimal or optimal cuts, proving the effectiveness of the
cost model and the proposed approach.

qâQ mâCONn,q /n

Intuitively, if more queries can reuse a node for further query
processing when the node is cached, the lower the constrained node
cost of the node is relative to the query workload Q.

3.3.1 1-Cut Selection Algorithm

3.3.2 k-Cut Selection Algorithm

In Alg. 4, we present the pseudo-code of 1-Cut Selection Algorithm, for Case 3 with multiple queries in the presence of a memory
constraint. Here, Savailable denotes the amount of memory available for adding nodes to a cut and SBn denotes the size of the
bitmap index of node n on the secondary storage. The first time the
algorithm is called, we initialize Savailable to the memory available
for the whole process, i.e., Stotal ; in subsequent calls, the amount
is reduced as new bitmaps are added to the cut. Note that

In this subsection, we note that the key weakness of the above
algorithm is that it considers only a single cut of the hierarchy:
When we choose to include a node in the cut, we remove all the
ancestors and descendants of the node from further consideration;
however, it is possible that a node can have the lowest cost, but
two or more of its ancestors or descendants combined can lead to a
better execution plan. A node n may be chosen before its ancestor
m, because cost(n) is lesser than cost(m). But, it is also possible
that choosing m could be a better choice than choosing n if m can
be used to execute a larger portion of the range nodes of the query.
Therefore, in Alg. 5, we present a variation of the algorithm,
called the k-Cut Selection Algorithm. In this variation, the algorithm considers k different cuts. When a node, n, is added to a
cut, the algorithm does not eliminate its ancestors and descendants
from further consideration; instead, it simply does not add these
ancestors and descendants to the same cut as n to follow the rules
of validity as described in Section 2.3.1. These ancestors and descendants however may be added to the other k-1 cuts.
In Algorithm 5, the ith cut has a corresponding memory requirement, Sci ,available .

â¢ In line 6, we choose a node that has the lowest node cost and
the size of the node is lesser than or equal to the remaining
memory availability.
â¢ In line 9, we ensure that the returned cut does not contain any
two nodes that are on the same root-to-leaf branch.
The stopping condition of the greedy process is reached when the
input set of nodes is empty (i.e. a complete cut is found) or when
each of the remaining nodes have sizes larger than Savailable . Note
that it is possible that in some cases the optimal subset of nodes required to execute the given query workload may all fit in the available memory. Our algorithm adds nodes until all nodes are seen
or no nodes can be added further due to memory constraints. In
order to avoid adding nodes that are not going to be used in query
processing, we introduce a new node label, unused, applied while
calculating the CN odeCost(n, Q) indicating that the node as unused if the node is not used by any query. This is easy to find out

â¢ In the algorithm, line 11 ensures that a node is not added
more than once to an empty cut. This prevents two cuts containing identical nodes.
â¢ Lines 16 and 17 are part of the replacement procedure. Ac-

278

Hybrid Cut

Leaf-only Cut

50
25

0
20

50

600

Inclusive Cut

Hybrid Cut

Leaf-only Cut

450
300
150
0
20

100

50

1000

Inclusive Cut

Exclusive Cut

Hybrid Cut

Leaf-only Cut

750
500
250
0
20

100

50

100

Hierarchy Size (Number of Leaf Nodes)

Hierarchy Size (Number of Leaf Nodes)

Hierarchy Size (Number of Leaf Nodes)

(b) synthetic data, 50% query range

(c) synthetic data, 90% query range

Amount of Data Read vs. Hierarchy Size,
Query Range Size=10%, TPC-H dataset

Amount of Data Read vs. Hierarchy Size,
Query Range Size=50%, TPC-H dataset

Amount of Data Read vs. Hierarchy Size,
Query Range Size=90%, TPC-H dataset

Inclusive Cut

160

Exclusive Cut

Hybrid Cut

Leaf-only Cut

120
80

40
0

20

50

600

Inclusive Cut

Exclusive Cut

Hybrid Cut

Amount of data read (in mb)

(a) synthetic data, 10% query range
Amount of data read (in mb)

Amount of data read (in mb)

Exclusive Cut

Amount of data read (in mb)

Exclusive Cut

75

Amount of data read (in mb)

Amount of data read (in mb)

Inclusive Cut

100

Amount of Data Read vs. Hierarchy Size,
Query Range Size=90%, Normal Dataset

Amount of Data Read vs. Hierarchy Size,
Query Range Size=50%, Normal Dataset

Amount of Data Read vs. Hierarchy Size,
Query Range Size=10%, Normal Dataset

Leaf-only Cut

450
300
150
0

100

20

Hierarchy Size (Number of Leaf Nodes)

50

1000

Inclusive Cut

Hybrid Cut

Leaf-only Cut

500
250
0
20

100

50

100

Hierarchy Size (Number of Leaf Nodes)

Hierarchy Size (Number of Leaf Nodes)

(d) TPC-H data, 10% query range

Exclusive Cut

750

(e) TPC-H data, 50% query range

(f) TPC-H data, 90% query range

Figure 2: Case 1, single query without memory constraints: effects of varying hierarchy and range sizes on the amount of data read
by the three different cut-selection algorithms

1000

Distribution of Nodes in Hybrid Cut, Queries=1,
Hierarchy Size = 100

Exhaustive Cut
Hybrid Cut

750

Average Cut
Leaf-only Cut

500

Inclusive-preferred

1.20
Distribution of Nodes
in Hybrid Cut

Amount of data read (in mb)

Amount of Data Read vs. Query Range Size,
Queries=1, Hierarchy Size = 100

Worst Cut

250

1.00

Empty Node

0.80

0.56

0.60
0.33

0.40
0.20

1.00

Exclusive-preferred

0.86

0.11

0.14
0.00

0.00

0.00

0.00

0
10%

50%

10%

90%

50%

90%

Query Range Size

Query Range Size

Figure 4: Case 1, single query without memory constraints:
percentages of nodes in each strategy (TPC-H data)

Figure 3: Case 1, single query without memory constraints:
comparing the proposed cut algorithm to (exhaustively found)
optimal, average, and worst cuts (TPC-H data)

set the value of k ahead of the time, we propose a Î´ auto-stop condition: after finding the iâth cut, we evaluate if costiâ1 âcosti < Î´,
for a user provided per-iteration cost gain value, Î´. The algorithm
auto-stops when the condition is satisfied (i.e., when the cost gain
of the iteration drops below the predetermined gain). In Section 4.3,
the auto-stop condition is effective, even when we simply set Î´ = 0;
i.e., we stop when the cost of the new cut has the same cost as the
previous cut (note that, for any two integers l, m > 1, and l > m,
the cost of l-greedy cut will always be equal to or lesser than the
cost of m-greedy cut; this is because whatever cut that is returned
by the m-greedy cut algorithm will always be enumerated and considered by the l-greedy cut algorithm).

cording to Section 2.3.1, a cut cannot have two nodes on the
same root-to-leaf branch. Hence, n cannot be added to the
existing cut if there is such a conflict. In these lines, when we
detect a conflict, we add the nodes of a cut to an empty cut
and replace the conflicting node with the current node. This
lets us construct multiple conflicting cuts that are individually conflict-free. Note that if after replacing the conflicting
node with the current node, the size of the cut exceeds the
size of available memory, then we ignore this node and the
corresponding conflicting cut.
â¢ In Line 21, we sort the cutList in ascending order based on
the overall cost of each discovered cut. We do this in order
to give more preference to the cuts with a lower cost during
the next iteration.

4.

EVALUATIONS

In order to evaluate the cut-selection algorithms presented in
this paper, we considered two datasets: (a) a synthetically generated dataset (with normal value distribution) and (b) the TPC-H
dataset [22], each with 150 million records. In particular, in the
TPC-H dataset, we focused on the account balance attribute whose
values demonstrate a near-uniform distribution, with spikes in the
occurrences for some values.
In this section, we have two main evaluation criteria: (1) query
execution IO cost and (2) optimization time. We compared the

3.3.3 Auto Selection of k
As we see in the next section, in practice it is sufficient to consider fairly small number of cuts to significantly improve the effectiveness of the proposed greedy algorithm (returning very close to
optimal cuts), without increasing the cost of the optimization step
significantly. However, in cases where it is difficult for the user to

279

Average Cut

Leaf-only Cut

Worst Cut

1200
800
400
0
5

15

25

Optimal Cut

Hybrid Cut

Average Cut

Leaf-only Cut

Worst Cut

1600

1200
800
400

0
5

15

Number of Queries

25

Optimal Cut
Amount of data read (in mb)

Hybrid Cut

1600

Amount of data read (in mb)

Amount of data read (in mb)

Optimal Cut

Amount of Data Read vs. Number of Queries,
Query Range Size=90%, Hierarchy Size = 100

Amount of Data Read vs. Number of Queries,
Query Range Size=50%, Hierarchy Size = 100

Amount of Data Read vs. Number of Queries,
Query Range Size=10%, Hierarchy Size = 100

Hybrid Cut

Leaf-only Cut

Worst Cut

800

400
0
5

15

Number of Queries

(a) 10% query range

Average Cut

1200

25

Number of Queries

(b) 50% query range

(c) 90% query range

Figure 5: Case 2, multiple queries without memory constraints (TPC-H data)

1-Cut

10-Cut

Average Cut

Worst Cut

2000
1500
1000
500
0
10%

30%

50%

70%

90%

Exhaustive Cut

1-Cut

Average Cut

Worst Cut

10000
7500

5000
2500
0

10%

30%

50%

70%

90%

Exhaustive Cut

1-Cut

(b) 50% query range

10-Cut

Average Cut

Worst Cut

10000
7500

5000
2500
0

10%

30%

Memory Availability

Memory Availability

(a) 10% query range

10-Cut

Amount of Data Read vs. Memory Availability,
Queries=15, Query Range Size=90%,
Hierarchy Size = 100
Amount of Data Read (in mb)

Exhaustive Cut

Amount of Data Read vs. Memory Availability,
Queries=15, Query Range Size=50%,
Hierarchy Size = 100
Amount of Data Read (in mb)

Amount of Data Read (in mb)

Amount of Data Read vs. Memory Availability,
Queries=15, Query Range Size=10%,
Hierarchy Size = 100

50%

70%

90%

Memory Availability

(c) 90% query range

Figure 6: Case 3, multiple queries with varying memory availability (TPC-H data)
results of our cut-selection algorithms against (a) leaf-only query
execution, (b) random cut-selection, and (c) exhaustive cut-search
strategies.
For both of the above data sets, we considered (balanced) attribute hierarchies of different depth and internal-node fanout:
these were generated for different numbers of leaf nodes and
maximum possible fanouts of the internal nodes of the hierarchy. Since finding the optimal cut using an exhaustive strategy
for comparison purposes is prohibitively expensive, we initially
considered small hierarchies, with 20, 50, and 100 leaf nodes and
heights of 4, 5, and 4 respectively (the root of the hierarchy being
considered at height 1).
In Section 4.4, we consider hierarchies of larger sizes and higher
number of queries to study the scalability of the cut-selection algorithms against the hierarchy size.
Bitmap indices were generated for the nodes of these hierarchies
using the Java library, WAH bitset [23] as explained in [10]. The
parameters of the read cost model presented in Section 2.2.1, and
shown in Figure 1 were computed based on these bitmap indices.
We have also created query workloads with different target range
sizes. For example, for a hierarchy of 100 leaf nodes, 10% query
range size indicates that each range query covers 10 consecutive
leaf nodes.
TM
R
We ran the experiments on a quad-core IntelâCore
i5-2400
CPU @ 3.10GHz machine with 8.00GB RAM. All codes are implemented and run using Java v1.7.

ficient than the inclusive strategy when the query ranges are larger.
Most importantly, in all cases, the hybrid strategy (H-CS) returns
the best cuts.
In Figure 3, we compare the hybrid (H-CS) strategy against (exhaustively found) optimal and average cuts. The figure also shows
the performance of the worst cut. As expected, the H-CS strategy
returns optimal cuts. On the average, randomly selecting a cut performs quite poorly (almost as bad as selecting the worst possible
cut), especially as the query range sizes increase. This highlights
the importance of utilizing an effective (hybrid) cut-selection algorithm for answering queries.
In Figure 4, we show the percentages of nodes that are labeled
inclusive-preferred or exclusive-preferred in a hybrid cut, as explained in Section 3.1.3, for different query ranges. As defined in
Section 3.1.3, empty nodes are nodes that are not used in query
processing. When the query range size is small, most of the query
processing can be done using the leaf nodes. Hence, we see in the
figure that most of the nodes in the cut are empty nodes. As expected, when the query range is small, the inclusive strategy dominates and when the range is large, the exclusive strategy dominates.
For ranges that are neither small nor large, the hybrid algorithm
leverages a mix of inclusive and exclusive strategies.

4.2

Case 2: Multiple Queries without Memory Constraints

In this section, we evaluate the hybrid cut selection algorithm
(Alg. 3) for query workloads with multiple queries. For our evaluations, we considered query workloads of different sizes (and with
different ranges). All reported costs are averages of the costs for 10
different runs.
Figure 5 shows the impact of using the proposed hybrid cut selection algorithm for different numbers of queries. As we see in
this figure, as expected, the hybrid cut selection algorithm returns
the optimal cut. The impact of the proposed cut selection algorithm
is especially strong when the query includes large ranges as when
there are large overlaps among the queries, the query evaluation algorithm has more opportunities for reusing cached nodes, and the
proposed hybrid cut strategy is able to leverage these opportunities

4.1 Case 1: Single Query without Memory
Constraints
We first evaluate the cut-selection algorithm for the single query
without memory constraints scenario. All reported costs are averages of the costs for 10 different runs.
Figures 2(a) through (f) compares the three different cutselection algorithms (I-CS, E-CS, and H-CS) presented in Section 3.1 for different data sets and varying hierarchy and range
query sizes. As we see in these charts, the inclusive strategy is
efficient when the query ranges are small; this is consistent with
the observation in [5]. The exclusive strategy, however, is more ef-

280

2.5

1-Cut

Auto-stop Cut

5-Cut

Amount of Data Read (in mb)

Cost Ratio of k-Greedy Cut with Exhaustive Cut vs.
Memory Availability, Queries=15,
Query Range Size=50%, Hierarchy Size=100
10-Cut

Cost Ratio

2
1.5
1
0.5
0
10%

30%

50%

70%

90%

Amount of Data Read vs. Query Range Size,
Queries=15, Hierarchy Size=100,
Memory Availability=90%
10000

Exhaustive Cut

Worst Cut

5000
2500
0
10%

50%

90%

Query Range Size

Figure 8: Case 3, effect of different query range sizes (TPC-H
data, 90% memory availability)

Figure 7: Case 3, multiple queries with varying memory availability (TPC-H data): impact of different k
most effectively.

Amount of Data Read (in mb)

4.3 Case 3: Multiple Queries under Memory
Constraints
In this section, we evaluate the effectiveness of the proposed khybrid cut algorithm (Alg. 5, described in Section 3.3.2), for multiple queries, but under memory constraints. We report the memory
availability in terms of the percentage of the memory needed to
store the bitmap indices corresponding to the maximum cut of the
given hierarchy. The presented results are averages of 10 different
runs.
Once again, we compare the proposed cut selection algorithm
against solutions found through exhaustive enumeration, average
solutions representing randomly selected cuts, and also the worst
solution. Remember, that under memory limitations, we need to
consider also the incomplete cuts of the input hierarchies.
Note that the number of incomplete cuts that an exhaustive algorithm would need to consider grows very fast:

Amount of Data Read vs. Number of Queries,
Query Range Size=50%, Hierarchy Size=100,
Memory Availability=90%
16000

Exhaustive Cut

10-Cut

Average Cut

Worst Cut

12000
8000
4000
0
5

15

25

Number of Queries

Figure 9: Case 3, effect of different number of queries (TPC-H
data, 90% memory availability)

Amount of Data Read vs. Hierarchy Size,
Queries=5, Query Range Size=50%,
Memory Availability=90%

Incomplete cuts
154
296,381
1,185,922

Amount of Data Read (in mb)

Height
4
5
4

Average Cut

7500

Memory Availability

Num. of leaves
20
50
100

10-Cut

However, since the number of incomplete cuts grow even faster
than the number of complete cuts, enumerating all incomplete cuts
for the exhaustive algorithm (which we use to locate the optimal cut
for comparison purposes), becomes prohibitive beyond hierarchies
with 100 leaf nodes.
Figure 6 shows that, in this case, the proposed hybrid cut selection algorithms are not optimal; however, they return cuts that are
very close to optimal. In fact, especially when the memory availability is very restricted (which is the expected situation in most
realistic deployments), even the 1-Cut algorithm is able to return
optimal or very close to optimal answers. As the available memory increases, the optimal cost decreases as there are more caching
opportunities, but 1-Cut strategy may not be able to leverage this
effectively, especially for larger query ranges. However, we see
that the multi-cut strategy (10-Cut in this figure) performs quite
close to optimal. Figure 7, which plots the ratio of the cost of the
solutions found by the multi-cut strategy (for different values of k)
to the cost of the optimal cut found through an exhaustive search,
confirms this observation: note the figure also shows that the autostop strategy described in Section 3.3.3 is effective in reducing the
cost, without having to fix the value k ahead of time.
Figures 8 through 10 further confirm that the proposed multi-cut
strategy is robust against changes in the size of the query ranges,

3000

Exhaustive Cut

10-Cut

Average Cut

Worst Cut

2250
1500
750
0
20

50

100

Hierarchy Size (Number of Leaf Nodes)

Figure 10: Case 3; effect of different hierarchy sizes (TPC-H
data, 90% memory availability)

number of queries, and hierarchy sizes.

4.4

Cut-Selection Time

Up to now, we considered query processing cost using cuts. We
now focus on the time needed to select cuts for hierarchies of different sizes. In Figures 11 and 12, we see the cut selection time as a
function of the size of the hierarchy (number of leaf nodes; i.e., the
size of the domain) and the number of queries, respectively. Please
note that, in these figures, we do not compare our algorithm with
exhaustively found cuts, and hence are able to consider larger hierarchy sizes and higher number of queries. The figures confirm
that the time taken to find the cut increases linearly with size of the
attribute domain and the number of queries.

281

systems. SIGMOD â06, pages 671â682, 2006.
[3] Oracle database 10g, 2013.
[4] Luciddb - home, 2013.
[5] J. Chmiel, T. Morzy, and R. Wrembel. Time-hobi: indexing
dimension hierarchies by means of hierarchically organized
bitmaps. DOLAP â10, 2010.
[6] S. Hong, B. Song, and S. Lee. Efficient execution of
range-aggregate queries in data warehouse environments. In
Conceptual Modeling ERâ01. 2001.
[7] Y. Feng and A. Makinouchi. Ag-tree: a novel structure for
range queries in data warehouse environments. DASFAAâ06,
pages 498â512, 2006.
[8] S. Muller and H. Plattner. Aggregates caching in columnar
in-memory databases. VLDB â13, 2013.
[9] T. Lauer, D. Mai, and P. Hagedorn. Efficient range-sum
queries along dimensional hierarchies in data cubes.
DBKDA â09, pages 7â12, 2009.
[10] K. Wu, E. Otoo, and A. Shoshani. On the performance of
bitmap indices for high cardinality attributes. VLDB â04,
pages 24â35, 2004.
[11] D. Rotem, K. Stockinger, and K. Wu. Optimizing candidate
check costs for bitmap indices. CIKM â05, 2005.
[12] O. Kaser, D. Lemire, and K. Aouiche. Histogram-aware
sorting for enhanced word-aligned compression in bitmap
indexes. DOLAP â08, 2008.
[13] K. Wu, K. Stockinger, and A. Shoshani. Breaking the curse
of cardinality on bitmap indexes. In Scientific and Statistical
Database Management, pages 348â365. 2008.
[14] R.R. Sinha, S. Mitra, and M. Winslett. Bitmap indexes for
large scientific data sets: a case study. 2006.
[15] R. Sinha and M. Winslett. Multi-resolution bitmap indexes
for scientific data. ACM Trans. Database Syst., August 2007.
[16] M. Morzy, T. Morzy, A. Nanopoulos, and Y. Manolopoulos.
Hierarchical bitmap index: An efficient and scalable
indexing technique for set-valued attributes. In Advances in
Databases and Information Systems, pages 236â252. 2003.
[17] M. Zaker, S. Phon-amnuaisuk, and S. Haw. An adequate
design for large data warehouse systems: Bitmap index
versus b-tree index, 2008.
[18] J. Chmiel, T. Morzy, and R. Wrembel. Hobi: Hierarchically
organized bitmap index for indexing dimensional data. In
DaWaK, pages 87â98, 2009.
[19] L. Bellatreche, R. Missaoui, H. Necir, and H. Drias.
Selection and pruning algorithms for bitmap index selection
problem using data mining. DaWaKâ07, pages 221â230,
2007.
[20] F. DeliÃ¨ge and T. Pedersen. Position list word aligned hybrid:
optimizing space and performance for compressed bitmaps.
EDBT â10, pages 228â239, 2010.
[21] K. Wu, E. J. Otoo, and A. Shoshani. An efficient
compression scheme for bitmap indices. Technical report,
ACM Transactions on Database Systems, 2004.
[22] Transaction Processing Performance Council. Tpc-h
benchmark specification, 2013.
[23] Compressedbitset - wah compressed bitset for java,
November 2007.

Optimization Time vs. Hierarchy Size,
Queries = 200, Range Nodes Size = 50%
Time required to find the cut
(in ms)

1400
1200
1000
800
600
400
200
0
0

500

1000

1500

2000

2500

3000

Hierarchy Size (Number of Leaf Nodes)

Figure 11: Effect of different hierarchy sizes on time taken to
find the hybrid cut
Optimization Time vs. Number of Queries,
Range Nodes Size = 50%, Hierarchy Size= 2000
Time required to find the cut
(in ms)

7000

6000
5000
4000
3000
2000
1000
0
0

200

400

600

800

1000

1200

Number of Queries

Figure 12: Effect of different number of queries on time taken
to find the hybrid cut

5. CONCLUSION
Column-stores use compressed bitmap-indices for answering
queries over data columns. When the data domain is hierarchical,
organizing the bitmap indices hierarchically can help more efficiently answer queries over different sub-ranges of the attribute
domain. In this paper, we showed that existing inclusive strategies
for leveraging hierarchically organized bitmap indices can be
sub-optimal in terms of their IO costs unless the query ranges are
small. We also showed that an exclusive (cut-selection) strategy
provides gains when the query ranges are large and that and
that a hybrid (cut-selection) strategy can provide best solutions,
improving over both strategies even when the ranges of interest
are relatively small. In this paper, we also presented algorithms
for implementing the hybrid strategy efficiently for a single query
or a workload of multiple queries, in scenarios with and without
memory limitations. In particular, we showed that when the
memory is constrained, selecting the right subset of bitmap indices
becomes difficult; but, we also showed that, even in this case, there
exists efficient cut-selection strategies that return close to optimal
results, especially in situations where the memory limitations are
very strict. Experiment results confirmed that the cut-selection
algorithms presented in this paper are efficient, scalable, and
highly-effective.

6. REFERENCES
[1] C. Ho, R. Agrawal, N. Megiddo, and R. Srikant. Range
queries in olap data cubes. SIGMOD â97, pages 73â88, 1997.
[2] D. Abadi, S. Madden, and M. Ferreira. Integrating
compression and execution in column-oriented database

282

Georgia State University

ScholarWorks @ Georgia State University
Public Health Faculty Publications School of Public Health

2015

NOTES2: Networks-of-Traces for Epidemic Spread Simulations
Sicong Liu
Arizona State University, s.liu@asu.edu

Yash Garg
Arizona State, yash.garg@asu.edu

K. Selçuk Candan
Arizona State University, candan@asu.edu

Maria Luisa Sapino
University of Torino, marialuisa.sapino@unito.it

Gerardo Chowell
Georgia State University, gchowell@gsu.edu

Follow this and additional works at: http://scholarworks.gsu.edu/iph_facpub Part of the Public Health Commons Recommended Citation
S. Liu, Y. Garg, K. S. Candan, M. L. Sapino, G. Chowell. NOTES2: Networks- of-Traces for Epidemic Spread Simulations. Computational Sustainability: Papers from the 2015 AAAI Workshop.

This Conference Proceeding is brought to you for free and open access by the School of Public Health at ScholarWorks @ Georgia State University. It has been accepted for inclusion in Public Health Faculty Publications by an authorized administrator of ScholarWorks @ Georgia State University. For more information, please contact scholarworks@gsu.edu.

Computational Sustainability: Papers from the 2015 AAAI Workshop

NOTES2: Networks-of-Traces for Epidemic Spread Simulations
Sicong Liu and Yash Garg and K. Selc ¸ uk Candan
Arizona State University email: {sliu104, ygarg, candan}@asu.edu

Maria Luisa Sapino
University of Torino email: marialuisa.sapino@unito.it Abstract
Decision making and intervention against infectious diseases require analysis of large volumes of data, including demographic data, contact networks, agespecific contact rates, mobility networks, and healthcare and control intervention data and models. In this paper, we present our Networks-Of-Traces for Epidemic Spread Simulations (NOTES2) model and system which aim at assisting experts and helping them explore existing simulation trace data sets. NOTES2 supports analysis and indexing of simulation data sets as well as parameter and feature analysis, including identification of unknown dependencies across the input parameters and output variables spanning the different layers of the observation and simulation data.

Gerardo Chowell-Puente
Arizona State University email: gchowell@asu.edu

Figure 1: Simulation trace exploration interface of NOTES2 preventive actions taken by individuals and public health interventions, requiring continuous adaptation. · Complexity of the simulation and observation data: Epidemic simulations track 10s or 100s of interdependent parameters, spanning multiple layers and geospatial frames, affected by complex dynamic processes operating at different resolutions. Moreover, generating an appropriate ensemble of stochastic epidemic realizations may require multiple simulations, each with different parameters settings corresponding to slightly different, but plausible, scenarios (Barrett, Eubank, and Smith 2005; Chao et al. 2010). Thus, running and interpreting simulation results (along with the real-world observations) to generate timely actionable results are difficult. In this paper, we present Networks-Of-Traces for Epidemic Spread Simulations (NOTES2) to assist experts in exploring large simulation ensembles (Figure 1). The NOTES2 system supports · analysis and indexing of simulation data sets, including extraction of salient multi-variate temporal features from the inter-dependent parameters, spanning multiple layers and spatial-temporal frames, driven by complex dynamic processes operating at different resolutions. · parameter and feature analysis, including identification of unknown dependencies across the input parameters and output variables spanning the different layers of the observation and simulation data.

Introduction
Real-time and continuous analysis and decision making for infectious disease understanding and intervention involve multiple aspects, including (i) estimating transmissibility of an epidemic disease, such as influenza (Abubakar et al. 2012); (ii) forecasting the spatio-temporal spread of pandemic disease at different spatial scales (Merler et al. 2011); (iii) assessing the effect of travel controls during the early stage of the pandemic (Colizza et al. 2007); (iv) predicting the effect of implementing school closures (Wu et al. 2010); and (v) assessing the impact of pharmaceutical interventions on pandemic disease (Ferguson et al. 2005; Deodhar et al. 2014) through simulations. While highly modular and flexible epidemic spread simulation software, such as GLEaMviz (Van den Broeck et al. 2011) and STEM (STEM 2014), exist, these suffer from two major challenges that prevent realtime decision making: · Data and model complexity: A sufficiently useful disease spreading simulation tool requires models, including demographic data, social contact networks, age-specific contact rates, local and global mobility patterns of individuals (Balcan et al. 2009; Merler and Ajelli 2014), epidemiological parameters for the infectious disease (e.g., infectious period), and control intervention data and models. Moreover, these dynamically evolve over time due to
Supported by NSF grants #1318788 and #1339835. Copyright c 2015, Association for the Advancement of Artificial Intelligence (www.aaai.org). All rights reserved.


79

Related Works
Temporal Data Analysis. There are various multi-variate temporal data models, such as the multi-variate structural time series model (Harvey and Koopman 1997; Silva, Hyndman, and Snyder 2010). Analysis of relationships (correlations, transfer functions, and causality) among time series is expensive (Reinsel 2003). A common representation of multi-variate data evolving over time is a tensor (multidimensional array) stream. Tensors and tensor streams are often analyzed for their underlying structures through tensor decomposition algorithms (Carroll and Chang 1970; Harshman 1970; Tucker 1966). An alternative to tensor decomposition is to use probabilistic and generative models, such as Hidden Markov models (HMMs) and Dynamic Topic Modeling, DTM (Blei and Lafferty 2006). A third alternative is to leverage AutoRegressive Integrated Moving-Average (ARIMA) and multi-variate ARIMA based analysis, which separates a time series into autoregressive, moving-average, and integrative components for modeling and forecasting (Mills 1990). Time Series Search. In many applications, when comparing two sequences or time series, exact alignment is not required. Instead, whether two sequences are going to be treated as matching depends on how similar they are; thus, this difference needs to be quantified. This is commonly done through distance measures which quantify the minimum number (or cost) of symbol insertions, deletions, and substitutions needed to convert one sequence to the other. Dynamic time warping (DTW) distance (Ding et al. 2008; Keogh 2002; L.Ye and E.Keogh 2009; Yu et al. 2007), used commonly when comparing continuous sequences or time series can be thought of as a special case. Diffusion in Networks. Kempe et al. were among the first teams who have investigated the problem of optimizing the network for maximum spread (Kempe, Kleinberg, and Tardos 2003). Watts and Dodds also studied the conditions under which nodes in a network become influential (Watts and Dodds 2007). (Chen, Wang, and Wang 2010) proposed a heuristic algorithm, based on local influence regions, to identify nodes in a network that maximize the spread of influence. In (Shakarian, Subrahmanian, and Sapino 2010), the authors focused on learning diffusion models and studying the impact of one node on the others in the network through reasoning with previously learned diffusion models, expressed via generalized annotated programs. (Leskovec et al. 2007) focuses on the related problem of optimal sensor placement to observe information cascades within the network, including disease outbreaks in a population, contaminant distribution within a water distribution network, and information flow within the blogosphere. (Kim, Candan, and Sapino 2012) noted that, while details differ, the various propagation models have two common properties: (a) decay with distance, and (b) reinforcement. Unfortunately, most models focus on the steady state of the propagation in the network and ignore the temporal dynamics of the diffusion itself. Moreover, most (if not all) of these works focus on a single parameter, whereas we need to track temporal

dynamics of multiple inter-dependent parameters.

Networks-of-Traces for Epidemic Simulations
If effectively leveraged, models reflecting past outbreaks, existing simulation traces obtained from simulation runs, and real-time observations incoming during an outbreak can be collectively used for better understanding the epidemic's characteristics and the underlying diffusion processes, forming and revising models, and performing exploratory, if-then type of hypothetical analyses of epidemic scenarios. There are five major types of data associated to epidemic spread simulations. · Network layers: An epidemic simulation requires one or more layers of networks, from local and global mobility patterns to contact networks. · Disease models, describing the epidemiological parameters relevant to a simulation and the parameter dependencies necessary in the computation of the disease spread. · Simulation traces: For a given disease study, researchers and decision makers often perform multiple simulations, each corresponding to different sets of assumptions (disease parameters or models) or context (e.g. spatiotemporal context, outbreak conditions, interventions). · Disease observation traces: These include real-world observations relating to particular epidemic, including the spread and severity of the disease and observations about other relevant parameters, such as the average length of recovery or percentage of infectious individuals that undergo pharmaceutical treatment. · External interventions: In an outbreak, public health and disease control agencies implement various medical or social interventions, quarantines and/or school closures. We collectively refer to these data (network layers, disease models, simulation traces, observation traces, and interventions) as the networks-of-traces (NT) data.

Leveraging the NT Model for Disease Spread Simulation Understanding and Analysis
Epidemic spread simulations are complex. However, parameter dependencies and the network structures of the layers (e.g. mobility, social contact networks) are implicitly evident in the simulation traces and these carry temporal features (that may correspond to major changes in the underlying networks and/or temporal dynamics) that are robust against noise. The detection of these robust multivariate features constitutes the first step towards leveraging the NT data for understanding epidemics' characteristics and the diffusion processes, revising models, and performing exploratory, ifthen type of hypothetical analyses of epidemic scenarios.

Networks-of-Traces (NT) Feature Extraction
An NT data trace is multi-variate and the analysis of the relevant processes requires multi-variate temporal features spanning multiple inter-dependent trace parameters. Intuitively, a robust temporal feature in a multi-variate time series corresponds to a multi-variate segment of the series which significantly differs from its neighborhood. The multi-variate

80

The time-and-variate smoothed version of Y (t, s) at scale s = st , sv is defined as Y(t, s) = (H1 (s); ...; Ht (s)), where Ht (s) = S (H, sv , Y (t, st )) is the version of Y (t, st ) = Y1 (t, st ), ..., Ym (t, sT ) , variate-smoothed at scale sv at time instant, t. Iterative Scale Space Construction. We construct the scale space by incrementally smoothing Y (both in time and variates) starting from an initial scale s0 = st,0 , sv,0 . Let Yi (t, s) be a time-and-variate smoothed version of Yi (t) at scale s = st , sv . Given a pair, k = kt , kv , of time and variate scale multipliers, we add the three scale-space neighbors (or ss-neighbors) of Yi (t) into the scale space: Figure 2: Each row corresponds to a time series of incidences for a sample epidemic simulation and each dot corresponds to the center of an identified multivariate feature segment is represented by a center, µt , µv , and a scope, t , v . Intuitively, µt marks the center of the segment in time and t is the corresponding time interval. On the other hand, µv is one or more nodes/variates of the graph on which the segment is centered and v denotes all the graph vertices covered by the segment. Figure 2 shows an epidemic simulation heatmap, where each row corresponds to a different state. In the figure, centers of identified features are highlighted by white dots. The figure also expands one of these robust features (tail end of the epidemic on a set of neighboring states): the center, µt = 125, µv = {T X }, is marked with a blue dot and its scope, time = [111, 139], v = {AR, LA, N M, OK, T X } , is visualized using rectangles. Robust Feature Detection Let Y (t) = Y1 (t), ..., Ym (t) be a multi-variate trace, from time t = 1 to t = n. As in (Lowe 2004), we detect stable multi-variate features at the extrema of the scale space. However, unlike (Lowe 2004) (which operates on images with two ordered dimensions; i.e., rows and columns of pixels), extracting multi-variate features of the simulation trace Y (at various temporal and variate scales) requires detecting local maxima and computing gradients relative to not only the (ordered) time dimension, but also to the underlying variate graph. Time-and-Variate Smoothing. We construct the scale space of Y (t) (corresponding to the versions of the series smoothed at different temporal and variate scales) relying on the following time-and-variate smoothing process: · Let Yi (t, st ) indicate a version of uni-variate series, Yi , smoothed with parameter st : Yi (t, st ) = G(t, st )  Yi (t), where  is convolution in t and G(t, st ) is the Gaussian. Let Y (t, st ) = Y1 (t, st ), .., Ym (t, st ) be a version of Y , where each uni-variate series is independently smoothed. · Let us also define the variate smoothing function,  S (R, sv , X ) = [G(0, sv )I + (j =1) 2 × G(j, sv )Rj ]X , where (a) R is an m × m matrix describing the variate dependencies, (b) X = X1 , ..., Xm is a m-vector, and (c) sv is a variate smoothing parameter. Since G(j, sv ) approaches 0 quickly as j increases, the smoothing term in front of X can be approximated by a finite summation. Yi (t, k t s) def Yi (t, k v s) def Yi (t, k t,v s) def Yi (t, kt × st , sv ), Yi (t, st , kv × sv ), and Yi (t, kt × st , kv × sv ).

The process continues iteratively until maximum temporal and variate scales (bounded by the length of the simulation trace and the number of variates) are met. Local Extrema Detection. For detecting extrema, for each Yi (t, s) in the constructed scale space, we compute
t Di (t, s) = abs(Yi (t, s) - Yi (t, k t s)), v Di (t, s) = abs(Yi (t, s) - Yi (t, k v s)), t,v Di (t, s)

= abs(Yi (t, s) - Yi (t, k t,v s)).

Local extrema are identified by considering each i, t, s t,v t v triple and comparing max(Di (t, s), Di (t, s), Di (t, s)) against the 78 ss-neighbors of i, t, s in terms of time (before, same time, after), variates (impacting, same variate, impacted by), and scales (smaller, same scale, larger). Poorly defined extrema (i.e., an extremum that has a large principal curvature in one direction but a small one in the perpendicular direction) are eliminated. Feature Descriptor Creation. Let us be given a triple i, t, s . Let also N and M be two integers such that N  3t and M  3v . We create the local feature descriptor corresponding to this triple using a 2N × 2N matrix W : Let Y(i,s) be the time series Yi at scale s; then, for all -N < a  N and -N < b  N , W [a, b] is defined as follows: (a) if b > 0, W [a, b] = (Rb Y(i,s) )[t + a]; (b) if b = 0, W [a, b] = Y(i,s) (t + a), and (c) if b < 0, W [a, b] = (R-1 )b Y(i,s) )[t + a]. Finally, we construct a (2u × 2v × c)-dimensional descriptor for the triple i, t, s in the form of a gradient histogram based on the matrix, W : we sample c gradient magnitudes on the descriptor using a 2u × 2v grid superimposed on the matrix, W . A Gaussian weighting function is used to reduce the magnitude of elements further from the center.

Feature Search and Alignment
Features extracted from a networks-of-traces data play important roles in the NOTES2 system. Here, we discuss how the similarity between two triples, i1 , t1 , s1 and i2 , t2 , s2 , and the corresponding descriptors, desc1 and desc2 , are computed in NOTES2. Depending on the use context, feature similarity has three major components:

81

Table 1: Target feature parameters
min target feature length max target feature length min target feature size max target feature size descriptor size  5 time units  40 time units  2 hops  10 hops 32 (= 2 × 2 × 8)

Table 2: Average confusions for simulations with different transmission and recovery rates.
T.Rate 1.0 0.75 0.25 1.0 0.75 0.5 R.Rate 0.5 0.5 0.5 0.25 0.25 0.25 BN 0.35 0.23 0.37 0.44 0.39 0.29 AN 0.36 (1.14×) 0.25 (1.06×) 0.63 (1.73×) 0.45 (1.02×) 0.45 (1.14×) 0.34 (1.15×) RN 0.44 (1.28×) 0.32(1.37×) 0.49 (1.33×) 0.54 (1.23×) 0.48 (1.23×) 0.39 (1.32×)

· Descriptor alignment: Since the feature descriptors are gradient histograms, their similarity is measured through a histogram similarity function (in the experiments, we use inverse of Euclidean distance). · Temporal alignment: For temporal alignment between two features, we consider both the distance between the temporal centers of the features as well as the degree of overlap between the temporal scopes of the features. · Variate alignment: For variate alignment, we consider both the distance between the variates in the underlying relationship graph as well as the degree of overlap between the variates within the scopes of the two features. Depending on the application, we also consider alignments of (a) the average amplitudes and (b) sizes of the temporal and variate scopes of the two triplets. These various components of feature similarity are combined using a similarity merge function, such as max, min, or product based on the desired matching semantics.

(a) features extracted using the border network

Evaluation
To assess whether the features extracted from epidemic simulations truly reflect the underlying variate networks, we created a set of simulations, using the STEM simulator, based on the US border network, where there is an edge between states if they share a border, and air network, which is a clique. For a given pair of transmission and recovery rates, we created 51 simulations (of length 213 units of time) assuming a different US state as the ground zero and recorded incidence rates1 . We then extracted three sets of features from each simulation, using parameters in Table 1, and assuming different connectivity structures: · Border network (BN): For this case, we used the border network denoting states sharing borders. · Air network (AN): In this case, features are extracted assuming the air network (which is a clique). · Random network (RN): In this case, a random graph (with the same number of edges as the border network) is used for extracting features. Given these features and their descriptors, we then computed the confusion for a simulation with ground zero sim(gz ,gzj ) . state, gzi , as conf usion(gzi ) = AV Ggzj =gzi sim(gzi i ,gzi ) Here the similarity, sim(gzi , gzj ), between two simulations with ground zero states, gzi and gzj , is defined as f f eatures(gzi ) sim(f, bestmatchj (f )). where bestmatchj (f ) is the best matching feature to f in the simulation with ground zero state, gzj . Temporal alignment parameters were set to be equal; t = t = 0.5. Variate alignment parameters were set to v = 0 and v = 0.1, to avoid
1

(b) features using the air (c) features using a rannetwork (clique) dom network Figure 3: Centers of features extracted using different networks (source = "NJ", trans. rate = 0.75 and rec. rate = 0.5) penalizing the wrong network alternative. We use the product merge function to combine alignment scores. Intuitively, large confusion implies poor differentiation power and, if the feature extraction process is effective, then we expect that (a) the overall confusion will be the lowest when using features extracted based on a network reflecting the underlying disease propagation, and (b) confusion will be the highest when we use an inappropriate network for feature extraction. Table 2 presents results for different transmission and recovery rates. As we see in this table, using the border network for feature extraction leads to least amount of confusion. Moreover, these results conform to our expectations listed above and Figure 3 helps see why: the (clique structured) air network ignores disease transmissions through land borders (especially when the transmission rate is too small for the flights to have a big impact on the epidemic's diffusion) and, thus, misses useful features. Random networks, on the other hand, result in significant noise.

Conclusions
In this paper, we presented our networks-of-traces model, which accounts for layers of disease networks (from local and global mobility patterns to contact networks), disease models, simulation and observation traces, and external interventions. The Networks-of-Traces for Epidemic Spread Simulations (NOTES2) system, based on this model, aims to assist experts in exploring large simulation trace data sets, through networks-of-traces feature analysis.

Unless otherwise stated, we use the default STEM parameters

82

References
Abubakar et al., I. 2012. Global perspectives for prevention of infectious diseases associated with mass gatherings. Lancet Infect Dis. 12(1):66­74. Balcan et al., D. 2009. Seasonal transmission potential and activity peaks of the new influenza a(h1n1): a monte carlo likelihood analysis based on human mobility. BMC Medicine 7(45). Barrett, C.; Eubank, S.; and Smith, J. 2005. If smallpox strikes portland. Scientific American 292(3). Blei, D. M., and Lafferty, J. D. 2006. Dynamic topic models. In ICML'06, 113­120. Carroll, J., and Chang, J.-J. 1970. Analysis of individual differences in multidimensional scaling via an n-way generalization of eckart-young decomposition. Psychometrika 35. Chao, D.; Halloran, M.; Obenchain, V.; and Longini Jr., I. 2010. FluTE, a publicly available stochastic influenza epidemic simulation model. PLoS Comput Biol 6(1). Chen, W.; Wang, C.; and Wang, Y. 2010. TScalable influence maximization for prevalent viral marketing in largescale social networks. In KDD'10. Colizza, V.; Barrat, A.; Barthelemy, M.; Valleron, A.; and Vespignani, A. 2007. Modeling the worldwide spread of pandemic influenza: baseline case and containment interventions. PLoS Comput Biol 4(1). Deodhar et al., S. 2014. An interactive, web-based high performance modeling environment for computational epidemiology. ACM TMIS 5(2):7:1­7:27. Ding et al., H. 2008. Querying and mining of time series data: experimental comparison of representations and distance measures. In VLDB08, 1542­1552. Ferguson, N.; Cummings, D.; Cauchemez, S.; Fraser, C.; Riley, S.; Meeyai, A.; Iamsirithaworn, S.; and Burke, D. 2005. Strategies for containing an emerging influenza pandemic in southeast asia. Nature 534(7046). Harshman, R. 1970. Foundations of the parafac procedure: Models and conditions for an explanatory multi-modal factor analysis. UCLA Working Pap. in Phonetics 16. Harvey, A. C., and Koopman, S. J. 1997. Multivariate structural time series models. System Dynamics in Economic and Financial Models 269­298. Kempe, D.; Kleinberg, J.; and Tardos, E. 2003. Maximizing the spread of influence through a social network. In KDD'03, 137­146. Keogh, E. 2002. Exact indexing of dynamic time warping. In VLDB02, 406417. Kim, J. H.; Candan, K. S.; and Sapino, M. L. 2012. Impact neighborhood indexing (ini) in diffusion graphs. In CIKM'12, 2184­2188. Leskovec et al., J. 2007. Cost-effective outbreak detection in networks. In KDD'07. Lowe, D. G. 2004. Distinctive image features from scaleinvariant keypoints. Int. J. Comput. Vision 60(2).

L.Ye, and E.Keogh. 2009. Time series shapelets: a new primitive for data mining. In KDD09. Merler, S., and Ajelli, M. 2014. The role of population heterogeneity and human mobility in the spread of pandemic influenza. Proc Biol Sci. 277(1681). Merler, S.; Ajelli, M.; Pugliese, A.; and Ferguson, N. 2011. Determinants of the spatiotemporal dynamics of the 2009 h1n1 pandemic in europe: implications for real-time modelling. PLoS Comput Biol. 7(9). Mills, T. C. 1990. Time Series Techniques for Economists. Cambridge University Press. Reinsel, G. 2003. Elements of Multivariate Time Series Analysis. Berlin-Heidelberg: Springer-Verlag. Shakarian, P.; Subrahmanian, V.; and Sapino, M. L. 2010. Using generalized annotated programs to solve social network optimization. In ICLP, 182­191. Silva, A.; Hyndman, R. J.; and Snyder, R. D. 2010. The vector innovation structural time series framework: a simple approach to multivariate forecasting. Statistical Modelling 10(4):353­374. STEM. 2014. The spatiotemporal epidemiological modeler project. http://www.eclipse.org/stem/. Tucker, L. 1966. Some mathematical notes on three-mode factor analysis. Psychometrika 31(3). Van den Broeck et al., W. 2011. The GLEaMviz computational tool, a publicly available software to explore realistic epidemic spreading scenarios at the global scale. BMC Infect Dis. 11(37). Watts, D., and Dodds, P. 2007. Influentials, networks, and public opinion formation. J. of Consumer Research 34(4). Wu et al., J. 2010. School closure and mitigation of pandemic (h1n1) 2009, hong kong. Emerg Infect Dis 16(3). Yu, D.; Yu, X.; Hu, Q.; Liu, J.; and Wu, A. 2007. Dynamic time warping constraint learning for large margin nearest neighbor classification. Inf. Sci. 18(13).

83

Hive Open Research Network Platform
Jung Hyun Kim, Xilun Chen,
K. SelÃ§uk Candan

Maria Luisa Sapino
Dipartimento di Informatica
Universita degli Studi di Torino
I-10149 Torino, Italy

Arizona State University
Tempe, AZ 85287, USA

{jkim294, xilun.chen, candan}@asu.edu

marialuisa.sapino@unito.it

ABSTRACT
Did you ever return back from a conference, having met a lot of
interesting folks, listened to many inspiring talks, or having your
presentation welcomed with a barrage of (of course, constructive!)
questions, wishing if only you managed to take record of all these
during the event? We are developing the Hive Open Research
Network1, a social platform for fostering scientific interactions
and reducing friction in scientific exchanges and the underlying
integrated services supporting content personalization, preview,
and social/scientific recommendations. Hive is a conferencecentric, but cross-conference platform, where researchers can seed
and expand their research networks, keep track of the technical
research sessions they are attending, meet new colleagues, share
their ideas, ask questions, give and receive comments, or simply
keep and/or view records of interactions at a conference they have
attended
(or wanted to attend, but missed due to other
commitments). In its core, Hive leverages dynamically evolving
knowledge structures, including user connections, concept maps,
co-authorship networks, content from papers and presentations,
and contextual knowledge to create and to promote networks of
peers. These peer networks support each other explicitly through
direct communication or indirectly through collaborative filtering.
Hive provides the following online integrated services: a)
understanding the personal activity context through access
patterns and analysis of user supplied content, b) context-aware
resource discovery, including search, presentation, and
exploration support within the scientific knowledge structures,
and c) peer discovery, and peer driven resource and knowledge
sharing and collaborative recommendations.

Figure 1. A screen shot from the MMâ11 edition of the
Hive open research network

1. INTRODUCTION
We demonstrate the Hive Open Research Network (Figure 1), a
social platform for fostering scientific interactions and reducing
friction in scientific exchanges and the underlying integrated
knowledge services for supporting content personalization,
preview, and social/scientific recommendations.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information filtering;
H.3.4 [Systems and Software]: User profiles and alert services;
H.3.5 [Online Information Services]: Data sharing, Web-based
services; H.3.7 [Digital Libraries]: Collection, Dissemination,
Systems issues, User issues; H.3.7 [Group and Organization
Interfaces]: Asynchronous interaction, Web-based interaction;

Hive is a conference-centric, yet cross-conference platform, where
researchers can seed and expand their research networks, keep
track of the technical research sessions they are attending, meet
new colleagues, share their ideas, ask questions, give and receive
comments, or simply keep and/or view records of interactions at a
conference they have attended (or wanted to attend, but missed
due to other commitments).

General Terms
Algorithms, Human Factors

Keywords
Social networks, Scientific networks, Peer discovery,
Recommendation, Collaborative filtering, Community discovery

Different from social networks, such as Facebook [2], Hive
focuses on professional research networks. In contrast to
professional social networks, such as LinkedIn [1], on the other
hand, it is (a) event centric, (b) content (presentations, data,
papers, posters) rich, and (c) research oriented. Hive also differs
from other conference services, such as Pathable [4] and
Iamresearcher [3], in many ways. Pathable focuses on a given
event and tries to make the physical scheduling at a conference
more convenient. Iamresearcher targets individual researchers or
conference organizers and helps them create research oriented

1
This work is supported by NSF Grant 1043583 âMiNC: NSDL
Middleware for Network- and Context-aware Recommendationsâ.

Copyright is held by the authors.
EDBT/ICDT '13, Mar 18-22 2013, Genoa, Italy
ACM 978-1-4503-1597-5/13/03.

733

and, when selected to be the active workpad, also gives
context to other recommendations Hive provides to Zach.

websites easily and conveniently. Hive, on the other hand, is not
event specific and focuses on peer network discovery and
management, as well as information sharing and dissemination
among researchers within and across conference events.
Concept
map
and
personalizati
on services

â¢ Learn key concepts to bootstrap concept
map from a given set of contextuallyrelevant documents

Peer
network
services

â¢ Select peer network
â¢ Locate similar peers (subject to peerâs
preferences)
â¢ Send request/reply to peers
â¢ Search peers and resources based on
context;
o alternative context representations that
the user can specify include concept
maps, access history, and knowledge
from peers (collaborative filtering)
â¢ Rank peers and resources based on context
â¢ Request resource recommendations based
on context
â¢ Relationship discovery and explanation
among peers and other resources
â¢ Community discovery and tracking
â¢ Generate summary previews and highlights
for updates and resources based on based
on context
â¢ Search and visualize personal, group, or
community activity history based on
current context

Discovery,
context- and
collaborativ
erecommenda
tion
and
preview
services

Personal
activity
history
services

Table 1. List of sample Hive services

â¢

In the first dayâs keynote presentation, Zach uses Hive to
follow the comment, question, and answer traffic, including
the related twitter activity. He notices that one of the
questions posted on Hive about the presentation is in fact in
his own research area and shares his opinion on the topic.

â¢

The next sessions in the conference are not in areas of direct
interest to Zach. But, Zach notices that a few of the
researchers he is following are checking-in into a session on
large scale graph processing. He also notices that a paper in
the session has cited a few of the papers he had cited in his
own work and that the author of the paper is a frequent coauthor of one of the researchers in his workpad. So Zach
decides to attend this session.

â¢

The presentation indeed raises his curiosity and he finds
himself posting a few questions about the details not clarified
in the presentation. A few minutes later, one of the coauthors of the presenter, who happens to follow the
discussion remotely, confirms his opinion. While this
exchange occurs in Hive, the exchange is also broadcasted in
twitter with the sessionâs hashtag. Zach adds this
presentation, the presenterâs avatar, and the link to the paper
into his âto investigate laterâ workpad. Zach also happily
notices that a few of the attendees of the session decide to
âfollowâ his questions.

â¢

In the break, Zach receives an update from Hive reporting
that there is already a question posted regarding the
presentation he had uploaded a few days back. He notices
that one of the researchers who were following him, named
Aaron, questions an equation he has included in one of his
slides. Indeed, when he checks the slides he notices that there
was a typo and he corrects the slide. Zach also sends a thank
you note to the researcher who highlighted the error in the
slide. After a brief exchange in which he learns that Aaron is
also working on a related problem, Zach sends a connection
request to Aaron and receives an acknowledgement a few
minutes later.

â¢

The next day, Zach presents his paper in an afternoon
session. After his presentation, the session chair takes a few
questions from the audience and then reads a few questions
posted on Hive, some of which through Twitter. Zach
answers to the questions as best as he could within the given
time limit. He then makes a note to answer these questions
on Hive in more detail after his presentation.

â¢

After his presentation, he notices that one of the audience
members, named Ann, who has asked questions to him had a
related paper in EDBT 2010, which he had cited and another
paper accepted to SIGMODâ13, which has not been
published yet. He also notes that she has check-in in a
session on community detection. He adds Annâs avatar to his
workpad and then goes to the session on community
detection.

â¢

Once he is back to the University, his advisor (who had
missed the conference due to another commitment) and Zach
spend some time in their first meeting, discussing his
activities and the connections he has made in EDBTâ13.

1.1 Use Scenario
A second year PhD student, Zach, is attending the EDBT 2013
conference to present a paper. He had also attended the last yearâs
edition of the same conference, where his advisor and groupmates
had a paper, as well as the ACM Sigmod 2012 conference, where
he has published his first paper on social media. In both
conferences, he had met and connected with researchers who were
working in topics that were of interest to him.
â¢

â¢

Before leaving for EDBTâ13, Zach logs in to Hive and
uploads his presentation slides; he also takes a quick look at
the already uploaded presentation slides of a few relevant
papers he had noticed in the published program. Zach also
checks which of the researchers who he had connected in the
past are coming to EDBTâ13. In addition, Hive proposes five
other researchers that Zach may want to connect during the
event and for each provides a list of sessions that the
researcher may most likely attend. Zach highlights the set of
researchers whose (session check-in, question, comment,
answer) activities he would like to follow and instructs Hive
to provide real-time updates regarding these during the
conference.
While Zach is checking the list of presentations in his
session, Hive reminds Zach that the chair of his session is
one of the authors whose paper he had cited in his Sigmod12
paper and that one of the other authors in the same session
was a co-author with his advisor a few years back. Zach
decides to follow these as well. In addition, he places the
avatars of these two researchers into his âsessionâ workpad
for quick access. This workpad serves both as a bookmark

734

Figure 3. Multiple integrated layers of the dynamic Hive
knowledge network
A preliminary version of Hive has been made available to the
attendees of the ACM Multimedia 2011 conference in November
2011 (this version of Hive is available to the public at
http://hive.asu.edu), and of ACM Sigmod 2012 conference in May
2012 (http://hive.asu.edu/sigmod12). Hive is also being used as a
learning support platform for the CS515 (graduate level) and
CS408 (senior level) courses at ASU, to encourage studentsâ
collaboration and studying material exchange.

Figure 2. Relationships between the users âK. Selcuk
Candanâ and âCarsten Griwodzâ are shown on the right
column
â¢
â¢
â¢
â¢
â¢
â¢

2. OVERVIEW of HIVE
Basic functionalities of Hive are built using JomSocial [5], a
Joomla-based platform for building social networks. These basic
functionalities include many of the Facebook like social features.

â¢

These relationships are contextualized based on userâs own
activity history, including the current session s/he is checked in as
well as the content of his or her active workpad (Figure 4). The
workpad interface is a tool to help the user keep record of the
things that attract his or her interest in the conference. When the
user sees a presentation that he/she likes, a question that tickles
his/her mind, or a colleague that she wants to connect later, she
can simply drag-and-drop the corresponding avatar into the
current workpad. The user can also name and save workpads and
can choose from different saved workpads, each corresponding to
a different context or state of mind. The content of the currently
active workpad defines the userâs activity context and all the
searches and recommendations are contextualized according to
this active workpad. The user can export workpads as collections
accessible to others or import a collection as active work pad.

Unlike other JomSocial-based social networks, however, Hive
also provides various knowledge rich services and functionalities.
In particular, we note that accessing scientific content effectively
requires a proper understanding of the personal activity context,
context-aware resource discovery, and peer-network driven
resource
and
knowledge
sharing
and
collaborative
recommendations. Therefore, in its core, Hive leverages
dynamically evolving knowledge structures, including concept
maps, co-authorship networks, content from papers and
presentation, and contextual knowledge to create and to promote
networks of peers (Figure 3).
These peer networks support each other explicitly through direct
communication or indirectly through collaborative filtering. Hive
provides the following online integrated services:
a)

understanding the personal activity context through
access patterns and analysis of user supplied content,

b)

context-aware resource discovery, including search,
presentation, and exploration support within the
scientific knowledge structures, and

For these knowledge management, content annotation, and peer
recommendation services, Hive relies on our Middleware for
Network- and Context-aware Recommendations (MiNC) engine,
which provides services that help minimize the extraneous load on
users, while they search, share, and access digital resources and
peer networks [8].

c)

peer discovery, and peer driven resource and knowledge
sharing and collaborative recommendations.

2.1 Understanding and Representing the
Personal Activity Context
The domain knowledge captured by the usage context includes
concepts, their significance in the learning domain, as well as the
strength of the inter-relationships between concepts and resources
[9]. To capture the structure of a knowledge domain as well as the
relationships between domain concepts, we leverage general
purpose ontologies as well as domain specific concept maps.

In particular, Hive uses the following evidences for discovering
and explaining relationships between individuals (peers) and for
recommending new peers or resources [6][7] (Figure 2):
â¢
â¢

co-authorship, direct citation, or indirect citation (e.g., citing
the same paper or transitive citation),
online âfollowâing,
conference participation (related conferences, same
conference different years, same conference same year),
session participation/check-in (related sessions or same
session/same time),
reciprocal question, comment, and answer activities,
user-provided content (publication, presentation, other
supporting material) similarity, and
activity (e.g. browsing, commenting) similarity.

profile and declared interest,
current and past affiliation, group membership,

735

techniques, which preserve maximal information while
minimizing the footprint of the reported information [13], and
context-aware snippet extraction algorithms [14].

2.4 Dynamic Peer-Networking and
Collaborative Recommendation Support

Figure 4. Two different work pads of the same user: the
work pads can contain many different types of
resources, including avatars of the users, presentations,
sessions, questions and answers, documents, and
collections. The work pads serve both as a book mark as
well as context for search and recommendations
To support services where the activity context is determined by
external materials, we apply novel concept map bootstrapping
algorithms that rely on user highlights, bookmarks, notes, or
documents. These algorithms, including [10], extract, in a semiautomated manner, dominant concepts and their relationships
specific to a given material.

Hive provides peer-network services in two ways: (a) peer
recommendation, where the system locates other peers with
similar interests or activity contexts and (b) peer-network based
resource recommendation. We have developed SCENT, an
innovative, scalable spectral analysis framework for internet scale
monitoring of multi-relational social media data, encoded in the
form of tensor streams. SCENT focuses on the computational cost
of structural change detection in tensor streams and extends
compressed sensing (CS) to tensor data. Through the use of
randomized tensor ensembles, SCENT is able to encode the
observed tensor streams in the form of compact descriptors and
detect significant changes in the underlying structure faster and
more accurately than the other methods 0.

3. CONCLUSIONS
In this paper, we demonstrate Hive, a social platform for fostering
scientific interactions and improving scientific exchanges. Hive is
a conference-centric, but cross-conference platform, which
leverages dynamically evolving knowledge structures, including
user connections, concept maps, co-authorship networks, content
from papers and presentation, and contextual knowledge to create
and to promote networks of peers.

4. ACKNOWLEDGMENTS
We thank Mijung Kim and Profs. Hari Sundaram and Hasan
Davulcu for their inputs on Hive and the underlying Middleware
for Network- and Context-aware Recommendations (MiNC).

2.2 Network Layer Alignment and
Integration

5. REFERENCES

Hive uses the multiple context layers of the âcontext networkâ,
shown in Figure 3, in an integrated manner search and
recommendation. Integration of layers starts with an alignment
phase, which requires identification of mappings between
concepts and relationships among different layers. In Hive, since
the original layers are likely to match partially and since layers
can conflict or reinforce each other, the result of the alignment
process is imprecise. For example, structures of the underlying
concept-maps and concept significances may differ from one
context layer to the other. The Hive services that rely on these
layers function in the presence of such imperfect alignments. For
weighted graph data management, Hive relies on our R2DB,
weighted RDF data management system [11][12].

2.3 Context-Aware Resource Discovery,
Search, and Exploration

[1]

http://www.linkedin.com

[2]

http://www.facebook.com/

[3]

http://www.iamresearcher.com

[4]

http://www.pathable.com/

[5]

http://www.jomsocial.com/

[6]

J.H. Kim, K.S. Candan, and M. L. Sapino. Impact Neighborhood
Indexing (INI) in Diffusion Graphs. Proc. CIKMâ12.

[7]

K.S. Candan, W.-S. Li: Reasoning for Web Document Associations
and its Applications in Site Map Construction. DKE 43(2), 2002.

[8]

http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=10
43583

[9]

J. W.Kim, K. S. Candan, J. Tatemura. Efficient Overlap and Content
Reuse Detection in Blogs and Online News Articles. WWWâ09

[10] L. Di Caro, K. S. Candan, and M. L. Sapino. Navigating within
News Collections using Tag-Flakes. J. Vis. Lang. and Comp., 2011.

Hive relies on the underlying integrated context network to filter,
summarize, and rank alternatives and adapt according to their
relevance. Context-aware ranking and preview services include
(a) relevant snippet extraction from documents, (b) key concept
extraction for automated annotations, and (c) content
summarization documents and update reports. Once all the
concepts are extracted and ranked (based on the context), Hive
propagates the concepts within the relevant neighborhoods of the
knowledge network using adaptation strategies, based on the
current active context (defined by the workpad). These
annotations are then used for ranking, recommendations, and
summarization tasks. Summarization of the scheduled update
reports are performed relying on hierarchical table summarization

[11] J. P. CedeÃ±o and K. S. Candan. R2DF framework for ranked path
queries over weighted RDF graphs. Proc.WIMS, 2011.
[12] S. Huang, X. Li, J. P. CedeÃ±o, K. Se. Candan, M. L. Sapino,
Representing and Querying Weighted Semantic Web with R2DB.
Under review for demonstration in EDBTâ13
[13] K. S. Candan, H. Cao, Yan Qi, M. L. Sapino: AlphaSum: sizeconstrained table summarization using value lattices. EDBTâ09.
[14] Q. Li, K. S. Candan, Y. Qi. Extracting Relevant Snippets for Web
Navigation. AAAI 2008.
[15] Yu-Ru Lin, K. S. Candan, H. Sundaram, L. Xie. SCENT: Scalable
Compressed Monitoring of Evolving Multi-Relational Social
Networks. ACM TOMCCAP, 7:29, 2011

736

PageRank Revisited: On the Relationship between Node
Degrees and Node Significances in Different Applications
Jung Hyun Kim

K. SelÃ§uk Candan

Maria Luisa Sapino

Arizona State University
Tempe, AZ 85287, USA

Arizona State University
Tempe, AZ 85287-8809

University of Torino
I-10149 Torino, Italy

jkim294@asu.edu

candan@asu.edu

marialuisa.sapino@unito.it

ABSTRACT
Random-walk based techniques, such as PageRank, encode the
structure of the graph in the form of a transition matrix of a stochastic process from which significances of the graph nodes can be inferred. Recommendation systems leverage such node significance
measures to rank the objects in the database. Context-aware recommendation techniques complement the data graph with additional
data that provide the recommendation context. However, despite
their wide-spread use in many graph-based knowledge discovery
and recommendation applications, conventional PageRank-based
measures have various shortcomings. As we experimentally show
in this paper, one such shortcoming is that PageRank scores are
tightly coupled with the degrees of the graph nodes, whereas in
many applications the relationship between the significance of the
node and its degree in the underlying network may not be as implied by PageRank-based measures. In fact, as we also show in
the paper, in certain applications, the significance of the node may
be negatively correlated with the node degree and in such applications a naive application of PageRank may return poor results.
To address these challenges, in this paper, we propose degree decoupled PageRank (D2PR) techniques to improve the effectiveness
of PageRank based knowledge discovery and recommendation systems. These suitably penalize or (if needed) boost the transition
strength based on the degree of a given node to adapt the node significances based on the network and application characteristics.

1.

INTRODUCTION

In recent years, there has been considerable interest in measuring
the significance of a node in a graph and relatedness between two
nodes in the graph, as if measured accurately, these can be used
for supporting many knowledge discovery, search, and recommenâ
This work is supported by NSF Grants 1339835 âE-SDMS: Energy Simulation Data Management System Softwareâ, 1318788 âData Management for Real-Time Data Driven Epidemic Spread Simulationsâ, 1518939
âRAPID: Understanding the Evolution Patterns of the Ebola Outbreak in
West-Africa and Supporting Real-Time Decision Making and Hypothesis
Testing through Large Scale Simulationsâ, and 1430144 âFraud Detection
via Visual Analytics: An Infrastructure to Support Complex Financial Patterns (CFP) based Real-Time Services Deliveryâ.

â

dation tasks [1, 7, 9, 12, 26]. The significance of a node in a given
graph often needs to reflect the topology of the graph. Measures
like the betweenness measure [27] and the centrality/cohesion [5],
help quantify how significant any node is on a given graph based on
the underlying graph topology. The betweenness measure [27], for
example, quantifies whether deleting the node would disconnect or
disrupt the graph. Centrality/cohesion [5] measures quantify how
close to a clique the given node and its neighbors are. Other authority, prestige, and prominence measures [1, 5, 6] quantify the
significance of the node through eigen-analysis or random walks,
which help measure how reachable a node is in the graph.

1.1 PageRank as a Measure of Significance
Since enumerating all paths among the graph nodes would require time exponential in the size of the graph, random-walk based
techniques encode the structure of the network in the form of a transition matrix of a stochastic process from which the node significance can be inferred.PageRank [6] is one of the most widely-used
random-walk based methods for measuring node significance and
has been used in a variety of application domains, including web
search, biology, and social networks. The basic thesis of PageRank is that a node is important if it is pointed to by other important
nodes â it takes into account the connectivity of nodes in the graph
by defining the score of the node vi â V as the amount of time
spent on vi in a sufficiently long random walk on the graph. More
specifically, given a graph G(V, E), the PageRank scores are represented as ~r, where
~r = Î±TG~r + (1 â Î±)~t
where TG is a transition matrix corresponding to the graph G, ~t is
a teleportation vector (such that ~t[i] = kV1 k ), and Î± is the residual
probability (or equivalently, (1 â Î±) is the so-called teleportation
probability). Unless the graph is weighted, the transition matrix,
TG , is constructed such that for a node v with k (outgoing) neighbors, the transition probability from v to each of its (outgoing)
neighbors will be 1/k. If the graph is weighted, then the transition probabilities are adjusted in a way to account for the relative
weights of the (outgoing) edges.

1.2 Tight Coupling of PageRank Scores of
Nodes and their Degrees
Let us consider an undirected graph G(V, E). There are two
factors that contribute to the PageRank of a given node, v â V :

c 2016, Copyright is with the authors. Published in the Workshop Pro
ceedings of the EDBT/ICDT 2016 Joint Conference (March 15, 2016, Bordeaux, France) on CEUR-WS.org (ISSN 1613-0073). Distribution of this
paper is permitted under the terms of the Creative Commons license CCby-nc-nd 4.0

â¢ Factor 1: Significance of Neighbors: The more significant
the neighbors of a node are, the higher its likelihood to be
also significant.
â¢ Factor 2: Number of Neighbors (Degree of the Node) : Even
if the neighbors are not all significant, a large number of

Data Set
Correlation between
PageRank and Degree

Listener Graph
(Friendship
edges, Last.fm)
0.988

Article Graph
(co-author
edges, DBLP)
0.997

Movie Graph
(co-contributor
edges, DBLP)
0.848

Table 1: Spearmanâs rank correlation between the node degree
ranks and the node ranksâ based on PageRank scores for various data graphs (see Section 4 for details of the data sets)
neighbors would imply that the node, v, is well-connected
and, thus, likely to be structurally important.
In theory, these two factors should complement each other. In practice, however, the PageRank formulation described above implies
that there is a very tight coupling between the degrees of the nodes
in the graph and their PageRank scores (see Table 1).

1.2.1 Problem I: When a Large Node Degree Does
Not Indicate High Node Significance
In this paper, we highlight (and experimentally show) that,
in many applications, node degree and node significance are in fact
inversely related and that the tight-coupling between node degrees
and PageRank scores might be counter-productive in generating accurate recommendations.
E XAMPLE 1. Consider, for example, a recommendation application where a movie graph, consisting of movie and actor
nodes, is used for generating movie recommendations. In this application, the first factor (significance of neighbors) clearly has a
positive contribution: a movie with good actors is likely to be a
good movie and an actress playing in good movies is likely to be
a good actress. On the other hand, the second factor (number of
neighbors) may in fact be a negative contributor to node significance: the fact that an actor has played in a large number of
movies may be a sign that he is a non-discriminating (âB movieâ)
actor, whereas an actress with relatively fewer movies may be a
more discriminating (âA movieâ) actress.
As we see in Section 4, this observation turns out to be true in many
applications, where (a) acquiring additional edges has a cost that is
correlated with the significance of the neighbor (e.g. the effort one
needs to invest to a high quality movie) and (b) each node has a
limited budget (e.g. total effort an actor/actress can invest in his/her
work).

1.2.2 Problem II: When PageRank Does Not Sufficiently Account for Contributions of Degrees
The mismatch between PageRank and node significance is not
limited to the cases where node degrees are inversely related to the
node significance. As we see in Section 4, there are other scenarios
where PageRank may, in fact, fail to sufficiently account for the
contribution of the node degrees to their significances.

1.3 PageRank Revisited: De-coupling Node
Significance from Node Degrees
As we discussed above, one key shortcoming of the conventional
PageRank scores is that they are often tightly coupled with the degrees of the graph nodes and in many applications the relationship
between the significance of the node and its degree in the underlying network may not be as implied by PageRank-based measure: in
certain applications, the significance of the node may be negatively
correlated with the node degree, whereas in others PageRank may
not be sufficient in accounting for degree contributions. Naturally,
in such applications a naive application of PageRank in generating
recommendations may return poor results.
To address these challenges, in this paper, we propose degree decoupled PageRank (D2PR) techniques to improve the effectiveness

of PageRank based knowledge discovery and recommendation systems. These techniques suitably penalize or (if needed) boost1 the
transition strength based on the degree of a given node to adapt the
node significances based on the network and application characteristics. This paper is organized as follows: Next, we discuss the
related literature. In Sections 3, we introduce the proposed degreedecoupled PageRank techniques. We evaluate the proposed techniques in Section 4 and conclude in Section 5.

2. RELATED WORKS
2.1 Context-Sensitive PageRank
Path-length based definitions of node relatedness, such as those
proposed by [4, 24] help capture the relatedness of a pair of nodes
solely based on the properties of the nodes and edges on the shortest
path between the pair. Random-walk based definitions, such as
hitting distance [10,21] and personalized page rank (PPR) score [1,
9, 16], of node relatedness further take into account the density of
the edges: as in path-length based definitions, random-walk based
definitions also recognize that a node is more related to another
node if there are short paths between them; however, random walkbased definitions of relatedness also consider how well the given
pair of nodes are connected.
In [7], authors construct a transition matrix, TS , where edges
leading away from the seed nodes are weighted less than those
edges leading towards the seed nodes. An alternative approach for
contextualizing PageRank scores is to use the PPR techniques [1,9]
discussed in the introduction. One key advantage of this teleportation vector modification based approach over modifying the
transition matrix, as in [7], is that the term Î± can be used to directly control the degree of seeding (or personalization) of the PPR
score. [10, 21] rely on a random walk hitting time based approach,
where the hitting time is defined as the expected number of steps a
random walk from the source vertex to the destination vertex will
take. [17] leveraged these properties of PPR to develop localitysensitive algorithms to rank nodes of graphs which are relative to a
given set of seed nodes efficiently.

2.2 Improvements to the PageRank Function
Due to the obvious relationship between ranking and monetary
rewards (e.g. through selling of advertisements on web search applications), there has been considerable effort in engineering (or
manipulating) graphs in a way to maximize ranking scores of particular nodes. This is commonly referred to as PageRank optimization. One way to achieve this goal is carefully adding or removing
certain links: If, for example, one or more colluding webmasters
can add or remove edges, PageRank scores of target web pages
or domains can be increased [23]. [20] established several bounds
indicating to what extent the rank of the pages of a website can
be changed and the authors derived an optimal referencing strategy to boost PageRank scores. A related, but opposite, problem is
to protect the PageRank scores against negative links (which may
indicate, for example, negative influence or distrust in a social network), artificial manipulation, and spam. [3], for example, focused
on identifying spam pages and link farms and showed that better
PageRank scores can be obtained after filtering spam pages and
links. In [14], authors show that PPR algorithms that do not differentiate among the seed nodes may not properly rank nodes and
present robust personalized PageRank (RPR) strategies, which are
insensitive to noise in the set of seed nodes.
1

In this context, de-coupled does not necessarily imply decorrelated. In fact, D2PR can boost correlation between node degree and PageRank if that is required by the application.

There are some efforts to change the impact of degrees on the
PageRank computation. [2] proposed a way to boost the power of
low-degree nodes in a network. The impact from nodes which are
important but are not hubs is relatively small compared to other
nodes which are less important with high degrees. To boost the
low-degree important nodes for equal opportunity, the teleportation
vector is modified with being proportional to the degrees of nodes.
[11] boosted the degrees of nodes to reduce the expected cover time
of the entire graph by the biassed random-walk.

3.

DEGREE DE-COUPLED PAGERANK

The key difficulty of de-coupling node degrees from the PageRank scores is that the definition of the PageRank, based on random
walk transitions, is inherently dependent on the number of transitions available from one node to the other. As we mentioned above,
the more ways there are to reach into a node, the higher will be its
PageRank score.

3.1 Desideratum
Therefore, to de-couple the PageRank score from node degrees,
we need to modify the transition matrix. In particular, for each node
vi in the graph, we would like to be able to control the transition
process with a single parameter (p), such that
â¢ if p âª â1, transitions from node vi are â¼ 100% towards
the neighbor with the highest degree,
â¢ if p = â1, transition probabilities from node vi are proportional to the degrees of its neighbors,
â¢ if p = 0, the transition probabilities mirror the standard
PageRank probabilities (assuming undifferentiated neighbors),
â¢ if p = 1, transition probabilities from node vi are inversely
proportional to the degrees of its neighbors,
â¢ if p â« 1, transitions from node vi are â¼ 100% towards the
neighbor with the lowest degree.
In other words, the transition function should de-couple the transition process from node-degrees and penalize or boost the contributions of node degrees in the transition process, as needed.

3.2 Degree De-coupling Transition Matrix
In this subsection, we will consider degree de-coupling of the
transition matrix as implied by the above desideratum.

3.2.1 Undirected Unweighted Graphs
Let G = (V, E) be an undirected and unweighted graph. Let
Î± also be a given residual probability parameter, and deg(v) be a
function which returns the number of edges on the node v. We
represent degree de-coupled PageRank (D2PR) scores in the form
of a vector
d~ = Î±TD d~ + (1 â Î±)~t,
where ~t is the teleportation vector, such that ~t[i] =
and TD is a degree de-coupled transition matrix,
TD (j, i) = P

1
kV k

deg(vj )âp
,
âp
vk âneighbor(vi ) deg(vk )

for all i

(1)

where
â¢ TD (j, i) denotes the degree de-coupled transition probability from node vi to node vj over an edge eij = [vi â vj ]
when there exists at least one edge between two nodes,
â¢ neighbor(vi ) is the set of all neighbors of the source node,
vi , and

A

B

E

C

F

D

(a) A sample graph

Dest.
vj

deg.
(vj )

B
C
D

2
3
1

Transition probability
from A to its neighbors vj
p=0
2
â2
0.33
0.18
0.29
0.33
0.08
0.64
0.33
0.74
0.07

(b) Transition probabilities from A

Figure 1: In conventional PageRank (p = 0), the transition
probabilities from node vi = A to all its neighbors vj are the
same. In degree de-coupled PageRank (D2PR), the value of p
can be used to penalize (p > 0) or boost (p < 0) transition
probabilities based on the degree of the destination
node
id
53608
351
...
79538
79917

node
degree
883
739
...
1
1

Ranks of the graph nodes
for different de-coupling weights (p)
â4
â2
0
2
4
1
1
69
5549
6793
2
12
425
1992
1935
...
...
...
...
...
7661
7545
4149
195
182
7793
7790
7522
2443
2043

Table 2: Ranks of graph nodes of different degrees on a sample graph for different de-coupling weights, p: as we see in this
figure, when p > 0, high degree nodes are pushed down in the
rankings (reducing the correlation between degree and rank),
while when p < 0, they are pulled up (improving the correlation between degree and rank)
â¢ p â R is a degree de-coupling weight.
Intuitively, the numerator term, deg(vj )âp , ensures that the edge
incoming to vj is weighted by its degree: if p > 0, then its degree negatively impacts (reduces) transition probabilities into vj , if
p < 0 then its degree positively impacts (boosts2 ) transition probabilities into vj , and if p = 0, we obtain the standard PageRank
formulation without degree de-coupling. In other words, the transition function satisfies our desideratum of de-coupling the transition process from node-degrees and penalizing or boosting the
contributions of node degrees on-demand. Note that, since all
transitions from the node vi are degree de-coupled individually
based
on the degrees of their destinations, the denominator term,
P
âp
, ensures that the transition probabilvk âneighbor(vi ) deg(vk )
ities from node vi add up to 1.0. Note also that when there is no
edge between node vi and vj , TD (j, i) = 0 and, consequently, the
term TD (j, i) is not affected by the degree de-coupling process.
E XAMPLE 2. Figure 1 shows how the random walk probabilities are differentiated in a degree de-coupled transition matrix on
a sample graph where a node A has three neighbors, B (with degree 2), C (with degree 3), and D (with degree 1). In conventional PageRank, the transition probabilities from node A to all its
neighbor nodes are equal to 0.33. In degree de-coupled PageRank
(D2PR), however, the value of p is used for explicitly accounting
for the impact of node degree on the transition probabilities: When
p = 2, the transition probabilities from A to its neighbors are 0.18,
0.08, and 0.74, which penalizes nodes which have larger degrees,
whereas when p = â2, D2PR boosts the transition probabilities
to large degree nodes leading to transition probabilities 0.29, 0.64,
and 0.07, respectively.
â
This example shows that, in degree de-coupled PageRank
(D2PR), as we also see in Table 2, the value of p can be used to
penalize (p > 0) or boost (p < 0) transition probabilities based on
the degree of the destination, vj .
2
In fact, a similar function was used in [11] to quickly locate nodes
with higher degrees in a given graph.

3.2.2 Directed Unweighted Graphs
The semantics of degree de-coupling is slightly different in directed graphs. In particular, edges incoming to vi often do not require a particular effort from vi to establish and hence are often out
of the control of vi , but indicate a certain degree of interestingness,
usefulness, or authority as perceived by others. The same is not
true for edges outgoing from vi ; in particular, a vertex with a large
number of outgoing edges may either indicate a potential hub or
simply indicate a non-discerning connection maker. The distinction
between these two situations gains importance especially in applications where establishing a new connection has a non-negligible
cost to the source node and, thus, a large number of outgoing edges
may indicate either (a) a very strong participant to the network or
(b) a very poor participant with a large number of weak linkages.
Let G = (V, E) be a directed graph and for the simplicity of the
discussion, without any loss of generality, let us assume that G is
unweighted. Let us also be given a residual probability parameter,
Î± and let outdeg(v) be a function which returns the number of
outgoing edges from the node v. The degree de-coupled PageRank
~ d~ =
(D2PR) scores can be represented in the form of a vector d,
~
Î±TD d + (1 â Î±)~t, where ~t is the teleportation vector, such that
~t[i] = 1 for all i and
kV k
TD (j, i) = P

outdeg(vj )âp
,
âp
[vi âvk ]âout_edges(vi ) outdeg(vk )

where TD (j, i) denotes the degree de-coupled transition probability from node vi to node vj over an edge eij = [vi â vj ],
out_edges(vi ) is the set of out-going edges from the source node,
vi , and p â R is a degree de-coupling weight.
E XAMPLE 3. Figure 2 (a) in Section 4 provides an example illustrating the correlations between the degree de-coupled PageRank (D2PR) scores and external evidence for different values of p
for some application: here, the higher the correlation, the better resulting ranking reflects the application semantics. As we see in this
example, which we will investigate in greater detail in Section 4,
the optimal de-coupling weight is not always p = 0 as implied by
the conventional PageRank measure. In this particular case, for
example, the correlation between D2PR and external evidence of
significance is maximized when the de-coupling weight, p, is equal
to 0.5, implying that in this application a moderate degree of penalization based on the node degrees is needed to align PageRank
scores and application semantics.
â

3.2.3 Weighted Graphs
Once again, the semantics of degree de-coupling need to be reconsidered for weighted graphs. Let G = (V, E, w) be a directed,
weighted graph, where w(e) is a function which returns the weight
of the edge associated with edge e. It is important to note that, in
such a graph, the weight of an edge can 1) indicate the strength
of the connection between two nodes (thus positively contributing
to the significance of the destination node); and at the same time
and 2) contribute to the degree of a node as a multiplier (thus positively or negatively contributing to the node significance depending
on the degree-sensitivity of the application). In other words, given
an edge eij = [vi â vj ], from node vi to node vj , the transition
probability from vi to vj can be written as
T(j, i) = Î²Tconn_strength (j, i) + (1 â Î²)TD (j, i),
where
Tconn_strength (j, i) = P

w(vi â vj )
,
w(vi â vh )

[vi âvh ]âout_edges(vi )

accounts for the connection strength (as in the conventional PageRank) whereas TD is a degree de-coupled transition matrix,
TD (j, i) = P

Î(vj )âp
[vi âvk ]âout_edges(vi )

Î(vk )âp

,

such that, TD (j, i) denotes the degree de-coupled transition probability from node vi to node vj over an edge eij = [vi â vj ],
p â R is a degree de-coupling weight, and
X
w(v â vh ).
Î(v) =
[vâvh ]âout_edges(v)

Note that, above, Î² controls whether accounting for the connection strength or degree de-coupling is more critical in a given application. In Section 4, we will study the impact of degree de-coupling
in weighted graphs for different scenarios.

4. CASE STUDIES
In this section, we present case studies assessing the effectiveness of the degree de-coupling process and the relationship between
the degree de-coupling weight p and recommendation accuracy for
different data graphs.

4.1 Setup
For all experiments, the degree de-coupling weight, p, is varied
between -4 and 4 with increments of 0.5. The residual probability,
Î±, is varied between 0.5 and 0.9, with default value chosen as 0.85.
We also varied the Î² parameter, which controls whether accounting
for the connection strength or degree de-coupling is more critical
in a given application, between 0.0 and 1.0, with the default value
set to 0 (indicating full decoupling).

4.1.1 Datasets
Four real data sets are used for the experiments. Each data set
is used to create two distinct data graphs and corresponding ratings
data. Table 3 provides further details about the various graphs created using these four data sets. These recommendation tasks based
on these data graphs are detailed below:
â¢ For the IMDB [15] data set, we created (a) a movie-movie graph,
where movie nodes are connected by an edge if they share common
contributors, such as actors, directors, writers, composers, editors,
cosmetic designers, and producers and (b) an actor-actor graph
based on whether two actors played in the same movie. Applications: For this data set, we consider applications where movies
are rated by the users: thus, we merged the IMDB data with the
MovieLens 10M [22] data (based on movie names) to identify user
ratings (between 1 and 5) for the movies in the graph. We consider the (a) average user rating as the significance of the movies
in the movie-movie graph and (b) average user rating of the movies
played in as the significance of the actors in the actor-actor graph.
â¢ For the DBLP [26] data set, we constructed (a) an article-article
graph where scientific articles were connected to each other if they
shared a co-author and (b) an author-author graph based on coauthorship. Applications: (a) In the article-article graph, the number of citations to an article is used to indicate its significance. Similarly, (b) in the author-author graph, average number of citations
to an authorâs papers is used as his/her significance.
â¢ For the Last.fm [18], we constructed (a) a listener-listener graph,
where the nodes are Last.FM listeners and undirected edges reflect
friendship information among these listeners. We also constructed
(b) an artist-artist graph based on shared listeners. Applications:
(a) In the listener-listener graph, we considered the total listening

Data

Graph

IMDB

movie-movie
actor-actor
article-article
author-author
listener-listener
artist-artist
commenter-commenter
product-product

DBLP
Last.fm
Epinions

# of
nodes
191,602
32,208
8,808
47,252
1,892
17,626
6,703
13,384

# of
edge
4,465,272
2,493,574
951,798
310,250
25,434
2,640,150
2,395,176
2,355,460

Average
node degree
23.30
77.42
108.06
6.57
13.44
149.79
425.05
175.99

Standard deviation of
node degrees
51.86
67.15
171.25
8.89
17.31
299.66
438.97
224.12

Median standard deviation of
neighborsâ node degrees
2.89
114.41
309.92
6.39
22.37
998.53
609.39
202.78

Table 3: Data sets and data graphs
activity of a given listener as his/her significance. (b) In the artistartist graph, the number of times an artist has been listened is considered as his/her significance.
â¢ For the Epinions [25]: We constructed (a) a commentercommenter graph based on the products on which two individuals
both commented and (b) a product-product graph based on shared
commenters. Applications: (a) For the nodes on the commentercommenter graph, the number of trusts the commenter received
from others is used as his/her commenter significance. (b) For
each product in the product-product graph, its average rating by
the commenters is used as its node significance.

4.2 Measures
In this section, our goal is to observe the impact of different D2PR degree de-coupling weights on the relationship between
D2PR rankings and application specific significance measures for
the above data sets3 . We also aim to verify whether de-coupling
weights can also be used to improve recommendation accuracies.
In order to measure the relationship between the degree decoupled PageRank (D2PR) scores and the application-specific node
significance, we used Spearmanâs rank correlation,
P
(xi â xÌ)(yi â yÌ)
pP i
,
P
2
2
i (xi â xÌ)
i (yi â yÌ)

which measures the agreement between the D2PR ranks of the
nodes in the graph and their application-specific significances.
Here, x are rankings by D2PR and y are significances for an application and xÌ and yÌ are averages of two values.

4.3 Impact of De-Coupling in Different Applications (Unweighted Graphs)
In this subsection, we present results that aim to assess D2PR under the settings described above. For these experiments, the residual probability, Î±, and the parameter, Î², are set to the default values, 0.85 and 0, respectively. In these experiments, we consider
only unweighted graphs (we will study the weighted graphs and
the impact of parameter Î² later in Section 4.5).
Figures 2 through 4 include charts showing the Spearmanâs correlations between the D2PR ranks and application specific node
significances for different values of p and for different data graphs.
These figures clearly illustrate that different data graphs require different degrees of de-coupling4 to best match the application specific
node significance criterion.

4.3.1 Application Group A: When Degree Penalization Helps
The actor-actor (based on common movies) and commentercommenter (based on common products) graphs have highest correlation at p = 0.5, with the correlations dropping significantly
3
In this paper, we are not proposing a new PageRank computation
mechanism. Because of this (and since the focus is not improving
scalability of PR), we do not report execution times and compare
our results with other PageRank computation mechanisms.
4
Degree penalization or degree-based boosting

when the degrees are over-penalized (i.e., when p â« 0.5). The
Epinions product-product graph (based on common commenters,
Figure 2(c)) also provides the highest correlations with p > 0, but
behaves somewhat differently from the other two cases: the correlations stabilize and do not deteriorate significantly when degrees
are over-penalized, indicating that the need for degree penalization
is especially critical in this case: this is due to the fact that, the
larger the number of comments a product has, the more likely it
is that the comments are negative (Figure 5). In fact, we see that,
among the three graphs, this is the only graph where the traditional
PageRank (with p = 0) leads to negative correlations between
node ranks and node significances.
These results indicate that actors who have had many co-actors,
commenters who commented on products also commented by
many others, or products which received comments from individuals who also commented on many other products are not good
candidates for transition during random walk. This aligns with
our expectation that, in applications where each new movie role or
comment requires additional effort, high degree may indicate lower
per-movie or per-comment effort and, hence, lower significance.

4.3.2 Application Group B: When Conventional
PageRank is Ideal
Figure 3 shows that, for movie-movie (based on common actors)
and author-author (based on common articles) graphs, the peak
correlation is at p = 0 indicating that the conventional PageRank
which gives positive weight to node degree, is appropriate.
This perhaps indicates that movies with a lot of actors tend to
be big-budget products and that authors with a large number of coauthors tend to be experts with whom others want to collaborate.
Note that, in these applications, additional boosting, with p < 0,
negatively affects the correlation, indicating that the relationship
between node degree and significance is not very strong (Figure 5).
The quick change when p < 0 is because, as we see in Table 3,
median standard deviations of neighborsâ degrees are low; i.e., degrees of neighbors of a node are comparable: there is no dominant
contributor to TD (j, i) in Equation 1 (Section 3) and, thus, the
transition probabilities are sensitive to changes in p, when p < 0.

4.3.3 Application Group C: When Degree Boosting
Helps
Figure 4 shows that there are scenarios where additional boosting based node degrees provides some benefits. The article-article
(based on common authors), listener-listener (based on common
artists), and artist-artist (based on common listeners) graphs reach
their peaks around p â¼ â1, indicating that these also benefit from
large node degrees though improvements over p = 0 are slight.
A significant difference between applications in Group B and
Group C is that, for p < 0, the correlation curve is more or less
stable. This is because, as we see in Table 3, in these graphs median
standard deviations of neighborsâ degrees are high: in other words,
for each node, there is a dominant neighbor with a high degree and
this neighbor has the highest contribution to TD (j, i); thus, the
rankings are not very sensitive to p, when p < 0.

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*8BC;DE*$27,%1$27,%*
3'5#4067#/*0%$"69**

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*8B"4'4,')C*2,&&#'7#%1
2,&&#'7#%*3'5#4067#/*0%$"69*

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*8B"4'4,')C*"%,/3271"%,/327*
3'5#4067#/*0%$"69*
"#"$%

!"#$%&$'()*+,%%#-$.,'*

!"&'#

!"!%#

!"!$#

($#

()"*#

()#

('"*#

('#

(&"*#

!"!!#
(!"*# !#

(&#

!"*#

&#

&"*#

'#

'"*#

)#

)"*#

"#"&%
!(%

!'#)%

!'%

!$#)%

!$%

!&#)%

!&%

!"#)%

"%

"#)%

&%

&#)%

$%

$#)%

'%

'#)%

(%

/#0%##*/#12,3"-4'0*5#4067*8"9*

$#

"#"&%

!&%

!'#(%

!'%

!)#(%

!)%

!*#(%

!*%

"#""%
!"#(% "%

+,'F#'.,'$-*8"GH9*

"#(%

*%

*#(%

)%

)#(%

'%

'#(%

&%

'#(%

$%

/#0%##*/#12,3"-4'0*5#4067*8"9*
!"#"&%

!"#"$%

!"#"$%

/#0%##*/#12,3"-4'0*5#4067*8"9*
;#0%##*/#12,3"-#/*

!"#$%&$'()*+,%%#-$.,'*

!"#$%&$'()*+,%%#-$.,'*

"#"'%

;#0%##*/#12,3"-#/*

+,'D#'.,'$-*8"EF9*

;#0%##*/#12,3"-#/*

+,'D#'.,'$-*8"EF9*

(a) IMDB (actor-actor)
(b) Epinions (commenter-commenter)
(c) Epinions (product-product)
Figure 2: Application Group A: p > 0 is optimal (i.e., node degrees need to be penalized)
+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*8;BC=D*$376,%1$376,%*
3'5#4067#/*0%$"69**

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*8BC;DE*&,F4#1&,F4#*
3'5#4067#/*0%$"69*
"#"(&

!"#$%&$'()*+,%%#-$.,'*

!"#$%&$'()*+,%%#-$.,'*

"#$%

"#&%

!'%

!(#)%

!(%

!$#)%

!$%

!&#)%

!&%

"#"%
!"#)% "%

"#)%

&%

&#)%

$%

$#)%

(%

'%

(#)%

/#0%##*/#12,3"-4'0*5#4067*8"9*
!"#&%

!(&

!)#*&

!)&

!%#*&

!%&

!$#*&

"#""&
!"#*& "&

!$&

"#*&

$&

$#*&

%&

%#*&

)&

)#*&

(&

/#0%##*/#12,3"-4'0*5#4067*8"9*
!"#"(&

!"#"'&

!"#$%

!"#$%&

;#0%##*/#12,3"-#/*

+,'E#'.,'$-*8"FG9*

;#0%##*/#12,3"-#/*

+,'F#'.,'$-*8"GH9*

(a) DBLP (author-author)
(b) IMDB (movie-movie)
Figure 3: Application Group B: p = 0 is optimal
01++)2*31-%1?%@#AB%B*-C/%*-4%D14)%'95-9E7*-7)%%=F*/<"?,G%29/<)-)+629/<)-)+%
8-:)95;<)4%5+*(;>%

01++)2*31-%1?%@ABC%C*-D/%*-4%E14)%'95-9F7*-7)%=@GHBI%*+372)6*+372)%
8-:)95;<)4%5+*(;>%%

"#"&%

!'%

!(#&%

!(%

!)#&%

!)%

!$#&%

!$%

"#""%
!"#&% "%

"#&%

$%

$#&%

)%

)#&%

(%

(#&%

'%

4)5+))%4)6718(29-5%:)95;<%=(>%
!"#"&%

"#$%

"#&%

!'%

!(#)%

!(%

!$#)%

!$%

!&#)%

!&%

"#"%
!"#)% "%

"#)%

&%

&#)%

$%

$#)%

4)5+))%4)6718(29-5%:)95;<%=(>%
!"#&%

!"#$"%

(%

(#)%

'%

()*+,-+./0&12,,*3+42.&

!"#!#%

'()*+,*-./%01++)2*31-%

'()*+,*-./%01++)2*31-%

"#$"%

12,,*3+42.&2@&A%BC&C+.D0&+.5&E25*&(:6.:F8+.8*&>G+0="@-H&+,40=7+,40=&
9.;*:6<=*5&6,+)<?&

!"##$% !"##&%

!"#!$% !"#!&%

!"#!$%& !"#!''&

"#&%

!$%

!'#(%

!'%

!&#(%

!&%

"#"%
!)#(%5*6,**&5*7829)3:.6&;*:6<=&>)?&
!)% !"#(% "%
"#(%
)%

01-J)-31-*2%=(K!>%

@)5+))%4)6718(2)4%

)#(%

&%

&#(%

'%

!"#&%

!"#$%

!"#$%

@)5+))%4)6718(2)4%

"#$%

01-H)-31-*2%=(I!>%

A*6,**&5*7829)3*5&

12.I*.42.+3&>)J!?&

(a) DBLP (article-article)
(b) Last.fm (listener-listener)
(c) Last.fm (artist-artist)
Figure 4: Application Group C: p < 0 is optimal (i.e., node degrees need to be boosted)
p<0

&#""-((#4((,#$+($+5"++(
),$(,#$+(3/5,/6&),&+(

"#'&%

2/3'+,+"(

"#'"%
"#$&%

)%'0#"(

"#$"%
"#"&%

*#./+(
&#**+,'-(

)"13'(
)"1&2+(

p=0

"#""%

)&'#"(

!"#"&%

!"#$%&'(
!"#$"%

p>0

Figure 5: Correlations between node degrees and application specific significances for different data graphs (each color
group is a distinct pattern in Figures 2 through 4).

4.3.4 Summary: Correlations between Node Degrees and Application Specific Significances
The experiments reported above show that degree de-coupling
is important as different applications, even on the same data set,
may associate different semantics to node degrees and the conventional PageRank scores are too tightly coupled with node degrees
to be effective in all scenarios. Figure 5, which plots correlations
between node degrees and application specific significances for different data graphs, re-confirms that the ideal value of the p is related
to the usefulness of the node degree in capturing the application
specific definition of node significance.

4.4 Relationship between Î± and p
In Figures 6 through 8, we investigate the relationship between
the value Î± and the degree de-coupling parameter p for different
application types. Here we use the default value, 0, for the parameter Î² and present the results for unweighted graphs (the results for

the weighted graphs are similar).
First thing to notice in these figures is that the grouping of the
applications (into those where, respectively, p > 0, p = 0, or p < 0
is useful) is preserved when different values of Î± are considered.
Figure 6 studies the impact of the value of Î± in application group
A, where degree penalization helps (p > 0). As we see here,
for the IMDB actor-actor (Figure 6(a)) and Epinions commentercommenter (Figure 6(b)) graphs, having a lower value of Î± (i.e.,
lower probability of forward movement during the random walk)
provides the highest possible correlations between D2PR ranks and
node significance (with the optimal value of p being â¼ 0.5 independent of the value of Î±). This indicates that in these graphs,
it is not necessary to traverse far during the random walk. Interestingly, though, when degrees are over-penalized (i.e., p â« 0),
smaller values of Î± start leading to worse correlations, indicating
that (while not being optimal) severe penalization of node degrees
helps make random traversals more useful than random jumps. As
we have already observed in Figure 2(c), the Epinions productproduct graph (Figure 6(c)) behaves somewhat differently from the
other two cases where degree penalization (p > 0) leads to larger
correlations: in this case, unlike the other two graphs, the highest
possible correlations between D2PR ranks and node significance
are obtained for large values of Î±, indicating that this application
benefits from longer random walks (though the differences among
the correlations for different Î± values are very small).
Figure 7 shows that the pattern is different for application group
B, where conventional PageRank is ideal (p = 0): in this case, having a larger value of Î± (i.e., larger probability of forward movement
during the random walk) provides the highest correlations between
ranks and significance. Interestingly, in these applications, when

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*,'*/4B#%#'7*C*D$-3#)*
8EF;GH*$27,%1$27,%*3'5#4067#/*0%$"69*

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*,'*/4B#%#'7*C*D$-3#)**
8E"4'4,')F*2,&&#'7#%12,&&#'7#%*3'5#4067#/*0%$"69*

"#"&%

"#"$%

!$%

!*#+%

!*%

!(#+%

!(%

!'#+%

!'%

"#+%

'%

'#+%

(%

(#+%

*%

*#+%

"#"'%
"#"$%
"#"&%

!(%

!'#)%

!'%

!$#)%

!$%

!&#)%

!&%

"#""%
!"#)% "%

"#)%

&%

&#)%

$%

$#)%

'%

'#)%

(%

"#"&%

!&%

!'#(%

!'%

!)#(%

!)%

!*#(%

!*%

"#""%
!"#(% "%

$%

/#0%##*/#12,3"-4'0*5#4067*8"9*

!"#"$%

CIJKNL*

*%

*#(%

)%

)#(%

'%

'#(%

&%

/#0%##*/#12,3"-4'0*5#4067*8"9*

!"#"$%

CIJKM*

"#(%

!"#"&%

!"#"&%

/#0%##*/#12,3"-4'0*5#4067*8"9*
CIJKL*

"#"$%

!"#$%&$'()*+,%%#-$.,'*

"#'(%

"#""%
!"#+% "%

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*,'*/4A#%#'7*B*C$-3#)*
8D"4'4,')E*"%,/3271"%,/327*3'5#4067#/*0%$"69*

"#"(%

!"#$%&$'()*+,%%#-$.,'*

!"#$%&$'()*+,%%#-$.,'*

"#')%

CIJKO*

CGHIJ*

!"#"$%

CGHIK*

CGHILJ*

CGHIM*

BFGHI*

BFGHJ*

BFGHKI*

BFGHL*

(a) IMDB (actor-actor)
(b) Epinions (commenter-commenter)
(c) Epinions (product-product)
Figure 6: Relationship between p and Î±, for application group A, where p > 0 is optimal (i.e., degrees need to be penalized)
+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*,'*/4B#%#'7*C*D$-3#)*
8;EF=G*$376,%1$376,%*3'5#4067#/*0%$"69*

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*,'*/4B#%#'7*C*D$-3#)*
8EF;GH*&,D4#1&,D4#*3'5#4067#/*0%$"69*
"#"(%

!"#$%&$'()*+,%%#-$.,'*

!"#$%&$'()*+,%%#-$.,'*

"#&%

"#'%

!(%

!$#)%

!$%

!&#)%

!&%

!'#)%

"#"%
!"#)% "%

!'%

"#)%

'%

'#)%

&%

&#)%

$%

$#)%

(%

/#0%##*/#12,3"-4'0*5#4067*8"9*
!"#'%

"#")%

!(%

!*#+%

!*%

!)#+%

!)%

!$#+%

!$%

"#""%
!"#+% "%
!"#")%

"#+%

$%

$#+%

)%

)#+%

*%

*#+%

(%

/#0%##*/#12,3"-4'0*5#4067*8"9*
!"#"(%
!"#"'%

!"#&%
!"#"&%
!"#$%

CHIJK*

CHIJL*

!"#$"%

CHIJMK*

CHIJN*

CIJKL*

CIJKM*

CIJKNL*

CIJKO*

(a) DBLP (author-author)
(b) IMDB (movie-movie)
Figure 7: Relationship between p and Î±, for application group B, where p = 0 is optimal
+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*,'*/4B#%#'7*C*D$-3#)*
8E$)7F:&G*-4)7#'#%1-4)7#'#%*3'5#4067#/*0%$"69*

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*,'*/4B#%#'7*C*D$-3#)**
8;EF=G*$%.2-#1$%.2-#*3'5#4067#/*0%$"69*

!(#%&

!(&

!)#%&

!)&

!$#%&

!$&

"#""&
!"#%& "&

"#%&

$&

$#%&

)&

)#%&

(&

(#%&

'&

!"#"%&

/#0%##*/#12,3"-4'0*5#4067*8"9*

"#&%

!(%

!'#)%

!'%

!$#)%

!$%

!&#)%

!&%

"#"%
!"#)% "%

&%

&#)%

$%

$#)%

'%

!"#&%

!"#$%&

CHIJL*

"#)%

/#0%##*/#12,3"-4'0*5#4067*8"9*

!"#$"&

CHIJK*

"#$%

"#$%

'#)%

(%

!"#$%&$'()*+,%%#-$.,'*

"#"%&

!"#$%&$'()*+,%%#-$.,'*

!"#$%&$'()*+,%%#-$.,'*

"#$"&

!'&

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*,'*/4B#%#'7*C*D$-3#)*
8E$)7F:&G*$%.)71$%.)7*3'5#4067#/*0%$"69*

"#'%

"#$%&

"#&%

!$%

!'#(%

!'%

!&#(%

!&%

!)#(%

!)%

"#"%
!"#(% "%

CHIJN*

CHIFJ*

CHIFK*

)%

)#(%

&%

&#(%

'%

'#(%

$%

!"#&%

!"#$%

CHIJMK*

"#(%

/#0%##*/#12,3"-4'0*5#4067*8"9*

!"#$%

CHIFLJ*

CHIFM*

CHIFJ*

CHIFK*

CHIFLJ*

CHIFM*

(a) DBLP (article-article)
(b) Last.fm (listener-listener)
(c) Last.fm (artist-artist)
Figure 8: Relationship between p and Î±, for application group C, where p < 0 is optimal (i.e., node degrees need to be boosted)
p âª 0 or p â« 0, higher probabilities of random walk traversal
(i.e., larger Î±) stop being beneficial and lower values of Î± lead to
larger correlations. This re-confirms that, for these applications,
p â¼ 0 leverages the random walk traversal the best.
As we see in Figure 8, in application group C, where degree
boosting helps (p < 0), it is also the case that larger values of Î±
(i.e., larger probabilities of forward transitions during the random
walk) provides the highest correlations between node ranks and significance. On the other hand, in these applications, p â¼ 0.5 serves
as a balance point where the value of Î± stops being relevant; in
fact, for p > 0.5 the higher values of Î± stops being beneficial and
lower values of Î± lead to larger correlations. This re-confirms that
smaller values of p (which provides degree boosting) help leverage
the random walk traversal the best.

4.5 Relationship between
Weighted Graphs

Î²

and

p

in

Finally, in Figures 9 through 11, we investigate the relationship between the value Î² (which controls whether accounting for
the connection strength or degree de-coupling is more critical in a
given application) and the degree de-coupling parameter p for different application types. Here we use the default value, 0.85, for
the parameter Î± and present the results for weighted graphs:
Figure 9 depicts the impact of the value of the parameter Î² in application group A, where degree penalization helps (p > 0). As we
see here, for all three weighted graphs, performing degree penalization (i.e., Î² < 1.0) provides better rank-significance correlation
than relying solely on the connection strength (i.e., Î² = 1.0). Note
that the value of Î² impacts the optimal value of degree penalization
parameter p: the more weight is given to connection strength (i.e.,

the greater Î² is), the larger is the optimal value of p.
Figure 10 shows that, for applications in group B, where p â¼ 0
is ideal, when the connection strength is given significantly more
weight than degree de-coupling (i.e., Î² â¼ 0), we observe high ranksignificance correlations. Interestingly however, for the moviemovie graph (where the edge weights denote common actors) the
highest correlations are obtained not with p = 0, but with p = 0.5
and Î² = 0.75, indicating that degree penalization is actually beneficial in this case: movies that share large numbers of actors with
other movies are likely to be B-movies, which are not good candidates for transitions during the random walk.
Figure 11 shows that in application group C, where degree boosting (p < 0) helps, giving more weight to connection strength (i.e.,
Î² â¼ 1.0) is a good, but not necessarily the best strategy. In fact,
in these graphs, the highest overall correlations are obtained with
Î² = 0 or Î² = 0.25, indicating that degree de-coupling is beneficial also in these cases. Interestingly, (unlike the case with the
unweighted listener-listener graph, where the best correlation was
obtained when p < 0) for the weighted version of the listenerlistener graph (where edge weights denote the number of shared
friends), when Î² = 0 through 0.5, p = 0 provides the highest
correlation and when Î² = 0.75, p = 0.5 provides the highest correlation â these indicate that listeners who have large numbers of
shared friends with others are good candidates for random walk.
Note that a key observation from the above results is that the
conventional PageRank, based on connection strength (i.e., Î² =
1.0), is not always the best strategy for the applications considered.

5. CONCLUSIONS
In this paper, we noted that in many applications the relation-

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*,'*/4B#%#'7*C*D$-3#)*8E"4'4,')F*
2,&&#'7#%12,&&#'7#%*5#4067#/*0%$"69*G#/0#15#4067H*I*,:*)6$%#/*"%,/327)J*

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*,'*/4B#%#'7*C*D$-3#)*
8EF;GH*$27,%1$27,%*5#4067#/*0%$"69*I#/0#15#4067J*K*,:*2,&&,'*&,D4#)L*

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*,'*/4A#%#'7*B*C$-3#)**
8D"4'4,')E*"%,/3271"%,/327*5#4067#/*0%$"69*F#/0#15#4067G*H*,:*)6$%#/*2,&&#'7#%)I*

"#)"%

"#$&%

"#"(%
"#"'%
"#"$%
"#"&%

!$%

!*#+%

!*%

!&#+%

!&%

!)#+%

!)%

"#""%
!"#+% "%
!"#"&%

"#+%

)%

)#+%

&%

&#+%

*%

*#+%

$%

"#"(%
"#"'%
"#"$%
"#"&%

!(%

!'#)%

!'%

!$#)%

!$%

!&#)%

!&%

/#0%##*/#12,3"-4'0*5#4067*8"9*

"#""%
!"#)% "%
!"#"&%

CMNO<P*

"#)%

&%

&#)%

$%

$#)%

'%

'#)%

(%

"#$"%

"#"&%

!'%

!(#&%

!(%

!)#&%

!)%

!$#&%

!$%

"#""%
!"#&% "%

!"#"$%

CMNOP*

CMNOQP*

CMR*

CKL*

CKLM<N*

"#&%

$%

$#&%

)%

)#&%

(%

(#&%

'%

!"#"&%

/#0%##*/#12,3"-4'0*5#4067*8"9*

/#0%##*/#12,3"-4'0*5#4067*8"9*

!"#"$%

CMN*

!"#$%&$'()*+,%%#-$.,'*

!"#$%&$'()*+,%%#-$.,'*

!"#$%&$'()*+,%%#-$.,'*

"#")%

!"#$"%

CKLMN*

CKLMON*

BJK*

CKP*

BJKL<M*

BJKLM*

BJKLNM*

BJO*

(a) IMDB (actor-actor)
(b) Epinions (commenter-commenter)
(c) Epinions (product-product)
Figure 9: Relationship between p and Î², for application group A, where p > 0 is optimal (i.e., node degrees need to be penalized)
+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*,'*/4B#%#'7*C*D$-3#)*
8;EF=G*$376,%1$376,%*5#4067#/*0%$"69*H#/0#15#4067I*J*,:*2,"$"#%)K*

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*,'*/4B#%#'7*C*D$-3#)*
8EF;GH*&,D4#1&,D4#*5#4067#/*0%$"69*I#/0#15#4067J*K*,:*2,&&,'*$27,%)L*
"#"(%

!"#$%&$'()*+,%%#-$.,'*

!"#$%&$'()*+,%%#-$.,'*

"#$%

"#&%

!'%

!(#)%

!(%

!$#)%

!$%

!&#)%

!&%

"#"%
!"#)% "%

"#)%

&%

&#)%

$%

$#)%

(%

(#)%

'%

/#0%##*/#12,3"-4'0*5#4067*8"9*
!"#&%

"#")%

!(%

!*#+%

!*%

!)#+%

!)%

!$#+%

!$%

"#""%
!"#+% "%
!"#")%

"#+%

$%

$#+%

)%

)#+%

*%

*#+%

(%

/#0%##*/#12,3"-4'0*5#4067*8"9*
!"#"(%
!"#"'%
!"#"&%

!"#$%

CLM*

CLMN<O*

!"#$"%

CLMNO*

CLMNPO*

CLQ*

CMN*

CMNO<P*

CMNOP*

CMNOQP*

CMR*

(a) DBLP (author-author)
(b) IMDB (movie-movie)
Figure 10: Relationship between p and Î², for application group B, where p = 0 is optimal
+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*,'*/4B#%#'7*C*D$-3#)*
8;EF=G*$%.2-#1$%.2-#*5#4067#/*0%$"69**H#/0#15#4067I*J*,:*2,$376,%)K*

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*,'*/4B#%#'7*C*D$-3#)*
8E$)7F:&G*-4)7#'#%1-4)7#'#%*5#4067#/*0%$"69*H#/0#15#4067I*J*,:*)6$%#/*:%4#'/)K*

"#"&%

!(#&%

!(%

!)#&%

!)%

!$#&%

!$%

"#""%
!"#&% "%

"#&%

$%

$#&%

)%

/#0%##*/#12,3"-4'0*5#4067*8"9*

)#&%

(%

(#&%

'%

"#&%

!(%

!'#)%

!'%

!$#)%

!$%

!&#)%

!&%

"#"%
!"#)% "%

&%

&#)%

$%

!"#&%

!"#$"%

CLMN<O*

"#)%

/#0%##*/#12,3"-4'0*5#4067*8"9*

!"#"&%

CLM*

"#$%

"#$%

$#)%

'%

'#)%

(%

!"#$%&$'()*+,%%#-$.,'*

"#$"%

!'%

+,%%#-$.,'*,:*;<=>*>$'?)*$'/*@,/#*!40'4A2$'2#*,'*/4B#%#'7*C*D$-3#)**
8E$)7F:&G*$%.)71$%.)7*5#4067#/*0%$"69*H#/0#15#4067I*J*,:*)6$%#/*-4)7#'#%)K*

"#'%

!"#$%&$'()*+,%%#-$.,'*

!"#$%&$'()*+,%%#-$.,'*

"#$&%

"#&%

!$%

!'#(%

!'%

!&#(%

!&%

!)#(%

!)%

"#"%
!"#(% "%

CLMNPO*

CLQ*

CLM*

CLMF<N*

)%

)#(%

&%

&#(%

'%

'#(%

$%

!"#&%

!"#$%

CLMNO*

"#(%

/#0%##*/#12,3"-4'0*5#4067*8"9*

!"#$%

CLMFN*

CLMFON*

CLP*

CLM*

CLMF<N*

CLMFN*

CLMFON*

CLP*

(a) DBLP (article-article)
(b) Last.fm (listener-listener)
(c) Last.fm (artist-artist)
Figure 11: Relationship between p and Î², for application group C, where p < 0 is optimal (i.e., node degrees need to be boosted)
ship between the significance of the node and its degree in the underlying network may not be as strong (or as weak) as implied
by PageRank-based measures. We proposed degree de-coupled
PageRank (D2PR) to improve the effectiveness of PageRank based
knowledge discovery and recommendation tasks. Evaluations on
different data graphs and recommendation tasks have confirmed
that degree de-coupling would be an effective way to match application specific node significances and improve recommendation
accuracies using PageRank based approaches.

6.

REFERENCES

[1] A. Balmin, V. Hristidis, and Y. Papakonstantinou. ObjectRank:
Authority-based keyword search in databases. In VLDBâ04.
[2] D. Banky, G. Ivan, V. Gromusz. Equal Opportunity for Low-Degree
Network Nodes: A PageRank-Based Method for Protein Target
Identification in Metabolic Graphs. PLoS One 8(1): e542-4, 2013.
[3] L. Becchetti et al. Using Rank Propagation and Probabilistic Counting
for Link-Based Spam Detection. WebKDD, 2006.
[4] P. Boldi et al. HyperANF: Approximating the neighbourhood function
of very large graphs on a budget. WWW, 2011.
[5] M.G.Borgatti, et al. Network measures of social capital. Connections
21(2):27-36, 1998.
[6] S. Brin and L. Page. The anatomy of a large-scale hypertextual web
search engine. Computer Networks and ISDN Systems, 30, 1998.
[7] K.S. Candan and W.D. Li. Using random walks for mining web
document associations. PAKDDâ00, pp. 294-305, 2000.
[8] M. Chen., et al. Clustering via random walk hitting time on directed
graphs. AAAIâ08.
[9] S. Chakrabarti. Dynamic personalized pagerank in entity-relation
graphs. In WWWâ07, pp 571â580, 2007.
[10] M. Chen, J. Liu, and X. Tang. Clustering via random walk hitting
time on directed graphs. AAAIâ08, pp. 616â621, 2008.

[11] C. Cooper, et al. A fast algorithm to find all high degree vertices in
graphs with a power law degree sequence. In WAWâ12, 2012.
[12] O. Fercoq. PageRank optimization applied to spam detection.
arXiv:1203.1457, 2012.
[13] T. H. Haveliwala. Topic-sensitive PageRank. WWW, 2002.
[14] S. Huang, X. Li, K.S Candan, M.L Sapino. âCan you really trust that
seed?â: Reducing the Impact of Seed Noise in Personalized
PageRank. ASONAMâ14, 2014.
[15] IMDB website: http://www.imdb.com/
[16] G. Jeh and J. Widom. Scaling personalized web search. Stanford
Univ. Tech. Report. 2002.
[17] J.H Kim, K.S Candan, M.L Sapino. Locality-sensitive and Re-use
Promoting Personalized PageRank Computations. Knowledge and
Information Systems. 10.1007/s10115-015-0843-6, 2015
[18] http://ir.ii.uam.es/hetrec2011/datasets.html.
[19] A.N. Nikolakopoulos and J. Garofalakis, NCDawareRank: A Novel
Ranking Method that Exploits the Decomposable Structure of the
Web. WSDM, 2013.
[20] F. Mathieu and L. Viennot, Local aspects of the global ranking of
web pages. In I2CSâ06, pp. 1â10, 2006.
[21] Q. Mei, D. Zhou, and K. Church. Query suggestion using hitting
time. CIKMâ08, 2008.
[22] http://grouplens.org/datasets/movielens
[23] M. Olsen. Maximizing PageRank with New Backlinks. CIAC, 2010.
[24] C. Palmer, P. Gibbons, and C. Faloutsos. Anf: a fast and scalable tool
for data mining in massive graphs. KDD, 2002.
[25] J. Tang, H. Gao, and H. Liu. mTrust: Discerning multi-faceted trust
in a connected world. WSDM, 2012.
[26] J. Tang, et al. ArnetMiner: Extraction and Mining of Academic
Social Networks. In SIGKDDâ08, pp 990â998, 2008
[27] D.R. White, et al. Betweenness centrality measures for directed
graphs. Social Networks, 16, 335-346,1994.

HCS: Hierarchical Cut Selection for Efficiently Processing
Queries on Data Columns using Hierarchical Bitmap
Indices â
Parth Nagarkar

K. SelÃ§uk Candan

School of Computing, Informatics, and Decision
Systems Engineering
Arizona State University
Tempe, AZ 85287-8809, USA

School of Computing, Informatics, and Decision
Systems Engineering
Arizona State University
Tempe, AZ 85287-8809, USA

nagarkar@asu.edu

candan@asu.edu

ABSTRACT

data [1]. When data are large and the query processing workloads
consist of such data selection and aggregation operations, columnoriented data stores are generally the preferred choice of data organization, especially because they enable effective data compression, leading to significantly reduced IO [2].
Recently, many databases have leveraged bitmap-indices, which
themselves can be compressed, for efficiently answering queries
[3], [4]. When column-domains (e.g., geographical data, categorical data, biological taxonomies, organizational data) are hierarchical in nature [5], it is often more advantageous to create hierarchical bitmap indices to efficiently answer queries over different
sub-ranges of the domain. [5] for example proposes a hierarchically organized bitmap index (HOBI) for answering OLAP queries
over data with hierarchical domains.
In this paper, we also focus on hierarchically organized bitmap
indices for answering queries over column-oriented data and
present efficient algorithms for selecting the subset of bitmap indices to answer queries efficiently over compressed data columns.
Before we detail the contributions of this paper in Section 1.2, we
first provide an overview of the related work in the area.

When data are large and query processing workloads consist of data
selection and aggregation operations (as in online analytical processing), column-oriented data stores are generally the preferred
choice of data organization, because they enable effective data
compression, leading to significantly reduced IO. Most columnstore architectures leverage bitmap indices, which themselves can
be compressed, for answering queries over data columns. Columndomains (e.g., geographical data, categorical data, biological taxonomies, organizational data) are hierarchical in nature, and it may
be more advantageous to create hierarchical bitmap indices, that
can help answer queries over different sub-ranges of the domain.
However, given a query workload, it is critical to choose the appropriate subset of bitmap indices from the given hierarchy. Thus,
in this paper, we introduce the cut-selection problem, which aims
to help identify a subset (cut) of the nodes of the domain hierarchy, with the appropriate bitmap indices. We discuss inclusive, exclusive, and hybrid strategies for cut-selection and show that the
hybrid strategy can be efficiently computed and returns optimal (in
terms of IO) results in cases where there are no memory constraints.
We also show that when there is a memory availability constraint,
the cut-selection problem becomes difficult and, thus, present efficient cut-selection strategies that return close to optimal results, especially in situations where the memory limitations are very strict
(i.e., the data and the hierarchy are much larger than the available
memory). Experiment results confirm the efficiency and effectiveness of the proposed cut-selection algorithms.

1.1

Related Work

Range and Aggregation Queries over Data Columns. As mentioned above, column-oriented data stores are generally the preferred choice of data organization for aggregation queries over single attribute columns, because they enable effective data compression, leading to significantly reduced IO [2]. Range queries are
used in data warehouse environments to perform aggregations over
a specified range for analysis. Research has been done on creation of specific data structures that can better the performance of
range queries. In [6], the authors propose an update to an existing data structure to store the aggregate values of the leaf nodes
in the internal nodes. This reduces the lookup at the leaf nodes
if all the leaf nodes fall under a range of an internal node in the
query. Their drawback is that the proposed approach stores aggregation values even for queries that do not require aggregation,
thus degrading their performance. In [7], the authors build upon
the work described in [6], and create a more generalized data structure that leverages upper levels to help aggregate queries but does
not store the aggregate values for queries that do not require any
aggregation. In [1], the authors present algorithms to solve range
queries for two types of aggregation operations, sum and max, by
using precomputed max over balanced hierarchical tree structures.
Caching results of query data has also been looked into, particularly for column store environments. In [8], the authorsâ main focus is to develop a system that can cache aggregate results as well

1. INTRODUCTION
Range selection queries are frequent in many applications, including online analytical processing (OLAP) scenarios, where an
aggregation operation needs to be applied over a certain range of
âThis work is supported by NSF grant #1116394 "RanKloud:
Data Partitioning and Resource Allocation Strategies for Scalable
Multimedia and Social Media Analysis"

(c) 2014, Copyright is with the authors. Published in Proc. 17th International Conference on Extending Database Technology (EDBT), March
24-28, 2014, Athens, Greece: ISBN 978-3-89318065-3, on OpenProceedings.org. Distribution of this paper is permitted under the terms of the Creative Commons license CC-by-nc-nd 4.0

271

10.5441/002/edbt.2014.26

as be able to handle transactional and analytical workloads in one
system. In [9], the authors present a hierarchical structure to efficiently execute range-sum queries. Their focus is on reducing number of cell accesses per query and improving update performance
specifically in a data cube. In this paper, we present algorithms that
choose specific bitmap indices to be cached in the memory to speed
up a given query workload.

hybrid strategy can be efficiently computed for a single query or
a workload of multiple queries and also that it returns optimal (in
terms of IO) results in cases where there are no memory constraints
(Section 4.2).
However, in cases where the memory is constrained, the cutselection problem becomes difficult to solve. To deal with these
cases, in Section 2.3.4, we present efficient cut-selection strategies
that return close to optimal results, especially in situations where
the memory limitations are very strict (i.e., the data and the hierarchy are much larger than the available memory).
Experiment results presented in Section 4 confirm the efficiency
and effectiveness of the proposed cut-selection algorithms.

Bitmap Indices. Bitmap indices have been used in OLAP queries
and data warehouses for their benefit of compression. There has
been significant amount of work to improve the performance of
bitmap indices as well as keeping the compression rates high [10],
[11], [12]. Most of the newer compression algorithms use runlength encoding for compression: it provides a good compression
ratio and one can do bitwise operations directly on decompressed
bitmaps without actually decompressing them [10]. Recently, researchers have shown that bitmap indices perform well even on
high-cardinality attributes [13].

2.

PROBLEM SPECIFICATION

In this section, we first introduce the relevant concepts and notations, provide a cost model, and introduce the cut-selection problem for identifying a subset of the nodes of the domain hierarchy,
containing the nodes with the bitmap indices to efficiently answer
a given query or a query workload.

Multi-level Indices. There has also been considerable research
done in the area of multi-level indices [14], [15], [16]. In data warehouse applications, bitmap indices are also shown to perform better
than traditional database index structures like the B-tree [10], [17].
Our work focuses on which bitmaps to read and cache in the memory from a given hierarchy. We introduce novel algorithms that
choose these bitmaps efficiently. As far as we know, most of the
existing approaches use what we term as an inclusive strategy:
they first identify upper level bitmaps for retrieving the data that
fully satisfies the given query and the lower level bitmaps to satisfy
the boundary bitmaps in the hierarchy that the upper level bitmaps
could not be used to give an exact answer [11]. As discussed in
the next subsection, however, we generalize the problem into a cutselection problem and introduce exclusive and hybrid strategies that
complement inclusive result construction. The works [5] and [18]
focus on building hierarchies on dimensions, specifically in a data
warehouse environment, to efficiently execute range queries. [19]
deals with the similar problem of choosing the appropriate set of
bitmap join indices of one or more attributes using data mining
techniques; we on the other hand focus on choosing the appropriate set of bitmap indices for a given domain hierarchy.

2.1

Key Concepts, Parameters, and Notations

We first provide an overview of the concepts and parameters necessary to formulate the problem described in this paper and introduce the relevant notations.

2.1.1 Columns and Domain Hierarchies
A database consists of relations, R = {R1 , . . . , Rmaxr }.
Each relation, Rr , consists of a set of attributes,
Ar
=
{Ar,1 , . . . , Ar,maxar }, with domains Dr
=
{Dr,1 , . . . , Dr,maxar } In this paper, without loss of generality, we associate to each attribute, Ar,a , a corresponding hierarchy, Hr,a , which consists of a set of nodes,
Nr,a = {Nr,a,1 , . . . , Nr,a,maxnr,a }. Also, since our goal is
to efficiently answer queries over a single data column, unless
necessary, we omit explicit references to relation Rr and attribute
Ar,a ; hence, when we do not need to refer to a specific relation
and attribute, we simply omit the relation and attribute subscripts;
e.g., we refer to H instead of Hr,a .
In this paper, when talking about the nodes of a domain hierarchy
H, we use the following notations:

1.2 Contributions of this Paper

â¢ Parent of a node: For all Nâ , parent(Nâ ) denotes the parent of Nâ in the corresponding hierarchy; if Nâ is the root,
then parent(Nâ ) = â¥.

Since IO is often the main bottleneck in processing OLAP workloads over large data sets, given a query or a workload consisting
of multiple queries, the main challenge in leveraging hierarchically
organized bitmap indices is to choose the appropriate subset of
bitmap indices from the given hierarchy to process the query. [5],
for example, proposes a (what we term as an âinclusiveâ) strategy which leverages bitmap indices associated to the internal nodes
along with the bitmap indices associated to the data leaves to bring
together the data elements needed to answer the query.
In this paper, we note that such inclusive strategies can be suboptimal. In fact, [5] shows that the inclusive strategy is effective
mainly for small query ranges. Therefore, in this paper, we introduce a more general cut-selection problem, which aims to help
identify a subset (referred to as a cut) of the nodes of the domain
hierarchy, which contain the operations nodes with the appropriate bitmap indices to efficiently answer queries. In particular, we
discuss inclusive, exclusive, and hybrid strategies for cut-selection
(Section 3.1) and experimentally show that the so-called exclusive
strategy provides gains when the query ranges are large and that
the hybrid strategy provides best solutions across all query range
sizes, improving over the inclusive strategy even when the ranges
of interest are relatively small (Section 4.1). We also show that the

â¢ Descendants of a Node: The set of descendants of node n
in the corresponding hierarchy is denoted as desc(n).
â¢ Leaves: LH denotes the set of leaf nodes of the hierarchy
H. Any other node in H that is not a leaf node is called an
internal node. The set of internal nodes of H is denoted by
IH . We assume that only the leaves of a hierarchy occur in
the database.
â¢ Leaf Descendants of a Node: Leaf descendants of a node
are the set of nodes such that they are leaf nodes as well as descendants of the given node; i.e., for a node n, leaf Desc(n)
returns a set of nodes such that
âbâleaf Desc(n) b â LH â§ b â desc(n).

2.1.2 Query Workload
In this paper, we focus on query workloads with range queries
on an attribute (i.e., column) of the database relations:

272

â¢ Range Specification: Given an attribute Aa and the start
and end points, i and j, we denote the corresponding range
specification as, rsa,i,j .

WAH Library vs. Our Cost Model
Bitmap File Size on Disk (in mb)

35

Given two range specifications, rsa,i,j and rsa,k,l ,
â if k > j, then these two range specifications are disjoint,
â if (i < k, l) â§ (j > k) â§ (j < l), then the two range
specifications are intersecting, and

30
25
20
WAH Library

15

Our Model

10
5
0
0.001

0.01

0.1

0.5

Bitmap Density (log scale)

â if (i < k, l) â§ (j > k, l), then the two range specifications are overlapping.
Figure 1: Comparison of our Cost Model and WAH Library
Model. Dx1 = 0.01, Dx2 = 0.015, Dx3 = 0.03, and a =
1043, b = 0.5895, on a 500 GB SATA Hard Drive with 7200
RPM, and 16 MB Buffer Size.

â¢ Range Queries: Each query q involves fetching one or more
sets of column values, such that each set of values belongs to
a continuous range over the domain hierarchy of the attribute.
A query, q, can have multiple range specifications. The set of
range specifications for a query q is denoted as RSq . Without
loss of generality, we assume that all range specifications in
RSq are disjoint. If a query has two intersecting or overlapping range specifications, rsa,i,j and rsa,k,l , then we partition the query into two subqueries, q1 and q2 , such that
range specification for q1 is rsa,i,j and specification for q2
is rsa,k,l . In Sections 3.2 and 3.3, we discuss algorithms for
handling multiple queries.

are performed on compressed versions of the bitmap indices, further boosting the query performance [10]. In general, the time taken
to read the bitmaps from secondary storage into the memory dominates the overall bitwise manipulation time [11], [20]. The cost of
this process is proportional to the size of the bitmap file on the secondary storage; the larger the size of a bitmap file on a secondary
storage, the longer it takes to bring the bitmap into the physical
memory.

â¢ Range Nodes: Given a range specification, rsa,i,j , the set
of leaf nodes that fall in this range is denoted as, RNa,i,j .
These nodes are also referred to as range nodes.

2.2.1 Read Cost of Compressed Bitmap Indices
Therefore, in this paper, we model the cost of a bitmap operation as proportional to the size of the corresponding (compressed)
bitmap file, which in turn determines the time taken to read a
bitmap into the memory. Note that in general the query performance of a bitmap index with density greater than 0.5 is equivalent
to the performance of a bitmap with density complement to the
original [21]. For example, performance of a bitmap with density
0.7 is often equivalent to the performance of a bitmap with density
0.3. This is because a bitmap with density 0.7 can be negated and
stored as a bitmap with density 0.3. We also include this behavior
in our cost model, readCost(Bn ), of reading a bitmap index, Bn ,
as follows:
ï£±
0
if DBn = 0 â¨ DBn = 1
ï£´
ï£´
ï£´
ï£´
ï£´
ï£´aDBn + b if (0 < DBn â¤ Dx1 ) â¨ (1 â Dx1 â¤ DBn < 1)
ï£´
ï£´
ï£´
ï£´
if (Dx1 < DBn â¤ Dx2 )â¨
ï£´
ï£²k1
(1 â Dx2 â¤ DBn < 1 â Dx1 )
ï£´
ï£´
ï£´
k2
if (Dx2 < DBn â¤ Dx3 )â¨
ï£´
ï£´
ï£´
ï£´
ï£´
(1
â Dx3 â¤ DBn < 1 â Dx2 )
ï£´
ï£´
ï£´
ï£³
k3
otherwise

Given a query, q, and a node n, Gq,n â {0, 1} denotes
whether the node n is a range node for query q. More specifically, if node n is a range node for any range specification
in RSq , then Gq,n = 1 and otherwise, Gq,n = 0.
The set of all range nodes for any range specification of query
q is denoted as RNq . If RNq is empty, the query returns
null, whereas if RNq has the exact same nodes as LH , then
the query returns the entire database content for the attribute
on which H is defined.

2.1.3 Hierarchically Organized Bitmap Indices
As described above, the query workload includes queries that
fetch ranges of values from columns of relations in the database,
before performing further operations on these ranges. When bitmap
indices are available, these operations are implemented in terms of
bitmap manipulations [11]: for example, intersection of two range
queries can be performed as bitwise-AND of two bitmap indices
representing the database values in the two ranges. This ensures
that those data objects that will be pruned as a result of the query
processing are never fetched into memory. In this paper, we assume
that indices are organized hierarchically; i.e., every node n in H has
a corresponding bitmap Bn denoting which of the leaf nodes of n
occur in attribute A of the database.

Here DBn is the bit density, 0 < Dx1 < Dx2 < Dx3 < 0.5 are
three bit density thresholds, and a, b, k1 , k2 , and k3 are constants.
Intuitively, when the bit density of a bitmap is 0 or 1, the size
of the bitmap on the disk is very negligible due to the high-level of
compression. Hence, we assume the size of these bitmaps as nonexistant on the secondary storage. The bit density thresholds, Dx1 ,
Dx2 , and Dx3 , and the constant values, a, b, k1 , k2 , and k3 , are
specific to the implementation of the bitmap library.
Figure 1 shows alignment of our cost model with the read cost
of the WAH library for different bit densities.

â¢ Bitmap Density: Each bitmap Bn has a bit density, 0 â¤
DBn â¤ 1, denoting ratio of bits set to 1 to the size in the
bitmap.
Note that bitmap Bn may or may not have been materialized in the
form of a bitmap index in the database.

2.2 Cost Model and Query Plans

2.2.2 Inclusive, Exclusive, and Hybrid Query Plans

Especially when the data sets are large, the bitmaps are often
stored in a compressed manner and the various bit-wise operations

For a query plan for q, we define the set of nodes that are required

273

2.3.1 Query Processing with Cuts

to execute q as its operation nodes. Naturally, a given query can be
executed in various different ways, each with a different set, ONq ,
of operation nodes. In particular, in this paper, we consider two
distinct types of query plans: inclusive and exclusive plans.

We define a cut, c, as a subset of internal nodes (including the
root node) in a hierarchy, H, satisfying the following two conditions:
â¢ validity: there is exactly one node on any root-to-leaf branch
in a given cut (note that, by this definition, the set containing
only the root node of the hierarchy by itself is a cut); and

U.S.

CA

AZ

â¢ completeness: the nodes in c collectively cover every possible root-to-leaf branch in the given hierarchy, H.
SFO

L.A.

S.D.

PHX Tempe Tucson

If a set of internal nodes of H only satisfies the first condition, then
we refer to the cut as an incomplete cut.
The challenge of course is to select the appropriate cut c of the hierarchy H that will minimize the query processing cost, but will not
add significant memory overhead (if the memory is a constraint).
We discuss the alternative formulations of the cut-selection problem, next.

Consider the 3-level location hierarchy, H, shown above. Here,
the leaf nodes (cities in U.S.) are the actual values in the database.
The node U.S. is the root node of the hierarchy. Let us consider a
query q that has a set of range nodes (shaded nodes in the figure)
RNq =[SF O, L.A., S.D., P HX]. Assume that we have bitmap
indices for all the nodes of H. There are at least two different plans
of executing q:

2.3.2 Cut Selection Case 1: Single Query without
Memory Constraints

â¢ Inclusive query plans: The first plan is to combine a subset
of the bitmaps of H. In the above example, one inclusive
way to do this would be to combine the bitmaps of RNq .

The simplest scenario is identifying the cut necessary to execute
a single range query. As we explained earlier, the cost for executing
a query is proportional to the size of the bitmaps that are read into
the memory from the secondary storage. Thus, given a query q and
cut c on H, problem
ï£±
ï£¼
ï£² â
ï£½
cost(c, q) =
M IN
readCost(Bn )
(1)
ï£¾
ONq â(câªLH ) ï£³

Another inclusive plan would be to combine the bitmaps of
CA and P HX (i.e. CA OR P HX). Note that this strategy
is similar to what was reported in the literature [5].
â¢ Exclusive query plans: Alternatively, we can remove the
bitmaps of the non-range nodes of q from the relevant internal nodes of H. For instance, in this example, we
can achieve this by first performing a bitwise-OR operation
on the bitmaps of T empe and T ucson and then doing a
bitwise-ANDNOT operation between the bitmap of U.S and
the resultant bitmap from the OR operation (i.e. U.S ANDNOT (T empe OR T ucson)).

nâONq

denotes the best execution cost for query q given the bitmaps for
the leaves, and the cut c.
The cut-selection problem for a given query q on hierarchy H
can be formulated as finding a cut c such that cost(c, q) is the
smallest among all cuts of the hierarchy H.

Another exclusive plan would be to do the following:
CA OR (AZ ANDNOT (T empe OR T ucson).

2.3.3 Cut Selection Case 2: Multiple Queries without Memory Constraints

It is easy to see that all four plans would return the same result;
however, these plans have different operation nodes: for the first
inclusive query plan, the operation nodes are ONq = [SF O,
L.A., S.D., P HX], whereas for the second inclusive query plan,
ONq = [CA, P HX]. Similarly, for the first exclusive query plan
ONq = [U.S., T empe, T ucson], and for the second exclusive
query plan ONq = [CA, AZ, T empe, T ucson]. As a result, each
execution plan also requires different amount of data being read.
In this paper, we consider inclusive and exclusive strategies
for answering range queries using hierarchical bitmaps. We also
consider hybrid strategies, which combine inclusive and exclusive
strategies (that may make inclusive or exclusive decisions at different nodes of the hierarchy) for better performance.

In general, we are not given a single range query, but a set of
range queries that need to be executed on the same data set. Therefore, we need to generalize the above formulation to scenarios with
multiple range queries. If we are given a set, Q, of queries on hierarchy H, then one way to formulate the cut-selection problem is to
search for a cut c such that cost(c, Q), defined as
cost(c, Q) =

â

cost(c, q)

(2)

qâQ

is the smallest among all cuts of the hierarchy H.
Note, however, that this formulation treats each query independently and implicitly assumes that each query plan accesses the
bitmaps of its operation nodes from the secondary storage; i.e. it
pays the cost of reading a bitmap from the secondary storage every
time the node is needed for query processing. This will obviously
be redundant when the different queries can be processed using the
same operation nodes: in such a case, it would be best to bring the
bitmap for the operation nodes to the memory and keep it to process
all the relevant queries.
This, however, changes the problem formulation significantly; in
particular, we now need to search for a cut c such that costâ² (c, Q),

2.3 Cut Selection Problem
As described above, any range query, q, on hierarchy H, can
be answered (through inclusive, exclusive, and hybrid strategies)
using bitmap indices for the leaves of the hierarchy. We note however that, if we are also given the bitmap indices for a subset of
the internal nodes of the hierarchy, we may be able to reduce the
overall cost of the query significantly by also leveraging the bitmap
indices for these internal nodes. We refer to these subsets as cuts of
the hierarchy.

274

defined as
(
) ï£«
â
readCost(Bn ) + ï£­
nâc

â

Algorithm 1 Inclusive Cut Selection Algorithm
1: Input: Hierarchy H, Set of internal nodes IH , Query q
2: Output: Set of nodes c
3: Initialize: Node n = root, c
4: procedure FIND N ODE I NCLUSIVE C UT(n)
5:
Set children = f indChildren(n, IH );
6:
if children is empty then
7:
add n to c;
8:
return nodeInclCost(n, q);
9:
else
10:
costChildren = 0;
11:
for each child m of n do
12:
costChild = f indN odeInclusiveCut(m);
13:
if costChild Ì¸= â then
14:
costChildren = costChildren + costChild;
15:
end if
16:
end for
17:
if costChildren = 0 then
18:
costChildren = â;
19:
end if
20:
costCurrN ode = nodeInclCost(n, q);
21:
if costCurrN ode â¤ costChildren then
22:
remove all descendants of n from c;
23:
add n to c;
24:
end if
25:
return min(costCurrN ode, costChildren)
26:
end if
27: end procedure

ï£¶
readCost(Bn ))ï£¸

nâ(âªqâQ ONq )/c

(3)
is the smallest among all cuts of the hierarchy H. Intuitively, the
cut is read into the memory once and for each query in Q the remaining operation nodes are brought to the memory as needed. The
first term in equation 3 is the cost of reading the bitmaps of the
nodes in c from the secondary storage into the memory. Once these
bitmaps have been read into the memory, we reuse them for further
query processing, i.e. the bitmaps of the cuts need to be read into
the memory only once. The second term denotes the cost of reading remaining bitmaps from the secondary storage every time it is
needed to execute a query. These remaining bitmaps are also read
only once and cached subsequently for further re-use for queries in
the workload.

2.3.4 Cut Selection Case 3: Multiple Queries with
Memory Constraints
The above formulations do not have any memory availability
constraints; i.e., as many bitmaps as needed can be read and cached
in memory for the given workload. In general, however, there may
be constraints on the amount of data we can cache in memory.
Therefore, we next consider scenarios where we have a constraint
on the amount of memory that can be used during query processing. Let us assume that we have a memory availability constraint
Stotal . Every bitmap has a size associated to it, SBn , denoting
the memory requirement of the bitmap file of node n in the main
memory. Given a query workload Q and Stotal , we want to find a
(potentially incomplete) cut c that minimizes the following cost:
ï£¶
(
) ï£«
â â
â
readCost(Bm )ï£¸
readCost(Bn ) + ï£­
nâc

3.1

As described in Section 2.2.2, queries can be processed using
inclusive, exclusive, and hybrid strategies. In this subsection,
we first provide three algorithms, corresponding to different
strategies, for the basic scenario with a single query without
memory constraint.

3.1.1 Inclusive Cut Selection (I-CS) Algorithm
The inclusive cut selection (I-CS) algorithm associates an inclusive cost to all nodes of the hierarchy and selects the cut using these inclusive costs. Given node v of the hierarchy H, let
l(v) = {mâ¥(m â leaf Desc(v)) â§ (Gq,m = 1)}. Formally, given
a query q and a node n on hierarchy H, we define the inclusive
cost, nodeInclCost(n, q), of the node in the cut as follows:
ï£±
â
if âmâleaf Desc(n) Gq,m = 0
ï£´
ï£´
ï£´
ï£´
ï£²readCost(Bn )
if âmâleaf Desc(n) Gq,m = 1
â
ï£´
readCost(Bm ) otherwise
ï£´
ï£´
ï£´
ï£³mâleaf Desc(n)

qâQ mâONq /c

(4)
subject to

â

SBn â¤ Stotal

Case 1: Single Query without Memory
Constraints

(5)

nâc

Note that the bitmaps for c are read into the memory once and for
each query in Q the remaining operation nodes are brought to the
memory as needed. The major difference from before is that due
to the constraint on the size of the nodes that can be maintained
in memory, c may be an incomplete cut. Moreover, the operation
nodes that are notâ
in the cut cannot be cached in memory for reuse
(unless Stotal > nâc SBn ).

â§Gq,m =1

Note that the inclusive cost is only applicable for the internal
nodes of a hierarchy; it is undefined for a leaf node.
In Alg. 1, we present the outline of the proposed algorithm which
uses the above definition of inclusive cost to find a cut c that gives
the optimal cost to execute a single range query q. Note that since
a valid cut does not include any leaf nodes, the algorithm considers
only the set of internal nodes, IH , of hierarchy H.
The inclusive cut selection algorithm presented in Alg. 1 is a
dynamic programming solution that traverses the nodes in the hierarchy in a bottom-up manner:

3. CUT SELECTION ALGORITHMS
As described in the previous section, query execution times can
be reduced if we are also given the bitmap indices for a subset of the
nodes in the domain hierarchy of the column. A key challenge is
to select the appropriate subset (or cut) of the hierarchy H to minimize the query processing cost, without adding significant memory
overhead. In this section, we present algorithms that search for a
cut, c, given a query q or a workflow of queries Q. It is important
to note that these algorithms do not directly return the operation
nodes required to execute q; instead they aim to find a cut, c, such
that there exists a set of operation nodes ONq â (c âª LH ) with a
small cost. Once a good cut of hierarchy is found, the necessary operation nodes ONq are identified in post-processing by searching
within the cut c.

â¢ In line 5 of the pseudo-code, the set children is empty for
a node on the second-to-last level of the hierarchy H, since
the input to the function f indChildren is the set of internal
nodes IH . Whenever the set children is empty, we add the
current node to the cut c, and return the inclusive cost of the
current node.

275

â¢ The condition on line 13 makes sure that the cost of children
of n does not include the cost when a child m has the cost â.
This will happen when none of the nodes in leaf Desc(m)
is a range node, i.e. q does not want the contents of m to be
included in the result of the query.

Given these node exclusive costs (which can again be computed
in O(1) time per node using a bottom-up algorithm), an optimal
exclusive cut can be find using a linear time algorithm similar to
the node inclusive cut algorithm presented in Alg. 1; the main difference being that each internal node in the hierarchy is associated with an exclusive cost, instead of an inclusive cost. In this
case, the results would be a cut c such that reading every node in
ONq â (c âª N Sq ), we can execute the query q optimally using the
exclusive strategy. If the output cut c is the root node of the hierarchy, then every node in N Sq has to be removed, i.e. an ANDNOT
operation has to be done between the root node and the nodes in
N Sq .

â¢ The condition on line 17 will be true if for every child m of
n, nodeInclCost(m) = â. This also means that no node
in leaf Desc(n) is a range node. In such a case, we want the
total cost of all the children of n to be equal to â.
â¢ The algorithm then compares the inclusive cost of the parent with the inclusive cost of the set of its children. If the
inclusive cost of the parent is cheaper than the combined inclusive cost of its children, then we remove the descendants
of n from c and add n to c. Otherwise, we keep the cut as it
is, since using the children of n is cheaper than using n.

3.1.3 Hybrid Cut Selection (H-CS) Algorithm
So far, we have considered inclusive and exclusive strategies independently from each other. However, we could consider both inclusive and exclusive strategies for each node in the hierarchy and
associate the better strategy to that node. In other words, we could
modify the linear-time, bottom-up algorithm presented in Alg. 1
using the following cost function for each internal node of the hierarchy, H:

If the resulting c contains only the root node of the hierarchy, then
it means that using the leaves is the cheapest option.
Note that the algorithm is very efficient: each internal node in
the hierarchy is considered only once and for each node only its
immediate children need to be considered; moreover, the function nodeInclCost(), which is called for each node, itself has a
bottom-up implementation with O(1) cost per node assuming that
node densities for each internal node has been computed ahead of
time Consequently, the cost of this algorithm is linear in the size of
the hierarchy, H.

nodeHybridCost(n, q) = min(

Unlike when searching for the inclusive or exclusive cuts of the
hierarchy, during the traversal, we also need to mark each node
as an inclusive-preferred or exclusive-preferred node based on the
contributor to the hybrid cost.
Naturally, in this case the resulting cut, c, can be partitioned into
two: an inclusive cut, ci (whose nodes are considered in an inclusive way), and an exclusive cut, ce (whose nodes are considered under the exclusive strategy). Those nodes that have a lower inclusive
cost are included in ci , whereas those that have a lower exclusive
cost are included in ce .

3.1.2 Exclusive Cut Selection (E-CS) Algorithm
Above, we considered the inclusive strategy which uses bitwise
OR operations among the selected bitmaps to execute the query q.
As we see in Section 4.1, this option may be costly when the query
ranges are large. Alternatively, we can identify query results using
an exclusive strategy: For a given query q, consider a leaf node
m such that Gq,m = 0. That means that this node is not a range
node. We call the leaf nodes (like m), which are outside of the
query range, the non-range nodes and denote them as N Sq . The
values of these leaf nodes are part of the actual data that q does not
want to be displayed in the result. The exclusive strategy, initially
introduced in Section 2.2.2, would first identify the non-range leaf
nodes and then use the rest to identify the query results.
Like the inclusive cost, we associate an exclusive cost to all internal nodes of the hierarchy. Consider an internal node n of the hierarchy. If every node in leaf Desc(n) is a range node, that means
that the q wants the content of n to be included in the result of the
query, i.e. leaf Desc(n) does not contain any non-range node. In
this case, we do not need to remove any node from n, and thus,
the exclusive cost of n is the read cost of the node n. Note, that in
the same scenario, the inclusive cost of n is also the read cost of n.
If, in contrast, none of the leaf descendants of n is a range node,
then the query results will not include n and in this case, the node
exclusive cost of n can be said to be â. The main difference is the
scenario when only some of leaf Desc(n) are non-range nodes.
In this case, the exclusive strategy removes the non-range nodes
from n, and thus, the exclusive cost of n is the read cost of reading all the non-range nodes under n, in addition to the read cost
of n. Based on these, we can formulate the node exclusive cost,
nodeExclCost(n, q) as follows:
ï£±
â
ï£´
ï£´
ï£´
ï£´
ï£²readCost(Bn )
ï£´
readCost(Bn )+
ï£´
ï£´
ï£´
ï£³

nodeInclCost(n, q),
nodeExclCost(n, q)).

â¢ If no leaf Desc(n) is in the range, then we call n, an empty
node. An empty node is not used in any query processing
and is ignored.
â¢ If all of leaf Desc(n) are in the range, then we call n, a
complete node. A complete node indicates that all the leaf
descendants of the node are needed for query processing.
Hence, both the inclusive and the exclusive costs of a complete node are same.
â¢ If only some of the leaf Desc(n) are part of the range, then
we call n, a partial node. Note that the only time n will have
potentially different inclusive and exclusive costs is when n
is a partial node. If a node is a partial node, we find both the
inclusive and exclusive costs, and choose the minimum of the
two costs. Subsequently, whichever cost is chosen, we label
the node accordingly as part of the inclusive or the exclusive
cut. This helps us in efficiently finding the operation nodes
as described further.
As we mentioned earlier, the algorithms described in this section return a cut c, but not the specific operation nodes that are
required to optimally execute the query q. Given a cut c, we need
an additional step in order to find the necessary operation nodes.
Alg. 2 provides the pseudo-code for finding the operation nodes
following execution of the H-CS algorithm. Here, the functions
nodeInclusiveCut(n, q) and nodeExclusiveCut(n, q) return
the set of operation nodes required to execute the relevant part of

if âmâleaf Desc(n) Gq,m = 0
if âmâleaf Desc(n) Gq,m = 1
â
mâleaf Desc(n)â§Gq,m =0 readCost(Bm )
otherwise

276

Algorithm 2 Finding the Operation Nodes
1: Input: Set of nodes c, Query q
2: Output: Set of operation nodes ONq
3: Initialize: ONq
4: procedure FIND O PERATION N ODES(c, q)
5:
for each node n in c do
6:
if n is a complete node then
7:
add n to ONq ;
8:
else if n is a partial node then
9:
inclusiveCost = nodeInclCost(n, q);
10:
exclusiveCost = nodeExclCost(n, q);
11:
if inclusiveCost â¤ exclusiveCost then
12:
add every node from nodeInclusiveCut(n, q) to ONq ;
13:
else
14:
add every node from nodeExclusiveCut(n, q) to ONq ;
15:
end if
16:
end if
17:
end for
18:
return ONq
19: end procedure

Algorithm 3 Hybrid Cut Multiple Query Algorithm
1: Input: Hierarchy H, Set of internal nodes IH , Query Workload Q
2: Output: Set of nodes c
3: Initialize: Node n = root, c
4: procedure FIND H YBRID C UT(n)
5:
Set children = f indChildren(n, IH );
6:
if children is empty then
7:
add n to c;
8:
return N CN odeCost(n, Q);
9:
else
10:
costChildren = 0;
11:
for each child m of n do
12:
costChildren = costChildren + costChild;
13:
end for
14:
costCurrN ode = N CN odeCost(n, Q);
15:
if costCurrN ode â¤ costChildren then
16:
remove all descendants of n from c;
17:
add n to c;
18:
end if
19:
end if
20:
return min(costCurrN ode, costChildren)
21: end procedure

the query q at an internal node n based on inclusive or exclusive
strategies, respectively and the algorithm follows the minimal cost
strategy to identify the operation nodes for the hybrid execution.
We explained our marking strategy earlier in this section. Based on
the marking of each node in the cut, we call the respective function
to get the corresponding inclusive or exclusive operation nodes.
Note that if the cut, c, includes the root of the hierarchy, then either
reading the nodes as part of the query range, or removing the nonrange nodes from the root is the cheapest option. This decision is
again made based on whether the root node was labeled as part of
the inclusive or exclusive cut. We do not need to recompute the two
individual costs to make that decision.

queryâs corresponding SNn,q are read:
N CN odeCost(n, Q) =
ï£«
(readCost(Bn )) + ï£­

â

ï£¶
readCost(Bm )ï£¸ .

mâ(âªqâQ SNn,q )/n

Intuitively, this cost tells us how important a particular node, n,
is relative to the query workload Q: If there are two nodes, na and
nb , such that na appears in SNna ,q for more than one query q â Q
and nb does not appear in any SNnb ,q for any q â Q, then the
N CN odeCost(na , Q) will be lower than N CN odeCost(nb , Q).
Consequently, we can say that a node that is included in the SNn,q
is more important (caching it would impact more queries) and such
important nodes have small N CN odeCost values. We use this
as the basis of our algorithm, shown in Alg. 3, to find the relevant hybrid cut given multiple queries. This bottom-up traversing algorithm is similar to the Hybrid Cut Algorithm explained in
the previous section. The main difference is that we use the cost
N CN odeCost(n, Q) for each node, which is derived using the
hybrid logic as explained in the previous section.

3.2 Case 2: Multiple Queries without Memory Constraint
In this previous section, we have shown that the simple case
where there is a single query to be executed can be handled in linear
time in the size of the hierarchy. In general, however, we may be
given a set of range queries and need to identify a cut of the hierarchy to help process this set of queries efficiently. In this subsection,
we present an algorithm to find a cut for multiple queries without
any memory constraints. We consider the more realistic case with
memory constraints in the next subsection.
Assume we are given a query workload Q that contains more
than one query (each with its corresponding range). Since we do
not have memory constraints, if a bitmap node in the hierarchy has
been read into the memory, it can also be cached to be reused by
other queries, without incurring any further read costs.
Remember that in Section 3.1.3 we have discussed how to find
a hybrid cut and the corresponding operation nodes given a single
query. Let us first assume that we use the algorithms discussed
in Section 3.1.3 to find the hybrid costs and the appropriate labeling for each query in the workload, Q, separately. In order to see
how important a particular node n is relative to a particular query
workload. Let us consider, Sub-Operation Nodes, SNn,q , which
denote the operation nodes required to execute the part of q (in
Q) that is under n. Hence, SNn,q will contain nodes that are in
n âª leaf Desc(n). In order to decide which nodes to choose in
the set n âª leaf Desc(n) given q, we use the same hybrid logic as
explained in Algorithm 2.
We associate to each node, n, in the hierarchy a new cost, called
no constraint node cost (N CN odeCost(n, Q)), defined as the cost
to perform the query workload such that (a) first the node is read
and cached into the memory and (b) the remaining nodes in each

3.3

Case 3: Multiple Queries with Memory
Constraint

In the previous subsection, we introduced a node cost (based on
the cost model as described in 2.3.3.) to capture the importance of a
node in a multiple query scenario without a memory constraint. In
this section, we relax the assumption of unlimited memory availability and consider the more general situation where we have a
memory constraint, limiting how many bitmaps we can keep in
memory at a time. More specifically, in this section, we present
two algorithms, namely 1-Cut Selection Algorithm and k-Cut Selection Algorithm, that find a cut given a query workload and a
memory constraint. Note that, as discussed in Section 2.3.4, due to
the memory constraint, the resulting cuts may be incomplete.
Let us consider a set of nodes for each query and each n,
called Constraint Operation Nodes, denoted by CONn,q . Here,
CONn,q â n âª LH . CONn,q chooses the set of nodes from
n âª LH that are required to execute q in the cheapest possible manner given n and the set of leaf nodes.
CONn,q consists of two sets of nodes. The first set is the set of
nodes that includes n and its leaf descendants. We have to decide
which nodes to choose in the set nâªleaf Desc(n) given q. In order
to make this decision, we use the same hybrid logic as explained in

277

Algorithm 4 1-Cut Selection Algorithm
1: Input: Hierarchy H, Set of internal nodes IH , Query Workload Q, Savailable
2: Output: Set of nodes c
3: Initialize: Savailable = Stotal
4: procedure FIND C UT C ONSTRAINT(DH , Savailable )
5:
while IH is not empty OR there exists a node n such that SBn â¤

Algorithm 5 k-Cut Selection Algorithm
1: Input: Hierarchy H, Set of internal nodes IH , Query Workload Q, Savailable ,
cutList
Output: Set of nodes c
Initialize: âcâcutList Sci ,available = Stotal .
procedure FINDK C UT C ONSTRAINT(H)
while each node n in IH is seen OR there exists a node n such that SBn â¤
Sci ,available for i â¤ k do
6:
choose node n such that n has the lowest CN odeCost(n, Q) among
nodes in H;
7:
mark n as seen;
8:
for each cut c in cutList do
9:
if SBn â¤ Sci ,available then
10:
if there is no conflict in c for node n then
11:
if n has not been added to any empty cut then
12:
add n to c;
13:
update Sci ,available = Sci ,available â SBn ;
14:
end if
15:
else
16:
copy each node in c to the next available empty cut;
17:
replace the conflicting node with node n;
18:
end if
19:
end if
20:
end for
21:
Sort the cutList based on the lowest cost for each cut;
22:
end while
23:
return the cut c in cutList that has the lowest total cost;
24: end procedure

2:
3:
4:
5:

Savailable do
choose node n such that n has the lowest CN odeCost(n, Q) among
nodes in IH & SBn â¤ Savailable ;
7:
add n to c;
8:
remove n from IH ;
9:
remove ancestors and descendants of n from IH ;
10:
update Savailable = Savailable â SBn ;
11:
end while
12:
return c
13: end procedure

6:

Algorithm 2. The second set of nodes, consists of the set of leaf
nodes that are not descendants of n , i.e. LH â© leaf Desc(n). In
order to execute q, all the query range nodes in this set have to be
read, and hence we include them in CONn,q .
As we have done in Case 2 (without memory constraints), we introduce a node cost to capture the importance of each internal node
in the hierarchy relative to query workload Q. This cost, called
constrained node cost (CN odeCost(n, Q)), reflects the cost of
performing the query in such a way that (a) only nodes with low
cost, and that can fit into the memory within the given constraint,
are read and cached into the memory and (b) the remaining nodes
in each queryâs CONn,q are read from the secondary storage as
needed.
CN odeCost(n, Q) =
ï£¶
ï£«
â
â
readCost(Bm )ï£¸
(readCost(Bn )) + ï£­

if for every q in Q, Pn,q does not include n, then the node is an
unused node.
It is important to note that the above algorithm does not necessarily return a cut that has the optimal cost. As we see in Section 4.3,
the sub-optimality of the algorithm is most apparent in situations
where we have plenty (yet still insufficient amount of) memory
and, consequently, the cost-sensitive greedy algorithm over-prunes
the solution space (though it still provides cuts that are significantly
more efficient than a naÃ¯ve execution plan). In situations where the
memory constraints are tight, however, the algorithm returns very
close to optimal or optimal cuts, proving the effectiveness of the
cost model and the proposed approach.

qâQ mâCONn,q /n

Intuitively, if more queries can reuse a node for further query
processing when the node is cached, the lower the constrained node
cost of the node is relative to the query workload Q.

3.3.1 1-Cut Selection Algorithm

3.3.2 k-Cut Selection Algorithm

In Alg. 4, we present the pseudo-code of 1-Cut Selection Algorithm, for Case 3 with multiple queries in the presence of a memory
constraint. Here, Savailable denotes the amount of memory available for adding nodes to a cut and SBn denotes the size of the
bitmap index of node n on the secondary storage. The first time the
algorithm is called, we initialize Savailable to the memory available
for the whole process, i.e., Stotal ; in subsequent calls, the amount
is reduced as new bitmaps are added to the cut. Note that

In this subsection, we note that the key weakness of the above
algorithm is that it considers only a single cut of the hierarchy:
When we choose to include a node in the cut, we remove all the
ancestors and descendants of the node from further consideration;
however, it is possible that a node can have the lowest cost, but
two or more of its ancestors or descendants combined can lead to a
better execution plan. A node n may be chosen before its ancestor
m, because cost(n) is lesser than cost(m). But, it is also possible
that choosing m could be a better choice than choosing n if m can
be used to execute a larger portion of the range nodes of the query.
Therefore, in Alg. 5, we present a variation of the algorithm,
called the k-Cut Selection Algorithm. In this variation, the algorithm considers k different cuts. When a node, n, is added to a
cut, the algorithm does not eliminate its ancestors and descendants
from further consideration; instead, it simply does not add these
ancestors and descendants to the same cut as n to follow the rules
of validity as described in Section 2.3.1. These ancestors and descendants however may be added to the other k-1 cuts.
In Algorithm 5, the ith cut has a corresponding memory requirement, Sci ,available .

â¢ In line 6, we choose a node that has the lowest node cost and
the size of the node is lesser than or equal to the remaining
memory availability.
â¢ In line 9, we ensure that the returned cut does not contain any
two nodes that are on the same root-to-leaf branch.
The stopping condition of the greedy process is reached when the
input set of nodes is empty (i.e. a complete cut is found) or when
each of the remaining nodes have sizes larger than Savailable . Note
that it is possible that in some cases the optimal subset of nodes required to execute the given query workload may all fit in the available memory. Our algorithm adds nodes until all nodes are seen
or no nodes can be added further due to memory constraints. In
order to avoid adding nodes that are not going to be used in query
processing, we introduce a new node label, unused, applied while
calculating the CN odeCost(n, Q) indicating that the node as unused if the node is not used by any query. This is easy to find out

â¢ In the algorithm, line 11 ensures that a node is not added
more than once to an empty cut. This prevents two cuts containing identical nodes.
â¢ Lines 16 and 17 are part of the replacement procedure. Ac-

278

Hybrid Cut

Leaf-only Cut

50
25

0
20

50

600

Inclusive Cut

Hybrid Cut

Leaf-only Cut

450
300
150
0
20

100

50

1000

Inclusive Cut

Exclusive Cut

Hybrid Cut

Leaf-only Cut

750
500
250
0
20

100

50

100

Hierarchy Size (Number of Leaf Nodes)

Hierarchy Size (Number of Leaf Nodes)

Hierarchy Size (Number of Leaf Nodes)

(b) synthetic data, 50% query range

(c) synthetic data, 90% query range

Amount of Data Read vs. Hierarchy Size,
Query Range Size=10%, TPC-H dataset

Amount of Data Read vs. Hierarchy Size,
Query Range Size=50%, TPC-H dataset

Amount of Data Read vs. Hierarchy Size,
Query Range Size=90%, TPC-H dataset

Inclusive Cut

160

Exclusive Cut

Hybrid Cut

Leaf-only Cut

120
80

40
0

20

50

600

Inclusive Cut

Exclusive Cut

Hybrid Cut

Amount of data read (in mb)

(a) synthetic data, 10% query range
Amount of data read (in mb)

Amount of data read (in mb)

Exclusive Cut

Amount of data read (in mb)

Exclusive Cut

75

Amount of data read (in mb)

Amount of data read (in mb)

Inclusive Cut

100

Amount of Data Read vs. Hierarchy Size,
Query Range Size=90%, Normal Dataset

Amount of Data Read vs. Hierarchy Size,
Query Range Size=50%, Normal Dataset

Amount of Data Read vs. Hierarchy Size,
Query Range Size=10%, Normal Dataset

Leaf-only Cut

450
300
150
0

100

20

Hierarchy Size (Number of Leaf Nodes)

50

1000

Inclusive Cut

Hybrid Cut

Leaf-only Cut

500
250
0
20

100

50

100

Hierarchy Size (Number of Leaf Nodes)

Hierarchy Size (Number of Leaf Nodes)

(d) TPC-H data, 10% query range

Exclusive Cut

750

(e) TPC-H data, 50% query range

(f) TPC-H data, 90% query range

Figure 2: Case 1, single query without memory constraints: effects of varying hierarchy and range sizes on the amount of data read
by the three different cut-selection algorithms

1000

Distribution of Nodes in Hybrid Cut, Queries=1,
Hierarchy Size = 100

Exhaustive Cut
Hybrid Cut

750

Average Cut
Leaf-only Cut

500

Inclusive-preferred

1.20
Distribution of Nodes
in Hybrid Cut

Amount of data read (in mb)

Amount of Data Read vs. Query Range Size,
Queries=1, Hierarchy Size = 100

Worst Cut

250

1.00

Empty Node

0.80

0.56

0.60
0.33

0.40
0.20

1.00

Exclusive-preferred

0.86

0.11

0.14
0.00

0.00

0.00

0.00

0
10%

50%

10%

90%

50%

90%

Query Range Size

Query Range Size

Figure 4: Case 1, single query without memory constraints:
percentages of nodes in each strategy (TPC-H data)

Figure 3: Case 1, single query without memory constraints:
comparing the proposed cut algorithm to (exhaustively found)
optimal, average, and worst cuts (TPC-H data)

set the value of k ahead of the time, we propose a Î´ auto-stop condition: after finding the iâth cut, we evaluate if costiâ1 âcosti < Î´,
for a user provided per-iteration cost gain value, Î´. The algorithm
auto-stops when the condition is satisfied (i.e., when the cost gain
of the iteration drops below the predetermined gain). In Section 4.3,
the auto-stop condition is effective, even when we simply set Î´ = 0;
i.e., we stop when the cost of the new cut has the same cost as the
previous cut (note that, for any two integers l, m > 1, and l > m,
the cost of l-greedy cut will always be equal to or lesser than the
cost of m-greedy cut; this is because whatever cut that is returned
by the m-greedy cut algorithm will always be enumerated and considered by the l-greedy cut algorithm).

cording to Section 2.3.1, a cut cannot have two nodes on the
same root-to-leaf branch. Hence, n cannot be added to the
existing cut if there is such a conflict. In these lines, when we
detect a conflict, we add the nodes of a cut to an empty cut
and replace the conflicting node with the current node. This
lets us construct multiple conflicting cuts that are individually conflict-free. Note that if after replacing the conflicting
node with the current node, the size of the cut exceeds the
size of available memory, then we ignore this node and the
corresponding conflicting cut.
â¢ In Line 21, we sort the cutList in ascending order based on
the overall cost of each discovered cut. We do this in order
to give more preference to the cuts with a lower cost during
the next iteration.

4.

EVALUATIONS

In order to evaluate the cut-selection algorithms presented in
this paper, we considered two datasets: (a) a synthetically generated dataset (with normal value distribution) and (b) the TPC-H
dataset [22], each with 150 million records. In particular, in the
TPC-H dataset, we focused on the account balance attribute whose
values demonstrate a near-uniform distribution, with spikes in the
occurrences for some values.
In this section, we have two main evaluation criteria: (1) query
execution IO cost and (2) optimization time. We compared the

3.3.3 Auto Selection of k
As we see in the next section, in practice it is sufficient to consider fairly small number of cuts to significantly improve the effectiveness of the proposed greedy algorithm (returning very close to
optimal cuts), without increasing the cost of the optimization step
significantly. However, in cases where it is difficult for the user to

279

Average Cut

Leaf-only Cut

Worst Cut

1200
800
400
0
5

15

25

Optimal Cut

Hybrid Cut

Average Cut

Leaf-only Cut

Worst Cut

1600

1200
800
400

0
5

15

Number of Queries

25

Optimal Cut
Amount of data read (in mb)

Hybrid Cut

1600

Amount of data read (in mb)

Amount of data read (in mb)

Optimal Cut

Amount of Data Read vs. Number of Queries,
Query Range Size=90%, Hierarchy Size = 100

Amount of Data Read vs. Number of Queries,
Query Range Size=50%, Hierarchy Size = 100

Amount of Data Read vs. Number of Queries,
Query Range Size=10%, Hierarchy Size = 100

Hybrid Cut

Leaf-only Cut

Worst Cut

800

400
0
5

15

Number of Queries

(a) 10% query range

Average Cut

1200

25

Number of Queries

(b) 50% query range

(c) 90% query range

Figure 5: Case 2, multiple queries without memory constraints (TPC-H data)

1-Cut

10-Cut

Average Cut

Worst Cut

2000
1500
1000
500
0
10%

30%

50%

70%

90%

Exhaustive Cut

1-Cut

Average Cut

Worst Cut

10000
7500

5000
2500
0

10%

30%

50%

70%

90%

Exhaustive Cut

1-Cut

(b) 50% query range

10-Cut

Average Cut

Worst Cut

10000
7500

5000
2500
0

10%

30%

Memory Availability

Memory Availability

(a) 10% query range

10-Cut

Amount of Data Read vs. Memory Availability,
Queries=15, Query Range Size=90%,
Hierarchy Size = 100
Amount of Data Read (in mb)

Exhaustive Cut

Amount of Data Read vs. Memory Availability,
Queries=15, Query Range Size=50%,
Hierarchy Size = 100
Amount of Data Read (in mb)

Amount of Data Read (in mb)

Amount of Data Read vs. Memory Availability,
Queries=15, Query Range Size=10%,
Hierarchy Size = 100

50%

70%

90%

Memory Availability

(c) 90% query range

Figure 6: Case 3, multiple queries with varying memory availability (TPC-H data)
results of our cut-selection algorithms against (a) leaf-only query
execution, (b) random cut-selection, and (c) exhaustive cut-search
strategies.
For both of the above data sets, we considered (balanced) attribute hierarchies of different depth and internal-node fanout:
these were generated for different numbers of leaf nodes and
maximum possible fanouts of the internal nodes of the hierarchy. Since finding the optimal cut using an exhaustive strategy
for comparison purposes is prohibitively expensive, we initially
considered small hierarchies, with 20, 50, and 100 leaf nodes and
heights of 4, 5, and 4 respectively (the root of the hierarchy being
considered at height 1).
In Section 4.4, we consider hierarchies of larger sizes and higher
number of queries to study the scalability of the cut-selection algorithms against the hierarchy size.
Bitmap indices were generated for the nodes of these hierarchies
using the Java library, WAH bitset [23] as explained in [10]. The
parameters of the read cost model presented in Section 2.2.1, and
shown in Figure 1 were computed based on these bitmap indices.
We have also created query workloads with different target range
sizes. For example, for a hierarchy of 100 leaf nodes, 10% query
range size indicates that each range query covers 10 consecutive
leaf nodes.
TM
R
We ran the experiments on a quad-core IntelâCore
i5-2400
CPU @ 3.10GHz machine with 8.00GB RAM. All codes are implemented and run using Java v1.7.

ficient than the inclusive strategy when the query ranges are larger.
Most importantly, in all cases, the hybrid strategy (H-CS) returns
the best cuts.
In Figure 3, we compare the hybrid (H-CS) strategy against (exhaustively found) optimal and average cuts. The figure also shows
the performance of the worst cut. As expected, the H-CS strategy
returns optimal cuts. On the average, randomly selecting a cut performs quite poorly (almost as bad as selecting the worst possible
cut), especially as the query range sizes increase. This highlights
the importance of utilizing an effective (hybrid) cut-selection algorithm for answering queries.
In Figure 4, we show the percentages of nodes that are labeled
inclusive-preferred or exclusive-preferred in a hybrid cut, as explained in Section 3.1.3, for different query ranges. As defined in
Section 3.1.3, empty nodes are nodes that are not used in query
processing. When the query range size is small, most of the query
processing can be done using the leaf nodes. Hence, we see in the
figure that most of the nodes in the cut are empty nodes. As expected, when the query range is small, the inclusive strategy dominates and when the range is large, the exclusive strategy dominates.
For ranges that are neither small nor large, the hybrid algorithm
leverages a mix of inclusive and exclusive strategies.

4.2

Case 2: Multiple Queries without Memory Constraints

In this section, we evaluate the hybrid cut selection algorithm
(Alg. 3) for query workloads with multiple queries. For our evaluations, we considered query workloads of different sizes (and with
different ranges). All reported costs are averages of the costs for 10
different runs.
Figure 5 shows the impact of using the proposed hybrid cut selection algorithm for different numbers of queries. As we see in
this figure, as expected, the hybrid cut selection algorithm returns
the optimal cut. The impact of the proposed cut selection algorithm
is especially strong when the query includes large ranges as when
there are large overlaps among the queries, the query evaluation algorithm has more opportunities for reusing cached nodes, and the
proposed hybrid cut strategy is able to leverage these opportunities

4.1 Case 1: Single Query without Memory
Constraints
We first evaluate the cut-selection algorithm for the single query
without memory constraints scenario. All reported costs are averages of the costs for 10 different runs.
Figures 2(a) through (f) compares the three different cutselection algorithms (I-CS, E-CS, and H-CS) presented in Section 3.1 for different data sets and varying hierarchy and range
query sizes. As we see in these charts, the inclusive strategy is
efficient when the query ranges are small; this is consistent with
the observation in [5]. The exclusive strategy, however, is more ef-

280

2.5

1-Cut

Auto-stop Cut

5-Cut

Amount of Data Read (in mb)

Cost Ratio of k-Greedy Cut with Exhaustive Cut vs.
Memory Availability, Queries=15,
Query Range Size=50%, Hierarchy Size=100
10-Cut

Cost Ratio

2
1.5
1
0.5
0
10%

30%

50%

70%

90%

Amount of Data Read vs. Query Range Size,
Queries=15, Hierarchy Size=100,
Memory Availability=90%
10000

Exhaustive Cut

Worst Cut

5000
2500
0
10%

50%

90%

Query Range Size

Figure 8: Case 3, effect of different query range sizes (TPC-H
data, 90% memory availability)

Figure 7: Case 3, multiple queries with varying memory availability (TPC-H data): impact of different k
most effectively.

Amount of Data Read (in mb)

4.3 Case 3: Multiple Queries under Memory
Constraints
In this section, we evaluate the effectiveness of the proposed khybrid cut algorithm (Alg. 5, described in Section 3.3.2), for multiple queries, but under memory constraints. We report the memory
availability in terms of the percentage of the memory needed to
store the bitmap indices corresponding to the maximum cut of the
given hierarchy. The presented results are averages of 10 different
runs.
Once again, we compare the proposed cut selection algorithm
against solutions found through exhaustive enumeration, average
solutions representing randomly selected cuts, and also the worst
solution. Remember, that under memory limitations, we need to
consider also the incomplete cuts of the input hierarchies.
Note that the number of incomplete cuts that an exhaustive algorithm would need to consider grows very fast:

Amount of Data Read vs. Number of Queries,
Query Range Size=50%, Hierarchy Size=100,
Memory Availability=90%
16000

Exhaustive Cut

10-Cut

Average Cut

Worst Cut

12000
8000
4000
0
5

15

25

Number of Queries

Figure 9: Case 3, effect of different number of queries (TPC-H
data, 90% memory availability)

Amount of Data Read vs. Hierarchy Size,
Queries=5, Query Range Size=50%,
Memory Availability=90%

Incomplete cuts
154
296,381
1,185,922

Amount of Data Read (in mb)

Height
4
5
4

Average Cut

7500

Memory Availability

Num. of leaves
20
50
100

10-Cut

However, since the number of incomplete cuts grow even faster
than the number of complete cuts, enumerating all incomplete cuts
for the exhaustive algorithm (which we use to locate the optimal cut
for comparison purposes), becomes prohibitive beyond hierarchies
with 100 leaf nodes.
Figure 6 shows that, in this case, the proposed hybrid cut selection algorithms are not optimal; however, they return cuts that are
very close to optimal. In fact, especially when the memory availability is very restricted (which is the expected situation in most
realistic deployments), even the 1-Cut algorithm is able to return
optimal or very close to optimal answers. As the available memory increases, the optimal cost decreases as there are more caching
opportunities, but 1-Cut strategy may not be able to leverage this
effectively, especially for larger query ranges. However, we see
that the multi-cut strategy (10-Cut in this figure) performs quite
close to optimal. Figure 7, which plots the ratio of the cost of the
solutions found by the multi-cut strategy (for different values of k)
to the cost of the optimal cut found through an exhaustive search,
confirms this observation: note the figure also shows that the autostop strategy described in Section 3.3.3 is effective in reducing the
cost, without having to fix the value k ahead of time.
Figures 8 through 10 further confirm that the proposed multi-cut
strategy is robust against changes in the size of the query ranges,

3000

Exhaustive Cut

10-Cut

Average Cut

Worst Cut

2250
1500
750
0
20

50

100

Hierarchy Size (Number of Leaf Nodes)

Figure 10: Case 3; effect of different hierarchy sizes (TPC-H
data, 90% memory availability)

number of queries, and hierarchy sizes.

4.4

Cut-Selection Time

Up to now, we considered query processing cost using cuts. We
now focus on the time needed to select cuts for hierarchies of different sizes. In Figures 11 and 12, we see the cut selection time as a
function of the size of the hierarchy (number of leaf nodes; i.e., the
size of the domain) and the number of queries, respectively. Please
note that, in these figures, we do not compare our algorithm with
exhaustively found cuts, and hence are able to consider larger hierarchy sizes and higher number of queries. The figures confirm
that the time taken to find the cut increases linearly with size of the
attribute domain and the number of queries.

281

systems. SIGMOD â06, pages 671â682, 2006.
[3] Oracle database 10g, 2013.
[4] Luciddb - home, 2013.
[5] J. Chmiel, T. Morzy, and R. Wrembel. Time-hobi: indexing
dimension hierarchies by means of hierarchically organized
bitmaps. DOLAP â10, 2010.
[6] S. Hong, B. Song, and S. Lee. Efficient execution of
range-aggregate queries in data warehouse environments. In
Conceptual Modeling ERâ01. 2001.
[7] Y. Feng and A. Makinouchi. Ag-tree: a novel structure for
range queries in data warehouse environments. DASFAAâ06,
pages 498â512, 2006.
[8] S. Muller and H. Plattner. Aggregates caching in columnar
in-memory databases. VLDB â13, 2013.
[9] T. Lauer, D. Mai, and P. Hagedorn. Efficient range-sum
queries along dimensional hierarchies in data cubes.
DBKDA â09, pages 7â12, 2009.
[10] K. Wu, E. Otoo, and A. Shoshani. On the performance of
bitmap indices for high cardinality attributes. VLDB â04,
pages 24â35, 2004.
[11] D. Rotem, K. Stockinger, and K. Wu. Optimizing candidate
check costs for bitmap indices. CIKM â05, 2005.
[12] O. Kaser, D. Lemire, and K. Aouiche. Histogram-aware
sorting for enhanced word-aligned compression in bitmap
indexes. DOLAP â08, 2008.
[13] K. Wu, K. Stockinger, and A. Shoshani. Breaking the curse
of cardinality on bitmap indexes. In Scientific and Statistical
Database Management, pages 348â365. 2008.
[14] R.R. Sinha, S. Mitra, and M. Winslett. Bitmap indexes for
large scientific data sets: a case study. 2006.
[15] R. Sinha and M. Winslett. Multi-resolution bitmap indexes
for scientific data. ACM Trans. Database Syst., August 2007.
[16] M. Morzy, T. Morzy, A. Nanopoulos, and Y. Manolopoulos.
Hierarchical bitmap index: An efficient and scalable
indexing technique for set-valued attributes. In Advances in
Databases and Information Systems, pages 236â252. 2003.
[17] M. Zaker, S. Phon-amnuaisuk, and S. Haw. An adequate
design for large data warehouse systems: Bitmap index
versus b-tree index, 2008.
[18] J. Chmiel, T. Morzy, and R. Wrembel. Hobi: Hierarchically
organized bitmap index for indexing dimensional data. In
DaWaK, pages 87â98, 2009.
[19] L. Bellatreche, R. Missaoui, H. Necir, and H. Drias.
Selection and pruning algorithms for bitmap index selection
problem using data mining. DaWaKâ07, pages 221â230,
2007.
[20] F. DeliÃ¨ge and T. Pedersen. Position list word aligned hybrid:
optimizing space and performance for compressed bitmaps.
EDBT â10, pages 228â239, 2010.
[21] K. Wu, E. J. Otoo, and A. Shoshani. An efficient
compression scheme for bitmap indices. Technical report,
ACM Transactions on Database Systems, 2004.
[22] Transaction Processing Performance Council. Tpc-h
benchmark specification, 2013.
[23] Compressedbitset - wah compressed bitset for java,
November 2007.

Optimization Time vs. Hierarchy Size,
Queries = 200, Range Nodes Size = 50%
Time required to find the cut
(in ms)

1400
1200
1000
800
600
400
200
0
0

500

1000

1500

2000

2500

3000

Hierarchy Size (Number of Leaf Nodes)

Figure 11: Effect of different hierarchy sizes on time taken to
find the hybrid cut
Optimization Time vs. Number of Queries,
Range Nodes Size = 50%, Hierarchy Size= 2000
Time required to find the cut
(in ms)

7000

6000
5000
4000
3000
2000
1000
0
0

200

400

600

800

1000

1200

Number of Queries

Figure 12: Effect of different number of queries on time taken
to find the hybrid cut

5. CONCLUSION
Column-stores use compressed bitmap-indices for answering
queries over data columns. When the data domain is hierarchical,
organizing the bitmap indices hierarchically can help more efficiently answer queries over different sub-ranges of the attribute
domain. In this paper, we showed that existing inclusive strategies
for leveraging hierarchically organized bitmap indices can be
sub-optimal in terms of their IO costs unless the query ranges are
small. We also showed that an exclusive (cut-selection) strategy
provides gains when the query ranges are large and that and
that a hybrid (cut-selection) strategy can provide best solutions,
improving over both strategies even when the ranges of interest
are relatively small. In this paper, we also presented algorithms
for implementing the hybrid strategy efficiently for a single query
or a workload of multiple queries, in scenarios with and without
memory limitations. In particular, we showed that when the
memory is constrained, selecting the right subset of bitmap indices
becomes difficult; but, we also showed that, even in this case, there
exists efficient cut-selection strategies that return close to optimal
results, especially in situations where the memory limitations are
very strict. Experiment results confirmed that the cut-selection
algorithms presented in this paper are efficient, scalable, and
highly-effective.

6. REFERENCES
[1] C. Ho, R. Agrawal, N. Megiddo, and R. Srikant. Range
queries in olap data cubes. SIGMOD â97, pages 73â88, 1997.
[2] D. Abadi, S. Madden, and M. Ferreira. Integrating
compression and execution in column-oriented database

282

SkySuite: A Framework of Skyline-Join Operators  for Static and Stream Environments
Mithila Nagendra
School of Computing, Informatics, and Decision Systems Engineering Arizona State University, Tempe, AZ, USA

K. Selc ¸ uk Candan
School of Computing, Informatics, and Decision Systems Engineering Arizona State University, Tempe, AZ, USA

mnagendra@asu.edu ABSTRACT
Efficient processing of skyline queries has been an area of growing interest over both static and stream environments. Most existing static and streaming techniques assume that the skyline query is applied to a single data source. Unfortunately, this is not true in many applications in which, due to the complexity of the schema, the skyline query may involve attributes belonging to multiple data sources. Recently, in the context of static environments, various hybrid skyline-join algorithms have been proposed. However, these algorithms suffer from several drawbacks: they often need to scan the data sources exhaustively in order to obtain the set of skyline-join results; moreover, the pruning techniques employed to eliminate the tuples are largely based on expensive pairwise tuple-to-tuple comparisons. On the other hand, most existing streaming methods focus on single stream skyline analysis, thus rendering these techniques unsuitable for applications that require a real-time "join" operation to be carried out before the skyline query can be answered. Based on these observations, we introduce and propose to demonstrate SkySuite: a framework of skyline-join operators that can be leveraged to efficiently process skyline-join queries over both static and stream environments. Among others, SkySuite includes (1) a novel Skyline-Sensitive Join (SSJ) operator that effectively processes skyline-join queries in static environments, and (2) a Layered Skyline-window-Join (LSJ) operator that incrementally maintains skyline-join results over stream environments.

candan@asu.edu
5 a

Restaurant Rating

b c

2 Dominance region of b 9 10 11 12 1

3

4

d 1

Time (at night)

Figure 1: Skyline of late-night restaurants in a feature space, the skyline of D consists of the points that are not dominated1 by any other data point in D [2]. Intuitively, the skyline is a set of interesting points that help paint the "bigger picture" of the data in question, providing insight into the diversity of the data across different features. Searching for non-dominated data is valuable in many applications that involve multi-criteria decision making [11]. For instance, students in a university who stay up late at night and need a snack at odd hours might find the skyline of late-night restaurants useful. Figure 1 shows the ratings and closing times of a set of restaurants: the points that are connected represent restaurants that are part of the skyline; this includes highest-rated restaurants that are open late into the night. Other restaurants are not part of the skyline because they are dominated in terms of time and/or rating by at least one restaurant that is in the skyline. The shaded area in Figure 1 is the dominance region of restaurant b : for any restaurant in this range, b is either open till a later time and/or has a better rating; therefore b is said to be more interesting than all restaurants it dominates. A particular shortcoming of existing static and stream skyline algorithms is that they primarily focus on singlesource skyline processing in which all required skyline attributes are present in the same source. However, there are many applications in both static and stream environments that require integration of data from different sources. In such scenarios, the skyline query may involve attributes belonging to different data sources, thus making the join operation an integral part of the overall process. For instance, in static environments integrated skyline-join queries maybe necessary over complex schemas in which the data is distributed onto many sources, whereas in stream environments such integration is needed for streams that originate from
1 A point dominates another point if it is as good or better in all dimensions, and better in at least one dimension.

1. INTRODUCTION
Recently, there has been a growing interest in the efficient processing of skyline queries over both static [5, 2] and stream environments [7, 14]. Given a set, D, of data points This work is supported by an NSF grant (#1116394 ­ RanKloud: Data Partitioning and Resource Allocation Strategies for Scalable Multimedia and Social Media Analysis ) and a KRF grant (A Framework for Real-time Context Monitoring in Sensor-rich Personal Mobile Environments ).
Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. To copy otherwise, to republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Articles from this volume were invited to present their results at The 39th International Conference on Very Large Data Bases, August 26th - 30th 2013, Riva del Garda, Trento, Italy. Proceedings of the VLDB Endowment, Vol. 6, No. 12 Copyright 2013 VLDB Endowment 2150-8097/13/10... $ 10.00.

1266

User Interface

Hybrid SSJ-LSJ Operator

2-ary

SSJ

n-ary

Hybrid SSJ-LSJ LSJ Operator

2-ary

LSJ

n-ary

S2J

S3J

Iteration-Fabric

Buffer

Stream 2 Local Disk Stream Stream 1

Buffer

Local Disk

Local Disk

Buffer

SKYSUITE

Figure 2: The SkySuite framework different sensors or from multiple sources in a distributed publish/subscribe architecture. Going back to our earlier example, in addition to the time and restaurant rating attributes shown in Figure 1, students might also consider the distance of a restaurant to the university to be a factor in their decision-making process. If this information is available from a different source, we would then need to join the relevant sources in order to obtain the restaurants that are part of the skyline. Motivated by the above observations, we propose SkySuite: a framework of skyline-join operators that can be used to process skyline-join queries over both static and stream environments (Figure 2). In particular, we demonstrate (1) the Skyline-Sensitive Join (SSJ) operator [8] that processes skyline-join queries in static environments, and (2) the Layered Skyline-window-Join (LSJ) operator [9] that incrementally maintains skyline-joins in stream environments. The rest of the paper is structured as follows: in Section 2, we give an overview of the existing work in the field of skyline query processing. Section 3 presents the suite of skylinejoin operators. In Section 4, we discuss the demonstration scenarios. Lastly, we conclude the paper in Section 5.

However, SFSJ does not carry out the pruning in a blockbased manner and largely depends on time-consuming tupleto-tuple comparisons to find the pruned region. The SkylineSensitive Join (SSJ) operator, demonstrated in this paper, overcomes this drawback by pruning the join space in terms of blocks of data, as opposed to individual data points, thereby avoiding excessive point-to-point dominance checks. Over the last decade, the advent of a wide array of streambased applications has necessitated a push towards the development of algorithms that take into consideration the constant changes in stream environments. The following sections provide an overview of the existing work in the fields of skyline and join query processing over streaming data.

SSJ Operator

2.3

Join Processing over Data Streams

[17] presents a symmetric hash join method that is optimized for in-memory performance. Following this, a plethora of techniques have been developed for processing join queries over data streams [6, 15]. Many of these focus on eliminating redundancy in join processing to maximize the output rate [6]. Others focus on memory; they present join processing and load shedding techniques that minimize loss in accuracy when the memory is insufficient [15].

2.4

Skyline Processing over Data Streams

2. RELATED WORK
The task of finding the non-dominated set of data points was attempted by Kung et al. [5] in 1975 under the name of the maximum vector problem. Kung's algorithm lead to the development of various skyline algorithms designed for static [2, 16] and stream environments [7, 14].

2.1 Skylines over a Single Static Data Source
Borzsonyi et al. [2] were the first to coin and investigate the skyline computation problem in the context of databases. Later contributions to skyline query processing include sortbased techniques (SFS [3]), progressive methods (bitmap and index [13]), and online algorithms [10].

2.2 Skylines on Multiple Static Data Sources
Some of the prior work on skylines over multiple static data sources include [4, 12, 16]. Sun et al. [12] introduce an operator called skyline-join, and two algorithms to support skyline-join queries. The first extends the SaLSa algorithm [1] to cope with multiple relations, whereas the second algorithm (Iterative ) prunes the search space iteratively. More recently, Vlachou et al. [16] introduced the SortFirst-Skyline-Join (SFSJ) algorithm that fuses the identification of skyline tuples with the computation of the join. SFSJ provides a way to prune the input tuples if they do not contribute to the set of skyline-join results, thus reducing the number of generated join results and dominance checks.

As mentioned earlier, in the conventional setting of static data, there is a large body of work for both single-source skyline processing [2, 3, 1] and multiple source skyline-join processing [12, 16]. These methods assume that the data is unchanging during query execution and focus on computing a single skyline rather than continuously tracking skyline changes. Recently, several algorithms have been developed to track skyline changes over data streams. These methods continuously monitor the changes in the skyline according to the arrival of new tuples and expiration of old ones. Data stream skyline processing under the sliding window model is addressed in [7] and [14]. An important issue that needs to be addressed here is the expiration of skyline objects. To tackle this issue, Tao et al. present the Eager algorithm [14] that employs an event list, while Lin et al. propose a method (StabSky ) that leverages dominance graphs [7]. Both these methods memorize the relationship between a current skyline object and its successor(s). Once skyline objects expire, their successor(s) can be presented as the updated skyline without any added computation. The above-mentioned approaches focus on skyline queries in which the skyline attributes belong to a single stream, thus rendering them inapplicable to the problem of computing skyline-joins over multiple streams. In this paper, we demonstrate the novel Layered Skyline-window-Join (LSJ) operator; this operator is first of its kind for answering skyline-window-join (SWJ) queries over data streams.

3.

SKYSUITE

This section introduces SkySuite: a framework of skylinejoin operators for processing skyline-join queries over both static and stream environments (Figure 2). In particular, we explain the methodologies behind the Skyline-Sensitive Join (SSJ) and Layered Skyline-window-Join (LSJ) operators.

3.1 SSJ Operator for Static Environments
At the core of the SSJ operator are two skyline-join algorithms, namely S2 J (skyline-sensitive join ) and S3 J (symmetric skyline-sensitive join ) [8]. Both S2 J and S3 J are single-

1267

Execution Ti ime (sec, log scale)

1E+04

Execution Time Gain (%)

60

Execution Time Gain (%)

S2J SFSJRR PrefJoin

S3J SFSJSC Iterative

Effect of Window Size (=100, SWJ of sensors at 10m)
60

Effect of Window Shift Length (=1000, SWJ of sensors at 10m)

40

40

1E+02

20

LSJ vs. Naïve N ï LSJ vs. ISJ LSJ vs. LSJ (l=1)
500 1000 2000

20

LSJ vs. Naïve N ï LSJ vs. ISJ LSJ vs. LSJ (l=1)
50 100 200

0

0

1E+00 Correlated Independent antiCorrelated

Window Size ()

Window Shift Length ()

Data Distribution

(a) Effect of window size

(b) Effect of shift length

Figure 3: S2 J and S3 J efficiently process skyline-join queries over static data [8]

 
            
3 
 
      

Figure 5: The LSJ operator effectively handles skyline-join queries over streaming data [9] the data streams, and eliminates redundant work between consecutive windows by leveraging shared skyline objects across all iteration layers of skyline-join processing. LSJ is based on the observation that the consecutive iterations of the algorithm, spanning multiple windows, can be viewed as separate iteration layers (Figure 4(a)). The key insight here is that overlaps exist not only at the lowest data layer (across consecutive data windows), but also at the individual iteration layers, where the tuples processed can be considered as "virtual streams" that evolve from one window to the next (see Figure 4(b) for a sample execution). Therefore, we argue that if we naively execute the SWJ operation by applying the iterative skyline join algorithm separately for each window, we can end up with significant amount of redundant work. We further argue that if we can quickly identify and eliminate these per-layer overlaps, we can achieve significant savings in processing time. Based on these insights, we develop the iterationfabric [9]; this forms the backbone of the LSJ operator. The iteration-fabric helps combine the advantages of two existing skyline methods, StabSky [7] and Iterative [12], in developing a Layered Skyline-window-Join (LSJ) operator that maintains skyline-join results in an incremental manner by continuously monitoring the changes in the input streams and leveraging any overlaps that exist between the data considered at individual layers of consecutive sliding windows. The efficiency of the LSJ operator compared to Naive, ISJ [9], and LSJ (l = 1) (where LSJ is applied only at the first layer of each window) is illustrated in the sample results shown in Figure 5. Please refer to [9] for further details.

  
       
 
 
! $*)3 # *)3     %
*))
 *&+

0. .) +. ) ) . *) *. +) +. ,)

 +&,  ,& -&.  .&/

,.


 
! '

(a)

(b)

Figure 4: (a) Viewing layers as separate "virtual streams" that feed the upper layers of iteration; (b) Sample SWJ execution for 6 consecutive windows (10% new and 10% expiring tuples per window): the plots show that the skyline-join process iterate somewhere between 20 to 35 times for different windows and the overlaps (among consecutive windows) of tuples considered at different layers of iterations remain high across layers of iteration pass, two-way skyline-join algorithms that avoid tuple-totuple dominance checks wherever possible. These algorithms rely on a novel layer/region pruning (LR-pruning) strategy in order to avoid excessive pairwise dominance checks. The key features of S2 J are as follows: · The tuples in the outer table are organized into layers of dominance. · The tuples of the inner table are clustered into regions based on the Z-values of the skyline attributes to support block-based pruning. · A trie-based data structure on the inner table keeps track of the so-called dominated, not-dominated, and partially-dominated regions of the inner table relative to the layers of the outer table. · S2 J obtains the skyline set by scanning the outer table, only once, while pruning the inner table. S 3 J is similar to S2 J in principle, but repeatedly swaps the roles of the outer and inner tables. One key outcome of this strategy is that (unlike S2 J, where the outer table is fully scanned), S3 J rarely needs to scan any of the input tables entirely in order to obtain the set of skyline points. The effectiveness of S2 J and S3 J compared to the SFSJ methods [16], PrefJoin [4], and iterative skyline-join [12] can be seen in the sample experimental result shown in Figure 3. Please refer to [8] for further details.

4.

USER INTERACTION SCENARIOS

This section describes the demonstration scenarios. We will use real data sets (JCI building energy simulation/observation2 , NBA3 and Intel Berkeley Research4 ) and the TPC-H benchmark data sets5 . Through an interactive graphical user interface, the attendees of this demonstration will be able to experience the suite of skyline-join operators up close and personal. Described next, are some example user interactive demonstration scenarios.

4.1

Two-way Skyline-Joins over Static Data

This scenario demonstrates how the SSJ operator is designed to handle skyline-joins between two static data sets.
2 JCI is an energy IT company, with access to model, simulation, and sensory data for buildings of all types and sizes. 3 http://skyline.dbai.tuwien.ac.at/datasets/nba/. 4 http://db.csail.mit.edu/labdata/labdata.html. 5 http://www.tpc.org/tpch/default.asp.

3.2 LSJ Operator for Stream Environments
The LSJ operator processes SWJ queries over two data streams by maintaining skyline-join results in a layered, incremental manner. It continuously monitors the changes in

1268

As seen in Figure 2, the SSJ operator utilizes the S2 J and S3 J algorithms, interchangeably, to execute skyline-join queries. This query scenario is executed over the NBA and TPC-H benchmark data sets. Attendees of this demonstration will be able to compare the performance of S2 J and S3 J against other algorithms, and will be able to observe the behaviour of the SSJ operator over different skyline-join queries. Example 1 (SSJ Operation). Give two tables, Player-points (playerID, points, fieldGoals) and Player-assists (playerID, assists, freeThrows), both derived from the NBA data set, a skyline-join query over Player-points and Player-assists could be: Skyline = SSJ * FROM Player-points P, Player-assists A, WHERE P.playerID = A.playerID, points MAX, fieldGoals MAX, assists MAX, freeThrows MAX. This query equi-joins the tables on playerID and returns results that are in the skyline based on the attributes points, fieldGoals, assists, freeThrows. Intuitively, this query obtains the skyline of good offensive players in the NBA. 

5.

CONCLUSION

This demonstration introduces SkySuite: a framework of skyline-join operators that can be leveraged to efficiently process skyline-join queries over both static and stream environments. In particular, we demonstrate the SkylineSensitive Join (SSJ) and the Layered Skyline-window-Join (LSJ) operators. The SSJ operator overcomes the drawbacks of existing static skyline-join algorithms by pruning the join space in terms of blocks of data, as opposed to individual data points, thereby avoiding excessive point-topoint dominance checks. While, the LSJ operator provides an efficient technique for computing skyline-joins over pairs of streams. LSJ is first of its kind for answering skylinewindow-join (SWJ) queries over data streams.

6.

ACKNOWLEDGMENTS

We would like to thank Dr. Youngchoon Park of Johnson Controls, Inc. (JCI) for allowing us access to JCI data sets.

4.2 Two-way SWJ queries over Data Streams
Through this scenario, we demonstrate how the LSJ operator handles skyline-window-join (SWJ) queries between two input data streams (Figure 2). The LSJ operator utilizes the iteration-fabric framework to run SWJ queries over the Intel Berkeley Research lab data streams. Attendees will have the opportunity to view the behaviour of the LSJ operator and observe the advantages that the iteration-fabric provides over other alternative solutions. Example 2 (LSJ Operation). Consider a scenario in which a set of sensors produce readings only related to temperature and voltage, while another set of sensors give readings of humidity and light. This results in two input streams, namely stream-1 (moteid, temperature, voltage, epoch) and stream-2 (moteid, humidity, light, epoch). Given these, a SWJ query over the set of sensors on the attributes temperature, voltage, humidity and light could be: Skyline = SWJ * FROM stream-1 S1, stream-2 S2, WHERE S1.moteid = S2.moteid, S1.epoch within last 24 hours, S2.epoch within last 24 hours, temperature MAX, voltage MAX, humidity MAX, light MAX. This query returns a set of interesting readings produced by the sensors over the past 24 hours. 

4.3 Other Skyline-Join Operations
In this scenario, we demonstrate SkySuite's ability to process skyline-join queries over multiple data sources. Additionally, we also show how SkySuite handles a scenario in which one of the data sources is static, while the other is streaming. As show in Figure 2, SkySuite utilizes a hybrid form of the SSJ and LSJ operators to tackle skyline-join queries over hybrid input sources.

[1] I. Bartolini, P. Ciaccia, and M. Patella. SaLSa: computing the skyline without scanning the whole sky. In CIKM, pages 405­414, 2006. [2] S. B¨ orzs¨ onyi, D. Kossmann, and K. Stocker. The Skyline operator. In ICDE, pages 421­430, 2001. [3] J. Chomicki, P. Godfrey, J. Gryz, and D. Liang. Skyline with presorting. In ICDE, pages 717­719, 2003. [4] M. E. Khalefa, M. F. Mokbel, and J. J. Levandoski. Prefjoin: An efficient preference-aware join operator. In ICDE, pages 995­1006, 2011. [5] H.-T. Kung, F. Luccio, and F. P. Preparata. On finding the maxima of a set of vectors. J. ACM, 22(4):469­476, 1975. [6] H.-G. Li, S. Chen, J. Tatemura, D. Agrawal, K. S. Candan, and W.-P. Hsiung. Safety guarantee of continuous join queries over punctuated data streams. In VLDB, pages 19­30, 2006. [7] X. Lin, Y. Yuan, W. Wang, and H. Lu. Stabbing the sky: Efficient skyline computation over sliding windows. In ICDE, pages 502­513, 2005. [8] M. Nagendra and K. S. Candan. Skyline-sensitive joins with LR-pruning. In EDBT, pages 252­263, 2012. [9] M. Nagendra and K. S. Candan. Layered processing of skyline-window-join (SWJ) queries using iteration-fabric. In ICDE, 2013 (to be published). [10] D. Papadias, Y. Tao, G. Fu, and B. Seeger. Progressive skyline computation in database systems. ACM Trans. Database Syst., 30(1):41­82, 2005. [11] R. E. Steuer. Multiple Criteria Optimization: Theory, Computation and Application. John Wiley, 1986. [12] D. Sun, S. Wu, J. Li, and A. K. H. Tung. Skyline-join in distributed databases. In ICDE Workshop, pages 176­181, 2008. [13] K.-L. Tan, P.-K. Eng, and B. C. Ooi. Efficient progressive skyline computation. In VLDB, pages 301­310, 2001. [14] Y. Tao and D. Papadias. Maintaining sliding window skylines on data streams. TKDE, 18(3):377­391, 2006. [15] N. Tatbul and S. Zdonik. Window-aware load shedding for aggregation queries over data streams. In VLDB, pages 799­810, 2006. [16] A. Vlachou, C. Doulkeridis, and N. Polyzotis. Skyline query processing over joins. In SIGMOD, pages 73­84, 2011. [17] A. N. Wilschut and P. M. G. Apers. Dataflow query execution in a parallel main-memory environment. In PDIS, pages 68­77, 1991.

7.

REFERENCES

1269

Compressed Spatial Hierarchical Bitmap (cSHB) Indexes
â
for Efficiently Processing Spatial Range Query Workloads
Parth Nagarkar

K. SelÃ§uk Candan

Aneesha Bhat

Arizona State University
Tempe, AZ 85287-8809, USA

Arizona State University
Tempe, AZ 85287-8809, USA

Arizona State University
Tempe, AZ 85287-8809, USA

nagarkar@asu.edu

candan@asu.edu

aneesha.bhat@asu.edu

ABSTRACT

terms of their coordinates in 2D space. Queries in this 2D
space are then processed using multidimensional/spatial index structures that help quick access to the data [28].

In most spatial data management applications, objects are
represented in terms of their coordinates in a 2-dimensional
space and search queries in this space are processed using
spatial index structures. On the other hand, bitmap-based
indexing, especially thanks to the compression opportunities bitmaps provide, has been shown to be highly eï¬ective
for query processing workloads including selection and aggregation operations. In this paper, we show that bitmapbased indexing can also be highly eï¬ective for managing
spatial data sets. More speciï¬cally, we propose a novel compressed spatial hierarchical bitmap (cSHB) index structure
to support spatial range queries. We consider query workloads involving multiple range queries over spatial data and
introduce and consider the problem of bitmap selection for
identifying the appropriate subset of the bitmap ï¬les for processing the given spatial range query workload. We develop
cost models for compressed domain range query processing
and present query planning algorithms that not only select
index nodes for query processing, but also associate appropriate bitwise logical operations to identify the data objects
satisfying the range queries in the given workload. Experiment results conï¬rm the eï¬ciency and eï¬ectiveness of the
proposed compressed spatial hierarchical bitmap (cSHB) index structure and the range query planning algorithms in
supporting spatial range query workloads.

1.

1.1 Spatial Data Structures

INTRODUCTION

Spatial and mobile applications are gaining in popularity, thanks to the wide-spread use of mobile devices, coupled with increasing availability of very detailed spatial
data (such as Google Maps and OpenStreetMap [3]), and
location-aware services (such as FourSquare and Yelp). For
implementing range queries (Section 3.1.2), many of these
applications and services rely on spatial database management systems, which represent objects in the database in
âThis work was supported by NSF grants 1116394, 1339835,
and 1318788
This work is licensed under the Creative Commons AttributionNonCommercial-NoDerivs 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/3.0/. Obtain permission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 41st International Conference on
Very Large Data Bases, August 31st - September 4th 2015, Kohala Coast,
Hawaii.
Proceedings of the VLDB Endowment, Vol. 8, No. 12
Copyright 2015 VLDB Endowment 2150-8097/15/08.

The key principle behind most indexing mechanisms is to
ensure that data objects closer to each other in the data
space are also closer to each other on the storage medium.
In the case of 1D data, this task is relatively easy as the
total order implicit in the 1D space helps sorting the objects so that they can be stored in a way that satisï¬es the
above principle. When the space in which the objects are
embedded has more than one dimension, however, the data
has multiple degrees of freedom and, as a consequence, there
are many diï¬erent ways in which the data can be ordered
on the storage medium and this complicates the design of
search data structures. One common approach to developing index structures for multi-dimensional data is to partition the space hierarchically in such a way that (a) nearby
points fall into the same partition and (b) point pairs that
are far from each other fall into diï¬erent partitions. The resulting hierarchy of partitions then can either be organized
in the form of trees (such as quadtrees, KD-trees, R-trees
and their many variants [28]) or, alternatively, the root-toleaf partition paths can be serialized in the form of strings
and these strings can be stored in a string-speciï¬c search
structure. Apache Lucene, a highly-popular search engine,
for example, leverages such serializations of quadtree partitions to store spatial data in a spatial preï¬x tree [1].
An alternative to applying the partitioning process in the
given multi-dimensional space is to map the coordinates of
the data into a 1D space and perform indexing and query
processing on this 1D space instead. Intuitively, in this alternative, one seeks an embedding from the 2D space to a
1D space such that (a) data objects closer to each other in
the original space are also closer to each other on the 1D
space, and (b) data objects further away from each other
in the original space are also further away from each other
on the 1D space. This embedding is often achieved through
fractal-based space-ï¬lling curves [11, 17]. In particular, the
Peano-Hilbert curve [17] and Z-order curve [23] have been
shown to be very eï¬ective in helping cluster nearby objects
in the space. Consequently, if data are stored in an order
implied by the space-ï¬lling curve, then the data elements
that are nearby in the data space are also clustered, thus
enabling eï¬cient retrieval. In this paper, we leverage these
properties of space-ï¬lling curves to develop a highly compressible bitmap-based index structure for spatial data.

1382

hierarchical bitmap ï¬les and (b) propose eï¬cient bitmap selection algorithms that select the best bitmap nodes from the
cSHB index structure to be fetched into the main-memory
for processing of the query workload. In this paper, we also
present an eï¬cient disk-based organization of compressed
bitmaps. To our best knowledge, this is the ï¬rst work that
provides an eï¬cient index structure to execute a query workload involving multiple spatial range queries by using bitmap
indexes. Experimental evaluations of the cSHB index structure and the bitmap selection algorithms show that cSHB is
highly eï¬cient in answering a given query workload.

Results
Query
Workload

Identify
Cut & Leaf Bitmaps

Construct
Result Bitmaps

Cutt Bitmaps
C
Bit
Buffer
Buffe
B
ff

Leaf Bitmaps Buffer

1.4 Paper Organization

Cut Bitmap
Cut Bitmap
Cut Bitmap
Cut Bitmap
Bitm
itmap
Leaf Bitmaps

Compressed Spatial Hierarchical Bitmaps (cSHB)

Figure 1: Processing a range query workload using
compressed spatial hierarchical bitmap (cSHB)

1.2 Bitmap-based Indexing
Bitmap indexes [29, 32] have been shown to be highly
eï¬ective in answering queries in data warehouses [34] and
column-oriented data stores [5]. There are two chief reasons for this: (a) ï¬rst of all, bitmap indexes provide an eï¬cient way to evaluate logical conditions on large data sets
thanks to eï¬cient implementations of the bitwise logical
âANDâ, âORâ, and âNOTâ operations; (b) secondly, especially when data satisfying a particular predicate are clustered, bitmap indexes provide signiï¬cant opportunities for
compression, enabling either reduced I/O or, even, complete
in-memory maintenance of large index structures. In addition, (c) existence of compression algorithms [15, 33] that
support compressed domain implementations of the bitwise
logical operations enables query processors to operate directly on compressed bitmaps without having to decompress
them until the query processing is over and the results are
to be fetched from the disk to be presented to the user.

1.3 Contributions of this Paper
In this paper, we show that bitmap-based indexing is also
an eï¬ective solution for managing spatial data sets. More
speciï¬cally, we ï¬rst propose compressed spatial hierarchical bitmap (cSHB) indexes to support spatial range queries.
In particular, we (a) convert the given 2D space into a 1D
space using Z-order traversal, (b) create a hierarchical representation of the resulting 2D space, where each node of the
hierarchy corresponds to a (sub-)quadrant (i.e., eï¬ectively
creating an implicit âquadtreeâ), and (c) associate a bitmap
ï¬le to each node in the quadtree representing the data elements that fall in the corresponding partition. We present
eï¬cient algorithms for answering range queries using a select
subset of bitmap ï¬les stored in a given cSHB index.
We then consider a service provider that has to answer
multiple concurrent queries over the same spatial data and,
thus, focus on query workloads involving multiple range
queries. Since the same set of queries can be answered using
diï¬erent subsets of the bitmaps in the cSHB index structure, we consider the problem of identifying the appropriate
bitmap nodes for processing the given query workload. More
speciï¬cally, as we visualize in Figure 1, (a) we develop cost
models for range query processing over compressed spatial

This paper is organized as follows. In the next section we
provide an overview of the related work. In Section 3.1, we
introduce the key concepts and notations, and in Section 3.2,
we present the proposed cSHB index structure. Then, in
Section 4, we describe how query workloads are processed using cSHB: in Section 4.1, we introduce the concepts of range
query plans, in Section 4.2, we present cost models for alternative execution strategies, and in Section 4.3, we present
algorithms for ï¬nding eï¬cient query plans for a given range
query workload. Experiment results are reported in Section 5. We ï¬nally conclude the paper in Section 6.

2. RELATED WORK
2.1 Multi-Dimensional Space Partitioning
Multi-dimensional space partitioning strategies can be
categorized into two: In the ï¬rst case, including quadtree,
BD-tree, G-Tree, and KD-tree variants, a given bounded region is divided into two or more âopenâ partitions such that
each partition borders a boundary of the input region. In
the latter case, some of the partitions (often referred to as
minimum bounding regions, MBRs) are âclosedâ regions of
the space, not necessarily bordering any boundary of the input region. An advantage of this latter category of index
structures, including the R-tree and its variants (R*-tree,
R+-tree, Hilbert R-tree, and others), is that these MBRs
can tightly cover the input data objects.
While most index structures have been designed to process individual queries, there are also works focusing on the
execution of a workload of multiple queries on the same index structure. In [27], the Hilbert values of the centroids
of the rectangles formed by the range queries are sorted,
and these queries are grouped accordingly to process them
over an R-tree. In [14], R-trees are used to execute multiple
range queries and, in their formulation, authors propose to
combine adjacent queries into one. Thus, the algorithm is
not able to diï¬erentiate results of individual queries.
There are two problems commonly associated with multidimensional index structures, namely overlaps between partitions (which cause redundant I/O) and empty spaces
within partitions (which cause unnecessary I/O). While
there has been a signiï¬cant amount of research in searching
for partitioning strategies that do not face these problems,
these two issues still remain [26] and are especially critical
in very high-dimensional vector spaces. One way to tackle
this problem has been to parallelize the work. For example,
in [6], the authors describe a Hadoop-based data warehousing system with spatial support, where the main focus is to
parallelize the building of the R*-tree index structure and
query processing over Hadoop.

1383

       


       

								 															








 

  



] 
]

Figure 2: Z-order curve for a sample 2D Space.

2.2 Space Filling Curve based Indexing
The two most common space-ï¬lling curves are the fractalbased Z-order curve [23] and the Peano-Hilbert curve [17].
While the Hilbert curve provides a better mapping from
the multidimensional space onto the 1D space, its generation is a complicated and costly process [26]. With Z-order
curve, however, mapping back-and-forth between the multidimensional space and the 1D space using a process called
bit-shuï¬ing (visualized in Figure 2) is very simple and eï¬cient. Consequently, the Z-order curve has been leveraged
to deal with spatial challenges [12], including construction
of and searches on R-trees [27] and others.
In [35], the authors present a parallel spatial query processing system called VegaGiStore that is built on top of
Hadoop. This system uses a two-tiered index structure that
consists of a quadtree-based global index (used for ï¬nding
the necessary data blocks) and a Hilbert-ordering based local
index (used for ï¬nding the spatial objects in the data block).
In [26], the authors present an index called BLOCK to process spatial range queries. Their main assumption is that
the data and index can ï¬t into the main memory, and hence
their aim is to reduce the number of comparisons between
the data points and the query range. They create a sorted
list of the Z-order values for all the data points. Given a
query, they start at the coarsest level. If a block lies entirely
within the given query range, they retrieve the data points
in this block, otherwise, based on a branching fact, they decide whether to search the next granular level. In [7], the
authors proposed a UB-tree index structure that also uses
Z-ordering for storing multidimensional data in a B+ tree
and in [22], the authors presented a hierarchical clustering
scheme for the fact table of a data warehouse in which the
data is stored using the above mentioned UB-tree. In [31],
the authors present a range query algorithm speciï¬cally for
the UB-tree. Unlike our approach, the above solutions are
not speciï¬cally designed for multiple query workloads.

2.3 Bitmap Indexes
There have been signiï¬cant amount of works on improving the performance of bitmap indexes and keeping compression rates high [20, 32, 33]. Most of the newer compression
algorithms use run-length encoding for compression: this
provides a good compression ratio and enables bitwise operations directly on compressed bitmaps without having to
decompress them ï¬rst [32]. Consequently, bitmap indexes
are also shown to perform better than other database index structures, especially in data warehouses and columnoriented systems [5, 32, 34].
For attributes with a large number of distinct values,
bitmaps are often created with binning, where the domain

is partitioned into bins and a bitmap is created for each bin.
Given a query, results are constructed by combining relevant bins using bitwise OR operations. Recognizing that
many data attributes have hierarchical domains, there has
also been research in the area of multi-level and hierarchical
bitmap indexes [13, 24, 25, 29]. When the bitmaps are partitioned (with potential overlaps), it is necessary to select an
appropriate set (or cut [25]) of bitmaps for query processing; results are often obtained by identifying a set of bitmaps
and combining them using bitwise ORs. This work builds on
some of the ideas presented in [25] from 1D data to spatial
data. In [25], the cost model only considered the dominant
I/O cost (reading the bitmaps from the disk), but in this
work, we present an updated cost model, that appropriately
includes the I/O cost as well as the cost of performing local
operations on the in-memory bitmaps.
There has been some prior attempts to leverage bitmaps
in spatial query processing. For example, an MBR-based
spatial index structure is proposed in [30], where the leaves
of the tree are encoded in the form of bitmaps. Given a
query, the proposed HSB-index is traversed top-down (as in
R-trees) to identify the relevant bitmaps to be combined.
In this paper, we note that not only leaves, but also internal nodes of the spatial hierarchy can be encoded as
bitmaps, leading to signiï¬cant savings in range search time,
especially for query workloads consisting of multiple spatial
range queries. Thus, our work focuses on which bitmaps
to read in the context of spatial range query workloads and
we introduce novel algorithms to choose which bitmaps to
use to answer a query workload eï¬ciently. We generalize
the problem of bitmap selection and consider alternative
strategies that complement OR-based result construction.
In [16], authors propose a storage and retrieval mechanism
for large multi-dimensional HDF5 ï¬les by using bitmap indexes. While range queries are supported on their architecture, they neither leverage Z-order indexing, nor hierarchical bitmaps as proposed in this work. Also, their proposed
mechanism is not optimized for multiple query workloads.

3. COMPRESSED SPATIAL HIERARCHICAL BITMAP (cSHB) INDEXES
In this section, we present the key concepts used in the paper and introduce the compressed spatial hierarchical bitmap
(cSHB) index structure for answering spatial range queries.

3.1 Key Concepts and Notations
3.1.1 Spatial Database
A multidimensional database, D, consists of points that
belong to a (bounded and of ï¬nite-granularity) multidimensional space S with d dimensions. A spatial database is a
special case where d = 2. We consider rectangular spaces
such that the boundaries of S can be described using a pair
of south-west and a north-east corner points, csw and cne
(csw .x â¤ cne .x and csw .y â¤ cne .y and âpâS csw .x â¤ p.x â¤
cne .x and csw .y â¤ p.y â¤ cne .y).

3.1.2 Spatial Query Workload
In this paper, we consider query workloads, Q, consisting
of a set of rectangular spatial range queries.
â¢ Spatial Range Query: A range query, q â Q, is
deï¬ned by a corresponding range speciï¬cation q.rs =

1384

qsw , qne , consisting of a south-west point and a northeast point, such that qsw .x â¤ qne .x and qsw .y â¤ qne .y.

root
00****

0000**

â¢ Leaves of the hierarchy: LH denotes the set of leaf
nodes of the hierarchy H and correspond to all potential point positions of the ï¬nite space S. Assuming that
the database, D, contains only points, only the leaves
of the spatial hierarchy occur in the database.

â¢ Children of a node: For all ni , children(ni ) denotes
the children of ni in the corresponding hierarchy; if
ni â LH , then children(ni ) = â. In this paper, we assume that the children induce a partition of the region
corresponding to the parent node:
â

â
Sh â  .

nh âchildren(ni )

â¢ Descendants of a Node: The set of descendants of
node ni in the corresponding hierarchy is denoted as
desc(ni ). Naturally, if ni â LH , then desc(ni ) = â.
â¢ Internal Nodes: Any node in H that is not a leaf
node is called an internal node. The set of internal
nodes of H is denoted by IH . Each internal node in
the hierarchy corresponds to a (non-point) sub-region
of the given space. If N (H, l) denotes the subset of the
nodes at level l of the hierarchy H, then we have
â
â



ân =n âN (H,l) Si â©Sj = â and âS =
Si â  .
i

A cSHB index structure can be created based on any hierarchy satisfying the requirements1 speciï¬ed in Section 3.1.3.
In this paper, without loss of generality, we discuss a Zcurve based construction scheme for cSHB. The resulting
hierarchy is analogous to the MX-quadtree data structure,
where all the leaves are at the same level and a given region
is always partitioned to its quadrants at the center [28]. As
introduced in Sections 1.1 and 2.2, a space-ï¬lling curve is a
fractal that maps a given ï¬nite multidimensional data space
onto a 1D curve, while preserving the locality of the multidimensional data points (Figure 2): in other words nearby
points in the data space tend to be mapped to nearby points
on the 1D curve. As we also discussed earlier, Z-curve is a
fractal commonly used as a space-ï¬lling curve (thanks to its
eï¬ectiveness in clustering the points in the data space and
the eï¬ciency with which the mapping can be computed).
A key advantage of the Z-order curve (for our work) is
that, due to the iterative (and self-similar) nature of the underlying fractal, the Z-curve can also be used to impose a
hierarchy on the space. As visualized in Figure 3, each internal node, ni , in the resulting hierarchy has four children
corresponding to the four quadrants of the space, Si . Consequently, given a 2h -by-2h space, this leads to an (h + 1)-level
hierarchy, (analogous to an MX-quadtree [28]) which can be
used to construct a cSHB index structure2 . As we show
in Section 5, this leads to highly compressible bitmaps and
eï¬cient execution plans.

3.2.2 Blocked Organization of Compressed Bitmaps

j

ni âN (H,l)

The root node corresponds to the entire space, S.
â¢ Leaf Descendants of a Node: Leaf descendants,
leaf Desc(ni ), of a node are the set of nodes such that
leaf Desc(ni ) = desc(ni ) â© LH .

3.2 Compressed Spatial Hierarchical Bitmap
(cSHB) Index Structure
In this section, we introduce the proposed compressed spatial hierarchical bitmap (cSHB) index structure:
Definition 3.1 (cSHB Index Stucture). Given a
spatial database D consisting of a space, S, and a spatial
hierarchy, H, a cSHB index is a set, B of bitmaps, such
that for each ni â N (H), there is a corresponding bitmap,
Bi â B, where the following holds:

ÍÍÍÍ ÍÍÍ
001000 001001 001010 001011

3.2.1 Our Implementation of cSHB

â¢ Parent of a node: For all ni , parent(ni ) denotes the
parent of ni in the corresponding hierarchy; if ni is the
root, then parent(ni ) = â¥.

nh =nj âchildren(ni )

0011**

â¢ if ni is an internal node (i.e., ni 	â IH ), then
âoâD ânh âleaf Desc(ni ) located at(o, nh ) â (Bi [o] =
1), whereas
â¢ if ni is a leaf node
	 (i.e., ni â LH ), then
â¦
âoâD located at(o, ni ) â (Bi [o] = 1)

â¢ Nodes of the hierarchy: Intuitively, each node, ni â
N (H) corresponds to a (bounded) subspace, Si â S,
described by a pair of corner points, ci,sw and ci,nw .



0010**

Figure 3: A sample 4-level hierarchy deï¬ned on the
Z-order space deï¬ned in Figure 2 (the string associated to each node corresponds to its unique label)

In cSHB, we associate to the space S a hierarchy H, which
consists of the node set N (H) = {n1 , . . . , nmaxn }:

â

Sh â©Sj = â and âSi =

0001**

ÍÍÍÍ
000000 000001 000010 000011

3.1.3 Spatial Hierarchy



ÍÍ
ÍÍÍ

Given a range query, q, with a range speciï¬cation,
q.rs = qsw , qne , a data point p â D is said to be
contained within the query range (or is a range point)
if and only if qsw .x â¤ p.x â¤ qne .x and qsw .y â¤ p.y â¤
qne .y.

Given a spatial database, D, with a corresponding hierarchy, H, we create and store a compressed bitmap for each
node in the hierarchy, except for those that correspond to
regions that are empty. These bitmaps are created in a
bottom-up manner, starting from the leaves (which encode
for each point in space, S, which data objects in D are located at that point) and merging bitmaps of children nodes
into the bitmaps of their parents. Each resulting bitmap is
stored as a compressed ï¬le on disk.
It is important to note that, while compression provides
signiï¬cant savings in storage and execution time, a naive
storage of compressed bitmaps can still be detrimental for
1
In fact, cSHB can be created even when some of the requirements are relaxed â for example children do not need
to cover the parent range entirely (as in R-trees).
2
Without loss of generality, we assume that the width and
height are 2h units for some integer h â¥ 1.

1385

000 001 010 011 100 101 110 111

nj âchildren(ni )

18:
19:
20:
21:
22:
23:
24:
25:

end if
if size(Bi ) â¥ K then
write Bi to disk;
else
T = append(T, Bi )
availableSize = availableSize â size(Bi )
if (availableSize â¤ 0) or (ni is the last
node at this level) then
write T to disk;
Block T = â
availableSize = K
end if
end if
end for
end for
end procedure

performance: in particular, in a data set with large number
of objects located at unique points, there is a possibility that
a very large number of leaf bitmaps need to be created on
the secondary storage. Thus, creating a separate bitmap ï¬le
for each node may lead to ineï¬ciencies in indexing as well as
during query processing (as directory and ï¬le management
overhead of these bitmaps may be non-negligible).
To overcome this problem, cSHB takes a target block size,
K, as input and ensures that all index-ï¬les written to the
disk (with the possible exception of the last bitmap ï¬le in
each level) are at least K bytes. This is achieved by concatenating, if needed, compressed bitmap ï¬les (corresponding to
nodes at the same level of hierarchy). In Algorithm 1, we
provide an overview of this block-based bottom-up cSHB index creation process. In Line 10, we see that the bitmap of
an internal node is created by performing a bitwise OR operation between the bitmaps of the children of the node. These
OR operations are implemented in the compressed bitmap
domain enabling fast creation of the bitmap hierarchy. As it
creates compressed bitmaps, the algorithm packs them into
a block (Line 15). When the size of the block exceeds K,
the compressed bitmaps in the block are written to the disk
(Line 18) as a single ï¬le and the block is re-initialized.
Example 3.1. Let us assume that K = 10 and also that
we are considering the following sequence of nodes with the
associated (compressed) bitmap sizes:
n1 , 3; n2 , 4; n3 , 2; n4 , 15; n5 , 3; . . .
This sequence of nodes will lead to following sequence of
bitmap ï¬les materialized on disk:
[B4 ] ; [B1 B2 B3 B5 ] ; . . .






size=15

size=3+4+2+3=12

Note that, since the bitmap for node n4 is larger than the
target block size, B4 is written to disk as a separate bitmap

0

5 6 7

5 6 7
3 4

1

2

3 4

5 6 7

Range
11100X2
[56,57]

2
1

1
0

(a) 2D query range

0

3 4

2

â¢ Minimum block size, K

11:
12:
13:
14:
15:
16:
17:

2

3 4

â¢ A spatial database, D, deï¬ned over 2h -by-2h size space, S
and a corresponding (h + 1)-level (Z-curve based) hierarchy,
H, with set of internal nodes, IH

2: procedure writeBitmaps
3:
Block T = â
4:
availableSize = K
5:
for level l = (h + 1) (i.e., leaves) to 0 (i.e., root) do
6:
for each node ni in l in increasing Z-order do
7:
if l == (h + 1) then
8:
Initialize a compressed bitmap Bi
9:
else
10:
Bi =
OR
Bj

1

5 6 7

0

000 001 010 011 100
0 101 110 111

Algorithm 1 Writing blocks of compressed bitmaps to disk
1: Input:

Range
1100XX1
[48,51]

(b) Corresponding 1D ranges

Figure 4: Mapping of a single spatial range query
to two 1D ranges on the Z-order space: (a) A contiguous 2D query range, [sw = (4, 4); ne = (6, 5)] and
(b) the corresponding contiguous 1D ranges, [48,51]
and [56,57], on the Z-curve
ï¬le; on the other hand, bitmaps for nodes n1 , n2 , n3 , and
n5 need to be concatenated into a single ï¬le to obtain a block
larger than K = 10 units.
3
Note that this block-based structure implies that the size
of the ï¬les and the number of bitmap ï¬les on the disk will be
upper bounded, but it also means that the cost of the bitmap
reads will be lower bounded by K. Therefore, to obtain
the best performance, repeated access to a block to fetch
diï¬erent bitmaps must be avoided through bitmap buï¬ering
and/or bitmap request clustering. In the next section, we
discuss the use of cSHB index for range query processing. In
Section 5, we experimentally analyze the impact of block-size
on the performance of the proposed cSHB index structure.

4. QUERY PROCESSING WITH THE cSHB
INDEX STRUCTURE
In this section, we describe how query workloads are processed using the cSHB index structure. In particular, we
consider query workloads involving multiple range queries
and propose spatial bitmap selection algorithms that select
a subset of the bitmap nodes from the cSHB index structure
for eï¬cient processing of the query workload.

4.1 Range Query Plans and Operating Nodes
In order to utilize the cSHB index for answering a spatial
range query, we ï¬rst need to map the range speciï¬cation
associated with the given query from the 2D space to the
1D space (deï¬ned by the Z-curve). As we see in Figure 4,
due to the way the Z-curve spans the 2D-space, it is possible
that a single contiguous query range in the 2D space may
be mapped to multiple contiguous ranges on the 1D space.
Therefore, given a 2D range query, q, we denote the resulting
set of (disjoint) 1D range speciï¬cations, as RSq .
Let us be given a query, q, with the set of 1D range speciï¬cations, RSq . Naturally, there may be many diï¬erent ways
to process the query, each using a diï¬erent set of bitmaps
in the cSHB index structure, including simply fetching and
combining only the relevant leaf bitmaps:
Example 4.1 (Alternative Range Query Plans).
Consider a query q with q.rs = (1, 0), (3, 1) on
the space shown in Figure 2.
The corresponding 1D range, [2, 11], would cover the following
leaf nodes of the hierarchy shown in Figure 3:
RSq = (000010, 000011, 001000, 001001, 001010, 001011).
The following are some of the alternative query plans for q
using the proposed cSHB index structure:

1386

â¢ Inclusive query plans: The most straightforward way
to execute the query would be to combine (bitwise OR
operation) the bitmaps of the leaf nodes covered in 1D
range, [2, 11]. We refer to such plans, which construct
the result by combining bitmaps of selected nodes using
the OR operator, as inclusive plans.
An alternative inclusive plan for this query would be to
combine the bitmaps of nodes 000010, 000011, 0010**:
B000010 OR B000011 OR B0010ââ .
â¢ Exclusive query plans: In general, an exclusive query
plan includes removal of some of the children or descendant bitmaps from the bitmaps of a parent or ancestor through the ANDNOT operation. One such exclusive plan would be to combine the bitmaps of all leafs
nodes, except for B000010 , B000011 , B001000 , B001001 ,
B001010 , B001011 , into a bitmap Bnon result and return
Broot ANDNOTBnon result .
â¢ Hybrid query plans: Both inclusive and exclusive only
query plans may miss eï¬cient query processing alternatives. Hybrid plans combine inclusive and exclusive
strategies at diï¬erent nodes of the hierarchy. A sample
hybrid query plan for the above query would be

	
B0000ââ ANDNOT (B000000 OR B000001 ) OR B0010ââ . 3
As illustrated in the above example, a range query, q, on
hierarchy H, can be answered using diï¬erent query plans,
involving bitmaps of the leaves and certain internal nodes of
the hierarchy, collectively referred to as the operating nodes
of a query plan. In Section 4.3, we present algorithms for selecting the operating nodes for a given workload, Q; but ï¬rst
we discuss the cost model that drives the selection process.

4.2 Cost Models and Execution Strategies
In cSHB, the bitwise operations needed to construct the
result are performed on compressed bitmaps directly, without having to decompress them.

4.2.1 Cost Model for Individual Operations
We consider two cases: (a) logical operations on diskresident compressed bitmaps and (b) logical operations on
in-buï¬er compressed bitmaps.

Operations on Disk-Resident Compressed Bitmaps.
In general, when the logical operations are implemented
on compressed bitmaps that reside on the disk, the time
taken to read a bitmap from the secondary storage to the
main memory dominates the overall bitwise manipulation
time [15]. The overall cost is hence proportional to the size
of the (compressed) bitmap ï¬le on the secondary storage.
Let us consider a logical operation on bitmaps Bi and Bj .
Let us assume that T (Bi ) and T (Bj ) denotes the blocks in
which Bi and Bj are stored, respectively. Since multiple
bitmaps can be stored in a single block, it is possible that
Bi and Bj are in the same block. Hence, let us further
assume that T(Bi ,Bj ) is the set of unique blocks that contain
the bitmaps, Bi and Bj . Then the overall I/O cost is:
	
 
costio (Bi op Bj ) = Î±IO
size(T ) ,
T âT(B ,B )
i
j

where Î±IO is an I/O cost multiplier and op is a binary bitwise
logical operator. A similar result also holds for the unary
operation NOT.

Operations on In-Buffer Compressed Bitmaps.
When the compressed bitmaps on which the logical operations are implemented are already in-memory, the disk
access cost is not a factor. However, also in this case, the
cost is proportional to the sizes of the compressed bitmap
ï¬les in the memory, independent of the speciï¬c logical operator that is involved [33], leading to
	

costcpu (Bi op Bj ) = Î±cpu size(Bi ) + size(Bj ) ,
where Î±cpu is the CPU cost multiplier. A similar result also
holds for the unary operation NOT.

4.2.2 Cost Models for Multiple Operations
In this section, we consider a cost model which assumes
that blocks are disk-resident. Therefore, we consider a storage hierarchy consisting of disk (storing all bitmaps), RAM
(as buï¬er storing relevant bitmaps), and L3/L2 caches (storing currently needed bitmaps).

Buffered Strategy.
In the buï¬ered strategy, visualized in Figure 1, the
bitmaps that correspond to any leaf or non-leaf operating
nodes for the query plan of a given query workload, Q, are
brought into the buï¬er once and cached for later use. Then,
for each query q â Q, the corresponding result bitmap is extracted using these buï¬ered operating node bitmaps. Consequently, if a node is an operating one for more than one
q â Q, it is read from the disk only once (and once for each
query from the memory). Let us assume that TONQ denotes
the set of unique blocks that contains all the necessary operating nodes given a query workload Q(ONQ ). This leads
to the overall processing cost, time costbuf (Q, ONQ ), of
â
â
â
â

 
size(T )â  + Î±cpu â
size(Bi )â  .
Î±IO â



T âTON

Q



read cost






qâQ ni âONq





operating cost

Since all operating nodes need to be buï¬ered, this execution
 strategy requires a total of storage costbuf (Q, ONQ ) =
Note that, in general,
ni âONQ size(Bi ) buï¬er space.
Î±IO > Î±cpu . However, in Section 5, we see that the number
of queries in the query workload and query ranges determine
the relative costs of in-buï¬er operations vs. disk I/O.
The buï¬ered strategy has the advantage that each query
can be processed individually on the buï¬ered bitmaps and
the results for each completed query can be pipelined to the
next operator without waiting for the results of the other
queries in the workload. This reduces the memory needed
to temporarily store the result bitmaps. However, in the
buï¬ered strategy, the buï¬er needed to store the operating
node bitmaps can be large.

Incremental Strategy.
The incremental strategy avoids buï¬ering of all operating
node bitmaps simultaneously. Instead, all leaf and non-leaf
operating nodes are fetched from the disk one at a time on
demand and results for each query are constructed incrementally. This is achieved by considering one internal operating
node at a time and, for each query, focusing only on the leaf
operating nodes under that internal node. For this purpose,
a result accumulator bitmap, Resj , is maintained for each
query in qj â Q and each operating node read from the disk
is applied directly on this result accumulator bitmap.

1387

	 




#!!"&
 

If a set of internal nodes of H only satisï¬es the ï¬rst condition, then we refer to the cut as an incomplete cut.
â¦



















Figure 5: Buï¬er misses and the overall read time
(data and other details are presented in Section 5)
While it does not need buï¬er to store all operating node
bitmaps, the incremental strategy may also beneï¬t from partial caching of the relevant blocks. This is because, while
each internal node needs to be accessed only once, each
leaf node under this internal node may need to be brought
to the memory for multiple queries. Moreover, since the
data is organized in terms of blocks, rather than individual nodes (Section 3.2.2), a single block may serve multiple
nodes to diï¬erent queries. When suï¬cient buï¬er is available
to store the working set of blocks (containing the operating
leaf nodes under the current internal node), the execution
cost, time costinc (Q, ONQ ), of the incremental strategy is
identical to that of the buï¬ered strategy. Otherwise, as illustrated in Figure 5, the read cost component is a function of
buï¬er misses, Î±IO Ã # buf f er misses, which itself depends
on the size of the buï¬er and the clustering of the data.
3
The storage complexity is storage costinc (Q, ONQ ) =
size(Res
)
plus
the
space needed to maintain the
j
qj âQ
most recently read blocks in the current working set. Experiments reported in Section 5 show that, for the considered
data sets, the sizes of the working sets are small enough to
ï¬t into the L3-caches of many modern hardware.

4.3 Selecting the Operating Bitmaps for a
Given Query Workload
To process a range query workload, Q, on a data set, D,
with the underlying cSHB hierarchy H, we need to select
a set of operating bitmap nodes, ONQ , of H from which
we can construct the results for all qj â Q, such that
time cost(Q, ONQ ) is the minimum among all possible sets
of operating bitmaps for Q. It is easy to see that the number
of alternative sets of operating bitmaps for a given query
workload Q is exponential in the size of the hierarchy H.
Therefore, instead of seeking the set of operating bitmaps
among all subsets of the nodes in H, we focus our attention
on the cuts of the hierarchy, deï¬ned as follows:
Definition 4.1 (Cuts of H Relative to Q). A
complete cut, C, of a hierarchy, H, relative to a query load,
Q, is a subset of the internal nodes (including the root) of
the hierarchy, satisfying the following two conditions:
â¢ validity: there is exactly one node on any root-to-leaf
branch in a given cut; and
â¢ completeness: the nodes in C collectively cover every
possible root-to-leaf branch for all leaf nodes in the result sets for queries in Q.
3
The space complexity of the incremental strategy can be
upper-bounded if the results for the queries in Q can be
pipelined to the next set of operators progressively as partial
results constructed incrementally.

As visualized in Figure 1, given a cut C, cSHB queries are
processed by using only the bitmaps of the nodes in this
cut, along with some of the leaf bitmaps necessary
to construct results of the queries in Q. In the rest of
this subsection, we ï¬rst describe how queries are processed
given a cut, C, of H and then present algorithms that search
for a cut, C, given a workload, Q.

4.3.1 Range Query Processing with Cuts
It is easy to see that any workload, Q, of queries can be
processed by any (even incomplete) cut, C, of the hierarchy
and a suitable set of leaf nodes: Let Rq denote the set of
leaf nodes that appear in the result set of query q â Q and
RÌq be the set of leaf nodes that do not appear in the result
set. Let also RqC be the set of the result leaves covered by a
node in C. Then, one possible way to construct the result
bitmap, Bq , is as follows:
â
â



â
â
â
â
OR
Bq = â OR Bi OR
Bi â ANDNOT
Bj
.


â ni âC
â  nj âRC
ni âRq \RC
q
q â©RÌq
exclusions




inclusions

Intuitively any result nodes that are not covered by the cut
need to be included in the result using a bitwise OR operation, whereas any leaf node that is not in any result needs
to be excluded using an ANDNOT operation. Consequently,
â¢ if C â© Rq = â, an inclusion-only plan is necessary,
â¢ an exclusion-only plan is possible only if C covers Rq
completely.
Naturally, given a range query workload, Q, diï¬erent query
plans with diï¬erent cuts will have diï¬erent execution costs.
The challenge is, then,
â¢ to select an appropriate cut, C, of the hierarchy, H,
for query workload, Q, and
â¢ to pick, for each query qj â Q, a subset Cj â C for
processing qj ,
in such a way that these will minimize the overall processing
cost for the set of range queries in Q. Intuitively, we want to
include in the cut, those nodes that will not lead to a large
number of exclusions and cannot be cheaply constructed by
combining bitmaps of the leaf nodes using OR operations.

4.3.2 Cut Bitmap Selection Process
Given the above cut-based query processing model, in this
section we propose a cut selection algorithm consisting of
two steps: (a) a per-node cost estimation step and (b) a
bottom-up cut-node selection step. We next describe each
of these two steps.

Node Cost Estimation.
First, the process assigns an estimated cost to those hierarchy nodes that are relevant to the given query workload,
Q. For this, the algorithm traverses through the hierarchy,
H, in a top-down manner and identiï¬es part, R, of the hierarchy relevant for the execution of at least one query, q â Q
(i.e., for at least one query, q, the range associated with the
node and the query range intersect). Note that this process

1388

â¢ Exclusive leaf access plan (Line 18): If query, q, is executed using an exclusive leaf access plan at node, ni ,
this means that the result for the range (q.rs â© Si ) will
be obtained by using Bi and then identifying and excluding (using bitwise ANDNOT operations) all irrelevant leaf bitmaps under node ni . Thus, we compute
the exclusive leaf access plan cost, ecost(ni , q), of this
query at node ni as

Algorithm 2 Cost and Leaf Access Plan Assignment Algorithm
1: Input: Hierarchy H, Query Workload Q
2: Outputs: Query workload, Q(ni ), and cost estimate, costi ,

3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:

for each node, ni â H; leaf access plan, Ei,j , for all
node/query pairs ni â H and qj â Q(ni ); a set, R â IH ,
or relevant internal nodes
Initialize: R = â
procedure Cost and LeafAccessPlanAssignment
for each internal node ni â IH in top-down fashion do
if ni = âroot then
Q(ni ) = Q
else
Q(ni ) = {q â Q(parent(ni )) s.t. (q.rs â© Si ) =
â}
end if
if Q(ni ) = â then
add ni into R
end if
end for
for each node ni â R in a bottom-up fashion do
for qj â Q(ni ) do
Compute icost(ni , q)
Compute ecost(ni , q)
Compute the leaf access plan, Ei,j , as
Ei,j = [ecost(ni , qj ) < icost(ni , qj )]
end for

 Compute the leaf access cost, leaf costi , as
qj âQ(ni ) Ei,j Ã ecost(ni , qj ) + (1 â Ei,j ) Ã icost(ni , qj )
end for
end procedure

also converts the range in 2-D space into 1-D space by identifying the relevant nodes in the hierarchy. Next, for each
internal node, ni â R, a cost, costi , is estimated assuming
that this node and its leaf descendants are used for identifying the matches in the range Si . The outline of this process
is presented in Algorithm 2 and is detailed below:
â¢ Top-Down Traversal and Pruning. Line 5 indicates that
the process starts at the root and moves towards the leaves.
For each internal node, ni , being visited, ï¬rst, the set,
Q(ni ) â Q, of queries for which ni is relevant is identiï¬ed by
intersecting the ranges of the queries relevant to the parent
(i.e., Q(parent(ni ))) with the range of ni . More speciï¬cally,
Q(ni ) = {q â Q(parent(ni )) s.t. (q.rs â© Si ) = â}.
If Q(ni ) = â, then ni and all its descendants are ignored,
otherwise ni is included in the set R.
â¢ Inclusive and Exclusive Cost Computation. Once the portion, R, of the hierarchy relevant to the query workload is
identiï¬ed, next, the algorithm re-visits all internal nodes in
R in a bottom-up manner and computes a cost estimate for
executing queries in Q(ni ): for each query, q â Q(ni ), the
algorithm computes inclusive and exclusive leaf access costs:
â¢ Inclusive leaf access plan (Line 17): If query, q, is executed using an inclusive plan at node, ni , this means
that the result for the range (q.rsâ©Si ) will be obtained
by identifying and combining (using bitwise ORs) all
relevant leaf bitmaps under node ni . Therefore, the
cost of this leaf access plan is

icost(ni , q) =
size(Bj ).
(nj âleaf Desc(ni ))â§((q.rsâ©Sj )=â)

This value can be computed incrementally, simply by
summing up the inclusive costs of the children of ni .

ecost(ni , q)

= size(Bi )
+



size(Bj )

(nj âleaf Desc(ni ))â§((q.rsâ©Sj )=â)

or equivalently as
ecost(ni , q)

â

= size(Bi ) + â



â
size(Bj )â 

nj âleaf Desc(ni )

â icost(ni , q)
Since the initial two terms above are recorded in the
index creation time, the computation of exclusive cost
is a constant time operation.
â¢ Overall Cost Estimation and the Leaf Access Plan. Given
the above, we can ï¬nd the best strategy for processing the
query set Q(ni ) at node ni by considering the overall estimated cost term, cost(ni , Q(ni )), deï¬ned as
â
â

â
Ei,j Ã ecost(ni , qj ) + (1 â Ei,j ) Ã icost(ni , qj )â 



qj âQ(ni )





leaf access cost f or all relevant queries

where Ei,j = 1 means an exclusive leaf access plan is chosen
for query, qj , at this node and Ei,j = 0 otherwise.

Cut Bitmap Selection.
Once the nodes in the hierarchy are assigned estimated
costs as described above, the cut that will be used for
query processing is found by traversing the hierarchy in a
bottom-up fashion and picking nodes based on their estimated costs4 . The process is outlined in Algorithm 3. Intuitively, for each internal node, ni â IH , the algorithm computes a revised cost estimate, rcosti , by comparing the cost,
costi , estimated in the earlier phase of the process, with the
total revised costs of ni âs children:
â¢ In Line 13, the function f indBlockIO(ni ) returns the
cost of reading the block T (Bi ). If this block has already been marked âto-readâ, then the reading cost has
already been accounted for, so the cost is zero. Otherwise, the cost is equal to the size of the block T (Bi ),
as explained in Section 4.2.1.
â¢ As we see in Line 21, it is possible that a block T is
ï¬rst marked âto-readâ and then, later in the process,
marked ânot-to-readâ, because for the corresponding
nodes in the cut, more suitable ancestors are found
and the block is no longer needed.
â¢ If costi is smaller (Line 17), then ni and its leaf descendants can be used for identifying the matches to
the queries in the range Si . In this case, no revision is
4

Note that this bottom-up traversal can be combined with
the bottom-up traversal of the prior phase. We are describing them as separate processes for clarity.

1389

Algorithm 3 Cut Selection Algorithm
1: Input: Hierarchy H; per-node query workload Q(ni ); per-

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:

node cost estimates costi ; and the corresponding leaf access
plans, Ei,j , for node/query pairs ni â H and qj â Q(ni ); the
set, R â IH , or relevant internal nodes
Output: All-inclusive, CI , and Exclusive, CE , cut nodes
Initialize: Cand = â
procedure findCut
for each relevant internal node ni in R in a bottomup fashion do
Set internal children = children(ni ) â© IH ;
if internal children = â then
add ni to Cand;
rcosti = costi
else

costChildren = nj âinternal children rcostj
rcostIOi = f indBlockIO(ni )
for each child nj in internal children do
costChildrenIO
=
costChildrenIO +
f indBlockIO(nj )
end for
if (rcosti + rcostIOi ) â¤ (costChildren +
costChildrenIO) then
for each descendant nk of ni in Cand do
remove nk from Cand;
if nk is the only node to read from
T (Bk ) then
mark T (Bk ) as ânot-to-readâ;
end if
end for
add ni to Cand;
rcosti = costi
mark T (Bi ) as âto-readâ;
else
rcosti = costChildren
end if
end if
end for
CE = {ni â Cand s.t. âqj âQ(ni ) Ei,j == 1}
CI = Cand/CE
end procedure

answered only by accessing relevant leaves under the nodes
in CI . We store the blocks containing the bitmaps of these
relevant leaves in an LRU-based cache so that leaf bitmaps
can be reused by multiple queries.

4.3.3 Complexity
The bitmap selection process consists of two steps: (a) a
per-node cost estimation step and (b) a cut bitmap selection
step. Each of these steps visit only the relevant nodes of the
hierarchy. Therefore, if we denote the set of nodes of the
hierarchy, H, that intersect with any query in Q, as H(Q),
then the overall work is linear in the size of H(Q).
During the cost estimation phase, for each visited node,
ni , an inclusive and exclusive cost is estimated for any query
that intersects with this node. Therefore, the worst case
time cost of the overall process (assuming that all queries in
Q intersect with all nodes in H(Q)) is O(|Q| Ã |H(Q)|).

5. EXPERIMENTAL EVALUATION
In this section, we evaluate the eï¬ectiveness of the proposed compressed spatial hierarchical bitmap (cSHB) index
structure using spatial data sets with diï¬erent characteristics, under diï¬erent system parameters. To assess the eï¬ectiveness of cSHB, we also compare it against alternatives.
We ran the experiments on a quad-core Intel Core i5-2400
CPU @ 3.10GHz machine with 8.00GB RAM, and a 3TB
SATA Hard Drive with 7200 RPM and 64MB Buï¬er Size,
and in the same Windows 7 environment. All codes were
implemented and run using Java v1.7.

5.1 Alternative Spatial Index Structures and
the Details of the cSHB Implementation
As alternatives to cSHB, we considered diï¬erent systems
operating based on diï¬erent spatial indexing paradigms. In
particular, we considered spatial extensions of PostgreSQL
called PostGIS [2], of a widely used commercial DBMS
(which we refer to as DBMS-X), and of Lucene [1]:
â¢ PostGIS [2] creates spatial index structures using an
R-tree index implemented on top of GiST.

necessary and the revised cost, rcosti is equal to costi .
Any descendants of ni are removed from the set, Cand,
of cut candidates and ni is inserted instead.
â¢ If, on the other hand, the total revised cost of ni âs children is smaller than costi , then matches to the queries
in the range Si can be more cheaply identiï¬ed by considering the descendants of ni , rather than ni itself
(Line 27). Consequently, in this case, the revised cost,
rcosti , is set to

rcostj .
rcosti =

â¢ DBMS-X maps 2D space into a 1D space using a variation of Hilbert space ï¬lling curve and then indexes
the data using B-trees.
â¢ Apache Lucene [1,18], a leading system for text indexing and search, provides a spatial module that supports
geo-spatial range queries in 2D space using quadtrees
and preï¬x-based indexing. Intuitively, the space is partitioned using a MX-quadtree structure (where all the
leaves are at the same level and a given region is always
partitioned to its quadrants at the center [28]) and each
root-to-leaf path is given a unique path-string. These
path-strings are then indexed (using eï¬cient preï¬xindexing algorithms) for spatial query processing.

nj âchildren(ni )

As we experimentally show in Section 5, the above process
has a small cost. This is primarily because, during bottomup traversal, only those nodes that have not been pruned
in the previous top-down phase are considered. Once the
traversal is over, the nodes in the set, Cand, of cut candidates are reconsidered and those that include exclusive leaf
access plans are included in the exclusive cut set, CE , and
the rest are included in the all-inclusive cut set, CI .

Since database systems potentially have overheads beyond
pure query processing needs, we also considered disk-based
implementations of R*-tree [8] and the Hilbert R-tree [19].
For this purpose, we used the popular XXL Java library [10]:
â¢ A packed R*-tree, with average leaf node utilization
â¼ 95% (page size 4MB).

Caching of Cut and Leaf Bitmaps.
During query execution, the bitmaps of the nodes in CE
are read into a cut bitmaps buï¬er, whereas the bitmaps for
the nodes in CI do not need to be read as the queries will be

â¢ A packed Hilbert R-tree, with average leaf node utilization â¼ 99% (page size 4MB).

1390

Table 1: Data sets and clustering
Data set

#points

Synthetic (Uniform)
Gowalla (Clustered)
OSM (Clustered)

100M
6.4M
688M

Data
set
Synthetic
Gowalla
OSM

Data
set
Synthetic
Gowalla
OSM

Clustered (6.4M; Gowalla)

25

Clustered (688M; OSM)

y = -2.0x + 15.0

Linear (Uniform (100M; Synth))

20

Linear (Clustered (6.4M; Gowalla))

y = -1.7x + 13.3

15

cSHB

Luc.

1601
24
2869

2396
114
12027

DBMS
-X
3865
232
30002

Post
GIS
4606
112
76238

R*tree
2160
22
18466

Hilb.
R-tree
2139
20
17511

Table 4: Index Size on Disk (MB)

Uniform (100M; Synth)

Data Skew
log (# of non-empty cells)

Table 3: Index Creation Time (sec.)

#points per (nonempty) cell (h = 10)
Min.
Avg.
Max.
54
95
143
1
352
312944
1
3422
1.2M

Linear (Clustered (688M; OSM))

cSHB

Luc.

10900
44
2440

5190
220
22200

DBMS
-X
1882
121
12959

Post
GIS
8076
600
61440

R*tree
3210
211
22100

Hilb.
R-tree
1510
100
10400

y = -1.3x + 11.1

10
5
0
-5

-3

-1

1

3

5

7

log (r)

Figure 6: Data Skew
Table 2: Parameters and default values (in bold)
Parameter
Block Size (MB)
Query range size
|Q|
h
Buï¬er size (MB)

Value range
0.5; 1; 2.5; 5; 10
0.5% 1%; 5%
100; 500; 1000
9; 10; 11
2; 3; 5; 10; 20; 100

We also implemented the proposed cSHB index structure on
top of Lucene. In particular, we used the MX-quadtree hierarchy created by Lucene as the spatial hierarchy for building cSHB. We also leveraged Luceneâs (Java-based) region
comparison libraries to implement range searches. The compressed bitmaps and compressed domain logical operations
were implemented using the JavaEWAH library [21]. Due
to space limitations, we only present results with the incremental strategy for query evaluation.

5.2 Data Sets
For our experiments, we used three data sets:
(a) a uniformly distributed data set that consists of 100
million synthetically generated data points.
These
points are mapped to the range â180, â90 to 180, 90,
(b) a clustered data set from Gowalla, which contains the
locations of check-ins made by users. This data set is downloaded from the Standford Large Network Dataset Collection [4], and (c) a clustered data set from OpenStreetMap
(OSM) [3] which contains locations of diï¬erent entities distributed across North America. The OSM data set consists
of approximately 688 million data points in North America. We also normalized both the real data sets to the range
â180, â90 to 180, 90. In order to obtain a fair comparison across all index structures and the data sets, all three
data sets are mapped onto a 2h Ã 2h space and the positions
of the points in this space are used for indexing. Table 1
provides an overview of the characteristics of these three
very diï¬erent data sets. Figure 6 re-conï¬rms the data skew
in the three data sets using the box-counting method proposed in [9]: in the ï¬gure, the lower the negative slope, the
more skewed the data. The ï¬gure shows that the clustered
Gowalla data set has the largest skew.

5.3 Evaluation Criteria and Parameters
We evaluate the eï¬ectiveness of the proposed compressed
spatial hierarchical bitmap (cSHB) index structure by com-

paring its (a) index creation time, (b) index size, and (c)
query processing time to those of the alternative index structures described above under diï¬erent parameter settings.
Table 2 describes the parameters considered in these experiments and the default parameter settings.
Since our goal is to assess the contribution of the index in
the cost of the query plans, all index structures in our comparison used index-only query plans. More speciï¬cally, we
executed a count(â) query and conï¬gured the index structures such that only the index is used to identify the relevant
entries and count them to return the results. Consequently,
only the index ï¬les are used and data ï¬les are not accessed.
Note that all considered index structures accept squareshaped query ranges. The range sizes indicated in Table 2
are the lengths of the boundaries relative to the size of the
considered 2D space. These query ranges in the query workloads are generated uniformly.

5.4 Discussion of the Indexing Results
Indexing Time. Table 3 shows the index creation times for
diï¬erent systems and index structures, for diï¬erent data sets
(with diï¬erent sizes and uniformity): cSHB index creation
is fastest for the larger Synthetic and OSM data sets, and
competitive for the smaller Gowalla data set. As the data
size gets larger, the alternative index structures become signiï¬cantly slower, whereas cSHB is minimally aï¬ected by the
increase in data size. The index creation time also includes
the time spent on creating the hierarchy for cSHB.
Index Size. Table 4 shows the sizes of the resulting index ï¬les for diï¬erent systems and index structures and for
diï¬erent data sets. As we see here, cSHB provides a competitive index size for uniform data (where compression is not
very eï¬ective). On the other hand, on clustered data, cSHB
provides very signiï¬cant gains in index size â in fact, even
though the clustered data set, OSM, contains more points,
cSHB requires less space for indexing this data set than it
does for indexing the uniform data set.
Impact of Block Size. As we discussed in Section 3.2.2,
cSHB writes data on the disk in a blocked manner. In Figure 7, we see the impact of the block sizes on the time needed
to create the bitmaps. As we see here, one advantage of using blocked storage is that the larger the blocks used, the
faster the index creation becomes.

5.5 Discussion of the Search Results
Impact of the Search Range. Table 5 shows the impact
of the query range on search times for 500 queries under
the default parameter settings, for diï¬erent systems. As we
expected, as the search range increases, the execution time

1391

Impact of the Block Size on
Index Creation Time

Writing Bitmaps

Time (sec.)

Time (sec., log. scale)

Creating Bitmaps

1800

1350
900
450
0
0.5

1

2.5

Read Hierarchy
Map Ranges
Search Cut Bitmaps
Read Bitmaps
Combine Bitmaps

5

1000
100

Tot: 42

Tot: 51

0.5%
1%
5%

35
42
137

123
131
187

0.5%
1%
5%

2
3
3

2
3
48

0.5%
1%
5%

13
15
28

23
30
66

1

90
Time (sec.)

cSHB
-LO

60

2
3
5
13
14
78

Tot: 22

Tot: 11

1

5%

Synthetic (Uniform; 100M)

0.5%

1%

5%

Gowalla (Clustered; 6.4M)

2.5
Block Size (MB)

1%

cSHB Time Breakdown
(1% Q. Range, Uniform Data)
Tot: 32

Data sets

Figure 8: cSHB execution breakdown
becomes larger for all alternatives. However, cSHB provides
the best performance for all ranges considered, especially for
the clustered data sets. Here, we also compare cSHB with
its leaf-only version (called cSHB-LO), where instead of a
cut consisting of potentially internal nodes, we only choose
the leaf nodes for query processing. As you can see from the
ï¬gure, while cSHB-LO is a good option for very small query
ranges (0.5% and 1%), it becomes very slow as the query
range increases (since the number of bitwise operations increases, and it is not able to beneï¬t from clustering).
Execution Time Breakdown. Figure 8 provides a breakdown of the various components of cSHB index search (for
500 queries under the default parameter settings): The
bitmap selection algorithm presented in Section 4.3 is extremely fast. In fact, the most signiï¬cant components of
the execution are the times needed for reading the hierarchy
into memory5 , and for fetching the selected bitmaps from
the disk into the buï¬er, and performing bitwise operations
on them. As expected, this component sees a major increase
5
Once a hierarchy is read into the memory, the hierarchy
does not need to be re-read for the following queries.

Tot: 42

Read Hierarchy
Map Ranges
Search Cut Bitmaps
Read Bitmaps
Combine Bitmaps

Tot: 61

10
1

0.1
500

1000

Number of Queries

5%

OSM (Clustered;
688M)

10

Figure 9: Impact of the block size (500 queries,
1% q. range, uniform data)

100

0.5%

5

(b) Impact of block size on bitmap reading time

100

1%

Tot: 55

Tot: 31
30

0.5

0.1
0.5%

Tot: 84

0

Time (sec., log. scale)

Time (sec., log. scale)

Impact of Block Sizes on
Bitmap Reading Time

52
59
1700

Read Hierarchy
Map Ranges
Search Node Bitmaps
Read Bitmaps
Combine Bitmaps

1

10

(a) Impact of block size on overall cSHB execution time

Range Search Times (500 Queries)

10

5

Block Size (MB)

DBMS
Post
R*Hilb.
-X
GIS
tree
R-tree
Synthetic (Uniform; 100M)
414
12887
2211
4391
345
28736
2329
4480
368
72005
2535
4881
Gowalla (Clustered; 6.4M)
24
19
8
24
29
34
11
26
37
194
20
45
OSM (Clustered; 688M)
303
1129
3486
4368
645
4117
3889
5599
15567
18172
4626
6402

100

2.5

Tot: 141

1

0.5

Table 5: Comparison of search times for alternative
schemes and impact of the search range on the time
to execute 500 range queries (sec.)
Luc.

Tot: 113

0.1

10

Figure 7: Impact of the block size on index creation
time of cSHB (uniform data set)

cSHB

Tot: 65

10

Block Size (MB)

Range

Impact of Block Sizes
on cSHB Time

Figure 10: Impact of the number of queries on the
execution time of cSHB (1% q. range, uniform data)
as the search range grows, whereas the other costs are more
or less independent of the sizes of the query ranges.
Impact of the Block Sizes. As we see above, reading
bitmaps from the disk and operating on them is a major
part of cSHB query execution cost; therefore these need to
be performed as eï¬ciently as possible. As we discussed in
Section 3.2.2, cSHB reads data from the disk in a blocked
manner. In Figure 9, we see the impact of the block sizes on
the execution time of cSHB, including the time needed to
read bitmaps from the disk. As we see here, small blocks are
disadvantageous (due to the directory management overhead
they cause). Very large blocks are also disadvantageous as,
the larger the block gets, the larger becomes the amount of
redundant data read for each block access. As we see in the
ï¬gure, for the conï¬guration considered in the experiments,
1MB blocks provided the best execution time.
Impact of the Number of Queries in the Workload.
Figure 10 shows the total execution times as well as the
breakdown of the execution times for cSHB for diï¬erent
number of simultaneously executing queries. While the total
execution time increases with the number of simultaneous
queries, the increase is sub-linear, indicating that there are
savings due to the shared processing across these queries.

1392

Read Hierarchy
Map Ranges
Search Cut Bitmaps
Read Bitmaps
Combine Bitmaps

cSHB Time Breakdown (500 Queries,
1% Q. Range, Uniform Data)
Time (sec., log. scale)

1000
100

Tot: 152

Tot: 42

Tot: 28

10
1
0.1
9

10
# of levels of the hierarchy

11

Figure 11: Impact of the depth of the hierarchy (500
queries, 1% query range, uniform data)
Table 6: Working set size in terms of 1MB blocks
Q.Range (on 100M data)
0.5%
1%
5%

Min
1
1
1

Avg.
2.82
2.51
1.02

Max.
36
178
95

Table 7: Impact of the buï¬er size on exec. time (in
seconds, for 500 queries, 100M data)
Query
Range
0.5%
1%
5%

2MB
11.8
24.2
823.8

3MB
11.3
19.1
399.9

Buï¬er Size
5MB
10MB
10.9
10.6
18.1
17.5
155.9
105.8

20MB
10.5
17.3
101.6

100MB
10.2
16.3
94.9

Also, in Section 4.2.2, we had observed that the number of
queries in the query workload and query ranges determine
the relative costs of in-buï¬er operations vs. disk I/O. In
Figures 8 and 10, we see that this is indeed the case.
Impact of the Depth of the Hierarchy. Figure 11 shows
the impact of the hierarchy depth on the execution time of
cSHB: a 4Ã increase in the number of cells in the space (due
to a 1-level increase in the number of levels of the hierarchy)
results in < 4Ã increase in the execution time. Most signiï¬cant contributors to this increase are the time needed to
read the hierarchy and the time for bitmap operations.
Impact of the Cache Buï¬er. As we discussed in Section 4.2.2, the incremental scheduling algorithm keeps a
buï¬er of blocks containing the working set of leaf bitmaps.
As Table 6 shows, the average size of the working set is
fairly small and can easily ï¬t into the L3 caches of modern
hardware. Table 7 conï¬rms that a small buï¬er, moderately
larger than the average working set size, is suï¬cient and
larger buï¬ers do not provide signiï¬cant gains.

6.

CONCLUSIONS

In this paper, we argued that bitmap-based indexing can
be highly eï¬ective for running range query workloads on
spatial data sets. We introduced a novel compressed spatial hierarchical bitmap (cSHB) index structure that takes
a spatial hierarchy and uses that to create a hierarchy
of compressed bitmaps to support spatial range queries.
Queries are processed on cSHB index structure by selecting a relevant subset of the bitmaps and performing
compressed-domain bitwise logical operations. We also developed bitmap selection algorithms that identify the subset
of the bitmap ï¬les in this hierarchy for processing a given
spatial range query workload. Experiments showed that the
proposed cSHB index structure is highly eï¬cient in supporting spatial range query workloads. Our future work will
include implementing and evaluating cSHB for data with
more than two dimensions.

7. REFERENCES
[1] Apache Lucene. http://lucene.apache.org/core/4 6 0/
spatial/org/apache/lucene/spatial/preï¬x/tree/
SpatialPreï¬xTree.html
[2] Using PostGIS: Data Management and Queries.
http://postgis.net/docs/using postgis dbmanagement.html
[3] OpenStreetMap. http://www.openstreetmap.org/
[4] J. Leskovec and A. Krevl. SNAP Datasets: Stanford Large
Network Dataset Collection. http://snap.stanford.edu/data
[5] D. Abadi et al. Compression and Execution in
Column-Oriented Database systems. SIGMOD 2006.
[6] A. Aji et al. Hadoop-GIS: A High Performance Spatial Data
Warehousing System over MapReduce. PVLDB 2013.
[7] Rudolf Bayer. The Universal B-tree for Multidimensional
Indexing: General concepts. WWCA 1997.
[8] N. Beckmann et al. The R*-tree: An Eï¬cient and Robust
Access Method for Points and Rectangles. SIGMOD 1990.
[9] A. Belussi and C. Faloutsos. Self-Spatial Join Selectivity
Estimation Using Fractal Concepts. TOIS 1998.
[10] J.Bercken et al. XXL - A Library Approach to Supporting
Eï¬cient Implementations of Advanced Database Queries.
VLDB 2001.
[11] A.R. Butz. Alternative Algorithm for Hilbertâs Space-Filling
Curve. TOC 1971.
[12] A. Cary et al. Experiences on Processing Spatial Data with
MapReduce. SSDBM 2009.
[13] J. Chmiel et al. Dimension Hierarchies by means of
Hierarchically Organized Bitmaps. DOLAP 2010.
[14] P. Chovanec and M. KraÌtkyÌ. On the Eï¬ciency of Multiple
Range Query Processing in Multidimensional Data Structures.
IDEAS 2013.
[15] F. DelieÌge and T. Pedersen. Position List Word Aligned
Hybrid: Optimizing Space and Performance for Compressed
Bitmaps. EDBT 2010.
[16] L.Gosink et al. HDF5-FastQuery: Accelerating Complex
Queries on HDF Datasets using Fast Bitmap Indices.
SSDM 2006.
[17] David Hilbert. Ueber stetige abbildung einer linie auf ein
ï¬achenstuck. Mathematische Annalen 1891.
[18] Y. Jing et al. An Empirical Study on Performance Comparison
of Lucene and Relational Database. ICCSN 2009.
[19] I. Kamel and C. Faloutsos. Hilbert R-tree: An Improved R-tree
using Fractals. VLDB 1994
[20] O. Kaser et al. Histogram-Aware Sorting for Enhanced
Word-Aligned Compression in Bitmap Indexes. DOLAP 2008.
[21] D. Lemire et al. Sorting improves Word-Aligned Bitmap
Indexes. DKE 2010.
[22] V. Markl et al. Improving OLAP Performance by
Multidimensional Hierarchical Clustering. IDEAS 1999.
[23] G. Morton. A Computer Oriented Geodetic Data Base and a
New Technique in File Sequencing. IBM 1966.
[24] M. Morzy et al. Scalable Indexing Technique for Set-Valued
Attributes. ADBIS 2003.
[25] P. Nagarkar and K. S. Candan. HCS:Hierarchical Cut Selection
for Eï¬ciently Processing Queries on Data Columns using
Hierarchical Bitmap Indices. EDBT 2014.
[26] M. A. Olma et al. BLOCK: Eï¬cient Execution of Spatial
Range Queries in Main-Memory. Technical report EPFL 2013.
[27] A. N. Papadopoulos and Y. Manolopoulos. Multiple Range
Query Optimization in Spatial Databases. ADBIS 1998.
[28] H. Samet. Foundations of Multidimensional and Metric Data
Structures, 2005.
[29] R. Sinha and M. Winslett. Multi-Resolution Bitmap Indexes for
Scientiï¬c Data. TODS 2007.
[30] T. Siqueira et al. The SB-index and the HSB-index: Eï¬cient
Indices for Spatial Data Warehouses. Geoinformatica 2012.
[31] T. Skopal et al. Algorithm for Universal B-trees. Inf. Syst. 2006.
[32] K. Wu et al. On the Performance of Bitmap Indices for High
Cardinality Attributes. VLDB 2004.
[33] K. Wu et al. An Eï¬cient Compression Scheme for Bitmap
Indices. TODS 2004.
[34] M. Zaker et al. An Adequate Design for Large Data Warehouse
Systems: Bitmap Index versus B-tree Index. IJCC 2008.
[35] Y. Zhong et al. Towards Parallel Spatial Query Processing for
Big Spatial Data. IPDPSW 2012.

1393

SkySuite: A Framework of Skyline-Join Operators
â
for Static and Stream Environments
K. SelcÌ§uk Candan
School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University, Tempe, AZ, USA

mnagendra@asu.edu

candan@asu.edu

1. INTRODUCTION
Recently, there has been a growing interest in the eï¬cient processing of skyline queries over both static [5, 2] and
stream environments [7, 14]. Given a set, D, of data points
âThis work is supported by an NSF grant (#1116394 â RanKloud: Data Partitioning and Resource Allocation Strategies
for Scalable Multimedia and Social Media Analysis) and a
KRF grant (A Framework for Real-time Context Monitoring
in Sensor-rich Personal Mobile Environments).
Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee. Articles from this volume were invited to present
their results at The 39th International Conference on Very Large Data Bases,
August 26th - 30th 2013, Riva del Garda, Trento, Italy.
Proceedings of the VLDB Endowment, Vol. 6, No. 12
Copyright 2013 VLDB Endowment 2150-8097/13/10... $ 10.00.

a

b

3

c

2
1

Restaurant Rating

ABSTRACT
Eï¬cient processing of skyline queries has been an area of
growing interest over both static and stream environments.
Most existing static and streaming techniques assume that
the skyline query is applied to a single data source. Unfortunately, this is not true in many applications in which,
due to the complexity of the schema, the skyline query may
involve attributes belonging to multiple data sources. Recently, in the context of static environments, various hybrid
skyline-join algorithms have been proposed. However, these
algorithms suï¬er from several drawbacks: they often need to
scan the data sources exhaustively in order to obtain the set
of skyline-join results; moreover, the pruning techniques employed to eliminate the tuples are largely based on expensive
pairwise tuple-to-tuple comparisons. On the other hand,
most existing streaming methods focus on single stream skyline analysis, thus rendering these techniques unsuitable for
applications that require a real-time âjoinâ operation to be
carried out before the skyline query can be answered. Based
on these observations, we introduce and propose to demonstrate SkySuite: a framework of skyline-join operators that
can be leveraged to eï¬ciently process skyline-join queries
over both static and stream environments. Among others,
SkySuite includes (1) a novel Skyline-Sensitive Join (SSJ)
operator that eï¬ectively processes skyline-join queries in
static environments, and (2) a Layered Skyline-window-Join
(LSJ) operator that incrementally maintains skyline-join results over stream environments.

4

5

Mithila Nagendra
School of Computing, Informatics,
and Decision Systems Engineering
Arizona State University, Tempe, AZ, USA

Dominance
region of b
9

10 11

d
12

1

Time (at night)

Figure 1: Skyline of late-night restaurants
in a feature space, the skyline of D consists of the points
that are not dominated1 by any other data point in D [2].
Intuitively, the skyline is a set of interesting points that help
paint the âbigger pictureâ of the data in question, providing
insight into the diversity of the data across diï¬erent features.
Searching for non-dominated data is valuable in many applications that involve multi-criteria decision making [11].
For instance, students in a university who stay up late at
night and need a snack at odd hours might ï¬nd the skyline
of late-night restaurants useful. Figure 1 shows the ratings
and closing times of a set of restaurants: the points that are
connected represent restaurants that are part of the skyline;
this includes highest-rated restaurants that are open late
into the night. Other restaurants are not part of the skyline
because they are dominated in terms of time and/or rating
by at least one restaurant that is in the skyline. The shaded
area in Figure 1 is the dominance region of restaurant b: for
any restaurant in this range, b is either open till a later time
and/or has a better rating; therefore b is said to be more
interesting than all restaurants it dominates.
A particular shortcoming of existing static and stream
skyline algorithms is that they primarily focus on singlesource skyline processing in which all required skyline attributes are present in the same source. However, there are
many applications in both static and stream environments
that require integration of data from diï¬erent sources. In
such scenarios, the skyline query may involve attributes belonging to diï¬erent data sources, thus making the join operation an integral part of the overall process. For instance,
in static environments integrated skyline-join queries maybe
necessary over complex schemas in which the data is distributed onto many sources, whereas in stream environments
such integration is needed for streams that originate from
1
A point dominates another point if it is as good or better
in all dimensions, and better in at least one dimension.

1266

S2J

Local
Disk

SSJ

n-ary

S3J

Local
Disk

2-ary

Hybrid SSJ-LSJ

Buffer

LSJ Operator

SSJ Operator

2-ary

Hybrid SSJ-LSJ Operator

User Interface

LSJ

Iteration-Fabric

Stream 2
Local
Disk

Stream

Stream 1

n-ary

Buffer

Buffer

SKYSUITE

Figure 2: The SkySuite framework
diï¬erent sensors or from multiple sources in a distributed
publish/subscribe architecture.
Going back to our earlier example, in addition to the time
and restaurant rating attributes shown in Figure 1, students
might also consider the distance of a restaurant to the university to be a factor in their decision-making process. If this
information is available from a diï¬erent source, we would
then need to join the relevant sources in order to obtain the
restaurants that are part of the skyline.
Motivated by the above observations, we propose
SkySuite: a framework of skyline-join operators that can
be used to process skyline-join queries over both static and
stream environments (Figure 2). In particular, we demonstrate (1) the Skyline-Sensitive Join (SSJ) operator [8] that
processes skyline-join queries in static environments, and (2)
the Layered Skyline-window-Join (LSJ) operator [9] that incrementally maintains skyline-joins in stream environments.
The rest of the paper is structured as follows: in Section 2,
we give an overview of the existing work in the ï¬eld of skyline
query processing. Section 3 presents the suite of skylinejoin operators. In Section 4, we discuss the demonstration
scenarios. Lastly, we conclude the paper in Section 5.

2. RELATED WORK
The task of ï¬nding the non-dominated set of data points
was attempted by Kung et al. [5] in 1975 under the name
of the maximum vector problem. Kungâs algorithm lead to
the development of various skyline algorithms designed for
static [2, 16] and stream environments [7, 14].

2.1 Skylines over a Single Static Data Source
Borzsonyi et al. [2] were the ï¬rst to coin and investigate
the skyline computation problem in the context of databases.
Later contributions to skyline query processing include sortbased techniques (SFS [3]), progressive methods (bitmap
and index [13]), and online algorithms [10].

2.2 Skylines on Multiple Static Data Sources
Some of the prior work on skylines over multiple static
data sources include [4, 12, 16]. Sun et al. [12] introduce
an operator called skyline-join, and two algorithms to support skyline-join queries. The ï¬rst extends the SaLSa algorithm [1] to cope with multiple relations, whereas the second
algorithm (Iterative) prunes the search space iteratively.
More recently, Vlachou et al. [16] introduced the SortFirst-Skyline-Join (SFSJ) algorithm that fuses the identiï¬cation of skyline tuples with the computation of the join.
SFSJ provides a way to prune the input tuples if they do not
contribute to the set of skyline-join results, thus reducing the
number of generated join results and dominance checks.

However, SFSJ does not carry out the pruning in a blockbased manner and largely depends on time-consuming tupleto-tuple comparisons to ï¬nd the pruned region. The SkylineSensitive Join (SSJ) operator, demonstrated in this paper,
overcomes this drawback by pruning the join space in terms
of blocks of data, as opposed to individual data points,
thereby avoiding excessive point-to-point dominance checks.
Over the last decade, the advent of a wide array of streambased applications has necessitated a push towards the development of algorithms that take into consideration the
constant changes in stream environments. The following sections provide an overview of the existing work in the ï¬elds
of skyline and join query processing over streaming data.

2.3

Join Processing over Data Streams

[17] presents a symmetric hash join method that is optimized for in-memory performance. Following this, a
plethora of techniques have been developed for processing
join queries over data streams [6, 15]. Many of these focus
on eliminating redundancy in join processing to maximize
the output rate [6]. Others focus on memory; they present
join processing and load shedding techniques that minimize
loss in accuracy when the memory is insuï¬cient [15].

2.4

Skyline Processing over Data Streams

As mentioned earlier, in the conventional setting of static
data, there is a large body of work for both single-source
skyline processing [2, 3, 1] and multiple source skyline-join
processing [12, 16]. These methods assume that the data is
unchanging during query execution and focus on computing
a single skyline rather than continuously tracking skyline
changes. Recently, several algorithms have been developed
to track skyline changes over data streams. These methods
continuously monitor the changes in the skyline according
to the arrival of new tuples and expiration of old ones.
Data stream skyline processing under the sliding window
model is addressed in [7] and [14]. An important issue
that needs to be addressed here is the expiration of skyline objects. To tackle this issue, Tao et al. present the
Eager algorithm [14] that employs an event list, while Lin
et al. propose a method (StabSky) that leverages dominance
graphs [7]. Both these methods memorize the relationship
between a current skyline object and its successor(s). Once
skyline objects expire, their successor(s) can be presented as
the updated skyline without any added computation.
The above-mentioned approaches focus on skyline queries
in which the skyline attributes belong to a single stream,
thus rendering them inapplicable to the problem of computing skyline-joins over multiple streams. In this paper, we
demonstrate the novel Layered Skyline-window-Join (LSJ)
operator; this operator is ï¬rst of its kind for answering
skyline-window-join (SWJ) queries over data streams.

3.

SKYSUITE

This section introduces SkySuite: a framework of skylinejoin operators for processing skyline-join queries over both
static and stream environments (Figure 2). In particular, we
explain the methodologies behind the Skyline-Sensitive Join
(SSJ) and Layered Skyline-window-Join (LSJ) operators.

3.1 SSJ Operator for Static Environments
At the core of the SSJ operator are two skyline-join algorithms, namely S2 J (skyline-sensitive join) and S3 J (symmetric skyline-sensitive join) [8]. Both S2 J and S3 J are single-

1267

Effect of Window Size
(!=100, SWJ of sensors at 10m)

S3J
SFSJ!SC
Iterative

1E+02

0

(")*+,-./".0)12"34/-+5"6)1578+,+8"
9),"3:)"6)15+64;*+":718):"

!"#$%&'())
*%+#$),/)
)

!
!
!

'%!
'(!
"#!

(a)

"#&%!

(a) Eï¬ect of window size

?+2,++")9"@*+,-./5"9),"A)15+64;*+"B718):5"
C+3:++1"D4/-+5"A)1578+,+8".3"E.6F"G.=+,""
H&!("1+:I"&!("+J/7,712"34/-+5"/+,":718):K"
&!!"
:715"&L#"

%$"

:715"#L'"
:715"'LM"

$!"

:715"ML$"
:715"$LN"

#$"
!"
!"

$"

1000

Window Size (")

0#12*"1 )

"#$%!

LSJ vs. NaÃ¯ve
N Ã¯
LSJ vs. ISJ
LSJ vs. LSJ (l=1)

20

antiCorrelated

Figure 3: S2 J and S3 J eï¬ciently process skyline-join
queries over static data [8]

!"#$%&'())
*%+#$),-)

40

500

Independent

Data Distribution

!"#$%&'())
*%+#$),.)

60

1E+00
Correlated

&!"

&$"

#!"

#$"

'!"

'$"

<3+,.;)1"-.=+,">""

(b)

Figure 4: (a) Viewing layers as separate âvirtual
streamsâ that feed the upper layers of iteration; (b)
Sample SWJ execution for 6 consecutive windows
(10% new and 10% expiring tuples per window):
the plots show that the skyline-join process iterate
somewhere between 20 to 35 times for diï¬erent windows and the overlaps (among consecutive windows)
of tuples considered at diï¬erent layers of iterations
remain high across layers of iteration
pass, two-way skyline-join algorithms that avoid tuple-totuple dominance checks wherever possible. These algorithms
rely on a novel layer/region pruning (LR-pruning) strategy
in order to avoid excessive pairwise dominance checks.
The key features of S2 J are as follows:
â¢ The tuples in the outer table are organized into layers
of dominance.
â¢ The tuples of the inner table are clustered into regions
based on the Z-values of the skyline attributes to support block-based pruning.
â¢ A trie-based data structure on the inner table keeps
track of the so-called dominated, not-dominated, and
partially-dominated regions of the inner table relative
to the layers of the outer table.
â¢ S2 J obtains the skyline set by scanning the outer table,
only once, while pruning the inner table.
S 3 J is similar to S2 J in principle, but repeatedly swaps
the roles of the outer and inner tables. One key outcome
of this strategy is that (unlike S2 J, where the outer table
is fully scanned), S3 J rarely needs to scan any of the input
tables entirely in order to obtain the set of skyline points.
The eï¬ectiveness of S2 J and S3 J compared to the SFSJ
methods [16], PrefJoin [4], and iterative skyline-join [12] can
be seen in the sample experimental result shown in Figure 3.
Please refer to [8] for further details.

3.2 LSJ Operator for Stream Environments
The LSJ operator processes SWJ queries over two data
streams by maintaining skyline-join results in a layered, incremental manner. It continuously monitors the changes in

Effect of Window Shift Length
(!=1000, SWJ of sensors at 10m)
Execution Time Gain (%)

S2J
SFSJ!RR
PrefJoin

Execution Time Gain (%)

Execution Tiime (sec, log scale)

1E+04

2000

60

40

LSJ vs. NaÃ¯ve
N Ã¯
LSJ vs. ISJ
LSJ vs. LSJ (l=1)

20

0
50

100

Window Shift Length (")

(b) Eï¬ect of shift length

Figure 5: The LSJ operator eï¬ectively handles
skyline-join queries over streaming data [9]
the data streams, and eliminates redundant work between
consecutive windows by leveraging shared skyline objects
across all iteration layers of skyline-join processing.
LSJ is based on the observation that the consecutive iterations of the algorithm, spanning multiple windows, can
be viewed as separate iteration layers (Figure 4(a)). The
key insight here is that overlaps exist not only at the lowest
data layer (across consecutive data windows), but also at the
individual iteration layers, where the tuples processed can be
considered as âvirtual streamsâ that evolve from one window
to the next (see Figure 4(b) for a sample execution).
Therefore, we argue that if we naively execute the SWJ
operation by applying the iterative skyline join algorithm
separately for each window, we can end up with signiï¬cant
amount of redundant work. We further argue that if we can
quickly identify and eliminate these per-layer overlaps, we
can achieve signiï¬cant savings in processing time.
Based on these insights, we develop the iterationfabric [9]; this forms the backbone of the LSJ operator. The
iteration-fabric helps combine the advantages of two existing skyline methods, StabSky [7] and Iterative [12], in developing a Layered Skyline-window-Join (LSJ) operator that
maintains skyline-join results in an incremental manner by
continuously monitoring the changes in the input streams
and leveraging any overlaps that exist between the data considered at individual layers of consecutive sliding windows.
The eï¬ciency of the LSJ operator compared to Naive,
ISJ [9], and LSJ (l = 1) (where LSJ is applied only at the
ï¬rst layer of each window) is illustrated in the sample results
shown in Figure 5. Please refer to [9] for further details.

4.

USER INTERACTION SCENARIOS

This section describes the demonstration scenarios. We
will use real data sets (JCI building energy simulation/observation2 , NBA3 and Intel Berkeley Research4 ) and
the TPC-H benchmark data sets5 . Through an interactive
graphical user interface, the attendees of this demonstration
will be able to experience the suite of skyline-join operators
up close and personal. Described next, are some example
user interactive demonstration scenarios.

4.1

Two-way Skyline-Joins over Static Data

This scenario demonstrates how the SSJ operator is designed to handle skyline-joins between two static data sets.
2
JCI is an energy IT company, with access to model, simulation, and sensory data for buildings of all types and sizes.
3
http://skyline.dbai.tuwien.ac.at/datasets/nba/.
4
http://db.csail.mit.edu/labdata/labdata.html.
5
http://www.tpc.org/tpch/default.asp.

1268

200

As seen in Figure 2, the SSJ operator utilizes the S2 J and S3 J
algorithms, interchangeably, to execute skyline-join queries.
This query scenario is executed over the NBA and TPC-H
benchmark data sets. Attendees of this demonstration will
be able to compare the performance of S2 J and S3 J against
other algorithms, and will be able to observe the behaviour
of the SSJ operator over diï¬erent skyline-join queries.
Example 1 (SSJ Operation). Give
two
tables,
Player-points (playerID, points, fieldGoals)
and
Player-assists (playerID, assists, freeThrows), both
derived from the NBA data set, a skyline-join query over
Player-points and Player-assists could be:
Skyline = SSJ * FROM Player-points P,
Player-assists A,
WHERE P.playerID = A.playerID,
points MAX, fieldGoals MAX,
assists MAX, freeThrows MAX.
This query equi-joins the tables on playerID and returns
results that are in the skyline based on the attributes points,
fieldGoals, assists, freeThrows. Intuitively, this query
obtains the skyline of good oï¬ensive players in the NBA. â

4.2 Two-way SWJ queries over Data Streams
Through this scenario, we demonstrate how the LSJ operator handles skyline-window-join (SWJ) queries between
two input data streams (Figure 2). The LSJ operator utilizes
the iteration-fabric framework to run SWJ queries over the
Intel Berkeley Research lab data streams. Attendees will
have the opportunity to view the behaviour of the LSJ operator and observe the advantages that the iteration-fabric
provides over other alternative solutions.
Example 2 (LSJ Operation). Consider a scenario
in which a set of sensors produce readings only related
to temperature and voltage, while another set of sensors give readings of humidity and light.
This results in two input streams, namely stream-1 (moteid,
temperature, voltage, epoch) and stream-2 (moteid,
humidity, light, epoch). Given these, a SWJ query over
the set of sensors on the attributes temperature, voltage,
humidity and light could be:
Skyline = SWJ * FROM stream-1 S1, stream-2 S2,
WHERE S1.moteid = S2.moteid,
S1.epoch within last 24 hours,
S2.epoch within last 24 hours,
temperature MAX, voltage MAX,
humidity MAX, light MAX.
This query returns a set of interesting readings produced by
the sensors over the past 24 hours.
â

4.3 Other Skyline-Join Operations
In this scenario, we demonstrate SkySuiteâs ability to process skyline-join queries over multiple data sources. Additionally, we also show how SkySuite handles a scenario in
which one of the data sources is static, while the other is
streaming. As show in Figure 2, SkySuite utilizes a hybrid form of the SSJ and LSJ operators to tackle skyline-join
queries over hybrid input sources.

5.

CONCLUSION

This demonstration introduces SkySuite: a framework of
skyline-join operators that can be leveraged to eï¬ciently
process skyline-join queries over both static and stream
environments. In particular, we demonstrate the SkylineSensitive Join (SSJ) and the Layered Skyline-window-Join
(LSJ) operators. The SSJ operator overcomes the drawbacks of existing static skyline-join algorithms by pruning
the join space in terms of blocks of data, as opposed to
individual data points, thereby avoiding excessive point-topoint dominance checks. While, the LSJ operator provides
an eï¬cient technique for computing skyline-joins over pairs
of streams. LSJ is ï¬rst of its kind for answering skylinewindow-join (SWJ) queries over data streams.

6.

ACKNOWLEDGMENTS

We would like to thank Dr. Youngchoon Park of Johnson
Controls, Inc. (JCI) for allowing us access to JCI data sets.

7.

REFERENCES

[1] I. Bartolini, P. Ciaccia, and M. Patella. SaLSa: computing
the skyline without scanning the whole sky. In CIKM,
pages 405â414, 2006.
[2] S. BoÌrzsoÌnyi, D. Kossmann, and K. Stocker. The Skyline
operator. In ICDE, pages 421â430, 2001.
[3] J. Chomicki, P. Godfrey, J. Gryz, and D. Liang. Skyline
with presorting. In ICDE, pages 717â719, 2003.
[4] M. E. Khalefa, M. F. Mokbel, and J. J. Levandoski.
Prefjoin: An eï¬cient preference-aware join operator. In
ICDE, pages 995â1006, 2011.
[5] H.-T. Kung, F. Luccio, and F. P. Preparata. On ï¬nding the
maxima of a set of vectors. J. ACM, 22(4):469â476, 1975.
[6] H.-G. Li, S. Chen, J. Tatemura, D. Agrawal, K. S. Candan,
and W.-P. Hsiung. Safety guarantee of continuous join
queries over punctuated data streams. In VLDB, pages
19â30, 2006.
[7] X. Lin, Y. Yuan, W. Wang, and H. Lu. Stabbing the sky:
Eï¬cient skyline computation over sliding windows. In
ICDE, pages 502â513, 2005.
[8] M. Nagendra and K. S. Candan. Skyline-sensitive joins
with LR-pruning. In EDBT, pages 252â263, 2012.
[9] M. Nagendra and K. S. Candan. Layered processing of
skyline-window-join (SWJ) queries using iteration-fabric. In
ICDE, 2013 (to be published).
[10] D. Papadias, Y. Tao, G. Fu, and B. Seeger. Progressive
skyline computation in database systems. ACM Trans.
Database Syst., 30(1):41â82, 2005.
[11] R. E. Steuer. Multiple Criteria Optimization: Theory,
Computation and Application. John Wiley, 1986.
[12] D. Sun, S. Wu, J. Li, and A. K. H. Tung. Skyline-join in
distributed databases. In ICDE Workshop, pages 176â181,
2008.
[13] K.-L. Tan, P.-K. Eng, and B. C. Ooi. Eï¬cient progressive
skyline computation. In VLDB, pages 301â310, 2001.
[14] Y. Tao and D. Papadias. Maintaining sliding window
skylines on data streams. TKDE, 18(3):377â391, 2006.
[15] N. Tatbul and S. Zdonik. Window-aware load shedding for
aggregation queries over data streams. In VLDB, pages
799â810, 2006.
[16] A. Vlachou, C. Doulkeridis, and N. Polyzotis. Skyline query
processing over joins. In SIGMOD, pages 73â84, 2011.
[17] A. N. Wilschut and P. M. G. Apers. Dataï¬ow query
execution in a parallel main-memory environment. In
PDIS, pages 68â77, 1991.

1269

Hive Open Research Network Platform
Jung Hyun Kim, Xilun Chen,
K. SelÃ§uk Candan

Maria Luisa Sapino
Dipartimento di Informatica
Universita degli Studi di Torino
I-10149 Torino, Italy

Arizona State University
Tempe, AZ 85287, USA

{jkim294, xilun.chen, candan}@asu.edu

marialuisa.sapino@unito.it

ABSTRACT
Did you ever return back from a conference, having met a lot of
interesting folks, listened to many inspiring talks, or having your
presentation welcomed with a barrage of (of course, constructive!)
questions, wishing if only you managed to take record of all these
during the event? We are developing the Hive Open Research
Network1, a social platform for fostering scientific interactions
and reducing friction in scientific exchanges and the underlying
integrated services supporting content personalization, preview,
and social/scientific recommendations. Hive is a conferencecentric, but cross-conference platform, where researchers can seed
and expand their research networks, keep track of the technical
research sessions they are attending, meet new colleagues, share
their ideas, ask questions, give and receive comments, or simply
keep and/or view records of interactions at a conference they have
attended
(or wanted to attend, but missed due to other
commitments). In its core, Hive leverages dynamically evolving
knowledge structures, including user connections, concept maps,
co-authorship networks, content from papers and presentations,
and contextual knowledge to create and to promote networks of
peers. These peer networks support each other explicitly through
direct communication or indirectly through collaborative filtering.
Hive provides the following online integrated services: a)
understanding the personal activity context through access
patterns and analysis of user supplied content, b) context-aware
resource discovery, including search, presentation, and
exploration support within the scientific knowledge structures,
and c) peer discovery, and peer driven resource and knowledge
sharing and collaborative recommendations.

Figure 1. A screen shot from the MMâ11 edition of the
Hive open research network

1. INTRODUCTION
We demonstrate the Hive Open Research Network (Figure 1), a
social platform for fostering scientific interactions and reducing
friction in scientific exchanges and the underlying integrated
knowledge services for supporting content personalization,
preview, and social/scientific recommendations.

Categories and Subject Descriptors
H.3.3 [Information Search and Retrieval]: Information filtering;
H.3.4 [Systems and Software]: User profiles and alert services;
H.3.5 [Online Information Services]: Data sharing, Web-based
services; H.3.7 [Digital Libraries]: Collection, Dissemination,
Systems issues, User issues; H.3.7 [Group and Organization
Interfaces]: Asynchronous interaction, Web-based interaction;

Hive is a conference-centric, yet cross-conference platform, where
researchers can seed and expand their research networks, keep
track of the technical research sessions they are attending, meet
new colleagues, share their ideas, ask questions, give and receive
comments, or simply keep and/or view records of interactions at a
conference they have attended (or wanted to attend, but missed
due to other commitments).

General Terms
Algorithms, Human Factors

Keywords
Social networks, Scientific networks, Peer discovery,
Recommendation, Collaborative filtering, Community discovery

Different from social networks, such as Facebook [2], Hive
focuses on professional research networks. In contrast to
professional social networks, such as LinkedIn [1], on the other
hand, it is (a) event centric, (b) content (presentations, data,
papers, posters) rich, and (c) research oriented. Hive also differs
from other conference services, such as Pathable [4] and
Iamresearcher [3], in many ways. Pathable focuses on a given
event and tries to make the physical scheduling at a conference
more convenient. Iamresearcher targets individual researchers or
conference organizers and helps them create research oriented

1
This work is supported by NSF Grant 1043583 âMiNC: NSDL
Middleware for Network- and Context-aware Recommendationsâ.

Copyright is held by the authors.
EDBT/ICDT '13, Mar 18-22 2013, Genoa, Italy
ACM 978-1-4503-1597-5/13/03.

733

and, when selected to be the active workpad, also gives
context to other recommendations Hive provides to Zach.

websites easily and conveniently. Hive, on the other hand, is not
event specific and focuses on peer network discovery and
management, as well as information sharing and dissemination
among researchers within and across conference events.
Concept
map
and
personalizati
on services

â¢ Learn key concepts to bootstrap concept
map from a given set of contextuallyrelevant documents

Peer
network
services

â¢ Select peer network
â¢ Locate similar peers (subject to peerâs
preferences)
â¢ Send request/reply to peers
â¢ Search peers and resources based on
context;
o alternative context representations that
the user can specify include concept
maps, access history, and knowledge
from peers (collaborative filtering)
â¢ Rank peers and resources based on context
â¢ Request resource recommendations based
on context
â¢ Relationship discovery and explanation
among peers and other resources
â¢ Community discovery and tracking
â¢ Generate summary previews and highlights
for updates and resources based on based
on context
â¢ Search and visualize personal, group, or
community activity history based on
current context

Discovery,
context- and
collaborativ
erecommenda
tion
and
preview
services

Personal
activity
history
services

Table 1. List of sample Hive services

â¢

In the first dayâs keynote presentation, Zach uses Hive to
follow the comment, question, and answer traffic, including
the related twitter activity. He notices that one of the
questions posted on Hive about the presentation is in fact in
his own research area and shares his opinion on the topic.

â¢

The next sessions in the conference are not in areas of direct
interest to Zach. But, Zach notices that a few of the
researchers he is following are checking-in into a session on
large scale graph processing. He also notices that a paper in
the session has cited a few of the papers he had cited in his
own work and that the author of the paper is a frequent coauthor of one of the researchers in his workpad. So Zach
decides to attend this session.

â¢

The presentation indeed raises his curiosity and he finds
himself posting a few questions about the details not clarified
in the presentation. A few minutes later, one of the coauthors of the presenter, who happens to follow the
discussion remotely, confirms his opinion. While this
exchange occurs in Hive, the exchange is also broadcasted in
twitter with the sessionâs hashtag. Zach adds this
presentation, the presenterâs avatar, and the link to the paper
into his âto investigate laterâ workpad. Zach also happily
notices that a few of the attendees of the session decide to
âfollowâ his questions.

â¢

In the break, Zach receives an update from Hive reporting
that there is already a question posted regarding the
presentation he had uploaded a few days back. He notices
that one of the researchers who were following him, named
Aaron, questions an equation he has included in one of his
slides. Indeed, when he checks the slides he notices that there
was a typo and he corrects the slide. Zach also sends a thank
you note to the researcher who highlighted the error in the
slide. After a brief exchange in which he learns that Aaron is
also working on a related problem, Zach sends a connection
request to Aaron and receives an acknowledgement a few
minutes later.

â¢

The next day, Zach presents his paper in an afternoon
session. After his presentation, the session chair takes a few
questions from the audience and then reads a few questions
posted on Hive, some of which through Twitter. Zach
answers to the questions as best as he could within the given
time limit. He then makes a note to answer these questions
on Hive in more detail after his presentation.

â¢

After his presentation, he notices that one of the audience
members, named Ann, who has asked questions to him had a
related paper in EDBT 2010, which he had cited and another
paper accepted to SIGMODâ13, which has not been
published yet. He also notes that she has check-in in a
session on community detection. He adds Annâs avatar to his
workpad and then goes to the session on community
detection.

â¢

Once he is back to the University, his advisor (who had
missed the conference due to another commitment) and Zach
spend some time in their first meeting, discussing his
activities and the connections he has made in EDBTâ13.

1.1 Use Scenario
A second year PhD student, Zach, is attending the EDBT 2013
conference to present a paper. He had also attended the last yearâs
edition of the same conference, where his advisor and groupmates
had a paper, as well as the ACM Sigmod 2012 conference, where
he has published his first paper on social media. In both
conferences, he had met and connected with researchers who were
working in topics that were of interest to him.
â¢

â¢

Before leaving for EDBTâ13, Zach logs in to Hive and
uploads his presentation slides; he also takes a quick look at
the already uploaded presentation slides of a few relevant
papers he had noticed in the published program. Zach also
checks which of the researchers who he had connected in the
past are coming to EDBTâ13. In addition, Hive proposes five
other researchers that Zach may want to connect during the
event and for each provides a list of sessions that the
researcher may most likely attend. Zach highlights the set of
researchers whose (session check-in, question, comment,
answer) activities he would like to follow and instructs Hive
to provide real-time updates regarding these during the
conference.
While Zach is checking the list of presentations in his
session, Hive reminds Zach that the chair of his session is
one of the authors whose paper he had cited in his Sigmod12
paper and that one of the other authors in the same session
was a co-author with his advisor a few years back. Zach
decides to follow these as well. In addition, he places the
avatars of these two researchers into his âsessionâ workpad
for quick access. This workpad serves both as a bookmark

734

Figure 3. Multiple integrated layers of the dynamic Hive
knowledge network
A preliminary version of Hive has been made available to the
attendees of the ACM Multimedia 2011 conference in November
2011 (this version of Hive is available to the public at
http://hive.asu.edu), and of ACM Sigmod 2012 conference in May
2012 (http://hive.asu.edu/sigmod12). Hive is also being used as a
learning support platform for the CS515 (graduate level) and
CS408 (senior level) courses at ASU, to encourage studentsâ
collaboration and studying material exchange.

Figure 2. Relationships between the users âK. Selcuk
Candanâ and âCarsten Griwodzâ are shown on the right
column
â¢
â¢
â¢
â¢
â¢
â¢

2. OVERVIEW of HIVE
Basic functionalities of Hive are built using JomSocial [5], a
Joomla-based platform for building social networks. These basic
functionalities include many of the Facebook like social features.

â¢

These relationships are contextualized based on userâs own
activity history, including the current session s/he is checked in as
well as the content of his or her active workpad (Figure 4). The
workpad interface is a tool to help the user keep record of the
things that attract his or her interest in the conference. When the
user sees a presentation that he/she likes, a question that tickles
his/her mind, or a colleague that she wants to connect later, she
can simply drag-and-drop the corresponding avatar into the
current workpad. The user can also name and save workpads and
can choose from different saved workpads, each corresponding to
a different context or state of mind. The content of the currently
active workpad defines the userâs activity context and all the
searches and recommendations are contextualized according to
this active workpad. The user can export workpads as collections
accessible to others or import a collection as active work pad.

Unlike other JomSocial-based social networks, however, Hive
also provides various knowledge rich services and functionalities.
In particular, we note that accessing scientific content effectively
requires a proper understanding of the personal activity context,
context-aware resource discovery, and peer-network driven
resource
and
knowledge
sharing
and
collaborative
recommendations. Therefore, in its core, Hive leverages
dynamically evolving knowledge structures, including concept
maps, co-authorship networks, content from papers and
presentation, and contextual knowledge to create and to promote
networks of peers (Figure 3).
These peer networks support each other explicitly through direct
communication or indirectly through collaborative filtering. Hive
provides the following online integrated services:
a)

understanding the personal activity context through
access patterns and analysis of user supplied content,

b)

context-aware resource discovery, including search,
presentation, and exploration support within the
scientific knowledge structures, and

For these knowledge management, content annotation, and peer
recommendation services, Hive relies on our Middleware for
Network- and Context-aware Recommendations (MiNC) engine,
which provides services that help minimize the extraneous load on
users, while they search, share, and access digital resources and
peer networks [8].

c)

peer discovery, and peer driven resource and knowledge
sharing and collaborative recommendations.

2.1 Understanding and Representing the
Personal Activity Context
The domain knowledge captured by the usage context includes
concepts, their significance in the learning domain, as well as the
strength of the inter-relationships between concepts and resources
[9]. To capture the structure of a knowledge domain as well as the
relationships between domain concepts, we leverage general
purpose ontologies as well as domain specific concept maps.

In particular, Hive uses the following evidences for discovering
and explaining relationships between individuals (peers) and for
recommending new peers or resources [6][7] (Figure 2):
â¢
â¢

co-authorship, direct citation, or indirect citation (e.g., citing
the same paper or transitive citation),
online âfollowâing,
conference participation (related conferences, same
conference different years, same conference same year),
session participation/check-in (related sessions or same
session/same time),
reciprocal question, comment, and answer activities,
user-provided content (publication, presentation, other
supporting material) similarity, and
activity (e.g. browsing, commenting) similarity.

profile and declared interest,
current and past affiliation, group membership,

735

techniques, which preserve maximal information while
minimizing the footprint of the reported information [13], and
context-aware snippet extraction algorithms [14].

2.4 Dynamic Peer-Networking and
Collaborative Recommendation Support

Figure 4. Two different work pads of the same user: the
work pads can contain many different types of
resources, including avatars of the users, presentations,
sessions, questions and answers, documents, and
collections. The work pads serve both as a book mark as
well as context for search and recommendations
To support services where the activity context is determined by
external materials, we apply novel concept map bootstrapping
algorithms that rely on user highlights, bookmarks, notes, or
documents. These algorithms, including [10], extract, in a semiautomated manner, dominant concepts and their relationships
specific to a given material.

Hive provides peer-network services in two ways: (a) peer
recommendation, where the system locates other peers with
similar interests or activity contexts and (b) peer-network based
resource recommendation. We have developed SCENT, an
innovative, scalable spectral analysis framework for internet scale
monitoring of multi-relational social media data, encoded in the
form of tensor streams. SCENT focuses on the computational cost
of structural change detection in tensor streams and extends
compressed sensing (CS) to tensor data. Through the use of
randomized tensor ensembles, SCENT is able to encode the
observed tensor streams in the form of compact descriptors and
detect significant changes in the underlying structure faster and
more accurately than the other methods 0.

3. CONCLUSIONS
In this paper, we demonstrate Hive, a social platform for fostering
scientific interactions and improving scientific exchanges. Hive is
a conference-centric, but cross-conference platform, which
leverages dynamically evolving knowledge structures, including
user connections, concept maps, co-authorship networks, content
from papers and presentation, and contextual knowledge to create
and to promote networks of peers.

4. ACKNOWLEDGMENTS
We thank Mijung Kim and Profs. Hari Sundaram and Hasan
Davulcu for their inputs on Hive and the underlying Middleware
for Network- and Context-aware Recommendations (MiNC).

2.2 Network Layer Alignment and
Integration

5. REFERENCES

Hive uses the multiple context layers of the âcontext networkâ,
shown in Figure 3, in an integrated manner search and
recommendation. Integration of layers starts with an alignment
phase, which requires identification of mappings between
concepts and relationships among different layers. In Hive, since
the original layers are likely to match partially and since layers
can conflict or reinforce each other, the result of the alignment
process is imprecise. For example, structures of the underlying
concept-maps and concept significances may differ from one
context layer to the other. The Hive services that rely on these
layers function in the presence of such imperfect alignments. For
weighted graph data management, Hive relies on our R2DB,
weighted RDF data management system [11][12].

2.3 Context-Aware Resource Discovery,
Search, and Exploration

[1]

http://www.linkedin.com

[2]

http://www.facebook.com/

[3]

http://www.iamresearcher.com

[4]

http://www.pathable.com/

[5]

http://www.jomsocial.com/

[6]

J.H. Kim, K.S. Candan, and M. L. Sapino. Impact Neighborhood
Indexing (INI) in Diffusion Graphs. Proc. CIKMâ12.

[7]

K.S. Candan, W.-S. Li: Reasoning for Web Document Associations
and its Applications in Site Map Construction. DKE 43(2), 2002.

[8]

http://www.nsf.gov/awardsearch/showAward.do?AwardNumber=10
43583

[9]

J. W.Kim, K. S. Candan, J. Tatemura. Efficient Overlap and Content
Reuse Detection in Blogs and Online News Articles. WWWâ09

[10] L. Di Caro, K. S. Candan, and M. L. Sapino. Navigating within
News Collections using Tag-Flakes. J. Vis. Lang. and Comp., 2011.

Hive relies on the underlying integrated context network to filter,
summarize, and rank alternatives and adapt according to their
relevance. Context-aware ranking and preview services include
(a) relevant snippet extraction from documents, (b) key concept
extraction for automated annotations, and (c) content
summarization documents and update reports. Once all the
concepts are extracted and ranked (based on the context), Hive
propagates the concepts within the relevant neighborhoods of the
knowledge network using adaptation strategies, based on the
current active context (defined by the workpad). These
annotations are then used for ranking, recommendations, and
summarization tasks. Summarization of the scheduled update
reports are performed relying on hierarchical table summarization

[11] J. P. CedeÃ±o and K. S. Candan. R2DF framework for ranked path
queries over weighted RDF graphs. Proc.WIMS, 2011.
[12] S. Huang, X. Li, J. P. CedeÃ±o, K. Se. Candan, M. L. Sapino,
Representing and Querying Weighted Semantic Web with R2DB.
Under review for demonstration in EDBTâ13
[13] K. S. Candan, H. Cao, Yan Qi, M. L. Sapino: AlphaSum: sizeconstrained table summarization using value lattices. EDBTâ09.
[14] Q. Li, K. S. Candan, Y. Qi. Extracting Relevant Snippets for Web
Navigation. AAAI 2008.
[15] Yu-Ru Lin, K. S. Candan, H. Sundaram, L. Xie. SCENT: Scalable
Compressed Monitoring of Evolving Multi-Relational Social
Networks. ACM TOMCCAP, 7:29, 2011

736

Decomposition-by-Normalization (DBN): Leveraging
Approximate Functional Dependencies for Efficient Tensor
Decompositionâ
Mijung Kim

K. SelÃ§uk Candan

Arizona State University
Tempe, AZ 85287, USA

Arizona State University
Tempe, AZ 85287, USA

mijung.kim.1@asu.edu

candan@asu.edu

ABSTRACT

1.

For many multi-dimensional data applications, tensor operations as well as relational operations need to be supported
throughout the data lifecycle. Although tensor decomposition is shown to be effective for multi-dimensional data analysis, the cost of tensor decomposition is often very high. We
propose a novel decomposition-by-normalization scheme
that first normalizes the given relation into smaller tensors based on the functional dependencies of the relation and then performs the decomposition using these
smaller tensors. The decomposition and recombination steps
of the decomposition-by-normalization scheme fit naturally in settings with multiple cores. This leads to a
highly efficient, effective, and parallelized decompositionby-normalization algorithm for both dense and sparse tensors. Experiments confirm the efficiency and effectiveness
of the proposed decomposition-by-normalization scheme
compared to the conventional nonnegative CP decomposition approach.

For many multi-dimensional data applications, tensor operations as well as relational operations need to be supported
throughout the data lifecycle (from collection to analysis).
Tensors are high-dimensional arrays. Due to the convenience
they provide when representing relationships among different types of entities, tensor representation has been increasingly used for relational data in various fields from scientific
data management to social network data analysis. Tensor
based representations have proven to be useful for multiaspect data analysis and tensor decomposition has been an
important tool to capture high-order structures in multidimensional data [2, 11, 12, 17, 20, 23].
Although tensor decomposition is shown to be effective for
multi-dimensional data analysis, the cost of tensor decomposition is often very high, especially in dense tensor representations where the cost increases exponentially with the
number of modes of the tensor. While decomposition cost increases more slowly (linearly with the number of nonzero entries in the tensor) for sparse tensors, the operation can still
be very expensive for large data sets. In fact, in many data
intensive systems, data is commonly high-dimensional and
large-scale and, consequently, of all the operations that need
to be supported to manage the data, tensor decompositions
tend to be the costliest ones. Recent attempts to parellelize
tensor decomposition [2, 17, 23] face difficulties, including
large synchronization and data exchange overheads.

Categories and Subject Descriptors
H.2.4 [Database Management]: SystemsâQuery processing, Relational databases; H.3.3 [Information Storage
and Retrieval]: Information Search and RetrievalâClustering

General Terms
Algorithms, Experimentation

Keywords
Tensor Decomposition, Tensor-based Relational Data Model
âThis work is supported by the NSF Grant #1043583 âMiNC: NSDL Middleware for Network- and Context-aware
Recommendationsâ and the NSF Grant #1116394 âRanKloud: Data Partitioning and Resource Allocation Strategies for Scalable Multimedia and Social Media Analysisâ

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
CIKMâ12, October 29âNovember 2, 2012, Maui, HI, USA.
Copyright 2012 ACM 978-1-4503-1156-4/12/10 ...$15.00.

INTRODUCTION

1.1

Contribution
of
this
Paper:
Decomposition-by-Normalization (DBN)

Our goal is to tackle the high computational cost of tensor
decomposition process. Since, especially for dense data sets,
the number of modes of the tensor data is one of the main
factors contributing to the cost of tensor operations, we focus
on how we reduce the dimensionality and the size of the
input tensor. In particular, we argue that if
â¢ a high-dimensional data set (i.e., a tensor with large
number of modes) can be normalized (i.e., vertically
partitioned) into lower-dimensional data sets (i.e., tensors with smaller number of modes) and
â¢ each vertical partition (sub-tensor) is decomposed independently,
then the resulting partial decompositions can be efficiently
combined to obtain the decomposition of the original data
set (tensor). We refer to this as the decomposition-bynormalization (DBN) scheme.

ID

occupation

income

Self-emp-not-inc

workclass

Masters

1

Execmanagerial

>50K

Never-worked

Bachelors

2

-

<=50K

Private

Some-college

3

Sales

>50K

!

!

!

!

!

workclass

education

education

ID

Self-emp-not-inc

Masters

1

Never-worked

Bachelors

2

Private

Some-college

3

!

!

!

ID

occupation

1

Execmanagerial

income

2

-

<=50K

3

Sales

>50K

!

!

!

>50K

(a) Normalization
occupation
education ID

education
ID

occupation
occupation

workclass

+!+

ID
workclass

workclass

â

workclass

â

workclass

â
+!+

occupation

ID

+!+

(b) decomposition-by-normalization (DBN) process
Figure 1: (a) Normalization of a relation R(workclass, education, ID, occupation, income) into two relations R1 (workclass,
education, ID) and R2 (ID, occupation, income) based on the key
(ID); (b) decomposition-by-normalization:

normalization of R

into R1 and R2 and rank-r1 CP decomposition of R1 and
rank-r2 CP decomposition of R2 that are combined on the ID
mode into rank-r CP decomposition of R.

Example 1.1. Consider
the
5-attribute
relation,
R(workclass, education, ID, occupation, income), in
Figure 1(a) and assume that we want to decompose the
corresponding tensor for multi-dimensional analysis.
Since high-dimensional tensor decomposition is expensive,
we argue that a better processing scheme would involve first
normalizing this relation into the smaller tensors (based on
the functional dependencies of the relation) and then performing the decompositions of these smaller tensors. These
normalized tensors are lower-dimensional than the original
input tensor, therefore these decompositions are likely to be
much less expensive than the decomposition of the original input tensor. Figure 1(a) illustrates an example normalization which divides the 5-attribute relations into two
smaller relations with 3 attributes, R1 (workclass, education, ID) and R2 (ID, occupation, income) respectively.
Figure 1(b) illustrates the proposed DBN scheme: Once
the two partitions are decomposed, we combine the resulting
core and factor matrices to obtain the decomposition of the
original tensor corresponding to the relation R.
In the above example, if the relation R is dense, we expect
that decompositions of relations R1 and R2 will be much
faster than that of the relation R and the gain will more
than compensate for the normalization cost of the initial
step and the combination cost of the last step of DBN.
If the relation R is sparse, on the other hand, the decomposition cost is not only determined by the number of
modes, but also the number of nonzero entries in the tensor.

As a result, unless the partitioning provides smaller number
of tuples for both relations, DBN may not provide sufficient savings to compensate the normalization and combination overheads. However, as we will experimentally verify
in Section 6, the decomposition and recombination steps of
the DBN scheme fit naturally in multiple cores. This leads
to a highly efficient, effective, and parallelized DBN strategy
under both dense and sparse tensors.
In general, a relation can be vertically partitioned into
two in many ways: we need to first select a join attribute
and then, we need to decide which attributes to include in
which vertical partition. The join attribute (ID in the above
example) around which we partition the data is important
for two different reasons. First of all, we need to ensure that
the join attribute is selected in such a way that the normalization (i.e., the vertical partitioning) process does not lead
to spurious tuples. Secondly, the join attribute need to partition the data in such a way that the later step in which decompositions of the individual partitions are combined into
an overall decomposition does not introduce errors.
Task 1.1. One way to prevent the normalization process from introducing spurious data is to select an attribute
which functionally determines the attributes that will be
moved to the second partition. This requires an efficient
method to determine functional dependencies in the data.
This is difficult because the total number of functional dependencies in the data can be exponential.
Task 1.2. A second difficulty is that many data sets
may not have perfect functional dependencies to leverage
for normalization. In that case, we need to identify and rely
on approximate functional dependencies in the data. In this
paper, we argue that we can rely on pairwise approximate
functional dependencies that are incomplete, yet very efficient to compute.
Task 1.3. Once the approximate functional dependencies
are identified, we need to partition the data into two partitions in such a way that will lead to least amount of errors
during later stages. In this paper, we argue that partitioning the attributes in a way that minimizes inter-partition
functional dependencies and maximizes intra-partition dependencies will lead to least amount of errors in recombination step. After data is vertically partitioned and individual
partitions are decomposed, the individual decompositions
need to be recombined to obtain the decomposition of the
original relation. This process needs to be done in a way
that is efficient and parallelizable.

1.2

Organization of the Paper

The organization of the paper is as follows:
â¢ We first provide the relevant background and discuss
the related works in Section 2.
â¢ We next provide an overview of the proposed DBN
scheme in Section 3.
â¢ We then focus on selecting the best partitions for the
normalization step of DBN (Section 4).
â¢ In Section 5, we present rank-pruning strategies to further reduce the cost of DBN.
â¢ Next, we experimentally evaluate DBN in Section 6
in both stand-alone and parallel configurations. We
focus on the accuracy and the running time of the alternative algorithms. Experimental results provide evidence that while being significantly faster, DBN can
approximate well the fitness of the conventional tensor
decomposition with respect to the original tensor.
We conclude the paper in Section 7.

2.

BACKGROUND AND RELATED WORK

Tensor Decomposition. The two most popular tensor decompositions are the Tucker [21] and the CANDECOMP/PARAFAC [7, 4] decompositions. The Tucker decomposition generalizes singular value matrix decomposition
(SVD) to higher-dimensional matrices. CANDECOMP [4]
and PARAFAC [7] decompositions (together known as the
CP decomposition) take a different approach and decompose
the input tensor into a sum of component rank-one tensors.
More specifically, the CP Decomposition of P is a weighted
sum of rank-one tensors as the following (an alternative way
to view the decomposition is in the form of a diagonal tensor
and a set of factor matrices of the input tensor):
PI1 ÃI2 ÃÂ·Â·Â·ÃIN â¼

r
X

(1)

Î»k â¦ Uk

(2)

â¦ Uk

(N )

â¦ Â· Â· Â· â¦ Uk

.

k=1

where Î» is a vector of size r and each U (n) is a matrix of
size In Ã r, for n = 1, Â· Â· Â· , N .
Many of the algorithms for decomposing tensors are based
on an iterative process that approximates the best solution
until a convergence condition is reached. The alternating
least squares (ALS) method is relatively old and has been
successfully applied to the problem of tensor decomposition [4, 7]. ALS estimates, at each iteration, one factor
matrix, maintaining other matrices fixed; this process is repeated for each factor matrix associated to the dimensions
of the input tensor.
Nonnegative Tensor Decomposition (NTF). Note that
these decompositions can be interpreted probabilistically, if
additional constraints (nonnegativity and summation to 1)
are imposed. In the case of the CP decomposition, for example, each nonzero element in the core and the values of
entries of the factor matrices can be thought of as a cluster
and the conditional probabilities of the entries given clusters,
respectively. The N-way Toolbox for MATLAB [1] provides
both CP and Tucker decomposition with nonnegativity constraints.
Scalable Tensor Decomposition. Tensor decomposition
is a costly process. In dense tensor representation where the
cost increases exponentially with the number of modes of
the tensor. While decomposition cost increases more slowly
(linearly with the number of nonzero entries in the tensor)
for sparse tensors, the operation can still be very expensive for large data sets due to the high computational cost
and memory requirement to build up the approximate tensor [17]. A modified ALS algorithm proposed in [17] computes Hadamard products instead of Khatri-Rao products
for efficient PARAFAC for large-scale tensors. Kolda et
al. [11] developed a greedy PARAFAC algorithm for largescale, sparse tensors in MATLAB.
The ALS in Tucker decompositions involves SVD which is
the most computationally challenging part [20]. To address
this, randomized sampling is used in [20]. MET decomposition proposed in [12] addresses intermediate blowup problem
in Tucker decomposition.
Parallel Tensor Decomposition. Phan and Cichocki [17]
proposed a modified ALS PARAFAC algorithm called grid
PARAFAC for large scale tensor data. The grid PARAFAC
divides a large tensor into sub-tensors that can be factorized using any available PARAFAC algorithm in a parallel
manner and iteratively combined into the final decomposi-

tion. The grid PARAFAC can be converted to grid NTF by
enforcing nonnegativity.
Zhang et al. [23] parallelized NTF for mining global climate data in such a way that the original 3-mode tensor is
divided into three semi-NMF sub-problems based on ALS
approach and these matrices are distributed to independent
processors to facilitate parallelization. Antikainen et al. [2]
presented an algorithm for NTF that is specialized for Compute Uniform Device Architecture (CUDA) framework that
provides a parallel running environment.
Note that since these block-based parallel algorithms are
based on an ALS approach where one variable can be optimized given that the other variables are fixed, the communication cost among each block is not avoidable. In the
parallelized DBN strategy, on the other hand, each block is
completely separable and run independently.
Functional Dependency (FD). A functional dependency
(FD) is a constraint between two sets of attributes X and
Y in a relation denoted by X â Y , which specifies that the
values of the X component of a tuple uniquely determine
the values of the Y component.
The discovery of FDs in a data set is a challenging problem
since the complexity increases exponentially in the number
of attributes [15]. Many algorithms for FD and approximate
FD discovery exist [8, 13, 15, 22]. TANE proposed in [8] used
the definition of approximate FDs based on the minimum
fraction of tuples that should be removed from the relation
to hold the exact FDs.
The computation of FDs in TANE and Dep-Miner [13] is
based on levelwise search [16]. Dep-Miner finds the minimal
FD cover of a hypergraph using a levelwise search. Similarly, FastFD [22] finds the minimal cover, however, differently from Dep-Miner, it uses a depth-first search to address
the problem in the levelwise search that the cost increases
exponentially in the number of attributes. The main factor
in the cost of FastFD is the input size. FastFD works well
when the number of attributes is large. TANE takes linear
time with respect to the size of the input whereas FastFD
takes more than linear time of the input size.
CORDS [9] generalized FDs to determine statistical dependencies, which is referred to as soft FD. In a soft FD, a
value of an attribute determines a value of another attribute
with high probability. CORDS only discovers pairwise correlations reducing a great amount of complexity that nevertheless can remove most of correlation-induced selectivity
error. In this paper, we also leverage pairwise FDs to measure dependency between partitions (interFD) and within a
partition (intraFD).

3.

DECOMPOSITION-BYNORMALIZATION (DBN)

As we discussed earlier, our goal in this paper is to
tackle the high computational cost of decomposition process.
Since, especially for dense data sets, the number of modes of
the tensor data is one of the main factors contributing to the
cost of tensor operations, we focus on how we reduce the dimensionality and the size of the input tensor. In particular,
we argue that a large relation can be vertically partitioned
into multiple relations relying on the approximate functional
dependencies in the data, and the decompositions of these
relations can be combined to obtain the overall decomposition. Without loss of generality, in this paper, we consider

2-way partitioning of the input data. The process can easily
be generalized for multiple partitions.
In this section, we provide an overview of this DBN process. We first introduce the relevant notations and provide
background on key concepts.

3.1

Background and Key Concepts

Without loss of generality, we assume that relations are
represented in the form of occurrence tensors.

3.1.1

Tensor Representation of Relational Data

Let A1 , . . . , An be a set of attributes in the schema of a
relation, R, and D1 , . . . , Dn be the attribute domains. Let
the relation instance R be a finite multi-set of tuples, where
each tuple t â D1 Ã . . . Ã Dn .
Definition 3.1 (Occurrence Tensor). An occurrence tensor Ro corresponding to the relation instance R
is an n-mode tensor, where each attribute A1 , . . . , An is
represented by a mode. For the ith mode, which corresponds
to Ai , let D0i â Di be the (finite) subset of the elements such
that
âv â D0i ât â R s.t. t.Ai = v
and let idx(v) denote the rank of v among the values in D0i
relative to an (arbitrary) total order, <i , defined over the
elements of the domain, Di . The cells of the occurrence
tensor Ro are such that
Ro [u1 , . . . , un ] = 1 â ât â R s.t.â1â¤jâ¤n idx(t.Aj ) = uj
and 0 otherwise. Intuitively, each cell indicates whether the
corresponding tuple exists in the multi-set corresponding to
the relation or not.

3.1.2

Normalization (Vertical Partitoning) and
Functional Dependencies

First introduced by Codd [5], the normalization process
of the relational model evaluates relations based on the underlying functional dependencies of the data and vertically
partitions a large relation into smaller relations (with lesser
number of attributes) to minimize redundancy and insertion,
deletion, and update anomalies.
Functional Dependency (FD). The functional dependency, between two sets of attributes X and Y, in a relational model is defined as the following.
Definition 3.2 (Functional dependency). A functional dependency (FD), denoted by X â Y, holds for relation instance R, if and only if for any two tuples t1 and t2
in R that have t1 [X ] = t2 [X ], t1 [Y] = t2 [Y] also holds.
The key idea of the normalization process is that if A =
{A1 , . . . , An } is a set of attributes in the schema of a relation, R, and X , Y â A are two subsets of attributes such that
X â Y, then the relation instance R can be vertically partitioned into two relation instances R1 , with attributes A \ Y,
and R2 , with attributes X âª Y, such that R = R1 1 R2 ; in
other words the set of attributes X serves as a foreign key
and joining vertical partitions R1 and R2 on X gives back
the relation instance R without any missing or spurious tuples. Note that, while for some data sets, FDs are known at
the database design time, for many data sets they need to
be discovered by analyzing the available data sets.
Approximate FD. As described above, the search for exact FDs is a costly process. Moreover, in many data sets,

attributes may not have perfect FDs due to exceptions and
outliers in the data. In such cases, we may search for approximate FDs instead of exact FDs.
Definition 3.3 (Approximate FD). An approximate
Ï
FD (aFD), denoted by X â Y holds for relation instance
R, if and only if
â¢ there is a subset R0 â R, such that |R0 | = Ï Ã |R| and,
for any two tuples t1 and t2 in R0 that have t1 [X ] =
t2 [X ], t1 [Y] = t2 [Y] also holds; and
â¢ there is no subset R00 â R, such that |R00 | > Ï Ã |R|
where the condition holds.
Ï

We refer to the value of Ï as the support of the aFD X â Y.
As described above, the concept of relational normalization (vertical partitioning) relies on the relational join operation. Since in this paper we represent relations as occurrence
tensors, we also need to define a corresponding tensor join
operation that operates in the domain of tensors.

3.1.3

Tensor Join

Let P and Q be two tensors, representing relation inP
stances P and Q, with attribute sets, AP = {AP
1 , . . . , An }
Q
Q
Q
and A = {A1 , . . . , Am }, respectively. In the rest of
this section, we denote the index of each cell of P as
(i1 , i2 , ..., in ); similarly, the index of each cell of Q is denoted
as (j1 , j2 , ..., jm ). The cell indexed as (i1 , . . . , in ) of P is denoted by P[i1 , . . . , in ] and the cell indexed as (j1 , . . . , jm ) of
Q is denoted by Q[j1 , . . . , jm ].
Definition 3.4 (Join (1)). In relational algebra,
given two relations P and Q, and a condition Ï, the join
operation is defined as a cartesian product of the input
relations followed by the selection operation. Therefore,
given two relational tensors P and Q, and a condition Ï,
we can define their join as
def

P 1Ï Q = ÏÏ (P Ã Q).
Given two relations P and Q, with attribute sets, AP =
P
Q
Q
{AP
= {AQ
1 , . . . , An } and A
1 , . . . , Am }, and a set of atP
Q
tributes A â A and A â A , the equi-join operation,
1=,A , is defined as the join operation, with the condition
that matching attributes in the two relations will have the
same values, followed by a projection operation that eliminates one instance of A from the resulting relation.

3.2

Overview of the Decomposition-byNormalization (DBN) Process

The pseudo-code of the DBN algorithm is shown in Figure 2. In this subsection, we provide an overview of the steps
of the algorithm. Then, in the following sections, we study
in detail the key steps of the process.
In its first step, DBN evaluates the pairwise FDs among
the attributes of the input relation. For this purpose, we
employ and extend TANE [8], an efficient algorithm for the
discovery of FDs. Our modification of the TANE algorithm
returns a set of (approximate) FDs between the attribute
pairs and, for each candidate dependency, Ai â Aj , it provides the corresponding support, Ïi,j .
The next steps involves selecting the attribute, X, that
will serve as the foreign key and partitioning the input relation R into R1 and R2 around X. Note that if the selected

â¢ Since for dense tensors, the complexity of the decomposition process is exponential in the number of
modes [11], we would prefer that the number of attributes in each partition should be balanced.

DBN algorithm (input: a relation R)
1: Evaluate pFD (pairwise FDs between all pairs
P of attributes of R)
2: Select the attribute Ak with the highest k6=j Ïk,j such that
Ïk,j â¥ Ïsupport as the vertical partitioning (and join) attribute
X (Desiderata 1 and 2)
3: if R is a sparse tensor then
4: if X (approximately) determines all attributes of R then
5:
findInterFDPartition(pFD,false) (see Figure 3)
6: else
7:
Move X and all attributes determined by X to R1 ; move X
and remaining attributes to R2 (Desideratum 4 and 5).
8: end if
9: else {i.e., R is a dense tensor}
10: if X (approximately) determines all attributes of R then
11:
findInterFDPartition(pFD,true)
12: else
13:
Move X and attributes determined by X to R1 ; move
X and remaining attributes to R2 â these moves are constrained such that the number of attributes of R1 and R2
are similar (Desideratum 3 and 5)
14: end if
15: end if
16: Partition R into R1 and R2 .
17: If the selected X does not perfectly determine the attributes of
R1 then remove sufficient number of outlier tuples from R to
enforce the FDs between X and the attributes of R1
18: Create occurrence tensors of R1 and R2
19: Decompose the tensors corresponding to R1 and R2 , join the decompositions to obtain candidate combined decompositions, and
select the one that is likely to provide the best fit to R

Figure 2:

â¢ For sparse tensors the number of modes impact the
decomposition cost only linearly. In fact, in this case,
the major contributor to the decomposition cost is the
number of nonzero entries in the tensor [11].
Desideratum 4: Therefore, for sparse tensors, the
vertical partitioning should be such that the total number of tuples of R1 and R2 are minimized.
â¢ Any information encoded with the FDs crossing relations R1 and R2 is risked to be lost when R1 and R2
are individually decomposed in the final step of the
DBN algorithm. This leads to our final desideratum:
Desideratum 5: The vertical partitioning should be
such that the support for the inter-partition FDs (except for the FDs involving the join attribute X) are
minimized.

Pseudo-code of DBN (see Section 4)

findInterFDPartition ( input: pFD, balanced)
1: Create a complete pairwise FD graph, where the nodes are attributes and edge weights are the support values of pFD.
2: if balanced == false then
3: Run minimum average cut on the pairwise FD graph to find a
maximally independent partitioning (Desideratum 5)
4: else {i.e., balanced == true}
5: Run minimum average cut on the pairwise FD graph to find a
balanced, maximally independent partitioning (Desideratum 3
and 5)
6: end if

Figure 3:

Desideratum 3: For dense tensors, the vertical partitioning should be such that the number of attributes
of R1 and R2 are similar.

Pseudo-code of interFD-based partition algo-

rithm; this is detailed in Section 4.3

join attribute X does not perfectly determine the attributes
of R1 , then to prevent introduction of spurious tuples, we
will need to remove sufficient number of outlier tuples from
R to enforce the FDs between the attribute, X, and the
attributes selected to be moved to R1 .
Desideratum 1: Therefore, to prevent over-thinning of
the relation R, the considered approximate FDs need to
have high support; i.e., Ïi,j â¥ Ïsupport , for a sufficiently
large support lower-bound, Ïsupport .
We next consider various criteria for vertical partitioning
of the input relation:
â¢ First of all, as we discussed earlier, when we vertically partition the relation R with attributes A =
{A1 , . . . , An } into R1 and R2 , one of the attributes
X of R2 should serve as a foreign key into R1 to ensure that joining vertical partitions R1 and R2 on X
gives back R without any missing or spurious tuples.
Desideratum 2: If A1 is the set of attributes of vertical partition R1 and A2 is the set of attributes of
vertical partition R2 , then there must be an attribute
Ï
X â A2 , such that for each attribute Y â A1 , X â Y ,
for Ï â¥ Ïsupport .

We elaborate on these desiderata and their implications on
the vertical partitioning of R into R1 and R2 in Section 4.
Once R1 and R2 are obtained, the final steps of the DBN
process include decomposition of the tensors corresponding
to R1 and R2 , joining of the decompositions to obtain candidate combined decompositions, and selecting the one that
is likely to provide the best fit to R. For these, we rely
on the join-by-decomposition scheme proposed in [10]:
in order to construct a rank-r decomposition of a joined
tensor, join-by-decomposition first finds all rank-r1 and
rank-r2 decompositions of the two input tensors, such that
r1 Ã r2 = r. The rank-r1 and rank-r2 decompositions of
the input decompositions are then combined along the given
factor matrix, which corresponds to the join attribute in the
equi-join operation. Finally the algorithm finds the r1 Ã r2
pair which provides the least approximation error, given all
possible factorizations, r1 Ã r2 of r. Note that join-bydecomposition involves creation of multiple alternative join
pairs (corresponding to different r1 Ã r2 factorizations of
r), which are independently evaluated for accuracy and the
one that is predicted to provide the best accuracy is used
for obtaining the final result. This provides a natural way
to parallelize the entire operation by associating each pair
of rank decompositions (and the computation of the corresponding pair selection measure) to a different processor
core. In Section 6, we also evaluate the applicability of such
parallelizations in the context of the DBN operation.

4.

VERTICAL PARTITIONING STRATEGIES

Based on the desiderata discussed in Section 3.2, here we
discuss the vertical partitioning strategies for sparse and
dense tensors. For each situation, we consider the cases:
(Case 1) the join attribute (approximately) determines a
subset of attributes and (Case 2) it determines all attributes
of the input relation.

4.1

Sparse Tensors

For the first case (i.e., the join attribute X (approximately) determines a subset of the attributes of R), we create a partition R1 with all the attributes determined with
a support higher than the threshold (Ïsupport ) by the join
attribute. This helps us satisfy Desiderata 1 and 2. The
second partition, R2 consists of the join attribute X and
all the remaining attributes. Since inter-partition FDs of
the form X â â are all less than Ïsupport , this also reflects
Desideratum 5.
Note that by construction, the size of R2 is equal to the
number of tuples in R independent of which attributes are
included in it. The size of R1 , on the other hand, can be
minimized (to satisfy Desideratum 4) down to the number
of unique values of X by eliminating all the duplicate tuples.
For the second case, where the join attribute X determines
all attributes of R, we apply the interFD-based vertical partitioning strategy detailed later in Section 4.3.

4.2

Dense Tensors

For the first case, similarly to sparse tensors, we create
partitions, R1 and R2 , by considering the FDs of the form
X â â, where X is the join attribute. Differently from the
sparse tensors, in this case we also consider Desideratum 3,
which prefers balanced partitions:
â¢ When there are fewer attributes with support higher
than Ïsupport , we move the attributes with the highest
support from R2 to R1 to promote balance. This is
equivalent to relaxing the support threshold.
â¢ When there are more attributes with support higher
than Ïsupport , we move the attributes with the lowest
support from R1 to R2 to promote balance. This is
equivalent to tightening the support threshold.
For the second case, as in the sparse tensors, we apply the
interFD-based vertical partitioning strategy detailed later in
Section 4.3. The major difference is that in this case, we also
promote balance.

4.3

Vertical Partitioning Based on Supports
of Attribute Pair Dependencies

Desideratum 5 implies that the vertical partitioning strategy should minimize inter-partition FDs. As discussed earlier, this helps minimize the likelihood of error when the
individual partitions are decomposed and the resulting decompositions are combined to obtain the overall decomposition. Kim and Candan [10] have shown that, given a join
operation, R = R1 1A R2 , it is possible to obtain a rank-r
decomposition of R by combining rank-r1 and rank-r2 decompositions of R1 and R2 , as long as r = r1 Ã r2 and that
rank r1 and r2 decompositions of the input tensors lead to
clusters that are independent relative to the join attribute,
A. Authors have also argued theoretically and experimentally that the accuracy of the decomposition is especially
high if the other attributes of the two relations R1 and R2
are independent from each other. Relying on this observation (which we also validate in Section 6), DBN tries to
partition the input relational tensor R in such a way that
the resulting partitions, R1 and R2 , are as independent from
each other as possible.
Remember that the support of an approximate FD is defined as the percentage of tuples in the data set for which

the FD holds. Thus, in order to quantify the dependence
of pairwise attributes, we rely on the supports of pairwise
FDs. Since we have two possible FDs (X â Y and Y â X)
for each pair of attributes, we use the average of the two
as the overall support of the pair of attributes X and Y .
Given these pairwise supports we approximate the overall
dependency between two partitions R1 and R2 using the
average support of the pairwise FDs (excluding the pairwise
FDs involving the join attribute) crossing the two partitions.
We refer to this as interFD and the partitioning based on
interFD as the interFD-based partitioning strategy.
We formulate this interFD-based partitioning problem as
a graph partitioning problem: Let the pairwise FD graph,
Gpf d (V, E), be a complete, weighted, and undirected graph,
where each vertex v â V represents an attribute and the
weights of the edge between nodes vi and vj is the average
support of the approximate FDs vi â vj and vj â vi . The
problem is then to locate a cut on Gpf d with the minimum
average weight.
This graph partitioning problem is similar to the minimum cut problem [19], with some key differences. The major difference is that we do not seek a cut with minimum
total weight, but a cut with minimum average weight. Also,
depending on whether we are operating on dense or sparse
networks, we may or may not seek to impose a balance criterion on the partitions: for sparse tensors, the cost of tensor
decomposition increases linearly with the number of modes
and since the total number of modes of the two partitions is
constant, we do not need to seek balance.
In DBN, we use a modified version of the minimum
cut algorithm proposed in [19] to seek minimum average
cuts. Given an undirected graph Gpf d (V, E), for each vertex v â V , the algorithm finds a vertex with the minimum
average cut that separates it from the rest of the graph and
0
creates a subset of vertices V where the vertex is merged
with its neighbor vertex, connected to the rest of the graph
with the least average weight; the edges from these two vertices are replaced by a new edge weighted by the average of
the weights of the original edges. The process is repeated
0
while V 6= V . The minimum of the minimum average cuts
at each step of the algorithm is returned as the overall minimum average cut. When balance of the number of attributes
is needed, the minimum is selected among the steps that
lead to similar number of attributes. The complexity of this
minimum average cut algorithm is O(|V ||E| + |V |2 log|V |).
Figure 4 shows an example of the process.

5.

RANK PRUNING BASED ON INTRAPARTITION DEPENDENCIES

Given a partitioning of R into of R1 and R2 , to obtain
a rank-r decomposition of R, we need to consider rank-r1
and rank-r2 decompositions of R1 and R2 , such that r =
r1 Ã r2 and pick the hr1 , r2 i pair which is likely to minimize
recombination errors.
We note, however, that we can rely on the supports of
the dependencies that make up the partitions R1 and R2
to prune hr1 , r2 i pairs which are not likely to give good fits.
In particular, we observe that the higher the overall dependency between the attributes that make up a partition, the
more likely the data in the partition can be described with a
smaller number of clusters. Since the number of clusters of
a data set is related to the rank of the decomposition, this
leads to the observation that

0.40

0.40
1

1

2

1

2

0.415

2,3
0.4575

0.44

0.43

0.44

0.43

2,3,4

1

1,2,3,4

0.45

0.45
4

3

0.445

3

4

4

(a) Gpf d (V, E)
(b) Step 1
(c) Step 2
Figure 4: Minimum average cut algorithm for (a) Gpf d (V, E). (b) Step 1:

(d) Step 3

(e) Step 4

a cut between {2} and {1,3,4} is the minimum average
cut (0.44). (c) Step 2: vertex 2 is merged with vertex 3 which has the minimum average weight to the rest of the vertices and
a cut between {2,3} and {1,4} is the minimum average cut (0.43). (d) Step 3: vertices 2 and 3 are merged with vertex 4 and a
cut between {1} and {2,3,4} is the minimum average cut (0.4575). (e) Step 4: the process ends since the subset of vertices is
equal to V . The minimum of the minimum average cuts at each step is (c) {2,3} and {1,4}.
Data set
D1
D2
D3
D4
D5
D6
D7
D8
D9
D10
D11
D12
D13
D14
D15

Adult

Breast Cancer Wisconsin [14]

IPUMS Census Database [18]
Mushroom
Dermatology

Table 1:

Size
118 Ã 90 Ã 20263 Ã 5 Ã 2
7 Ã 20263 Ã 5 Ã 6 Ã 16
72 Ã 20263 Ã 90 Ã 2 Ã 2
20263 Ã 14 Ã 2 Ã 6 Ã 94
20263 Ã 5 Ã 2 Ã 90 Ã 72
645 Ã 10 Ã 11 Ã 2 Ã 10
10 Ã 645 Ã 9 Ã 10 Ã 10
10 Ã 10 Ã 11 Ã 10 Ã 645
2 Ã 10 Ã 10 Ã 10 Ã 645
10 Ã 10 Ã 645 Ã 9 Ã 10
3890 Ã 4 Ã 13 Ã 3 Ã 3
545 Ã 3 Ã 17 Ã 3 Ã 2
11 Ã 3 Ã 4 Ã 5 Ã 3
10 Ã 3 Ã 5 Ã 2 Ã 7
62 Ã 5 Ã 5 Ã 5 Ã 3

Relational tensor data sets with 5 attributes

the higher the overall dependency between the
attributes in a partition, the smaller should be
the decomposition rank of that partition.

Data set

Attributes

D1
D2
D3
D4
D5
D6
D7
D8
D9
D10
D11
D12
D13
D14
D15

{A11 , A12 , A3 , A9 , A10 }
{A2 , A3 , A9 , A8 , A4 }
{A1 , A3 , A12 , A15 , A10 }
{A3 , A7 , A15 , A8 , A13 }
{A3 , A9 , A15 , A12 , A1 }
{A1 , A4 , A7 , A11 , A6 }
{A4 , A1 , A10 , A8 , A9 }
{A6 , A5 , A7 , A8 , A1 }
{A11 , A9 , A6 , A3 , A1 }
{A5 , A4 , A1 , A10 , A8 }
{A8 , A17 , A19 , A3 , A2 }
{A53 , A2 , A21 , A3 , A4 }
{A13 , A48 , A17 , A14 , A2 }
{A4 , A9 , A18 , A17 , A2 }
{A34 , A24 , A33 , A25 , A11 }

Table 2:

Join
attr.
(X)
A3
A3
A3
A3
A3
A1
A1
A1
A1
A1
A8
A53
A13
A2
A34

Support
of X
97%
80%
80%
75%
80%
96%
96%
96%
98%
96%
99%
98%
98%
88%
80%

exec.
time for
FDs
0.024s
0.022s
0.025s
0.023s
0.023s
0.004s
0.003s
0.002s
0.003s
0.003s
0.007s
0.006s
0.005s
0.004s
0.002s

Different attribute sets, join attributes (X), sup-

ports of X (the lowest of all the supports of X â â), and
execution times for FDs discovery for D1-D15 where An is
the nth attribute of each data set

Thus, given R1 and R2 , we need to consider only those
rank pairs hr1 , r2 i, where if the average intra-partition FD
support for R1 is larger than the support for R2 , then r1 <
r2 and vice versa. We refer to this as the intraFD criterion
for rank pruning. Similarly to interFD, given the supports
of FDs, we define intraFD as the average support of the
pairwise FDs (excluding the pairwise FDs involving the join
attribute) within each partition.
In the next section, we evaluate the effect of the interFDbased partitioning and intraFD-based rank pruning strategy
of DBN for both dense and sparse tensor decomposition in
terms of the efficiency and the accuracy.

6.

EXPERIMENTAL EVALUATION

Here, we present experimental results assessing the efficiency and effectiveness of the proposed DBN scheme relative to the conventional implementation of the tensor decomposition in both stand-alone and parallelized versions. We
used various data sets from UCI Machine Learning Repository [6].

6.1

Partitioning Cases

We consider two partitioning cases discussed in Section 4:
Case 1. Firstly, we evaluate DBN for the case where the
join attribute X determines only a subset of the attributes
of the relation R. In this case, as long as one partition R1
has X and the determined attributes of X, the number of
nonzero entries of R1 is less than or equal to that of R and
the number of nonzero entries of the other partition R2 is
same as that of R. For dense tensor decomposition, we make
the number of attributes of R1 and R2 similar.

For this case, we used a 5-mode relational tensor of dimensions 118 Ã 90 Ã 20263 Ã 5 Ã 2 of the Adult data set
(D1). We ran TANE [8] to find FDs on this data set. TANE
identified almost exact FDs A3 â A9 and A3 â A10 (with
99% and 98% confidence respectively) where An is the nth
attribute. Now we take A3 as the join attribute and remove
unmatched tuples (1.72%) for these two FDs to get the exact
FDs. Next the tensor is normalized into two 3-mode tensors
R1 {A3 , A9 , A10 } and R2 {A3 , A11 , A12 }. We then create
relational tensors corresponding to different table sizes by
randomly selecting entries from the data.
Case 2. Secondly, we evaluate DBN in the case where the
join attribute X determines all attributes of the relation R.
In this case, all partitioning cases generate as many tuple
(nonzero entries) as the relation R and thus, it is not possible to select a configuration which better minimizes the
total number of nonzero entries than the rest. In this case,
for each pairwise FD support above a support threshold,
Ïsupport = 75%, we take an attribute with the highest total
pairwise FD support among all attributes of the input relation as the join attribute. For these experiments, we took
15 different data sets (D1-D15) with different sizes and different attribute sets (see Table 1) and we experimented all
partitioning cases with the same number of tuples for R,
R1 , and R2 for each data set.
All tensors were encoded as occurrence tensors (see Definition 3.1), where each entry is set to 1 or 0, indicating
whether the corresponding tuple exists or not. Therefore,
we report the number of nonzero entries. We also report
the tensor size for the dense tensor decomposition since the

decomposition cost of dense tensor decomposition depends
on the size of dense tensors.
We experimented with rank-12 decompositions. DBN uses
6 combinations (1 Ã 12, 2 Ã 6, 3 Ã 4, 4 Ã 3, 6 Ã 2, and 12 Ã 1)
for rank-12 decomposition for each relation in Table 1.

Algorithm
DBN-NWAY
DBN-CP
NNCP-NWAY
NNCP-CP
pp-DBN-NWAY
NNCP-NWAY-GRID2

6.2

NNCP-NWAY-GRID6

Implementation

Discovery of FDs. We employed TANE [8] and made
an extension to detect approximate FDs for all pairs of attributes for pairwise FDs and remove all unmatched tuples
to get the exact FDs for the normalization process based on
the supports of the approximate FDs.
The supports of the approximate FDs for each attribute
set of different relational data sets are shown in Table 2.
The table also shows the execution times to discover FDs
for each data set. The modified TANE algorithm is efficient when the number of attributes is small (5 attributes
in these experiments). The overall cost increases linearly in
the size of the input [8]. Since the execution times for finding approximate FDs are negligible compared to the tensor
decomposition time, we focus on the decomposition times.
Tensor Decomposition. We experimented with alternative algorithms for both nonnegative CP tensor decomposition on the original tensors (NNCP) and DBN. Table 3
shows the various algorithms we use in our experiments.
The first decomposition algorithm we considered is the Nway PARAFAC algorithm with nonnegativity constraint (we
call this N-way PARAFAC in the rest of the paper) which is
available in the N-way Toolbox for MATLAB [1]. We refer
to DBN and NNCP using N-way PARAFAC implementation
as DBN-NWAY and NNCP-NWAY respectively.
Since MATLABâs N-way PARAFAC implementation uses
a dense tensor (multi-dimensional array) representation,
it is too costly to be practical, especially for sparse tensors. Another PARAFAC implementation, the CP-ALS algorithm [3], on the other hand, can run with both sparse
and dense tensors. In the sparse tensor model, the cost
increases linearly as the number of nonzero entries of the
tensor increases. The CP-ALS, however, does not support
nonnegative constraints. Therefore, we implemented a variant of the single grid NTF [17] using CP-ALS as the base
PARAFAC algorithm. We refer to DBN and NNCP based
on CP-ALS as DBN-CP and NNCP-CP respectively.
For the parallel version of the NNCP, we implemented the
grid NTF algorithm [17] with two different partition strategies (2 and 6 grid cells along the join mode) using N-way
PARAFAC and CP-ALS as the base PARAFAC algorithms.
Each grid is run with the base PARAFAC algorithm separately in parallel and results are iteratively combined into
the final decomposition based on an ALS-like approach. We
refer to the grid NTF algorithm for the parallel NNCP using
N-way PARAFAC with 2 and 6 grid cells as NNCP-NWAYGRID2, and NNCP-NWAY-GRID6 respectively. Similarly,
we refer to CP-ALS based implementations as NNCP-CPGRID2 and NNCP-CP-GRID6.
For the parallel version of DBN, we implemented pairwise
parallel DBN-NWAY and DBN-CP, referred to as pp-DBNNWAY and pp-DBN-CP, respectively.
Finally, for DBN with a subset of pairs, DBN with 2 and
3 pairs selected based on the intraFD-based rank pruning
strategy are referred to as DBN2 and DBN3 respectively.
For all alternative DBN strategies can be DBN2 or DBN3

pp-DBN-CP
NNCP-CP-GRID2
NNCP-CP-GRID6
DBN2
DBN3

Table 3:

Description
DBN using N-way PARAFAC
DBN using single grid NTF (CP-ALS)
NNCP using N-way PARAFAC
NNCP using single grid NTF (CP-ALS)
pairwise parallel DBN-NWAY
NNCP using grid NTF with 2 grid cells
(N-way PARAFAC)
NNCP using grid NTF with 6 grid cells
(N-way PARAFAC)
pairwise parallel DBN-CP
NNCP-CP with 2 grid cells
NNCP-CP with 6 grid cells
intraFD-based DBN with 2 pairs
intraFD-based DBN with 3 pairs

Algorithms.

Note that the decomposition algo-

rithms in parentheses are the base PARAFAC algorithm for
the grid NTF.

according to the number of pairs selected (e.g., DBN2-CP
for DBN-CP with 2 pairs selected).
We ran our experiments on an 6 cores Intel(R) Xeon(R)
CPU X5355 @ 2.66GHz with 24GB of RAM. We used MATLAB Version 7.11.0.584 (R2010b) 64-bit (glnxa64) for the
general implementation and MATLAB Parallel Computing
Toolbox for the parallel implementation of DBN and NNCP.

6.2.1

Accuracy

We use the following fit function to measure tensor decomposition accuracy: The fit measure is defined as
fit(X, XÌ) = 1 â

kX, XÌk
,
kXk

(1)

where kXk is the Frobenius norm of a tensor X. The fit is a
normalized measure of how accurate a tensor decomposition
of X, XÌ with respect to a tensor X.
kXÌk is also used as an approximate fit measure of X to
substitute for fit computation for large data sets. The intuition behind this measure is as follows: For any W it can be
shown that kW â WÌk â¥ kWk â kWÌk. Therefore, as long as
kWk â¥ kWÌk holds, we can minimize the term kW â WÌk by
maximizing kWÌk.
Our evaluation criteria also include fit ratio which indicates how accurate one strategy compared with the other
strategy in terms of fit to the input tensor. For example,
the fit ratio of DBN to NNCP is defined as
fit ratio =

fit(X, XÌdbn )
fit(X, XÌnncp )

(2)

where X is the input tensor, XÌnncp is the tensor obtained
by re-composing the NNCP tensor, and XÌdbn is the tensor
obtained by re-composing the DBN tensor.

6.3
6.3.1

Results
Execution time

As discussed in Section 6.1, we evaluate the execution
times of DBN vs. the conventional nonnegative CP (NNCP)
algorithms (Table 3) for two partitioning cases: in the first
case, the join attribute X determines only a subset of the
attributes of the relation R; that is, nnz(R1 ) â¤ nnz(R) and
nnz(R2 ) = nnz(R) where nnz(X ) denotes the number of
nonzero entries of X . In the second case, the join attribute X

&!!!"

Running Time on 6 cores

01--&-2$3&."$$
45567859:;$<!=$>?5859:;@$>AB$

1200
1000

5567859:;$
>?5859:;$

%'!!"

800

%&!!"

600
sec

!"#$

(NNCP-NWAY-GRID vs. pp-DBN-NWAY; D1)

$!!"
#!!"

NNCP-NWAY-GRID2
NNCP-NWAY-GRID6
pp-DBN-NWAY

400
200
0

!"
&(!"

270

')!"
**!"
%)+!"
%&'"$()$*+"$,(&-$.(/"$

630
990
Size of the join mode

(a)
Figure 5:

(b)

Average running times of (a) NNCP-NWAY vs.

Running Time
(NNCP-CP vs. DBN-CP; D1)
NNCP-CP

DBN-CP

DBN3-CP

DBN2-CP

sec

sec

DBN-NWAY and (b) NNCP-NWAY-GRID vs.
NWAY on 6 cores for Adult data set (D1)
8
7
6
5
4
3
2
1
0

1350

7
6
5
4
3
2
1
0

Running Time on 6 cores
(NNCP-CP-GRID vs. pp-DBN-CP; D1)
NNCP-CP-GRID2

pp-DBN-CP

NNCP-CP-GRID6

2 4 6 8 10 12 14 16 18 20 22
# nonzero (Ã1000)

2

(a)
Figure 6:

pp-DBN-

4

6

8 10 12 14 16 18 20 22
# nonzero (Ã1000)

(b)

Average running times of (a) NNCP-CP vs.

DBN-CP with different pair selections and (b) NNCP-CPGRID vs. pp-DBN-CP on 6 cores for Adult data set (D1)

0233435)647,)83)9)-8:,+)

:<==>=.*?>@4*-=*A*2-B41*

("
!!"#$%&"'()*+,-.)

!!"#$%&"%'()*
+,-./0*123,45*1426*

#!!!$
#!!$
#!$
#$
!"#$
!"#$

#$

#!$

#!!$ #!!!$

%%78"%'()"9:;#**
+,-./0*123,45*1426*

(a)

'"
&"
%"
$"
#"
!"
!" #" $" %" &" '" ("
%%'("'("/01#)*+,-.)

(b)

Figure 7:

Average running times of (a) NNCP-NWAYGRID (avg of GRID2 and GRID6) vs. pp-DBN3-NWAY and
(b) NNCP-CP-GRID vs. pp-DBN3-CP on 6 cores for D1D15. In both cases, most of data points are located under
the diagonal, which indicates that DBN outperforms NNCP

determines all attributes of the relation R; that is nnz(R1 )
= nnz(R2 ) = nnz(R).
Case 1: X does not determine all attributes of R.
First we present the execution times for dense tensor decomposition. As seen in Figure 5, for NNCP-NWAY, as
the tensor size increases, the execution time increases fast.
DBN-NWAY, running on smaller number of modes than
NNCP, saves significant amount of time.
Figure 5 (b) shows the running time of NNCP-NWAYGRID2 and NNCP-NWAY-GRID6 vs. pp-DBN-NWAY on
6 cores. Comparing the execution times in the figure against
the single core execution times in Figure 5 (a) shows that the
DBN-NWAY benefits more from parallelization. Since the
grid NTF divides a larger tensor into smaller sub-tensors but
with the same number of modes as the original tensor, parallelization does not save as much as in pp-DBN-NWAY where
a larger tensor is divided into sub-tensors with a smaller
number of modes.
Figure 6 presents results for sparse tensor decompositions, NNCP-CP vs. DBN-CP with different pair selections

on single-core and NNCP-CP-GRID vs. pp-DBN-CP on 6
cores. Since we select a foreign key for normalizing the input
relation, one of the normalized relations in DBN-CP has the
same number of nonzero entries as that of the original relation. As a result, when using a single core, the sparse tensor
DBN time (which is determined by the number of tuples)
exceeds the time required to decompose the original relation. As discussed in Section 5, we address this using the
intraFD-based rank pruning strategy. Figure 6 (a) shows
that we achieve the better performance by choosing 2 pairs
or 3 pairs in DBN based on intraFD.
Furthermore, as shown in Figure 6 (b), when using multiple cores, the proposed pp-DBN-CP achieves greater execution time savings and, therefore, significantly outperforms
parallelized versions of NNCP. As seen in the figure, ppDBN-CP has the lowest execution time. The grid based
parallelization of NNCP does not improve the running time
much, since the underlying ALS-based combining approach
involves significant communication overheads.
Case 2: X determines all attributes of R.
Since in this case, selection of a configuration is especially
difficult, we also consider intraFD-based rank pruning strategy that can maximize the performance of DBN. We experiment NNCP-NWAY-GRID (avg. of NNCP-NWAY-GRID2
and NNCP-NWAY-GRID6) vs. pp-DBN3-NWAY for dense
tensor decomposition and NNCP-CP-GRID (avg. of NNCPCP-GRID2 and NNCP-CP-GRID6) vs. pp-DBN3-CP for
sparse tensor decomposition for D1-D15 on 6 cores. We use
the average of all different partitioning cases for each data
set. As shown in Figure 7, in most cases of both experiments, DBN outperforms NNCP, especially for dense tensor
decomposition. In few cases, the running times are too small
to make a significant difference between NNCP and DBN.

6.3.2

Result Accuracy

We compare the accuracy of DBN against standard
NNCP. Note that we include results for DBN-CP and
NNCP-CP (results for DBN-NWAY and NNCP-NWAY are
similar). We measured the fit ratios for all different partitioning cases of D1-D15 (Table 1). Note that fit computation
requires a lot of memory (the computation is not feasible for
some larger datasets). Thus, for large datasets with the size
of the join mode bigger than 1,000, we created subsets of
each tensor by sampling random 1,000 entries from the join
mode and compute fit for NNCP-CP and DBN-CP.
As shown in Figure 8 (a), for an overwhelming majority
of the experimented data sets, fit ratio falls in the range
between 0.8 and 1 indicating that the proposed DBN scheme
is, in addition to being efficient, also highly effective.

6.3.3

InterFD-based Partitioning

As discussed in Section 4.3, the proposed DBN strategy
first identifies alternative normalized partitions of the relational tensor and then selects the most promising pair of
partitions to compute the final decomposition. More specifically, the algorithm picks the most independent partitions
according to the interFD.
In these experiments, we use 15 different configurations in
Table 1. In order to quantify the benefits of the interFDbased partitioning strategy, we measure the ratio of the difference between the approximate fits of interFD-based partitions and the minimum approximate fits against the difference between the maximum and minimum approximate fits
among all partitioning alternatives of each data set.

!"#$,&'($6"-#,&!)47&1+8$.&",$1+9+0'(-:);<4=>?$

'()#&*+!,+(#-.()+(#/0'."1+(#-.()+(#/$

);<@4=>:);<4=>$

("

!"#$%&'($

!#'"
!#&"
!#%"
!#$"
!"
0

10

20

30

40

50

Frequency

(a)
Figure 8:

);<A4=>:);<4=>$

("
!#'"
!#&"
!#%"
!#$"
!"

)("
)$"
)*"
)%"
)+"
)&"
),"
)'"
)-"
)(!"
)(("
)($"
)(*"
)(%"
)(+"

Fit Ratio

Fit Ratio (DBN-CP/NNCP-CP)
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

)"*+,+-#$.&,''(-"-/$0&1+1$(2$)34)35$

!"#"$%&#$

(b)

(c)

(a) Frequencies of fit ratios DBN-CP/NNCP-CP for all partitioning cases (108 cases) of D1-D15 (Table 1), (b)

ratios of the difference between the approximate fits of interFD-based partitions and the minimum approximate fits against the
difference between the maximum and minimum approximate fits among different partitioning cases of each data set, D1-D15,
and (c) approximate fit ratio of DBN-CP with 2 and 3 pairs to DBN-CP with original 6 pairs for the partitioning cases (41
cases) of D1-D15 where each partition has more than 2 attributes

Results shown in Figure 8 (b) indicates that the interFDbased partitioning strategy results in fits very close to the
optimal partitioning strategy. The few cases where the
interFD-based strategy provides only 70 â 80% of the optimal attribute partitioning is largely due to the pairwise
nature of interFD. This tends to miss multi-attribute dependencies of the form AB â C with high support, e.g., in
D3, we miss A1 A3 â A15 whose support is 94%.

6.3.4

IntraFD-based Rank Pruning

We compare the approximate fit of DBN-CP with 2 and
3 pairs selected based on the intraFD-based rank pruning
strategy against that of DBN-CP using the original 6 pairs.
Note that we use only the cases (41 cases) where each subtensor has more than 2 attributes (only the join attribute
and a determined attribute) for each data set since the intraFD does not include pairwise FDs involving the join attribute. As shown in Figure 8 (c), the intraFD-based rank
pruning strategy of DBN-CP has a good accuracy. The approximate fit ratios in the most of cases of DBN3-CP are
close to 1; in other words, one of the 3 pairs in DBN3-CP
gives the best approximate fit. Note that the data set D4
(with partitions {A3 , A7 , A15 } and {A3 , A8 , A13 }) gives the
worst approximate fit for both DBN2-CP and DBN3-CP,
with fit ratios 0.63 and 0.80 respectively. This is because,
the intraFD strategy with only pairwise dependencies may
on occasion fail to take into account critical dependencies,
such as A3 A7 â A15 with 92% support.

7.

CONCLUSIONS

Lifecycle of most data includes various operations, from
capture, integration, projection, to data decomposition and
analysis. To address the high cost of tensor decompsition, which is highest among these operations, we proposed
a highly efficient, effective, and parallelized DBN strategy
for approximately evaluating decompositions by normalizing a large relation into the smaller tensors based on the
FDs of the relation and then performing the decompositions
of these smaller tensors. We also proposed interFD-based
partitioning and intraFD-based rank pruning strategies for
DBN based on pairwise FDs across the normalized partitions
and within each normalized partition, respectively.
Experimental results confirmed the efficiency and effectiveness of the proposed DBN scheme, and its interFD and
intraFD based optimization strategies, compared to the conventional NNCP approach.

8.

REFERENCES

[1] C. A. Andersson and R. Bro. The N-way Toolbox for
MATLAB. Chemometr. Intell. Lab., 52(1):1-4, 2000.
[2] J. Antikainen et al. Nonnegative Tensor Factorization
Accelerated Using GPGPU. In TPDS, 22(7):1135-1141, 2011.
National Laboratories, 2006.
[3] B. W. Bader and T. G. Kolda. MATLAB Tensor Toolbox
Ver. 2.2, 2007.
[4] J. D. Carroll and J. J. Chang. Analysis of individual
differences in multidimensional scaling via an N-way
generalization of âEckart-Youngâ decomposition.
Psychometrika, 35:283-319, 1970.
[5] E. F. Codd. Further normalization of the data base relational
model. In Rustin, 1972.
[6] A. Frank and A. Asuncion. UCI Machine Learning
Repository. Irvine, CA: U. of California, School of ICS, 2010.
[7] R. A. Harshman. Foundations of the PARAFAC procedure:
Models and conditions for an âexplanatoryâ multi-modal
factor analysis. UCLA working papers in phonetics, 1970.
[8] Y. Huhtala et al. TANE: An efficient algorithm for
discovering functional and approximate dependencies.
Comput. J., 42 (2):100-111, 1999.
[9] I. F. Ilyas et al. CORDS: Automatic discovery of correlations
and soft functional dependencies. In SIGMOD, 647-658, 2004.
[10] M. Kim and K. S. Candan. Approximate tensor
decomposition within a tensor-relational algebraic
framework. In CIKM, 2011.
[11] T. G. Kolda et al. Higher-order web link analysis using
multilinear algebra. In ICDM, 2005.
[12] T. G. Kolda and J. Sun. Scalable tensor decompositions for
multi-aspect data mining. In ICDM, 363-372, 2008.
[13] S. Lopes et al. Efficient discovery of functional dependencies
and armstrong relations. In EDBT, 2000.
[14] O. L. Mangasarian and W. H. Wolberg. Cancer diagnosis via
linear programming. SIAM News, 23(5):1-18, 1990.
[15] H. Mannila and K. RaÌihaÌ. On the complexity of inferring
functional dependencies. Discrete Appl. Math., 40:237-243,
1992.
[16] H. Mannila and H. Toivonen. Levelwise search and borders of
theories in knowledge discovery. Data Min. Knowl. Discov.,
1(3):259-289, 1997.
[17] A. H. Phan and A. Cichocki. PARAFAC algorithms for
large-scale problems. Neurocomputing, 74(11):1970-1984,
2011.
[18] S. Ruggles and M. Sobek. Integrated Public Use Microdata
Series: Version 2.0 Minneapolis: Historical Census Projects,
U. of Minnesota, 1997.
[19] M. Stoer and F. Wagner. A simple min-cut algorithm. J.
ACM, 44 (4):585-59, 1997.
[20] C. E. Tsourakakis. Mach: Fast randomized tensor
decompositions. In SDM, 2010.
[21] L. R. Tucker. Some mathematical notes on three-mode factor
analysis. Psychometrika, 31, 1966.
[22] C. M. Wyss et al. FastFDs: A heuristic-driven, depth-first
algorithm for mining functional dependencies from relation
instances. In DaWak, 2001.
[23] Q. Zhang et al. A parallel nonnegative tensor factorization
algorithm for mining global climate data. ICCS, 2009.

LWI-SVD: Low-rank, Windowed, Incremental Singular Value Decompositions on Time-Evolving Data Sets


Xilun Chen, K. Selçuk Candan
Computer Science and Engineering School of Computing-IDSE (CIDSE), Arizona State University Tempe, AZ, USA xilun.chen@asu.edu, candan@asu.edu

ABSTRACT
Singular Value Decomposition (SVD) is computationally costly and therefore a naive implementation does not scale to the needs of scenarios where data evolves continuously. While there are various on-line analysis and incremental decomposition techniques, these may not accurately represent the data or may be slow for the needs of many applications. To address these challenges, in this paper, we propose a Low-rank, Windowed, Incremental SVD (LWI-SVD) algorithm, which (a) leverages efficient and accurate low-rank approximations to speed up incremental SVD updates and (b) uses a window-based approach to aggregate multiple incoming updates (insertions or deletions of rows and columns) and, thus, reduces online processing costs. We also present an LWI-SVD with restarts (LWI2-SVD) algorithm which leverages a novel highly efficient partial reconstruction based change detection scheme to support timely refreshing of the decomposition with significant changes in the data and prevent accumulation of errors over time. Experiment results, including comparisons to other state of the art techniques on different data sets and under different parameter settings, confirm that LWI-SVD and LWI2-SVD are both efficient and accurate in maintaining decompositions.

of the database [7]: Intuitively, the columns of U can be thought of as the eigen-objects of the data, each corresponding to one independent concept/cluster, and the columns of V can be thought of as the eigen-features of the collection, each, once again, corresponding to a concept/cluster in the database. In other words, SVD can be used for co-clustering both data-objects and features simultaneously. The r × r diagonal matrix S , can be considered to represent the strength of the corresponding latent concepts in the database: the amount of error caused by the removal of a concept from the database is proportional to the corresponding singular value.

1.1 Incremental SVD and Related Works
SVD is computationally costly and therefore a naive implementation does not match the real-time needs of scenarios where data evolve continuously: decomposition of an n × m matrix requires O(n × m × min(n, m)) time. While there are various on-line techniques, these are often slow or inaccurate. For example, one of the fastest techniques, SPIRIT [13] focuses on row insertions and cannot directly handle row deletions or column insertions/deletions. While a forgetting factor can be introduced to discount old objects, it cannot immediately reflect the properties of the removed entries on the decomposition. Moreover, since SPIRIT primarily considers data insertions and deletions, it is not applicable in situations where features of interest themselves evolve with the data (examples include weights of tags extracted from data and proximity to the hubs within an evolving network). As we see in Section 5, it has a higher inaccuracy compared to other incremental techniques, such as [5]. Other incremental SVD algorithms, such as [5, 6, 8, 9, 11, 12, 14, 15, 17], operate on an existing SV decomposition by folding-in new data and features into an existing (often low-rank) SVD; algebraic matrix manipulation techniques are used to rewrite the new SV decomposition matrices in terms of the old SV decomposition and update (including downdating) matrices. [5] showed that a number of database updates (including removal of columns) can all be cast as additive modifications to the original n × m database matrix, A. These updates then can be reflected on the SVD in O(nmr ) time as long as the rank, r , of the matrix A is such that r  min(p, q ), where p is the number of new rows and q is the number of new columns. In other words, as long as the latent dimensionality of the database is low, the singular value decomposition can be updated in linear time. [5] further showed that the update to SVD can be computed in a single pass over the data matrix making the process highly efficient for large data. This and other existing algorithms can nevertheless be slow for many real-time applications.

1.

INTRODUCTION

Feature selection and dimensionality reduction techniques [20] usually involve some (often linear) transformation of the vector space containing the data to help focus on a few features (or combinations of features) that best discriminate the data in a given corpus. For example, the singular value decomposition (SVD [7]) of a data feature matrix A is of the form A = U SV T , where the r orthogonal column vectors of U form an r dimensional basis in which the n data objects can be described. Also, the r orthogonal column vectors of V (or the rows vector of V T ) form an r dimensional basis in which the m features can be placed. These r dimensions are referred to as the latent variables [16] or the latent semantics
 This work is partially funded by NSF grants #1339835 and #1318788. This work is also supported in part by the NSF I/UCRC Center for Embedded Systems established through the NSF grant #0856090.

Permission to make digital or hard copies of all or part of this work for personal or classroom use is granted without fee provided that copies are not made or distributed for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission and/or a fee. Request permissions from permissions@acm.org. KDD'14, August 24­27, 2014, New York, NY, USA. Copyright 2014 ACM 978-1-4503-2956-9/14/08 ...$15.00. http://dx.doi.org/10.1145/2623330.2623671.

1.2 Contributions of this Paper
In Section 2 we formalize these challenges and in Section 3, we propose a Low-rank, Windowed, Incremental SVD (LWI-SVD) algorithm, which leverages efficient and accurate low-rank approxi-

987

mations to speed up incremental SVD updates and uses a windowbased approach to aggregate multiple incoming updates (insertion or deletions) and, thus reduces on-line costs. We also present, in Section 4, an LWI2-SVD algorithm which leverages a novel partial reconstruction based change detection technique to support timely refreshing of the decompositions to prevent accumulation of errors. Experiment results reported in Section 5 confirm that LWI-SVD and LWI2-SVD are both efficient and accurate in maintaining decompositions. We conclude the paper in Section 6.

where K is equal to
K = = I 0 Sx 0
TA Ux RA

Sx 0 +
TA Ux RA

0 I

I 0

TB Vx RB T

T

(6) (7)

0 0

TB Vx RB

2.2.2 Matrix K
Let us remember that X is an n × m matrix and X  is an n × m matrix. Given this · if n  n and m  m , K is a matrix of size (n + 1) × (m + 1). This is because, if n  n , then Ux is an orthogonal T matrix and Ux Ux is equal to I . Consequently, QA RA = T (I - Ux Ux )A = 0 and this implies that RA is simply 0. The same is true for RB . · if n < n and m < m , K is a matrix of size n × m . In Section 3.2.1, we discuss the shape K takes in this case and the resulting properties in detail. · if n  n and m < m , K is a matrix of size (n + 1) × m . · if n < n and m  m , K is a matrix of size n × (m + 1).

2.

BACKGROUND

2.1 Problem Definition
At time stamp i, we are given a set of n data tuples Di = {ti,1 , ti,2 , ..., ti,n }, each with a set, Fi , of m features. We are also given the set, Li , containing the r latent semantics of Di (and their weights). As time moves, new tuples arrive and some of the existing tuples expire: at the next time stamp, ti+1 , the tuple set is - + - Di+1 = (Di \Di +1 )  Di+1 , where Di+1 are the tuples + that expired and Di+1 are the new tuples that arrived. Moreover, at time (i + 1), we have a new set, Fi+1 , of features, where + - Fi+1 = (Fi \Fi- +1 )  Fi+1 , where Fi+1 are features that are + not of interest anymore and Fi+1 are the new features of interest. Our goal is to quickly obtain Li+1 containing the r latent semantics corresponding to time instance, i + 1, and efficiently maintain these r latent semantics as time further moves.

2.2.3 Using the Decomposition of K to Obtain the Decomposition of X 
T . Let us consider the SV decomposition of K ; i.e., K = UK SK VK Equation 1 can be rewritten [5] as

X  = X + AB T = [ Ux

QA

UK )SK [ Vx

QB

VK )T (8)

2.2 Basic Incremental SVD [5]
Let us be given an n × m data matrix X = and an n × m updated data matrix X  = X + , where  is a max(n, n ) × max(m, m ) change matrix. Note that if X  has larger dimension than X , X is padded with n - n rows of zero and m - m columns of zero to match the dimension of , the removal of rows and columns are modeled by additions that result in zeroing of the corresponding rows and columns (which are then dropped from the matrix). Let us further assume that the change matrix  can be decomposed into  = AB T . Note that we can rewrite the matrix X  as
X  = X + AB T = Ux A Sx 0 0 I Vx B
T T Ux Sx Vx

giving us the SVD of the new tuple matrix, X  . The challenge, of course, is to obtain the matrices, QA and QB , and the SV decomposition of K efficiently. In order to keep the complexity down, [5] suggests that A and B should be taken as combination of simple column vectors so that AB T can be the sum of multiple rank-1 matrices. This, however, may be a significant constraint in real-applications where the change matrix  itself can have a large size, indicating great amount of rank-1 matrices it produces and updating a sequence of rank-1 matrix is not effective as treating them as a whole. In the next section, we discuss how to relax this assumption of [5] without impacting efficiency and accuracy.

.

(1)

3. LWI-SVD
We now present our key ideas for efficient incremental SVD operations. As described above, this involves efficiently searching for matrices, QA and QB , and the SVD of K .

Given these, [5] incrementally maintains SVD as follows:

2.2.1 QR Decompositions
T Let us also define QA as the orthogonal basis of (I - Ux Ux )A T and QB as the orthogonal basis of (I - Vx Vx )B . Both QA and QB T can be obtained through QR decomposition [4] of (I - Ux Ux )A T and (I - Vx Vx )B : T QA RA = (I - Ux Ux )A; T QB RB = (I - Vx Vx )B

3.1 Efficiently Obtaining QA and QB
T As described above, QA is the orthogonal basis of (I - Ux Ux )A T and QB is the orthogonal basis of (I - Vx Vx )B . These can be obtained using two expensive QR decomposition operations for both QA and QB . One way to reduce the number of QR decomposition operations would be to seek a decomposition of  where X  = X +  = AAT ; i.e., A = B . However, not all  will have such a convenient decomposition. When  is negative definite, it cannot be written as the format of A × B where A = B . Instead, in this paper, we propose to reduce the cost of the overall QR decomposition step by setting A to the identity matrix I and setting B T to . This does not lose any generality on the algorithn since  (B T ) can be any matrix. When we do this, since A = I , 0 it would also be the case that QA = . Therefore, we need only I one QR decomposition. What is more, if the  only reflect a small amount of data insertions and deletions, then it will be a sparse matrix with last few rows and columns of nonzero values. This

(2)

Here QA and QB are orthogonal matrices and RA and RB are upper-triangular. It is easy to see, through basic matrix algebra, that the following holds:
Ux Vx A B = = Ux Vx QA QB I 0 I 0
TA Ux RA TB Vx RB

(3) (4)

Moreover, by substituting Equations 3 and 4 into Equation 1, we can get  T T
X = X + AB = Ux QA K Vx QB (5)

988

T T lead to efficient computation of (I - Vx Vx )B and Vx B by block matrix multiplication. Let's first find the zero block of B when it is data insertion. Then  (B ) is a n × m matrix with a block of zero values on the first n × m position. We can rewrite B as

Secondly, using a similar zero-padding, we can get the following equalities:
(In - U  U T )A = (Im - V  V T )B = 0 0 0 In -n 0 Bm -m


(9) (10)

B=

0 B2

B1 B3

T T Then, we can divide (I - Vx Vx ) and Vx into the same block T size as B . For example, we can rewrite Vx as T Vx =

The right hand side of Equation 9 has n - n independent columns and, thus, it has a simple QR decomposition:
QA = 0 In -n and RA = 0 In -n

Vx T 0 Vx T 2

Vx T 1 Vx T 3

T Then, the multiplication of Vx B becomes

T Vx ×B =

Vx T 1 × B2 Vx T 3 × B2

T Vx T 0 × B1 + Vx 1 × B3 T Vx 2 × B1 + Vx T 3 × B3

Since the right hand side of the Equation 10 consists of 0s except for the last m - m rows, the QR decomposition of the left hand    side will be such that QB  R(m -m)×m and RB  R(m -m)×n . Let us further partition RB into two,
RB = where RB1 
 R(m -m)×n

RB1

RB2

,

Note that, the multiplication of Vx T 0 and the corresponding block of B is avoided since the corresponding block of B is all zeros. T Also, the other part of Vx and B are small size thin matrices. Thus, T the multiplication of Vx ×B can be done very efficiently. The same T applies to (I - Vx Vx ) × B and when the data are deleted. As we experimentally show in Section 5.4.1, this optimization provides significant gains in time, without any noticeable loss in the final accuracy.

and RB2 

  R(m -m)×(n -n) .

Given the above, we can rewrite the matrix, K , as
K= S 0 0 0 + U T A RA V T B RB
T

where
U T A RA V T B RB
T

= =

UT 0 0 RB1

0 In -n Y RB2
T

3.2 Efficiently Decomposing K
The next challenge is to obtain the singular value decomposition of the matrix, K . Performing SVD on K directly would be costly as the SVD operation is expensive. However, as we prove next, in Section 3.2.1, in the presence of row and column insertions, K takes a special structure:
K = Sx 0 0 0 +
TA Ux RA TB Vx RB T

.

Here Y is a m × (n - n) matrix. Note that we can further rewrite
U T A RA V T B RB
T

=

U T A RA

(V T B ) RB1 RB2

T

=

Sx 

 . 

as
UT 0 0 In -n 0 RB1 Y RB2
T

More specifically, in the presence of insertions, (a) since Sx is diagonal, K is mostly sparse, and (b) it is shaped like an arrow: (aside from the diagonal) there are non-zeros only on its last rows and columns. We verify these next.

=

0 YT

U T RT B1 RT B2

.

Thus, K simplifies to
K= S YT U T RT B1 RT B2 = S   . 

3.2.1 Shape of K
Let X be an n × m matrix and X  be an n × m matrix. In Section 2.2.2, we have seen that K is either of size n × m , (n + 1) × (m + 1), (n + 1) × m, or n × (m + 1), depending on whether the numbers of rows and columns increase or decrease when the data matrix transforms from X to X  . Let us further assume that n  m and m > m and n > n, which is rows and columns insertion. As we already discussed before, let us set A = In and       B T = , where A  Rn ×n , B  Rm ×n and   Rn ×m so that AB T is equal to the update matrix . Finally, let SVD of X T be X = Ux Sx Vx , or simply X = U SV T . Given the fact that X  = X + , we can also deduce that X  = U  SV T + , where
U = U 0  Rn


This confirms that when m < m and n < n , K is shaped like an arrow: it is diagonal, except for the last n - n rows and last m - m columns. This, however, is not true when m  m or n  n ; in this case K can be a dense matrix, with its last row and columns equal to 0. In the rest of this section, we argue that, especially when m < m and n < n , we can leverage K 's specific structure (sparse, arrow-like) to quickly obtain a highly-accurate ^S ^V ^ T and use it instead of the approximate decomposition, K  U exact decomposition K = U  S  V T . In particular we propose to build on the SVD through QR decomposition with column pivoting technique proposed in [4]. Experiment results reported in Section 5 show that this leads to efficient and accurate decompositions even in cases where m  m or n  n .

×n

and V  =

V 0

 Rm




×m

.

Intuitively, U and V are augmented by padding n - n rows of zeros to U and m - m rows of zeros to V to make it compatible with . This padding gives us
X = X 0 0 0 +  = U  SV T + .

3.2.2 Decomposition of K through Pivoted QR Pivoted QR Factorization. Let E be a matrix. A pivoted QR factorization of E has the form EP = Qe Re where P is a permutation matrix, Qe is orthonormal and Re is upper triangular. [4] has shown that a rank-k approximation can be obtained efficiently through a pivoting process where columns of E are considered one at a time and used to compute an additional column of Qe and

989

row of Re . The kth round of the process leads to a rank-k approximation of the pivoted QR factorization of E . In particular, let us assume that we are given a QR decomposition of the form F = Qf Rf and need to compute QR decomposition of [F a] for some column vector a: [F a] = [Qf q ] Rf 0  

The rank-k approximation can be obtained efficiently by the quasiGram-Schmidt method, which further eliminates the need to store dense Qf matrices [4]: the quasi-Gram-Schmidt process can be applied successively to columns of a given input matrix E to produce a pivoted QR factorization for E . Low-Rank Decomposition of K . Let us assume that we are targeting a rank-k decomposition of K . We first sample k columns to obtain column-sample matrix C ; we then sample k columns from K T to obtain a row-sample matrix RT . We then apply the QR decomposition with column pivoting to C and RT to obtain upper triangular matrices, Rc and Rr . The sampling is done by selecting the longest row and column vectors. We note that when m < m and n < n , K is not only sparse, but also has an arrow-like shape: K= Sx   , 

Algorithm 1 LWI-SVD. Input: T The Base Matrix, X , and its SV decomposition Ux Sx Vx ; T The update matrix,  = AB , corresponding to a window of updates; Target rank, r ; Output:    The new SVD results, Ux , Sx , and Vx ; 1: Calculate factors RA and RB in Equation 2 which, as discussed in Section 3.1, involves a QR Decomposition and several matrix multiplications; 2: Calculate the matrix K in Equation 7; 3: Obtain the low-rank (rank-r ) decomposition of K into K = T UK SK VK ; 4: Combine the factors as shown in Equation 8 to obtain rank-r    decomposition Ux , Sx , and Vx ;    5: return Ux , Sx , and Vx ;

the previous step as its input, there is a likelihood that errors will accumulate over time and the reconstruction error relative to the actual matrix will reach an unacceptable rate. To prevent errors to accumulate, in the next section we propose a novel LWI-SVD with Restart (LWI2-SVD) algorithm which restarts the SVD by performing a fresh SVD on the current data matrix.

where the n × m matrix Sx is diagonal, whereas n × (m - m) matrix , (n - n) × m matrix , and (n - n) × (m - m) matrix  are potentially dense as we discussed in Section 3.2.1. As a result, the sampling is arrow-sensitive in the sense that it focuses on the last few rows and columns: The sampled columns usually come from the first few columns (which contain the largest singular values at the top-left corner of the matrix) and the last few columns,  which contain entries from the dense, . Similarly, the sampled  rows come from the first few rows (which contain large singular values in Sx ) and the last few rows from   . Given these, to obtain a decomposition of K , we need to find a matrix H such that K -C H RT is minimized. According to [15], the value of H which minimizes this can be computed as
-1 -T -1 -T (Rc Rc )(C T K R)(Rr Rr ).

4. LWI2-SVD: LWI-SVD WITH RESTART
In this section, we build on LWI-SVD and propose a novel LWISVD with Restart (LWI2-SVD) algorithm which punctuates the incremental SVD sequence by occasionally performing a full SVD on the current data matrix. Obviously, there is a direct, positive correlation between the frequency of restarts and the overall accuracy of the LWI2-SVD algorithm. Unfortunately, however, there is also a strong positive correlation between the cost of LWI2-SVD and the frequency of restarts. Therefore, restart rate should be such that the process is restarted only when the costly SVD is in fact needed to help reduce the overall error.

4.1 Types of Errors
We see that there are two distinct types of errors: · Accumulated approximation errors (and periodic restarts): The first type of error that accumulates over time is due to the various approximation terms, including the low-rank approximation of K as discussed in Section 3.2. While the absolute value of this error will be different from one iteration of the algorithm to the next, its long term behavior will be roughly constant. Therefore, this type of accumulated approximation errors are best dealt with periodic restarts. · Error bursts due to structural changes in the input data (and on-demand restarts): The second type of error in the incremental SVD occurs when there is a significant structural (or spectral) change in the data, necessitating large changes in the SVD. Since the incremental process described in Section 3 assumes that the changes are relatively small, a significant structural change in the factor matrices, Ux and Vx , or the core matrix Sx may not be correctly captured, resulting in a large burst of reconstruction error. These bursts are best dealt with on-demand restarts that are triggered through a change detection process that tracks the updates to identify when major structural changes in the data occur. Figure 1 shows an example run with and without restarts. Note that without the restarts errors continuously accumulate due to structural changes in the data. Restarts (both periodic and on-demand)

Thus, we can rewrite C H RT as
-1 -T T -1 -T (C Rc )(Rc C K RRr )(Rr RT ). -T T -1 If we further set W = Rc C K RRr and decompose W into T , then we can obtain the SV decomposition of K W = Uw Sw Vw T as K = UK SK , VK , where -1 T T -1 -T UK = C Rc Uw , VK = Vw Rr Rr , and SK = Sw ,

where UK and VK are orthonormal and SK is diagonal. While this process also involves an SV decomposition step involving W , since W is a much smaller, k × k, matrix, its decomposition is much faster than the direct decomposition of K .

3.3 Pseudocode of LWI-SVD
Algorithm 1 provides the pseudo-code of the proposed Lowrank, Windowed, Incremental Singular Value Decomposition (LWISVD) algorithm for incrementally maintaining the SVD of an evolving matrix X . As we later see in Section 5, the LWI-SVD algorithm has a smaller approximation error than other algorithms, such as SPIRIT [13], yet is also much faster than optimal as well as the basic incremental SVD [5] algorithms. Yet, as in any incremental approximate algorithm, in which each step takes the output of

990

!"#$%&'()'*+,%"$-'./0-$#"$,1,$2$"$,'3$22-4'"('"5$',%"%'&%"#/6

! ! ! " ! ! ! ! ! ! "

"

"

"

"

" ! ! ! !

.%4'7$0$#%"$'#%0,(&'0*&8$#9':9 8$";$$0'<'%0,'3*##$0"'/0,$6 .84'/)':'?';9' #$+2%3$';/"5'%'#%0,(&2@'-$2$3"$,' 3$22'/0'"5$'#$-$#A(/#

!"#"$

.84'/)':'=>';9' /70(#$'*+,%"$

#$-$#A(/#'.-/:$'>';4

(a) reservoir maintenance for si = + rowi , coli Figure 1: Example runs with and without restarts
!"!#!$%&'$#(!) /.*$#+&0 /.*$#+&1

! ! ! " !

! ! ! ! "

"

"

"

"

" ! ! " !

!
!"!#!$%&*+,-'. *+,-'.&0 *+,-'.&1

!"#$%&'$%($)(*"%+$%,+ &+-(.+/$*+))$01$ %,+$&+2+&.(0& !6#$07$7(81/9$&+:)"*+$%,+$ +)+-+1%$50%,$%,+$1+;% <=<$01$%,+$2%&+"-

!"!#!$%& (+2+(3-!(

.$(#!$% (+,-"2#/,#!-" (+2+(3-!(&0 (+2+(3-!(&1

.$(#!$% (+,-"2#/,#!-"

&+2+&.(0&$!203+$4$5#

(b) reservoir maintenance for si = - rowi , coli Figure 3: Overview of the reservoir based matrix sampling
,4$"5+&6789:; ,4$"5+&6789:;

Figure 2: Overview of the change detection process can limit the accumulation of errors. Error accumulations due to approximations generally show a regular behavior and the frequency with which periodic restarts are scheduled can be set empirically. The structural changes in the data, however, do not necessarily have a regular behavior; therefore, the challenge is to quickly and efficiently detect the structural changes in the data. We will discuss this next.

4.2 Change Detection through Partial Reconstruction
In order to detect major structural changes in the data we need to measure or estimate the reconstruction errors. The naive way to achieve this would be to reconstruct the entire matrix from the incrementally maintained decomposition and compare the reconstructed matrix to the ground truth (which is the actual, revised data matrix). If the difference is high, it means that due to some structural changes, the incrementally maintained decomposition deviated from the true decomposition of the matrix. Obviously, performing a full reconstruction of the matrix at each time step would be extremely costly. Instead, in this section, we propose a change detection scheme which relies on a partial reconstruction as depicted in Figure 2: (a) a fair data matrix sampler, which identifies a small subset of the matrix cells as ground truth and (b) a partial reconstructor, which reconstructs a given subset of matrix cells, without reconstructing the full data matrix.

4.2.1 Fair Sampling of an Evolving Matrix
We propose a fair sampler, where all matrix cells have a uniform probability of being selected independently of when they are updated. Basic Reservoir Sampling. Reservoir sampling [18] is a random sampling method that works well in characterizing data streams. It is especially efficient because (a) it needs to see the data only once

and (b) it uses a fixed (and small) buffer, referred to as the "reservoir". Furthermore, while (c) it does not require a priori knowledge of the data size, it (d) ensures that each data element has an equal chance of being represented in the sample. Let S be a data stream consisting of a sequence of elements si . The reservoir sample keeps a fixed reservoir of, say w elements. Once the reservoir is full, each new element, si , replaces a (randomly) selected element in the reservoir with a decreasing probability, inversely proportional to the index, i, of the new element si . More specifically, a random element in the reservoir is replaced by si with probability w . Intui itively, in a fair sampling, each element up to i should have a w/i chance of being in the random sample of size w. Therefore, si is selected to be included in the reservoir with probability w . The i sample it replaces, on the other hand, is chosen randomly among the existing w samples in the reservoir to ensure that the reservoir forms a random sample of the first i elements in the stream. Matrix-Reservoir Model. As we described earlier, we consider the general case where the data matrix can grow or shrink with insertions or deletions of rows and columns. More specifically, we model the evolving data matrix as a stream, S , of si = ± rowi , coli , where rowi and coli are the row and columns affected in the update with index i: + rowi , coli indicates that the update inserts a new cell in the matrix at location rowi , coli , whereas - rowi , coli indicates that the cell at location rowi , coli is being removed. The reservoir, Ri = {ri,1 , . . . , ri,w }, at time i consists of w matrix cell positions, which serve as the representatives for the current matrix. In other words, each ri,j  Ri is a triple of the form ri,j = indexi,j , rowi,j , coli,j , where indexi,j is the index of the update that deposited the cell, located at rowi,j and coli,j , into the reservoir. Matrix-Reservoir Maintenance for si = + rowi , coli . As discussed earlier, reservoir sampling randomly selects some of the incoming stream elements for the updating the contents of the reservoir When the (probabilistically) selected incoming stream entry si is of the form + rowi , coli , the basic reservoir sampling process is applied: a random element, ri-1,j from the current reservoir

991

Ri-1 is selected and this is replaced with i, rowi , coli . This process is visualized in Figure 3(a). Matrix-Reservoir Maintenance for si = - rowi , coli . When the (probabilistically) selected incoming entry si is of the form - rowi , coli , on the other hand, the basic reservoir sampling process cannot be applied as this denotes removal of a cell, not insertion. We handle deletions as follows: · if there exists no ri-1,j = indexi-1,j , rowi-1,j , coli-1,j  Ri-1 , such that rowi-1,j = rowi and coli-1,j = coli , then si is simply ignored; · if, on the other hand, there exists a ri-1,j = indexi-1,j , rowi-1,j , coli-1,j  Ri-1 , such that rowi-1,j = rowi and coli-1,j = coli , then ­ we drop ri-1,j from the reservoir and ­ we keep the j th position reserved for a future update of the form sh = + rowh , colh . Intuitively, the matrix reservoir (and its history) is revised as if the future insertion sh had in fact arrived in the past, instead of sindexi-1,j , which had originally deposited the cell, rowi-1,j , coli-1,j (which is being deleted) into the reservoir. This process is visualized in Figure 3(b).

Algorithm 2 LWI2-SVD Input: T The Base Matrix, X , and its SV decomposition Ux Sx Vx ; T The update matrix,  = AB , corresponding to a window of updates; Target rank, r ; Reservoir, R; Restart Threshold, ; Periodic Restart Flag, f ; Output:    The new SVD results, Ux , Sx , and Vx ;  The new Reservoir, R ; 1: X  = X +  2: if f = true then    3: Ux , Sx , Vx = topK_SVD(X  , r );  4: R = updateReservoir(R, ); 5: else    6: Ux , Sx , Vx = LWI-SVD(X, Ux , Sx , Vx , , r );  7: R = updateReservoir(R, );    ^ = partialReconstruct(R , Ux 8: V , Sx Vx );   ^ 9: E = measurePartialError(V, R , X ); 10: if E >  then    11: Ux , Sx , Vx = topK_SVD(X  , r ); 12: end if 13: end if    14: return Ux , Sx , Vx , R ;

4.2.2 Partial Matrix Reconstruction
At time t = i, let us have the reservoir Ri = {ri,1 , . . . , ri,w }, where for all 1  h  w, ri,h = indexi,h , rowi,h , coli,h . Intuitively, the reservoir consists of a set of matrix cell positions (that were fairly sampled from the overall matrix). During the partial reconstruction step, we use the (incrementally maintained) SV decomposition, Ui , Si , and Vi , of the data matrix Xi to reconstruct only the row and column positions that appear in the reservoir, ri,h . ^i = More formally, the partially reconstructed matrix value set V ^ ^ {vi,1 , . . . , vi,w }, is such that for all 1  h  w, ^ i [rowi,h , coli,h ], where ^ vi,h = X ^ i [rowi,h , coli,h ] = (Ui [rowi,h , ]) Si ViT [, coli,h ] . X Note that the cost of the partial reconstruction of the matrix depends on the size of the reservoir and when |Ri |  |Xi |, partial reconstruction is much faster than full reconstruction. Symbol dim(n × n) Table 1: Parameters Desc. Default Initial(for inser- 100 × 100 tions)/Final(for deletions) dimensions of X Target rank 5 Length of the data 50 stream Numbers of 2:2 columns:rows updated at a given iteration Strength of the updates 5 (for synth. data) Reservoir size 50 On-demand restart 20% threshold Restart period 15 Alternative 300 × 300

r len numupd upd w  per

10 50 6:6 10 150 10% 5

4.2.3 Change Detector
At time t = i, given the reservoir Ri = {ri,1 , . . . , ri,w }, we construct a ground truth value set Vi = {vi,1 , . . . , vi,w }, where for all 1  h  w, vi,h = Xi [rowi,h , coli,h ]. Similarly, we also ^ i = {^ have the partially reconstructed value set V vi,1 , . . . , ^ vi,w }, ^ i [rowi,h , coli,h ], where where for all 1  h  w, ^ vi,h = X ^ i [rowi,h , coli,h ] is the partially reconstructed value for the cell X location rowi,h , coli,h . Given these, we detect a major structural change in the data matrix if
w

5. EXPERIMENTS
In this section, we evaluate the efficiency and effectiveness of LWI-SVD and LWI2-SVD on both synthetic and real datasets and for different scenarios and parameter settings. Each experiment, consisting of len consecutive update iterations, was run 10 times and averages are reported. Note that to simplify the interpretation of the results we have considered insertion sequences and deletion sequences; but not hybrid insertion/deletion sequences. Also, to make sure that the results for experiments involving sequences of insertions and deletions are comparable, we have set the initial dimensions for an insertion sequence and the final dimensions of a deletion sequence to the same value, dim. The various parameters varied in the experiments, default values, and value ranges are presented in Table 1. Below we describe the experimental setting, including the data sets, in greater detail.

(vi,h - ^ vi,h )2  ,
h=1

where  is the inaccuracy threshold.

4.3 Pseudocode of the LWI2-SVD Algorithm
We provide the pseudocode of the LWI-SVD with Restart (LWI2SVD), which was detailed in this section, in Algorithm 2. In the next section, we evaluate the efficiency and effectiveness gains of LWI2-SVD algorithm on top of the gains provided by LWI-SVD.

5.1 Real Data: Digg.com Traces
We use Digg.com data set [2] from Infochimps to evaluate the effectiveness and efficiency for real data. The complete data set

992

was recorded from August to November 2008 and has 3 main components: stories, comments and replies. "Stories" contain 1490 articles that users have posted within the time period. For our experiments, we created data streams by considering the first n + len × numupd articles in the data set(the first n articles make up the initial data matrix; for each of the len iterations in the update stream, we considered numupd new articles). Given this data set, we removed the stop words and applied stemming. We then selected the first n stories and identified the most frequent n keywords 1 . Xij denotes occurrence of keyword j in story i. Intuitively, the low-rank decomposition of the data matrix X simultaneously cluster stories and keywords, resulting a coclustering of the data matrix X . We moved the window at each iteration by inserting or deleting numupd records of the story trace and recomputing the n most frequent keywords (meaning that numupd many rows and columns are inserted and deleted). These correspond to row and column insertions/deletions on X .

deletions and column insertions and deletions (for our experiments, we used the implementation obtained from [3]). LWI-SVD family of the algorithms extend our implementation of the Brand's algorithm described in [5] along with the Algorithm 844 [4] obtained from [1]. As evaluation criteria, we use three metrics: reconstruction error overhead, execution time, and execution time gain: · average relative reconstruction error (errrel ) ­ this accuracy measure is defined as 1 len
len

i=1

^ i, , Xi ) - rec_error (X ^ i,SV D , Xi ) rec_error (X , ^ rec_error (Xi,SV D , Xi )

5.2 Synthetic Data: Random Traces
We have also experimented with synthetic data sets where we could freely vary the characteristics of the data and updates to observe the accuracy and efficiency of our algorithms under different scenarios. For these experiments, we have created synthetic activity traces which we then converted into data matrices as before. Since the matrices for real data is sparse, we focus on dense matrices. In particular, we have generated an initial n-length random sequence of 5 dimensional data, where each dimension has a value from 0 to 10. Given these n consecutive records in the trace, we have created a n × n initial matrix measuring pairwise Euclidean distances of the records in the sequence. Insertions in the random trace were generated by randomly picking numbers with exponential distribution, with the rate parameter, upd (i.e., prob(x) = exp_dist(x, upd ) = upd e-upd x ). Intuitively, if the rate parameter upd is large, there is a higher likelihood of having more large amplitude changes. If the rate parameter upd is low, there is a lower frequency of large amplitude changes in the trace. As before, we enlarged or shrank X at each iteration by adding or deleting numupd units of the random activity trace (meaning that numupd many rows and columns are inserted to or deleted from into the matrix, X ).

where ­ len is the number of iterations (length of the stream), ^ i, denotes the decomposition of the data matrix at ­ X time i obtained using the algorithm "", and ­ rec_error (Y, X ) denotes the reconstruction error of the decomposition Y against the data matrix X , measured in terms of F robenius norm. Note that a low-rank decomposition of Xi would lead to a reconstruction error, even if it is obtained using full SVD followed by selection of the top r components. Therefore, the denominator of the above term is not equal to 0. · absolute execution time (texec ) ­ this is the time, in seconds, that is required to complete len consecutive decompositions using the algorithm under consideration. · time gain (gaintime ) ­ the gain in time is the execution time measured against the execution time of the full SVD; i.e., texec,svd -texec, . texec,svd All experiments were conducted using a 4-core Intel Core i5-2400, 3.10GHz, machine with 8GB memory, running 64-bit Windows 7 Enterprise. The codes were executed using Matlab 7.11.0(2010b).

5.4 Evaluation with the Default Settings
5.4.1 Real Trace Data Set
Figure 4 presents the accuracy and efficiency results for the real trace data for the default parameters reported in Table 1. Accuracy. The first thing to note in Figure 4(a), which reports average relative reconstruction errors for the various versions of the LWI-SVD algorithm proposed in this paper, is that restarts discussed in Section 4 are highly effective in reducing the overall error. While both partial reconstruction-based and periodic restarts used in LWI2-SVD are effective in improving accuracy over the LWISVD (which does not use restarts), the best results are obtained when these are used together, bringing down the average relative reconstruction error to 0.3-0.7% of the low-rank decomposition obtained through full SVD. The second thing to note in Figure 4(a) is that row/column insertions, which bring in new data into the matrix, results in larger relative reconstruction errors than row/column deletions. Note that, when both reservoir-based and periodic restarts are employed, the accuracy penalty relative to the low-rank decomposition of full SVD is negligibly low for both insertions and deletions. Efficiency. Figure 4(b) shows the efficiency results for this data set under the default parameter configuration. The first thing to note is that there is minimal time difference between the LWI-SVD and LWI2-SVD algorithm. This indicates

5.3 Evaluation Criteria and Competitors
We evaluate the LWI-SVD and LWI-SVD with Restart (LWI2SVD) algorithms by comparing them to alternative approaches: · Full SVD and SVDS ­ SVD is the full SV decomposition of the matrix, we used Matlab's [U, S, V] = svd(X) command for this. We also considered with Matlab's [U, S, V] = svds(X,r) command which returns the composition results for the top-r components, where r is the desired rank (SVDS tends to perform more efficiently than SVD when r is small and X is large and sparse); · Naive Incremental SVD ­ this is our implementation of the Brand's algorithm described in [5], it involves a full SVD and pivoted QR based approximation is not leveraged (to implement LWI-SVD and LWI2-SVD, we use this implementation as the basis); and · SPIRIT ­ this is the algorithm described in [13] which provides fast decompositions, but does not have various desirable properties of incremental SVD; including explicit data
1 In these experiments, without loss of generality, we kept the matrix in square shape, i.e., n = m

993

+&,-$.'/)0&'$.'1#234"-$!""#"$
$#%&

728#&93:5;23&93<0=>?/#&.//0/&
&!"#

5"'/)0&'$4#$#607)/$4#68"9$

!"#$"%&

@/3:5;23&?0&0A;B5:&?0AC/D&

!""#"$%&'"(')*$

.//0/&123/4356&

C@@>D=E7& 16F6D=E7&

%!"#

@<<:A9B3# -2C2A9B3#

!"#$%& $"')%& '"#(%& #")*%& '"'$%& #"(!%& #"()%& #")$%&

$!"#

#%& +,-./01&&&&&&&&&&&&&&&&&&&&&&&& +,-$./01&&&&&&&&&&&&&&&& +,-$./01&&&&&&&&&&&&&&&& +,-$./01&&&&&&&&&&&&&&&& 2,34&56789:8;& 25676:<=>:&56789:8;& 2?6:>=@>A&56789:8;& 2567"&B&?6:>=@>A&& 56789:8;&
!"#

"#")%&

!#$'%& ,#(-%&

!#()%& *#,*%& *#)+%& ,#+'%&

'()*+,-#################### '()$*+,-############# '()$*+,-############## '()$*+,-################## .(/0#12345647# .1232689:6#12345647# .;26:9<:=#12345647# .123>?;26>##12345647#

(a) average relative reconstruction error
+,-./012%345-%
!"$%

(a) average relative reconstruction error
-./01234%567/%
!"$%

9::40127%

!"''%

!"'(%

;<<62349%

+,-./012%345-%67-.8%

-./01234%567/%89/0:%

;-<-0127%
!"*%
!"()% !"(#% !"('% !"($%

=/>/2349%
!"(%
!"(!% !")+% !")+% !")'%

!"(&%

!"('%

!")%

!"#$% !"#$% !"#$% !"#$% !"#*% !"#*%

!"#&%

!"#$%

!",%

!"#'% !"#&% !"#$% !"#&% !"#$% !"#&% !"#*% !"#(%

!%
!"#$%&'()"* +(,-./01/2( !"#3$%&'( ),-.-14561( ,-./01/2( !"#3$%&'( )7-165869( ,-./01/2( !"#3$%&'( ),-.:;7-1:(( ,-./01/2( %&'%( <=>>(%&'( ?064-#@A$ %&'(

!%
!"#$%&'()"* +(,-./01/2( !"#3$%&'( ),-.-14561( ,-./01/2( !"#3$%&'( )7-165869( ,-./01/2( !"#3$%&'( ),-.:;7-1:(( ,-./01/2( %&'%( <=>>(%&'( ?064-#@A$ %&'(

(b) execution time Figure 4: Accuracy and efficiency for the real trace data set default settings

(b) execution time Figure 6: Accuracy and efficiency for synthetic trace data set default settings Impact of the QR-Elimination Optimization. In Section 3.1, we had discussed an optimization strategy whereby we eliminate one of the two expensive QR operations by forcing A to be equal to the identity matrix, I . As shown in Table 2, setting A = I causes less than half percentage point impact on the accuracy; on the other hand, this optimization helps save close to 12% in execution time.

<=>7>!&51/&?@>A;<BC&51/&D:'($%&#":;+&<BC&&
678)

!"#$%&'()&*"+&,-&.#)+/&01)23&

*+,-./)01+%2)&'()

67$) !"#$%&'()

&3#4#5) 6) 69) $69) 869) :69) ;69) <669) <$69) 456/&7)%$'5)&7)2"81#+/&9++"+& 0+)%$'5)&#"&":'($%&#":;+3&

5.4.2 Synthetic Trace Data Set
Figure 6 presents results for the synthetic trace data set under the default parameter settings. The key observation from this figure is that the accuracy and efficiency results for the synthetic trace data set are very similar to the results for real trace data set, reported in Figure 4. The similarity is especially pronounced in the execution time results in Figure 6(b). This indicates that the execution time gains of the LWI-SVD family of algorithms (and to a certain degree, the accuracies they provide ­ especially with the help of periodic and reservoir-based restarts) are inherent properties of these algorithms rather than being highly data specific. SPIRIT. Since the SPIRIT [13] algorithm approaches the problem differently (e.g. cannot directly handle deletions, cannot handle deletions/insertion of columns), we present it separately from the rest in Figure 5. For these experiments, we use a synthetic data trace that does not include any column insertions or deletions on the data matrix X . As the figure shows, SPIRIT algorithm works much faster than SVD or LWI2-SVD for the default configuration. However, this speed comes with a significant increase in the reconstruction error, relative to the optimal low-rank decomposition using SVD. In contrast, LWI2-SVD achieves an accuracy almost identical to the optimal, yet costs only half as much.

Figure 5: Accuracy and efficiency results for the synthetic trace data set for SVD, Spirit, and LWI2-SVD with periodic and ondemand refreshes Table 2: Impact of setting A = I in Section 3.1 A=I A is free Impact Rec. error 5.786 5.765 +0.36% Exec time 0.174 sec 0.197 sec -11.71% that the time overhead of reservoir maintenance and occasional ondemand full decompositions are negligible in the long run. Secondly, performing full SVD takes  75-100% more than the proposed LWI-SVD family of algorithms. Under this configuration, the naive incremental SVD takes a little more time than full SVD, as the basic algorithm reported in [5] involves a full SVD with same dimension as the original matrix and several matrix multiplications. Further-more, under this configuration, SVDS takes even longer than the full SVD. Finally, a close look at the LWI-SVD family of algorithms indicates that insertions require slightly longer time to maintain than deletions. This is expected because, as discussed in Section 2.2.1, there is no need for computing RA and RB since they are all zero.

5.5 Impacts of Data and System Parameters
In this subsection, we evaluate the impacts of the various data and systems parameters on the efficiency and effectiveness of the LWI-SVD family of algorithms. As representative, we select the

994

:9/;+,<%69/<.=%49,>-% ?4.93%6/9@.%A9=9B%
/9,>%/CD% /9,>%/C*!%
&&$% '($% '#$% &#$%

98.:*+;%<=18>-%38>-,%% ?3-82%5.8@-%A8>8B%
+C6C=1DE% +C6C=1D(%
'($% ''$% '#$% ')$%

!"#$% )$% +,-./01,-%

!"&$% *$% 2.3.01,-% +,-./01,-% 2.3.01,-%

!"#$%!"#$% *+,-./0+,%

!"&$%!"&$% 1-2-/0+,% *+,-./0+,% 1-2-/0+,%

4.3"%5//1/%

6+7.%89+,%

3-2"%4..0.%

5*6-%78*+%

Figure 7: Accuracy and efficiency for real trace data set for different rank, r
:9/;+,<%=,+093%>9?/+@%A+B.% C4.93%6/9D.%E9?9F%
2+7GH!!@H!!% 2+7G&!!@&!!%
'($% ''$% '#$% '*$%

Figure 9: Accuracy and efficiency results for the real trace data set varying the amount updates per iteration, numupd
98.:*+;%3-,-.<0*.%=*>-,%% ?3-82%5.8@-%A8B8C%
DEF!% DEGF!%
'($% '($% '#$% ')$%

!"#$% !"!)$% +,-./01,-%

!"&$% !"!'$% 2.3.01,-% +,-./01,-% 2.3.01,-%

!"#$%!"#$% *+,-./0+,%

!"&$%!"&$% 1-2-/0+,% *+,-./0+,% 1-2-/0+,%

4.3"%5//1/%

6+7.%89+,%

3-2"%4..0.%

5*6-%78*+%

Figure 8: Accuracy and efficiency results for the real trace data set varying the size, dim, of the initial (for insertions) / final (for deletions) matrix LWI2-SVD with the default parameters. We then vary, one-by-one, the various data and system parameters, and compare the results against the optimal SVD based rank-r decomposition. Since, as we have seen, the results are similar for real and synthetic data, for the most part we report the results with the real trace data. We use the synthetic trace only for experiments where we vary the strengths of the updates.

Figure 10: Accuracy and efficiency for real trace data varying reservoir size, w
98.:*+;%5<.-,<021,% =3-82%5.8>-%?8@8A%
@<-@8B!"C% @<-@8B!"D%
'($% ''$% '#$%
')$%

!"#$%!")$% *+,-./0+,%

!"&$%!"&$% 1-2-/0+,% *+,-./0+,% 1-2-/0+,%

5.5.1 Varying the Target Rank, r
Figure 7 presents efficiency and accuracy results for the real trace data set where the target rank, r is varied. The results show that, as expected (due to the low-rank nature of the LWI-SVD family of algorithms), as the target rank increases, the time gain drops and the relative error rate slightly increases. The drop in time gains is because the incremental process involves a lot of matrix multiplications where the sizes of matrices are directly related to the target rank. This confirms the observation that LWI-SVD and LWI2-SVD are most effective when the target rank is low.

3-2"%4..0.%

5*6-%78*+%

Figure 11: Accuracy and efficiency results for the real trace data set varying the change threshold, , for on-demand restarts the approximation nature of the algorithm. The impact on the time gain is due to more on-demand restarts. 5.5.4 Varying the Reservoir Size, w Figure 10 presents efficiency and accuracy results for the real trace data set where the reservoir size, w, is varied. The results confirm that a larger reservoir (even only  1.5% of the matrix) can help to trigger on-demand restarts more fairly, since larger reservoir has more accurate amortized error measuring. 5.5.5 Varying the Change Threshold,  Figure 11 confirms that a slightly tighter threshold,  = 0.1 instead of the default  = 0.2 will trigger more on-demand restarts and thus can further reduce the error rates (which are already very low), with little impact on execution time gains.

5.5.2 Varying the Dimensions, dim, of the Matrix
Figure 8 presents accuracy and efficiency results when we change the dimensions, dim, of the initial data matrix (for insertions) and the final data matrix (for deletions). Here, we see that increasing the size of matrix does not have a big impact on accuracy and efficiency.

5.5.3 Varying the Rate of Updates, numupd
Figure 9 presents efficiency and accuracy results for the real trace data set where the number, numupd , of row and column updates per each iteration is varied. The results indicate that, as expected, an increase in the number of updates per iteration impacts accuracy as well as efficiency. The slight impact on the accuracy is due to

5.5.6 Varying the Restart Period, per
Figure 12 confirms that increasing the number of restarts by reducing the restart period, per , may improve the final accuracy. However, unlike the on-demand restarts based on change detec-

995

98.:*+;%3-,<8.<%=-.*01% >3-82%5.8?-%@8<8A%
B-.CDE% B-.CE%
'($% '#$% '&$% ''$%

!"#$% !")$% *+,-./0+,%

!"&$% !")$% 1-2-/0+,% *+,-./0+,% 1-2-/0+,%

Table 3: Results for Large Dim LWI2 LWI2 SVDS Exec. Rel. Exec. Time(s) Error Time(s) 1000  100 8.2604 0.143% 15.91 1000  1000 6.8087 0.06% 10.839 1500  1500 17.483 0.03% 23.469 2000  2000 34.35 0.002% 41.491 3000  3000 96.577 0.00097% 93.622 dim

3-2"%4..0.%

5*6-%78*+%

6. CONCLUSIONS
Figure 12: Accuracy and efficiency results for the real trace data set varying the restart period, per , for periodic restarts
76,8()9#:;<6=+#>=,+)9?=*# @>8)=?1#3,6A+#B6=6C#
064D<6E$# 064D<6EFG#
$!"# &'"#

In this paper, we presented a Low-rank, Windowed, Incremental SVD (LWI-SVD) algorithm, which relies on low-rank approximations to speed up incremental SVD updates. LWI-SVD algorithm also aggregates multiple row/column insertions and deletions to further reduce on-line processing cost. We also presented a LWISVD with restarts (LWI2-SVD) algorithm which performs periodic and change detection based on-demand refreshing of the decomposition to prevent accumulation of errors. Experiment results on real and synthetic data sets have shown that the LWI-SVD family of incremental SVD algorithms are highly efficient and accurate compared to alternative schemes under different settings.

!"#

%"# ()*+,-.)*#

()*+,-.)*#

7. REFERENCES
[1] Algorithm 844. Sparse Reduced-Rank Approximations to Sparse Matrices. http://dl.acm.org/citation.cfm?id=1067972. Downloaded 2012. [2] Digg.com Data Set. http://www.infochimps.com/datasets/diggcom-data-set. Downloaded 2013. [3] SPIRIT http://www.cs.cmu.edu/afs/cs/project/spirit-1/www/. Downloaded in 2012. [4] M.W. Berry, S.A. Pulatova, and G.W. Stewart. Algorithm 844. Computing Sparse Reduced-Rank Approximations to Sparse Matrices. ACM Trans. Math. Softw. 31, 2, pp. 252-269, June 2005. [5] M. Brand. Fast low-rank modifications of the thin singular value decomposition. Linear Algebra and its Appl., 415(1):20-30, 2006. [6] S. Chandrasekaran, B.S. Manjunath, Y.F. Wang, J. Winkeler, and H. Zhang. An Eigenspace Update Algorithm for Image Analysis. Graphical Models and Image processing: GMIP, 59(5), 1997. [7] S. Deerwester, S. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman. Indexing by Latent Semantic Analysis. Journal of the American Society for Information Science, 41(6):391-407, 1990. [8] M. Gu and S. Eisenstat. Downdating the Singular Value Decomposition. SIAM J. Matrix Analysis and Applications, 1995. [9] M. Gu and S.C. Eisenstat. A Stable and Fast Algorithm for Updating the Singular Value Decomposition. Tech. Report YALEU/DCS/RR-966, Department of Computer Science, Yale University, 1993. [10] T. Kolda and B. Bader. Tensor Decompositions and Applications. SIAM Rev. 51, 3, 455-500. 2009. [11] A. Levy and M. Lindenbaum. Sequential Karhunen-Loeve Basis Extraction and its Application to Images. IEEE Transactions on Image Processing, 9:1371-1374, 2000. [12] G. O'Brien. Information Management Tools for Updating an SVD-Encoded Indexing Scheme, MS Thesis. 1994. [13] S. Papadimitriou, J. Sun, and C. Faloutsos. Streaming Pattern Discovery in Multiple Time-Series. VLDB, pp. 697-708, 2005. [14] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Incremental Singular Value Decomposition Algorithms for Highly Scalable Recommender Systems. ICIS, pp. 27­28, 2002. [15] G.W. Steward. Four Algorithms for the Efficient Computation of Truncated Pivoted QR Approximations to a Sparse Matrix. Numerische Mathematik 83, 313-323, 1999. [16] J. Sun, S. Papadimitriou, and C. Faloutsos. Online Latent Variable Detection in Sensor Networks, ICDE, 2005. [17] D.I. Witter and M.W. Berry. Downdating the Latent Semantic Indexing Model for Conceptual Information Retrieval. The Computer Journal, 1998. [18] J.S. Vitter. Random Sampling with a Reservoir. ACM Trans. Math. Softw. 11, 1, pp. 37-57, March 1985. [19] H. Zha and H.D. Simon. On Updating Problems in Latent Semantic Indexing. SIAM J. Sci. Comput., 21(2):782-791, 1999. [20] Z.A. Zhao and H. Liu. Spectral Feature Selection for Data Mining, Chapman and Hall/CRC Press, 2012.

/+01#2,,.,#

3(4+#56()#

Figure 13: Accuracy and efficiency results for the synthetic trace data set varying the update strength, upd tion (shown in Figure 11), blindly increasing the frequency of the periodic restarts may negatively impact the time gain.

5.5.7 Varying the Update Strength, upd
Finally, in Figure 13, we see the impact of the strength (in amplitude) of the incoming insertions. The figure shows that, when upd increases, the LWI2-SVD algorithm adjusts its operation by scheduling more on-demand restarts at a cost of decreasing the time gain.

5.6 Scalability of LWI Algorithms
The results shown above are conducted with small window size, however, in some cases, we need large windows to monitor and analyze a large portion of the data. In this subsection, we analyze the scalability of LWI Algorithm by choosing large base number. Since we have shown that under the small base number condition, SVD out performs SVDS in execution time, however, when the base number is large, seeking a low rank deposition using SVDS is more efficient. Also, as we know that SVDS is very efficient when the data is sparse, but performs less efficient on dense data. We showed that LWI algorithm can concur this short coming when the data is dense. Recall in section 3.2.1, we showed that K is an arrow-like matrix which is very sparse, this leads to the efficiency by using pivoted QR compared to a direct SVDS on the dense data. Therefore, in the incremental maintenance of SVD on a dense matrix, we are actually seeking a second layer reduced rank approximation of a sparse matrix K . It is the main advantage of LWI algorithm compared to SVDS when the data is dense and the base dimension is large. Table 3 shows the execution time and error overhead results under a synthetic dense data, the results confirm that with big base number especially when the base is a thin and tall matrix, LWI algorithm can have advantages in execution time with negligible error overhead .

996

BibTeX:

Combining a Gauss-Markov model and Gaussian process
for traffic prediction in Dublin city center
FranÃ§ois Schnitzler

Thomas Liebig

Technion
Fishbach Building
32000 Haifa, Israel

TU Dortmund University
Artificial Intelligence Group
Dortmund, Germany

Shie Mannor

thomas.liebig@tudortmund.de
Katharina Morik

Technion
Fishbach Building
32000 Haifa, Israel

TU Dortmund University
Artificial Intelligence Group
Dortmund, Germany

francois@ee.technion.ac.il

shie@ee.technion.ac.il
ABSTRACT
We consider a city where induction-based vehicle count sensors are installed at some, but not all street junctions. Each
sensor regularly outputs a count and a saturation value. We
first use a discrete time Gauss-Markov model based on historical data to predict the evolution of these saturation values, and then a Gaussian Process derived from the street
graph to extend these predictions to all junctions. We construct this model based on real data collected in Dublin city.

Categories and Subject Descriptors
G.3 [Probability and Statistics]: Markov processes, multivariate statistics, stochastic processes, time series analysis;
I.2.6 [Artificial Intelligence]: Learningâparameter learning; J.7 [Computer in Other Systems]: Real time

Keywords
traffic prediction, Gaussian Process, Gauss-Markov, autoregressive, smart cities, time series, spatio-temporal

1.

INTRODUCTION

In the Greater Dublin Area, 750 (4%) junctions are covered by one or several SCATS (Sydney Co-ordinated Adaptive Traffic System) vehicle count sensors. Our goal is to
provide estimates of the saturation at each junction, for the
current and future times, whereas our previous work [1] only
did so for each junction at the current time.
High traffic saturation (cars/km) co-occurs with low traffic flux (cars/hour) and is an indicator for congestions [3].

(c) 2014, Copyright is with the authors. Published in the Workshop Proceedings of the EDBT/ICDT 2014 Joint Conference (March 28, 2014,
Athens, Greece) on CEUR-WS.org (ISSN 1613-0073). Distribution of this
paper is permitted under the terms of the Creative Commons license CCby-nc-nd 4.0.

katharina.morik@tudortmund.de
observed measurement, at current time
predicted measurement for a future time step
predicted value for an unobserved junction
GaussMarkov
Section 2
current time

Gaussian
Process
Section 3
future time steps

Figure 1: Future measurements are estimated by
a Gauss-Markov process (Section 2). Estimates for
junctions without sensors, are provided by a Gaussian Process (Section 3).

Our work can be used for online signaling and trip planning.
The urban street network is a graph (V, E), where the
vertices V are the junctions and the edges E the street segments. Let u be the set of unobserved junctions, with no
SCATS sensor, and âu = V \u the junctions with sensors.
The saturation of a junction vi at a time t is a continuous
random variable yi,t . Furthermore, yu,t â¡ {yi,t }i:vi âu .
We combine two components to obtain an estimate of the
saturation of all junctions at future time steps, yV,t+ât , conditioned on the current observations, yâu,t (ât â N0 ).
The first one, P (yâu,t+ât |yâu,t ), models historical measurements. It can estimate future measurements yÌâu,t+ât ,
based on the current observations yÌâu,t :
yÌâu,t+ât = E(yâu,t+ât |yÌâu,t ) .

(1)

The second is a Gaussian Process (GP) based on the street
network and defining a multivariate Gaussian distribution
P (yV,t ) over the saturations at all junctions. Conditioning
this distribution on yâu provides P (yu,t+ât |yâu,t+ât ) and
allows to estimate saturations at junctions without sensors:
P (yu,t+ât |yâu,t ) â P (yu,t+ât |yÌâu,t+ât ) .
Figure 1 illustrates the resulting prediction procedure.

(2)

2.

GAUSS-MARKOV

x1 â¼ N (yÌ0 , Î£0 ), a multivariate Gaussian distribution of
mean yÌ0 and covariance matrix Î£0 . The Kalman filter can
compute P (yt+ât |yt ) = N (yÌt+ât , Î£Ìt+ât ) recursively.
Sensor measurements were collected from 2013-01-01 to
2013-05-141 by 512 (470 non trivial ones) vehicle count sensors located in central Dublin. We average all measurements
received on non-overlapping 4 minutes intervals, because of
missing values, and model the resulting averages from 5am
to 12am. The parameters At , wÌt , Î£wt change for every time
step but are identical for every day. So are yÌ0 and Î£0 .
Following the methodology of [6], each matrix At is learned
using (averaged) measurements for t0 â {t â Î´t , . . . , t + Î´t },
weighted by a Gaussian kernel: exp(â(t â t0 )2 /Î´t ). We arbitrarily use Î´t = 3. For each matrix At , each row ri,t is estimated using an elastic net [7] and ten-fold cross-validation.
Î£0 and each Î£wt are diagonal covariance matrices estimated
by maximum likelihood. Alternatively, penalized estimation
algorithms such as the graphical lasso [2] could be used.

A similar approach was proposed to provide dynamic cost
predictions for a trip planner in the same workshop [4]. Instead of a linear dynamical system (LDS), a spatio-temporal
Markov random field (STMRF) is used. It models discretized
saturation values only, and inference is approximated by belief propagation whereas it is computationally tractable and
performed exactly in LDS. Our model also has a finer temporal resolution. Therefore, it can be used for signaling or
online adaptation of the route in addition to offline trip planning. Comparing these two models in terms of precision and
speed would be interesting.
The Gauss Markov model assumes the dynamics are linear, first-order Markov and perturbed by Gaussian noise.
More refined models could be considered and might lead to
better estimations.In particular, we could assume the measurements are noisy observations of a hidden process.
Other information could also be leveraged. For example,
the street network could be used to derive a prior on the coefficient of the transition matrix, influencing the model only.
Irregular, pointwise traffic estimation (for example based on
mobile phones or GPS) could be integrated into the Gaussian Process to produce finer saturation estimates. Finally,
different dynamics could be estimated and used in the presence or the absence of rain, modifying both the model and
the estimation process.

3.

5.

A linear dynamical system models the evolution of a set
of state variables y â Rp , where we omit the subscript âu:
yt+1 = At yt + wt
wt â¼ N (wÌt , Î£wt ) .

(3)
(4)

GAUSSIAN PROCESS

P (yu,t+ât |yâu,t+ât ) is derived from a GP regression framework modeling traffic saturation values of all junctions at a
given time, similar to [5]. Multiple sensors at a junction are
averaged. For each vertex vi , we introduce a latent variable
fi , the true traffic saturation at vi :
yi = fi + i

(5)
2

i â¼ N (0, Ï ) .

(6)

We assume that the random vector of all latent variables
follows a GP: any finite set f = {fi }i=1,...,M has a multivariate Gaussian distribution. Therefore, the vector of observed
traffic saturations (yâu ) and unobserved traffic saturations
(du ) follows a Gaussian distribution


 

yâu
Kâu,âu + Ï 2 I Kâu,u
â¼ N 0,
,
(7)
du
Ku,âu
Ku,u
where I is an identity matrix, K the so-called kernel and
Ku,âu , Kâu,âu , Ku,u , and Kâu,u the corresponding entries
of K. Conditioning on y produces P (yu,t+ât |yâu,t+ât ).
We use the common regularized Laplacian kernel function

â1
K = Î²(L + I/Î±2 )
,
(8)
where Î± and Î² are hyperparameters. L denotes the combinatorial Laplacian, L = D â G. G denotes the adjacency
matrixP
of the graph G and D a diagonal matrix with entries
di,i = j Gi,j . Variables adjacent in G are highly correlated.

4.

DISCUSSION

We have described a combination of two models able to
respectively predict future traffic saturations at junctions
with sensors and to extend these predictions to junctions
without sensors, in a city. To the best of our knowledge, no
similar model has been proposed before.
1

http://dublinked.ie/datastore/datasets/dataset-305.php

ACKNOWLEDGMENTS

This work was supported by the European FP7 project
INSIGHT under grant 318225.

6.

REFERENCES

[1] A. Artikis, M. Weidlich, F. Schnitzler, I. Boutsis,
T. Liebig, N. Piatkowski, C. Bockermann, K. Morik,
V. Kalogeraki, J. Marecek, A. Gal, S. Mannor,
D. Gunopulos, and D. Kinane. Heterogeneous stream
processing and crowdsourcing for urban traffic
management. In Proceedings of the 17th International
Conference on Extending Database Technology, page (to
appear), 2014.
[2] J. Friedman, T. Hastie, and R. Tibshirani. Sparse
inverse covariance estimation with the graphical lasso.
Biostatistics, 9(3):432â441, 2008.
[3] W. Leutzbach. Introduction to the Theory of Traffic
Flow. Springer, 1988.
[4] T. Liebig, N. Piatkowski, C. Bokermann, and K. Morik.
Predictive trip planning â smart routing in smart cities.
In Workshop Proceedings of the EDBT/ICDT 2014
Joint Conference, 2014.
[5] T. Liebig, Z. Xu, M. May, and S. Wrobel. Pedestrian
quantity estimation with trajectory patterns. In
Machine Learning and Knowledge Discovery in
Databases, volume 7524 of Lecture Notes in Computer
Science, pages 629â643. Springer Berlin Heidelberg,
2012.
[6] L. Song, M. Kolar, and E. P. Xing. Time-varying
dynamic bayesian networks. Advances in Neural
Information Processing Systems, 22:1732â1740, 2009.
[7] H. Zou and T. Hastie. Regularization and variable
selection via the elastic net. Journal of the Royal
Statistical Society: Series B (Statistical Methodology),
67(2):301â320, 2005.

KSGM: Keynode-driven Scalable Graph Matching
Xilun Chen, K. SelÃ§uk Candan

Maria Luisa Sapino

Paulo Shakarian

Arizona State University
Tempe, AZ, USA
{xilun.chen, candan}@asu.edu

University of Torino
Torino, Italy
marialuisa.sapino@unito.it

Arizona State University
Tempe, AZ, USA
shak@asu.edu

ABSTRACT
Understanding how a given pair of graphs align with each other
(also known as the graph matching problem) is a critical task in
many search, classification, and analysis applications. Unfortunately, the problem of maximum common subgraph isomorphism
between two graphs is a well known NP-hard problem, rendering
it impractical to search for exact graph alignments. While there
are several heuristics, most of these analyze and encode global and
local structural information for every node of the graph and then
rank pairs of nodes across the two graphs based on their structural
similarities. Moreover, many algorithms involve a post-processing
(or refinement) step which aims to improve the initial matching
accuracy. In this paper 1 we note that the expensive refinement
phase of graph matching algorithms is not practical in any application where scalability is critical. It is also impractical to seek
structural similarity between all pairs of nodes. We argue that a
more practical and scalable solution is to seek structural keynodes
of the input graphs that can be used to limit the amount of time
needed to search for alignments. Naturally, these keynodes need to
be selected carefully to prevent any degradations in accuracy during the alignment process. Given this motivation, in this paper,
we first present a structural keynode extraction (SKE) algorithm and
then use structural keynodes obtained during off-line processing
for keynode-driven scalable graph matching (KSGM). Experiments
show that the proposed keynode-driven scalable graph matching algorithms produce alignments that are as accurate as (or better than)
the state-of-the-art algorithms, with significantly faster online executions.

1.

!"#$%&'&

!"#$%&(&

Figure 1: Graph matching/alignment problem seeks a maximum common subgraph isomorphism between two input graphs
they represent the pairwise relationships between the nodes of the
graph. Edges can be directed or undirected, meaning that the relationship can be non-symmetric or symmetric, respectively. Nodes
and edges of the graph can also be labeled or non-labeled. The
label of an edge, for example, may denote the name of the relationship between the corresponding pair of nodes or may represent
other meta-data, such as the certainty of the relationship or the cost
of leveraging that relationship within an application.
Due to the success of the graph model as a powerful and flexible data representation, graph analysis and search tasks are also
increasingly critical in many application domains. In particular,
understanding how a given set of graphs align with each other (also
known as the graph matching/alignment problem, Figure 1) forms
the core task in many search, classification, and analysis applications. Unfortunately, the problem of maximum common subgraph
isomorphism between two graphs is a well known NP-hard problem [24], making it impractical to search for exact or maximal
graph alignments. As a result, while there are some attempts to
improve the performance of exact maximum common subgraph
matching solutions [23], most of the recent efforts in the area have
focused on seeking approximate/inexact graph alignments [3, 18,
22, 19, 29].
While these algorithms differ in their specific techniques, most
of them rely on a four phase process:

INTRODUCTION

Graphs have been used to represent a large variety of complex
data, from multimedia objects, social networks, hypertext/Web,
knowledge graphs (RDF), mobility graphs,to protein interactions.
Let D be a set of entities of interest, a graph, G(V, E), defined over
V = D describes the relationships between pairs of objects in D.
The elements in the set V are referred to as the nodes or vertices of
the graph. The elements of the set E are referred to as the edges and
1
This work is supported by NSF Grants #1339835 and #1318788.
This work is also supported in part by NSF grant #0856090.

1. First, the matching algorithm analyzes and encodes the
global structural information (for example a spectral signature [23]) corresponding to the nodes of the graph.
2. Secondly, the algorithm analyzes and encodes the local structural information (such as neighborhood degree distribution [29]) for the nodes of the graph.
3. Once these global and local signatures are encoded, the
matching algorithm compares the signatures of pairs of
nodes across the given graphs to rank these pairs of nodes
(for example using a stable matching algorithm, like the Hungarian algorithm [16]) based on their overall structural similarities.

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are not
made or distributed for profit or commercial advantage and that copies bear
this notice and the full citation on the first page. Copyrights for components
of this work owned by others than ACM must be honored. Abstracting with
credit is permitted. To copy otherwise, or republish, to post on servers or to
redistribute to lists, requires prior specific permission and/or a fee. Request
permissions from Permissions@acm.org.
CIKMâ15, October 19â23, 2015, Melbourne, Australia.
c 2015 ACM. ISBN 978-1-4503-3794-6/15/10 ...$15.00.

DOI: http://dx.doi.org/10.1145/2806416.2806577.

1101

)&

)&
)&

)&

)&
)&

*&

)&
)&

!"#$%&'&

)&

)&
)&

)&

!"#$%&(&

Figure 2: Keynode selection problem for scalable graph matching: the nodes marked with "*" are keynodes of the input
graphs that can be used to reduce the amount of time needed to
search for alignments
4. Finally, a post-processing, or refinement, step (involving, for
example, a vertex cover operation) is used to improve the
accuracy of the initial matching [29].

2.

BACKGROUND AND RELATED WORK

In this section, we review key concepts related to the graph
matching problem and discuss the existing algorithms.
Graph Isomorphism: Given two graphs G and H, G is isomorphic to H if there exists a bijective mapping from the nodes of G
to the nodes H that preserves the edge structure [13]: for any two
vertices that are adjacent on G, the vertices they are mapped to are
also adjacent on H, and vice versa.
Subgraph Isomorphism: Subgraph isomorphism seeks a bijective
function, f , such that there is a subgraph G0 of G and a subgraph
H 0 of H, such that G0 is isomorphic to H 0 , with respect to f .
Maximum Common Subgraph Isomorphism: Maximum common subgraph isomorphism seeks the largest subgraph of G isomorphic to a subgraph of H [24]. Intuitively, the larger the maximum common subgraph of two graphs is, the more similar the
graphs are to each other.
One of the first exact graph matching algorithms was proposed
by Ullman [24]. An alternative way to search for a matching between two graphs is to rely on graph edit distance algorithms:
given two graphs the corresponding graph edit distance is the least
cost sequence of edit operations that transforms G1 into G2 . Commonly used graph edit operations include substitution, deletion, and
insertion of graph nodes and edges. Unfortunately, the graph edit
distance problem is also known to be NP-complete [24]. In fact,
even approximating graph-edit distance is very costly; the edit distance problem is known to be APX-hard [8]. [8] shows that graph
isomorphism, subgraph isomorphism, and maximum common subgraph problem are special instances of the graph edit distance computation problem. Many subgraph isomorphism search algorithms
have been developed, such as [15, 29, 14].
Approximate Graph Matching: In order to be applicable to large
graphs, many heuristic and approximate graph matching algorithms
have been proposed.
While, as we discussed above, graph matching through edit distance computation is an expensive task, there are various heuristics
that have been developed to perform this operation more efficiently.
GraphGrep [14] is one such technique, relying on a path-based
representation of graphs. GraphGrep takes an undirected, nodelabeled graph and for each node in the graph, it finds all paths that
start at this node and have length up to a given, small upper bound,
lp . Given a path in the graph, the corresponding id-path is the list
of the ids of the nodes on the path. The corresponding label-path is
the list of the labels of the nodes on the path. The fingerprint of the
graph, then, is a hash table, where each row contains the hash of the
label-path and the corresponding number of id-paths in the graph.
Irrelevant graphs are filtered out by comparing the numbers of idpaths for each matching hash key and by discarding those graphs
which have at least one value in its fingerprint less than the corresponding value in the fingerprint of the query. Matching sub-graphs
are found by focusing on the parts of the graph which correspond
to the label-paths in the query. After, the relevant id-path sets are

Unfortunately, many of these steps result in significant scalability
challenges in terms of the matching time needed to compare the
pairs of nodes:
â¢ In particular, the expensive refinement phase of graph matching algorithms is not practical in applications where scalability of the graph matching operation is critical.
â¢ Moreover, especially in very large graphs, it is also impractical to seek pairwise structural similarities for all node pairs
during the graph matching process.

1.1

Organization of this Paper

The paper is organized as follows: in the next section, we first
introduce basic concepts and review existing graph matching algorithms. In Section 3, we provide overviews of the general graph
matching process as well as the proposed keynode-driven scalable
graph matching (KSGM) algorithm. Then, in Section 4, we present
our structural keynode extraction (SKE) algorithm. In Sections 5
and 6, we discuss how to use these structural keynodes for obtaining graph alignments. We discuss the complexity of the proposed algorithms and parallelization opportunities in Section 7. We
present experimental evaluations with various real and synthetic
data sets in Section 8. These confirm that the proposed approximate graph matching algorithm is highly effective and efficient.
Finally, we conclude the paper in Section 9.

)&

)&
)&
)&

1.2

)&

Contributions of this Paper

Based on these observations, in this paper, we argue that a more
practical and scalable solution would be to seek structural keynodes of the input graphs that can be used to reduce the amount of
time needed to search for alignments (Figure 2). Of course, these
keynodes must be selected carefully to prevent any degradations in
accuracy during the alignment process, especially because, as mentioned above, refinement post-processes are detrimental to scalability of matching algorithms.
Given this motivation, in this paper, we first present a highly efficient and effective structural keynode extraction (SKE) algorithm.
The SKE algorithm, which is executed off-line, relies on a 3-step
process:
1. In the first step, a PageRank algorithm [7] is ran to associate
a structural score to each node in the graph.
2. In the second step, a scale-space (based on a difference-ofGaussians (DoG) function defined over different scales of the
graph) is constructed.
3. In the third step, keynode candidates are extracted by analyzing the resulting scale-space for extrema of the DoG function and a subset of these candidates are selected as structural
keynodes.
We then propose a graph matching algorithm that uses these structural keynodes (obtained during off-line processing) for keynodedriven scalable graph matching (KSGM). In particular, KSGM extracts only local signatures and relies on the structural keynodes for
fast node-to-node similarity searching. In addition, we also show
that this keynode-driven approach not only reduces the number of
comparisons that need to be performed online, but it also enables
effective matching, even without having to rely on an expensive assignment algorithm, like the Hungarian algorithm (with O(|V |3 )
complexity). Experiment results show that the proposed structural
keynode extraction and keynode-driven scalable graph matching
algorithms produce alignments that are as accurate as (or better
than) the state-of-the-art algorithms, while requiring significantly
less online execution time without refinement.

1102

Algorithm 1 Overview of keynodes based graph matching
Input:
A set G = {G1 , G2 , ...Gg } of graphs
A query graph Gq â G.
Output:
Rank Gi â G in terms of matching quality
Offline process:
1: for all Gi â G (including Gq ) do
2:
Perform structural keynode extraction (SKE) for Gi
3:
Extract local-signatures for all nodes in Gi
4:
(Optional) Extract global-signatures for all nodes in Gi
5: end for
Online process:
6: for all Gi â G do
7:
Compute local similarities for keynode pairs from Gi and
Gq .
8:
(Optional) Compute global similarities for keynode pairs
from Gi and Gq and combine these with local similarities.
9:
Select anchors to obtain a base matching
10:
Expand the base matching to obtain Mq,i
11:
Compute matching quality, quality(Mq,i )
12: end for
13: Rank Gi â G in terms of quality(Mq,i )

selected and overlapping id-paths are found and concatenated to
build matching sub-graphs.
A common method to obtain an approximate graph matching
is to use the eigenvectors derived from the adjacency matrix of
the graph [23]: intuitively, two similar graphs should have similar eigenvectors; moreover, if we construct a |V | Ã |V | matrix (for
example the Laplacian of the graph or a matrix encoding node distances) and decompose it into three matrices of |V | Ã c, c Ã c,
and c Ã |V | elements using an eigen-decomposition technique like
SVD, the c-length vector corresponding the node v â V can be
used as a global-signature corresponding to node v. Once node-tonode similarities are computed, an assignment is usually found using an assignment algorithm, such as the Hungarian algorithm [16],
which uses a primal-dual strategy to solve this problem in O(|V |3 )
time. This simple observation, led to several works leveraging different global-signatures for identifying node matches across different graphs [3, 18, 22, 19, 29]. [28] formulates the labeled weighted
graph matching problem in the form of a convex-concave program,
which searches for appropriate permutation matrices by solving a
least-square problem. In addition, feature selection techniques are
used for more accurate calculation [11, 12, 20]. In order to improve
matching accuracy, [29] proposes to enrich the global-signatures
associated to the graph nodes with local-signatures, encoding the
properties of the immediate neighborhood of each node.

3.

OVERVIEW OF KEYNODE-DRIVEN
GRAPH MATCHING

combines (by multiplying) the global and local similarities
of each pair of nodes into a single value, thereby quantifying
the overall similarity of the pair.
4. Once the overall similarities for |Vq | Ã |Vi | pairs of nodes
are computed, [29] drops node pairs with small degrees and,
then, expands the remaining set of anchor pairs by adding,
in an iterative manner, immediate good nearby pairs to this
anchor set.
5. When no more pairs can be added to the anchor set, [29] uses
the Hungarian algorithm to identify an initial node matching
in O(max{|Vq |, |Vi |}3 ) time.
6. Finally, as a post-processing step, [29] applies a vertex cover
based refinement, which explores different subsets of the
nodes and searches for better alignments than the one initially identified. In particular, the algorithm seeks small vertex covers, which are likely to give the mismatched nodes additional chances to be refined. Note that since the minimum
vertex cover problem is known to be NP-hard, the algorithm
searches for minimal vertex covers in O(mÃn3 ) time, where
m = min{|Eq |, |Ei |} and n = max{|Vq |, |Vi |}.

Given a set G = {G1 , G2 , ..., Gg } of graphs and a query graph
Gq â G, in this paper, we seek the maximum graph matching between Gq and all Gi â G (i 6= q). Note that the exact solution for
this problem is NP-hard [24]. Since we treat scalability as a key
constraint, we consider inexact solutions and rely on the matching quality measure proposed in [29] to evaluate the accuracies of
the resulting alignments: Let Gq (Vq , Eq ) be a query graph and let
Gi (Vi , Ei ) be a graph in G. Let Mq,i (Vq,i , Eq,i ) be a subgraph of
both Gq and Gi , returned by an inexact subgraph search algorithm.
[29] defines the matching quality function as follows:
quality(Mq,i ) =

|Eq,i |
,
min(|Eq |, |Ei |)

Intuitively, the quality function describes how similar the given
query graph Gq and known graph Gi are by using the ratio of
matched edges and the maximum number of edges that can be possibly matched, which is equal to the minimum number of edges between two graphs. In other words, the larger the number of edges
in the graph Mq,i , the better is the quality of the matching (or the
more similar the two graphs are).

3.1

This process includes a number of very expensive steps: The first
two steps, involving global and local analysis are expensive, but can
be performed off-line and indexed for later reuse assuming that the
graphs are available ahead of time. The last four steps, however,
need to be performed on-line, yet they consist of operations that
are quadratic or higher. In particular, the last refinement step, with
O(m Ã n3 ) time cost is impractical for most large data graphs.
In this paper, we note that Step 3 can be significantly sped up
if the similarity computations are limited to only a small subset of
the vertices in Vq and Vi (which we refer to as keynodes of Vq and
Vi ). However, the use of keynodes for node similarity computation
is not sufficient to reduce the overall complexity as, once the keynodes are identified and the keynode pairs set is expanded, solving
the assignment problem needed to return the matching would still
take O(max{|Vq |, |Vi |}3 ) time, if we were to apply the Hungarian
algorithm on the extracted keynodes. Therefore, we also need to reduce the time complexity of this step significantly. It is especially
important that the initial keynode based similarity computation is
accurate as we cannot afford a cubic algorithm like Hungarian algorithm to return a high-quality matching.

Challenges

Given a query graph Gq , our goal is to rank the graphs in G according to their matching similarities against Gq (and eventually
return the top few matches to the user). [29] solves this problem by
relying on a 6-step process, common to many graph search algorithms:
1. [29], first, analyzes the global structure of each graph
through eigen-decomposition of the graph Laplacian matrix
and encodes this in the form of a c-length vector associated
to each node in the graph.
2. Secondly, [29] encodes the structural information local to
each node, vj , in the form of an sj -length degree distribution vector, where sj is the number of nodes in the kneighborhood of the node.
3. Given the global and local signatures of all nodes in Gq and
Gi , [29] then computes the global and local similarities for
each pair of nodes from the two graphs, in O(|Vq | Ã |Vi | Ã c)
and O(|Vq | Ã |Vi | Ã maxvj (sj )) time, respectively. It then

1103

3.2

Outline of KSGM
Algorithm 1 illustrates an overview of the keynode-driven scalable graph matching (KSGM) process. In the rest of the paper, we
study each step in detail. First, in the next two sections, we focus on
the offline steps of KSGM, which involve identifying keynodes and
extracting local-signatures. The online steps of the KSGM algorithm
are discussed in Section 6.
4.

4.2

We note that the above alternative has a number of disadvantages:
â¢ First of all, many of the structural significance measures,
such as PageRank, are not entirely robust against modifications in the graph. The PageRank score of a node, for example, can jump significantly, if a new edge connects the
sub-graph in which the node is contained to a high PageRank node in the graph.
â¢ Secondly, common structural significance measures, like
PageRank, capture the significance of a node in the whole
graph and favor nodes that are overall central. However, this
may be disadvantageous as there is a possibility that smaller
scale, but distinct (and, therefore, useful for matching) structural features of the graph may be missed.
We therefore argue that we need a better alternative, which is both
robust and multi-scale. We build the proposed SKE algorithm based
on three key insights:
â¢ Robustness: Even when the PageRank scores of the nodes
themselves vary due to graph transformations, such as edge
insertions and removals, a given nodeâs PageRank score relative to the scores of the nodes in its neighborhood is likely
to be stable.
â¢ Structural distinctiveness: A node is structurally distinctive
in its neighborhood, if "the relationship between its PageRank score to the PageRank scores of its neighbors" is different from the "relationships between the nodeâs neighborsâ PageRank scores and the PageRank scores of their own
neighbors".
â¢ Multi-scale: Since we do not know the scale of the structurally distinctive features of the graph, we need to search
for features of potentially different sizes.
It is important to note that similar requirements also exist in
other application domains. For example, algorithms for extracting such robust, local features have been developed for 2D images
(SIFT [21]), uni-variate time series [9], and multi-variate time series [25]. In this paper, we argue that a similar process can be used
to identify keynodes (corresponding to robust, multi-scale structural features) of a graph, if the nodes are annotated with PageRank
scores ahead of the time. Let G(V, E, p) be a PageRank-labeled
graph, where p() is a mapping from the nodes to the corresponding PageRank scores. What makes the problem of extracting local features from PageRank-labeled graphs challenging is that the
concepts of neighborhood, gradient, and smoothing are not welldefined for graphs.
Therefore, before we describe the keynode extraction process,
we describe how to smooth a PageRank-labeled graph. Intuitively,
smoothing the graph with respect to the scores associated to the
graph nodes creates versions of the given graph at different resolutions and, thus, helps identify features with different amounts of
details.

STRUCTURAL KEYNODE EXTRACTION

In this section, we propose an off-line structural keynode extraction (SKE) algorithm which identifies Î% (where Î is a user
provided parameter) of the nodes in V as the keynode set, K, of
a given graph, G(V, E) to support scalable graph matching. The
proposed SKE algorithm has a number of advantages: (a) First of
all, the identified keynodes are robust against noise, such as random edge insertion/removal; and (b) the identified nodes represent
structural features of the graph of different sizes and complexities
(i.e., correspond to neighborhoods of different sizes).

4.1

Proposed Solution - Robust Keynode Extraction through Scale Space Analysis

Naive Solution - Selecting Structural
Keynodes based on Node Significance

As described above, the keynodes of the graph need to represent
the structural properties of the graph well (i.e., extracted keynodes
need to be structurally significant in the graph) to support effective
matching. Therefore, the first alternative is to rely on traditional
node significance measures.
Measures like betweenness [26] and the centrality/cohesion [5],
help quantify how significant any node is on a given graph based on
the underlying graph topology. The betweenness measure [26], for
example, quantifies the number of shortest paths that pass through
a given node. The centrality/cohesion [5] measures quantify how
close to a clique the given node and its neighbors are. Other authority, prestige, and prominence measures [4, 7, 5] quantify the
significance of the node in the graph through eigen-analysis or random walks, which help measure how reachable a node is in the
graph. PageRank [7] is one of the most widely-used random-walk
based methods for measuring node significance and has been used
in a variety of application domains, including web search, biology,
and social networks. The basic thesis of PageRank is that a node
is important if it is pointed to by other important nodes â it takes
into account the connectivity of nodes in the graph by defining the
score of the node vi â V as the amount of time spent on vi in a sufficiently long random walk on the graph. Given a graph G(V, E),
the PageRank scores are represented as ~r, where
~r = Î±TG~r + (1 â Î±)~t
where TG is a transition matrix corresponding to the graph G, ~t is
a teleportation vector (such that ~t[i] = |V1 | ), and Î± if the residual
probability (or equivalently, (1 â Î±) is the so-called teleportation
probability). Unless the graph is weighted, the transition matrix,
TG , is constructed such that for a node v with k (outgoing) neighbors, the transition probability from v to each of its (outgoing)
neighbors will be 1/k. If the graph is weighted, then the transition probabilities are adjusted in a way to account for the relative
weights of the edges.
Therefore, as the first alternative, we consider a PageRank based
keynode selection scheme: in this scheme, given a graph G(V, E),
we would (a) first identify the PageRank scores, p(vi ), of all vi â
V , then (b) we would rank the nodes in non-increasing order of
PageRank scores, and finally, (c) we would return the top Î% of
the nodes in V as the keynode set, K.

4.2.1

Gaussian Smoothing of a PageRank-Labeled
Graph

1D or 2D data are commonly smoothed by applying a convolution operation with a Gaussian window. For example, if Y =
ht0 , t1 , . . . , tl i is a time series data and Ï is a smoothing parameter,
its smoothed version, YÌ (t, Ï), is obtained through G(t, Ï) â Y (t)
where â is the convolution operation in t and G(t, Ï) is the Gaussian function
ât2
1
G(t, Ï) = â
e 2Ï2 .
2ÏÏ
Essentially, the Gaussian smoothing process takes a weighted
average of values of the points in the vicinity of a given point, t.

1104

The closer a point to t, the higher is the weight. Therefore, in order
to implement a similar Gaussian smoothing of the given graph, we
first need to define a distance function to measure how close different nodes are to each other. Common applicable definitions of
node distance include the hop distance (determined by the shortest
edge distance between the nodes on the given graph) or hitting distance [10]. In this paper, we use hop distance to measure how far
nodes are to each other:

!"#$%"&'()"*+$,(,-../0$1(23/0(4#$%"&'4!"((

6!"77(6.8(9$)$*(!"
!"#$%"&'()"*+$,(,-../0$1(23/0(4!75(

â¢ Nj , for j â¥ 0, is an n Ã n 0, 1-valued matrix, where for a
given node vi in the graph, G, the ith row in the matrix, Nj
is 1 only for nodes that have node distance exactly j from the
node vi , and

!#

!"""""#

D EFINITION 1 (N ODE D ISTANCE M ATRIX ). Let us be given
a graph G(V, E) with n nodes. The ordering among the nodes is
described through a set of node distance matrices, Nj , where

6!75(77(6.8(9$)$*(;!75<(

6:(77(6.8(9$)$*(:(

!"#$%"&'()"*+$,(,-../0$1(23/0(45(

65(77(6.8(9$)$*(5(
!"#$%"&'()"*+$,(,-../0$1(23/0(4-3&(

â¢ Nj , for j â¤ 0, is an n Ã n 0, 1-valued matrix, where for a
given node vi , the ith column in the matrix, N(j, G) = 1
only for nodes that have distance exactly j on the inverted
graph, where all edges are inverted.


Figure 3: Computing the Difference-of-Gaussian (DoG) series
of the PageRank values of a graph

Intuitively, the cell Nj [v1 , v2 ] = 1 if the node v2 is exactly j hops
from v1 . When j is positive the hop-distance is measured following
outgoing edges, whereas when j is negative, incoming edges are
followed. Given this, we construct multiple scales of the given
graph G by using a Gaussian graph smoothing function defined as
follows.

Intuitively, the smoothing function S applies Gaussian smoothing
on the X values (associated with the nodes, vi â V ) based on the
hop-distances between nodes and returns a vector
SG,Ï (X) = hxÌ(v1 ), xÌ(v2 ), . . . , xÌ(vn )i

D EFINITION 2 (G AUSSIAN G RAPH S MOOTHING F UNCTION ).
Let us be given a labeled graph G(V, E, x) and let
â¢ Ï be a smoothing parameter.
â¢ X = hx(v1 ), x(v2 ), ..., x(vn )i be a vector encoding the labels associated with the nodes, vi â V .
Then, if G is a directed graph, the non-normalized Gaussian graph

smoothing function, SG,Ï
() is defined as
n
X

G(j, Ï)Nj X
SG,Ï (X) = G(0, Ï)IX +

encoding the smoothed X values associated with the graph nodes.
Note that, since at a given hop distance there may be more than
one node, all the nodes at the same distance have the same degree
of contribution and the degree of contribution gets progressively
smaller as we get further away from the node for which the smoothing is performed.
Therefore, given a PageRank-labeled graph, G(V, E, p), and a
corresponding PageRank vector, P = hp(v1 ), p(v2 ), ..., p(vn )i,
encoding PageRank scores associated with the nodes, vi â V , the
vector
SG,Ï (P ) = hpÌ(v1 ), pÌ(v2 ), . . . , pÌ(vn )i,

j=1

+

n
X

encodes the Ï-smoothing of the PageRank-annotated graph,
G(V, E, p). We also say that SG,Ï (P ) encodes the PageRank
scores of G at scale Ï.
We next describe how to construct a scale-space for the given
graph through an iterative smoothing process leveraging the PageRank vector and the structure of the graph.

G(j, Ï)Nj X,

j=1

where G(0, Ï) is a Gaussian function with zero mean and Ï standard deviation. If, on the other hand, G is an undirected graph, then
the non-normalized Gaussian graph smoothing function is

SG,Ï
(X)

= G(0, Ï)IX +

n
X

4.2.2

Graph Scale-Space Construction

The first step in identifying robust graph features is to generate a
scale-space representing versions of the given graph with different
amounts of details. In particular, building on the observation that
features are often located where the differences between neighboring regions (also in different scales) are large, we seek structural
features of the given graph at the extrema of the scale space defined
by the difference-of-the-Gaussian (DoG) series. More specifically,
given
â¢ a PageRank-labeled graph, G(V, E, p),
â¢ the corresponding vector, P = hp(v1 ), p(v2 ), ..., p(vn )i encoding the scores associated with the nodes, vi â V ,
â¢ a minimum smoothing scale, Ïmin ,
â¢ a maximum smoothing scale, Ïmax ,
â¢ the number, l, of levels of the scale space,
then, we compute a difference-of-Gaussians (DoG) series,
D(G, P, Ïmin , Ïmax , l) = {D1 , D2 , ..., Dl }, where each Di encodes the differences of two nearby scales separated by a multiplicative factor k:

2G(j, Ï)Nj X.

j=1

Intuitively, S  applies Gaussian-based weighted averaging to the
entries of vector X based on the hop-distances2 . However, unlike
the basic Gaussian smoothing, during (non-normalized) relationship smoothing, there may be more than one node at the same distance and all such nodes have the same degree of contribution. As
a consequence, the sum of all contributions may exceed 1.0. Therefore, the normalized Gaussian graph smoothing function, S(G, Ï),
discounts weights based on the number of nodes at a given distance:

 



SG,Ï (X) = SG,Ï
X Ã· SG,Ï
1(n) ,
where 1(n) is an n-vector such that all values are 1 and âÃ·â is a
pairwise division operation.

2
In practice, since the Gaussian function drops fast as we move
away from the mean, we need to consider only a small window, w,
of hops

Di = SG,ki Ïmin (P ) â SG,kiâ1 Ïmin (P ),

1105

56'
-2'

-3'

â¢ DoG-neighbors numbered #2 and #7 correspond to the
DoG values of the same node at the previous and next levels of the scale space. Therefore, we have

!"#$%&'(),+'

-4'

Nhvi ,Ïj i [2] = Diâ1 [j] and Nhvi ,Ïj i [7] = Di+1 [j].

!"#$%&'()'

-0'

7)869'

-+'

-.'

-1'
!"#$%&'()*+'

â¢ In contrast, DoG-neighbors #3, #5, and #8 correspond to
the (average) DoG values of the forward neighbors of the
node vj , at the previous, current, and next levels of the scale
space, respectively. Therefore, we have

-/'

Nhvi ,Ïj i [3] = (FDiâ1 ) [j],

Figure 4: Extrema detection

Nhvi ,Ïj i [5] = (FDi ) [j], Nhvi ,Ïj i [8] = (FDi+1 ) [j],
where k =

q
l

Ïmax
.
Ïmin

Figure 3 visualizes the process:

where, F is a row-normalized adjacency matrix accumulating
the (averaged) contributions of the nodes to their neighbors
along the forward edges.

â¢ On the left hand side of the figure, we have the incrementally smoothed versions of the PageRank vector, P . Here,
the lowest level, SG,Ïmin (P ), corresponds to the most detailed version of the graph (with the least amount of smoothing), whereas SG,Ïmax (P ) corresponds to the least detailed
(most smoothed) version of the graph. In other words, Ïmin
determines the sizes of the smallest structural features we
can locate and Ïmax = kl Ïmin determines the sizes of the
largest structural features we can identify. In particular, since
under Gaussian smoothing, a diameter of 6Ï would cover
â¼ 99.73% of the weights, the diameter of the smallest structural feature that can be identified using SKE is â¼ 6Ïmin
hops, whereas the diameter of the largest feature would be
â¼ 6Ïmax hops.

â¢ Similarly, DoG-neighbors #1, #4, and #6 correspond to
the (average) DoG values of the backward neighbors at the
previous, current, and next levels of the scale space. Therefore, we have
Nhvi ,Ïj i [1] = (BDiâ1 ) [j],
Nhvi ,Ïj i [4] = (BDi ) [j], Nhvi ,Ïj i [6] = (BDi+1 ) [j],
where, B is a row-normalized backward-adjacency matrix
(where all edges are reversed) accumulating the (averaged)
contributions of the nodes to their neighbors along the backward direction of the edges.
Given these, the pair hvj , Ïi i is an extremum (i.e., vj is a keynode
candidate at scale Ïi ), iff Di [j] is a local maximum

The number of levels, l, denotes the number of detail levels
(or scales) we explore between Ïmin and Ïmax . Intuitively,
each of these levels corresponds to a different target size for
the structural features of the graph.

Di [j] â¥ M AX Nhvj ,Ïi i [h]
1â¤hâ¤8

â¢ On the right hand side of the figure, we have the resulting
Difference-of-Gaussian (DoG) series, consisting of vectors,
D1 through Dl .

or it is a local minimum
Di [j] â¤ M IN Nhvj ,Ïi i [h].
1â¤hâ¤8

Note that, intuitively, Di [j] measures how different the PageRank
values of the neighborhood around vj at scale Ïiâ1 (= kiâ1 Ïmin )
are from the PageRank values of the neighborhood around vj at
scale Ïi (= ki Ïmin ).
Therefore, a large Di [j] value would indicate a major structural
change when neighborhoods of different size around vj are considered (e.g., a node with a high PageRank score is included when
considering a neighborhood of larger scale). In contrast, a small
Di [j] indicates that there is minimal structural change when considering neighborhoods of different scales.

4.2.3

Intuitively, since Di [j] measures how different the PageRank values of the neighborhood around vj at scale Ïi are from the PageRank values of the neighborhood around vj at scale Ïiâ1 , a local
maximum corresponds to a highly scale-sensitive region (amidst
relatively scale-insensitive regions), whereas a local minimum corresponds to a scale-insentive region (amidst more scale-sensitive
regions), of the graph.

4.2.4

Selecting the Best Keynodes

In the final step, we need to rank the keynode candidates and
Î
return the top 100
Ã |V | of them, where Î is a user provided parameter, as the keynode set, K. We propose Extremum Ranking to
select the best keynodes.
Since keynodes are located at the local extrema of the DoG series, we can rank the keynode candidates based on their extremum
score defined as follows: Let the pair hvj , Ïi i be a local extremum.
The corresponding extremum score, Î¾(hvj , Ïi i), is defined as


ï£±
ï£´
if hvj , Ïi i is max.
ï£´ Di [j] â M AX Nhvj ,Ïi i [h]
ï£´
ï£²
1â¤hâ¤8


ï£´
ï£´
ï£´
ï£³ M IN Nhv ,Ï i [h] â Di [j] if hvj , Ïi i is min.
j i

Identifying Keynode Candidates

As we mentioned earlier, our intuition is that a graph node is
structurally distinctive in its neighborhood, if "the relationship between its PageRank score to the PageRank scores of its neighbors"
is different from the "relationships between the nodeâs neighborsâ
PageRank scores and the PageRank scores of their own neighbors,
at multiple scales". Therefore, to locate the keynode candidates,
we focus on the local extrema of the difference-of-Gaussian (DoG)
series D. More specifically, we identify hvj , Ïi i pairs where the
DoG value for node vj at scale, Ïi = ki Ïmin , is an extremium
(maximum and/or minimum) with respect to the neighbors of vj in
the same scale as well as neighbors in the previous and next levels
of the scale space.
In order to verify if the pair hvj , Ïi i is an extremium or not,
we compare Di [j] with the values corresponding to eight DoGneighbors in the scale-space, as visualized in Figure 4:

1â¤hâ¤8

Intuitively, the higher the extremum score is, the better local extremum (and, thus, a better keynode) is hvj , Ïi i.

1106

5.

LOCAL NODE SIGNATURES

nbhd_sim(vi , vj ) proposed by [29] accounts for the alignment between the degree distributions in these neighborhood graphs3 :

The next step in the process is to extract the local signatures (to
be used to compute local node similarities) for the nodes in the
graph. Note that this process is also offline.
While there are different local signatures proposed in the literature, in our work we build on the k-neighborhood degree distribution based local signature proposed in [29] (both because it is
simple and effective and also because this helps us compare our
keynode-driven approach to the approach proposed in [29] more
directly). Briefly, for each node vj â V and for a user provided k,
[29] first identifies the set, Nk (vj ) â V , of nodes that are at most
k hops from vj and extracts a subgraph, Gk (vj ) â G, induced by
vj and its k-hop neighbors. Then the degree sequence,

nbhd_sim(vi , vj ) =
where

dmin = min{degree(vi ), degree(vj )}
nmin = min{Î½i , Î½j }
P min â1
min{di,h , dj,h }
dmin + n
h=1
.
D(vi , vj ) =
2

Node Pair Ranking with Extended Similarity.

Îºj = [dj,1 , dj,2 , . . . , dj,|Nk (vj )| ]

While the local neighborhood similarity computation we use is
similar to the one proposed in [29], we rank pairs of nodes differently. Let vi and vj be two nodes (from two different graphs). In
particular, [29] ranks the pair hvi , vj i of nodes based on their neighborhood similarities, nbhd_sim(vi , vj ). We, however, argue that
neighborhood similarity is not sufficient for accounting for how effective the node pair is in supporting expansion. More specifically,
we observe that a pair, hvi , vj i, is likely to be a better anchor for
expansion than the pair hva , vb i if not only (a) the neighborhoods
of vi and vj are more similar to each other than the pair, va and
vb , but also (b) if vi and vj have degrees that are more aligned with
each other than va and vb . Based on this observation, instead of applying a degree threshold, we propose that the pair hvi , vj i should
be ranked based on the ranking function

consisting of the degrees of nodes in Gk (vj ) (excluding vj ), sorted
in non-increasing order, along with the degree of the node vj and
the numbers of vertices and edges in its k-hop neighborhood, form
the local signature of node vj :
local_signature(vj ) = hÎºj , degree(vj ), Î½j , Îµj i,
where Î½j = |Nk (vj ) âª {vj }| is the number of nodes in Gk (vj ) and
Îµj = |Ek (vj )| is the number of edges.
Note that, while we use a local signature similar to that proposed
in [29], we extend the node pair ranking function to better account
for the node degrees as discussed later in Section 6.1.1. As we see
in Section 8, this extension provides a significant boost in accuracy.

6.

(KEYNODE-BASED) GRAPH MATCHING

Ï(vi , vj ) = nbhd_sim(vi , vj )Ã

As discussed in Section 3 and outlined in Algorithm 1, once the
keynodes are extracted and local signatures are computed offline,
the next steps of the algorithm are to

6.1.2

We now describe how these steps are implemented in the keynodedriven scalable graph matching (KSGM) algorithm.

Keynode pair Selection

[29] uses the Hungarian algorithm to identify an initial node
matching in O(n3 ) time, where n = max{|V1 |, |V2 |}. To reduce
the execution time, [29] prunes those node pairs for which the similarity is â¤ 0.5. Since, instead of considering the node pairs in
V1 Ã V2 , we only need to consider pairs of nodes in K1 Ã K2 , and
since |K1 |  |V1 | and |K2 |  |V2 |, keynode-driven processing
is likely to be faster even without using the threshold. However,
the cubic time of the Hungarian algorithm is still prohibitive and
impractical for scalable graph matching. Therefore, we propose a
greedy anchor selection algorithm, which (as we see in Section 8)
performs very well when used along with keynodes selected in Section 4 and the proposed ranking function, Ï(). In particular, we first
include all keynode pairs in K1 Ã K2 into a queue in the order of
their ranks based on Ï(), then, until the queue is empty, we remove
and consider the keynode pair, hv, ui at the head of the queue. If
neither v nor u has been marked anchored, we include hv, ui as
an anchor and we mark v and u as anchored, otherwise, we drop
the pair, hv, ui.
Note that this process has O((|K1 | Ã |K2 |) Ã log(|K1 | Ã |K2 |))
time cost (instead of the cubic cost of the Hungarian algorithm) and,

Anchor Set Selection

Let G1 (V1 , E1 ) and G2 (V2 , E2 ) be two graphs and let K1 â V1
and K2 â V2 be the corresponding keynodes identified by the SKE
algorithm proposed in Section 4. The next step is to select a subset,
A, of the pairs of nodes in K1 Ã K2 as the anchor set of alignments
based on a ranking function (a) evaluating how structurally similar
a pair of nodes are and (b) how likely they are to lead to an effective
expansion process to discover other alignments.

6.1.1

min{degree(vi ), degree(vj )}
.
max{degree(vi ), degree(vj )}

Note that [29] simply drops node pairs where the minumum of the
two node degrees is smaller than the larger average degree of the
two input graphs. We, however, argue that such node pairs may
be useful, especially if the degrees in the graph are not uniformly
distributed and the maximum matching occurs at the sparse portions of the graph. Therefore, we keep such pairs as long as they
rank highly based on Ï(). We evaluate this ranking function in Section 8.

â¢ compare the signatures of pairs of nodes across the given
graphs to rank these pairs of nodes,
â¢ select a set of pairs of keynodes (we refer it as anchor set)
that serve as the base matching, and
â¢ expand this base matching to obtain Mq,i .

6.1

nmin + D(vi , vj )
,
(Î½i + Îµi )(Î½j + Îµj )

Node Similarity Matching and Node Pair
Ranking

As we discussed in Section 5, KSGM uses a local node signature similar to the one proposed by [29]: hÎºj , degree(vj ), Î½j , Îµj i,
where Î½j is the number of nodes in the neighborhood of
vj , Îµj is the number of neighborhood edges, and Îºj =
[dj,1 , dj,2 , . . . , dj,|Nk (vj )| ] consists of the degrees of nodes in the
k-neighborhood of vj (excluding vj ), sorted in non-increasing order.

3
In addition to using local similarities, [29] also extracts global
signatures along with the local-signatures to compute node similarities. As we see in Section 8, the proposed keynode-driven graph
matching algorithm achieves good results without having to rely on
such global-signatures.

Local Neighborhood Similarity.
Let vi and vj be two nodes (from two different graphs)
and let Gk (vi ) and G0k (vj ) be the corresponding induced
kâneighborhood graphs.
Then, local similarity function

1107

as we see in Section 8, performs very well in practice. Furthermore,
the nature of Hungarian algorithm, which forces to pair all possible
nodes to produce the optimal bipartite matching for the given two
sets of nodes, is not guaranteed to provide a better matching in this
case. Since the extracted keynodes are not all necessarily perfectly
paired with each other, some keynodes can be a unique feature of
the given graph, which does not align with other graphs, by forcing
them to pair with other keynodes, it in fact introduces a bad initial
base matching, and thus expand into an even worse matching. The
proposed greedy matching algorithm, however, only consider the
highly aligned keynodes, which in practice provides better results
than the optimal bipartite matching.

6.2

more efficient randomized algorithms exist [27]. Once the node
distances have been computed, we construct the scale-space in
O(l Ã |V | Ã max_w_nbhd_size), as for each of the l scales, the
score of each node needs to be smoothed considering the scores
of the vertices in its w-hop neighborhood (w is the Gaussian window size and max_w_nbhd_size is the size of the largest w-hop
neighborhood in G).
Once the scale-space is constructed, next, we identify the keynode candidates. This involves O(l Ã |V |) time, because for each
of the l scales, each node needs to be compared with a constant
number (8) of DoG-neighbors in the scale-space.
Finally, we rank the keynode candidates to select the top K =
Î
V many as the keynodes to bootstrap the online matching pro100
cess. Let there be C many keynode candidates. Computing the
ranking scores for these takes O(C) time, because each keynode
candidate needs to be compared with a constant number of DoGneighbors and obtaining the top K takes O(C Ã log(K)) time.

Matching List Expansion

Because keynodes are inherently sparsely localized, the anchor
set, A, is not necessarily a good final matching for graphs G1 and
G2 . We therefore need to expand this anchor list. Here, we follow [29]âs recommendation and expand the list incrementally by
considering the neighbors (and their neighbors) until no effective
expansion is possible (but we use the ranking function Ï() instead
of the node similarity function):

7.1.2

1. we first include all node pairs in A into a ranked queue (i.e.,
max-heap) in the order of their ranks based on the ranking
function, Ï(),
2. then, for each node pair hv, ui â A, we also include the node
pairs in neighbors(u) Ã neighbors(v) in the same ranked
queue

7.2

Online Time Complexity

Let G1 (V1 , E1 ) and G2 (V2 , E2 ) be two graphs. The online process includes the following operations.

3. then, until the ranked queue is empty, we remove and consider the node pair, hv, ui at the head of the ranked queue

7.2.1

Local Similarity Computation for Keynodes

This process has O(|K1 | Ã |K2 | Ã compare_length) complexity, where
compare_length = min{max_k_nbhd_size1 , max_k_nbhd_size2 })
since signatures (of length are compared for each pair of nodes in
the keynode sets K1 and K2 .

(a) if either v or u has not yet been marked matched, then
i. we include the pair, hv, ui, in the expanded matching list, L,
ii. we mark both v and u as matched, and
iii. then, the pairs in neighbors(u) Ã neighbors(v)
are included in the ranked queue
(b) otherwise, we drop the pair, hv, ui

7.2.2

Anchor Set Selection

This greedy process has O((|K1 | Ã |K2 |) Ã log(|K1 | Ã |K2 |))
time cost as each pair off nodes among the keynode sets need to be
considered only once in ranked order.

Once the anchor list is expanded, [29] relies on a post-process,
with time complexity, O(m Ã n3 ), where m = min{|E1 |, |E2 |}
and n = max{|V1 |, |V2 |}. This step is not scalable due to its prohibitive time complexity. Therefore, the proposed keynode-driven
scalable graph matching (KSGM) algorithm omits this refinement
post-process, due to its high time complexity4 . Instead, the set,
L, of node pairs remaining after the expansion process is directly
returned as the aligned nodes of the matching, M1,2 , for the input
graphs, G1 and G2 .

7.2.3

Anchor Set Expansion

This has O((|V1 | Ã |V2 |) Ã log(|V1 | Ã |V2 |)) worst case time
cost, as in the worst case, all pairs of vertices across the two graphs
may need to be considered for expansion in ranked order.

8.

EXPERIMENTS

In this section, we present experimental evaluations of the proposed keynode-driven scalable graph matching (KSGM) algorithm.
In particular, we compare KSGM to the graph matching algorithm
presented in [29] in terms of efficiency and accuracy.

7. TIME COMPLEXITY ANALYSIS
7.1 Offline Time Complexity

8.1

Let G(V, E) be a graph to be indexed in the database.

7.1.1

Local Signature Extraction

Since the local signature extraction process needs to extract the
k-hop neighborhoods around the nodes, the complexity of this step
is O(|V | Ã max_k_nbhd_size), where max_k_nbhd_size is the
size of the largest k-hop neighborhood in G. Note that this step
can also leverage the node distance matrix constructed during the
offline keynode extraction process.

8.1.1

Structural Keynode Extraction

Data Sets
Facebook Data Graph

The first data set we used is the Facebook social circles data
graph obtained from the Stanford Large Network Dataset Collection [2]. This is a connected graph with 4039 nodes and 88234
edges. The graph has a diameter of 8 and a 90-percentile effective
diameter of 4.7. For the experiments, we constructed 10 subgraphs
by uniformly sampling connected subsets, containing 60 â 70% of
the original graph nodes. Once the subgraphs are obtained, each
of the subgraphs is used as a query against the rest. We report the
averages of execution time and accuracy.

The first step in structural keynode extraction is to obtain the
PageRank scores for the nodes of the two graphs. While, this is an
expensive operation (involving a matrix inversion with O(|V |2.373 )
complexity for a graph with |V | nodes), there are many efficient,
approximate implementations of PageRank, including sublinear approximations [6].
The second step is the creation of an l-layer scale-space for
G. To construct the scale space, we first construct a node distance matrix, which requires an all-pairs shortest path computation, with complexity O(|V |3 ) for a graph with |V | nodes, but

8.1.2

4

Synthetic Graph Data Sets

In addition to the Facebook graph, we also used synthetic graphs,
where we controlled the topology, size, and node degree to explore

Though, in cases where scalability is not critical, this refinement
can be implemented without any change in the rest of the algorithm.

1108

Table 1: Synthetic graph topologies and configurations
Graph topology
Erdos-Renyi (ER)
Power law (PL)

Number of nodes
5000, 7500 (plus 1 to 10%)
5000, 7500 (plus 1 to 10%)

the advantages and disadvantages of the algorithms under different
scenarios.
We generated the synthetic graphs using the well known random
graph generating tool, NetworkX [1]. We consider two common
graph topologies: the Erdos-Renyi (ER) model and the power law
topology (PL) under the Barabasi-Albert model. Table 1 lists the
number of nodes and average degree settings that we used for assessing our algorithms. For each configuration, we generated 10
graphs. Note that, in addition to the base sizes (of 5000 and 7500),
we randomly created an additional 1 to 10% more nodes to ensure
that the different graphs in the data set have slightly different numbers of nodes. As before, once the 10 graphs are obtained for each
configuration, each of the subgraphs is used as query against the
rest. We report the averages of execution time and accuracy.

8.2

Table 2: Experiment results for the Facebook Graph (default
parameters)

Average degree
4, 8, 16
4, 8, 16

KSGM
6.4
38.0%

PR
7.25
34.12%

Random
6.0
15.4%

Extraction time (offline, sec.)

11.5

110.4

0.39

-

Matching time (online, sec.)
Accuracy

2%
6.5
37.6%

3%
6.4
38.0%

4%
6.3
38.7%

6%
6.4
36.4%

8%
7.2
33.2%

Table 4: Impact of the node-pair ranking function, Ï(), for the
Facebook Graph
Matching time (online sec.)
Accuracy

Ï()
6.4
38.0%

w/o Degree
6.0
31.8%

with Global
4.0
22.9%

Table 5: Impact of the local-signature neighborhood size, k, for
the Facebook Graph

Evaluation Criteria

Matching time (online, sec.)
Accuracy

2 hops (default)
6.4
38.0%

3 hops
5.5
33.9%

4 hops
4.3
23.3%

processing, its online matching time is 3Ã faster than that of [29].
Moreover, the matching accuracy of KSGM is 1.2Ã better than that
of the competitor, through it does not use global signatures, nor it
relies on the optimal Hungarian algorithm for anchor selection.
The table also lists the performance of KSGM when using top
PageRank (PR) scored nodes instead of those returned by the SKE
algorithm. As we see here, while the offline process is faster when
using PageRank scoring nodes, the runtime performance (both in
terms of execution time and accuracy) is worse when using SKE
keynodes. In addition, to see whether it is possible to achieve a
competitive accuracy if we were to select a similar percentage of
node randomly, in Table 2, we also include results where random
keynodes are used in the matching online phase. As we can see, the
accuracy drops significantly when we use random keynodes instead
of using robust structural keynodes extracted by the proposed SKE
algorithm6 . These indicate that SKE is indeed effective in extracting
structurally distinct and useful keynodes.

Execution Time

We report both offline and online execution times. As shown in
Algorithm 1 in Section 3, for KSGM, the offline execution includes
structural keynode and local-signature extraction steps. Online execution includes similarity computation, anchor selection, and expansion steps. [29] does not perform structural keynode extraction;
instead, offline execution includes eigen-decomposition for global
signatures.
For both KSGM and [29], we omit the refinement step as its complexity is prohibitive for scalable graph matching. For instance, for
the Facebook graph for which KSGM takes â¼ 6 seconds for online
processing for a pair of graphs, refinement takes â¼ 30 minutes â
i.e., it causes a â¼ 260Ã slowdown5 .

8.3

[29]
19.2
35.4%

Table 3: Impact of the keynode percentage, Î, for the Facebook
Graph

All experiments were conducted using a 4-core Intel Core i52400, 3.10GHz, machine with 8GB memory, running 64-bit Windows 7 Enterprise. The codes were executed using Matlab 2013b
and Visual Studio 2012. To evaluate accuracy, we use the matching
quality defined in Section 3.

8.2.1

Matching time (online, sec.)
Accuracy

Experiment Parameters

The default parameters for the structural keynode extraction
(SKE) algorithm are as follows:

8.4.2

Impact of the Keynode Percentage

Table 3 studies the impact of the percentage, Î, of the nodes used
as keynodes. As we see, up to a point, the more keynodes we use,
the more accurate and faster the matching becomes. Beyond that
point, however, additional keynodes become disadvantageous. This
indicates that top keynodes are the most effective in serving as good
starting points and, as expected, below a certain rank they loose
distinctiveness, resulting in increased cost and loss in accuracy.

â¢ PageRank teleportation probability (1âÎ±) = 0.15 (as is commonly assumed),
â¢ least smoothing factor (Ïmin ) = 0.275, corresponding to â¼
2-hop neighborhoods,
â¢ maximum smoothing factor (Ïmax ) = 0.777, corresponding
to â¼ 5-hop neighborhoods, and
â¢ number (l) of smoothing levels = 6.

8.4.3

In addition, for KSGM, the default percentage (Î) of keynodes selected from the graph was set to 3%. Also, for all algorithms, local
signatures were extracted from 2-hop neighborhoods (i.e., k = 2),
as recommended by the authors of [29].

Impact of the Node-Pair Ranking Function

Table 2 lists the online and offline processing times and accuracy for the Facebook graph under the default parameter settings.
As we see here, while KSGM spends more time in one-time, offline

Table 4 studies the impact of the node-pair ranking function,
Ï(). In particular, we compare the performance of the ranking
function proposed in Section 6.1.1, to the ranking function without
degree extension and ranking function including additional globalsignature similarity as proposed in [29]. As we see here, the proposed node-pair ranking function provides the best expansion opportunities (and thus provides the highest accuracy, with slight expansion time overhead). Also, the "with Global" optional provides
a much worse matching. Thus, while the algorithm allows, we encourage the users not to use "with Global" option.

5
For this data configuration, when using expensive refinement postprocessing, KSGM and [29]âs accuracies are 0.72 and 0.696, respectively.

6
The slight time gain when using random keynodes is due to the
fact that random keynodes are not good starting points for expansion and, thus, the expansion process ends earlier.

8.4
8.4.1

Results for the Facebook Graph
Default Configuration

1109

gorithm works faster than the state-of-the-art algorithms without
refinement, yet produces alignments that are as good or better.

Table 6: Experiment results for the synthetic data sets (avg.
degree=4, varying models and number of nodes)
KSGM Online time (sec.)
[29] Online time (sec.)
KSGM Accuracy
[29] Accuracy

PL(5000)
6.9
99.3
45.3%
42.9%

PL(7500)
13.9
223.7
45.1%
42.8%

ER(5000)
6.5
746.7
45.7%
46.3%

ER(7500)
14.5
745.1
51.2%
53.0%

KSGM Offline time (sec.)
[29] Offline time (sec.)

130.0
23.8

284.7
82.8

134.8
24.9

270.8
84.5

Acknowledgments
We thank the authors of [29] for sharing their source code and data.

10.

Table 7: Impact of the average node degree (number of
nodes=5000, power law model)
KSGM Online time (sec.)
[29] Online time (sec.)
KSGM Accuracy
[29] Accuracy

degree=4
6.9
99.3
45.3%
42.9%

degree=8
7.5
116.7
25.2%
25.1%

degree=16
9.3
141.2
13.9%
14.1%

KSGM Offline time (sec.)
[29] Offline time (sec.)

130.0
23.8

199.1
27.9

396.7
53.2

8.4.4

Impact of the Neighborhood Size, k, for LocalSignatures

Table 5 studies the impact of the neighborhood size, k, for localsignature extraction. As we see in the table, the highest accuracy is
at 2 hops7 , increasing the neighborhood size negatively affects the
accuracy, indicating that unless locally meaningful signatures are
used, the resulting node-pair ranking is not effective for expansion.
This shows the keynode matching process is more accurate when
keynodes are easy to localize and this requires them to be distinct
and locally representative. Large neighborhoods potentially violate
both. Note that this is in line with the observation in Table 4.

8.5

Results for the Synthetic Data Sets

In this subsection, we consider the impacts of graph topology,
size, and node degree using ER and PL topologies. We omit discussions of the impacts of the other parameters, as they mirror those
presented in Tables 3 through 5.

8.5.1

Default Configurations

Table 6 lists the performances of KSGM and [29] for synthetic
graphs for different topologies and numbers of nodes under the default parameter settings. As we see here, the online execution time
of KSGM is significantly (10Ã to 115Ã) faster than that of [29], especially for the ER topology. Moreover, on both Erdos-Renyi (ER)
and power law (PL) topologies, the accuracy is highly competitive, with KSGM providing non-negligible accuracy gains for the PL
model (where it is relatively easier to identify effective keynodes).

8.5.2

Impact of Average Node Degree

Table 6 studies the impact of average node degree on matching
accuracies for the power law graph. As we see in the table, both
algorithms see a drop in the matching accuracy with larger node
degrees. However, KSGM stays competitive in terms of accuracy,
whereas it provides more gains in terms of online execution time.

9.

REFERENCES

[1] http://networkx.github.io/
[2] http://snap.stanford.edu/index.html
[3] X. Bai, H. Yu, and E. R. Hancock. Graph matching using
spectrament. ICPR 2004.
[4] A. Balmin, et al. ObjectRank: Authority-based keyword search in
databases. VLDB, 2004.
[5] M.G. Borgatti, et al. Network measures of social capital.
Connections 21(2):27-36, 1998.
[6] C. Borgs, M. Brautbar, J. T. Chayes, S.-H. Teng. Multiscale
Matrix Sampling and Sublinear-Time PageRank Computation.
Internet Mathematics 10(1-2): 20-48, 2014.
[7] S. Brin, et al. The anatomy of a large-scale hypertextual Web
search engine. Computer Networks and ISDN Systems 30:
107-117, 1998.
[8] H. Bunke. Error correcting graph matching: On the influence of
the underlying cost function. IEEE TPAMI, 21(9):917â922, 1999.
[9] K. S. Candan, R. Rossini, M. L. Sapino, X. Wang. sDTW:
Computing DTW Distances using Locally Relevant Constraints
based on Salient Feature Alignments. PVLDB, 1519-1530, 2012.
[10] M. Chen, J. Liu, and X. Tang. Clustering via random walk hitting
time on directed graphs. AAAI 2008.
[11] Xilun Chen, K. Selcuk Candan. LWI-SVD: Low-rank,
Windowed, Incremental Singular Value Decompositions on
Time-Evolving Data Sets. KDD 2014.
[12] Xilun Chen, K. Selcuk Candan. GI-NMF: Group Incremental
Non-Negative Matrix Factorization on Data Streams. CIKM 2014.
[13] T. H. Cormen, C. E. Leiserson, R. L. Rivest, and C. Stein.
Introduction to Algorithms. 2001.
[14] R. Giugno and D. Shasha. Graphgrep: A fast and universal
method for querying graphs. ICPR, pp. 112-115, 2002.
[15] W. S. Han, J. Lee, and J. H. Lee. TurboISO: Towards ultrafast and
robust subgraph isomorphism search in large graph databases.
SIGMOD 2013
[16] R. Jonker and T. Volgenant.Improving the Hungarian assignment
algorithm. Oper. Res. 171-175. 1986.
[17] G. Karypis and V. Kumar "A fast and high quality multilevel
scheme for partitioning irregular graphs". SIAM Journal on
Scientific Computing 20 (1), 1999.
[18] D. Knossow, A. Sharma, D. Mateus, and R. Horaud. Inexact
matching of large and sparse graphs using laplacian eigenvectors.
GbRPR, 2009.
[19] W.-J. Lee and R. P. W. Duin. An inexact graph comparison
approach in joint eigenspace. In SSPR/SPR, 35-44, 2008.
[20] Jundong Li, Xia Hu, Jiliang Tang, Huan Liu. Unsupervised
Streaming Feature Selection in Social Media. CIKM 2015
[21] D. G. Lowe. Distinctive Image Features from Scale-Invariant
Keypoints. Int. Journal of Computer Vision, 60, 2, 2004.
[22] K. Riesen, X. Jiang, and H. Bunke. Exact and inexact graph
matching: Methodology and applications. Managing and Mining
Graph Data, pages 217-247, 2010.
[23] S. Umeyama. An eigen decomposition approach to weighted
graph matching problems. IEEE TPAMI, 10(5):695-703, 1988.
[24] J. R. Ullman. An algorithm for subgraph isomorphism, JACM
Vol. 23, No. 1, pp. 31-42. 1976
[25] X. Wang, K. S. Candan, M. L. Sapino: Leveraging metadata for
identifying local, robust multi-variate temporal (RMT) features.
ICDE, 2014.
[26] White D.R., et al. Betweenness centrality measures for directed
graphs. Social Networks, 16, 335-346,1994.
[27] R. Williams. Faster all-pairs shortest paths via circuit complexity.
STOC, 664-673. 2014.
[28] M. Zaslavskiy, F. R. Bach, and J.-P. Vert. A path following
algorithm for the graph matching problem. IEEE Trans. Pattern
Anal. Mach. Intell., 31(12):2227-2242, 2009.
[29] Y. Zhu, L. Qin, J. X. Yu, et al. High Efficiency and Quality: Large
Graphs Matching. CIKM, pp. 1755-1764. 2011.

CONCLUSIONS

Noticing that existing solutions to the graph matching problem
face major scalability challenges, we argue that it is impractical to
seek alignment among all pairs of nodes. Given these observations,
in this paper, we first presented an offline structural keynode extraction (SKE) algorithm and then discussed how to use these structural keynodes in a novel keynode-driven scalable graph matching
(KSGM) algorithm. Keynodes are selected carefully especially because a post refinement step is not feasible due to scalability requirements. Experiment results show that the proposed KSGM al7
Coincidentally, this also is the scale at which the SKE algorithm
located an overwhelming majority of the keynodes for this graph.

1110

Georgia State University

ScholarWorks @ Georgia State University
Public Health Faculty Publications

School of Public Health

2015

NOTES2: Networks-of-Traces for Epidemic
Spread Simulations
Sicong Liu
Arizona State University, s.liu@asu.edu

Yash Garg
Arizona State, yash.garg@asu.edu

K. SelÃ§uk Candan
Arizona State University, candan@asu.edu

Maria Luisa Sapino
University of Torino, marialuisa.sapino@unito.it

Gerardo Chowell
Georgia State University, gchowell@gsu.edu

Follow this and additional works at: http://scholarworks.gsu.edu/iph_facpub
Part of the Public Health Commons
Recommended Citation
S. Liu, Y. Garg, K. S. Candan, M. L. Sapino, G. Chowell. NOTES2: Networks- of-Traces for Epidemic Spread Simulations.
Computational Sustainability: Papers from the 2015 AAAI Workshop.

This Conference Proceeding is brought to you for free and open access by the School of Public Health at ScholarWorks @ Georgia State University. It
has been accepted for inclusion in Public Health Faculty Publications by an authorized administrator of ScholarWorks @ Georgia State University. For
more information, please contact scholarworks@gsu.edu.

Computational Sustainability: Papers from the 2015 AAAI Workshop

NOTES2: Networks-of-Traces for Epidemic Spread Simulationsâ
Sicong Liu and Yash Garg and K. SelcÌ§uk Candan
Arizona State University
email: {sliu104, ygarg, candan}@asu.edu

Maria Luisa Sapino

Gerardo Chowell-Puente

University of Torino
email: marialuisa.sapino@unito.it

Arizona State University
email: gchowell@asu.edu

Abstract
Decision making and intervention against infectious
diseases require analysis of large volumes of data,
including demographic data, contact networks, agespecific contact rates, mobility networks, and healthcare and control intervention data and models. In this
paper, we present our Networks-Of-Traces for Epidemic
Spread Simulations (NOTES2) model and system which
aim at assisting experts and helping them explore existing simulation trace data sets. NOTES2 supports analysis and indexing of simulation data sets as well as parameter and feature analysis, including identification of
unknown dependencies across the input parameters and
output variables spanning the different layers of the observation and simulation data.

Figure 1: Simulation trace exploration interface of NOTES2

Introduction

preventive actions taken by individuals and public health
interventions, requiring continuous adaptation.

Real-time and continuous analysis and decision making for
infectious disease understanding and intervention involve
multiple aspects, including (i) estimating transmissibility
of an epidemic disease, such as influenza (Abubakar et al.
2012); (ii) forecasting the spatio-temporal spread of pandemic disease at different spatial scales (Merler et al. 2011);
(iii) assessing the effect of travel controls during the early
stage of the pandemic (Colizza et al. 2007); (iv) predicting
the effect of implementing school closures (Wu et al. 2010);
and (v) assessing the impact of pharmaceutical interventions
on pandemic disease (Ferguson et al. 2005; Deodhar et al.
2014) through simulations. While highly modular and flexible epidemic spread simulation software, such as GLEaMviz
(Van den Broeck et al. 2011) and STEM (STEM 2014), exist, these suffer from two major challenges that prevent realtime decision making:
â¢ Data and model complexity: A sufficiently useful disease spreading simulation tool requires models, including
demographic data, social contact networks, age-specific
contact rates, local and global mobility patterns of individuals (Balcan et al. 2009; Merler and Ajelli 2014), epidemiological parameters for the infectious disease (e.g.,
infectious period), and control intervention data and models. Moreover, these dynamically evolve over time due to

â¢ Complexity of the simulation and observation data:
Epidemic simulations track 10s or 100s of interdependent parameters, spanning multiple layers and geospatial frames, affected by complex dynamic processes
operating at different resolutions. Moreover, generating
an appropriate ensemble of stochastic epidemic realizations may require multiple simulations, each with different parameters settings corresponding to slightly different, but plausible, scenarios (Barrett, Eubank, and Smith
2005; Chao et al. 2010). Thus, running and interpreting simulation results (along with the real-world observations) to generate timely actionable results are difficult.
In this paper, we present Networks-Of-Traces for Epidemic Spread Simulations (NOTES2) to assist experts in exploring large simulation ensembles (Figure 1). The NOTES2
system supports
â¢ analysis and indexing of simulation data sets, including
extraction of salient multi-variate temporal features from
the inter-dependent parameters, spanning multiple layers
and spatial-temporal frames, driven by complex dynamic
processes operating at different resolutions.
â¢ parameter and feature analysis, including identification
of unknown dependencies across the input parameters and
output variables spanning the different layers of the observation and simulation data.

â

Supported by NSF grants #1318788 and #1339835.
c 2015, Association for the Advancement of Artificial
Copyright 
Intelligence (www.aaai.org). All rights reserved.

79

Related Works

dynamics of multiple inter-dependent parameters.

Temporal Data Analysis. There are various multi-variate
temporal data models, such as the multi-variate structural
time series model (Harvey and Koopman 1997; Silva, Hyndman, and Snyder 2010).
Analysis of relationships (correlations, transfer functions, and causality) among time series is expensive (Reinsel 2003). A common representation of multi-variate data
evolving over time is a tensor (multidimensional array)
stream.
Tensors and tensor streams are often analyzed for their
underlying structures through tensor decomposition algorithms (Carroll and Chang 1970; Harshman 1970; Tucker
1966). An alternative to tensor decomposition is to use probabilistic and generative models, such as Hidden Markov
models (HMMs) and Dynamic Topic Modeling, DTM (Blei
and Lafferty 2006).
A third alternative is to leverage AutoRegressive Integrated Moving-Average (ARIMA) and multi-variate
ARIMA based analysis, which separates a time series into
autoregressive, moving-average, and integrative components
for modeling and forecasting (Mills 1990).
Time Series Search. In many applications, when comparing two sequences or time series, exact alignment is not
required. Instead, whether two sequences are going to be
treated as matching depends on how similar they are; thus,
this difference needs to be quantified. This is commonly
done through distance measures which quantify the minimum number (or cost) of symbol insertions, deletions, and
substitutions needed to convert one sequence to the other.
Dynamic time warping (DTW) distance (Ding et al. 2008;
Keogh 2002; L.Ye and E.Keogh 2009; Yu et al. 2007), used
commonly when comparing continuous sequences or time
series can be thought of as a special case.
Diffusion in Networks. Kempe et al. were among the first
teams who have investigated the problem of optimizing the
network for maximum spread (Kempe, Kleinberg, and Tardos 2003). Watts and Dodds also studied the conditions under which nodes in a network become influential (Watts
and Dodds 2007). (Chen, Wang, and Wang 2010) proposed
a heuristic algorithm, based on local influence regions, to
identify nodes in a network that maximize the spread of
influence. In (Shakarian, Subrahmanian, and Sapino 2010),
the authors focused on learning diffusion models and studying the impact of one node on the others in the network
through reasoning with previously learned diffusion models,
expressed via generalized annotated programs. (Leskovec et
al. 2007) focuses on the related problem of optimal sensor placement to observe information cascades within the
network, including disease outbreaks in a population, contaminant distribution within a water distribution network,
and information flow within the blogosphere. (Kim, Candan,
and Sapino 2012) noted that, while details differ, the various propagation models have two common properties: (a)
decay with distance, and (b) reinforcement. Unfortunately,
most models focus on the steady state of the propagation in
the network and ignore the temporal dynamics of the diffusion itself. Moreover, most (if not all) of these works focus
on a single parameter, whereas we need to track temporal

Networks-of-Traces for Epidemic Simulations
If effectively leveraged, models reflecting past outbreaks,
existing simulation traces obtained from simulation runs,
and real-time observations incoming during an outbreak can
be collectively used for better understanding the epidemicâs
characteristics and the underlying diffusion processes, forming and revising models, and performing exploratory, if-then
type of hypothetical analyses of epidemic scenarios.
There are five major types of data associated to epidemic
spread simulations.
â¢ Network layers: An epidemic simulation requires one or
more layers of networks, from local and global mobility
patterns to contact networks.
â¢ Disease models, describing the epidemiological parameters relevant to a simulation and the parameter dependencies necessary in the computation of the disease spread.
â¢ Simulation traces: For a given disease study, researchers
and decision makers often perform multiple simulations, each corresponding to different sets of assumptions
(disease parameters or models) or context (e.g. spatiotemporal context, outbreak conditions, interventions).
â¢ Disease observation traces: These include real-world observations relating to particular epidemic, including the
spread and severity of the disease and observations about
other relevant parameters, such as the average length of
recovery or percentage of infectious individuals that undergo pharmaceutical treatment.
â¢ External interventions: In an outbreak, public health and
disease control agencies implement various medical or social interventions, quarantines and/or school closures.
We collectively refer to these data (network layers, disease models, simulation traces, observation traces, and interventions) as the networks-of-traces (NT) data.

Leveraging the NT Model for Disease Spread
Simulation Understanding and Analysis
Epidemic spread simulations are complex. However, parameter dependencies and the network structures of the layers
(e.g. mobility, social contact networks) are implicitly evident
in the simulation traces and these carry temporal features
(that may correspond to major changes in the underlying
networks and/or temporal dynamics) that are robust against
noise. The detection of these robust multivariate features
constitutes the first step towards leveraging the NT data for
understanding epidemicsâ characteristics and the diffusion
processes, revising models, and performing exploratory, ifthen type of hypothetical analyses of epidemic scenarios.

Networks-of-Traces (NT) Feature Extraction
An NT data trace is multi-variate and the analysis of the relevant processes requires multi-variate temporal features spanning multiple inter-dependent trace parameters. Intuitively, a
robust temporal feature in a multi-variate time series corresponds to a multi-variate segment of the series which significantly differs from its neighborhood. The multi-variate

80

The time-and-variate smoothed version of Y (t, s) at scale
s = hst , sv i is defined as Y(t, s) = (H1 (s); ...; Ht (s)),
where Ht (s) = S(H, sv , Y (t, st )) is the version of
Y (t, st ) = hY1 (t, st ), ..., Ym (t, sT )i, variate-smoothed at
scale sv at time instant, t.
Iterative Scale Space Construction. We construct the scale
space by incrementally smoothing Y (both in time and variates) starting from an initial scale s0 = hst,0 , sv,0 i. Let
Yi (t, s) be a time-and-variate smoothed version of Yi (t) at
scale s = hst , sv i. Given a pair, k = hkt , kv i, of time and
variate scale multipliers, we add the three scale-space neighbors (or ss-neighbors) of Yi (t) into the scale space:
Yi (t, k â¦t s) â¡def
Yi (t, k â¦v s) â¡def
Yi (t, k â¦t,v s) â¡def

Figure 2: Each row corresponds to a time series of incidences for a sample epidemic simulation and each dot corresponds to the center of an identified multivariate feature

Yi (t, hkt Ã st , sv i),
Yi (t, hst , kv Ã sv i), and
Yi (t, hkt Ã st , kv Ã sv i).

The process continues iteratively until maximum temporal
and variate scales (bounded by the length of the simulation
trace and the number of variates) are met.
Local Extrema Detection. For detecting extrema, for each
Yi (t, s) in the constructed scale space, we compute

segment is represented by a center, hÂµt , Âµv i, and a scope,
hÏt , Ïv i. Intuitively, Âµt marks the center of the segment in
time and Ït is the corresponding time interval. On the other
hand, Âµv is one or more nodes/variates of the graph on which
the segment is centered and Ïv denotes all the graph vertices
covered by the segment.
Figure 2 shows an epidemic simulation heatmap, where
each row corresponds to a different state. In the figure, centers of identified features are highlighted by white
dots. The figure also expands one of these robust features (tail end of the epidemic on a set of neighboring
states): the center, hÂµt = 125, Âµv = {T X}, is marked
with a blue dot and its scope, hÏtime = [111, 139], Ïv =
{AR, LA, N M, OK, T X}i, is visualized using rectangles.

Dit (t, s) = abs(Yi (t, s) â Yi (t, k â¦t s)),
Div (t, s) = abs(Yi (t, s) â Yi (t, k â¦v s)),
Dit,v (t, s)

= abs(Yi (t, s) â Yi (t, k â¦t,v s)).

Local extrema are identified by considering each hi, t, si
triple and comparing max(Dit (t, s), Div (t, s), Dit,v (t, s))
against the 78 ss-neighbors of hi, t, si in terms of time
(before, same time, after), variates (impacting, same variate, impacted by), and scales (smaller, same scale, larger).
Poorly defined extrema (i.e., an extremum that has a large
principal curvature in one direction but a small one in the
perpendicular direction) are eliminated.
Feature Descriptor Creation. Let us be given a triple
hi, t, si. Let also N and M be two integers such that N â¼
3Ït and M â¼ 3Ïv . We create the local feature descriptor corresponding to this triple using a 2N Ã 2N matrix
W : Let Y(i,s) be the time series Yi at scale s; then, for
all âN < a â¤ N and âN < b â¤ N , W [a, b] is defined as follows: (a) if b > 0, W [a, b] = (Rb Y(i,s) )[t + a];
(b) if b = 0, W [a, b] = Y(i,s) (t + a), and (c) if b < 0,
W [a, b] = (Râ1 )b Y(i,s) )[t + a].
Finally, we construct a (2uÃ2v Ãc)-dimensional descriptor for the triple hi, t, si in the form of a gradient histogram
based on the matrix, W : we sample c gradient magnitudes
on the descriptor using a 2u Ã 2v grid superimposed on the
matrix, W . A Gaussian weighting function is used to reduce
the magnitude of elements further from the center.

Robust Feature Detection Let Y (t) = hY1 (t), ..., Ym (t)i
be a multi-variate trace, from time t = 1 to t = n. As
in (Lowe 2004), we detect stable multi-variate features at the
extrema of the scale space. However, unlike (Lowe 2004)
(which operates on images with two ordered dimensions;
i.e., rows and columns of pixels), extracting multi-variate
features of the simulation trace Y (at various temporal and
variate scales) requires detecting local maxima and computing gradients relative to not only the (ordered) time dimension, but also to the underlying variate graph.
Time-and-Variate Smoothing. We construct the scale space
of Y (t) (corresponding to the versions of the series
smoothed at different temporal and variate scales) relying
on the following time-and-variate smoothing process:
â¢ Let Yi (t, st ) indicate a version of uni-variate series, Yi ,
smoothed with parameter st : Yi (t, st ) = G(t, st ) â Yi (t),
where â is convolution in t and G(t, st ) is the Gaussian.
Let Y (t, st ) = hY1 (t, st ), .., Ym (t, st )i be a version of Y ,
where each uni-variate series is independently smoothed.

Feature Search and Alignment

â¢ Let us also define the variate
Pâ smoothing function,
S(R, sv , X) = [G(0, sv )I + (j=1) 2 Ã G(j, sv )Rj ]X,
where (a) R is an m Ã m matrix describing the variate
dependencies, (b) X = hX1 , ..., Xm i is a m-vector, and
(c) sv is a variate smoothing parameter. Since G(j, sv ) approaches 0 quickly as j increases, the smoothing term in
front of X can be approximated by a finite summation.

Features extracted from a networks-of-traces data play important roles in the NOTES2 system. Here, we discuss
how the similarity between two triples, hi1 , t1 , s1 i and
hi2 , t2 , s2 i, and the corresponding descriptors, desc1 and
desc2 , are computed in NOTES2. Depending on the use context, feature similarity has three major components:

81

Table 1: Target feature parameters
min target feature length
max target feature length
min target feature size
max target feature size
descriptor size

Table 2: Average confusions for simulations with different
transmission and recovery rates.

â¼ 5 time units
â¼ 40 time units
â¼ 2 hops
â¼ 10 hops
32 (= 2 Ã 2 Ã 8)

T.Rate
1.0
0.75
0.25
1.0
0.75
0.5

â¢ Descriptor alignment: Since the feature descriptors are
gradient histograms, their similarity is measured through
a histogram similarity function (in the experiments, we
use inverse of Euclidean distance).
â¢ Temporal alignment: For temporal alignment between
two features, we consider both the distance between the
temporal centers of the features as well as the degree of
overlap between the temporal scopes of the features.
â¢ Variate alignment: For variate alignment, we consider
both the distance between the variates in the underlying
relationship graph as well as the degree of overlap between the variates within the scopes of the two features.
Depending on the application, we also consider alignments
of (a) the average amplitudes and (b) sizes of the temporal
and variate scopes of the two triplets. These various components of feature similarity are combined using a similarity
merge function, such as max, min, or product based on the
desired matching semantics.

BN
0.35
0.23
0.37
0.44
0.39
0.29

AN
0.36 (1.14Ã)
0.25 (1.06Ã)
0.63 (1.73Ã)
0.45 (1.02Ã)
0.45 (1.14Ã)
0.34 (1.15Ã)

RN
0.44 (1.28Ã)
0.32(1.37Ã)
0.49 (1.33Ã)
0.54 (1.23Ã)
0.48 (1.23Ã)
0.39 (1.32Ã)

(a) features extracted using the border network

(b) features using the air (c) features using a rannetwork (clique)
dom network
Figure 3: Centers of features extracted using different networks (source = âNJâ, trans. rate = 0.75 and rec. rate = 0.5)

Evaluation
To assess whether the features extracted from epidemic simulations truly reflect the underlying variate networks, we
created a set of simulations, using the STEM simulator,
based on the US border network, where there is an edge between states if they share a border, and air network, which is
a clique. For a given pair of transmission and recovery rates,
we created 51 simulations (of length 213 units of time) assuming a different US state as the ground zero and recorded
incidence rates1 . We then extracted three sets of features
from each simulation, using parameters in Table 1, and assuming different connectivity structures:
â¢ Border network (BN): For this case, we used the border
network denoting states sharing borders.
â¢ Air network (AN): In this case, features are extracted assuming the air network (which is a clique).
â¢ Random network (RN): In this case, a random graph (with
the same number of edges as the border network) is used
for extracting features.
Given these features and their descriptors, we then computed the confusion for a simulation with ground zero
sim(gz ,gz )
state, gzi , as conf usion(gzi ) = AV Ggzj 6=gzi sim(gzii ,gzji ) .
Here the similarity, sim(gzi , gzj ), between two simulations with
P ground zero states, gzi and gzj , is defined as
f âf eatures(gzi ) sim(f, bestmatchj (f )). where
bestmatchj (f ) is the best matching feature to f in the simulation with ground zero state, gzj . Temporal alignment parameters were set to be equal; Î±t = Î²t = 0.5. Variate alignment parameters were set to Î±v = 0 and Î²v = 0.1, to avoid
1

R.Rate
0.5
0.5
0.5
0.25
0.25
0.25

penalizing the wrong network alternative. We use the product merge function to combine alignment scores.
Intuitively, large confusion implies poor differentiation
power and, if the feature extraction process is effective, then
we expect that (a) the overall confusion will be the lowest
when using features extracted based on a network reflecting the underlying disease propagation, and (b) confusion
will be the highest when we use an inappropriate network
for feature extraction. Table 2 presents results for different
transmission and recovery rates. As we see in this table, using the border network for feature extraction leads to least
amount of confusion. Moreover, these results conform to our
expectations listed above and Figure 3 helps see why: the
(clique structured) air network ignores disease transmissions
through land borders (especially when the transmission rate
is too small for the flights to have a big impact on the epidemicâs diffusion) and, thus, misses useful features. Random
networks, on the other hand, result in significant noise.

Conclusions
In this paper, we presented our networks-of-traces model,
which accounts for layers of disease networks (from local
and global mobility patterns to contact networks), disease
models, simulation and observation traces, and external interventions. The Networks-of-Traces for Epidemic Spread
Simulations (NOTES2) system, based on this model, aims to
assist experts in exploring large simulation trace data sets,
through networks-of-traces feature analysis.

Unless otherwise stated, we use the default STEM parameters

82

References

L.Ye, and E.Keogh. 2009. Time series shapelets: a new
primitive for data mining. In KDD09.
Merler, S., and Ajelli, M. 2014. The role of population
heterogeneity and human mobility in the spread of pandemic
influenza. Proc Biol Sci. 277(1681).
Merler, S.; Ajelli, M.; Pugliese, A.; and Ferguson, N. 2011.
Determinants of the spatiotemporal dynamics of the 2009
h1n1 pandemic in europe: implications for real-time modelling. PLoS Comput Biol. 7(9).
Mills, T. C. 1990. Time Series Techniques for Economists.
Cambridge University Press.
Reinsel, G. 2003. Elements of Multivariate Time Series
Analysis. Berlin-Heidelberg: Springer-Verlag.
Shakarian, P.; Subrahmanian, V.; and Sapino, M. L. 2010.
Using generalized annotated programs to solve social network optimization. In ICLP, 182â191.
Silva, A.; Hyndman, R. J.; and Snyder, R. D. 2010. The
vector innovation structural time series framework: a simple
approach to multivariate forecasting. Statistical Modelling
10(4):353â374.
STEM. 2014. The spatiotemporal epidemiological modeler
project. http://www.eclipse.org/stem/.
Tucker, L. 1966. Some mathematical notes on three-mode
factor analysis. Psychometrika 31(3).
Van den Broeck et al., W. 2011. The GLEaMviz computational tool, a publicly available software to explore realistic
epidemic spreading scenarios at the global scale. BMC Infect Dis. 11(37).
Watts, D., and Dodds, P. 2007. Influentials, networks, and
public opinion formation. J. of Consumer Research 34(4).
Wu et al., J. 2010. School closure and mitigation of pandemic (h1n1) 2009, hong kong. Emerg Infect Dis 16(3).
Yu, D.; Yu, X.; Hu, Q.; Liu, J.; and Wu, A. 2007. Dynamic
time warping constraint learning for large margin nearest
neighbor classification. Inf. Sci. 18(13).

Abubakar et al., I. 2012. Global perspectives for prevention of infectious diseases associated with mass gatherings.
Lancet Infect Dis. 12(1):66â74.
Balcan et al., D. 2009. Seasonal transmission potential
and activity peaks of the new influenza a(h1n1): a monte
carlo likelihood analysis based on human mobility. BMC
Medicine 7(45).
Barrett, C.; Eubank, S.; and Smith, J. 2005. If smallpox
strikes portland. Scientific American 292(3).
Blei, D. M., and Lafferty, J. D. 2006. Dynamic topic models.
In ICMLâ06, 113â120.
Carroll, J., and Chang, J.-J. 1970. Analysis of individual
differences in multidimensional scaling via an n-way generalization of eckart-young decomposition. Psychometrika
35.
Chao, D.; Halloran, M.; Obenchain, V.; and Longini Jr., I.
2010. FluTE, a publicly available stochastic influenza epidemic simulation model. PLoS Comput Biol 6(1).
Chen, W.; Wang, C.; and Wang, Y. 2010. TScalable influence maximization for prevalent viral marketing in largescale social networks. In KDDâ10.
Colizza, V.; Barrat, A.; Barthelemy, M.; Valleron, A.; and
Vespignani, A. 2007. Modeling the worldwide spread of
pandemic influenza: baseline case and containment interventions. PLoS Comput Biol 4(1).
Deodhar et al., S. 2014. An interactive, web-based high
performance modeling environment for computational epidemiology. ACM TMIS 5(2):7:1â7:27.
Ding et al., H. 2008. Querying and mining of time series
data: experimental comparison of representations and distance measures. In VLDB08, 1542â1552.
Ferguson, N.; Cummings, D.; Cauchemez, S.; Fraser, C.; Riley, S.; Meeyai, A.; Iamsirithaworn, S.; and Burke, D. 2005.
Strategies for containing an emerging influenza pandemic in
southeast asia. Nature 534(7046).
Harshman, R. 1970. Foundations of the parafac procedure:
Models and conditions for an explanatory multi-modal factor analysis. UCLA Working Pap. in Phonetics 16.
Harvey, A. C., and Koopman, S. J. 1997. Multivariate structural time series models. System Dynamics in Economic and
Financial Models 269â298.
Kempe, D.; Kleinberg, J.; and Tardos, E. 2003. Maximizing the spread of influence through a social network. In
KDDâ03, 137â146.
Keogh, E. 2002. Exact indexing of dynamic time warping.
In VLDB02, 406417.
Kim, J. H.; Candan, K. S.; and Sapino, M. L. 2012. Impact neighborhood indexing (ini) in diffusion graphs. In
CIKMâ12, 2184â2188.
Leskovec et al., J. 2007. Cost-effective outbreak detection
in networks. In KDDâ07.
Lowe, D. G. 2004. Distinctive image features from scaleinvariant keypoints. Int. J. Comput. Vision 60(2).

83

LWI-SVD: Low-rank, Windowed, Incremental Singular
Value Decompositions on Time-Evolving Data Sets
â

Xilun Chen, K. SelÃ§uk Candan
Computer Science and Engineering School of Computing-IDSE (CIDSE), Arizona State University
Tempe, AZ, USA
xilun.chen@asu.edu, candan@asu.edu

of the database [7]: Intuitively, the columns of U can be thought
of as the eigen-objects of the data, each corresponding to one independent concept/cluster, and the columns of V can be thought of as
the eigen-features of the collection, each, once again, corresponding to a concept/cluster in the database. In other words, SVD can
be used for co-clustering both data-objects and features simultaneously. The r Ã r diagonal matrix S, can be considered to represent
the strength of the corresponding latent concepts in the database:
the amount of error caused by the removal of a concept from the
database is proportional to the corresponding singular value.

ABSTRACT
Singular Value Decomposition (SVD) is computationally costly and
therefore a naive implementation does not scale to the needs of scenarios where data evolves continuously. While there are various
on-line analysis and incremental decomposition techniques, these
may not accurately represent the data or may be slow for the needs
of many applications. To address these challenges, in this paper,
we propose a Low-rank, Windowed, Incremental SVD (LWI-SVD)
algorithm, which (a) leverages efficient and accurate low-rank approximations to speed up incremental SVD updates and (b) uses
a window-based approach to aggregate multiple incoming updates
(insertions or deletions of rows and columns) and, thus, reduces online processing costs. We also present an LWI-SVD with restarts
(LWI2-SVD) algorithm which leverages a novel highly efficient
partial reconstruction based change detection scheme to support
timely refreshing of the decomposition with significant changes in
the data and prevent accumulation of errors over time. Experiment
results, including comparisons to other state of the art techniques
on different data sets and under different parameter settings, confirm that LWI-SVD and LWI2-SVD are both efficient and accurate
in maintaining decompositions.

1.

1.1 Incremental SVD and Related Works
SVD is computationally costly and therefore a naive implementation does not match the real-time needs of scenarios where data
evolve continuously: decomposition of an n Ã m matrix requires
O(nÃmÃmin(n, m)) time. While there are various on-line techniques, these are often slow or inaccurate. For example, one of the
fastest techniques, SPIRIT [13] focuses on row insertions and cannot directly handle row deletions or column insertions/deletions.
While a forgetting factor can be introduced to discount old objects,
it cannot immediately reflect the properties of the removed entries
on the decomposition. Moreover, since SPIRIT primarily considers
data insertions and deletions, it is not applicable in situations where
features of interest themselves evolve with the data (examples include weights of tags extracted from data and proximity to the hubs
within an evolving network). As we see in Section 5, it has a higher
inaccuracy compared to other incremental techniques, such as [5].
Other incremental SVD algorithms, such as [5, 6, 8, 9, 11, 12, 14,
15, 17], operate on an existing SV decomposition by folding-in new
data and features into an existing (often low-rank) SVD; algebraic
matrix manipulation techniques are used to rewrite the new SV decomposition matrices in terms of the old SV decomposition and
update (including downdating) matrices. [5] showed that a number
of database updates (including removal of columns) can all be cast
as additive modifications to the original n Ã m database matrix, A.
These updates then can be reflected on the SVD in O(nmr)
time as
p
long as the rank, r, of the matrix A is such that r â¤ min(p, q),
where p is the number of new rows and q is the number of new
columns. In other words, as long as the latent dimensionality of the
database is low, the singular value decomposition can be updated
in linear time. [5] further showed that the update to SVD can be
computed in a single pass over the data matrix making the process
highly efficient for large data. This and other existing algorithms
can nevertheless be slow for many real-time applications.

INTRODUCTION

Feature selection and dimensionality reduction techniques [20]
usually involve some (often linear) transformation of the vector
space containing the data to help focus on a few features (or combinations of features) that best discriminate the data in a given corpus.
For example, the singular value decomposition (SVD [7]) of a data
feature matrix A is of the form A = U SV T , where the r orthogonal column vectors of U form an r dimensional basis in which
the n data objects can be described. Also, the r orthogonal column
vectors of V (or the rows vector of V T ) form an r dimensional
basis in which the m features can be placed. These r dimensions
are referred to as the latent variables [16] or the latent semantics
â
This work is partially funded by NSF grants #1339835 and
#1318788. This work is also supported in part by the NSF I/UCRC
Center for Embedded Systems established through the NSF grant
#0856090.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation on the first page. Copyrights for components of this work owned by others than
ACM must be honored. Abstracting with credit is permitted. To copy otherwise, or republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
KDDâ14, August 24â27, 2014, New York, NY, USA.
Copyright 2014 ACM 978-1-4503-2956-9/14/08 ...$15.00.
http://dx.doi.org/10.1145/2623330.2623671.

1.2 Contributions of this Paper
In Section 2 we formalize these challenges and in Section 3, we
propose a Low-rank, Windowed, Incremental SVD (LWI-SVD) algorithm, which leverages efficient and accurate low-rank approxi-

987

where K is equal to

mations to speed up incremental SVD updates and uses a windowbased approach to aggregate multiple incoming updates (insertion
or deletions) and, thus reduces on-line costs. We also present, in
Section 4, an LWI2-SVD algorithm which leverages a novel partial
reconstruction based change detection technique to support timely
refreshing of the decompositions to prevent accumulation of errors.
Experiment results reported in Section 5 confirm that LWI-SVD
and LWI2-SVD are both efficient and accurate in maintaining decompositions. We conclude the paper in Section 6.

2.

K

=

At time stamp i, we are given a set of n data tuples Di =
{ti,1 , ti,2 , ..., ti,n }, each with a set, Fi , of m features. We are also
given the set, Li , containing the r latent semantics of Di (and their
weights). As time moves, new tuples arrive and some of the existing tuples expire: at the next time stamp, ti+1 , the tuple set is
â
+
â
Di+1 = (Di \âDi+1
) âª âDi+1
, where âDi+1
are the tuples
+
that expired and âDi+1 are the new tuples that arrived. Moreover, at time (i + 1), we have a new set, Fi+1 , of features, where
â
+
â
Fi+1 = (Fi \âFi+1
) âª âFi+1
, where âFi+1
are features that are
+
not of interest anymore and âFi+1 are the new features of interest.
Our goal is to quickly obtain Li+1 containing the r latent semantics corresponding to time instance, i + 1, and efficiently maintain
these r latent semantics as time further moves.

X â² = X +AB T = [ Ux



0
I

Sx
0





Vx

B

T

.

(1)



Vx

A
B



=



=



Ux



Vx

QA
QB







I
0
I
0

(2)

UxT A
RA



(3)

VxT B
RB



(4)

Moreover, by substituting Equations 3 and 4 into Equation 1, we
can get â²

 
T
T
X = X + AB

=

Ux

QA

K

Vx

QB

QA



UK )SK [ Vx

QB



VK )T (8)

As described above, QA is the orthogonal basis of (I âUx UxT )A
and QB is the orthogonal basis of (I â Vx VxT )B. These can be obtained using two expensive QR decomposition operations for both
QA and QB . One way to reduce the number of QR decomposition operations would be to seek a decomposition of â where
X â² = X + â = AAT ; i.e., A = B. However, not all â will have
such a convenient decomposition. When â is negative definite, it
cannot be written as the format of A Ã B where A = B.
Instead, in this paper, we propose to reduce the cost of the overall
QR decomposition step by setting A to the identity matrix I and
setting B T to â. This does not lose any generality on the algorithn
since â (B T ) can be any matrix. When
  we do this, since A = I,
0
it would also be the case that QA =
. Therefore, we need only
I
one QR decomposition. What is more, if the â only reflect a small
amount of data insertions and deletions, then it will be a sparse
matrix with last few rows and columns of nonzero values. This

Here QA and QB are orthogonal matrices and RA and RB are
upper-triangular. It is easy to see, through basic matrix algebra,
that the following holds:
Ux

(7)

3.1 Efficiently Obtaining QA and QB

Let us also define QA as the orthogonal basis of (I â Ux UxT )A
and QB as the orthogonal basis of (I âVx VxT )B. Both QA and QB
can be obtained through QR decomposition [4] of (I â Ux UxT )A
and (I â Vx VxT )B:



(6)

We now present our key ideas for efficient incremental SVD operations. As described above, this involves efficiently searching for
matrices, QA and QB , and the SVD of K.

2.2.1 QR Decompositions

QB RB = (I â Vx VxT )B

T

3. LWI-SVD

Given these, [5] incrementally maintains SVD as follows:

QA RA = (I â Ux UxT )A;

I
0

giving us the SVD of the new tuple matrix, X â² .
The challenge, of course, is to obtain the matrices, QA and QB ,
and the SV decomposition of K efficiently. In order to keep the
complexity down, [5] suggests that A and B should be taken as
combination of simple column vectors so that AB T can be the sum
of multiple rank-1 matrices. This, however, may be a significant
constraint in real-applications where the change matrix â itself can
have a large size, indicating great amount of rank-1 matrices it produces and updating a sequence of rank-1 matrix is not effective as
treating them as a whole. In the next section, we discuss how to
relax this assumption of [5] without impacting efficiency and accuracy.

Let us be given an n Ã m data matrix X =
and
an nâ² Ã mâ² updated data matrix X â² = X + â, where â is a
max(n, nâ² ) Ã max(m, mâ² ) change matrix. Note that if X â² has
larger dimension than X, X is padded with nâ² â n rows of zero
and mâ² â m columns of zero to match the dimension of â, the
removal of rows and columns are modeled by additions that result
in zeroing of the corresponding rows and columns (which are then
dropped from the matrix). Let us further assume that the change
matrix â can be decomposed into â = AB T . Note that we can
rewrite the matrix X â² as


0
I

Let us consider the SV decomposition of K; i.e., K = UK SK VKT .
Equation 1 can be rewritten [5] as

Ux Sx VxT

A



VxT B
RB

T
UxT A
VxT B
RA
RB

Sx
0

2.2.3 Using the Decomposition of K to Obtain the
Decomposition of X â²

2.2 Basic Incremental SVD [5]

Ux


UxT A
RA

 
Sx 0
+
0 0
I
0

Let us remember that X is an nÃm matrix and X â² is an nâ² Ãmâ²
matrix. Given this
â¢ if n â¥ nâ² and m â¥ mâ² , K is a matrix of size (n + 1) Ã (m +
1). This is because, if n â¥ nâ² , then Ux is an orthogonal
matrix and Ux UxT is equal to I. Consequently, QA RA =
(I â Ux UxT )A = 0 and this implies that RA is simply 0.
The same is true for RB .
â¢ if n < nâ² and m < mâ² , K is a matrix of size nâ² Ã mâ² . In
Section 3.2.1, we discuss the shape K takes in this case and
the resulting properties in detail.
â¢ if n â¥ nâ² and m < mâ² , K is a matrix of size (n + 1) Ã mâ² .
â¢ if n < nâ² and m â¥ mâ² , K is a matrix of size nâ² Ã (m + 1).

2.1 Problem Definition





2.2.2 Matrix K

BACKGROUND

X â² = X + AB T =

=

(5)

988

lead to efficient computation of (I â Vx VxT )B and VxT B by block
matrix multiplication. Letâs first find the zero block of B when it
is data insertion. Then â (B) is a nâ² Ã mâ² matrix with a block of
zero values on the first n Ã m position. We can rewrite B as


0 B1
B=
B2 B3

Secondly, using a similar zero-padding, we can get the following
equalities:
(Inâ² â U â² U â²T )A =

(Imâ² â V â² V â²T )B =

Vx T1 Ã B2
Vx T3 Ã B2

Vx T0 Ã B1 + Vx T1 Ã B3
Vx T2 Ã B1 + Vx T3 Ã B3

QA =



where RB1 â

0
0

Sx
0



+



UxT A
RA



VxT B
RB

T


Sx
Î¦

=

0
Inâ² ân

â²
R(m âm)Ãn

K=



S
0

â²

â Rn

Ãn

and V â² =



V
0



â²

â Rm

Ãm




Î 
.
Î



X
0

0
0







RB1



RB2

and RB2 â

,

â²
â²
R(m âm)Ã(n ân) .

0
0



+



U â²T A
RA



V â²T B
RB

T

U â²T A
RA



V â²T B
RB

T

U â²T A
RA



Y
RB2

T

=



U T RT
B1
RT
B2



=



=



(V â²T B)
RB1 RB2

T

UT
0

0
Inâ² ân



0
RB1

0
YT

U T RT
B1
RT
B2



.

Thus, K simplifies to


S
YT

S
Î¦


Î 
.
Î

This confirms that when m < mâ² and n < nâ² , K is shaped like
an arrow: it is diagonal, except for the last nâ² â n rows and last
mâ² â m columns. This, however, is not true when m â¥ mâ² or
n â¥ nâ² ; in this case K can be a dense matrix, with its last row
and columns equal to 0. In the rest of this section, we argue that,
especially when m < mâ² and n < nâ² , we can leverage Kâs specific
structure (sparse, arrow-like) to quickly obtain a highly-accurate
approximate decomposition, K â¼ UÌ SÌ VÌ T and use it instead of the
exact decomposition K = U â² S â² V â²T . In particular we propose to
build on the SVD through QR decomposition with column pivoting
technique proposed in [4]. Experiment results reported in Section 5
show that this leads to efficient and accurate decompositions even
in cases where m â¥ mâ² or n â¥ nâ² .

.

3.2.2 Decomposition of K through Pivoted QR
Pivoted QR Factorization. Let E be a matrix. A pivoted QR
factorization of E has the form EP = Qe Re where P is a permutation matrix, Qe is orthonormal and Re is upper triangular. [4]
has shown that a rank-k approximation can be obtained efficiently
through a pivoting process where columns of E are considered one
at a time and used to compute an additional column of Qe and

Intuitively, U and V are augmented by padding n â n rows of
zeros to U and mâ² â m rows of zeros to V to make it compatible
with â. This padding gives us


Inâ² ân

as

â²

Xâ² =

0

Here Y is a m Ã (nâ² â n) matrix. Note that we can further rewrite

Let X be an n Ã m matrix and X â² be an nâ² Ã mâ² matrix. In
Section 2.2.2, we have seen that K is either of size nâ² Ã mâ² , (n +
1) Ã (m + 1), (n + 1) Ã m, or nâ² Ã (m + 1), depending on whether
the numbers of rows and columns increase or decrease when the
data matrix transforms from X to X â² . Let us further assume that
n â¤ m and mâ² > m and nâ² > n, which is rows and columns
insertion. As we already discussed before, let us set A = Inâ² and
â²
â²
â²
â²
â²
â²
B T = â, where A â Rn Ãn , B â Rm Ãn and â â Rn Ãm so
that AB T is equal to the update matrix â. Finally, let SVD of X
be X = Ux Sx VxT , or simply X = U SV T .
Given the fact that X â² = X + â, we can also deduce that X â² =
U â² SV â²T + â, where




and RA =

  T

U
0
U â²T A
=
0
Inâ² ân
RA
T

T
 â²T
0
Y
V B
=
.
RB1 RB2
RB

3.2.1 Shape of K

U
0





K=



(10)

where

More specifically, in the presence of insertions, (a) since Sx is
diagonal, K is mostly sparse, and (b) it is shaped like an arrow:
(aside from the diagonal) there are non-zeros only on its last rows
and columns. We verify these next.

Uâ² =

0
Bmâ² âm

Given the above, we can rewrite the matrix, K, as

The next challenge is to obtain the singular value decomposition
of the matrix, K. Performing SVD on K directly would be costly
as the SVD operation is expensive. However, as we prove next,
in Section 3.2.1, in the presence of row and column insertions, K
takes a special structure:




RB =

3.2 Efficiently Decomposing K

=

(9)



Since the right hand side of the Equation 10 consists of 0s except
for the last mâ² â m rows, the QR decomposition of the left hand
â²
â²
â²
side will be such that QB â R(m âm)Ãm and RB â R(m âm)Ãn .
Let us further partition RB into two,

Note that, the multiplication of Vx T0 and the corresponding block
of B is avoided since the corresponding block of B is all zeros.
Also, the other part of VxT and B are small size thin matrices. Thus,
the multiplication of VxT ÃB can be done very efficiently. The same
applies to (I â Vx VxT ) Ã B and when the data are deleted.
As we experimentally show in Section 5.4.1, this optimization
provides significant gains in time, without any noticeable loss in
the final accuracy.

K

Inâ² ân



0

The right hand side of Equation 9 has n â n independent columns
and, thus, it has a simple QR decomposition:

Then, the multiplication of VxT B becomes


0
0


â²

Then, we can divide (I â Vx VxT ) and VxT into the same block
size as B. For example, we can rewrite VxT as


Vx T0 Vx T1
VxT =
Vx T2 Vx T3

VxT Ã B =



+ â = U â² SV â²T + â.

989

row of Re . The kth round of the process leads to a rank-k approximation of the pivoted QR factorization of E. In particular,
let us assume that we are given a QR decomposition of the form
F = Qf Rf and need to compute QR decomposition of [F a] for
some column vector a:


R
Ç«
[F a] = [Qf q] f
0 Ï

Algorithm 1 LWI-SVD.
Input:
The Base Matrix, X, and its SV decomposition Ux Sx VxT ;
The update matrix, â = AB T , corresponding to a window of
updates;
Target rank, r;
Output:
The new SVD results, Uxâ² ,Sxâ² , and Vxâ² ;
1: Calculate factors RA and RB in Equation 2 which, as discussed in Section 3.1, involves a QR Decomposition and several matrix multiplications;
2: Calculate the matrix K in Equation 7;
3: Obtain the low-rank (rank-r) decomposition of K into K =
UK SK VKT ;
4: Combine the factors as shown in Equation 8 to obtain rank-r
decomposition Uxâ² , Sxâ² , and Vxâ² ;
5: return Uxâ² , Sxâ² , and Vxâ² ;

The rank-k approximation can be obtained efficiently by the quasiGram-Schmidt method, which further eliminates the need to store
dense Qf matrices [4]: the quasi-Gram-Schmidt process can be applied successively to columns of a given input matrix E to produce
a pivoted QR factorization for E.
Low-Rank Decomposition of K. Let us assume that we are targeting a rank-k decomposition of K. We first sample k columns to
obtain column-sample matrix C; we then sample k columns from
K T to obtain a row-sample matrix RT . We then apply the QR
decomposition with column pivoting to C and RT to obtain upper
triangular matrices, Rc and Rr .
The sampling is done by selecting the longest row and column
vectors. We note that when m < mâ² and n < nâ² , K is not only
sparse, but also has an arrow-like shape:


S
Î 
,
K= x
Î¦ Î

the previous step as its input, there is a likelihood that errors will
accumulate over time and the reconstruction error relative to the
actual matrix will reach an unacceptable rate. To prevent errors
to accumulate, in the next section we propose a novel LWI-SVD
with Restart (LWI2-SVD) algorithm which restarts the SVD by
performing a fresh SVD on the current data matrix.

4. LWI2-SVD: LWI-SVD WITH RESTART

where the n Ã m matrix Sx is diagonal, whereas n Ã (mâ² â m)
matrix Î , (nâ² â n) Ã m matrix Î¦, and (nâ² â n) Ã (mâ² â m)
matrix Î are potentially dense as we discussed in Section 3.2.1. As
a result, the sampling is arrow-sensitive in the sense that it focuses
on the last few rows and columns: The sampled columns usually
come from the first few columns (which contain the largest singular
values at the top-left corner of the matrix)
 and the last few columns,
Î 
which contain entries from the dense,
. Similarly, the sampled
Î
rows come from the first few rows (which
 contain
 large singular
values in Sx ) and the last few rows from Î¦ Î .
Given these, to obtain a decomposition of K, we need to find a
matrix H such that kKâCHRT k is minimized. According to [15],
the value of H which minimizes this can be computed as

In this section, we build on LWI-SVD and propose a novel LWISVD with Restart (LWI2-SVD) algorithm which punctuates the incremental SVD sequence by occasionally performing a full SVD
on the current data matrix. Obviously, there is a direct, positive
correlation between the frequency of restarts and the overall accuracy of the LWI2-SVD algorithm. Unfortunately, however, there is
also a strong positive correlation between the cost of LWI2-SVD
and the frequency of restarts. Therefore, restart rate should be such
that the process is restarted only when the costly SVD is in fact
needed to help reduce the overall error.

4.1 Types of Errors
We see that there are two distinct types of errors:

(Rcâ1 RcâT )(C T KR)(Rrâ1 RrâT ).

â¢ Accumulated approximation errors (and periodic restarts):
The first type of error that accumulates over time is due to the
various approximation terms, including the low-rank approximation of K as discussed in Section 3.2. While the absolute
value of this error will be different from one iteration of the
algorithm to the next, its long term behavior will be roughly
constant. Therefore, this type of accumulated approximation
errors are best dealt with periodic restarts.

Thus, we can rewrite CHRT as
(CRcâ1 )(RcâT C T KRRrâ1 )(RrâT RT ).
If we further set W = RcâT C T KRRrâ1 and decompose W into
W = Uw Sw VwT , then we can obtain the SV decomposition of K
as K = UK SK , VKT , where
UK = CRcâ1 Uw , VKT = VwT Rrâ1 RrâT , and SK = Sw ,

â¢ Error bursts due to structural changes in the input data (and
on-demand restarts): The second type of error in the incremental SVD occurs when there is a significant structural (or
spectral) change in the data, necessitating large changes in
the SVD. Since the incremental process described in Section 3 assumes that the changes are relatively small, a significant structural change in the factor matrices, Ux and Vx ,
or the core matrix Sx may not be correctly captured, resulting in a large burst of reconstruction error. These bursts are
best dealt with on-demand restarts that are triggered through
a change detection process that tracks the updates to identify
when major structural changes in the data occur.
Figure 1 shows an example run with and without restarts. Note
that without the restarts errors continuously accumulate due to structural changes in the data. Restarts (both periodic and on-demand)

where UK and VK are orthonormal and SK is diagonal. While
this process also involves an SV decomposition step involving W ,
since W is a much smaller, k Ã k, matrix, its decomposition is
much faster than the direct decomposition of K.

3.3 Pseudocode of LWI-SVD
Algorithm 1 provides the pseudo-code of the proposed Lowrank, Windowed, Incremental Singular Value Decomposition (LWISVD) algorithm for incrementally maintaining the SVD of an evolving matrix X. As we later see in Section 5, the LWI-SVD algorithm
has a smaller approximation error than other algorithms, such as
SPIRIT [13], yet is also much faster than optimal as well as the
basic incremental SVD [5] algorithms. Yet, as in any incremental approximate algorithm, in which each step takes the output of

990

!"#$%&'()'*+,%"$-'./0-$#"$,1,$2$"$,'3$22-4'"('"5$',%"%'&%"#/6

! ! ! " ! ! ! ! ! ! "

"

"

"

"

" ! ! ! !

.%4'7$0$#%"$'#%0,(&'0*&8$#9':9
8$";$$0'<'%0,'3*##$0"'/0,$6
.84'/)':'?';9'
#$+2%3$';/"5'%'#%0,(&2@'-$2$3"$,'
3$22'/0'"5$'#$-$#A(/#

!"#"$

.84'/)':'=>';9'
/70(#$'*+,%"$

#$-$#A(/#'.-/:$'>';4

(a) reservoir maintenance for si = +hrowi , coli i
Figure 1: Example runs with and without restarts
!"!#!$%&'$#(!)
/.*$#+&0

/.*$#+&1

! ! ! " !

! ! ! ! "

!
!"!#!$%&*+,-'.

!"!#!$%&
(+2+(3-!(

*+,-'.&0

*+,-'.&1

.$(#!$%
(+,-"2#/,#!-"
(+2+(3-!(&0

"

"

"

"

" ! ! " !

!"#$%&'$%($)(*"%+$%,+
&+-(.+/$*+))$01$
%,+$&+2+&.(0&
!6#$07$7(81/9$&+:)"*+$%,+$
+)+-+1%$50%,$%,+$1+;%
<=<$01$%,+$2%&+"-

.$(#!$%
(+,-"2#/,#!-"
(+2+(3-!(&1

&+2+&.(0&$!203+$4$5#

(b) reservoir maintenance for si = âhrowi , coli i
Figure 3: Overview of the reservoir based matrix sampling
,4$"5+&6789:;

,4$"5+&6789:;

and (b) it uses a fixed (and small) buffer, referred to as the âreservoirâ. Furthermore, while (c) it does not require a priori knowledge
of the data size, it (d) ensures that each data element has an equal
chance of being represented in the sample. Let S be a data stream
consisting of a sequence of elements si . The reservoir sample keeps
a fixed reservoir of, say w elements. Once the reservoir is full,
each new element, si , replaces a (randomly) selected element in
the reservoir with a decreasing probability, inversely proportional
to the index, i, of the new element si . More specifically, a random
element in the reservoir is replaced by si with probability wi . Intuitively, in a fair sampling, each element up to i should have a w/i
chance of being in the random sample of size w. Therefore, si is
selected to be included in the reservoir with probability wi . The
sample it replaces, on the other hand, is chosen randomly among
the existing w samples in the reservoir to ensure that the reservoir
forms a random sample of the first i elements in the stream.
Matrix-Reservoir Model. As we described earlier, we consider
the general case where the data matrix can grow or shrink with
insertions or deletions of rows and columns. More specifically, we
model the evolving data matrix as a stream, S, of si = Â±hrowi , coli i,
where rowi and coli are the row and columns affected in the update
with index i: +hrowi , coli i indicates that the update inserts a new
cell in the matrix at location hrowi , coli i, whereas âhrowi , coli i
indicates that the cell at location hrowi , coli i is being removed.
The reservoir, Ri = {ri,1 , . . . , ri,w }, at time i consists of w
matrix cell positions, which serve as the representatives for the current matrix. In other words, each ri,j â Ri is a triple of the form
ri,j = hindexi,j , rowi,j , coli,j i, where indexi,j is the index of
the update that deposited the cell, located at rowi,j and coli,j , into
the reservoir.
Matrix-Reservoir Maintenance for si = +hrowi , coli i. As discussed earlier, reservoir sampling randomly selects some of the incoming stream elements for the updating the contents of the reservoir When the (probabilistically) selected incoming stream entry
si is of the form +hrowi , coli i, the basic reservoir sampling process is applied: a random element, riâ1,j from the current reservoir

Figure 2: Overview of the change detection process
can limit the accumulation of errors. Error accumulations due to
approximations generally show a regular behavior and the frequency
with which periodic restarts are scheduled can be set empirically.
The structural changes in the data, however, do not necessarily have
a regular behavior; therefore, the challenge is to quickly and efficiently detect the structural changes in the data. We will discuss
this next.

4.2 Change Detection through Partial Reconstruction
In order to detect major structural changes in the data we need
to measure or estimate the reconstruction errors. The naive way
to achieve this would be to reconstruct the entire matrix from the
incrementally maintained decomposition and compare the reconstructed matrix to the ground truth (which is the actual, revised data
matrix). If the difference is high, it means that due to some structural changes, the incrementally maintained decomposition deviated from the true decomposition of the matrix. Obviously, performing a full reconstruction of the matrix at each time step would
be extremely costly. Instead, in this section, we propose a change
detection scheme which relies on a partial reconstruction as depicted in Figure 2: (a) a fair data matrix sampler, which identifies
a small subset of the matrix cells as ground truth and (b) a partial reconstructor, which reconstructs a given subset of matrix cells,
without reconstructing the full data matrix.

4.2.1 Fair Sampling of an Evolving Matrix
We propose a fair sampler, where all matrix cells have a uniform probability of being selected independently of when they are
updated.
Basic Reservoir Sampling. Reservoir sampling [18] is a random
sampling method that works well in characterizing data streams. It
is especially efficient because (a) it needs to see the data only once

991

Algorithm 2 LWI2-SVD
Input:
The Base Matrix, X, and its SV decomposition Ux Sx VxT ;
The update matrix, â = AB T , corresponding to a window of
updates;
Target rank, r;
Reservoir, R;
Restart Threshold, Î;
Periodic Restart Flag, f ;
Output:
The new SVD results, Uxâ² ,Sxâ² , and Vxâ² ;
such that rowiâ1,j = rowi and coliâ1,j = coli , then si is
The new Reservoir, Râ² ;
simply ignored;
1: X â² = X + â
â¢ if, on the other hand, there exists a
2: if f = true then
3:
hUxâ² , Sxâ² , Vxâ² i = topK_SVD(X â² , r);
riâ1,j = hindexiâ1,j , rowiâ1,j , coliâ1,j i â Riâ1 ,
4:
Râ² = updateReservoir(R, â);
such that rowiâ1,j = rowi and coliâ1,j = coli , then
5: else
â we drop riâ1,j from the reservoir and
6:
hUxâ² , Sxâ² , Vxâ² i = LWI-SVD(X, Ux , Sx , Vx , â, r);
7:
Râ² = updateReservoir(R, â);
â we keep the j th position reserved for a future update of
8:
VÌ = partialReconstruct(Râ² , Uxâ² , Sxâ² Vxâ² );
the form sh = +hrowh , colh i.
9:
E = measurePartialError(VÌ, Râ² , X â² );
Intuitively, the matrix reservoir (and its history) is revised as if
10:
if E > Î then
the future insertion sh had in fact arrived in the past, instead of
11:
hUxâ² , Sxâ² , Vxâ² i = topK_SVD(X â² , r);
sindexiâ1,j , which had originally deposited the cell, hrowiâ1,j , coliâ1,j i 12:
end if
(which is being deleted) into the reservoir. This process is visual13: end if
14: return Uxâ² , Sxâ² , Vxâ² , Râ² ;
ized in Figure 3(b).

Riâ1 is selected and this is replaced with hi, rowi , coli i. This process is visualized in Figure 3(a).
Matrix-Reservoir Maintenance for si = âhrowi , coli i. When
the (probabilistically) selected incoming entry si is of the form
âhrowi , coli i, on the other hand, the basic reservoir sampling process cannot be applied as this denotes removal of a cell, not insertion. We handle deletions as follows:
â¢ if there exists no
riâ1,j = hindexiâ1,j , rowiâ1,j , coliâ1,j i â Riâ1 ,

4.2.2 Partial Matrix Reconstruction
At time t = i, let us have the reservoir Ri = {ri,1 , . . . , ri,w },
where for all 1 â¤ h â¤ w, ri,h = hindexi,h , rowi,h , coli,h i. Intuitively, the reservoir consists of a set of matrix cell positions (that
were fairly sampled from the overall matrix). During the partial
reconstruction step, we use the (incrementally maintained) SV decomposition, Ui , Si , and Vi , of the data matrix Xi to reconstruct
only the row and column positions that appear in the reservoir, ri,h .
More formally, the partially reconstructed matrix value set VÌi =
{vÌi,1 , . . . , vÌi,w }, is such that for all 1 â¤ h â¤ w,
vÌi,h = XÌi [rowi,h , coli,h ], where


XÌi [rowi,h , coli,h ] = (Ui [rowi,h , â]) Si ViT [â, coli,h ] .

Symbol
dim(n Ã n)

r
len
numupd
Î»upd
w
Î

Note that the cost of the partial reconstruction of the matrix depends on the size of the reservoir and when |Ri | âª |Xi |, partial
reconstruction is much faster than full reconstruction.

per

Table 1: Parameters
Desc.
Default
Initial(for
inser- 100 Ã 100
tions)/Final(for
deletions) dimensions of
X
Target rank
5
Length of the data
50
stream
Numbers
of
2:2
columns:rows updated
at a given iteration
Strength of the updates
5
(for synth. data)
Reservoir size
50
On-demand
restart
20%
threshold
Restart period
15

Alternative
300 Ã 300

10
50
6:6
10
150
10%
5

4.2.3 Change Detector
At time t = i, given the reservoir Ri = {ri,1 , . . . , ri,w }, we
construct a ground truth value set Vi = {vi,1 , . . . , vi,w }, where
for all 1 â¤ h â¤ w, vi,h = Xi [rowi,h , coli,h ]. Similarly, we also
have the partially reconstructed value set VÌi = {vÌi,1 , . . . , vÌi,w },
where for all 1 â¤ h â¤ w, vÌi,h = XÌi [rowi,h , coli,h ], where
XÌi [rowi,h , coli,h ] is the partially reconstructed value for the cell
location hrowi,h , coli,h i. Given these, we detect a major structural
change in the data matrix if
w
X

5. EXPERIMENTS
In this section, we evaluate the efficiency and effectiveness of
LWI-SVD and LWI2-SVD on both synthetic and real datasets and
for different scenarios and parameter settings.
Each experiment, consisting of len consecutive update iterations,
was run 10 times and averages are reported. Note that to simplify
the interpretation of the results we have considered insertion sequences and deletion sequences; but not hybrid insertion/deletion
sequences. Also, to make sure that the results for experiments involving sequences of insertions and deletions are comparable, we
have set the initial dimensions for an insertion sequence and the
final dimensions of a deletion sequence to the same value, dim.
The various parameters varied in the experiments, default values,
and value ranges are presented in Table 1. Below we describe the
experimental setting, including the data sets, in greater detail.

(vi,h â vÌi,h )2 â¥ Î,

h=1

where Î is the inaccuracy threshold.

4.3 Pseudocode of the LWI2-SVD Algorithm
We provide the pseudocode of the LWI-SVD with Restart (LWI2SVD), which was detailed in this section, in Algorithm 2. In the
next section, we evaluate the efficiency and effectiveness gains of
LWI2-SVD algorithm on top of the gains provided by LWI-SVD.

5.1 Real Data: Digg.com Traces
We use Digg.com data set [2] from Infochimps to evaluate the
effectiveness and efficiency for real data. The complete data set

992

deletions and column insertions and deletions (for our experiments, we used the implementation obtained from [3]).

was recorded from August to November 2008 and has 3 main components: stories, comments and replies. "Stories" contain 1490 articles that users have posted within the time period. For our experiments, we created data streams by considering the first n + len Ã
numupd articles in the data set(the first n articles make up the initial data matrix; for each of the len iterations in the update stream,
we considered numupd new articles).
Given this data set, we removed the stop words and applied stemming. We then selected the first n stories and identified the most
frequent n keywords 1 . Xij denotes occurrence of keyword j in
story i. Intuitively, the low-rank decomposition of the data matrix X simultaneously cluster stories and keywords, resulting a coclustering of the data matrix X. We moved the window at each iteration by inserting or deleting numupd records of the story trace and
recomputing the n most frequent keywords (meaning that numupd
many rows and columns are inserted and deleted). These correspond to row and column insertions/deletions on X.

LWI-SVD family of the algorithms extend our implementation
of the Brandâs algorithm described in [5] along with the Algorithm
844 [4] obtained from [1].
As evaluation criteria, we use three metrics: reconstruction error
overhead, execution time, and execution time gain:
â¢ average relative reconstruction error (errrel ) â this accuracy
measure is defined as
len
1 X rec_error(XÌi,â , Xi ) â rec_error(XÌi,SV D , Xi )
,
len i=1
rec_error(XÌi,SV D , Xi )

where
â len is the number of iterations (length of the stream),
â XÌi,â denotes the decomposition of the data matrix at
time i obtained using the algorithm âââ, and
â rec_error(Y, X) denotes the reconstruction error of
the decomposition Y against the data matrix X, measured in terms of F robenius norm.
Note that a low-rank decomposition of Xi would lead to a
reconstruction error, even if it is obtained using full SVD
followed by selection of the top r components. Therefore,
the denominator of the above term is not equal to 0.

5.2 Synthetic Data: Random Traces
We have also experimented with synthetic data sets where we
could freely vary the characteristics of the data and updates to observe the accuracy and efficiency of our algorithms under different
scenarios. For these experiments, we have created synthetic activity
traces which we then converted into data matrices as before. Since
the matrices for real data is sparse, we focus on dense matrices.
In particular, we have generated an initial n-length random sequence of 5 dimensional data, where each dimension has a value
from 0 to 10. Given these n consecutive records in the trace, we
have created a n Ã n initial matrix measuring pairwise Euclidean
distances of the records in the sequence. Insertions in the random
trace were generated by randomly picking numbers with exponential distribution, with the rate parameter, Î»upd (i.e., prob(x) =
exp_dist(x, Î»upd ) = Î»upd eâÎ»upd x ). Intuitively, if the rate parameter Î»upd is large, there is a higher likelihood of having more
large amplitude changes. If the rate parameter Î»upd is low, there is
a lower frequency of large amplitude changes in the trace.
As before, we enlarged or shrank X at each iteration by adding
or deleting numupd units of the random activity trace (meaning
that numupd many rows and columns are inserted to or deleted
from into the matrix, X).

â¢ absolute execution time (texec ) â this is the time, in seconds,
that is required to complete len consecutive decompositions
using the algorithm under consideration.
â¢ time gain (gaintime ) â the gain in time is the execution time
measured against the execution time of the full SVD; i.e.,
texec,svd âtexec,â
.
texec,svd
All experiments were conducted using a 4-core Intel Core i5-2400,
3.10GHz, machine with 8GB memory, running 64-bit Windows 7
Enterprise. The codes were executed using Matlab 7.11.0(2010b).

5.4 Evaluation with the Default Settings
5.4.1 Real Trace Data Set

5.3 Evaluation Criteria and Competitors

Figure 4 presents the accuracy and efficiency results for the real
trace data for the default parameters reported in Table 1.
Accuracy. The first thing to note in Figure 4(a), which reports
average relative reconstruction errors for the various versions of
the LWI-SVD algorithm proposed in this paper, is that restarts discussed in Section 4 are highly effective in reducing the overall error.
While both partial reconstruction-based and periodic restarts used
in LWI2-SVD are effective in improving accuracy over the LWISVD (which does not use restarts), the best results are obtained
when these are used together, bringing down the average relative
reconstruction error to 0.3-0.7% of the low-rank decomposition obtained through full SVD.
The second thing to note in Figure 4(a) is that row/column insertions, which bring in new data into the matrix, results in larger
relative reconstruction errors than row/column deletions. Note that,
when both reservoir-based and periodic restarts are employed, the
accuracy penalty relative to the low-rank decomposition of full SVD
is negligibly low for both insertions and deletions.
Efficiency. Figure 4(b) shows the efficiency results for this data set
under the default parameter configuration.
The first thing to note is that there is minimal time difference
between the LWI-SVD and LWI2-SVD algorithm. This indicates

We evaluate the LWI-SVD and LWI-SVD with Restart (LWI2SVD) algorithms by comparing them to alternative approaches:
â¢ Full SVD and SVDS â SVD is the full SV decomposition
of the matrix, we used Matlabâs [U, S, V] = svd(X)
command for this. We also considered with Matlabâs [U,
S, V] = svds(X,r) command which returns the composition results for the top-r components, where r is the desired rank (SVDS tends to perform more efficiently than SVD
when r is small and X is large and sparse);
â¢ Naive Incremental SVD â this is our implementation of the
Brandâs algorithm described in [5], it involves a full SVD and
pivoted QR based approximation is not leveraged (to implement LWI-SVD and LWI2-SVD, we use this implementation
as the basis); and
â¢ SPIRIT â this is the algorithm described in [13] which provides fast decompositions, but does not have various desirable properties of incremental SVD; including explicit data
1
In these experiments, without loss of generality, we kept the matrix in square shape, i.e., n = m

993

728#&93:5;23&93<0=>?/#&.//0/&

+&,-$.'/)0&'$.'1#234"-$!""#"$
$#%&

&!"#

!""#"$%&'"(')*$

5"'/)0&'$4#$#607)/$4#68"9$

.//0/&123/4356&

C@@>D=E7&
16F6D=E7&

!"#$%&
$"')%&

'"#(%&

'"'$%&

#")*%&

#"()%&

#"(!%&

@/3:5;23&?0&0A;B5:&?0AC/D&

!"#$"%&

%!"#

@<<:A9B3#
-2C2A9B3#

$!"#

#")$%&

#%&

"#")%&
+,-./01&&&&&&&&&&&&&&&&&&&&&&&& +,-$./01&&&&&&&&&&&&&&&&
+,-$./01&&&&&&&&&&&&&&&&
+,-$./01&&&&&&&&&&&&&&&&
2,34&56789:8;&
25676:<=>:&56789:8;&
2?6:>=@>A&56789:8;&
2567"&B&?6:>=@>A&&
56789:8;&

+,-./012%345-%67-.8%

-./01234%567/%
;<<62349%

;-<-0127%
!"('%

!"($%

!"()% !"(#%

!"#$%
!"#$% !"#$% !"#$%
!"#*%
!"#*%

!"#&%

,#+'%&

!"$%
!"'(%

!"*%

!")%

*#)+%&

(a) average relative reconstruction error

-./01234%567/%89/0:%

!"''%

*#,*%&

'()*+,-#################### '()$*+,-#############
'()$*+,-##############
'()$*+,-##################
.(/0#12345647#
.1232689:6#12345647# .;26:9<:=#12345647# .123>?;26>##12345647#

+,-./012%345-%
9::40127%

,#(-%&

!"#

(a) average relative reconstruction error

!"$%

!#()%&

!#$'%&

!"#$%

!"(&%

=/>/2349%

!",%

!"('%

!"(!% !")+%
!")+%
!")'%

!"(%

!"#'%
!"#&% !"#$% !"#&%
!"#$%
!"#&%
!"#*%
!"#(%

!%
!"#$%&'()"*
+(,-./01/2(

!"#3$%&'(
),-.-14561(
,-./01/2(

!"#3$%&'(
)7-165869(
,-./01/2(

!"#3$%&'(
),-.:;7-1:((
,-./01/2(

%&'%(

<=>>(%&'(

?064-#@A$
%&'(

!%
!"#$%&'()"*
+(,-./01/2(

(b) execution time

<=>7>!&51/&?@>A;<BC&51/&D:'($%&#":;+&<BC&&
!"#$%&'()&*"+&,-&.#)+/&01)23&

*+,-./)01+%2)&'()

67$)
!"#$%&'()

&3#4#5)
:69)

;69)

<669)

%&'%(

<=>>(%&'(

?064-#@A$
%&'(

5.4.2 Synthetic Trace Data Set

6)
869)

!"#3$%&'(
),-.:;7-1:((
,-./01/2(

Impact of the QR-Elimination Optimization. In Section 3.1, we
had discussed an optimization strategy whereby we eliminate one
of the two expensive QR operations by forcing A to be equal to
the identity matrix, I. As shown in Table 2, setting A = I causes
less than half percentage point impact on the accuracy; on the other
hand, this optimization helps save close to 12% in execution time.

678)

$69)

!"#3$%&'(
)7-165869(
,-./01/2(

(b) execution time
Figure 6: Accuracy and efficiency for synthetic trace data set default settings

Figure 4: Accuracy and efficiency for the real trace data set default settings

69)

!"#3$%&'(
),-.-14561(
,-./01/2(

<$69)

Figure 6 presents results for the synthetic trace data set under the
default parameter settings. The key observation from this figure is
that the accuracy and efficiency results for the synthetic trace data
set are very similar to the results for real trace data set, reported
in Figure 4. The similarity is especially pronounced in the execution time results in Figure 6(b). This indicates that the execution
time gains of the LWI-SVD family of algorithms (and to a certain
degree, the accuracies they provide â especially with the help of periodic and reservoir-based restarts) are inherent properties of these
algorithms rather than being highly data specific.
SPIRIT. Since the SPIRIT [13] algorithm approaches the problem
differently (e.g. cannot directly handle deletions, cannot handle
deletions/insertion of columns), we present it separately from the
rest in Figure 5. For these experiments, we use a synthetic data
trace that does not include any column insertions or deletions on
the data matrix X. As the figure shows, SPIRIT algorithm works
much faster than SVD or LWI2-SVD for the default configuration.
However, this speed comes with a significant increase in the reconstruction error, relative to the optimal low-rank decomposition
using SVD. In contrast, LWI2-SVD achieves an accuracy almost
identical to the optimal, yet costs only half as much.

456/&7)%$'5)&7)2"81#+/&9++"+&
0+)%$'5)&#"&":'($%&#":;+3&

Figure 5: Accuracy and efficiency results for the synthetic trace
data set for SVD, Spirit, and LWI2-SVD with periodic and ondemand refreshes
Table 2: Impact of setting A = I in Section 3.1
A=I
A is free
Impact
Rec. error
5.786
5.765
+0.36%
Exec time 0.174 sec 0.197 sec â11.71%
that the time overhead of reservoir maintenance and occasional ondemand full decompositions are negligible in the long run. Secondly, performing full SVD takes â¼ 75-100% more than the proposed LWI-SVD family of algorithms. Under this configuration,
the naive incremental SVD takes a little more time than full SVD,
as the basic algorithm reported in [5] involves a full SVD with same
dimension as the original matrix and several matrix multiplications.
Further-more, under this configuration, SVDS takes even longer
than the full SVD.
Finally, a close look at the LWI-SVD family of algorithms indicates that insertions require slightly longer time to maintain than
deletions. This is expected because, as discussed in Section 2.2.1,
there is no need for computing RA and RB since they are all zero.

5.5 Impacts of Data and System Parameters
In this subsection, we evaluate the impacts of the various data
and systems parameters on the efficiency and effectiveness of the
LWI-SVD family of algorithms. As representative, we select the

994

:9/;+,<%69/<.=%49,>-%
?4.93%6/9@.%A9=9B%
/9,>%/CD%

98.:*+;%<=18>-%38>-,%%
?3-82%5.8@-%A8>8B%
'#$%

'($%

/9,>%/C*!%
&&$%

!"#$% )$%

!"&$% *$%

+,-./01,-%

2.3.01,-%

4.3"%5//1/%

+,-./01,-%

+C6C=1DE%

2.3.01,-%

!"#$%!"#$%

!"&$%!"&$%

*+,-./0+,%

1-2-/0+,%

6+7.%89+,%

3-2"%4..0.%

*+,-./0+,%

1-2-/0+,%

'#$%

5*6-%78*+%

Figure 9: Accuracy and efficiency results for the real trace data
set varying the amount updates per iteration, numupd
98.:*+;%3-,-.<0*.%=*>-,%%
?3-82%5.8@-%A8B8C%

:9/;+,<%=,+093%>9?/+@%A+B.%
C4.93%6/9D.%E9?9F%
'($% ''$%

'#$% ')$%

+C6C=1D(%

Figure 7: Accuracy and efficiency for real trace data set for different rank, r

2+7GH!!@H!!%

'($% ''$%

&#$%

DEF!%

'*$%

'($% '($%

'#$% ')$%

*+,-./0+,%

1-2-/0+,%

2+7G&!!@&!!%
DEGF!%

!"#$%
!"!)$%
+,-./01,-%

!"&$%
!"!'$%

!"#$%!"#$%

!"&$%!"&$%

2.3.01,-%

*+,-./0+,%

1-2-/0+,%

4.3"%5//1/%

+,-./01,-%

2.3.01,-%

3-2"%4..0.%

6+7.%89+,%

5*6-%78*+%

Figure 10: Accuracy and efficiency for real trace data varying
reservoir size, w

Figure 8: Accuracy and efficiency results for the real trace data
set varying the size, dim, of the initial (for insertions) / final (for
deletions) matrix

98.:*+;%5<.-,<021,%
=3-82%5.8>-%?8@8A%

LWI2-SVD with the default parameters. We then vary, one-by-one,
the various data and system parameters, and compare the results
against the optimal SVD based rank-r decomposition. Since, as we
have seen, the results are similar for real and synthetic data, for the
most part we report the results with the real trace data. We use the
synthetic trace only for experiments where we vary the strengths of
the updates.

@<-@8B!"C%

'($% ''$%

'#$%

*+,-./0+,%

1-2-/0+,%

')$%

@<-@8B!"D%

5.5.1 Varying the Target Rank, r

!"#$%!")$%

!"&$%!"&$%

*+,-./0+,%

1-2-/0+,%

3-2"%4..0.%

Figure 7 presents efficiency and accuracy results for the real trace
data set where the target rank, r is varied. The results show that,
as expected (due to the low-rank nature of the LWI-SVD family of
algorithms), as the target rank increases, the time gain drops and
the relative error rate slightly increases. The drop in time gains is
because the incremental process involves a lot of matrix multiplications where the sizes of matrices are directly related to the target
rank. This confirms the observation that LWI-SVD and LWI2-SVD
are most effective when the target rank is low.

5*6-%78*+%

Figure 11: Accuracy and efficiency results for the real trace data
set varying the change threshold, Î, for on-demand restarts
the approximation nature of the algorithm. The impact on the time
gain is due to more on-demand restarts.
5.5.4 Varying the Reservoir Size, w
Figure 10 presents efficiency and accuracy results for the real
trace data set where the reservoir size, w, is varied. The results
confirm that a larger reservoir (even only â¼ 1.5% of the matrix) can
help to trigger on-demand restarts more fairly, since larger reservoir
has more accurate amortized error measuring.
5.5.5 Varying the Change Threshold, Î
Figure 11 confirms that a slightly tighter threshold, Î = 0.1
instead of the default Î = 0.2 will trigger more on-demand restarts
and thus can further reduce the error rates (which are already very
low), with little impact on execution time gains.

5.5.2 Varying the Dimensions, dim, of the Matrix
Figure 8 presents accuracy and efficiency results when we change
the dimensions, dim, of the initial data matrix (for insertions) and
the final data matrix (for deletions). Here, we see that increasing
the size of matrix does not have a big impact on accuracy and efficiency.

5.5.3 Varying the Rate of Updates, numupd
Figure 9 presents efficiency and accuracy results for the real trace
data set where the number, numupd , of row and column updates
per each iteration is varied. The results indicate that, as expected,
an increase in the number of updates per iteration impacts accuracy
as well as efficiency. The slight impact on the accuracy is due to

5.5.6 Varying the Restart Period, per
Figure 12 confirms that increasing the number of restarts by reducing the restart period, per, may improve the final accuracy.
However, unlike the on-demand restarts based on change detec-

995

98.:*+;%3-,<8.<%=-.*01%
>3-82%5.8?-%@8<8A%
B-.CDE%

'($%

'#$%

Table 3: Results for Large Dim
LWI2
LWI2
SVDS
Exec.
Rel.
Exec.
Time(s)
Error
Time(s)
1000 â 100 8.2604
0.143%
15.91
1000 â 1000 6.8087
0.06%
10.839
1500 â 1500 17.483
0.03%
23.469
2000 â 2000 34.35
0.002%
41.491
3000 â 3000 96.577
0.00097% 93.622
dim

'&$%

''$%

*+,-./0+,%

1-2-/0+,%

B-.CE%

!"#$%!")$%

!"&$%!")$%

*+,-./0+,%

1-2-/0+,%

3-2"%4..0.%

5*6-%78*+%

6. CONCLUSIONS
Figure 12: Accuracy and efficiency results for the real trace data
set varying the restart period, per, for periodic restarts

In this paper, we presented a Low-rank, Windowed, Incremental SVD (LWI-SVD) algorithm, which relies on low-rank approximations to speed up incremental SVD updates. LWI-SVD algorithm also aggregates multiple row/column insertions and deletions
to further reduce on-line processing cost. We also presented a LWISVD with restarts (LWI2-SVD) algorithm which performs periodic
and change detection based on-demand refreshing of the decomposition to prevent accumulation of errors. Experiment results on
real and synthetic data sets have shown that the LWI-SVD family
of incremental SVD algorithms are highly efficient and accurate
compared to alternative schemes under different settings.

76,8()9#:;<6=+#>=,+)9?=*#
@>8)=?1#3,6A+#B6=6C#
$!"#

064D<6E$#

&'"#

064D<6EFG#

!"#

%"#

()*+,-.)*#

()*+,-.)*#

/+01#2,,.,#

3(4+#56()#

7. REFERENCES
[1] Algorithm 844. Sparse Reduced-Rank Approximations to Sparse Matrices.
http://dl.acm.org/citation.cfm?id=1067972. Downloaded 2012.
[2] Digg.com Data Set. http://www.infochimps.com/datasets/diggcom-data-set.
Downloaded 2013.
[3] SPIRIT http://www.cs.cmu.edu/afs/cs/project/spirit-1/www/. Downloaded in
2012.
[4] M.W. Berry, S.A. Pulatova, and G.W. Stewart. Algorithm 844. Computing
Sparse Reduced-Rank Approximations to Sparse Matrices. ACM Trans.
Math. Softw. 31, 2, pp. 252-269, June 2005.
[5] M. Brand. Fast low-rank modifications of the thin singular value
decomposition. Linear Algebra and its Appl., 415(1):20-30, 2006.
[6] S. Chandrasekaran, B.S. Manjunath, Y.F. Wang, J. Winkeler, and H. Zhang.
An Eigenspace Update Algorithm for Image Analysis. Graphical Models and
Image processing: GMIP, 59(5), 1997.
[7] S. Deerwester, S. Dumais, G.W. Furnas, T.K. Landauer, and R. Harshman.
Indexing by Latent Semantic Analysis. Journal of the American Society for
Information Science, 41(6):391-407, 1990.
[8] M. Gu and S. Eisenstat. Downdating the Singular Value Decomposition.
SIAM J. Matrix Analysis and Applications, 1995.
[9] M. Gu and S.C. Eisenstat. A Stable and Fast Algorithm for Updating the
Singular Value Decomposition. Tech. Report YALEU/DCS/RR-966,
Department of Computer Science, Yale University, 1993.
[10] T. Kolda and B. Bader. Tensor Decompositions and Applications. SIAM Rev.
51, 3, 455-500. 2009.
[11] A. Levy and M. Lindenbaum. Sequential Karhunen-Loeve Basis Extraction
and its Application to Images. IEEE Transactions on Image Processing,
9:1371-1374, 2000.
[12] G. OâBrien. Information Management Tools for Updating an SVD-Encoded
Indexing Scheme, MS Thesis. 1994.
[13] S. Papadimitriou, J. Sun, and C. Faloutsos. Streaming Pattern Discovery in
Multiple Time-Series. VLDB, pp. 697-708, 2005.
[14] B. Sarwar, G. Karypis, J. Konstan, and J. Riedl. Incremental Singular Value
Decomposition Algorithms for Highly Scalable Recommender Systems.
ICIS, pp. 27â28, 2002.
[15] G.W. Steward. Four Algorithms for the Efficient Computation of Truncated
Pivoted QR Approximations to a Sparse Matrix. Numerische Mathematik 83,
313-323, 1999.
[16] J. Sun, S. Papadimitriou, and C. Faloutsos. Online Latent Variable Detection
in Sensor Networks, ICDE, 2005.
[17] D.I. Witter and M.W. Berry. Downdating the Latent Semantic Indexing
Model for Conceptual Information Retrieval. The Computer Journal, 1998.
[18] J.S. Vitter. Random Sampling with a Reservoir. ACM Trans. Math. Softw. 11,
1, pp. 37-57, March 1985.
[19] H. Zha and H.D. Simon. On Updating Problems in Latent Semantic
Indexing. SIAM J. Sci. Comput., 21(2):782-791, 1999.
[20] Z.A. Zhao and H. Liu. Spectral Feature Selection for Data Mining, Chapman
and Hall/CRC Press, 2012.

Figure 13: Accuracy and efficiency results for the synthetic
trace data set varying the update strength, Î»upd
tion (shown in Figure 11), blindly increasing the frequency of the
periodic restarts may negatively impact the time gain.

5.5.7 Varying the Update Strength, Î»upd
Finally, in Figure 13, we see the impact of the strength (in amplitude) of the incoming insertions. The figure shows that, when
Î»upd increases, the LWI2-SVD algorithm adjusts its operation by
scheduling more on-demand restarts at a cost of decreasing the time
gain.

5.6 Scalability of LWI Algorithms
The results shown above are conducted with small window size,
however, in some cases, we need large windows to monitor and
analyze a large portion of the data. In this subsection, we analyze
the scalability of LWI Algorithm by choosing large base number.
Since we have shown that under the small base number condition,
SVD out performs SVDS in execution time, however, when the
base number is large, seeking a low rank deposition using SVDS
is more efficient. Also, as we know that SVDS is very efficient
when the data is sparse, but performs less efficient on dense data.
We showed that LWI algorithm can concur this short coming when
the data is dense. Recall in section 3.2.1, we showed that K is an
arrow-like matrix which is very sparse, this leads to the efficiency
by using pivoted QR compared to a direct SVDS on the dense data.
Therefore, in the incremental maintenance of SVD on a dense matrix, we are actually seeking a second layer reduced rank approximation of a sparse matrix K. It is the main advantage of LWI
algorithm compared to SVDS when the data is dense and the base
dimension is large. Table 3 shows the execution time and error
overhead results under a synthetic dense data, the results confirm
that with big base number especially when the base is a thin and
tall matrix, LWI algorithm can have advantages in execution time
with negligible error overhead .

996

Compressed Spatial Hierarchical Bitmap (cSHB) Indexes
â
for Efficiently Processing Spatial Range Query Workloads
Parth Nagarkar

K. SelÃ§uk Candan

Aneesha Bhat

Arizona State University
Tempe, AZ 85287-8809, USA

Arizona State University
Tempe, AZ 85287-8809, USA

Arizona State University
Tempe, AZ 85287-8809, USA

nagarkar@asu.edu

candan@asu.edu

aneesha.bhat@asu.edu

ABSTRACT

terms of their coordinates in 2D space. Queries in this 2D
space are then processed using multidimensional/spatial index structures that help quick access to the data [28].

In most spatial data management applications, objects are
represented in terms of their coordinates in a 2-dimensional
space and search queries in this space are processed using
spatial index structures. On the other hand, bitmap-based
indexing, especially thanks to the compression opportunities bitmaps provide, has been shown to be highly eï¬ective
for query processing workloads including selection and aggregation operations. In this paper, we show that bitmapbased indexing can also be highly eï¬ective for managing
spatial data sets. More speciï¬cally, we propose a novel compressed spatial hierarchical bitmap (cSHB) index structure
to support spatial range queries. We consider query workloads involving multiple range queries over spatial data and
introduce and consider the problem of bitmap selection for
identifying the appropriate subset of the bitmap ï¬les for processing the given spatial range query workload. We develop
cost models for compressed domain range query processing
and present query planning algorithms that not only select
index nodes for query processing, but also associate appropriate bitwise logical operations to identify the data objects
satisfying the range queries in the given workload. Experiment results conï¬rm the eï¬ciency and eï¬ectiveness of the
proposed compressed spatial hierarchical bitmap (cSHB) index structure and the range query planning algorithms in
supporting spatial range query workloads.

1.

1.1 Spatial Data Structures

INTRODUCTION

Spatial and mobile applications are gaining in popularity, thanks to the wide-spread use of mobile devices, coupled with increasing availability of very detailed spatial
data (such as Google Maps and OpenStreetMap [3]), and
location-aware services (such as FourSquare and Yelp). For
implementing range queries (Section 3.1.2), many of these
applications and services rely on spatial database management systems, which represent objects in the database in
âThis work was supported by NSF grants 1116394, 1339835,
and 1318788
This work is licensed under the Creative Commons AttributionNonCommercial-NoDerivs 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/3.0/. Obtain permission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 41st International Conference on
Very Large Data Bases, August 31st - September 4th 2015, Kohala Coast,
Hawaii.
Proceedings of the VLDB Endowment, Vol. 8, No. 12
Copyright 2015 VLDB Endowment 2150-8097/15/08.

The key principle behind most indexing mechanisms is to
ensure that data objects closer to each other in the data
space are also closer to each other on the storage medium.
In the case of 1D data, this task is relatively easy as the
total order implicit in the 1D space helps sorting the objects so that they can be stored in a way that satisï¬es the
above principle. When the space in which the objects are
embedded has more than one dimension, however, the data
has multiple degrees of freedom and, as a consequence, there
are many diï¬erent ways in which the data can be ordered
on the storage medium and this complicates the design of
search data structures. One common approach to developing index structures for multi-dimensional data is to partition the space hierarchically in such a way that (a) nearby
points fall into the same partition and (b) point pairs that
are far from each other fall into diï¬erent partitions. The resulting hierarchy of partitions then can either be organized
in the form of trees (such as quadtrees, KD-trees, R-trees
and their many variants [28]) or, alternatively, the root-toleaf partition paths can be serialized in the form of strings
and these strings can be stored in a string-speciï¬c search
structure. Apache Lucene, a highly-popular search engine,
for example, leverages such serializations of quadtree partitions to store spatial data in a spatial preï¬x tree [1].
An alternative to applying the partitioning process in the
given multi-dimensional space is to map the coordinates of
the data into a 1D space and perform indexing and query
processing on this 1D space instead. Intuitively, in this alternative, one seeks an embedding from the 2D space to a
1D space such that (a) data objects closer to each other in
the original space are also closer to each other on the 1D
space, and (b) data objects further away from each other
in the original space are also further away from each other
on the 1D space. This embedding is often achieved through
fractal-based space-ï¬lling curves [11, 17]. In particular, the
Peano-Hilbert curve [17] and Z-order curve [23] have been
shown to be very eï¬ective in helping cluster nearby objects
in the space. Consequently, if data are stored in an order
implied by the space-ï¬lling curve, then the data elements
that are nearby in the data space are also clustered, thus
enabling eï¬cient retrieval. In this paper, we leverage these
properties of space-ï¬lling curves to develop a highly compressible bitmap-based index structure for spatial data.

1382

hierarchical bitmap ï¬les and (b) propose eï¬cient bitmap selection algorithms that select the best bitmap nodes from the
cSHB index structure to be fetched into the main-memory
for processing of the query workload. In this paper, we also
present an eï¬cient disk-based organization of compressed
bitmaps. To our best knowledge, this is the ï¬rst work that
provides an eï¬cient index structure to execute a query workload involving multiple spatial range queries by using bitmap
indexes. Experimental evaluations of the cSHB index structure and the bitmap selection algorithms show that cSHB is
highly eï¬cient in answering a given query workload.

Results
Query
Workload

Identify
Cut & Leaf Bitmaps

Construct
Result Bitmaps

Cutt Bitmaps
C
Bit
Buffer
Buffe
B
ff

Leaf Bitmaps Buffer

1.4 Paper Organization

Cut Bitmap
Cut Bitmap
Cut Bitmap
Cut Bitmap
Bitm
itmap
Leaf Bitmaps

Compressed Spatial Hierarchical Bitmaps (cSHB)

Figure 1: Processing a range query workload using
compressed spatial hierarchical bitmap (cSHB)

1.2 Bitmap-based Indexing
Bitmap indexes [29, 32] have been shown to be highly
eï¬ective in answering queries in data warehouses [34] and
column-oriented data stores [5]. There are two chief reasons for this: (a) ï¬rst of all, bitmap indexes provide an eï¬cient way to evaluate logical conditions on large data sets
thanks to eï¬cient implementations of the bitwise logical
âANDâ, âORâ, and âNOTâ operations; (b) secondly, especially when data satisfying a particular predicate are clustered, bitmap indexes provide signiï¬cant opportunities for
compression, enabling either reduced I/O or, even, complete
in-memory maintenance of large index structures. In addition, (c) existence of compression algorithms [15, 33] that
support compressed domain implementations of the bitwise
logical operations enables query processors to operate directly on compressed bitmaps without having to decompress
them until the query processing is over and the results are
to be fetched from the disk to be presented to the user.

1.3 Contributions of this Paper
In this paper, we show that bitmap-based indexing is also
an eï¬ective solution for managing spatial data sets. More
speciï¬cally, we ï¬rst propose compressed spatial hierarchical bitmap (cSHB) indexes to support spatial range queries.
In particular, we (a) convert the given 2D space into a 1D
space using Z-order traversal, (b) create a hierarchical representation of the resulting 2D space, where each node of the
hierarchy corresponds to a (sub-)quadrant (i.e., eï¬ectively
creating an implicit âquadtreeâ), and (c) associate a bitmap
ï¬le to each node in the quadtree representing the data elements that fall in the corresponding partition. We present
eï¬cient algorithms for answering range queries using a select
subset of bitmap ï¬les stored in a given cSHB index.
We then consider a service provider that has to answer
multiple concurrent queries over the same spatial data and,
thus, focus on query workloads involving multiple range
queries. Since the same set of queries can be answered using
diï¬erent subsets of the bitmaps in the cSHB index structure, we consider the problem of identifying the appropriate
bitmap nodes for processing the given query workload. More
speciï¬cally, as we visualize in Figure 1, (a) we develop cost
models for range query processing over compressed spatial

This paper is organized as follows. In the next section we
provide an overview of the related work. In Section 3.1, we
introduce the key concepts and notations, and in Section 3.2,
we present the proposed cSHB index structure. Then, in
Section 4, we describe how query workloads are processed using cSHB: in Section 4.1, we introduce the concepts of range
query plans, in Section 4.2, we present cost models for alternative execution strategies, and in Section 4.3, we present
algorithms for ï¬nding eï¬cient query plans for a given range
query workload. Experiment results are reported in Section 5. We ï¬nally conclude the paper in Section 6.

2. RELATED WORK
2.1 Multi-Dimensional Space Partitioning
Multi-dimensional space partitioning strategies can be
categorized into two: In the ï¬rst case, including quadtree,
BD-tree, G-Tree, and KD-tree variants, a given bounded region is divided into two or more âopenâ partitions such that
each partition borders a boundary of the input region. In
the latter case, some of the partitions (often referred to as
minimum bounding regions, MBRs) are âclosedâ regions of
the space, not necessarily bordering any boundary of the input region. An advantage of this latter category of index
structures, including the R-tree and its variants (R*-tree,
R+-tree, Hilbert R-tree, and others), is that these MBRs
can tightly cover the input data objects.
While most index structures have been designed to process individual queries, there are also works focusing on the
execution of a workload of multiple queries on the same index structure. In [27], the Hilbert values of the centroids
of the rectangles formed by the range queries are sorted,
and these queries are grouped accordingly to process them
over an R-tree. In [14], R-trees are used to execute multiple
range queries and, in their formulation, authors propose to
combine adjacent queries into one. Thus, the algorithm is
not able to diï¬erentiate results of individual queries.
There are two problems commonly associated with multidimensional index structures, namely overlaps between partitions (which cause redundant I/O) and empty spaces
within partitions (which cause unnecessary I/O). While
there has been a signiï¬cant amount of research in searching
for partitioning strategies that do not face these problems,
these two issues still remain [26] and are especially critical
in very high-dimensional vector spaces. One way to tackle
this problem has been to parallelize the work. For example,
in [6], the authors describe a Hadoop-based data warehousing system with spatial support, where the main focus is to
parallelize the building of the R*-tree index structure and
query processing over Hadoop.

1383

       


       

								 															








 

  



] 
]

Figure 2: Z-order curve for a sample 2D Space.

2.2 Space Filling Curve based Indexing
The two most common space-ï¬lling curves are the fractalbased Z-order curve [23] and the Peano-Hilbert curve [17].
While the Hilbert curve provides a better mapping from
the multidimensional space onto the 1D space, its generation is a complicated and costly process [26]. With Z-order
curve, however, mapping back-and-forth between the multidimensional space and the 1D space using a process called
bit-shuï¬ing (visualized in Figure 2) is very simple and eï¬cient. Consequently, the Z-order curve has been leveraged
to deal with spatial challenges [12], including construction
of and searches on R-trees [27] and others.
In [35], the authors present a parallel spatial query processing system called VegaGiStore that is built on top of
Hadoop. This system uses a two-tiered index structure that
consists of a quadtree-based global index (used for ï¬nding
the necessary data blocks) and a Hilbert-ordering based local
index (used for ï¬nding the spatial objects in the data block).
In [26], the authors present an index called BLOCK to process spatial range queries. Their main assumption is that
the data and index can ï¬t into the main memory, and hence
their aim is to reduce the number of comparisons between
the data points and the query range. They create a sorted
list of the Z-order values for all the data points. Given a
query, they start at the coarsest level. If a block lies entirely
within the given query range, they retrieve the data points
in this block, otherwise, based on a branching fact, they decide whether to search the next granular level. In [7], the
authors proposed a UB-tree index structure that also uses
Z-ordering for storing multidimensional data in a B+ tree
and in [22], the authors presented a hierarchical clustering
scheme for the fact table of a data warehouse in which the
data is stored using the above mentioned UB-tree. In [31],
the authors present a range query algorithm speciï¬cally for
the UB-tree. Unlike our approach, the above solutions are
not speciï¬cally designed for multiple query workloads.

2.3 Bitmap Indexes
There have been signiï¬cant amount of works on improving the performance of bitmap indexes and keeping compression rates high [20, 32, 33]. Most of the newer compression
algorithms use run-length encoding for compression: this
provides a good compression ratio and enables bitwise operations directly on compressed bitmaps without having to
decompress them ï¬rst [32]. Consequently, bitmap indexes
are also shown to perform better than other database index structures, especially in data warehouses and columnoriented systems [5, 32, 34].
For attributes with a large number of distinct values,
bitmaps are often created with binning, where the domain

is partitioned into bins and a bitmap is created for each bin.
Given a query, results are constructed by combining relevant bins using bitwise OR operations. Recognizing that
many data attributes have hierarchical domains, there has
also been research in the area of multi-level and hierarchical
bitmap indexes [13, 24, 25, 29]. When the bitmaps are partitioned (with potential overlaps), it is necessary to select an
appropriate set (or cut [25]) of bitmaps for query processing; results are often obtained by identifying a set of bitmaps
and combining them using bitwise ORs. This work builds on
some of the ideas presented in [25] from 1D data to spatial
data. In [25], the cost model only considered the dominant
I/O cost (reading the bitmaps from the disk), but in this
work, we present an updated cost model, that appropriately
includes the I/O cost as well as the cost of performing local
operations on the in-memory bitmaps.
There has been some prior attempts to leverage bitmaps
in spatial query processing. For example, an MBR-based
spatial index structure is proposed in [30], where the leaves
of the tree are encoded in the form of bitmaps. Given a
query, the proposed HSB-index is traversed top-down (as in
R-trees) to identify the relevant bitmaps to be combined.
In this paper, we note that not only leaves, but also internal nodes of the spatial hierarchy can be encoded as
bitmaps, leading to signiï¬cant savings in range search time,
especially for query workloads consisting of multiple spatial
range queries. Thus, our work focuses on which bitmaps
to read in the context of spatial range query workloads and
we introduce novel algorithms to choose which bitmaps to
use to answer a query workload eï¬ciently. We generalize
the problem of bitmap selection and consider alternative
strategies that complement OR-based result construction.
In [16], authors propose a storage and retrieval mechanism
for large multi-dimensional HDF5 ï¬les by using bitmap indexes. While range queries are supported on their architecture, they neither leverage Z-order indexing, nor hierarchical bitmaps as proposed in this work. Also, their proposed
mechanism is not optimized for multiple query workloads.

3. COMPRESSED SPATIAL HIERARCHICAL BITMAP (cSHB) INDEXES
In this section, we present the key concepts used in the paper and introduce the compressed spatial hierarchical bitmap
(cSHB) index structure for answering spatial range queries.

3.1 Key Concepts and Notations
3.1.1 Spatial Database
A multidimensional database, D, consists of points that
belong to a (bounded and of ï¬nite-granularity) multidimensional space S with d dimensions. A spatial database is a
special case where d = 2. We consider rectangular spaces
such that the boundaries of S can be described using a pair
of south-west and a north-east corner points, csw and cne
(csw .x â¤ cne .x and csw .y â¤ cne .y and âpâS csw .x â¤ p.x â¤
cne .x and csw .y â¤ p.y â¤ cne .y).

3.1.2 Spatial Query Workload
In this paper, we consider query workloads, Q, consisting
of a set of rectangular spatial range queries.
â¢ Spatial Range Query: A range query, q â Q, is
deï¬ned by a corresponding range speciï¬cation q.rs =

1384

qsw , qne , consisting of a south-west point and a northeast point, such that qsw .x â¤ qne .x and qsw .y â¤ qne .y.

root
00****

0000**

â¢ Leaves of the hierarchy: LH denotes the set of leaf
nodes of the hierarchy H and correspond to all potential point positions of the ï¬nite space S. Assuming that
the database, D, contains only points, only the leaves
of the spatial hierarchy occur in the database.

â¢ Children of a node: For all ni , children(ni ) denotes
the children of ni in the corresponding hierarchy; if
ni â LH , then children(ni ) = â. In this paper, we assume that the children induce a partition of the region
corresponding to the parent node:
â

â
Sh â  .

nh âchildren(ni )

â¢ Descendants of a Node: The set of descendants of
node ni in the corresponding hierarchy is denoted as
desc(ni ). Naturally, if ni â LH , then desc(ni ) = â.
â¢ Internal Nodes: Any node in H that is not a leaf
node is called an internal node. The set of internal
nodes of H is denoted by IH . Each internal node in
the hierarchy corresponds to a (non-point) sub-region
of the given space. If N (H, l) denotes the subset of the
nodes at level l of the hierarchy H, then we have
â
â



ân =n âN (H,l) Si â©Sj = â and âS =
Si â  .
i

A cSHB index structure can be created based on any hierarchy satisfying the requirements1 speciï¬ed in Section 3.1.3.
In this paper, without loss of generality, we discuss a Zcurve based construction scheme for cSHB. The resulting
hierarchy is analogous to the MX-quadtree data structure,
where all the leaves are at the same level and a given region
is always partitioned to its quadrants at the center [28]. As
introduced in Sections 1.1 and 2.2, a space-ï¬lling curve is a
fractal that maps a given ï¬nite multidimensional data space
onto a 1D curve, while preserving the locality of the multidimensional data points (Figure 2): in other words nearby
points in the data space tend to be mapped to nearby points
on the 1D curve. As we also discussed earlier, Z-curve is a
fractal commonly used as a space-ï¬lling curve (thanks to its
eï¬ectiveness in clustering the points in the data space and
the eï¬ciency with which the mapping can be computed).
A key advantage of the Z-order curve (for our work) is
that, due to the iterative (and self-similar) nature of the underlying fractal, the Z-curve can also be used to impose a
hierarchy on the space. As visualized in Figure 3, each internal node, ni , in the resulting hierarchy has four children
corresponding to the four quadrants of the space, Si . Consequently, given a 2h -by-2h space, this leads to an (h + 1)-level
hierarchy, (analogous to an MX-quadtree [28]) which can be
used to construct a cSHB index structure2 . As we show
in Section 5, this leads to highly compressible bitmaps and
eï¬cient execution plans.

3.2.2 Blocked Organization of Compressed Bitmaps

j

ni âN (H,l)

The root node corresponds to the entire space, S.
â¢ Leaf Descendants of a Node: Leaf descendants,
leaf Desc(ni ), of a node are the set of nodes such that
leaf Desc(ni ) = desc(ni ) â© LH .

3.2 Compressed Spatial Hierarchical Bitmap
(cSHB) Index Structure
In this section, we introduce the proposed compressed spatial hierarchical bitmap (cSHB) index structure:
Definition 3.1 (cSHB Index Stucture). Given a
spatial database D consisting of a space, S, and a spatial
hierarchy, H, a cSHB index is a set, B of bitmaps, such
that for each ni â N (H), there is a corresponding bitmap,
Bi â B, where the following holds:

ÍÍÍÍ ÍÍÍ
001000 001001 001010 001011

3.2.1 Our Implementation of cSHB

â¢ Parent of a node: For all ni , parent(ni ) denotes the
parent of ni in the corresponding hierarchy; if ni is the
root, then parent(ni ) = â¥.

nh =nj âchildren(ni )

0011**

â¢ if ni is an internal node (i.e., ni 	â IH ), then
âoâD ânh âleaf Desc(ni ) located at(o, nh ) â (Bi [o] =
1), whereas
â¢ if ni is a leaf node
	 (i.e., ni â LH ), then
â¦
âoâD located at(o, ni ) â (Bi [o] = 1)

â¢ Nodes of the hierarchy: Intuitively, each node, ni â
N (H) corresponds to a (bounded) subspace, Si â S,
described by a pair of corner points, ci,sw and ci,nw .



0010**

Figure 3: A sample 4-level hierarchy deï¬ned on the
Z-order space deï¬ned in Figure 2 (the string associated to each node corresponds to its unique label)

In cSHB, we associate to the space S a hierarchy H, which
consists of the node set N (H) = {n1 , . . . , nmaxn }:

â

Sh â©Sj = â and âSi =

0001**

ÍÍÍÍ
000000 000001 000010 000011

3.1.3 Spatial Hierarchy



ÍÍ
ÍÍÍ

Given a range query, q, with a range speciï¬cation,
q.rs = qsw , qne , a data point p â D is said to be
contained within the query range (or is a range point)
if and only if qsw .x â¤ p.x â¤ qne .x and qsw .y â¤ p.y â¤
qne .y.

Given a spatial database, D, with a corresponding hierarchy, H, we create and store a compressed bitmap for each
node in the hierarchy, except for those that correspond to
regions that are empty. These bitmaps are created in a
bottom-up manner, starting from the leaves (which encode
for each point in space, S, which data objects in D are located at that point) and merging bitmaps of children nodes
into the bitmaps of their parents. Each resulting bitmap is
stored as a compressed ï¬le on disk.
It is important to note that, while compression provides
signiï¬cant savings in storage and execution time, a naive
storage of compressed bitmaps can still be detrimental for
1
In fact, cSHB can be created even when some of the requirements are relaxed â for example children do not need
to cover the parent range entirely (as in R-trees).
2
Without loss of generality, we assume that the width and
height are 2h units for some integer h â¥ 1.

1385

000 001 010 011 100 101 110 111

nj âchildren(ni )

18:
19:
20:
21:
22:
23:
24:
25:

end if
if size(Bi ) â¥ K then
write Bi to disk;
else
T = append(T, Bi )
availableSize = availableSize â size(Bi )
if (availableSize â¤ 0) or (ni is the last
node at this level) then
write T to disk;
Block T = â
availableSize = K
end if
end if
end for
end for
end procedure

performance: in particular, in a data set with large number
of objects located at unique points, there is a possibility that
a very large number of leaf bitmaps need to be created on
the secondary storage. Thus, creating a separate bitmap ï¬le
for each node may lead to ineï¬ciencies in indexing as well as
during query processing (as directory and ï¬le management
overhead of these bitmaps may be non-negligible).
To overcome this problem, cSHB takes a target block size,
K, as input and ensures that all index-ï¬les written to the
disk (with the possible exception of the last bitmap ï¬le in
each level) are at least K bytes. This is achieved by concatenating, if needed, compressed bitmap ï¬les (corresponding to
nodes at the same level of hierarchy). In Algorithm 1, we
provide an overview of this block-based bottom-up cSHB index creation process. In Line 10, we see that the bitmap of
an internal node is created by performing a bitwise OR operation between the bitmaps of the children of the node. These
OR operations are implemented in the compressed bitmap
domain enabling fast creation of the bitmap hierarchy. As it
creates compressed bitmaps, the algorithm packs them into
a block (Line 15). When the size of the block exceeds K,
the compressed bitmaps in the block are written to the disk
(Line 18) as a single ï¬le and the block is re-initialized.
Example 3.1. Let us assume that K = 10 and also that
we are considering the following sequence of nodes with the
associated (compressed) bitmap sizes:
n1 , 3; n2 , 4; n3 , 2; n4 , 15; n5 , 3; . . .
This sequence of nodes will lead to following sequence of
bitmap ï¬les materialized on disk:
[B4 ] ; [B1 B2 B3 B5 ] ; . . .






size=15

size=3+4+2+3=12

Note that, since the bitmap for node n4 is larger than the
target block size, B4 is written to disk as a separate bitmap

0

5 6 7

5 6 7
3 4

1

2

3 4

5 6 7

Range
11100X2
[56,57]

2
1

1
0

(a) 2D query range

0

3 4

2

â¢ Minimum block size, K

11:
12:
13:
14:
15:
16:
17:

2

3 4

â¢ A spatial database, D, deï¬ned over 2h -by-2h size space, S
and a corresponding (h + 1)-level (Z-curve based) hierarchy,
H, with set of internal nodes, IH

2: procedure writeBitmaps
3:
Block T = â
4:
availableSize = K
5:
for level l = (h + 1) (i.e., leaves) to 0 (i.e., root) do
6:
for each node ni in l in increasing Z-order do
7:
if l == (h + 1) then
8:
Initialize a compressed bitmap Bi
9:
else
10:
Bi =
OR
Bj

1

5 6 7

0

000 001 010 011 100
0 101 110 111

Algorithm 1 Writing blocks of compressed bitmaps to disk
1: Input:

Range
1100XX1
[48,51]

(b) Corresponding 1D ranges

Figure 4: Mapping of a single spatial range query
to two 1D ranges on the Z-order space: (a) A contiguous 2D query range, [sw = (4, 4); ne = (6, 5)] and
(b) the corresponding contiguous 1D ranges, [48,51]
and [56,57], on the Z-curve
ï¬le; on the other hand, bitmaps for nodes n1 , n2 , n3 , and
n5 need to be concatenated into a single ï¬le to obtain a block
larger than K = 10 units.
3
Note that this block-based structure implies that the size
of the ï¬les and the number of bitmap ï¬les on the disk will be
upper bounded, but it also means that the cost of the bitmap
reads will be lower bounded by K. Therefore, to obtain
the best performance, repeated access to a block to fetch
diï¬erent bitmaps must be avoided through bitmap buï¬ering
and/or bitmap request clustering. In the next section, we
discuss the use of cSHB index for range query processing. In
Section 5, we experimentally analyze the impact of block-size
on the performance of the proposed cSHB index structure.

4. QUERY PROCESSING WITH THE cSHB
INDEX STRUCTURE
In this section, we describe how query workloads are processed using the cSHB index structure. In particular, we
consider query workloads involving multiple range queries
and propose spatial bitmap selection algorithms that select
a subset of the bitmap nodes from the cSHB index structure
for eï¬cient processing of the query workload.

4.1 Range Query Plans and Operating Nodes
In order to utilize the cSHB index for answering a spatial
range query, we ï¬rst need to map the range speciï¬cation
associated with the given query from the 2D space to the
1D space (deï¬ned by the Z-curve). As we see in Figure 4,
due to the way the Z-curve spans the 2D-space, it is possible
that a single contiguous query range in the 2D space may
be mapped to multiple contiguous ranges on the 1D space.
Therefore, given a 2D range query, q, we denote the resulting
set of (disjoint) 1D range speciï¬cations, as RSq .
Let us be given a query, q, with the set of 1D range speciï¬cations, RSq . Naturally, there may be many diï¬erent ways
to process the query, each using a diï¬erent set of bitmaps
in the cSHB index structure, including simply fetching and
combining only the relevant leaf bitmaps:
Example 4.1 (Alternative Range Query Plans).
Consider a query q with q.rs = (1, 0), (3, 1) on
the space shown in Figure 2.
The corresponding 1D range, [2, 11], would cover the following
leaf nodes of the hierarchy shown in Figure 3:
RSq = (000010, 000011, 001000, 001001, 001010, 001011).
The following are some of the alternative query plans for q
using the proposed cSHB index structure:

1386

â¢ Inclusive query plans: The most straightforward way
to execute the query would be to combine (bitwise OR
operation) the bitmaps of the leaf nodes covered in 1D
range, [2, 11]. We refer to such plans, which construct
the result by combining bitmaps of selected nodes using
the OR operator, as inclusive plans.
An alternative inclusive plan for this query would be to
combine the bitmaps of nodes 000010, 000011, 0010**:
B000010 OR B000011 OR B0010ââ .
â¢ Exclusive query plans: In general, an exclusive query
plan includes removal of some of the children or descendant bitmaps from the bitmaps of a parent or ancestor through the ANDNOT operation. One such exclusive plan would be to combine the bitmaps of all leafs
nodes, except for B000010 , B000011 , B001000 , B001001 ,
B001010 , B001011 , into a bitmap Bnon result and return
Broot ANDNOTBnon result .
â¢ Hybrid query plans: Both inclusive and exclusive only
query plans may miss eï¬cient query processing alternatives. Hybrid plans combine inclusive and exclusive
strategies at diï¬erent nodes of the hierarchy. A sample
hybrid query plan for the above query would be

	
B0000ââ ANDNOT (B000000 OR B000001 ) OR B0010ââ . 3
As illustrated in the above example, a range query, q, on
hierarchy H, can be answered using diï¬erent query plans,
involving bitmaps of the leaves and certain internal nodes of
the hierarchy, collectively referred to as the operating nodes
of a query plan. In Section 4.3, we present algorithms for selecting the operating nodes for a given workload, Q; but ï¬rst
we discuss the cost model that drives the selection process.

4.2 Cost Models and Execution Strategies
In cSHB, the bitwise operations needed to construct the
result are performed on compressed bitmaps directly, without having to decompress them.

4.2.1 Cost Model for Individual Operations
We consider two cases: (a) logical operations on diskresident compressed bitmaps and (b) logical operations on
in-buï¬er compressed bitmaps.

Operations on Disk-Resident Compressed Bitmaps.
In general, when the logical operations are implemented
on compressed bitmaps that reside on the disk, the time
taken to read a bitmap from the secondary storage to the
main memory dominates the overall bitwise manipulation
time [15]. The overall cost is hence proportional to the size
of the (compressed) bitmap ï¬le on the secondary storage.
Let us consider a logical operation on bitmaps Bi and Bj .
Let us assume that T (Bi ) and T (Bj ) denotes the blocks in
which Bi and Bj are stored, respectively. Since multiple
bitmaps can be stored in a single block, it is possible that
Bi and Bj are in the same block. Hence, let us further
assume that T(Bi ,Bj ) is the set of unique blocks that contain
the bitmaps, Bi and Bj . Then the overall I/O cost is:
	
 
costio (Bi op Bj ) = Î±IO
size(T ) ,
T âT(B ,B )
i
j

where Î±IO is an I/O cost multiplier and op is a binary bitwise
logical operator. A similar result also holds for the unary
operation NOT.

Operations on In-Buffer Compressed Bitmaps.
When the compressed bitmaps on which the logical operations are implemented are already in-memory, the disk
access cost is not a factor. However, also in this case, the
cost is proportional to the sizes of the compressed bitmap
ï¬les in the memory, independent of the speciï¬c logical operator that is involved [33], leading to
	

costcpu (Bi op Bj ) = Î±cpu size(Bi ) + size(Bj ) ,
where Î±cpu is the CPU cost multiplier. A similar result also
holds for the unary operation NOT.

4.2.2 Cost Models for Multiple Operations
In this section, we consider a cost model which assumes
that blocks are disk-resident. Therefore, we consider a storage hierarchy consisting of disk (storing all bitmaps), RAM
(as buï¬er storing relevant bitmaps), and L3/L2 caches (storing currently needed bitmaps).

Buffered Strategy.
In the buï¬ered strategy, visualized in Figure 1, the
bitmaps that correspond to any leaf or non-leaf operating
nodes for the query plan of a given query workload, Q, are
brought into the buï¬er once and cached for later use. Then,
for each query q â Q, the corresponding result bitmap is extracted using these buï¬ered operating node bitmaps. Consequently, if a node is an operating one for more than one
q â Q, it is read from the disk only once (and once for each
query from the memory). Let us assume that TONQ denotes
the set of unique blocks that contains all the necessary operating nodes given a query workload Q(ONQ ). This leads
to the overall processing cost, time costbuf (Q, ONQ ), of
â
â
â
â

 
size(T )â  + Î±cpu â
size(Bi )â  .
Î±IO â



T âTON

Q



read cost






qâQ ni âONq





operating cost

Since all operating nodes need to be buï¬ered, this execution
 strategy requires a total of storage costbuf (Q, ONQ ) =
Note that, in general,
ni âONQ size(Bi ) buï¬er space.
Î±IO > Î±cpu . However, in Section 5, we see that the number
of queries in the query workload and query ranges determine
the relative costs of in-buï¬er operations vs. disk I/O.
The buï¬ered strategy has the advantage that each query
can be processed individually on the buï¬ered bitmaps and
the results for each completed query can be pipelined to the
next operator without waiting for the results of the other
queries in the workload. This reduces the memory needed
to temporarily store the result bitmaps. However, in the
buï¬ered strategy, the buï¬er needed to store the operating
node bitmaps can be large.

Incremental Strategy.
The incremental strategy avoids buï¬ering of all operating
node bitmaps simultaneously. Instead, all leaf and non-leaf
operating nodes are fetched from the disk one at a time on
demand and results for each query are constructed incrementally. This is achieved by considering one internal operating
node at a time and, for each query, focusing only on the leaf
operating nodes under that internal node. For this purpose,
a result accumulator bitmap, Resj , is maintained for each
query in qj â Q and each operating node read from the disk
is applied directly on this result accumulator bitmap.

1387

	 




#!!"&
 

If a set of internal nodes of H only satisï¬es the ï¬rst condition, then we refer to the cut as an incomplete cut.
â¦



















Figure 5: Buï¬er misses and the overall read time
(data and other details are presented in Section 5)
While it does not need buï¬er to store all operating node
bitmaps, the incremental strategy may also beneï¬t from partial caching of the relevant blocks. This is because, while
each internal node needs to be accessed only once, each
leaf node under this internal node may need to be brought
to the memory for multiple queries. Moreover, since the
data is organized in terms of blocks, rather than individual nodes (Section 3.2.2), a single block may serve multiple
nodes to diï¬erent queries. When suï¬cient buï¬er is available
to store the working set of blocks (containing the operating
leaf nodes under the current internal node), the execution
cost, time costinc (Q, ONQ ), of the incremental strategy is
identical to that of the buï¬ered strategy. Otherwise, as illustrated in Figure 5, the read cost component is a function of
buï¬er misses, Î±IO Ã # buf f er misses, which itself depends
on the size of the buï¬er and the clustering of the data.
3
The storage complexity is storage costinc (Q, ONQ ) =
size(Res
)
plus
the
space needed to maintain the
j
qj âQ
most recently read blocks in the current working set. Experiments reported in Section 5 show that, for the considered
data sets, the sizes of the working sets are small enough to
ï¬t into the L3-caches of many modern hardware.

4.3 Selecting the Operating Bitmaps for a
Given Query Workload
To process a range query workload, Q, on a data set, D,
with the underlying cSHB hierarchy H, we need to select
a set of operating bitmap nodes, ONQ , of H from which
we can construct the results for all qj â Q, such that
time cost(Q, ONQ ) is the minimum among all possible sets
of operating bitmaps for Q. It is easy to see that the number
of alternative sets of operating bitmaps for a given query
workload Q is exponential in the size of the hierarchy H.
Therefore, instead of seeking the set of operating bitmaps
among all subsets of the nodes in H, we focus our attention
on the cuts of the hierarchy, deï¬ned as follows:
Definition 4.1 (Cuts of H Relative to Q). A
complete cut, C, of a hierarchy, H, relative to a query load,
Q, is a subset of the internal nodes (including the root) of
the hierarchy, satisfying the following two conditions:
â¢ validity: there is exactly one node on any root-to-leaf
branch in a given cut; and
â¢ completeness: the nodes in C collectively cover every
possible root-to-leaf branch for all leaf nodes in the result sets for queries in Q.
3
The space complexity of the incremental strategy can be
upper-bounded if the results for the queries in Q can be
pipelined to the next set of operators progressively as partial
results constructed incrementally.

As visualized in Figure 1, given a cut C, cSHB queries are
processed by using only the bitmaps of the nodes in this
cut, along with some of the leaf bitmaps necessary
to construct results of the queries in Q. In the rest of
this subsection, we ï¬rst describe how queries are processed
given a cut, C, of H and then present algorithms that search
for a cut, C, given a workload, Q.

4.3.1 Range Query Processing with Cuts
It is easy to see that any workload, Q, of queries can be
processed by any (even incomplete) cut, C, of the hierarchy
and a suitable set of leaf nodes: Let Rq denote the set of
leaf nodes that appear in the result set of query q â Q and
RÌq be the set of leaf nodes that do not appear in the result
set. Let also RqC be the set of the result leaves covered by a
node in C. Then, one possible way to construct the result
bitmap, Bq , is as follows:
â
â



â
â
â
â
OR
Bq = â OR Bi OR
Bi â ANDNOT
Bj
.


â ni âC
â  nj âRC
ni âRq \RC
q
q â©RÌq
exclusions




inclusions

Intuitively any result nodes that are not covered by the cut
need to be included in the result using a bitwise OR operation, whereas any leaf node that is not in any result needs
to be excluded using an ANDNOT operation. Consequently,
â¢ if C â© Rq = â, an inclusion-only plan is necessary,
â¢ an exclusion-only plan is possible only if C covers Rq
completely.
Naturally, given a range query workload, Q, diï¬erent query
plans with diï¬erent cuts will have diï¬erent execution costs.
The challenge is, then,
â¢ to select an appropriate cut, C, of the hierarchy, H,
for query workload, Q, and
â¢ to pick, for each query qj â Q, a subset Cj â C for
processing qj ,
in such a way that these will minimize the overall processing
cost for the set of range queries in Q. Intuitively, we want to
include in the cut, those nodes that will not lead to a large
number of exclusions and cannot be cheaply constructed by
combining bitmaps of the leaf nodes using OR operations.

4.3.2 Cut Bitmap Selection Process
Given the above cut-based query processing model, in this
section we propose a cut selection algorithm consisting of
two steps: (a) a per-node cost estimation step and (b) a
bottom-up cut-node selection step. We next describe each
of these two steps.

Node Cost Estimation.
First, the process assigns an estimated cost to those hierarchy nodes that are relevant to the given query workload,
Q. For this, the algorithm traverses through the hierarchy,
H, in a top-down manner and identiï¬es part, R, of the hierarchy relevant for the execution of at least one query, q â Q
(i.e., for at least one query, q, the range associated with the
node and the query range intersect). Note that this process

1388

â¢ Exclusive leaf access plan (Line 18): If query, q, is executed using an exclusive leaf access plan at node, ni ,
this means that the result for the range (q.rs â© Si ) will
be obtained by using Bi and then identifying and excluding (using bitwise ANDNOT operations) all irrelevant leaf bitmaps under node ni . Thus, we compute
the exclusive leaf access plan cost, ecost(ni , q), of this
query at node ni as

Algorithm 2 Cost and Leaf Access Plan Assignment Algorithm
1: Input: Hierarchy H, Query Workload Q
2: Outputs: Query workload, Q(ni ), and cost estimate, costi ,

3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:

for each node, ni â H; leaf access plan, Ei,j , for all
node/query pairs ni â H and qj â Q(ni ); a set, R â IH ,
or relevant internal nodes
Initialize: R = â
procedure Cost and LeafAccessPlanAssignment
for each internal node ni â IH in top-down fashion do
if ni = âroot then
Q(ni ) = Q
else
Q(ni ) = {q â Q(parent(ni )) s.t. (q.rs â© Si ) =
â}
end if
if Q(ni ) = â then
add ni into R
end if
end for
for each node ni â R in a bottom-up fashion do
for qj â Q(ni ) do
Compute icost(ni , q)
Compute ecost(ni , q)
Compute the leaf access plan, Ei,j , as
Ei,j = [ecost(ni , qj ) < icost(ni , qj )]
end for

 Compute the leaf access cost, leaf costi , as
qj âQ(ni ) Ei,j Ã ecost(ni , qj ) + (1 â Ei,j ) Ã icost(ni , qj )
end for
end procedure

also converts the range in 2-D space into 1-D space by identifying the relevant nodes in the hierarchy. Next, for each
internal node, ni â R, a cost, costi , is estimated assuming
that this node and its leaf descendants are used for identifying the matches in the range Si . The outline of this process
is presented in Algorithm 2 and is detailed below:
â¢ Top-Down Traversal and Pruning. Line 5 indicates that
the process starts at the root and moves towards the leaves.
For each internal node, ni , being visited, ï¬rst, the set,
Q(ni ) â Q, of queries for which ni is relevant is identiï¬ed by
intersecting the ranges of the queries relevant to the parent
(i.e., Q(parent(ni ))) with the range of ni . More speciï¬cally,
Q(ni ) = {q â Q(parent(ni )) s.t. (q.rs â© Si ) = â}.
If Q(ni ) = â, then ni and all its descendants are ignored,
otherwise ni is included in the set R.
â¢ Inclusive and Exclusive Cost Computation. Once the portion, R, of the hierarchy relevant to the query workload is
identiï¬ed, next, the algorithm re-visits all internal nodes in
R in a bottom-up manner and computes a cost estimate for
executing queries in Q(ni ): for each query, q â Q(ni ), the
algorithm computes inclusive and exclusive leaf access costs:
â¢ Inclusive leaf access plan (Line 17): If query, q, is executed using an inclusive plan at node, ni , this means
that the result for the range (q.rsâ©Si ) will be obtained
by identifying and combining (using bitwise ORs) all
relevant leaf bitmaps under node ni . Therefore, the
cost of this leaf access plan is

icost(ni , q) =
size(Bj ).
(nj âleaf Desc(ni ))â§((q.rsâ©Sj )=â)

This value can be computed incrementally, simply by
summing up the inclusive costs of the children of ni .

ecost(ni , q)

= size(Bi )
+



size(Bj )

(nj âleaf Desc(ni ))â§((q.rsâ©Sj )=â)

or equivalently as
ecost(ni , q)

â

= size(Bi ) + â



â
size(Bj )â 

nj âleaf Desc(ni )

â icost(ni , q)
Since the initial two terms above are recorded in the
index creation time, the computation of exclusive cost
is a constant time operation.
â¢ Overall Cost Estimation and the Leaf Access Plan. Given
the above, we can ï¬nd the best strategy for processing the
query set Q(ni ) at node ni by considering the overall estimated cost term, cost(ni , Q(ni )), deï¬ned as
â
â

â
Ei,j Ã ecost(ni , qj ) + (1 â Ei,j ) Ã icost(ni , qj )â 



qj âQ(ni )





leaf access cost f or all relevant queries

where Ei,j = 1 means an exclusive leaf access plan is chosen
for query, qj , at this node and Ei,j = 0 otherwise.

Cut Bitmap Selection.
Once the nodes in the hierarchy are assigned estimated
costs as described above, the cut that will be used for
query processing is found by traversing the hierarchy in a
bottom-up fashion and picking nodes based on their estimated costs4 . The process is outlined in Algorithm 3. Intuitively, for each internal node, ni â IH , the algorithm computes a revised cost estimate, rcosti , by comparing the cost,
costi , estimated in the earlier phase of the process, with the
total revised costs of ni âs children:
â¢ In Line 13, the function f indBlockIO(ni ) returns the
cost of reading the block T (Bi ). If this block has already been marked âto-readâ, then the reading cost has
already been accounted for, so the cost is zero. Otherwise, the cost is equal to the size of the block T (Bi ),
as explained in Section 4.2.1.
â¢ As we see in Line 21, it is possible that a block T is
ï¬rst marked âto-readâ and then, later in the process,
marked ânot-to-readâ, because for the corresponding
nodes in the cut, more suitable ancestors are found
and the block is no longer needed.
â¢ If costi is smaller (Line 17), then ni and its leaf descendants can be used for identifying the matches to
the queries in the range Si . In this case, no revision is
4

Note that this bottom-up traversal can be combined with
the bottom-up traversal of the prior phase. We are describing them as separate processes for clarity.

1389

Algorithm 3 Cut Selection Algorithm
1: Input: Hierarchy H; per-node query workload Q(ni ); per-

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:

node cost estimates costi ; and the corresponding leaf access
plans, Ei,j , for node/query pairs ni â H and qj â Q(ni ); the
set, R â IH , or relevant internal nodes
Output: All-inclusive, CI , and Exclusive, CE , cut nodes
Initialize: Cand = â
procedure findCut
for each relevant internal node ni in R in a bottomup fashion do
Set internal children = children(ni ) â© IH ;
if internal children = â then
add ni to Cand;
rcosti = costi
else

costChildren = nj âinternal children rcostj
rcostIOi = f indBlockIO(ni )
for each child nj in internal children do
costChildrenIO
=
costChildrenIO +
f indBlockIO(nj )
end for
if (rcosti + rcostIOi ) â¤ (costChildren +
costChildrenIO) then
for each descendant nk of ni in Cand do
remove nk from Cand;
if nk is the only node to read from
T (Bk ) then
mark T (Bk ) as ânot-to-readâ;
end if
end for
add ni to Cand;
rcosti = costi
mark T (Bi ) as âto-readâ;
else
rcosti = costChildren
end if
end if
end for
CE = {ni â Cand s.t. âqj âQ(ni ) Ei,j == 1}
CI = Cand/CE
end procedure

answered only by accessing relevant leaves under the nodes
in CI . We store the blocks containing the bitmaps of these
relevant leaves in an LRU-based cache so that leaf bitmaps
can be reused by multiple queries.

4.3.3 Complexity
The bitmap selection process consists of two steps: (a) a
per-node cost estimation step and (b) a cut bitmap selection
step. Each of these steps visit only the relevant nodes of the
hierarchy. Therefore, if we denote the set of nodes of the
hierarchy, H, that intersect with any query in Q, as H(Q),
then the overall work is linear in the size of H(Q).
During the cost estimation phase, for each visited node,
ni , an inclusive and exclusive cost is estimated for any query
that intersects with this node. Therefore, the worst case
time cost of the overall process (assuming that all queries in
Q intersect with all nodes in H(Q)) is O(|Q| Ã |H(Q)|).

5. EXPERIMENTAL EVALUATION
In this section, we evaluate the eï¬ectiveness of the proposed compressed spatial hierarchical bitmap (cSHB) index
structure using spatial data sets with diï¬erent characteristics, under diï¬erent system parameters. To assess the eï¬ectiveness of cSHB, we also compare it against alternatives.
We ran the experiments on a quad-core Intel Core i5-2400
CPU @ 3.10GHz machine with 8.00GB RAM, and a 3TB
SATA Hard Drive with 7200 RPM and 64MB Buï¬er Size,
and in the same Windows 7 environment. All codes were
implemented and run using Java v1.7.

5.1 Alternative Spatial Index Structures and
the Details of the cSHB Implementation
As alternatives to cSHB, we considered diï¬erent systems
operating based on diï¬erent spatial indexing paradigms. In
particular, we considered spatial extensions of PostgreSQL
called PostGIS [2], of a widely used commercial DBMS
(which we refer to as DBMS-X), and of Lucene [1]:
â¢ PostGIS [2] creates spatial index structures using an
R-tree index implemented on top of GiST.

necessary and the revised cost, rcosti is equal to costi .
Any descendants of ni are removed from the set, Cand,
of cut candidates and ni is inserted instead.
â¢ If, on the other hand, the total revised cost of ni âs children is smaller than costi , then matches to the queries
in the range Si can be more cheaply identiï¬ed by considering the descendants of ni , rather than ni itself
(Line 27). Consequently, in this case, the revised cost,
rcosti , is set to

rcostj .
rcosti =

â¢ DBMS-X maps 2D space into a 1D space using a variation of Hilbert space ï¬lling curve and then indexes
the data using B-trees.
â¢ Apache Lucene [1,18], a leading system for text indexing and search, provides a spatial module that supports
geo-spatial range queries in 2D space using quadtrees
and preï¬x-based indexing. Intuitively, the space is partitioned using a MX-quadtree structure (where all the
leaves are at the same level and a given region is always
partitioned to its quadrants at the center [28]) and each
root-to-leaf path is given a unique path-string. These
path-strings are then indexed (using eï¬cient preï¬xindexing algorithms) for spatial query processing.

nj âchildren(ni )

As we experimentally show in Section 5, the above process
has a small cost. This is primarily because, during bottomup traversal, only those nodes that have not been pruned
in the previous top-down phase are considered. Once the
traversal is over, the nodes in the set, Cand, of cut candidates are reconsidered and those that include exclusive leaf
access plans are included in the exclusive cut set, CE , and
the rest are included in the all-inclusive cut set, CI .

Since database systems potentially have overheads beyond
pure query processing needs, we also considered disk-based
implementations of R*-tree [8] and the Hilbert R-tree [19].
For this purpose, we used the popular XXL Java library [10]:
â¢ A packed R*-tree, with average leaf node utilization
â¼ 95% (page size 4MB).

Caching of Cut and Leaf Bitmaps.
During query execution, the bitmaps of the nodes in CE
are read into a cut bitmaps buï¬er, whereas the bitmaps for
the nodes in CI do not need to be read as the queries will be

â¢ A packed Hilbert R-tree, with average leaf node utilization â¼ 99% (page size 4MB).

1390

Table 1: Data sets and clustering
Data set

#points

Synthetic (Uniform)
Gowalla (Clustered)
OSM (Clustered)

100M
6.4M
688M

Data
set
Synthetic
Gowalla
OSM

Data
set
Synthetic
Gowalla
OSM

Clustered (6.4M; Gowalla)

25

Clustered (688M; OSM)

y = -2.0x + 15.0

Linear (Uniform (100M; Synth))

20

Linear (Clustered (6.4M; Gowalla))

y = -1.7x + 13.3

15

cSHB

Luc.

1601
24
2869

2396
114
12027

DBMS
-X
3865
232
30002

Post
GIS
4606
112
76238

R*tree
2160
22
18466

Hilb.
R-tree
2139
20
17511

Table 4: Index Size on Disk (MB)

Uniform (100M; Synth)

Data Skew
log (# of non-empty cells)

Table 3: Index Creation Time (sec.)

#points per (nonempty) cell (h = 10)
Min.
Avg.
Max.
54
95
143
1
352
312944
1
3422
1.2M

Linear (Clustered (688M; OSM))

cSHB

Luc.

10900
44
2440

5190
220
22200

DBMS
-X
1882
121
12959

Post
GIS
8076
600
61440

R*tree
3210
211
22100

Hilb.
R-tree
1510
100
10400

y = -1.3x + 11.1

10
5
0
-5

-3

-1

1

3

5

7

log (r)

Figure 6: Data Skew
Table 2: Parameters and default values (in bold)
Parameter
Block Size (MB)
Query range size
|Q|
h
Buï¬er size (MB)

Value range
0.5; 1; 2.5; 5; 10
0.5% 1%; 5%
100; 500; 1000
9; 10; 11
2; 3; 5; 10; 20; 100

We also implemented the proposed cSHB index structure on
top of Lucene. In particular, we used the MX-quadtree hierarchy created by Lucene as the spatial hierarchy for building cSHB. We also leveraged Luceneâs (Java-based) region
comparison libraries to implement range searches. The compressed bitmaps and compressed domain logical operations
were implemented using the JavaEWAH library [21]. Due
to space limitations, we only present results with the incremental strategy for query evaluation.

5.2 Data Sets
For our experiments, we used three data sets:
(a) a uniformly distributed data set that consists of 100
million synthetically generated data points.
These
points are mapped to the range â180, â90 to 180, 90,
(b) a clustered data set from Gowalla, which contains the
locations of check-ins made by users. This data set is downloaded from the Standford Large Network Dataset Collection [4], and (c) a clustered data set from OpenStreetMap
(OSM) [3] which contains locations of diï¬erent entities distributed across North America. The OSM data set consists
of approximately 688 million data points in North America. We also normalized both the real data sets to the range
â180, â90 to 180, 90. In order to obtain a fair comparison across all index structures and the data sets, all three
data sets are mapped onto a 2h Ã 2h space and the positions
of the points in this space are used for indexing. Table 1
provides an overview of the characteristics of these three
very diï¬erent data sets. Figure 6 re-conï¬rms the data skew
in the three data sets using the box-counting method proposed in [9]: in the ï¬gure, the lower the negative slope, the
more skewed the data. The ï¬gure shows that the clustered
Gowalla data set has the largest skew.

5.3 Evaluation Criteria and Parameters
We evaluate the eï¬ectiveness of the proposed compressed
spatial hierarchical bitmap (cSHB) index structure by com-

paring its (a) index creation time, (b) index size, and (c)
query processing time to those of the alternative index structures described above under diï¬erent parameter settings.
Table 2 describes the parameters considered in these experiments and the default parameter settings.
Since our goal is to assess the contribution of the index in
the cost of the query plans, all index structures in our comparison used index-only query plans. More speciï¬cally, we
executed a count(â) query and conï¬gured the index structures such that only the index is used to identify the relevant
entries and count them to return the results. Consequently,
only the index ï¬les are used and data ï¬les are not accessed.
Note that all considered index structures accept squareshaped query ranges. The range sizes indicated in Table 2
are the lengths of the boundaries relative to the size of the
considered 2D space. These query ranges in the query workloads are generated uniformly.

5.4 Discussion of the Indexing Results
Indexing Time. Table 3 shows the index creation times for
diï¬erent systems and index structures, for diï¬erent data sets
(with diï¬erent sizes and uniformity): cSHB index creation
is fastest for the larger Synthetic and OSM data sets, and
competitive for the smaller Gowalla data set. As the data
size gets larger, the alternative index structures become signiï¬cantly slower, whereas cSHB is minimally aï¬ected by the
increase in data size. The index creation time also includes
the time spent on creating the hierarchy for cSHB.
Index Size. Table 4 shows the sizes of the resulting index ï¬les for diï¬erent systems and index structures and for
diï¬erent data sets. As we see here, cSHB provides a competitive index size for uniform data (where compression is not
very eï¬ective). On the other hand, on clustered data, cSHB
provides very signiï¬cant gains in index size â in fact, even
though the clustered data set, OSM, contains more points,
cSHB requires less space for indexing this data set than it
does for indexing the uniform data set.
Impact of Block Size. As we discussed in Section 3.2.2,
cSHB writes data on the disk in a blocked manner. In Figure 7, we see the impact of the block sizes on the time needed
to create the bitmaps. As we see here, one advantage of using blocked storage is that the larger the blocks used, the
faster the index creation becomes.

5.5 Discussion of the Search Results
Impact of the Search Range. Table 5 shows the impact
of the query range on search times for 500 queries under
the default parameter settings, for diï¬erent systems. As we
expected, as the search range increases, the execution time

1391

Impact of the Block Size on
Index Creation Time

Writing Bitmaps

Time (sec.)

Time (sec., log. scale)

Creating Bitmaps

1800

1350
900
450
0
0.5

1

2.5

Read Hierarchy
Map Ranges
Search Cut Bitmaps
Read Bitmaps
Combine Bitmaps

5

1000
100

Tot: 42

Tot: 51

0.5%
1%
5%

35
42
137

123
131
187

0.5%
1%
5%

2
3
3

2
3
48

0.5%
1%
5%

13
15
28

23
30
66

1

90
Time (sec.)

cSHB
-LO

60

2
3
5
13
14
78

Tot: 22

Tot: 11

1

5%

Synthetic (Uniform; 100M)

0.5%

1%

5%

Gowalla (Clustered; 6.4M)

2.5
Block Size (MB)

1%

cSHB Time Breakdown
(1% Q. Range, Uniform Data)
Tot: 32

Data sets

Figure 8: cSHB execution breakdown
becomes larger for all alternatives. However, cSHB provides
the best performance for all ranges considered, especially for
the clustered data sets. Here, we also compare cSHB with
its leaf-only version (called cSHB-LO), where instead of a
cut consisting of potentially internal nodes, we only choose
the leaf nodes for query processing. As you can see from the
ï¬gure, while cSHB-LO is a good option for very small query
ranges (0.5% and 1%), it becomes very slow as the query
range increases (since the number of bitwise operations increases, and it is not able to beneï¬t from clustering).
Execution Time Breakdown. Figure 8 provides a breakdown of the various components of cSHB index search (for
500 queries under the default parameter settings): The
bitmap selection algorithm presented in Section 4.3 is extremely fast. In fact, the most signiï¬cant components of
the execution are the times needed for reading the hierarchy
into memory5 , and for fetching the selected bitmaps from
the disk into the buï¬er, and performing bitwise operations
on them. As expected, this component sees a major increase
5
Once a hierarchy is read into the memory, the hierarchy
does not need to be re-read for the following queries.

Tot: 42

Read Hierarchy
Map Ranges
Search Cut Bitmaps
Read Bitmaps
Combine Bitmaps

Tot: 61

10
1

0.1
500

1000

Number of Queries

5%

OSM (Clustered;
688M)

10

Figure 9: Impact of the block size (500 queries,
1% q. range, uniform data)

100

0.5%

5

(b) Impact of block size on bitmap reading time

100

1%

Tot: 55

Tot: 31
30

0.5

0.1
0.5%

Tot: 84

0

Time (sec., log. scale)

Time (sec., log. scale)

Impact of Block Sizes on
Bitmap Reading Time

52
59
1700

Read Hierarchy
Map Ranges
Search Node Bitmaps
Read Bitmaps
Combine Bitmaps

1

10

(a) Impact of block size on overall cSHB execution time

Range Search Times (500 Queries)

10

5

Block Size (MB)

DBMS
Post
R*Hilb.
-X
GIS
tree
R-tree
Synthetic (Uniform; 100M)
414
12887
2211
4391
345
28736
2329
4480
368
72005
2535
4881
Gowalla (Clustered; 6.4M)
24
19
8
24
29
34
11
26
37
194
20
45
OSM (Clustered; 688M)
303
1129
3486
4368
645
4117
3889
5599
15567
18172
4626
6402

100

2.5

Tot: 141

1

0.5

Table 5: Comparison of search times for alternative
schemes and impact of the search range on the time
to execute 500 range queries (sec.)
Luc.

Tot: 113

0.1

10

Figure 7: Impact of the block size on index creation
time of cSHB (uniform data set)

cSHB

Tot: 65

10

Block Size (MB)

Range

Impact of Block Sizes
on cSHB Time

Figure 10: Impact of the number of queries on the
execution time of cSHB (1% q. range, uniform data)
as the search range grows, whereas the other costs are more
or less independent of the sizes of the query ranges.
Impact of the Block Sizes. As we see above, reading
bitmaps from the disk and operating on them is a major
part of cSHB query execution cost; therefore these need to
be performed as eï¬ciently as possible. As we discussed in
Section 3.2.2, cSHB reads data from the disk in a blocked
manner. In Figure 9, we see the impact of the block sizes on
the execution time of cSHB, including the time needed to
read bitmaps from the disk. As we see here, small blocks are
disadvantageous (due to the directory management overhead
they cause). Very large blocks are also disadvantageous as,
the larger the block gets, the larger becomes the amount of
redundant data read for each block access. As we see in the
ï¬gure, for the conï¬guration considered in the experiments,
1MB blocks provided the best execution time.
Impact of the Number of Queries in the Workload.
Figure 10 shows the total execution times as well as the
breakdown of the execution times for cSHB for diï¬erent
number of simultaneously executing queries. While the total
execution time increases with the number of simultaneous
queries, the increase is sub-linear, indicating that there are
savings due to the shared processing across these queries.

1392

Read Hierarchy
Map Ranges
Search Cut Bitmaps
Read Bitmaps
Combine Bitmaps

cSHB Time Breakdown (500 Queries,
1% Q. Range, Uniform Data)
Time (sec., log. scale)

1000
100

Tot: 152

Tot: 42

Tot: 28

10
1
0.1
9

10
# of levels of the hierarchy

11

Figure 11: Impact of the depth of the hierarchy (500
queries, 1% query range, uniform data)
Table 6: Working set size in terms of 1MB blocks
Q.Range (on 100M data)
0.5%
1%
5%

Min
1
1
1

Avg.
2.82
2.51
1.02

Max.
36
178
95

Table 7: Impact of the buï¬er size on exec. time (in
seconds, for 500 queries, 100M data)
Query
Range
0.5%
1%
5%

2MB
11.8
24.2
823.8

3MB
11.3
19.1
399.9

Buï¬er Size
5MB
10MB
10.9
10.6
18.1
17.5
155.9
105.8

20MB
10.5
17.3
101.6

100MB
10.2
16.3
94.9

Also, in Section 4.2.2, we had observed that the number of
queries in the query workload and query ranges determine
the relative costs of in-buï¬er operations vs. disk I/O. In
Figures 8 and 10, we see that this is indeed the case.
Impact of the Depth of the Hierarchy. Figure 11 shows
the impact of the hierarchy depth on the execution time of
cSHB: a 4Ã increase in the number of cells in the space (due
to a 1-level increase in the number of levels of the hierarchy)
results in < 4Ã increase in the execution time. Most signiï¬cant contributors to this increase are the time needed to
read the hierarchy and the time for bitmap operations.
Impact of the Cache Buï¬er. As we discussed in Section 4.2.2, the incremental scheduling algorithm keeps a
buï¬er of blocks containing the working set of leaf bitmaps.
As Table 6 shows, the average size of the working set is
fairly small and can easily ï¬t into the L3 caches of modern
hardware. Table 7 conï¬rms that a small buï¬er, moderately
larger than the average working set size, is suï¬cient and
larger buï¬ers do not provide signiï¬cant gains.

6.

CONCLUSIONS

In this paper, we argued that bitmap-based indexing can
be highly eï¬ective for running range query workloads on
spatial data sets. We introduced a novel compressed spatial hierarchical bitmap (cSHB) index structure that takes
a spatial hierarchy and uses that to create a hierarchy
of compressed bitmaps to support spatial range queries.
Queries are processed on cSHB index structure by selecting a relevant subset of the bitmaps and performing
compressed-domain bitwise logical operations. We also developed bitmap selection algorithms that identify the subset
of the bitmap ï¬les in this hierarchy for processing a given
spatial range query workload. Experiments showed that the
proposed cSHB index structure is highly eï¬cient in supporting spatial range query workloads. Our future work will
include implementing and evaluating cSHB for data with
more than two dimensions.

7. REFERENCES
[1] Apache Lucene. http://lucene.apache.org/core/4 6 0/
spatial/org/apache/lucene/spatial/preï¬x/tree/
SpatialPreï¬xTree.html
[2] Using PostGIS: Data Management and Queries.
http://postgis.net/docs/using postgis dbmanagement.html
[3] OpenStreetMap. http://www.openstreetmap.org/
[4] J. Leskovec and A. Krevl. SNAP Datasets: Stanford Large
Network Dataset Collection. http://snap.stanford.edu/data
[5] D. Abadi et al. Compression and Execution in
Column-Oriented Database systems. SIGMOD 2006.
[6] A. Aji et al. Hadoop-GIS: A High Performance Spatial Data
Warehousing System over MapReduce. PVLDB 2013.
[7] Rudolf Bayer. The Universal B-tree for Multidimensional
Indexing: General concepts. WWCA 1997.
[8] N. Beckmann et al. The R*-tree: An Eï¬cient and Robust
Access Method for Points and Rectangles. SIGMOD 1990.
[9] A. Belussi and C. Faloutsos. Self-Spatial Join Selectivity
Estimation Using Fractal Concepts. TOIS 1998.
[10] J.Bercken et al. XXL - A Library Approach to Supporting
Eï¬cient Implementations of Advanced Database Queries.
VLDB 2001.
[11] A.R. Butz. Alternative Algorithm for Hilbertâs Space-Filling
Curve. TOC 1971.
[12] A. Cary et al. Experiences on Processing Spatial Data with
MapReduce. SSDBM 2009.
[13] J. Chmiel et al. Dimension Hierarchies by means of
Hierarchically Organized Bitmaps. DOLAP 2010.
[14] P. Chovanec and M. KraÌtkyÌ. On the Eï¬ciency of Multiple
Range Query Processing in Multidimensional Data Structures.
IDEAS 2013.
[15] F. DelieÌge and T. Pedersen. Position List Word Aligned
Hybrid: Optimizing Space and Performance for Compressed
Bitmaps. EDBT 2010.
[16] L.Gosink et al. HDF5-FastQuery: Accelerating Complex
Queries on HDF Datasets using Fast Bitmap Indices.
SSDM 2006.
[17] David Hilbert. Ueber stetige abbildung einer linie auf ein
ï¬achenstuck. Mathematische Annalen 1891.
[18] Y. Jing et al. An Empirical Study on Performance Comparison
of Lucene and Relational Database. ICCSN 2009.
[19] I. Kamel and C. Faloutsos. Hilbert R-tree: An Improved R-tree
using Fractals. VLDB 1994
[20] O. Kaser et al. Histogram-Aware Sorting for Enhanced
Word-Aligned Compression in Bitmap Indexes. DOLAP 2008.
[21] D. Lemire et al. Sorting improves Word-Aligned Bitmap
Indexes. DKE 2010.
[22] V. Markl et al. Improving OLAP Performance by
Multidimensional Hierarchical Clustering. IDEAS 1999.
[23] G. Morton. A Computer Oriented Geodetic Data Base and a
New Technique in File Sequencing. IBM 1966.
[24] M. Morzy et al. Scalable Indexing Technique for Set-Valued
Attributes. ADBIS 2003.
[25] P. Nagarkar and K. S. Candan. HCS:Hierarchical Cut Selection
for Eï¬ciently Processing Queries on Data Columns using
Hierarchical Bitmap Indices. EDBT 2014.
[26] M. A. Olma et al. BLOCK: Eï¬cient Execution of Spatial
Range Queries in Main-Memory. Technical report EPFL 2013.
[27] A. N. Papadopoulos and Y. Manolopoulos. Multiple Range
Query Optimization in Spatial Databases. ADBIS 1998.
[28] H. Samet. Foundations of Multidimensional and Metric Data
Structures, 2005.
[29] R. Sinha and M. Winslett. Multi-Resolution Bitmap Indexes for
Scientiï¬c Data. TODS 2007.
[30] T. Siqueira et al. The SB-index and the HSB-index: Eï¬cient
Indices for Spatial Data Warehouses. Geoinformatica 2012.
[31] T. Skopal et al. Algorithm for Universal B-trees. Inf. Syst. 2006.
[32] K. Wu et al. On the Performance of Bitmap Indices for High
Cardinality Attributes. VLDB 2004.
[33] K. Wu et al. An Eï¬cient Compression Scheme for Bitmap
Indices. TODS 2004.
[34] M. Zaker et al. An Adequate Design for Large Data Warehouse
Systems: Bitmap Index versus B-tree Index. IJCC 2008.
[35] Y. Zhong et al. Towards Parallel Spatial Query Processing for
Big Spatial Data. IPDPSW 2012.

1393

Decomposition-by-Normalization (DBN): Leveraging
Approximate Functional Dependencies for Efficient Tensor
Decompositionâ
Mijung Kim

K. SelÃ§uk Candan

Arizona State University
Tempe, AZ 85287, USA

Arizona State University
Tempe, AZ 85287, USA

mijung.kim.1@asu.edu

candan@asu.edu

ABSTRACT

1.

For many multi-dimensional data applications, tensor operations as well as relational operations need to be supported
throughout the data lifecycle. Although tensor decomposition is shown to be effective for multi-dimensional data analysis, the cost of tensor decomposition is often very high. We
propose a novel decomposition-by-normalization scheme
that first normalizes the given relation into smaller tensors based on the functional dependencies of the relation and then performs the decomposition using these
smaller tensors. The decomposition and recombination steps
of the decomposition-by-normalization scheme fit naturally in settings with multiple cores. This leads to a
highly efficient, effective, and parallelized decompositionby-normalization algorithm for both dense and sparse tensors. Experiments confirm the efficiency and effectiveness
of the proposed decomposition-by-normalization scheme
compared to the conventional nonnegative CP decomposition approach.

For many multi-dimensional data applications, tensor operations as well as relational operations need to be supported
throughout the data lifecycle (from collection to analysis).
Tensors are high-dimensional arrays. Due to the convenience
they provide when representing relationships among different types of entities, tensor representation has been increasingly used for relational data in various fields from scientific
data management to social network data analysis. Tensor
based representations have proven to be useful for multiaspect data analysis and tensor decomposition has been an
important tool to capture high-order structures in multidimensional data [2, 11, 12, 17, 20, 23].
Although tensor decomposition is shown to be effective for
multi-dimensional data analysis, the cost of tensor decomposition is often very high, especially in dense tensor representations where the cost increases exponentially with the
number of modes of the tensor. While decomposition cost increases more slowly (linearly with the number of nonzero entries in the tensor) for sparse tensors, the operation can still
be very expensive for large data sets. In fact, in many data
intensive systems, data is commonly high-dimensional and
large-scale and, consequently, of all the operations that need
to be supported to manage the data, tensor decompositions
tend to be the costliest ones. Recent attempts to parellelize
tensor decomposition [2, 17, 23] face difficulties, including
large synchronization and data exchange overheads.

Categories and Subject Descriptors
H.2.4 [Database Management]: SystemsâQuery processing, Relational databases; H.3.3 [Information Storage
and Retrieval]: Information Search and RetrievalâClustering

General Terms
Algorithms, Experimentation

Keywords
Tensor Decomposition, Tensor-based Relational Data Model
âThis work is supported by the NSF Grant #1043583 âMiNC: NSDL Middleware for Network- and Context-aware
Recommendationsâ and the NSF Grant #1116394 âRanKloud: Data Partitioning and Resource Allocation Strategies for Scalable Multimedia and Social Media Analysisâ

Permission to make digital or hard copies of all or part of this work for
personal or classroom use is granted without fee provided that copies are
not made or distributed for profit or commercial advantage and that copies
bear this notice and the full citation on the first page. To copy otherwise, to
republish, to post on servers or to redistribute to lists, requires prior specific
permission and/or a fee.
CIKMâ12, October 29âNovember 2, 2012, Maui, HI, USA.
Copyright 2012 ACM 978-1-4503-1156-4/12/10 ...$15.00.

INTRODUCTION

1.1

Contribution
of
this
Paper:
Decomposition-by-Normalization (DBN)

Our goal is to tackle the high computational cost of tensor
decomposition process. Since, especially for dense data sets,
the number of modes of the tensor data is one of the main
factors contributing to the cost of tensor operations, we focus
on how we reduce the dimensionality and the size of the
input tensor. In particular, we argue that if
â¢ a high-dimensional data set (i.e., a tensor with large
number of modes) can be normalized (i.e., vertically
partitioned) into lower-dimensional data sets (i.e., tensors with smaller number of modes) and
â¢ each vertical partition (sub-tensor) is decomposed independently,
then the resulting partial decompositions can be efficiently
combined to obtain the decomposition of the original data
set (tensor). We refer to this as the decomposition-bynormalization (DBN) scheme.

ID

occupation

income

Self-emp-not-inc

workclass

Masters

1

Execmanagerial

>50K

Never-worked

Bachelors

2

-

<=50K

Private

Some-college

3

Sales

>50K

!

!

!

!

!

workclass

education

education

ID

Self-emp-not-inc

Masters

1

Never-worked

Bachelors

2

Private

Some-college

3

!

!

!

ID

occupation

1

Execmanagerial

income

2

-

<=50K

3

Sales

>50K

!

!

!

>50K

(a) Normalization
occupation
education ID

education
ID

occupation
occupation

workclass

+!+

ID
workclass

workclass

â

workclass

â

workclass

â
+!+

occupation

ID

+!+

(b) decomposition-by-normalization (DBN) process
Figure 1: (a) Normalization of a relation R(workclass, education, ID, occupation, income) into two relations R1 (workclass,
education, ID) and R2 (ID, occupation, income) based on the key
(ID); (b) decomposition-by-normalization:

normalization of R

into R1 and R2 and rank-r1 CP decomposition of R1 and
rank-r2 CP decomposition of R2 that are combined on the ID
mode into rank-r CP decomposition of R.

Example 1.1. Consider
the
5-attribute
relation,
R(workclass, education, ID, occupation, income), in
Figure 1(a) and assume that we want to decompose the
corresponding tensor for multi-dimensional analysis.
Since high-dimensional tensor decomposition is expensive,
we argue that a better processing scheme would involve first
normalizing this relation into the smaller tensors (based on
the functional dependencies of the relation) and then performing the decompositions of these smaller tensors. These
normalized tensors are lower-dimensional than the original
input tensor, therefore these decompositions are likely to be
much less expensive than the decomposition of the original input tensor. Figure 1(a) illustrates an example normalization which divides the 5-attribute relations into two
smaller relations with 3 attributes, R1 (workclass, education, ID) and R2 (ID, occupation, income) respectively.
Figure 1(b) illustrates the proposed DBN scheme: Once
the two partitions are decomposed, we combine the resulting
core and factor matrices to obtain the decomposition of the
original tensor corresponding to the relation R.
In the above example, if the relation R is dense, we expect
that decompositions of relations R1 and R2 will be much
faster than that of the relation R and the gain will more
than compensate for the normalization cost of the initial
step and the combination cost of the last step of DBN.
If the relation R is sparse, on the other hand, the decomposition cost is not only determined by the number of
modes, but also the number of nonzero entries in the tensor.

As a result, unless the partitioning provides smaller number
of tuples for both relations, DBN may not provide sufficient savings to compensate the normalization and combination overheads. However, as we will experimentally verify
in Section 6, the decomposition and recombination steps of
the DBN scheme fit naturally in multiple cores. This leads
to a highly efficient, effective, and parallelized DBN strategy
under both dense and sparse tensors.
In general, a relation can be vertically partitioned into
two in many ways: we need to first select a join attribute
and then, we need to decide which attributes to include in
which vertical partition. The join attribute (ID in the above
example) around which we partition the data is important
for two different reasons. First of all, we need to ensure that
the join attribute is selected in such a way that the normalization (i.e., the vertical partitioning) process does not lead
to spurious tuples. Secondly, the join attribute need to partition the data in such a way that the later step in which decompositions of the individual partitions are combined into
an overall decomposition does not introduce errors.
Task 1.1. One way to prevent the normalization process from introducing spurious data is to select an attribute
which functionally determines the attributes that will be
moved to the second partition. This requires an efficient
method to determine functional dependencies in the data.
This is difficult because the total number of functional dependencies in the data can be exponential.
Task 1.2. A second difficulty is that many data sets
may not have perfect functional dependencies to leverage
for normalization. In that case, we need to identify and rely
on approximate functional dependencies in the data. In this
paper, we argue that we can rely on pairwise approximate
functional dependencies that are incomplete, yet very efficient to compute.
Task 1.3. Once the approximate functional dependencies
are identified, we need to partition the data into two partitions in such a way that will lead to least amount of errors
during later stages. In this paper, we argue that partitioning the attributes in a way that minimizes inter-partition
functional dependencies and maximizes intra-partition dependencies will lead to least amount of errors in recombination step. After data is vertically partitioned and individual
partitions are decomposed, the individual decompositions
need to be recombined to obtain the decomposition of the
original relation. This process needs to be done in a way
that is efficient and parallelizable.

1.2

Organization of the Paper

The organization of the paper is as follows:
â¢ We first provide the relevant background and discuss
the related works in Section 2.
â¢ We next provide an overview of the proposed DBN
scheme in Section 3.
â¢ We then focus on selecting the best partitions for the
normalization step of DBN (Section 4).
â¢ In Section 5, we present rank-pruning strategies to further reduce the cost of DBN.
â¢ Next, we experimentally evaluate DBN in Section 6
in both stand-alone and parallel configurations. We
focus on the accuracy and the running time of the alternative algorithms. Experimental results provide evidence that while being significantly faster, DBN can
approximate well the fitness of the conventional tensor
decomposition with respect to the original tensor.
We conclude the paper in Section 7.

2.

BACKGROUND AND RELATED WORK

Tensor Decomposition. The two most popular tensor decompositions are the Tucker [21] and the CANDECOMP/PARAFAC [7, 4] decompositions. The Tucker decomposition generalizes singular value matrix decomposition
(SVD) to higher-dimensional matrices. CANDECOMP [4]
and PARAFAC [7] decompositions (together known as the
CP decomposition) take a different approach and decompose
the input tensor into a sum of component rank-one tensors.
More specifically, the CP Decomposition of P is a weighted
sum of rank-one tensors as the following (an alternative way
to view the decomposition is in the form of a diagonal tensor
and a set of factor matrices of the input tensor):
PI1 ÃI2 ÃÂ·Â·Â·ÃIN â¼

r
X

(1)

Î»k â¦ Uk

(2)

â¦ Uk

(N )

â¦ Â· Â· Â· â¦ Uk

.

k=1

where Î» is a vector of size r and each U (n) is a matrix of
size In Ã r, for n = 1, Â· Â· Â· , N .
Many of the algorithms for decomposing tensors are based
on an iterative process that approximates the best solution
until a convergence condition is reached. The alternating
least squares (ALS) method is relatively old and has been
successfully applied to the problem of tensor decomposition [4, 7]. ALS estimates, at each iteration, one factor
matrix, maintaining other matrices fixed; this process is repeated for each factor matrix associated to the dimensions
of the input tensor.
Nonnegative Tensor Decomposition (NTF). Note that
these decompositions can be interpreted probabilistically, if
additional constraints (nonnegativity and summation to 1)
are imposed. In the case of the CP decomposition, for example, each nonzero element in the core and the values of
entries of the factor matrices can be thought of as a cluster
and the conditional probabilities of the entries given clusters,
respectively. The N-way Toolbox for MATLAB [1] provides
both CP and Tucker decomposition with nonnegativity constraints.
Scalable Tensor Decomposition. Tensor decomposition
is a costly process. In dense tensor representation where the
cost increases exponentially with the number of modes of
the tensor. While decomposition cost increases more slowly
(linearly with the number of nonzero entries in the tensor)
for sparse tensors, the operation can still be very expensive for large data sets due to the high computational cost
and memory requirement to build up the approximate tensor [17]. A modified ALS algorithm proposed in [17] computes Hadamard products instead of Khatri-Rao products
for efficient PARAFAC for large-scale tensors. Kolda et
al. [11] developed a greedy PARAFAC algorithm for largescale, sparse tensors in MATLAB.
The ALS in Tucker decompositions involves SVD which is
the most computationally challenging part [20]. To address
this, randomized sampling is used in [20]. MET decomposition proposed in [12] addresses intermediate blowup problem
in Tucker decomposition.
Parallel Tensor Decomposition. Phan and Cichocki [17]
proposed a modified ALS PARAFAC algorithm called grid
PARAFAC for large scale tensor data. The grid PARAFAC
divides a large tensor into sub-tensors that can be factorized using any available PARAFAC algorithm in a parallel
manner and iteratively combined into the final decomposi-

tion. The grid PARAFAC can be converted to grid NTF by
enforcing nonnegativity.
Zhang et al. [23] parallelized NTF for mining global climate data in such a way that the original 3-mode tensor is
divided into three semi-NMF sub-problems based on ALS
approach and these matrices are distributed to independent
processors to facilitate parallelization. Antikainen et al. [2]
presented an algorithm for NTF that is specialized for Compute Uniform Device Architecture (CUDA) framework that
provides a parallel running environment.
Note that since these block-based parallel algorithms are
based on an ALS approach where one variable can be optimized given that the other variables are fixed, the communication cost among each block is not avoidable. In the
parallelized DBN strategy, on the other hand, each block is
completely separable and run independently.
Functional Dependency (FD). A functional dependency
(FD) is a constraint between two sets of attributes X and
Y in a relation denoted by X â Y , which specifies that the
values of the X component of a tuple uniquely determine
the values of the Y component.
The discovery of FDs in a data set is a challenging problem
since the complexity increases exponentially in the number
of attributes [15]. Many algorithms for FD and approximate
FD discovery exist [8, 13, 15, 22]. TANE proposed in [8] used
the definition of approximate FDs based on the minimum
fraction of tuples that should be removed from the relation
to hold the exact FDs.
The computation of FDs in TANE and Dep-Miner [13] is
based on levelwise search [16]. Dep-Miner finds the minimal
FD cover of a hypergraph using a levelwise search. Similarly, FastFD [22] finds the minimal cover, however, differently from Dep-Miner, it uses a depth-first search to address
the problem in the levelwise search that the cost increases
exponentially in the number of attributes. The main factor
in the cost of FastFD is the input size. FastFD works well
when the number of attributes is large. TANE takes linear
time with respect to the size of the input whereas FastFD
takes more than linear time of the input size.
CORDS [9] generalized FDs to determine statistical dependencies, which is referred to as soft FD. In a soft FD, a
value of an attribute determines a value of another attribute
with high probability. CORDS only discovers pairwise correlations reducing a great amount of complexity that nevertheless can remove most of correlation-induced selectivity
error. In this paper, we also leverage pairwise FDs to measure dependency between partitions (interFD) and within a
partition (intraFD).

3.

DECOMPOSITION-BYNORMALIZATION (DBN)

As we discussed earlier, our goal in this paper is to
tackle the high computational cost of decomposition process.
Since, especially for dense data sets, the number of modes of
the tensor data is one of the main factors contributing to the
cost of tensor operations, we focus on how we reduce the dimensionality and the size of the input tensor. In particular,
we argue that a large relation can be vertically partitioned
into multiple relations relying on the approximate functional
dependencies in the data, and the decompositions of these
relations can be combined to obtain the overall decomposition. Without loss of generality, in this paper, we consider

2-way partitioning of the input data. The process can easily
be generalized for multiple partitions.
In this section, we provide an overview of this DBN process. We first introduce the relevant notations and provide
background on key concepts.

3.1

Background and Key Concepts

Without loss of generality, we assume that relations are
represented in the form of occurrence tensors.

3.1.1

Tensor Representation of Relational Data

Let A1 , . . . , An be a set of attributes in the schema of a
relation, R, and D1 , . . . , Dn be the attribute domains. Let
the relation instance R be a finite multi-set of tuples, where
each tuple t â D1 Ã . . . Ã Dn .
Definition 3.1 (Occurrence Tensor). An occurrence tensor Ro corresponding to the relation instance R
is an n-mode tensor, where each attribute A1 , . . . , An is
represented by a mode. For the ith mode, which corresponds
to Ai , let D0i â Di be the (finite) subset of the elements such
that
âv â D0i ât â R s.t. t.Ai = v
and let idx(v) denote the rank of v among the values in D0i
relative to an (arbitrary) total order, <i , defined over the
elements of the domain, Di . The cells of the occurrence
tensor Ro are such that
Ro [u1 , . . . , un ] = 1 â ât â R s.t.â1â¤jâ¤n idx(t.Aj ) = uj
and 0 otherwise. Intuitively, each cell indicates whether the
corresponding tuple exists in the multi-set corresponding to
the relation or not.

3.1.2

Normalization (Vertical Partitoning) and
Functional Dependencies

First introduced by Codd [5], the normalization process
of the relational model evaluates relations based on the underlying functional dependencies of the data and vertically
partitions a large relation into smaller relations (with lesser
number of attributes) to minimize redundancy and insertion,
deletion, and update anomalies.
Functional Dependency (FD). The functional dependency, between two sets of attributes X and Y, in a relational model is defined as the following.
Definition 3.2 (Functional dependency). A functional dependency (FD), denoted by X â Y, holds for relation instance R, if and only if for any two tuples t1 and t2
in R that have t1 [X ] = t2 [X ], t1 [Y] = t2 [Y] also holds.
The key idea of the normalization process is that if A =
{A1 , . . . , An } is a set of attributes in the schema of a relation, R, and X , Y â A are two subsets of attributes such that
X â Y, then the relation instance R can be vertically partitioned into two relation instances R1 , with attributes A \ Y,
and R2 , with attributes X âª Y, such that R = R1 1 R2 ; in
other words the set of attributes X serves as a foreign key
and joining vertical partitions R1 and R2 on X gives back
the relation instance R without any missing or spurious tuples. Note that, while for some data sets, FDs are known at
the database design time, for many data sets they need to
be discovered by analyzing the available data sets.
Approximate FD. As described above, the search for exact FDs is a costly process. Moreover, in many data sets,

attributes may not have perfect FDs due to exceptions and
outliers in the data. In such cases, we may search for approximate FDs instead of exact FDs.
Definition 3.3 (Approximate FD). An approximate
Ï
FD (aFD), denoted by X â Y holds for relation instance
R, if and only if
â¢ there is a subset R0 â R, such that |R0 | = Ï Ã |R| and,
for any two tuples t1 and t2 in R0 that have t1 [X ] =
t2 [X ], t1 [Y] = t2 [Y] also holds; and
â¢ there is no subset R00 â R, such that |R00 | > Ï Ã |R|
where the condition holds.
Ï

We refer to the value of Ï as the support of the aFD X â Y.
As described above, the concept of relational normalization (vertical partitioning) relies on the relational join operation. Since in this paper we represent relations as occurrence
tensors, we also need to define a corresponding tensor join
operation that operates in the domain of tensors.

3.1.3

Tensor Join

Let P and Q be two tensors, representing relation inP
stances P and Q, with attribute sets, AP = {AP
1 , . . . , An }
Q
Q
Q
and A = {A1 , . . . , Am }, respectively. In the rest of
this section, we denote the index of each cell of P as
(i1 , i2 , ..., in ); similarly, the index of each cell of Q is denoted
as (j1 , j2 , ..., jm ). The cell indexed as (i1 , . . . , in ) of P is denoted by P[i1 , . . . , in ] and the cell indexed as (j1 , . . . , jm ) of
Q is denoted by Q[j1 , . . . , jm ].
Definition 3.4 (Join (1)). In relational algebra,
given two relations P and Q, and a condition Ï, the join
operation is defined as a cartesian product of the input
relations followed by the selection operation. Therefore,
given two relational tensors P and Q, and a condition Ï,
we can define their join as
def

P 1Ï Q = ÏÏ (P Ã Q).
Given two relations P and Q, with attribute sets, AP =
P
Q
Q
{AP
= {AQ
1 , . . . , An } and A
1 , . . . , Am }, and a set of atP
Q
tributes A â A and A â A , the equi-join operation,
1=,A , is defined as the join operation, with the condition
that matching attributes in the two relations will have the
same values, followed by a projection operation that eliminates one instance of A from the resulting relation.

3.2

Overview of the Decomposition-byNormalization (DBN) Process

The pseudo-code of the DBN algorithm is shown in Figure 2. In this subsection, we provide an overview of the steps
of the algorithm. Then, in the following sections, we study
in detail the key steps of the process.
In its first step, DBN evaluates the pairwise FDs among
the attributes of the input relation. For this purpose, we
employ and extend TANE [8], an efficient algorithm for the
discovery of FDs. Our modification of the TANE algorithm
returns a set of (approximate) FDs between the attribute
pairs and, for each candidate dependency, Ai â Aj , it provides the corresponding support, Ïi,j .
The next steps involves selecting the attribute, X, that
will serve as the foreign key and partitioning the input relation R into R1 and R2 around X. Note that if the selected

â¢ Since for dense tensors, the complexity of the decomposition process is exponential in the number of
modes [11], we would prefer that the number of attributes in each partition should be balanced.

DBN algorithm (input: a relation R)
1: Evaluate pFD (pairwise FDs between all pairs
P of attributes of R)
2: Select the attribute Ak with the highest k6=j Ïk,j such that
Ïk,j â¥ Ïsupport as the vertical partitioning (and join) attribute
X (Desiderata 1 and 2)
3: if R is a sparse tensor then
4: if X (approximately) determines all attributes of R then
5:
findInterFDPartition(pFD,false) (see Figure 3)
6: else
7:
Move X and all attributes determined by X to R1 ; move X
and remaining attributes to R2 (Desideratum 4 and 5).
8: end if
9: else {i.e., R is a dense tensor}
10: if X (approximately) determines all attributes of R then
11:
findInterFDPartition(pFD,true)
12: else
13:
Move X and attributes determined by X to R1 ; move
X and remaining attributes to R2 â these moves are constrained such that the number of attributes of R1 and R2
are similar (Desideratum 3 and 5)
14: end if
15: end if
16: Partition R into R1 and R2 .
17: If the selected X does not perfectly determine the attributes of
R1 then remove sufficient number of outlier tuples from R to
enforce the FDs between X and the attributes of R1
18: Create occurrence tensors of R1 and R2
19: Decompose the tensors corresponding to R1 and R2 , join the decompositions to obtain candidate combined decompositions, and
select the one that is likely to provide the best fit to R

Figure 2:

â¢ For sparse tensors the number of modes impact the
decomposition cost only linearly. In fact, in this case,
the major contributor to the decomposition cost is the
number of nonzero entries in the tensor [11].
Desideratum 4: Therefore, for sparse tensors, the
vertical partitioning should be such that the total number of tuples of R1 and R2 are minimized.
â¢ Any information encoded with the FDs crossing relations R1 and R2 is risked to be lost when R1 and R2
are individually decomposed in the final step of the
DBN algorithm. This leads to our final desideratum:
Desideratum 5: The vertical partitioning should be
such that the support for the inter-partition FDs (except for the FDs involving the join attribute X) are
minimized.

Pseudo-code of DBN (see Section 4)

findInterFDPartition ( input: pFD, balanced)
1: Create a complete pairwise FD graph, where the nodes are attributes and edge weights are the support values of pFD.
2: if balanced == false then
3: Run minimum average cut on the pairwise FD graph to find a
maximally independent partitioning (Desideratum 5)
4: else {i.e., balanced == true}
5: Run minimum average cut on the pairwise FD graph to find a
balanced, maximally independent partitioning (Desideratum 3
and 5)
6: end if

Figure 3:

Desideratum 3: For dense tensors, the vertical partitioning should be such that the number of attributes
of R1 and R2 are similar.

Pseudo-code of interFD-based partition algo-

rithm; this is detailed in Section 4.3

join attribute X does not perfectly determine the attributes
of R1 , then to prevent introduction of spurious tuples, we
will need to remove sufficient number of outlier tuples from
R to enforce the FDs between the attribute, X, and the
attributes selected to be moved to R1 .
Desideratum 1: Therefore, to prevent over-thinning of
the relation R, the considered approximate FDs need to
have high support; i.e., Ïi,j â¥ Ïsupport , for a sufficiently
large support lower-bound, Ïsupport .
We next consider various criteria for vertical partitioning
of the input relation:
â¢ First of all, as we discussed earlier, when we vertically partition the relation R with attributes A =
{A1 , . . . , An } into R1 and R2 , one of the attributes
X of R2 should serve as a foreign key into R1 to ensure that joining vertical partitions R1 and R2 on X
gives back R without any missing or spurious tuples.
Desideratum 2: If A1 is the set of attributes of vertical partition R1 and A2 is the set of attributes of
vertical partition R2 , then there must be an attribute
Ï
X â A2 , such that for each attribute Y â A1 , X â Y ,
for Ï â¥ Ïsupport .

We elaborate on these desiderata and their implications on
the vertical partitioning of R into R1 and R2 in Section 4.
Once R1 and R2 are obtained, the final steps of the DBN
process include decomposition of the tensors corresponding
to R1 and R2 , joining of the decompositions to obtain candidate combined decompositions, and selecting the one that
is likely to provide the best fit to R. For these, we rely
on the join-by-decomposition scheme proposed in [10]:
in order to construct a rank-r decomposition of a joined
tensor, join-by-decomposition first finds all rank-r1 and
rank-r2 decompositions of the two input tensors, such that
r1 Ã r2 = r. The rank-r1 and rank-r2 decompositions of
the input decompositions are then combined along the given
factor matrix, which corresponds to the join attribute in the
equi-join operation. Finally the algorithm finds the r1 Ã r2
pair which provides the least approximation error, given all
possible factorizations, r1 Ã r2 of r. Note that join-bydecomposition involves creation of multiple alternative join
pairs (corresponding to different r1 Ã r2 factorizations of
r), which are independently evaluated for accuracy and the
one that is predicted to provide the best accuracy is used
for obtaining the final result. This provides a natural way
to parallelize the entire operation by associating each pair
of rank decompositions (and the computation of the corresponding pair selection measure) to a different processor
core. In Section 6, we also evaluate the applicability of such
parallelizations in the context of the DBN operation.

4.

VERTICAL PARTITIONING STRATEGIES

Based on the desiderata discussed in Section 3.2, here we
discuss the vertical partitioning strategies for sparse and
dense tensors. For each situation, we consider the cases:
(Case 1) the join attribute (approximately) determines a
subset of attributes and (Case 2) it determines all attributes
of the input relation.

4.1

Sparse Tensors

For the first case (i.e., the join attribute X (approximately) determines a subset of the attributes of R), we create a partition R1 with all the attributes determined with
a support higher than the threshold (Ïsupport ) by the join
attribute. This helps us satisfy Desiderata 1 and 2. The
second partition, R2 consists of the join attribute X and
all the remaining attributes. Since inter-partition FDs of
the form X â â are all less than Ïsupport , this also reflects
Desideratum 5.
Note that by construction, the size of R2 is equal to the
number of tuples in R independent of which attributes are
included in it. The size of R1 , on the other hand, can be
minimized (to satisfy Desideratum 4) down to the number
of unique values of X by eliminating all the duplicate tuples.
For the second case, where the join attribute X determines
all attributes of R, we apply the interFD-based vertical partitioning strategy detailed later in Section 4.3.

4.2

Dense Tensors

For the first case, similarly to sparse tensors, we create
partitions, R1 and R2 , by considering the FDs of the form
X â â, where X is the join attribute. Differently from the
sparse tensors, in this case we also consider Desideratum 3,
which prefers balanced partitions:
â¢ When there are fewer attributes with support higher
than Ïsupport , we move the attributes with the highest
support from R2 to R1 to promote balance. This is
equivalent to relaxing the support threshold.
â¢ When there are more attributes with support higher
than Ïsupport , we move the attributes with the lowest
support from R1 to R2 to promote balance. This is
equivalent to tightening the support threshold.
For the second case, as in the sparse tensors, we apply the
interFD-based vertical partitioning strategy detailed later in
Section 4.3. The major difference is that in this case, we also
promote balance.

4.3

Vertical Partitioning Based on Supports
of Attribute Pair Dependencies

Desideratum 5 implies that the vertical partitioning strategy should minimize inter-partition FDs. As discussed earlier, this helps minimize the likelihood of error when the
individual partitions are decomposed and the resulting decompositions are combined to obtain the overall decomposition. Kim and Candan [10] have shown that, given a join
operation, R = R1 1A R2 , it is possible to obtain a rank-r
decomposition of R by combining rank-r1 and rank-r2 decompositions of R1 and R2 , as long as r = r1 Ã r2 and that
rank r1 and r2 decompositions of the input tensors lead to
clusters that are independent relative to the join attribute,
A. Authors have also argued theoretically and experimentally that the accuracy of the decomposition is especially
high if the other attributes of the two relations R1 and R2
are independent from each other. Relying on this observation (which we also validate in Section 6), DBN tries to
partition the input relational tensor R in such a way that
the resulting partitions, R1 and R2 , are as independent from
each other as possible.
Remember that the support of an approximate FD is defined as the percentage of tuples in the data set for which

the FD holds. Thus, in order to quantify the dependence
of pairwise attributes, we rely on the supports of pairwise
FDs. Since we have two possible FDs (X â Y and Y â X)
for each pair of attributes, we use the average of the two
as the overall support of the pair of attributes X and Y .
Given these pairwise supports we approximate the overall
dependency between two partitions R1 and R2 using the
average support of the pairwise FDs (excluding the pairwise
FDs involving the join attribute) crossing the two partitions.
We refer to this as interFD and the partitioning based on
interFD as the interFD-based partitioning strategy.
We formulate this interFD-based partitioning problem as
a graph partitioning problem: Let the pairwise FD graph,
Gpf d (V, E), be a complete, weighted, and undirected graph,
where each vertex v â V represents an attribute and the
weights of the edge between nodes vi and vj is the average
support of the approximate FDs vi â vj and vj â vi . The
problem is then to locate a cut on Gpf d with the minimum
average weight.
This graph partitioning problem is similar to the minimum cut problem [19], with some key differences. The major difference is that we do not seek a cut with minimum
total weight, but a cut with minimum average weight. Also,
depending on whether we are operating on dense or sparse
networks, we may or may not seek to impose a balance criterion on the partitions: for sparse tensors, the cost of tensor
decomposition increases linearly with the number of modes
and since the total number of modes of the two partitions is
constant, we do not need to seek balance.
In DBN, we use a modified version of the minimum
cut algorithm proposed in [19] to seek minimum average
cuts. Given an undirected graph Gpf d (V, E), for each vertex v â V , the algorithm finds a vertex with the minimum
average cut that separates it from the rest of the graph and
0
creates a subset of vertices V where the vertex is merged
with its neighbor vertex, connected to the rest of the graph
with the least average weight; the edges from these two vertices are replaced by a new edge weighted by the average of
the weights of the original edges. The process is repeated
0
while V 6= V . The minimum of the minimum average cuts
at each step of the algorithm is returned as the overall minimum average cut. When balance of the number of attributes
is needed, the minimum is selected among the steps that
lead to similar number of attributes. The complexity of this
minimum average cut algorithm is O(|V ||E| + |V |2 log|V |).
Figure 4 shows an example of the process.

5.

RANK PRUNING BASED ON INTRAPARTITION DEPENDENCIES

Given a partitioning of R into of R1 and R2 , to obtain
a rank-r decomposition of R, we need to consider rank-r1
and rank-r2 decompositions of R1 and R2 , such that r =
r1 Ã r2 and pick the hr1 , r2 i pair which is likely to minimize
recombination errors.
We note, however, that we can rely on the supports of
the dependencies that make up the partitions R1 and R2
to prune hr1 , r2 i pairs which are not likely to give good fits.
In particular, we observe that the higher the overall dependency between the attributes that make up a partition, the
more likely the data in the partition can be described with a
smaller number of clusters. Since the number of clusters of
a data set is related to the rank of the decomposition, this
leads to the observation that

0.40

0.40
1

1

2

1

2

0.415

2,3
0.4575

0.44

0.43

0.44

0.43

2,3,4

1

1,2,3,4

0.45

0.45
4

3

0.445

3

4

4

(a) Gpf d (V, E)
(b) Step 1
(c) Step 2
Figure 4: Minimum average cut algorithm for (a) Gpf d (V, E). (b) Step 1:

(d) Step 3

(e) Step 4

a cut between {2} and {1,3,4} is the minimum average
cut (0.44). (c) Step 2: vertex 2 is merged with vertex 3 which has the minimum average weight to the rest of the vertices and
a cut between {2,3} and {1,4} is the minimum average cut (0.43). (d) Step 3: vertices 2 and 3 are merged with vertex 4 and a
cut between {1} and {2,3,4} is the minimum average cut (0.4575). (e) Step 4: the process ends since the subset of vertices is
equal to V . The minimum of the minimum average cuts at each step is (c) {2,3} and {1,4}.
Data set
D1
D2
D3
D4
D5
D6
D7
D8
D9
D10
D11
D12
D13
D14
D15

Adult

Breast Cancer Wisconsin [14]

IPUMS Census Database [18]
Mushroom
Dermatology

Table 1:

Size
118 Ã 90 Ã 20263 Ã 5 Ã 2
7 Ã 20263 Ã 5 Ã 6 Ã 16
72 Ã 20263 Ã 90 Ã 2 Ã 2
20263 Ã 14 Ã 2 Ã 6 Ã 94
20263 Ã 5 Ã 2 Ã 90 Ã 72
645 Ã 10 Ã 11 Ã 2 Ã 10
10 Ã 645 Ã 9 Ã 10 Ã 10
10 Ã 10 Ã 11 Ã 10 Ã 645
2 Ã 10 Ã 10 Ã 10 Ã 645
10 Ã 10 Ã 645 Ã 9 Ã 10
3890 Ã 4 Ã 13 Ã 3 Ã 3
545 Ã 3 Ã 17 Ã 3 Ã 2
11 Ã 3 Ã 4 Ã 5 Ã 3
10 Ã 3 Ã 5 Ã 2 Ã 7
62 Ã 5 Ã 5 Ã 5 Ã 3

Relational tensor data sets with 5 attributes

the higher the overall dependency between the
attributes in a partition, the smaller should be
the decomposition rank of that partition.

Data set

Attributes

D1
D2
D3
D4
D5
D6
D7
D8
D9
D10
D11
D12
D13
D14
D15

{A11 , A12 , A3 , A9 , A10 }
{A2 , A3 , A9 , A8 , A4 }
{A1 , A3 , A12 , A15 , A10 }
{A3 , A7 , A15 , A8 , A13 }
{A3 , A9 , A15 , A12 , A1 }
{A1 , A4 , A7 , A11 , A6 }
{A4 , A1 , A10 , A8 , A9 }
{A6 , A5 , A7 , A8 , A1 }
{A11 , A9 , A6 , A3 , A1 }
{A5 , A4 , A1 , A10 , A8 }
{A8 , A17 , A19 , A3 , A2 }
{A53 , A2 , A21 , A3 , A4 }
{A13 , A48 , A17 , A14 , A2 }
{A4 , A9 , A18 , A17 , A2 }
{A34 , A24 , A33 , A25 , A11 }

Table 2:

Join
attr.
(X)
A3
A3
A3
A3
A3
A1
A1
A1
A1
A1
A8
A53
A13
A2
A34

Support
of X
97%
80%
80%
75%
80%
96%
96%
96%
98%
96%
99%
98%
98%
88%
80%

exec.
time for
FDs
0.024s
0.022s
0.025s
0.023s
0.023s
0.004s
0.003s
0.002s
0.003s
0.003s
0.007s
0.006s
0.005s
0.004s
0.002s

Different attribute sets, join attributes (X), sup-

ports of X (the lowest of all the supports of X â â), and
execution times for FDs discovery for D1-D15 where An is
the nth attribute of each data set

Thus, given R1 and R2 , we need to consider only those
rank pairs hr1 , r2 i, where if the average intra-partition FD
support for R1 is larger than the support for R2 , then r1 <
r2 and vice versa. We refer to this as the intraFD criterion
for rank pruning. Similarly to interFD, given the supports
of FDs, we define intraFD as the average support of the
pairwise FDs (excluding the pairwise FDs involving the join
attribute) within each partition.
In the next section, we evaluate the effect of the interFDbased partitioning and intraFD-based rank pruning strategy
of DBN for both dense and sparse tensor decomposition in
terms of the efficiency and the accuracy.

6.

EXPERIMENTAL EVALUATION

Here, we present experimental results assessing the efficiency and effectiveness of the proposed DBN scheme relative to the conventional implementation of the tensor decomposition in both stand-alone and parallelized versions. We
used various data sets from UCI Machine Learning Repository [6].

6.1

Partitioning Cases

We consider two partitioning cases discussed in Section 4:
Case 1. Firstly, we evaluate DBN for the case where the
join attribute X determines only a subset of the attributes
of the relation R. In this case, as long as one partition R1
has X and the determined attributes of X, the number of
nonzero entries of R1 is less than or equal to that of R and
the number of nonzero entries of the other partition R2 is
same as that of R. For dense tensor decomposition, we make
the number of attributes of R1 and R2 similar.

For this case, we used a 5-mode relational tensor of dimensions 118 Ã 90 Ã 20263 Ã 5 Ã 2 of the Adult data set
(D1). We ran TANE [8] to find FDs on this data set. TANE
identified almost exact FDs A3 â A9 and A3 â A10 (with
99% and 98% confidence respectively) where An is the nth
attribute. Now we take A3 as the join attribute and remove
unmatched tuples (1.72%) for these two FDs to get the exact
FDs. Next the tensor is normalized into two 3-mode tensors
R1 {A3 , A9 , A10 } and R2 {A3 , A11 , A12 }. We then create
relational tensors corresponding to different table sizes by
randomly selecting entries from the data.
Case 2. Secondly, we evaluate DBN in the case where the
join attribute X determines all attributes of the relation R.
In this case, all partitioning cases generate as many tuple
(nonzero entries) as the relation R and thus, it is not possible to select a configuration which better minimizes the
total number of nonzero entries than the rest. In this case,
for each pairwise FD support above a support threshold,
Ïsupport = 75%, we take an attribute with the highest total
pairwise FD support among all attributes of the input relation as the join attribute. For these experiments, we took
15 different data sets (D1-D15) with different sizes and different attribute sets (see Table 1) and we experimented all
partitioning cases with the same number of tuples for R,
R1 , and R2 for each data set.
All tensors were encoded as occurrence tensors (see Definition 3.1), where each entry is set to 1 or 0, indicating
whether the corresponding tuple exists or not. Therefore,
we report the number of nonzero entries. We also report
the tensor size for the dense tensor decomposition since the

decomposition cost of dense tensor decomposition depends
on the size of dense tensors.
We experimented with rank-12 decompositions. DBN uses
6 combinations (1 Ã 12, 2 Ã 6, 3 Ã 4, 4 Ã 3, 6 Ã 2, and 12 Ã 1)
for rank-12 decomposition for each relation in Table 1.

Algorithm
DBN-NWAY
DBN-CP
NNCP-NWAY
NNCP-CP
pp-DBN-NWAY
NNCP-NWAY-GRID2

6.2

NNCP-NWAY-GRID6

Implementation

Discovery of FDs. We employed TANE [8] and made
an extension to detect approximate FDs for all pairs of attributes for pairwise FDs and remove all unmatched tuples
to get the exact FDs for the normalization process based on
the supports of the approximate FDs.
The supports of the approximate FDs for each attribute
set of different relational data sets are shown in Table 2.
The table also shows the execution times to discover FDs
for each data set. The modified TANE algorithm is efficient when the number of attributes is small (5 attributes
in these experiments). The overall cost increases linearly in
the size of the input [8]. Since the execution times for finding approximate FDs are negligible compared to the tensor
decomposition time, we focus on the decomposition times.
Tensor Decomposition. We experimented with alternative algorithms for both nonnegative CP tensor decomposition on the original tensors (NNCP) and DBN. Table 3
shows the various algorithms we use in our experiments.
The first decomposition algorithm we considered is the Nway PARAFAC algorithm with nonnegativity constraint (we
call this N-way PARAFAC in the rest of the paper) which is
available in the N-way Toolbox for MATLAB [1]. We refer
to DBN and NNCP using N-way PARAFAC implementation
as DBN-NWAY and NNCP-NWAY respectively.
Since MATLABâs N-way PARAFAC implementation uses
a dense tensor (multi-dimensional array) representation,
it is too costly to be practical, especially for sparse tensors. Another PARAFAC implementation, the CP-ALS algorithm [3], on the other hand, can run with both sparse
and dense tensors. In the sparse tensor model, the cost
increases linearly as the number of nonzero entries of the
tensor increases. The CP-ALS, however, does not support
nonnegative constraints. Therefore, we implemented a variant of the single grid NTF [17] using CP-ALS as the base
PARAFAC algorithm. We refer to DBN and NNCP based
on CP-ALS as DBN-CP and NNCP-CP respectively.
For the parallel version of the NNCP, we implemented the
grid NTF algorithm [17] with two different partition strategies (2 and 6 grid cells along the join mode) using N-way
PARAFAC and CP-ALS as the base PARAFAC algorithms.
Each grid is run with the base PARAFAC algorithm separately in parallel and results are iteratively combined into
the final decomposition based on an ALS-like approach. We
refer to the grid NTF algorithm for the parallel NNCP using
N-way PARAFAC with 2 and 6 grid cells as NNCP-NWAYGRID2, and NNCP-NWAY-GRID6 respectively. Similarly,
we refer to CP-ALS based implementations as NNCP-CPGRID2 and NNCP-CP-GRID6.
For the parallel version of DBN, we implemented pairwise
parallel DBN-NWAY and DBN-CP, referred to as pp-DBNNWAY and pp-DBN-CP, respectively.
Finally, for DBN with a subset of pairs, DBN with 2 and
3 pairs selected based on the intraFD-based rank pruning
strategy are referred to as DBN2 and DBN3 respectively.
For all alternative DBN strategies can be DBN2 or DBN3

pp-DBN-CP
NNCP-CP-GRID2
NNCP-CP-GRID6
DBN2
DBN3

Table 3:

Description
DBN using N-way PARAFAC
DBN using single grid NTF (CP-ALS)
NNCP using N-way PARAFAC
NNCP using single grid NTF (CP-ALS)
pairwise parallel DBN-NWAY
NNCP using grid NTF with 2 grid cells
(N-way PARAFAC)
NNCP using grid NTF with 6 grid cells
(N-way PARAFAC)
pairwise parallel DBN-CP
NNCP-CP with 2 grid cells
NNCP-CP with 6 grid cells
intraFD-based DBN with 2 pairs
intraFD-based DBN with 3 pairs

Algorithms.

Note that the decomposition algo-

rithms in parentheses are the base PARAFAC algorithm for
the grid NTF.

according to the number of pairs selected (e.g., DBN2-CP
for DBN-CP with 2 pairs selected).
We ran our experiments on an 6 cores Intel(R) Xeon(R)
CPU X5355 @ 2.66GHz with 24GB of RAM. We used MATLAB Version 7.11.0.584 (R2010b) 64-bit (glnxa64) for the
general implementation and MATLAB Parallel Computing
Toolbox for the parallel implementation of DBN and NNCP.

6.2.1

Accuracy

We use the following fit function to measure tensor decomposition accuracy: The fit measure is defined as
fit(X, XÌ) = 1 â

kX, XÌk
,
kXk

(1)

where kXk is the Frobenius norm of a tensor X. The fit is a
normalized measure of how accurate a tensor decomposition
of X, XÌ with respect to a tensor X.
kXÌk is also used as an approximate fit measure of X to
substitute for fit computation for large data sets. The intuition behind this measure is as follows: For any W it can be
shown that kW â WÌk â¥ kWk â kWÌk. Therefore, as long as
kWk â¥ kWÌk holds, we can minimize the term kW â WÌk by
maximizing kWÌk.
Our evaluation criteria also include fit ratio which indicates how accurate one strategy compared with the other
strategy in terms of fit to the input tensor. For example,
the fit ratio of DBN to NNCP is defined as
fit ratio =

fit(X, XÌdbn )
fit(X, XÌnncp )

(2)

where X is the input tensor, XÌnncp is the tensor obtained
by re-composing the NNCP tensor, and XÌdbn is the tensor
obtained by re-composing the DBN tensor.

6.3
6.3.1

Results
Execution time

As discussed in Section 6.1, we evaluate the execution
times of DBN vs. the conventional nonnegative CP (NNCP)
algorithms (Table 3) for two partitioning cases: in the first
case, the join attribute X determines only a subset of the
attributes of the relation R; that is, nnz(R1 ) â¤ nnz(R) and
nnz(R2 ) = nnz(R) where nnz(X ) denotes the number of
nonzero entries of X . In the second case, the join attribute X

&!!!"

Running Time on 6 cores

01--&-2$3&."$$
45567859:;$<!=$>?5859:;@$>AB$

1200
1000

5567859:;$
>?5859:;$

%'!!"

800

%&!!"

600
sec

!"#$

(NNCP-NWAY-GRID vs. pp-DBN-NWAY; D1)

$!!"
#!!"

NNCP-NWAY-GRID2
NNCP-NWAY-GRID6
pp-DBN-NWAY

400
200
0

!"
&(!"

270

')!"
**!"
%)+!"
%&'"$()$*+"$,(&-$.(/"$

630
990
Size of the join mode

(a)
Figure 5:

(b)

Average running times of (a) NNCP-NWAY vs.

Running Time
(NNCP-CP vs. DBN-CP; D1)
NNCP-CP

DBN-CP

DBN3-CP

DBN2-CP

sec

sec

DBN-NWAY and (b) NNCP-NWAY-GRID vs.
NWAY on 6 cores for Adult data set (D1)
8
7
6
5
4
3
2
1
0

1350

7
6
5
4
3
2
1
0

Running Time on 6 cores
(NNCP-CP-GRID vs. pp-DBN-CP; D1)
NNCP-CP-GRID2

pp-DBN-CP

NNCP-CP-GRID6

2 4 6 8 10 12 14 16 18 20 22
# nonzero (Ã1000)

2

(a)
Figure 6:

pp-DBN-

4

6

8 10 12 14 16 18 20 22
# nonzero (Ã1000)

(b)

Average running times of (a) NNCP-CP vs.

DBN-CP with different pair selections and (b) NNCP-CPGRID vs. pp-DBN-CP on 6 cores for Adult data set (D1)

0233435)647,)83)9)-8:,+)

:<==>=.*?>@4*-=*A*2-B41*

("
!!"#$%&"'()*+,-.)

!!"#$%&"%'()*
+,-./0*123,45*1426*

#!!!$
#!!$
#!$
#$
!"#$
!"#$

#$

#!$

#!!$ #!!!$

%%78"%'()"9:;#**
+,-./0*123,45*1426*

(a)

'"
&"
%"
$"
#"
!"
!" #" $" %" &" '" ("
%%'("'("/01#)*+,-.)

(b)

Figure 7:

Average running times of (a) NNCP-NWAYGRID (avg of GRID2 and GRID6) vs. pp-DBN3-NWAY and
(b) NNCP-CP-GRID vs. pp-DBN3-CP on 6 cores for D1D15. In both cases, most of data points are located under
the diagonal, which indicates that DBN outperforms NNCP

determines all attributes of the relation R; that is nnz(R1 )
= nnz(R2 ) = nnz(R).
Case 1: X does not determine all attributes of R.
First we present the execution times for dense tensor decomposition. As seen in Figure 5, for NNCP-NWAY, as
the tensor size increases, the execution time increases fast.
DBN-NWAY, running on smaller number of modes than
NNCP, saves significant amount of time.
Figure 5 (b) shows the running time of NNCP-NWAYGRID2 and NNCP-NWAY-GRID6 vs. pp-DBN-NWAY on
6 cores. Comparing the execution times in the figure against
the single core execution times in Figure 5 (a) shows that the
DBN-NWAY benefits more from parallelization. Since the
grid NTF divides a larger tensor into smaller sub-tensors but
with the same number of modes as the original tensor, parallelization does not save as much as in pp-DBN-NWAY where
a larger tensor is divided into sub-tensors with a smaller
number of modes.
Figure 6 presents results for sparse tensor decompositions, NNCP-CP vs. DBN-CP with different pair selections

on single-core and NNCP-CP-GRID vs. pp-DBN-CP on 6
cores. Since we select a foreign key for normalizing the input
relation, one of the normalized relations in DBN-CP has the
same number of nonzero entries as that of the original relation. As a result, when using a single core, the sparse tensor
DBN time (which is determined by the number of tuples)
exceeds the time required to decompose the original relation. As discussed in Section 5, we address this using the
intraFD-based rank pruning strategy. Figure 6 (a) shows
that we achieve the better performance by choosing 2 pairs
or 3 pairs in DBN based on intraFD.
Furthermore, as shown in Figure 6 (b), when using multiple cores, the proposed pp-DBN-CP achieves greater execution time savings and, therefore, significantly outperforms
parallelized versions of NNCP. As seen in the figure, ppDBN-CP has the lowest execution time. The grid based
parallelization of NNCP does not improve the running time
much, since the underlying ALS-based combining approach
involves significant communication overheads.
Case 2: X determines all attributes of R.
Since in this case, selection of a configuration is especially
difficult, we also consider intraFD-based rank pruning strategy that can maximize the performance of DBN. We experiment NNCP-NWAY-GRID (avg. of NNCP-NWAY-GRID2
and NNCP-NWAY-GRID6) vs. pp-DBN3-NWAY for dense
tensor decomposition and NNCP-CP-GRID (avg. of NNCPCP-GRID2 and NNCP-CP-GRID6) vs. pp-DBN3-CP for
sparse tensor decomposition for D1-D15 on 6 cores. We use
the average of all different partitioning cases for each data
set. As shown in Figure 7, in most cases of both experiments, DBN outperforms NNCP, especially for dense tensor
decomposition. In few cases, the running times are too small
to make a significant difference between NNCP and DBN.

6.3.2

Result Accuracy

We compare the accuracy of DBN against standard
NNCP. Note that we include results for DBN-CP and
NNCP-CP (results for DBN-NWAY and NNCP-NWAY are
similar). We measured the fit ratios for all different partitioning cases of D1-D15 (Table 1). Note that fit computation
requires a lot of memory (the computation is not feasible for
some larger datasets). Thus, for large datasets with the size
of the join mode bigger than 1,000, we created subsets of
each tensor by sampling random 1,000 entries from the join
mode and compute fit for NNCP-CP and DBN-CP.
As shown in Figure 8 (a), for an overwhelming majority
of the experimented data sets, fit ratio falls in the range
between 0.8 and 1 indicating that the proposed DBN scheme
is, in addition to being efficient, also highly effective.

6.3.3

InterFD-based Partitioning

As discussed in Section 4.3, the proposed DBN strategy
first identifies alternative normalized partitions of the relational tensor and then selects the most promising pair of
partitions to compute the final decomposition. More specifically, the algorithm picks the most independent partitions
according to the interFD.
In these experiments, we use 15 different configurations in
Table 1. In order to quantify the benefits of the interFDbased partitioning strategy, we measure the ratio of the difference between the approximate fits of interFD-based partitions and the minimum approximate fits against the difference between the maximum and minimum approximate fits
among all partitioning alternatives of each data set.

!"#$,&'($6"-#,&!)47&1+8$.&",$1+9+0'(-:);<4=>?$

'()#&*+!,+(#-.()+(#/0'."1+(#-.()+(#/$

);<@4=>:);<4=>$

("

!"#$%&'($

!#'"
!#&"
!#%"
!#$"
!"
0

10

20

30

40

50

Frequency

(a)
Figure 8:

);<A4=>:);<4=>$

("
!#'"
!#&"
!#%"
!#$"
!"

)("
)$"
)*"
)%"
)+"
)&"
),"
)'"
)-"
)(!"
)(("
)($"
)(*"
)(%"
)(+"

Fit Ratio

Fit Ratio (DBN-CP/NNCP-CP)
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

)"*+,+-#$.&,''(-"-/$0&1+1$(2$)34)35$

!"#"$%&#$

(b)

(c)

(a) Frequencies of fit ratios DBN-CP/NNCP-CP for all partitioning cases (108 cases) of D1-D15 (Table 1), (b)

ratios of the difference between the approximate fits of interFD-based partitions and the minimum approximate fits against the
difference between the maximum and minimum approximate fits among different partitioning cases of each data set, D1-D15,
and (c) approximate fit ratio of DBN-CP with 2 and 3 pairs to DBN-CP with original 6 pairs for the partitioning cases (41
cases) of D1-D15 where each partition has more than 2 attributes

Results shown in Figure 8 (b) indicates that the interFDbased partitioning strategy results in fits very close to the
optimal partitioning strategy. The few cases where the
interFD-based strategy provides only 70 â 80% of the optimal attribute partitioning is largely due to the pairwise
nature of interFD. This tends to miss multi-attribute dependencies of the form AB â C with high support, e.g., in
D3, we miss A1 A3 â A15 whose support is 94%.

6.3.4

IntraFD-based Rank Pruning

We compare the approximate fit of DBN-CP with 2 and
3 pairs selected based on the intraFD-based rank pruning
strategy against that of DBN-CP using the original 6 pairs.
Note that we use only the cases (41 cases) where each subtensor has more than 2 attributes (only the join attribute
and a determined attribute) for each data set since the intraFD does not include pairwise FDs involving the join attribute. As shown in Figure 8 (c), the intraFD-based rank
pruning strategy of DBN-CP has a good accuracy. The approximate fit ratios in the most of cases of DBN3-CP are
close to 1; in other words, one of the 3 pairs in DBN3-CP
gives the best approximate fit. Note that the data set D4
(with partitions {A3 , A7 , A15 } and {A3 , A8 , A13 }) gives the
worst approximate fit for both DBN2-CP and DBN3-CP,
with fit ratios 0.63 and 0.80 respectively. This is because,
the intraFD strategy with only pairwise dependencies may
on occasion fail to take into account critical dependencies,
such as A3 A7 â A15 with 92% support.

7.

CONCLUSIONS

Lifecycle of most data includes various operations, from
capture, integration, projection, to data decomposition and
analysis. To address the high cost of tensor decompsition, which is highest among these operations, we proposed
a highly efficient, effective, and parallelized DBN strategy
for approximately evaluating decompositions by normalizing a large relation into the smaller tensors based on the
FDs of the relation and then performing the decompositions
of these smaller tensors. We also proposed interFD-based
partitioning and intraFD-based rank pruning strategies for
DBN based on pairwise FDs across the normalized partitions
and within each normalized partition, respectively.
Experimental results confirmed the efficiency and effectiveness of the proposed DBN scheme, and its interFD and
intraFD based optimization strategies, compared to the conventional NNCP approach.

8.

REFERENCES

[1] C. A. Andersson and R. Bro. The N-way Toolbox for
MATLAB. Chemometr. Intell. Lab., 52(1):1-4, 2000.
[2] J. Antikainen et al. Nonnegative Tensor Factorization
Accelerated Using GPGPU. In TPDS, 22(7):1135-1141, 2011.
National Laboratories, 2006.
[3] B. W. Bader and T. G. Kolda. MATLAB Tensor Toolbox
Ver. 2.2, 2007.
[4] J. D. Carroll and J. J. Chang. Analysis of individual
differences in multidimensional scaling via an N-way
generalization of âEckart-Youngâ decomposition.
Psychometrika, 35:283-319, 1970.
[5] E. F. Codd. Further normalization of the data base relational
model. In Rustin, 1972.
[6] A. Frank and A. Asuncion. UCI Machine Learning
Repository. Irvine, CA: U. of California, School of ICS, 2010.
[7] R. A. Harshman. Foundations of the PARAFAC procedure:
Models and conditions for an âexplanatoryâ multi-modal
factor analysis. UCLA working papers in phonetics, 1970.
[8] Y. Huhtala et al. TANE: An efficient algorithm for
discovering functional and approximate dependencies.
Comput. J., 42 (2):100-111, 1999.
[9] I. F. Ilyas et al. CORDS: Automatic discovery of correlations
and soft functional dependencies. In SIGMOD, 647-658, 2004.
[10] M. Kim and K. S. Candan. Approximate tensor
decomposition within a tensor-relational algebraic
framework. In CIKM, 2011.
[11] T. G. Kolda et al. Higher-order web link analysis using
multilinear algebra. In ICDM, 2005.
[12] T. G. Kolda and J. Sun. Scalable tensor decompositions for
multi-aspect data mining. In ICDM, 363-372, 2008.
[13] S. Lopes et al. Efficient discovery of functional dependencies
and armstrong relations. In EDBT, 2000.
[14] O. L. Mangasarian and W. H. Wolberg. Cancer diagnosis via
linear programming. SIAM News, 23(5):1-18, 1990.
[15] H. Mannila and K. RaÌihaÌ. On the complexity of inferring
functional dependencies. Discrete Appl. Math., 40:237-243,
1992.
[16] H. Mannila and H. Toivonen. Levelwise search and borders of
theories in knowledge discovery. Data Min. Knowl. Discov.,
1(3):259-289, 1997.
[17] A. H. Phan and A. Cichocki. PARAFAC algorithms for
large-scale problems. Neurocomputing, 74(11):1970-1984,
2011.
[18] S. Ruggles and M. Sobek. Integrated Public Use Microdata
Series: Version 2.0 Minneapolis: Historical Census Projects,
U. of Minnesota, 1997.
[19] M. Stoer and F. Wagner. A simple min-cut algorithm. J.
ACM, 44 (4):585-59, 1997.
[20] C. E. Tsourakakis. Mach: Fast randomized tensor
decompositions. In SDM, 2010.
[21] L. R. Tucker. Some mathematical notes on three-mode factor
analysis. Psychometrika, 31, 1966.
[22] C. M. Wyss et al. FastFDs: A heuristic-driven, depth-first
algorithm for mining functional dependencies from relation
instances. In DaWak, 2001.
[23] Q. Zhang et al. A parallel nonnegative tensor factorization
algorithm for mining global climate data. ICCS, 2009.

LR-PPR: Locality-Sensitive, Re-use Promoting,
Approximate Personalized PageRank Computation
Jung Hyun Kim

K. SelÃ§uk Candan

Maria Luisa Sapino

Arizona State University
Tempe, AZ 85287, USA

Arizona State University
Tempe, AZ 85287, USA

University of Torino
I-10149 Torino, Italy

jkim294@asu.edu

candan@asu.edu

mlsapino@di.unito.it
G

ABSTRACT
Personalized PageRank (PPR) based measures of node proximity
have been shown to be highly effective in many prediction and recommendation applications. The use of personalized PageRank for
large graphs, however, is difficult due to its high computation cost.
In this paper, we propose a Locality-sensitive, Re-use promoting,
approximate personalized PageRank (LR-PPR) algorithm for efficiently computing the PPR values relying on the localities of the
given seed nodes on the graph: (a) The LR-PPR algorithm is locality sensitive in the sense that it reduces the computational cost
of the PPR computation process by focusing on the local neighborhoods of the seed nodes. (b) LR-PPR is re-use promoting in that
instead of performing a monolithic computation for the given seed
node set using the entire graph, LR-PPR divides the work into localities of the seeds and caches the intermediary results obtained
during the computation. These cached results are then reused for
future queries sharing seed nodes. Experiment results for different
data sets and under different scenarios show that LR-PPR algorithm
is highly-efficient and accurate.

Categories and Subject Descriptors
H.3 [Information Storage and Retrieval]: Miscellaneous

Keywords
Personalized PageRank; Locality-Sensitivity; Reuse-Promotion

1.

INTRODUCTION

Node distance/proximity measures are commonly used for quantifying how nearby or otherwise related to two or more nodes on
a graph are. Path-length based definitions [17] are useful when the
relatedness can be captured solely based on the properties of the
nodes and edges on the shortest path (based on some definition of
path-length). Random-walk based definitions, such as hitting distance [16] and personalized page rank (PPR) score [4, 13, 21] of
â
This work is supported by NSF Grant 1043583 âMiNC: NSDL
Middleware for Network- and Context-aware Recommendationsâ.

Permission to make digital or hard copies of all or part of this work for personal or
classroom use is granted without fee provided that copies are not made or distributed
for profit or commercial advantage and that copies bear this notice and the full citation
on the first page. Copyrights for components of this work owned by others than the
author(s) must be honored. Abstracting with credit is permitted. To copy otherwise, or
republish, to post on servers or to redistribute to lists, requires prior specific permission
and/or a fee. Request permissions from permissions@acm.org.
CIKMâ13, Oct. 27âNov. 1, 2013, San Francisco, CA, USA.
Copyright is held by the owner/author(s). Publication rights licensed to ACM.
ACM 978-1-4503-2263-8/13/10 ...$15.00.
http://dx.doi.org/10.1145/2505515.2505651.

â

v1

b
v2

d
c
a

v3

Figure 1: Key questions: Given a graph, G, and a seed set of
nodes S = {v1 , v2 , v3 } in G, can we rank the remaining nodes
in the graph regarding their relationships to the set S? Which
of the nodes a through d is the most interesting given the seed
set of nodes v1 through v3 ?
node relatedness, on the other hand, also take into account the density of the edges: unlike in path-based definitions, random walkbased definitions of relatedness also consider how tightly connected
two nodes are and argue that nodes that have many paths between
them can be considered more related. Random-walk based techniques encode the structure of the network in the form a transition matrix of a stochastic process from which the node relationships can be inferred. When it exists, the convergence probability of a node n gives the ratio of the time spent at that node in a
sufficiently long random walk and, therefore, neatly captures the
connectivity of the node n in the graph. Therefore, many web
search and recommendation algorithms, such as PageRank [5],
rely on random-walks to identify significant nodes in the graph:
let us consider a weighted, directed graph G(V, E), where the
weight
j â E is denoted as wj (â¥ 0) and where
 of the edge e
= 1.0. The PageRank score of the node
w
j
ej âoutedge(vi )
vi â V is the stationary distribution of a random walk on G, where
at each step with probability 1âÎ², the random walk moves along an
outgoing edge of the current node with a probability proportional
to the edge weights and with probability Î², the walk jumps to a
random node in V . In other words, if we denote all the PageRank
scores of the nodes in V with a vector Ï , then
Ï = (1 â Î²)TG Ã Ï + Î²j,
where TG denotes the transition matrix corresponding to the graph
G (and the underlying edge weights) and j is a teleportation vector
where all entries are V1  .

1.1 Proximity and PageRank
An early attempt to contextualize the PageRank scores is the
topic sensitive PageRank [12] approach which adjusts the PageRank scores of the nodes by assigning the teleportation probabilities in vector j in a way that reflects the graph nodesâ degrees of

G
G1

G

incoming bnd. node
of G1

G1

G2

v1

G2

v2

a shared
node
v1
v3

outgoing bnd. node
of G1

Figure 2: Locality-sensitivity: Computation of PPR should focus on the neighborhoods (localities) of the seeds
G
G1

G
G1

G2

v1

G7

v1
v7

v2

v6

v3

Figure 4: Incoming and outgoing boundary nodes/edges and a
node shared between two localities
â¢ locality sensitive in the sense that it reduces the computational cost of the PPR computation process and improve accuracy by focusing on the neighborhoods of the seed nodes
(Figure 2); and

v9

G6

G3

G9

(a) PPR query 1

v2

G3

(b) PPR query 2

â¢ re-use promoting in that it enables caching and re-use of significant portions of the intermediary work for the individual
seed nodes in future queries (Figure 3).

Figure 3: Re-use promotion: Two PPR queries sharing a seed
node (v1 ) should also share relevant work
match to the search topic. [6, 7] were among the first works which
recognized that random-walks can also be used for measuring the
degree of association, relatedness, or proximity of the graph nodes
to a given seed node set, S â V (Figure 1). An alternative to
this approach is to modify (as in topic sensitive PageRank [12]) the
teleportation vector, j: instead of jumping to a random node in V
with probability Î², the random walk jumps to one of the nodes in
the seed set, S, given by the user. More specifically, if we denote
the personalized PageRank scores of the nodes in V with a vector
 then
Ï,
 = (1 â Î²)TG Ã Ï
 + Î²s,
Ï
1
where s is a re-seeding vector, such that if vi â S, then s[i] = S
and s[i] = 0, otherwise. One key advantage of this approach over
modifying the transition matrix as in [6] is that the term Î² can be
used to directly control the degree of seeding (or personalization)
of the PPR score. However, the use of personalized PageRank for
large graphs is difficult due to the high cost of solving for the vec given Î², transition matrix TG , and the seeding vector s.
tor Ï,
 is to solve the above equation for Ï
 matheOne way to obtain Ï
matically. Alternatively, PowerIteration methods [14] simulate the
dissemination of probability mass by repeatedly applying the tran 0 until a convergence crisition process to an initial distribution Ï
terion is satisfied. For large data sets, both of these processes are
prohibitively expensive. Recent advances on personalized PageRank includes top-k and approximate personalized PageRank algorithms [1, 3, 8, 10, 11, 20, 22] and parallelized implementations on
MapReduce or Pregel based batch data processing systems [2, 15].
The FastRWR algorithm presented in [22] for example partitions
the graph into subgraphs and indexes partial intermediary solutions.
Unfortunately, for large data sets, FastRWR requires large number
of partitions to ensure that the intermediary metadata (which requires dense matrix representation) fits into the available memory
and this negatively impacts execution time and accuracy.

1.2 Contributions of this Paper
In this paper, we argue that we can improve both scalability and
accuracy through a Locality-sensitive, Re-use promoting, approximate personalized PageRank (LR-PPR) algorithm: LR-PPR is

In the following section, we first formally introduce the problem
and then present our solution for locality-sensitive, re-use promoting, approximate personalized PageRank computations. We evaluate LR-PPR for different data sets and under different scenarios in
Section 3. We conclude in Section 4.

2. PROPOSED APPROACH
Let G = (V, E) be a directed graph. For the simplicity of the
discussion, without any loss of generality, let us assume that G is
unweighted1 . Let us be given a set S â V of seed nodes (Figure 1)
and a personalization parameter, Î². Let GS = {Gh (Vh , Eh ) | 1 â¤
h â¤ K} be K = S subgraphs of G, such that
â¢ for each vi â S, there exists a corresponding Gi â GS such
that vi â Vi and
â¢ for all Gh â GS , Gh   G.
We first formalize the locality-sensitivity goal (Figure 2):
Desideratum 1: Locality-Sensitivity. Our goal is to compute an
 apx , using GS instead of G, such that
approximate PPR vector, Ï
 apx â¼ Ï,
 where Ï
 represents the true PPR scores of the nodes in
Ï
V relative to S: i.e.,
 = (1 â Î²)TG Ã Ï
 + Î²s,
 apx â¼ Ï
Ï
where TG is the transition matrix corresponding to G and s is the
re-seeding vector corresponding to the seed nodes in S.
We next formalize the re-use promotion goal (Figure 3):
Desideratum 2: Reuse-Promotion. Let S1 and S2 be two sets of
seed nodes and let vi be a node such that vi â S1 â© S2 . Let also the
 apx,1 corresponding to S1 have already
approximate PPR vector, Ï
been computed using GS1 and let us assume that the approximate
 apx,2 corresponding to S2 is being requested. The
PPR vector, Ï
part of the work performed when processing Gi â GS1 (corresponding to vi ) should not need to be re-performed when process apx,2 using GS .
ing Gi â GS , when computing Ï
2

2.1

2

Combined Locality and its Boundary

Unlike existing approximate PPR algorithms [1, 3, 8, 10, 11, 20,
22], LR-PPR is location sensitive. Therefore, given the set, S, of
1

Extending the proposed algorithms to weighted graphs is trivial.

G1
v1

GK

G3

G2
v2

v3

â

vK

â 0V2 ÃV1 
â
...
â
â0
VK ÃV1 
01ÃV1 

A node shared by multiple seed locality graphs

Figure 5: An equivalence set consists of the copies of a node
shared across multiple seed locality graphs
seed nodes and the corresponding localities, GS , the computation
focuses on the combined locality G+ (V + , E + ) â G, where


Vl and E + =
El .
V+ =
1â¤lâ¤K

1â¤lâ¤K

Given a combined locality, G+ , we can also define its external
graph, Gâ (V â , E â ), as the set of nodes and edges of G that
are outside of G+ and boundary nodes and edges. As shown in
Figure 4, we refer to vi â Vl as an outgoing boundary node of
Gl if there is an outgoing edge ei,j = [vi â vj ] â E, where
vj â
/ Vl ; the edge ej is also referred to as an outgoing boundary
edge of Gl . The set of all outgoing boundary nodes of Gl is denoted as Voutbound,l and the set of all outgoing boundary edges of
Gl is denoted as Eoutbound,l. Note that Voutbound,l â Vl , whereas
Eoutbound,l â© El = â.
We also define incoming boundary nodes (Vinbound,l ) and incoming boundary edges (Einbound,l ) similarly to the outgoing
boundary nodes and edges of Gl , but considering inbound edges
to these subgraphs. More specifically, Einbound,l consists of edges
of the form [vi â vj ] â E, where vj â Vl and vi â
/ Vl .

2.2 Localized Transition Matrix
Since LR-PPR focuses on the combined locality, G+ , the next
step is to combine the transition matrices of the individual localities
into a combined transition matrix. To produce accurate approximations, this localized transition matrix, however, should nevertheless
take the external graph, Gâ , and the boundaries between Gâ and
G+ , into account.

2.2.1 Transition Matrices of Individual Localities
Let v(l,i) (1 â¤ l â¤ K) denote a re-indexing of vertices in Vl .
If v(l,i) â Vl and vc â V s.t. v(l,i) = vc , we say that v(l,i) is a
member of an equivalence set, Vc (Figure 5). Intuitively, the equivalence sets capture the common parts across the localities of the
individual seed nodes. Given Gl (Vl , El ) â G and an appropriate
re-indexing, we define the corresponding local transition matrix,
Ml , as a Vl  Ã Vl  matrix, where


â¢ ei,j = [v(l,i) â v(l,j) ] â El â Ml [j, i] = 0 and


â¢ âei,j = [v(l,i) â v(l,j) ] â El â Ml [j, i] =

1
,
out(v(l,i) )

where out(v(l,i) ) is the number of outgoing edges of vi .

2.2.2 Localization of the Transition Matrix
Given the local transition matrices, M1 through MK , we localize the transition matrix of G by approximating it as
Mapx = Mbd + M0 ,
where Mbd is a block-diagonal matrix of the form

M1

0V1 ÃV2 
M2
...
0VK ÃV2 
01ÃV2 

. . . 0V1 ÃVK 
. . . 0V2 ÃVK 
...
...
...
MK
...
01ÃVK 

â
0V1 Ã1
0V2 Ã1 â
â
... â,
0VK Ã1 â 
MK+1

where MK+1 is equal to the 1 Ã 1 matrix 01Ã1 . Intuitively,
Mbd combines the K subgraphs into one transition matrix, without
considering common nodes/edges or incoming/outgoing boundary
edges and ignoring all outgoing and incoming edges. All the external nodes in Gâ are accounted by a single node represented by the
1 Ã 1 matrix MK+1 .
A key advantage of Mbd is that it is block-diagonal and, hence,
there are efficient ways to process it. However, this block-diagonal
matrix, Mbd , cannot accurately represent the graph G as it ignores
potential overlaps among the individual localities and ignores all
the nodes and edges outside of G+ . We therefore need a compensation matrix to
â¢ make sure that nodes and edges shared between the localities
are not double counted during PPR computation and
â¢ take into account the topology of the graph external to both
localities G1 through GK .

2.2.3 Compensation Matrix, M 0
Let t be (V1  + V2  + . . . + VK  + 1). The compensation
matrix, M0 , is a t Ã t matrix accounting for the boundary edges
of the seed localities as well as the nodes/edges in Gâ . M0 also
ensures that the common nodes in V1 through VK are not double
counted during PPR calculations. M0 is constructed as follows:
Row/column indexing: Let vl,i be a vertex in Vl . We introduce a
row/column indexing function, ind(), defined as follows:
â
â

ind(l, i) = â
Vh â  + i
1â¤h<l

Intuitively the indexing function, ind(), maps the relevant nodes in
the graph to their positions in the M0 matrix.
Compensation for the common nodes: Let el,i,j be an edge
[v(l,i) â v(l,j) ] â El and let v(l,j) be a member of the equivalence set Vc for some vc â V . Then, if Vc  > 1
1
c â1
Ã out(G,v
and
â¢ M0 [ind(l, j), ind(l, i)] = â VV
c
l,i )
â¢ âv(h,k) â Vc s.t. v(h,k) = v(l,j) , we have
M0 [ind(h, k), ind(l, i)] = â

1
1
Ã
,
Vc 
out(G, vl,i )

where out(G, v) is the outdegree of node v in G. Intuitively, the
compensation matrix re-routes a portion of the transitions going
towards a shared node in a given locality Vl to the copies in other
seed localities. This prevents the transitions to and from the shared
node from being mis-counted.
Compensation for outgoing boundary edges: The compensation matrix needs to account also for outgoing boundary edges that
are not accounted for by the neighborhood transition matrices M1
through MK :
â¢ Accounting for boundary edges from nodes in Vl to nodes in
Vh : â[v(l,i) â v(h,j) ] â Eoutbound,l
â M0 [ind(h, j), ind(l, i)] =

1
out(v(l,i) )

â¢ Accounting for boundary edges from nodes in Vl to graph
nodes that are in V â :
if â[v(l,i) â v] â Eoutbound,l s.t. v â V â

â M0 [t, ind(l, i)] =

bnd(v(l,i) )
,
out(v(l,i) )

where bnd(v(l,i) ) is

the number of edges of the form [v(l,i) â v] â
Eoutbound,l where v â V â
else M0 [t, ind(l, i)] = 0
The compensation matrix records all outgoing edges, whether they
cross into another locality or they are into external nodes in Gâ . If
a node has more than one outgoing edge into the nodes in Gâ , all
such edges are captured using one single compensation edge which
aggregates all the corresponding transition probabilities.
Compensation for incoming boundary edges (from Gâ ): Similarly to the outgoing boundary edges, the compensation matrix
needs also to account for incoming boundary edges that are not
accounted for by the neighborhood transition matrices M1 through
MK . Since incoming edges from other localities have been accounted for in the previous step, here we only need to consider
incoming boundary edges (from Gâ ). Following the formulation
in [23], we account for incoming edges where the source is external to G+ and the destination is a vertex v(l,i) in Vl by inserting
an edge from the dummy node to v(l,i) with a weight that considers the outdegrees of all external source nodes; i.e., âv(l,i) s.t.
â[vk â v(l,i) ] â Einbound,l where vk â V â and v(l,i) is in the
equivalence set Vc for a vc â V , M0 [ind(l, i), t] is equal to

1
([vk âv(l,i) ]âEinbound,l )â§(vk âV â ) out(G,vk )
1
,
Vc 
V â 
where out(G, v) is the outdegree of node v in G.
Compensation for the edges in Gâ : We account for edges that
are entirely in Gâ by creating a self-loop that represents the sum of
outdegree flow between all external nodes averaged by the number
of external nodes; i.e.,

out(Gâ ,v)
M0 [t, t] =

vâV â

out(G,v)

V â 

,

where out(Gâ , v) and out(G, v) are the outdegrees of node v in
Gâ and G, respectively.
Completion: For any matrix position p, q not considered above, no
compensation is necessary; i.e., M0 [p, q] = 0.

2.3 L-PPR: Locality Sensitive PPR
Once the block-diagonal local transition matrix, Mbd , and the
compensation matrix, M0 , are obtained, the next step is to obtain
the PPR scores of the nodes in V + . This can be performed using
any fast PPR computation algorithm discussed in Section 1.1.
Note that the overall transition matrix Mapx = Mbd + M0 is
approximate in the sense that all the nodes external to G+ are clustered into a single node, represented by the last row and column of
the matrix. Otherwise, the combined matrix Mapx accurately represents the nodes and edges in the âmerged localities graphâ combining the seed localities, G1 through GK . As we see in Section 3,
this leads to highly accurate PPR scores with better scalability than
existing techniques.

the nodes in V + . In particular, we rely on the following result due
to [22], which itself relies on the Sherman-Morisson lemma [18]:
Let C = A + USV. Let also (I â cA)â1 = Qâ1 . Then,
the equation
r = (1 â c)(I â cA)â1e
has the solution
r = (1 â c)(Qâ1e + cQâ1 UÎVQâ1e),
where
Î = (Sâ1 â cVQâ1 U)â1 .
If A is a block diagonal matrix consisting of k blocks, A1
through Ak , then Qâ1 is also a block diagonal matrix conâ1
sisting of k corresponding blocks, Qâ1
1 through Qk , where
â1
â1
Qi = (I â cAi ) .
We use the above observation to efficiently obtain PPR scores by
setting c = (1 â Î²), C = Mapx , A = Mbd , and USV = M0 .
In particular, we divide the PPR computation into two steps: a
locality-sensitive and re-usable step involving the computation of
the Qâ1 term using the local transition matrices and a run-time
computation step involving the compensation matrix.

2.4.1 Locality-sensitive and Re-usable Q â1
bd
Local transition matrices, M1 through MK corresponding to the
seeds v1 through vK are constant (unless the graph itself evolves
â1
is computed
over time). Therefore, if Qâ1
h = (I â (1 â Î²)Mh )
and cached once, it can be reused for obtaining Qâ1
bd , which is a
â1
block diagonal matrix consisting of Qâ1
1 through QK+1 (as before,
â1
the last block, QK+1 , is simply equal to 11Ã1 ):
â
â
Qâ1
0V1 ÃV2  . . . 0V1 ÃVK  0V1 Ã1
1
â1
â 0V ÃV 
Q2
. . . 0V2 ÃVK  0V2 Ã1 â
2
1
â
â
â
.
.
.
.
.
.
.
..
...
... â
â,
â
â1
â 
â0V ÃV1  0V ÃV2  . . .
Q
0
V
Ã1
K
K
K
K
â1
01ÃV1 
01ÃV2 
...
01ÃVK 
QK+1

2.4.2 Computation of the LR-PPR Scores
In order to be able to use the above formulation for obtaining the
PPR scores of the nodes in V + , in the query time, we need to decompose the compensation matrix, M0 , into U0 S0 V0 . While obtaining a precise decomposition in run-time would be prohibitively
expensive, since M0 is sparse and since we are looking for an approximation of the PPR scores, we can obtain a fairly accurate lowrank approximation of M0 efficiently [22]:
M0  UÌ0 SÌ0 VÌ0 .
 apx , which contains
Given this decomposition, the result vector Ï
the (approximate) PPR scores of the nodes in V + , is computed as


 apx = Î² Qâ1s + (1 â Î²)Qâ1 UÌ0 ÎVÌ0 Qâ1 s ,
Ï
bd
bd
bd
where

2.4 LR-PPR: Locality Sensitive and Reuse
Promoting PPR

â1

â1
Î = SÌâ1
.
0 â (1 â Î²)VÌ0 Qbd UÌ0

Our goal is not only to leverage locality-sensitivity as in L-PPR,
but also to boost sub-result re-use. Remember that, as discussed
above, the localized transition matrix Mapx is equal to Mbd + M0
where (by construction) Mbd is a block-diagonal matrix, whereas
M0 (which accounts for shared, boundary, and external nodes) is
relatively sparse. We next use these two properties of the decomposition of Mapx to efficiently compute approximate PPR scores of

Note that the compensation matrix M0 is query specific and,
thus, the work done for the last step cannot be reused across queries.
However, as we experimentally verify in Section 3, the last step
is relatively cheap and the earlier(costlier) steps involve re-usable
work. Thus, caching and re-use through LR-PPR enables significant savings in execution time. We discuss the overall complexity
and the opportunities for re-use next.

2.5 Complexity and Re-use

3.

EXPERIMENTAL EVALUATION

In this section, we present results of experiments assessing
the efficiency and effectiveness of the Locality-Sensitive, Re-use
Promoting Approximate Personalized PageRank (LR-PPR) algorithm. Table 1 provides overviews of the three data sets (from
http : //snap.stanford.edu/data/) considered in the experiments. We considered graphs with different sizes and edge densities. We also varied numbers of seeds and the distances between the
seeds (thereby varying the overlaps among seed localities). We also
considered seed neighborhoods (or localities) of different sizes.
Experiments were carried out using a 4-core Intel Core i5-2400,
3.10GHz, machine with 8GB memory and 64-bit Windows 7 Enterprise. Codes were executed using Matlab 7.11.0(2010b). All
experiments were run 10 times and averages are reported.

3.1 Alternative Approaches
Global PPR: This is the default approach where the entire graph is
used for PPR computation. We compute the PPR scores by solving
the equation presented in Section 1.1.
FastRWR: This is an approximation algorithm, referred to as
NB_LIN in [22]. The algorithm reduces query execution times by
partitioning the graph into subgraphs and preprocessing each partition. The pre-computed files are stored on disk and loaded to the
memory during the query stage. To be fair to FastRWR, we selected
the number of partitions in a way that minimizes its execution time
and memory and maximizes its quality.
L-PPR: This is our locality sensitive algorithm, where instead of
using the whole graph, we use the localized graph created by combining the locality nodes and edges as described in Section 2.2.
Once the localized transition matrix is created, the PPR scores are
computed by solving the equation presented in Section 1.1.
LR-PPR: This is the locality sensitive and re-use promoting algorithm proposed described in detail in Section 2.4.
The restart probability, Î², is set to 0.15 for all approaches.

3.2 Evaluation Measures
Efficiency: This is the amount of time taken to load the relevant
(cached) data from the disk plus the time needed to carry out the
operations to obtain the PPR scores.

&%

	"

Analysis of LR-PPR points to the following advantages: First of
all, computation is done using only local nodes and edges. Secondly, most of the results of the expensive sub-tasks can be cached
and re-used. Moreover, costly matrix inversions are limited to the
smaller matrices representing localities and small matrices of size
r Ã r. Various subtasks
have complexity proportional to V + 2 ,

where V +  = 1â¤lâ¤K Vl . While in theory the locality Vl can
be arbitrarily large, in practice we select localities with a bounded
number of nodes; i.e., â1â¤lâ¤K , Vl  â¤ L for some L  V .
As described above LR-PPR algorithm supports caching and reuse of some of the intermediary work. The process results in local transition matrices, each of which can be cached in O(El )
space (where El is the number edges in the locality) assuming a
sparse representation. The algorithm also involves a matrix inversion, which results in a dense matrix; as a result, caching the
inverted matrix takes O(Vl 2 ) space (where Vl is the number of
vertices in the locality). If the locality is size-constrained, this leads
to constant space usage of O(L2 ), where L is the maximum number of nodes in the locality. If the inverted matrix of a locality is
cached, then the local transition matrix does not need to be maintained further. For cache replacement, any frequency-based or predictive cache-replacement policy can be used.

1	"

()!
%..%..
%.'

%.-%.,
%-,

%.*%.*

%--
%-+

%,(
%*'

%*

%%

%-'
%,*

%'.

*

&%

'*
1

*%

,*

  

Figure 6: Accuracies of L-PPR, LR-PPR, and FastRWR
against the Global PPR for different numbers of target nodes
Accuracy: For different algorithm pairs, we report the Spearmanâs
rank correlation

(xi â xÌ)(yi â yÌ)
 i
,

2
2
i (xi â xÌ)
i (yi â yÌ)
which measures the agreement between two rankings (nodes with
the same score are assigned the average of their positions in the
ranking). Here, x and y are rankings by two algorithms and xÌ and yÌ
are average ranks. To compute the rank coefficient, a portion of the
highest ranked nodes in the merged graph according to x are considered. As default, we considered 10% highest ranked nodes; but
we also varied the target percentage (5%, 10%, 25%, 50%, 75%) to
observe how the accuracy varies with result size.
Memory: We also report the amount of data read from the cache.

3.3 Results and Discussions
Table 2 presents experimental results for FastRWR, L-PPR, and
LR-PPR. First of all, all three algorithms are much faster than
Global PPR. As expected, in small data sets (Epinions and Slashdot) FastRWR works faster than L-PPR and LR-PPR, though in
many cases, it requires more memory. In large data sets, however,
L-PPR and LR-PPR significantly outperform FastRWR in terms of
query processing efficiency and run-time memory requirement.
In terms of accuracy, the proposed locality sensitive techniques,
L-PPR and LR-PPR, constantly outperform FastRWR. This is because, FastRWR tries to approximate the whole graph, whereas
the proposed algorithms focus on the relevant localities. FastRWR requires large number of partitions to ensure that the intermediary metadata (which requires dense matrix representation) fits
into memory and this negatively impacts accuracy. Our localitysensitive algorithms, L-PPR and LR-PPR, avoid this and provide
high accuracy with low memory consumption, especially in large
graphs, like WikiTalk.
Figure 6 confirms that the accuracies of L-PPR and LR-PPR
both stay high as we consider larger numbers of top ranked network nodes for accuracy assessment, whereas the accuracy of FastRWR suffers significantly when we consider larger portions of the
merged locality graph.
Figure 7 studies the execution time behavior for L-PPR, LRPPR, and FastRWR for different number of seed nodes. As the
figure shows, the time cost increases for both L-PPR and LR-PPR
algorithms as the number of seeds increases. But, the cost of LRPPR (which leverages re-use) increases much slower than the cost
of L-PPR and both remain significantly cheaper than FastRWR.

Table 1: Data sets
Data Set

Overall Graph Characteristics
# nodes
# edges
â¼76K
â¼500K
â¼82K
â¼870K
â¼2.4M
â¼5M

Epinions
SlashDot
WikiTalk

Locality Graph Characteristics
# nodes per neighborhood
# edges per neighborhood
from â¼200 to â¼2000
from â¼10K to â¼75K
from â¼700 to â¼5000
from â¼10K to â¼75K
from â¼700 to â¼6000
from â¼10K to â¼75K

# seeds
2-3
2-3
2-8

Seeds
seed distances (hops)
3-4
3-4
3-4

Table 2: Summary of the results for different configurations (in all scenarios, individual seed localities have â¼75K edges)
Seeds
Data set
Epinions
â¼76K nodes
â¼500K edges
SlashDot
â¼82K nodes
â¼870K edges
WikiTalk
â¼2.4M nodes
â¼5M edges

#
seeds
2
2
3
3
2
2
3
3
2
2
3
3

Merged Network

Dist
(#hops)
3
4
3
4
3
4
3
4
3
4
3
4

Avg
# nodes
â¼2.2K
â¼3.0K
â¼2.7K
â¼3.5K
â¼5.9K
â¼5.7K
â¼7.1K
â¼7.2K
â¼5.7K
â¼5.8K
â¼6.3K
â¼6.7K

Execution Time (sec.)

Avg
# edges
â¼90K
â¼99K
â¼108K
â¼120K
â¼117K
â¼125K
â¼141K
â¼159K
â¼102K
â¼100K
â¼101K
â¼103K

	#

"*+((*
!
',)

',+

',*

',)

Global
PPR
27.81
27.58
27.30
27.90
21.79
21.85
21.74
22.93
681.08
693.44
701.34
706.26

	 	!







&- &*


(&

*&

)&
'(

&-







#
 




',


	

Figure 7: Execution times of L-PPR, LR-PPR, and FastRWR
for different numbers of seed nodes

4.

CONCLUSIONS

In this paper, we presented a Locality-sensitive, Re-use promoting, approximate Personalized PageRank (LR-PPR) algorithm for
efficiently computing the PPR values relying on the localities of
the seed nodes on the graph. Instead of performing a monolithic
computation for the given seed node set using the entire graph, LRPPR divides the work into localities of the seeds and caches the intermediary results obtained during the computation. These cached
results can then be reused for future queries sharing seed nodes.
Experiments showed that the proposed LR-PPR approach provides
significant gains in execution time relative to existing approximate
PPR computation techniques, where the PPR scores are computed
from scratch using the whole network. LR-PPR also outperforms
L-PPR, where the PPR scores are computed in a locality-sensitive
manner, but without significant re-use.

5.

REFERENCES
[1] K. Avrachenkov, N. Litvak, D. Nemirovsky, E. Smirnova, and M.
Sokol. Quick Detection of Top-k Personalized PageRank Lists.
WAWâ11, 2011.
[2] B.Bahmani, K.Chakrabarti, and D. Xin. Fast personalized
PageRank on MapReduce. In SIGMODâ11. 973-984. 2011.
[3] B.Bahmani, A.Chowdhury, and A.Goel. Fast incremental and
personalized PageRank. PVLDB. 4, 3, 173-184, 2010.
[4] A. Balmin, V. Hristidis, and Y.Papakonstantinou. ObjectRank:
Authority-based keyword search in databases. VLDB, 2004.

Fast
RWR
0.21
0.22
0.21
0.22
0.35
0.35
0.36
0.38
16.28
16.22
16.32
16.34

LPPR
0.37
0.51
0.58
0.76
0.70
0.78
1.12
1.39
0.75
0.73
0.75
0.78

LRPPR
0.14
0.20
0.26
0.36
0.53
0.42
0.95
0.83
0.37
0.37
0.37
0.36

Top-10%
Spearmanâs
Correl. (vs. Global PPR)
Fast
LLRRWR
PPR
PPR
0.963
0.997
0.990
0.960
0.998
0.990
0.967
0.998
0.990
0.967
0.997
0.991
0.955
0.973
0.990
0.943
0.965
0.983
0.957
0.971
0.990
0.958
0.976
0.986
0.868
0.958
0.944
0.870
0.930
0.909
0.877
0.937
0.902
0.869
0.976
0.967

Memory usage(MB)
Fast
RWR
178.3

302.1

1429.0

LPPR
2.9
3.1
4.6
4.7
5.0
4.9
7.6
7.2
15.5
16.2
24.0
28.7

LRPPR
36.3
55.2
57.6
77.7
228.1
172.8
325.9
256.0
114.5
120.7
211.6
197.5

[5] S. Brin and L. Page. "The anatomy of a large-scale hypertextual
Web search engine". Computer Networks and ISDN Systems 30:
107-117, 1998.
[6] K. S. Candan and W.-S. Li. Using random walks for mining web
document associations. In PAKDD, pp. 294-305, 2000.
[7] K. S. Candan and W.-S. Li. Reasoning for Web document
associations and its applications in site map construction. Data
Knowl. Eng. 43(2), 2002.
[8] K. Csalogany, D.Fogaras, B. Racz, and T. Sarlos. Towards Scaling
Fully Personalized PageRank: Algorithms, Lower Bounds, and
Experiments Internet Math. 2,3, 333-358, 2005.
[9] F. Fouss, A. Pirotte, J. Renders, and M. Saerens. Random-walk
computation of similarities between nodes of a graph with
application to collaborative recommendation. TKDE, 2007.
[10] Y. Fujiwara, M. Nakatsuji, M. Onizuka, and M. Kitsuregawa. Fast
and exact top-k search for random walk with restart. PVLDB. 5, 5,
442-453. 2012.
[11] M. Gupta, A. Pathak, and S. Chakrabarti. Fast algorithms for Top-k
Personalized PageRank Queries. In WWWâ08. 1225-1226. 2008.
[12] T.H. Haveliwala. Topic-sensitive PageRank. WWWâ02. 517-526.
2002.
[13] G. Jeh and J. Widom. Scaling personalized web search. Stanford
University Technical Report. 2002.
[14] S.D. Kamvar, T.H. Haveliwala, C.D. Manning, and G.H. Golub.
Extrapolation methods for accelerating PageRank computations.
In WWWâ03 261-270. 2003.
[15] G. Malewicz, et al. Pregel: a system for large-scale graph
processing. SIGMODâ10, 2010.
[16] Q. Mei, D. Zhou, and K. Church. Query suggestion using hitting
time, CIKMâ08, 2008.
[17] C. Palmer, P. Gibbons, and C. Faloutsos. Anf: a fast and scalable
tool for data mining in massive graphs. KDDâ02, 2002.
[18] W. Piegorsch and G. E. Casella. Inverting a sum of matrices. In
SIAM Review, 1990.
[19] P. Sarkar, A.W. Moore, and A. Prakash. Fast incremental
proximity search in large graphs. ICMLâ08, 2008.
[20] H. H. Song, et al. Scalable proximity estimation and link
prediction in online social networks. In Internet Measurement
Conference, pp. 322â335. 2009.
[21] H. Tong, C. Faloutsos, and Y. Koren. Fast direction-aware
proximity for graph mining. KDD, pp. 747â756, 2007.
[22] H. Tong, C. Faloutsos, and J.-Y. Pan. Fast Random Walk with
Restart and Its Applications. In ICDMâ06. 613-622. 2006.
[23] Y. Wu and L. Raschid, ApproxRank: Estimating Rank for a
Subgraph, ICDEâ09, 54-65, 2009.

