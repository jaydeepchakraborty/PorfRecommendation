Compressed Spatial Hierarchical Bitmap (cSHB) Indexes
‚àó
for Efficiently Processing Spatial Range Query Workloads
Parth Nagarkar

K. Sel√ßuk Candan

Aneesha Bhat

Arizona State University
Tempe, AZ 85287-8809, USA

Arizona State University
Tempe, AZ 85287-8809, USA

Arizona State University
Tempe, AZ 85287-8809, USA

nagarkar@asu.edu

candan@asu.edu

aneesha.bhat@asu.edu

ABSTRACT

terms of their coordinates in 2D space. Queries in this 2D
space are then processed using multidimensional/spatial index structures that help quick access to the data [28].

In most spatial data management applications, objects are
represented in terms of their coordinates in a 2-dimensional
space and search queries in this space are processed using
spatial index structures. On the other hand, bitmap-based
indexing, especially thanks to the compression opportunities bitmaps provide, has been shown to be highly eÔ¨Äective
for query processing workloads including selection and aggregation operations. In this paper, we show that bitmapbased indexing can also be highly eÔ¨Äective for managing
spatial data sets. More speciÔ¨Åcally, we propose a novel compressed spatial hierarchical bitmap (cSHB) index structure
to support spatial range queries. We consider query workloads involving multiple range queries over spatial data and
introduce and consider the problem of bitmap selection for
identifying the appropriate subset of the bitmap Ô¨Åles for processing the given spatial range query workload. We develop
cost models for compressed domain range query processing
and present query planning algorithms that not only select
index nodes for query processing, but also associate appropriate bitwise logical operations to identify the data objects
satisfying the range queries in the given workload. Experiment results conÔ¨Årm the eÔ¨Éciency and eÔ¨Äectiveness of the
proposed compressed spatial hierarchical bitmap (cSHB) index structure and the range query planning algorithms in
supporting spatial range query workloads.

1.

1.1 Spatial Data Structures

INTRODUCTION

Spatial and mobile applications are gaining in popularity, thanks to the wide-spread use of mobile devices, coupled with increasing availability of very detailed spatial
data (such as Google Maps and OpenStreetMap [3]), and
location-aware services (such as FourSquare and Yelp). For
implementing range queries (Section 3.1.2), many of these
applications and services rely on spatial database management systems, which represent objects in the database in
‚àóThis work was supported by NSF grants 1116394, 1339835,
and 1318788
This work is licensed under the Creative Commons AttributionNonCommercial-NoDerivs 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/3.0/. Obtain permission prior to any use beyond those covered by the license. Contact
copyright holder by emailing info@vldb.org. Articles from this volume
were invited to present their results at the 41st International Conference on
Very Large Data Bases, August 31st - September 4th 2015, Kohala Coast,
Hawaii.
Proceedings of the VLDB Endowment, Vol. 8, No. 12
Copyright 2015 VLDB Endowment 2150-8097/15/08.

The key principle behind most indexing mechanisms is to
ensure that data objects closer to each other in the data
space are also closer to each other on the storage medium.
In the case of 1D data, this task is relatively easy as the
total order implicit in the 1D space helps sorting the objects so that they can be stored in a way that satisÔ¨Åes the
above principle. When the space in which the objects are
embedded has more than one dimension, however, the data
has multiple degrees of freedom and, as a consequence, there
are many diÔ¨Äerent ways in which the data can be ordered
on the storage medium and this complicates the design of
search data structures. One common approach to developing index structures for multi-dimensional data is to partition the space hierarchically in such a way that (a) nearby
points fall into the same partition and (b) point pairs that
are far from each other fall into diÔ¨Äerent partitions. The resulting hierarchy of partitions then can either be organized
in the form of trees (such as quadtrees, KD-trees, R-trees
and their many variants [28]) or, alternatively, the root-toleaf partition paths can be serialized in the form of strings
and these strings can be stored in a string-speciÔ¨Åc search
structure. Apache Lucene, a highly-popular search engine,
for example, leverages such serializations of quadtree partitions to store spatial data in a spatial preÔ¨Åx tree [1].
An alternative to applying the partitioning process in the
given multi-dimensional space is to map the coordinates of
the data into a 1D space and perform indexing and query
processing on this 1D space instead. Intuitively, in this alternative, one seeks an embedding from the 2D space to a
1D space such that (a) data objects closer to each other in
the original space are also closer to each other on the 1D
space, and (b) data objects further away from each other
in the original space are also further away from each other
on the 1D space. This embedding is often achieved through
fractal-based space-Ô¨Ålling curves [11, 17]. In particular, the
Peano-Hilbert curve [17] and Z-order curve [23] have been
shown to be very eÔ¨Äective in helping cluster nearby objects
in the space. Consequently, if data are stored in an order
implied by the space-Ô¨Ålling curve, then the data elements
that are nearby in the data space are also clustered, thus
enabling eÔ¨Écient retrieval. In this paper, we leverage these
properties of space-Ô¨Ålling curves to develop a highly compressible bitmap-based index structure for spatial data.

1382

hierarchical bitmap Ô¨Åles and (b) propose eÔ¨Écient bitmap selection algorithms that select the best bitmap nodes from the
cSHB index structure to be fetched into the main-memory
for processing of the query workload. In this paper, we also
present an eÔ¨Écient disk-based organization of compressed
bitmaps. To our best knowledge, this is the Ô¨Årst work that
provides an eÔ¨Écient index structure to execute a query workload involving multiple spatial range queries by using bitmap
indexes. Experimental evaluations of the cSHB index structure and the bitmap selection algorithms show that cSHB is
highly eÔ¨Écient in answering a given query workload.

Results
Query
Workload

Identify
Cut & Leaf Bitmaps

Construct
Result Bitmaps

Cutt Bitmaps
C
Bit
Buffer
Buffe
B
ff

Leaf Bitmaps Buffer

1.4 Paper Organization

Cut Bitmap
Cut Bitmap
Cut Bitmap
Cut Bitmap
Bitm
itmap
Leaf Bitmaps

Compressed Spatial Hierarchical Bitmaps (cSHB)

Figure 1: Processing a range query workload using
compressed spatial hierarchical bitmap (cSHB)

1.2 Bitmap-based Indexing
Bitmap indexes [29, 32] have been shown to be highly
eÔ¨Äective in answering queries in data warehouses [34] and
column-oriented data stores [5]. There are two chief reasons for this: (a) Ô¨Årst of all, bitmap indexes provide an eÔ¨Écient way to evaluate logical conditions on large data sets
thanks to eÔ¨Écient implementations of the bitwise logical
‚ÄúAND‚Äù, ‚ÄúOR‚Äù, and ‚ÄúNOT‚Äù operations; (b) secondly, especially when data satisfying a particular predicate are clustered, bitmap indexes provide signiÔ¨Åcant opportunities for
compression, enabling either reduced I/O or, even, complete
in-memory maintenance of large index structures. In addition, (c) existence of compression algorithms [15, 33] that
support compressed domain implementations of the bitwise
logical operations enables query processors to operate directly on compressed bitmaps without having to decompress
them until the query processing is over and the results are
to be fetched from the disk to be presented to the user.

1.3 Contributions of this Paper
In this paper, we show that bitmap-based indexing is also
an eÔ¨Äective solution for managing spatial data sets. More
speciÔ¨Åcally, we Ô¨Årst propose compressed spatial hierarchical bitmap (cSHB) indexes to support spatial range queries.
In particular, we (a) convert the given 2D space into a 1D
space using Z-order traversal, (b) create a hierarchical representation of the resulting 2D space, where each node of the
hierarchy corresponds to a (sub-)quadrant (i.e., eÔ¨Äectively
creating an implicit ‚Äúquadtree‚Äù), and (c) associate a bitmap
Ô¨Åle to each node in the quadtree representing the data elements that fall in the corresponding partition. We present
eÔ¨Écient algorithms for answering range queries using a select
subset of bitmap Ô¨Åles stored in a given cSHB index.
We then consider a service provider that has to answer
multiple concurrent queries over the same spatial data and,
thus, focus on query workloads involving multiple range
queries. Since the same set of queries can be answered using
diÔ¨Äerent subsets of the bitmaps in the cSHB index structure, we consider the problem of identifying the appropriate
bitmap nodes for processing the given query workload. More
speciÔ¨Åcally, as we visualize in Figure 1, (a) we develop cost
models for range query processing over compressed spatial

This paper is organized as follows. In the next section we
provide an overview of the related work. In Section 3.1, we
introduce the key concepts and notations, and in Section 3.2,
we present the proposed cSHB index structure. Then, in
Section 4, we describe how query workloads are processed using cSHB: in Section 4.1, we introduce the concepts of range
query plans, in Section 4.2, we present cost models for alternative execution strategies, and in Section 4.3, we present
algorithms for Ô¨Ånding eÔ¨Écient query plans for a given range
query workload. Experiment results are reported in Section 5. We Ô¨Ånally conclude the paper in Section 6.

2. RELATED WORK
2.1 Multi-Dimensional Space Partitioning
Multi-dimensional space partitioning strategies can be
categorized into two: In the Ô¨Årst case, including quadtree,
BD-tree, G-Tree, and KD-tree variants, a given bounded region is divided into two or more ‚Äúopen‚Äù partitions such that
each partition borders a boundary of the input region. In
the latter case, some of the partitions (often referred to as
minimum bounding regions, MBRs) are ‚Äúclosed‚Äù regions of
the space, not necessarily bordering any boundary of the input region. An advantage of this latter category of index
structures, including the R-tree and its variants (R*-tree,
R+-tree, Hilbert R-tree, and others), is that these MBRs
can tightly cover the input data objects.
While most index structures have been designed to process individual queries, there are also works focusing on the
execution of a workload of multiple queries on the same index structure. In [27], the Hilbert values of the centroids
of the rectangles formed by the range queries are sorted,
and these queries are grouped accordingly to process them
over an R-tree. In [14], R-trees are used to execute multiple
range queries and, in their formulation, authors propose to
combine adjacent queries into one. Thus, the algorithm is
not able to diÔ¨Äerentiate results of individual queries.
There are two problems commonly associated with multidimensional index structures, namely overlaps between partitions (which cause redundant I/O) and empty spaces
within partitions (which cause unnecessary I/O). While
there has been a signiÔ¨Åcant amount of research in searching
for partitioning strategies that do not face these problems,
these two issues still remain [26] and are especially critical
in very high-dimensional vector spaces. One way to tackle
this problem has been to parallelize the work. For example,
in [6], the authors describe a Hadoop-based data warehousing system with spatial support, where the main focus is to
parallelize the building of the R*-tree index structure and
query processing over Hadoop.

1383

       


       

								 															








 

  



] 
]

Figure 2: Z-order curve for a sample 2D Space.

2.2 Space Filling Curve based Indexing
The two most common space-Ô¨Ålling curves are the fractalbased Z-order curve [23] and the Peano-Hilbert curve [17].
While the Hilbert curve provides a better mapping from
the multidimensional space onto the 1D space, its generation is a complicated and costly process [26]. With Z-order
curve, however, mapping back-and-forth between the multidimensional space and the 1D space using a process called
bit-shuÔ¨Ñing (visualized in Figure 2) is very simple and eÔ¨Écient. Consequently, the Z-order curve has been leveraged
to deal with spatial challenges [12], including construction
of and searches on R-trees [27] and others.
In [35], the authors present a parallel spatial query processing system called VegaGiStore that is built on top of
Hadoop. This system uses a two-tiered index structure that
consists of a quadtree-based global index (used for Ô¨Ånding
the necessary data blocks) and a Hilbert-ordering based local
index (used for Ô¨Ånding the spatial objects in the data block).
In [26], the authors present an index called BLOCK to process spatial range queries. Their main assumption is that
the data and index can Ô¨Åt into the main memory, and hence
their aim is to reduce the number of comparisons between
the data points and the query range. They create a sorted
list of the Z-order values for all the data points. Given a
query, they start at the coarsest level. If a block lies entirely
within the given query range, they retrieve the data points
in this block, otherwise, based on a branching fact, they decide whether to search the next granular level. In [7], the
authors proposed a UB-tree index structure that also uses
Z-ordering for storing multidimensional data in a B+ tree
and in [22], the authors presented a hierarchical clustering
scheme for the fact table of a data warehouse in which the
data is stored using the above mentioned UB-tree. In [31],
the authors present a range query algorithm speciÔ¨Åcally for
the UB-tree. Unlike our approach, the above solutions are
not speciÔ¨Åcally designed for multiple query workloads.

2.3 Bitmap Indexes
There have been signiÔ¨Åcant amount of works on improving the performance of bitmap indexes and keeping compression rates high [20, 32, 33]. Most of the newer compression
algorithms use run-length encoding for compression: this
provides a good compression ratio and enables bitwise operations directly on compressed bitmaps without having to
decompress them Ô¨Årst [32]. Consequently, bitmap indexes
are also shown to perform better than other database index structures, especially in data warehouses and columnoriented systems [5, 32, 34].
For attributes with a large number of distinct values,
bitmaps are often created with binning, where the domain

is partitioned into bins and a bitmap is created for each bin.
Given a query, results are constructed by combining relevant bins using bitwise OR operations. Recognizing that
many data attributes have hierarchical domains, there has
also been research in the area of multi-level and hierarchical
bitmap indexes [13, 24, 25, 29]. When the bitmaps are partitioned (with potential overlaps), it is necessary to select an
appropriate set (or cut [25]) of bitmaps for query processing; results are often obtained by identifying a set of bitmaps
and combining them using bitwise ORs. This work builds on
some of the ideas presented in [25] from 1D data to spatial
data. In [25], the cost model only considered the dominant
I/O cost (reading the bitmaps from the disk), but in this
work, we present an updated cost model, that appropriately
includes the I/O cost as well as the cost of performing local
operations on the in-memory bitmaps.
There has been some prior attempts to leverage bitmaps
in spatial query processing. For example, an MBR-based
spatial index structure is proposed in [30], where the leaves
of the tree are encoded in the form of bitmaps. Given a
query, the proposed HSB-index is traversed top-down (as in
R-trees) to identify the relevant bitmaps to be combined.
In this paper, we note that not only leaves, but also internal nodes of the spatial hierarchy can be encoded as
bitmaps, leading to signiÔ¨Åcant savings in range search time,
especially for query workloads consisting of multiple spatial
range queries. Thus, our work focuses on which bitmaps
to read in the context of spatial range query workloads and
we introduce novel algorithms to choose which bitmaps to
use to answer a query workload eÔ¨Éciently. We generalize
the problem of bitmap selection and consider alternative
strategies that complement OR-based result construction.
In [16], authors propose a storage and retrieval mechanism
for large multi-dimensional HDF5 Ô¨Åles by using bitmap indexes. While range queries are supported on their architecture, they neither leverage Z-order indexing, nor hierarchical bitmaps as proposed in this work. Also, their proposed
mechanism is not optimized for multiple query workloads.

3. COMPRESSED SPATIAL HIERARCHICAL BITMAP (cSHB) INDEXES
In this section, we present the key concepts used in the paper and introduce the compressed spatial hierarchical bitmap
(cSHB) index structure for answering spatial range queries.

3.1 Key Concepts and Notations
3.1.1 Spatial Database
A multidimensional database, D, consists of points that
belong to a (bounded and of Ô¨Ånite-granularity) multidimensional space S with d dimensions. A spatial database is a
special case where d = 2. We consider rectangular spaces
such that the boundaries of S can be described using a pair
of south-west and a north-east corner points, csw and cne
(csw .x ‚â§ cne .x and csw .y ‚â§ cne .y and ‚àÄp‚ààS csw .x ‚â§ p.x ‚â§
cne .x and csw .y ‚â§ p.y ‚â§ cne .y).

3.1.2 Spatial Query Workload
In this paper, we consider query workloads, Q, consisting
of a set of rectangular spatial range queries.
‚Ä¢ Spatial Range Query: A range query, q ‚àà Q, is
deÔ¨Åned by a corresponding range speciÔ¨Åcation q.rs =

1384

qsw , qne , consisting of a south-west point and a northeast point, such that qsw .x ‚â§ qne .x and qsw .y ‚â§ qne .y.

root
00****

0000**

‚Ä¢ Leaves of the hierarchy: LH denotes the set of leaf
nodes of the hierarchy H and correspond to all potential point positions of the Ô¨Ånite space S. Assuming that
the database, D, contains only points, only the leaves
of the spatial hierarchy occur in the database.

‚Ä¢ Children of a node: For all ni , children(ni ) denotes
the children of ni in the corresponding hierarchy; if
ni ‚àà LH , then children(ni ) = ‚àÖ. In this paper, we assume that the children induce a partition of the region
corresponding to the parent node:
‚àÄ

‚éû
Sh ‚é† .

nh ‚ààchildren(ni )

‚Ä¢ Descendants of a Node: The set of descendants of
node ni in the corresponding hierarchy is denoted as
desc(ni ). Naturally, if ni ‚àà LH , then desc(ni ) = ‚àÖ.
‚Ä¢ Internal Nodes: Any node in H that is not a leaf
node is called an internal node. The set of internal
nodes of H is denoted by IH . Each internal node in
the hierarchy corresponds to a (non-point) sub-region
of the given space. If N (H, l) denotes the subset of the
nodes at level l of the hierarchy H, then we have
‚éõ
‚éû



‚àÄn =n ‚ààN (H,l) Si ‚à©Sj = ‚àÖ and ‚éùS =
Si ‚é† .
i

A cSHB index structure can be created based on any hierarchy satisfying the requirements1 speciÔ¨Åed in Section 3.1.3.
In this paper, without loss of generality, we discuss a Zcurve based construction scheme for cSHB. The resulting
hierarchy is analogous to the MX-quadtree data structure,
where all the leaves are at the same level and a given region
is always partitioned to its quadrants at the center [28]. As
introduced in Sections 1.1 and 2.2, a space-Ô¨Ålling curve is a
fractal that maps a given Ô¨Ånite multidimensional data space
onto a 1D curve, while preserving the locality of the multidimensional data points (Figure 2): in other words nearby
points in the data space tend to be mapped to nearby points
on the 1D curve. As we also discussed earlier, Z-curve is a
fractal commonly used as a space-Ô¨Ålling curve (thanks to its
eÔ¨Äectiveness in clustering the points in the data space and
the eÔ¨Éciency with which the mapping can be computed).
A key advantage of the Z-order curve (for our work) is
that, due to the iterative (and self-similar) nature of the underlying fractal, the Z-curve can also be used to impose a
hierarchy on the space. As visualized in Figure 3, each internal node, ni , in the resulting hierarchy has four children
corresponding to the four quadrants of the space, Si . Consequently, given a 2h -by-2h space, this leads to an (h + 1)-level
hierarchy, (analogous to an MX-quadtree [28]) which can be
used to construct a cSHB index structure2 . As we show
in Section 5, this leads to highly compressible bitmaps and
eÔ¨Écient execution plans.

3.2.2 Blocked Organization of Compressed Bitmaps

j

ni ‚ààN (H,l)

The root node corresponds to the entire space, S.
‚Ä¢ Leaf Descendants of a Node: Leaf descendants,
leaf Desc(ni ), of a node are the set of nodes such that
leaf Desc(ni ) = desc(ni ) ‚à© LH .

3.2 Compressed Spatial Hierarchical Bitmap
(cSHB) Index Structure
In this section, we introduce the proposed compressed spatial hierarchical bitmap (cSHB) index structure:
Definition 3.1 (cSHB Index Stucture). Given a
spatial database D consisting of a space, S, and a spatial
hierarchy, H, a cSHB index is a set, B of bitmaps, such
that for each ni ‚àà N (H), there is a corresponding bitmap,
Bi ‚àà B, where the following holds:

ÕôÕôÕòÕò ÕôÕòÕò
001000 001001 001010 001011

3.2.1 Our Implementation of cSHB

‚Ä¢ Parent of a node: For all ni , parent(ni ) denotes the
parent of ni in the corresponding hierarchy; if ni is the
root, then parent(ni ) = ‚ä•.

nh =nj ‚ààchildren(ni )

0011**

‚Ä¢ if ni is an internal node (i.e., ni 	‚àà IH ), then
‚àÉo‚ààD ‚àÉnh ‚ààleaf Desc(ni ) located at(o, nh ) ‚Üî (Bi [o] =
1), whereas
‚Ä¢ if ni is a leaf node
	 (i.e., ni ‚àà LH ), then
‚ó¶
‚àÉo‚ààD located at(o, ni ) ‚Üî (Bi [o] = 1)

‚Ä¢ Nodes of the hierarchy: Intuitively, each node, ni ‚àà
N (H) corresponds to a (bounded) subspace, Si ‚äÜ S,
described by a pair of corner points, ci,sw and ci,nw .



0010**

Figure 3: A sample 4-level hierarchy deÔ¨Åned on the
Z-order space deÔ¨Åned in Figure 2 (the string associated to each node corresponds to its unique label)

In cSHB, we associate to the space S a hierarchy H, which
consists of the node set N (H) = {n1 , . . . , nmaxn }:

‚éõ

Sh ‚à©Sj = ‚àÖ and ‚éùSi =

0001**

ÕôÕôÕòÕò
000000 000001 000010 000011

3.1.3 Spatial Hierarchy



ÕôÕô
ÕôÕòÕò

Given a range query, q, with a range speciÔ¨Åcation,
q.rs = qsw , qne , a data point p ‚àà D is said to be
contained within the query range (or is a range point)
if and only if qsw .x ‚â§ p.x ‚â§ qne .x and qsw .y ‚â§ p.y ‚â§
qne .y.

Given a spatial database, D, with a corresponding hierarchy, H, we create and store a compressed bitmap for each
node in the hierarchy, except for those that correspond to
regions that are empty. These bitmaps are created in a
bottom-up manner, starting from the leaves (which encode
for each point in space, S, which data objects in D are located at that point) and merging bitmaps of children nodes
into the bitmaps of their parents. Each resulting bitmap is
stored as a compressed Ô¨Åle on disk.
It is important to note that, while compression provides
signiÔ¨Åcant savings in storage and execution time, a naive
storage of compressed bitmaps can still be detrimental for
1
In fact, cSHB can be created even when some of the requirements are relaxed ‚Äì for example children do not need
to cover the parent range entirely (as in R-trees).
2
Without loss of generality, we assume that the width and
height are 2h units for some integer h ‚â• 1.

1385

000 001 010 011 100 101 110 111

nj ‚ààchildren(ni )

18:
19:
20:
21:
22:
23:
24:
25:

end if
if size(Bi ) ‚â• K then
write Bi to disk;
else
T = append(T, Bi )
availableSize = availableSize ‚àí size(Bi )
if (availableSize ‚â§ 0) or (ni is the last
node at this level) then
write T to disk;
Block T = ‚àÖ
availableSize = K
end if
end if
end for
end for
end procedure

performance: in particular, in a data set with large number
of objects located at unique points, there is a possibility that
a very large number of leaf bitmaps need to be created on
the secondary storage. Thus, creating a separate bitmap Ô¨Åle
for each node may lead to ineÔ¨Éciencies in indexing as well as
during query processing (as directory and Ô¨Åle management
overhead of these bitmaps may be non-negligible).
To overcome this problem, cSHB takes a target block size,
K, as input and ensures that all index-Ô¨Åles written to the
disk (with the possible exception of the last bitmap Ô¨Åle in
each level) are at least K bytes. This is achieved by concatenating, if needed, compressed bitmap Ô¨Åles (corresponding to
nodes at the same level of hierarchy). In Algorithm 1, we
provide an overview of this block-based bottom-up cSHB index creation process. In Line 10, we see that the bitmap of
an internal node is created by performing a bitwise OR operation between the bitmaps of the children of the node. These
OR operations are implemented in the compressed bitmap
domain enabling fast creation of the bitmap hierarchy. As it
creates compressed bitmaps, the algorithm packs them into
a block (Line 15). When the size of the block exceeds K,
the compressed bitmaps in the block are written to the disk
(Line 18) as a single Ô¨Åle and the block is re-initialized.
Example 3.1. Let us assume that K = 10 and also that
we are considering the following sequence of nodes with the
associated (compressed) bitmap sizes:
n1 , 3; n2 , 4; n3 , 2; n4 , 15; n5 , 3; . . .
This sequence of nodes will lead to following sequence of
bitmap Ô¨Åles materialized on disk:
[B4 ] ; [B1 B2 B3 B5 ] ; . . .






size=15

size=3+4+2+3=12

Note that, since the bitmap for node n4 is larger than the
target block size, B4 is written to disk as a separate bitmap

0

5 6 7

5 6 7
3 4

1

2

3 4

5 6 7

Range
11100X2
[56,57]

2
1

1
0

(a) 2D query range

0

3 4

2

‚Ä¢ Minimum block size, K

11:
12:
13:
14:
15:
16:
17:

2

3 4

‚Ä¢ A spatial database, D, deÔ¨Åned over 2h -by-2h size space, S
and a corresponding (h + 1)-level (Z-curve based) hierarchy,
H, with set of internal nodes, IH

2: procedure writeBitmaps
3:
Block T = ‚àÖ
4:
availableSize = K
5:
for level l = (h + 1) (i.e., leaves) to 0 (i.e., root) do
6:
for each node ni in l in increasing Z-order do
7:
if l == (h + 1) then
8:
Initialize a compressed bitmap Bi
9:
else
10:
Bi =
OR
Bj

1

5 6 7

0

000 001 010 011 100
0 101 110 111

Algorithm 1 Writing blocks of compressed bitmaps to disk
1: Input:

Range
1100XX1
[48,51]

(b) Corresponding 1D ranges

Figure 4: Mapping of a single spatial range query
to two 1D ranges on the Z-order space: (a) A contiguous 2D query range, [sw = (4, 4); ne = (6, 5)] and
(b) the corresponding contiguous 1D ranges, [48,51]
and [56,57], on the Z-curve
Ô¨Åle; on the other hand, bitmaps for nodes n1 , n2 , n3 , and
n5 need to be concatenated into a single Ô¨Åle to obtain a block
larger than K = 10 units.
3
Note that this block-based structure implies that the size
of the Ô¨Åles and the number of bitmap Ô¨Åles on the disk will be
upper bounded, but it also means that the cost of the bitmap
reads will be lower bounded by K. Therefore, to obtain
the best performance, repeated access to a block to fetch
diÔ¨Äerent bitmaps must be avoided through bitmap buÔ¨Äering
and/or bitmap request clustering. In the next section, we
discuss the use of cSHB index for range query processing. In
Section 5, we experimentally analyze the impact of block-size
on the performance of the proposed cSHB index structure.

4. QUERY PROCESSING WITH THE cSHB
INDEX STRUCTURE
In this section, we describe how query workloads are processed using the cSHB index structure. In particular, we
consider query workloads involving multiple range queries
and propose spatial bitmap selection algorithms that select
a subset of the bitmap nodes from the cSHB index structure
for eÔ¨Écient processing of the query workload.

4.1 Range Query Plans and Operating Nodes
In order to utilize the cSHB index for answering a spatial
range query, we Ô¨Årst need to map the range speciÔ¨Åcation
associated with the given query from the 2D space to the
1D space (deÔ¨Åned by the Z-curve). As we see in Figure 4,
due to the way the Z-curve spans the 2D-space, it is possible
that a single contiguous query range in the 2D space may
be mapped to multiple contiguous ranges on the 1D space.
Therefore, given a 2D range query, q, we denote the resulting
set of (disjoint) 1D range speciÔ¨Åcations, as RSq .
Let us be given a query, q, with the set of 1D range speciÔ¨Åcations, RSq . Naturally, there may be many diÔ¨Äerent ways
to process the query, each using a diÔ¨Äerent set of bitmaps
in the cSHB index structure, including simply fetching and
combining only the relevant leaf bitmaps:
Example 4.1 (Alternative Range Query Plans).
Consider a query q with q.rs = (1, 0), (3, 1) on
the space shown in Figure 2.
The corresponding 1D range, [2, 11], would cover the following
leaf nodes of the hierarchy shown in Figure 3:
RSq = (000010, 000011, 001000, 001001, 001010, 001011).
The following are some of the alternative query plans for q
using the proposed cSHB index structure:

1386

‚Ä¢ Inclusive query plans: The most straightforward way
to execute the query would be to combine (bitwise OR
operation) the bitmaps of the leaf nodes covered in 1D
range, [2, 11]. We refer to such plans, which construct
the result by combining bitmaps of selected nodes using
the OR operator, as inclusive plans.
An alternative inclusive plan for this query would be to
combine the bitmaps of nodes 000010, 000011, 0010**:
B000010 OR B000011 OR B0010‚àó‚àó .
‚Ä¢ Exclusive query plans: In general, an exclusive query
plan includes removal of some of the children or descendant bitmaps from the bitmaps of a parent or ancestor through the ANDNOT operation. One such exclusive plan would be to combine the bitmaps of all leafs
nodes, except for B000010 , B000011 , B001000 , B001001 ,
B001010 , B001011 , into a bitmap Bnon result and return
Broot ANDNOTBnon result .
‚Ä¢ Hybrid query plans: Both inclusive and exclusive only
query plans may miss eÔ¨Écient query processing alternatives. Hybrid plans combine inclusive and exclusive
strategies at diÔ¨Äerent nodes of the hierarchy. A sample
hybrid query plan for the above query would be

	
B0000‚àó‚àó ANDNOT (B000000 OR B000001 ) OR B0010‚àó‚àó . 3
As illustrated in the above example, a range query, q, on
hierarchy H, can be answered using diÔ¨Äerent query plans,
involving bitmaps of the leaves and certain internal nodes of
the hierarchy, collectively referred to as the operating nodes
of a query plan. In Section 4.3, we present algorithms for selecting the operating nodes for a given workload, Q; but Ô¨Årst
we discuss the cost model that drives the selection process.

4.2 Cost Models and Execution Strategies
In cSHB, the bitwise operations needed to construct the
result are performed on compressed bitmaps directly, without having to decompress them.

4.2.1 Cost Model for Individual Operations
We consider two cases: (a) logical operations on diskresident compressed bitmaps and (b) logical operations on
in-buÔ¨Äer compressed bitmaps.

Operations on Disk-Resident Compressed Bitmaps.
In general, when the logical operations are implemented
on compressed bitmaps that reside on the disk, the time
taken to read a bitmap from the secondary storage to the
main memory dominates the overall bitwise manipulation
time [15]. The overall cost is hence proportional to the size
of the (compressed) bitmap Ô¨Åle on the secondary storage.
Let us consider a logical operation on bitmaps Bi and Bj .
Let us assume that T (Bi ) and T (Bj ) denotes the blocks in
which Bi and Bj are stored, respectively. Since multiple
bitmaps can be stored in a single block, it is possible that
Bi and Bj are in the same block. Hence, let us further
assume that T(Bi ,Bj ) is the set of unique blocks that contain
the bitmaps, Bi and Bj . Then the overall I/O cost is:
	
 
costio (Bi op Bj ) = Œ±IO
size(T ) ,
T ‚ààT(B ,B )
i
j

where Œ±IO is an I/O cost multiplier and op is a binary bitwise
logical operator. A similar result also holds for the unary
operation NOT.

Operations on In-Buffer Compressed Bitmaps.
When the compressed bitmaps on which the logical operations are implemented are already in-memory, the disk
access cost is not a factor. However, also in this case, the
cost is proportional to the sizes of the compressed bitmap
Ô¨Åles in the memory, independent of the speciÔ¨Åc logical operator that is involved [33], leading to
	

costcpu (Bi op Bj ) = Œ±cpu size(Bi ) + size(Bj ) ,
where Œ±cpu is the CPU cost multiplier. A similar result also
holds for the unary operation NOT.

4.2.2 Cost Models for Multiple Operations
In this section, we consider a cost model which assumes
that blocks are disk-resident. Therefore, we consider a storage hierarchy consisting of disk (storing all bitmaps), RAM
(as buÔ¨Äer storing relevant bitmaps), and L3/L2 caches (storing currently needed bitmaps).

Buffered Strategy.
In the buÔ¨Äered strategy, visualized in Figure 1, the
bitmaps that correspond to any leaf or non-leaf operating
nodes for the query plan of a given query workload, Q, are
brought into the buÔ¨Äer once and cached for later use. Then,
for each query q ‚àà Q, the corresponding result bitmap is extracted using these buÔ¨Äered operating node bitmaps. Consequently, if a node is an operating one for more than one
q ‚àà Q, it is read from the disk only once (and once for each
query from the memory). Let us assume that TONQ denotes
the set of unique blocks that contains all the necessary operating nodes given a query workload Q(ONQ ). This leads
to the overall processing cost, time costbuf (Q, ONQ ), of
‚éû
‚éû
‚éõ
‚éõ

 
size(T )‚é† + Œ±cpu ‚éù
size(Bi )‚é† .
Œ±IO ‚éù



T ‚ààTON

Q



read cost






q‚ààQ ni ‚ààONq





operating cost

Since all operating nodes need to be buÔ¨Äered, this execution
 strategy requires a total of storage costbuf (Q, ONQ ) =
Note that, in general,
ni ‚ààONQ size(Bi ) buÔ¨Äer space.
Œ±IO > Œ±cpu . However, in Section 5, we see that the number
of queries in the query workload and query ranges determine
the relative costs of in-buÔ¨Äer operations vs. disk I/O.
The buÔ¨Äered strategy has the advantage that each query
can be processed individually on the buÔ¨Äered bitmaps and
the results for each completed query can be pipelined to the
next operator without waiting for the results of the other
queries in the workload. This reduces the memory needed
to temporarily store the result bitmaps. However, in the
buÔ¨Äered strategy, the buÔ¨Äer needed to store the operating
node bitmaps can be large.

Incremental Strategy.
The incremental strategy avoids buÔ¨Äering of all operating
node bitmaps simultaneously. Instead, all leaf and non-leaf
operating nodes are fetched from the disk one at a time on
demand and results for each query are constructed incrementally. This is achieved by considering one internal operating
node at a time and, for each query, focusing only on the leaf
operating nodes under that internal node. For this purpose,
a result accumulator bitmap, Resj , is maintained for each
query in qj ‚àà Q and each operating node read from the disk
is applied directly on this result accumulator bitmap.

1387

	 




#!!"&
 

If a set of internal nodes of H only satisÔ¨Åes the Ô¨Årst condition, then we refer to the cut as an incomplete cut.
‚ó¶



















Figure 5: BuÔ¨Äer misses and the overall read time
(data and other details are presented in Section 5)
While it does not need buÔ¨Äer to store all operating node
bitmaps, the incremental strategy may also beneÔ¨Åt from partial caching of the relevant blocks. This is because, while
each internal node needs to be accessed only once, each
leaf node under this internal node may need to be brought
to the memory for multiple queries. Moreover, since the
data is organized in terms of blocks, rather than individual nodes (Section 3.2.2), a single block may serve multiple
nodes to diÔ¨Äerent queries. When suÔ¨Écient buÔ¨Äer is available
to store the working set of blocks (containing the operating
leaf nodes under the current internal node), the execution
cost, time costinc (Q, ONQ ), of the incremental strategy is
identical to that of the buÔ¨Äered strategy. Otherwise, as illustrated in Figure 5, the read cost component is a function of
buÔ¨Äer misses, Œ±IO √ó # buf f er misses, which itself depends
on the size of the buÔ¨Äer and the clustering of the data.
3
The storage complexity is storage costinc (Q, ONQ ) =
size(Res
)
plus
the
space needed to maintain the
j
qj ‚ààQ
most recently read blocks in the current working set. Experiments reported in Section 5 show that, for the considered
data sets, the sizes of the working sets are small enough to
Ô¨Åt into the L3-caches of many modern hardware.

4.3 Selecting the Operating Bitmaps for a
Given Query Workload
To process a range query workload, Q, on a data set, D,
with the underlying cSHB hierarchy H, we need to select
a set of operating bitmap nodes, ONQ , of H from which
we can construct the results for all qj ‚àà Q, such that
time cost(Q, ONQ ) is the minimum among all possible sets
of operating bitmaps for Q. It is easy to see that the number
of alternative sets of operating bitmaps for a given query
workload Q is exponential in the size of the hierarchy H.
Therefore, instead of seeking the set of operating bitmaps
among all subsets of the nodes in H, we focus our attention
on the cuts of the hierarchy, deÔ¨Åned as follows:
Definition 4.1 (Cuts of H Relative to Q). A
complete cut, C, of a hierarchy, H, relative to a query load,
Q, is a subset of the internal nodes (including the root) of
the hierarchy, satisfying the following two conditions:
‚Ä¢ validity: there is exactly one node on any root-to-leaf
branch in a given cut; and
‚Ä¢ completeness: the nodes in C collectively cover every
possible root-to-leaf branch for all leaf nodes in the result sets for queries in Q.
3
The space complexity of the incremental strategy can be
upper-bounded if the results for the queries in Q can be
pipelined to the next set of operators progressively as partial
results constructed incrementally.

As visualized in Figure 1, given a cut C, cSHB queries are
processed by using only the bitmaps of the nodes in this
cut, along with some of the leaf bitmaps necessary
to construct results of the queries in Q. In the rest of
this subsection, we Ô¨Årst describe how queries are processed
given a cut, C, of H and then present algorithms that search
for a cut, C, given a workload, Q.

4.3.1 Range Query Processing with Cuts
It is easy to see that any workload, Q, of queries can be
processed by any (even incomplete) cut, C, of the hierarchy
and a suitable set of leaf nodes: Let Rq denote the set of
leaf nodes that appear in the result set of query q ‚àà Q and
RÃÑq be the set of leaf nodes that do not appear in the result
set. Let also RqC be the set of the result leaves covered by a
node in C. Then, one possible way to construct the result
bitmap, Bq , is as follows:
‚éõ
‚éû



‚éú
‚éü
‚éú
‚éü
OR
Bq = ‚éú OR Bi OR
Bi ‚éü ANDNOT
Bj
.


‚éù ni ‚ààC
‚é† nj ‚ààRC
ni ‚ààRq \RC
q
q ‚à©RÃÑq
exclusions




inclusions

Intuitively any result nodes that are not covered by the cut
need to be included in the result using a bitwise OR operation, whereas any leaf node that is not in any result needs
to be excluded using an ANDNOT operation. Consequently,
‚Ä¢ if C ‚à© Rq = ‚àÖ, an inclusion-only plan is necessary,
‚Ä¢ an exclusion-only plan is possible only if C covers Rq
completely.
Naturally, given a range query workload, Q, diÔ¨Äerent query
plans with diÔ¨Äerent cuts will have diÔ¨Äerent execution costs.
The challenge is, then,
‚Ä¢ to select an appropriate cut, C, of the hierarchy, H,
for query workload, Q, and
‚Ä¢ to pick, for each query qj ‚àà Q, a subset Cj ‚àà C for
processing qj ,
in such a way that these will minimize the overall processing
cost for the set of range queries in Q. Intuitively, we want to
include in the cut, those nodes that will not lead to a large
number of exclusions and cannot be cheaply constructed by
combining bitmaps of the leaf nodes using OR operations.

4.3.2 Cut Bitmap Selection Process
Given the above cut-based query processing model, in this
section we propose a cut selection algorithm consisting of
two steps: (a) a per-node cost estimation step and (b) a
bottom-up cut-node selection step. We next describe each
of these two steps.

Node Cost Estimation.
First, the process assigns an estimated cost to those hierarchy nodes that are relevant to the given query workload,
Q. For this, the algorithm traverses through the hierarchy,
H, in a top-down manner and identiÔ¨Åes part, R, of the hierarchy relevant for the execution of at least one query, q ‚àà Q
(i.e., for at least one query, q, the range associated with the
node and the query range intersect). Note that this process

1388

‚Ä¢ Exclusive leaf access plan (Line 18): If query, q, is executed using an exclusive leaf access plan at node, ni ,
this means that the result for the range (q.rs ‚à© Si ) will
be obtained by using Bi and then identifying and excluding (using bitwise ANDNOT operations) all irrelevant leaf bitmaps under node ni . Thus, we compute
the exclusive leaf access plan cost, ecost(ni , q), of this
query at node ni as

Algorithm 2 Cost and Leaf Access Plan Assignment Algorithm
1: Input: Hierarchy H, Query Workload Q
2: Outputs: Query workload, Q(ni ), and cost estimate, costi ,

3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:

for each node, ni ‚àà H; leaf access plan, Ei,j , for all
node/query pairs ni ‚àà H and qj ‚àà Q(ni ); a set, R ‚äÜ IH ,
or relevant internal nodes
Initialize: R = ‚àÖ
procedure Cost and LeafAccessPlanAssignment
for each internal node ni ‚àà IH in top-down fashion do
if ni = ‚Äúroot then
Q(ni ) = Q
else
Q(ni ) = {q ‚àà Q(parent(ni )) s.t. (q.rs ‚à© Si ) =
‚àÖ}
end if
if Q(ni ) = ‚àÖ then
add ni into R
end if
end for
for each node ni ‚àà R in a bottom-up fashion do
for qj ‚àà Q(ni ) do
Compute icost(ni , q)
Compute ecost(ni , q)
Compute the leaf access plan, Ei,j , as
Ei,j = [ecost(ni , qj ) < icost(ni , qj )]
end for

 Compute the leaf access cost, leaf costi , as
qj ‚ààQ(ni ) Ei,j √ó ecost(ni , qj ) + (1 ‚àí Ei,j ) √ó icost(ni , qj )
end for
end procedure

also converts the range in 2-D space into 1-D space by identifying the relevant nodes in the hierarchy. Next, for each
internal node, ni ‚àà R, a cost, costi , is estimated assuming
that this node and its leaf descendants are used for identifying the matches in the range Si . The outline of this process
is presented in Algorithm 2 and is detailed below:
‚Ä¢ Top-Down Traversal and Pruning. Line 5 indicates that
the process starts at the root and moves towards the leaves.
For each internal node, ni , being visited, Ô¨Årst, the set,
Q(ni ) ‚äÜ Q, of queries for which ni is relevant is identiÔ¨Åed by
intersecting the ranges of the queries relevant to the parent
(i.e., Q(parent(ni ))) with the range of ni . More speciÔ¨Åcally,
Q(ni ) = {q ‚àà Q(parent(ni )) s.t. (q.rs ‚à© Si ) = ‚àÖ}.
If Q(ni ) = ‚àÖ, then ni and all its descendants are ignored,
otherwise ni is included in the set R.
‚Ä¢ Inclusive and Exclusive Cost Computation. Once the portion, R, of the hierarchy relevant to the query workload is
identiÔ¨Åed, next, the algorithm re-visits all internal nodes in
R in a bottom-up manner and computes a cost estimate for
executing queries in Q(ni ): for each query, q ‚àà Q(ni ), the
algorithm computes inclusive and exclusive leaf access costs:
‚Ä¢ Inclusive leaf access plan (Line 17): If query, q, is executed using an inclusive plan at node, ni , this means
that the result for the range (q.rs‚à©Si ) will be obtained
by identifying and combining (using bitwise ORs) all
relevant leaf bitmaps under node ni . Therefore, the
cost of this leaf access plan is

icost(ni , q) =
size(Bj ).
(nj ‚ààleaf Desc(ni ))‚àß((q.rs‚à©Sj )=‚àÖ)

This value can be computed incrementally, simply by
summing up the inclusive costs of the children of ni .

ecost(ni , q)

= size(Bi )
+



size(Bj )

(nj ‚ààleaf Desc(ni ))‚àß((q.rs‚à©Sj )=‚àÖ)

or equivalently as
ecost(ni , q)

‚éõ

= size(Bi ) + ‚éù



‚éû
size(Bj )‚é†

nj ‚ààleaf Desc(ni )

‚àí icost(ni , q)
Since the initial two terms above are recorded in the
index creation time, the computation of exclusive cost
is a constant time operation.
‚Ä¢ Overall Cost Estimation and the Leaf Access Plan. Given
the above, we can Ô¨Ånd the best strategy for processing the
query set Q(ni ) at node ni by considering the overall estimated cost term, cost(ni , Q(ni )), deÔ¨Åned as
‚éû
‚éõ

‚éù
Ei,j √ó ecost(ni , qj ) + (1 ‚àí Ei,j ) √ó icost(ni , qj )‚é†



qj ‚ààQ(ni )





leaf access cost f or all relevant queries

where Ei,j = 1 means an exclusive leaf access plan is chosen
for query, qj , at this node and Ei,j = 0 otherwise.

Cut Bitmap Selection.
Once the nodes in the hierarchy are assigned estimated
costs as described above, the cut that will be used for
query processing is found by traversing the hierarchy in a
bottom-up fashion and picking nodes based on their estimated costs4 . The process is outlined in Algorithm 3. Intuitively, for each internal node, ni ‚àà IH , the algorithm computes a revised cost estimate, rcosti , by comparing the cost,
costi , estimated in the earlier phase of the process, with the
total revised costs of ni ‚Äôs children:
‚Ä¢ In Line 13, the function f indBlockIO(ni ) returns the
cost of reading the block T (Bi ). If this block has already been marked ‚Äúto-read‚Äù, then the reading cost has
already been accounted for, so the cost is zero. Otherwise, the cost is equal to the size of the block T (Bi ),
as explained in Section 4.2.1.
‚Ä¢ As we see in Line 21, it is possible that a block T is
Ô¨Årst marked ‚Äúto-read‚Äù and then, later in the process,
marked ‚Äúnot-to-read‚Äù, because for the corresponding
nodes in the cut, more suitable ancestors are found
and the block is no longer needed.
‚Ä¢ If costi is smaller (Line 17), then ni and its leaf descendants can be used for identifying the matches to
the queries in the range Si . In this case, no revision is
4

Note that this bottom-up traversal can be combined with
the bottom-up traversal of the prior phase. We are describing them as separate processes for clarity.

1389

Algorithm 3 Cut Selection Algorithm
1: Input: Hierarchy H; per-node query workload Q(ni ); per-

2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:

node cost estimates costi ; and the corresponding leaf access
plans, Ei,j , for node/query pairs ni ‚àà H and qj ‚àà Q(ni ); the
set, R ‚äÜ IH , or relevant internal nodes
Output: All-inclusive, CI , and Exclusive, CE , cut nodes
Initialize: Cand = ‚àÖ
procedure findCut
for each relevant internal node ni in R in a bottomup fashion do
Set internal children = children(ni ) ‚à© IH ;
if internal children = ‚àÖ then
add ni to Cand;
rcosti = costi
else

costChildren = nj ‚ààinternal children rcostj
rcostIOi = f indBlockIO(ni )
for each child nj in internal children do
costChildrenIO
=
costChildrenIO +
f indBlockIO(nj )
end for
if (rcosti + rcostIOi ) ‚â§ (costChildren +
costChildrenIO) then
for each descendant nk of ni in Cand do
remove nk from Cand;
if nk is the only node to read from
T (Bk ) then
mark T (Bk ) as ‚Äúnot-to-read‚Äù;
end if
end for
add ni to Cand;
rcosti = costi
mark T (Bi ) as ‚Äúto-read‚Äù;
else
rcosti = costChildren
end if
end if
end for
CE = {ni ‚àà Cand s.t. ‚àÉqj ‚ààQ(ni ) Ei,j == 1}
CI = Cand/CE
end procedure

answered only by accessing relevant leaves under the nodes
in CI . We store the blocks containing the bitmaps of these
relevant leaves in an LRU-based cache so that leaf bitmaps
can be reused by multiple queries.

4.3.3 Complexity
The bitmap selection process consists of two steps: (a) a
per-node cost estimation step and (b) a cut bitmap selection
step. Each of these steps visit only the relevant nodes of the
hierarchy. Therefore, if we denote the set of nodes of the
hierarchy, H, that intersect with any query in Q, as H(Q),
then the overall work is linear in the size of H(Q).
During the cost estimation phase, for each visited node,
ni , an inclusive and exclusive cost is estimated for any query
that intersects with this node. Therefore, the worst case
time cost of the overall process (assuming that all queries in
Q intersect with all nodes in H(Q)) is O(|Q| √ó |H(Q)|).

5. EXPERIMENTAL EVALUATION
In this section, we evaluate the eÔ¨Äectiveness of the proposed compressed spatial hierarchical bitmap (cSHB) index
structure using spatial data sets with diÔ¨Äerent characteristics, under diÔ¨Äerent system parameters. To assess the eÔ¨Äectiveness of cSHB, we also compare it against alternatives.
We ran the experiments on a quad-core Intel Core i5-2400
CPU @ 3.10GHz machine with 8.00GB RAM, and a 3TB
SATA Hard Drive with 7200 RPM and 64MB BuÔ¨Äer Size,
and in the same Windows 7 environment. All codes were
implemented and run using Java v1.7.

5.1 Alternative Spatial Index Structures and
the Details of the cSHB Implementation
As alternatives to cSHB, we considered diÔ¨Äerent systems
operating based on diÔ¨Äerent spatial indexing paradigms. In
particular, we considered spatial extensions of PostgreSQL
called PostGIS [2], of a widely used commercial DBMS
(which we refer to as DBMS-X), and of Lucene [1]:
‚Ä¢ PostGIS [2] creates spatial index structures using an
R-tree index implemented on top of GiST.

necessary and the revised cost, rcosti is equal to costi .
Any descendants of ni are removed from the set, Cand,
of cut candidates and ni is inserted instead.
‚Ä¢ If, on the other hand, the total revised cost of ni ‚Äôs children is smaller than costi , then matches to the queries
in the range Si can be more cheaply identiÔ¨Åed by considering the descendants of ni , rather than ni itself
(Line 27). Consequently, in this case, the revised cost,
rcosti , is set to

rcostj .
rcosti =

‚Ä¢ DBMS-X maps 2D space into a 1D space using a variation of Hilbert space Ô¨Ålling curve and then indexes
the data using B-trees.
‚Ä¢ Apache Lucene [1,18], a leading system for text indexing and search, provides a spatial module that supports
geo-spatial range queries in 2D space using quadtrees
and preÔ¨Åx-based indexing. Intuitively, the space is partitioned using a MX-quadtree structure (where all the
leaves are at the same level and a given region is always
partitioned to its quadrants at the center [28]) and each
root-to-leaf path is given a unique path-string. These
path-strings are then indexed (using eÔ¨Écient preÔ¨Åxindexing algorithms) for spatial query processing.

nj ‚ààchildren(ni )

As we experimentally show in Section 5, the above process
has a small cost. This is primarily because, during bottomup traversal, only those nodes that have not been pruned
in the previous top-down phase are considered. Once the
traversal is over, the nodes in the set, Cand, of cut candidates are reconsidered and those that include exclusive leaf
access plans are included in the exclusive cut set, CE , and
the rest are included in the all-inclusive cut set, CI .

Since database systems potentially have overheads beyond
pure query processing needs, we also considered disk-based
implementations of R*-tree [8] and the Hilbert R-tree [19].
For this purpose, we used the popular XXL Java library [10]:
‚Ä¢ A packed R*-tree, with average leaf node utilization
‚àº 95% (page size 4MB).

Caching of Cut and Leaf Bitmaps.
During query execution, the bitmaps of the nodes in CE
are read into a cut bitmaps buÔ¨Äer, whereas the bitmaps for
the nodes in CI do not need to be read as the queries will be

‚Ä¢ A packed Hilbert R-tree, with average leaf node utilization ‚àº 99% (page size 4MB).

1390

Table 1: Data sets and clustering
Data set

#points

Synthetic (Uniform)
Gowalla (Clustered)
OSM (Clustered)

100M
6.4M
688M

Data
set
Synthetic
Gowalla
OSM

Data
set
Synthetic
Gowalla
OSM

Clustered (6.4M; Gowalla)

25

Clustered (688M; OSM)

y = -2.0x + 15.0

Linear (Uniform (100M; Synth))

20

Linear (Clustered (6.4M; Gowalla))

y = -1.7x + 13.3

15

cSHB

Luc.

1601
24
2869

2396
114
12027

DBMS
-X
3865
232
30002

Post
GIS
4606
112
76238

R*tree
2160
22
18466

Hilb.
R-tree
2139
20
17511

Table 4: Index Size on Disk (MB)

Uniform (100M; Synth)

Data Skew
log (# of non-empty cells)

Table 3: Index Creation Time (sec.)

#points per (nonempty) cell (h = 10)
Min.
Avg.
Max.
54
95
143
1
352
312944
1
3422
1.2M

Linear (Clustered (688M; OSM))

cSHB

Luc.

10900
44
2440

5190
220
22200

DBMS
-X
1882
121
12959

Post
GIS
8076
600
61440

R*tree
3210
211
22100

Hilb.
R-tree
1510
100
10400

y = -1.3x + 11.1

10
5
0
-5

-3

-1

1

3

5

7

log (r)

Figure 6: Data Skew
Table 2: Parameters and default values (in bold)
Parameter
Block Size (MB)
Query range size
|Q|
h
BuÔ¨Äer size (MB)

Value range
0.5; 1; 2.5; 5; 10
0.5% 1%; 5%
100; 500; 1000
9; 10; 11
2; 3; 5; 10; 20; 100

We also implemented the proposed cSHB index structure on
top of Lucene. In particular, we used the MX-quadtree hierarchy created by Lucene as the spatial hierarchy for building cSHB. We also leveraged Lucene‚Äôs (Java-based) region
comparison libraries to implement range searches. The compressed bitmaps and compressed domain logical operations
were implemented using the JavaEWAH library [21]. Due
to space limitations, we only present results with the incremental strategy for query evaluation.

5.2 Data Sets
For our experiments, we used three data sets:
(a) a uniformly distributed data set that consists of 100
million synthetically generated data points.
These
points are mapped to the range ‚àí180, ‚àí90 to 180, 90,
(b) a clustered data set from Gowalla, which contains the
locations of check-ins made by users. This data set is downloaded from the Standford Large Network Dataset Collection [4], and (c) a clustered data set from OpenStreetMap
(OSM) [3] which contains locations of diÔ¨Äerent entities distributed across North America. The OSM data set consists
of approximately 688 million data points in North America. We also normalized both the real data sets to the range
‚àí180, ‚àí90 to 180, 90. In order to obtain a fair comparison across all index structures and the data sets, all three
data sets are mapped onto a 2h √ó 2h space and the positions
of the points in this space are used for indexing. Table 1
provides an overview of the characteristics of these three
very diÔ¨Äerent data sets. Figure 6 re-conÔ¨Årms the data skew
in the three data sets using the box-counting method proposed in [9]: in the Ô¨Ågure, the lower the negative slope, the
more skewed the data. The Ô¨Ågure shows that the clustered
Gowalla data set has the largest skew.

5.3 Evaluation Criteria and Parameters
We evaluate the eÔ¨Äectiveness of the proposed compressed
spatial hierarchical bitmap (cSHB) index structure by com-

paring its (a) index creation time, (b) index size, and (c)
query processing time to those of the alternative index structures described above under diÔ¨Äerent parameter settings.
Table 2 describes the parameters considered in these experiments and the default parameter settings.
Since our goal is to assess the contribution of the index in
the cost of the query plans, all index structures in our comparison used index-only query plans. More speciÔ¨Åcally, we
executed a count(‚àó) query and conÔ¨Ågured the index structures such that only the index is used to identify the relevant
entries and count them to return the results. Consequently,
only the index Ô¨Åles are used and data Ô¨Åles are not accessed.
Note that all considered index structures accept squareshaped query ranges. The range sizes indicated in Table 2
are the lengths of the boundaries relative to the size of the
considered 2D space. These query ranges in the query workloads are generated uniformly.

5.4 Discussion of the Indexing Results
Indexing Time. Table 3 shows the index creation times for
diÔ¨Äerent systems and index structures, for diÔ¨Äerent data sets
(with diÔ¨Äerent sizes and uniformity): cSHB index creation
is fastest for the larger Synthetic and OSM data sets, and
competitive for the smaller Gowalla data set. As the data
size gets larger, the alternative index structures become signiÔ¨Åcantly slower, whereas cSHB is minimally aÔ¨Äected by the
increase in data size. The index creation time also includes
the time spent on creating the hierarchy for cSHB.
Index Size. Table 4 shows the sizes of the resulting index Ô¨Åles for diÔ¨Äerent systems and index structures and for
diÔ¨Äerent data sets. As we see here, cSHB provides a competitive index size for uniform data (where compression is not
very eÔ¨Äective). On the other hand, on clustered data, cSHB
provides very signiÔ¨Åcant gains in index size ‚Äì in fact, even
though the clustered data set, OSM, contains more points,
cSHB requires less space for indexing this data set than it
does for indexing the uniform data set.
Impact of Block Size. As we discussed in Section 3.2.2,
cSHB writes data on the disk in a blocked manner. In Figure 7, we see the impact of the block sizes on the time needed
to create the bitmaps. As we see here, one advantage of using blocked storage is that the larger the blocks used, the
faster the index creation becomes.

5.5 Discussion of the Search Results
Impact of the Search Range. Table 5 shows the impact
of the query range on search times for 500 queries under
the default parameter settings, for diÔ¨Äerent systems. As we
expected, as the search range increases, the execution time

1391

Impact of the Block Size on
Index Creation Time

Writing Bitmaps

Time (sec.)

Time (sec., log. scale)

Creating Bitmaps

1800

1350
900
450
0
0.5

1

2.5

Read Hierarchy
Map Ranges
Search Cut Bitmaps
Read Bitmaps
Combine Bitmaps

5

1000
100

Tot: 42

Tot: 51

0.5%
1%
5%

35
42
137

123
131
187

0.5%
1%
5%

2
3
3

2
3
48

0.5%
1%
5%

13
15
28

23
30
66

1

90
Time (sec.)

cSHB
-LO

60

2
3
5
13
14
78

Tot: 22

Tot: 11

1

5%

Synthetic (Uniform; 100M)

0.5%

1%

5%

Gowalla (Clustered; 6.4M)

2.5
Block Size (MB)

1%

cSHB Time Breakdown
(1% Q. Range, Uniform Data)
Tot: 32

Data sets

Figure 8: cSHB execution breakdown
becomes larger for all alternatives. However, cSHB provides
the best performance for all ranges considered, especially for
the clustered data sets. Here, we also compare cSHB with
its leaf-only version (called cSHB-LO), where instead of a
cut consisting of potentially internal nodes, we only choose
the leaf nodes for query processing. As you can see from the
Ô¨Ågure, while cSHB-LO is a good option for very small query
ranges (0.5% and 1%), it becomes very slow as the query
range increases (since the number of bitwise operations increases, and it is not able to beneÔ¨Åt from clustering).
Execution Time Breakdown. Figure 8 provides a breakdown of the various components of cSHB index search (for
500 queries under the default parameter settings): The
bitmap selection algorithm presented in Section 4.3 is extremely fast. In fact, the most signiÔ¨Åcant components of
the execution are the times needed for reading the hierarchy
into memory5 , and for fetching the selected bitmaps from
the disk into the buÔ¨Äer, and performing bitwise operations
on them. As expected, this component sees a major increase
5
Once a hierarchy is read into the memory, the hierarchy
does not need to be re-read for the following queries.

Tot: 42

Read Hierarchy
Map Ranges
Search Cut Bitmaps
Read Bitmaps
Combine Bitmaps

Tot: 61

10
1

0.1
500

1000

Number of Queries

5%

OSM (Clustered;
688M)

10

Figure 9: Impact of the block size (500 queries,
1% q. range, uniform data)

100

0.5%

5

(b) Impact of block size on bitmap reading time

100

1%

Tot: 55

Tot: 31
30

0.5

0.1
0.5%

Tot: 84

0

Time (sec., log. scale)

Time (sec., log. scale)

Impact of Block Sizes on
Bitmap Reading Time

52
59
1700

Read Hierarchy
Map Ranges
Search Node Bitmaps
Read Bitmaps
Combine Bitmaps

1

10

(a) Impact of block size on overall cSHB execution time

Range Search Times (500 Queries)

10

5

Block Size (MB)

DBMS
Post
R*Hilb.
-X
GIS
tree
R-tree
Synthetic (Uniform; 100M)
414
12887
2211
4391
345
28736
2329
4480
368
72005
2535
4881
Gowalla (Clustered; 6.4M)
24
19
8
24
29
34
11
26
37
194
20
45
OSM (Clustered; 688M)
303
1129
3486
4368
645
4117
3889
5599
15567
18172
4626
6402

100

2.5

Tot: 141

1

0.5

Table 5: Comparison of search times for alternative
schemes and impact of the search range on the time
to execute 500 range queries (sec.)
Luc.

Tot: 113

0.1

10

Figure 7: Impact of the block size on index creation
time of cSHB (uniform data set)

cSHB

Tot: 65

10

Block Size (MB)

Range

Impact of Block Sizes
on cSHB Time

Figure 10: Impact of the number of queries on the
execution time of cSHB (1% q. range, uniform data)
as the search range grows, whereas the other costs are more
or less independent of the sizes of the query ranges.
Impact of the Block Sizes. As we see above, reading
bitmaps from the disk and operating on them is a major
part of cSHB query execution cost; therefore these need to
be performed as eÔ¨Éciently as possible. As we discussed in
Section 3.2.2, cSHB reads data from the disk in a blocked
manner. In Figure 9, we see the impact of the block sizes on
the execution time of cSHB, including the time needed to
read bitmaps from the disk. As we see here, small blocks are
disadvantageous (due to the directory management overhead
they cause). Very large blocks are also disadvantageous as,
the larger the block gets, the larger becomes the amount of
redundant data read for each block access. As we see in the
Ô¨Ågure, for the conÔ¨Åguration considered in the experiments,
1MB blocks provided the best execution time.
Impact of the Number of Queries in the Workload.
Figure 10 shows the total execution times as well as the
breakdown of the execution times for cSHB for diÔ¨Äerent
number of simultaneously executing queries. While the total
execution time increases with the number of simultaneous
queries, the increase is sub-linear, indicating that there are
savings due to the shared processing across these queries.

1392

Read Hierarchy
Map Ranges
Search Cut Bitmaps
Read Bitmaps
Combine Bitmaps

cSHB Time Breakdown (500 Queries,
1% Q. Range, Uniform Data)
Time (sec., log. scale)

1000
100

Tot: 152

Tot: 42

Tot: 28

10
1
0.1
9

10
# of levels of the hierarchy

11

Figure 11: Impact of the depth of the hierarchy (500
queries, 1% query range, uniform data)
Table 6: Working set size in terms of 1MB blocks
Q.Range (on 100M data)
0.5%
1%
5%

Min
1
1
1

Avg.
2.82
2.51
1.02

Max.
36
178
95

Table 7: Impact of the buÔ¨Äer size on exec. time (in
seconds, for 500 queries, 100M data)
Query
Range
0.5%
1%
5%

2MB
11.8
24.2
823.8

3MB
11.3
19.1
399.9

BuÔ¨Äer Size
5MB
10MB
10.9
10.6
18.1
17.5
155.9
105.8

20MB
10.5
17.3
101.6

100MB
10.2
16.3
94.9

Also, in Section 4.2.2, we had observed that the number of
queries in the query workload and query ranges determine
the relative costs of in-buÔ¨Äer operations vs. disk I/O. In
Figures 8 and 10, we see that this is indeed the case.
Impact of the Depth of the Hierarchy. Figure 11 shows
the impact of the hierarchy depth on the execution time of
cSHB: a 4√ó increase in the number of cells in the space (due
to a 1-level increase in the number of levels of the hierarchy)
results in < 4√ó increase in the execution time. Most signiÔ¨Åcant contributors to this increase are the time needed to
read the hierarchy and the time for bitmap operations.
Impact of the Cache BuÔ¨Äer. As we discussed in Section 4.2.2, the incremental scheduling algorithm keeps a
buÔ¨Äer of blocks containing the working set of leaf bitmaps.
As Table 6 shows, the average size of the working set is
fairly small and can easily Ô¨Åt into the L3 caches of modern
hardware. Table 7 conÔ¨Årms that a small buÔ¨Äer, moderately
larger than the average working set size, is suÔ¨Écient and
larger buÔ¨Äers do not provide signiÔ¨Åcant gains.

6.

CONCLUSIONS

In this paper, we argued that bitmap-based indexing can
be highly eÔ¨Äective for running range query workloads on
spatial data sets. We introduced a novel compressed spatial hierarchical bitmap (cSHB) index structure that takes
a spatial hierarchy and uses that to create a hierarchy
of compressed bitmaps to support spatial range queries.
Queries are processed on cSHB index structure by selecting a relevant subset of the bitmaps and performing
compressed-domain bitwise logical operations. We also developed bitmap selection algorithms that identify the subset
of the bitmap Ô¨Åles in this hierarchy for processing a given
spatial range query workload. Experiments showed that the
proposed cSHB index structure is highly eÔ¨Écient in supporting spatial range query workloads. Our future work will
include implementing and evaluating cSHB for data with
more than two dimensions.

7. REFERENCES
[1] Apache Lucene. http://lucene.apache.org/core/4 6 0/
spatial/org/apache/lucene/spatial/preÔ¨Åx/tree/
SpatialPreÔ¨ÅxTree.html
[2] Using PostGIS: Data Management and Queries.
http://postgis.net/docs/using postgis dbmanagement.html
[3] OpenStreetMap. http://www.openstreetmap.org/
[4] J. Leskovec and A. Krevl. SNAP Datasets: Stanford Large
Network Dataset Collection. http://snap.stanford.edu/data
[5] D. Abadi et al. Compression and Execution in
Column-Oriented Database systems. SIGMOD 2006.
[6] A. Aji et al. Hadoop-GIS: A High Performance Spatial Data
Warehousing System over MapReduce. PVLDB 2013.
[7] Rudolf Bayer. The Universal B-tree for Multidimensional
Indexing: General concepts. WWCA 1997.
[8] N. Beckmann et al. The R*-tree: An EÔ¨Écient and Robust
Access Method for Points and Rectangles. SIGMOD 1990.
[9] A. Belussi and C. Faloutsos. Self-Spatial Join Selectivity
Estimation Using Fractal Concepts. TOIS 1998.
[10] J.Bercken et al. XXL - A Library Approach to Supporting
EÔ¨Écient Implementations of Advanced Database Queries.
VLDB 2001.
[11] A.R. Butz. Alternative Algorithm for Hilbert‚Äôs Space-Filling
Curve. TOC 1971.
[12] A. Cary et al. Experiences on Processing Spatial Data with
MapReduce. SSDBM 2009.
[13] J. Chmiel et al. Dimension Hierarchies by means of
Hierarchically Organized Bitmaps. DOLAP 2010.
[14] P. Chovanec and M. KraÃÅtkyÃÅ. On the EÔ¨Éciency of Multiple
Range Query Processing in Multidimensional Data Structures.
IDEAS 2013.
[15] F. DelieÃÄge and T. Pedersen. Position List Word Aligned
Hybrid: Optimizing Space and Performance for Compressed
Bitmaps. EDBT 2010.
[16] L.Gosink et al. HDF5-FastQuery: Accelerating Complex
Queries on HDF Datasets using Fast Bitmap Indices.
SSDM 2006.
[17] David Hilbert. Ueber stetige abbildung einer linie auf ein
Ô¨Çachenstuck. Mathematische Annalen 1891.
[18] Y. Jing et al. An Empirical Study on Performance Comparison
of Lucene and Relational Database. ICCSN 2009.
[19] I. Kamel and C. Faloutsos. Hilbert R-tree: An Improved R-tree
using Fractals. VLDB 1994
[20] O. Kaser et al. Histogram-Aware Sorting for Enhanced
Word-Aligned Compression in Bitmap Indexes. DOLAP 2008.
[21] D. Lemire et al. Sorting improves Word-Aligned Bitmap
Indexes. DKE 2010.
[22] V. Markl et al. Improving OLAP Performance by
Multidimensional Hierarchical Clustering. IDEAS 1999.
[23] G. Morton. A Computer Oriented Geodetic Data Base and a
New Technique in File Sequencing. IBM 1966.
[24] M. Morzy et al. Scalable Indexing Technique for Set-Valued
Attributes. ADBIS 2003.
[25] P. Nagarkar and K. S. Candan. HCS:Hierarchical Cut Selection
for EÔ¨Éciently Processing Queries on Data Columns using
Hierarchical Bitmap Indices. EDBT 2014.
[26] M. A. Olma et al. BLOCK: EÔ¨Écient Execution of Spatial
Range Queries in Main-Memory. Technical report EPFL 2013.
[27] A. N. Papadopoulos and Y. Manolopoulos. Multiple Range
Query Optimization in Spatial Databases. ADBIS 1998.
[28] H. Samet. Foundations of Multidimensional and Metric Data
Structures, 2005.
[29] R. Sinha and M. Winslett. Multi-Resolution Bitmap Indexes for
ScientiÔ¨Åc Data. TODS 2007.
[30] T. Siqueira et al. The SB-index and the HSB-index: EÔ¨Écient
Indices for Spatial Data Warehouses. Geoinformatica 2012.
[31] T. Skopal et al. Algorithm for Universal B-trees. Inf. Syst. 2006.
[32] K. Wu et al. On the Performance of Bitmap Indices for High
Cardinality Attributes. VLDB 2004.
[33] K. Wu et al. An EÔ¨Écient Compression Scheme for Bitmap
Indices. TODS 2004.
[34] M. Zaker et al. An Adequate Design for Large Data Warehouse
Systems: Bitmap Index versus B-tree Index. IJCC 2008.
[35] Y. Zhong et al. Towards Parallel Spatial Query Processing for
Big Spatial Data. IPDPSW 2012.

1393

