Compressed Spatial Hierarchical Bitmap (cSHB) Indexes  for Efficiently Processing Spatial Range Query Workloads
Parth Nagarkar
Arizona State University Tempe, AZ 85287-8809, USA

K. Selçuk Candan
Arizona State University Tempe, AZ 85287-8809, USA

Aneesha Bhat
Arizona State University Tempe, AZ 85287-8809, USA

nagarkar@asu.edu ABSTRACT

candan@asu.edu

aneesha.bhat@asu.edu

In most spatial data management applications, objects are represented in terms of their coordinates in a 2-dimensional space and search queries in this space are processed using spatial index structures. On the other hand, bitmap-based indexing, especially thanks to the compression opportunities bitmaps provide, has been shown to be highly effective for query processing workloads including selection and aggregation operations. In this paper, we show that bitmapbased indexing can also be highly effective for managing spatial data sets. More specifically, we propose a novel compressed spatial hierarchical bitmap (cSHB) index structure to support spatial range queries. We consider query workloads involving multiple range queries over spatial data and introduce and consider the problem of bitmap selection for identifying the appropriate subset of the bitmap files for processing the given spatial range query workload. We develop cost models for compressed domain range query processing and present query planning algorithms that not only select index nodes for query processing, but also associate appropriate bitwise logical operations to identify the data objects satisfying the range queries in the given workload. Experiment results confirm the efficiency and effectiveness of the proposed compressed spatial hierarchical bitmap (cSHB) index structure and the range query planning algorithms in supporting spatial range query workloads.

terms of their coordinates in 2D space. Queries in this 2D space are then processed using multidimensional/spatial index structures that help quick access to the data [28].

1.1 Spatial Data Structures
The key principle behind most indexing mechanisms is to ensure that data objects closer to each other in the data space are also closer to each other on the storage medium. In the case of 1D data, this task is relatively easy as the total order implicit in the 1D space helps sorting the objects so that they can be stored in a way that satisfies the above principle. When the space in which the objects are embedded has more than one dimension, however, the data has multiple degrees of freedom and, as a consequence, there are many different ways in which the data can be ordered on the storage medium and this complicates the design of search data structures. One common approach to developing index structures for multi-dimensional data is to partition the space hierarchically in such a way that (a) nearby points fall into the same partition and (b) point pairs that are far from each other fall into different partitions. The resulting hierarchy of partitions then can either be organized in the form of trees (such as quadtrees, KD-trees, R-trees and their many variants [28]) or, alternatively, the root-toleaf partition paths can be serialized in the form of strings and these strings can be stored in a string-specific search structure. Apache Lucene, a highly-popular search engine, for example, leverages such serializations of quadtree partitions to store spatial data in a spatial prefix tree [1]. An alternative to applying the partitioning process in the given multi-dimensional space is to map the coordinates of the data into a 1D space and perform indexing and query processing on this 1D space instead. Intuitively, in this alternative, one seeks an embedding from the 2D space to a 1D space such that (a) data objects closer to each other in the original space are also closer to each other on the 1D space, and (b) data objects further away from each other in the original space are also further away from each other on the 1D space. This embedding is often achieved through fractal-based space-filling curves [11, 17]. In particular, the Peano-Hilbert curve [17] and Z-order curve [23] have been shown to be very effective in helping cluster nearby objects in the space. Consequently, if data are stored in an order implied by the space-filling curve, then the data elements that are nearby in the data space are also clustered, thus enabling efficient retrieval. In this paper, we leverage these properties of space-filling curves to develop a highly compressible bitmap-based index structure for spatial data.

1.

INTRODUCTION

Spatial and mobile applications are gaining in popularity, thanks to the wide-spread use of mobile devices, coupled with increasing availability of very detailed spatial data (such as Google Maps and OpenStreetMap [3]), and location-aware services (such as FourSquare and Yelp). For implementing range queries (Section 3.1.2), many of these applications and services rely on spatial database management systems, which represent objects in the database in This work was supported by NSF grants 1116394, 1339835, and 1318788
This work is licensed under the Creative Commons AttributionNonCommercial-NoDerivs 3.0 Unported License. To view a copy of this license, visit http://creativecommons.org/licenses/by-nc-nd/3.0/. Obtain permission prior to any use beyond those covered by the license. Contact copyright holder by emailing info@vldb.org. Articles from this volume were invited to present their results at the 41st International Conference on Very Large Data Bases, August 31st - September 4th 2015, Kohala Coast, Hawaii. Proceedings of the VLDB Endowment, Vol. 8, No. 12 Copyright 2015 VLDB Endowment 2150-8097/15/08.

1382

Results Query Workload

Identify Cut & Leaf Bitmaps

Construct Result Bitmaps

Cut C t Bitmaps Bit Buffer Buffe B ff

Leaf Bitmaps Buffer

hierarchical bitmap files and (b) propose efficient bitmap selection algorithms that select the best bitmap nodes from the cSHB index structure to be fetched into the main-memory for processing of the query workload. In this paper, we also present an efficient disk-based organization of compressed bitmaps. To our best knowledge, this is the first work that provides an efficient index structure to execute a query workload involving multiple spatial range queries by using bitmap indexes. Experimental evaluations of the cSHB index structure and the bitmap selection algorithms show that cSHB is highly efficient in answering a given query workload.

Cut Bitmap Cut Bitmap Cut Bitmap Cut Bitmap Bitm itmap Leaf Bitmaps

1.4 Paper Organization
This paper is organized as follows. In the next section we provide an overview of the related work. In Section 3.1, we introduce the key concepts and notations, and in Section 3.2, we present the proposed cSHB index structure. Then, in Section 4, we describe how query workloads are processed using cSHB: in Section 4.1, we introduce the concepts of range query plans, in Section 4.2, we present cost models for alternative execution strategies, and in Section 4.3, we present algorithms for finding efficient query plans for a given range query workload. Experiment results are reported in Section 5. We finally conclude the paper in Section 6.

Compressed Spatial Hierarchical Bitmaps (cSHB)

Figure 1: Processing a range query workload using compressed spatial hierarchical bitmap (cSHB)

1.2 Bitmap-based Indexing
Bitmap indexes [29, 32] have been shown to be highly effective in answering queries in data warehouses [34] and column-oriented data stores [5]. There are two chief reasons for this: (a) first of all, bitmap indexes provide an efficient way to evaluate logical conditions on large data sets thanks to efficient implementations of the bitwise logical "AND", "OR", and "NOT" operations; (b) secondly, especially when data satisfying a particular predicate are clustered, bitmap indexes provide significant opportunities for compression, enabling either reduced I/O or, even, complete in-memory maintenance of large index structures. In addition, (c) existence of compression algorithms [15, 33] that support compressed domain implementations of the bitwise logical operations enables query processors to operate directly on compressed bitmaps without having to decompress them until the query processing is over and the results are to be fetched from the disk to be presented to the user.

2. RELATED WORK 2.1 Multi-Dimensional Space Partitioning
Multi-dimensional space partitioning strategies can be categorized into two: In the first case, including quadtree, BD-tree, G-Tree, and KD-tree variants, a given bounded region is divided into two or more "open" partitions such that each partition borders a boundary of the input region. In the latter case, some of the partitions (often referred to as minimum bounding regions, MBRs) are "closed" regions of the space, not necessarily bordering any boundary of the input region. An advantage of this latter category of index structures, including the R-tree and its variants (R*-tree, R+-tree, Hilbert R-tree, and others), is that these MBRs can tightly cover the input data objects. While most index structures have been designed to process individual queries, there are also works focusing on the execution of a workload of multiple queries on the same index structure. In [27], the Hilbert values of the centroids of the rectangles formed by the range queries are sorted, and these queries are grouped accordingly to process them over an R-tree. In [14], R-trees are used to execute multiple range queries and, in their formulation, authors propose to combine adjacent queries into one. Thus, the algorithm is not able to differentiate results of individual queries. There are two problems commonly associated with multidimensional index structures, namely overlaps between partitions (which cause redundant I/O) and empty spaces within partitions (which cause unnecessary I/O). While there has been a significant amount of research in searching for partitioning strategies that do not face these problems, these two issues still remain [26] and are especially critical in very high-dimensional vector spaces. One way to tackle this problem has been to parallelize the work. For example, in [6], the authors describe a Hadoop-based data warehousing system with spatial support, where the main focus is to parallelize the building of the R*-tree index structure and query processing over Hadoop.

1.3 Contributions of this Paper
In this paper, we show that bitmap-based indexing is also an effective solution for managing spatial data sets. More specifically, we first propose compressed spatial hierarchical bitmap (cSHB) indexes to support spatial range queries. In particular, we (a) convert the given 2D space into a 1D space using Z-order traversal, (b) create a hierarchical representation of the resulting 2D space, where each node of the hierarchy corresponds to a (sub-)quadrant (i.e., effectively creating an implicit "quadtree"), and (c) associate a bitmap file to each node in the quadtree representing the data elements that fall in the corresponding partition. We present efficient algorithms for answering range queries using a select subset of bitmap files stored in a given cSHB index. We then consider a service provider that has to answer multiple concurrent queries over the same spatial data and, thus, focus on query workloads involving multiple range queries. Since the same set of queries can be answered using different subsets of the bitmaps in the cSHB index structure, we consider the problem of identifying the appropriate bitmap nodes for processing the given query workload. More specifically, as we visualize in Figure 1, (a) we develop cost models for range query processing over compressed spatial

1383

       


        





 

  



]  ]



Figure 2: Z-order curve for a sample 2D Space.

2.2 Space Filling Curve based Indexing
The two most common space-filling curves are the fractalbased Z-order curve [23] and the Peano-Hilbert curve [17]. While the Hilbert curve provides a better mapping from the multidimensional space onto the 1D space, its generation is a complicated and costly process [26]. With Z-order curve, however, mapping back-and-forth between the multidimensional space and the 1D space using a process called bit-shuffling (visualized in Figure 2) is very simple and efficient. Consequently, the Z-order curve has been leveraged to deal with spatial challenges [12], including construction of and searches on R-trees [27] and others. In [35], the authors present a parallel spatial query processing system called VegaGiStore that is built on top of Hadoop. This system uses a two-tiered index structure that consists of a quadtree-based global index (used for finding the necessary data blocks) and a Hilbert-ordering based local index (used for finding the spatial objects in the data block). In [26], the authors present an index called BLOCK to process spatial range queries. Their main assumption is that the data and index can fit into the main memory, and hence their aim is to reduce the number of comparisons between the data points and the query range. They create a sorted list of the Z-order values for all the data points. Given a query, they start at the coarsest level. If a block lies entirely within the given query range, they retrieve the data points in this block, otherwise, based on a branching fact, they decide whether to search the next granular level. In [7], the authors proposed a UB-tree index structure that also uses Z-ordering for storing multidimensional data in a B+ tree and in [22], the authors presented a hierarchical clustering scheme for the fact table of a data warehouse in which the data is stored using the above mentioned UB-tree. In [31], the authors present a range query algorithm specifically for the UB-tree. Unlike our approach, the above solutions are not specifically designed for multiple query workloads.

is partitioned into bins and a bitmap is created for each bin. Given a query, results are constructed by combining relevant bins using bitwise OR operations. Recognizing that many data attributes have hierarchical domains, there has also been research in the area of multi-level and hierarchical bitmap indexes [13, 24, 25, 29]. When the bitmaps are partitioned (with potential overlaps), it is necessary to select an appropriate set (or cut [25]) of bitmaps for query processing; results are often obtained by identifying a set of bitmaps and combining them using bitwise ORs. This work builds on some of the ideas presented in [25] from 1D data to spatial data. In [25], the cost model only considered the dominant I/O cost (reading the bitmaps from the disk), but in this work, we present an updated cost model, that appropriately includes the I/O cost as well as the cost of performing local operations on the in-memory bitmaps. There has been some prior attempts to leverage bitmaps in spatial query processing. For example, an MBR-based spatial index structure is proposed in [30], where the leaves of the tree are encoded in the form of bitmaps. Given a query, the proposed HSB-index is traversed top-down (as in R-trees) to identify the relevant bitmaps to be combined. In this paper, we note that not only leaves, but also internal nodes of the spatial hierarchy can be encoded as bitmaps, leading to significant savings in range search time, especially for query workloads consisting of multiple spatial range queries. Thus, our work focuses on which bitmaps to read in the context of spatial range query workloads and we introduce novel algorithms to choose which bitmaps to use to answer a query workload efficiently. We generalize the problem of bitmap selection and consider alternative strategies that complement OR-based result construction. In [16], authors propose a storage and retrieval mechanism for large multi-dimensional HDF5 files by using bitmap indexes. While range queries are supported on their architecture, they neither leverage Z-order indexing, nor hierarchical bitmaps as proposed in this work. Also, their proposed mechanism is not optimized for multiple query workloads.







 

  

3. COMPRESSED SPATIAL HIERARCHICAL BITMAP (cSHB) INDEXES
In this section, we present the key concepts used in the paper and introduce the compressed spatial hierarchical bitmap (cSHB) index structure for answering spatial range queries.

3.1 Key Concepts and Notations
3.1.1 Spatial Database
A multidimensional database, D, consists of points that belong to a (bounded and of finite-granularity) multidimensional space S with d dimensions. A spatial database is a special case where d = 2. We consider rectangular spaces such that the boundaries of S can be described using a pair of south-west and a north-east corner points, csw and cne (csw .x  cne .x and csw .y  cne .y and pS csw .x  p.x  cne .x and csw .y  p.y  cne .y ).

2.3 Bitmap Indexes
There have been significant amount of works on improving the performance of bitmap indexes and keeping compression rates high [20, 32, 33]. Most of the newer compression algorithms use run-length encoding for compression: this provides a good compression ratio and enables bitwise operations directly on compressed bitmaps without having to decompress them first [32]. Consequently, bitmap indexes are also shown to perform better than other database index structures, especially in data warehouses and columnoriented systems [5, 32, 34]. For attributes with a large number of distinct values, bitmaps are often created with binning, where the domain

3.1.2 Spatial Query Workload
In this paper, we consider query workloads, Q, consisting of a set of rectangular spatial range queries. · Spatial Range Query: A range query, q  Q, is defined by a corresponding range specification q.rs =

1384

qsw , qne , consisting of a south-west point and a northeast point, such that qsw .x  qne .x and qsw .y  qne .y . Given a range query, q , with a range specification, q.rs = qsw , qne , a data point p  D is said to be contained within the query range (or is a range point) if and only if qsw .x  p.x  qne .x and qsw .y  p.y  qne .y .
0000**
0001**

root 00****

 
0010** 0011**


000000 000001 000010 000011 001000 001001 001010 001011

 

3.1.3 Spatial Hierarchy
In cSHB, we associate to the space S a hierarchy H , which consists of the node set N (H ) = {n1 , . . . , nmaxn }: · Nodes of the hierarchy: Intuitively, each node, ni  N (H ) corresponds to a (bounded) subspace, Si  S , described by a pair of corner points, ci,sw and ci,nw . · Leaves of the hierarchy: LH denotes the set of leaf nodes of the hierarchy H and correspond to all potential point positions of the finite space S . Assuming that the database, D, contains only points, only the leaves of the spatial hierarchy occur in the database. · Parent of a node: For all ni , parent(ni ) denotes the parent of ni in the corresponding hierarchy; if ni is the root, then parent(ni ) = . · Children of a node: For all ni , children(ni ) denotes the children of ni in the corresponding hierarchy; if ni  LH , then children(ni ) = . In this paper, we assume that the children induce a partition of the region corresponding to the parent node:

nh =nj children(ni )

Figure 3: A sample 4-level hierarchy defined on the Z-order space defined in Figure 2 (the string associated to each node corresponds to its unique label) · if ni is an internal node (i.e., ni  IH ), then oD nh leaf Desc(ni ) located at(o, nh )  (Bi [o] = 1), whereas · if ni is a leaf node (i.e., ni  LH ), then  oD located at(o, ni )  (Bi [o] = 1)

3.2.1 Our Implementation of cSHB
A cSHB index structure can be created based on any hierarchy satisfying the requirements1 specified in Section 3.1.3. In this paper, without loss of generality, we discuss a Zcurve based construction scheme for cSHB. The resulting hierarchy is analogous to the MX-quadtree data structure, where all the leaves are at the same level and a given region is always partitioned to its quadrants at the center [28]. As introduced in Sections 1.1 and 2.2, a space-filling curve is a fractal that maps a given finite multidimensional data space onto a 1D curve, while preserving the locality of the multidimensional data points (Figure 2): in other words nearby points in the data space tend to be mapped to nearby points on the 1D curve. As we also discussed earlier, Z-curve is a fractal commonly used as a space-filling curve (thanks to its effectiveness in clustering the points in the data space and the efficiency with which the mapping can be computed). A key advantage of the Z-order curve (for our work) is that, due to the iterative (and self-similar) nature of the underlying fractal, the Z-curve can also be used to impose a hierarchy on the space. As visualized in Figure 3, each internal node, ni , in the resulting hierarchy has four children corresponding to the four quadrants of the space, Si . Consequently, given a 2h -by-2h space, this leads to an (h + 1)-level hierarchy, (analogous to an MX-quadtree [28]) which can be used to construct a cSHB index structure2 . As we show in Section 5, this leads to highly compressible bitmaps and efficient execution plans.

 Sh  .
nh children(ni )



Sh Sj =  and Si =

· Descendants of a Node: The set of descendants of node ni in the corresponding hierarchy is denoted as desc(ni ). Naturally, if ni  LH , then desc(ni ) = . · Internal Nodes: Any node in H that is not a leaf node is called an internal node. The set of internal nodes of H is denoted by IH . Each internal node in the hierarchy corresponds to a (non-point) sub-region of the given space. If N (H, l) denotes the subset of the nodes at level l of the hierarchy H , then we have   ni =nj N (H,l) Si Sj =  and S =
ni N (H,l)

Si  .

3.2.2 Blocked Organization of Compressed Bitmaps
Given a spatial database, D, with a corresponding hierarchy, H , we create and store a compressed bitmap for each node in the hierarchy, except for those that correspond to regions that are empty. These bitmaps are created in a bottom-up manner, starting from the leaves (which encode for each point in space, S , which data objects in D are located at that point) and merging bitmaps of children nodes into the bitmaps of their parents. Each resulting bitmap is stored as a compressed file on disk. It is important to note that, while compression provides significant savings in storage and execution time, a naive storage of compressed bitmaps can still be detrimental for
1 In fact, cSHB can be created even when some of the requirements are relaxed ­ for example children do not need to cover the parent range entirely (as in R-trees). 2 Without loss of generality, we assume that the width and height are 2h units for some integer h  1.

The root node corresponds to the entire space, S . · Leaf Descendants of a Node: Leaf descendants, leaf Desc(ni ), of a node are the set of nodes such that leaf Desc(ni ) = desc(ni )  LH .

3.2 Compressed Spatial Hierarchical Bitmap (cSHB) Index Structure
In this section, we introduce the proposed compressed spatial hierarchical bitmap (cSHB) index structure: Definition 3.1 (cSHB Index Stucture). Given a spatial database D consisting of a space, S , and a spatial hierarchy, H , a cSHB index is a set, B of bitmaps, such that for each ni  N (H ), there is a corresponding bitmap, Bi  B, where the following holds:

1385

· A spatial database, D , defined over 2h -by-2h size space, S and a corresponding (h + 1)-level (Z-curve based) hierarchy, H , with set of internal nodes, IH · Minimum block size, K

000 001 010 011 100 0 101 110 111

5 6 7

5 6 7

Algorithm 1 Writing blocks of compressed bitmaps to disk 1: Input:

000 001 010 011 100 101 110 111

0

1

2

3 4

5 6 7

0

1

2

3 4

5 6 7

2

2

3 4

3 4

Range 11100X2 [56,57]

2: procedure writeBitmaps 3: Block T =  4: availableSize = K 5: for level l = (h + 1) (i.e., leaves) to 0 (i.e., root) do 6: for each node ni in l in increasing Z-order do 7: if l == (h + 1) then 8: Initialize a compressed bitmap Bi 9: else 10: Bi = OR Bj
nj children(ni )

(a) 2D query range

(b) Corresponding 1D ranges

11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25:

end if if size(Bi )  K then write Bi to disk; else T = append(T, Bi ) availableSize = availableSize - size(Bi ) if (availableSize  0) or (ni is the last node at this level) then write T to disk; Block T =  availableSize = K end if end if end for end for end procedure

Figure 4: Mapping of a single spatial range query to two 1D ranges on the Z-order space: (a) A contiguous 2D query range, [sw = (4, 4); ne = (6, 5)] and (b) the corresponding contiguous 1D ranges, [48,51] and [56,57], on the Z-curve file; on the other hand, bitmaps for nodes n1 , n2 , n3 , and n5 need to be concatenated into a single file to obtain a block larger than K = 10 units. 3 Note that this block-based structure implies that the size of the files and the number of bitmap files on the disk will be upper bounded, but it also means that the cost of the bitmap reads will be lower bounded by K . Therefore, to obtain the best performance, repeated access to a block to fetch different bitmaps must be avoided through bitmap buffering and/or bitmap request clustering. In the next section, we discuss the use of cSHB index for range query processing. In Section 5, we experimentally analyze the impact of block-size on the performance of the proposed cSHB index structure.

performance: in particular, in a data set with large number of objects located at unique points, there is a possibility that a very large number of leaf bitmaps need to be created on the secondary storage. Thus, creating a separate bitmap file for each node may lead to inefficiencies in indexing as well as during query processing (as directory and file management overhead of these bitmaps may be non-negligible). To overcome this problem, cSHB takes a target block size, K , as input and ensures that all index-files written to the disk (with the possible exception of the last bitmap file in each level) are at least K bytes. This is achieved by concatenating, if needed, compressed bitmap files (corresponding to nodes at the same level of hierarchy). In Algorithm 1, we provide an overview of this block-based bottom-up cSHB index creation process. In Line 10, we see that the bitmap of an internal node is created by performing a bitwise OR operation between the bitmaps of the children of the node. These OR operations are implemented in the compressed bitmap domain enabling fast creation of the bitmap hierarchy. As it creates compressed bitmaps, the algorithm packs them into a block (Line 15). When the size of the block exceeds K , the compressed bitmaps in the block are written to the disk (Line 18) as a single file and the block is re-initialized. Example 3.1. Let us assume that K = 10 and also that we are considering the following sequence of nodes with the associated (compressed) bitmap sizes: n1 , 3 ; n2 , 4 ; n3 , 2 ; n4 , 15 ; n5 , 3 ; . . . This sequence of nodes will lead to following sequence of bitmap files materialized on disk: [B4 ] ; [B1 B2 B3 B5 ] ; . . .
size=15 size=3+4+2+3=12

4. QUERY PROCESSING WITH THE cSHB INDEX STRUCTURE
In this section, we describe how query workloads are processed using the cSHB index structure. In particular, we consider query workloads involving multiple range queries and propose spatial bitmap selection algorithms that select a subset of the bitmap nodes from the cSHB index structure for efficient processing of the query workload.

4.1 Range Query Plans and Operating Nodes
In order to utilize the cSHB index for answering a spatial range query, we first need to map the range specification associated with the given query from the 2D space to the 1D space (defined by the Z-curve). As we see in Figure 4, due to the way the Z-curve spans the 2D-space, it is possible that a single contiguous query range in the 2D space may be mapped to multiple contiguous ranges on the 1D space. Therefore, given a 2D range query, q , we denote the resulting set of (disjoint) 1D range specifications, as RSq . Let us be given a query, q , with the set of 1D range specifications, RSq . Naturally, there may be many different ways to process the query, each using a different set of bitmaps in the cSHB index structure, including simply fetching and combining only the relevant leaf bitmaps: Example 4.1 (Alternative Range Query Plans). Consider a query q with q.rs = (1, 0), (3, 1) on the space shown in Figure 2. The corresponding 1D range, [2, 11], would cover the following leaf nodes of the hierarchy shown in Figure 3: RSq = (000010, 000011, 001000, 001001, 001010, 001011). The following are some of the alternative query plans for q using the proposed cSHB index structure:

Note that, since the bitmap for node n4 is larger than the target block size, B4 is written to disk as a separate bitmap

1386

0

Range 1 1100XX [48,51]

1

0

1

· Inclusive query plans: The most straightforward way to execute the query would be to combine (bitwise OR operation) the bitmaps of the leaf nodes covered in 1D range, [2, 11]. We refer to such plans, which construct the result by combining bitmaps of selected nodes using the OR operator, as inclusive plans. An alternative inclusive plan for this query would be to combine the bitmaps of nodes 000010, 000011, 0010**: B000010 OR B000011 OR B0010 . · Exclusive query plans: In general, an exclusive query plan includes removal of some of the children or descendant bitmaps from the bitmaps of a parent or ancestor through the ANDNOT operation. One such exclusive plan would be to combine the bitmaps of all leafs nodes, except for B000010 , B000011 , B001000 , B001001 , B001010 , B001011 , into a bitmap Bnon result and return Broot ANDNOTBnon result . · Hybrid query plans: Both inclusive and exclusive only query plans may miss efficient query processing alternatives. Hybrid plans combine inclusive and exclusive strategies at different nodes of the hierarchy. A sample hybrid query plan for the above query would be B0000 ANDNOT (B000000 OR B000001 ) OR B0010 . 3 As illustrated in the above example, a range query, q , on hierarchy H , can be answered using different query plans, involving bitmaps of the leaves and certain internal nodes of the hierarchy, collectively referred to as the operating nodes of a query plan. In Section 4.3, we present algorithms for selecting the operating nodes for a given workload, Q; but first we discuss the cost model that drives the selection process.

Operations on In-Buffer Compressed Bitmaps.
When the compressed bitmaps on which the logical operations are implemented are already in-memory, the disk access cost is not a factor. However, also in this case, the cost is proportional to the sizes of the compressed bitmap files in the memory, independent of the specific logical operator that is involved [33], leading to costcpu (Bi op Bj ) = cpu size(Bi ) + size(Bj ) , where cpu is the CPU cost multiplier. A similar result also holds for the unary operation NOT.

4.2.2 Cost Models for Multiple Operations
In this section, we consider a cost model which assumes that blocks are disk-resident. Therefore, we consider a storage hierarchy consisting of disk (storing all bitmaps), RAM (as buffer storing relevant bitmaps), and L3/L2 caches (storing currently needed bitmaps).

Buffered Strategy.
In the buffered strategy, visualized in Figure 1, the bitmaps that correspond to any leaf or non-leaf operating nodes for the query plan of a given query workload, Q, are brought into the buffer once and cached for later use. Then, for each query q  Q, the corresponding result bitmap is extracted using these buffered operating node bitmaps. Consequently, if a node is an operating one for more than one q  Q, it is read from the disk only once (and once for each query from the memory). Let us assume that TONQ denotes the set of unique blocks that contains all the necessary operating nodes given a query workload Q(ONQ ). This leads to the overall processing cost, time costbuf (Q, ONQ ), of     IO 
T TON
Q

4.2 Cost Models and Execution Strategies
In cSHB, the bitwise operations needed to construct the result are performed on compressed bitmaps directly, without having to decompress them.

size(T ) + cpu 
q Q ni ONq read cost

size(Bi ) .
operating cost

4.2.1 Cost Model for Individual Operations
We consider two cases: (a) logical operations on diskresident compressed bitmaps and (b) logical operations on in-buffer compressed bitmaps.

Operations on Disk-Resident Compressed Bitmaps.
In general, when the logical operations are implemented on compressed bitmaps that reside on the disk, the time taken to read a bitmap from the secondary storage to the main memory dominates the overall bitwise manipulation time [15]. The overall cost is hence proportional to the size of the (compressed) bitmap file on the secondary storage. Let us consider a logical operation on bitmaps Bi and Bj . Let us assume that T (Bi ) and T (Bj ) denotes the blocks in which Bi and Bj are stored, respectively. Since multiple bitmaps can be stored in a single block, it is possible that Bi and Bj are in the same block. Hence, let us further assume that T(Bi ,Bj ) is the set of unique blocks that contain the bitmaps, Bi and Bj . Then the overall I/O cost is: costio (Bi op Bj ) = IO
T T(B ,B ) i j

Since all operating nodes need to be buffered, this execution strategy requires a total of storage costbuf (Q, ONQ ) = Note that, in general, ni ONQ size(Bi ) buffer space. IO > cpu . However, in Section 5, we see that the number of queries in the query workload and query ranges determine the relative costs of in-buffer operations vs. disk I/O. The buffered strategy has the advantage that each query can be processed individually on the buffered bitmaps and the results for each completed query can be pipelined to the next operator without waiting for the results of the other queries in the workload. This reduces the memory needed to temporarily store the result bitmaps. However, in the buffered strategy, the buffer needed to store the operating node bitmaps can be large.

Incremental Strategy.
The incremental strategy avoids buffering of all operating node bitmaps simultaneously. Instead, all leaf and non-leaf operating nodes are fetched from the disk one at a time on demand and results for each query are constructed incrementally. This is achieved by considering one internal operating node at a time and, for each query, focusing only on the leaf operating nodes under that internal node. For this purpose, a result accumulator bitmap, Resj , is maintained for each query in qj  Q and each operating node read from the disk is applied directly on this result accumulator bitmap.

size(T ) ,

where IO is an I/O cost multiplier and op is a binary bitwise logical operator. A similar result also holds for the unary operation NOT.

1387

      #!!  "&   
 
    

If a set of internal nodes of H only satisfies the first condition, then we refer to the cut as an incomplete cut.  As visualized in Figure 1, given a cut C , cSHB queries are processed by using only the bitmaps of the nodes in this cut, along with some of the leaf bitmaps necessary to construct results of the queries in Q. In the rest of this subsection, we first describe how queries are processed given a cut, C , of H and then present algorithms that search for a cut, C , given a workload, Q.

  



Figure 5: Buffer misses and the overall read time (data and other details are presented in Section 5) While it does not need buffer to store all operating node bitmaps, the incremental strategy may also benefit from partial caching of the relevant blocks. This is because, while each internal node needs to be accessed only once, each leaf node under this internal node may need to be brought to the memory for multiple queries. Moreover, since the data is organized in terms of blocks, rather than individual nodes (Section 3.2.2), a single block may serve multiple nodes to different queries. When sufficient buffer is available to store the working set of blocks (containing the operating leaf nodes under the current internal node), the execution cost, time costinc (Q, ONQ ), of the incremental strategy is identical to that of the buffered strategy. Otherwise, as illustrated in Figure 5, the read cost component is a function of buffer misses, IO × # buf f er misses, which itself depends on the size of the buffer and the clustering of the data. The storage complexity3 is storage costinc (Q, ONQ ) = qj Q size(Resj ) plus the space needed to maintain the most recently read blocks in the current working set. Experiments reported in Section 5 show that, for the considered data sets, the sizes of the working sets are small enough to fit into the L3-caches of many modern hardware.

4.3.1 Range Query Processing with Cuts
It is easy to see that any workload, Q, of queries can be processed by any (even incomplete) cut, C , of the hierarchy and a suitable set of leaf nodes: Let Rq denote the set of leaf nodes that appear in the result set of query q  Q and ¯ q be the set of leaf nodes that do not appear in the result R C be the set of the result leaves covered by a set. Let also Rq node in C . Then, one possible way to construct the result bitmap, Bq , is as follows:     Bq =   OR Bi OR   Bi  ANDNOT ¯  nj RC ni Rq \RC q q  Rq OR
inclusions

ni C

Bj
exclusions

.

Intuitively any result nodes that are not covered by the cut need to be included in the result using a bitwise OR operation, whereas any leaf node that is not in any result needs to be excluded using an ANDNOT operation. Consequently, · if C  Rq = , an inclusion-only plan is necessary, · an exclusion-only plan is possible only if C covers Rq completely. Naturally, given a range query workload, Q, different query plans with different cuts will have different execution costs. The challenge is, then, · to select an appropriate cut, C , of the hierarchy, H , for query workload, Q, and · to pick, for each query qj  Q, a subset Cj  C for processing qj , in such a way that these will minimize the overall processing cost for the set of range queries in Q. Intuitively, we want to include in the cut, those nodes that will not lead to a large number of exclusions and cannot be cheaply constructed by combining bitmaps of the leaf nodes using OR operations.

4.3 Selecting the Operating Bitmaps for a Given Query Workload
To process a range query workload, Q, on a data set, D, with the underlying cSHB hierarchy H , we need to select a set of operating bitmap nodes, ONQ , of H from which we can construct the results for all qj  Q, such that time cost(Q, ONQ ) is the minimum among all possible sets of operating bitmaps for Q. It is easy to see that the number of alternative sets of operating bitmaps for a given query workload Q is exponential in the size of the hierarchy H . Therefore, instead of seeking the set of operating bitmaps among all subsets of the nodes in H , we focus our attention on the cuts of the hierarchy, defined as follows: Definition 4.1 (Cuts of H Relative to Q). A complete cut, C , of a hierarchy, H , relative to a query load, Q, is a subset of the internal nodes (including the root) of the hierarchy, satisfying the following two conditions: · validity: there is exactly one node on any root-to-leaf branch in a given cut; and · completeness: the nodes in C collectively cover every possible root-to-leaf branch for all leaf nodes in the result sets for queries in Q. 3 The space complexity of the incremental strategy can be upper-bounded if the results for the queries in Q can be pipelined to the next set of operators progressively as partial results constructed incrementally.

4.3.2 Cut Bitmap Selection Process
Given the above cut-based query processing model, in this section we propose a cut selection algorithm consisting of two steps: (a) a per-node cost estimation step and (b) a bottom-up cut-node selection step. We next describe each of these two steps.

Node Cost Estimation.
First, the process assigns an estimated cost to those hierarchy nodes that are relevant to the given query workload, Q. For this, the algorithm traverses through the hierarchy, H , in a top-down manner and identifies part, R, of the hierarchy relevant for the execution of at least one query, q  Q (i.e., for at least one query, q , the range associated with the node and the query range intersect). Note that this process

1388

Algorithm 2 Cost and Leaf Access Plan Assignment Algorithm 1: Input: Hierarchy H , Query Workload Q 2: Outputs: Query workload, Q(ni ), and cost estimate, costi ,

3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23:

for each node, ni  H ; leaf access plan, Ei,j , for all node/query pairs ni  H and qj  Q(ni ); a set, R  IH , or relevant internal nodes Initialize: R =  procedure Cost and LeafAccessPlanAssignment for each internal node ni  IH in top-down fashion do if ni = "root then Q(ni ) = Q else Q(ni ) = {q  Q(parent(ni )) s.t. (q.rs  Si ) = } end if if Q(ni ) =  then add ni into R end if end for for each node ni  R in a bottom-up fashion do for qj  Q(ni ) do Compute icost(ni , q ) Compute ecost(ni , q ) Compute the leaf access plan, Ei,j , as Ei,j = [ecost(ni , qj ) < icost(ni , qj )] end for Compute the leaf access cost, leaf costi , as qj Q(ni ) Ei,j × ecost(ni , qj ) + (1 - Ei,j ) × icost(ni , qj ) end for end procedure

· Exclusive leaf access plan (Line 18): If query, q , is executed using an exclusive leaf access plan at node, ni , this means that the result for the range (q.rs  Si ) will be obtained by using Bi and then identifying and excluding (using bitwise ANDNOT operations) all irrelevant leaf bitmaps under node ni . Thus, we compute the exclusive leaf access plan cost, ecost(ni , q ), of this query at node ni as ecost(ni , q ) = size(Bi ) +
(nj leaf Desc(ni ))((q.rsSj )=)

size(Bj )

or equivalently as ecost(ni , q )


nj leaf Desc(ni )

 size(Bj )

= size(Bi ) +  - icost(ni , q )

Since the initial two terms above are recorded in the index creation time, the computation of exclusive cost is a constant time operation. · Overall Cost Estimation and the Leaf Access Plan. Given the above, we can find the best strategy for processing the query set Q(ni ) at node ni by considering the overall estimated cost term, cost(ni , Q(ni )), defined as    Ei,j × ecost(ni , qj ) + (1 - Ei,j ) × icost(ni , qj )
leaf access cost f or all relevant queries qj Q(ni )

also converts the range in 2-D space into 1-D space by identifying the relevant nodes in the hierarchy. Next, for each internal node, ni  R, a cost, costi , is estimated assuming that this node and its leaf descendants are used for identifying the matches in the range Si . The outline of this process is presented in Algorithm 2 and is detailed below: · Top-Down Traversal and Pruning. Line 5 indicates that the process starts at the root and moves towards the leaves. For each internal node, ni , being visited, first, the set, Q(ni )  Q, of queries for which ni is relevant is identified by intersecting the ranges of the queries relevant to the parent (i.e., Q(parent(ni ))) with the range of ni . More specifically, Q(ni ) = {q  Q(parent(ni )) s.t. (q.rs  Si ) = }. If Q(ni ) = , then ni and all its descendants are ignored, otherwise ni is included in the set R. · Inclusive and Exclusive Cost Computation. Once the portion, R, of the hierarchy relevant to the query workload is identified, next, the algorithm re-visits all internal nodes in R in a bottom-up manner and computes a cost estimate for executing queries in Q(ni ): for each query, q  Q(ni ), the algorithm computes inclusive and exclusive leaf access costs: · Inclusive leaf access plan (Line 17): If query, q , is executed using an inclusive plan at node, ni , this means that the result for the range (q.rs  Si ) will be obtained by identifying and combining (using bitwise ORs) all relevant leaf bitmaps under node ni . Therefore, the cost of this leaf access plan is icost(ni , q ) =
(nj leaf Desc(ni ))((q.rsSj )=)

where Ei,j = 1 means an exclusive leaf access plan is chosen for query, qj , at this node and Ei,j = 0 otherwise.

Cut Bitmap Selection.
Once the nodes in the hierarchy are assigned estimated costs as described above, the cut that will be used for query processing is found by traversing the hierarchy in a bottom-up fashion and picking nodes based on their estimated costs4 . The process is outlined in Algorithm 3. Intuitively, for each internal node, ni  IH , the algorithm computes a revised cost estimate, rcosti , by comparing the cost, costi , estimated in the earlier phase of the process, with the total revised costs of ni 's children: · In Line 13, the function f indBlockIO(ni ) returns the cost of reading the block T (Bi ). If this block has already been marked "to-read", then the reading cost has already been accounted for, so the cost is zero. Otherwise, the cost is equal to the size of the block T (Bi ), as explained in Section 4.2.1. · As we see in Line 21, it is possible that a block T is first marked "to-read" and then, later in the process, marked "not-to-read", because for the corresponding nodes in the cut, more suitable ancestors are found and the block is no longer needed. · If costi is smaller (Line 17), then ni and its leaf descendants can be used for identifying the matches to the queries in the range Si . In this case, no revision is Note that this bottom-up traversal can be combined with the bottom-up traversal of the prior phase. We are describing them as separate processes for clarity.
4

size(Bj ).

This value can be computed incrementally, simply by summing up the inclusive costs of the children of ni .

1389

Algorithm 3 Cut Selection Algorithm 1: Input: Hierarchy H ; per-node query workload Q(ni ); pernode cost estimates costi ; and the corresponding leaf access plans, Ei,j , for node/query pairs ni  H and qj  Q(ni ); the set, R  IH , or relevant internal nodes Output: All-inclusive, CI , and Exclusive, CE , cut nodes Initialize: Cand =  procedure findCut for each relevant internal node ni in R in a bottomup fashion do Set internal children = children(ni )  IH ; if internal children =  then add ni to Cand; rcosti = costi else costChildren = nj internal children rcostj rcostIOi = f indBlockIO (ni ) for each child nj in internal children do costChildrenIO = costChildrenIO + f indBlockIO (nj ) end for if (rcosti + rcostIOi )  (costChildren + costChildrenIO ) then for each descendant nk of ni in Cand do remove nk from Cand; if nk is the only node to read from T (Bk ) then mark T (Bk ) as "not-to-read"; end if end for add ni to Cand; rcosti = costi mark T (Bi ) as "to-read"; else rcosti = costChildren end if end if end for CE = {ni  Cand s.t. qj Q(ni ) Ei,j == 1} CI = Cand/CE end procedure

answered only by accessing relevant leaves under the nodes in CI . We store the blocks containing the bitmaps of these relevant leaves in an LRU-based cache so that leaf bitmaps can be reused by multiple queries.

2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34:

4.3.3 Complexity
The bitmap selection process consists of two steps: (a) a per-node cost estimation step and (b) a cut bitmap selection step. Each of these steps visit only the relevant nodes of the hierarchy. Therefore, if we denote the set of nodes of the hierarchy, H , that intersect with any query in Q, as H (Q), then the overall work is linear in the size of H (Q). During the cost estimation phase, for each visited node, ni , an inclusive and exclusive cost is estimated for any query that intersects with this node. Therefore, the worst case time cost of the overall process (assuming that all queries in Q intersect with all nodes in H (Q)) is O(|Q| × |H (Q)|).

5. EXPERIMENTAL EVALUATION
In this section, we evaluate the effectiveness of the proposed compressed spatial hierarchical bitmap (cSHB) index structure using spatial data sets with different characteristics, under different system parameters. To assess the effectiveness of cSHB, we also compare it against alternatives. We ran the experiments on a quad-core Intel Core i5-2400 CPU @ 3.10GHz machine with 8.00GB RAM, and a 3TB SATA Hard Drive with 7200 RPM and 64MB Buffer Size, and in the same Windows 7 environment. All codes were implemented and run using Java v1.7.

5.1 Alternative Spatial Index Structures and the Details of the cSHB Implementation
As alternatives to cSHB, we considered different systems operating based on different spatial indexing paradigms. In particular, we considered spatial extensions of PostgreSQL called PostGIS [2], of a widely used commercial DBMS (which we refer to as DBMS-X), and of Lucene [1]: · PostGIS [2] creates spatial index structures using an R-tree index implemented on top of GiST. · DBMS-X maps 2D space into a 1D space using a variation of Hilbert space filling curve and then indexes the data using B-trees. · Apache Lucene [1,18], a leading system for text indexing and search, provides a spatial module that supports geo-spatial range queries in 2D space using quadtrees and prefix-based indexing. Intuitively, the space is partitioned using a MX-quadtree structure (where all the leaves are at the same level and a given region is always partitioned to its quadrants at the center [28]) and each root-to-leaf path is given a unique path-string. These path-strings are then indexed (using efficient prefixindexing algorithms) for spatial query processing. Since database systems potentially have overheads beyond pure query processing needs, we also considered disk-based implementations of R*-tree [8] and the Hilbert R-tree [19]. For this purpose, we used the popular XXL Java library [10]: · A packed R*-tree, with average leaf node utilization  95% (page size 4MB). · A packed Hilbert R-tree, with average leaf node utilization  99% (page size 4MB).

necessary and the revised cost, rcosti is equal to costi . Any descendants of ni are removed from the set, Cand, of cut candidates and ni is inserted instead. · If, on the other hand, the total revised cost of ni 's children is smaller than costi , then matches to the queries in the range Si can be more cheaply identified by considering the descendants of ni , rather than ni itself (Line 27). Consequently, in this case, the revised cost, rcosti , is set to rcosti =
nj children(ni )

rcostj .

As we experimentally show in Section 5, the above process has a small cost. This is primarily because, during bottomup traversal, only those nodes that have not been pruned in the previous top-down phase are considered. Once the traversal is over, the nodes in the set, Cand, of cut candidates are reconsidered and those that include exclusive leaf access plans are included in the exclusive cut set, CE , and the rest are included in the all-inclusive cut set, CI .

Caching of Cut and Leaf Bitmaps.
During query execution, the bitmaps of the nodes in CE are read into a cut bitmaps buffer, whereas the bitmaps for the nodes in CI do not need to be read as the queries will be

1390

Table 1: Data sets and clustering
Data set #points #points per (nonempty) cell (h = 10) Min. Avg. Max. 54 95 143 1 352 312944 1 3422 1.2M
Uniform (100M; Synth)
Clustered (6.4M; Gowalla) Clustered (688M; OSM)

Table 3: Index Creation Time (sec.)
Data set Synthetic Gowalla OSM cSHB 1601 24 2869 Luc. 2396 114 12027 DBMS -X 3865 232 30002 Post GIS 4606 112 76238 R*tree 2160 22 18466 Hilb. R-tree 2139 20 17511

Synthetic (Uniform) Gowalla (Clustered) OSM (Clustered)

100M 6.4M 688M

Table 4: Index Size on Disk (MB)
Data set Synthetic Gowalla OSM cSHB 10900 44 2440 Luc. 5190 220 22200 DBMS -X 1882 121 12959 Post GIS 8076 600 61440 R*tree 3210 211 22100 Hilb. R-tree 1510 100 10400

Data Skew
log (# of non-empty cells)
25

y = -2.0x + 15.0
20 15 10 5 0 -5 -3 -1 1 log (r) y = -1.7x + 13.3 y = -1.3x + 11.1

Linear (Uniform (100M; Synth)) Linear (Clustered (6.4M; Gowalla))
Linear (Clustered (688M; OSM))

3

5

7

Figure 6: Data Skew Table 2: Parameters and default values (in bold)
Parameter Block Size (MB) Query range size |Q| h Buffer size (MB) Value range 0.5; 1; 2.5; 5; 10 0.5% 1%; 5% 100; 500; 1000 9; 10; 11 2; 3; 5; 10; 20; 100

We also implemented the proposed cSHB index structure on top of Lucene. In particular, we used the MX-quadtree hierarchy created by Lucene as the spatial hierarchy for building cSHB. We also leveraged Lucene's (Java-based) region comparison libraries to implement range searches. The compressed bitmaps and compressed domain logical operations were implemented using the JavaEWAH library [21]. Due to space limitations, we only present results with the incremental strategy for query evaluation.

paring its (a) index creation time, (b) index size, and (c) query processing time to those of the alternative index structures described above under different parameter settings. Table 2 describes the parameters considered in these experiments and the default parameter settings. Since our goal is to assess the contribution of the index in the cost of the query plans, all index structures in our comparison used index-only query plans. More specifically, we executed a count() query and configured the index structures such that only the index is used to identify the relevant entries and count them to return the results. Consequently, only the index files are used and data files are not accessed. Note that all considered index structures accept squareshaped query ranges. The range sizes indicated in Table 2 are the lengths of the boundaries relative to the size of the considered 2D space. These query ranges in the query workloads are generated uniformly.

5.4 Discussion of the Indexing Results
Indexing Time. Table 3 shows the index creation times for different systems and index structures, for different data sets (with different sizes and uniformity): cSHB index creation is fastest for the larger Synthetic and OSM data sets, and competitive for the smaller Gowalla data set. As the data size gets larger, the alternative index structures become significantly slower, whereas cSHB is minimally affected by the increase in data size. The index creation time also includes the time spent on creating the hierarchy for cSHB. Index Size. Table 4 shows the sizes of the resulting index files for different systems and index structures and for different data sets. As we see here, cSHB provides a competitive index size for uniform data (where compression is not very effective). On the other hand, on clustered data, cSHB provides very significant gains in index size ­ in fact, even though the clustered data set, OSM, contains more points, cSHB requires less space for indexing this data set than it does for indexing the uniform data set. Impact of Block Size. As we discussed in Section 3.2.2, cSHB writes data on the disk in a blocked manner. In Figure 7, we see the impact of the block sizes on the time needed to create the bitmaps. As we see here, one advantage of using blocked storage is that the larger the blocks used, the faster the index creation becomes.

5.2 Data Sets
For our experiments, we used three data sets: (a) a uniformly distributed data set that consists of 100 million synthetically generated data points. These points are mapped to the range -180, -90 to 180, 90 , (b) a clustered data set from Gowalla, which contains the locations of check-ins made by users. This data set is downloaded from the Standford Large Network Dataset Collection [4], and (c) a clustered data set from OpenStreetMap (OSM) [3] which contains locations of different entities distributed across North America. The OSM data set consists of approximately 688 million data points in North America. We also normalized both the real data sets to the range -180, -90 to 180, 90 . In order to obtain a fair comparison across all index structures and the data sets, all three data sets are mapped onto a 2h × 2h space and the positions of the points in this space are used for indexing. Table 1 provides an overview of the characteristics of these three very different data sets. Figure 6 re-confirms the data skew in the three data sets using the box-counting method proposed in [9]: in the figure, the lower the negative slope, the more skewed the data. The figure shows that the clustered Gowalla data set has the largest skew.

5.5 Discussion of the Search Results
Impact of the Search Range. Table 5 shows the impact of the query range on search times for 500 queries under the default parameter settings, for different systems. As we expected, as the search range increases, the execution time

5.3 Evaluation Criteria and Parameters
We evaluate the effectiveness of the proposed compressed spatial hierarchical bitmap (cSHB) index structure by com-

1391

Impact of the Block Size on Index Creation Time
1800

Writing Bitmaps

Time (sec., log. scale)

Creating Bitmaps

1000 100
10 1

Read Hierarchy Map Ranges Search Cut Bitmaps Read Bitmaps Combine Bitmaps

Impact of Block Sizes on cSHB Time
Tot: 65

Time (sec.)

1350 900 450 0
0.5 1 2.5 Block Size (MB) 5 10

Tot: 113

Tot: 141

Tot: 51

Tot: 42

0.1
0.5 1 2.5 5 10

Block Size (MB)

Figure 7: Impact of the block size on index creation time of cSHB (uniform data set) Table 5: Comparison of search times for alternative schemes and impact of the search range on the time to execute 500 range queries (sec.)
Range cSHB Luc. DBMS Post R*Hilb. -X GIS tree R-tree Synthetic (Uniform; 100M) 414 12887 2211 4391 345 28736 2329 4480 368 72005 2535 4881 Gowalla (Clustered; 6.4M) 24 19 8 24 29 34 11 26 37 194 20 45 OSM (Clustered; 688M) 303 1129 3486 4368 645 4117 3889 5599 15567 18172 4626 6402 cSHB -LO 52 59 1700 2 3 5 13 14 78

(a) Impact of block size on overall cSHB execution time
90
Time (sec.)

60

Impact of Block Sizes on Bitmap Reading Time
Tot: 31 Tot: 22 Tot: 11

Tot: 84 Tot: 55

30 0

0.5% 1% 5% 0.5% 1% 5% 0.5% 1% 5%

35 42 137 2 3 3 13 15 28

123 131 187 2 3 48 23 30 66

0.5

1

2.5 Block Size (MB)

5

10

(b) Impact of block size on bitmap reading time Figure 9: Impact of the block size (500 queries, 1% q. range, uniform data)
cSHB Time Breakdown (1% Q. Range, Uniform Data)
100 Read Hierarchy Map Ranges Search Cut Bitmaps Read Bitmaps Combine Bitmaps

Range Search Times (500 Queries)
Time (sec., log. scale)
100 10 1 0.1 0.5% 1% 5% 0.5% 1% 5% 0.5% 1% 5%
Synthetic (Uniform; 100M) Gowalla (Clustered; 6.4M) Data sets OSM (Clustered; 688M)

Time (sec., log. scale)

Read Hierarchy Map Ranges Search Node Bitmaps Read Bitmaps Combine Bitmaps

Tot: 32
10
1

Tot: 42

Tot: 61

0.1
100 500 Number of Queries 1000

Figure 10: Impact of the number of queries on the execution time of cSHB (1% q. range, uniform data) as the search range grows, whereas the other costs are more or less independent of the sizes of the query ranges. Impact of the Block Sizes. As we see above, reading bitmaps from the disk and operating on them is a major part of cSHB query execution cost; therefore these need to be performed as efficiently as possible. As we discussed in Section 3.2.2, cSHB reads data from the disk in a blocked manner. In Figure 9, we see the impact of the block sizes on the execution time of cSHB, including the time needed to read bitmaps from the disk. As we see here, small blocks are disadvantageous (due to the directory management overhead they cause). Very large blocks are also disadvantageous as, the larger the block gets, the larger becomes the amount of redundant data read for each block access. As we see in the figure, for the configuration considered in the experiments, 1MB blocks provided the best execution time. Impact of the Number of Queries in the Workload. Figure 10 shows the total execution times as well as the breakdown of the execution times for cSHB for different number of simultaneously executing queries. While the total execution time increases with the number of simultaneous queries, the increase is sub-linear, indicating that there are savings due to the shared processing across these queries.

Figure 8: cSHB execution breakdown becomes larger for all alternatives. However, cSHB provides the best performance for all ranges considered, especially for the clustered data sets. Here, we also compare cSHB with its leaf-only version (called cSHB-LO), where instead of a cut consisting of potentially internal nodes, we only choose the leaf nodes for query processing. As you can see from the figure, while cSHB-LO is a good option for very small query ranges (0.5% and 1%), it becomes very slow as the query range increases (since the number of bitwise operations increases, and it is not able to benefit from clustering). Execution Time Breakdown. Figure 8 provides a breakdown of the various components of cSHB index search (for 500 queries under the default parameter settings): The bitmap selection algorithm presented in Section 4.3 is extremely fast. In fact, the most significant components of the execution are the times needed for reading the hierarchy into memory5 , and for fetching the selected bitmaps from the disk into the buffer, and performing bitwise operations on them. As expected, this component sees a major increase
5 Once a hierarchy is read into the memory, the hierarchy does not need to be re-read for the following queries.

1392

cSHB Time Breakdown (500 Queries, 1% Q. Range, Uniform Data)
1000

Time (sec., log. scale)

Read Hierarchy Map Ranges Search Cut Bitmaps Read Bitmaps Combine Bitmaps

7. REFERENCES
[1] Apache Lucene. http://lucene.apache.org/core/4 6 0/ spatial/org/apache/lucene/spatial/prefix/tree/ SpatialPrefixTree.html [2] Using PostGIS: Data Management and Queries. http://postgis.net/docs/using postgis dbmanagement.html [3] OpenStreetMap. http://www.openstreetmap.org/ [4] J. Leskovec and A. Krevl. SNAP Datasets: Stanford Large Network Dataset Collection. http://snap.stanford.edu/data [5] D. Abadi et al. Compression and Execution in Column-Oriented Database systems. SIGMOD 2006. [6] A. Aji et al. Hadoop-GIS: A High Performance Spatial Data Warehousing System over MapReduce. PVLDB 2013. [7] Rudolf Bayer. The Universal B-tree for Multidimensional Indexing: General concepts. WWCA 1997. [8] N. Beckmann et al. The R*-tree: An Efficient and Robust Access Method for Points and Rectangles. SIGMOD 1990. [9] A. Belussi and C. Faloutsos. Self-Spatial Join Selectivity Estimation Using Fractal Concepts. TOIS 1998. [10] J.Bercken et al. XXL - A Library Approach to Supporting Efficient Implementations of Advanced Database Queries. VLDB 2001. [11] A.R. Butz. Alternative Algorithm for Hilbert's Space-Filling Curve. TOC 1971. [12] A. Cary et al. Experiences on Processing Spatial Data with MapReduce. SSDBM 2009. [13] J. Chmiel et al. Dimension Hierarchies by means of Hierarchically Organized Bitmaps. DOLAP 2010. [14] P. Chovanec and M. Kr´ atk´ y. On the Efficiency of Multiple Range Query Processing in Multidimensional Data Structures. IDEAS 2013. [15] F. Deli` ege and T. Pedersen. Position List Word Aligned Hybrid: Optimizing Space and Performance for Compressed Bitmaps. EDBT 2010. [16] L.Gosink et al. HDF5-FastQuery: Accelerating Complex Queries on HDF Datasets using Fast Bitmap Indices. SSDM 2006. [17] David Hilbert. Ueber stetige abbildung einer linie auf ein flachenstuck. Mathematische Annalen 1891. [18] Y. Jing et al. An Empirical Study on Performance Comparison of Lucene and Relational Database. ICCSN 2009. [19] I. Kamel and C. Faloutsos. Hilbert R-tree: An Improved R-tree using Fractals. VLDB 1994 [20] O. Kaser et al. Histogram-Aware Sorting for Enhanced Word-Aligned Compression in Bitmap Indexes. DOLAP 2008. [21] D. Lemire et al. Sorting improves Word-Aligned Bitmap Indexes. DKE 2010. [22] V. Markl et al. Improving OLAP Performance by Multidimensional Hierarchical Clustering. IDEAS 1999. [23] G. Morton. A Computer Oriented Geodetic Data Base and a New Technique in File Sequencing. IBM 1966. [24] M. Morzy et al. Scalable Indexing Technique for Set-Valued Attributes. ADBIS 2003. [25] P. Nagarkar and K. S. Candan. HCS:Hierarchical Cut Selection for Efficiently Processing Queries on Data Columns using Hierarchical Bitmap Indices. EDBT 2014. [26] M. A. Olma et al. BLOCK: Efficient Execution of Spatial Range Queries in Main-Memory. Technical report EPFL 2013. [27] A. N. Papadopoulos and Y. Manolopoulos. Multiple Range Query Optimization in Spatial Databases. ADBIS 1998. [28] H. Samet. Foundations of Multidimensional and Metric Data Structures, 2005. [29] R. Sinha and M. Winslett. Multi-Resolution Bitmap Indexes for Scientific Data. TODS 2007. [30] T. Siqueira et al. The SB-index and the HSB-index: Efficient Indices for Spatial Data Warehouses. Geoinformatica 2012. [31] T. Skopal et al. Algorithm for Universal B-trees. Inf. Syst. 2006. [32] K. Wu et al. On the Performance of Bitmap Indices for High Cardinality Attributes. VLDB 2004. [33] K. Wu et al. An Efficient Compression Scheme for Bitmap Indices. TODS 2004. [34] M. Zaker et al. An Adequate Design for Large Data Warehouse Systems: Bitmap Index versus B-tree Index. IJCC 2008. [35] Y. Zhong et al. Towards Parallel Spatial Query Processing for Big Spatial Data. IPDPSW 2012.

100 10 1 0.1

Tot: 28

Tot: 42

Tot: 152

9

10 # of levels of the hierarchy

11

Figure 11: Impact of the depth of the hierarchy (500 queries, 1% query range, uniform data) Table 6: Working set size in terms of 1MB blocks
Q.Range (on 100M data) 0.5% 1% 5% Min 1 1 1 Avg. 2.82 2.51 1.02 Max. 36 178 95

Table 7: Impact of the buffer size on exec. time (in seconds, for 500 queries, 100M data)
Query Range 0.5% 1% 5% 2MB 11.8 24.2 823.8 3MB 11.3 19.1 399.9 Buffer Size 5MB 10MB 10.9 10.6 18.1 17.5 155.9 105.8 20MB 10.5 17.3 101.6 100MB 10.2 16.3 94.9

Also, in Section 4.2.2, we had observed that the number of queries in the query workload and query ranges determine the relative costs of in-buffer operations vs. disk I/O. In Figures 8 and 10, we see that this is indeed the case. Impact of the Depth of the Hierarchy. Figure 11 shows the impact of the hierarchy depth on the execution time of cSHB: a 4× increase in the number of cells in the space (due to a 1-level increase in the number of levels of the hierarchy) results in < 4× increase in the execution time. Most significant contributors to this increase are the time needed to read the hierarchy and the time for bitmap operations. Impact of the Cache Buffer. As we discussed in Section 4.2.2, the incremental scheduling algorithm keeps a buffer of blocks containing the working set of leaf bitmaps. As Table 6 shows, the average size of the working set is fairly small and can easily fit into the L3 caches of modern hardware. Table 7 confirms that a small buffer, moderately larger than the average working set size, is sufficient and larger buffers do not provide significant gains.

6.

CONCLUSIONS

In this paper, we argued that bitmap-based indexing can be highly effective for running range query workloads on spatial data sets. We introduced a novel compressed spatial hierarchical bitmap (cSHB) index structure that takes a spatial hierarchy and uses that to create a hierarchy of compressed bitmaps to support spatial range queries. Queries are processed on cSHB index structure by selecting a relevant subset of the bitmaps and performing compressed-domain bitwise logical operations. We also developed bitmap selection algorithms that identify the subset of the bitmap files in this hierarchy for processing a given spatial range query workload. Experiments showed that the proposed cSHB index structure is highly efficient in supporting spatial range query workloads. Our future work will include implementing and evaluating cSHB for data with more than two dimensions.

1393

