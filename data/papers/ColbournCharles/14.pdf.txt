ATLAS: Adaptive Topology- and Load-Aware Scheduling
Jonathan Lutz, Charles J. Colbourn, and Violet R. Syrotiuk CIDSE, Arizona State University, Tempe, AZ 85287-8809 Email: {jlutz, colbourn, syrotiuk}@asu.edu

arXiv:1305.4897v2 [cs.NI] 4 Nov 2013

Abstract--The largest strength of contention-based MAC protocols is simultaneously the largest weakness of their scheduled counterparts: the ability to adapt to changes in network conditions. For scheduling to be competitive in mobile wireless networks, continuous adaptation must be addressed. We propose ATLAS, an Adaptive Topology- and Load-Aware Scheduling protocol to address this problem. In ATLAS, each node employs a random schedule achieving its persistence, the fraction of time a node is permitted to transmit, that is computed in a topology and load dependent manner. A distributed auction (REACT) piggybacks offers and claims onto existing network traffic to compute a lexicographic max-min channel allocation. A node's persistence p is related to its allocation. Its schedule achieving p is updated where and when needed, without waiting for a frame boundary. We study how ATLAS adapts to controlled changes in topology and load. Our results show that ATLAS adapts to most network changes in less than 0.1s, with about 20% relative error, scaling with network size. We further study ATLAS in more dynamic networks showing that it keeps up with changes in topology and load sufficient for TCP to sustain multi-hop flows, a struggle in IEEE 802.11 networks. The stable performance of ATLAS supports the design of higher-layer services that inform, and are informed by, the underlying communication network. Index Terms--Wireless networks, medium access control, adaptation.

I. I NTRODUCTION Despite the well known shortcomings of IEEE 802.11 and other contention-based MAC protocols for mobile wireless networks--such as probabilistic delay guarantees, severe short-term unfairness, and poor performance at high load-- they remain the access method of choice. The primary reason is their ease in adapting to changes in network conditions, specifically to changes in topology and in load. The lack of timely adaptation is the most serious limitation facing scheduled MAC protocols. For scheduling to be competitive, continuous adaptation is required. Topology-dependent approaches to adaptation in scheduling alternate a contention phase with a scheduled phase. In the contention phase, nodes exchange topology information used to compute a conflict-free schedule that is followed in the subsequent scheduled phase (see, as examples, [5], [30]). However, changes in topology and load do not always align with the phases of the algorithm resulting in a schedule that often lags behind the network state. In contrast, the idea behind topology-transparent scheduling is to design schedules independent of the detailed network topology [3], [15]. Specifically, the schedules do not depend on the identity of a node's neighbours, but rather on how many

of them are transmitting. Even if a node's neighbours change, its schedule does not; if the number of neighbours does not exceed the designed bound then the schedule guarantees success. Though such schedules are robust to network conditions that deviate from the design parameters [27], because the schedules do not adapt, the technique remains a theoretical curiosity. In contention-based schemes, such as IEEE 802.11, a node computes implicitly when to access the channel, basing its decisions on perceived channel contention. We instead compute a node's persistence--the fraction of time it is permitted to transmit--explicitly in a way that tracks the current topology and load. To achieve this, we propose ATLAS, an Adaptive Topology- and Load-Aware Scheduling protocol. Channel allocation is a resource allocation problem where the demands correspond to transmitters, and the resources to receivers. ATLAS implements the REsource AlloCaTion computed by REACT, a distributed auction that runs continuously. REACT piggybacks offers and claims onto existing network traffic to compute the lexicographic max-min allocation to transmitters which we call the TLA allocation, emphasizing that it is both topology- and load-aware. Each node's random schedule, achieving a persistence informed by its allocation, is updated whenever a change in topology or load results in a change in allocation. While the slots of the schedule are grouped into frames, this is done only to reduce the variance in delay [6]; there is no need to wait for a frame boundary to update the schedule. Even though the random schedules may not be conflict-free, ATLAS is not contention-based; it does not select persistences or make scheduling decisions based on perceived channel contention--its decisions are based solely on topology and load. We study how ATLAS adapts to controlled changes in topology and load, measuring convergence time, relative error, and scalability. We also assess the ability of ATLAS to adapt in more dynamic network conditions. To the best of our knowledge, ATLAS is the first scheduled MAC protocol able to adapt to changes in topology and load that is competitive with contention-based protocols in throughput and delay while realizing superior delay variance. It achieves this through the continuous computation of the TLA allocation, and updating the schedule on-the-fly. These updates occur only where and when needed. By not requiring phases of execution and by computing persistences rather than conflict-free schedules, ATLAS eliminates the complexity of, and lag inherent in, topology-dependent approaches. By not being dependent on the identity of neighbours, ATLAS shares the best of topology-transparent schemes (and also their

Submitted to IEEE Transactions on Mobile Computing ­ c 2013 IEEE

2

potential for collisions) yet overcomes its weakness by being adaptive. By not forcing updates to be frame synchronized, ATLAS shares the critical features of continuous adaptation with contention-based protocols. As a result, ATLAS achieves predictable throughput and delay characteristics. Such characteristics and information about localized capacity at the MAC layer may be used to inform higher layers, while end-toend characteristics at higher layers may be used to inform ATLAS. This may support the development of an agile, higher performing protocol stack. The primary contributions of this paper are twofold: (1) The REACT algorithm, an asynchronous, adaptive, and distributed auction that solves a general resource allocation problem to produce the TLA allocation. (2) ATLAS, a MAC protocol that uses REACT to solve the specific problem of channel allocation in a wireless network where each node produces a random schedule with the number of transmission slots determined by its allocation. The sequel is organized as follows: Section II defines a general resource allocation problem and presents the REACT algorithm, proving its correctness. Section III expresses channel allocation as a resource allocation problem and defines ATLAS. Related work is described in Section IV. After describing the simulation set-up in Section V, Section VI studies how ATLAS adapts to controlled changes in topology and load, and to dynamic network conditions. In Section VII, we discuss open issues and potential applications of REACT, including the design of higher-layer services that inform, and are informed by, the underlying communication channel. II. D ISTRIBUTED R ESOURCE A LLOCATION -- REACT We consider a general resource allocation problem. Let R be a set of N resources with capacity c = (c1 , . . . , cN ). Let D be a set of M demands with magnitudes w = (w1 , . . . , wM ). Resource j  R is required by demands Dj  D. Demand i  D consumes capacity at all resources in Ri  R simultaneously. The resource allocation s = (s1 , . . . , sM ), si  0 defines the capacity reserved for the demands. Resource allocation s is feasible if iDj si  cj for all j  R and si  wi for all i  D. Demand i is satisfied if si  wi . Resource j is saturated if iDj si  cj . Throughout, capacity refers to the magnitude of a resource. Definition 1: [22] A feasible allocation s is lexicographically max-min if, for every demand i  D, either i is satisfied, or there exists a saturated resource j with i  Dj where si = max(sk : k  Dj ). We now describe REACT, a distributed auction that computes the lexicographic max-min allocation. In it, resources are represented by auctioneers and demands by bidders. Each auctioneer maintains an offer--the maximum capacity consumed by any adjacent bidder--and each bidder maintains a claim--the capacity the bidder intends to consume at adjacent auctions. The final claim of bidder i defines allocation si . Auctioneer j satisfies Def. 1 locally by increasing its offer in an attempt to become saturated while maintaining a feasible allocation. Bidder i satisfies Def. 1 locally for demand i by increasing its claim until it is satisfied or has a maximal claim

Algorithm 1 REACT Bidder for Demand i.
1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: upon initialization Ri   wi  0 U PDATE C LAIM () end upon upon receiving a new demand magnitude wi U PDATE C LAIM () end upon upon receiving offer from auctioneer j offers[j ]  offer // Remember the offer of auctioneer j . U PDATE C LAIM () end upon upon bidder i joining auction j Ri  Ri  j // Resource j is now required by demand i. U PDATE C LAIM () end upon upon bidder i leaving auction j Ri  Ri \ j // Resource j is no longer required by demand i. U PDATE C LAIM () end upon procedure U PDATE C LAIM () // Select the claim to be no larger than the smallest offer or wi . claim  min ({offers[j ] : j  Ri }, wi ) send claim to all auctions in Ri end procedure

at an adjacent auction. Through continuous updates of offers and claims, the auctioneers and bidders eventually converge on the lexicographic max-min allocation. We give precise definitions of auction and bidder behaviour next. Bidder i knows wi and maintains set Ri . Offers are stored in offers[]; offers[j ] holds the offer last received from auctioneer j . Bidder i constrains its claim to be no larger than wi or the smallest offer from auctioneers in Ri , claim = min ({offers[j ] : j  Ri }, wi ) . (1)

Auctioneer j knows cj and maintains set Dj . Bidder claims are stored in claims[]; claims[i] holds the claim last received  from bidder i. Auctioneer j identifies set Dj  Dj containing bidders with claims strictly smaller than its offer,
 Dj = {b : b  Dj , claims[b] < offer}.

(2)

 Bidders in Dj are either satisfied or are constrained by another auction and cannot increase their claims in response to a larger  offer from auctioneer j . Bidders in Dj \ Dj are constrained by auction j . They may increase their claims in response to a  larger offer. Resources left unclaimed by bidders in Dj ,

Aj = cj -

 iDj

claims[i] ,

(3)

remain available to be offered in equal portions to bidders in  Dj \Dj . If claims of all bidders in Dj are smaller than the offer  (i.e., Dj = Dj ), there are no bidders to share the available resources in Aj . The auctioneer sets its offer to Aj plus the largest claim, ensuring that any bidder in Dj can increase its claim to consume resources in Aj : offer =
  Aj /|Dj \ Dj |, if Dj = Dj , (4) Aj + max (claims[i] : i  Dj ) , otherwise.

Alg. 1 and Alg. 2 describe actions taken by the bidders and auctioneers of REACT in response to externally triggered

3

Algorithm 2 REACT Auctioneer for Resource j .
1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43: 44: upon initialization Dj   cj  0 U PDATE O FFER () end upon upon receiving a new capacity of cj U PDATE O FFER () end upon upon receiving claim from bidder i claims[i]  claim // Remember the claim of bidder i. U PDATE O FFER () end upon upon bidder i joining auction j Dj  Dj  i // Demand i now requires resource j . U PDATE O FFER () end upon upon bidder i leaving auction j Dj  Dj \ i // Demand i no longer requires resource j . U PDATE O FFER () end upon procedure U PDATE O FFER ()   Dj Aj  cj done  False while ( done = False ) do  contains all bidders in D , then auction j does not // If Dj j // constrain any of the bidders in Dj .  = D ) then if ( Dj j done  True offer  Aj + max ({claims[i] : i  Dj }) // Otherwise, auction j constrains at least one bidder in Dj . else done  True // What remains available is offered in equal portions to the // bidders constrained by auction j . | offer  Aj /|Dj \ Dj  and compute A for the new offer. // Construct Dj j  } do for all b  {Dj \ Dj if ( claims[b] < offer ) then   D  b Dj j Aj  Aj - claims[b] done  False send offer to all bidders in Dj end procedure

events. Collectively, auctioneers and bidders know the inputs to the allocation problem and bidder claims converge on the lexicographic max-min allocation; the claim of bidder i converges on si . The correctness of Alg. 1 and Alg. 2 is established in two steps: Lemma 1 establishes forward progress on the number of auctioneers to have converged on their final offer. Theorem 1 employs Lemma 1 to show eventual convergence to the lexicographic max-min allocation. Let claimi denote the claim of bidder i and offerj the offer of auctioneer j . Assume that the resource allocation remains constant for the period of analysis, that bidder i knows Ri and wi , and that auctioneer j knows Dj and cj . Further assume communication between adjacent auctioneers and bidders is not delayed indefinitely. A claim or offer is stable if it has converged on its final value. Denote by Astable the set of auctioneers whose offers are stable and remain the smallest among all offers. Lemma 1: Suppose Astable contains k auctioneers, 0  k < N . Then, within finite time, at least one auctioneer converges

on the next smallest offer omin . Offers equal to omin are stable and remain smaller than all other offers not in Astable . Proof: Wait sufficient time for every bidder i to send a new claim to auctioneers in Ri and for every auctioneer j to send a new offer to bidders in Dj . Let omin be the smallest offer of an auctioneer not in Astable . Assume to the contrary that offerx for some x  / Astable is the first to become smaller than omin . By Eq. 2 and 4, a decrease to offerx can only occur after a bidder y at auction x with claimy < offerx increases its claim. By Eq. 1, claimy can increase only after its limiting constraint starts out smaller than offerx and increases. Constraints in the system smaller than offerx are maximum claims, offers from Astable , and offers equal to omin . Maximum claims and offers from Astable do not change, leaving some x with offerx = omin as the only potential limiting constraint for claimy . By Eq. 2 and 4, offerx can increase only after one of its bidders y reduces its claim to be smaller than offerx . By Eq. 1, claimy can get smaller only after one of its auctioneers, say x , reduces its offer to be offerx < omin = offerx contradicting the assumption that offerx is the first to become smaller than omin . Therefore, offers equal to omin remain smaller than offers not from Astable . By Eq. 2 and 4, any j offering omin can change only after a bidder i at auction j with claimi  omin changes. By Eq. 1, claimi only changes if its limiting constraint changes. Potential limiting constraints include wi , offers from Astable , and offers equal to omin . These constraints are stable; therefore, offers equal to omin are stable. Theorem 1: Bidders and auctioneers of Alg. 1 and Alg. 2 compute the lexicographic max-min allocation. Proof: We apply Lemma 1 to show by induction that every auctioneer eventually computes a stable offer. Base Case: Consider an allocation problem with arbitrary wi , cj , Ri , and Dj for 1  i  M , 1  j  N . Let |Astable | = 0. By Lemma 1, at least one auctioneer eventually converges on a smallest offer omin . Offers equal to omin are stable and remain smallest among all offers. Add auctioneers offering omin to Astable ; |Astable |  1. Inductive Step: Let |Astable | = k , 1  k < N . Then, by Lemma 1 a non-empty set of auctioneers A+ with A+  Astable =  eventually converge on the next smallest offer. Offers from A+ remain smaller than offers not from A+ or Astable and are stable. Add A+ to Astable ; |Astable |  k + 1. By induction, all auctioneers are eventually added to Astable . Wait for auctioneers to send their offers to adjacent bidders. Bidder claims are now stable. By Eq. 1, bidder i is either satisfied with its claim (claimi = wi ) or its claim is maximal at an auction in Ri . By Definition 1, the claims are lexicographic max-min. III. T HE ATLAS MAC P ROTOCOL Channel allocation in wireless networks can be expressed as a resource allocation problem. In this context, transmitters correspond to the demands in D and receivers to the resources in R. Label transmitters {1, . . . , M } and receivers {1, . . . , N }. A transmitter with a non-zero demand magnitude is active. Receiver j is in Ri if it is within transmission range of

4

Slot x Node A Node B Node C
Header Fields pkt #1 collision pkt #2 claim offer

Slot x + 1

Slot x + 2
pkt #1 pkt #1 pkt #1 ack #1 ack #1 ack #1

Slot x + 3
pkt #2 pkt #2 pkt #2 ack #2 ack #2 ack #2
Node # 1 w1 = 0.45 s1 = 0.25 s 1 = 0.20

Node # 7 w7 = 0.30 s7 = 0.30 s 7 = 0.20

MAC Payload

Header Fields

ACK Fields

MAC Header

MAC Header

Node # 3 w3 = 0.50 s3 = 0.25 s 3 = 0.20 Node # 4 w4 = 0.40 s4 = 0.25 s 4 = 0.20 Bidirectional Link New Bidirectional Link

claim offer

Node # 5 w5 = 0.75 s5 = 0.45 s 5 = 0.55

Fig. 1. Example transmissions in ATLAS of two packets in a network of three fully connected nodes. The first packet is sent from node A to node C . The second packet is sent from node C to node B . Transmissions are coloured white and receptions are shaded grey. The frame structure is shown for a data packet and an acknowledgement.

Node # 2 w2 = 0.55 s2 = 0.25 s 2 = 0.20

Node # 6 w6 = 0.05 s6 = 0.05 s 6 = 0.05

transmitter i and transmitter i is active. Dj contains the active transmitters for which receiver j is within transmission range. Receiver j is adjacent to transmitter i if j  Ri and i  Dj . The sets Dj and Ri capture the network topology for active transmitters. For load, wi is set to the percentage of slots required to support the demand at transmitter i. Transmitters with no demand (i.e., wi = 0) receive an allocation of zero slots: they are not active. Receiver capacities are set to one, targeting 100% channel allocation. The lexicographic max-min solution s = (s1 , . . . , sM ) for a given topology and traffic load is the TLA allocation. To apply REACT to channel allocation, we integrate it into ATLAS, a simple random scheduled MAC protocol. Although REACT could instead augment contention-based schemes, we choose to work within a scheduled environment, a traditionally difficult setting for adaptation. In ATLAS, each node runs a REACT bidder (Alg. 1) and a REACT auctioneer (Alg. 2) continuously. Auctioneers and bidders discover each other as they hear from one another and rely on the host node to detect lost adjacencies. The network topology is implicit in the sets Ri and Dj . Each node updates its bidder's demand magnitude to accurately reflect its traffic load. Offers and claims are encoded using eight bits each and are embedded within the MAC header of all transmissions to be piggybacked on existing network traffic. The encoding supports a total of 256 values for offers, claims, and persistences uniformly distributed between 0 and 1; the error in the representation does not exceed 0.004. Adding fields for an offer and claim to data packets and acknowledgements results in a communication overhead of four bytes per packet. For the slot size and data rate simulated in Section VI, the overhead is 0.36%. A node's offer and claim are eventually received by all single-hop neighbours reaching the bidders and auctioneers that need to know the offer and claim. In time, the bidder claims in REACT converge on the TLA allocation s. Packets are acknowledged within the slot they are transmitted and slots are sized accordingly. Unacknowledged MAC packets are retransmitted up to ten times before they are dropped by the sender. Fig. 1 shows that collisions are possible in ATLAS, and that successful transmissions are acknowledged in the same slot. The transmissions collide in slot x; they are repeated (successfully) in slots x + 2 and x + 3. Fig. 1 also shows the frame structure. The TLA allocation can be interpreted directly as a set of

Fig. 2. Example network showing the TLA allocation computed by REACT before and after an added link in the topology. wi identifies a node's demand, si its initial TLA allocation, and s i its TLA allocation after the added link. Resource capacities are set to one. Double-lined circles identify nodes with saturated resources.

persistences in a p-persistent MAC [28]. However, we achieve lower variation in delay by introducing the notion of a frame [6]. Specifically, ATLAS divides time into slots which are organized into frames of v slots. Node i operates at persistence pi = si . At the start of every frame and upon any change to pi , node i computes ki = pi v + 1 with probability i and ki = pi v with probability 1 - i where i = pi v - pi v . Node i constructs a transmission schedule of ki slots selected uniformly at random. Over many frames, E [ki ]/v equals pi where E [ki ] is the expectation for ki . Fig. 2 shows the TLA allocation in a small example network before and after a change in topology. Node 7 starts out disconnected from the other nodes and moves within range of node 3. In REACT, node 3 starts out offering 0.25 which is claimed by the bidders of nodes 1, 2, 3, and 4. With the claims of node 3 and 4 limited by the offer of node 3 and the claim of node 6 limited by its demand, the auctioneer at node 4 is free to offer 0.45, which is claimed by node 5. Upon detecting node 7 as a neighbour, the auctioneer at node 3 decreases its offer to 0.20. The bidders at nodes 1, 2, 3, 4, and 7 respond by reducing their claims accordingly. The smaller claims of the bidders at nodes 3 and 4 allow the auctioneer at node 4 to increase its offer to 0.55. The bidder at node 5 responds by increasing its claim to 0.55. It can be verified that, before and after the topology change, the claims of the bidders (i.e., the values of si and s i ) are lexicographically max-min; that is, every claim is satisfied or is maximal at an adjacent auction. Consider the topology with node 3 and node 7 connected. The bidder at node 6 is satisfied. The bidders at nodes 1, 2, 3, 4, and 7 are maximal at the auction of node 3. The bidder at node 5 is maximal at the auction of node 4. There are many implementation choices to be made in applying REACT to channel allocation. We identify three binary choices--lazy or eager persistences, physical layer or MAC layer receivers, and weighted or non-weighted bidders--and three configurable parameters--pmin , pdefault , and tlostNbr . The choices are described here; they are evaluated in Section VI.

5

A. Lazy or Eager Persistences A lazy approach sets persistence pi equal to the claim of bidder i. Once converged, pi matches the TLA allocation interpreted as a persistence. There is a potential disadvantage with being lazy. For many applications, nodes cannot predict future demand for the channel; they can only estimate demand based on past events, i.e., packet arrival rate or queue depth. As a consequence, wi lags the true magnitude of the demand at node i. If wi is the limiting constraint for the claim of bidder i, pi can be sluggish in response to increases in demand. Alternatively, an eager approach sets persistence pi = min (offers[j ] : j  Ri ), breaking the direct dependence on wi . Under stable conditions, a node's channel occupancy, the fraction of time it spends transmitting, matches its TLA allocation; its occupancy is limited by the availability of packets to transmit which is no larger than wi , even when pi > wi . By allowing pi > wi , the persistence is made more responsive to sudden increases in demand. B. Physical Layer or MAC Layer Receivers A central objective of the TLA allocation is to ensure that no receiver is overrun. In a wireless network, receivers can be defined in terms of physical layer or MAC layer communication. At the physical layer, every node is a receiver. At the MAC layer, packets are filtered by destination address; a node is only a receiver if one of its neighbours has MAC packets destined to it. MAC layer receivers can increase channel allocation by over-allocating at non-receiving nodes. However, the overallocation can slow detection of new receivers. Physical receivers prevent overallocation at any receiver, making the allocation more responsive to changes in traffic where nodes become receivers. C. Weighted or Non-Weighted Bidders We have described a MAC protocol where transmitters are represented by equally weighted bidders. For applications requiring multiple demands per transmitter, i.e., nodes servicing more than one traffic flow, we propose the weighted TLA allocation. The demands of weighted bidders are comprised of one or more demand fragments; the number of fragments accumulated into a demand is the demand's weight. Let i be the weight for demand i. Demand fragments in demand i have magnitude wi /i . The weighted TLA allocation defines the lexicographically max-min vector u = (u1 , . . . , uN ) where ui is the allocation to each demand fragment in demand i for a total allocation of ui i to demand i. REACT can be extended to compute the weighted TLA allocation. To do this, each bidder must inform adjacent auctions of its weight. Sixteen unique weights (with a four-bit representation) may be sufficient for many applications. D. Minimum Persistence pmin A node can maintain a persistence of zero without impacting the communication requirements of its bidder. For auctioneers, a persistence of zero is problematic. If a receiver becomes

overwhelmed by neighbouring transmitters, a non-zero persistence is needed to quiet the neighbours. To accomplish this, the node enforces a minimum persistence pmin , creating dummy packets if necessary, whenever the sum of claims from adjacent bidders exceeds the auction capacity. E. Overriding the TLA Allocation with pdefault There are two conditions where a node constrains its persistence to be no larger than pdefault . The first is when it has no neighbours. While the TLA allocation permits an isolated node to consume 100% of the channel, it cannot discover new neighbours if it does so. The second time a node employs pdefault is for a short period after the discovery of a new neighbour. It is possible for several nodes operating with large persistences to join a neighbourhood at about the same time. If the persistences are large enough, neighbour discovery can be hindered. For both scenarios, limiting the persistence to pdefault facilitates efficient neighbour discovery. F. Adaptation to Topology Changes and tlostNbr Changes in network topology are detected externally to REACT. In ATLAS, neighbour discovery is performed independently by each node. If a node hears from a new neighbour, then the node notifies its bidder of the new auction and its auctioneer of the new bidder. Conversely, if a node has not heard from a neighbour in more than tlostNbr seconds, it presumes the node is no longer a neighbour and informs its auctioneer and bidder accordingly. IV. R ELATED W ORK This paper focuses on the TLA allocation, its continuous distributed computation, and its application to setting transmitter persistences. In this section, we review a representative set of scheduled MAC protocols, observing how each selects a node's persistence and adapts to topology and load. Any finite schedule used in a cyclically repeated way can be generalized as a (k, v )-schedule with k transmission slots per frame of v slots, producing an effective persistence of p = k/v . Examples include the random schedules of [6], [18] where each node selects its k transmission slots randomly from the set of v slots in the frame. Topology transparent schemes [3], [15], [27] also implement (k, v )-schedules. These schedules rely on only two design parameters: N , the number of nodes in the network, and Dmax , the maximum supported neighbourhood size. These schedules guarantee each node a collision-free transmission opportunity from each of its neighbours at least once per frame, provided the node's neighbourhood size does not exceed Dmax . (k, v )-schedules do not adapt to variations in neighbourhood size or traffic load. The combinatorial requirements for variable-weight topology transparent schedules (variable k ) are explored in [19], but no construction nor protocol using them is given. A class of topology-dependent scheduled protocols compute distance-2 vertex colourings of the network graph to achieve TDMA schedules with spatial reuse. The colourings assign one transmission slot to each node and do not adapt to

6

TABLE I ATLAS CONFIGURATIONS SELECTED FOR SIMULATION . Configuration Name Nominal Lazy Persistences Physical Receivers Weighted Bidders Eager (0) or Lazy (1) 0 1 0 0 MAC (0) or Physical (1) 0 0 1 0 Unweighted (0) or Weighted (1) 0 0 0 1

A. Scenario Details Unless otherwise noted, all four configurations run with pdefault = 0.05, tlostNbr = 0.5s, and pmin = 0.01. The selection of pdefault and tlostNbr are justified by results in Figs. 5, 11a, and 11b. The selection of pmin is based on [20]. Frames contain v = 100 slots of length 800µs (1100 bytes per slot). Simulations are run using the ns-2 simulator [21]. Each wireless node is equipped with a single half-duplex transceiver and omni-directional antenna whose physical properties match those of the 914 MHz Lucent WaveLAN DSS radio. The data rate for all simulations is 11 Mbps. The transmission and carrier sense ranges are 250m. Each simulation runs a network scenario composed of a randomly generated topology and a randomly generated traffic load. Unless specified otherwise, topologies contain 50 randomly placed nodes constrained to a 300 × 1500m2 area. With the exception of the multi-hop TCP flows in Section VI-F, each traffic load consists of single-hop constant rate traffic. Four traffic loads are simulated: 20% and 80% of nodes loaded with small demands (75 ± 50 pkts/s), 20% and 80% of nodes loaded with large demands (500 ± 50 pkts/s). Nodes loaded with traffic are selected at random and the demand magnitudes are selected uniformly at random from the specified range. The packet destination is selected dynamically from the set of neighbouring nodes as the packet is passed down to the MAC layer. For the Weighted Bidders configuration, each demand is assigned a random integer weight between one and five. Traffic is generated by constant bit rate generators and transported over UDP; packets are 900 bytes in length, leaving room in each slot for header bytes and a MAC layer acknowledgement. Combined with the random placement of nodes and the addition of mobility, these four traffic loads enable simulation of a wide variety of network conditions. B. Relative Error A metric of interest is the average relative error for a node's persistence with respect to the TLA allocation. Error is reported in two parts: relative excess and deficit persistence error. Errors are measured per node over 80ms consecutive intervals in time (equal to the length of one MAC frame). We compute the average relative excess error and average relative deficit error for a given sample set of persistence measurements. The relative errors are ratios, requiring use of the geometric rather than arithmetic mean. But, the errors are often zero, preventing direct use of their mean. Instead, we convert errors into accuracies eliminating zeros from the data set for a more meaningful geometric average. The average relative accuracies are converted back to relative errors. VI. E VALUATION OF ATLAS Results from [20] show the TLA allocation applied in a static network to maintain expected delay and throughput compared to IEEE 802.11, while reducing the variance for both metrics. The TLA allocation nearly eliminates packets dropped by the MAC layer. In this section, we build on these results, focusing on the efficient distributed computation of the

traffic load. One of the first distributed protocols to bound the number of colours is proposed in [5]. Distributed-RAND (DRAND) [25] is a distributed implementation of RAND (a centralized algorithm for distance-2 colouring [23]). DRAND runs a series of loosely synchronized rounds. A colour is assigned in each round to one or more nodes in different two-hop neighbourhoods. DRAND is employed by ZebraMAC (Z-MAC) [24] to compute schedules over which to run CSMA/CA. Nodes are given priority access to their own slot, but also allowed to contend for access in other unused slots, as is done in [4]. Due to the complexity of DRAND, schedules are only computed once during network initialization. Other topology-dependent schemes support variable persistences. The periodic slot chains proposed in [14] are not limited to the structure of a fixed length frame and can support variable and arbitrarily precise persistences. A slot chain is defined by its starting transmission slot and period between its consecutive transmission slots. By combining multiple slot chains with different periods, schedules are constructed targeting any rational persistence in the range [0, 1]. The computation of slot chains provided in [14] is centralized; a distributed mechanism to adaptively compute the slot chains remains an open problem. In [30], a five phase reservation protocol (FPRP) computes conflict-free schedules where a node can reserve one or more transmission slots in the frame to achieve variable persistences. Reservation frames are run periodically rather than on a demand basis and, therefore, may not accommodate the current topology and traffic load. In SEEDEX [26], nodes do not attempt to derive conflictfree schedules. They learn the identities of their two-hop neighbours and adjust transmission probabilities (i.e., persistences) to improve the likelihood of collision-free transmissions. The transmission probabilities accommodate the number and identity of neighbours, but not traffic load. In our earlier work [20], a distributed algorithm for computing the TLA allocation is provided; however, the algorithm assumes a fixed topology and does not adapt to changes in the network. REACT solves these limitations by asynchronously adapting to changes in both topology and traffic demand. V. S IMULATION S ET- UP We now describe the simulations used to produce the experimental results presented in Section VI. Table I lists the four ATLAS configurations simulated. The Nominal configuration employs eager persistences, defines receivers in terms of MAC layer communication, and operates with unweighted bidders. The other three configurations differ from the Nominal case by a single choice and are named accordingly.

7

Fig. 3.

Convergence time following network initialization.

simulations of 1000 network scenarios, 250 of each traffic load. The scenarios are simulated eight times each, once per default persistence: 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, and 0.4. Small default persistences (pdefault  0.01) limit a node's ability to communicate during neighbour discovery, slowing convergence. Large default persistences (pdefault  0.3) permit nodes to transmit with large persistences before they discover their neighbours. In networks with 40 large demands, the large persistences can overwhelm the channel preventing neighbour discovery and delaying convergence. ATLAS is robust to the selection of pdefault with a suitable range of [0.05­0.2]. For the remaining simulations, pdefault is 0.05. B. Convergence after a Change in Demand

TLA allocation in the face of changes in topology and load. The results presented here work to answer four questions: 1) Can ATLAS converge quickly on the TLA allocation? 2) Can ATLAS scale to larger networks? 3) Can ATLAS keep up with changes in a mobile network? 4) Can ATLAS adapt to multi-hop traffic flows? The first question is addressed Sections VI-A, VI-B, and VI-C. The second is addressed in Section VI-D. The third and fourth questions are addressed in Sections VI-E and VI-F, respectively. Continuing the focus on adaptation, Section VI-G provides comparisons with several scheduled protocols. A. Convergence after Network Initialization Fig. 3 reports average convergence times for all four ATLAS configurations. Error bars denote the arithmetic standard deviation from the mean for each sample set. Convergence is measured from network initialization (time = 0) to the time ATLAS converges on the TLA allocation. Times are collected from simulations of 1000 network scenarios simulated four times each, once per configuration. There are 250 scenarios for each traffic load. The Physical Receivers configuration converges fastest in less than 0.4s on average for networks with 40 large demands and faster for other traffic loads. The extra step of detecting MAC receivers slows convergence. The Lazy Persistences configuration is the slowest with an average convergence time of 0.67s for networks with 40 large demands. The strict limit on persistences enforced by this configuration slows convergence compared to the others. Fig. 4a shows average excess and deficit relative persistence errors for all four configurations. The averages are computed for nodes with a non-zero TLA allocation and only during convergence. Nodes are observed to operate within approximately 20% of their TLA allocation regardless of configuration. Deficit errors are larger than excess errors reflecting a tendency to converge from below, rather than above, the TLA allocation. In Fig. 4b, each data point reflects the convergence time (xcoordinate) and total relative persistence error (y -coordinate) for one simulation of the nominal configuration. The data shows relative persistence error to be fairly consistent from network to network with a maximum observed error of 27%. Fig. 5 reports convergence time for the Nominal configuration while varying pdefault . Convergence is measured for

Fig. 6 reports convergence times and relative persistence errors for the Nominal configuration following a change to a single demand magnitude. Four types of demand are simulated: a new small demand, a new large demand, a removed small demand, and a removed large demand. New demands start with magnitude zero and change to 75 ± 50 pkts/s for small demands and to 500 ± 50 pkts/s for large demands. Removed small demands and removed large demands start at 75 ± 50 pkts/s and at 500 ± 50 pkts/s, respectively; both change to zero. The four demand change types are simulated under the four traffic loads. REACT is allowed to converge on the initial TLA allocation prior to the demand change. Convergence times and error measurements are taken from simulations of 4000 network scenarios, 250 for each of the 16 demand change and traffic load combinations. Fig. 6a reports convergence times measured from the time of the change to the time of convergence on the new TLA allocation. The largest convergence times of approximately 0.175s are found in networks loaded with 40 demands. The average convergence time for the other scenarios is 0.125s or smaller. Fig. 6b shows relative persistence errors measured during convergence at nodes whose TLA allocation are affected by the demand change. Persistences are observed to be within 10% of the TLA allocation. C. Convergence after a Change in Topology Fig. 7 reports convergence time and relative persistence error following two types of topology change: the creation of a link and the removal of a link between a pair of nodes. Simulations are run on 2000 network scenarios, 250 for each topology change type and traffic load combination. Networks that lose a link are simulated once per neighbour timeout tlostNbr of 0.5s, 2.0s and 5.0s. Network topologies are generated as follows. A first node is placed at a random location in the simulation area. For topologies gaining a link, a second node is placed just outside the transmission range of the first node with a trajectory toward the first node. For topologies losing a link, the second node is placed just inside the transmission range of the first node with a trajectory away from the first node. The remaining 48 nodes are placed at random locations in the simulation area. The distance travelled by the second node is constrained to avoid unintentional topology changes.

8

(a) Relative excess and deficit persistence errors. Fig. 4. Relative persistence error for ATLAS.

(b) Convergence time vs. error for the Nominal configuration.

Fig. 5.

Convergence times when run with varying default persistences.

convergence is reached in an average of 0.89s, a mere 40% increase compared to networks spanning 4.8 hops. The impressive convergence times, particularly those of networks spanning 12 or more hops, suggest that convergence happens locally, allowing distant neighbourhoods to converge in parallel. This local behaviour is captured in Fig. 9 which reports the average distance between a network change and a node whose bidder changes its claim in response. Distances are reported in hops. A node that changes its demand or gains/loses a neighbour has distance zero. Neighbours of this node have distance one, and so on. Range of impact is reported for the six types of change evaluated in Sections VI-B and VI-C. Each type of change is simulated in 1000 network scenarios, 250 of each traffic load. The range of impact is less than 1.75 hops on average. E. Performance with Node Mobility Section VI-C addresses the robustness of ATLAS to single topology changes. We now evaluate its performance in networks with continuous mobility which may not have the opportunity to converge on the TLA allocation. Fig. 10a reports persistence error for node speeds ranging from 0 m/s to 120 m/s with 200 scenarios simulated for each node speed, 50 of each traffic load. Node movements are generated using the steady-state mobility model generator of [13] with a pause time of zero. Simulations are run for 20s. As node speeds increase, so do deficit persistence errors. The larger deficit errors are an artifact of lost neighbour detection which is delayed by tlostNbr = 0.5s. As a result, nodes tend to think their neighbourhoods are more crowded than they are, a tendency that gets worse as node speeds increase. In terms of REACT, auctioneers and bidders unnecessarily constrain their offers and claims to accommodate lost neighbours. The deficit persistences translate to degraded throughput. Fig. 10b reports MAC throughput for the simulations of Fig. 10a. Even with node speeds of 120 m/s where a node travels its transmission range in 2.1s, throughput degrades modestly, decreasing by less than 20% compared to static networks. Fig. 11 shows that a large tlostNbr exacerbates deficit persistence error and further degrades throughput. Data is collected from 200 scenarios, 50 of each traffic load. Each scenario is simulated five times with neighbour timeouts ranging from

The expected convergence time following the addition of a new link is 0.025s. For tlostNbr =0.5s, convergence is reached in less than 0.13s on average. For tlostNbr =2.0s and tlostNbr =5.0s, the large convergence times are dominated by tlostNbr . Except for the simulations of Fig. 11, all others configure ATLAS with tlostNbr =0.5s. During convergence, nodes affected by the topology change are observed to operate within 4% of their TLA allocation on average. These numbers are striking. The small convergence times stem from a counterintuitive feature of the TLA allocation: the majority of topology changes do not affect the TLA allocation. A new link only has an effect if the link connects a bidder with an auction that lacks the capacity to support the bidder's claim. Even in heavily loaded networks, many auctions have spare capacity to support a new bidder. For these scenarios, convergence is instantaneous. D. Scalability to Large Networks We now turn to results demonstrating ATLAS's scalability. We simulate 10 network sizes with the x-dimension ranging from 600m (2.4 hops) to 6000m (24 hops) in 600m increments; the y -dimension is held constant at 300m. The number of nodes is selected to keep the average neighbourhood density constant across all network sizes. Fig. 8 reports convergence times for 4000 network scenarios, 100 of each traffic load and network size combination. The convergence of ATLAS in large networks is striking. In networks spanning 24 hops,

9

(a) Convergence time after a demand change. Fig. 6.

(b) Relative persistence error after a demand change.

Convergence time and relative persistence error during convergence following a single demand change.

(a) Convergence time after a topology change. Fig. 7.

(b) Relative persistence error after a topology change.

Convergence time and relative persistence error following a single topology change.

Fig. 8.

Convergence times as the width of the network grows.

0.1s to 15.0s. Node speeds are fixed at 30 m/s. Degraded performance is observed for large timeouts, tlostNbr  0.5s, but also for small timeouts, tlostNbr = 0.1s. In networks loaded with 10 large demands, tlostNbr = 0.1s causes nodes to falsely identify lost neighbours that must be rediscovered. The remaining simulations are run with tlostNbr = 0.5s. Fig. 12 reports packet delay for ATLAS and IEEE 802.11 for the 200 network scenarios of Fig. 10 with node speeds equal to 30 m/s. IEEE 802.11 is configured with a maximum packet retry count of seven for RTS, CTS, and ACKs and four for data packets [11], a mini-slot length of 20µs, and minimum and maximum contention window sizes of 32 and 1024 slots, respectively. Each point in the scatter plot reports the average packet delay (x-coordinate) and variation in packet delay (y coordinate) for a single node. The largest reported average delay is 0.047s for ATLAS and 0.058s for IEEE 802.11. The largest reported variation in delay for ATLAS is 0.0016s2 , just 3.6% of the 0.0444s2 reported for IEEE 802.11. This impressive reduction in delay variance is crucial to the support of TCP, which we evaluate next. F. Multi-hop TCP Flows To this point, we have used MAC layer traffic to simulate a diverse set of network scenarios. We now evaluate the performance of ATLAS using multi-hop TCP flows. To accommodate the dynamic nature of these flows, each node estimates its own demand by monitoring queue behaviour. Demand is estimated as the sum of two parts: wenqueue and wlevel . wenqueue

Fig. 9.

Average range of impact (in hops) for a demand or topology change.

10

(a) Relative persistence error. Fig. 10. Relative persistence error and total MAC throughput for varying levels of node mobility.

(b) Total MAC throughput.

(a) Relative persistence error. Fig. 11. Relative persistence error and total MAC throughput for varying neighbour timeouts.

(b) Total MAC throughput.

Fig. 12.

Delays for ATLAS and IEEE 802.11 with node speeds of 30 m/s.

is the percentage of channel required to keep up with the current enqueue packet rate, wenqueue = (packet enqueue rate) × (slot length). wlevel is the percentage of channel required to transmit all packets in the queue within 0.2s (i.e., 25 slots), wlevel = [(# packets in queue)/0.02s] × (slot length). To avoid cross-layer interactions between the MAC and routing protocols, Dijkstra's shortest path algorithm [28] using accurate knowledge of the global topology computes the next hop address for all packet transmissions. FTP agents emulate transfer of infinite size files to create flows with throughput limited only by the performance of the network. Transfers start at time zero and run for 20s. Nodes are statically placed at random locations in a 300 × 1500m2 simulation area. The source and destination nodes for each file transfer are selected

at random. Each FTP transfer is transported over TCP Reno configured for selective acknowledgements, the extensions of RFC 1323 [1], and 900 byte TCP segments. The return ACKs are not combined with each other or with other data packets. Consequently, the transmission of a single 40-byte TCP ACK consumes an entire transmission slot in ATLAS. The maximum congestion window size is 32 packets. Network scenarios are simulated for three traffic loads: networks with 2, 8, and 25 TCP flows. The number of replicates per traffic load are chosen so that 3000 TCP flows are simulated for each. Fifteen hundred scenarios are simulated with two TCP flows, 375 with eight TCP flows, and 120 with 25 TCP flows. We simulate TCP traffic on five MAC protocols: the four configurations of ATLAS and IEEE 802.11. The configurations of ATLAS use pdefault = 0.05, tlostNbr = 0.5s, and pmin = 0.01. IEEE 802.11 parameters match those described in Section VI-E. Each node dynamically sets its bidder weight to one or the number of outgoing TCP flows it services, whichever is larger. The 15 sub-plots in Fig. 13 show the percentage of flows (y -axis) achieving a minimum throughput (x-axis). The distinguishing characteristics of the three unweighted ATLAS configurations are seen in the throughput curves for networks with two flows. These networks are loaded lightly enough for the auctions at non-receiver nodes to make a difference in the allocation, improving throughput for 2- and 4-hop flows. These networks also demonstrate how the longer initial packet delays of the Lazy Persistences configuration increase round trip time

11

Networks with 2 Flows

Networks with 8 Flows

Networks with 25 Flows

All flows

y -axis shows % of TCP flows achieving minimum required throughput.

1-hop flows 2-hop flows 3-hop flows 4- and 5-hop flows

x-axis shows minimum required throughput for TCP flows in packets/second.
Fig. 13. Percent of TCP flows (y -axis) achieving a minimum throughput (x-axis). Plots in the left, center, and right columns report on flows from simulations of 2, 8, and 25 flows, respectively. The plots in the top row report on all flows, regardless of hop count. Plots in the second, third, and fourth rows report on 1-hop, 2-hop, and 3-hop flows, respectively. Plots in the fifth row report on 4- and 5-hop flows.

for 4- and 5-hop flows, preventing TCP from achieving its best throughput. The Weighted Bidders configuration performs well for multi-hop flows in networks with eight and 25 flows by allocating more to multi-hop flows at the expense of singlehop flows. Because one-hop flows tend to achieve higher throughput, the configuration maintains a tighter variation in flow throughputs as indicated by the steeper slope of the Weighted Bidders curve in the top right plot of Fig. 13. Regardless of configuration, ATLAS surpasses IEEE 802.11

in support of concurrent multi-hop flows. The interaction between the IEEE 802.11 back-off algorithm and TCP's congestion control is well known [9]. In testbed experiments, a single TCP flow with no competition has difficulty reaching a destination four hops away [16]. Our simulations corroborate these findings, as approximately 50% of the 4- and 5-hop flows report a throughput of zero. For networks with 25 demands, nearly 75% of 2-hop flows are non-functional; 3-, 4-, and 5hop flows are almost completely shut out. The throughput of

12

ATLAS is achieved in spite of channel wasted transmitting 40 byte TCP ACKs in their own slots.

VII. D ISCUSSION In this section we discuss open issues and suggest potential applications for REACT and ATLAS. A. Improved Reliable Transport TCP's congestion control algorithm is known to suffer cross-layer interactions with binary exponential back-off (BEB) employed by IEEE 802.11 [9]. BEB is short term unfair, allowing a single node to capture the channel at the expense of its neighbours [2], [10] causing high variation in packet delay and making it difficult for TCP to estimate roundtrip delay. Many modifications have been proposed to improve TCP performance over wireless networks [17]; common approaches are detection of packet loss (differentiating it from congestion) and improved estimation of round trip time. An alternative is to minimize packet loss and control variation in packet delay at the MAC layer. ATLAS demonstrates a remarkable control of variation in delay (Fig. 12) enabling TCP to reliably support 3-, 4-, and 5-hop flows over heavily loaded networks (Fig. 13). However, TCP throughput still degrades considerably as the number of hops grows. Potential areas for future work include the integration of ATLAS into a cross-layer solution for reliable transport over wireless networks and the use of REACT to inform TCP's congestion window size. B. Selection of Configurable Parameters ATLAS has three configurable parameters: pdefault , tlostNbr , and pmin . Based on our simulations, [0.01­0.2] is an acceptable range for pdefault (Fig. 5) and [0.1s­2s] is an acceptable range for tlostNbr (Figs. 11a, 11b). In [20], pmin =0.1s is found to be acceptable for a protocol that enforces pmin at all nodes and at all times. Because ATLAS employs pmin temporarily, and only when needed, it is less sensitive to the selection of pmin . Although results show ATLAS to be robust to parameter selection, tuning may be required in other scenarios or in a hardware implementation. C. Dynamic Selection of Auction Capacity ATLAS targets 100% channel allocation by setting auction capacities in REACT to one. Although simulation results show this to be an adequate choice, it is not clear whether performance can be improved by under- or over-allocating the channel. Indeed, optimal auction capacities (however optimal is defined) are dependent on network topology and quality of the communication channel. We leave a thorough analysis of auction capacity selection to future work, pointing out here that REACT adapts continuously, allowing auction capacities to be adjusted dynamically, if necessary. D. Potential Applications for REACT The weighted TLA allocation opens doors for several potential uses. In the simulations of Section VI-F, a bidder's weight is set according to the number of flows it services. It may be desirable to set weights according to queue levels,

G. Comparison with other Scheduled MAC Protocols Here, we compare the adaptation of ATLAS with several other scheduled protocols including DRAND, Z-MAC, FPRP, and SEEDEX. Although the first three compute conflict-free schedules, an NP-hard problem [7], a comparison highlights the agility of ATLAS. 1) Adaptation to Topology Changes: For the simulations of Section VI-E, the number of neighbour changes (i.e., gained or lost neighbours) per second experienced by a node is correlated to the node speed. When the nodes move at 30 m/s, each node is expected to gain, or lose, a neighbour 2.21 times per second; within 6.3s, the number of neighbour changes is expected to exceed the neighbourhood size. Based on the run times reported in [25, Fig. 10], we estimate DRAND to compute schedules for the networks in Section VI-E in approximately 4.9s (adjusting for data rate and a two-hop neighbourhood size of 27). In this time, the topology changes caused by nodes moving at 30 m/s are expected to invalidate the computed schedule. Z-MAC has the same limitation and, although it compensates by running CSMA/CA to resolve collisions, it does not benefit from its TDMA schedule when nodes are mobile. In [25], the run times reported for FPRP schedule generation are comparable to DRAND. For SEEDEX, nodes discover their two-hop neighbours using a fan-in/fan-out procedure described in [26]. However, a practical integration of the procedure into the MAC protocol is not described or evaluated, preventing a comparison of its agility with other MAC protocols. In contrast to the slow schedule computation times of DRAND, Z-MAC, and FPRP, ATLAS is shown to handle node speeds of up to 120 m/s with only moderate degradation to MAC throughput. 2) Adaptation to Changes in Traffic Load: The persistences achieved by DRAND and SEEDEX are dependent on topology alone; neither adapts to traffic load. Although Z-MAC adapts to load, it does so by deviating from its underlying schedule, which does not adapt. FPRP can adapt to load by scheduling a variable number of slots per node; this is done at the expense of both longer frame lengths and longer run times for schedule computation. In contrast, ATLAS adapts to traffic load, responding quickly enough to establish and maintain multi-hop TCP flows. 3) Continuous Adaptation: Common to the scheduled schemes mentioned here is the use of a distinct phase for schedule computation (or neighbour discovery for SEEDEX). The schedules must be updated in order for the MAC to adapt. Any fixed period between schedule updates must be selected a priori; it cannot be adjusted for variations in network mobility. If schedules are to be updated when needed, a mechanism is required to trigger the schedule update. This coordination, by itself, is a challenge in an ad hoc network. In contrast, ATLAS does not employ a schedule computation (or a neighbour discovery) phase and adapts continuously to changes in both topology and traffic load.

13

demand magnitudes, neighbourhood sizes, node betweenness [8], distance from a point of interest (i.e., an access point or a common sink), position in a multicast/broadcast tree, or path hop count. The key observation is that ATLAS maintains flexibility by allowing nodes to define bidder weights arbitrarily to suit the needs of the network. While computation of persistences is the primary motivation for this work, REACT is not limited to this purpose. Consider the Physical Receivers configuration with node demands set to one. The resulting allocation is independent of actions taken by the upper network layers and, therefore, can inform decisions made by those layers. It can serve as a measure of potential network congestion--small allocations are assigned in dense neighbourhoods containing many potentially active neighbours. The routing protocol can use the allocation to discover alternate routes around congestion. An intriguing application is the implementation of differentiated service at the MAC layer. IEEE 802.11e [12] enhances the distributed coordination function by implementing four access categories; an instance of the back-off algorithm is run per access category, each with its own queue. The probability of transmission of each access category is manipulated independently through selection of contention window size and inter-frame space. This permits higher priority traffic to capture the channel from lower priority traffic. Similar results can be achieved by four instances of REACT, each computing the allocation for a single access category. Prioritization is achieved through dynamic coordination of the four auction capacities at each node. A potential strategy sets the capacity for each access category equal to one minus the allocation to higher priority access categories. As a result, higher priority auctions are permitted to starve lower priority auctions of capacity, effectively distributing channel access to high priority traffic. Alternatively, auction capacities can be selected to ensure a minimum or maximum percentage of the channel is offered to an access category. A network can run multiple instances of REACT. For example, an instance of the Physical Receivers configuration with all demands set to one can be run concurrently with four instances configured to support differentiated service. Alternatively, multiple instances of REACT can be used to allocate more than one set of resources concurrently. E. Assumptions Made by ATLAS Two key assumptions are made by ATLAS in its computation of the TLA allocation using REACT: (1) The offers and claims received by a node are accurate. (2) The offers and claims of a node are eventually received by all neighbouring nodes. The first assumption is reasonable, provided received packets are checked for errors by the link layer. The second assumption is almost certainly invalid; asymmetric communication, interference beyond the range of transmission, and signal fading are common in wireless communication and can prevent the delivery of offers and claims. Under realistic conditions, REACT may not converge on the TLA allocation, risking overallocation of the channel. In practice, auctions can adjust their capacities to mitigate the over-allocation. Every node knows

the persistences of its neighbours (from bidder claims) and can compute the expectation for collisions on the channel. Significant deviations above this expectation can trigger the auction to lower its capacity. An evaluation in a testbed of real radios is necessary to understand the sensitivity to anomalies on the wireless channel and the effectiveness of adjusting auction capacities to accommodate channel conditions. The evaluation of ATLAS in Section VI assumes both slot and frame synchronization; ATLAS does not require either. The computation of the TLA allocation by REACT does not rely on a frame structure and the expected performance of the random schedules is not affected by loss of frame synchronization. Even without slot synchronization, REACT can compute the TLA allocation; however, loss of slot synchronization may reduce channel capacity by 50% (see Aloha vs. slotted Aloha in [28]). ATLAS can accommodate the lower channel capacity by reducing auction capacity. This technique may allow ATLAS to be run on commodity IEEE 802.11 hardware [29] that lacks native support for slot synchronization. This is a subject of our current research. F. Enhancing Existing MAC Protocols We have used REACT to compute persistences to be employed within ATLAS, a slotted MAC protocol. Alternatively, REACT can be run on top of the IEEE 802.11 MAC by embedding claims and offers in the headers of existing control and data messages. The TLA allocation can be used to inform the selection of contention window sizes, eliminating the need for (and negative side effects of) binary exponential back off. We are currently working to integrate REACT into IEEE 802.11. Another alternative (and more ambitious) approach is to implement TLA persistences in a topology-dependent MAC that computes conflict-free schedules. Only a few topologydependent schemes allow a node to reserve more than one slot in a frame (i.e., [14], [30]), and those do not define how many slots a node should reserve. The TLA allocation can establish a permissible number of slots to be reserved by each node, given the current topology and traffic load. VIII. C ONCLUSION We have proposed REACT, a distributed auction that converges continuously on the TLA allocation, adapting to changes in both topology and traffic load. The utility of REACT is demonstrated through integration into ATLAS which we simulate under a wide variety of network scenarios. The results presented suggest that REACT can effectively inform the selection of transmitter persistences, and that ATLAS can provide robust, reliable, and scalable services. The application of REACT is not restricted to the computation of transmitter persistences. It has the potential to inform routing and admission control decisions, to enable differentiation of service at the MAC layer, and even to allocate other node resources. In this context, the REACT algorithm provides a potential solution to the immediate challenge of medium access control, but also shows promise as a tool for use in network protocol design in general.

14

ACKNOWLEDGEMENT The authors appreciate the useful comments provided by the anonymous reviewers. R EFERENCES
[1] RFC 1323: TCP Extentions for High Performance, 1992. [2] V. Bharghavan, A. Demers, S. Shenker, and L. Zhang. MACAW: A medium access protocol for wireless LANs. In Proceedings of the ACM Conference on Communications Architectures, Protocols and Applications (SIGCOMM'94), pages 212­225, 1994. [3] I. Chlamtac and A. Farag´ o. Making transmission schedules immune to topology changes in multi-hop packet radio networks. IEEE/ACM Transactions on Networking, 2(1):23­29, 1994. [4] I. Chlamtac, A. Farag´ o, A. D. Myers, V. R. Syrotiuk, and G. Z´ aruba. ADAPT: A dynamically self-adjusting media access control protocol for ad hoc networks. In Proceedings of the IEEE Global Telecommunications Conference (GLOBECOM'99), pages 11­15, 1999. [5] I. Chlamtac and S. S. Pinter. Distributed nodes organization algorithm for channel access in a multihop dynamic radio network. IEEE Transactions on Computers, C-36(6):728­737, June 1987. [6] C. J. Colbourn and V. R. Syrotiuk. Scheduled persistence for medium access control in sensor networks. In Proceedings from the First IEEE International Conference on Mobile Ad hoc and Sensor Systems (MASS'04), pages 264­273, 2004. [7] S. Even, O. Goldreich, S. Moran, and P. Tong. On the NP-completeness of certain network testing problems. Networks, 14(1):1­24, 1984. [8] L. C. Freeman. A set of measures of centrality based on betweenness. Sociometry, 40(1):35­41, 1977. [9] M. Gerla, R. Bagrodia, L. Zhang, K. Tang, and L. Wang. TCP over wireless multi-hop protocols: Simulation and experiments. In Proceedings of the 1999 IEEE International Conference on Communication (ICC'99), pages 1089­1094, 1999. [10] J. Hastad, T. Leighton, and B. Rogoff. Analysis of backoff protocols for multiple access channels. In Proceedings of the 19th annual ACM Symposium on Theory of Computing (STOC'87), pages 740­744, 1987. [11] IEEE. IEEE 802.11, Wireless LAN medium access control (MAC) and physical layer (PHY) specifications, 1997. [12] IEEE. IEEE 802.11e, Enhancements: QoS, including packet bursting, 2007. [13] J. Boleng, N. Bauer, T. Camp, and W. Navidi. Random Waypoint Steady State Mobility Generator (mobgen-ss). http://toilers.mines.edu/. [14] G. Jakllari, M. Neufeld, and R. Ramanathan. A framework for frameless TDMA using slot chains. In Proceedings of the 9th IEEE International Conference on Mobile Ad hoc and Sensor Systems (MASS'12), 2012. [15] J. Ju and V. O. K. Li. An optimal topology-transparent scheduling method in multihop packet radio networks. IEEE/ACM Transactions on Networking, 6(3):298­305, 1998. [16] D. Koutsonikolas, J. Dyaberi, P. Garimella, S. Fahmy, and Y. C. Hu. On TCP throughput and window size in a multihop wireless network testbed. In Proceedings of the 2nd ACM International Workshop on Wireless network testbeds, experimental evaluation and characterization (WiNTECH'07), 2007. [17] K. Leung and V. O. K. Li. Transmission control protocol (TCP) in wireless networks: Issues, approaches, and challenges. IEEE Communications Surveys & Tutorials, 8:64­79, 2006. [18] J. Lutz, C. J. Colbourn, and V. R. Syrotiuk. Apples and oranges: Comparing schedule- and contention-based medium access control. In Proceedings of the 13th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems (MSWiM'10), pages 319­326, 2010. [19] J. Lutz, C. J. Colbourn, and V. R. Syrotiuk. Variable weight sequences for adaptive scheduled access in MANETs. In Proceedings of Sequences and their Applications (SETA'12), pages 53­64, 2012. [20] J. Lutz, C. J. Colbourn, and V. R. Syrotiuk. Topological persistence for medium access control. IEEE Transactions on Mobile Computing, 12(8):1598­1612, 2013. [21] The Network Simulator ns-2. http://www.isi.edu/nsnam/ns/. [22] M. Pi´ oro and D. Medhi. Routing, Flow, and Capacity Design in Communication and Computer Networks. Elsevier Inc., 2004. [23] R. Ramanathan. A unified framework and algorithm for (T/F/C)DMA channel assignment in wireless networks. In Proceedings of the 16th Annual Joint Conference of the IEEE Computer and Communications Societies (INFOCOM'97), pages 900­907, 1997.

[24] I. Rhee, A. Warrier, M. Aia, J. Min, and M. L. Sichitiu. Z-MAC: A hybrid MAC for wireless sensor networks. IEEE Transactions on Networking, 16(3):511­524, 2008. [25] I. Rhee, A. Warrier, J. Min, and L. Xu. DRAND: Distributed randomized TDMA scheduling for wireless ad-hoc networks. IEEE Transactions on Mobile Computing, 8(10):1384­1396, 2009. [26] R. Rozovsky and P. R. Kumar. SEEDEX: A MAC protocol for ad hoc networks. In Proceedings of the 2nd ACM International Symposium on Mobile Ad Hoc Networking and Computing (MOBIHOC'01), pages 67­75, 2001. [27] V. R. Syrotiuk, C. J. Colbourn, and S. Yellamraju. Rateless forward error correction for topology-transparent scheduling. IEEE/ACM Transactions on Networking, 16(2):464­472, 2008. [28] A. S. Tanenbaum. Computer Networks. McGraw Hill, fourth edition, 2003. [29] I. Tinnirello, G. Bianchi, P. Gallo, D. Garlisi, F. Giuliano, and F. Gringoli. Wireless MAC processors: Programming MAC protocols on commodity hardware. In Proceedings of the 31st Annual Joint Conference of the IEEE Computer and Communications Societies (INFOCOM'12), pages 1269­1277, 2012. [30] C. Zhu and M. S. Corson. A five-phase reservation protocol (FPRP) for mobile ad hoc networks. Wireless Networks, 7(4):371­384, 2001.

Jonathan Lutz earned his B.S. in Electrical Engineering from Arizona State University, Tempe, Arizona, in 2000 and his M.S. in Computer Engineering from the University of Waterloo, Waterloo, Canada, in 2003. He is currently working on his Ph.D. in Computer Science at Arizona State University. His research interests include medium access control in mobile ad hoc networks.

Charles J. Colbourn earned his Ph.D. in 1980 from the University of Toronto, and is a Professor of Computer Science and Engineering at Arizona State University. He is the author of The Combinatorics of Network Reliability (Oxford), Triple Systems (Oxford), and 320 refereed journal papers focussing on combinatorial designs and graphs with applications in networking, computing, and communications. In 2004, he was awarded the Euler Medal for Lifetime Research Achievement by the Institute for Combinatorics and its Applications.

Violet R. Syrotiuk earned her Ph.D. in Computer Science from the University of Waterloo (Canada). She is an Associate Professor of Computer Science and Engineering at Arizona State University. Her research has been supported by grants from NSF, ONR, and DSTO, and contracts with LANL, Raytheon, General Dynamics, and ATC. She serves on the editorial boards of Computer Networks and Computer Communications, as well as on the technical program and organizing committees of several major conferences sponsored by ACM and IEEE.

