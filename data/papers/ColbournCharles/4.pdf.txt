Two-stage algorithms for covering array construction

arXiv:1606.06730v1 [cs.DS] 21 Jun 2016

Kaushik Sarkar and Charles J. Colbourn
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, PO Box 878809
Tempe, Arizona, 85287-8809, U.S.A.
June 23, 2016
Abstract
Modern software systems often consist of many different components, each with a number of options.
Although unit tests may reveal faulty options for individual components, functionally correct components
may interact in unforeseen ways to cause a fault. Covering arrays are used to test for interactions
among components systematically. A two-stage framework, providing a number of concrete algorithms,
is developed for the efficient construction of covering arrays. In the first stage, a time and memory
efficient randomized algorithm covers most of the interactions. In the second stage, a more sophisticated
search covers the remainder in relatively few tests. In this way, the storage limitations of the sophisticated
search algorithms are avoided; hence the range of the number of components for which the algorithm can
be applied is extended, without increasing the number of tests. Many of the framework instantiations
can be tuned to optimize a memory-quality trade-off, so that fewer tests can be achieved using more
memory. The algorithms developed outperform the currently best known methods when the number
of components ranges from 20 to 60, the number of options for each ranges from 3 to 6, and t-way
interactions are covered for t ∈ {5, 6}. In some cases a reduction in the number of tests by more than
50% is achieved.

Keywords: Covering array, Software interaction testing, Combinatorial construction algorithm

1

Introduction

Real world software and engineered systems are composed of many different components, each with a number
of options, that are required to work together in a variety of circumstances. Components are factors, and
options for a component form the levels of its factor. Although each level for an individual factor can be tested
in isolation, faults in deployed software can arise from interactions among levels of different factors. When
an interaction involves levels of t different factors, it is a t-way interaction. Testing for faults caused by t-way
interactions for every t is generally infeasible, as a result of a combinatorial explosion. However, empirical
research on real world software systems indicates that testing all possible 2-way (or 3-way) interactions
would detect 70% (or 90%) of all faults [25]. Moreover, testing all possible 6-way interactions is sufficient
for detection of 100% of all faults in the systems examined in [25]. Testing all possible t-way interactions for
some 2 ≤ t ≤ 6 is pseudo-exhaustive testing [24], and is accomplished with a combinatorial array known as
a covering array.
Formally, let N, t, k, and v be integers with k ≥ t ≥ 2 and v ≥ 2. A covering array CA(N ; t, k, v) is an
N × k array A in which each entry is from a v-ary alphabet Σ, and for every N × t sub-array B of A and
every x ∈ Σt , there is a row of B that equals x. Then t is the strength of the covering array, k is the number
of factors, and v is the number of levels.
When k is a positive integer, [k] denotes the set {1, . . . , k}. A t-way interaction is {(ci , ai ) : 1 ≤ i ≤
t, ci ∈ [k], ci 6= cj for i 6= j, and ai ∈ Σ}. So an interaction is an assignment of levels from Σ to t of the k

factors. It,k,v denotes the set of all kt v t interactions for given t, k and v. An N × k array A covers the
1

interaction ι = {(ci , ai ) : 1 ≤ i ≤ t, ci ∈ [k], ci 6= cj for i 6= j, and ai ∈ Σ} if there is a row r in A such that
A(r, ci ) = ai for 1 ≤ i ≤ t. When there is no such row in A, ι is not covered in A. Hence a CA(N ; t, k, v)
covers all interactions in It,k,v .
Covering arrays are used extensively for interaction testing in complex engineered systems. To ensure
that all possible combinations of options of t components function together correctly, one needs examine
all possible t-way interactions. When the number of components is k, and the number of different options
available for each component is v, each row of CA(N ; t, k, v) represents a test case. The N test cases
collectively test all t-way interactions. For this reason, covering arrays have been used in combinatorial
interaction testing in varied fields like software and hardware engineering, design of composite materials,
and biological networks [8, 24, 26, 32, 34].
The cost of testing is directly related to the number of test cases. Therefore, one is interested in covering
arrays with the fewest rows. The smallest value of N for which CA(N ; t, k, v) exists is denoted by CAN(t, k, v).
Efforts to determine or bound CAN(t, k, v) have been extensive; see [12, 14, 24, 31] for example. Naturally one
would prefer to determine CAN(t, k, v) exactly. Katona [22] and Kleitman and Spencer [23] independently
showed that
 for t = v = 2, the minimum number of rows N in a CA(N ; 2, k, 2) is the smallest N for which
−1
. Exact determination of CAN(t, k, v) for other values of t and v has remained open. However,
k≤ N
N
d2e
some progress has been made in determining upper bounds for CAN(t, k, v) in the general case; for recent
results, see [33].
For practical applications such bounds are often unhelpful, because one needs explicit covering arrays to
use as test suites. Explicit constructions can be recursive, producing larger covering arrays using smaller ones
as ingredients (see [14] for a survey), or direct. Direct methods for some specific cases arise from algebraic,
geometric, or number-theoretic techniques; general direct methods are computational in nature. Indeed
when k is relatively small, the best known results arise from computational techniques [13], and these are
in turn essential for the successes of recursive methods. Unfortunately, the existing computational methods
encounter difficulties as k increases, but is still within the range needed for practical applications. Typically
such difficulties arise either as a result of storage or time limitations or by producing covering arrays that
are too big to compete with those arising from simpler recursive methods.
Cohen [11] discusses commercial software where the number of factors often exceeds 50. Aldaco et al. [1]
analyze a complex engineered system having 75 factors, using a variant of covering arrays. Android [3] uses
a Configuration class to describe the device configuration; there are 17 different configuration parameters
with 3 − 20 different levels. In each of these cases, while existing techniques are effective when the strength
is small, these moderately large values of k pose concerns for larger strengths.
In this paper, we focus on situations in which every factor has the same number of levels. These cases
have been most extensively studied, and hence provide a basis for making comparisons. In practice, however,
often different components have different number of levels, which is captured by extending the notion of a
covering array. A mixed covering array MCA(N ; t, k, (v1 , v2 , . . . , vk )) is an N × k array in which the ith
column contains vi symbols for 1 ≤ i ≤ k. When {i1 , . . . , it } ⊆ {1, . . . , k} is
Qat set of t columns, in the N × t
subarray obtained by selecting columns i1 , . . . , it of the MCA, each of the j=1 vij distinct t-tuples appears
as a row at least once. Although we examine the uniform case in which v1 = · · · = vk , the methods developed
here can all be directly applied to mixed covering arrays as well.
Inevitably, when k > max(t + 1, v + 2), a covering array must cover some interactions more than once,
for if not they are orthogonal arrays [20]. Treating the rows of a covering array in a fixed order, each row
covers some number of interactions not covered by any earlier row. For a variety of known constructions,
the initial rows cover many new interactions, while the later ones cover very few [7]. Comparing this rate
of coverage for a purely random method and for one of the sophisticated search techniques, one finds little
difference in the initial rows, but very substantial differences in the final ones. This suggests strategies to
build the covering array in stages, investing more effort as the number of remaining uncovered interactions
declines.
In this paper we propose a new algorithmic framework for covering array construction, the two-stage
framework. In the first stage, a randomized row construction method builds a specified number of rows to
cover all but at most a specified, small number of interactions. As we see later, by dint of being randomized

2

this uses very little memory. The second stage is a more sophisticated search that adds few rows to cover the
remaining uncovered interactions. We choose search algorithms whose requirements depend on the number
of interactions to be covered, to profit from the fact that few interactions remain. By mixing randomized and
deterministic methods, we hope to retain the fast execution and small storage of the randomized methods,
along with the accuracy of the deterministic search techniques.
We introduce a number of algorithms within the two-stage framework. Some improve upon best known
bounds on CAN(t, k, v) (see [33]) in principle. But our focus is on the practical consequences: The twostage algorithms are indeed quite efficient for higher strength (t ∈ {5, 6}) and moderate number of levels
(v ∈ {3, 4, 5, 6}), when the number of factors k is moderately high (approximately in the range of 20 − 80
depending on value of t and v). In fact, for many combination of t, k and v values the two-stage algorithms
beat the previously best known bounds.
Torres-Jimenez et al. [36] explore a related two-stage strategy. In their first stage, an in-parameter-order
greedy strategy (as used in ACTS [24]) adds a column to an existing array; in their second stage, simulated
annealing is applied to cover the remaining interactions. They apply their methods when t = v = 3, when
the storage and time requirements for both stages remain acceptable. In addition to the issues in handling
larger strengths, their methods provide no a priori bound on the size of the resulting array. In contrast with
their methods, ours provide a guarantee prior to execution with much more modest storage and time.
The rest of the paper is organized as follows. Section 2 reviews algorithmic methods of covering array
construction, specifically the randomized algorithm and the density algorithm. This section contrasts these
two methods and points out their limitations. Then it gives an intuitive answer to the question of why a two
stage based strategy might work and introduces the general two-stage framework. Section 3 introduces some
specific two-stage algorithms. Section 3.1 analyzes and evaluates the naı̈ve strategy. Section 3.2 describes a
two-stage algorithm that combines the randomized and the density algorithm. Section 3.3 introduces graph
coloring based techniques in the second stage. Section 3.4 examines the effect of group action on the size
of the constructed covering arrays. Section 4 compares the results of various two-stage algorithms with the
presently best known sizes. In Section 5 we discuss the Lovász local lemma (LLL) bounds on CAN(t, k, v)
and provide a Moser-Tardos type randomized algorithm for covering array construction that matches the
bound. Although the bound was known [18], the proof was non-constructive, and a constructive algorithm to
match this bound seems to be absent in the literature. We explore potentially better randomized algorithms
for the first stage using LLL based techniques, We also obtain a two-stage bound that improves the LLL
bound for CAN(t, k, v). We conclude the paper in Section 6.

2

Algorithmic construction of covering arrays

Available algorithms for the construction of covering arrays are primarily heuristic in nature; indeed exact
algorithms have succeeded for very few cases. Computationally intensive metaheuristic search methods such
as simulated annealing, tabu search, constraint programming, and genetic algorithms have been employed
when the strength is relatively small or the number of factors and levels is small. These methods have
established many of the best known bounds on sizes of covering arrays [13], but for many problems of
practical size their time and storage requirements are prohibitive. For larger problems, the best available
methods are greedy. The IPO family of algorithms [24] repeatedly adds one column at a time, and
 then adds
new rows to ensure complete coverage. In this way, at any point in time, the status of v t k−1
t−1 interactions
may be stored. AETG [10] pioneered a different method, which greedily selects one row at a time to cover
a large number of as-yet-uncovered interactions. They establish that if a row can be chosen that covers the
maximum number, a good a priori bound on the size of the covering array can be computed. Unfortunately
selecting the maximum is NP-hard, and even if one selects the maximum there is no guarantee that the
covering array is the smallest possible
 [7], so AETG resorts to a good heuristic selection of the next row by
examining the stored status of v t kt interactions. None of the methods so far mentioned therefore guarantee
to reach an a priori bound. An
 extension of the AETG strategy, the density algorithm [5, 6, 15], stores
additional statistics for all v t kt interactions in order to ensure the selection of a good next row, and hence
guarantees to produce an array with at most the precomputed number of rows. Variants of the density
3

Algorithm 1: A randomized algorithm for covering array construction.
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; 
t, k, v)
log (kt)+t log v


1 Set N :=
;
vt
log

2
3

4
5
6
7
8
9
10
11
12

v t −1

repeat
Construct an N × k array A where each entry is chosen independently and uniformly at random
from a v-ary alphabet;
Set covered := true;
for each interaction ι ∈ It,k,v do
if ι is not covered in A then
Set covered := false;
break;
end
end
until covered = true;
Output A;

algorithm have proved to be most effective for problems of moderately large size. For even larger problems,
pure random approaches have been applied.
To produce methods that provide a guarantee on size, it is natural to focus on the density algorithm
in order to understand its strengths and weaknesses. To do this, we contrast it with a basic randomized
algorithm. Algorithm 1 shows a simple randomized algorithm for covering array construction. The algorithm
constructs an array of a particular size randomly and checks whether all the interactions are covered. It
repeats until it finds an array that covers all the interactions.
log (kt)+t log v

 is guaranteed to exist:
A CA(N ; t, k, v) with N =
vt
log

v t −1

Theorem 1. [21, 27, 35] (Stein-Lovász-Johnson (SLJ) bound): Let t, k, v be integers with k ≥ t ≥ 2, and
v ≥ 2. Then as k → ∞,

log kt + t log v


CAN(t, k, v) ≤
t
log vtv−1
In fact, the probability that the N × k array constructed in line 3 of Algorithm 1 is a valid covering array
is high enough that the expected number of times the loop in line 2 is repeated is a small constant.
An alternative strategy is to add rows one by one instead of constructing the full array at the outset. We
start with an empty array, and whenever we add a new row we ensure that it covers at least the expected
number of previously uncovered interactions for a randomly chosen row. The probability that an uncovered
interaction is covered by a random row is 1/v t . If the number of uncovered interactions is u, then by linearity
of expectation, the expected number of newly covered interactions in a randomly chosen row is uv −t . If each
row added covers exactly this expected number, we obtain the same number of rows as the SLJ bound,
realized in Algorithm 1. But because the actual number of newly covered interactions is always an integer,
each added row covers at least duv −t e interactions. This is especially helpful towards the end when the
expected number is a small fraction.
Algorithm 2 follows this strategy. Again the probability that a randomly chosen row covers at least the
expected number of previously uncovered interactions is high enough that the expected number of times the
row selection loop in line 6 of Algorithm 2 is repeated is bounded by a small constant.
We can obtain an upper bound on the size produced by Algorithm 2 by assuming that each new row
added covers exactly duv −t e previously uncovered interactions. This bound is the discrete Stein-Lovász4

Algorithm 2: A randomized algorithm for covering array construction using the discrete SLJ strategy.
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; t, k, v)
1 Let A be an empty array;

k t
2 Initialize a table T indexed by all t v interactions, marking every interaction “uncovered”;
3 while there is an interaction marked “uncovered” in T do
4
Let u be the number of interactions marked “uncovered” in T ;
5
Set expectedCoverage := d vut e;
6
repeat
7
Let r be a row of length k where each entry is chosen independently and uniformly at random
from a v-ary alphabet;
8
Let coverage be the number of “uncovered” interactions in T that are covered in row r;
9
until coverage > expectedCoverage;
10
Add r to A;
11
Mark all interactions covered by r as “covered” in T ;
12 end
13 Output A;

Johnson (discrete SLJ) bound. Figure 1 compares the sizes of covering arrays obtained from the SLJ and
the discrete SLJ bounds for different values of k when t = 6 and v = 3. Consider a concrete example, when
t = 5, k = 20, and v = 3. The SLJ bound guarantees the existence of a covering array with 12499 rows,
whereas the discrete SLJ bound guarantees the existence of a covering array with only 8117 rows.
The density algorithm replaces the loop at line 6 of Algorithm 2 by a conditional expectation derandomized method. For fixed v and t the density algorithm selects a row efficiently (time polynomial in k) and
deterministically that is guaranteed to cover at least duv −t e previously uncovered interactions. In practice,
for small values of k the density algorithm works quite well, often covering many more interactions than
the minimum. Many of the currently best known CAN(t, k, v) upper bounds are obtained by the density
algorithm in combination with various post-optimization techniques [13].
However, the practical applicability of Algorithm 2 and the density algorithm is limited by the storage
of the table T , representing each of the kt v t interactions. Even when t = 6, v = 3, and k = 54, there are
18,828,003,285 6-way interactions. This huge memory requirement renders the density algorithm impractical
for rather small values of k when t ∈ {5, 6} and v ≥ 3. We present an idea to circumvent this large
requirement for memory, and develop it in full in Section 3.

2.1

Why does a two stage based strategy make sense?

Compare the two extremes, the density algorithm and Algorithm 1. On one hand, Algorithm 1 does not suffer
from any substantial storage restriction, but appears to generate many more rows than the density algorithm.
On the other hand, the density algorithm constructs fewer rows for small values of k, but becomes impractical
when k is moderately large. One wants algorithms that behave like Algorithm 1 in terms of memory, but
yield a number of rows competitive with the density algorithm.
For t = 6, k = 16, and v = 3, Figure 2 compares the coverage profile for the density algorithm and
Algorithm 1. We plot the number of newly covered interactions for each row in the density algorithm, and
the expected number of newly covered interactions for each row for Algorithm 1. The qualitative features
exhibited by this plot are representative of the rates of coverage for other parameters.
Two key observations are suggested by Figure 2. First, the expected coverage in the initial random rows
is similar to the rows chosen by the density algorithm. In this example, the partial arrays consisting of the
first 1000 rows exhibit similar coverage, yet the randomized algorithm needed no extensive bookkeeping.
Secondly, as later rows are added, the judicious selections of the density algorithm produce much larger
coverage per row than Algorithm 1. Consequently it appears sensible to invest few computational resources
5

4

3.5

x 10

SLJ bound
Discrete SLJ bound

N − number of rows

3

2.5

2

1.5

1

0.5
0

100

200

300

400

500
k

600

700

800

900

1000

Figure 1: Comparison of covering array sizes obtained from SLJ bound and discrete SLJ bound for different
values of k, when t = 6 and v = 3.

9000
Density
Basic Random

Number of newly covered interactions

8000
7000
6000
5000
4000
3000
2000
1000
0

0

2000

4000

6000
Row number

8000

10000

12000

Figure 2: For t = 6, k = 16 and v = 3, the actual number of newly covered interactions of the density
algorithm and the expected number of newly covered interactions in a random array.

6

on the initial rows, while making more careful selections in the later ones. This forms the blueprint of our
general two-stage algorithmic framework shown in Algorithm 3.
Algorithm 3: The general two-stage framework for covering array construction.
Input: t : strength of the required covering array, k : number of factors, v : number of levels for each
factor
Output: A : a CA(N ; t, k, v)
1 Choose a number n of rows and a number ρ of interactions;
// First Stage
0
2 Use a randomized algorithm to construct an n × k array A ;
0
3 Ensure that A covers all but at most ρ interactions;
0
4 Make a list L of interactions that are not covered in A (L contains at most ρ interactions);
// Second Stage
0
5 Use a deterministic procedure to add N − n rows to A to cover all the interactions in L;
6 Output A;
A specific covering array construction algorithm results by specifying the randomized method in the first
stage, the deterministic method in the second stage, and the computation of n and ρ. Any such algorithm
produces a covering array, but we wish to make selections so that the resulting algorithms are practical while
still providing a guarantee on the size of the array. In Section 3 we describe different algorithms from the
two-stage family, determine the size of the partial array to be constructed in the first stage, and establish
upper bound guarantees. In Section 4 we explore how good the algorithms are in practice.

3

Two-stage framework

For the first stage we consider two methods:
Rand
MT

the basic randomized algorithm
the Moser-Tardos type algorithm

We defer the development of method MT until Section 5. Method Rand uses a simple variant of Algorithm
1, choosing a random n × k array.
For the second stage we consider four methods:
Naive
Greedy
Den
Col

the
the
the
the

naı̈ve strategy, one row per uncovered interaction
online greedy coloring strategy
density algorithm
graph coloring algorithm

Using these abbreviations, we adopt a uniform naming convention for the algorithms: TS hA, Bi is the algorithm in which A is used in the first stage, and B is used in the second stage. For example, TS hMT, Greedyi
denotes a two-stage algorithm where the first stage is a Moser-Tardos type randomized algorithm and the
second stage is a greedy coloring algorithm. Later when the need arises we refine these algorithm names.

3.1

One row per uncovered interaction in the second stage (TS hRand, Naivei)

In the second stage each of the uncovered interactions after the first stage is covered using a new row.
Algorithm 4 describes the method in more detail.
This simple strategy improves on the basic randomized strategy when n is chosen judiciously. For
example, when t = 6, k = 54 and v = 3, Algorithm 1 constructs a covering array with 17, 236 rows. Figure
3 plots an upper bound on the size of the covering array against the number n of rows in the partial array.

7

Algorithm 4: Naı̈ve two-stage algorithm (TS hRand, Naivei).
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; t, k, v)
 t 
log (kt)+t log v+log log vtv−1


;
1 Let n :=
vt
log

1

v t −1

2

Let ρ =

3

repeat
Let A be an n × k array where each entry is chosen independently and uniformly at random from
a v-ary alphabet;
Let uncovNum := 0 and unCovList be an empty list of interactions;
Set covered := true;
for each interaction ι ∈ It,k,v do
if ι is not covered in A then
Set uncovNum :=uncovNum+1;
Add ι to unCovList;
if uncovNum > ρ then
Set covered := false;
break;
end
end
end
until covered= true;
for each interaction ι ∈uncovList do
Add a row to A that covers ι;
end
Output A;

4

5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

log

vt
v t −1

;

8

4

Total number of rows in the Covering array

1.75

x 10

1.7
1.65
1.6
1.55
1.5
1.45
1.4
1.35
1.3

1

1.1
1.2
1.3
1.4
1.5
1.6
1.7
n −− number of rows in the partial array of the first stage

1.8
4

x 10

Figure 3: An upper bound on the size of the covering array against n, the size of the partial array constructed
in the first stage when t = 6, k = 54, and v = 3, with one new row added per uncovered interaction in the
second stage. The minimum size of 13, 162 is obtained when n = 12, 402. Algorithm 1 requires 17, 236 rows,
and the currently best known covering array has 17, 197 rows.

9

The smallest covering array is obtained when n = 12, 402 which, when completed, yields a covering array
with at most 13, 162 rows—a big improvement over Algorithm 1.
A theorem from [33] tells us the optimal value of n in general:
Theorem 2. [33] Let t, k, v be integers with k ≥ t ≥ 2, and v ≥ 2. Then
 t 

log kt + t log v + log log vtv−1 + 1


.
CAN(t, k, v) ≤
t
log vtv−1
log (kt)+t log v+log log


t
log vtv−1



vt
v t −1



. The expected number of uncovered
The bound is obtained by setting n =
 t 
interactions is exactly ρ = 1/ log vtv−1 .
Figure 4 compares SLJ, discrete SLJ and two-stage bounds for k ≤ 100, when t = 6 and v = 3. The
two-stage bound does not deteriorate in comparison to discrete SLJ bound as k increases; it consistently
takes only 307-309 more rows. Thus when k = 12 the two-stage bound requires only 6% more rows and
when k = 100 only 2% more rows than the discrete SLJ bound.
4

2.2

x 10

SLJ bound
Discrete SLJ bound
Two−stage bound

2

N − number of rows

1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
10

20

30

40

50

60

70

80

90

100

k

Figure 4: Comparison of covering array sizes obtained from SLJ bound, discrete SLJ bound and two-stage
bound for k ≤ 100, when t = 6 and v = 3. In this range of k values the two-stage bound requires 307-309
more rows than the discrete SLJ bound, that is, 2-6% more rows.
To ensure that the loop in line 7 of Algorithm 4 does not repeat too many times we need to know the
probability with which a random n × k array leaves at most ρ interactions uncovered. Using Chebyshev’s
inequality and the second moment method developed in [2, Chapter 4], we next show that in a random n ×
nk
array the number of uncovered interactions is almost always close to its expectation, i.e. kt v t 1 − v1t .
Substituting the value of n from line 1, this expected value is equal to µ, as in line 2. Therefore, the probability
that a random n × k array covers the desired number of interactions is constant, and the expected number
of times the loop in line 7 is repeated is also a constant (around 2 in practice).

10

Because the theory of the second moment method is developed
considerable detail in [2], here we briefly
Pin
m
mention the relevant concepts and results. Suppose that X = i=1 Xi , where Xi is the indicator random
variable for event Ai for 1 ≤ i ≤ m. For indices i, j, we write i ∼ j if i 6= j and the events Ai , Aj are not
independent. Also suppose that X1 , . . . , Xm are symmetric, i.e. for every i 6= j there is a measure
preserving
P
mapping of the underlying probability space that sends event Ai to event Aj . Define ∆∗ = j∼i Pr [Aj |Ai ].
Then by [2, Corollary 4.3.4]:
Lemma 3. [2] If E[X] → ∞ and ∆∗ = o(E[X]) then X ∼ E[X] almost always.
In our case, Ai denotes the event that the ith interaction is not covered in a n × k array where each entry
n
is chosen independently and uniformly at random from a v-ary alphabet. Then Pr[Xi ] = 1 − v1t . Because



n
there are kt v t interactions in total, by linearity of expectation, E[X] = kt v t 1 − v1t , and E[X] → ∞ as
k → ∞.
Distinct events Ai and Aj are independent if the ith and jth interactions share no column. Therefore,

P
P
k
the event Ai is not independent of at most t t−1
other events Aj . So ∆∗ = j∼i Pr [Aj |Ai ] ≤ j∼i 1 ≤

k
t t−1
= o(E[X]) when v and t are constants. By Lemma 3, the number of uncovered interactions in a random
n × k array is close to the expected number of uncovered interactions. This guarantees that Algorithm 4 is
an efficient randomized algorithm for constructing covering arrays with a number of rows upper bounded by
Theorem 2.
In keeping with the general two-stage framework, Algorithm 4 does not store the coverage status of
each interaction. We only need store the interactions that are uncovered in A, of which there are at most
1
 ≈ v t . This quantity depends only on v and t and is independent of k, so is effectively a
ρ =
t
log vtv−1

constant that is much smaller than kt v t , the storage requirement for the density algorithm. Hence the
algorithm can be applied to a higher range of k values.
Although Theorem 5 provides asymptotically tighter bounds than Theorem 2, in a range of k values that
are relevant for practical application, Theorem 2 provides better results. Figure 5 compares the bounds on
CAN(t, k, v) with the currently best known results.
4

3

6

x 10

2.5

Best known
Two−stage (simple)
GSS bound
2

2

N − number of rows

N − number of rows

2.5

1.5

1

1.5

1

0.5

0.5

0
0

x 10

Best known
Two−stage (simple)
GSS bound

10

20

30

40

50

60

70

80

90

k

0

5

10

15

20

25

30

35

40

45

50

k

(a) t = 6, v = 3

(b) t = 6, v = 6

Figure 5: Comparison of GSS bound and two-stage bound with the currently best known results

3.2

The density algorithm in the second stage (TS hRand, Deni)

Next we apply the density algorithm in the second stage. Figure 6 plots an upper bound on the size of the
covering array against the size of the partial array constructed in the first stage when the density algorithm
is used in the second stage, and compares it with TS hRand, Naivei. The size of the covering array decreases
11

as n decreases. This is expected because with smaller partial arrays, more interactions remain for the second
stage to be covered by the density algorithm. In fact if we cover all the interactions using the density
algorithm (as when n = 0) we would get an even smaller covering array. However, our motivation was
precisely to avoid doing that. Therefore, we need a ”cut-off” for the first stage.
4

Total number of rows in the Covering array

1.9

x 10

Basic Two−stage
Two−stage with density in second stage

1.8

1.7

1.6

1.5

1.4

1.3

1.2

1

1.1
1.2
1.3
1.4
1.5
1.6
1.7
n −− number of rows in the partial array of the first stage

1.8
4

x 10

Figure 6: Comparison of covering array sizes from two-stage algorithms with Den and Naive in the second
stage. With Den there is no minimum point in the curve; the size of the covering array keeps decreasing as
we leave more uncovered interactions for the second stage.
We are presented with a trade-off. If we construct a smaller partial array in the first stage, we obtain a
smaller covering array overall. But we then pay for more storage and computation time for the second stage.
To appreciate the nature of this trade-off, look at Figure 7, which plots an upper bound on the covering
array size and the number of uncovered interactions in the first stage against n. The improvement in the
covering array size plateaus after a certain point. The three horizontal lines indicate ρ (≈ v t ), 2ρ and 3ρ
uncovered interactions in the first stage. (In the naı̈ve method of Section 3.1, the partial array after the first
stage leaves at most ρ uncovered interactions.) In Figure 7 the final covering array size appears to plateau
when the number of uncovered interactions left by the first stage is around 2ρ. After that we see diminishing
returns — the density algorithm needs to cover more interactions in return for a smaller improvement in the
covering array size.
Let r be the maximum number of interactions allowed to remain uncovered after the first stage. Then r
can be specified in the two-stage algorithm. To accommodate this, we denote by TS hA, B; ri the two-stage
algorithm where A is the first stage strategy, B is the second stage strategy, and r is the maximum number
of uncovered interactions after the first stage. For example, TS hRand, Den; 2ρi applies the basic randomized
algorithm in the first stage to cover all but at most 2ρ interactions, and the density algorithm to cover the
remaining interactions in the second stage.

3.3

Coloring in the second stage (TS hRand, Coli and TS hRand, Greedyi)

Now we describe strategies using graph coloring in the second stage. Construct a graph G = (V, E), the
incompatibility graph, in which V is the set of uncovered interactions, and there is an edge between two
12

Number of rows / Number of uncovered interactions

18000
16000

Num. of rows in the completed CA
Num. of uncovered interaction in first stage

14000
12000
10000
8000
6000
4000
2000
0
0.8

0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
4
n −− number of rows in the partial array of the first stage
x 10

Figure 7: Final covering array size against the number of uncovered interactions after the first stage. As the
size n of the partial array decreases, the number of uncovered interactions in the first stage increases. Den is
used in the second stage. From bottom to top, the green lines denote ρ, 2ρ, and 3ρ uncovered interactions.
interactions exactly when they share a column in which they have different symbols. A single row can cover
a set of interactions if and only if it forms an independent set in G. Hence the minimum number of rows
required to cover all interactions of G is exactly its chromatic number χ(G), the minimum number of colors
in a proper coloring of G. Graph coloring is an NP-hard problem, so we employ heuristics to bound the
chromatic number. Moreover, G only has vertices for the uncovered interactions after the first stage, so is
size is small relative to the total number of interactions.
The expected number of edges in the incompatibility
graph after
n choosing n rows uniformly at random
 k  t Pt

 t
 
t k−t
1
1 n
1
t−i
is γ = 2 t v
) 1 − vt
1 − (vt −vt−i ) . Using the elementary upper bound on
i=1 i t−i (v − v
q
the chromatic number χ ≤ 12 + 2m + 14 , where m is the number of edges [16, Chapter 5.2], we can surely
q
cover the remaining interactions with at most 12 + 2m + 14 rows.
The actual number of edges m that remain after the first stage is a random variable with mean γ. In
principle, the first stage could be repeatedly applied until m ≤ γ, so we call m = γ the optimistic estimate.
To ensure that the first stage is expected to be run a small constant number of times, we increase the
estimate. With probability more than 1/2 the incompatibility graph has m ≤ 2γ edges, so m = 2γ is the
conservative estimate.
For t = 6, k = 56, and v = 3, Figure 8 shows the effect on the minimum number of rows when the bound
on the chromatic number in the second stage is used, for the conservative or optimistic estimates. The Naı̈ve
method is plotted for comparison. Better coloring bounds shift the minima leftward, reducing the number
of rows produced in both stages.
Thus far we have considered bounds on the chromatic number. Better estimation of χ(G) is complicated
by the fact that we do not have much information about the structure of G until the first stage is run. In
practice, however, G is known after the first stage and hence an algorithmic method to bound its chromatic
number can be applied. Because the number of vertices in G equals the number of uncovered interactions

13

4

2.2

x 10

Conservative estimate
Optimistic estimate
Simple

Number of rows required

2

1.8

1.6

1.4

1.2

1

1

1.1

1.2

1.3

1.4
N

1.5

1.6

1.7

1.8
4

x 10

Figure 8: Size of the partial array vs. size of the complete CA. t = 6, k = 56, v = 3. Stein-Lovász-Johnson
bound requires 17, 403 rows, discrete Stein-Lovász-Johnson bound requires 13, 021 rows. Simple estimate
for the two stage algorithm is 13, 328 rows, conservative estimate assuming m = 2γ is 12, 159 rows, and
optimistic estimate assuming m = γ is 11, 919 rows. Even the conservative estimate beats the discrete
Stein-Lovász-Johnson bound.

14

after the first stage, we encounter the same trade-off between time and storage, and final array size, as seen
earlier for density. Hence we again parametrize by the expected number of uncovered interactions in the
first stage.
We employ two different greedy algorithms to color the incompatibility graph. In method Col we first
construct the incompatibility graph G after the first stage. Then we apply the commonly used smallest last
order heuristic to order the vertices for greedy coloring: At each stage, find a vertex vi of minimum degree
in Gi , order the vertices of Gi − vi , and then place vi at the end. More precisely, we order the vertices of G
as v1 , v2 , . . . , vn , such that vi is a vertex of minimum degree in Gi , where Gi = G − {vi+1 , . . . , vn }. A graph
is d-degenerate if all of its subgraphs have a vertex with degree at most d. When G is d-degenerate but not
(d − 1)-degenerate, the Coloring number col(G) is d + 1. If we then greedily color the vertices with the first
available color, at most col(G) colors are used.
In method Greedy we employ an on-line, greedy approach that colors the interactions as they are discovered in the first stage. In this way, the incompatibility graph is never constructed. We instead maintain a set
of rows. Some entries in rows are fixed to a specific value; others are flexible to take on any value. Whenever
a new interaction is found to be uncovered in the first stage, we check if any of the rows is compatible with
this interaction. If such a row is found then entries in the row are fixed so that the row now covers the
interaction. If no such row exists, a new row with exactly t fixed entries corresponding to the interaction is
added to the set of rows. This method is much faster than method Col in practice.

3.4

Using group action

Covering arrays that are invariant under the action of a permutation group on their symbols can be easier
to construct and are often smaller [15]. Direct and computational constructions using group actions are
explored in [9, 28]. Sarkar et al. [33] establish the asymptotically tightest known bounds on CAN(t, k, v)
using group actions. In this section we explore the implications of group actions on two-stage algorithms.
Let Γ be a permutation group on the set of symbols. The action of this group partitions the set of
t-way interactions into orbits. We construct an array A such that for every orbit, at least one row covers
an interaction from that orbit. Then we develop the rows of A over Γ to obtain a covering array that is
invariant under the action of Γ. Effort then focuses on covering all the orbits of t-way interactions, instead
of the individual interactions.
If Γ acts sharply transitively
on the set of symbols

 (for example, if Γ is a cyclic group of order v) then
the action of Γ partitions kt v t interactions into kt v t−1 orbits of length v each. Following
the lines of the

v t−1
+1
log (kt)+(t−1) log v+log log t−1
−1

 v
that covers at
proof of Theorem 2, there exists an n × k array with n =
v t−1
log

v t−1 −1

least one interaction from each orbit. Therefore,
log
CAN(t, k, v) ≤ v

k
t



+ (t − 1) log v + log log


v t−1
log vt−1
−1



v t−1
v t−1 −1



+1
.

(1)

Similarly, we can employ a Frobenius group. When v is a prime power, the Frobenius group is the group
of permutations of Fv of the form {x 7→ ax + b : a, b ∈ Fv , a 6= 0}. The action of the Frobenius group
t−1
partitions the set of t-tuples on v symbols into v v−1−1 orbits of length v(v − 1) (full orbits) each and 1 orbit
of length v (a short orbit). The short orbit consists of tuples of the form (x1 , . . . , xt ) where x1 = . . . = xt .
Therefore, we can obtain a covering array by first constructing an array that covers all the full orbits, and
then developing all the rows over the Frobenius group and adding v constant rows. Using the two stage
strategy in conjunction with the Frobenius group action we obtain:


 t−1 

v t−1
log kt + log v v−1−1 + log log vt−1
−v+1 + 1


CAN(t, k, v) ≤ v(v − 1)
+ v.
(2)
t−1
v
log vt−1
−v+1

15

4

1.5

x 10

N − number of rows

1.45

Two−stage (simple)
Two−stage (cyclic group action)
Two−stage (Frobenius group action)

1.4

1.35

1.3

1.25
50

55

60

65

70

75

k

Figure 9: Comparison of the simple two-stage bound with the cyclic and the Frobenius two-stage bounds.
t = 6, v = 3 and 50 ≤ k ≤ 75. Group action reduces the required number of rows slightly.
Figure 9 compares the simple two-stage bound with the cyclic and Frobenius two-stage bounds. For
t = 6, v = 3 and 12 ≤ k ≤ 100, the cyclic bound requires 7-21 (on average 16) fewer rows than the simple
bound. In the same range the Frobenius bound requires 17 − 51 (on average 40) fewer rows.
Group action can be applied in other methods for the second stage as well. Colbourn [15] incorporates
group action into the density algorithm, allowing us to apply method Den in the second stage.
Greedy extends easily to use group action, as we do not construct an explicit incompatibility graph.
Whenever we fix entries in a row to cover an uncovered orbit, we commit to a specific orbit representative.
However, applying group action to the incompatibility graph coloring for Col is more complicated. We
need to modify the definition of the incompatibility graph for two reasons. First the vertices no longer
represent uncovered interactions, but rather uncovered orbits of interaction. Secondly, and perhaps more
importantly, pairwise compatibility between every two orbits in a set no longer implies mutual compatibility
among all orbits in the set.
One approach is to form a vertex for each uncovered orbit, placing an edge between two when they share
a column. Rather than the usual coloring, however, one asks for a partition of the vertex set into classes so
that every class induces an acyclic subgraph. Problems of this type are generalized graph coloring problems
[4]. Within each class of such a vertex partition, consistent representatives of each orbit can be selected to
form a row; when a cycle is present, this may not be possible. Unfortunately, heuristics for solving these
types of problems appear to be weak, so we adopt a second approach. As we build the incompatibility
graph, we commit to specific orbit representatives. When a vertex for an uncovered orbit is added, we check
its compatibility with the orbit representatives chosen for the orbits already handled with which it shares
columns; we commit to an orbit representative and add edges to those with which it is now incompatible.
Once completed, we have a (standard) coloring problem for the resulting graph.
Because group action can be applied using each of the methods for the two stages, we extend our naming
to TS hA, B; r, Γi, where Γ can be Trivial (i.e. no group action), Cyclic, or Frobenius.

16

4

Computational results

Figure 5 indicates that even a simple two-stage bound can improve on best known covering array numbers.
Therefore we investigate the actual performance of our two-stage algorithms for covering arrays of strength
5 and 6.
First we present results for t = 6, when v ∈ {3, 4, 5, 6} and no group action is assumed. Table 1 shows the
results for different v values. In each case we select the range of k values where the two-stage bound predicts
smaller covering arrays than
 previously known best ones, setting the maximum number of uncovered
 tthe
v
interactions as ρ = 1/ log vt −1 ≈ v t . For each value of k we construct a single partial array and then run
the different second stage algorithms on it consecutively. In this way all the second stage algorithms cover
the same set of uncovered interactions.
The column tab lists the best known CAN(t, k, v) upper bounds from [13]. The column bound shows the
upper bounds obtained from the two-stage bound (2). The columns naı̈ve, greedy, col and den show results
obtained from running the TS hRand, Naive; ρ, Triviali, TS hRand, Greedy; ρ, Triviali, TS hRand, Col; ρ, Triviali
and TS hRand, Den; ρ, Triviali algorithms, respectively.
The naı̈ve method always finds a covering array that is smaller than the two-stage bound. This happens
because we repeat the first stage of Algorithm 4 until the array has fewer than v t uncovered interactions.
(If the first stage were not repeated, the algorithm still produce covering arrays that are not too far from
the bound.) For v = 3 Greedy and Den have comparable performance. Method Col produces covering arrays
that are smaller. However, for v ∈ {4, 5, 6} Den and Col are competitive.
Table 2 shows the results obtained by the different second stage algorithms when the maximum number
of uncovered interactions in the first stage is set to 2ρ and 3ρ respectively. When more interactions are
covered in the second stage, we obtain smaller arrays as expected. However, the improvement in size does
not approach 50%. There is no clear winner.
Next we investigate the covering arrays that are invariant under the action of a cyclic group. In Table 3 the
column bound shows the upper bounds from Equation (1). The columns naı̈ve, greedy, col and den show
results obtained from running TS hRand, Naive; ρ, Cyclici, TS hRand, Greedy; ρ, Cyclici, TS hRand, Col; ρ, Cyclici
and TS hRand, Den; ρ, Cyclici, respectively.
Table 4 presents results for cyclic group action based algorithms when the number of maximum uncovered
interactions in the first stage is set to 2ρ and 3ρ respectively.
For the Frobenius group action, we show results only for v ∈ {3, 5} in Table 5. The column bound shows
the upper bounds obtained from Equation (2).
Table 6 presents results for Frobenius group action algorithms when the number of maximum uncovered
interactions in the first stage is 2ρ or 3ρ.
Next we present a handful of results when t = 5. In the cases examined, using the trivial group action
is too time consuming to be practical. However, the cyclic or Frobenius cases are feasible. Tables 7 and 8
compare two stage algorithms when the number of uncovered interactions in the first stage is at most 2ρ.
In almost all cases there is no clear winner among the three second stage methods. Methods Den and
Greedy are, however, substantially faster and use less memory than method Col; for practical purposes they
would be preferred.
All code used in this experimentation is available from the github repository
https://github.com/ksarkar/CoveringArray
under an open source GPLv3 license.

5

Limited dependence and the Moser-Tardos algorithm

Here we explore a different randomized algorithm that produces smaller covering arrays than Algorithm 1.
When k > 2t, there are interactions that share no column. The events of coverage of such interactions are
independent. Moser et al. [29, 30] provide an efficient randomized construction method that exploits this

17

k

tab

bound

53
54
55
56
57

13021
14155
17161
19033
20185

13076
13162
13246
13329
13410

39
40
41
42
43
44

68314
71386
86554
94042
99994
104794

65520
66186
66834
67465
68081
68681

31
32
33
34
35
36

233945
258845
281345
293845
306345
356045

226700
229950
233080
236120
239050
241900

17
18
19
20
21
22
23
24
25
26
27

506713
583823
653756
694048
783784
844834
985702
1035310
1112436
1146173
1184697

486310
505230
522940
539580
555280
570130
584240
597660
610460
622700
634430

naı̈ve
greedy
t = 6, v = 3
13056
12421
13160
12510
13192
12590
13304
12671
13395
12752
t = 6, v = 4
65452
61913
66125
62573
66740
63209
67408
63819
68064
64438
68556
65021
t = 6, v = 5
226503 213244
229829 216444
232929 219514
235933 222516
238981 225410
241831 228205
t = 6, v = 6
486302 449950
505197 468449
522596 485694
539532 502023
555254 517346
569934 531910
584194 545763
597152 558898
610389 571389
622589 583473
634139 594933

col

den

12415
12503
12581
12665
12748

12423
12512
12591
12674
12757

61862
62826
63160
64077
64935
65739

61886
62835
63186
64082
64907
65703

212942
217479
219215
222242
226379
230202

212940
217326
219241
222244
226270
229942

448922
467206
484434
500788
516083
530728
544547
557917
570316
582333
593857

447864
466438
483820
500194
515584
530242
548307
557316
569911
582028
593546

Table 1: Comparison of different TS hRand, −; ρ, Triviali algorithms.

18

k
greedy

2ρ
col

53
54
55
56
57

11968
12135
12286
12429
12562

11958
12126
12129
12204
12290

39
40
41
42
43
44

59433
60090
60715
61330
61936
62530

59323
60479
61527
62488
61839
62899

31
32
33
34
35
36

204105
207243
210308
213267
216082
218884

203500
206659
209716
212675
215521
218314

17
18
19
20
21
22
23
24
25
26
27

425053
443236
460315
476456
491570
505966
519611
532612
544967
556821
568135

-

den
t = 6, v
11968
12050
12131
12218
12296
t = 6, v
59326
59976
60615
61242
61836
62428
t = 6, v
203302
206440
209554
212508
215389
218172
t = 6, v
420333
438754
455941
472198
487501
502009
515774
528868
541353
553377
564827

greedy
=3
11716
11804
11877
11961
12044
=4
58095
58742
59369
59974
60575
61158
=5
199230
202342
205386
208285
211118
213872
=6
412275
430402
447198
463071
478269
492425
505980
518746
531042
542788
554052

3ρ
col

den

11705
11787
11875
12055
12211

11708
11790
11872
11950
12034

57951
58583
59867
61000
60407
61004

57888
58544
59187
59796
60393
60978

198361
201490
204548
-

197889
201068
204107
207060
209936
212707

-

405093
423493
440532
456725
471946
486306
500038
513047
525536
537418
548781

Table 2: Comparison of TS hRand, −; 2ρ, Triviali and TS hRand, −; 3ρ, Triviali algorithms.

19

k

tab

bound

53
54
55
56
57

13021
14155
17161
19033
20185

13059
13145
13229
13312
13393

k
39
40
41
42
43
44

tab
68314
71386
86554
94042
99994
104794

bound
65498
66163
66811
67442
68057
68658

31
32
33
34
35
36

226000
244715
263145
235835
238705
256935

226680
229920
233050
236090
239020
241870

17
18
19
20
21
22
23
24
25
26
27

506713
583823
653756
694048
783784
844834
985702
1035310
1112436
1146173
1184697

486290
505210
522910
539550
555250
570110
584210
597630
610430
622670
624400

naı̈ve
greedy
t = 6, v = 3
13053
12405
13119
12489
13209
12573
13284
12660
13368
12744
t = 6, v = 4
naı̈ve
greedy
65452
61896
66080
62516
66740
63184
67408
63800
68032
64408
68556
64988
t = 6, v = 5
226000 213165
229695 216440
233015 219450
235835 222450
238705 225330
241470 228140
t = 6, v = 6
485616 449778
504546 468156
522258 485586
539280 501972
554082 517236
569706 531852
583716 545562
597378 558888
610026 571380
622290 583320
633294 594786

col

den

12405
12543
12663
12651
12744

12411
12546
12663
12663
12750

col
61860
62820
63144
63780
64692
64964

den
61864
62784
63152
63784
64680
64976

212945
217585
221770
222300
225130
229235

212890
217270
221290
222210
225120
229020

448530
467232
490488
500880
521730
530832
549660
557790
575010
582546
598620

447732
466326
488454
500172
519966
530178
548196
557280
573882
582030
597246

Table 3: Comparison of TS hRand, −; ρ, Cyclicialgorithms.

20

k
greedy

2ρ
col

53
54
55
56
57

11958
12039
12120
12204
12276

11955
12027
12183
12342
12474

39
40
41
42
43
44

59412
60040
60700
61320
61908
62512

59336
59996
61156
62196
63192
64096

31
32
33
34
35
36

204060
207165
207165
213225
216050
218835

203650
209110
209865
212830
217795
218480

17
18
19
20
21
22
23
24
25
26
27

424842
443118
460014
476328
491514
505884
519498
532368
544842
543684
568050

422736
440922
457944
474252
489270
503580
517458
530340
542688
543684
566244

den
t = 6, v
11958
12036
12195
12324
12450
t = 6, v
59304
59964
61032
61976
62852
63672
t = 6, v
203265
208225
209540
212510
217070
218155
t = 6, v
420252
438762
455994
472158
487500
501852
515718
528828
541332
543684
564756

greedy
=3
11700
11790
11862
11949
12027
=4
58076
58716
59356
59932
60568
61152
=5
199180
202255
205380
208225
211080
213770
=6
411954
430506
447186
463062
478038
492372
505824
518700
530754
542664
553704

3ρ
col

den

11691
11874
12057
11937
12021

11694
11868
12027
11943
12024

57976
58616
59252
59840
61124
61048

57864
58520
59160
59760
60904
60988

198455
204495
204720
207790
213425
213185

197870
203250
204080
207025
212040
212695

409158
427638
456468
460164
486180
489336
502806
515754
538056
539922
560820

405018
423468
449148
456630
479970
486264
500040
512940
532662
537396
555756

Table 4: Comparison of TS hRand, −; 2ρ, Cyclici and TS hRand, −; 3ρ, Cyclici algorithms.

21

k

tab

bound

53
54
55
56
57

13021
14155
17161
19033
20185

13034
13120
13203
13286
13366

31
32
33
34
35
36

233945
258845
281345
293845
306345
356045

226570
229820
232950
235980
238920
241760

naı̈ve
greedy
t = 6, v = 3
13029
12393
13071
12465
13179
12561
13245
12633
13365
12723
t = 6, v = 5
226425 213025
229585 216225
232725 219285
234905 222265
238185 225205
241525 227925

col

den

12387
12513
12549
12627
12717

12393
12531
12567
12639
12735

212865
216085
219205
223445
227445
231145

212865
216065
219145
223265
227065
230645

Table 5: Comparison of TS hRand, −; ρ, Frobeniusi algorithms.

k
greedy

2ρ
col

53
54
55
56
57
70
75
80
85
90

11931
12021
12105
12171
12255
13167
13473
13773
14031
14289

11919
12087
12237
12171
12249
13155
13473
13767
14025
14283

31
32
33
34
35
36
50
55
60
65

203785
206965
209985
213005
215765
218605
250625
259785
268185
275785

203485
208965
209645
214825
215545
218285
250365
259625
268025
275665

den
greedy
t = 6, v = 3
11931
11700
12087
11790
12231
11862
12183
11949
12255
12027
13179
13479
13779
14037
14301
t = 6, v = 5
203225 198945
208065 201845
209405 205045
214145 208065
215265 210705
218025 213525
250325
259565
267945
275665
-

3ρ
col

den

11691
11874
12057
11937
12021
-

11694
11868
12027
11943
12024
-

198445
204505
209845
207545
210365
213105
-

197825
203105
207865
206985
209885
212645
-

Table 6: Comparison of TS hRand, −; 2ρ, Frobeniusi and TS hRand, −; 3ρ, Frobeniusi algorithms.

22

k
67
68
69
70
71

tab
59110
60991
60991
60991
60991

greedy
48325
48565
48765
49005
49245

col
48285
48565
49005
48985
49205

den
48305
48585
48985
49025
49245

Table 7: Comparison of TS hRand, −; 2ρ, Frobeniusi algorithms. t = 5, v = 5
k
49
50
51
52
53

tab
122718
125520
128637
135745
137713

greedy
108210
109014
109734
110556
111306

col
108072
108894
110394
110436
111180

den
107988
108822
110166
110364
111120

Table 8: Comparison of TS hRand, −; 2ρ, Cyclici algorithms. t = 5, v = 6
limited dependence. Specializing their method to covering arrays, we obtain Algorithm 5. For the specified
value of N in the algorithm it is guaranteed that the expected number of times the loop in line 3 of Algorithm
5 is repeated is linearly bounded in k (See Theorem 1.2 of [30]).
The upper bound on CAN(t, k, v) guaranteed by Algorithm 5 is obtained by applying the Lovász local
lemma (LLL).
Lemma 4. (Lovász local lemma; symmetric case) (see [2]) Let A1 , A2 , . . . , An events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at
most d, and that Pr[Ai ] ≤ p for all 1 ≤ i ≤ n. If ep(d + 1) ≤ 1, then Pr[∩ni=1 Āi ] > 0.
The symmetric version of Lovász local lemma provides an upper bound on the probability of a “bad”
event in terms of the maximum degree of a bad event in a dependence graph, so that the probability that
all the bad events are avoided is non zero. Godbole, Skipper, and Sunley [18] apply Lemma 4 essentially to
obtain the bound on CAN(t, k, v) in line 1 of Algorithm 5.
Theorem 5. [18] Let t, v and k ≥ 2t be integers with t, v ≥ 2. Then
n 
o
+ t log v + 1
log kt − k−t
t


CAN (t, k, v) ≤
t
log vtv−1
The bound on the size of covering arrays obtained from Theorem 5 is asymptotically tighter than the
one obtained from Theorem 1. Figure 10 compares the bounds for t = 6 and v = 3.
The original proof of LLL is essentially non-constructive and does not immediately lead to a polynomial
time construction algorithm for covering arrays satisfying the bound of Theorem 5. Indeed no previous
construction algorithms appear to be based on it. However the Moser-Tardos method of Algorithm 5 does
provide a construction algorithm running in expected polynomial time. For sufficiently large values of k
Algorithm 5 produces smaller covering arrays than the Algorithm 1.
But the question remains: Does Algorithm 5 produce smaller covering arrays than the currently best
known results within the range that it can be effectively computed? Perhaps surprisingly, we show that
the answer is affirmative. In Algorithm 5 we do not need to store the coverage information of individual
interactions in memory because each time an uncovered interaction is encountered we re-sample the columns
involved in that interaction and start the check afresh (checking the coverage in interactions in the same
order each time). Consequently, Algorithm 5 can be applied for larger values of k than the density algorithm.
23

Algorithm 5: Moser-Tardos type algorithm for covering array construction.
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; t, k, v)
log{(kt)−(k−t
log v+1
)}+t.
t

;
1 Let N :=
vt
log

2

3
4
5
6
7
8
9
10
11
12
13

14
15
16

v t −1

Construct an N × k array A where each entry is chosen independently and uniformly at random from
a v-ary alphabet;
repeat
Set covered := true;
for each interaction ι ∈ It,k,v do
if ι is not covered in A then
Set covered := false;
Set missing-interaction := ι;
break;
end
end
if covered = false then
Choose all the entries in the t columns involved in missing-interaction independently and
uniformly at random from the v-ary alphabet;
end
until covered = true;
Output A;

Smaller covering arrays can be obtained by exploiting a group action using LLL, as shown in [33]. Table
9 shows the sizes of the covering arrays constructed by a variant of Algorithm 5 that employs cyclic and
Frobenius group actions. While this single stage algorithm produces smaller arrays than the currently best
known [13], these are already superseded by the two-stage based algorithms.
k
56
57
58
59
60

tab
19033
20185
23299
23563
23563

MT
16281
16353
16425
16491
16557

(a) Frobenius. t = 6, v = 3

k
44
45
46
47
48

tab
411373
417581
417581
423523
423523

MT
358125
360125
362065
363965
365805

(b) Frobenius. t = 6, v = 5

k
25
26
27
28
29

tab
1006326
1040063
1082766
1105985
1149037

MT
1020630
1032030
1042902
1053306
1063272

(c) Cyclic. t = 6, v = 6

Table 9: Comparison of covering array size from Algorithm 5 (MT) with the best known results [13] (tab).

5.1

Moser-Tardos type algorithm for the first stage

The linearity of expectation arguments used in the SLJ bounds permit one to consider situations in which
a few of the “bad” events are allowed to occur, a fact that we exploited in the first stage of the algorithms
thus far. However, the Lovász local lemma does not address this situation directly. The conditional Lovász
local lemma (LLL) distribution, introduced in [19], is a very useful tool.
Lemma 6. (Conditional LLL distribution; symmetric case) (see [2, 33]) Let A = {A1 , A2 , . . . , Al } be a set
of l events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of
all other events Aj except for at most d, and that Pr[Ai ] ≤ p for all 1 ≤ i ≤ l. Also suppose that ep(d+1) ≤ 1
(Therefore, by LLL (Lemma 4) Pr[∩li=1 Āi ] > 0). Let B ∈
/ A be another event in the same probability space
24

5

10

N − number of rows

SLJ bound
GSS bound

4

10

3

10
1
10

2

3

10

10
k

4

10

5

10

Figure 10: Comparison of SLJ (Theorem 1) and GSS (Theorem 5) bounds for t = 6 and v = 3. The graph
is plotted in log-log scale to highlight the asymptotic difference between the two bounds.
with Pr[B] ≤ q, such that B is also mutually independent of a set of all other events Aj ∈ A except for at
most d. Then Pr[B| ∩li=1 Āi ] ≤ eq.
We apply the conditional
LLL distribution to obtain an upper bound on the size of partial array that
 t 
v
leaves at most log vt −1 ≈ v t interactions uncovered. For a positive integer k, let I = {j1 , . . . , jρ } ⊆ [k]
where j1 < . . . < jρ . Let A be an n × k array where each entry is from the set [v]. Let AI denote the n × ρ
array in which AI (i, `) = A(i, j` ) for 1 ≤ i ≤ N and 1 ≤ ` ≤ ρ; AI is the projection of A onto the columns
in I.

Let M ⊆ [v]t be a set of m t-tuples of symbols, and C ∈ [k]
be a set of t columns. Suppose the
t
entries in the array A are chosen independently from [v] with uniform probability.
 Let BC denote the event
that at least one of the tuples in M is not covered in AC . There are η = kt such events, and for all of
n
them Pr[BC ] ≤ m 1 − v1t . Moreover, when k ≥ 2t, each of the events is mutually independent of all



k
other events except for at most ρ = kt − k−t
− 1 < t t−1
. Therefore, by the Lovász local lemma, when
t

1 n
eρm 1 − vt ≤ 1, none of the events BC occur. Solving for n, when
n≥

log(eρm)


t
log vtv−1

(3)


there exists an n × k array A over [v] such that for all C ∈ [k]
t , AC covers all the m tuples in M . In fact
we can use a Moser-Tardos type algorithm to construct such an array.
Let ι be an interaction whose t-tuple
n of symbols is not in M . Then the probability that ι is not covered
in an n × k array is at most 1 − v1t
when each entry of the array is chosen independently from [v] with
uniform probability. Therefore, by the
 conditional LLL distribution the probability that ι1 is
n not covered
in the array A where for all C ∈ [k]
. Moreover,
t , AC covers all the m tuples in M is at most e 1 − v t
there are η(v t − m) such interactions ι. By the linearity of expectation, the expected number of uncovered
25

n
interactions in A is less than v t when η(v t − m)e 1 − v1t ≤ v t . Solving for n, we obtain
	

log ηe 1 − vmt

 .
n≥
t
log vtv−1

Therefore, there exists an n × k array with n = max

log{ηe(1− vmt )}
log(eρm)


,

t
t
log vtv−1
log vtv−1



(4)

that has at most v t

uncovered interactions. To compute n explicitly, we must choose m. We can select a value of m to minimize
n graphically for given values of t, k and v. For example, Figure 11 plots Equations 3 and 4 against m for
t = 3, k = 350, v = 3, and finds the minimum value of n.
460

445
max(Equation (3), Equation (4))

440

n − number of rows in the partial array

n − number of rows in the partial array

Equation (3)
Equation (4)

420

400

380

360

340
0

5

10

15
m

20

25

30

(a) Equations 3 and 4 against m.

440

435

430

425

420
0

5

10

15
m

20

25

30

(b) Maximum of the two sizes against m.

Figure 11: The minimum is at n = 422, when m = 16. t = 3, k = 350, v = 3
We compare the size of the partial array from the naı̈ve two-stage method (Algorithm 4) with the size
obtained by the graphical methods in Figure 12. The Lovász local lemma based method is asymptotically
better than the simple randomized method. However, except for the small values of t and v, in the range
of k values relevant for practical applications the simple randomized algorithm requires fewer rows than the
Lovász local lemma based method.

5.2

Lovász local lemma based two-stage bound

We can apply the techniques from Section 5.1 to obtain a two-stage bound similar to Theorem 2 using the
Lovász local lemma and conditional LLL distribution. First we extend a result from [33].



Theorem
7. Let t, k, v be integers with k ≥ t ≥ 2, v ≥ 2 and let η = kt , and ρ = kt − k−t
t . If


vt
v t −1

ηv t log

ρ

≤ v t Then
log
CAN(t, k, v) ≤

k
t



+ t log v + log log


t
log vtv−1



vt
v t −1



+2

η
− .
ρ

Proof. Let M ⊆ [v]t be a set of m t-tuples of symbols. Following the arguments of Section 5.1, when


 there exists an n × k array A over [v] such that for all C ∈ [k] , AC covers all m tuples in M .
n ≥ log(eρm)
t
vt
log

v t −1

At most η(v t − m) interactions are uncovered in such an array. Using the conditional
n LLL distribution,
the probability that one such interaction is not covered in A is at most e 1 − v1t . Therefore, by the
26

n − number of rows in partial array with vt missing interactions

n − number of rows in partial array with vt missing interactions

550
500
450
400
350
300
250
200
150
Randomized (Algorithm 4)
LLL based

100
50

0

200

400

600
k

800

1000

1200

2500

2000

1500

1000

500

0

0

Randomized (Algorithm 4)
LLL based

500

1000

1500

2000

2500

3000

3500

4000

4500

k

(a) t = 3, v = 3.

(b) t = 4, v = 3.

Figure 12: Comparison of the size of the partial array constructed in the first stage. Compares the size of
the partial array specified in Algorithm 4 in Section 3.1, and the size derived in Section 5.1.
 t

n
linearity of expectation, we can find one such array A that leaves at most eη(v t − m) 1 − v1t = ηρ vm − 1
interactions uncovered. Adding one row per uncovered interactions to A, we obtain a covering array with at
most N rows, where


log(eρm)
η vt

+
N=
−
1
t
ρ m
log tv
The value of N is minimized when m =

v −1
 t 
ηv t log vtv−1
ρ

. Because m ≤ v t , we obtain the desired bound.

When m = v t this recaptures the bound of Theorem 5.
Figure 13 compares the LLL based two-stage bound from Theorem 7 to the standard two-stage bound
from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1. Although the
LLL based two-stage bound is tighter than the LLL based Godbole et al. bound, even for quite large values
of k the standard two-stage bound is tighter than the LLL based two-stage bound. In practical terms, this
specific LLL based two-stage method does not look very promising, unless the parameters are quite large.

6

Conclusion and open problems

Many concrete algorithms within a two-stage framework for covering array construction have been introduced
and evaluated. The two-stage approach extends the range of parameters for which competitive covering arrays
can be constructed, each meeting an a priori guarantee on its size. Indeed as a consequence a number of
best known covering arrays have been improved upon. Although each of the methods proposed has useful
features, our experimental evaluation suggests that TS hRand, Greedy; 2ρ, Γi and TS hRand, Den; 2ρ.Γi with
Γ ∈ {Cyclic, Frobenius} realize a good trade-off between running time and size of the constructed covering
array.
Improvements in the bounds, or in the algorithms that realize them, are certainly of interest. We
mention some specific directions. Establishing tighter bounds on the coloring based methods of Section 3.3
is a challenging problem. Either better estimates of the chromatic number of the incompatibility graph after
a random first stage, or a first stage designed to limit the chromatic number, could lead to improvements in
the bounds.
In Section 5.1 and 5.2 we explored using a Moser-Tardos type algorithm for the first stage. Although this
is useful for asymptotic bounds, practical improvements appear to be limited. Perhaps a different approach of
27

10000

500

9000

450

8000

400

7000

N − number of rows

N − number of rows

550

350
300
250

SLJ bound
Godbole
LLL−2−stage
2−stage

200
150
100

0

50

100

150

200

250

300

350

400

6000
5000
4000

SLJ bound
Godbole
LLL−2−stage
2−stage

3000
2000

450

k

(a) t = 3, v = 3.

1000
0

1000

2000

3000

4000
k

5000

6000

7000

8000

(b) t = 4, v = 4.

Figure 13: Comparison among the LLL based two-stage bound from Theorem 7, the standard two-stage
bound from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1.
reducing the number of bad events to be avoided explicitly by the algorithm may lead to a better algorithm.
A potential approach may look like following: “Bad” events would denote non-coverage of an interaction
over a t-set of columns. We would select a set of column t-sets such that the dependency graph of the
corresponding bad events have a bounded maximum degree (less than the original dependency graph). We
would devise a Moser-Tardos type algorithm for covering all the interactions on our chosen column t-sets,
and then apply the conditional LLL distribution to obtain an upper bound on the number of uncovered
interactions. However, the difficulty lies in the fact that “all vertices have degree ≤ ρ” is a non-trivial,
“hereditary” property for induced subgraphs, and for such properties finding a maximum induced subgraph
with the property is an NP-hard optimization problem [17]. There is still hope for a randomized or “nibble”
like strategy that may find a reasonably good induced subgraph with a bounded maximum degree. Further
exploration of this idea seems to be a promising research avenue.
In general, one could consider more than two stages. Establishing the benefit (or not) of having more
than two stages is also an interesting open problem. Finally, the application of the methods developed to
mixed covering arrays appears to provide useful techniques for higher strengths; this merits further study as
well.

Acknowledgments
The research was supported in part by the National Science Foundation under Grant No. 1421058.

References
[1] A. N. Aldaco, C. J. Colbourn, and V. R. Syrotiuk. Locating arrays: A new experimental design for
screening complex engineered systems. SIGOPS Oper. Syst. Rev., 49(1):31–40, Jan. 2015.
[2] N. Alon and J. H. Spencer. The probabilistic method. Wiley-Interscience Series in Discrete Mathematics
and Optimization. John Wiley & Sons, Inc., Hoboken, NJ, third edition, 2008. With an appendix on
the life and work of Paul Erdős.
[3] Android. Android configuration class, 2016. http://developer.android.com/reference/android/content/
res/Configuration.html.
[4] J. I. Brown. The complexity of generalized graph colorings. Discrete Appl. Math., 69(3):257–270, 1996.
28

[5] R. C. Bryce and C. J. Colbourn. The density algorithm for pairwise interaction testing. Software
Testing, Verification, and Reliability, 17:159–182, 2007.
[6] R. C. Bryce and C. J. Colbourn. A density-based greedy algorithm for higher strength covering arrays.
Software Testing, Verification, and Reliability, 19:37–53, 2009.
[7] R. C. Bryce and C. J. Colbourn. Expected time to detection of interaction faults. Journal of Combinatorial Mathematics and Combinatorial Computing, 86:87–110, 2013.
[8] J. N. Cawse. Experimental design for combinatorial and high throughput materials development. GE
Global Research Technical Report, 29:769–781, 2002.
[9] M. A. Chateauneuf, C. J. Colbourn, and D. L. Kreher. Covering arrays of strength 3. Des. Codes
Crypt., 16:235–242, 1999.
[10] D. M. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG system: An approach to
testing based on combinatorial design. IEEE Transactions on Software Engineering, 23:437–44, 1997.
[11] M. B. Cohen. Designing test suites for software interaction testing. PhD thesis, The University of
Auckland, Department of Computer Science, 2004.
[12] C. J. Colbourn. Combinatorial aspects of covering arrays. Le Matematiche (Catania), 58:121–167, 2004.
[13] C. J. Colbourn. Covering array tables, 2005-2015. http://www.public.asu.edu/∼ccolbou/src/tabby.
[14] C. J. Colbourn. Covering arrays and hash families. In Information Security and Related Combinatorics,
NATO Peace and Information Security, pages 99–136. IOS Press, 2011.
[15] C. J. Colbourn. Conditional expectation algorithms for covering arrays. Journal of Combinatorial
Mathematics and Combinatorial Computing, 90:97–115, 2014.
[16] R. Diestel. Graph Theory. Graduate Texts in Mathematics. Springer, fourth edition, 2010.
[17] M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NPCompleteness. W. H. Freeman & Co., New York, NY, USA, 1979.
[18] A. P. Godbole, D. E. Skipper, and R. A. Sunley. t-covering arrays: upper bounds and Poisson approximations. Combinatorics, Probability and Computing, 5:105–118, 1996.
[19] B. Haeupler, B. Saha, and A. Srinivasan. New constructive aspects of the Lovász local lemma. J. ACM,
58(6):Art. 28, 28, 2011.
[20] A. S. Hedayat, N. J. A. Sloane, and J. Stufken. Orthogonal Arrays. Springer-Verlag, New York, 1999.
[21] D. S. Johnson. Approximation algorithms for combinatorial problems. J. Comput. System Sci., 9:256–
278, 1974.
[22] G. O. H. Katona. Two applications (for search theory and truth functions) of Sperner type theorems.
Periodica Math., 3:19–26, 1973.
[23] D. Kleitman and J. Spencer. Families of k-independent sets. Discrete Math., 6:255–262, 1973.
[24] D. R. Kuhn, R. Kacker, and Y. Lei. Introduction to Combinatorial Testing. CRC Press, 2013.
[25] D. R. Kuhn and M. Reilly. An investigation of the applicability of design of experiments to software
testing. In Proc. 27th Annual NASA Goddard/IEEE Software Engineering Workshop, pages 91–95, Los
Alamitos, CA, 2002. IEEE.

29

[26] D. R. Kuhn, D. R. Wallace, and A. M. Gallo. Software fault interactions and implications for software
testing. IEEE Trans. Software Engineering, 30:418–421, 2004.
[27] L. Lovász. On the ratio of optimal integral and fractional covers. Discrete Math., 13(4):383–390, 1975.
[28] K. Meagher and B. Stevens. Group construction of covering arrays. J. Combin. Des., 13:70–77, 2005.
[29] R. A. Moser. A constructive proof of the Lovász local lemma. In STOC’09—Proceedings of the 2009
ACM International Symposium on Theory of Computing, pages 343–350. ACM, New York, 2009.
[30] R. A. Moser and G. Tardos. A constructive proof of the general Lovász local lemma. J. ACM, 57(2):Art.
11, 15, 2010.
[31] C. Nie and H. Leung. A survey of combinatorial testing. ACM Computing Surveys, 43(2):#11, 2011.
[32] A. H. Ronneseth and C. J. Colbourn. Merging covering arrays and compressing multiple sequence
alignments. Discrete Appl. Math., 157:2177–2190, 2009.
[33] K. Sarkar and C. J. Colbourn. Upper bounds on the size of covering arrays. ArXiv e-prints.
[34] G. Seroussi and N. H. Bshouty. Vector sets for exhaustive testing of logic circuits. IEEE Trans. Inform.
Theory, 34:513–522, 1988.
[35] S. K. Stein. Two combinatorial covering theorems. J. Combinatorial Theory Ser. A, 16:391–397, 1974.
[36] J. Torres-Jimenez, H. Avila-George, and I. Izquierdo-Marquez. A two-stage algorithm for combinatorial
testing. Optimization Letters, pages 1–13, 2016.

30

