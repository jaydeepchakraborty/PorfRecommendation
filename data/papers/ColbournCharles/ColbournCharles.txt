Augmentation is an operation to increase the number of symbols in a covering array, without unnecessarily increasing the number of rows. For covering arrays of strength two, one type of augmentation forms a covering array on  vv  symbols from one on  vâˆ’1vâˆ’1  symbols together with  vâˆ’1vâˆ’1  covering arrays each on two symbols. A careful analysis of the structure of the optimal binary covering arrays underlies an augmentation operation that reduces the number of rows required. Consequently a number of covering array numbers are improved.Des. Codes Cryptogr. (2015) 77:479â€“491
DOI 10.1007/s10623-015-0084-4

Optimal low-power coding for error correction
and crosstalk avoidance in on-chip data buses
Yeow Meng Chee1 Â· Charles J. Colbourn2 Â·
Alan Chi Hung Ling3 Â· Hui Zhang1 Â· Xiande Zhang1

Received: 30 October 2014 / Revised: 12 April 2015 / Accepted: 16 April 2015 /
Published online: 8 May 2015
Â© Springer Science+Business Media New York 2015

Abstract Coupled switched capacitance causes crosstalk in ultra deep submicron/nanometer
VLSI fabrication, which leads to power dissipation, delay faults, and logical malfunctions. We
present the first memoryless transition bus-encoding technique for power minimization, errorcorrection, and elimination of crosstalk simultaneously. To accomplish this, we generalize
balanced sampling plans avoiding adjacent units, which are widely used in the statistical
design of experiments. Optimal or asymptotically optimal constant weight codes eliminating
each kind of crosstalk are constructed.
Keywords Constant weight codes Â· Packing sampling plan avoiding adjacent units Â·
Crosstalk avoidance Â· Low power code Â· Packing by triples Â· Balanced sampling plan
Mathematics Subject Classification

94B25 Â· 05B40 Â· 05B07 Â· 62K10

This is one of several papers published in Designs, Codes and Cryptography comprising the â€œSpecial Issue
on Cryptography, Codes, Designs and Finite Fields: In Memory of Scott A. Vanstoneâ€.

B

Charles J. Colbourn
charles.colbourn@asu.edu
Yeow Meng Chee
ymchee@ntu.edu.sg
Alan Chi Hung Ling
aling@emba.uvm.edu
Hui Zhang
huizhang@ntu.edu.sg
Xiande Zhang
xiandezhang@ntu.edu.sg

1

Division of Mathematical Sciences, School of Physical and Mathematical Sciences,
Nanyang Technological University, Singapore 637371, Singapore

2

School of Computing, Informatics, and Decision Systems Engineering,
Arizona State University, Tempe, AZ 85287, USA

3

Department of Computer Science, University of Vermont, Burlington, VT 05405, USA

123

480

Y. M. Chee et al.

1 Introduction
The ever-decreasing feature size of VLSI fabrication process has led to many challenges in
VLSI circuit design. One of the most important issues concerns the characteristics of onchip wires [11]. The wiresâ€™ cross-sectional areas and spacings have fallen dramatically with
the move into the ultra deep submicron/nanometer (UDSM) regime. This has increased the
resistance and capacitance of wires. To help reduce resistance, wires today are taller than
they are wide, and they are poised to grow even taller as technology continues to scale.
The resulting growth of side-to-side capacitance between long parallel wires causes coupled
switch capacitance to dominate the wire-to-substrate capacitance in UDSM circuits by several
orders of magnitude [21]. Coupled switched capacitance in turn leads to crosstalks, which
result in power dissipation, delay faults, and logical malfunctions. The problem of eliminating
or minimising crosstalks is considered the biggest signal integrity challenge for long on-chip
buses implemented in UDSM CMOS technology [12].
The worst crosstalk couplings have been classified into four types [6,12], as described in
Table 1. The coupled switched capacitance resulting from type-1, -2, -3, and -4 crosstalks is
in the ratio of 1:2:3:4. Hence, it is particularly important to avoid crosstalks of higher types.
Type-1 crosstalks cannot be avoided in any useful communication channel. However, type-1
crosstalks give rise to power dissipation and must be limited, because low power is a critical
design objective in recent years.
Another factor that has emerged as a new challenge for VLSI circuit designers is UDSM
noise, caused by high-leakage transistors, power-grid fluctuations, ground bounce, IR drops,
clock jitter, and electromagnetic radiation. The effects of such noise are difficult to predict
or prevent. For example, noise in radiation-hardened circuits for satellite communication
systems is random and does not correlate with particular switching patterns on the buses.
A further source of faults is manufacturing defects. In nanotechnology, circuits are manufactured with a significant proportion of faults, and occasional errors may be unavoidable.
Hence, preventive techniques are insufficient, and active error correction is required.
Various researchers have proposed coding techniques to encode data on a bus for crosstalk
avoidance [6,17,28], for low power dissipation [3,15,19,22,26], and for error correction
[1,8]. Coding schemes that simultaneously satisfy two of these three criteria have also been
investigated:
â€¢ crosstalk avoidance and low power dissipation [12,27];
â€¢ crosstalk avoidance and error correction [14]; and
â€¢ low power dissipation and error correction [2,16,18].

Table 1 Types of worst crosstalk couplings
Type-1

Type-2

Type-3

Type-4

0 â†â†’ 1

001 â†â†’ 110
011 â†â†’ 100

001 â†â†’ 010
010 â†â†’ 100
011 â†â†’ 101
101 â†â†’ 110

010 â†â†’ 101

Single wire undergoes
transition. Adjacent
wires maintain previous states

Center wire in opposite
transition to an adjacent wire.
The other wire in same transition as center wire

Center wire in opposite
transition to an adjacent
wire. The other wire maintains previous state

123

All three adjacent
wires undergo opposite transitions

Optimal low-power coding for crosstalk avoidance

481

Despite many efforts, the only families of optimal codes known are those for low power
dissipation [3]. Many of the results on the comparative performance of existing codes are
based on simulations rather than rigorous mathematical analysis.
In this paper, we begin the study of codes for UDSM buses that simultaneously provide
for low power dissipation, crosstalk avoidance, and error correction. In particular, we exhibit
the first infinite families of such codes that are provably optimal.
The paper is organized as follows. Section 2 establishes necessary terminology and gives a
mathematical formulation of the problem of designing low-power codes that avoid crosstalks
and correct errors. In Sect. 3, we present the relation of codes of each type with packing
sampling plans avoiding adjacent units. In Sect. 4, we focus on optimal solutions for k = 3
for all positive integer n. In Sect. 5, the sizes of optimal codes of all types with small lengths
are determined by computer search, and brief conclusion is given.

2 Background
2.1 Coding framework
A coding framework for data buses was introduced by Ramprasad et al. [15]. A bus interconnecting two embedded systems on a systems-on-chip (SoC) platform can be modelled
generically as in Fig. 1. The source encoder (decoder) compresses (decompresses) the input
data so that the number of bits required in the representation of the source is minimised. While
the source encoder removes redundancy, the channel encoder adds redundancy to combat
errors that may arise due to noise in the bus.
Ramprasad et al. [15] considered various combinations of source-channel encoder-decoder
pairs and presented simulation results for their power dissipation. Their approach is what
is known as joint source-channel coding in the information theory literature. Shannonâ€™s
information separation theorem [20] states that reliable transmission can be accomplished
by separate source and channel coding, where the source encoder and decoder need not take
into account the channel statistics and the channel encoder and decoder need not take into
account the source statistics. This applies, however, only for point-to-point transmissions
and for infinite sequence length. The first condition (point-to-point transmission) holds for
a UDSM bus but the second requirement for infinite sequence length is clearly undesirable
for bus coding, because it could give rise to circuits of unbounded delay. Moreover, joint
source-channel coding is useful only when we know the statistics of the source and channel.
In the absence of such statistics, one can only fall back on optimising the source and channel
separately. Indeed, Ramprasad et al. [15] considered coding schemes and simulations on
certain source data with better understood statistics (for example, pop music, classical music,
video, and speech).

noisy channel
source
encoder

channel
encoder

transmitter

channel
decoder

source
decoder

receiver

Fig. 1 Framework for systems-on-chip

123

482

Y. M. Chee et al.

In many systems, the behaviour of source data is hard to predict and so the joint sourcechannel coding approach loses its power. Many researchers have therefore fallen back on
addressing the source coding and channel coding problems separately. This is also the
approach taken in this paper. We focus on designing optimal channel coding schemes for the
scenario where the source statistics are unknown.

2.2 Codes
The Hamming n-space is the set H(n) = {0, 1}n , endowed with the (Hamming) distance
dH (Â·, Â·) defined as follows: for u, v âˆˆ H(n), dH (u, v) is the number of positions where u and
v differ. The (Hamming) weight of a vector u âˆˆ H(n) is the number of positions in u with
nonzero value, and is denoted wH (u). The ith component of u is denoted ui . The support of
a vector u âˆˆ H(n), denoted supp(u), is the set {i : ui = 1}.
A (binary) code of length n is a subset C âŠ† H(n). C is said to be of constant weight w if
wH (u) = w for all u âˆˆ C . The elements of a code are called codewords and the size of a code
is the number of codewords it contains. The support of C is supp(C ) = {supp(u) : u âˆˆ C }. The
minimum distance of C is dmin (C ) = min{dH (u, v) : u, v âˆˆ C and u  = v}. A constant-weight
code of length n, minimum distance d, and weight w is denoted as an (n, d, w) code.
A code that is capable of correcting any occurrence of e or fewer symbol errors is said to
be e-error-correcting. A code C is e-error-correcting if and only if dmin (C ) â‰¥ 2e + 1 [9].

2.3 Set systems and graphs
For integers i < j, the set {i, i + 1, . . . , j} is abbreviated as [i, j]. We further abbreviate
[1, j] to [ j]. For a finite set X and k â‰¤ |X |, we define
 
X
X
2 = {B : B âŠ† X }, and
= {B âŠ† X : |B| = k}.
k
A set system is a pair S = (X, B), where X is a finite set of points and B âŠ† 2 X . The
elements of B are called blocks. The order of S is the number of points, |X |, and the size
 of
S is the number of blocks, |B|. A set system (X, B) is said to be k-uniform if B âŠ† Xk . A
graph is a 2-uniform set system and it is common to refer to the points and blocks of a graph
as vertices and edges, respectively. A path of length n is an alternating sequence of vertices
and edges W = v0 , e1 , v1 , e2 , . . . , en , vn , such that all the vertices vi , i âˆˆ [0, n] and edges
ei , i âˆˆ [n] are all distinct from one another, except possibly the first and last vertices. A cycle
is a path in which the first and last vertices are the same.
Let (X, B) be a set system of order n. The incidence vector of a block B âˆˆ B is the vector
Î¹(B) âˆˆ H(n) such that

1, if i âˆˆ B
Î¹(B)i =
0, otherwise.
There is a natural correspondence between the Hamming n-space and the complete set system
(X, 2 X ): the positions of vectors in H(n) correspond to points in X , a vector u âˆˆ H(n)
corresponds to the block supp(u), and dH (u, v) = |(supp(u)\supp(v))âˆª(supp(v)\supp(u))|.
From this, it follows that there is a bijection between the set of all codes of length n and the
set of all set systems of order n.
An (n, k,
is a k-uniform set system (X, B) with |X | = n such that every

 Î»)-packing
element of X2 is contained in at most Î» blocks of B. Let D(n, k, Î») denote the largest size
among all (n, k, Î»)-packings. The leave graph of (X, B) is the multigraph (X, E), where E

123

Optimal low-power coding for crosstalk avoidance

483

 
contains each e âˆˆ X2 exactly Î» âˆ’ d(e) times, where d(e) is the number of blocks containing
e. When Î» = 1, we omit Î» in the notation; in this case, the leave is a simple graph. When the
leave contains no edges, the packing is a balanced incomplete block design.
The balanced sampling plan avoiding adjacent units (BSA) was introduced to design
sampling plans that exclude contiguous units in statistical experiments [10,25]; for more
recent work, see [7,29]. In statistical applications, in a circular or linear order of the elements,
elements that are â€œcloseâ€ do not appear together, while those more distant all appear the same
number of times together. A (circular) BSAÎ» (n, k; Î±) is an (n, k, Î»)-packing (X, B) with X =
Zn whose leave graph consists of all the edges {i, j} with i âˆ’ j â‰¡ Â±1, . . . , Â±Î± (mod n), and
every other pair appears in Î» blocks. A (linear) LBSAÎ» (n, k; Î±) is an (n, k, Î»)-packing (X, B)
with X = [0, n âˆ’ 1] whose leave graph consists of all the edges {i, j} with 0 â‰¤ i < j < n
for which j âˆ’ i â‰¤ Î±, and every other pair appears in Î» blocks. We employ these only when
Î» = 1, and so omit Î» in the notation.
We generalize circular and linear BSAs (with Î» = 1) to a packing sampling plan avoiding
adjacent units (PSA). A (circular) CPSA(n, k; Î±) is an (n, k)-packing (X, B) with X = Zn
whose leave graph contains all the edges {i, j} with i âˆ’ j â‰¡ Â±1, . . . , Â±Î± (mod n), and every
other pair appears in at most one block. A (linear) LPSA(n, k; Î±) is an (n, k)-packing (X, B)
with X = [0, n âˆ’ 1] whose leave graph contains all the edges {i, j} with 0 â‰¤ i < j < n
for which j âˆ’ i â‰¤ Î±, and every other pair appears in at most one block. (In this case, every
CPSA(n, k; Î±) is an LPSA(n, k; Î±) but the converse need not hold.) Let B(n, k; Î±) denote
the largest size of any LPSA(n, k; Î±); the LPSA is optimal if its size is B(n, k; Î±). Similarly,
let B â—¦ (n, k; Î±) denote the largest size of any CPSA(n, k; Î±); the CPSA is optimal if its size
is B â—¦ (n, k; Î±).

	


	


Let U (n, k; Î±) =

2

Î±âˆ’1
i=0

nâˆ’Î±âˆ’iâˆ’1
kâˆ’1

+(nâˆ’2Î±)

nâˆ’2Î±âˆ’1
kâˆ’1

k

.

Lemma 2.1 B(n, k; Î±) â‰¤ U (n, k; Î±).
Proof For an LPSA(n, k; Î±) constructed
on
	

 [0, n âˆ’ 1], for each i âˆˆ [0, Î± âˆ’ 1], the points i
nâˆ’Î±âˆ’iâˆ’1
and n âˆ’ 1 âˆ’ i appear in at most
blocks, and all the other points appear in at most
kâˆ’1
	


	


Î±âˆ’1 	 nâˆ’Î±âˆ’iâˆ’1 

nâˆ’2Î±âˆ’1
blocks. Then k B(n, k; Î±) â‰¤ 2 i=0
+ (n âˆ’ 2Î±) nâˆ’2Î±âˆ’1
.


kâˆ’1
kâˆ’1
kâˆ’1
When Î± = 1, we omit it in the notation. If there is an (n, k)-packing with leave graph
containing a path of length n âˆ’ 1, we can always relabel the points to get an LPSA(n, k).
 	 

	


Corollary 2.2 B(n, k) â‰¤

2

nâˆ’2
kâˆ’1

+(nâˆ’2)
k

nâˆ’3
kâˆ’1

.

Theorem 4.1 shows that when k = 3, this inequality is tight.

2.4 Problem formulation
Limited weight codes have been widely exploited for the case of on-chip communication
to achieve crosstalk coupling elimination and energy efficiency [12,23]. We consider an nbit parallel bus in a single metal layer, for which we want memoryless codes to weaken
crosstalk, reduce power consumption, and correct errors. We use constant weight codes with
small weight to achieve low power similarly by reducing the node switching activity, that is,
reducing the total number of transitions occurring between the newly arrived data and the
present data on the bus.

123

484

Y. M. Chee et al.

Assume an n-bit bus, consisting of signals b0 , b1 , b2 , . . . , bnâˆ’1 . Consider a group of three
wires in an on-chip bus, which are driven by signals biâˆ’1 , bi and bi+1 . The delay and energy
consumption are primarily affected by transition patterns based on the bus signals biâˆ’1 , bi
and bi+1 as the crosstalk patterns in Table 1.
The selection of codeword does not depend on previous history, so the environment is
memoryless. Consequently coding must address the possibility that any two codewords can
appear one after the other. Therefore to avoid crosstalk and correct errors, we are interested
in constant weight codes of length n, weight w and minimum distance d â‰¥ 3 satisfying the
condition that there do not exist three consecutive coordinates i âˆ’ 1, i, i + 1 such that the
crosstalk couplings of type-2 (or -3, -4) occur in any two different codewords.
We denote such a code avoiding crosstalk of each type as an (n, d, w)-II (or -III, -IV)
code. The maximum size of these codes are denoted as A I I (n, d, w) (or A I I I (n, d, w),
A I V (n, d, w)), and any code achieving this size is optimal. When S âŠ† {I I, I I I, I V }, the
maximum size of a code that is simultaneously an (n, d, w)-S code for each S âˆˆ S is denoted
by AS (n, d, w).
When d = 2w, the following results are straightforward.
Lemma 2.3 For all positive integers n and w,
 
(i) A I I (n, 2w, w) = A I V (n, 2w, w) = wn ;




(ii) A I I I (n, 2w, w) = wn when w  = 1; A I I I (n, 2, 1) = n+1
2 .
 
Proof The quantity s = wn is an upper bound on the size of the desired code in each case.
We construct codes of size s as follows. The code with support
{{i, s + i, 2s + i, . . . , (w âˆ’ 1)s + i} : i âˆˆ [0, s âˆ’ 1]}
is an optimal (n, 2w, w)-II code. The code with support
{{wi, 1 + wi, . . . , (w âˆ’ 1) + wi} : i âˆˆ [0, s âˆ’ 1]}
is an optimal (n, 2w, w)-IV code, and an optimal

(n, 2w, w)-III code when w  = 1. When
w = 1, the code with support {{2i} : i âˆˆ [0, nâˆ’1


2 ]} is an optimal (n, 2, 1)-III code.
Next we show there is close connection between (n, 2k âˆ’ 2, k) codes of each type and
optimal LPSA(n, k)s. Hence, optimal codes are constructed based on the construction of
optimal LPSA(n, k)s.

3 Codes and LPSA(n, k; Î±)s
In this section, we establish connections between optimal LPSA(n, k; Î±)s and the codes of
each type. We begin with optimal (n, 2k âˆ’ 2, k)-II codes for sufficiently large n.
Theorem 3.1 Let k â‰¥ 3. Then A I I (n, 2k âˆ’ 2, k) â‰¥ B(n, k). Further, if B(n, k) = U (n, k)
and n â‰¥ 3k 2 + 2k âˆ’ 3, then A I I (n, 2k âˆ’ 2, k) = B(n, k).
Proof Whenever (X, B) is an LPSA(n, k), the code with support B is an (n, 2kâˆ’2, k)-II code.
Now suppose that (X, B) is an optimal LPSA(n, k) of size U (n, k). We prove that U (n, k)
is the largest possible size of an (n, 2k âˆ’ 2, k)-II code. Assume that D is an (n, 2k âˆ’ 2, k)-II
code of size M. Partition the code into three parts as follows.
The first part A contains all codewords with at least one segment â€œ11â€. Because n > k, for
each codeword in A, there always exist three adjacent coordinates such that â€œ110â€ or â€œ011â€

123

Optimal low-power coding for crosstalk avoidance

485

appears in these coordinates. Let S = {i : âˆƒu âˆˆ A, s.t.,u has â€œ110 in coordinates i âˆ’ 2, i âˆ’
1, i, or â€œ011â€ in coordinates i, i + 1, i + 2}, and let s = |S|. For each i âˆˆ S, there exist at
most two codewords in A that have â€œ110â€ in i âˆ’ 2, i âˆ’ 1, i or â€œ011â€ in i, i + 1, i + 2. Hence
|A| â‰¤ 2s.
The second part T âŠ† D \ A contains all codewords with â€œ1â€ in at least one coordinate in
S. Without loss of generality, if there exists a codeword in A with â€œ110â€ in the coordinates
i âˆ’ 2, i âˆ’ 1, i for some i, then the codewords in T with â€œ1â€ in i must have segment â€œ101â€
in these coordinates to avoid type-2 crosstalk. Because dmin (D) = 2k âˆ’ 2, there is only one
such codeword. So for each i âˆˆ S, there is at most one codeword in T with â€œ1â€ in i. Hence
|T | â‰¤ s.
Finally, let C = D \ (A âˆª T ). Then M = |A| + |T | + |C |. Because each codeword in C
has â€œ0â€ in all coordinates in S, we can shorten C to a code C  by deleting all coordinates in
S. Then C  is an (n âˆ’ s, 2k âˆ’ 2, k) code, and supp(C  ) is an (n âˆ’ s, k)-packing.
The shortening process partitions the coordinates of C  into at most s +1 classes, separated
in C by the coordinates deleted to form C  . No codeword of C  has â€œ11â€ in consecutive
coordinates of any single class. Let x be the number of isolated coordinates in this partition,
and m be the number of classes with at least two coordinates; then x + m â‰¤ s + 1. We now
estimate the 	
size of C
 using the




	 packing.
	
nâˆ’sâˆ’2
nâˆ’sâˆ’3
Let a0 = nâˆ’sâˆ’1
,
a
,
a
. Then we have:
=
=
1
2
kâˆ’1
kâˆ’1
kâˆ’1


x Â· a0 + 2m Â· a1 + (n âˆ’ s âˆ’ 2m âˆ’ x) Â· a2

| C | = |C | â‰¤
.
k
Because x âˆ’ y âˆ’ 1 â‰¤ x âˆ’ y â‰¤ x âˆ’ y, we have:

 	


	

 	


	

â¥
â¢ 	
â¢ x nâˆ’sâˆ’1 âˆ’ nâˆ’sâˆ’3 +2m nâˆ’sâˆ’2 âˆ’ nâˆ’sâˆ’3 + (n âˆ’ s) nâˆ’sâˆ’3 â¥
â¥
â¢
kâˆ’1
kâˆ’1
kâˆ’1
kâˆ’1
kâˆ’1
â¦
M â‰¤ 3s + â£
k



	



	

â¥
â¢ 	
2
1
nâˆ’sâˆ’3 â¥
â¢x
+
1
+
2m
+
1
+
(n
âˆ’
s)
â¢
â¥
kâˆ’1
kâˆ’1
kâˆ’1
â¦
â‰¤ 3s + â£
k
	
	

â¥

â¥
â¢
â¢
â¢ 2(s + 1) + (n âˆ’ s) nâˆ’sâˆ’3 â¥
â¢ 2x + 2m + (n âˆ’ s) nâˆ’sâˆ’3 â¥
â¢
â¢
â¥
â¥
kâˆ’1
kâˆ’1
â¦ â‰¤ 3s + â£
â¦.
â‰¤ 3s + â£
k
k

Let F(s) = 3s +

	
2(s+1)+(nâˆ’s)

Because F(s) = 3s +


	
2(s+2)+(nâˆ’sâˆ’1)
k

nâˆ’sâˆ’4
kâˆ’1




. We claim that because n â‰¥ 3k 2 + 2k âˆ’ 3,

k

U (n, k) â‰¥ maxsâˆˆ[1,n] F(s).


nâˆ’sâˆ’3
kâˆ’1



	
2(s+1)+(nâˆ’s)

, we have

	

nâˆ’sâˆ’3
kâˆ’1

k
nâˆ’sâˆ’4
kâˆ’1

k




âˆ’2






and F(s + 1) = 3(s + 1) +
	




âˆ’3 â‰¤ F(s)âˆ’F(s+1) â‰¤

nâˆ’sâˆ’4
kâˆ’1

+nâˆ’sâˆ’2

k

âˆ’2. Further, we have:




n âˆ’ 2k âˆ’ s
n âˆ’ 3k 2 âˆ’ 1 âˆ’ s
â‰¤ F(s) âˆ’ F(s + 1) â‰¤
.
k(k âˆ’ 1)
kâˆ’1
So when s â‰¤ n âˆ’ 3k 2 âˆ’ 1, F(s) âˆ’ F(s + 1) â‰¥ 0, i.e., F(s) is decreasing; and when
s â‰¥ n âˆ’ 2k, F(s) âˆ’ F(s + 1) â‰¤ 0, i.e., F(s) is increasing. When s âˆˆ [n âˆ’ 3k 2 , n âˆ’ 2k âˆ’ 1],

123

486

Y. M. Chee et al.

F(s) â‰¤ F(1); because the verification is tedious, we omit it here. We therefore only need to
compare F(1) and F(n) to find the maximum value of F(s).
	

â¥
â¢
â¢ 4+(n âˆ’ 1) nâˆ’4 â¥

 

2
â¢
kâˆ’1 â¥
â¦ âˆ’ 3n âˆ’ 2(n +1) â‰¥ (n âˆ’ 1)(n âˆ’ 3k âˆ’ 1) .
F(1) âˆ’ F(n) = 3+ â£
k
k
k(k âˆ’ 1)

Because n â‰¥ 3k 2 + 2k âˆ’ 3 â‰¥ 3k 2 + 1, F(1) â‰¥ F(n) and maxsâˆˆ[1,n] F(s) = F(1).


	


	

â¥
â¢ 	
â¢ 2 nâˆ’2 + (n âˆ’ 2) nâˆ’3 âˆ’ 4 âˆ’ (n âˆ’ 1) nâˆ’4 â¥
â¢ kâˆ’1
kâˆ’1
kâˆ’1 â¥
â¦âˆ’3
U (n, k) âˆ’ F(1) â‰¥ â£
k


	


	

â¥
â¢	
â¢ nâˆ’2 + (n âˆ’ 1) nâˆ’3 âˆ’ 4 âˆ’ (n âˆ’ 1) nâˆ’4 â¥
â¢ kâˆ’1
kâˆ’1
kâˆ’1 â¥
â¦âˆ’3
â‰¥â£
k


â¢	
â¥
â¢ nâˆ’2 âˆ’ 4 â¥


2
â¢ kâˆ’1
â¥
â¦ âˆ’ 3 â‰¥ n âˆ’ 3k âˆ’ 2k + 3 â‰¥ 0.
â‰¥â£
k
k(k âˆ’ 1)

Hence U (n, k) â‰¥ maxsâˆˆ[1,n] F(s).




For (n, 2k âˆ’ 2, k)-III codes and (n, 2k âˆ’ 2, k)-IV codes, we establish lower bounds.
 
Lemma 3.2 1. A I I I (n, 2k âˆ’ 2, k) â‰¥ A I I,I I I,I V (n, 2k âˆ’ 2, k) â‰¥ D( n2 , k).


2. A I I I (n, 4, 3) â‰¥ A I I,I I I,I V (n, 4, 3) â‰¥ B( n2 , 3) +  nâˆ’1
2 .
 
3. A I I I (n, 4, 3) â‰¥ A I I,I I I,I V (n, 4, 3) â‰¥ B â—¦ ( n2 , 3) +  n2 .
n
Proof For
 n the first inequality, take an ( 2 , k)-packing (X,
 B), and construct a code C of
length 2 by taking supp(C ) = B. View C as an |B| Ã— n2 array. When n â‰¡ 1 (mod 2),
we add one column of all zeroes between every two consecutive columns of C , and when
n â‰¡ 0 (mod 2) we add one further column of all zeroes after C to get an (n, 2k âˆ’ 2, k)-III
code. The verification is straightforward, because every second column is all zeroes.
  
The construction for the second is similar. Apply the same inflation to an LPSA n2 , 3
  
of size B n2 , 3 to obtain a code C1 . In every codeword of C1 , two 1s are separated by three
(or more) coordinates, and different codewords cannot have 1s in adjacent coordinates. Now
form code C2 , consisting of all codewords with support {2i, 2i +1, 2i +2} for 0 â‰¤ i <  nâˆ’1
2 .
No prohibited situation arises from 000 or 111 in three consecutive coordinates of a codeword.
In consecutive coordinates in which two codewords of C2 are neither 000 nor 111, the two
codewords contain 011 and 110, which is permitted. So we consider one codeword from
C1 and one from C2 . The coordinates with indices in {2i + 1 : 0 â‰¤ i <  nâˆ’1
2 } appear in
only one codeword, which is {2i, 2i + 1, 2i + 2}. So in the consecutive coordinates in which
two such codewords are neither 000 nor 111, and are not equal, the two codewords contain
{001, 100}, {010, 011}, or {010, 110}. All are permitted.
 n 
â—¦
2 ,3 =
 The
 n bound
 in the third case is equal to that in the second unless n is even and B
B 2 , 3 . When both occur, use a CPSA to form C1 and C2 as in the second case; one further
codeword can be added with support {0, n âˆ’ 2, n âˆ’ 1}.



123

Optimal low-power coding for crosstalk avoidance

487

Lemma 3.3 A I V (n, 2k âˆ’ 2, k) â‰¥ B(n, k).
Proof Take an LPSA(n, k) (X, B) of size B(n, k). Apply to the points in [0, n âˆ’ 1] the
permutation

i â†’ 2i, if i < n/2, and
i â†’ 2i âˆ’ 2n/2 + 1, if i â‰¥ n/2,
to get (X, B ). The code C  with supp(C  ) = B is an (n, 2k âˆ’ 2, k)-IV code.




We give another construction for an (n, 2kâˆ’2, k)-IV code from an optimal LPSA(n, k; kâˆ’
1). When k = 3, this construction gives a better lower bound than Lemma 3.3.
Lemma 3.4 Let k â‰¥ 3.
1. A I I,I V (n, 2k âˆ’ 2, k) â‰¥ B(n, k; k âˆ’ 1),	


2. A I V (n, 2k âˆ’ 2, k) â‰¥ B(n, k; k âˆ’ 1) + nâˆ’1
, and
	kâˆ’1 

n
I
V
â—¦
3. A (n, 2k âˆ’ 2, k) â‰¥ B (n, k; k âˆ’ 1) + kâˆ’1 .


	
Proof Let s = nâˆ’1
kâˆ’1 , and (X, B ) be an LPSA(n, k; k âˆ’ 1) of size B(n, k; k âˆ’ 1). Then the
code C with supp(C ) = {B : B âˆˆ B} is an (n, 2k âˆ’2, k)-II code and an (n, 2k âˆ’2, k)-IV code.
Further, the code C with supp(C ) = {B : B âˆˆ B}âˆª{{(kâˆ’1)i, (kâˆ’1)i +1, . . . , (kâˆ’1)i +kâˆ’1} :
i âˆˆ [0, s âˆ’1]} is an (n, 2k âˆ’2, k)-IV code. When n  â‰¡ 0 (mod k âˆ’ 1), statement (3) is implied
by statement (2). So suppose that n â‰¡ 0 (mod k âˆ’ 1). Using instead a CPSA(n, k; k âˆ’ 1) of
size B â—¦ (n, k; k âˆ’ 1), adjoin the block {(k âˆ’ 1)s, (k âˆ’ 1)s + 1, . . . , (k âˆ’ 1)s + k âˆ’ 2, 0}. 

Lemma 3.5 A I I,I V (n, 4, 3) â‰¤ U (n, 3; 2) when n â‰¥ 13.
Proof Computational results reported in Table 2 show that A I I,I V (13, 4, 3) = U (13, 3; 2) =
16, A I I,I V (14, 4, 3) = U (14, 3; 2) = 20, A I I,I V (15, 4, 3) = U (15, 3; 2) = 25, and
Table 2 Sizes of optimal codes for n â‰¤ 20
n

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

D (n, 3)

1

1

2

4

7

8

12

13

17

20

26

28

35

37

44

48

57

60

B (n, 3)

0

0

1

2

4

6

9

10

14

16

21

24

30

32

39

42

50

54

B â—¦ (n, 3)

0

0

0

2

3

5

9

10

13

16

20

23

30

32

38

42

49

53

B (n, 3; 2)

0

0

0

0

1

2

4

6

9

12

16

20

25

28

34

37

45

48

B â—¦ (n, 3; 2)

0

0

0

0

0

0

3

5

8

12

15

18

25

26

34

36

43

46

A I I .(n, 4, 3)

1

1

2

4

5

6

9

10

14

16

21

24

30

32

39

42

50

54

A I I I (n, 4, 3)

1

1

2

3

4

5

6

7

8

9

10

11

13

14

17

18

19

21

A I V (n, 4, 3)

1

1

2

4

6

7

10

12

15

19

23

26

32

35

42

45

54

57

A I I,I I I (n, 4, 3)

1

1

2

3

3

4

5

6

7

8

10

11

13

13

17

18

19

20

A I I,I V (n, 4, 3)

1

1

2

4

4

4

7

8

12

13

16

20

25

28

34

37

45

48

A I I I,I V (n, 4, 3)

1

1

2

3

4

5

6

7

8

9

10

11

13

14

17

18

19

21

A I I,I I I,I V (n, 4, 3)

1

1

2

3

3

4

5

6

7

8

10

11

13

13

17

18

19

20

Lower bounds and exact values

123

488

Y. M. Chee et al.

A I I,I V (16, 4, 3) = U (16, 3; 2) = 32. Suppose to the contrary that A I I,I V (n, 4, 3) >
U (n, 3; 2) for some n â‰¥ 17, and let n be the smallest such value. When n â‰¥ 17 we have
U (n, 3; 2) â‰¥ U (n âˆ’ 1, 3; 2) + 3 and U (n, 3; 2) â‰¥ U (n âˆ’ 3, 3; 2) + 4. (See Table 2 for small
values.)
Let (X, B) be the support of an (n, 4, 3)-{II,IV} code of size A I I,I V (n, 4, 3). Some triple
of B covers a pair of the form {a, b} âˆˆ {{i, i +1}, {i, i +2}} because it is not an LPSA(n, 3; 2).
Case 1 Some element appears in at most one triple. Suppose that element i appears in no
triple. Shorten the code by deleting coordinate i and delete the triples (if any) containing
pairs {i âˆ’ 1, i + 1}, {i âˆ’ 2, i + 1}, and {i âˆ’ 1, i + 2}. The result is a type II and IV code, so
the given code has at most A I I,I V (n âˆ’ 1, 4, 3) + 3 triples, a contradiction. Suppose now that
element i appears in exactly one triple T . Then if i âˆˆ {0, n âˆ’ 1}, delete coordinate i and triple
T to get a contradiction. If i âˆˆ {1, n âˆ’ 2}, delete coordinate i and delete triple T , along with
triples containing {0, 2} and {0, 3} when i = 1 or {n âˆ’ 4, n âˆ’ 1} and {n âˆ’ 3, n âˆ’ 1}, to get a
contradiction. So 2 â‰¤ i â‰¤ nâˆ’3. If T contains neither i âˆ’1 nor i +1, then no triple contains both
i âˆ’1 and i +1, because the code is type IV. Shorten by deleting coordinate i and delete triple T
and the triples (if any) containing pairs {i âˆ’2, i +1} and {i âˆ’1, i +2}, yielding a contradiction.
Otherwise, without loss of generality T also contains i âˆ’ 1 but does not contain i + 1. But
then if some triple T  contains i âˆ’ 2 and i + 1, it cannot contain i. If T  does not also contain
i âˆ’ 1, then we have T âˆ© {i âˆ’ 1, i, i + 1} = {i âˆ’ 1, i} and T  âˆ© {i âˆ’ 1, i, i + 1} = {i + 1},
which cannot happen in a type II code. So T  = {i âˆ’ 2, i, i + 1}. Hence there are at most
two triples among those containing pairs {i âˆ’ 1, i + 1}, {i âˆ’ 2, i + 1}, and {i âˆ’ 1, i + 2}, so
shorten as before.
Case 2 Some triple T satisfies |T âˆ© {i, i + 1, i + 2}| = 2 for some 0 â‰¤ i â‰¤ n âˆ’ 3. Suppose
that {a, b} = T âˆ© {i, i + 1, i + 2} and let {c} = {i, i + 1, i + 2} \ {a, b}. There can be no triple
containing c but neither a nor b, because the code is type II and type IV. So c is in exactly two
triples, T  that contains a and T  that contains b; only T contains both a and b. Applying the
same argument to T  and T  , a and b each appear in exactly two triples. So there are only
three triples (T , T  , and T  ) that contain a, b, or c. Shorten by deleting coordinate i + 1 and
the triples T , T  , and T  to obtain a contradiction.
Case 3 No triple T satisfies |T âˆ© {i, i + 1, i + 2}| = 2 for any 0 â‰¤ i â‰¤ n âˆ’ 3. If a triple
T satisfies |T âˆ© {i, i + 1, i + 2}| = 3 for some 0 â‰¤ i â‰¤ n âˆ’ 3, equivalently it satisfies
|T âˆ© {i + 1, i + 2, i + 3}| = 2 for some 0 â‰¤ i â‰¤ n âˆ’ 4 or |T âˆ© {i âˆ’ 1, i, i + 1}| = 2 for some
1 â‰¤ i â‰¤ n âˆ’ 3. Apply Case 2. Otherwise every triple T satisfies |T âˆ© {i, i + 1, i + 2}| â‰¤ 1 for
0 â‰¤ i â‰¤ n âˆ’ 3. But then (X, B) is an LPSA(n, 3; 2) and hence we have at most B(n, 3; 2) â‰¤
U (n, 3; 2) triples, the final contradiction.



4 Optimal packing sampling plans
By Corollary 2.2, we have the upper bound:

â§ 2
n âˆ’4n+4
âª
,
âª
6
âª
 




âª
2 âˆ’3n
nâˆ’2
nâˆ’3
â¨
n
2 2 + (n âˆ’ 2) 2
6 ,
U (n, 3) =
= n 2 âˆ’4n
âª
3
âª 6 ,
âª
âª
â© n 2 âˆ’3nâˆ’4 ,
6

Theorem 4.1 B(n, 3) = U (n, 3) for all n â‰¥ 0.

123

if n â‰¡ 2 (mod 6),
if n â‰¡ 3 (mod 6),
if n â‰¡ 0, 4 (mod 6),
if n â‰¡ 1, 5 (mod 6).

Optimal low-power coding for crosstalk avoidance

489

Proof When n â‰¡ 3 (mod 6), Colbourn and Rosa [4] (and Colbourn and Ling [5]) construct
2
a BSA(n, 3) of size n âˆ’3n
6 , which is an optimal LPSA(n, 3). Because each point appears in

(nâˆ’1) âˆ’4(nâˆ’1)+4
blocks, we get an LPSA(n âˆ’ 1, 3) of size n âˆ’3n
âˆ’ nâˆ’3
by removing
6
2 =
6
the point n âˆ’ 1 and all blocks containing it, which is optimal.
When n â‰¡ 1, 5 (mod 6), Colbourn and Rosa [4] showed there exists an (n, 3)-packing of
2
size n âˆ’3n+2
, whose leave graph consists of a cycle of length n âˆ’ 1 and one isolated point.
6
Assume n âˆ’1 is the isolated point. Remove the block {x, n âˆ’2, n âˆ’1} for some x âˆˆ [0, n âˆ’3];
the result is an optimal LPSA(n, 3). Now, n âˆ’ 1 appears in nâˆ’3
2 blocks. Removing n âˆ’ 1
and all blocks containing it from the optimal LPSA(n, 3) constructed above, we obtain an
2
(nâˆ’1)2 âˆ’4(nâˆ’1)
LPSA(n âˆ’ 1, 3) of size n âˆ’3nâˆ’4
âˆ’ nâˆ’3
, which is optimal.


6
2 =
6
2

2

nâˆ’3
2

Theorem 4.2 1. B â—¦ (n, 3) = U (n, 3) when n â‰¡ 0, 3, 4 (mod 6).
2. B â—¦ (n, 3) = U (n, 3) âˆ’ 1 when n â‰¡ 1, 2, 5 (mod 6).
blocks when n â‰¡
Proof The constructions in Theorem 4.1 yield a CPSA(n, 3) with n(nâˆ’3)
6
blocks
when
n
â‰¡
0,
4
(mod
6).
A
CPSA(n,
3)
can have at most
3 (mod 6) and with n(nâˆ’4)
6
 n  nâˆ’3 
blocks,
which
equals
U
(n,
3)
when
n
â‰¡
0,
3,
4
(mod
6),
so
these
are optimal.
3
2
 n  nâˆ’3 
â—¦
When n â‰¡ 2 (mod 6), 3 2
= U (n, 3) âˆ’ 1 so B (n, 3) â‰¤ U (n, 3) âˆ’ 1. When
n â‰¡ 1, 5 (mod 6), if there were a CPSA(n, 3) with U (n, 3) =
n(nâˆ’1)
2

3(n 2 âˆ’3nâˆ’4)

n 2 âˆ’3nâˆ’4
6

codewords, then the

âˆ’
= n + 2. The leave must be an
number of edges in the leave graph is
6
n-cycle with two additional edges, but every vertex in the leave must have even degree, which
cannot occur. So B â—¦ (n, 3) â‰¤ U (n, 3) âˆ’ 1. To establish equality when n â‰¡ 1, 2, 5 (mod 6),
remove the block {0, n âˆ’ 1, x} from an LPSA(n, 3) from Theorem 4.1.


Lemma 4.3 B â—¦ (n, 3; 2) = B(n, 3; 2) = U (n, 3; 2) whenever n â‰¡ 3, 5 (mod 6) and n â‰¥
15. B â—¦ (n, 3; 2) + 2 = B(n, 3; 2) = U (n, 3; 2) whenever n â‰¡ 2, 4 (mod 6) and n â‰¥ 14.
Proof Zhang and Chang [30] establish that whenever n â‰¥ 15 and n â‰¡ 3, 5 (mod 6), there is a
BSA(n, 3; 2) having n(nâˆ’5)
blocks; this is also an optimal CPSA(n, 3; 2) and LPSA(n, 3; 2).
6
Now suppose that n â‰¥ 14 and n â‰¡ 2, 4 (mod 6). When n â‰¡ 2 (mod 6), writing n = 6t + 2,
U (6t + 2, 3; 2) = (2t)(3t âˆ’ 1). Delete element 6t + 2 from a BSA(6t + 3, 3; 2) with
(2t + 1)(3t âˆ’ 1) blocks, removing 3t âˆ’ 1 blocks to obtain an LPSA(6t + 2, 3; 2), which is
therefore optimal. When n â‰¡ 4 (mod 6), writing n = 6t + 4, U (6t + 4, 3; 2) = t (6t + 2).
Delete element 6t + 4 from a BSA(6t + 5, 3; 2) with t (6t + 5) blocks, removing 3t blocks to
obtain an LPSA(6t + 4, 3; 2), which is therefore optimal. Remove the blocks {0, n âˆ’ 2, x},
{1, n âˆ’ 1, y} for some x and y from the optimal LPSA(n, 3; 2) constructed above to obtain
an optimal CPSA(n, 3; 2) when n â‰¡ 2, 4 (mod 6).


	 	



For n = 6t, U (6t, 3) = 6t (t âˆ’ 1) + 1, and 6t3 6tâˆ’5
= 6t (t âˆ’ 1). For n = 6t + 1,
2
 6t+1  6tâˆ’4 
U (6t +1, 3) = t (6t âˆ’3), and 3
= t (6t âˆ’3)âˆ’1. However, if a CPSA(6t +1, 3; 2)
2
were to have t (6t âˆ’ 3) âˆ’ 1 blocks, its leave must have 2(6t + 1) + 1 edges and every such
graph with minimum degree 4 has two vertices of degree 5. Because all vertices in the leave
must have even degree, no CPSA(6t + 1, 3; 2) can exist with more than t (6t âˆ’ 3) âˆ’ 2 blocks.
We provide bounds to apply when n â‰¡ 0, 1 (mod 6).
Lemma 4.4 B(2n, 3; 2) â‰¥ 4B(n, 3), and B(2n + 1, 3; 2) â‰¥ 4B(n, 3) + n âˆ’ 2. In addition,
B â—¦ (2n, 3; 2) â‰¥ 4B â—¦ (n, 3), and B â—¦ (2n + 1, 3; 2) â‰¥ 4B â—¦ (n, 3) + n âˆ’ 3.

123

490

Y. M. Chee et al.

Proof Start with an LPSA(n, 3) on [0, n âˆ’ 1]. We form an LPSA(2n, 3; 2) on [0, 2n âˆ’ 1].
For each block {a, b, c} in the LPSA, form four blocks {{2a + Î±, 2b + Î², 2c + Î³ } : Î±, Î², Î³ âˆˆ
{0, 1}, Î± + Î² + Î³ â‰¡ 0 (mod 2)}. The verification is straightforward. To form an LPSA(2n +
1, 3) on [0, 2n], adjoin {{2i, 2i + 3, 2n} : 0 â‰¤ i â‰¤ n âˆ’ 3}.
The construction for CPSAs is the same, except that one does not adjoin {0, 3, 2n}.



5 Conclusion
Applying Theorem 3.1 with the results in Theorem 4.1, we have optimal (n, 4, 3)-II codes for
all n â‰¥ 30. By computer search (using cliquer [13] and hill-climbing (a variant of [24])),
we determined the sizes of optimal LPSA(n, 3; Î±)s, CPSA(n, 3; Î±)s, and (n, 4, 3) codes of
lengths n â‰¤ 20. The sizes are listed in Table 2 and corresponding optimal codes are available
from the authors; those in slanted font are lower bounds from Theorem 3.1 and Lemma 3.4.
In this paper, we present the first memoryless transition bus-encoding technique for power
minimization, error-correcting and elimination of crosstalk simultaneously. We establish the
connection between codes avoiding crosstalk of each type with packing sampling plans
avoiding adjacent units. Optimal codes of each type are constructed.

References
1. Bertozzi D., Benini L., de Micheli G.: Low power error resilient encoding for on-chip data buses. In:
DATEâ€™02: Proceedings of the Conference on Design. Automation and Test in Europe, pp. 102â€“109. IEEE
Computer Society, Washington, DC (2002).
2. Bertozzi D. Benini L., Ricco B.: Energy-efficient and reliable low-swing signaling for on-chip buses
based on redundant coding. In: ISCASâ€™02: Proceedings of the IEEE International Symposium on Circuits
and Systems, vol. 1, pp. 93â€“96. IEEE Press, Piscataway, NJ (2002).
3. Chee Y.M., Colbourn C.J., Ling A.C.H.: Optimal memoryless encoding for low power off-chip data
buses. In: ICCADâ€™06: Proceedings of the 2006 IEEE/ACM International Conference on Computer-Aided
Design, pp. 369â€“374. ACM Press, New York (2006).
4. Colbourn C.J., Rosa A.: Quadratic leaves of maximal partial triple systems. Graphs Comb. 2(4), 317â€“337
(1986).
5. Colbourn C.J., Ling A.C.H.: A class of partial triple systems with applications in survey sampling.
Commun. Stat. Theory Methods 27(4), 1009â€“1018 (1998).
6. Duan C., Tirumala A., Khatri S.P.: Analysis and avoidance of crosstalk in on-chip buses. In: Hot Interconnectsâ€™01: Proceedings of the 9th Annual Symposium on High-Performance Interconnects, pp. 133â€“138.
IEEE, Piscataway (2001).
7. Dukes P.J., Ling A.C.H.: Existence of balanced sampling plans avoiding cyclic distances. Metrika 70,
131â€“140 (2009).
8. Favalli M., Metra C.: Bus crosstalk fault-detection capabilities of error-detecting codes for on-line testing.
IEEE Trans. Very Large Scale Integr. Syst. 7, 392â€“396 (1999).
9. Hamming R.W.: Error detecting and error correcting codes. Bell Syst. Tech. J. 29(2), 147â€“160 (1950).
10. Hedayat A.S., Rao C.R., Stufken J.: Sampling plans excluding contiguous units. J. Stat. Plan. Inference
19(2), 159â€“170 (1988).
11. Ho R.: On-chip wires: Scaling and efficiency, Ph.D. dissertation, Department of Electrical Engineering,
Stanford University, Palo Alto, CA (2003).
12. Khan Z., Arslan T., Erdogan A.T.: A dual low-power and crosstalk immune encoding scheme for systemon-chip buses. In: PATMOSâ€™04: Proceedings of the 14th International Workshop on Power and Timing
Modeling, Optimization and Simulation. Lecture Notes in Computer Science, vol. 3254, pp. 585â€“592.
Springer, Berlin (2004).
13. Niskanen S., Ã–stergÃ¥rd P.R.J.: Cliquer Userâ€™s Guide, Version 1.0, Communications Laboratory, Helsinki
University of Technology, Espoo. Tech. Rep. T48, (2003).
14. Patel K.N., Markov I.L.: Error-correction and crosstalk avoidance in DSM busses. IEEE Trans. Very
Large Scale Integr. Syst. 12(10), 1076â€“1080 (2004).

123

Optimal low-power coding for crosstalk avoidance

491

15. Ramprasad S., Shanbhag N.R., Hajj I.N.: A coding framework for low-power address and data busses.
IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 7, 212â€“221 (1999).
16. Rossi D., van Dijk E.S., Kleihorst R.P., Nieuwland A.K., Metra C.: Coding scheme for low energy
consumption fault-tolerant bus. In: IOLTWâ€™02: Proceedings of the Eighth IEEE International On-Line
Testing Workshop, pp. 8â€“12. IEEE Computer Society, Washington, DC (2002).
17. Rossi D., Cavallotti S., Metra C.: Error correcting codes for crosstalk effect minimization. In: DFTâ€™03:
Proceedings of the 18th IEEE International Symposium on Defect and Fault-Tolerance in VLSI Systems,
pp. 257â€“266. IEEE Computer Society, Washington, DC (2003).
18. Rossi D., van Dijk E.S., Kleihorst R.P., Nieuwland, A.K., Metra, C.: Power consumption of fault tolerant
codes: the active elements. In IOLTWâ€™03: Proceedings of the Ninth IEEE International On-line Testing
Workshop, pp. 61â€“67. IEEE Computer Society, Washington, DC (2003).
19. Samala N.K., Radhakrishnan D., Izadi B.: A novel deep sub-micron bus coding for low energy. In: ESAâ€™04:
Proceedings of the International Conference on Embedded Systems and Applications, pp. 25â€“30. CSREA
Press, Leuven (2004).
20. Shannon C.E.: A mathematical theory of communications, Bell Syst. Tech. J. 27(3), 379â€“423, 623â€“656
(1948).
21. Sotiriadis P.P., Chandrakasan A.: Bus energy minimization by transition pattern coding (TPC) in deep
sub-micron technologies. In: ICCADâ€™00â€”Proceedings of the 2000 IEEE/ACM International Conference
on Computer-Aided Design, pp. 322â€“327. IEEE, Piscataway, NJ (2000).
22. Stan M.R., Burleson W.P.: Bus-invert coding for low-power I/O. IEEE Trans. Very Large Scale Integr.
Syst. 3(1), 49â€“58 (1995).
23. Stan M.R., Burleson W.P.: Coding a terminated bus for low power. In: Great Lakes Symposium VLSI,
pp. 70â€“73, Buffalo, NY (1995).
24. Stinson D.R.: Hill-climbing algorithms for the construction of combinatorial designs. In: Algorithms in
Combinatorial Design Theory. Annals of Discrete Mathematics, vol. 26, pp. 321â€“334. Elsevier, NorthHolland (1985).
25. Stufken J.: Combinatorial and statistical aspects of sampling plans to avoid the selection of adjacent units.
J. Comb. Inf. Syst. Sci. 18(1â€“2), 149â€“160 (1993).
26. Su C.L., Tsui C.Y., Despain A.M.: Saving power in the control path of embedded processors. IEEE Des.
Comput. 11, 24â€“30 (1994).
27. Subrahmanya P., Manimegalai R., Kamakoti V., Mutyam M.: A bus encoding technique for power and
cross-talk minimization. In: VLSI Design 2004: 17th International Conference on VLSI Design, pp.
443â€“448. IEEE Computer Society, Xiâ€™an (2004).
28. Victor B., Keutzer K.: Bus encoding to prevent crosstalk delay. In: ICCADâ€™01â€”Proceedings of the
2001 IEEE/ACM International Conference on Computer-Aided Design, pp. 57â€“63. IEEE, Piscataway,
NJ (2001).
29. Wright J.H., Stufken J.: New balanced sampling plans excluding adjacent units. J. Stat. Plan. Inference
138, 3326â€“3335 (2008).
30. Zhang J., Chang Y.X.: The spectrum of BSA(v, 3, Î»; Î±) with Î± = 2, 3. J. Comb. Des. 15, 61â€“76 (2007).

123

SIAM J. DISCRETE MATH. Vol. 27, No. 4, pp. 1844­1861

c 2013 Society for Industrial and Applied Mathematics 

SEQUENCE COVERING ARRAYS

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

YEOW MENG CHEE , CHARLES J. COLBOURN , DANIEL HORSLEY§ , AND JUNLING ZHOU¶ Abstract. Sequential processes can encounter faults as a result of improper ordering of subsets of the events. In order to reveal faults caused by the relative ordering of t or fewer of v events, for some fixed t, a test suite must provide tests so that every ordering of every set of t or fewer events is exercised. Such a test suite is equivalent to a sequence covering array, a set of permutations on v events for which every subsequence of t or fewer events arises in at least one of the permutations. Equivalently it is a (different) set of permutations, a completely t-scrambling set of permutations, in which the images of every set of t chosen events include each of the t! possible "patterns." In event sequence testing, minimizing the number of permutations used is the principal objective. By developing a connection with covering arrays, lower bounds on this minimum in terms of the minimum number of rows in covering arrays are obtained. An existing bound on the largest v for which the minimum can equal t! is improved. A conditional expectation algorithm is developed to generate sequence covering arrays whose number of permutations never exceeds a specified logarithmic function of v when t is fixed, and this method is shown to operate in polynomial time. A recursive product construction is established when t = 3 to construct sequence covering arrays on vw events from ones on v and w events. Finally computational results are given for t  {3, 4, 5} to demonstrate the utility of the conditional expectation algorithm and the product construction. Key words. sequence covering array, completely scrambling set of permutations, covering array, directed t-design AMS subject classifications. 05B40, 05B15, 05B30, 05A05 DOI. 10.1137/120894099

1. Introduction. A set of permutations {1 , . . . , N } of a v -element set X is completely t-scrambling if for every ordered t-set (x1 , . . . , xt ) with xi  X for 1  i  t, there is some  (1    N ) for which  (xi ) <  (xj ) if and only if i < j . Spencer [32] first explored the existence of completely t-scrambling sets of permutations in generalizing a question of Dushnik [15] on linear extensions. Recently Kuhn et al. [23, 24] examined an equivalent combinatorial object, the sequence covering array. For parameters N , t, and v , such an array is a set of n permutations of v letters so that every permutation of every t of the v letters appears--in the specified order--in at least one of the n permutations. The motivation for finding sequence covering arrays with small values of n arises in event sequence testing. Suppose that a process involves a sequence of v tasks or events. The operator may, unfortunately, fail to do the tasks in the correct sequence. When this happens, errors may occur. But we anticipate that errors can be attributed to the (improper) ordering of a small
 Received by the editors October 8, 2012; accepted for publication (in revised form) September 4, 2013; published electronically October 28, 2013. http://www.siam.org/journals/sidma/27-4/89409.html  School of Physical and Mathematical Sciences, Nanyang Technological University 637371, Singapore (YMChee@ntu.edu.sg).  School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85257, and State Key Laboratory of Software Development Environment, Beihang University, Beijing 100191, China (colbourn@asu.edu). This author was supported by Australian Research Council grant DP120103067. § School of Mathematical Sciences, Monash University, Melbourne, Australia (danhorsley@ gmail.com). This author was supported by Australian Research Council grants DP120103067 and DE120100040. ¶ Department of Mathematics, Beijing Jiaotong University, Beijing, China (jlzhou@bjtu.edu.cn).

1844

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1845

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

subset of tasks. When each permutation of a sequence covering array is used in turn to specify a task order, every potential ordering of t or fewer tasks will be tried and hence all errors found that result solely from the improper ordering of t or fewer tasks. Applications are discussed further in [19, 23, 39, 40]; related event sequence testing problems in which tasks can be repeated are discussed in [42, 43, 44]. While the application of these combinatorial structures is of much practical concern, our interest is in bounds on the size of sequence covering arrays and their explicit construction. We first state the problem formally. Let  = {0, . . . , v - 1} be symbols that represent the v tasks or events. A t-subsequence of  is a t-tuple (x1 , . . . , xt ) with xi   for 1  i  t, and xi = xj when i = j . A permutation  of  covers the t-subsequence (x1 , . . . , xt ) if  -1 (xi ) <  -1 (xj ) whenever i < j . For example, with v = 5 and t = 3, (4, 0, 3) is a 3-subsequence that is covered by the permutation 4 2 0 3 1. A sequence covering array of order v and strength t, or SeqCA(N ; t, v ), is a set  = {1 , . . . , N }, where i is a permutation of , and every t-subsequence of  is covered by at least one of the permutations {1 , . . . , N }. Often the permutations are written as an N × v array. We use an array representation for completely t-scrambling sets of permutations as well. An N × v array is a completely t-scrambling set of permutations of strength t on v symbols, or CSSP(N ; t, v ), when the columns are indexed by  and the symbols by , and for every way c1 , . . . , ct to choose t distinct columns and every permutation  of {1, . . . , t}, there is a row  for which, for every 1  a < b  t, the entry in cell (, c(a) ) is less than the entry in cell (, c(b) ). Lemma 1.1. A CSSP(N ; t, v ) is equivalent to a SeqCA(N ; t, v ). Proof. If 1 , . . . , N are the N permutations of a SeqCA(N ; t, v ), form an N × v -1 array A in which cell (i, j ) contains i (j ). Then A is a CSSP(N ; t, v ). In the opposite direction, if A is a CSSP(N ; t, v ), define permutation i by setting i (aij ) = j for 1  i  N and 0  j < v . Then 1 , . . . , N form the N permutations of a SeqCA(N ; t, v ). In the CSSP(8;3,5) of Table 1.1, the symbols {1, 2, 3} appear as 123 once, 132 once, 213 once, 231 zero times, 312 four times, and 321 once. Hence the rows of a completely t-scrambling set of permutations do not necessarily produce a sequence covering array; nevertheless they are conjugates, obtained by interchanging the roles of columns and symbols. Permutation problems concerning the avoidance of specified patterns of subsequences have been extensively studied in algebraic and probabilistic combinatorics; see [33] for an excellent survey. (Here two subsequences (x1 , . . . , xt ) and (y1 , . . . , yt ) have the same pattern when, for 1  i < t, xi < xi+1 if and only if yi < yi+1 .)
Table 1.1 Example: SeqCA(8;3,5) ­ t = 3, v = 5, N = 8. SeqCA 4203 1430 3120 0241 2134 0341 3021 4120 CSSP 2413 3042 3120 0314 4102 0341 1320 3124

1 2 4 3 0 2 4 3

0 1 4 2 3 2 4 0

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1846

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

While cosmetically similar to pattern avoidance problems, the existence problem for sequence covering arrays requires coverage rather than avoidance and requires that all subsequences be covered and not simply every pattern. The question of principal concern in this paper is as follows: Given t and v , what is the smallest N for which a CSSP(N ; t, v ) (equivalently, a SeqCA(N ; t, v )) exists? Call this number SeqCAN(t, v ). In the vernacular of completely t-scrambling sets of permutations, Spencer [32] did the foundational work, and F¨ uredi [17], Ishigami [20, 21], Radhakrishnan [31], and Tarui [36] made improvements. In a sequence covering array, every t symbols must appear in each of the t! possible orderings, and there are v t t! t-subsequences in total, so t!  SeqCAN(t, v )  v t! t

Both bounds are trivial, but the lower bound is the correct one when t = 2. Lemma 1.2. SeqCAN(2, v ) = 2 for all v  2. Proof. Any permutation on v symbols and its reversal form a SeqCA(2; 2, v ). When t  3, neither bound is correct as v increases. Indeed the growth as a function of v for fixed t is logarithmic. Theorem 1.3 (see [31, 32]). For t  3, 1+ 2 log2 (e) (t - 1)! v 2v - t + 1 log2 (v - t + 2)  SeqCAN(t, v )  t log(v ) log
t! t!-1

.

The question of when SeqCAN(t, v ) = t! is of independent interest in yet another setting. Let V be a finite set; an element of V is a vertex. A transitive tournament on V is a directed graph in which (1) for all x  V , (x, x) is not an arc; (2) for distinct x, y  V , (x, y ) is an arc if and only if (y, x) is not an arc; and (3) whenever (x, y ) and (y, z ) are arcs, so is (x, z ). A transitive tournament T = (V, A) has transitive tournament T  = (W, B ) as a subdigraph, denoted T   T , whenever W  V and B  A. Let (V, T ) be a finite set V of cardinality v and a collection T with every T  T being a transitive tournament on k of the vertices in V ; members of T are blocks. Then (V, T ) is a (t, )-directed packing of blocksize k and order v , or DP (t, k, v ), if for every X  V with |X | = t and every transitive tournament T  on vertex set X , |{T  T : T   T }|  . On the other hand, (V, T ) is a (t, )-directed covering of blocksize k and order v , or DC (t, k, v )), if for every X  V with |X | = t and every transitive tournament T  on vertex set X , |{T  T : T   T }|  . When (V, T ) is a DP (t, k, v ) and also a DC (t, k, v ), it is a (t, )-directed design of blocksize k and order v , or DD (t, k, v ). In this notation, the subscript is often omitted when  = 1. Directed designs with t = 2 have been extensively studied as generalizations of balanced incomplete block designs. The study of (t, 1)-directed packings has also been extensive as a result of their equivalence to "deletion-correcting codes" (see Levenshtein [25]). The connection with our investigation follows.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1847

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Lemma 1.4. A CSSP(N ; t, v ) is equivalent to a DC(t, v, v ) with N blocks. Moreover, a DD(t, v, v ) exists if and only if SeqCAN(t, v ) = t!. Proof. For each row (a0 , . . . , av-1 ) of the CSSP, form a transitive tournament on vertex set {0, . . . , v - 1} by including, for 0  i < j < v , arc (i, j ) if ai < aj and arc (j, i) otherwise. Each transitive tournament on t of these vertices is a subdigraph of at least one of these N tournaments. The other direction is similar. When N = t!, every t-subsequence is covered exactly once, and every transitive tournament of order t arises as a subdigraph exactly once. Theorem 1.5. Sufficient conditions for a DD(t, v, v ) to exist include 1. t  3 and t  v  t + 1 [25] and 2. t = 4 and v = 6 [28]. Necessary conditions for a DD(t, v, v ) to exist include 1. v  t + 1 for t  {3, 5, 6} [28], 2. v  t + 2 for t = 4 [28], and - 1 for t  7 [28]. 3. v  t+1 2 Levenshtein [25] had conjectured that v  t + 1 whenever a DD(t, v, v ) exists for t  3. As stated in Theorem 1.5, this does not hold for t = 4, but this is the only known exception to Levenshtein's conjecture. In the next section we significantly reduce the upper bound on the largest v for which SeqCAN(t, v ) can equal t!. 2. Lower bounds. Here we extend a technique used in [17, Theorem 5.1], improving on a method of Ishigami [21]. We require a number of previous results on covering arrays, introduced next. See [8] for a more thorough introduction to them. Let N , k , t, and v be positive integers. Let C be an N × k array with entries from an alphabet  of size v ; we typically take  = {0, . . . , v - 1}. When (1 , . . . , t ) is a t-tuple with i   for 1  i  t, (c1 , . . . , ct ) is a tuple of t column indices (ci  {1, . . . , k }), and ci = cj whenever i = j , the t-tuple {(ci , i ) : 1  i  t} is a t-way interaction. The array covers the t-way interaction {(ci , i ) : 1  i  t} if, in at least one row  of C, the entry in row  and column ci is i for 1  i  t. Array C is a covering array CA(N ; t, k, v ) of strength t if it covers every t-way interaction. CAN(t, k, v ) is the minimum N for which a CA(N ; t, k, v ) exists. The basic goal is to minimize the number of rows (tests) required and hence to determine CAN(t, k, v ). When t  2 and v  2 are both fixed, CAN(t, k, v ) is (log k ) (see, for example, [8]). We strengthen this standard definition somewhat. For a t-way interaction T = {(ci , i ) : 1  i  t} with symbols chosen from  = {0, . . . , v - 1}, let  (T ) = -1 |{i : i =  }|. Then define µ(T ) = v =0  (T )!. The natural interpretation is that µ(T ) is the number of ways to permute the columns (c1 , . . . , ct ) so that the symbols appear in the same order. A covering array provides excess coverage when every tway interaction T is covered by at least µ(T ) rows; such a covering array is denoted by CAX (N ; t, k, v ). More generally, the subscript X is used to extend notation from covering arrays to those having excess coverage. Theorem 2.1. Let v , t, and a be integers satisfying v  t  3 and t  a  0. Then SeqCAN(t, v )  a!CANX (t - a, v - a, a + 1). Proof. Let S be a CSSP(N ; t, v ). Choose any a columns of S, {e1 , . . . , ea }. For each ordering  of these columns, form a matrix C that contains all rows of S in which the entry in column  (ei ) is less than that in column  (ei+1 ) for 1  i < a. Because there are a! orderings and every row of S appears in exactly one of the {C }, it suffices to show that for every choice of  the number n of rows in C is at least CANX (t - a, v - a, a + 1). To do this, form an n × (v - a) array A whose columns are the columns of C that are not among the a selected. To determine the content of

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1848

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

cell (r, c) of A , examine the symbol  in column c and row r of C . If  is less than the symbol in row r and column  (e1 ), the entry is set to 0. If  is greater than the symbol in row r and column  (ea ), the entry is set to a. Otherwise find the unique j for which  is greater than the symbol in row r and column  (ej ) but less than the symbol in row r and column  (ej +1 ), and set the entry to j . Now we claim that A is a CAX (n; t - a, v - a, a + 1). The verification requires demonstrating that every (t - a)-way interaction T is covered at least µ(T ) times. So let T = {(fi , i ) : 1  i  t - a}, noting that i  {0, . . . , a} and fi indexes a column of A. We form permutations of {e1 , . . . , ea }  {f1 , . . . , ft-a } that are consistent with  on {e1 , . . . , ea } in that these columns appear in the order prescribed by  . To do this, there are 0 (T ) columns with entry 0; place one of the 0 (T )! orderings of these columns so that all appear before  (e1 ). There are a (T ) columns with entry a; place one of the a (T )! orderings of these columns so that all appear after  (ea ). For 1  j < a, there are j (T ) columns with entry j ; place one of the j (T )! orderings of these columns so that all appear before  (ej ) and after  (ej +1 ). In this way we can form µ(T ) permutations of {e1 , . . . , ea }  {f1 , . . . , ft-a }, each consistent with  . Because each is consistent with  , it appears in C . But each such appearance in C results in a different row of A that covers T , and hence T is indeed covered at least µ(T ) times. The easiest applications of Theorem 2.1 result from using CAN(t, k, v ) as a lower bound for CANX (t, k, v ). Apply it with a = t - 1, noting that CAN(1, k, t) = t for all k  1, to recover the trivial lower bound that SeqCAN(t, v )  t!. Apply it with a = t - 2 to establish that SeqCAN(t, v )  (t - 2)!CAN(2, v - t + 2, t - 1). Now we return to the question of when SeqCAN(t, v ) can equal t!, or equivalently when a DD(t, v, v ) can exist. Theorem 2.1 ensures that SeqCAN(t, v )  (t - 2)!CANX (2, v - t + 2, t - 1), so SeqCAN(t, v ) = t! can hold only when CANX (2, v - t + 2, t - 1)  t(t - 1). The 2-way interaction T = {(c1 , 1 ), (c2 , 2 )} has µ(T ) = 2 exactly when 1 = 2 (called a constant pair) and has µ(T ) = 1 otherwise (a nonconstant pair). Because, for each pair of columns, t - 1 constant pairs must be covered twice each, and (t - 1)(t - 2) nonconstant pairs must be covered at least once each, CANX (2, v - t + 2, t - 1)  t(t - 1). So we are concerned with when equality can hold. Lemma 2.2. For v  4, CANX (2, k, v ) = v (v + 1) only if k  v + 2. Proof. Suppose that a CAX (v (v + 1); 2, k, v ) exists with columns indexed by {1, . . . , k } and symbols by {0, . . . , v - 1}. We form sets on symbols V = ({1, . . . , k } × {0, . . . , v - 1})  {}. The system of sets (blocks) B is formed as follows. For every row (x1 , . . . , xk ) of the covering array, a set {(i, xi ) : 1  i  k } is placed in B . Then for every 1  i  k , a set {(i, j ) : 0  j < v }  {} is placed in B . The set system (V, B ) has kv + 1 symbols and v (v + 1) + k blocks. By construction, every two different symbols appear together in exactly one block, unless the pair is of the form {(i, j ), (i , j )} corresponding to a constant pair and therefore occurring in exactly two blocks. Now form a (kv + 1) × (v (v + 1) + k ) matrix A, which is the symbol-block incidence matrix, as follows. Rows are indexed by symbols, columns by blocks. The matrix contains the entry 1 in row r and column c when symbol r appears in block c and 0 otherwise. Now examine B = AAT , which has rows and columns indexed by V . Its diagonal entries are k in entry (, ) and v + 2 elsewhere. Its off-diagonal entries are 2 in cells indexed by ((i, j ), (i , j )) with i = i and 1 otherwise. The rank of B cannot exceed the number of columns in A, namely, v (v + 1) + k . So in order to bound k , we bound the rank of B.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1849

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Write D = B - J. The rows indexed by V \ {} can be partitioned into v parts; a part is formed by including all rows and columns with indices {(i, j ) : 1  i  k } for some j with 0  j < v . Then D can be written as a block diagonal matrix with v k × k block matrices each equal to X = v I + J and a single 1 × 1 block matrix with entry k - 1. Now det(D) = (k - 1)(det(X))v , and det(X) = v k (1 + k v ) by the matrix determinant lemma. Hence rank(D) = kv + 1. Because B is obtained from D by a rank one update, rank(B)  kv . v +1) Consequently, kv  v (v + 1) + k , or k  v( v -1 . Thus k  v + 2 when v  4, because k is an integer. This enables us to establish a substantial improvement on the bound of Mathon and Tran Van Trung [28] stated in Theorem 1.5. Theorem 2.3. If t  3 and SeqCAN(t, v ) = t! (or equivalently, a DD(t, v, v ) exists), then v  2t - 1. Proof. If 3  t  6, this follows from Theorem 1.5. By Theorem 2.1, t! = SeqCAN(t, v )  (t - 2)!CANX (2, v - t + 2, t - 1) and thus CANX (2, v - t + 2, t - 1) = t(t - 1). By Lemma 2.2, v - t - 2  t - 1 + 2 and hence v  2t - 1 as required. It appears plausible that the bound should be t +2 rather than 2t - 1; nevertheless, the method here gives the first bound that is linear in t. 3. Upper bounds from probabilistic methods. Spencer [32] analyzed a method that selects a set of N permutations on v symbols uniformly at random; we explore this first. v! t! Lemma 3.1. For fixed t  3, SeqCAN(t, v )  1 + (log( (v- t)! ))/(log( t!-1 )). Proof. A permutation of {0, . . . , v - 1} chosen uniformly at random covers any 1 specific t-subsequence with probability t ! and so fails to cover it with probability t!-1 . Then N permutations of { 0 , . . . , v - 1} chosen uniformly at random and int!
-1 dependently together fail to cover a specific t-subsequence with probability t!t . ! v! There are (v-t)! t-subsequences. When N permutations are chosen, each subsequence N t!-1 N . t!

is not covered with probability
v! (v -t)!

Thus the expected number of uncovered 1, a SeqCA(N ; t, v ) must

t-subsequences is

t!-1 N . t!

exist. This holds whenever N >

t!-1 N < t! v! t! (log( (v-t)! ))/(log( t!-1 )).

When

v! (v -t)!

3.1. One permutation at a time. Lemma 3.1 provides a useful upper bound on the size of completely t-scrambling sets of permutations but does not provide an effective method to find such arrays. Stein [34], Lov´ asz [26], and Johnson [22] develop a general strategy for finding solutions to covering problems; this algorithm has been shown to lead to polynomial time methods in many combinatorial covering problems [3, 4, 7, 9, 10]. We extend that strategy here to treat sequence covering arrays. The basic approach is greedy. Repeatedly select one permutation to add that covers a large number of as-yet-uncovered t-subsequences, until all are covered. Stein [34], Lov´ asz [26], and Johnson [22] each suggest selecting to maximize the number of newly covered elements, but their analyses require only that the next selection cover at least the average. If after i permutations are selected there remain Ui uncovered t-subsequences, then a permutation selected uniformly at random is expected to cover 1 Ui t ! t-subsequences for the first time. Provided that we select the (i +1)st permutation 1 t!-1 to cover at least Ui t ! t-subsequences for the first time, we have that Ui+1  Ui t! . v! v! t!-1 i Because U0 = (v-t)! , we have that Ui  (v-t)! ( t! ) . Choose N to be the smallest

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1850

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

value for which UN < 1; then there must be a sequence covering array with N permutations. This simply restates the argument of Lemma 3.1, but with two important improvements. It derandomizes the method by ensuring that appropriate selection of each permutation guarantees that the bound is met, rather than asserting the existence of a set of permutations that meets it. More importantly, the time to construct the sequence covering array is polynomial in the number of permutations and the time to select a permutation that covers at least the average. For fixed t the number of permutations is logarithmic in v , and so an algorithm with running time polynomial in v will result if we can select the next permutation in time polynomial in v . Because the details are quite similar to earlier approaches, we merely outline how this can be done. Suppose that U consists of the as-yet-uncovered t-subsequences. For U  U , let cov( , U ) = 1 when  covers U , and cov( , U ) = 0 otherwise. Let R be an rsubsequence; the r symbols in R are fixed, and the remaining v - r are free. There ! is a set PR of v r ! permutations that cover R. We focus on the expected number of members of U that are covered by a member of PR chosen uniformly at random; this is ec(R) = r! v! cov( , U ).
 PR U U

The strategy is to find a sequence of subsequences P0 , . . . , Pv , so that Pi is an i-subsequence, Pi+1 covers Pi for 0  i < v , symbol i is free in Pi but fixed in Pi+1 , 1 and ec(Pi+1 )  ec(Pi ) for 0  i < v . Because ec(P0 ) = t ! |U|, it follows that Pv is a 1 permutation that covers at least t! |U| of the as-yet-uncovered t-subsequences. Given a selection of Pi , there are precisely i + 1 candidates {C1 , . . . , Ci+1 } for Pi+1 obtained by placing symbol i in one of the i + 1 positions of Pi . Our task is to choose one for which ec(Cj )  ec(Pi ), in order to set Pi+1 = Cj . A naive computation of ec(Cj ) would enumerate members of U and of PCj , but the latter may have size exponential in v . Instead, for U  U let ec(U, R) = r!  PR cov( , U ), and observe that v! ec(R) =
U U

ec(U, R).

When t is fixed, U contains fewer than v t subsequences, which is polynomial in v . Therefore it suffices to compute ec(U, R) efficiently, given a t-subsequence U and an r-subsequence R. Let  be the number of symbols appearing in both U and R. When the  symbols in common do not appear in the same order in U and R, ec(U, R) = 0. Otherwise let T be the  -subsequence that they have in common. Then ! ec(U, R) = ec(U, T ) =  t! . i+1 1 The key observation in selecting Pi+1 is that ec(Pi ) = i+1 j =1 ec(Cj ). Computing ec(Cj ) for 1  j  i + 1, and selecting Pi+1 to be the one that maximizes ec(Cj ), we are then sure that ec(Pi+1 )  ec(Pi ). Combining all of these arguments, we have established the next theorem. Theorem 3.2. For fixed t and input v , there is an algorithm to construct a v! t! SeqCA(N ; t, v ) having N  1 + (log( (v- t)! ))/(log( t!-1 )) permutations in time that is polynomial in v . This algorithm can be easily implemented, and we report results from it in section 5. One immediate improvement results from observing that the counts of

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1851

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

as-yet-uncovered t-subsequences (R0 , R1 , . . . , RN ) must be integers. Hence we have -1 that Ri+1  Ri t!t ! . In specific cases this improves on the bound, without the need to construct the sequence covering array. If, however, the sequence covering array is explicitly constructed, at each selection of Pi+1 from Pi , we can choose Pi+1 to maximize ec() among the i + 1 candidates. 3.2. Greedy methods with reversals. The methods developed are greedy in that they attempt to cover the largest number of as-yet-uncovered t-subsequences. The very first permutation chosen is arbitrary; all are equally effective at coverage. Once one is selected, however, there is a genuine choice for the second. Greedy selection indicates that we should choose one that covers no t-subsequence already covered by the first. Indeed when t = 2, choosing the reversal of this first covers all remaining t-subsequences. For t  3, suppose that we have chosen 2s permutations 1 , . . . , 2s , and suppose further that 2i is the reverse of 2i-1 for 1  i  s. It follows that the number of as-yet-uncovered t-subsequences covered by a permutation  is precisely the same as the number covered by the reverse of  . Yet  and its reverse never cover the same t-subsequence. Hence if the algorithm were to select  next, the reverse of  remains an equally beneficial choice immediately thereafter. Therefore a useful variant of the algorithm developed, after adding a permutation  to the array, always adds the reverse of  as well. Pursuing this, we obtain the following. Theorem 3.3. For fixed t and input v , there is an algorithm to construct a v! t! SeqCA(N ; t, v ) having N  2(log( (v- t)! ))/(log( t!-2 )) permutations in time that is polynomial in v . Naturally, we could again obtain small improvements in practice because every count of as-yet-uncovered t-subsequences is an integer. In principle, always including reversals improves slightly on the bound (that is, Theorem 3.3 improves on Theorem 3.2). Whether this is a practical improvement remains to be seen; we return to this point. 4. Product constructions. Product (or "cut-and-paste" or "Roux-type") constructions are well studied for covering arrays; see, for example, [11, 6]. We develop a product construction for completely 3-scrambling sets of permutations. To do this, we first introduce an auxiliary property. A signing of a CSSP(N ; t, v ) A = (aij ) is an N × v matrix S = (sij ) with entries {, }. A CSSP(N ; t, v ) A is properly signed by an N × v matrix S with entries {, }. When for every set of t - 1 distinct columns c1 , . . . , ct-1 , each sign s  {, }, and every permutation  of 1, . . . , t - 1, there exists a row  of A for which, for every 1  a < b < t, the entry in cell (, c(a) ) is less than the entry in row (, c(b) ), and the sign s,c1 = s. A properly signed CSSP(7;3,5) is shown in Table 4.1. We defer for the moment the question of how to sign a completely t-scrambling set of permutations. 4.1. Products for strength three. Theorem 4.1. If a properly signed CSSP(N ; 3, v ) and a properly signed CSSP(M ; 3, w) both exist, so does a properly signed CSSP(N + M ; 3, vw). Proof. Let A = (aij ) be a CSSP(N ; 3, v ) having sign matrix S = (sij ) with columns indexed by {0, . . . , v - 1}. Let B = (bij ) be a CSSP(M ; 3, w) having sign matrix T = (tij ) with columns indexed by {0, . . . , w - 1}. Form an array C = (c,(i,j ) ) on N + M rows and vw columns with columns indexed by {0, . . . , v - 1} × {0, . . . , w - 1}. In row  for 1    N , in column (i, j ), place the entry ai w + j if si =, ai w + (w - 1 - j )

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1852

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Table 4.1 Properly signed CSSP(7; 3, 5) ­ t = 3, v = 5, N = 7. All signs not specified can be selected arbitrarily.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

0 1 2 2 2 4 4

4 2 0 1 3 1 3

2 3 4 0 0 2 2 (7;3,5)

3 0 3 3 4 3 0

1 4 1 4 1 0 1

if si =. In row N +  for 1    M , in column (i, j ), place the entry bj v + i if tj =, bj v + (v - 1 - i) if tj =. To show that C is a CSSP(N + M ; 3, vw), we must establish that every 3subsequence is covered. Consider three columns (i1 , j1 ), (i2 , j2 ), (i3 , j3 ), in this order. If i1 , i2 , and i3 are all distinct, there is a row  of A in which ai1 < ai2 < ai3 . Then in C row  has the three specified columns in the chosen order. By the same token, if j1 , j2 , and j3 are all distinct, there is a row  of B in which bj1 < bj2 < bj3 . Then in C row  + N has the three specified columns in the chosen order. If {i1 , i2 , i3 } contains only one element, {j1 , j2 , j3 } contains three distinct elements; symmetrically, if {j1 , j2 , j3 } contains only one element, {i1 , i2 , i3 } contains three distinct elements. So it remains only to treat cases in which both {i1 , i2 , i3 } and {j1 , j2 , j3 } contain two distinct elements. Now suppose that the three columns are {(i1 , j1 ), (i1 , j2 ), (i2 , j1 )}; we are concerned with the six orderings of the elements {c(i1 ,j1 ) , c(i1 ,j2 ) , c(i2 ,j1 ) }, which we represent by giving the indices in the sorted order for {c(i1 ,j1 ) , c(i1 ,j2 ) , c(i2 ,j1 ) }, as shown next. j2 j1 i1 c(i1 ,j1 ) c(i1 ,j2 ) 1 2 1 3 2 1 2 3 3 1 3 2 i2 c(i2 ,j1 ) 3 2 3 1 2 1 Table 4.2 gives the rows in which each of the six orderings is covered; we use sgn(x - y ) to be  if x < y ,  if x > y . Two of the orderings are covered at least twice. To sign C properly, assign  to each entry in each of the first N rows and  to each entry in each of the last M rows. A small example, combining two CSSP(6;3,4)s to form a CSSP(12;3,16), is shown in Table 4.3. Use the strategy in the proof of Theorem 4.1, taking B and T from   0 1  0 1     1  0  , 1 0

to establish the next theorem. Theorem 4.2. If a properly signed CSSP(N ; 3, v ) exists, so does a properly signed CSSP(N + 4; 3, 2v ).

4.2. Signing a completely t -scrambling set of permutations. We first give one technique for signing that applies for all strengths.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS Table 4.2 Verification for six orderings.

1853

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Row in C     +N +N +N +N

Condition on  ai1 < ai2 ai1 < ai2 ai1 > ai2 ai1 > ai2 bj1 < bj2 bj1 < bj2 bj1 > bj2 bj1 > bj2

Sign condition si1 = sgn(j2 - j1 ) si1 = sgn(j1 - j2 ) si1 = sgn(j2 - j1 ) si1 = sgn(j1 - j2 ) tj1 = sgn(i2 - i1 ) tj1 = sgn(i1 - i2 ) tj1 = sgn(i2 - i1 ) tj1 = sgn(i1 - i2 )

Ordering 1 3 2 3 2 1 3 1 1 2 2 1 2 3 3 2 2 1 3 2 3 3 1 1

Table 4.3 Example: Properly signed CSSP(6; 3, 4) and its product with itself, a CSSP(12; 3, 16). 3 4 4 12 4 15 3 4 4 12 4 15 2 5 5 13 5 14 4 3 15 4 12 4 1 6 6 14 6 13 8 15 3 0 8 8 0 7 7 15 7 12 15 8 8 8 0 3 4 3 15 4 12 4 2 5 5 13 5 14 5 2 14 5 13 5 5 2 14 5 13 5 6 1 13 6 14 6 9 14 2 1 9 9 7 0 12 7 15 7 14 9 9 9 1 2 8 15 3 0 8 8 1 6 6 14 6 13 9 14 2 1 9 9 6 1 13 6 14 6 10 13 1 2 10 10 10 13 1 2 10 10 11 12 0 3 11 11 13 10 10 10 2 1 15 8 8 8 0 3 0 7 7 15 7 12 14 9 9 9 1 2 7 0 12 7 15 7 13 10 10 10 2 1 11 12 0 3 11 11 12 11 11 11 3 0 12 11 11 11 3 0

0 1 1 3 1 3

1 0 3 1 3 1

2 3 0 0 2 2

3 2 2 2 0 0

Lemma 4.3. Whenever a CSSP(N ; t, v ) exists, a properly signed CSSP(N ; t, v - 1) exists. Proof. Let A = (aij ) be a CSSP(N ; t, v ). Form an N × (v - 1) array S = (sij ) with sij = sgn(aij - ai,v-1 ). Form an N × (v - 1) array B = (bij ) with bij = aij if aij < ai,v-1 and bij = aij - 1 otherwise. Then B is a CSSP(N ; t, v - 1) that is properly signed by S. Let A be a CSSP(N ; t, v ) and A1 , A2 be arrays that partition the rows of A. When, for i = 1, 2, Ai is a CSSP(Ni ; t - 1, v ), A is a partitionable CSSP. Lemma 4.4. Whenever a partitionable CSSP(N ; t, v ) exists, a properly signed CSSP(N ; t, v ) exists. Proof. Let A be a CSSP(N ; t, v ) with partition A1 , A2 . Assign sign  to every entry of A1 and  to every entry of A2 . Corollary 4.5. Whenever a CSSP(N ; 3, v ) contains a row and its reverse, it is partitionable and hence can be properly signed. Proof. Place the row and its reverse in A1 and all other rows in A2 . Then A1 is a CSSP(N ; 2, v ). Moreover, A2 is a CSSP(N ; 2, v ) because for every i, j  {0, . . . , v - 1} with i = j , in A there are at least three rows in which the entry in column i is less than that in column j . Then A is partitionable, so apply Lemma 4.4.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1854

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Lemma 4.4 provides a sufficient condition for a CSSP(N ; 3, v ) to have a proper signing but considers only signings in which all entries in each row receive the same sign. Column c is properly signed when, for every set of t - 1 distinct columns c1 , c2 , . . . , ct-1 with c1 = c, each sign s  {+, -}, and every permutation  of 1, . . . , t - 1, there exists a row  of A for which, for every 1  a < b < t, the entry in cell (, c(a) ) is less than the entry in row (, c(b) ), and the sign s,c = s. Properly signing the N × v array A is equivalent to properly signing each column of A; the important fact is that signs assigned in one column are unrelated to signs in any other, and so one can (hope to) proceed by signing each column separately. Consider the case of strength t = 3. What does it mean to properly sign a specific column c? For every column i other than c we form two sets: Ai contains the row indices in which the entry in column i is larger than that in column c, and Bi (= {1, . . . , N }\ Ai ) contains the row indices in which the entry in column i is smaller than that in column c. We can consider these sets as the edges of a hypergraph H on vertex set {1, . . . , N }. Then H has 2v - 2 edges each containing at least three vertices and a proper 2-coloring of H corresponds to a proper signing of c. Lemma 4.4 and Corollary 4.5 give proper 2-colorings. In all examples that we have examined, each column can be properly signed by finding a suitable 2-coloring. Hence it is plausible that every CSSP(N ; 3, v ) can be properly signed, but if this is true the proof is elusive at the moment. 5. Computational results. In [23], a simple greedy method is used to compute upper bounds on SeqCAN(t, v ) for t  {3, 4} and small values of v . These are reported in column K in Tables 5.1 and 5.2. Results from a more sophisticated greedy method by Erdem et al. [16] are reported in column ER. Using techniques from constraint satisfaction, in particular answer set programming, much more sophisticated search methods have been applied to strengths three and four [1, 2, 16]. Banbara, Tamura, and Inoue [1] implement an answer set programming method and show bounds for SeqCAN(3, v ) for v  80 and for SeqCAN(4, v ) for v  23. These bounds appear in column BTI in Tables 5.1 and 5.2. Results by Brain et al. [2] are reported in column BR in Tables 5.1 and 5.2. In [18], bounds for t  {3, 4, 5} and v  10 are reported from a method called the "bees algorithm"; these offer modest improvements on the greedy method in [23]. We do not report them for t  {3, 4} because they are not competitive with the results in [1]; we do report them for t = 5 in column BA, because they are the only published computational results. When t = 3, Tarui [36] establishes q/2 )  q for all q  4; these are reported in by direct construction that SeqCAN(3,  q/4 column TA. The bound U is obtained by computing the number Ui of as-yet-uncovered t-1 subsequences using Ui+1 =  t!t ! Ui  and terminating with the first value N for which UN = 0. (This does not explicitly construct the array but rather yields a number of rows for which it can surely be produced.) In the same manner, bound UR is obtained 1 by including reversals, so that Ui+2 = Ui - 2 t ! Ui  when i is even. The bound D is obtained by applying the algorithm that establishes Theorem 3.2 and that of DR by applying the algorithm that establishes Theorem 3.3. Table 5.1 reports results for t = 3. The theoretical results indicate that including reversals accelerates coverage, and so the bound UR improves on the bound U. Neither is competitive with greedy bounds from [23], given by K. In turn these are improved upon by the greedy method from [16], given by ER. Implementing our greedy approach nevertheless results in useful improvement to the two earlier greedy bounds, whether reversals are included or not. It comes as no surprise that the answer set programming

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS Table 5.1 Upper bounds on SeqCAN(3, v).

1855

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Events v 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 40 50 60 70 80 90

TA 8 8 8 10 10 10 10 12 12 12 12 12 12 12 12 12 12 14 14 14 14 14 14 14 14 14 14 16 16 16 16 18 18

U 12 17 20 23 26 28 30 32 33 35 36 37 39 40 41 42 42 43 44 45 46 46 47 48 48 49 49 54 58 61 64 66 68

UR 12 16 18 22 24 26 28 30 30 32 34 34 36 36 38 38 38 40 40 40 42 42 42 44 44 44 46 50 52 56 58 60 62

t=3 K ER 8 10 12 12 14 14 11 14 16 16 16 18 18 20 20 22 22 19 22 22 24 24 24 24 26 26 26 26 23 32 27 34 31 38 34 40 36 42 38 44

DR 8 10 10 12 12 12 14 14 14 16 16 16 16 18 18 18 18 18 20 20 20 20 20 20 20 22 22 24 26 26 28 30 30

D 6 8 8 9 10 11 12 12 13 13 14 14 15 15 16 16 16 17 17 17 17 18 18 18 18 19 19 21 23 24 25 26 27

BTI 7 8 8 8 9 9 10 10 10 10 10 11 11 12 12 12 12 13 14 14 14 14 14 14 15 15 17 19 21 22 24

BR 7 8 8 8 9 9 10 10 10 10 10 10 11 12 12 12 12 12 13 13 14 14 14 14 14 15 17 18 20 22 23

methods from [1, 2] (BTI, BR) yield consistent improvements on all the greedy methods for strength three. However, the direct construction of Tarui [36] provides better results at this time whenever v  30. Perhaps the most perplexing pattern is the regularity with which D yields a better bound than does DR . Remarkably, we consistently produce smaller sequence covering arrays when we do not automatically include reversals! The reasons for this are quite unclear at the present time. Table 5.2 gives results for strengths four and five. For strength four, our improvements on the method from [23] are more dramatic than for strength three. Surprisingly, the answer set programming technique from [1] obtains a better result than our greedy methods only when v  8. For 9  v  23, our greedy method yields much smaller arrays. (For v = 23, we employ 98 permutations as opposed to the 112 in BTI.) A similar comparison applies with the results from [2, 16] reported in column BR. Of course, we expect that given enough time, the answer set programming techniques would improve upon our greedy bounds. However, our methods require polynomial time in theory and are effective in practice for larger problems than those considered in [2, 1]; despite these "limitations," our methods appear to yield better results within the time available.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1856

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU Table 5.2 Upper bounds on SeqCAN(t, v) for t  {4, 5}.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Events v 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 40 50 60 70 80 90

U 24 54 79 98 114 128 140 151 160 169 177 184 191 197 203 209 214 219 224 228 232 236 240 244 248 251 255 283 305 322 337 350 361

UR 24 54 78 96 112 126 138 148 158 166 174 180 188 194 200 204 210 214 220 224 228 232 236 240 242 246 250 278 298 316 330 342 354

K 24 29 38 50 56 68 72 78 86 92 100 108 112 118 122 128 134 134 140 146 146 152 158 160 162 166 166 198 214 238 250 264 -

t=4 DR 24 24 32 40 44 50 56 60 64 70 74 78 80 84 86 90 92 96 98 98 102 104 106 108 110 112 114 132 146 154 166 174 180

D 24 26 34 41 47 52 57 61 66 71 73 78 81 84 86 91 92 95 97 99 101 104 105 107 110 111 113 128 141 151 160 168 176

BTI 24 24 38 44 52 58 65 69 77 81 84 89 91 97 100 105 104 111 112

BR

U 120 294 437 552 648 731 803 868 926 978 1027 1072 1113 1152 1189 1223 1256 1286 1316 1344 1370 1396 1420 1444 1466 1488 1671 1811 1924 2019 2101 2173

UR 120 294 436 550 646 728 800 864 922 976 1024 1068 1110 1148 1184 1218 1252 1282 1310 1338 1366 1390 1416 1438 1460 1482 1664 1804 1916 2012 2092 2164

t=5 DR 120 148 198 242 282 318 354 384 416 446 470 496 518 540 560 582 600 622 636 654 674 688 706 718 734 748

D 120 149 200 243 284 322 356 386 419 448 475 501 521 547 570 590 610 629 646 665 682 698 715 732 746 760

BA

55

159 212 271 329 383

104

149 181

A somewhat different pattern with respect to reversals is evident for strength four: The theoretical bound profits by including reversals throughout, but the implemented construction method appears first to benefit from reversals (for v  20) but later no longer benefit (for 40  v  90). Again the reasons for this are unclear. When v = 90, our methods track the coverage of 61,324,560 4-subsequences; thus, while the methods scale polynomially with v , the computations are nonetheless quite extensive. There is a CSSP(24;4,6) [28], but our methods do not yield fewer than 32 permutations. For strength five, none of the published methods in [1, 16, 23] report computational results, so it is difficult to make any comments about relative accuracy. However, the answer set programming methods do appear to require substantially more storage, which limits to a degree their effective range. To apply our methods would require tracking the coverage of 78,960,960 5-subsequences for v = 40; despite the efficiency of our methods, a straightforward implementation encounters both storage and time limitations. The method BA [18] is not competitive with our greedy methods. Within the range computed, including reversals improves our results. The pattern thereafter is unknown. Again, there is a CSSP(120;5,6) [25], but our methods do not yield fewer than 148 permutations.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1857

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

6. Using the product construction. For strength three, Theorem 4.1 provides substantial improvements on the computational results from the greedy methods. We properly signed a CSSP(6;3,4) in Table 4.3 and a CSSP(7;3,5) in Table 4.1. Table 6.1 shows proper signings for further arrays from [1]. We obtain CSSP(N ; 3, v ) for (v, N )  {(40, 15), (80, 17), (128, 18), (160, 19), (256, 20), (288, 21)} by using these in Theorem 4.1. These improve upon all the computational results! For example, while in [1] it is shown that SeqCAN(3, 80)  24 and in [2] that SeqCAN(3, 80)  23, here it is shown that SeqCAN(3, 80)  17. The examples given also provide better bounds than those of Tarui [36], but Theorem 4.1 does not outperform the direct construction asymptotically. 7. Constraints. In the testing application, it may happen that not every permutation of the events can in fact be executed; see [2, 23, 24]. It is therefore reasonable to ask how constraints on the execution order affect the number of permutations needed and how they affect the difficulty of finding a sequence covering array. We briefly consider the latter, in order to examine connections with further problems. Let  = {0, . . . , v - 1}. Let C be a set of subpermutations of , called constraints. A constrained sequence covering array SeqCA(N ; t, v, C ) is a set  = {1 , . . . , N } where i is a permutation of  that does not cover any subpermutation in C , and every t-subsequence of  that does not cover any subpermutation in C is covered by at least one of the permutations {1 , . . . , N }. Even in the easiest case, when t = 2 and all constraints are 2-subpermutations, the nature of the problem changes dramatically. Imposing the subpermutation constraint that b cannot precede a is the same as enforcing the precedence constraint that a precede b. When the precedence constraints contain a cycle, it is impossible to meet all constraints. This can be easily checked. When the constraints are acyclic, there is a permutation that covers no constraint. However, covering all 2-subpermutations not in C requires more. Let C r be the set of 2-subpermutations obtained by reversing each 2-subpermutation in C . Suppose that a SeqCA(N ; 2, v, C ) exists. Every permutation in the sequence covering array covers every 2-subpermutation in C r . Equivalently, treating C r as a partial order, every permutation gives a linear extension of the partial order. When (a, b)  C , (b, a) must be covered by every permutation in the sequence covering array. When {(a, b), (b, a)}  C = , some but not all permutations in the sequence covering array cover (a, b)--and the rest cover (b, a). Hence the set of 2-subpermutations covered by every permutation in the sequence covering array is exactly C r . This establishes a connection with the theory of partial orders. The dimension of a partial order is the smallest number of linear extensions whose intersection is the partial order [37, 38]. Our discussion establishes that a SeqCA(N ; 2, v, C ) exists if and only if the dimension of the partial order induced by C r is at most N . Hence we have the next lemma. Lemma 7.1. Deciding whether a SeqCA(N ; 2, v, C ) exists is NP-complete, even when C is an acyclic set of 2-subpermutations. Proof. Yannakakis [41] shows that determining whether a partial order has dimension at most 3 is NP-complete. Brain et al. [2] establish the NP-completeness of a related problem in which the subsequences to be covered, the constraints, and the permutations allowed are all specified. Lemma 7.1 is in stark contrast with the existence of sequence covering arrays of strength two without constraints. Nevertheless, the complexity arises in determining whether a small sequence covering array exists in these cases, not in

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1858

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU Table 6.1 Small properly signed CSSP(N ; 3, v)s. Signs not shown can be chosen arbitrarily.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

8 5 6 2 3 7 1 4 1 2 8 4 5 7 6 3 6 5 7 8 3 2 4 1 5 7 4 3 8 1 6 2 3 7 4 2 1 5 8 6 6 2 1 3 8 7 4 5 2 7 3 6 4 5 1 8 5 1 3 8 4 2 6 7 (8;3,8)

10 4 5 6 8 1 3 2 9 7 2 10 6 5 4 8 7 1 9 3 8 6 1 5 10 3 9 7 4 2 5 3 9 10 1 8 4 6 7 2 2 5 7 4 8 9 3 10 1 6 6 8 1 10 7 5 2 4 3 9 9 10 8 3 5 2 4 7 1 6 5 1 7 3 2 4 9 8 6 10 3 5 1 2 9 7 10 4 6 8 (9;3,10)

2 14 15 5 8 3 1 11 9 7 13 12 16 6 4 10 14 8 3 6 7 11 1 16 12 13 2 10 15 5 9 4 12 16 4 6 3 5 7 2 1 9 15 11 8 14 10 13 12 2 13 4 3 14 9 6 11 1 5 15 10 7 16 8 16 4 10 5 12 9 11 14 1 8 13 2 7 15 6 3 6 4 12 15 3 1 11 13 8 14 10 5 2 9 16 7 2 15 12 11 6 8 13 5 14 7 3 4 9 16 10 1 5 9 7 14 16 13 6 10 12 1 11 2 8 3 4 15 5 6 1 15 12 16 10 2 3 13 4 11 9 8 7 14 11 7 8 4 15 5 16 6 14 12 9 13 1 2 3 10 (10;3,16) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 4 14 11 10 6 9 13 7 17 1 18 3 2 12 16 5 8 15 4 17 3 2 18 16 14 7 9 13 11 12 10 8 1 6 15 5 6 13 7 8 12 10 14 4 16 1 2 18 17 15 9 11 3 5 7 18 6 16 4 5 1 17 13 11 10 12 14 15 8 2 9 3 8 3 15 4 9 17 2 16 11 12 14 13 10 7 6 1 5 18 8 11 3 14 6 15 13 5 2 16 17 1 18 7 10 9 4 12 10 7 16 4 5 9 13 15 1 18 3 17 2 12 14 8 6 11 13 9 16 14 15 11 7 12 5 4 2 1 3 10 6 18 17 8 18 4 15 17 16 2 8 1 11 13 9 12 10 3 6 14 7 5 18 7 2 5 4 3 17 10 15 11 14 12 13 1 16 9 8 6 (11;3,18) 19 3 5 2 6 18 9 10 14 21 20 1 15 4 12 22 16 7 13 8 11 23 17 2 22 13 21 7 4 1 6 18 23 15 12 8 10 19 16 17 11 20 3 5 14 9 1 21 6 7 4 20 16 18 10 23 3 13 17 9 11 14 2 19 8 22 12 15 5 10 12 5 15 8 13 23 17 22 11 18 14 9 3 4 1 16 2 20 21 19 6 7 3 17 22 4 14 2 7 13 1 10 9 16 11 12 23 8 20 19 18 15 21 5 6 10 1 19 15 3 14 23 4 12 11 8 18 2 21 16 6 17 20 7 5 9 13 22 20 13 3 17 18 7 14 10 22 9 1 16 11 21 15 8 4 6 2 5 23 19 12 8 18 12 3 22 11 14 1 15 6 21 5 10 19 9 13 4 2 20 17 16 7 23 21 2 12 15 22 23 3 19 8 5 16 17 1 20 6 18 13 7 9 11 14 4 10 18 10 9 14 7 17 8 6 5 15 16 13 23 11 4 12 20 21 1 22 3 2 19 16 10 14 9 21 12 7 23 13 2 4 19 20 1 22 3 8 17 18 6 5 15 11 15 16 23 21 12 3 19 17 4 9 13 1 18 14 5 22 7 11 10 8 6 20 2 (12;3,23)

determining whether a sequence covering array exists. The situation is worse when constraints have strength three. Consider a collection T of ordered triples of distinct elements of , and associate with (a, b, c) the constraints {(b, a, c), (b, c, a), (a, c, b), (c, a, b)}. Meeting these constraints requires that b lie between a and c, and a collection of constraints of this type forms an instance of the betweenness problem [5] in which one is required to

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1859

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

order all items so that for every triple (a, b, c)  T , b lies between a and c. Forming C = {(b, a, c), (b, c, a), (a, c, b), (c, a, b) : (a, b, c)  T }, even finding a single permutation that covers no 3-subpermutation in C appears hard: Lemma 7.2 (see [30]). Determining whether an instance of the betweenness problem has a solution is NP-complete. These complexity results suggest that constraints pose severe additional challenges in the construction of sequence covering arrays. Checking feasibility can become difficult; even when feasibility is easily checked, the minimization problem is substantially more complicated. 8. Conclusions. The close connection between sequence covering arrays and covering arrays has proved useful in establishing bounds on the sizes of sequence covering arrays. The efficient conditional expectation algorithm for generating sequence covering arrays and the product construction for strength three parallel analogous results for covering arrays. Unfortunately, while sequence covering arrays lead to covering arrays with excess coverage, additional conditions on such a covering array would be required in order to recover a sequence covering array. Hence the parallels between the extensive literature on covering arrays and the existence problem for sequence covering arrays are primarily by analogy. We have examined numerous formulations for sequence covering arrays. In closing, we indicate one more (see also [27]). A perfect hash family PHF(N ; k, w, t) is an N × k array on w symbols in which in every N × t subarray, at least one row consists of distinct symbols. Mehlhorn [29] introduced perfect hash families as an efficient tool for compact storage and fast retrieval of frequently used information; see also [14]. Stinson et al. [35] establish that perfect hash families can be used to construct separating systems, key distribution patterns, group testing algorithms, cover-free families, and secure frameproof codes. They are also used extensively in product constructions for covering arrays [8, 12, 13]. Completely t-scrambling sets of permutations can be viewed as an ordered analogue of perfect hash families in which k = w, no element appears twice in a row, and for every way to select t distinct columns in order there is a row in which the elements in these columns are in increasing order. In particular, a completely t-scrambling set of permutations provides a perfect hash family in which, for every set of t columns, there are at least t! rows containing distinct symbols in the chosen columns, and at least one for each of the t! symbol orderings. For this reason, it appears reasonable to expect that constructions for perfect hash families may also prove to be useful for sequence covering arrays. Acknowledgments. Thanks to two anonymous referees for pointing out relevant references. Special thanks to Mutsunori Banbara and Johannes Oetsch for providing explicit solutions for small sequence covering arrays with t = 3.
REFERENCES [1] M. Banbara, N. Tamura, and K. Inoue, Generating event-sequence test cases by answer set programming with the incidence matrix, in Technical Communications of the 28th International Conference on Logic Programming (ICLP12), 2012, pp. 86­97. ¨ hrer, H. Tompits, and C. Yilmaz, Event[2] M. Brain, E. Erdem, K. Inoue, J. Oetsch, J. Pu sequence testing using answer-set programming, Internat. J. Advances Software, 5 (2012), pp. 237­251. [3] R. C. Bryce and C. J. Colbourn, The density algorithm for pairwise interaction testing, Software Testing, Verification, and Reliability, 17 (2007), pp. 159­182.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1860

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

[4] R. C. Bryce and C. J. Colbourn, A density-based greedy algorithm for higher strength covering arrays, Software Testing Verification Reliability, 19 (2009), pp. 37­53. [5] B. Chor and M. Sudan, A geometric approach to betweenness, SIAM J. Discrete Math., 11 (1998), pp. 511­523. [6] M. B. Cohen, C. J. Colbourn, and A. C. H. Ling, Constructing strength three covering arrays with augmented annealing, Discrete Math., 308 (2008), pp. 2709­2722. [7] C. J. Colbourn, Constructing perfect hash families using a greedy algorithm, in Coding and Cryptology, Y. Li, S. Zhang, S. Ling, H. Wang, C. Xing, and H. Niederreiter, eds., World Scientific, Singapore, 2008, pp. 109­118. [8] C. J. Colbourn, Covering arrays and hash families, in Information Security and Related Combinatorics, NATO Peace and Information Security, IOS Press, Amsterdam, 2011, pp. 99­136. [9] C. J. Colbourn, Efficient conditional expectation algorithms for constructing hash families, in Combinatorial Algorithms, Lecture Notes in Comput. Sci., 7056, Springer-Verlag, Berlin, 2011, pp. 144­155. [10] C. J. Colbourn, D. Horsley, and V. R. Syrotiuk, Strengthening hash families and compressive sensing, J. Discrete Algorithms, 16 (2012), pp. 170­186. [11] C. J. Colbourn, S. S. Martirosyan, Tran Van Trung, and R. A. Walker II, Roux-type constructions for covering arrays of strengths three and four, Des. Codes Cryptogr., 41 (2006), pp. 33­57. [12] C. J. Colbourn and J. Torres-Jim´ enez, Heterogeneous hash families and covering arrays, Contemp. Math., 523 (2010), pp. 3­15. [13] C. J. Colbourn and J. Zhou, Improving two recursive constructions for covering arrays, J. Statist. Theory Practice, 6 (2012), pp. 30­47. [14] Z. J. Czech, G. Havas, and B. S. Majewski, Perfect hashing, Theoret. Comput. Sci., 182 (1997), pp. 1­143. [15] B. Dushnik, Concerning a certain set of arrangements, Proc. Amer. Math. Soc., 1 (1950), pp. 788­796. ¨ hrer, H. Tompits, and C. Yilmaz, Answer-set pro[16] E. Erdem, K. Inoue, J. Oetsch, J. Pu gramming as a new approach to event-sequence testing, in Proceedings of the 2nd International Conference on Advances in System Testing and Validation Lifecycle, Xpert Publishing Services, 2011, pp. 25­34. ¨ redi, Scrambling permutations and entropy of hypergraphs, Random Structures [17] Z. Fu Algorithms, 8 (1996), pp. 97­104. [18] M. M. Z. Hazli, K. Z. Zamli, and R. R. Othman, Sequence-based interaction testing implementation using bees algorithm, in Proceedings of the IEEE Symposium on Computers and Informatics, 2012, pp. 81­85. [19] S. Huang, M. B. Cohen, and A. M. Memon, Repairing GUI test suites using a genetic algorithm, in Proceedings of the 3rd International Conference on Software Testing, Verification and Validation (ICST), 2010, pp. 245­254. [20] Y. Ishigami, Containment problems in high-dimensional spaces, Graphs Combin., 11 (1995), pp. 327­335. [21] Y. Ishigami, An extremal problem of d permutations containing every permutation of every t elements, Discrete Math., 159 (1996), pp. 279­283. [22] D. S. Johnson, Approximation algorithms for combinatorial problems, J. Comput. System Sci., 9 (1974), pp. 256­278. [23] D. R. Kuhn, J. M. Higdon, J. F. Lawrence, R. N. Kacker, and Y. Lei, Combinatorial methods for event sequence testing, in Proceedings of the IEEE 5th International Conference on Software Testing, Verification and Validation (ICST), 2012, pp. 601­609. [24] D. R. Kuhn, J. M. Higdon, J. F. Lawrence, R. N. Kacker, and Y. Lei, Combinatorial methods for event sequence testing, CrossTalk J. Defense Software Engineering, 25 (2012), pp. 15­18. [25] V. I. Levenshtein, Perfect codes in the metric of deletions and insertions, Diskret. Mat., 3 (1991), pp. 3­20. ´ sz, On the ratio of optimal integral and fractional covers, Discrete Math., 13 (1975), [26] L. Lova pp. 383­390. [27] O. Margalit, Better bounds for event sequence testing, in Proceedings of the 2nd International Workshop on Combinatorial Testing, 2013. [28] R. Mathon and Tran Van Trung, Directed t-packings and directed t-Steiner systems, Des. Codes Cryptogr., 18 (1999), pp. 187­198. [29] K. Mehlhorn, Data Structures and Algorithms 1: Sorting and Searching, Springer-Verlag, Berlin, 1984.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1861

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

´ , Total ordering problem, SIAM J. Comput., 8 (1979), pp. 111­114. [30] J. Opatrny [31] J. Radhakrishnan, A note on scrambling permutations, Random Structures Algorithms, 22 (2003), pp. 435­439. [32] J. Spencer, Minimal scrambling sets of simple orders, Acta Math. Acad. Sci. Hungar., 22 (1971/72), pp. 349­353. [33] R. P. Stanley, Increasing and decreasing subsequences and their variants, in Proceedings of the International Congress of Mathematicians, vol. I, Madrid, 2007, pp. 545­579. [34] S. K. Stein, Two combinatorial covering theorems, J. Combin. Theory Ser. A, 16 (1974), pp. 391­397. [35] D. R. Stinson, Tran Van Trung, and R. Wei, Secure frameproof codes, key distribution patterns, group testing algorithms and related structures, J. Statist. Plann. Inference, 86 (2000), pp. 595­617. [36] J. Tarui, On the minimum number of completely 3-scrambling permutations, Discrete Math., 308 (2008), pp. 1350­1354. [37] W. T. Trotter, Jr., Some combinatorial problems for permutations, in Proceedings of the 8th Southeastern Conference on Combinatorics, Graph Theory and Computing, Baton Rouge, La., 1977, Utilitas Mathematica, Winnipeg, pp. 619­632. [38] W. T. Trotter, Jr., Combinatorics and partially ordered sets, in Dimension Theory, Johns Hopkins Ser. Math. Sci., Johns Hopkins University Press, Baltimore, MD, 1992. [39] W. Wang, Y. Lei, S. Sampath, R. Kacker, D. Kuhn, and J. Lawrence, A combinatorial approach to building navigation graphs for dynamic web applications, in Proceedings of the 25th International Conference on Software Maintenance, 2009, pp. 211­220. [40] W. Wang, S. Sampath, Y. Lei, and R. Kacker, An interaction-based test sequence generation approach for testing web applications, in 11th IEEE High Assurance Systems Engineering Symposium, 2008, pp. 209­218. [41] M. Yannakakis, The complexity of the partial order dimension problem, SIAM J. Algebraic Discrete Methods, 3 (1982), pp. 351­358. [42] X. Yuan, M. B. Cohen, and A. M. Memon, Towards dynamic adaptive automated test generation for graphical user interfaces, in International Conference on Software Testing, Verification and Validation Workshops, 2009, pp. 263­266. [43] X. Yuan, M. B. Cohen, and A. M. Memon, GUI interaction testing: Incorporating event context, IEEE Trans. Software Engrg., 37 (2011), pp. 559­574. [44] X. Yuan and A. M. Memon, Generating event sequence-based test cases using GUI runtime state feedback, IEEE Trans. Software Engrg., 36 (2010), pp. 81­95.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

575

A Discrete Particle Swarm Optimization for
Covering Array Generation
Huayao Wu, Changhai Nie, Member, IEEE, Fei-Ching Kuo, Member, IEEE,
Hareton Leung, Member, IEEE, and Charles J. Colbourn

Abstractâ€”Software behavior depends on many factors.
Combinatorial testing (CT) aims to generate small sets of test
cases to uncover defects caused by those factors and their
interactions. Covering array generation, a discrete optimization
problem, is the most popular research area in the field of CT.
Particle swarm optimization (PSO), an evolutionary search-based
heuristic technique, has succeeded in generating covering arrays
that are competitive in size. However, current PSO methods for
covering array generation simply round the particleâ€™s position
to an integer to handle the discrete search space. Moreover, no
guidelines are available to effectively set PSOs parameters for
this problem. In this paper, we extend the set-based PSO, an
existing discrete PSO (DPSO) method, to covering array generation. Two auxiliary strategies (particle reinitialization and
additional evaluation of gbest) are proposed to improve performance, and thus a novel DPSO for covering array generation
is developed. Guidelines for parameter settings both for conventional PSO (CPSO) and for DPSO are developed systematically
here. Discrete extensions of four existing PSO variants are developed, in order to further investigate the effectiveness of DPSO
for covering array generation. Experiments show that CPSO can
produce better results using the guidelines for parameter settings,
and that DPSO can generate smaller covering arrays than CPSO
and other existing evolutionary algorithms. DPSO is a promising
improvement on PSO for covering array generation.
Index Termsâ€”Combinatorial testing (CT), covering array
generation, particle swarm optimization (PSO).

I. I NTRODUCTION
S SOFTWARE functions and run-time environments
become more complex, testing of modern software systems is becoming more expensive. Effective detection of

A

Manuscript received May 12, 2013; revised December 29, 2013 and
May 18, 2014; accepted September 28, 2014. Date of publication October 9,
2014; date of current version July 28, 2015. This work was supported in part
by the National Natural Science Foundation of China under Grant 61272079,
in part by the Research Fund for the Doctoral Program of Higher Education
of China under Grant 20130091110032, in part by the Science Fund for
Creative Research Groups of the National Natural Science Foundation of
China under Grant 61321491, in part by the Major Program of National
Natural Science Foundation of China under Grant 91318301, and in part
by the Australian Research Council Linkage under Grant LP100200208.
(Corresponding author: Changhai Nie.)
H. Wu and C. Nie are with the State Key Laboratory for Novel
Software Technology, Nanjing University, Nanjing 210023, China (e-mail:
hywu@outlook.com; changhainie@nju.edu.cn).
F.-C. Kuo is with the Faculty of Information and Communication
Technologies, Swinburne University of Technology, Hawthorn, VIC 3122,
Australia (e-mail: dkuo@swin.edu.au).
H. Leung is with the Department of Computing, Hong Kong Polytechnic
University, Hong Kong (e-mail: hareton.leung@polyu.edu.hk).
C. J. Colbourn is with Arizona State University, Tempe, AZ 85287-8809,
USA (e-mail: colbourn@asu.edu).
Digital Object Identifier 10.1109/TEVC.2014.2362532

failures at a low cost is a key issue for test case generation. Combinatorial testing (CT) is a popular testing method
to detect failures triggered by various factors and their interactions [1]. By employing covering arrays as test suites, the CT
method aims to sample the large combination space with few
test cases to cover different interactions among a fixed number of factors. Kuhn and Reilly [2] shows that more than 70%
of the failures in certain software were caused by the interactions of one or two factors, and almost all the failures could
be detected by checking the interactions among six factors.
Therefore, CT can be an effective method in practice.
Generating a covering array with fewest tests (minimum
size) is a major challenge in CT. In general, the minimum
size of a covering array is unknown; hence, methods have
focused on finding covering arrays that have as few tests as
possible at reasonable search cost. The many methods that
have been proposed can be classified into two main groups:
1) mathematical methods and 2) computational methods [1].
Mathematical (algebraic or combinatorial) methods typically
exploit some known combinatorial structure. Computational
methods primarily use greedy strategies or heuristic-search
techniques to generate covering arrays, due to the size of the
search space.
Mathematical methods yield the best possible covering
arrays in certain cases. For example, orthogonal arrays used
in the design of experiments provide covering arrays with a
number of tests that is provably minimum. However, all known
mathematical methods can be applied only for restrictive sets
of factors. This limitation has led to an emphasis on computational methods. Greedy algorithms have been quite effective
in generating covering arrays, but their accuracy suffers from
becoming trapped in local optima.
In recent years, search-based software engineering (SBSE)
has focused on using search-based optimization algorithms
to find high-quality solutions for software engineering problems. Inspired by SBSE, many artificial intelligence-based
heuristic-search techniques have been applied to software
testing. For example, simulated annealing (SA) [3]â€“[7],
genetic algorithm (GA) [8]â€“[10], and ant colony
optimization (ACO) [9], [11], [12] have all been applied
to covering array generation. These techniques can generate
any types of covering arrays, and the constraint solving
and prioritization techniques can be easily integrated. Their
applications have been shown to be effective, producing
relatively small covering arrays in many cases. Particle swarm
optimization (PSO), a relatively new evolutionary algorithm,

c 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
1089-778X 
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

576

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

has also been used in this area [13]â€“[16]. It is easy to
implement and has fast initial progress.
The conventional PSO (CPSO) algorithm was originally
designed to find optimal or near optimal solutions in a
continuous space. Nevertheless, many discrete PSO (DPSO)
algorithms and frameworks have been developed to solve
discrete problems [17]â€“[22]. For covering array generation,
current discrete methods [13]â€“[16] simply round the particleâ€™s
position to an integer while keeping the velocity as a real
number. They suffer from two main shortcomings. First, the
performance of PSO is significantly impacted by its parameter
settings. In [23], effects of the general parameter selection and
initial population of PSO have been analyzed, but no guidelines on parameter settings have been reported for covering
array generation. Hence, a clear understanding of how to set
these execution parameters is needed. Second, simple rounding
fractional positions to integers introduces a substantial source
of errors in the search. Instead, a specialized DPSO version is
needed. Because current PSO methods show promise for generating covering arrays, these two main shortcomings should
be addressed.
In this paper, we adapt set-based PSO (S-PSO) [18] to
generate covering arrays. S-PSO utilizes set and probability
theories to develop a new representation scheme for combinatorial optimization problems as well as refining the related
evolution operators. In our adaptation of S-PSO, two auxiliary strategies are proposed to enhance performance, and
a novel DPSO algorithm is thus proposed. DPSO has the
same conceptual basis and exhibits similar search behavior to
CPSO, with parameters playing similar roles. Then, we explore
the optimal parameter settings for both CPSO and DPSO to
improve their performance, and identify recommended settings. Furthermore, because many CPSO variants [24]â€“[27]
can be easily extended to discrete versions based on our DPSO,
the performance of these discrete versions is also compared
with their original ones. Finally, we compare CPSO and DPSO
with existing GA and ACO [9], [11] algorithms to generate
covering arrays.
The main contributions of this paper are as follows.
1) Based on the set-based representation, we design a
version of S-PSO [18] for covering array generation.
2) We propose two auxiliary strategies, particle reinitialization, and additional evaluation of gbest, to enhance the
performance of PSO. A novel DPSO for covering array
generation is proposed.
3) We design experiments to explore the optimal parameter settings for CPSO and DPSO for covering array
generation.
4) We implement original and discrete versions of four
representative PSO variants (TVAC [24], CLPSO [25],
APSO [26], and DMS-PSO [27]) to compare their
efficacy for covering array generation.
The rest of this paper is organized as follows. Section II
gives background on CT, covering array generation, and
the CPSO algorithm. Section III summarizes related work.
Section IV presents our DPSO algorithm, including the
representation scheme, related operators, and two auxiliary
strategies. Section V evaluates the performance of CPSO and

TABLE I
E-C OMMERCE S OFTWARE S YSTEM

DPSO, and explores optimal parameter settings. Section VI
gives a comparison among CPSO, DPSO, and original and
discrete variants. Section VII compares CPSO and DPSO
with GA and ACO. Section VIII concludes this paper and
outlines future work.
II. BACKGROUND
A. CT
Suppose that the behavior of the software under test (SUT)
is controlled by n independent factors, which may represent configuration parameters, internal or external events, user
inputs, and the like. The ith factor has i discrete levels, chosen from the finite set Vi of size i . An n-tuple (x1 , x2 , . . . , xn )
forms a test case, where xi âˆˆ Vi for 1 â‰¤ i â‰¤ n.
Consider a simple e-commerce software system [15]. This
system consists of five different components. Each of these five
components can be regarded as a factor, and its configurations
can be regarded as different levels. Table I shows these five
factors and their corresponding levels. In this example, n = 5,
1 = 2 = 3 = 2, 4 = 5 = 3.
System failures are often triggered by interactions among
some factors, which can be represented by the combinations
of factor levels. In order to detect these failures, their combinations should be covered at least once by the test suite. A
t-way schema can be used to represent them.
Definition 1 (t-way schema): The n-tuple (âˆ’, y1 , . . . ,
yt , . . . ) is a t-way schema (t > 0) when some t factors
have fixed levels and the others can take any valid levels,
represented as â€œâˆ’.â€
For example, suppose that when factor Payment Server
takes the level Master and factor Web Server takes the level
Apache, a system failure occurs. To detect this failure, the
2-way schema (Master, â€“, Apache, â€“, â€“) must be covered
at least once by the test suite. To simplify later discussion,
we use the index in the level set of each factor to present
a schema. For example, (0, â€“, 1, â€“, â€“) is used to represent
(Master, â€“, Apache, â€“, â€“).
Exhaustive testing covers all n-way schemas of the system; in our example, it uses 2 Ã— 2 Ã— 2 Ã— 3 Ã— 3 = 72 test
cases. Testing cost becomes prohibitive as the numbers of factors and levels increase. Moreover, because only interactions
among few factors are likely to trigger failures [2], testing
high-way schemas can lead to many uninformative test cases.
At the other extreme, if we only guarantee to cover each 1-way
schema once, only three test cases are needed (a single test
case can cover five 1-way schemas at most). But we may
fail to detect some interaction triggered failures involving two
factors.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

TABLE II
C OVERING A RRAY CA(9; 2, 23 32 )

Instead, CT covers all t-way schemas. Such a test suite is a
t-way covering array, with t being the covering strength. The
value of t determines the depth of coverage. It is a key setting
of CT, and should be decided by the testers. We give a precise
definition.
Definition 2 (Covering Array): If an NÃ—n array, where N is
the number of test cases, has the following properties: 1) each
column i (1 â‰¤ i â‰¤ n) contains only elements from the set Vi
with i = |Vi | and 2) the rows of each N Ã— t sub array cover
all |Vk1 | Ã— |Vk2 | Ã— . . . Ã— |Vkt | combinations of the t columns at
least once, where t â‰¤ n and 1 â‰¤ k1 < . . . < kt â‰¤ n, then it is a
t-way covering array, denoted by CA(N; t, n, (1 , 2 , . . . , n )).
When 1 = 2 = . . . = n = , it is denoted by CA(N; t, n, ).
Reference [2] demonstrated that more than 70% failures can
be detected by a 2-way covering array, and almost all failures
can be detected by a 6-way covering array. Hence, using CT,
we can detect many failures of the system by applying relatively low strength covering arrays. In other words, CT can
greatly reduce the size of test suite while maintaining high
fault detection ability.
In the example of Table I, if we only consider the interactions between any two factors, only nine test cases are
required to construct a 2-way covering array instead of 72
for exhaustive testing. Table II shows a covering array, where
each row represents one test case. This table covers all possible 2-way schemas for any two factors corresponding to the
columns. For example, consider factor Payment Server and
User Browser, all 2Ã—3 = 6 schemas, (Master, â€“, â€“, Firefox, â€“)
(Master, â€“, â€“, Explorer, â€“), (Master, â€“, â€“, Chrome, â€“),
(VISA, â€“, â€“, Firefox, â€“), (VISA, â€“, â€“, Explorer, â€“),
(VISA, â€“, â€“, Chrome, â€“), can be found in the table.
For convenience, if several groups of gi factors (gi < n) have
g
the same number of levels ak , ak i can be used to represent
these factors and their levels. Thus, the coveringarray can
g
g
g
gi = n,
be denoted by CA(N; t, a11 , a22 , . . . , ak k ) where
n
or CA(N; t, a ) when g1 = n and a1 = a. For example, the
covering array in Table II is a CA(9; 2, 23 32 ).
In many software systems, the impacts of the interactions
among factors are not uniform. Some interactions may be more
prone to trigger system failures, while other may have little or no impact on the system. To effectively detect these
different interactions, variable strength (VS) covering arrays
can be applied. This can offer different covering strengths

577

TABLE III
A DDING T HREE T EST C ASES TO C ONSTRUCT
VCA(12; 2, 23 32 , CA(3, 22 31 ))

to different groups of factors, and can therefore provide a
practical approach to test real applications.
Definition 3 (VS Covering Array): A VS covering array,
m

1
denoted by VCA(N; t, a11 . . . akk , CA1 (t1 , bm
. . . bp p ), . . . ,
1
n
CAj (tj , cn11 . . . cqq )), is an N Ã— n covering array of covering
strength t containing one or more sub covering arrays, namely
CA1 , . . . , CAj , each of which has covering strength t1 , . . . , tj
all larger than t.
Consider the e-commerce system shown in Table I. If the
interactions of three factors, Payment Server, Web Server, and
Business Database, have a higher probability to trigger system failures, then a VS covering array can be constructed.
As in Table II, only three more test cases (Table III) are
needed to construct the VCA(12; 2, 23 32 , CA(3, 22 31 )). With
these 12 test cases, not only are all 2-way schemas of all five
factors covered, but also all 3-way schemas of these three factors (Payment Server, Web Server, and Business Database) are
covered.
B. Covering Array Generation
Covering arrays are used as test suites in CT. Covering array
generation is the process of test suite construction. It is the
most active area in CT with more than 50% of research papers
focusing on this field [1]. Due to limited testing resources, all
aim to construct a minimal covering array while still maintaining full coverage of combinations. Computational methods
have been used widely for covering array generation because
they can be applied to any systems. In general, these methods
generate all possible combinations first. Then they generate
test cases to cover these combinations one-by-one. One-testat-a-time is the most widely used strategy among evolutionary
algorithms to generate a covering array.
The one-test-at-a-time strategy was popularized by
AETG [28] and was further used by Bryce and Colbourn [29].
This strategy takes the model of SUT(n, 1 , . . . , n ) where
n is the number of factors and i is the number of valid
levels of factor i, and the covering strength t as input. At
first, an empty test suite TS and a set S of t-way schemas
to be covered are initialized. In each iteration, a test case is
generated with the highest fitness value according to some
heuristic techniques. Then it is added to TS and the t-way
schemas covered by it are removed. When all the t-way
schemas have been covered, the final test suite TS is returned.
This process is shown in Algorithm 1.
In this strategy, a fitness function must be used to evaluate
the quality of a candidate test case (line 6 in Algorithm 1). It is
an important part of all heuristic techniques. In covering array
generation, the fitness function takes the test case as the input
and then outputs a fitness value representing its â€œgoodness.â€
It is defined as follows.

578

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

Algorithm 1 One-Test-at-a-Time Strategy
1: Input: SUT(n, 1 , . . . , n ) and covering strength t
2: Output: covering array TS
3: TS = âˆ…
4: Construct S (all the t-way schemas to be covered) based
on n, 1 , . . . , n and t
5: while S  = âˆ… do
6:
Generate a test case p with the highest fitness value
according to some heuristics
7:
Add p to the test suite TS
8:
Remove the t-way schemas covered by p from S
9: end while
10: return TS

Definition 4 (Fitness Function): Let TS be the generated
test set, and p be a test case. Then fitness(p) is the number of
uncovered t-way schemas in TS that are covered by p.
The fitness function can be formulated as
fitness(p) = |schemat ({p}) âˆ’ schemat (TS)|

(1)

where schemat (TS) represents the set of all t-way schemas
covered by test set TS, and | Â· | stands for cardinality. When
all Cnt t-way schemas covered by p are not covered by TS,
the fitness function reaches the maximum value fitness(p) =
|schemat ({p})| = Cnt .
For example, consider the 2-way covering array generation
of the e-commerce system shown in Table II. Suppose that
TS consists of test cases (0, 0, 0, 0, 0) and (0, 0, 1, 1, 1).
The fitness function computes the fitness value for a candidate test case p = (1, 0, 1, 2, 2) as follows: because p covers
ten 2-way schemas, namely schema2 ({p}) = {(1, 0, â€“, â€“, â€“),
(1, â€“, 1, â€“, â€“,), (1, â€“, â€“, 2, â€“), (1, â€“, â€“, â€“, 2), (â€“, 0, 1, â€“, â€“),
(â€“, 0, â€“, 2, â€“), (â€“, 0, â€“, â€“, 2), (â€“, â€“, 1, 2, â€“), (â€“, â€“, 1, â€“, 2),
(â€“, â€“, â€“, 2, 2)} and TS only covers (â€“, 0, 1, â€“, â€“) in p, the
function returns fitness(p) = 9.
C. PSO
PSO is a swarm-based evolutionary computation technique.
It was developed by Kennedy et al. [30], inspired by the social
behavior of bird flocking and fish schooling. PSO utilizes a
population of particles as a set of candidate solutions. Each
of the particles represents a certain position in the problem
hyperspace with a given velocity. A fitness function is used
to evaluate the quality of each particle. Initially, particles are
distributed in the hyperspace uniformly. Then each particle
repeatedly updates its state according to the individual best
position in its history (pbest) and the current global best position (gbest). Eventually, each particle possibly moves toward
the direction of the individual optimum and global optimum,
and finds an optimal or near optimal solution.
Suppose that the problem domain is a Dâˆ’dimensional
hyperspace. Then the position and velocity of particle i can
be represented by xi âˆˆ RD and vi âˆˆ RD respectively. CPSO
uses the following equations to update a particleâ€™s velocity
and position, where vi,j (k) represents the jth component of the
velocity of particle i at the kth iteration, and xi,j (k) represents

its corresponding position:
vi,j (k + 1) = Ï‰ Ã— vi,j (k) + c1 Ã— r1,j Ã— (pbesti,j âˆ’ xi,j (k))
(2)
+ c2 Ã— r2,j Ã— (gbestj âˆ’ xi,j (k))
xi,j (k + 1) = xi,j (k) + vi,j (k + 1).

(3)

The best position of particle i in its history is pbesti , and
gbest is the best position among all particles. The velocityupdate equation (2) captures the three basic concepts of PSO.
The first is inertia, the tendency of the particle to continue in
the direction it has been moving. The second is memory of
the best position ever found by itself. The third is cooperation
using the best position found by other particles.
The parameter Ï‰ is inertia weight. It controls the balance between exploration (global search state) and exploitation
(local search state). Two positive real numbers c1 and c2
are acceleration constants that control the movement tendency toward the individual and global best position. Most
studies set Ï‰ = 0.9, and c1 = c2 = 2 to get the best
balance [17], [31], [32]. In addition, r1,j and r2,j are two uniformly distributed random numbers in the range of [0.0, 1.0],
used to ensure the diversity of the population.
If the problem domain (the search space of particles) has
bounds, a bound handling strategy is adopted to keep the particles inside the search space. Many different strategies have
been proposed [33]. In the reflecting strategy, when a particle
exceeds the bound of the search space in any dimension, the
particle direction is reversed in this dimension to get back to
the search space. For example, in case of a dimension with
a range of values from 0 to 2, if a particle moves to 3, its
position is reversed to 1.
In addition, as the velocity can increase over time, a limit
is set on velocity to prevent an infinite velocity or invalid
position for the particle. Setting a maximum velocity, which
determines the distance of movement from the current position
to the possible target position, can reduce the likelihood of
explosion of the swarm traveling distance [31]. Generally, the
value of the maximum velocity is selected as i /2, where i
is the range of dimension i.
The pseudocode in Algorithm 2 presents the process of generating a test case by PSO. This algorithm can be invoked
by line 6 of Algorithm 1 to generate a test case for t-way
schemas. The n factors of the test case can be treated as an
n-dimensional hyperspace. A particle pi = (x1 , x2 , . . . , xn ) can
be regarded as a candidate test case. The fitness function is
that of Definition 4, the number of uncovered t-way schemas
in the generated test suite that are covered by particle pi . PSO
employs real numbers but the valid values are integers for covering array generation, so each dimension of particleâ€™s position
can be rounded to an integer while maintaining the velocity
as a real number. This method is used in all prior research
applying PSO to covering array generation [13]â€“[16].
III. R ELATED W ORK
In this section, three different but related aspects are discussed. We first summarize search-based CT, especially the
current applications of PSO for covering array generation.
Then, we introduce prior research on discrete versions of PSO,

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

Algorithm 2 Generate Test Case by PSO
1: Input: SUT(n, 1 , . . . , n ), covering strength t and the
related parameters of PSO
2: Output: the best test case gbest
3: it = 0, gbest = NULL
4: for each particle pi do
5:
Initialize the position xi and velocity vi randomly
6: end for
7: while it < maximum iteration do
8:
for each particle pi do
9:
Compute the fitness value fitness(pi )
10:
if fitness(pi ) > fitness(pbesti ) then
11:
pbesti = pi
12:
end if
13:
if fitness(pi ) > fitness(gbest) then
14:
gbest = pi
15:
end if
16:
end for
17:
for each particle pi do
18:
Update the velocity and position according to
Equations 2 and 3
19:
Apply maximum velocity limitation and bound
handling strategy
20:
end for
21:
it = it + 1
22: end while
23: return gbest

to improve CPSO for discrete problems. Finally, some PSO
variants are introduced. These try to enhance PSO for conventional continuous problems by adopting strategies that can
further improve covering array generation.
A. PSO in Search-Based CT
SBSE has grown quickly in recent years. Many problems
in software engineering have been formulated into searchbased optimization problems, and heuristic techniques have
been used to find solutions. Software testing is a major topic
in software engineering. Many heuristic techniques have also
been applied to this field, including functional testing, mutation testing, stress testing, temporal testing, CT, and regression
testing. Currently, many classic heuristic techniques, such as
SA [3]â€“[7], GA [8]â€“[10], and ACO [9], [11], [12] have been
applied to generate uniform and VS covering arrays
successfully.
PSO has also been applied to software testing.
Windisch et al. [34] applied PSO to structural testing,
and compared its performance with GA. They showed that
PSO outperformed GA for most cases in terms of effectiveness and efficiency. Ganjali [35] proposed a framework for
automatic test partitioning based on PSO, observing that PSO
performed better than other existing heuristic techniques.
PSO has been applied to covering array generation.
Ahmed and Zamli [15] proposed frameworks of PSO to generate different kinds of covering arrays, including t-way
(PSTG [14]) and VS-PSTG. Then they extended PSTG to support VS covering array generation [16]. Chen et al. [18] also

579

applied PSO to 2-way covering array generation. They further
used a test suite minimization algorithm to reduce the size of
the generated covering array.
Current applications of PSO for covering array generation
can yield smaller covering arrays than most greedy algorithms,
but they all apply the same rounding operator to the particleâ€™s
position, and they lack guidelines on the parameter settings.
B. Discrete Versions of PSO
PSO was initially developed to solve problems in continuous
space, but PSO can also be applied to some discrete optimization problems including binary or general integer variables.
Many discrete versions of PSO have been proposed [17]â€“[22].
Chen et al. [18] classify existing algorithms into four
types.
1) Swap operator-based PSO [19] uses a permutation of
numbers as position and a set of swaps as velocity.
2) Space transformation-based PSO [20] uses various techniques to transform the position, defined as a vector of
real numbers, to the corresponding solution in discrete
space.
3) Fuzzy matrix-based PSO [21] defines the position and
velocity as a fuzzy matrix, and decode it to a feasible
solution.
4) Incorporating-based PSO [22] consists of hybrid methods incorporating some heuristic or problem dependent
techniques.
Chen et al. [18] also propose a S-PSO method based on
sets with probabilities, which we later adapt to represent a
particleâ€™s velocity. S-PSO uses a set-based scheme to represent the discrete search space. The velocity is defined as
a set with probabilities, and the operators are all replaced
by procedures defined on the set. They extend some PSO
variants to discrete versions and test them on the traveling
salesman problem and the multidimensional knapsack problem. They show that the discrete version of CLPSO [25] can
perform better than other variants. S-PSO was found to characterize the combinatorial optimization problems very well.
Gong et al. [36] then employed such a set-based representation scheme for solving vehicle routing problems to obtain a
new method, S-PSO-VRPTW.
C. PSO Variants
The original PSO may become trapped in a local optimum.
In order to improve the performance, many variants have been
proposed [17], [24]â€“[27]. Chen et al. [37] classify these variants into four different groups. The first group of variants aims
to adjust the control parameters during the evolution, for example by decreasing inertia weight Ï‰ linearly from 0.9 to 0.4 over
the search process. The second group employs different neighborhood topologies. The basic local version of PSO makes
the particle learn from the local best position lbesti found by
particle iâ€™s neighborhood instead of the global best position
gbest. RPSO and VPSO are two common versions which use
a ring topology and a Von Neumann topology, respectively.
The third group uses hybrid strategies with other search techniques. Many types of operators, such as genetic operators

580

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE IV
D IFFERENT G ROUPS OF DPSO AND PSO VARIANTS

(selection, crossover, and mutation), evolutionary computation
paradigms and biology inspired operators have been used. The
fourth uses multiswarm techniques. Several sets of swarms
optimize different components of the solution concurrently or
cooperatively.
In our experiments, four representative PSO variants are
included, as follows.
1) Ratnaweera et al. [24] proposed a typical variant of
the first group, TVAC, which uses a time varying
inertia weight and acceleration constant to adjust the
parameters.
2) Liang et al. [25] proposed the CLPSO method, a wellknown variant of the second group, which allows the
particle to learn from other particlesâ€™ individual best
positions in different dimensions.
3) Zhan et al. [26] proposed an adaptive PSO (APSO)
that can be seen as a variant of the third group. They
developed an evolutionary state estimation to adaptively control the parameter and used an elitist learning
strategy.
4) DMS-PSO [27], a variant of the fourth group, was proposed by Liang and Suganthan [27]. It is characterized
by a set of swarms with small sizes and these swarms
are frequently regrouped.
In summary, Table IV lists the different groups of discrete
versions of PSO and PSO variants.
IV. DPSO
In this section, a new DPSO for covering array generation
is presented. We firstly illustrate the weakness of CPSO with a
simple example. Then the representation scheme of a particleâ€™s
velocity and the corresponding redefined operators are introduced. Finally, we give two auxiliary strategies to enhance the
performance of DPSO.
A. Example
In CPSO, a particleâ€™s position represents a candidate test
case; its velocity, a real vector, represents the movement tendency of this particle in each dimension. This scheme is
meaningful in a continuous optimization problem, because
an optimal solution may exist near the current best particleâ€™s
position. So it is desirable to move the particle to this area
for further search. This may not hold for covering array
generation.

Here, we use CA(N; 2, 34 ) as an example. Suppose that
three test cases (0, 1, 0, 1), (2, 1, 1, 2), and (1, 2, 0, 0) have
been generated and added to TS in Algorithm 1, and the fourth
one is to be generated according to CPSO. A possible candidate particle pi may have the position (0, 0, 0, 0) and velocity
(0.5, 0.6, âˆ’0.4, 0.2) with fitness value 4, and its individual
best position pbesti may be (0, 0, 1, 1) with fitness value 5.
The global best position gbest may be (0, 2, 2, 2) with fitness
value 6. According to the update (2), if we take Ï‰ = 0.9 and
c1 Ã— r1 = c2 Ã— r2 â‰ˆ 0.65, in the next iteration, pi â€™s velocity
may be transformed to (0.45, 1.84, 1.59, 2.13). Thus, pi moves
to the new position (0, 1, 1, 1) with a limitation on the velocity (the velocity is kept in [âˆ’i /2, i /2]), or (0, 1, 1, 2) with
no limitation. In both cases, pi only has fitness value 2 after
updating. When this occurs, pi evolves to a worse situation,
although its position is closer to the pbesti and gbest than its
original one.
Analyzing the fitness measurement, the main contribution
to the fitness value is the combinations that the test case can
cover, not the concrete â€œpositionâ€ at which it is located. For
example, test case (2, 1, 1, 2) has a larger fitness value than
(0, 0, 0, 0) because it covers six new schemas [(2, 1, â€“, â€“),
(2, â€“, 1, â€“), (2, â€“, â€“, 2) etc.], not because of its relative distance
to other particles.
B. DPSO
To overcome this weakness, the movement of particles
should be modified. Inspired by the set-based representation scheme of velocity, DPSO is designed as a version of
S-PSO [18] to make the particle learn from the individual and
global best more effectively when generating covering arrays.
Unlike S-PSO, the element of the velocity set in DPSO is
designed for covering array generation, and DPSO does not
classify the velocity set into different dimensions to avoid the
inconsistency of different dimensions when updating velocities
in S-PSO.
In DPSO, a particleâ€™s position represents a candidate test
case, while its velocity is changed to a set of t-way schemas
with probabilities. Other than the velocityâ€™s representation and
the newly defined operators, the evolution procedure of DPSO
is the same as CPSO (Algorithm 2).
Definition 5 (Velocity): The velocity is a set of pairs
(s, p(s)), where s is a possible t-way schema of the covering
array and p(s) is a real number between 0 and 1 representing the probability of the selection of schema s to update the
current position.
In the initialization of the swarm, the particleâ€™s position is
randomly assigned. Then Cnt possible different schemas are
selected randomly and each of them is assigned a random
probability p(s) âˆˆ (0, 1). These Cnt pairs form the initial velocity set of this particle; the size of this set changes dynamically
during the evaluation.
We consider the same example CA(N; 2, 34 ). In DPSO,
when particle pi is initialized, its position xi (k) may be
(0, 0, 0, 0) representing a candidate test case as before, and
its velocity vi (k) may be such a set {((1, 1, â€“, â€“), 0.7),
((0, â€“, 0, â€“), 0.3), ((â€“, 0, â€“, 1), 0.8), ((0, â€“, â€“, 2), 0.9),

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

(a)

(b)

(c)

581

(d)

(e)

Fig. 1. Example of pi â€™s velocity updating. (a) 0.9 Ã— vi (k). (b) 2 Ã— r1 Ã— (pbesti âˆ’ xi (k)). (c) 2 Ã— r2 Ã— (gbest âˆ’ xi (k)). (d) vi (k + 1). (e) Final vi (k + 1) where
pro1 = 0.5.

((â€“, 2, 1, â€“), 0.5), ((â€“, â€“, 0, 1), 0.2)} which contains C42 = 6
pairs.
DPSO follows the conventional evolution procedure
(Algorithm 2) that uses (2) and (3) to update the velocity and
position of a particle. To adapt the new scheme for velocity
in Definition 5, the related operators in these equations must
be redefined.
1) Coefficient Ã— Velocity: The coefficient is a real number
which may be a parameter or a random number. It modifies
all the probabilities in the velocity.
Definition 6 (Coefficient Ã— Velocity): Let a be a nonnegative real number and v be a velocity, a Ã— v = {(s, p(s) Ã—
a)|(s, p(s)) âˆˆ v}. (If p(s) Ã— a > 1, p(s) Ã— a = 1.)
For example, Fig. 1(a) shows the result for Ï‰ Ã— vi (k) where
Ï‰ = 0.9.
2) Positionâ€“Position: The difference of two positions gives
the direction on which a particle moves. The results of the
minus operator is a set of (s, p(s)) pairs, as velocity.
Definition 7 (Positionâ€“Position): Let x1 and x2 be two positions. Then x1 âˆ’ x2 = {(s, 0.5)|s is a schema that exists in x1
but not in x2 }.
In the newly generated schema, probability p(s) for s is
set to 0.5 so that the acceleration constants take similar
values in both CPSO and DPSO. As in (2), the result of
positionâ€“position is multiplied by ci Ã— ri . In CPSO, ci is
often set to 2 and ri is a random number between 0 and 1
(recall Section II-C). In DPSO, we want the value of final
probability to have a range between 0 and 1 after multiplying
by ci . Setting p(s) to 0.5 puts the result in [0, 1] when ci = 2.
For example, suppose that xi (k) = (0, 0, 0, 0), pbesti =
(0, 0, 1, 1), and gbest = (0, 2, 2, 2) as before.
We can get pbesti âˆ’ xi (k) = {((0, â€“, 1, â€“, â€“), 0.5),
((0, â€“, â€“, 1), 0.5), ((â€“, 0, 1, â€“), 0.5), ((â€“, 0, â€“, 1), 0.5),
((â€“, â€“, 1, 1), 0.5)}. Fig. 1(b) and (c) shows the results for
2Ã—r1 Ã—(pbesti âˆ’xi (k)) and 2Ã—r2 Ã—(gbestâˆ’xi (k)) respectively.
3) Velocity + Velocity: The addition of velocities gives a
particleâ€™s movement path. The plus operator results in the
union of two velocities.
Definition 8 (Velocity + Velocity): Let v1 and v2 be two
velocities. Then v1 + v2 = {(s, p(s))| if (s, p1 (s)) âˆˆ v1 and
(s, p2 (s)) âˆˆ v2 , p(s) = p1 (s) + p2 (s); if (s, pi (s)) âˆˆ v1
and (s, pi (s)) âˆˆ
/ v2 or (s, pi (s)) âˆˆ v2 and (s, pi (s)) âˆˆ
/ v1 ,
p(s) = pi (s)}. (If p1 (s) + p2 (s) > 1, p1 (s) + p2 (s) = 1.)
For example, Fig. 1(d) shows the results for pi â€™s new
velocity vi (k + 1) after updating velocity.

Algorithm 3 Position Updating
1: Input: position xi (k), velocity vi (k + 1), pro2 and pro3
2: Output: new position xi (k + 1)
3: xi (k + 1) = (â€“, â€“, . . . , â€“)
4: Sort vi (k + 1) in descending order of p(s)
5: for each pair (si ,p(si )) in vi (k + 1) do
6:
Generate a random number Î± âˆˆ [0, 1]
7:
if Î± < p(si ) then
8:
for each fixed level  in si do
9:
Generate a random number Î² âˆˆ [0, 1]
10:
if Î² < pro2 and the corresponding factor
of  has not been fixed in xi (k + 1) then
11:
Update xi (k + 1) with 
12:
end if
13:
end for
14:
end if
15: end for
16: if xi (k + 1) has unfixed factors then
17:
Fill these factors by the same levels of previous
position xi (k)
18: end if
19: Generate a random number Î³ âˆˆ [0, 1]
20: if Î³ < pro3 then
21:
randomly change the level of one factor of xi (k + 1)
22: end if
23: return xi (k + 1)

In the velocity-updating phase of DPSO (2), we introduce
a new parameter pro1 to control the size of the final velocity
set. If p(si ) < pro1 , the pair (si , p(si )) is removed from the
final velocity. For example, if we set pro1 = 0.5, vi (k + 1) is
removed as shown in Fig. 1(e). Here, the velocity has been
sorted in descending order of p(s), where if several p(s) have
the same value, they are in an arbitrary order. If vi (k + 1)
becomes empty, it stays empty until new pairs are added to
it. As long as the velocity is empty, the particleâ€™s position is
not updated and no better solutions can be found from this
particle. In Section IV-C1, we discuss how to reinitialize this
particle.
4) Position+Velocity: Position plus velocity is the position
updating phase. Algorithm 3 gives the pseudo code of this
procedure. Here, two new parameters, pro2 and pro3 , numbers
in the range [0, 1], are introduced. pro2 is used to determine

582

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

the probability of selecting each fixed level in schema s and
pro3 is a mutation probability to make the particle mutate
randomly.
An example helps to describe this procedure. We already
have pi â€™s current position xi (k) = (0, 0, 0, 0), and its updated
velocity vi (k + 1) in descending order of p(s) as shown in
Fig. 1(e). We also assume that pro2 and pro3 are both set to
0.5. Each schema si here is selected to update the position
with probability p(si ). For the first pair ((0, â€“, â€“, 2), 1.0),
suppose that the random number Î± satisfies Î± < 1.0. Then,
for each fixed level of this pair, namely level 0 of the first
factor and level 2 of the fourth factor, its corresponding factor
has not been fixed in xi (k + 1). Suppose that we have the
first Î² < 0.5 but the second Î² > 0.5, the first factor will be
selected to update the position and the second factor will not.
So the new position becomes (0, â€“, â€“, â€“). For the second pair,
we regenerate the random number Î±, and compare it with the
probability 0.9. If Î± < 0.9, the second pair is selected. If we
generate Î² < 0.5 in two rounds, the new position becomes
(0, 2, 2, â€“). Accordingly, if the third pair is selected, and its
second factorâ€™s level 0 is chosen to update, it does not change
position because this factor has been set to a fixed level 2.
This procedure is repeated until all factors in the new position
xi (k+1) are set to fixed levels. If all pairs in velocity have been
considered, unfixed factors of xi (k + 1) are filled by the same
levels of previous position xi (k). For example, after finishing
the For loop in line 15, if the fourth factor of xi (k + 1) has
not been given any level, the fourth factor of xi (k) is used to
update it. Then xi (k + 1) becomes (0, 2, 2, 0).
C. Auxiliary Strategies
Auxiliary strategies can be introduced to improve the performance of PSO. Two of them are added to DPSO: 1) particle
reinitialization to make the search more effective and 2) additional evaluation of gbest to improve the selection of the global
best test case.
1) Particle Reinitialization: The PSO algorithm starts with
a random distribution of particles, which finally converge.
Then the best position that has been found is returned. The
swarm may jump out of a local optimum, but this can not
be guaranteed because CPSO lacks specific strategies for this.
When applying PSO for covering array generation, increasing
the number of iterations does not improve the ability to escape
a local optimum. Hence, particle reinitialization, a widely used
method, is employed to help DPSO to jump out of the local
optimum.
The main issue with particle reinitialization is when to reinitialize the particle. We may select a threshold, so that the
reinitialization is done when the number of iterations exceeds
the threshold. In DPSO, a better method can be applied. Using
the new representation for velocity, when the current particle pi â€™s position equals its individual best position pbesti
and global best position gbest, the size (norm) of pi â€™s velocity reduces gradually, because no pairs are generated from
(pbesti âˆ’ xi ) and (gbest âˆ’ xi ), and the original pairs in velocity
are removed gradually under the influence of Ï‰ Ã— v (reduce
the p(s) of original pairs) and parameter pro1 . After a few
fluctuations around gbest, the particle may stay at gbest, and

TABLE V
T WO D IFFERENT C ONSTRUCTIONS OF CA(N; 2, 34 )

its size (norm) of velocity is zero. We can use this scenario to
trigger reinitialization of the particle. When the reinitialization
is done, each dimension of a particleâ€™s position is randomly
assigned a valid value, and its velocity is regenerated as in the
initialization of the swarm.
2) Additional Evaluation of gbest: Current PSO methods
to generate covering arrays generally employ the fitness function in Definition 4. This fitness function only focuses on
the current candidate, and does not consider the partial test
suite TS.
Consider the generation of CA(N; 2, 34 ). Table V shows two
different construction processes. Both constructions generate
(0, 0, 0, 0) as the first test case. Then they choose different test
cases, but each of the first three reaches the largest number of
newly covered combinations, C42 = 6. The difference between
these two constructions emerges when generating the fourth
test case. In Construction 1, because the combinations with the
same level between any two factors have all been covered, we
cannot find a new test case that can still cover six combinations. However, in Construction 2, such a new test case can be
found, (1, 0, 1, 2). Because the minimum size of CA(N; 2, 34 )
is 9, each test case is required to cover six new combinations.
Thus, Construction 1 cannot generate the minimum test suite,
but Construction 2 can.
In general, there may exist multiple test cases with the same
highest fitness value, which make them equally qualified to be
gbest in Algorithm 2. Instead of arbitrarily selecting one as
gbest, it is better to apply additional distance metric to select
one among them. As shown in Table V, if the new test case
is similar to the existing tests [as (0, 1, 1, 1) is closer to
(0, 0, 0, 0) than (1, 1, 1, 1)], there may be a better chance to
find test cases with larger fitness subsequently.
In order to measure the â€œsimilarityâ€ between a test case t
and an existing test suite TS, we use the average Hamming
distance. The Hamming distance d12 indicates the number of
factors that have different levels between two test cases t1 and
t2 . Hence, the similarity between t and TS can be defined by
the average Hamming distance
H(t, TS) =

1 
dtk .
|TS|

(4)

kâˆˆTS

The new gbest selection strategy selects the best candidate gbest based on two metrics. A test case that has the
minimum average Hamming distance (i.e., the maximum similarity) from the set of test cases that all have the same highest
fitness value is selected as gbest in Algorithm 2. For example, when we generate the second test case for CA(N; 2, 34 )
in Table V, although (1, 1, 1, 1) and (0, 1, 1, 1) both have
the highest fitness value 6, (0, 1, 1, 1) is selected as gbest

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

583

TABLE VI
C OMPARING P ERFORMANCE B ETWEEN CPSO AND DPSO

because its average Hamming distance, 3, is smaller than that
of (1, 1, 1, 1), 4. We expect that this additional evaluation can
enhance the probability of generating a smaller test suite. In
addition, because we still want to make the particle follow the
conventional search behavior on the individual best direction,
this additional distance metric will not be used in updating the
pbest of DPSO.
V. E VALUATION AND PARAMETER T UNING
In this section, we first evaluate the effectiveness of DPSO
in some representative cases, and compare the results against
CPSO. Then the optimal parameter settings for both CPSO
and DPSO are explored. The goal of evaluation and parameter
tuning is to make the size of generated covering array as small
as possible. Five representative cases of covering arrays, listed
below, are selected for our experiments
CA1 (N; 2, 610 )

CA2 (N; 3, 57 )

CA3 (N; 4, 39 )

CA4 (N; 2, 43 53 62 ) VCA(N; 2, 315 , CA(3, 34 )).
We consider four independent parameters, iteration number
(iter), population size (size), inertia weight (Ï‰), and acceleration constant (c), which play similar roles in both CPSO
and DPSO, and three new parameters for DPSO, pro1 , pro2 ,
and pro3 . We carry out a base choice experiment to study the
impact of various values of these parameters on CPSO and
DPSOâ€™s performance and find the recommended settings for
them. First, a base setting is chosen as the basic configuration. Then the value of each parameter is changed to create
a series of configurations, while leaving the other parameters
unchanged. Initially, we set iter = 50, size = 20, Ï‰ = 0.9,
c = 1.3, and pro1 = pro2 = pro3 = 0.5 as a basic configuration based on the previous studies [13], [16] and our
empirical experience. To obtain statistically significant results,
the generation of each case of covering array is executed
30 times.
A. Evaluation of DPSO
We compare the performance between CPSO and DPSO
with the basic configuration. Five classes of covering arrays
are generated by these two algorithms. The sizes obtained and
average execution time per test case are shown as CPSO1 and
DPSO in Table VI. The best and mean array sizes of DPSO are
all better than those of CPSO1 . The new discrete representation scheme and auxiliary strategies improve PSO for covering
array generation.
DPSO can produce smaller covering arrays than CPSO with
the basic configuration. However, DPSO spends more time

during the evolution, because its new operators for updating
are more intricate than the conventional ones. DPSO needs
to deal with many elements of the velocity set, whereas the
conventional scheme only needs simple arithmetic operations.
To compare the performance between CPSO and DPSO given
the same execution time, for each case we let the execution
time per test case for CPSO equal to that for DPSO, so that
CPSO can spend more time in searching. We refer to this
version of CPSO as CPSO2 . In addition, a t-test between
CPSO2 and DPSO is conducted and the corresponding p-value
is presented. A p-value smaller than 0.05 indicates that the performance of these two algorithms is statistically different with
95% confidence.
From CPSO1 , CPSO2 , and DPSO in Table VI, the performance of CPSO is improved with more search time. DPSO,
which must use fewer iterations, still works better than CPSO.
The effectiveness of DPSO comes from the essential improvement of its new representation scheme for velocity and the
auxiliary strategies. The results of the t-test demonstrate the
significance of these differences. Therefore, we can conclude
that DPSO performs better than CPSO with fewer iterations
for covering array generation.
B. Parameter Tuning
In the base choice experiment, because the sizes of the various covering arrays differ, the mean size of each case (si )
obtained is normalized using
si =

si âˆ’ smin
.
smax âˆ’ smin

This normalization enables the graphical representation of
different cases on a common scale.
Because some parameters may not significantly impact the
performance, we use ANOVA (significance level = 0.05) to
test whether there exist significant differences among the mean
results obtained by different parameter settings. When changing the parameter settings have no significant impact on the
generation results, these results will be presented as dotted
lines in the corresponding figures. For example, CA4 is presented as a dotted line in Fig. 2(b). It means that the iteration
number does not significantly impact the generation of this
case of covering array, and so this case will not be further
considered when identifying the optimal settings.
1) Iteration Number (iter): Iteration number determines the
number of updates. PSO typically requires thousands of iterations to evolve. Nevertheless, only a few iterations are required
to generate covering array according to [14]â€“[16]. So we

584

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

(a)
Fig. 2.

(b)

Comparing array sizes under different iteration numbers. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 3.

Comparing array sizes under different population sizes. (a) CPSO. (b) DPSO.

(a)
Fig. 4.

(b)

Comparing array sizes under different settings of population size and iteration number. (a) CPSO. (b) DPSO.

change its value from 50, the base setting, to 1950 incrementally. Much larger values are not used because the execution
time may increase markedly without a commensurate increase
in the quality of the results.
Fig. 2 shows the results for different choices of iteration numbers, where each line represents a covering array.
Performance is improved with increasing iterations for both
CPSO and DPSO. A small number of iterations may not be
appropriate due to insufficient searching. Because the optimal
settings are different among different cases, and several settings can be chosen to generate the minimum covering arrays,
it is open to debate which setting is the â€œbestâ€ one. Given
time constraints, for a population size of 20, a good setting
of iteration number could be approximately 1000 for both
CPSO and DPSO.
2) Population Size (Size): Population size determines the
initial search space. Generally a small value, e.g., 20, is satisfactory for most cases. A large population size may bring a
higher diversity and find a better solution, but it also increases
the evolving time. For the same reason as before, we change
the value of population size from 10 to 100.

Fig. 3 shows the results for different choices of population size. There is no doubt that the mean array size obtained
decreases as population size increases. A large population size
can have more chances to generate smaller covering arrays, but
its execution time can become prohibitive.
In addition, because iteration number and population size
together determine the search effort of PSO, we explore the
combinations between these two parameters. We let iter Ã— size
be a constant 20 000, and generate each case under different
settings of these two parameters. Fig. 4 shows the results,
where PSO prefers a relatively larger population size. In
CPSO, ten particles with 2000 iterations is the worst setting,
and most cases produce good results with 60 or 80 particles.
In DPSO, the best choice of population size is still 80. In
both CPSO and DPSO, the largest population size 100 cannot
produce as good results as that of 80 due to having fewer iterations. So achieving smallest covering arrays requires a good
balance between these two parameters, and a moderately large
population size is necessary for both CPSO and DPSO. Thus,
we can set iteration number to 250, and population size to 80,
as the recommended settings for both CPSO and DPSO.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

(b)

(a)
Fig. 5.

Comparing array sizes under different inertia weights. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 6.

Comparing array sizes under different acceleration constants. (a) CPSO. (b) DPSO.

(a)
Fig. 7.

585

(b)

(c)

Comparing array sizes under different settings of (a) pro1 , (b) pro2 , and (c) pro3 .

3) Inertia Weight (Ï‰): Inertia weight determines the tendency of the particle to continue in the same direction. A
small value help the particle move primarily toward the best
position, while a large one is helpful to continue its previous
movement. A linearly decreasing value is also used as it can
make the swarm gradually narrow the search space. Here, we
investigate both fixed values and a linearly decreasing value
from 0.9 to 0.4 over the whole evolution (presented as â€œdecâ€).
Fig. 5 shows the results for different choices of inertia
weight. In CPSO, most of the smallest covering arrays are
generated by large fixed inertia weights. The decreasing value
does not perform as well as the large values, such as 0.9 for
CA1 , CA2 , and CA4 . So 0.9 can be the recommended choice
for CPSO. In DPSO, 1.1 is the worst choice because the particle may update its position based on its own velocity and
thus fail to learn from individual and global best positions.
The decreasing value can perform reasonably well, but a fixed
value 0.5 may be a better choice in that it keeps the effort of
global search moderate. Thus, we can recommend the fixed
inertia weight of 0.9 for CPSO, and 0.5 for DPSO.
4) Acceleration Constant (c): Acceleration Constants
c1 and c2 control the learning from pbest and gbest, respectively. Generally, they are set to the same value to balance

TABLE VII
R ECOMMENDED PARAMETER S ETTINGS

the influence of these two positions. Setting c1 and c2 to a
large value may make the particle more likely attracted to the
best position ever found, while a small one may make the
search far from the current optimal region. In this paper, we
set c1 = c2 = c, and vary c from 0.1 to 2.9.
Fig. 6 shows the results for different choices of acceleration
constant. Unlike other parameters, there is a consistent trend in
all five cases. In CPSO, the values larger than 0.5 all produce
good results. In DPSO, 1.3 can definitely be regarded as the
optimal value. Thus, we set 1.3 as the recommended value of
acceleration constant for both CPSO and DPSO.
5) pro1 , pro2 , and pro3 : These three parameters are new to
DPSO. Parameter pro1 determines the size of the final velocity. As those pairs whose probabilities are smaller than pro1
are removed from the final velocity set, a small value may
keep more pairs in the velocity for the next evolution, but it

586

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE VIII
S IZES (N) OF C OVERING A RRAYS FOR n FACTORS , E ACH H AVING T HREE L EVELS , W ITH C OVERING S TRENGTH t, CA(N; t, n, 3)

also requires more computation time. Parameter pro2 determines the probability of selecting each fixed level from each
schema when updating positions. A larger value may lead to
a quick construction of new position, but it may also lead
to fast convergence toward a local optimum. Parameter pro3
determines the mutation probability when updating positions.
A larger value may enhance the randomness, but it also lowers
the convergence speed. In this paper, values from 0.1 to 1.0
for these three parameters are investigated.
Fig. 7 shows the results. For pro1 , a large value is not
appropriate because it removes nearly all pairs from the final
velocity set. A medium value 0.5, which appears to lead to the
best result, may be the best choice. For pro2 , the smallest value
0.1 yields the best results. Slower construction of the new position may slow the convergence toward a local optimum, but it
also takes longer. So, in order to balance performance and execution time, we take 0.3 as the recommended value for pro2 .
For pro3 , a larger value may be a good choice. The frequent
mutation of new position may bring better results, but also
takes longer to converge. So, we take 0.7 as the recommended
value for pro3 .
In summary, the recommended parameter settings for PSO
for covering array generation are different from previously
suggested ones [13], [16]. Some parameters may significantly
impact the performance in some cases, and parameter tuning is necessary to enhance heuristic techniques for particular
applications. Naturally, the optimal settings vary for different cases of covering arrays. There may not exist a common
setting that can always lead to the best results. Because it is

impractical to tune parameters for each case in real life, we
suggest two general settings for CPSO and DPSO, as shown
in Table VII, which can typically lead to better performance
within reasonable execution time.
VI. C OMPARING A MONG PSO S
In this section, we compare the best reported array sizes
generated by PSO in [16] with our findings for CPSO, DPSO,
and four representative variants. Because the research in [16]
demonstrated that their generation results typically outperform
greedy algorithms, in this paper, we do not compare CPSO and
DPSO with greedy algorithms.
We implement both the original and discrete versions of
four variants (TVAC [24], CLPSO [25], APSO [26], and
DMS-PSO [27]) to generate covering arrays. Their discrete
versions are extended based on new representation scheme of
velocity and auxiliary strategies. We name them D-TVAC, DCLPSO, D-APSO, and D-DMS-PSO, respectively. In CLPSO,
a particle can learn from different particlesâ€™ pbest in each
dimension whereas we do not distinguish dimensions strictly
in DPSO. So in D-CLPSO, a particle can fully learn from
different particlesâ€™ pbest in all dimensions. That may weaken
the search ability of CLPSO. For the other three variants, they
can be directly extended based on DPSO.
All algorithms are compared using the same number of
fitness function evaluations. CPSO and DPSO use the recommended settings shown in Table VII. For the variants, iter,
size and pro1 , pro2 and pro3 for discrete versions are set to

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

587

TABLE IX
S IZES (N) OF C OVERING A RRAYS FOR S EVEN FACTORS , E ACH H AVING  L EVELS , W ITH C OVERING S TRENGTH t, CA(N; t, 7, )

their recommended values. Ï‰ and c are also set to recommended values unless they are adaptively adjusted during the
evolution, in which case their range is set to [0.4, 0.9] and
[0.8, 1.8], respectively. The new control parameters for these
variants follow their suggested settings.
Tables VIIIâ€“XIII give the results. Because of the execution
time, we only consider covering strengths from 2 to 4, and the
generation of each covering array is repeated 30 times. A t-test
(significance level = 0.05) is also conducted to test whether
there exists a significant difference between the mean sizes
produced by the two algorithms. In the first three columns, we
report the best and mean array sizes obtained from previous
results, CPSO and DPSO, where boldface numbers indicate
that the difference between CPSO and DPSO is significant
based on the t-test. In the last four columns, we report the mean
array sizes from the original and discrete versions of each PSO
variant (presented as meanc and meand respectively), where
boldface numbers indicate that the difference between meanc
and meand of each variant is significant.
A. Uniform Covering Arrays
Tables VIIIâ€“X present the results for uniform covering
arrays. We extend the cases considered in [16], where â€œâ€“â€
indicates the not available cases. In Tables VIII, we report
array sizes for n factors, each having three levels. In
Tables IX and X, we report array sizes for 7 and 10 factors,
each having  levels. Their covering strengths all range from
2 to 4.
Typically, CPSO can produce smaller sizes than those
reported in [16], demonstrating the effectiveness of parameter tuning. Furthermore, DPSO can produce the smallest best
and mean sizes in almost all cases, and usually the performance of DPSO is significantly better than CPSO. When the
covering strength t = 2 is adopted (Table VIII), DPSO does

not generally outperform CPSO. But when covering strength
increases, DPSO performs better. Sometimes the mean sizes
for DPSO are smaller than the best sizes for CPSO. Because
generating a covering array with higher covering strength is
more complicated, DPSO may be more appropriate for generating such covering arrays. In Tables IX and X, we also find
that DPSO is superior for covering arrays with high covering strength. Overall, these results show the effectiveness of
DPSO for uniform covering array generation.
Surprisingly, DPSO does not beat previous results for
CPSO when covering arrays have two levels for each factor
(Table IX). This appears to be a weakness of DPSO. Although
DPSO generates smaller covering arrays than previous results
and CPSO, other techniques may still yield better results than
DPSO (some best known sizes can be found in [38]).
B. VS Covering Array
Tables XIâ€“XIII give the results for VS covering arrays.
Based on CA(N; 2, 315 ), CA(N; 3, 315 ), and CA(N; 2, 42 52 63 ),
some different cases of sub covering arrays conducted in [16]
are examined. Their covering strengths are at most 4.
Generally, we can draw similar conclusions as for uniform
covering arrays. CPSO with the suggested parameter setting
can produce better results than reported sizes in some cases.
DPSO also usually beats them on the best and mean sizes.
In Table XI, often the difference between CPSO and DPSO
is not significant. In part this is because for the CA(3, 33 ),
CA(3, 33 )2 , CA(3, 33 )3 , and CA(4, 34 ), the same best results
are provably the minimum (e.g., the minimum size of the
CA(3, 33 ) is 3 Ã— 3 Ã— 3 = 27). For the other cases, although
sometimes the difference is not significant, DPSO can still generate smaller covering arrays. DPSO remains a good choice
for generating VS covering arrays. In Tables XII and XIII,
similar results can be found.

588

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE X
S IZES (N) OF C OVERING A RRAYS FOR T EN FACTORS , E ACH H AVING  L EVELS , W ITH C OVERING S TRENGTH t, CA(N; t, 10, )

TABLE XI
S IZES (N) OF VS C OVERING A RRAYS VCA(N; 2, 315 , CA)

TABLE XII
S IZES (N) OF VS C OVERING A RRAYS VCA(N; 3, 315 , CA)

For both uniform and variable cases of covering arrays,
parameter tuning can enhance CPSO to generate smaller covering arrays than previous work. DPSO can produce the smallest
covering arrays in nearly all cases, and in general the difference of performance between CPSO and DPSO is significant.
DPSO is an effective discrete version of PSO for covering
array generation.
C. PSO Variants
In order to further investigate the effectiveness of DPSO for
covering array generation, we implement the original versions
of four representative variants of PSO and extend them to their
discrete versions based on DPSO. We apply them to generate

the same cases for covering arrays. The mean sizes obtained
are shown in the last four columns in Tables VIIIâ€“XIII.
We first compare the mean sizes of original PSO variants
with those of CPSO and DPSO. In our experiments, TVACâ€™s
mean sizes are always larger than CPSOâ€™s. The linear adjustment of inertia weight and acceleration constant is not helpful
for CPSO for covering array generation. Typically, APSOâ€™s
mean sizes are also larger than CPSOâ€™s. Because APSO uses
a fuzzy system to classify different evolutionary states, its
ineffectiveness may result from inappropriate parameter settings. CLPSO and DMS-PSO can outperform CPSO typically.
Although on occasion they achieve comparable performance
with DPSO, they cannot perform as well as DPSO in most

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

589

TABLE XIII
S IZES (N) OF VS C OVERING A RRAYS VCA(N; 2, 43 53 62 , CA)

TABLE XIV
C OMPARING CPSO AND DPSO W ITH GA AND ACO

cases. That further demonstrates the effectiveness of DPSO
for covering array generation. Because these four algorithms
are representative PSO variants (as shown in Table IV), designing different neighborhood topologies and using multiswarm
techniques may have potential to improve CPSO for covering
array generation.
We next compare the original and discrete versions of each
variant. Except for CLPSO, the other three variants can be
improved using their discrete versions, and the improvement
is also significant. TVAC cannot outperform CPSO, but it
is enhanced by DPSO so that D-TVAC can produce smaller
mean sizes than CPSO in most cases. The linear adjustment
is helpful for the discrete version. For CLPSO, only in a few
cases is it improved by DPSO. Sometimes D-CLPSO even
leads to worse results (see Table X), due primarily to the
weakened search ability of its discrete version as explained in
Section VI. For APSO, DPSO can enhance its original version,
but D-APSO is still worse than DPSO. That may result from
inappropriate settings as explained before. For DMS-PSO,
sometimes DPSO does not enhance it (see Table XI).
However, in most cases D-DMS-PSO can outperform
DMS-PSO and has comparable performance with D-TVAC.
The multiswarm strategy is also helpful for the discrete
version.

In summary, the comparison study reveals that our suggested parameter settings are more suitable for covering array
generation. DPSO is an effective discrete version of PSO.
It can significantly outperform previous results and CPSO
in nearly all cases for uniform and VS covering arrays.
Furthermore, DPSOâ€™s representation scheme of a particleâ€™s
velocity and auxiliary strategies not only enhance CPSO, but
also typically enhance PSO variants. DPSO is a promising
improvement on PSO for covering array generation.
VII. C OMPARING DPSO W ITH GA AND ACO
Because GAs and ACO [8]â€“[12] have also been successfully
used for covering array generation, we compare CPSO and
DPSO with the reported array sizes in [9] and [11]. There are
no widely accepted benchmarks for the comparison of searchbased covering array generation, so we only consider these
two representative and competitive works.
Shiba et al. [9] applied both GA and ACO to generate
uniform covering arrays (CA1 to CA8 in Table XIV), and
Chen et al. [11] applied ACO to generate VS covering arrays
(VCA9 to VCA12 in Table XIV). They both set algorithm
parameters according to recommendations in related research
fields without parameter tuning, while our CPSO and DPSO

590

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

use the recommended settings shown in Table VII. To compare
these algorithms under the same number of fitness evaluations, for each case of covering array, the population size and
the number of iterations of CPSO and DPSO are modified
accordingly to satisfy the settings in [9] and [11]. Moreover,
Shiba et al. [9] and Chen et al. [11] both applied test minimization algorithms to further reduce the size of generated
covering arrays, while our CPSO and DPSO do not apply any
minimization algorithms.
Table XIV shows the comparison results, where boldface
numbers indicate the best array sizes obtained, and â€œâ€“â€ represents that the corresponding data is not available. The
generation of each case of covering array of CPSO and DPSO
is executed 30 times and the best and average results are
presented. Because we do not implement GA and ACO for
covering array generation, no statistical tests can be conducted
here. In addition, because the platforms used for collecting the
results differ, the comparison of computational time would not
be informative. We nevertheless present the execution times
of our CPSO and DPSO, which can serve as references for
practitioners.
From Table XIV, DPSO can outperform existing GA and
ACO for covering array generation, despite the latter two
applying test minimization algorithms. Because our DPSO is
a version of PSO designed and tuned for covering array generation, it suggests that tuned versions of GAs and ACOs may
be a promising area for further study. In addition, CPSO performs worse than GA and ACO in seven of 12 cases. That
may result from the improvement by minimization algorithms
in [9] and [11]. But for CA2 , CA3 , and CA6 , CPSO can still
achieve smaller covering arrays.
In summary, the results further demonstrate that DPSO is
an effective discrete version of PSO for covering array generation. Further investigations of tuned versions of GA and ACO
should be considered.
VIII. C ONCLUSION
Covering array generation is a key issue in CT. We developed a new DPSO for covering array generation, by adapting
S-PSO to generate covering arrays and incorporating two
auxiliary strategies to improve the performance. Parameter tuning was applied to both CPSO and DPSO to identify their
best parameter settings. The original and discrete versions of
four representative PSO variants were implemented and their
efficacy for covering array generation was compared. The performance of DPSO was also compared with other existing
evolutionary algorithms, GA and ACO.
DPSO can perform better than CPSO with fewer iterations, and the performance of CPSO and DPSO is significantly
impacted by their parameter settings. Different cases require
different parameter settings; there may not exist a single choice
that leads to the best results. After parameter tuning, we identified two recommended settings, which lead to relatively good
performance for CPSO and DPSO. Indeed, CPSO with our recommended parameter settings improves on previously reported
results, and our DPSO with recommended parameter settings usually beats CPSO. Typically, DPSO also significantly

enhances the performance of PSO variants. In addition, DPSO
often outperforms GA and ACO to generate covering arrays.
Consequently, DPSO is a promising improvement of PSO for
covering array generation.
Improvements on the methods here may be possible in
a number of ways. One would be to investigate further
evolution procedures and strategies proposed in PSO, such
as hybridizing with penalty approaches to handle discrete
unknowns [39], and compare the results with some exact
schemes like branch and bound method. A second would be
to examine one-column-at-a-time approaches or methods that
construct the entire array, rather than the one-row-at-a-time
approach adopted here. A third would be to incorporate
DPSO with other methods, in particular with test minimization
methods.
R EFERENCES
[1] C. Nie and H. Leung, â€œA survey of combinatorial testing,â€ ACM Comput.
Surv., vol. 43, no. 2, pp. 11.1â€“11.29, 2011.
[2] D. Kuhn and M. Reilly, â€œAn investigation of the applicability of
design of experiments to software testing,â€ in Proc. 27th Annu. NASA
Goddard/IEEE Softw. Eng. Workshop, Greenbelt, MD, USA, 2002,
pp. 91â€“95.
[3] M. Cohen, P. Gibbons, W. Mugridge, and C. Colbourn, â€œConstructing
test suites for interaction testing,â€ in Proc. 25th Int. Conf. Softw. Eng.,
Portland, OR, USA, 2003, pp. 38â€“48.
[4] M. B. Cohen, C. J. Colbourn, and A. C. Ling, â€œConstructing strength
three covering arrays with augmented annealing,â€ Discrete Math.,
vol. 308, no. 13, pp. 2709â€“2722, 2008.
[5] J. Torres-Jimenez and E. Rodriguez-Tello, â€œSimulated annealing for constructing binary covering arrays of variable strength,â€ in Proc. Congr.
Evol. Comput., Barcelona, Spain, Jul. 2010, pp. 1â€“8.
[6] B. Garvin, M. Cohen, and M. Dwyer, â€œEvaluating improvements to a
meta-heuristic search for constrained interaction testing,â€ Empir. Softw.
Eng., vol. 16, no. 1, pp. 61â€“102, 2011.
[7] J. Torres-Jimenez and E. Rodriguez-Tello, â€œNew bounds for binary
covering arrays using simulated annealing,â€ Inf. Sci., vol. 185, no. 1,
pp. 137â€“152, 2012.
[8] S. Ghazi and M. Ahmed, â€œPair-wise test coverage using genetic algorithms,â€ in Proc. Congr. Evol. Comput., vol. 2. Canberra, ACT, Australia,
2003, pp. 1420â€“1424.
[9] T. Shiba, T. Tsuchiya, and T. Kikuno, â€œUsing artificial life techniques to
generate test cases for combinatorial testing,â€ in Proc. 28th Annu. Int.
Comput. Softw. Appl. Conf., vol. 1. Hong Kong, 2004, pp. 72â€“77.
[10] J. McCaffrey, â€œAn empirical study of pairwise test set generation using
a genetic algorithm,â€ in Proc. 7th Int. Conf. Inf. Technol. New Gener.,
Las Vegas, NV, USA, 2010, pp. 992â€“997.
[11] X. Chen, Q. Gu, A. Li, and D. Chen, â€œVariable strength interaction
testing with an ant colony system approach,â€ in Proc. Asia-Pacific Softw.
Eng. Conf., Penang, Malaysia, 2009, pp. 160â€“167.
[12] X. Chen, Q. Gu, X. Zhang, and D. Chen, â€œBuilding prioritized pairwise
interaction test suites with ant colony optimization,â€ in Proc. 9th Int.
Conf. Qual. Softw., Jeju-do, Korea, 2009, pp. 347â€“352.
[13] X. Chen, Q. Gu, J. Qi, and D. Chen, â€œApplying particle swarm optimization to pairwise testing,â€ in Proc. 34th Annu. Comput. Softw. Appl.
Conf., Seoul, Korea, 2010, pp. 107â€“116.
[14] B. S. Ahmed and K. Z. Zamli, â€œPSTG: A T-way strategy adopting particle swarm optimization,â€ in Proc. 4th Asia Int. Conf. Math. Anal. Model.
Comput. Simulat., Kota Kinabalu, Malaysia, 2010, pp. 1â€“5.
[15] B. S. Ahmed and K. Z. Zamli, â€œA variable strength interaction test suites
generation strategy using particle swarm optimization,â€ J. Syst. Softw.,
vol. 84, no. 12, pp. 2171â€“2185, 2011.
[16] B. S. Ahmed, K. Z. Zamli, and C. P. Lim, â€œApplication of particle
swarm optimization to uniform and variable strength covering array
construction,â€ Appl. Soft Comput., vol. 12, no. 4, pp. 1330â€“1347, 2012.
[17] Y. del Valle, G. Venayagamoorthy, S. Mohagheghi, J.-C. Hernandez, and
R. Harley, â€œParticle swarm optimization: Basic concepts, variants and
applications in power systems,â€ IEEE Trans. Evol. Comput., vol. 12,
no. 2, pp. 171â€“195, Apr. 2008.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

[18] W.-N. Chen et al., â€œA novel set-based particle swarm optimization
method for discrete optimization problems,â€ IEEE Trans. Evol. Comput.,
vol. 14, no. 2, pp. 278â€“300, Apr. 2010.
[19] M. Clerc, Discrete Particle Swarm Optimization (New Optimization
Techniques in Engineering). New York, NY, USA: Springer, 2004.
[20] W. Pang et al., â€œModified particle swarm optimization based on space
transformation for solving traveling salesman problem,â€ in Proc. Int.
Conf. Mach. Learn. Cybern., vol. 4. Shanghai, China, Aug. 2004,
pp. 2342â€“2346.
[21] W. Pang, K.-P. Wang, C.-G. Zhou, and L.-J. Dong, â€œFuzzy discrete particle swarm optimization for solving traveling salesman problem,â€ in Proc.
4th Int. Conf. Comput. Inf. Technol., Wuhan, China, 2004, pp. 796â€“800.
[22] Y. Wang et al., â€œA novel quantum swarm evolutionary algorithm and its
applications,â€ Neurocomputing, vol. 70, nos. 4â€“6, pp. 633â€“640, 2007.
[23] E. Campana, G. Fasano, and A. Pinto, â€œDynamic analysis for the
selection of parameters and initial population, in particle swarm optimization,â€ J. Global Optim., vol. 48, no. 3, pp. 347â€“397, 2010.
[24] A. Ratnaweera, S. Halgamuge, and H. Watson, â€œSelf-organizing hierarchical particle swarm optimizer with time-varying acceleration coefficients,â€ IEEE Trans. Evol. Comput., vol. 8, no. 3, pp. 240â€“255,
Jun. 2004.
[25] J. Liang, A. Qin, P. Suganthan, and S. Baskar, â€œComprehensive learning particle swarm optimizer for global optimization of multimodal
functions,â€ IEEE Trans. Evol. Comput., vol. 10, no. 3, pp. 281â€“295,
Jun. 2006.
[26] Z.-H. Zhan, J. Zhang, Y. Li, and H.-H. Chung, â€œAdaptive particle swarm
optimization,â€ IEEE Trans. Syst., Man, Cybern., B, Cybern., vol. 39,
no. 6, pp. 1362â€“1381, Dec. 2009.
[27] J. Liang and P. Suganthan, â€œDynamic multi-swarm particle swarm optimizer,â€ in Proc. IEEE Swarm Intell. Symp., Pasadena, CA, USA, 2005,
pp. 124â€“129.
[28] D. Cohen, S. Dalal, M. Fredman, and G. Patton, â€œThe AETG system:
An approach to testing based on combinatorial design,â€ IEEE Trans.
Softw. Eng., vol. 23, no. 7, pp. 437â€“444, Jul. 1997.
[29] R. C. Bryce and C. J. Colbourn, â€œOne-test-at-a-time heuristic search for
interaction test suites,â€ in Proc. 9th Annu. Conf. Genet. Evol. Comput.,
London, U.K., 2007, pp. 1082â€“1089.
[30] J. Kennedy and R. Eberhart, â€œParticle swarm optimization,â€ in Proc. Int.
Conf. Neural Netw., vol. 4. Perth, WA, Australia, 1995, pp. 1942â€“1948.
[31] R. Eberhart and Y. Shi, â€œParticle swarm optimization: Developments,
applications and resources,â€ in Proc. Congr. Evol. Comput., vol. 1. Seoul,
Korea, 2001, pp. 81â€“86.
[32] R. Poli, J. Kennedy, and T. Blackwell, â€œParticle swarm optimization,â€
Swarm Intell., vol. 1, no. 1, pp. 33â€“57, 2007.
[33] S. Helwig, J. Branke, and S. Mostaghim, â€œExperimental analysis of
bound handling techniques in particle swarm optimization,â€ IEEE Trans.
Evol. Comput., vol. 17, no. 2, pp. 259â€“271, Apr. 2013.
[34] A. Windisch, S. Wappler, and J. Wegener, â€œApplying particle swarm
optimization to software testing,â€ in Proc. 9th Annu. Conf. Genet. Evol.
Comput., London, U.K., 2007, pp. 1121â€“1128.
[35] A. Ganjali, â€œA requirements-based partition testing framework using particle swarm optimization technique,â€ M.S. thesis, Dept. Electr. Comput.
Eng., Univ. Waterloo, Waterloo, ON, Canada, 2008.
[36] Y.-J. Gong et al., â€œOptimizing the vehicle routing problem with time
windows: A discrete particle swarm optimization approach,â€ IEEE
Trans. Syst., Man, Cybern. C, Appl. Rev., vol. 42, no. 2, pp. 254â€“267,
Mar. 2012.
[37] W.-N. Chen et al., â€œParticle swarm optimization with an aging leader and
challengers,â€ IEEE Trans. Evol. Comput., vol. 17, no. 2, pp. 241â€“258,
Apr. 2013.
[38] C. J. Colbourn. (Apr. 2013). Covering Array Tables for t=2, 3,
4, 5, 6. [Online]. Available: http://www.public.asu.edu/ ccolbou/src/
tabby/catable.html
[39] M. Corazza, G. Fasano, and R. Gusso, â€œParticle swarm optimization
with non-smooth penalty reformulation, for a complex portfolio selection
problem,â€ Appl. Math. Comput., vol. 224, pp. 611â€“624, Nov. 2013.

591

Huayao Wu received the B.S degree from Southeast
University, Nanjing, China and the M.S degree from
Nanjing University, Nanjing, China, where he is
currently working toward the Ph.D. degree from
Nanjing University, Nanjing.
His research interests include software testing,
especially on combinatorial testing and search-based
software testing.

Changhai Nie (Mâ€™12) received the B.S. and M.S.
degrees in mathematics from Harbin Institute of
Technology, Harbin, China, and the Ph.D. degree
in computer science from Southeast University,
Nanjing, China.
He is a Professor of Software Engineering
with State Key Laboratory for Novel Software
Technology, Department of Computer Science
and Technology, Nanjing University, Nanjing. His
research interests include software analysis, testing
and debugging.

Fei-Ching Kuo (Mâ€™06) received the B.Sc. (Hons.)
degree in computer science and the Ph.D. degree in
software engineering from Swinburne University of
Technology, Hawthorn, VIC, Australia.
She was a Lecturer with University of
Wollongong, Wollongong, NSW, Australia. She is
currently a Senior Lecturer with the Swinburne
University of Technology. Her research interests
include software analysis, testing, and debugging.

Hareton Leung (Mâ€™90) received the Ph.D. degree
in computer science from University of Alberta,
Edmonton, AB, Canada.
He is an Associate Professor and a Director
of the Laboratory for Software Development
and Management, Department of Computing,
Hong Kong Polytechnic University, Hong Kong. His
research interests include software testing, project
management, risk management, quality and process
improvement, and software metrics.

Charles J. Colbourn received the Ph.D. degree
from University of Toronto, Toronto, ON, Canada,
in 1980.
He is a Professor of Computer Science and
Engineering with Arizona State University, Tempe,
AZ, USA. He has authored the books The
Combinatorics of Network Reliability (Oxford) and
Triple Systems (Oxford), and also 320 refereed journal papers focussing on combinatorial designs and
graphs with applications in networking, computing,
and communications.
Prof. Colbourn received the Euler Medal for Lifetime Research
Achievement by the Institute for Combinatorics and its Applications in 2004.

Test Algebra for Combinatorial Testing
Wei-Tek Tsai , Charles J. Colbourn , Jie Luo , Guanqiu Qi , Qingyang Li , Xiaoying Bai
 School

of Computing, Informatics, and Decision Systems Engineering Arizona State University, Tempe, AZ, USA  State Key Laboratory of Software Development Environment School of Computer Science and Engineering, Beihang University, Beijing, China  Department of Computer Science and Technology, INLIST Tsinghua University, Beijing, China {wtsai, colbourn}@asu.edu, luojie@nlsde.buaa.edu.cn {guanqiuq, qingyan2b}@asu.edu, baixy@tsinghua.edu.cn

Abstract--This paper proposes a new algebraic system, Test Algebra (T A), for identifying faults in combinatorial testing for SaaS (Software-as-a-Service) applications. SaaS as a part of cloud computing is a new software delivery model, and mission-critical applications are composed, deployed, and executed in cloud platforms. Testing SaaS applications is a challenging task because new applications need to be tested when they are composed before they can be deployed for execution. Combinatorial testing algorithms can be used to identify faulty configurations and interactions from 2-way all the way to k-way where k is the number of components in the application. The T A defines rules to identify faulty configurations and interactions. Using the rules defined in the T A, a collection of configurations can be tested concurrently in different servers and in any order and the results obtained will be still same due to the algebraic constraints. Index Terms--Combinatorial testing, algebra, SaaS

I. I NTRODUCTION Software-as-a-Service (SaaS) is a new software delivery model. SaaS often supports three features: customization, multi-tenancy architecture (MTA), and scalability. MTA means using one code base to develop multiple tenant applications, and each tenant application essentially is a customization of the base code [12]. A SaaS system often also supports scalability as it can supply additional computing resources when the workload is heavy. Tenants' applications are often customized by using components stored in the SaaS database [14], [1], [11] including GUI, workflow, service, and data components. Once tenant applications are composed, they need to be tested, but a SaaS system can have millions of components, and hundreds of thousands of tenant applications. Testing tenant applications becomes a challenge as new tenant applications and components are added into the SaaS system continuously. New tenant applications are added on a daily basis while other tenant applications are running on the SaaS platform. As new tenant applications are composed, new components are added into the SaaS system. Each tenant application represents a customer for the SaaS system, and thus it needs to be tested. Combinatorial testing is a popular testing technique to test an application with different configurations. It often assumes that each component within a configuration has been tested

already, but interactions among components in the configuration may cause failures. Traditional combinatorial testing techniques often focus on test case generation to detect the presence of faults, but fault location is an active research area. Each configuration needs to be tested, as each configuration represents a tenant application. Traditional combinatorial testing such as AETG [2] can reveal the existence of faults by using few test cases to support t-way coverage for t  2. But knowing the existence of a fault does not indicate which tway interactions are faulty. When the problem size is small, an engineer can identify faults. However, when the problem is large, it can be a challenge to identify faults. As the IT industries moves to Big Data and cloud computing where hundreds of thousand processors are available, potentially, a large number of processors with distributed databases can be used to perform large-scale combinatorial testing. Furthermore, they also provide concurrent and asynchronous computing mechanisms such as MapReduce, automated redundancy and recovery management, automated resource provisioning, and automated migration for scalability. These capabilities provide significant computing power that was not available before. One simple way of performing combinatorial testing in a cloud environment is: 1) Partition the testing tasks; 2) Allocate these testing tasks to different processors in the cloud platform for test execution; 3) Collect results done by these processors. However, this is not efficient as while the number of computing and storage resources have increased significantly, the number of combinations to be considered is still too high. For example, a large SaaS system may have millions of components, and testing all of these combinations can still consume all the resources in a cloud platform. Two ways to improve this approach and both are based on learning from the previous test results:
·

·

Devise a mechanism to merge test results from different processors so that testing results can be merged quickly, and detect any inconsistency in testing; Based on the existing testing results, eliminate any con-

figurations or interactions from future testing. Due to the asynchronous and autonomous nature of cloud computing, test results may arrive asynchronously and autonomously, and this necessitates a new testing framework. This paper proposes a new algebraic system, Test Algebra (TA), to facilitate concurrent combinatorial testing. The key feature of TA is that the algebraic rules follow the combinatorial structure, and thus can track the test results obtained. The TA can then be used to determine whether a tenant application is faulty, and which interactions need to be tested. The TA is an algebraic system in which elements and operations are formally defined. Each element represents a unique component in the SaaS system, and a set of components represents a tenant application. Assuming each component has been tested by developers, testing a tenant application is equivalent to ensuring that there is no t-way interaction faults for t  2 among the elements in a set. The TA uses the principle that if a t-way interaction is faulty, every (t + 1)-way interaction that contains the t-way interaction as a subset is necessarily faulty. The TA provides guidance for the testing process based on test results so far. Each new test result may indicate if additional tests are needed to test a specific configuration. The TA is an algebraic system, primarily intended to track the test results without knowing how these results were obtained. Specifically, it does not record the execution sequence of previously executed test cases. Because of this, it is possible to allocate different configurations to different processors for execution in parallel or in any order, and the test results are merged following the TA rules. The execution order and the merge order do not affect the merged results if the merging follows the TA operation rules. This paper is structured as follows: Section II discusses the related work; Section III proposes TA and shows its details; and Section IV concludes this paper. Appendix provides proofs of TA associativity properties. II. R ELATED W ORK SaaS testing is a new research topic [14], [4], [10]. Using policies and metadata, test cases can be generated to test SaaS applications. Testing can be embedded in the cloud platform where tenant applications are run [14]. Gao proposed a framework for testing cloud applications [4], and proposed a scalability measure for testing cloud application scalability. Another scalability measure was proposed by [10]. Testing all combinations of inputs and preconditions is not feasible, even with a simple product [6], [8]. The number of defects in a software product can be large, and defects occurring infrequently are difficult to find [15]. Combinatorial test design is used to identify a small number of tests needed to get the coverage of important combinations. Combinatorial test design methods enable one to build structure variation into test cases for having greater test coverage with fewer test cases. Determining the presence of faults caused by a small number of interacting elements has been extensively studied

in component-based software testing. When interactions are to be examined, testing involves a combination-based strategy [5]. When every interaction among t or fewer elements is to be tested, methods have been developed that provide pairwise or t-way coverage. Among the early methods, AET G [2] popularized greedy one-test-at-a-time methods for constructing test suites. In the literature, the test suite is usually called a covering array, defined as follows. Suppose that there are k configurable elements, numbered from 1 to k . Suppose that for element c, there are vc valid options. A t-way interaction is a selection of t of the k configurable elements, and a valid option for each. A test selects a valid option for every element, and it covers a t-way interaction if, when one restricts the attention to the t selected elements, each has the same option in the interaction as it does in the test. A covering array of strength t is a collection of tests so that every t-way interaction is covered by at least one of the tests. Covering arrays reveal faults that arise from improper interaction of t or fewer elements [9]. There are numerous computational and mathematical approaches for construction of covering arrays with a number of tests as small as possible [3], [7]. If a t-way interaction causes a fault, then executing all tests of a covering array will reveal the presence of at least one faulty interaction. SaaS testing is interested in identifying those interactions that are faulty including their numbers and locations, as faulty configurations cannot be used in tenant applications. Furthermore, the number and location of faults keep on changing as new components can be added into the SaaS database continuously. By then executing each test, certain interactions are known not to be faulty, while others appear only in tests that reveal faults, and hence may be faulty. At this point, a classification tree analysis builds decision trees for characterizing possible sets of faults. This classification analysis is then used either to permit a system developer to focus on a small collection of possible faults, or to design additional tests to further restrict the set of possible faults. In [16], empirical results demonstrate the effectiveness of this strategy at limiting the possible faulty interactions to a manageable number. Assuming that interactions of more than t elements do not produce faults, a covering array can use few tests to certify that no fault arises from a t-way interaction. The Adaptive Reasoning algorithm (AR) is a strategy to detect faults in SaaS [13]. The algorithm uses earlier test results to generate new test cases to detect faults in tenant applications. It uses three principles:
·

·

·

Principle 1: When a tenant application (or configuration) fails the testing, there is at least one fault (but there may be more) in the tenant configuration. Principle 2: When a tenant application passes the testing, there is no fault in the tenant configuration resulting from a t-way interactions among components in the configuration. Principle 3: Whenever a configuration contains one or more faulty interactions, it is faulty.

III. T EST A LGEBRA Let C be a finite set of components. A configuration is a subset T  C . One is concerned with determining the operational status of configurations. To do this, one can execute certain tests; every test is a configuration, but there may be restrictions on which configurations can be used as tests. If a certain test can be executed, its execution results in an outcome of passed (operational) or failed (faulty). When a test execution yields result, all configurations that are subsets of the test are operational. However, when a test execution yields a faulty result, one only knows that at least one subset causes the fault, but it is unclear which of these subsets caused the failure. Among a set of configurations that may be responsible for faults, the objective is to determine, which cause faults and which do not. To do this, one must identify the set of candidates to be faulty. Because faults are expected to arise from an interaction among relatively few components, one considers t-way interactions. The t-way interactions are It = {U  C : |U | = t}. Hence the goal is to select tests, so that from the execution results of these tests, one can ascertain the status of all t-way interactions for some fixed small value of t. Because interactions and configurations are represented as subsets, one can use set-theoretic operations such as union, and their associated algebraic properties such as commutativity, associativity, and self-absorption. The structure of subsets and supersets also plays a key role. To permit this classification, one can use a valuation function V , so that for every subset S of components, V (S ) indicates the current knowledge about the operational status consistent with the components in S . The focus is on determining V (S ) whenever S is an interaction in I1  · · ·  It . These interactions can have one of five states. · Infeasible (X): For certain interactions, it may happen that no feasible test is permitted to contain this interaction. For example, it may be infeasible to select two GUI components in one configuration such that one says the wall is GREEN but the other says RED. · Faulty (F): If the interaction has been found to be faulty. · Operational (P): Among the rest, if an interaction has appeared in a test whose execution gave an operational result, the interaction cannot be faulty. · Irrelevant (N): For some feasible interactions, it may be the case that certain interactions are not expected to arise, so while it is possible to run a test containing the interaction, there is no requirement to do so. · Unknown (U): If neither of these occurs then the status of the interaction is required but not currently known. Any given stage of testing, an interaction has one of five possible status indicators. These five status indicators are ordered by X F P N U under a relation , and it has a natural interpretation to be explained in a moment. A. Learning from Previous Test Results The motivation for developing an algebra is to automate the deduction of the status of an interaction from the status

of tests and other interactions, particularly in combining the status of two interactions. Specifically, one is often interested in determining V (T1  T2 ) from V (T1 ) and V (T2 ). To do this, a binary operation  on {X, F, P, N, U} can be defined, with operation table as follows:  X F P N U X X X X X X F X F F F F P X F U N U N X F N N N U X F U N U

Using this definition, one can verify that the binary operation  has the following properties of commutativity and associativity. V (T1 )  V (T2 ) = V (T2 )  V (T1 ), V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). Using this operation, one observes that V (T1  T2 ) V (T1 )  V (T2 ). It follows that 1) Every superset of an infeasible interaction is infeasible. 2) Every superset of a failed interaction is failed or infeasible. 3) Every superset of an irrelevant interaction is irrelevant, failed, passed, or infeasible. A set S is an X-implicant if V (S ) = X but whenever S  S , V (S )  X. The X-implicants provide a compact representation for all interactions that are infeasible. Indeed for any interaction T that contains an X-implicant, V (T ) = X. Furthermore, a set S is an F-implicant if V (S ) = F but whenever S  S , V (S )  F. For any interaction T that contains an F-implicant, V (T ) F. In the same way, a set S is an N-implicant if V (S ) = N but whenever S  S , V (S ) = U. For any interaction T that contains an N-implicant, V (T ) N. An analogous statement holds for passed interactions, but here the implication is for subsets. A set S is a P-implicant if V (S ) = P but whenever S  S , V (S ) F. For any interaction T that is contained in a P-implicant, V (T ) = P. Implicants are defined with respect to the current knowledge about the status of interactions. When a t-way interaction is known to be infeasible, failed, or irrelevant, it must contain an X-, F-, or N-implicant. By repeatedly proceeding from tway to (t + 1)-way interactions, then, one avoids the need for any tests for (t + 1)-way interactions that contain any infeasible, failed, or irrelevant t-way interaction. Hence testing typically proceeds by determining the status of the 1-way interactions, then proceeding to 2-way, 3-way, and so on. The operation  is useful in determining the implied status of (t + 1)-way interactions from the computed results for t-way interactions, by examining unions of the t-way and smaller interactions and determining implications of the rule that V (T1  T2 ) V (T1 )  V (T2 ). Moreover, when adding further interactions to consider, all interactions previously tested that passed are contained in a P-implicant, and every (t + 1) interaction contained in one of these interactions can be assigned status P.

For example, V (a, b) = P, V (a, e) = X, and other 2way interactions exist as atomic interaction in TA. Based on the defined  operation, values of t-way interactions can be deduced from the atomic interactions and their contained interactions, such as V (a, b, e) V (a, b)  V (a, e) = X, i.e. V (a, b, e) = X. The 3-way interaction (a, b, c) can have inferred results from 2-way interactions (a, b), (a, c), (b, c). If any contained 2-way interaction has value F, the determining value of 3-way is F, without further testing needed. But if all values of contained 2-way interactions are P, (a, b, c) the interaction needs to be tested. In this case, U needs to be changed to non-U such as F or P, assuming the 3-way is not X or N. B. Changing Test Result Status When testing a configuration with n components, one should test individual components, 2-way interactions, 3-way interactions, all the way to n-way interactions. Since any combination of interactions is relevant in this case, the status of any interaction can be either X, F, P, or U. The status of a configuration is determined by the status of all interactions. 1) If an interaction has status X (F), the configuration has status X (F). 2) If all interactions have status P, the configuration has status P. 3) If some interactions still have status U, further tests are needed. It is important to determine when an interaction with status U can be deduced to have status F or P instead. It can never obtain status X or N once having had status U. To change U to P: An interaction is assigned status P if and only if it is a subset of a test that leads to proper operation. To change U to F: Consider the candidate T , one can conclude that V (T ) = F if there is a test containing T that yields a failed result, but for every other candidate interaction T that appears in this test, V (T ) = P. In other words, the only possible explanation for the failure is the failure of T . C. Matrix Representation Suppose that each individual component passed the testing. Then the operation table starts from 2-way interactions, then enlarges to t-way interactions step by step. During the procedure, many test results can be deduced from the existing results following TA rules. For example, all possible configurations of (a, b, c, d, e, f ) can be expressed in the form of matrix, or operation table. First, we show the operation table for 2-way interactions. The entries in the operation table are symmetric and those on the main diagonal are not necessary. So only half of the entries are shown. As shown in Figure 1, 3-way interactions can be composed by using 2-way interactions and components. Thus, following the TA implication rules, the 3-way interactions operation table is composed based on the results of 2-way combinations. Here, (a, b, c, d, e, f ) has more 3-way interactions than 2-way interactions. As seen in Figure 1, a 3-way interaction can be obtained through different combinations of 2-way interactions

and components. For example, {a, b, c} = {a}  {b, c} = {b}{a, c} = {c}{a, b} = {a, b}{a, c} = {a, b}{b, c} = {a, c}  {b, c}. V (a)  V (b, c) = V (c)  V (a, b) = V (a, b)  V (b, c) = P  P = U. But V (b)  V (a, c) = V (a, b)  V (a, c) = V (b, c)  V (a, c) = P  F = F. As TA defines the order of the five status indicators, the result should be the value with highest order. So V (a, b, c) = F.  a a b c d e f b P c d e F N X P X N F P F f U F P X U

D. Merging Concurrent Testing Results One way to achieve efficient testing is to allocate (overlapping or non-overlapping) tenant applications into different clusters, and each cluster is sent to a different set of servers for execution. Once each cluster completes its execution, the test results can be merged. The testing results of a specific interaction T in different servers should satisfy the following constraints. · If V (T ) = U in one cluster, then in other clusters, the same V (T ) can be either F, P, N, or U. · If V (T ) = N in one cluster, then in other clusters, the same V (T ) can be either F, P, N, or U. · If V (T ) = P in one cluster, then the same V (T ) can be either P, N, or U in all clusters; · If V (T ) = F in one cluster, then in other clusters, the same V (T ) can be F, N, or U. · If V (T ) = X in one cluster, then in other clusters, the same V (T ) can be X only. If these constraints are satisfied, then the testing results can be merged. Otherwise, there must be an error in the testing results. To represent this situation, a new status indicator, error (E), is introduced and E X. We define a binary operation  on {E, X, F, P, N, U}, with operation table as follows:  E X F P N U E E E E E E E X E X E E E E F E E F E F F P E E E P P P N E E F P N U U E E F P U U

 also has the properties of commutativity and associativity. See Appendix for proof of associativity. Using this operation, merging two testing results from two different servers can be defined as Vmerged (T ) = Vcluster1 (T )  Vcluster2 (T ). The merge can be performed in any order due to the commutativity and associativity of , and if the constraints of merge are satisfied and V (T ) = X, F, or P, the results cannot be changed by any further testing or merging of test results unless there are some errors in testing. If V (T ) = E, the testing

 a b c . . . f (a, b) (a, c) . . . (b, c) . . . (e, f )

a (a)

b (a, b) (b)

c (a, c) (b, c) (c)

··· ··· ··· ··· .. .

f (a, f ) (b, f ) (c, f ) . . . (f )

(a, b) (a, b) (a, b) (a, b, c) . . . (a, b, f ) (a, b)

(a, c) (a, c) (a, b, c) (a, c) . . .

··· ··· ··· ··· . . .

(b, c) (a, b, c) (b, c) (b, c) . . .

··· ··· ··· ··· . . .

(e, f ) (a, e, f ) (b, e, f ) (c, e, f ) . . . (e, f ) (a, b, e, f ) (a, c, e, f ) . . . (b, c, e, f ) . . . (e, f )

(a, c, f ) · · · (a, b, c) · · · (a, c) ··· .. .

(b, c, f ) · · · (a, b, c) · · · (a, b, c) · · · . . . . . . (b, c) ··· .. . ··· ··· ··· ··· . . . ··· ··· ··· . . . ··· .. . (e, f ) U U U . . . U U F . . . U . . .

 a b c . . . f (a, b) (a, c) . . . (b, c) . . . (e, f )

a

b P

c F P

··· ··· ··· ··· .. .

f U F P . . .

(a, b) U U U . . . U

(a, c) F F F . . . F F

··· ··· ··· ··· . . . ··· ··· ··· .. .

(b, c) U U U . . . U U F . . .

Fig. 1. 3-way interaction operation table

environment must be corrected and tests executed again after fixing the error(s) in testing. For example, Vcluster1 (a, c, e) = X and Vcluster2 (a, c, e) = F, so Vmerged (a, c, e) = X  F = E. It means that there is something wrong with the tests of interaction (a, c, e), and the problem must be fixed before doing further testing. Following the  associative rule, one can derive the following. V1 (T )  V2 (T )  V3 (T ) = (V1 (T )  V2 (T ))  V3 (T ) = V1 (T )  (V2 (T )  V3 (T )) = V1 (T )  V2 (T )  V3 (T )  V3 (T ) = (V1 (T )  V2 (T ))  (V2 (T )  V3 (T )) = ((V1 (T )  V2 (T ))  V2 (T ))  V3 (T ) = (V3 (T )  V2 (T ))  (V3 (T )  V1 (T )) Thus the  rule allows one to partition the configurations into different sets for different servers to run testing, and these sets do not need to be non-overlapping. In conventional cloud computing operations such as MapReduce, data should not overlap, otherwise incorrect data may be produced. For example, counting the items in a set can be performed by MapReduce, but data allocated to different servers cannot overlap, otherwise items may be counted more than once. In TA, this is not a concern due to the nature of the TA operations

. Once the results are available from each server, the testing results can be merged either incrementally, in parallel, or in any order. Furthermore, test results can be merged repeatedly without changing the final results. Using this approach, one can test say 10 batches of 100 tenant applications each, for a total of 1, 000 tenant applications. Once this is done, another batch of 1, 000 tenant applications can be tested with each 100 tenant application allocated to a server for execution. In this way, after running 100 batches, 100, 000 tenant applications can be evaluated completely. The following example illustrates the testing process of fifteen configurations. According to the definition of configuration testing, all feasible interactions should be tested. For simplicity, assume that only interaction (c, d, f ) is faulty, and only interaction (c, d, e) is infeasible, and all other interactions pass the testing. If one assigns configurations 1, 3, 5, 7, 9, 11, 13, 15 into Server1 , configurations 2, 4, 6, 8, 10, 12, 14 into Server2 , and 4-11 configurations into Server3 . If Server1 and Server3 do their own testing first, Server2 can reuse test results of interactions from them to eliminate interactions that need to be tested. For example, when testing 2-way interactions of configuration (b, c, d, f ) in Server2 , it can reuse the test results of (b, c), (b, d) of configuration (b, c, d, e) from Server3 , (b, f ) of configuration (a, b, c, f )

from Server1 . They are all passed, and it can reuse the test results of (b, c, d) of configuration (a, b, c, d) from Server1 , (b, c, f ) of configuration (a, b, c, f ) from Server1 , (b, d, f ) of configuration (a, b, d, f ) from Server1 , and (c, d, f ) of configuration (a, c, d, f ) from Server3 . Because (c, d, f ) is faulty, it can deduce that 4-way interaction (b, c, d, f ) is also faulty. For the sets of configuration that are overlapping, their returned test results from different servers are the same. The merged results of these results also stay the same. Not only interactions, sets of configurations, CS1 , CS2 , . . . , CSK can be allocated to different processors (or clusters) for testing, and the test results can then be merged. The sets can be non-overlapping or overlapping, and the merge process can be arbitrary. For example, say the result of CSi is RCSi , the merge process can be (· · · ((((RCS1 + RCS2 ) + RCS3 ) + RCS4 ) + · · · + RCSK ), or (· · · ((((RCSK + RCSk-1 ) + RCSk-2 ) + · · · + RCS1 ), or any other sequence that includes all RCSi , for i = 1 to K . This is true because RCS is simply a set of V (Tj ) for any intercation Tj in the configuration CSi . (a,b,c,d) (a,b,c,e) (a,b,c,f) (a,b,d,e) (a,b,d,f) (a,b,e,f) (a,c,d,e) (a,c,d,f) (a,c,e,f) (a,d,e,f) (b,c,d,e) (b,c,d,f) (b,c,e,f) (b,d,e,f) (c,d,e,f) Server1 P P P P P X F P P X F P P X P P P X F P P X Server2 P Server3

IV. C ONCLUSION This paper proposes TA to address SaaS combinatorial testing. The TA provides a foundation for concurrent combinatorial testing. The TA has two operations and test results can have five states with a priority. By using the TA operations, many combinatorial tests can be eliminated as the TA identifies those interactions that need not be tested. Also the TA defines operation rules to merge test results done by different processors, so that combinatorial tests can be done in a concurrent manner. The TA rules ensure that either merged results are consistent or a testing error has been detected so that retest is needed. In this way, large-scale combinatorial testing can be carried out in a cloud platform with a large number of processors to perform test execution in parallel to identify faulty interactions. ACKNOWLEDGMENT

This project is sponsored by U.S. National Science Foundation project DUE 0942453 and National Science Foundation M erged Results China (No.61073003), National Basic Research Program of China (No.2011CB302505), and the State Key Laboratory of P Software Development Environment (No. SKLSDE-2012ZXP 18), and Fujitsu Laboratory. P P R EFERENCES P [1] X. Bai, M. Li, B. Chen, W.-T. Tsai, and J. Gao. Cloud Testing Tools. In P Proceedings of IEEE 6th International Symposium on Service Oriented X System Engineering (SOSE), pages 1­12, Irvine, CA, USA, 2011. F [2] D. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG System: An Approach to Testing Based on Combinatorial Design. P Journal of IEEE Transactions on Software Engineering, 23:437­444, P 1997. X [3] C. J. Colbourn. Covering arrays and hash families. In D. Crnkovic and V. D. Tonchev, editors, Information Security, Coding Theory and Related F Combinatorics, volume 29 of NATO Science for Peace and Security P Series - D: Information and Communication Security. IOS Press, 2011. P [4] J. Gao, X. Bai, and W.-T. Tsai. SaaS Performance and Scalability Evaluation in Cloud. In Proceedings of The 6th IEEE International X
Symposium on Service Oriented System Engineering, SOSE '11, 2011. [5] M. Grindal, J. Offutt, and S. F. Andler. Combination Testing Strategies: A Survey. Software Testing, Verification, and Reliability, 15:167­199, 2005. [6] C. Kaner, J. Falk, and H. Q. Nguyen. Testing Computer Software, 2nd Edition. Wiley, New York, NY, USA, 1999. [7] V. V. Kuliamin and A. A. Petukhov. A Survey of Methods for Constructing Covering Arrays. Journal of Program Computer Software, 37(3):121­146, may 2011. [8] T. Muller and D. Friedenberg. Certified Tester Foundation Level Syllabus. Journal of International Software Testing Qualifications Board. [9] A. A. Porter, C. Yilmaz, A. M. Memon, D. C. Schmidt, and B. Natarajan. Skoll: A Process and Infrastructure for Distributed Continuous Quality Assurance. IEEE Transactions on Software Engineering, 33(8):510­525, 2007. [10] W.-T. Tsai, Y. Huang, X. Bai, and J. Gao. Scalable Architecture for SaaS. In Proceedings of 15th IEEE International Symposium on Object Component Service-oriented Real-time Distributed Computing, ISORC '12, Apr. 2012. [11] W.-T. Tsai, Y. Huang, and Q. Shao. Testing the Scalability of SaaS Applications. In Proceedings of IEEE International Conference on ServiceOriented Computing and Applications (SOCA), pages 1­4, Irvine, CA, USA, 2011. [12] W.-T. Tsai, Y. Huang, Q. Shao, and X. Bai. Data Partitioning and Redundancy Management for Robust Multi-Tenancy SaaS. International Journal of Software and Informatics (IJSI), 4(3):437­471, 2010.

E. Modified Testing Process Perform 2-way interaction testing first. Before going on to 3way interaction testing, use the results of 2-way testing to eliminate cases. The testing process stops when finishing testing all t-way interactions. The analysis of t-way interactions is based on the P T Rs of all (t - i)-way interactions for 1  i < t. The superset of infeasible, irrelevant, and faulty test cases do not need to be tested. The test results of the superset can be obtained by TA operations and must be infeasible, irrelevant, or faulty. But the superset of test cases with unknown indicator must be tested. In this way, a large repeating testing workload can be reduced. For n components, all t-way interactions for t  2 are composed by 2-way, 3-way, ..., t-way interactions. In n components combinatorial testing, the number of 2-way interactions is equal to n 2 . In general, the number of t-way interactions is equal to n t . More interactions are treated when n n > , which happens when t  n 2 . The total number t t-1 t of interactions examined is i=2 n . i

[13] W.-T. Tsai, Q. Li, C. J. Colbourn, and X. Bai. Adaptive Fault Detection for Testing Tenant Applications in Multi-Tenancy SaaS Systems. In Proceedings of IEEE International Conference on Cloud Engineering (IC2E), March 2013. [14] W.-T. Tsai, Q. Shao, and W. Li. OIC: Ontology-based Intelligent Customization Framework for SaaS. In Proceedings of International Conference on Service Oriented Computing and Applications(SOCA'10), Perth, Australia, Dec. 2010. [15] Wikipedia. Software Testing, 2013. [16] C. Yilmaz, M. B. Cohen, and A. Porter. Covering Arrays for Efficient Fault Characterization in Complex Configuration Spaces. In Proceedings of the 2004 ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA '04, pages 45­54, New York, NY, USA, 2004. ACM.

A PPENDIX The associativity of binary operation . V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). Proof: We will prove this property in the following cases. (1) At least one of V (T1 ), V (T2 ), and V (T3 ) is X. Without loss of generality, suppose that V (T1 ) = X, then according to the operation table of , V (T1 )  (V (T2 )  V (T3 )) = X  (V (T2 )  V (T3 )) = X, (V (T1 )  V (T2 ))  V (T3 ) = (X  V (T2 ))  V (T3 ) = X  V (T3 ) = X. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (2) V (T1 ), V (T2 ), and V (T3 ) are not X and at least one of V (T1 ), V (T2 ), and V (T3 ) is F. Without loss of generality, suppose that V (T1 ) = F, then according to the operation table of , the value of V (T2 )  V (T3 ) can only be F, N or U. So V (T1 )  (V (T2 )  V (T3 )) = F  (V (T2 )  V (T3 )) = F, (V (T1 )  V (T2 ))  V (T3 ) = (F  V (T2 ))  V (T3 ) = F  V (T3 ) = F. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (3) V (T1 ), V (T2 ), and V (T3 ) are not X or F and at least one of V (T1 ), V (T2 ), and V (T3 ) is N. Without loss of generality, suppose that V (T1 ) = N, then according to the operation table of , the value of V (T2 )  V (T3 ) can only be N or U. So V (T1 )  (V (T2 )  V (T3 )) = N  (V (T2 )  V (T3 )) = N, (V (T1 )  V (T2 ))  V (T3 ) = (N  V (T2 ))  V (T3 ) = N  V (T3 ) = N. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (4) V (T1 ), V (T2 ), and V (T3 ) are not X, F or N. In this case, V (T1 ), V (T2 ), and V (T3 ) can only be P or U. According to the operation table of , the value of V (T1 )  V (T2 ) and V (T2 )  V (T3 ) are U. So V (T1 )  (V (T2 )  V (T3 )) = V (T1 )  U = U, (V (T1 )  V (T2 ))  V (T3 ) = U  V (T3 ) = U. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). The associativity of binary operation . V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ). Proof: We will prove this property in the following cases. (1) One of V1 (T ), V2 (T ), and V3 (T ) is E. Without loss of generality, suppose that V1 (T ) = E, then according to the operation table of , V1 (T )(V2 (T )V3 (T )) = E(V2 (T ) V3 (T )) = E, (V1 (T )  V2 (T ))  V3 (T ) = (E  V2 (T ))  V3 (T ) = E  V3 (T ) = E. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ).

(2) V1 (T ), V2 (T ), and V3 (T ) are not E, and there is a pair of V1 (T ), V2 (T ), and V3 (T ) does not satisfy the constrains. Without loss of generality, suppose that V1 (T ) and V2 (T ) does not satisfy the constrains, then according to the operation table of , V1 (T )  V2 (T ) = E. So (V1 (T )  V2 (T ))  V3 (T ) = E  V3 (T ) = E. Since V1 (T ) and V2 (T ) does not satisfy the constrains, there can be two cases: (a) one of them is X and the other is not, or (b) one of them is P and the other is F. (a) If V1 (T ) = X, then V2 (T )  V3 (T ) cannot be X because V2 (T ) cannot be X. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. If V2 (T ) = X, then V2 (T )  V3 (T ) = X can only be E or X. Since V1 (T ) cannot be X, V1 (T )  (V2 (T )  V3 (T )) = E. (b) If V1 (T ) = P and V2 (T ) = F, then V2 (T )  V3 (T ) can only be E or F. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. If V1 (T ) = F and V2 (T ) = P, then V2 (T )  V3 (T ) can only be E or P. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ). (3) V1 (T ), V2 (T ), and V3 (T ) are not E, and V1 (T ), V2 (T ), and V3 (T ) satisfy the constrains. (a) One of V1 (T ), V2 (T ), and V3 (T ) is X. Without loss of generality, suppose that V1 (T ) = X, then V2 (T ) = V3 (T ) = X. So V1 (T )  (V2 (T )  V3 (T )) = X  (X  X) = X  X = X and (V1 (T )  V2 (T ))  V3 (T ) = (X  X)  X = X  X = X. (b) V1 (T ), V2 (T ), and V3 (T ) are not X, and one of V1 (T ), V2 (T ), and V3 (T ) is F. Without loss of generality, suppose that V1 (T ) = F, then V2 (T ) and V3 (T ) can only be F, N, or U. According to operation table of , V2 (T )  V3 (T ) can only be F, N, or U, and V1 (T )  V2 (T ) can only be F. So V1 (T )  (V2 (T )  V3 (T )) = F  (V2 (T )  V3 (T )) = F and (V1 (T )  V2 (T ))  V3 (T ) = F  V3 (T ) = F. (c) V1 (T ), V2 (T ), and V3 (T ) are not X or F, and one of V1 (T ), V2 (T ), and V3 (T ) is P. Without loss of generality, suppose that V1 (T ) = P, then V2 (T ) and V3 (T ) can only be P, N, or U. According to operation table of , V2 (T )  V3 (T ) can only be P, N, or U, and V1 (T )  V2 (T ) can only be F. So V1 (T )  (V2 (T )  V3 (T )) = P  (V2 (T )  V3 (T )) = P and (V1 (T )  V2 (T ))  V3 (T ) = P  V3 (T ) = P. (d) V1 (T ), V2 (T ), and V3 (T ) are not X, F or P, and one of V1 (T ), V2 (T ), and V3 (T ) is U. Without loss of generality, suppose that V1 (T ) = U, then V2 (T ) and V3 (T ) can only be N, or U. According to operation table of , V2 (T )  V3 (T ) can only be N, or U, and V1 (T )  V2 (T ) can only be U. So V1 (T )  (V2 (T )  V3 (T )) = U  (V2 (T )  V3 (T )) = U and (V1 (T )  V2 (T ))  V3 (T ) = U  V3 (T ) = U. (e) V1 (T ), V2 (T ), and V3 (T ) are N. V1 (T )  (V2 (T )  V3 (T )) = N  (N  N) = N  N = N and (V1 (T )  V2 (T ))  V3 (T ) = (N  N)  N = N  N = N. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ).

Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

Contents lists available at SciVerse ScienceDirect

Journal of Combinatorial Theory, Series A
www.elsevier.com/locate/jcta

Covering and packing for pairs
Yeow Meng Chee a , Charles J. Colbourn b , Alan C.H. Ling c , Richard M. Wilson d
a

Division of Mathematical Sciences, School of Physical and Mathematical Sciences, Nanyang Technological University, 21 Nanyang Link, Singapore 637371, Singapore b Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85287-8809, USA c Department of Computer Science, University of Vermont, Burlington, VT 05405, USA d Department of Mathematics, 253-37, California Institute of Technology, Pasadena, CA 91125, USA

a r t i c l e

i n f o

a b s t r a c t
When a v -set can be equipped with a set of k-subsets so that every 2-subset of the v -set appears in exactly (or at most, or at least) one of the chosen k-subsets, the result is a balanced incomplete block design (or packing, or covering, respectively). For each k, balanced incomplete block designs are known to exist for all sufficiently large values of v that meet certain divisibility conditions. When these conditions are not met, one can ask for the packing with the most blocks and/or the covering with the fewest blocks. Elementary necessary conditions furnish an upper bound on the number of blocks in a packing and a lower bound on the number of blocks in a covering. In this paper it is shown that for all sufficiently large values of v , a packing and a covering on v elements exist whose numbers of blocks differ from the basic bounds by no more than an additive constant depending only on k. © 2013 Elsevier Inc. All rights reserved.

Article history: Received 24 September 2011 Available online xxxx Keywords: Balanced incomplete block design Pair packing Pair covering Group divisible design Pairwise balanced design

1. Introduction Let v , k , and t be integers with v > k > t 2. Let  be a positive integer. A (t , )-packing of order v and blocksize k is a set V of v elements, and a collection B of k -element subsets (blocks) of V , so that every t -subset of V appears in at most  blocks. A (t , )-covering of order v and blocksize k is a set V of v elements, and a collection B of k -element subsets (blocks) of V , so that every t -subset

E-mail addresses: ymchee@ntu.edu.sg (Y.M. Chee), charles.colbourn@asu.edu (C.J. Colbourn), aling@cems.uvm.edu (A.C.H. Ling), rmw@caltech.edu (R.M. Wilson). 0097-3165/$ ­ see front matter © 2013 Elsevier Inc. All rights reserved. http://dx.doi.org/10.1016/j.jcta.2013.04.005

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1441

of V appears in at least  blocks. When  = 1, the simpler notation of t -packing or t -covering is used. When ( V , B ) is both a (t , )-packing and a (t , )-covering with blocksize k , it is a t -( v , k, ) design. v k A t -( v , k, ) design, if one exists, has  t / t blocks. When the required number of blocks is not integral, no such design can exist. Selecting all blocks containing a particular element x  V and deleting x from each forms the derived (t - 1)-( v - 1, k - 1, ) design (with respect to x). For a design v -i k -i to exist, evidently the derived design must exist; hence for a t -( v , k, ) design to exist,  t -i / t -i must be integral for every 0 i t . When these conditions are not all met, one can ask instead for the largest (t , )-packing, or for the smallest (t , )-covering, of order v and blocksize k . The Johnson bound [13] states that such a packing can have no more than

n k

···

n - t + 2 (n - t + 1) k-t +2 k-t +1

···

blocks, while the Schönheim bound [21] states that such a covering can have no fewer than

n k

···

n - t + 2 (n - t + 1) k-t +2 k-t +1

···

blocks. Our main result is that when t = 2 and  = 1, there exist packings and coverings whose sizes are within a constant of these bounds. Determining when these bounds are met exactly is a challenging question.  and Hanani [9] conjectured that, for fixed k and t , with all blocks of size k , In 1963, Erdos n a t -packing on n elements with /k (1 - o(1)) blocks and a t -covering on n elements with t t

/ t (1 + o(1)) blocks both exist. This was proved by Rödl [20], and has spawned a large literature t (for example, [10,11,14,15,23]). However, even when t = 2, all of these general constructions deviate from the Johnson and Schönheim bounds by an amount that grows as a function of the number of elements. Wilson [25] established that the necessary divisibility conditions for a 2-( v , k, ) design to exist are asymptotically sufficient (i.e., for fixed k and , and sufficiently large v ). This provides a  different means to establish the Erdos­Hanani conjecture for t = 2, but also does not immediately imply that one can find packings or coverings whose sizes are within a constant of the optimal sizes. Wilson [24] earlier considered this more challenging problem for packings, but the solution for the analogous problem for coverings has remained elusive. We focus on the case when t = 2 and  = 1 here. Caro and Yuster state stronger results for covering [3] and packing [2] than we prove here. Their approach relies in an essential manner on a strong statement by Gustavsson [12]:
Proposition 1.1. Let H be a graph with  vertices and h edges, having degree sequence (d1 , . . . , d ). Then there exist a constant N H and a constant H > 0, both depending only on H , such that for all n > N H , if G is a graph on n vertices, m edges, and degree sequence (1 , . . . , n ) so that min(1 , . . . , n ) n(1 - H ), gcd(d1 , . . . , d ) | gcd(1 , . . . , n ), and h | m, then G has an edge partition (decomposition) into graphs isomorphic to H . We have not been able to verify the proof of Proposition 1.1. Indeed, while the result has been used a number of times in the literature, no satisfactory proof of it appears there. While we expect that the statement is true, we do not think that the proof in [12] is sufficient at this time to employ the statement as a foundation for further results. Therefore we adopt a strategy that is completely independent of Proposition 1.1, and independent of the results built on it. In the remainder of the paper, we first recall relevant known results. Then in Section 3, we determine the possible structure of optimal packings and coverings, in order to determine what can remain uncovered in a packing, and what must be covered more than once in a covering. This is done in general for packings and coverings with a single hole, in order to limit any deviation from the desired bound to the manner in which a (fixed size) hole is filled. In Section 4, the most important part of the proof is established, namely that in each congruence class, one finite example can be produced. Finally in Section 5, these single examples are shown to form the required ingredients to establish asymptotic existence.

n

k

1442

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

2. Background To proceed more formally, we require a number of definitions and preliminary results from combinatorial design theory; related background material can be found in [1,22]. A balanced incomplete block design (BIBD) is a 2-( v , k, ) design. Balanced incomplete block designs have been extensively studied because of their central role in numerous applications in experimental design, coding and information theory, communications, and connections with fundamental topics in algebra, finite geometry, number theory, and combinatorics (see [5,7] for examples). The general divisibility conditions (stated for v k general t earlier) require that  2  0 mod 2 and ( v - 1)  0 (mod k - 1). A group divisible design ( V , G , B ) is a finite set V of elements or points; a partition G = {G 1 , . . . , G s } of V (groups); and a set B of subsets of V (blocks), with the property that every 2-subset of V lying within a group appears in no block, while every 2-subset of V with elements from different groups appears in exactly  blocks. When K is a set of positive integers for which | B |  K whenever B  B , the design is a ( K , )-GDD. When  = 1, we write simply K -GDD. Its order is | V |, its index is , and u u when the multiset of group sizes {|G i |: 1 i s} is the same as the multiset its type is 1 1 · · ·  j . We write (k, )-GDD (or k -GDD formed by including u j copies of  j when  j = 0, for all 1 when  = 1) when K = {k}. A transversal design TD (k, n) is a (k, )-GDD of type nk . We write TD(k, n) when  = 1. A transversal design is idempotent if its element set is {1, . . . , k} × {1, . . . , n}, and its block set contains {{(i , j ): 1 i k}: 1 j n}. A pairwise balanced design with blocksizes K and order v (( K , )-PBD of order v ) is a ( K , )-GDD of type 1 v ; we write K -PBD when  = 1. Then a balanced incomplete block design ((k, )-BIBD) is a (k, )-PBD; we write k -BIBD when  = 1. An incomplete pairwise balanced design of order v with holesize h , blocksizes K , and index  is a triple ( V , H , B ) for which | V | = v , | H | = h , H  V , B contains a set of subsets of V for which | B |  K whenever B  B , and for every pair of distinct elements x, y  V , the number of blocks in {{x, y }  B  B } is 0 if {x, y }  H and  otherwise. The notation ( K , )-IPBD( v , h) is used; we may omit  when it is 1, and write k instead of K when K = {k}. Let K be a set of positive integers, each at least 2. Then define  ( K ) = gcd{k - 1: k  K } and k ( K ) = gcd 2 : kK . Wilson establishes a crucial asymptotic existence result: Theorem 2.1. (See [25].) Let K be a set of integers, each at least 2. Let  be a positive integer. For all sufficiently n large n satisfying (n - 1)  0 (mod  ( K )) and  2  0 (mod ( K )), there exists a ( K , )-PBD of order n. In particular for K = {k}, when (n - 1)  0 (mod k - 1),  exists a (k, )-BIBD of order n. Colbourn and Rödl prove a variant that we use: Theorem 2.2. (See [6].) Let  > 0. Let K = {k1 , . . . , km } be a set of block sizes. Let { p 1 , . . . , pm } be nonm negative numbers with i =1 p i = 1. For all sufficiently large v satisfying v - 1  0 (mod  ( K )) and v  0 (mod ( K )), there is a K -PBD of order v in which, for each 1 i m, the fraction of pairs appearing 2 in blocks having size ki is in the range [ p i -  , p i +  ]. A stronger version of Theorem 2.2 is given in [26], and a variant for resolvable designs appears in [8]. Perhaps the most powerful generalization of Theorem 2.1 is due to Lamken and Wilson [16]. We in(r ,) be a complete digraph on n vertices with exactly  edges of color i joining troduce this next. Let K n (r ,) is any vertex x to any vertex y for every color i in a set of r colors. A family F of subgraphs of K n (r ,) (r ,) if every edge e  E ( K n ) belongs to exactly one member in F . Given a fama decomposition of K n (r ,) is a decomposition F such that every ily  of edge-r -colored digraphs, a  -decomposition of K n graph F  F is isomorphic to some graph G   . For a vertex x of an edge-r -colored digraph G , the degree-vector of x is the 2r -vector d(x) = (in1 (x), out1 (x), in2 (x), out2 (x), . . . , inr (x), outr (x)), where in j (x) and out j (x) denote the indegree and outdegree of vertex x in the spanning subgraph of G by
n 2

 0 mod

k 2

, and n is sufficiently large, there

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1443

edges of color j , respectively, for 1 j r . We denote by  (G ) the greatest common divisor of the integers t such that the 2r -vector (t , t , . . . , t ) is an integral linear combination of the vectors d(x) as x ranges over the vertex set V (G ) of G . Equivalently,  (G ) is the smallest positive integer t 0 such that (t 0 , t 0 , . . . , t 0 ) is an integral linear combination of the vectors {d(x)}. Let  be a family of simple edge-r -colored digraphs and let  () denote the greatest common divisor of the integers t such that the 2r -vector (t , t , . . . , t ) is an integral linear combination of the vectors {d(x)} as x ranges over all vertices of all graphs in  . For each graph G   , let (G ) = (m1 , m2 , . . . , mr ), where mi is the number of edges of color i in G . We denote by () the greatest common divisor of the integers m such that (m, m, . . . , m) is an integral linear combination of the vectors {(G ): G   }. Equivalently, () is the smallest positive integer m0 such that (m0 , m0 , . . . , m0 ) is an integral linear combination of the (r ,) vectors {(G )}. A graph G 0   is useless when it cannot occur in any  -decomposition of K n .  is admissible when no member of  is useless. Theorem 2.3. (See [16].) Let  be an admissible family of simple edge-r-colored digraphs. For all sufficiently (r ,) large n satisfying (n - 1)  0 (mod  ()) and n(n - 1)  0 (mod ()), a  -decomposition of K n exists. Theorem 2.3 has numerous consequences for the existence of various classes of combinatorial designs. Building on Theorem 2.3, Liu establishes the following: Theorem 2.4. (See [17].) Let K be a set of integers, each at least 2. Let m and  be positive integers. For all n sufficiently large n satisfying m(n - 1)  0 (mod  ( K )) and m2 2  0 (mod ( K )), there exists a ( K , )n GDD of order m . Mohácsy and Ray-Chaudhuri prove a result for a fixed number of groups when the index is 1. Theorem 2.5. (See [18,19].) Let k and u be integers with u k 2. For all sufficiently large m satisfying m(u - 1)  0 (mod k - 1) and m2 u (u - 1)  0 (mod k(k - 1)), there exists a k-GDD of type mu .  and Straus: This subsumes a classical result of Chowla, Erdos, Theorem 2.6. (See [4].) Let k 2 be an integer. For all sufficiently large m, there exists a TD(k, m).

3. Packings, coverings, and the optima We use known asymptotic existence results to treat asymptotic existence of packings and coverings in the cases that a k -BIBD does not exist. We require further definitions, to extend packings and coverings to have a `hole'. A packing with blocksize k with a hole ( V , H , B ) is a set V of elements, a subset (hole) H  V , and a set B of k -subsets of V , so that for every {x, y }  V , {x, y }  H , there is at most one B  B with {x, y }  B ; when {x, y }  H , there is no block B  B with {x, y }  B . The leave  of ( V , B ) is a graph with vertex set V ; pair {x, y } appears as an edge if and only if {x, y } H and is not a subset of any block of B . A covering with blocksize k with a hole ( V , H , B ) is a set V of elements, a subset H  V , and a set B of k -subsets of V , so that for every {x, y }  V , {x, y }  H , there is at least one B  B with {x, y }  B . The excess  of ( V , B ) is a multigraph with vertex set V ; the number of times pair {x, y } appears as an edge is exactly xy when {x, y }  H , and xy - 1 otherwise, where xy is the number of blocks of B that contain {x, y }. A packing with blocksize k ( V , B ) is a packing with blocksize k with a hole ( V , , B ), and a covering with blocksize k ( V , B ) is a covering with blocksize k with a hole ( V , , B ). A maximum packing with blocksize k is a packing with blocksize k ( V , B ) with the most blocks among all packings with blocksize k on | V | elements; equivalently, its leave has the fewest edges. A minimum covering with blocksize

1444

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

k is a covering with blocksize k ( V , B ) with fewest blocks among all coverings with blocksize k on | V | elements; equivalently, its excess has the fewest edges. Suppose that ( V , H , B ) is a packing with blocksize k with a hole, with v = | V |, h = | H |, and n = | V \ H |. Let x be a vertex in V \ H . The number of pairs on V that contain x is congruent to v - 1 modulo k - 1. The number containing x that appear in blocks of B is congruent to 0 modulo k - 1. Hence x has degree congruent to v - 1 modulo k - 1 in the leave. When the hole is nonempty, elements in the hole have degrees congruent to n modulo k - 1 in the leave. By the same token, in the excess of a covering with blocksize k with a hole, x has degree congruent to -( v - 1) modulo k - 1; elements in the hole have degrees congruent to -n modulo k - 1. We employ specific types of packings and coverings with holes in which the leave or excess has all vertices in the hole of degree 0. For an integer n  0 (mod k(k - 1)) and an integer h 1, let   h - 1 (mod k - 1) and  -(h - 1) (mod k - 1) with 0 , < k - 1. Then an optimum packing with blocksize k with a hole, k -OP(n + h, h), is a packing with blocksize k on n + h elements whose leave has degree  on each vertex not in the hole, and 0 on each vertex in the hole; and an optimum covering with blocksize k with a hole, k -OC(n + h, h), is a covering with blocksize k on n + h elements whose excess has degree on each vertex not in the hole, and 0 on each vertex in the hole. When h  1 (mod k - 1),  = = 0. In this case, a k-OP( v , h) and a k-OC( v , h) are the same, and are equivalent to a k -IPBD( v , h). In any packing with blocksize k on v = n + h elements with n  0 (mod k(k - 1)), no vertex can have degree smaller than  in the leave; and in any covering with blocksize k on v = n + h in the excess. Indeed, elements with n  0 (mod k(k - 1)), no vertex can have degree smaller than choosing and  so that v - 1 ; ,   v - 1 (mod k - 1);  - < k - 1; and =  when v v  1 (mod k - 1), every packing with blocksize k on v elements contains at most v ,k = k(k- 1)
v blocks, while every covering with blocksize k on v elements contains at least L v ,k = k( blocks. k-1) Then v ,k is at least the Johnson bound, and  v ,k is at most the Schönheim bound. The purpose of this paper is to prove the following two results.

Theorem 3.1. There is a constant pk such that for all v k, the number of blocks in a maximum packing with blocksize k on v elements is at least v ,k - pk and at most v ,k . Theorem 3.2. There is a constant ak such that for all v k, the number of blocks in a minimum covering with blocksize k on v elements is at least L v ,k and at most L v ,k + ak . We establish these results in a number of steps. Treating an arbitrary but fixed value of k , in Section 4, we show that for every c satisfying 0 c < k(k - 1), there exist positive integers nc  0 (mod k(k - 1)) and hc  c (mod k(k - 1)) so that a k -OP(nc + hc , hc ) exists; we also show that for every c satisfying 0 c < k(k - 1), there exist positive integers mc  0 (mod k(k - 1)) and c  c (mod k(k - 1)) so that a k -OC(mc + c , c ) exists. This provides a single example for optimal packings and coverings with a hole in every congruence class modulo k(k - 1). In Section 5, we use these results to establish that there exist integers k and uk , depending only on k , so that whenever v k , uk for which a k -OP( v , h) exists, and there also exists an uk for which a there exists an h k -OC( v , ) exists. From this, because uk is fixed and independent of v , we establish Theorems 3.1 and 3.2 by filling the holes. The crucial step, particularly for coverings, is producing one example in each congruence class. We treat this next. 4. One example in each congruence class In the case when h  1 (mod k - 1), a k -OP( v , h) and a k -OC( v , h) coincide with a k -IPBD( v , h), so we treat this situation first; subsequently the packing and covering cases differ. 4.1. Packing and covering: v  1 (mod k - 1) An incomplete transversal design ITD(k, n +  ; ) is a set V of k(n + ) elements, of which k form a hole H . The elements are partitioned into k groups G 1 , . . . , G k so that |G i  H | =  for 1 i k .

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1445

This set is equipped with a set of k -subsets (blocks) with the property that every pair of elements that appears in a group or appears in the hole H appears in no block, and every other pair appears in exactly one block. Lemma 4.1. Let k 2 be an integer. Let 0



k. For all sufficiently large n, an ITD(k, n +  ; ) exists.

Proof. Using Theorem 2.6, choose  so that a TD(k + 1, ), a TD(k + 1,  + 1), a TD(k + 1,  + 2), and a TD(k + 1,  + 3) all exist. Delete one group in each to form an idempotent TD(k, v ) for each v  {,  + 1,  + 2,  + 3}. For n sufficiently large, there is an { + 1,  + 2,  + 3}-PBD of order n +  + 1 containing a block of size  + 1 by Theorem 2.2. (Because  ({ + 1,  + 2,  + 3}) = 1 and ({ + 1,  + 2,  + 3}) = 1, this follows by choosing 0 <  < 1 and choosing the fraction of pairs 4 in blocks of size  + 1 to be 2 .) Delete all but  elements from a block of size  + 1, and remove the block of size  making a hole, to form an {,  + 1,  + 2,  + 3}-IPBD(n + , ). Give every element weight k , and use the idempotent TDs to inflate all blocks. The k elements arising from the  elements of hole in the IPBD form the hole of the ITD. 2 Lemma 4.2. Let h be an integer for which h  1 (mod k - 1) and k infinitely many integers  for which a k-IPBD( k(k - 1) + h, h) exists. h k(k - 1) + 1. Then there exist

h-k Proof. Let  = k -1 . Choose  so that a k -BIBD of order  (k - 1) + k and an ITD(k,  (k - 1) +  ; ) both exist. (Use Lemma 4.1 for the existence of the ITD.) Start with the ITD on the elements of V having a hole on the elements in H  V . Add k -  new elements N  . For 1 i k , let N i consist of the  elements in the i th group of the ITD that appear in H . Place on the elements of the i th group, together with N  , the blocks of a copy of the k -BIBD, omitting a block on the elements of N i  N  . On the  k(k - 1) + (k - 1) + k =  k(k - 1) + h elements of V  N  , all pairs are covered except k those within the hole on elements N   i =1 N i of size h = (k - 1) + k . 2

Corollary 4.3. Whenever c  1 (mod k - 1) and 0 c < k(k - 1), there are infinitely many integers nc and hc with nc  0 (mod k(k - 1)) and hc  c (mod k(k - 1)) so that a k-OP(nc + hc , hc ) exists. Proof. Set hc = c if c nc =  k(k - 1). 2 k , and hc = k(k - 1) + 1 if c = 1. Apply Lemma 4.2 with h = hc , and set

The same argument establishes: Corollary 4.4. Whenever c  1 (mod k - 1) and 0 c < k(k - 1), there are infinitely many integers mc and with mc  0 (mod k(k - 1)) and c  c (mod k(k - 1)) so that a k-OC(mc + c , c ) exists. 4.2. Packing: v  1 (mod k - 1) Lemma 4.5. For every integer c satisfying 0 c < k(k - 1), there exist an nc  0 (mod k(k - 1)) and an hc  c (mod k(k - 1)) for which a k-OP(nc + hc , hc ) exists. Proof. When c > 0, write c = s(k - 1) + d with 1 d < k . When c = 0, set s = d = k - 1. If d = 1, apply Lemma 4.3. Otherwise choose   1 (mod k(k - 1)) and N >  so that N   (mod k - 1); a k -GDD of type d exists (Theorem 2.4); an  -BIBD of order N exists (Theorem 2.1); and an ITD(k, d( N -  ) + s, s) exists (Lemma 4.1). Treat the  -BIBD as an  -GDD of type 1 N -  1 by removing a block, and inflate using the k GDD of type d to form a k -GDD of type d N - (d )1 . Adjoin d - s infinite elements to the
c

1446

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

ITD(k, d( N -  ) + s, s). On each group together with the infinite elements, place a copy of the k GDD of type d N - (d )1 , aligning the group of size d on the s elements in the intersection of the group and the hole of the ITD, together with the d - s infinite elements. The result is a k GDD of type dk( N - ) (d + s(k - 1))1 . Treat this as a packing. On the dk( N -  ) points not in the large hole, the leave has degree d - 1, so the result is a k -OP(nc + hc , hc ) with nc = dk( N -  ) and hc = d + s(k - 1). Because dk  0 (mod k) and N -   0 (mod k - 1), nc  0 (mod k(k - 1)). Because d  d (mod k(k - 1)), hc  c (mod k(k - 1)). 2 4.3. Covering: v  1 (mod k - 1) We employ some further, more specialized, combinatorial objects to treat coverings for the remaining congruence classes. Let V be a set of elements; B be a set of k -subsets of V ; G = {G 1 , . . . , G r } be a partition of V , j t. and H = { H 1 , . . . , H t } be a partition of V . Suppose that |G i  H j | =  for all 1 i r , 1
j i Further suppose that for every 2-subset {x, y }  V , either {x, y }  , or there  i =1 2 j =1 2 is exactly one B  B with {x, y }  B , but not both. Then ( V , G , H, B ) is a double group divisible design with blocksize k (k -DGDD) of type (r )t . A holey transversal design with blocksize k (k -HTD) of type r is a k -DGDD of type (r )k .

r

G

t

H

Theorem 4.6. Let k

2 be an integer. For all sufficiently large r, there exists a k-HTD of type 2r .

Proof. Choose K = {x1 , . . . , xs } so that  ( K ) = ( K ) = 1, and so that for each 1 i s , xi is large enough to ensure that Theorem 2.6 yields a TD(k + 1, xi ). Remove one group (and rename elements as needed) to form an idempotent TD(k, xi ). When r is large enough, Theorem 2.4 yields a K -GDD ( V , G , B ) of type 2r with groups G = {G 1 , . . . , G r }. The elements of the k-HTD to be formed are V × {0, . . . , k - 1}. For each block B  B , on the elements B × {0, . . . , k - 1}, align the k groups on { B × {i }: 0 i < k} to place the blocks of an idempotent TD(k, | B |). In the resulting design, one set of j r. 2 groups is formed by V × {i } for 0 i < k , the other by G j × {0, . . . , k - 1} for 1 Theorem 4.7. Let k 2 be an integer. For all sufficiently large integers r and t satisfying t - 1  0 (mod k - 1) t k and 2  0 mod 2 , there exists a k-DGDD of type (2r )t . Proof. Apply Theorem 2.1 to form a k -BIBD ( V , B ) with t elements. Apply Theorem 4.6 to form a k -HTD of type 2r . To form the k -DGDD, use elements V × {a, b} × {1, . . . , r }. For every B  B , place a copy of the HTD on B × {a, b} × {1, . . . , r }, aligning groups of size 2k on B × {a, b} × {i } for 1 i r , and groups of size 2r on {x} × {a, b} × {1, . . . , r } for x  B . 2 The key construction follows: Theorem 4.8. Let t , r , y be positive integers so that r  0 (mod k(k - 1)), t  1 (mod k(k - 1)), and y  2 (mod k - 1). Suppose that there exist (1) a k-DGDD of type (2r )t ; (2) a k-BIBD on 2t + k - 2 elements; (3) a k-OC(2r + y , y ). Then there is a k-OC(2rt + k - 2 + y , 2r + y + k - 2). Proof. Let V = {ai , j , b i , j : 1 i r , 1 j t } be the elements of the k -DGDD, with groups aligned j t } and H j = {ai , j , b i , j : 1 i r }. Let B be its set of blocks. Adjoin a so that G i = {ai , j , b i , j : 1 set C of k - 2 new elements. For 1 i r , on C  G i , form a k -BIBD on 2t + k - 2 elements, aligning a block on C  {ait , b it }; then delete that block, and call the resulting set of blocks Di . Adjoin a set R

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1447

with y further new elements. For 1 j < t , on R  H j place a k -OC(2r + y , y ) with the hole aligned on R , whose block set is E j . r We consider the design on the 2rt + k - 2 + y elements V  R  C with block set B  i =1 Di  All blocks have size k because each ingredient contains only blocks of size k . First we show that the design is a covering with a hole on R  C  H t . Two elements in the hole do not appear together in a block. An element from G i  H j with j < t appears in a block with each element of C  (G i  H t ) in Di ; it appears in a block with each element of R in E j ; and it appears with each element of H t \ G i in a block of B . Consider two distinct elements x  G i  H j and y  G m  H n with j , n < t . If i = m and j = n, then {x, y } = {ai , j , b i , j } appears in a block of Di (and also in at least one block of E j ). If i = m and j = n, then {x, y } appears in at least one block of E j . If i = m and j = n, then {x, y } appears in one block of Di . If i = m and j = n, then {x, y } appears in one block of B . Hence the design is a covering with a hole on R  C  H t . Secondly, we establish that it has the correct excess degrees to be an optimal covering with a hole, a k -OC. The design has 2rt + k - 2 + y elements. Because r  0 (mod k - 1), the number of elements satisfies 2rt + k - 2 + y  y - 1 (mod k - 1). The hole has 2r + k - 2 + y elements. Because r  0 (mod k - 1), the number of elements in the hole satisfies 2r + k - 2 + y  y - 1 (mod k - 1). y < k - 1. We must show that every element not in the Let y  -( y - 1) (mod k - 1) with 0 hole has degree y + 1 in the excess, and every element in the hole has degree 0 in the excess. r }. It We treat elements in the hole first. Each element of C appears only in blocks {Di : 1 i appears in r (t - 1) pairs to be covered, and appears in r (t - 1)/(k - 1) blocks, with (t - 1)/(k - 1) blocks arising in each of {Di : 1 i r } because this was constructed from a BIBD. Each element j < t }. Because elements of R have excess degree 0 in the k of R appears only in blocks {E j : 1 OC(2r + y , y ) forming E j , they have excess degree 0 in the union. Each element of H t appears only in blocks of B , and has excess degree 0. Now consider an element x  G i  H j , with j = t so that x is not in the hole. Then x appears in elements of B , Di and E j . It appears in 2(r - 1)(t - 1)/(k - 1) blocks of B , because it arises from the DGDD. It appears in (2t + k - 3)/(k - 1) blocks of Di , because it arises from a BIBD. Now in E j , x is not in the hole of the k -OC(2r + y , y ), and hence it arises 1 (2rt + k - 2 + y + y ) blocks, and in (2r - 1 + y + y )/(k - 1) blocks. So in total x appears in k- 1 because it appears in (2rt + k - 2 + y ) - 1 pairs, its excess degree is y + 1. Because 2r + y + k - 2  y - 1 (mod k - 1) and y  2 (mod k - 1), the result is the k -OC(2rt + k - 2 + y , 2r + y + k - 2). 2 Corollary 4.9. For each 0 c < k(k - 1), there exist integers mc and c  c (mod k(k - 1)) for which a k-OC(mc + c , c ) exists.
c t -1 j =1 E j .

with mc  0 (mod k(k - 1)) and

Proof. Let t 0 and r0 be integers with t 0  1 (mod k(k - 1)) and t 0 > 1 so that whenever r and t  1 (mod k(k - 1)), (1) there is a k -DGDD of type (2r )t (apply Theorem 4.7), and (2) there is a k -BIBD on 2t + k - 2 ( k (mod k(k - 1))) elements (apply Theorem 2.1).

r0 , t

t0 ,

When c  1 (mod k - 1), apply Corollary 4.4 to choose one k -OC(mc + c , c ) with mc  0 (mod k(k - 1)) and mc r0 . In general, when a k -OC(mc + c , c ) with mc  0 (mod k(k - 1)) and mc r0 exists, Theorem 4.8 produces a k -OC(mc t 0 + k - 2 + c , mc + c + k - 2). Set mc +k-2 mod k(k-1) = mc (t 0 - 1), which exceeds r0 and is a multiple of k(k - 1). Set c +k-2 mod k(k-1) = mc + c + k - 2  c + k - 2 (mod k(k - 1)). Then k - 2 applications of Theorem 4.8 handle all congruence classes. 2 5. Asymptotic existence Our next task is to handle not just one example for hole size in each congruence class modulo k(k - 1), but to extend to all sufficiently large orders. Theorem 5.1. Let k 2 be an integer. Then there are constants k and uk so that whenever v k-OP( v , h) and a k-OC( v , h) with h uk .

k , there is a

1448

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

Proof. By Corollary 4.9, for 0 c < k(k - 1) there are integers mc  0 (mod k(k - 1)) and c  c (mod k(k - 1)) for which a k -OC(mc + c , c ) exists. By Lemma 4.5, for 0 c < k(k - 1) there are integers nc  0 (mod k(k - 1)) and hc  c (mod k(k - 1)) for which a k -OP(nc + hc , hc ) exists. Set uk = max{nc + hc , mc + c : 0 c < k(k - 1)}. Using Theorem 2.4, choose an integer x for which k -GDDs of type (k(k - 1))x exist for all x  {x, x + 1, x + 2, x + 3}. Again using Theorem 2.4, choose an integer r for which an {x + 1, x + 2, x + 3}p uk and all g r . Then set k = rk(k - 1) + uk , a constant GDD of type p g exists for all 1 depending only on k . We develop the remainder of the proof for packings; that for coverings parallels it very closely. Let v k be an integer, and write v =  k(k - 1) + c with 0 c < k(k - 1). Write v - hc = gnc + d so that d  0 (mod k(k - 1)) and d < nc . Let n = nc /(k(k - 1)) and d = d/(k(k - 1)). g Construct a k -GDD of type nc d1 as follows. Form an {x + 1, x + 2, x + 3}-GDD of type (n ) g +1 . Delete all but d elements in one group to form an {x, x + 1, x + 2, x + 3}-GDD of type (n ) g (d )1 . Inflate using weight k(k - 1), employing k -GDDs of type (k(k - 1))x for x  {x, x + 1, x + 2, x + 3}, to g form a k -GDD of type nc d1 . Then add hc new elements, and place a k -OP(nc + hc , hc ) on each group of size nc together with the hc new elements, aligning the hole on these hc elements. The result is a k -OP( v , hc + d), and hc + d uk as required. 2 Proof of Theorem 3.1. When v < k , a maximum packing with blocksize k contains at least v ,k - 2k  blocks, and 2k is a constant. When v k , form a k -OP( v , h) with h uk , which has at least v ,k - uk uk blocks and is a constant. 2 2 2 Proof of Theorem 3.2. When v < k , a minimum covering with blocksize k requires at most 2k blocks, which is a constant. When v k , form a k-OC( v , h) with h uk , which has at most L v ,k blocks. A covering on h points in which every block contains some pair that is covered only once has u at most 2k blocks, which is a constant independent of v . Use this to fill the hole. 2 6. Conclusion For t = 2, our results establish that the elementary Johnson and Schönheim bounds are essentially the correct ones, in that the respective optima cannot differ from them by more than an additive constant. Unless this constant can be shown to be quite small, the specific value obtained for the constant is not of particular interest. Without recourse to Proposition 1.1 or a similar statement, we see no way at present to obtain differences from the bounds that are bounded by a quantity as small as (say) k in general, although it is plausible that such bounds hold. Acknowledgment We thank an anonymous referee for helpful comments on the presentation. References
[1] T. Beth, D. Jungnickel, H. Lenz, Design Theory, vol. I, second edition, Encyclopedia Math. Appl., vol. 69, Cambridge University Press, Cambridge, 1999. [2] Y. Caro, R. Yuster, Packing graphs: the packing problem solved, Electron. J. Combin. 4 (1) (1997), Research Paper 1, approx. 7 pp. (electronic). [3] Y. Caro, R. Yuster, Covering graphs: the covering problem solved, J. Combin. Theory Ser. A 83 (2) (1998) 273­282.  E.G. Straus, On the maximal number of pairwise orthogonal Latin squares of a given order, Canad. J. [4] S. Chowla, P. Erdos, Math. 12 (1960) 204­208. [5] C.J. Colbourn, J.H. Dinitz, D.R. Stinson, Applications of combinatorial designs to communications, cryptography, and networking, in: Surveys in Combinatorics, Canterbury, 1999, in: London Math. Soc. Lecture Note Ser., vol. 267, Cambridge Univ. Press, Cambridge, 1999, pp. 37­100. [6] C.J. Colbourn, V. Rödl, Percentages in pairwise balanced designs, Discrete Math. 77 (1­3) (1989) 57­63. [7] C.J. Colbourn, P.C. van Oorschot, Applications of combinatorial designs in computer science, ACM Comput. Surv. 21 (2) (1989) 223­250. [8] P. Dukes, A.C.H. Ling, Asymptotic existence of resolvable graph designs, Canad. Math. Bull. 50 (4) (2007) 504­518.





Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1449

[9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24]

[25] [26]

 H. Hanani, On a limit theorem in combinatorial analysis, Publ. Math. Debrecen 10 (1963) 10­13. P. Erdos, P. Frankl, V. Rödl, Near perfect coverings in graphs and hypergraphs, European J. Combin. 6 (4) (1985) 317­326. D.A. Grable, More-than-nearly-perfect packings and partial designs, Combinatorica 19 (2) (1999) 221­239. T. Gustavsson, Decompositions of large graphs and digraphs with high minimum degree, PhD thesis, Dept. of Mathematics, Univ. of Stockholm, 1991. S.M. Johnson, A new upper bound for error-correcting codes, IRE Trans. IT-8 (1962) 203­207. A.V. Kostochka, V. Rödl, Partial Steiner systems and matchings in hypergraphs, Random Structures Algorithms 13 (3­4) (1998) 335­347. N.N. Kuzjurin, On the difference between asymptotically good packings and coverings, European J. Combin. 16 (1) (1995) 35­40. E.R. Lamken, R.M. Wilson, Decompositions of edge-colored complete graphs, J. Combin. Theory Ser. A 89 (2) (2000) 149­ 200. J. Liu, Asymptotic existence theorems for frames and group divisible designs, J. Combin. Theory Ser. A 114 (3) (2007) 410­420. H. Mohácsy, The asymptotic existence of group divisible designs of large order with index one, J. Combin. Theory Ser. A 118 (7) (2011) 1915­1924. H. Mohácsy, D.K. Ray-Chaudhuri, An existence theorem for group divisible designs of large order, J. Combin. Theory Ser. A 98 (1) (2002) 163­174. V. Rödl, On a packing and covering problem, European J. Combin. 6 (1) (1985) 69­78. J. Schönheim, On coverings, Pacific J. Math. 14 (1964) 1405­1411. D.R. Stinson, Combinatorial Designs, Springer-Verlag, New York, 2004. V.H. Vu, New bounds on nearly perfect matchings in hypergraphs: higher codegrees do help, Random Structures Algorithms 17 (1) (2000) 29­63. R.M. Wilson, The construction of group divisible designs and partial planes having the maximum number of lines of a given size, in: Proc. Second Chapel Hill Conf. on Combinatorial Mathematics and its Applications, Univ. North Carolina, Chapel Hill, NC, 1970, Univ. North Carolina, Chapel Hill, NC, 1970, pp. 488­497. R.M. Wilson, An existence theory for pairwise balanced designs. III. Proof of the existence conjectures, J. Combin. Theory Ser. A 18 (1975) 71­79. R.M. Wilson, The proportion of various graphs in graph designs, in: R.A. Brualdi, S. Hedayat, H. Kharaghani, G. Khosrovshahi, S. Shahriari (Eds.), Combinatorics and Graphs: The Twentieth Anniversary Conference of IPM Combinatorics, American Mathematical Society, Providence, RI, 2010, pp. 251­255.

Test Algebra for Combinatorial Testing
Wei-Tek Tsaiâˆ— , Charles J. Colbournâˆ—â€  , Jie Luoâ€  , Guanqiu Qiâˆ— , Qingyang Liâˆ— , Xiaoying Baiâ€¡
âˆ— School

of Computing, Informatics, and Decision Systems Engineering
Arizona State University, Tempe, AZ, USA
â€  State Key Laboratory of Software Development Environment
School of Computer Science and Engineering,
Beihang University, Beijing, China
â€¡ Department of Computer Science and Technology, INLIST
Tsinghua University, Beijing, China
{wtsai, colbourn}@asu.edu, luojie@nlsde.buaa.edu.cn
{guanqiuq, qingyan2b}@asu.edu, baixy@tsinghua.edu.cn

Abstractâ€”This paper proposes a new algebraic system, Test
Algebra (T A), for identifying faults in combinatorial testing for
SaaS (Software-as-a-Service) applications. SaaS as a part of cloud
computing is a new software delivery model, and mission-critical
applications are composed, deployed, and executed in cloud
platforms. Testing SaaS applications is a challenging task because
new applications need to be tested when they are composed
before they can be deployed for execution. Combinatorial testing
algorithms can be used to identify faulty configurations and
interactions from 2-way all the way to k-way where k is the
number of components in the application. The T A defines rules
to identify faulty configurations and interactions. Using the rules
defined in the T A, a collection of configurations can be tested
concurrently in different servers and in any order and the results
obtained will be still same due to the algebraic constraints.
Index Termsâ€”Combinatorial testing, algebra, SaaS

I. I NTRODUCTION
Software-as-a-Service (SaaS) is a new software delivery
model. SaaS often supports three features: customization,
multi-tenancy architecture (MTA), and scalability. MTA means
using one code base to develop multiple tenant applications,
and each tenant application essentially is a customization
of the base code [12]. A SaaS system often also supports
scalability as it can supply additional computing resources
when the workload is heavy. Tenantsâ€™ applications are often
customized by using components stored in the SaaS database
[14], [1], [11] including GUI, workflow, service, and data
components.
Once tenant applications are composed, they need to be tested, but a SaaS system can have millions of components, and
hundreds of thousands of tenant applications. Testing tenant
applications becomes a challenge as new tenant applications
and components are added into the SaaS system continuously.
New tenant applications are added on a daily basis while other
tenant applications are running on the SaaS platform. As new
tenant applications are composed, new components are added
into the SaaS system. Each tenant application represents a
customer for the SaaS system, and thus it needs to be tested.
Combinatorial testing is a popular testing technique to test
an application with different configurations. It often assumes
that each component within a configuration has been tested

already, but interactions among components in the configuration may cause failures. Traditional combinatorial testing
techniques often focus on test case generation to detect the
presence of faults, but fault location is an active research area.
Each configuration needs to be tested, as each configuration
represents a tenant application. Traditional combinatorial testing such as AETG [2] can reveal the existence of faults by
using few test cases to support t-way coverage for t â‰¥ 2. But
knowing the existence of a fault does not indicate which tway interactions are faulty. When the problem size is small,
an engineer can identify faults. However, when the problem
is large, it can be a challenge to identify faults.
As the IT industries moves to Big Data and cloud computing where hundreds of thousand processors are available,
potentially, a large number of processors with distributed
databases can be used to perform large-scale combinatorial testing. Furthermore, they also provide concurrent and
asynchronous computing mechanisms such as MapReduce,
automated redundancy and recovery management, automated
resource provisioning, and automated migration for scalability.
These capabilities provide significant computing power that
was not available before. One simple way of performing
combinatorial testing in a cloud environment is:
1) Partition the testing tasks;
2) Allocate these testing tasks to different processors in the
cloud platform for test execution;
3) Collect results done by these processors.
However, this is not efficient as while the number of computing
and storage resources have increased significantly, the number
of combinations to be considered is still too high. For example,
a large SaaS system may have millions of components, and
testing all of these combinations can still consume all the
resources in a cloud platform. Two ways to improve this
approach and both are based on learning from the previous
test results:
â€¢

â€¢

Devise a mechanism to merge test results from different
processors so that testing results can be merged quickly,
and detect any inconsistency in testing;
Based on the existing testing results, eliminate any con-

figurations or interactions from future testing.
Due to the asynchronous and autonomous nature of cloud
computing, test results may arrive asynchronously and autonomously, and this necessitates a new testing framework.
This paper proposes a new algebraic system, Test Algebra
(TA), to facilitate concurrent combinatorial testing. The key
feature of TA is that the algebraic rules follow the combinatorial structure, and thus can track the test results obtained. The
TA can then be used to determine whether a tenant application
is faulty, and which interactions need to be tested. The TA
is an algebraic system in which elements and operations are
formally defined. Each element represents a unique component
in the SaaS system, and a set of components represents a
tenant application. Assuming each component has been tested
by developers, testing a tenant application is equivalent to
ensuring that there is no t-way interaction faults for t â‰¥ 2
among the elements in a set.
The TA uses the principle that if a t-way interaction is
faulty, every (t + 1)-way interaction that contains the t-way
interaction as a subset is necessarily faulty. The TA provides
guidance for the testing process based on test results so far.
Each new test result may indicate if additional tests are needed
to test a specific configuration. The TA is an algebraic system,
primarily intended to track the test results without knowing
how these results were obtained. Specifically, it does not
record the execution sequence of previously executed test
cases. Because of this, it is possible to allocate different
configurations to different processors for execution in parallel
or in any order, and the test results are merged following
the TA rules. The execution order and the merge order do
not affect the merged results if the merging follows the TA
operation rules.
This paper is structured as follows: Section II discusses the
related work; Section III proposes TA and shows its details;
and Section IV concludes this paper. Appendix provides proofs
of TA associativity properties.
II. R ELATED W ORK
SaaS testing is a new research topic [14], [4], [10]. Using
policies and metadata, test cases can be generated to test
SaaS applications. Testing can be embedded in the cloud
platform where tenant applications are run [14]. Gao proposed
a framework for testing cloud applications [4], and proposed
a scalability measure for testing cloud application scalability.
Another scalability measure was proposed by [10].
Testing all combinations of inputs and preconditions is not
feasible, even with a simple product [6], [8]. The number
of defects in a software product can be large, and defects
occurring infrequently are difficult to find [15]. Combinatorial
test design is used to identify a small number of tests needed
to get the coverage of important combinations. Combinatorial
test design methods enable one to build structure variation
into test cases for having greater test coverage with fewer test
cases.
Determining the presence of faults caused by a small
number of interacting elements has been extensively studied

in component-based software testing. When interactions are
to be examined, testing involves a combination-based strategy
[5]. When every interaction among t or fewer elements is to
be tested, methods have been developed that provide pairwise
or t-way coverage. Among the early methods, AET G [2]
popularized greedy one-test-at-a-time methods for constructing
test suites. In the literature, the test suite is usually called a
covering array, defined as follows. Suppose that there are k
configurable elements, numbered from 1 to k. Suppose that
for element c, there are vc valid options. A t-way interaction
is a selection of t of the k configurable elements, and a valid
option for each. A test selects a valid option for every element,
and it covers a t-way interaction if, when one restricts the
attention to the t selected elements, each has the same option
in the interaction as it does in the test.
A covering array of strength t is a collection of tests so
that every t-way interaction is covered by at least one of the
tests. Covering arrays reveal faults that arise from improper
interaction of t or fewer elements [9]. There are numerous
computational and mathematical approaches for construction
of covering arrays with a number of tests as small as possible
[3], [7].
If a t-way interaction causes a fault, then executing all
tests of a covering array will reveal the presence of at least
one faulty interaction. SaaS testing is interested in identifying
those interactions that are faulty including their numbers and
locations, as faulty configurations cannot be used in tenant
applications. Furthermore, the number and location of faults
keep on changing as new components can be added into
the SaaS database continuously. By then executing each test,
certain interactions are known not to be faulty, while others
appear only in tests that reveal faults, and hence may be faulty.
At this point, a classification tree analysis builds decision trees
for characterizing possible sets of faults. This classification
analysis is then used either to permit a system developer to
focus on a small collection of possible faults, or to design
additional tests to further restrict the set of possible faults.
In [16], empirical results demonstrate the effectiveness of
this strategy at limiting the possible faulty interactions to a
manageable number. Assuming that interactions of more than
t elements do not produce faults, a covering array can use few
tests to certify that no fault arises from a t-way interaction.
The Adaptive Reasoning algorithm (AR) is a strategy to
detect faults in SaaS [13]. The algorithm uses earlier test
results to generate new test cases to detect faults in tenant
applications. It uses three principles:
â€¢

â€¢

â€¢

Principle 1: When a tenant application (or configuration)
fails the testing, there is at least one fault (but there may
be more) in the tenant configuration.
Principle 2: When a tenant application passes the testing,
there is no fault in the tenant configuration resulting
from a t-way interactions among components in the
configuration.
Principle 3: Whenever a configuration contains one or
more faulty interactions, it is faulty.

III. T EST A LGEBRA
Let C be a finite set of components. A configuration is
a subset T âŠ† C. One is concerned with determining the
operational status of configurations. To do this, one can
execute certain tests; every test is a configuration, but there
may be restrictions on which configurations can be used as
tests. If a certain test can be executed, its execution results in
an outcome of passed (operational) or failed (faulty).
When a test execution yields result, all configurations that
are subsets of the test are operational. However, when a test
execution yields a faulty result, one only knows that at least
one subset causes the fault, but it is unclear which of these
subsets caused the failure. Among a set of configurations that
may be responsible for faults, the objective is to determine,
which cause faults and which do not. To do this, one must
identify the set of candidates to be faulty. Because faults
are expected to arise from an interaction among relatively
few components, one considers t-way interactions. The t-way
interactions are It = {U âŠ† C : |U | = t}. Hence the goal is to
select tests, so that from the execution results of these tests,
one can ascertain the status of all t-way interactions for some
fixed small value of t.
Because interactions and configurations are represented as
subsets, one can use set-theoretic operations such as union, and
their associated algebraic properties such as commutativity,
associativity, and self-absorption. The structure of subsets and
supersets also plays a key role.
To permit this classification, one can use a valuation function V , so that for every subset S of components, V (S)
indicates the current knowledge about the operational status
consistent with the components in S. The focus is on determining V (S) whenever S is an interaction in I1 âˆª Â· Â· Â· âˆª It .
These interactions can have one of five states.
â€¢ Infeasible (X): For certain interactions, it may happen
that no feasible test is permitted to contain this interaction. For example, it may be infeasible to select two GUI
components in one configuration such that one says the
wall is GREEN but the other says RED.
â€¢ Faulty (F): If the interaction has been found to be faulty.
â€¢ Operational (P): Among the rest, if an interaction has
appeared in a test whose execution gave an operational
result, the interaction cannot be faulty.
â€¢ Irrelevant (N): For some feasible interactions, it may
be the case that certain interactions are not expected to
arise, so while it is possible to run a test containing the
interaction, there is no requirement to do so.
â€¢ Unknown (U): If neither of these occurs then the status
of the interaction is required but not currently known.
Any given stage of testing, an interaction has one of five
possible status indicators. These five status indicators are
ordered by X  F  P  N  U under a relation , and
it has a natural interpretation to be explained in a moment.
A. Learning from Previous Test Results
The motivation for developing an algebra is to automate
the deduction of the status of an interaction from the status

of tests and other interactions, particularly in combining the
status of two interactions. Specifically, one is often interested
in determining V (T1 âˆª T2 ) from V (T1 ) and V (T2 ). To do this,
a binary operation âŠ— on {X, F, P, N, U} can be defined, with
operation table as follows:
âŠ—
X
F
P
N
U

X
X
X
X
X
X

F
X
F
F
F
F

P
X
F
U
N
U

N
X
F
N
N
N

U
X
F
U
N
U

Using this definition, one can verify that the binary operation âŠ— has the following properties of commutativity and
associativity.
V (T1 ) âŠ— V (T2 ) = V (T2 ) âŠ— V (T1 ),
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
Using this operation, one observes that V (T1 âˆª T2 ) 
V (T1 ) âŠ— V (T2 ). It follows that
1) Every superset of an infeasible interaction is infeasible.
2) Every superset of a failed interaction is failed or infeasible.
3) Every superset of an irrelevant interaction is irrelevant,
failed, passed, or infeasible.
A set S is an X-implicant if V (S) = X but whenever
S 0 âŠ‚ S, V (S 0 ) â‰º X. The X-implicants provide a compact
representation for all interactions that are infeasible. Indeed
for any interaction T that contains an X-implicant, V (T ) = X.
Furthermore, a set S is an F-implicant if V (S) = F but
whenever S 0 âŠ‚ S, V (S 0 ) â‰º F. For any interaction T that
contains an F-implicant, V (T )  F. In the same way, a set S is
an N-implicant if V (S) = N but whenever S 0 âŠ‚ S, V (S 0 ) = U.
For any interaction T that contains an N-implicant, V (T )  N.
An analogous statement holds for passed interactions, but here
the implication is for subsets. A set S is a P-implicant if
V (S) = P but whenever S 0 âŠƒ S, V (S 0 )  F. For any
interaction T that is contained in a P-implicant, V (T ) = P.
Implicants are defined with respect to the current knowledge
about the status of interactions. When a t-way interaction is
known to be infeasible, failed, or irrelevant, it must contain
an X-, F-, or N-implicant. By repeatedly proceeding from tway to (t + 1)-way interactions, then, one avoids the need
for any tests for (t + 1)-way interactions that contain any
infeasible, failed, or irrelevant t-way interaction. Hence testing
typically proceeds by determining the status of the 1-way
interactions, then proceeding to 2-way, 3-way, and so on.
The operation âŠ— is useful in determining the implied status
of (t + 1)-way interactions from the computed results for
t-way interactions, by examining unions of the t-way and
smaller interactions and determining implications of the rule
that V (T1 âˆª T2 )  V (T1 ) âŠ— V (T2 ). Moreover, when adding
further interactions to consider, all interactions previously
tested that passed are contained in a P-implicant, and every
(t + 1) interaction contained in one of these interactions can
be assigned status P.

For example, V (a, b) = P, V (a, e) = X, and other 2way interactions exist as atomic interaction in TA. Based
on the defined âŠ— operation, values of t-way interactions can
be deduced from the atomic interactions and their contained
interactions, such as V (a, b, e)  V (a, b) âŠ— V (a, e) = X, i.e.
V (a, b, e) = X.
The 3-way interaction (a, b, c) can have inferred results from
2-way interactions (a, b), (a, c), (b, c). If any contained 2-way
interaction has value F, the determining value of 3-way is F,
without further testing needed. But if all values of contained
2-way interactions are P, (a, b, c) the interaction needs to be
tested. In this case, U needs to be changed to non-U such as F
or P, assuming the 3-way is not X or N.
B. Changing Test Result Status
When testing a configuration with n components, one
should test individual components, 2-way interactions, 3-way
interactions, all the way to n-way interactions. Since any
combination of interactions is relevant in this case, the status
of any interaction can be either X, F, P, or U. The status of a
configuration is determined by the status of all interactions.
1) If an interaction has status X (F), the configuration has
status X (F).
2) If all interactions have status P, the configuration has
status P.
3) If some interactions still have status U, further tests are
needed.
It is important to determine when an interaction with status
U can be deduced to have status F or P instead. It can never
obtain status X or N once having had status U.
To change U to P: An interaction is assigned status P if and
only if it is a subset of a test that leads to proper operation.
To change U to F: Consider the candidate T , one can
conclude that V (T ) = F if there is a test containing T that
yields a failed result, but for every other candidate interaction
T 0 that appears in this test, V (T 0 ) = P. In other words, the
only possible explanation for the failure is the failure of T .
C. Matrix Representation
Suppose that each individual component passed the testing.
Then the operation table starts from 2-way interactions, then
enlarges to t-way interactions step by step. During the procedure, many test results can be deduced from the existing results
following TA rules. For example, all possible configurations
of (a, b, c, d, e, f ) can be expressed in the form of matrix, or
operation table. First, we show the operation table for 2-way
interactions. The entries in the operation table are symmetric
and those on the main diagonal are not necessary. So only half
of the entries are shown.
As shown in Figure 1, 3-way interactions can be composed
by using 2-way interactions and components. Thus, following
the TA implication rules, the 3-way interactions operation table
is composed based on the results of 2-way combinations. Here,
(a, b, c, d, e, f ) has more 3-way interactions than 2-way
interactions. As seen in Figure 1, a 3-way interaction can be
obtained through different combinations of 2-way interactions

and components. For example, {a, b, c} = {a} âˆª {b, c} =
{b}âˆª{a, c} = {c}âˆª{a, b} = {a, b}âˆª{a, c} = {a, b}âˆª{b, c} =
{a, c} âˆª {b, c}. V (a) âŠ— V (b, c) = V (c) âŠ— V (a, b) = V (a, b) âŠ—
V (b, c) = PâŠ—P = U. But V (b)âŠ—V (a, c) = V (a, b)âŠ—V (a, c) =
V (b, c) âŠ— V (a, c) = P âŠ— F = F. As TA defines the order of
the five status indicators, the result should be the value with
highest order. So V (a, b, c) = F.
âŠ— a
a
b
c
d
e
f

b
P

c d e
F N X
P X N
F P
F

f
U
F
P
X
U

D. Merging Concurrent Testing Results
One way to achieve efficient testing is to allocate (overlapping or non-overlapping) tenant applications into different
clusters, and each cluster is sent to a different set of servers
for execution. Once each cluster completes its execution, the
test results can be merged. The testing results of a specific
interaction T in different servers should satisfy the following
constraints.
â€¢ If V (T ) = U in one cluster, then in other clusters, the
same V (T ) can be either F, P, N, or U.
â€¢ If V (T ) = N in one cluster, then in other clusters, the
same V (T ) can be either F, P, N, or U.
â€¢ If V (T ) = P in one cluster, then the same V (T ) can be
either P, N, or U in all clusters;
â€¢ If V (T ) = F in one cluster, then in other clusters, the
same V (T ) can be F, N, or U.
â€¢ If V (T ) = X in one cluster, then in other clusters, the
same V (T ) can be X only.
If these constraints are satisfied, then the testing results can
be merged. Otherwise, there must be an error in the testing
results. To represent this situation, a new status indicator, error
(E), is introduced and E  X. We define a binary operation âŠ•
on {E, X, F, P, N, U}, with operation table as follows:
âŠ•
E
X
F
P
N
U

E
E
E
E
E
E
E

X
E
X
E
E
E
E

F
E
E
F
E
F
F

P
E
E
E
P
P
P

N
E
E
F
P
N
U

U
E
E
F
P
U
U

âŠ• also has the properties of commutativity and associativity.
See Appendix for proof of associativity.
Using this operation, merging two testing results from two
different servers can be defined as Vmerged (T ) = Vcluster1 (T ) âŠ•
Vcluster2 (T ). The merge can be performed in any order due to
the commutativity and associativity of âŠ•, and if the constraints
of merge are satisfied and V (T ) = X, F, or P, the results cannot
be changed by any further testing or merging of test results
unless there are some errors in testing. If V (T ) = E, the testing

âˆª
a
b
c
..
.

a
(a)

b
(a, b)
(b)

c
(a, c)
(b, c)
(c)

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

f
(a, f )
(b, f )
(c, f )
..
.

(a, b)
(a, b)
(a, b)
(a, b, c)
..
.

(a, c)
(a, c)
(a, b, c)
(a, c)
..
.

(f )

(a, b, f )
(a, b)

(a, c, f ) Â· Â· Â·
(a, b, c) Â· Â· Â·
(a, c)
Â·Â·Â·
..
.

f
(a, b)
(a, c)
..
.
(b, c)
..
.

(b, c)
(a, b, c)
(b, c)
(b, c)
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

(b, c, f ) Â· Â· Â·
(a, b, c) Â· Â· Â·
(a, b, c) Â· Â· Â·
..
..
.
.
(b, c)
Â·Â·Â·
..
.

(e, f )
(a, e, f )
(b, e, f )
(c, e, f )
..
.
(e, f )
(a, b, e, f )
(a, c, e, f )
..
.
(b, c, e, f )
..
.
(e, f )

(e, f )
âŠ—
a
b
c
..
.
f
(a, b)
(a, c)
..
.

a

b
P

c
F
P

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

f
U
F
P
..
.

(a, b)
U
U
U
..
.

(a, c)
F
F
F
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

(b, c)
U
U
U
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

(e, f )
U
U
U
..
.

U

F
F

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

U
U
F
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

U
U
F
..
.

Â·Â·Â·
..
.

U
..
.

(b, c)
..
.
(e, f )
Fig. 1. 3-way interaction operation table

environment must be corrected and tests executed again after
fixing the error(s) in testing. For example, Vcluster1 (a, c, e) = X
and Vcluster2 (a, c, e) = F, so Vmerged (a, c, e) = X âŠ• F = E.
It means that there is something wrong with the tests of
interaction (a, c, e), and the problem must be fixed before
doing further testing.
Following the âŠ• associative rule, one can derive the following.
V1 (T ) âŠ• V2 (T ) âŠ• V3 (T )
= (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T )
= V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T ))
= V1 (T ) âŠ• V2 (T ) âŠ• V3 (T ) âŠ• V3 (T )
= (V1 (T ) âŠ• V2 (T )) âŠ• (V2 (T ) âŠ• V3 (T ))
= ((V1 (T ) âŠ• V2 (T )) âŠ• V2 (T )) âŠ• V3 (T )
= (V3 (T ) âŠ• V2 (T )) âŠ• (V3 (T ) âŠ• V1 (T ))
Thus the âŠ• rule allows one to partition the configurations
into different sets for different servers to run testing, and
these sets do not need to be non-overlapping. In conventional
cloud computing operations such as MapReduce, data should
not overlap, otherwise incorrect data may be produced. For
example, counting the items in a set can be performed by
MapReduce, but data allocated to different servers cannot
overlap, otherwise items may be counted more than once. In
TA, this is not a concern due to the nature of the TA operations

âŠ•. Once the results are available from each server, the testing
results can be merged either incrementally, in parallel, or in
any order. Furthermore, test results can be merged repeatedly
without changing the final results.
Using this approach, one can test say 10 batches of 100 tenant applications each, for a total of 1, 000 tenant applications.
Once this is done, another batch of 1, 000 tenant applications
can be tested with each 100 tenant application allocated to a
server for execution. In this way, after running 100 batches,
100, 000 tenant applications can be evaluated completely.
The following example illustrates the testing process of
fifteen configurations. According to the definition of configuration testing, all feasible interactions should be tested. For
simplicity, assume that only interaction (c, d, f ) is faulty, and
only interaction (c, d, e) is infeasible, and all other interactions
pass the testing. If one assigns configurations 1, 3, 5, 7, 9, 11,
13, 15 into Server1 , configurations 2, 4, 6, 8, 10, 12, 14 into
Server2 , and 4-11 configurations into Server3 .
If Server1 and Server3 do their own testing first, Server2
can reuse test results of interactions from them to eliminate
interactions that need to be tested. For example, when testing
2-way interactions of configuration (b, c, d, f ) in Server2 ,
it can reuse the test results of (b, c), (b, d) of configuration
(b, c, d, e) from Server3 , (b, f ) of configuration (a, b, c, f )

from Server1 . They are all passed, and it can reuse the test
results of (b, c, d) of configuration (a, b, c, d) from Server1 ,
(b, c, f ) of configuration (a, b, c, f ) from Server1 , (b, d, f )
of configuration (a, b, d, f ) from Server1 , and (c, d, f ) of
configuration (a, c, d, f ) from Server3 . Because (c, d, f ) is
faulty, it can deduce that 4-way interaction (b, c, d, f ) is also
faulty. For the sets of configuration that are overlapping, their
returned test results from different servers are the same. The
merged results of these results also stay the same.
Not only interactions, sets of configurations, CS1 , CS2 ,
. . . , CSK can be allocated to different processors (or clusters)
for testing, and the test results can then be merged. The sets
can be non-overlapping or overlapping, and the merge process
can be arbitrary. For example, say the result of CSi is RCSi ,
the merge process can be (Â· Â· Â· ((((RCS1 + RCS2 ) + RCS3 ) +
RCS4 ) + Â· Â· Â· + RCSK ), or (Â· Â· Â· ((((RCSK + RCSkâˆ’1 ) +
RCSkâˆ’2 ) + Â· Â· Â· + RCS1 ), or any other sequence that includes
all RCSi , for i = 1 to K. This is true because RCS is simply
a set of V (Tj ) for any intercation Tj in the configuration CSi .
(a,b,c,d)
(a,b,c,e)
(a,b,c,f)
(a,b,d,e)
(a,b,d,f)
(a,b,e,f)
(a,c,d,e)
(a,c,d,f)
(a,c,e,f)
(a,d,e,f)
(b,c,d,e)
(b,c,d,f)
(b,c,e,f)
(b,d,e,f)
(c,d,e,f)

Server1
P

Server2

Server3

P
P
P
P
P
X
F
P
P
X
F
P
P
X

P
P
P
X
F
P
P
X

IV. C ONCLUSION
This paper proposes TA to address SaaS combinatorial
testing. The TA provides a foundation for concurrent combinatorial testing. The TA has two operations and test results
can have five states with a priority. By using the TA operations, many combinatorial tests can be eliminated as the
TA identifies those interactions that need not be tested. Also
the TA defines operation rules to merge test results done by
different processors, so that combinatorial tests can be done in
a concurrent manner. The TA rules ensure that either merged
results are consistent or a testing error has been detected so
that retest is needed. In this way, large-scale combinatorial
testing can be carried out in a cloud platform with a large
number of processors to perform test execution in parallel to
identify faulty interactions.
ACKNOWLEDGMENT

This project is sponsored by U.S. National Science Foundation project DUE 0942453 and National Science Foundation
M erged Results China (No.61073003), National Basic Research Program of
China (No.2011CB302505), and the State Key Laboratory of
P
Software Development Environment (No. SKLSDE-2012ZXP
18), and Fujitsu Laboratory.
P
P
R EFERENCES
P
[1] X. Bai, M. Li, B. Chen, W.-T. Tsai, and J. Gao. Cloud Testing Tools. In
P
Proceedings of IEEE 6th International Symposium on Service Oriented
X
System Engineering (SOSE), pages 1â€“12, Irvine, CA, USA, 2011.
F
[2] D. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The
AETG System: An Approach to Testing Based on Combinatorial Design.
P
Journal of IEEE Transactions on Software Engineering, 23:437â€“444,
P
1997.
X
[3] C. J. Colbourn. Covering arrays and hash families. In D. Crnkovic and
V. D. Tonchev, editors, Information Security, Coding Theory and Related
F
Combinatorics, volume 29 of NATO Science for Peace and Security
P
Series - D: Information and Communication Security. IOS Press, 2011.
P
[4] J. Gao, X. Bai, and W.-T. Tsai. SaaS Performance and Scalability
Evaluation in Cloud. In Proceedings of The 6th IEEE International
X

E. Modified Testing Process
Perform 2-way interaction testing first. Before going on to 3way interaction testing, use the results of 2-way testing to eliminate cases. The testing process stops when finishing testing all
t-way interactions. The analysis of t-way interactions is based
on the P T Rs of all (t âˆ’ i)-way interactions for 1 â‰¤ i < t.
The superset of infeasible, irrelevant, and faulty test cases do
not need to be tested. The test results of the superset can be
obtained by TA operations and must be infeasible, irrelevant,
or faulty. But the superset of test cases with unknown indicator
must be tested. In this way, a large repeating testing workload
can be reduced.
For n components, all t-way interactions for t â‰¥ 2
are composed by 2-way, 3-way, ..., t-way interactions. In
n components combinatorial
 testing, the number of 2-way
interactions is equal to n2 . In general, the number of t-way
n
interactions
are treated when

is equal to t . More interactions
n
n
n
.
The
total number
>
,
which
happens
when
t
â‰¤
t
tâˆ’1
 2
Pt
of interactions examined is i=2 ni .

Symposium on Service Oriented System Engineering, SOSE â€™11, 2011.
[5] M. Grindal, J. Offutt, and S. F. Andler. Combination Testing Strategies:
A Survey. Software Testing, Verification, and Reliability, 15:167â€“199,
2005.
[6] C. Kaner, J. Falk, and H. Q. Nguyen. Testing Computer Software, 2nd
Edition. Wiley, New York, NY, USA, 1999.
[7] V. V. Kuliamin and A. A. Petukhov. A Survey of Methods for
Constructing Covering Arrays. Journal of Program Computer Software,
37(3):121â€“146, may 2011.
[8] T. Muller and D. Friedenberg. Certified Tester Foundation Level
Syllabus. Journal of International Software Testing Qualifications
Board.
[9] A. A. Porter, C. Yilmaz, A. M. Memon, D. C. Schmidt, and B. Natarajan.
Skoll: A Process and Infrastructure for Distributed Continuous Quality
Assurance. IEEE Transactions on Software Engineering, 33(8):510â€“525,
2007.
[10] W.-T. Tsai, Y. Huang, X. Bai, and J. Gao. Scalable Architecture for
SaaS. In Proceedings of 15th IEEE International Symposium on Object
Component Service-oriented Real-time Distributed Computing, ISORC
â€™12, Apr. 2012.
[11] W.-T. Tsai, Y. Huang, and Q. Shao. Testing the Scalability of SaaS Applications. In Proceedings of IEEE International Conference on ServiceOriented Computing and Applications (SOCA), pages 1â€“4, Irvine, CA,
USA, 2011.
[12] W.-T. Tsai, Y. Huang, Q. Shao, and X. Bai. Data Partitioning and
Redundancy Management for Robust Multi-Tenancy SaaS. International
Journal of Software and Informatics (IJSI), 4(3):437â€“471, 2010.

[13] W.-T. Tsai, Q. Li, C. J. Colbourn, and X. Bai. Adaptive Fault Detection
for Testing Tenant Applications in Multi-Tenancy SaaS Systems. In
Proceedings of IEEE International Conference on Cloud Engineering
(IC2E), March 2013.
[14] W.-T. Tsai, Q. Shao, and W. Li. OIC: Ontology-based Intelligent
Customization Framework for SaaS. In Proceedings of International
Conference on Service Oriented Computing and Applications(SOCAâ€™10),
Perth, Australia, Dec. 2010.
[15] Wikipedia. Software Testing, 2013.
[16] C. Yilmaz, M. B. Cohen, and A. Porter. Covering Arrays for Efficient
Fault Characterization in Complex Configuration Spaces. In Proceedings
of the 2004 ACM SIGSOFT International Symposium on Software
Testing and Analysis, ISSTA â€™04, pages 45â€“54, New York, NY, USA,
2004. ACM.

A PPENDIX
The associativity of binary operation âŠ—.
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
Proof: We will prove this property in the following cases.
(1) At least one of V (T1 ), V (T2 ), and V (T3 ) is X. Without
loss of generality, suppose that V (T1 ) = X, then according
to the operation table of âŠ—, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) =
X âŠ— (V (T2 ) âŠ— V (T3 )) = X, (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) =
(X âŠ— V (T2 )) âŠ— V (T3 ) = X âŠ— V (T3 ) = X. Thus, in this case,
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
(2) V (T1 ), V (T2 ), and V (T3 ) are not X and at least one
of V (T1 ), V (T2 ), and V (T3 ) is F. Without loss of generality,
suppose that V (T1 ) = F, then according to the operation table
of âŠ—, the value of V (T2 ) âŠ— V (T3 ) can only be F, N or U.
So V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = F âŠ— (V (T2 ) âŠ— V (T3 )) = F,
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) = (F âŠ— V (T2 )) âŠ— V (T3 ) = F âŠ—
V (T3 ) = F. Thus, in this case, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) =
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
(3) V (T1 ), V (T2 ), and V (T3 ) are not X or F and at least one
of V (T1 ), V (T2 ), and V (T3 ) is N. Without loss of generality,
suppose that V (T1 ) = N, then according to the operation table
of âŠ—, the value of V (T2 ) âŠ— V (T3 ) can only be N or U. So
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = N âŠ— (V (T2 ) âŠ— V (T3 )) = N,
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) = (N âŠ— V (T2 )) âŠ— V (T3 ) = N âŠ—
V (T3 ) = N. Thus, in this case, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) =
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
(4) V (T1 ), V (T2 ), and V (T3 ) are not X, F or N. In this case,
V (T1 ), V (T2 ), and V (T3 ) can only be P or U. According to the
operation table of âŠ—, the value of V (T1 )âŠ—V (T2 ) and V (T2 )âŠ—
V (T3 ) are U. So V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = V (T1 ) âŠ— U = U,
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) = U âŠ— V (T3 ) = U. Thus, in this
case, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
The associativity of binary operation âŠ•.
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ).
Proof: We will prove this property in the following cases.
(1) One of V1 (T ), V2 (T ), and V3 (T ) is E. Without loss
of generality, suppose that V1 (T ) = E, then according to the
operation table of âŠ•, V1 (T )âŠ•(V2 (T )âŠ•V3 (T )) = EâŠ—(V2 (T )âŠ•
V3 (T )) = E, (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = (E âŠ• V2 (T )) âŠ•
V3 (T ) = EâŠ•V3 (T ) = E. Thus, in this case, V1 (T )âŠ•(V2 (T )âŠ•
V3 (T )) = (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ).

(2) V1 (T ), V2 (T ), and V3 (T ) are not E, and there is a pair
of V1 (T ), V2 (T ), and V3 (T ) does not satisfy the constrains.
Without loss of generality, suppose that V1 (T ) and V2 (T ) does
not satisfy the constrains, then according to the operation table
of âŠ•, V1 (T ) âŠ• V2 (T ) = E. So (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) =
E âŠ• V3 (T ) = E. Since V1 (T ) and V2 (T ) does not satisfy the
constrains, there can be two cases: (a) one of them is X and
the other is not, or (b) one of them is P and the other is F.
(a) If V1 (T ) = X, then V2 (T ) âŠ• V3 (T ) cannot be X because
V2 (T ) cannot be X. Thus, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E. If
V2 (T ) = X, then V2 (T ) âŠ• V3 (T ) 6= X can only be E or X.
Since V1 (T ) cannot be X, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E.
(b) If V1 (T ) = P and V2 (T ) = F, then V2 (T ) âŠ• V3 (T )
can only be E or F. Thus, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E. If
V1 (T ) = F and V2 (T ) = P, then V2 (T ) âŠ• V3 (T ) can only be
E or P. Thus, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E.
Thus, in this case, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = (V1 (T ) âŠ•
V2 (T )) âŠ• V3 (T ).
(3) V1 (T ), V2 (T ), and V3 (T ) are not E, and V1 (T ), V2 (T ),
and V3 (T ) satisfy the constrains.
(a) One of V1 (T ), V2 (T ), and V3 (T ) is X. Without loss of
generality, suppose that V1 (T ) = X, then V2 (T ) = V3 (T ) = X.
So V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = X âŠ• (X âŠ• X) = X âŠ• X = X and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = (X âŠ• X) âŠ• X = X âŠ• X = X.
(b) V1 (T ), V2 (T ), and V3 (T ) are not X, and one of V1 (T ),
V2 (T ), and V3 (T ) is F. Without loss of generality, suppose
that V1 (T ) = F, then V2 (T ) and V3 (T ) can only be F, N,
or U. According to operation table of âŠ•, V2 (T ) âŠ• V3 (T ) can
only be F, N, or U, and V1 (T ) âŠ• V2 (T ) can only be F. So
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = F âŠ• (V2 (T ) âŠ• V3 (T )) = F and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = F âŠ• V3 (T ) = F.
(c) V1 (T ), V2 (T ), and V3 (T ) are not X or F, and one of
V1 (T ), V2 (T ), and V3 (T ) is P. Without loss of generality,
suppose that V1 (T ) = P, then V2 (T ) and V3 (T ) can only be
P, N, or U. According to operation table of âŠ•, V2 (T ) âŠ• V3 (T )
can only be P, N, or U, and V1 (T ) âŠ• V2 (T ) can only be F. So
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = P âŠ• (V2 (T ) âŠ• V3 (T )) = P and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = P âŠ• V3 (T ) = P.
(d) V1 (T ), V2 (T ), and V3 (T ) are not X, F or P, and one
of V1 (T ), V2 (T ), and V3 (T ) is U. Without loss of generality,
suppose that V1 (T ) = U, then V2 (T ) and V3 (T ) can only be
N, or U. According to operation table of âŠ•, V2 (T ) âŠ• V3 (T )
can only be N, or U, and V1 (T ) âŠ• V2 (T ) can only be U. So
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = U âŠ• (V2 (T ) âŠ• V3 (T )) = U and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = U âŠ• V3 (T ) = U.
(e) V1 (T ), V2 (T ), and V3 (T ) are N. V1 (T ) âŠ• (V2 (T ) âŠ•
V3 (T )) = N âŠ• (N âŠ• N) = N âŠ• N = N and (V1 (T ) âŠ• V2 (T )) âŠ•
V3 (T ) = (N âŠ• N) âŠ• N = N âŠ• N = N.
Thus, in this case, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = (V1 (T ) âŠ•
V2 (T )) âŠ• V3 (T ).

Augmentation is an operation to increase the number of symbols in a covering array, without unnecessarily increasing the number of rows. For covering arrays of strength two, one type of augmentation forms a covering array on  vv  symbols from one on  vâˆ’1vâˆ’1  symbols together with  vâˆ’1vâˆ’1  covering arrays each on two symbols. A careful analysis of the structure of the optimal binary covering arrays underlies an augmentation operation that reduces the number of rows required. Consequently a number of covering array numbers are improved.Des. Codes Cryptogr. (2015) 77:479â€“491
DOI 10.1007/s10623-015-0084-4

Optimal low-power coding for error correction
and crosstalk avoidance in on-chip data buses
Yeow Meng Chee1 Â· Charles J. Colbourn2 Â·
Alan Chi Hung Ling3 Â· Hui Zhang1 Â· Xiande Zhang1

Received: 30 October 2014 / Revised: 12 April 2015 / Accepted: 16 April 2015 /
Published online: 8 May 2015
Â© Springer Science+Business Media New York 2015

Abstract Coupled switched capacitance causes crosstalk in ultra deep submicron/nanometer
VLSI fabrication, which leads to power dissipation, delay faults, and logical malfunctions. We
present the first memoryless transition bus-encoding technique for power minimization, errorcorrection, and elimination of crosstalk simultaneously. To accomplish this, we generalize
balanced sampling plans avoiding adjacent units, which are widely used in the statistical
design of experiments. Optimal or asymptotically optimal constant weight codes eliminating
each kind of crosstalk are constructed.
Keywords Constant weight codes Â· Packing sampling plan avoiding adjacent units Â·
Crosstalk avoidance Â· Low power code Â· Packing by triples Â· Balanced sampling plan
Mathematics Subject Classification

94B25 Â· 05B40 Â· 05B07 Â· 62K10

This is one of several papers published in Designs, Codes and Cryptography comprising the â€œSpecial Issue
on Cryptography, Codes, Designs and Finite Fields: In Memory of Scott A. Vanstoneâ€.

B

Charles J. Colbourn
charles.colbourn@asu.edu
Yeow Meng Chee
ymchee@ntu.edu.sg
Alan Chi Hung Ling
aling@emba.uvm.edu
Hui Zhang
huizhang@ntu.edu.sg
Xiande Zhang
xiandezhang@ntu.edu.sg

1

Division of Mathematical Sciences, School of Physical and Mathematical Sciences,
Nanyang Technological University, Singapore 637371, Singapore

2

School of Computing, Informatics, and Decision Systems Engineering,
Arizona State University, Tempe, AZ 85287, USA

3

Department of Computer Science, University of Vermont, Burlington, VT 05405, USA

123

480

Y. M. Chee et al.

1 Introduction
The ever-decreasing feature size of VLSI fabrication process has led to many challenges in
VLSI circuit design. One of the most important issues concerns the characteristics of onchip wires [11]. The wiresâ€™ cross-sectional areas and spacings have fallen dramatically with
the move into the ultra deep submicron/nanometer (UDSM) regime. This has increased the
resistance and capacitance of wires. To help reduce resistance, wires today are taller than
they are wide, and they are poised to grow even taller as technology continues to scale.
The resulting growth of side-to-side capacitance between long parallel wires causes coupled
switch capacitance to dominate the wire-to-substrate capacitance in UDSM circuits by several
orders of magnitude [21]. Coupled switched capacitance in turn leads to crosstalks, which
result in power dissipation, delay faults, and logical malfunctions. The problem of eliminating
or minimising crosstalks is considered the biggest signal integrity challenge for long on-chip
buses implemented in UDSM CMOS technology [12].
The worst crosstalk couplings have been classified into four types [6,12], as described in
Table 1. The coupled switched capacitance resulting from type-1, -2, -3, and -4 crosstalks is
in the ratio of 1:2:3:4. Hence, it is particularly important to avoid crosstalks of higher types.
Type-1 crosstalks cannot be avoided in any useful communication channel. However, type-1
crosstalks give rise to power dissipation and must be limited, because low power is a critical
design objective in recent years.
Another factor that has emerged as a new challenge for VLSI circuit designers is UDSM
noise, caused by high-leakage transistors, power-grid fluctuations, ground bounce, IR drops,
clock jitter, and electromagnetic radiation. The effects of such noise are difficult to predict
or prevent. For example, noise in radiation-hardened circuits for satellite communication
systems is random and does not correlate with particular switching patterns on the buses.
A further source of faults is manufacturing defects. In nanotechnology, circuits are manufactured with a significant proportion of faults, and occasional errors may be unavoidable.
Hence, preventive techniques are insufficient, and active error correction is required.
Various researchers have proposed coding techniques to encode data on a bus for crosstalk
avoidance [6,17,28], for low power dissipation [3,15,19,22,26], and for error correction
[1,8]. Coding schemes that simultaneously satisfy two of these three criteria have also been
investigated:
â€¢ crosstalk avoidance and low power dissipation [12,27];
â€¢ crosstalk avoidance and error correction [14]; and
â€¢ low power dissipation and error correction [2,16,18].

Table 1 Types of worst crosstalk couplings
Type-1

Type-2

Type-3

Type-4

0 â†â†’ 1

001 â†â†’ 110
011 â†â†’ 100

001 â†â†’ 010
010 â†â†’ 100
011 â†â†’ 101
101 â†â†’ 110

010 â†â†’ 101

Single wire undergoes
transition. Adjacent
wires maintain previous states

Center wire in opposite
transition to an adjacent wire.
The other wire in same transition as center wire

Center wire in opposite
transition to an adjacent
wire. The other wire maintains previous state

123

All three adjacent
wires undergo opposite transitions

Optimal low-power coding for crosstalk avoidance

481

Despite many efforts, the only families of optimal codes known are those for low power
dissipation [3]. Many of the results on the comparative performance of existing codes are
based on simulations rather than rigorous mathematical analysis.
In this paper, we begin the study of codes for UDSM buses that simultaneously provide
for low power dissipation, crosstalk avoidance, and error correction. In particular, we exhibit
the first infinite families of such codes that are provably optimal.
The paper is organized as follows. Section 2 establishes necessary terminology and gives a
mathematical formulation of the problem of designing low-power codes that avoid crosstalks
and correct errors. In Sect. 3, we present the relation of codes of each type with packing
sampling plans avoiding adjacent units. In Sect. 4, we focus on optimal solutions for k = 3
for all positive integer n. In Sect. 5, the sizes of optimal codes of all types with small lengths
are determined by computer search, and brief conclusion is given.

2 Background
2.1 Coding framework
A coding framework for data buses was introduced by Ramprasad et al. [15]. A bus interconnecting two embedded systems on a systems-on-chip (SoC) platform can be modelled
generically as in Fig. 1. The source encoder (decoder) compresses (decompresses) the input
data so that the number of bits required in the representation of the source is minimised. While
the source encoder removes redundancy, the channel encoder adds redundancy to combat
errors that may arise due to noise in the bus.
Ramprasad et al. [15] considered various combinations of source-channel encoder-decoder
pairs and presented simulation results for their power dissipation. Their approach is what
is known as joint source-channel coding in the information theory literature. Shannonâ€™s
information separation theorem [20] states that reliable transmission can be accomplished
by separate source and channel coding, where the source encoder and decoder need not take
into account the channel statistics and the channel encoder and decoder need not take into
account the source statistics. This applies, however, only for point-to-point transmissions
and for infinite sequence length. The first condition (point-to-point transmission) holds for
a UDSM bus but the second requirement for infinite sequence length is clearly undesirable
for bus coding, because it could give rise to circuits of unbounded delay. Moreover, joint
source-channel coding is useful only when we know the statistics of the source and channel.
In the absence of such statistics, one can only fall back on optimising the source and channel
separately. Indeed, Ramprasad et al. [15] considered coding schemes and simulations on
certain source data with better understood statistics (for example, pop music, classical music,
video, and speech).

noisy channel
source
encoder

channel
encoder

transmitter

channel
decoder

source
decoder

receiver

Fig. 1 Framework for systems-on-chip

123

482

Y. M. Chee et al.

In many systems, the behaviour of source data is hard to predict and so the joint sourcechannel coding approach loses its power. Many researchers have therefore fallen back on
addressing the source coding and channel coding problems separately. This is also the
approach taken in this paper. We focus on designing optimal channel coding schemes for the
scenario where the source statistics are unknown.

2.2 Codes
The Hamming n-space is the set H(n) = {0, 1}n , endowed with the (Hamming) distance
dH (Â·, Â·) defined as follows: for u, v âˆˆ H(n), dH (u, v) is the number of positions where u and
v differ. The (Hamming) weight of a vector u âˆˆ H(n) is the number of positions in u with
nonzero value, and is denoted wH (u). The ith component of u is denoted ui . The support of
a vector u âˆˆ H(n), denoted supp(u), is the set {i : ui = 1}.
A (binary) code of length n is a subset C âŠ† H(n). C is said to be of constant weight w if
wH (u) = w for all u âˆˆ C . The elements of a code are called codewords and the size of a code
is the number of codewords it contains. The support of C is supp(C ) = {supp(u) : u âˆˆ C }. The
minimum distance of C is dmin (C ) = min{dH (u, v) : u, v âˆˆ C and u  = v}. A constant-weight
code of length n, minimum distance d, and weight w is denoted as an (n, d, w) code.
A code that is capable of correcting any occurrence of e or fewer symbol errors is said to
be e-error-correcting. A code C is e-error-correcting if and only if dmin (C ) â‰¥ 2e + 1 [9].

2.3 Set systems and graphs
For integers i < j, the set {i, i + 1, . . . , j} is abbreviated as [i, j]. We further abbreviate
[1, j] to [ j]. For a finite set X and k â‰¤ |X |, we define
 
X
X
2 = {B : B âŠ† X }, and
= {B âŠ† X : |B| = k}.
k
A set system is a pair S = (X, B), where X is a finite set of points and B âŠ† 2 X . The
elements of B are called blocks. The order of S is the number of points, |X |, and the size
 of
S is the number of blocks, |B|. A set system (X, B) is said to be k-uniform if B âŠ† Xk . A
graph is a 2-uniform set system and it is common to refer to the points and blocks of a graph
as vertices and edges, respectively. A path of length n is an alternating sequence of vertices
and edges W = v0 , e1 , v1 , e2 , . . . , en , vn , such that all the vertices vi , i âˆˆ [0, n] and edges
ei , i âˆˆ [n] are all distinct from one another, except possibly the first and last vertices. A cycle
is a path in which the first and last vertices are the same.
Let (X, B) be a set system of order n. The incidence vector of a block B âˆˆ B is the vector
Î¹(B) âˆˆ H(n) such that

1, if i âˆˆ B
Î¹(B)i =
0, otherwise.
There is a natural correspondence between the Hamming n-space and the complete set system
(X, 2 X ): the positions of vectors in H(n) correspond to points in X , a vector u âˆˆ H(n)
corresponds to the block supp(u), and dH (u, v) = |(supp(u)\supp(v))âˆª(supp(v)\supp(u))|.
From this, it follows that there is a bijection between the set of all codes of length n and the
set of all set systems of order n.
An (n, k,
is a k-uniform set system (X, B) with |X | = n such that every

 Î»)-packing
element of X2 is contained in at most Î» blocks of B. Let D(n, k, Î») denote the largest size
among all (n, k, Î»)-packings. The leave graph of (X, B) is the multigraph (X, E), where E

123

Optimal low-power coding for crosstalk avoidance

483

 
contains each e âˆˆ X2 exactly Î» âˆ’ d(e) times, where d(e) is the number of blocks containing
e. When Î» = 1, we omit Î» in the notation; in this case, the leave is a simple graph. When the
leave contains no edges, the packing is a balanced incomplete block design.
The balanced sampling plan avoiding adjacent units (BSA) was introduced to design
sampling plans that exclude contiguous units in statistical experiments [10,25]; for more
recent work, see [7,29]. In statistical applications, in a circular or linear order of the elements,
elements that are â€œcloseâ€ do not appear together, while those more distant all appear the same
number of times together. A (circular) BSAÎ» (n, k; Î±) is an (n, k, Î»)-packing (X, B) with X =
Zn whose leave graph consists of all the edges {i, j} with i âˆ’ j â‰¡ Â±1, . . . , Â±Î± (mod n), and
every other pair appears in Î» blocks. A (linear) LBSAÎ» (n, k; Î±) is an (n, k, Î»)-packing (X, B)
with X = [0, n âˆ’ 1] whose leave graph consists of all the edges {i, j} with 0 â‰¤ i < j < n
for which j âˆ’ i â‰¤ Î±, and every other pair appears in Î» blocks. We employ these only when
Î» = 1, and so omit Î» in the notation.
We generalize circular and linear BSAs (with Î» = 1) to a packing sampling plan avoiding
adjacent units (PSA). A (circular) CPSA(n, k; Î±) is an (n, k)-packing (X, B) with X = Zn
whose leave graph contains all the edges {i, j} with i âˆ’ j â‰¡ Â±1, . . . , Â±Î± (mod n), and every
other pair appears in at most one block. A (linear) LPSA(n, k; Î±) is an (n, k)-packing (X, B)
with X = [0, n âˆ’ 1] whose leave graph contains all the edges {i, j} with 0 â‰¤ i < j < n
for which j âˆ’ i â‰¤ Î±, and every other pair appears in at most one block. (In this case, every
CPSA(n, k; Î±) is an LPSA(n, k; Î±) but the converse need not hold.) Let B(n, k; Î±) denote
the largest size of any LPSA(n, k; Î±); the LPSA is optimal if its size is B(n, k; Î±). Similarly,
let B â—¦ (n, k; Î±) denote the largest size of any CPSA(n, k; Î±); the CPSA is optimal if its size
is B â—¦ (n, k; Î±).

	


	


Let U (n, k; Î±) =

2

Î±âˆ’1
i=0

nâˆ’Î±âˆ’iâˆ’1
kâˆ’1

+(nâˆ’2Î±)

nâˆ’2Î±âˆ’1
kâˆ’1

k

.

Lemma 2.1 B(n, k; Î±) â‰¤ U (n, k; Î±).
Proof For an LPSA(n, k; Î±) constructed
on
	

 [0, n âˆ’ 1], for each i âˆˆ [0, Î± âˆ’ 1], the points i
nâˆ’Î±âˆ’iâˆ’1
and n âˆ’ 1 âˆ’ i appear in at most
blocks, and all the other points appear in at most
kâˆ’1
	


	


Î±âˆ’1 	 nâˆ’Î±âˆ’iâˆ’1 

nâˆ’2Î±âˆ’1
blocks. Then k B(n, k; Î±) â‰¤ 2 i=0
+ (n âˆ’ 2Î±) nâˆ’2Î±âˆ’1
.


kâˆ’1
kâˆ’1
kâˆ’1
When Î± = 1, we omit it in the notation. If there is an (n, k)-packing with leave graph
containing a path of length n âˆ’ 1, we can always relabel the points to get an LPSA(n, k).
 	 

	


Corollary 2.2 B(n, k) â‰¤

2

nâˆ’2
kâˆ’1

+(nâˆ’2)
k

nâˆ’3
kâˆ’1

.

Theorem 4.1 shows that when k = 3, this inequality is tight.

2.4 Problem formulation
Limited weight codes have been widely exploited for the case of on-chip communication
to achieve crosstalk coupling elimination and energy efficiency [12,23]. We consider an nbit parallel bus in a single metal layer, for which we want memoryless codes to weaken
crosstalk, reduce power consumption, and correct errors. We use constant weight codes with
small weight to achieve low power similarly by reducing the node switching activity, that is,
reducing the total number of transitions occurring between the newly arrived data and the
present data on the bus.

123

484

Y. M. Chee et al.

Assume an n-bit bus, consisting of signals b0 , b1 , b2 , . . . , bnâˆ’1 . Consider a group of three
wires in an on-chip bus, which are driven by signals biâˆ’1 , bi and bi+1 . The delay and energy
consumption are primarily affected by transition patterns based on the bus signals biâˆ’1 , bi
and bi+1 as the crosstalk patterns in Table 1.
The selection of codeword does not depend on previous history, so the environment is
memoryless. Consequently coding must address the possibility that any two codewords can
appear one after the other. Therefore to avoid crosstalk and correct errors, we are interested
in constant weight codes of length n, weight w and minimum distance d â‰¥ 3 satisfying the
condition that there do not exist three consecutive coordinates i âˆ’ 1, i, i + 1 such that the
crosstalk couplings of type-2 (or -3, -4) occur in any two different codewords.
We denote such a code avoiding crosstalk of each type as an (n, d, w)-II (or -III, -IV)
code. The maximum size of these codes are denoted as A I I (n, d, w) (or A I I I (n, d, w),
A I V (n, d, w)), and any code achieving this size is optimal. When S âŠ† {I I, I I I, I V }, the
maximum size of a code that is simultaneously an (n, d, w)-S code for each S âˆˆ S is denoted
by AS (n, d, w).
When d = 2w, the following results are straightforward.
Lemma 2.3 For all positive integers n and w,
 
(i) A I I (n, 2w, w) = A I V (n, 2w, w) = wn ;




(ii) A I I I (n, 2w, w) = wn when w  = 1; A I I I (n, 2, 1) = n+1
2 .
 
Proof The quantity s = wn is an upper bound on the size of the desired code in each case.
We construct codes of size s as follows. The code with support
{{i, s + i, 2s + i, . . . , (w âˆ’ 1)s + i} : i âˆˆ [0, s âˆ’ 1]}
is an optimal (n, 2w, w)-II code. The code with support
{{wi, 1 + wi, . . . , (w âˆ’ 1) + wi} : i âˆˆ [0, s âˆ’ 1]}
is an optimal (n, 2w, w)-IV code, and an optimal

(n, 2w, w)-III code when w  = 1. When
w = 1, the code with support {{2i} : i âˆˆ [0, nâˆ’1


2 ]} is an optimal (n, 2, 1)-III code.
Next we show there is close connection between (n, 2k âˆ’ 2, k) codes of each type and
optimal LPSA(n, k)s. Hence, optimal codes are constructed based on the construction of
optimal LPSA(n, k)s.

3 Codes and LPSA(n, k; Î±)s
In this section, we establish connections between optimal LPSA(n, k; Î±)s and the codes of
each type. We begin with optimal (n, 2k âˆ’ 2, k)-II codes for sufficiently large n.
Theorem 3.1 Let k â‰¥ 3. Then A I I (n, 2k âˆ’ 2, k) â‰¥ B(n, k). Further, if B(n, k) = U (n, k)
and n â‰¥ 3k 2 + 2k âˆ’ 3, then A I I (n, 2k âˆ’ 2, k) = B(n, k).
Proof Whenever (X, B) is an LPSA(n, k), the code with support B is an (n, 2kâˆ’2, k)-II code.
Now suppose that (X, B) is an optimal LPSA(n, k) of size U (n, k). We prove that U (n, k)
is the largest possible size of an (n, 2k âˆ’ 2, k)-II code. Assume that D is an (n, 2k âˆ’ 2, k)-II
code of size M. Partition the code into three parts as follows.
The first part A contains all codewords with at least one segment â€œ11â€. Because n > k, for
each codeword in A, there always exist three adjacent coordinates such that â€œ110â€ or â€œ011â€

123

Optimal low-power coding for crosstalk avoidance

485

appears in these coordinates. Let S = {i : âˆƒu âˆˆ A, s.t.,u has â€œ110 in coordinates i âˆ’ 2, i âˆ’
1, i, or â€œ011â€ in coordinates i, i + 1, i + 2}, and let s = |S|. For each i âˆˆ S, there exist at
most two codewords in A that have â€œ110â€ in i âˆ’ 2, i âˆ’ 1, i or â€œ011â€ in i, i + 1, i + 2. Hence
|A| â‰¤ 2s.
The second part T âŠ† D \ A contains all codewords with â€œ1â€ in at least one coordinate in
S. Without loss of generality, if there exists a codeword in A with â€œ110â€ in the coordinates
i âˆ’ 2, i âˆ’ 1, i for some i, then the codewords in T with â€œ1â€ in i must have segment â€œ101â€
in these coordinates to avoid type-2 crosstalk. Because dmin (D) = 2k âˆ’ 2, there is only one
such codeword. So for each i âˆˆ S, there is at most one codeword in T with â€œ1â€ in i. Hence
|T | â‰¤ s.
Finally, let C = D \ (A âˆª T ). Then M = |A| + |T | + |C |. Because each codeword in C
has â€œ0â€ in all coordinates in S, we can shorten C to a code C  by deleting all coordinates in
S. Then C  is an (n âˆ’ s, 2k âˆ’ 2, k) code, and supp(C  ) is an (n âˆ’ s, k)-packing.
The shortening process partitions the coordinates of C  into at most s +1 classes, separated
in C by the coordinates deleted to form C  . No codeword of C  has â€œ11â€ in consecutive
coordinates of any single class. Let x be the number of isolated coordinates in this partition,
and m be the number of classes with at least two coordinates; then x + m â‰¤ s + 1. We now
estimate the 	
size of C
 using the




	 packing.
	
nâˆ’sâˆ’2
nâˆ’sâˆ’3
Let a0 = nâˆ’sâˆ’1
,
a
,
a
. Then we have:
=
=
1
2
kâˆ’1
kâˆ’1
kâˆ’1


x Â· a0 + 2m Â· a1 + (n âˆ’ s âˆ’ 2m âˆ’ x) Â· a2

| C | = |C | â‰¤
.
k
Because x âˆ’ y âˆ’ 1 â‰¤ x âˆ’ y â‰¤ x âˆ’ y, we have:

 	


	

 	


	

â¥
â¢ 	
â¢ x nâˆ’sâˆ’1 âˆ’ nâˆ’sâˆ’3 +2m nâˆ’sâˆ’2 âˆ’ nâˆ’sâˆ’3 + (n âˆ’ s) nâˆ’sâˆ’3 â¥
â¥
â¢
kâˆ’1
kâˆ’1
kâˆ’1
kâˆ’1
kâˆ’1
â¦
M â‰¤ 3s + â£
k



	



	

â¥
â¢ 	
2
1
nâˆ’sâˆ’3 â¥
â¢x
+
1
+
2m
+
1
+
(n
âˆ’
s)
â¢
â¥
kâˆ’1
kâˆ’1
kâˆ’1
â¦
â‰¤ 3s + â£
k
	
	

â¥

â¥
â¢
â¢
â¢ 2(s + 1) + (n âˆ’ s) nâˆ’sâˆ’3 â¥
â¢ 2x + 2m + (n âˆ’ s) nâˆ’sâˆ’3 â¥
â¢
â¢
â¥
â¥
kâˆ’1
kâˆ’1
â¦ â‰¤ 3s + â£
â¦.
â‰¤ 3s + â£
k
k

Let F(s) = 3s +

	
2(s+1)+(nâˆ’s)

Because F(s) = 3s +


	
2(s+2)+(nâˆ’sâˆ’1)
k

nâˆ’sâˆ’4
kâˆ’1




. We claim that because n â‰¥ 3k 2 + 2k âˆ’ 3,

k

U (n, k) â‰¥ maxsâˆˆ[1,n] F(s).


nâˆ’sâˆ’3
kâˆ’1



	
2(s+1)+(nâˆ’s)

, we have

	

nâˆ’sâˆ’3
kâˆ’1

k
nâˆ’sâˆ’4
kâˆ’1

k




âˆ’2






and F(s + 1) = 3(s + 1) +
	




âˆ’3 â‰¤ F(s)âˆ’F(s+1) â‰¤

nâˆ’sâˆ’4
kâˆ’1

+nâˆ’sâˆ’2

k

âˆ’2. Further, we have:




n âˆ’ 2k âˆ’ s
n âˆ’ 3k 2 âˆ’ 1 âˆ’ s
â‰¤ F(s) âˆ’ F(s + 1) â‰¤
.
k(k âˆ’ 1)
kâˆ’1
So when s â‰¤ n âˆ’ 3k 2 âˆ’ 1, F(s) âˆ’ F(s + 1) â‰¥ 0, i.e., F(s) is decreasing; and when
s â‰¥ n âˆ’ 2k, F(s) âˆ’ F(s + 1) â‰¤ 0, i.e., F(s) is increasing. When s âˆˆ [n âˆ’ 3k 2 , n âˆ’ 2k âˆ’ 1],

123

486

Y. M. Chee et al.

F(s) â‰¤ F(1); because the verification is tedious, we omit it here. We therefore only need to
compare F(1) and F(n) to find the maximum value of F(s).
	

â¥
â¢
â¢ 4+(n âˆ’ 1) nâˆ’4 â¥

 

2
â¢
kâˆ’1 â¥
â¦ âˆ’ 3n âˆ’ 2(n +1) â‰¥ (n âˆ’ 1)(n âˆ’ 3k âˆ’ 1) .
F(1) âˆ’ F(n) = 3+ â£
k
k
k(k âˆ’ 1)

Because n â‰¥ 3k 2 + 2k âˆ’ 3 â‰¥ 3k 2 + 1, F(1) â‰¥ F(n) and maxsâˆˆ[1,n] F(s) = F(1).


	


	

â¥
â¢ 	
â¢ 2 nâˆ’2 + (n âˆ’ 2) nâˆ’3 âˆ’ 4 âˆ’ (n âˆ’ 1) nâˆ’4 â¥
â¢ kâˆ’1
kâˆ’1
kâˆ’1 â¥
â¦âˆ’3
U (n, k) âˆ’ F(1) â‰¥ â£
k


	


	

â¥
â¢	
â¢ nâˆ’2 + (n âˆ’ 1) nâˆ’3 âˆ’ 4 âˆ’ (n âˆ’ 1) nâˆ’4 â¥
â¢ kâˆ’1
kâˆ’1
kâˆ’1 â¥
â¦âˆ’3
â‰¥â£
k


â¢	
â¥
â¢ nâˆ’2 âˆ’ 4 â¥


2
â¢ kâˆ’1
â¥
â¦ âˆ’ 3 â‰¥ n âˆ’ 3k âˆ’ 2k + 3 â‰¥ 0.
â‰¥â£
k
k(k âˆ’ 1)

Hence U (n, k) â‰¥ maxsâˆˆ[1,n] F(s).




For (n, 2k âˆ’ 2, k)-III codes and (n, 2k âˆ’ 2, k)-IV codes, we establish lower bounds.
 
Lemma 3.2 1. A I I I (n, 2k âˆ’ 2, k) â‰¥ A I I,I I I,I V (n, 2k âˆ’ 2, k) â‰¥ D( n2 , k).


2. A I I I (n, 4, 3) â‰¥ A I I,I I I,I V (n, 4, 3) â‰¥ B( n2 , 3) +  nâˆ’1
2 .
 
3. A I I I (n, 4, 3) â‰¥ A I I,I I I,I V (n, 4, 3) â‰¥ B â—¦ ( n2 , 3) +  n2 .
n
Proof For
 n the first inequality, take an ( 2 , k)-packing (X,
 B), and construct a code C of
length 2 by taking supp(C ) = B. View C as an |B| Ã— n2 array. When n â‰¡ 1 (mod 2),
we add one column of all zeroes between every two consecutive columns of C , and when
n â‰¡ 0 (mod 2) we add one further column of all zeroes after C to get an (n, 2k âˆ’ 2, k)-III
code. The verification is straightforward, because every second column is all zeroes.
  
The construction for the second is similar. Apply the same inflation to an LPSA n2 , 3
  
of size B n2 , 3 to obtain a code C1 . In every codeword of C1 , two 1s are separated by three
(or more) coordinates, and different codewords cannot have 1s in adjacent coordinates. Now
form code C2 , consisting of all codewords with support {2i, 2i +1, 2i +2} for 0 â‰¤ i <  nâˆ’1
2 .
No prohibited situation arises from 000 or 111 in three consecutive coordinates of a codeword.
In consecutive coordinates in which two codewords of C2 are neither 000 nor 111, the two
codewords contain 011 and 110, which is permitted. So we consider one codeword from
C1 and one from C2 . The coordinates with indices in {2i + 1 : 0 â‰¤ i <  nâˆ’1
2 } appear in
only one codeword, which is {2i, 2i + 1, 2i + 2}. So in the consecutive coordinates in which
two such codewords are neither 000 nor 111, and are not equal, the two codewords contain
{001, 100}, {010, 011}, or {010, 110}. All are permitted.
 n 
â—¦
2 ,3 =
 The
 n bound
 in the third case is equal to that in the second unless n is even and B
B 2 , 3 . When both occur, use a CPSA to form C1 and C2 as in the second case; one further
codeword can be added with support {0, n âˆ’ 2, n âˆ’ 1}.



123

Optimal low-power coding for crosstalk avoidance

487

Lemma 3.3 A I V (n, 2k âˆ’ 2, k) â‰¥ B(n, k).
Proof Take an LPSA(n, k) (X, B) of size B(n, k). Apply to the points in [0, n âˆ’ 1] the
permutation

i â†’ 2i, if i < n/2, and
i â†’ 2i âˆ’ 2n/2 + 1, if i â‰¥ n/2,
to get (X, B ). The code C  with supp(C  ) = B is an (n, 2k âˆ’ 2, k)-IV code.




We give another construction for an (n, 2kâˆ’2, k)-IV code from an optimal LPSA(n, k; kâˆ’
1). When k = 3, this construction gives a better lower bound than Lemma 3.3.
Lemma 3.4 Let k â‰¥ 3.
1. A I I,I V (n, 2k âˆ’ 2, k) â‰¥ B(n, k; k âˆ’ 1),	


2. A I V (n, 2k âˆ’ 2, k) â‰¥ B(n, k; k âˆ’ 1) + nâˆ’1
, and
	kâˆ’1 

n
I
V
â—¦
3. A (n, 2k âˆ’ 2, k) â‰¥ B (n, k; k âˆ’ 1) + kâˆ’1 .


	
Proof Let s = nâˆ’1
kâˆ’1 , and (X, B ) be an LPSA(n, k; k âˆ’ 1) of size B(n, k; k âˆ’ 1). Then the
code C with supp(C ) = {B : B âˆˆ B} is an (n, 2k âˆ’2, k)-II code and an (n, 2k âˆ’2, k)-IV code.
Further, the code C with supp(C ) = {B : B âˆˆ B}âˆª{{(kâˆ’1)i, (kâˆ’1)i +1, . . . , (kâˆ’1)i +kâˆ’1} :
i âˆˆ [0, s âˆ’1]} is an (n, 2k âˆ’2, k)-IV code. When n  â‰¡ 0 (mod k âˆ’ 1), statement (3) is implied
by statement (2). So suppose that n â‰¡ 0 (mod k âˆ’ 1). Using instead a CPSA(n, k; k âˆ’ 1) of
size B â—¦ (n, k; k âˆ’ 1), adjoin the block {(k âˆ’ 1)s, (k âˆ’ 1)s + 1, . . . , (k âˆ’ 1)s + k âˆ’ 2, 0}. 

Lemma 3.5 A I I,I V (n, 4, 3) â‰¤ U (n, 3; 2) when n â‰¥ 13.
Proof Computational results reported in Table 2 show that A I I,I V (13, 4, 3) = U (13, 3; 2) =
16, A I I,I V (14, 4, 3) = U (14, 3; 2) = 20, A I I,I V (15, 4, 3) = U (15, 3; 2) = 25, and
Table 2 Sizes of optimal codes for n â‰¤ 20
n

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

D (n, 3)

1

1

2

4

7

8

12

13

17

20

26

28

35

37

44

48

57

60

B (n, 3)

0

0

1

2

4

6

9

10

14

16

21

24

30

32

39

42

50

54

B â—¦ (n, 3)

0

0

0

2

3

5

9

10

13

16

20

23

30

32

38

42

49

53

B (n, 3; 2)

0

0

0

0

1

2

4

6

9

12

16

20

25

28

34

37

45

48

B â—¦ (n, 3; 2)

0

0

0

0

0

0

3

5

8

12

15

18

25

26

34

36

43

46

A I I .(n, 4, 3)

1

1

2

4

5

6

9

10

14

16

21

24

30

32

39

42

50

54

A I I I (n, 4, 3)

1

1

2

3

4

5

6

7

8

9

10

11

13

14

17

18

19

21

A I V (n, 4, 3)

1

1

2

4

6

7

10

12

15

19

23

26

32

35

42

45

54

57

A I I,I I I (n, 4, 3)

1

1

2

3

3

4

5

6

7

8

10

11

13

13

17

18

19

20

A I I,I V (n, 4, 3)

1

1

2

4

4

4

7

8

12

13

16

20

25

28

34

37

45

48

A I I I,I V (n, 4, 3)

1

1

2

3

4

5

6

7

8

9

10

11

13

14

17

18

19

21

A I I,I I I,I V (n, 4, 3)

1

1

2

3

3

4

5

6

7

8

10

11

13

13

17

18

19

20

Lower bounds and exact values

123

488

Y. M. Chee et al.

A I I,I V (16, 4, 3) = U (16, 3; 2) = 32. Suppose to the contrary that A I I,I V (n, 4, 3) >
U (n, 3; 2) for some n â‰¥ 17, and let n be the smallest such value. When n â‰¥ 17 we have
U (n, 3; 2) â‰¥ U (n âˆ’ 1, 3; 2) + 3 and U (n, 3; 2) â‰¥ U (n âˆ’ 3, 3; 2) + 4. (See Table 2 for small
values.)
Let (X, B) be the support of an (n, 4, 3)-{II,IV} code of size A I I,I V (n, 4, 3). Some triple
of B covers a pair of the form {a, b} âˆˆ {{i, i +1}, {i, i +2}} because it is not an LPSA(n, 3; 2).
Case 1 Some element appears in at most one triple. Suppose that element i appears in no
triple. Shorten the code by deleting coordinate i and delete the triples (if any) containing
pairs {i âˆ’ 1, i + 1}, {i âˆ’ 2, i + 1}, and {i âˆ’ 1, i + 2}. The result is a type II and IV code, so
the given code has at most A I I,I V (n âˆ’ 1, 4, 3) + 3 triples, a contradiction. Suppose now that
element i appears in exactly one triple T . Then if i âˆˆ {0, n âˆ’ 1}, delete coordinate i and triple
T to get a contradiction. If i âˆˆ {1, n âˆ’ 2}, delete coordinate i and delete triple T , along with
triples containing {0, 2} and {0, 3} when i = 1 or {n âˆ’ 4, n âˆ’ 1} and {n âˆ’ 3, n âˆ’ 1}, to get a
contradiction. So 2 â‰¤ i â‰¤ nâˆ’3. If T contains neither i âˆ’1 nor i +1, then no triple contains both
i âˆ’1 and i +1, because the code is type IV. Shorten by deleting coordinate i and delete triple T
and the triples (if any) containing pairs {i âˆ’2, i +1} and {i âˆ’1, i +2}, yielding a contradiction.
Otherwise, without loss of generality T also contains i âˆ’ 1 but does not contain i + 1. But
then if some triple T  contains i âˆ’ 2 and i + 1, it cannot contain i. If T  does not also contain
i âˆ’ 1, then we have T âˆ© {i âˆ’ 1, i, i + 1} = {i âˆ’ 1, i} and T  âˆ© {i âˆ’ 1, i, i + 1} = {i + 1},
which cannot happen in a type II code. So T  = {i âˆ’ 2, i, i + 1}. Hence there are at most
two triples among those containing pairs {i âˆ’ 1, i + 1}, {i âˆ’ 2, i + 1}, and {i âˆ’ 1, i + 2}, so
shorten as before.
Case 2 Some triple T satisfies |T âˆ© {i, i + 1, i + 2}| = 2 for some 0 â‰¤ i â‰¤ n âˆ’ 3. Suppose
that {a, b} = T âˆ© {i, i + 1, i + 2} and let {c} = {i, i + 1, i + 2} \ {a, b}. There can be no triple
containing c but neither a nor b, because the code is type II and type IV. So c is in exactly two
triples, T  that contains a and T  that contains b; only T contains both a and b. Applying the
same argument to T  and T  , a and b each appear in exactly two triples. So there are only
three triples (T , T  , and T  ) that contain a, b, or c. Shorten by deleting coordinate i + 1 and
the triples T , T  , and T  to obtain a contradiction.
Case 3 No triple T satisfies |T âˆ© {i, i + 1, i + 2}| = 2 for any 0 â‰¤ i â‰¤ n âˆ’ 3. If a triple
T satisfies |T âˆ© {i, i + 1, i + 2}| = 3 for some 0 â‰¤ i â‰¤ n âˆ’ 3, equivalently it satisfies
|T âˆ© {i + 1, i + 2, i + 3}| = 2 for some 0 â‰¤ i â‰¤ n âˆ’ 4 or |T âˆ© {i âˆ’ 1, i, i + 1}| = 2 for some
1 â‰¤ i â‰¤ n âˆ’ 3. Apply Case 2. Otherwise every triple T satisfies |T âˆ© {i, i + 1, i + 2}| â‰¤ 1 for
0 â‰¤ i â‰¤ n âˆ’ 3. But then (X, B) is an LPSA(n, 3; 2) and hence we have at most B(n, 3; 2) â‰¤
U (n, 3; 2) triples, the final contradiction.



4 Optimal packing sampling plans
By Corollary 2.2, we have the upper bound:

â§ 2
n âˆ’4n+4
âª
,
âª
6
âª
 




âª
2 âˆ’3n
nâˆ’2
nâˆ’3
â¨
n
2 2 + (n âˆ’ 2) 2
6 ,
U (n, 3) =
= n 2 âˆ’4n
âª
3
âª 6 ,
âª
âª
â© n 2 âˆ’3nâˆ’4 ,
6

Theorem 4.1 B(n, 3) = U (n, 3) for all n â‰¥ 0.

123

if n â‰¡ 2 (mod 6),
if n â‰¡ 3 (mod 6),
if n â‰¡ 0, 4 (mod 6),
if n â‰¡ 1, 5 (mod 6).

Optimal low-power coding for crosstalk avoidance

489

Proof When n â‰¡ 3 (mod 6), Colbourn and Rosa [4] (and Colbourn and Ling [5]) construct
2
a BSA(n, 3) of size n âˆ’3n
6 , which is an optimal LPSA(n, 3). Because each point appears in

(nâˆ’1) âˆ’4(nâˆ’1)+4
blocks, we get an LPSA(n âˆ’ 1, 3) of size n âˆ’3n
âˆ’ nâˆ’3
by removing
6
2 =
6
the point n âˆ’ 1 and all blocks containing it, which is optimal.
When n â‰¡ 1, 5 (mod 6), Colbourn and Rosa [4] showed there exists an (n, 3)-packing of
2
size n âˆ’3n+2
, whose leave graph consists of a cycle of length n âˆ’ 1 and one isolated point.
6
Assume n âˆ’1 is the isolated point. Remove the block {x, n âˆ’2, n âˆ’1} for some x âˆˆ [0, n âˆ’3];
the result is an optimal LPSA(n, 3). Now, n âˆ’ 1 appears in nâˆ’3
2 blocks. Removing n âˆ’ 1
and all blocks containing it from the optimal LPSA(n, 3) constructed above, we obtain an
2
(nâˆ’1)2 âˆ’4(nâˆ’1)
LPSA(n âˆ’ 1, 3) of size n âˆ’3nâˆ’4
âˆ’ nâˆ’3
, which is optimal.


6
2 =
6
2

2

nâˆ’3
2

Theorem 4.2 1. B â—¦ (n, 3) = U (n, 3) when n â‰¡ 0, 3, 4 (mod 6).
2. B â—¦ (n, 3) = U (n, 3) âˆ’ 1 when n â‰¡ 1, 2, 5 (mod 6).
blocks when n â‰¡
Proof The constructions in Theorem 4.1 yield a CPSA(n, 3) with n(nâˆ’3)
6
blocks
when
n
â‰¡
0,
4
(mod
6).
A
CPSA(n,
3)
can have at most
3 (mod 6) and with n(nâˆ’4)
6
 n  nâˆ’3 
blocks,
which
equals
U
(n,
3)
when
n
â‰¡
0,
3,
4
(mod
6),
so
these
are optimal.
3
2
 n  nâˆ’3 
â—¦
When n â‰¡ 2 (mod 6), 3 2
= U (n, 3) âˆ’ 1 so B (n, 3) â‰¤ U (n, 3) âˆ’ 1. When
n â‰¡ 1, 5 (mod 6), if there were a CPSA(n, 3) with U (n, 3) =
n(nâˆ’1)
2

3(n 2 âˆ’3nâˆ’4)

n 2 âˆ’3nâˆ’4
6

codewords, then the

âˆ’
= n + 2. The leave must be an
number of edges in the leave graph is
6
n-cycle with two additional edges, but every vertex in the leave must have even degree, which
cannot occur. So B â—¦ (n, 3) â‰¤ U (n, 3) âˆ’ 1. To establish equality when n â‰¡ 1, 2, 5 (mod 6),
remove the block {0, n âˆ’ 1, x} from an LPSA(n, 3) from Theorem 4.1.


Lemma 4.3 B â—¦ (n, 3; 2) = B(n, 3; 2) = U (n, 3; 2) whenever n â‰¡ 3, 5 (mod 6) and n â‰¥
15. B â—¦ (n, 3; 2) + 2 = B(n, 3; 2) = U (n, 3; 2) whenever n â‰¡ 2, 4 (mod 6) and n â‰¥ 14.
Proof Zhang and Chang [30] establish that whenever n â‰¥ 15 and n â‰¡ 3, 5 (mod 6), there is a
BSA(n, 3; 2) having n(nâˆ’5)
blocks; this is also an optimal CPSA(n, 3; 2) and LPSA(n, 3; 2).
6
Now suppose that n â‰¥ 14 and n â‰¡ 2, 4 (mod 6). When n â‰¡ 2 (mod 6), writing n = 6t + 2,
U (6t + 2, 3; 2) = (2t)(3t âˆ’ 1). Delete element 6t + 2 from a BSA(6t + 3, 3; 2) with
(2t + 1)(3t âˆ’ 1) blocks, removing 3t âˆ’ 1 blocks to obtain an LPSA(6t + 2, 3; 2), which is
therefore optimal. When n â‰¡ 4 (mod 6), writing n = 6t + 4, U (6t + 4, 3; 2) = t (6t + 2).
Delete element 6t + 4 from a BSA(6t + 5, 3; 2) with t (6t + 5) blocks, removing 3t blocks to
obtain an LPSA(6t + 4, 3; 2), which is therefore optimal. Remove the blocks {0, n âˆ’ 2, x},
{1, n âˆ’ 1, y} for some x and y from the optimal LPSA(n, 3; 2) constructed above to obtain
an optimal CPSA(n, 3; 2) when n â‰¡ 2, 4 (mod 6).


	 	



For n = 6t, U (6t, 3) = 6t (t âˆ’ 1) + 1, and 6t3 6tâˆ’5
= 6t (t âˆ’ 1). For n = 6t + 1,
2
 6t+1  6tâˆ’4 
U (6t +1, 3) = t (6t âˆ’3), and 3
= t (6t âˆ’3)âˆ’1. However, if a CPSA(6t +1, 3; 2)
2
were to have t (6t âˆ’ 3) âˆ’ 1 blocks, its leave must have 2(6t + 1) + 1 edges and every such
graph with minimum degree 4 has two vertices of degree 5. Because all vertices in the leave
must have even degree, no CPSA(6t + 1, 3; 2) can exist with more than t (6t âˆ’ 3) âˆ’ 2 blocks.
We provide bounds to apply when n â‰¡ 0, 1 (mod 6).
Lemma 4.4 B(2n, 3; 2) â‰¥ 4B(n, 3), and B(2n + 1, 3; 2) â‰¥ 4B(n, 3) + n âˆ’ 2. In addition,
B â—¦ (2n, 3; 2) â‰¥ 4B â—¦ (n, 3), and B â—¦ (2n + 1, 3; 2) â‰¥ 4B â—¦ (n, 3) + n âˆ’ 3.

123

490

Y. M. Chee et al.

Proof Start with an LPSA(n, 3) on [0, n âˆ’ 1]. We form an LPSA(2n, 3; 2) on [0, 2n âˆ’ 1].
For each block {a, b, c} in the LPSA, form four blocks {{2a + Î±, 2b + Î², 2c + Î³ } : Î±, Î², Î³ âˆˆ
{0, 1}, Î± + Î² + Î³ â‰¡ 0 (mod 2)}. The verification is straightforward. To form an LPSA(2n +
1, 3) on [0, 2n], adjoin {{2i, 2i + 3, 2n} : 0 â‰¤ i â‰¤ n âˆ’ 3}.
The construction for CPSAs is the same, except that one does not adjoin {0, 3, 2n}.



5 Conclusion
Applying Theorem 3.1 with the results in Theorem 4.1, we have optimal (n, 4, 3)-II codes for
all n â‰¥ 30. By computer search (using cliquer [13] and hill-climbing (a variant of [24])),
we determined the sizes of optimal LPSA(n, 3; Î±)s, CPSA(n, 3; Î±)s, and (n, 4, 3) codes of
lengths n â‰¤ 20. The sizes are listed in Table 2 and corresponding optimal codes are available
from the authors; those in slanted font are lower bounds from Theorem 3.1 and Lemma 3.4.
In this paper, we present the first memoryless transition bus-encoding technique for power
minimization, error-correcting and elimination of crosstalk simultaneously. We establish the
connection between codes avoiding crosstalk of each type with packing sampling plans
avoiding adjacent units. Optimal codes of each type are constructed.

References
1. Bertozzi D., Benini L., de Micheli G.: Low power error resilient encoding for on-chip data buses. In:
DATEâ€™02: Proceedings of the Conference on Design. Automation and Test in Europe, pp. 102â€“109. IEEE
Computer Society, Washington, DC (2002).
2. Bertozzi D. Benini L., Ricco B.: Energy-efficient and reliable low-swing signaling for on-chip buses
based on redundant coding. In: ISCASâ€™02: Proceedings of the IEEE International Symposium on Circuits
and Systems, vol. 1, pp. 93â€“96. IEEE Press, Piscataway, NJ (2002).
3. Chee Y.M., Colbourn C.J., Ling A.C.H.: Optimal memoryless encoding for low power off-chip data
buses. In: ICCADâ€™06: Proceedings of the 2006 IEEE/ACM International Conference on Computer-Aided
Design, pp. 369â€“374. ACM Press, New York (2006).
4. Colbourn C.J., Rosa A.: Quadratic leaves of maximal partial triple systems. Graphs Comb. 2(4), 317â€“337
(1986).
5. Colbourn C.J., Ling A.C.H.: A class of partial triple systems with applications in survey sampling.
Commun. Stat. Theory Methods 27(4), 1009â€“1018 (1998).
6. Duan C., Tirumala A., Khatri S.P.: Analysis and avoidance of crosstalk in on-chip buses. In: Hot Interconnectsâ€™01: Proceedings of the 9th Annual Symposium on High-Performance Interconnects, pp. 133â€“138.
IEEE, Piscataway (2001).
7. Dukes P.J., Ling A.C.H.: Existence of balanced sampling plans avoiding cyclic distances. Metrika 70,
131â€“140 (2009).
8. Favalli M., Metra C.: Bus crosstalk fault-detection capabilities of error-detecting codes for on-line testing.
IEEE Trans. Very Large Scale Integr. Syst. 7, 392â€“396 (1999).
9. Hamming R.W.: Error detecting and error correcting codes. Bell Syst. Tech. J. 29(2), 147â€“160 (1950).
10. Hedayat A.S., Rao C.R., Stufken J.: Sampling plans excluding contiguous units. J. Stat. Plan. Inference
19(2), 159â€“170 (1988).
11. Ho R.: On-chip wires: Scaling and efficiency, Ph.D. dissertation, Department of Electrical Engineering,
Stanford University, Palo Alto, CA (2003).
12. Khan Z., Arslan T., Erdogan A.T.: A dual low-power and crosstalk immune encoding scheme for systemon-chip buses. In: PATMOSâ€™04: Proceedings of the 14th International Workshop on Power and Timing
Modeling, Optimization and Simulation. Lecture Notes in Computer Science, vol. 3254, pp. 585â€“592.
Springer, Berlin (2004).
13. Niskanen S., Ã–stergÃ¥rd P.R.J.: Cliquer Userâ€™s Guide, Version 1.0, Communications Laboratory, Helsinki
University of Technology, Espoo. Tech. Rep. T48, (2003).
14. Patel K.N., Markov I.L.: Error-correction and crosstalk avoidance in DSM busses. IEEE Trans. Very
Large Scale Integr. Syst. 12(10), 1076â€“1080 (2004).

123

Optimal low-power coding for crosstalk avoidance

491

15. Ramprasad S., Shanbhag N.R., Hajj I.N.: A coding framework for low-power address and data busses.
IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 7, 212â€“221 (1999).
16. Rossi D., van Dijk E.S., Kleihorst R.P., Nieuwland A.K., Metra C.: Coding scheme for low energy
consumption fault-tolerant bus. In: IOLTWâ€™02: Proceedings of the Eighth IEEE International On-Line
Testing Workshop, pp. 8â€“12. IEEE Computer Society, Washington, DC (2002).
17. Rossi D., Cavallotti S., Metra C.: Error correcting codes for crosstalk effect minimization. In: DFTâ€™03:
Proceedings of the 18th IEEE International Symposium on Defect and Fault-Tolerance in VLSI Systems,
pp. 257â€“266. IEEE Computer Society, Washington, DC (2003).
18. Rossi D., van Dijk E.S., Kleihorst R.P., Nieuwland, A.K., Metra, C.: Power consumption of fault tolerant
codes: the active elements. In IOLTWâ€™03: Proceedings of the Ninth IEEE International On-line Testing
Workshop, pp. 61â€“67. IEEE Computer Society, Washington, DC (2003).
19. Samala N.K., Radhakrishnan D., Izadi B.: A novel deep sub-micron bus coding for low energy. In: ESAâ€™04:
Proceedings of the International Conference on Embedded Systems and Applications, pp. 25â€“30. CSREA
Press, Leuven (2004).
20. Shannon C.E.: A mathematical theory of communications, Bell Syst. Tech. J. 27(3), 379â€“423, 623â€“656
(1948).
21. Sotiriadis P.P., Chandrakasan A.: Bus energy minimization by transition pattern coding (TPC) in deep
sub-micron technologies. In: ICCADâ€™00â€”Proceedings of the 2000 IEEE/ACM International Conference
on Computer-Aided Design, pp. 322â€“327. IEEE, Piscataway, NJ (2000).
22. Stan M.R., Burleson W.P.: Bus-invert coding for low-power I/O. IEEE Trans. Very Large Scale Integr.
Syst. 3(1), 49â€“58 (1995).
23. Stan M.R., Burleson W.P.: Coding a terminated bus for low power. In: Great Lakes Symposium VLSI,
pp. 70â€“73, Buffalo, NY (1995).
24. Stinson D.R.: Hill-climbing algorithms for the construction of combinatorial designs. In: Algorithms in
Combinatorial Design Theory. Annals of Discrete Mathematics, vol. 26, pp. 321â€“334. Elsevier, NorthHolland (1985).
25. Stufken J.: Combinatorial and statistical aspects of sampling plans to avoid the selection of adjacent units.
J. Comb. Inf. Syst. Sci. 18(1â€“2), 149â€“160 (1993).
26. Su C.L., Tsui C.Y., Despain A.M.: Saving power in the control path of embedded processors. IEEE Des.
Comput. 11, 24â€“30 (1994).
27. Subrahmanya P., Manimegalai R., Kamakoti V., Mutyam M.: A bus encoding technique for power and
cross-talk minimization. In: VLSI Design 2004: 17th International Conference on VLSI Design, pp.
443â€“448. IEEE Computer Society, Xiâ€™an (2004).
28. Victor B., Keutzer K.: Bus encoding to prevent crosstalk delay. In: ICCADâ€™01â€”Proceedings of the
2001 IEEE/ACM International Conference on Computer-Aided Design, pp. 57â€“63. IEEE, Piscataway,
NJ (2001).
29. Wright J.H., Stufken J.: New balanced sampling plans excluding adjacent units. J. Stat. Plan. Inference
138, 3326â€“3335 (2008).
30. Zhang J., Chang Y.X.: The spectrum of BSA(v, 3, Î»; Î±) with Î± = 2, 3. J. Comb. Des. 15, 61â€“76 (2007).

123

SIAM J. DISCRETE MATH. Vol. 27, No. 4, pp. 1844­1861

c 2013 Society for Industrial and Applied Mathematics 

SEQUENCE COVERING ARRAYS

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

YEOW MENG CHEE , CHARLES J. COLBOURN , DANIEL HORSLEY§ , AND JUNLING ZHOU¶ Abstract. Sequential processes can encounter faults as a result of improper ordering of subsets of the events. In order to reveal faults caused by the relative ordering of t or fewer of v events, for some fixed t, a test suite must provide tests so that every ordering of every set of t or fewer events is exercised. Such a test suite is equivalent to a sequence covering array, a set of permutations on v events for which every subsequence of t or fewer events arises in at least one of the permutations. Equivalently it is a (different) set of permutations, a completely t-scrambling set of permutations, in which the images of every set of t chosen events include each of the t! possible "patterns." In event sequence testing, minimizing the number of permutations used is the principal objective. By developing a connection with covering arrays, lower bounds on this minimum in terms of the minimum number of rows in covering arrays are obtained. An existing bound on the largest v for which the minimum can equal t! is improved. A conditional expectation algorithm is developed to generate sequence covering arrays whose number of permutations never exceeds a specified logarithmic function of v when t is fixed, and this method is shown to operate in polynomial time. A recursive product construction is established when t = 3 to construct sequence covering arrays on vw events from ones on v and w events. Finally computational results are given for t  {3, 4, 5} to demonstrate the utility of the conditional expectation algorithm and the product construction. Key words. sequence covering array, completely scrambling set of permutations, covering array, directed t-design AMS subject classifications. 05B40, 05B15, 05B30, 05A05 DOI. 10.1137/120894099

1. Introduction. A set of permutations {1 , . . . , N } of a v -element set X is completely t-scrambling if for every ordered t-set (x1 , . . . , xt ) with xi  X for 1  i  t, there is some  (1    N ) for which  (xi ) <  (xj ) if and only if i < j . Spencer [32] first explored the existence of completely t-scrambling sets of permutations in generalizing a question of Dushnik [15] on linear extensions. Recently Kuhn et al. [23, 24] examined an equivalent combinatorial object, the sequence covering array. For parameters N , t, and v , such an array is a set of n permutations of v letters so that every permutation of every t of the v letters appears--in the specified order--in at least one of the n permutations. The motivation for finding sequence covering arrays with small values of n arises in event sequence testing. Suppose that a process involves a sequence of v tasks or events. The operator may, unfortunately, fail to do the tasks in the correct sequence. When this happens, errors may occur. But we anticipate that errors can be attributed to the (improper) ordering of a small
 Received by the editors October 8, 2012; accepted for publication (in revised form) September 4, 2013; published electronically October 28, 2013. http://www.siam.org/journals/sidma/27-4/89409.html  School of Physical and Mathematical Sciences, Nanyang Technological University 637371, Singapore (YMChee@ntu.edu.sg).  School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85257, and State Key Laboratory of Software Development Environment, Beihang University, Beijing 100191, China (colbourn@asu.edu). This author was supported by Australian Research Council grant DP120103067. § School of Mathematical Sciences, Monash University, Melbourne, Australia (danhorsley@ gmail.com). This author was supported by Australian Research Council grants DP120103067 and DE120100040. ¶ Department of Mathematics, Beijing Jiaotong University, Beijing, China (jlzhou@bjtu.edu.cn).

1844

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1845

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

subset of tasks. When each permutation of a sequence covering array is used in turn to specify a task order, every potential ordering of t or fewer tasks will be tried and hence all errors found that result solely from the improper ordering of t or fewer tasks. Applications are discussed further in [19, 23, 39, 40]; related event sequence testing problems in which tasks can be repeated are discussed in [42, 43, 44]. While the application of these combinatorial structures is of much practical concern, our interest is in bounds on the size of sequence covering arrays and their explicit construction. We first state the problem formally. Let  = {0, . . . , v - 1} be symbols that represent the v tasks or events. A t-subsequence of  is a t-tuple (x1 , . . . , xt ) with xi   for 1  i  t, and xi = xj when i = j . A permutation  of  covers the t-subsequence (x1 , . . . , xt ) if  -1 (xi ) <  -1 (xj ) whenever i < j . For example, with v = 5 and t = 3, (4, 0, 3) is a 3-subsequence that is covered by the permutation 4 2 0 3 1. A sequence covering array of order v and strength t, or SeqCA(N ; t, v ), is a set  = {1 , . . . , N }, where i is a permutation of , and every t-subsequence of  is covered by at least one of the permutations {1 , . . . , N }. Often the permutations are written as an N × v array. We use an array representation for completely t-scrambling sets of permutations as well. An N × v array is a completely t-scrambling set of permutations of strength t on v symbols, or CSSP(N ; t, v ), when the columns are indexed by  and the symbols by , and for every way c1 , . . . , ct to choose t distinct columns and every permutation  of {1, . . . , t}, there is a row  for which, for every 1  a < b  t, the entry in cell (, c(a) ) is less than the entry in cell (, c(b) ). Lemma 1.1. A CSSP(N ; t, v ) is equivalent to a SeqCA(N ; t, v ). Proof. If 1 , . . . , N are the N permutations of a SeqCA(N ; t, v ), form an N × v -1 array A in which cell (i, j ) contains i (j ). Then A is a CSSP(N ; t, v ). In the opposite direction, if A is a CSSP(N ; t, v ), define permutation i by setting i (aij ) = j for 1  i  N and 0  j < v . Then 1 , . . . , N form the N permutations of a SeqCA(N ; t, v ). In the CSSP(8;3,5) of Table 1.1, the symbols {1, 2, 3} appear as 123 once, 132 once, 213 once, 231 zero times, 312 four times, and 321 once. Hence the rows of a completely t-scrambling set of permutations do not necessarily produce a sequence covering array; nevertheless they are conjugates, obtained by interchanging the roles of columns and symbols. Permutation problems concerning the avoidance of specified patterns of subsequences have been extensively studied in algebraic and probabilistic combinatorics; see [33] for an excellent survey. (Here two subsequences (x1 , . . . , xt ) and (y1 , . . . , yt ) have the same pattern when, for 1  i < t, xi < xi+1 if and only if yi < yi+1 .)
Table 1.1 Example: SeqCA(8;3,5) ­ t = 3, v = 5, N = 8. SeqCA 4203 1430 3120 0241 2134 0341 3021 4120 CSSP 2413 3042 3120 0314 4102 0341 1320 3124

1 2 4 3 0 2 4 3

0 1 4 2 3 2 4 0

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1846

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

While cosmetically similar to pattern avoidance problems, the existence problem for sequence covering arrays requires coverage rather than avoidance and requires that all subsequences be covered and not simply every pattern. The question of principal concern in this paper is as follows: Given t and v , what is the smallest N for which a CSSP(N ; t, v ) (equivalently, a SeqCA(N ; t, v )) exists? Call this number SeqCAN(t, v ). In the vernacular of completely t-scrambling sets of permutations, Spencer [32] did the foundational work, and F¨ uredi [17], Ishigami [20, 21], Radhakrishnan [31], and Tarui [36] made improvements. In a sequence covering array, every t symbols must appear in each of the t! possible orderings, and there are v t t! t-subsequences in total, so t!  SeqCAN(t, v )  v t! t

Both bounds are trivial, but the lower bound is the correct one when t = 2. Lemma 1.2. SeqCAN(2, v ) = 2 for all v  2. Proof. Any permutation on v symbols and its reversal form a SeqCA(2; 2, v ). When t  3, neither bound is correct as v increases. Indeed the growth as a function of v for fixed t is logarithmic. Theorem 1.3 (see [31, 32]). For t  3, 1+ 2 log2 (e) (t - 1)! v 2v - t + 1 log2 (v - t + 2)  SeqCAN(t, v )  t log(v ) log
t! t!-1

.

The question of when SeqCAN(t, v ) = t! is of independent interest in yet another setting. Let V be a finite set; an element of V is a vertex. A transitive tournament on V is a directed graph in which (1) for all x  V , (x, x) is not an arc; (2) for distinct x, y  V , (x, y ) is an arc if and only if (y, x) is not an arc; and (3) whenever (x, y ) and (y, z ) are arcs, so is (x, z ). A transitive tournament T = (V, A) has transitive tournament T  = (W, B ) as a subdigraph, denoted T   T , whenever W  V and B  A. Let (V, T ) be a finite set V of cardinality v and a collection T with every T  T being a transitive tournament on k of the vertices in V ; members of T are blocks. Then (V, T ) is a (t, )-directed packing of blocksize k and order v , or DP (t, k, v ), if for every X  V with |X | = t and every transitive tournament T  on vertex set X , |{T  T : T   T }|  . On the other hand, (V, T ) is a (t, )-directed covering of blocksize k and order v , or DC (t, k, v )), if for every X  V with |X | = t and every transitive tournament T  on vertex set X , |{T  T : T   T }|  . When (V, T ) is a DP (t, k, v ) and also a DC (t, k, v ), it is a (t, )-directed design of blocksize k and order v , or DD (t, k, v ). In this notation, the subscript is often omitted when  = 1. Directed designs with t = 2 have been extensively studied as generalizations of balanced incomplete block designs. The study of (t, 1)-directed packings has also been extensive as a result of their equivalence to "deletion-correcting codes" (see Levenshtein [25]). The connection with our investigation follows.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1847

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Lemma 1.4. A CSSP(N ; t, v ) is equivalent to a DC(t, v, v ) with N blocks. Moreover, a DD(t, v, v ) exists if and only if SeqCAN(t, v ) = t!. Proof. For each row (a0 , . . . , av-1 ) of the CSSP, form a transitive tournament on vertex set {0, . . . , v - 1} by including, for 0  i < j < v , arc (i, j ) if ai < aj and arc (j, i) otherwise. Each transitive tournament on t of these vertices is a subdigraph of at least one of these N tournaments. The other direction is similar. When N = t!, every t-subsequence is covered exactly once, and every transitive tournament of order t arises as a subdigraph exactly once. Theorem 1.5. Sufficient conditions for a DD(t, v, v ) to exist include 1. t  3 and t  v  t + 1 [25] and 2. t = 4 and v = 6 [28]. Necessary conditions for a DD(t, v, v ) to exist include 1. v  t + 1 for t  {3, 5, 6} [28], 2. v  t + 2 for t = 4 [28], and - 1 for t  7 [28]. 3. v  t+1 2 Levenshtein [25] had conjectured that v  t + 1 whenever a DD(t, v, v ) exists for t  3. As stated in Theorem 1.5, this does not hold for t = 4, but this is the only known exception to Levenshtein's conjecture. In the next section we significantly reduce the upper bound on the largest v for which SeqCAN(t, v ) can equal t!. 2. Lower bounds. Here we extend a technique used in [17, Theorem 5.1], improving on a method of Ishigami [21]. We require a number of previous results on covering arrays, introduced next. See [8] for a more thorough introduction to them. Let N , k , t, and v be positive integers. Let C be an N × k array with entries from an alphabet  of size v ; we typically take  = {0, . . . , v - 1}. When (1 , . . . , t ) is a t-tuple with i   for 1  i  t, (c1 , . . . , ct ) is a tuple of t column indices (ci  {1, . . . , k }), and ci = cj whenever i = j , the t-tuple {(ci , i ) : 1  i  t} is a t-way interaction. The array covers the t-way interaction {(ci , i ) : 1  i  t} if, in at least one row  of C, the entry in row  and column ci is i for 1  i  t. Array C is a covering array CA(N ; t, k, v ) of strength t if it covers every t-way interaction. CAN(t, k, v ) is the minimum N for which a CA(N ; t, k, v ) exists. The basic goal is to minimize the number of rows (tests) required and hence to determine CAN(t, k, v ). When t  2 and v  2 are both fixed, CAN(t, k, v ) is (log k ) (see, for example, [8]). We strengthen this standard definition somewhat. For a t-way interaction T = {(ci , i ) : 1  i  t} with symbols chosen from  = {0, . . . , v - 1}, let  (T ) = -1 |{i : i =  }|. Then define µ(T ) = v =0  (T )!. The natural interpretation is that µ(T ) is the number of ways to permute the columns (c1 , . . . , ct ) so that the symbols appear in the same order. A covering array provides excess coverage when every tway interaction T is covered by at least µ(T ) rows; such a covering array is denoted by CAX (N ; t, k, v ). More generally, the subscript X is used to extend notation from covering arrays to those having excess coverage. Theorem 2.1. Let v , t, and a be integers satisfying v  t  3 and t  a  0. Then SeqCAN(t, v )  a!CANX (t - a, v - a, a + 1). Proof. Let S be a CSSP(N ; t, v ). Choose any a columns of S, {e1 , . . . , ea }. For each ordering  of these columns, form a matrix C that contains all rows of S in which the entry in column  (ei ) is less than that in column  (ei+1 ) for 1  i < a. Because there are a! orderings and every row of S appears in exactly one of the {C }, it suffices to show that for every choice of  the number n of rows in C is at least CANX (t - a, v - a, a + 1). To do this, form an n × (v - a) array A whose columns are the columns of C that are not among the a selected. To determine the content of

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1848

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

cell (r, c) of A , examine the symbol  in column c and row r of C . If  is less than the symbol in row r and column  (e1 ), the entry is set to 0. If  is greater than the symbol in row r and column  (ea ), the entry is set to a. Otherwise find the unique j for which  is greater than the symbol in row r and column  (ej ) but less than the symbol in row r and column  (ej +1 ), and set the entry to j . Now we claim that A is a CAX (n; t - a, v - a, a + 1). The verification requires demonstrating that every (t - a)-way interaction T is covered at least µ(T ) times. So let T = {(fi , i ) : 1  i  t - a}, noting that i  {0, . . . , a} and fi indexes a column of A. We form permutations of {e1 , . . . , ea }  {f1 , . . . , ft-a } that are consistent with  on {e1 , . . . , ea } in that these columns appear in the order prescribed by  . To do this, there are 0 (T ) columns with entry 0; place one of the 0 (T )! orderings of these columns so that all appear before  (e1 ). There are a (T ) columns with entry a; place one of the a (T )! orderings of these columns so that all appear after  (ea ). For 1  j < a, there are j (T ) columns with entry j ; place one of the j (T )! orderings of these columns so that all appear before  (ej ) and after  (ej +1 ). In this way we can form µ(T ) permutations of {e1 , . . . , ea }  {f1 , . . . , ft-a }, each consistent with  . Because each is consistent with  , it appears in C . But each such appearance in C results in a different row of A that covers T , and hence T is indeed covered at least µ(T ) times. The easiest applications of Theorem 2.1 result from using CAN(t, k, v ) as a lower bound for CANX (t, k, v ). Apply it with a = t - 1, noting that CAN(1, k, t) = t for all k  1, to recover the trivial lower bound that SeqCAN(t, v )  t!. Apply it with a = t - 2 to establish that SeqCAN(t, v )  (t - 2)!CAN(2, v - t + 2, t - 1). Now we return to the question of when SeqCAN(t, v ) can equal t!, or equivalently when a DD(t, v, v ) can exist. Theorem 2.1 ensures that SeqCAN(t, v )  (t - 2)!CANX (2, v - t + 2, t - 1), so SeqCAN(t, v ) = t! can hold only when CANX (2, v - t + 2, t - 1)  t(t - 1). The 2-way interaction T = {(c1 , 1 ), (c2 , 2 )} has µ(T ) = 2 exactly when 1 = 2 (called a constant pair) and has µ(T ) = 1 otherwise (a nonconstant pair). Because, for each pair of columns, t - 1 constant pairs must be covered twice each, and (t - 1)(t - 2) nonconstant pairs must be covered at least once each, CANX (2, v - t + 2, t - 1)  t(t - 1). So we are concerned with when equality can hold. Lemma 2.2. For v  4, CANX (2, k, v ) = v (v + 1) only if k  v + 2. Proof. Suppose that a CAX (v (v + 1); 2, k, v ) exists with columns indexed by {1, . . . , k } and symbols by {0, . . . , v - 1}. We form sets on symbols V = ({1, . . . , k } × {0, . . . , v - 1})  {}. The system of sets (blocks) B is formed as follows. For every row (x1 , . . . , xk ) of the covering array, a set {(i, xi ) : 1  i  k } is placed in B . Then for every 1  i  k , a set {(i, j ) : 0  j < v }  {} is placed in B . The set system (V, B ) has kv + 1 symbols and v (v + 1) + k blocks. By construction, every two different symbols appear together in exactly one block, unless the pair is of the form {(i, j ), (i , j )} corresponding to a constant pair and therefore occurring in exactly two blocks. Now form a (kv + 1) × (v (v + 1) + k ) matrix A, which is the symbol-block incidence matrix, as follows. Rows are indexed by symbols, columns by blocks. The matrix contains the entry 1 in row r and column c when symbol r appears in block c and 0 otherwise. Now examine B = AAT , which has rows and columns indexed by V . Its diagonal entries are k in entry (, ) and v + 2 elsewhere. Its off-diagonal entries are 2 in cells indexed by ((i, j ), (i , j )) with i = i and 1 otherwise. The rank of B cannot exceed the number of columns in A, namely, v (v + 1) + k . So in order to bound k , we bound the rank of B.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1849

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Write D = B - J. The rows indexed by V \ {} can be partitioned into v parts; a part is formed by including all rows and columns with indices {(i, j ) : 1  i  k } for some j with 0  j < v . Then D can be written as a block diagonal matrix with v k × k block matrices each equal to X = v I + J and a single 1 × 1 block matrix with entry k - 1. Now det(D) = (k - 1)(det(X))v , and det(X) = v k (1 + k v ) by the matrix determinant lemma. Hence rank(D) = kv + 1. Because B is obtained from D by a rank one update, rank(B)  kv . v +1) Consequently, kv  v (v + 1) + k , or k  v( v -1 . Thus k  v + 2 when v  4, because k is an integer. This enables us to establish a substantial improvement on the bound of Mathon and Tran Van Trung [28] stated in Theorem 1.5. Theorem 2.3. If t  3 and SeqCAN(t, v ) = t! (or equivalently, a DD(t, v, v ) exists), then v  2t - 1. Proof. If 3  t  6, this follows from Theorem 1.5. By Theorem 2.1, t! = SeqCAN(t, v )  (t - 2)!CANX (2, v - t + 2, t - 1) and thus CANX (2, v - t + 2, t - 1) = t(t - 1). By Lemma 2.2, v - t - 2  t - 1 + 2 and hence v  2t - 1 as required. It appears plausible that the bound should be t +2 rather than 2t - 1; nevertheless, the method here gives the first bound that is linear in t. 3. Upper bounds from probabilistic methods. Spencer [32] analyzed a method that selects a set of N permutations on v symbols uniformly at random; we explore this first. v! t! Lemma 3.1. For fixed t  3, SeqCAN(t, v )  1 + (log( (v- t)! ))/(log( t!-1 )). Proof. A permutation of {0, . . . , v - 1} chosen uniformly at random covers any 1 specific t-subsequence with probability t ! and so fails to cover it with probability t!-1 . Then N permutations of { 0 , . . . , v - 1} chosen uniformly at random and int!
-1 dependently together fail to cover a specific t-subsequence with probability t!t . ! v! There are (v-t)! t-subsequences. When N permutations are chosen, each subsequence N t!-1 N . t!

is not covered with probability
v! (v -t)!

Thus the expected number of uncovered 1, a SeqCA(N ; t, v ) must

t-subsequences is

t!-1 N . t!

exist. This holds whenever N >

t!-1 N < t! v! t! (log( (v-t)! ))/(log( t!-1 )).

When

v! (v -t)!

3.1. One permutation at a time. Lemma 3.1 provides a useful upper bound on the size of completely t-scrambling sets of permutations but does not provide an effective method to find such arrays. Stein [34], Lov´ asz [26], and Johnson [22] develop a general strategy for finding solutions to covering problems; this algorithm has been shown to lead to polynomial time methods in many combinatorial covering problems [3, 4, 7, 9, 10]. We extend that strategy here to treat sequence covering arrays. The basic approach is greedy. Repeatedly select one permutation to add that covers a large number of as-yet-uncovered t-subsequences, until all are covered. Stein [34], Lov´ asz [26], and Johnson [22] each suggest selecting to maximize the number of newly covered elements, but their analyses require only that the next selection cover at least the average. If after i permutations are selected there remain Ui uncovered t-subsequences, then a permutation selected uniformly at random is expected to cover 1 Ui t ! t-subsequences for the first time. Provided that we select the (i +1)st permutation 1 t!-1 to cover at least Ui t ! t-subsequences for the first time, we have that Ui+1  Ui t! . v! v! t!-1 i Because U0 = (v-t)! , we have that Ui  (v-t)! ( t! ) . Choose N to be the smallest

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1850

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

value for which UN < 1; then there must be a sequence covering array with N permutations. This simply restates the argument of Lemma 3.1, but with two important improvements. It derandomizes the method by ensuring that appropriate selection of each permutation guarantees that the bound is met, rather than asserting the existence of a set of permutations that meets it. More importantly, the time to construct the sequence covering array is polynomial in the number of permutations and the time to select a permutation that covers at least the average. For fixed t the number of permutations is logarithmic in v , and so an algorithm with running time polynomial in v will result if we can select the next permutation in time polynomial in v . Because the details are quite similar to earlier approaches, we merely outline how this can be done. Suppose that U consists of the as-yet-uncovered t-subsequences. For U  U , let cov( , U ) = 1 when  covers U , and cov( , U ) = 0 otherwise. Let R be an rsubsequence; the r symbols in R are fixed, and the remaining v - r are free. There ! is a set PR of v r ! permutations that cover R. We focus on the expected number of members of U that are covered by a member of PR chosen uniformly at random; this is ec(R) = r! v! cov( , U ).
 PR U U

The strategy is to find a sequence of subsequences P0 , . . . , Pv , so that Pi is an i-subsequence, Pi+1 covers Pi for 0  i < v , symbol i is free in Pi but fixed in Pi+1 , 1 and ec(Pi+1 )  ec(Pi ) for 0  i < v . Because ec(P0 ) = t ! |U|, it follows that Pv is a 1 permutation that covers at least t! |U| of the as-yet-uncovered t-subsequences. Given a selection of Pi , there are precisely i + 1 candidates {C1 , . . . , Ci+1 } for Pi+1 obtained by placing symbol i in one of the i + 1 positions of Pi . Our task is to choose one for which ec(Cj )  ec(Pi ), in order to set Pi+1 = Cj . A naive computation of ec(Cj ) would enumerate members of U and of PCj , but the latter may have size exponential in v . Instead, for U  U let ec(U, R) = r!  PR cov( , U ), and observe that v! ec(R) =
U U

ec(U, R).

When t is fixed, U contains fewer than v t subsequences, which is polynomial in v . Therefore it suffices to compute ec(U, R) efficiently, given a t-subsequence U and an r-subsequence R. Let  be the number of symbols appearing in both U and R. When the  symbols in common do not appear in the same order in U and R, ec(U, R) = 0. Otherwise let T be the  -subsequence that they have in common. Then ! ec(U, R) = ec(U, T ) =  t! . i+1 1 The key observation in selecting Pi+1 is that ec(Pi ) = i+1 j =1 ec(Cj ). Computing ec(Cj ) for 1  j  i + 1, and selecting Pi+1 to be the one that maximizes ec(Cj ), we are then sure that ec(Pi+1 )  ec(Pi ). Combining all of these arguments, we have established the next theorem. Theorem 3.2. For fixed t and input v , there is an algorithm to construct a v! t! SeqCA(N ; t, v ) having N  1 + (log( (v- t)! ))/(log( t!-1 )) permutations in time that is polynomial in v . This algorithm can be easily implemented, and we report results from it in section 5. One immediate improvement results from observing that the counts of

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1851

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

as-yet-uncovered t-subsequences (R0 , R1 , . . . , RN ) must be integers. Hence we have -1 that Ri+1  Ri t!t ! . In specific cases this improves on the bound, without the need to construct the sequence covering array. If, however, the sequence covering array is explicitly constructed, at each selection of Pi+1 from Pi , we can choose Pi+1 to maximize ec() among the i + 1 candidates. 3.2. Greedy methods with reversals. The methods developed are greedy in that they attempt to cover the largest number of as-yet-uncovered t-subsequences. The very first permutation chosen is arbitrary; all are equally effective at coverage. Once one is selected, however, there is a genuine choice for the second. Greedy selection indicates that we should choose one that covers no t-subsequence already covered by the first. Indeed when t = 2, choosing the reversal of this first covers all remaining t-subsequences. For t  3, suppose that we have chosen 2s permutations 1 , . . . , 2s , and suppose further that 2i is the reverse of 2i-1 for 1  i  s. It follows that the number of as-yet-uncovered t-subsequences covered by a permutation  is precisely the same as the number covered by the reverse of  . Yet  and its reverse never cover the same t-subsequence. Hence if the algorithm were to select  next, the reverse of  remains an equally beneficial choice immediately thereafter. Therefore a useful variant of the algorithm developed, after adding a permutation  to the array, always adds the reverse of  as well. Pursuing this, we obtain the following. Theorem 3.3. For fixed t and input v , there is an algorithm to construct a v! t! SeqCA(N ; t, v ) having N  2(log( (v- t)! ))/(log( t!-2 )) permutations in time that is polynomial in v . Naturally, we could again obtain small improvements in practice because every count of as-yet-uncovered t-subsequences is an integer. In principle, always including reversals improves slightly on the bound (that is, Theorem 3.3 improves on Theorem 3.2). Whether this is a practical improvement remains to be seen; we return to this point. 4. Product constructions. Product (or "cut-and-paste" or "Roux-type") constructions are well studied for covering arrays; see, for example, [11, 6]. We develop a product construction for completely 3-scrambling sets of permutations. To do this, we first introduce an auxiliary property. A signing of a CSSP(N ; t, v ) A = (aij ) is an N × v matrix S = (sij ) with entries {, }. A CSSP(N ; t, v ) A is properly signed by an N × v matrix S with entries {, }. When for every set of t - 1 distinct columns c1 , . . . , ct-1 , each sign s  {, }, and every permutation  of 1, . . . , t - 1, there exists a row  of A for which, for every 1  a < b < t, the entry in cell (, c(a) ) is less than the entry in row (, c(b) ), and the sign s,c1 = s. A properly signed CSSP(7;3,5) is shown in Table 4.1. We defer for the moment the question of how to sign a completely t-scrambling set of permutations. 4.1. Products for strength three. Theorem 4.1. If a properly signed CSSP(N ; 3, v ) and a properly signed CSSP(M ; 3, w) both exist, so does a properly signed CSSP(N + M ; 3, vw). Proof. Let A = (aij ) be a CSSP(N ; 3, v ) having sign matrix S = (sij ) with columns indexed by {0, . . . , v - 1}. Let B = (bij ) be a CSSP(M ; 3, w) having sign matrix T = (tij ) with columns indexed by {0, . . . , w - 1}. Form an array C = (c,(i,j ) ) on N + M rows and vw columns with columns indexed by {0, . . . , v - 1} × {0, . . . , w - 1}. In row  for 1    N , in column (i, j ), place the entry ai w + j if si =, ai w + (w - 1 - j )

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1852

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Table 4.1 Properly signed CSSP(7; 3, 5) ­ t = 3, v = 5, N = 7. All signs not specified can be selected arbitrarily.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

0 1 2 2 2 4 4

4 2 0 1 3 1 3

2 3 4 0 0 2 2 (7;3,5)

3 0 3 3 4 3 0

1 4 1 4 1 0 1

if si =. In row N +  for 1    M , in column (i, j ), place the entry bj v + i if tj =, bj v + (v - 1 - i) if tj =. To show that C is a CSSP(N + M ; 3, vw), we must establish that every 3subsequence is covered. Consider three columns (i1 , j1 ), (i2 , j2 ), (i3 , j3 ), in this order. If i1 , i2 , and i3 are all distinct, there is a row  of A in which ai1 < ai2 < ai3 . Then in C row  has the three specified columns in the chosen order. By the same token, if j1 , j2 , and j3 are all distinct, there is a row  of B in which bj1 < bj2 < bj3 . Then in C row  + N has the three specified columns in the chosen order. If {i1 , i2 , i3 } contains only one element, {j1 , j2 , j3 } contains three distinct elements; symmetrically, if {j1 , j2 , j3 } contains only one element, {i1 , i2 , i3 } contains three distinct elements. So it remains only to treat cases in which both {i1 , i2 , i3 } and {j1 , j2 , j3 } contain two distinct elements. Now suppose that the three columns are {(i1 , j1 ), (i1 , j2 ), (i2 , j1 )}; we are concerned with the six orderings of the elements {c(i1 ,j1 ) , c(i1 ,j2 ) , c(i2 ,j1 ) }, which we represent by giving the indices in the sorted order for {c(i1 ,j1 ) , c(i1 ,j2 ) , c(i2 ,j1 ) }, as shown next. j2 j1 i1 c(i1 ,j1 ) c(i1 ,j2 ) 1 2 1 3 2 1 2 3 3 1 3 2 i2 c(i2 ,j1 ) 3 2 3 1 2 1 Table 4.2 gives the rows in which each of the six orderings is covered; we use sgn(x - y ) to be  if x < y ,  if x > y . Two of the orderings are covered at least twice. To sign C properly, assign  to each entry in each of the first N rows and  to each entry in each of the last M rows. A small example, combining two CSSP(6;3,4)s to form a CSSP(12;3,16), is shown in Table 4.3. Use the strategy in the proof of Theorem 4.1, taking B and T from   0 1  0 1     1  0  , 1 0

to establish the next theorem. Theorem 4.2. If a properly signed CSSP(N ; 3, v ) exists, so does a properly signed CSSP(N + 4; 3, 2v ).

4.2. Signing a completely t -scrambling set of permutations. We first give one technique for signing that applies for all strengths.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS Table 4.2 Verification for six orderings.

1853

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Row in C     +N +N +N +N

Condition on  ai1 < ai2 ai1 < ai2 ai1 > ai2 ai1 > ai2 bj1 < bj2 bj1 < bj2 bj1 > bj2 bj1 > bj2

Sign condition si1 = sgn(j2 - j1 ) si1 = sgn(j1 - j2 ) si1 = sgn(j2 - j1 ) si1 = sgn(j1 - j2 ) tj1 = sgn(i2 - i1 ) tj1 = sgn(i1 - i2 ) tj1 = sgn(i2 - i1 ) tj1 = sgn(i1 - i2 )

Ordering 1 3 2 3 2 1 3 1 1 2 2 1 2 3 3 2 2 1 3 2 3 3 1 1

Table 4.3 Example: Properly signed CSSP(6; 3, 4) and its product with itself, a CSSP(12; 3, 16). 3 4 4 12 4 15 3 4 4 12 4 15 2 5 5 13 5 14 4 3 15 4 12 4 1 6 6 14 6 13 8 15 3 0 8 8 0 7 7 15 7 12 15 8 8 8 0 3 4 3 15 4 12 4 2 5 5 13 5 14 5 2 14 5 13 5 5 2 14 5 13 5 6 1 13 6 14 6 9 14 2 1 9 9 7 0 12 7 15 7 14 9 9 9 1 2 8 15 3 0 8 8 1 6 6 14 6 13 9 14 2 1 9 9 6 1 13 6 14 6 10 13 1 2 10 10 10 13 1 2 10 10 11 12 0 3 11 11 13 10 10 10 2 1 15 8 8 8 0 3 0 7 7 15 7 12 14 9 9 9 1 2 7 0 12 7 15 7 13 10 10 10 2 1 11 12 0 3 11 11 12 11 11 11 3 0 12 11 11 11 3 0

0 1 1 3 1 3

1 0 3 1 3 1

2 3 0 0 2 2

3 2 2 2 0 0

Lemma 4.3. Whenever a CSSP(N ; t, v ) exists, a properly signed CSSP(N ; t, v - 1) exists. Proof. Let A = (aij ) be a CSSP(N ; t, v ). Form an N × (v - 1) array S = (sij ) with sij = sgn(aij - ai,v-1 ). Form an N × (v - 1) array B = (bij ) with bij = aij if aij < ai,v-1 and bij = aij - 1 otherwise. Then B is a CSSP(N ; t, v - 1) that is properly signed by S. Let A be a CSSP(N ; t, v ) and A1 , A2 be arrays that partition the rows of A. When, for i = 1, 2, Ai is a CSSP(Ni ; t - 1, v ), A is a partitionable CSSP. Lemma 4.4. Whenever a partitionable CSSP(N ; t, v ) exists, a properly signed CSSP(N ; t, v ) exists. Proof. Let A be a CSSP(N ; t, v ) with partition A1 , A2 . Assign sign  to every entry of A1 and  to every entry of A2 . Corollary 4.5. Whenever a CSSP(N ; 3, v ) contains a row and its reverse, it is partitionable and hence can be properly signed. Proof. Place the row and its reverse in A1 and all other rows in A2 . Then A1 is a CSSP(N ; 2, v ). Moreover, A2 is a CSSP(N ; 2, v ) because for every i, j  {0, . . . , v - 1} with i = j , in A there are at least three rows in which the entry in column i is less than that in column j . Then A is partitionable, so apply Lemma 4.4.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1854

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Lemma 4.4 provides a sufficient condition for a CSSP(N ; 3, v ) to have a proper signing but considers only signings in which all entries in each row receive the same sign. Column c is properly signed when, for every set of t - 1 distinct columns c1 , c2 , . . . , ct-1 with c1 = c, each sign s  {+, -}, and every permutation  of 1, . . . , t - 1, there exists a row  of A for which, for every 1  a < b < t, the entry in cell (, c(a) ) is less than the entry in row (, c(b) ), and the sign s,c = s. Properly signing the N × v array A is equivalent to properly signing each column of A; the important fact is that signs assigned in one column are unrelated to signs in any other, and so one can (hope to) proceed by signing each column separately. Consider the case of strength t = 3. What does it mean to properly sign a specific column c? For every column i other than c we form two sets: Ai contains the row indices in which the entry in column i is larger than that in column c, and Bi (= {1, . . . , N }\ Ai ) contains the row indices in which the entry in column i is smaller than that in column c. We can consider these sets as the edges of a hypergraph H on vertex set {1, . . . , N }. Then H has 2v - 2 edges each containing at least three vertices and a proper 2-coloring of H corresponds to a proper signing of c. Lemma 4.4 and Corollary 4.5 give proper 2-colorings. In all examples that we have examined, each column can be properly signed by finding a suitable 2-coloring. Hence it is plausible that every CSSP(N ; 3, v ) can be properly signed, but if this is true the proof is elusive at the moment. 5. Computational results. In [23], a simple greedy method is used to compute upper bounds on SeqCAN(t, v ) for t  {3, 4} and small values of v . These are reported in column K in Tables 5.1 and 5.2. Results from a more sophisticated greedy method by Erdem et al. [16] are reported in column ER. Using techniques from constraint satisfaction, in particular answer set programming, much more sophisticated search methods have been applied to strengths three and four [1, 2, 16]. Banbara, Tamura, and Inoue [1] implement an answer set programming method and show bounds for SeqCAN(3, v ) for v  80 and for SeqCAN(4, v ) for v  23. These bounds appear in column BTI in Tables 5.1 and 5.2. Results by Brain et al. [2] are reported in column BR in Tables 5.1 and 5.2. In [18], bounds for t  {3, 4, 5} and v  10 are reported from a method called the "bees algorithm"; these offer modest improvements on the greedy method in [23]. We do not report them for t  {3, 4} because they are not competitive with the results in [1]; we do report them for t = 5 in column BA, because they are the only published computational results. When t = 3, Tarui [36] establishes q/2 )  q for all q  4; these are reported in by direct construction that SeqCAN(3,  q/4 column TA. The bound U is obtained by computing the number Ui of as-yet-uncovered t-1 subsequences using Ui+1 =  t!t ! Ui  and terminating with the first value N for which UN = 0. (This does not explicitly construct the array but rather yields a number of rows for which it can surely be produced.) In the same manner, bound UR is obtained 1 by including reversals, so that Ui+2 = Ui - 2 t ! Ui  when i is even. The bound D is obtained by applying the algorithm that establishes Theorem 3.2 and that of DR by applying the algorithm that establishes Theorem 3.3. Table 5.1 reports results for t = 3. The theoretical results indicate that including reversals accelerates coverage, and so the bound UR improves on the bound U. Neither is competitive with greedy bounds from [23], given by K. In turn these are improved upon by the greedy method from [16], given by ER. Implementing our greedy approach nevertheless results in useful improvement to the two earlier greedy bounds, whether reversals are included or not. It comes as no surprise that the answer set programming

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS Table 5.1 Upper bounds on SeqCAN(3, v).

1855

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Events v 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 40 50 60 70 80 90

TA 8 8 8 10 10 10 10 12 12 12 12 12 12 12 12 12 12 14 14 14 14 14 14 14 14 14 14 16 16 16 16 18 18

U 12 17 20 23 26 28 30 32 33 35 36 37 39 40 41 42 42 43 44 45 46 46 47 48 48 49 49 54 58 61 64 66 68

UR 12 16 18 22 24 26 28 30 30 32 34 34 36 36 38 38 38 40 40 40 42 42 42 44 44 44 46 50 52 56 58 60 62

t=3 K ER 8 10 12 12 14 14 11 14 16 16 16 18 18 20 20 22 22 19 22 22 24 24 24 24 26 26 26 26 23 32 27 34 31 38 34 40 36 42 38 44

DR 8 10 10 12 12 12 14 14 14 16 16 16 16 18 18 18 18 18 20 20 20 20 20 20 20 22 22 24 26 26 28 30 30

D 6 8 8 9 10 11 12 12 13 13 14 14 15 15 16 16 16 17 17 17 17 18 18 18 18 19 19 21 23 24 25 26 27

BTI 7 8 8 8 9 9 10 10 10 10 10 11 11 12 12 12 12 13 14 14 14 14 14 14 15 15 17 19 21 22 24

BR 7 8 8 8 9 9 10 10 10 10 10 10 11 12 12 12 12 12 13 13 14 14 14 14 14 15 17 18 20 22 23

methods from [1, 2] (BTI, BR) yield consistent improvements on all the greedy methods for strength three. However, the direct construction of Tarui [36] provides better results at this time whenever v  30. Perhaps the most perplexing pattern is the regularity with which D yields a better bound than does DR . Remarkably, we consistently produce smaller sequence covering arrays when we do not automatically include reversals! The reasons for this are quite unclear at the present time. Table 5.2 gives results for strengths four and five. For strength four, our improvements on the method from [23] are more dramatic than for strength three. Surprisingly, the answer set programming technique from [1] obtains a better result than our greedy methods only when v  8. For 9  v  23, our greedy method yields much smaller arrays. (For v = 23, we employ 98 permutations as opposed to the 112 in BTI.) A similar comparison applies with the results from [2, 16] reported in column BR. Of course, we expect that given enough time, the answer set programming techniques would improve upon our greedy bounds. However, our methods require polynomial time in theory and are effective in practice for larger problems than those considered in [2, 1]; despite these "limitations," our methods appear to yield better results within the time available.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1856

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU Table 5.2 Upper bounds on SeqCAN(t, v) for t  {4, 5}.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Events v 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 40 50 60 70 80 90

U 24 54 79 98 114 128 140 151 160 169 177 184 191 197 203 209 214 219 224 228 232 236 240 244 248 251 255 283 305 322 337 350 361

UR 24 54 78 96 112 126 138 148 158 166 174 180 188 194 200 204 210 214 220 224 228 232 236 240 242 246 250 278 298 316 330 342 354

K 24 29 38 50 56 68 72 78 86 92 100 108 112 118 122 128 134 134 140 146 146 152 158 160 162 166 166 198 214 238 250 264 -

t=4 DR 24 24 32 40 44 50 56 60 64 70 74 78 80 84 86 90 92 96 98 98 102 104 106 108 110 112 114 132 146 154 166 174 180

D 24 26 34 41 47 52 57 61 66 71 73 78 81 84 86 91 92 95 97 99 101 104 105 107 110 111 113 128 141 151 160 168 176

BTI 24 24 38 44 52 58 65 69 77 81 84 89 91 97 100 105 104 111 112

BR

U 120 294 437 552 648 731 803 868 926 978 1027 1072 1113 1152 1189 1223 1256 1286 1316 1344 1370 1396 1420 1444 1466 1488 1671 1811 1924 2019 2101 2173

UR 120 294 436 550 646 728 800 864 922 976 1024 1068 1110 1148 1184 1218 1252 1282 1310 1338 1366 1390 1416 1438 1460 1482 1664 1804 1916 2012 2092 2164

t=5 DR 120 148 198 242 282 318 354 384 416 446 470 496 518 540 560 582 600 622 636 654 674 688 706 718 734 748

D 120 149 200 243 284 322 356 386 419 448 475 501 521 547 570 590 610 629 646 665 682 698 715 732 746 760

BA

55

159 212 271 329 383

104

149 181

A somewhat different pattern with respect to reversals is evident for strength four: The theoretical bound profits by including reversals throughout, but the implemented construction method appears first to benefit from reversals (for v  20) but later no longer benefit (for 40  v  90). Again the reasons for this are unclear. When v = 90, our methods track the coverage of 61,324,560 4-subsequences; thus, while the methods scale polynomially with v , the computations are nonetheless quite extensive. There is a CSSP(24;4,6) [28], but our methods do not yield fewer than 32 permutations. For strength five, none of the published methods in [1, 16, 23] report computational results, so it is difficult to make any comments about relative accuracy. However, the answer set programming methods do appear to require substantially more storage, which limits to a degree their effective range. To apply our methods would require tracking the coverage of 78,960,960 5-subsequences for v = 40; despite the efficiency of our methods, a straightforward implementation encounters both storage and time limitations. The method BA [18] is not competitive with our greedy methods. Within the range computed, including reversals improves our results. The pattern thereafter is unknown. Again, there is a CSSP(120;5,6) [25], but our methods do not yield fewer than 148 permutations.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1857

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

6. Using the product construction. For strength three, Theorem 4.1 provides substantial improvements on the computational results from the greedy methods. We properly signed a CSSP(6;3,4) in Table 4.3 and a CSSP(7;3,5) in Table 4.1. Table 6.1 shows proper signings for further arrays from [1]. We obtain CSSP(N ; 3, v ) for (v, N )  {(40, 15), (80, 17), (128, 18), (160, 19), (256, 20), (288, 21)} by using these in Theorem 4.1. These improve upon all the computational results! For example, while in [1] it is shown that SeqCAN(3, 80)  24 and in [2] that SeqCAN(3, 80)  23, here it is shown that SeqCAN(3, 80)  17. The examples given also provide better bounds than those of Tarui [36], but Theorem 4.1 does not outperform the direct construction asymptotically. 7. Constraints. In the testing application, it may happen that not every permutation of the events can in fact be executed; see [2, 23, 24]. It is therefore reasonable to ask how constraints on the execution order affect the number of permutations needed and how they affect the difficulty of finding a sequence covering array. We briefly consider the latter, in order to examine connections with further problems. Let  = {0, . . . , v - 1}. Let C be a set of subpermutations of , called constraints. A constrained sequence covering array SeqCA(N ; t, v, C ) is a set  = {1 , . . . , N } where i is a permutation of  that does not cover any subpermutation in C , and every t-subsequence of  that does not cover any subpermutation in C is covered by at least one of the permutations {1 , . . . , N }. Even in the easiest case, when t = 2 and all constraints are 2-subpermutations, the nature of the problem changes dramatically. Imposing the subpermutation constraint that b cannot precede a is the same as enforcing the precedence constraint that a precede b. When the precedence constraints contain a cycle, it is impossible to meet all constraints. This can be easily checked. When the constraints are acyclic, there is a permutation that covers no constraint. However, covering all 2-subpermutations not in C requires more. Let C r be the set of 2-subpermutations obtained by reversing each 2-subpermutation in C . Suppose that a SeqCA(N ; 2, v, C ) exists. Every permutation in the sequence covering array covers every 2-subpermutation in C r . Equivalently, treating C r as a partial order, every permutation gives a linear extension of the partial order. When (a, b)  C , (b, a) must be covered by every permutation in the sequence covering array. When {(a, b), (b, a)}  C = , some but not all permutations in the sequence covering array cover (a, b)--and the rest cover (b, a). Hence the set of 2-subpermutations covered by every permutation in the sequence covering array is exactly C r . This establishes a connection with the theory of partial orders. The dimension of a partial order is the smallest number of linear extensions whose intersection is the partial order [37, 38]. Our discussion establishes that a SeqCA(N ; 2, v, C ) exists if and only if the dimension of the partial order induced by C r is at most N . Hence we have the next lemma. Lemma 7.1. Deciding whether a SeqCA(N ; 2, v, C ) exists is NP-complete, even when C is an acyclic set of 2-subpermutations. Proof. Yannakakis [41] shows that determining whether a partial order has dimension at most 3 is NP-complete. Brain et al. [2] establish the NP-completeness of a related problem in which the subsequences to be covered, the constraints, and the permutations allowed are all specified. Lemma 7.1 is in stark contrast with the existence of sequence covering arrays of strength two without constraints. Nevertheless, the complexity arises in determining whether a small sequence covering array exists in these cases, not in

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1858

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU Table 6.1 Small properly signed CSSP(N ; 3, v)s. Signs not shown can be chosen arbitrarily.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

8 5 6 2 3 7 1 4 1 2 8 4 5 7 6 3 6 5 7 8 3 2 4 1 5 7 4 3 8 1 6 2 3 7 4 2 1 5 8 6 6 2 1 3 8 7 4 5 2 7 3 6 4 5 1 8 5 1 3 8 4 2 6 7 (8;3,8)

10 4 5 6 8 1 3 2 9 7 2 10 6 5 4 8 7 1 9 3 8 6 1 5 10 3 9 7 4 2 5 3 9 10 1 8 4 6 7 2 2 5 7 4 8 9 3 10 1 6 6 8 1 10 7 5 2 4 3 9 9 10 8 3 5 2 4 7 1 6 5 1 7 3 2 4 9 8 6 10 3 5 1 2 9 7 10 4 6 8 (9;3,10)

2 14 15 5 8 3 1 11 9 7 13 12 16 6 4 10 14 8 3 6 7 11 1 16 12 13 2 10 15 5 9 4 12 16 4 6 3 5 7 2 1 9 15 11 8 14 10 13 12 2 13 4 3 14 9 6 11 1 5 15 10 7 16 8 16 4 10 5 12 9 11 14 1 8 13 2 7 15 6 3 6 4 12 15 3 1 11 13 8 14 10 5 2 9 16 7 2 15 12 11 6 8 13 5 14 7 3 4 9 16 10 1 5 9 7 14 16 13 6 10 12 1 11 2 8 3 4 15 5 6 1 15 12 16 10 2 3 13 4 11 9 8 7 14 11 7 8 4 15 5 16 6 14 12 9 13 1 2 3 10 (10;3,16) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 4 14 11 10 6 9 13 7 17 1 18 3 2 12 16 5 8 15 4 17 3 2 18 16 14 7 9 13 11 12 10 8 1 6 15 5 6 13 7 8 12 10 14 4 16 1 2 18 17 15 9 11 3 5 7 18 6 16 4 5 1 17 13 11 10 12 14 15 8 2 9 3 8 3 15 4 9 17 2 16 11 12 14 13 10 7 6 1 5 18 8 11 3 14 6 15 13 5 2 16 17 1 18 7 10 9 4 12 10 7 16 4 5 9 13 15 1 18 3 17 2 12 14 8 6 11 13 9 16 14 15 11 7 12 5 4 2 1 3 10 6 18 17 8 18 4 15 17 16 2 8 1 11 13 9 12 10 3 6 14 7 5 18 7 2 5 4 3 17 10 15 11 14 12 13 1 16 9 8 6 (11;3,18) 19 3 5 2 6 18 9 10 14 21 20 1 15 4 12 22 16 7 13 8 11 23 17 2 22 13 21 7 4 1 6 18 23 15 12 8 10 19 16 17 11 20 3 5 14 9 1 21 6 7 4 20 16 18 10 23 3 13 17 9 11 14 2 19 8 22 12 15 5 10 12 5 15 8 13 23 17 22 11 18 14 9 3 4 1 16 2 20 21 19 6 7 3 17 22 4 14 2 7 13 1 10 9 16 11 12 23 8 20 19 18 15 21 5 6 10 1 19 15 3 14 23 4 12 11 8 18 2 21 16 6 17 20 7 5 9 13 22 20 13 3 17 18 7 14 10 22 9 1 16 11 21 15 8 4 6 2 5 23 19 12 8 18 12 3 22 11 14 1 15 6 21 5 10 19 9 13 4 2 20 17 16 7 23 21 2 12 15 22 23 3 19 8 5 16 17 1 20 6 18 13 7 9 11 14 4 10 18 10 9 14 7 17 8 6 5 15 16 13 23 11 4 12 20 21 1 22 3 2 19 16 10 14 9 21 12 7 23 13 2 4 19 20 1 22 3 8 17 18 6 5 15 11 15 16 23 21 12 3 19 17 4 9 13 1 18 14 5 22 7 11 10 8 6 20 2 (12;3,23)

determining whether a sequence covering array exists. The situation is worse when constraints have strength three. Consider a collection T of ordered triples of distinct elements of , and associate with (a, b, c) the constraints {(b, a, c), (b, c, a), (a, c, b), (c, a, b)}. Meeting these constraints requires that b lie between a and c, and a collection of constraints of this type forms an instance of the betweenness problem [5] in which one is required to

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1859

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

order all items so that for every triple (a, b, c)  T , b lies between a and c. Forming C = {(b, a, c), (b, c, a), (a, c, b), (c, a, b) : (a, b, c)  T }, even finding a single permutation that covers no 3-subpermutation in C appears hard: Lemma 7.2 (see [30]). Determining whether an instance of the betweenness problem has a solution is NP-complete. These complexity results suggest that constraints pose severe additional challenges in the construction of sequence covering arrays. Checking feasibility can become difficult; even when feasibility is easily checked, the minimization problem is substantially more complicated. 8. Conclusions. The close connection between sequence covering arrays and covering arrays has proved useful in establishing bounds on the sizes of sequence covering arrays. The efficient conditional expectation algorithm for generating sequence covering arrays and the product construction for strength three parallel analogous results for covering arrays. Unfortunately, while sequence covering arrays lead to covering arrays with excess coverage, additional conditions on such a covering array would be required in order to recover a sequence covering array. Hence the parallels between the extensive literature on covering arrays and the existence problem for sequence covering arrays are primarily by analogy. We have examined numerous formulations for sequence covering arrays. In closing, we indicate one more (see also [27]). A perfect hash family PHF(N ; k, w, t) is an N × k array on w symbols in which in every N × t subarray, at least one row consists of distinct symbols. Mehlhorn [29] introduced perfect hash families as an efficient tool for compact storage and fast retrieval of frequently used information; see also [14]. Stinson et al. [35] establish that perfect hash families can be used to construct separating systems, key distribution patterns, group testing algorithms, cover-free families, and secure frameproof codes. They are also used extensively in product constructions for covering arrays [8, 12, 13]. Completely t-scrambling sets of permutations can be viewed as an ordered analogue of perfect hash families in which k = w, no element appears twice in a row, and for every way to select t distinct columns in order there is a row in which the elements in these columns are in increasing order. In particular, a completely t-scrambling set of permutations provides a perfect hash family in which, for every set of t columns, there are at least t! rows containing distinct symbols in the chosen columns, and at least one for each of the t! symbol orderings. For this reason, it appears reasonable to expect that constructions for perfect hash families may also prove to be useful for sequence covering arrays. Acknowledgments. Thanks to two anonymous referees for pointing out relevant references. Special thanks to Mutsunori Banbara and Johannes Oetsch for providing explicit solutions for small sequence covering arrays with t = 3.
REFERENCES [1] M. Banbara, N. Tamura, and K. Inoue, Generating event-sequence test cases by answer set programming with the incidence matrix, in Technical Communications of the 28th International Conference on Logic Programming (ICLP12), 2012, pp. 86­97. ¨ hrer, H. Tompits, and C. Yilmaz, Event[2] M. Brain, E. Erdem, K. Inoue, J. Oetsch, J. Pu sequence testing using answer-set programming, Internat. J. Advances Software, 5 (2012), pp. 237­251. [3] R. C. Bryce and C. J. Colbourn, The density algorithm for pairwise interaction testing, Software Testing, Verification, and Reliability, 17 (2007), pp. 159­182.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

1860

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

[4] R. C. Bryce and C. J. Colbourn, A density-based greedy algorithm for higher strength covering arrays, Software Testing Verification Reliability, 19 (2009), pp. 37­53. [5] B. Chor and M. Sudan, A geometric approach to betweenness, SIAM J. Discrete Math., 11 (1998), pp. 511­523. [6] M. B. Cohen, C. J. Colbourn, and A. C. H. Ling, Constructing strength three covering arrays with augmented annealing, Discrete Math., 308 (2008), pp. 2709­2722. [7] C. J. Colbourn, Constructing perfect hash families using a greedy algorithm, in Coding and Cryptology, Y. Li, S. Zhang, S. Ling, H. Wang, C. Xing, and H. Niederreiter, eds., World Scientific, Singapore, 2008, pp. 109­118. [8] C. J. Colbourn, Covering arrays and hash families, in Information Security and Related Combinatorics, NATO Peace and Information Security, IOS Press, Amsterdam, 2011, pp. 99­136. [9] C. J. Colbourn, Efficient conditional expectation algorithms for constructing hash families, in Combinatorial Algorithms, Lecture Notes in Comput. Sci., 7056, Springer-Verlag, Berlin, 2011, pp. 144­155. [10] C. J. Colbourn, D. Horsley, and V. R. Syrotiuk, Strengthening hash families and compressive sensing, J. Discrete Algorithms, 16 (2012), pp. 170­186. [11] C. J. Colbourn, S. S. Martirosyan, Tran Van Trung, and R. A. Walker II, Roux-type constructions for covering arrays of strengths three and four, Des. Codes Cryptogr., 41 (2006), pp. 33­57. [12] C. J. Colbourn and J. Torres-Jim´ enez, Heterogeneous hash families and covering arrays, Contemp. Math., 523 (2010), pp. 3­15. [13] C. J. Colbourn and J. Zhou, Improving two recursive constructions for covering arrays, J. Statist. Theory Practice, 6 (2012), pp. 30­47. [14] Z. J. Czech, G. Havas, and B. S. Majewski, Perfect hashing, Theoret. Comput. Sci., 182 (1997), pp. 1­143. [15] B. Dushnik, Concerning a certain set of arrangements, Proc. Amer. Math. Soc., 1 (1950), pp. 788­796. ¨ hrer, H. Tompits, and C. Yilmaz, Answer-set pro[16] E. Erdem, K. Inoue, J. Oetsch, J. Pu gramming as a new approach to event-sequence testing, in Proceedings of the 2nd International Conference on Advances in System Testing and Validation Lifecycle, Xpert Publishing Services, 2011, pp. 25­34. ¨ redi, Scrambling permutations and entropy of hypergraphs, Random Structures [17] Z. Fu Algorithms, 8 (1996), pp. 97­104. [18] M. M. Z. Hazli, K. Z. Zamli, and R. R. Othman, Sequence-based interaction testing implementation using bees algorithm, in Proceedings of the IEEE Symposium on Computers and Informatics, 2012, pp. 81­85. [19] S. Huang, M. B. Cohen, and A. M. Memon, Repairing GUI test suites using a genetic algorithm, in Proceedings of the 3rd International Conference on Software Testing, Verification and Validation (ICST), 2010, pp. 245­254. [20] Y. Ishigami, Containment problems in high-dimensional spaces, Graphs Combin., 11 (1995), pp. 327­335. [21] Y. Ishigami, An extremal problem of d permutations containing every permutation of every t elements, Discrete Math., 159 (1996), pp. 279­283. [22] D. S. Johnson, Approximation algorithms for combinatorial problems, J. Comput. System Sci., 9 (1974), pp. 256­278. [23] D. R. Kuhn, J. M. Higdon, J. F. Lawrence, R. N. Kacker, and Y. Lei, Combinatorial methods for event sequence testing, in Proceedings of the IEEE 5th International Conference on Software Testing, Verification and Validation (ICST), 2012, pp. 601­609. [24] D. R. Kuhn, J. M. Higdon, J. F. Lawrence, R. N. Kacker, and Y. Lei, Combinatorial methods for event sequence testing, CrossTalk J. Defense Software Engineering, 25 (2012), pp. 15­18. [25] V. I. Levenshtein, Perfect codes in the metric of deletions and insertions, Diskret. Mat., 3 (1991), pp. 3­20. ´ sz, On the ratio of optimal integral and fractional covers, Discrete Math., 13 (1975), [26] L. Lova pp. 383­390. [27] O. Margalit, Better bounds for event sequence testing, in Proceedings of the 2nd International Workshop on Combinatorial Testing, 2013. [28] R. Mathon and Tran Van Trung, Directed t-packings and directed t-Steiner systems, Des. Codes Cryptogr., 18 (1999), pp. 187­198. [29] K. Mehlhorn, Data Structures and Algorithms 1: Sorting and Searching, Springer-Verlag, Berlin, 1984.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

SEQUENCE COVERING ARRAYS

1861

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

´ , Total ordering problem, SIAM J. Comput., 8 (1979), pp. 111­114. [30] J. Opatrny [31] J. Radhakrishnan, A note on scrambling permutations, Random Structures Algorithms, 22 (2003), pp. 435­439. [32] J. Spencer, Minimal scrambling sets of simple orders, Acta Math. Acad. Sci. Hungar., 22 (1971/72), pp. 349­353. [33] R. P. Stanley, Increasing and decreasing subsequences and their variants, in Proceedings of the International Congress of Mathematicians, vol. I, Madrid, 2007, pp. 545­579. [34] S. K. Stein, Two combinatorial covering theorems, J. Combin. Theory Ser. A, 16 (1974), pp. 391­397. [35] D. R. Stinson, Tran Van Trung, and R. Wei, Secure frameproof codes, key distribution patterns, group testing algorithms and related structures, J. Statist. Plann. Inference, 86 (2000), pp. 595­617. [36] J. Tarui, On the minimum number of completely 3-scrambling permutations, Discrete Math., 308 (2008), pp. 1350­1354. [37] W. T. Trotter, Jr., Some combinatorial problems for permutations, in Proceedings of the 8th Southeastern Conference on Combinatorics, Graph Theory and Computing, Baton Rouge, La., 1977, Utilitas Mathematica, Winnipeg, pp. 619­632. [38] W. T. Trotter, Jr., Combinatorics and partially ordered sets, in Dimension Theory, Johns Hopkins Ser. Math. Sci., Johns Hopkins University Press, Baltimore, MD, 1992. [39] W. Wang, Y. Lei, S. Sampath, R. Kacker, D. Kuhn, and J. Lawrence, A combinatorial approach to building navigation graphs for dynamic web applications, in Proceedings of the 25th International Conference on Software Maintenance, 2009, pp. 211­220. [40] W. Wang, S. Sampath, Y. Lei, and R. Kacker, An interaction-based test sequence generation approach for testing web applications, in 11th IEEE High Assurance Systems Engineering Symposium, 2008, pp. 209­218. [41] M. Yannakakis, The complexity of the partial order dimension problem, SIAM J. Algebraic Discrete Methods, 3 (1982), pp. 351­358. [42] X. Yuan, M. B. Cohen, and A. M. Memon, Towards dynamic adaptive automated test generation for graphical user interfaces, in International Conference on Software Testing, Verification and Validation Workshops, 2009, pp. 263­266. [43] X. Yuan, M. B. Cohen, and A. M. Memon, GUI interaction testing: Incorporating event context, IEEE Trans. Software Engrg., 37 (2011), pp. 559­574. [44] X. Yuan and A. M. Memon, Generating event sequence-based test cases using GUI runtime state feedback, IEEE Trans. Software Engrg., 36 (2010), pp. 81­95.

Copyright © by SIAM. Unauthorized reproduction of this article is prohibited.

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

575

A Discrete Particle Swarm Optimization for
Covering Array Generation
Huayao Wu, Changhai Nie, Member, IEEE, Fei-Ching Kuo, Member, IEEE,
Hareton Leung, Member, IEEE, and Charles J. Colbourn

Abstractâ€”Software behavior depends on many factors.
Combinatorial testing (CT) aims to generate small sets of test
cases to uncover defects caused by those factors and their
interactions. Covering array generation, a discrete optimization
problem, is the most popular research area in the field of CT.
Particle swarm optimization (PSO), an evolutionary search-based
heuristic technique, has succeeded in generating covering arrays
that are competitive in size. However, current PSO methods for
covering array generation simply round the particleâ€™s position
to an integer to handle the discrete search space. Moreover, no
guidelines are available to effectively set PSOs parameters for
this problem. In this paper, we extend the set-based PSO, an
existing discrete PSO (DPSO) method, to covering array generation. Two auxiliary strategies (particle reinitialization and
additional evaluation of gbest) are proposed to improve performance, and thus a novel DPSO for covering array generation
is developed. Guidelines for parameter settings both for conventional PSO (CPSO) and for DPSO are developed systematically
here. Discrete extensions of four existing PSO variants are developed, in order to further investigate the effectiveness of DPSO
for covering array generation. Experiments show that CPSO can
produce better results using the guidelines for parameter settings,
and that DPSO can generate smaller covering arrays than CPSO
and other existing evolutionary algorithms. DPSO is a promising
improvement on PSO for covering array generation.
Index Termsâ€”Combinatorial testing (CT), covering array
generation, particle swarm optimization (PSO).

I. I NTRODUCTION
S SOFTWARE functions and run-time environments
become more complex, testing of modern software systems is becoming more expensive. Effective detection of

A

Manuscript received May 12, 2013; revised December 29, 2013 and
May 18, 2014; accepted September 28, 2014. Date of publication October 9,
2014; date of current version July 28, 2015. This work was supported in part
by the National Natural Science Foundation of China under Grant 61272079,
in part by the Research Fund for the Doctoral Program of Higher Education
of China under Grant 20130091110032, in part by the Science Fund for
Creative Research Groups of the National Natural Science Foundation of
China under Grant 61321491, in part by the Major Program of National
Natural Science Foundation of China under Grant 91318301, and in part
by the Australian Research Council Linkage under Grant LP100200208.
(Corresponding author: Changhai Nie.)
H. Wu and C. Nie are with the State Key Laboratory for Novel
Software Technology, Nanjing University, Nanjing 210023, China (e-mail:
hywu@outlook.com; changhainie@nju.edu.cn).
F.-C. Kuo is with the Faculty of Information and Communication
Technologies, Swinburne University of Technology, Hawthorn, VIC 3122,
Australia (e-mail: dkuo@swin.edu.au).
H. Leung is with the Department of Computing, Hong Kong Polytechnic
University, Hong Kong (e-mail: hareton.leung@polyu.edu.hk).
C. J. Colbourn is with Arizona State University, Tempe, AZ 85287-8809,
USA (e-mail: colbourn@asu.edu).
Digital Object Identifier 10.1109/TEVC.2014.2362532

failures at a low cost is a key issue for test case generation. Combinatorial testing (CT) is a popular testing method
to detect failures triggered by various factors and their interactions [1]. By employing covering arrays as test suites, the CT
method aims to sample the large combination space with few
test cases to cover different interactions among a fixed number of factors. Kuhn and Reilly [2] shows that more than 70%
of the failures in certain software were caused by the interactions of one or two factors, and almost all the failures could
be detected by checking the interactions among six factors.
Therefore, CT can be an effective method in practice.
Generating a covering array with fewest tests (minimum
size) is a major challenge in CT. In general, the minimum
size of a covering array is unknown; hence, methods have
focused on finding covering arrays that have as few tests as
possible at reasonable search cost. The many methods that
have been proposed can be classified into two main groups:
1) mathematical methods and 2) computational methods [1].
Mathematical (algebraic or combinatorial) methods typically
exploit some known combinatorial structure. Computational
methods primarily use greedy strategies or heuristic-search
techniques to generate covering arrays, due to the size of the
search space.
Mathematical methods yield the best possible covering
arrays in certain cases. For example, orthogonal arrays used
in the design of experiments provide covering arrays with a
number of tests that is provably minimum. However, all known
mathematical methods can be applied only for restrictive sets
of factors. This limitation has led to an emphasis on computational methods. Greedy algorithms have been quite effective
in generating covering arrays, but their accuracy suffers from
becoming trapped in local optima.
In recent years, search-based software engineering (SBSE)
has focused on using search-based optimization algorithms
to find high-quality solutions for software engineering problems. Inspired by SBSE, many artificial intelligence-based
heuristic-search techniques have been applied to software
testing. For example, simulated annealing (SA) [3]â€“[7],
genetic algorithm (GA) [8]â€“[10], and ant colony
optimization (ACO) [9], [11], [12] have all been applied
to covering array generation. These techniques can generate
any types of covering arrays, and the constraint solving
and prioritization techniques can be easily integrated. Their
applications have been shown to be effective, producing
relatively small covering arrays in many cases. Particle swarm
optimization (PSO), a relatively new evolutionary algorithm,

c 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
1089-778X 
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

576

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

has also been used in this area [13]â€“[16]. It is easy to
implement and has fast initial progress.
The conventional PSO (CPSO) algorithm was originally
designed to find optimal or near optimal solutions in a
continuous space. Nevertheless, many discrete PSO (DPSO)
algorithms and frameworks have been developed to solve
discrete problems [17]â€“[22]. For covering array generation,
current discrete methods [13]â€“[16] simply round the particleâ€™s
position to an integer while keeping the velocity as a real
number. They suffer from two main shortcomings. First, the
performance of PSO is significantly impacted by its parameter
settings. In [23], effects of the general parameter selection and
initial population of PSO have been analyzed, but no guidelines on parameter settings have been reported for covering
array generation. Hence, a clear understanding of how to set
these execution parameters is needed. Second, simple rounding
fractional positions to integers introduces a substantial source
of errors in the search. Instead, a specialized DPSO version is
needed. Because current PSO methods show promise for generating covering arrays, these two main shortcomings should
be addressed.
In this paper, we adapt set-based PSO (S-PSO) [18] to
generate covering arrays. S-PSO utilizes set and probability
theories to develop a new representation scheme for combinatorial optimization problems as well as refining the related
evolution operators. In our adaptation of S-PSO, two auxiliary strategies are proposed to enhance performance, and
a novel DPSO algorithm is thus proposed. DPSO has the
same conceptual basis and exhibits similar search behavior to
CPSO, with parameters playing similar roles. Then, we explore
the optimal parameter settings for both CPSO and DPSO to
improve their performance, and identify recommended settings. Furthermore, because many CPSO variants [24]â€“[27]
can be easily extended to discrete versions based on our DPSO,
the performance of these discrete versions is also compared
with their original ones. Finally, we compare CPSO and DPSO
with existing GA and ACO [9], [11] algorithms to generate
covering arrays.
The main contributions of this paper are as follows.
1) Based on the set-based representation, we design a
version of S-PSO [18] for covering array generation.
2) We propose two auxiliary strategies, particle reinitialization, and additional evaluation of gbest, to enhance the
performance of PSO. A novel DPSO for covering array
generation is proposed.
3) We design experiments to explore the optimal parameter settings for CPSO and DPSO for covering array
generation.
4) We implement original and discrete versions of four
representative PSO variants (TVAC [24], CLPSO [25],
APSO [26], and DMS-PSO [27]) to compare their
efficacy for covering array generation.
The rest of this paper is organized as follows. Section II
gives background on CT, covering array generation, and
the CPSO algorithm. Section III summarizes related work.
Section IV presents our DPSO algorithm, including the
representation scheme, related operators, and two auxiliary
strategies. Section V evaluates the performance of CPSO and

TABLE I
E-C OMMERCE S OFTWARE S YSTEM

DPSO, and explores optimal parameter settings. Section VI
gives a comparison among CPSO, DPSO, and original and
discrete variants. Section VII compares CPSO and DPSO
with GA and ACO. Section VIII concludes this paper and
outlines future work.
II. BACKGROUND
A. CT
Suppose that the behavior of the software under test (SUT)
is controlled by n independent factors, which may represent configuration parameters, internal or external events, user
inputs, and the like. The ith factor has i discrete levels, chosen from the finite set Vi of size i . An n-tuple (x1 , x2 , . . . , xn )
forms a test case, where xi âˆˆ Vi for 1 â‰¤ i â‰¤ n.
Consider a simple e-commerce software system [15]. This
system consists of five different components. Each of these five
components can be regarded as a factor, and its configurations
can be regarded as different levels. Table I shows these five
factors and their corresponding levels. In this example, n = 5,
1 = 2 = 3 = 2, 4 = 5 = 3.
System failures are often triggered by interactions among
some factors, which can be represented by the combinations
of factor levels. In order to detect these failures, their combinations should be covered at least once by the test suite. A
t-way schema can be used to represent them.
Definition 1 (t-way schema): The n-tuple (âˆ’, y1 , . . . ,
yt , . . . ) is a t-way schema (t > 0) when some t factors
have fixed levels and the others can take any valid levels,
represented as â€œâˆ’.â€
For example, suppose that when factor Payment Server
takes the level Master and factor Web Server takes the level
Apache, a system failure occurs. To detect this failure, the
2-way schema (Master, â€“, Apache, â€“, â€“) must be covered
at least once by the test suite. To simplify later discussion,
we use the index in the level set of each factor to present
a schema. For example, (0, â€“, 1, â€“, â€“) is used to represent
(Master, â€“, Apache, â€“, â€“).
Exhaustive testing covers all n-way schemas of the system; in our example, it uses 2 Ã— 2 Ã— 2 Ã— 3 Ã— 3 = 72 test
cases. Testing cost becomes prohibitive as the numbers of factors and levels increase. Moreover, because only interactions
among few factors are likely to trigger failures [2], testing
high-way schemas can lead to many uninformative test cases.
At the other extreme, if we only guarantee to cover each 1-way
schema once, only three test cases are needed (a single test
case can cover five 1-way schemas at most). But we may
fail to detect some interaction triggered failures involving two
factors.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

TABLE II
C OVERING A RRAY CA(9; 2, 23 32 )

Instead, CT covers all t-way schemas. Such a test suite is a
t-way covering array, with t being the covering strength. The
value of t determines the depth of coverage. It is a key setting
of CT, and should be decided by the testers. We give a precise
definition.
Definition 2 (Covering Array): If an NÃ—n array, where N is
the number of test cases, has the following properties: 1) each
column i (1 â‰¤ i â‰¤ n) contains only elements from the set Vi
with i = |Vi | and 2) the rows of each N Ã— t sub array cover
all |Vk1 | Ã— |Vk2 | Ã— . . . Ã— |Vkt | combinations of the t columns at
least once, where t â‰¤ n and 1 â‰¤ k1 < . . . < kt â‰¤ n, then it is a
t-way covering array, denoted by CA(N; t, n, (1 , 2 , . . . , n )).
When 1 = 2 = . . . = n = , it is denoted by CA(N; t, n, ).
Reference [2] demonstrated that more than 70% failures can
be detected by a 2-way covering array, and almost all failures
can be detected by a 6-way covering array. Hence, using CT,
we can detect many failures of the system by applying relatively low strength covering arrays. In other words, CT can
greatly reduce the size of test suite while maintaining high
fault detection ability.
In the example of Table I, if we only consider the interactions between any two factors, only nine test cases are
required to construct a 2-way covering array instead of 72
for exhaustive testing. Table II shows a covering array, where
each row represents one test case. This table covers all possible 2-way schemas for any two factors corresponding to the
columns. For example, consider factor Payment Server and
User Browser, all 2Ã—3 = 6 schemas, (Master, â€“, â€“, Firefox, â€“)
(Master, â€“, â€“, Explorer, â€“), (Master, â€“, â€“, Chrome, â€“),
(VISA, â€“, â€“, Firefox, â€“), (VISA, â€“, â€“, Explorer, â€“),
(VISA, â€“, â€“, Chrome, â€“), can be found in the table.
For convenience, if several groups of gi factors (gi < n) have
g
the same number of levels ak , ak i can be used to represent
these factors and their levels. Thus, the coveringarray can
g
g
g
gi = n,
be denoted by CA(N; t, a11 , a22 , . . . , ak k ) where
n
or CA(N; t, a ) when g1 = n and a1 = a. For example, the
covering array in Table II is a CA(9; 2, 23 32 ).
In many software systems, the impacts of the interactions
among factors are not uniform. Some interactions may be more
prone to trigger system failures, while other may have little or no impact on the system. To effectively detect these
different interactions, variable strength (VS) covering arrays
can be applied. This can offer different covering strengths

577

TABLE III
A DDING T HREE T EST C ASES TO C ONSTRUCT
VCA(12; 2, 23 32 , CA(3, 22 31 ))

to different groups of factors, and can therefore provide a
practical approach to test real applications.
Definition 3 (VS Covering Array): A VS covering array,
m

1
denoted by VCA(N; t, a11 . . . akk , CA1 (t1 , bm
. . . bp p ), . . . ,
1
n
CAj (tj , cn11 . . . cqq )), is an N Ã— n covering array of covering
strength t containing one or more sub covering arrays, namely
CA1 , . . . , CAj , each of which has covering strength t1 , . . . , tj
all larger than t.
Consider the e-commerce system shown in Table I. If the
interactions of three factors, Payment Server, Web Server, and
Business Database, have a higher probability to trigger system failures, then a VS covering array can be constructed.
As in Table II, only three more test cases (Table III) are
needed to construct the VCA(12; 2, 23 32 , CA(3, 22 31 )). With
these 12 test cases, not only are all 2-way schemas of all five
factors covered, but also all 3-way schemas of these three factors (Payment Server, Web Server, and Business Database) are
covered.
B. Covering Array Generation
Covering arrays are used as test suites in CT. Covering array
generation is the process of test suite construction. It is the
most active area in CT with more than 50% of research papers
focusing on this field [1]. Due to limited testing resources, all
aim to construct a minimal covering array while still maintaining full coverage of combinations. Computational methods
have been used widely for covering array generation because
they can be applied to any systems. In general, these methods
generate all possible combinations first. Then they generate
test cases to cover these combinations one-by-one. One-testat-a-time is the most widely used strategy among evolutionary
algorithms to generate a covering array.
The one-test-at-a-time strategy was popularized by
AETG [28] and was further used by Bryce and Colbourn [29].
This strategy takes the model of SUT(n, 1 , . . . , n ) where
n is the number of factors and i is the number of valid
levels of factor i, and the covering strength t as input. At
first, an empty test suite TS and a set S of t-way schemas
to be covered are initialized. In each iteration, a test case is
generated with the highest fitness value according to some
heuristic techniques. Then it is added to TS and the t-way
schemas covered by it are removed. When all the t-way
schemas have been covered, the final test suite TS is returned.
This process is shown in Algorithm 1.
In this strategy, a fitness function must be used to evaluate
the quality of a candidate test case (line 6 in Algorithm 1). It is
an important part of all heuristic techniques. In covering array
generation, the fitness function takes the test case as the input
and then outputs a fitness value representing its â€œgoodness.â€
It is defined as follows.

578

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

Algorithm 1 One-Test-at-a-Time Strategy
1: Input: SUT(n, 1 , . . . , n ) and covering strength t
2: Output: covering array TS
3: TS = âˆ…
4: Construct S (all the t-way schemas to be covered) based
on n, 1 , . . . , n and t
5: while S  = âˆ… do
6:
Generate a test case p with the highest fitness value
according to some heuristics
7:
Add p to the test suite TS
8:
Remove the t-way schemas covered by p from S
9: end while
10: return TS

Definition 4 (Fitness Function): Let TS be the generated
test set, and p be a test case. Then fitness(p) is the number of
uncovered t-way schemas in TS that are covered by p.
The fitness function can be formulated as
fitness(p) = |schemat ({p}) âˆ’ schemat (TS)|

(1)

where schemat (TS) represents the set of all t-way schemas
covered by test set TS, and | Â· | stands for cardinality. When
all Cnt t-way schemas covered by p are not covered by TS,
the fitness function reaches the maximum value fitness(p) =
|schemat ({p})| = Cnt .
For example, consider the 2-way covering array generation
of the e-commerce system shown in Table II. Suppose that
TS consists of test cases (0, 0, 0, 0, 0) and (0, 0, 1, 1, 1).
The fitness function computes the fitness value for a candidate test case p = (1, 0, 1, 2, 2) as follows: because p covers
ten 2-way schemas, namely schema2 ({p}) = {(1, 0, â€“, â€“, â€“),
(1, â€“, 1, â€“, â€“,), (1, â€“, â€“, 2, â€“), (1, â€“, â€“, â€“, 2), (â€“, 0, 1, â€“, â€“),
(â€“, 0, â€“, 2, â€“), (â€“, 0, â€“, â€“, 2), (â€“, â€“, 1, 2, â€“), (â€“, â€“, 1, â€“, 2),
(â€“, â€“, â€“, 2, 2)} and TS only covers (â€“, 0, 1, â€“, â€“) in p, the
function returns fitness(p) = 9.
C. PSO
PSO is a swarm-based evolutionary computation technique.
It was developed by Kennedy et al. [30], inspired by the social
behavior of bird flocking and fish schooling. PSO utilizes a
population of particles as a set of candidate solutions. Each
of the particles represents a certain position in the problem
hyperspace with a given velocity. A fitness function is used
to evaluate the quality of each particle. Initially, particles are
distributed in the hyperspace uniformly. Then each particle
repeatedly updates its state according to the individual best
position in its history (pbest) and the current global best position (gbest). Eventually, each particle possibly moves toward
the direction of the individual optimum and global optimum,
and finds an optimal or near optimal solution.
Suppose that the problem domain is a Dâˆ’dimensional
hyperspace. Then the position and velocity of particle i can
be represented by xi âˆˆ RD and vi âˆˆ RD respectively. CPSO
uses the following equations to update a particleâ€™s velocity
and position, where vi,j (k) represents the jth component of the
velocity of particle i at the kth iteration, and xi,j (k) represents

its corresponding position:
vi,j (k + 1) = Ï‰ Ã— vi,j (k) + c1 Ã— r1,j Ã— (pbesti,j âˆ’ xi,j (k))
(2)
+ c2 Ã— r2,j Ã— (gbestj âˆ’ xi,j (k))
xi,j (k + 1) = xi,j (k) + vi,j (k + 1).

(3)

The best position of particle i in its history is pbesti , and
gbest is the best position among all particles. The velocityupdate equation (2) captures the three basic concepts of PSO.
The first is inertia, the tendency of the particle to continue in
the direction it has been moving. The second is memory of
the best position ever found by itself. The third is cooperation
using the best position found by other particles.
The parameter Ï‰ is inertia weight. It controls the balance between exploration (global search state) and exploitation
(local search state). Two positive real numbers c1 and c2
are acceleration constants that control the movement tendency toward the individual and global best position. Most
studies set Ï‰ = 0.9, and c1 = c2 = 2 to get the best
balance [17], [31], [32]. In addition, r1,j and r2,j are two uniformly distributed random numbers in the range of [0.0, 1.0],
used to ensure the diversity of the population.
If the problem domain (the search space of particles) has
bounds, a bound handling strategy is adopted to keep the particles inside the search space. Many different strategies have
been proposed [33]. In the reflecting strategy, when a particle
exceeds the bound of the search space in any dimension, the
particle direction is reversed in this dimension to get back to
the search space. For example, in case of a dimension with
a range of values from 0 to 2, if a particle moves to 3, its
position is reversed to 1.
In addition, as the velocity can increase over time, a limit
is set on velocity to prevent an infinite velocity or invalid
position for the particle. Setting a maximum velocity, which
determines the distance of movement from the current position
to the possible target position, can reduce the likelihood of
explosion of the swarm traveling distance [31]. Generally, the
value of the maximum velocity is selected as i /2, where i
is the range of dimension i.
The pseudocode in Algorithm 2 presents the process of generating a test case by PSO. This algorithm can be invoked
by line 6 of Algorithm 1 to generate a test case for t-way
schemas. The n factors of the test case can be treated as an
n-dimensional hyperspace. A particle pi = (x1 , x2 , . . . , xn ) can
be regarded as a candidate test case. The fitness function is
that of Definition 4, the number of uncovered t-way schemas
in the generated test suite that are covered by particle pi . PSO
employs real numbers but the valid values are integers for covering array generation, so each dimension of particleâ€™s position
can be rounded to an integer while maintaining the velocity
as a real number. This method is used in all prior research
applying PSO to covering array generation [13]â€“[16].
III. R ELATED W ORK
In this section, three different but related aspects are discussed. We first summarize search-based CT, especially the
current applications of PSO for covering array generation.
Then, we introduce prior research on discrete versions of PSO,

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

Algorithm 2 Generate Test Case by PSO
1: Input: SUT(n, 1 , . . . , n ), covering strength t and the
related parameters of PSO
2: Output: the best test case gbest
3: it = 0, gbest = NULL
4: for each particle pi do
5:
Initialize the position xi and velocity vi randomly
6: end for
7: while it < maximum iteration do
8:
for each particle pi do
9:
Compute the fitness value fitness(pi )
10:
if fitness(pi ) > fitness(pbesti ) then
11:
pbesti = pi
12:
end if
13:
if fitness(pi ) > fitness(gbest) then
14:
gbest = pi
15:
end if
16:
end for
17:
for each particle pi do
18:
Update the velocity and position according to
Equations 2 and 3
19:
Apply maximum velocity limitation and bound
handling strategy
20:
end for
21:
it = it + 1
22: end while
23: return gbest

to improve CPSO for discrete problems. Finally, some PSO
variants are introduced. These try to enhance PSO for conventional continuous problems by adopting strategies that can
further improve covering array generation.
A. PSO in Search-Based CT
SBSE has grown quickly in recent years. Many problems
in software engineering have been formulated into searchbased optimization problems, and heuristic techniques have
been used to find solutions. Software testing is a major topic
in software engineering. Many heuristic techniques have also
been applied to this field, including functional testing, mutation testing, stress testing, temporal testing, CT, and regression
testing. Currently, many classic heuristic techniques, such as
SA [3]â€“[7], GA [8]â€“[10], and ACO [9], [11], [12] have been
applied to generate uniform and VS covering arrays
successfully.
PSO has also been applied to software testing.
Windisch et al. [34] applied PSO to structural testing,
and compared its performance with GA. They showed that
PSO outperformed GA for most cases in terms of effectiveness and efficiency. Ganjali [35] proposed a framework for
automatic test partitioning based on PSO, observing that PSO
performed better than other existing heuristic techniques.
PSO has been applied to covering array generation.
Ahmed and Zamli [15] proposed frameworks of PSO to generate different kinds of covering arrays, including t-way
(PSTG [14]) and VS-PSTG. Then they extended PSTG to support VS covering array generation [16]. Chen et al. [18] also

579

applied PSO to 2-way covering array generation. They further
used a test suite minimization algorithm to reduce the size of
the generated covering array.
Current applications of PSO for covering array generation
can yield smaller covering arrays than most greedy algorithms,
but they all apply the same rounding operator to the particleâ€™s
position, and they lack guidelines on the parameter settings.
B. Discrete Versions of PSO
PSO was initially developed to solve problems in continuous
space, but PSO can also be applied to some discrete optimization problems including binary or general integer variables.
Many discrete versions of PSO have been proposed [17]â€“[22].
Chen et al. [18] classify existing algorithms into four
types.
1) Swap operator-based PSO [19] uses a permutation of
numbers as position and a set of swaps as velocity.
2) Space transformation-based PSO [20] uses various techniques to transform the position, defined as a vector of
real numbers, to the corresponding solution in discrete
space.
3) Fuzzy matrix-based PSO [21] defines the position and
velocity as a fuzzy matrix, and decode it to a feasible
solution.
4) Incorporating-based PSO [22] consists of hybrid methods incorporating some heuristic or problem dependent
techniques.
Chen et al. [18] also propose a S-PSO method based on
sets with probabilities, which we later adapt to represent a
particleâ€™s velocity. S-PSO uses a set-based scheme to represent the discrete search space. The velocity is defined as
a set with probabilities, and the operators are all replaced
by procedures defined on the set. They extend some PSO
variants to discrete versions and test them on the traveling
salesman problem and the multidimensional knapsack problem. They show that the discrete version of CLPSO [25] can
perform better than other variants. S-PSO was found to characterize the combinatorial optimization problems very well.
Gong et al. [36] then employed such a set-based representation scheme for solving vehicle routing problems to obtain a
new method, S-PSO-VRPTW.
C. PSO Variants
The original PSO may become trapped in a local optimum.
In order to improve the performance, many variants have been
proposed [17], [24]â€“[27]. Chen et al. [37] classify these variants into four different groups. The first group of variants aims
to adjust the control parameters during the evolution, for example by decreasing inertia weight Ï‰ linearly from 0.9 to 0.4 over
the search process. The second group employs different neighborhood topologies. The basic local version of PSO makes
the particle learn from the local best position lbesti found by
particle iâ€™s neighborhood instead of the global best position
gbest. RPSO and VPSO are two common versions which use
a ring topology and a Von Neumann topology, respectively.
The third group uses hybrid strategies with other search techniques. Many types of operators, such as genetic operators

580

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE IV
D IFFERENT G ROUPS OF DPSO AND PSO VARIANTS

(selection, crossover, and mutation), evolutionary computation
paradigms and biology inspired operators have been used. The
fourth uses multiswarm techniques. Several sets of swarms
optimize different components of the solution concurrently or
cooperatively.
In our experiments, four representative PSO variants are
included, as follows.
1) Ratnaweera et al. [24] proposed a typical variant of
the first group, TVAC, which uses a time varying
inertia weight and acceleration constant to adjust the
parameters.
2) Liang et al. [25] proposed the CLPSO method, a wellknown variant of the second group, which allows the
particle to learn from other particlesâ€™ individual best
positions in different dimensions.
3) Zhan et al. [26] proposed an adaptive PSO (APSO)
that can be seen as a variant of the third group. They
developed an evolutionary state estimation to adaptively control the parameter and used an elitist learning
strategy.
4) DMS-PSO [27], a variant of the fourth group, was proposed by Liang and Suganthan [27]. It is characterized
by a set of swarms with small sizes and these swarms
are frequently regrouped.
In summary, Table IV lists the different groups of discrete
versions of PSO and PSO variants.
IV. DPSO
In this section, a new DPSO for covering array generation
is presented. We firstly illustrate the weakness of CPSO with a
simple example. Then the representation scheme of a particleâ€™s
velocity and the corresponding redefined operators are introduced. Finally, we give two auxiliary strategies to enhance the
performance of DPSO.
A. Example
In CPSO, a particleâ€™s position represents a candidate test
case; its velocity, a real vector, represents the movement tendency of this particle in each dimension. This scheme is
meaningful in a continuous optimization problem, because
an optimal solution may exist near the current best particleâ€™s
position. So it is desirable to move the particle to this area
for further search. This may not hold for covering array
generation.

Here, we use CA(N; 2, 34 ) as an example. Suppose that
three test cases (0, 1, 0, 1), (2, 1, 1, 2), and (1, 2, 0, 0) have
been generated and added to TS in Algorithm 1, and the fourth
one is to be generated according to CPSO. A possible candidate particle pi may have the position (0, 0, 0, 0) and velocity
(0.5, 0.6, âˆ’0.4, 0.2) with fitness value 4, and its individual
best position pbesti may be (0, 0, 1, 1) with fitness value 5.
The global best position gbest may be (0, 2, 2, 2) with fitness
value 6. According to the update (2), if we take Ï‰ = 0.9 and
c1 Ã— r1 = c2 Ã— r2 â‰ˆ 0.65, in the next iteration, pi â€™s velocity
may be transformed to (0.45, 1.84, 1.59, 2.13). Thus, pi moves
to the new position (0, 1, 1, 1) with a limitation on the velocity (the velocity is kept in [âˆ’i /2, i /2]), or (0, 1, 1, 2) with
no limitation. In both cases, pi only has fitness value 2 after
updating. When this occurs, pi evolves to a worse situation,
although its position is closer to the pbesti and gbest than its
original one.
Analyzing the fitness measurement, the main contribution
to the fitness value is the combinations that the test case can
cover, not the concrete â€œpositionâ€ at which it is located. For
example, test case (2, 1, 1, 2) has a larger fitness value than
(0, 0, 0, 0) because it covers six new schemas [(2, 1, â€“, â€“),
(2, â€“, 1, â€“), (2, â€“, â€“, 2) etc.], not because of its relative distance
to other particles.
B. DPSO
To overcome this weakness, the movement of particles
should be modified. Inspired by the set-based representation scheme of velocity, DPSO is designed as a version of
S-PSO [18] to make the particle learn from the individual and
global best more effectively when generating covering arrays.
Unlike S-PSO, the element of the velocity set in DPSO is
designed for covering array generation, and DPSO does not
classify the velocity set into different dimensions to avoid the
inconsistency of different dimensions when updating velocities
in S-PSO.
In DPSO, a particleâ€™s position represents a candidate test
case, while its velocity is changed to a set of t-way schemas
with probabilities. Other than the velocityâ€™s representation and
the newly defined operators, the evolution procedure of DPSO
is the same as CPSO (Algorithm 2).
Definition 5 (Velocity): The velocity is a set of pairs
(s, p(s)), where s is a possible t-way schema of the covering
array and p(s) is a real number between 0 and 1 representing the probability of the selection of schema s to update the
current position.
In the initialization of the swarm, the particleâ€™s position is
randomly assigned. Then Cnt possible different schemas are
selected randomly and each of them is assigned a random
probability p(s) âˆˆ (0, 1). These Cnt pairs form the initial velocity set of this particle; the size of this set changes dynamically
during the evaluation.
We consider the same example CA(N; 2, 34 ). In DPSO,
when particle pi is initialized, its position xi (k) may be
(0, 0, 0, 0) representing a candidate test case as before, and
its velocity vi (k) may be such a set {((1, 1, â€“, â€“), 0.7),
((0, â€“, 0, â€“), 0.3), ((â€“, 0, â€“, 1), 0.8), ((0, â€“, â€“, 2), 0.9),

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

(a)

(b)

(c)

581

(d)

(e)

Fig. 1. Example of pi â€™s velocity updating. (a) 0.9 Ã— vi (k). (b) 2 Ã— r1 Ã— (pbesti âˆ’ xi (k)). (c) 2 Ã— r2 Ã— (gbest âˆ’ xi (k)). (d) vi (k + 1). (e) Final vi (k + 1) where
pro1 = 0.5.

((â€“, 2, 1, â€“), 0.5), ((â€“, â€“, 0, 1), 0.2)} which contains C42 = 6
pairs.
DPSO follows the conventional evolution procedure
(Algorithm 2) that uses (2) and (3) to update the velocity and
position of a particle. To adapt the new scheme for velocity
in Definition 5, the related operators in these equations must
be redefined.
1) Coefficient Ã— Velocity: The coefficient is a real number
which may be a parameter or a random number. It modifies
all the probabilities in the velocity.
Definition 6 (Coefficient Ã— Velocity): Let a be a nonnegative real number and v be a velocity, a Ã— v = {(s, p(s) Ã—
a)|(s, p(s)) âˆˆ v}. (If p(s) Ã— a > 1, p(s) Ã— a = 1.)
For example, Fig. 1(a) shows the result for Ï‰ Ã— vi (k) where
Ï‰ = 0.9.
2) Positionâ€“Position: The difference of two positions gives
the direction on which a particle moves. The results of the
minus operator is a set of (s, p(s)) pairs, as velocity.
Definition 7 (Positionâ€“Position): Let x1 and x2 be two positions. Then x1 âˆ’ x2 = {(s, 0.5)|s is a schema that exists in x1
but not in x2 }.
In the newly generated schema, probability p(s) for s is
set to 0.5 so that the acceleration constants take similar
values in both CPSO and DPSO. As in (2), the result of
positionâ€“position is multiplied by ci Ã— ri . In CPSO, ci is
often set to 2 and ri is a random number between 0 and 1
(recall Section II-C). In DPSO, we want the value of final
probability to have a range between 0 and 1 after multiplying
by ci . Setting p(s) to 0.5 puts the result in [0, 1] when ci = 2.
For example, suppose that xi (k) = (0, 0, 0, 0), pbesti =
(0, 0, 1, 1), and gbest = (0, 2, 2, 2) as before.
We can get pbesti âˆ’ xi (k) = {((0, â€“, 1, â€“, â€“), 0.5),
((0, â€“, â€“, 1), 0.5), ((â€“, 0, 1, â€“), 0.5), ((â€“, 0, â€“, 1), 0.5),
((â€“, â€“, 1, 1), 0.5)}. Fig. 1(b) and (c) shows the results for
2Ã—r1 Ã—(pbesti âˆ’xi (k)) and 2Ã—r2 Ã—(gbestâˆ’xi (k)) respectively.
3) Velocity + Velocity: The addition of velocities gives a
particleâ€™s movement path. The plus operator results in the
union of two velocities.
Definition 8 (Velocity + Velocity): Let v1 and v2 be two
velocities. Then v1 + v2 = {(s, p(s))| if (s, p1 (s)) âˆˆ v1 and
(s, p2 (s)) âˆˆ v2 , p(s) = p1 (s) + p2 (s); if (s, pi (s)) âˆˆ v1
and (s, pi (s)) âˆˆ
/ v2 or (s, pi (s)) âˆˆ v2 and (s, pi (s)) âˆˆ
/ v1 ,
p(s) = pi (s)}. (If p1 (s) + p2 (s) > 1, p1 (s) + p2 (s) = 1.)
For example, Fig. 1(d) shows the results for pi â€™s new
velocity vi (k + 1) after updating velocity.

Algorithm 3 Position Updating
1: Input: position xi (k), velocity vi (k + 1), pro2 and pro3
2: Output: new position xi (k + 1)
3: xi (k + 1) = (â€“, â€“, . . . , â€“)
4: Sort vi (k + 1) in descending order of p(s)
5: for each pair (si ,p(si )) in vi (k + 1) do
6:
Generate a random number Î± âˆˆ [0, 1]
7:
if Î± < p(si ) then
8:
for each fixed level  in si do
9:
Generate a random number Î² âˆˆ [0, 1]
10:
if Î² < pro2 and the corresponding factor
of  has not been fixed in xi (k + 1) then
11:
Update xi (k + 1) with 
12:
end if
13:
end for
14:
end if
15: end for
16: if xi (k + 1) has unfixed factors then
17:
Fill these factors by the same levels of previous
position xi (k)
18: end if
19: Generate a random number Î³ âˆˆ [0, 1]
20: if Î³ < pro3 then
21:
randomly change the level of one factor of xi (k + 1)
22: end if
23: return xi (k + 1)

In the velocity-updating phase of DPSO (2), we introduce
a new parameter pro1 to control the size of the final velocity
set. If p(si ) < pro1 , the pair (si , p(si )) is removed from the
final velocity. For example, if we set pro1 = 0.5, vi (k + 1) is
removed as shown in Fig. 1(e). Here, the velocity has been
sorted in descending order of p(s), where if several p(s) have
the same value, they are in an arbitrary order. If vi (k + 1)
becomes empty, it stays empty until new pairs are added to
it. As long as the velocity is empty, the particleâ€™s position is
not updated and no better solutions can be found from this
particle. In Section IV-C1, we discuss how to reinitialize this
particle.
4) Position+Velocity: Position plus velocity is the position
updating phase. Algorithm 3 gives the pseudo code of this
procedure. Here, two new parameters, pro2 and pro3 , numbers
in the range [0, 1], are introduced. pro2 is used to determine

582

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

the probability of selecting each fixed level in schema s and
pro3 is a mutation probability to make the particle mutate
randomly.
An example helps to describe this procedure. We already
have pi â€™s current position xi (k) = (0, 0, 0, 0), and its updated
velocity vi (k + 1) in descending order of p(s) as shown in
Fig. 1(e). We also assume that pro2 and pro3 are both set to
0.5. Each schema si here is selected to update the position
with probability p(si ). For the first pair ((0, â€“, â€“, 2), 1.0),
suppose that the random number Î± satisfies Î± < 1.0. Then,
for each fixed level of this pair, namely level 0 of the first
factor and level 2 of the fourth factor, its corresponding factor
has not been fixed in xi (k + 1). Suppose that we have the
first Î² < 0.5 but the second Î² > 0.5, the first factor will be
selected to update the position and the second factor will not.
So the new position becomes (0, â€“, â€“, â€“). For the second pair,
we regenerate the random number Î±, and compare it with the
probability 0.9. If Î± < 0.9, the second pair is selected. If we
generate Î² < 0.5 in two rounds, the new position becomes
(0, 2, 2, â€“). Accordingly, if the third pair is selected, and its
second factorâ€™s level 0 is chosen to update, it does not change
position because this factor has been set to a fixed level 2.
This procedure is repeated until all factors in the new position
xi (k+1) are set to fixed levels. If all pairs in velocity have been
considered, unfixed factors of xi (k + 1) are filled by the same
levels of previous position xi (k). For example, after finishing
the For loop in line 15, if the fourth factor of xi (k + 1) has
not been given any level, the fourth factor of xi (k) is used to
update it. Then xi (k + 1) becomes (0, 2, 2, 0).
C. Auxiliary Strategies
Auxiliary strategies can be introduced to improve the performance of PSO. Two of them are added to DPSO: 1) particle
reinitialization to make the search more effective and 2) additional evaluation of gbest to improve the selection of the global
best test case.
1) Particle Reinitialization: The PSO algorithm starts with
a random distribution of particles, which finally converge.
Then the best position that has been found is returned. The
swarm may jump out of a local optimum, but this can not
be guaranteed because CPSO lacks specific strategies for this.
When applying PSO for covering array generation, increasing
the number of iterations does not improve the ability to escape
a local optimum. Hence, particle reinitialization, a widely used
method, is employed to help DPSO to jump out of the local
optimum.
The main issue with particle reinitialization is when to reinitialize the particle. We may select a threshold, so that the
reinitialization is done when the number of iterations exceeds
the threshold. In DPSO, a better method can be applied. Using
the new representation for velocity, when the current particle pi â€™s position equals its individual best position pbesti
and global best position gbest, the size (norm) of pi â€™s velocity reduces gradually, because no pairs are generated from
(pbesti âˆ’ xi ) and (gbest âˆ’ xi ), and the original pairs in velocity
are removed gradually under the influence of Ï‰ Ã— v (reduce
the p(s) of original pairs) and parameter pro1 . After a few
fluctuations around gbest, the particle may stay at gbest, and

TABLE V
T WO D IFFERENT C ONSTRUCTIONS OF CA(N; 2, 34 )

its size (norm) of velocity is zero. We can use this scenario to
trigger reinitialization of the particle. When the reinitialization
is done, each dimension of a particleâ€™s position is randomly
assigned a valid value, and its velocity is regenerated as in the
initialization of the swarm.
2) Additional Evaluation of gbest: Current PSO methods
to generate covering arrays generally employ the fitness function in Definition 4. This fitness function only focuses on
the current candidate, and does not consider the partial test
suite TS.
Consider the generation of CA(N; 2, 34 ). Table V shows two
different construction processes. Both constructions generate
(0, 0, 0, 0) as the first test case. Then they choose different test
cases, but each of the first three reaches the largest number of
newly covered combinations, C42 = 6. The difference between
these two constructions emerges when generating the fourth
test case. In Construction 1, because the combinations with the
same level between any two factors have all been covered, we
cannot find a new test case that can still cover six combinations. However, in Construction 2, such a new test case can be
found, (1, 0, 1, 2). Because the minimum size of CA(N; 2, 34 )
is 9, each test case is required to cover six new combinations.
Thus, Construction 1 cannot generate the minimum test suite,
but Construction 2 can.
In general, there may exist multiple test cases with the same
highest fitness value, which make them equally qualified to be
gbest in Algorithm 2. Instead of arbitrarily selecting one as
gbest, it is better to apply additional distance metric to select
one among them. As shown in Table V, if the new test case
is similar to the existing tests [as (0, 1, 1, 1) is closer to
(0, 0, 0, 0) than (1, 1, 1, 1)], there may be a better chance to
find test cases with larger fitness subsequently.
In order to measure the â€œsimilarityâ€ between a test case t
and an existing test suite TS, we use the average Hamming
distance. The Hamming distance d12 indicates the number of
factors that have different levels between two test cases t1 and
t2 . Hence, the similarity between t and TS can be defined by
the average Hamming distance
H(t, TS) =

1 
dtk .
|TS|

(4)

kâˆˆTS

The new gbest selection strategy selects the best candidate gbest based on two metrics. A test case that has the
minimum average Hamming distance (i.e., the maximum similarity) from the set of test cases that all have the same highest
fitness value is selected as gbest in Algorithm 2. For example, when we generate the second test case for CA(N; 2, 34 )
in Table V, although (1, 1, 1, 1) and (0, 1, 1, 1) both have
the highest fitness value 6, (0, 1, 1, 1) is selected as gbest

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

583

TABLE VI
C OMPARING P ERFORMANCE B ETWEEN CPSO AND DPSO

because its average Hamming distance, 3, is smaller than that
of (1, 1, 1, 1), 4. We expect that this additional evaluation can
enhance the probability of generating a smaller test suite. In
addition, because we still want to make the particle follow the
conventional search behavior on the individual best direction,
this additional distance metric will not be used in updating the
pbest of DPSO.
V. E VALUATION AND PARAMETER T UNING
In this section, we first evaluate the effectiveness of DPSO
in some representative cases, and compare the results against
CPSO. Then the optimal parameter settings for both CPSO
and DPSO are explored. The goal of evaluation and parameter
tuning is to make the size of generated covering array as small
as possible. Five representative cases of covering arrays, listed
below, are selected for our experiments
CA1 (N; 2, 610 )

CA2 (N; 3, 57 )

CA3 (N; 4, 39 )

CA4 (N; 2, 43 53 62 ) VCA(N; 2, 315 , CA(3, 34 )).
We consider four independent parameters, iteration number
(iter), population size (size), inertia weight (Ï‰), and acceleration constant (c), which play similar roles in both CPSO
and DPSO, and three new parameters for DPSO, pro1 , pro2 ,
and pro3 . We carry out a base choice experiment to study the
impact of various values of these parameters on CPSO and
DPSOâ€™s performance and find the recommended settings for
them. First, a base setting is chosen as the basic configuration. Then the value of each parameter is changed to create
a series of configurations, while leaving the other parameters
unchanged. Initially, we set iter = 50, size = 20, Ï‰ = 0.9,
c = 1.3, and pro1 = pro2 = pro3 = 0.5 as a basic configuration based on the previous studies [13], [16] and our
empirical experience. To obtain statistically significant results,
the generation of each case of covering array is executed
30 times.
A. Evaluation of DPSO
We compare the performance between CPSO and DPSO
with the basic configuration. Five classes of covering arrays
are generated by these two algorithms. The sizes obtained and
average execution time per test case are shown as CPSO1 and
DPSO in Table VI. The best and mean array sizes of DPSO are
all better than those of CPSO1 . The new discrete representation scheme and auxiliary strategies improve PSO for covering
array generation.
DPSO can produce smaller covering arrays than CPSO with
the basic configuration. However, DPSO spends more time

during the evolution, because its new operators for updating
are more intricate than the conventional ones. DPSO needs
to deal with many elements of the velocity set, whereas the
conventional scheme only needs simple arithmetic operations.
To compare the performance between CPSO and DPSO given
the same execution time, for each case we let the execution
time per test case for CPSO equal to that for DPSO, so that
CPSO can spend more time in searching. We refer to this
version of CPSO as CPSO2 . In addition, a t-test between
CPSO2 and DPSO is conducted and the corresponding p-value
is presented. A p-value smaller than 0.05 indicates that the performance of these two algorithms is statistically different with
95% confidence.
From CPSO1 , CPSO2 , and DPSO in Table VI, the performance of CPSO is improved with more search time. DPSO,
which must use fewer iterations, still works better than CPSO.
The effectiveness of DPSO comes from the essential improvement of its new representation scheme for velocity and the
auxiliary strategies. The results of the t-test demonstrate the
significance of these differences. Therefore, we can conclude
that DPSO performs better than CPSO with fewer iterations
for covering array generation.
B. Parameter Tuning
In the base choice experiment, because the sizes of the various covering arrays differ, the mean size of each case (si )
obtained is normalized using
si =

si âˆ’ smin
.
smax âˆ’ smin

This normalization enables the graphical representation of
different cases on a common scale.
Because some parameters may not significantly impact the
performance, we use ANOVA (significance level = 0.05) to
test whether there exist significant differences among the mean
results obtained by different parameter settings. When changing the parameter settings have no significant impact on the
generation results, these results will be presented as dotted
lines in the corresponding figures. For example, CA4 is presented as a dotted line in Fig. 2(b). It means that the iteration
number does not significantly impact the generation of this
case of covering array, and so this case will not be further
considered when identifying the optimal settings.
1) Iteration Number (iter): Iteration number determines the
number of updates. PSO typically requires thousands of iterations to evolve. Nevertheless, only a few iterations are required
to generate covering array according to [14]â€“[16]. So we

584

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

(a)
Fig. 2.

(b)

Comparing array sizes under different iteration numbers. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 3.

Comparing array sizes under different population sizes. (a) CPSO. (b) DPSO.

(a)
Fig. 4.

(b)

Comparing array sizes under different settings of population size and iteration number. (a) CPSO. (b) DPSO.

change its value from 50, the base setting, to 1950 incrementally. Much larger values are not used because the execution
time may increase markedly without a commensurate increase
in the quality of the results.
Fig. 2 shows the results for different choices of iteration numbers, where each line represents a covering array.
Performance is improved with increasing iterations for both
CPSO and DPSO. A small number of iterations may not be
appropriate due to insufficient searching. Because the optimal
settings are different among different cases, and several settings can be chosen to generate the minimum covering arrays,
it is open to debate which setting is the â€œbestâ€ one. Given
time constraints, for a population size of 20, a good setting
of iteration number could be approximately 1000 for both
CPSO and DPSO.
2) Population Size (Size): Population size determines the
initial search space. Generally a small value, e.g., 20, is satisfactory for most cases. A large population size may bring a
higher diversity and find a better solution, but it also increases
the evolving time. For the same reason as before, we change
the value of population size from 10 to 100.

Fig. 3 shows the results for different choices of population size. There is no doubt that the mean array size obtained
decreases as population size increases. A large population size
can have more chances to generate smaller covering arrays, but
its execution time can become prohibitive.
In addition, because iteration number and population size
together determine the search effort of PSO, we explore the
combinations between these two parameters. We let iter Ã— size
be a constant 20 000, and generate each case under different
settings of these two parameters. Fig. 4 shows the results,
where PSO prefers a relatively larger population size. In
CPSO, ten particles with 2000 iterations is the worst setting,
and most cases produce good results with 60 or 80 particles.
In DPSO, the best choice of population size is still 80. In
both CPSO and DPSO, the largest population size 100 cannot
produce as good results as that of 80 due to having fewer iterations. So achieving smallest covering arrays requires a good
balance between these two parameters, and a moderately large
population size is necessary for both CPSO and DPSO. Thus,
we can set iteration number to 250, and population size to 80,
as the recommended settings for both CPSO and DPSO.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

(b)

(a)
Fig. 5.

Comparing array sizes under different inertia weights. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 6.

Comparing array sizes under different acceleration constants. (a) CPSO. (b) DPSO.

(a)
Fig. 7.

585

(b)

(c)

Comparing array sizes under different settings of (a) pro1 , (b) pro2 , and (c) pro3 .

3) Inertia Weight (Ï‰): Inertia weight determines the tendency of the particle to continue in the same direction. A
small value help the particle move primarily toward the best
position, while a large one is helpful to continue its previous
movement. A linearly decreasing value is also used as it can
make the swarm gradually narrow the search space. Here, we
investigate both fixed values and a linearly decreasing value
from 0.9 to 0.4 over the whole evolution (presented as â€œdecâ€).
Fig. 5 shows the results for different choices of inertia
weight. In CPSO, most of the smallest covering arrays are
generated by large fixed inertia weights. The decreasing value
does not perform as well as the large values, such as 0.9 for
CA1 , CA2 , and CA4 . So 0.9 can be the recommended choice
for CPSO. In DPSO, 1.1 is the worst choice because the particle may update its position based on its own velocity and
thus fail to learn from individual and global best positions.
The decreasing value can perform reasonably well, but a fixed
value 0.5 may be a better choice in that it keeps the effort of
global search moderate. Thus, we can recommend the fixed
inertia weight of 0.9 for CPSO, and 0.5 for DPSO.
4) Acceleration Constant (c): Acceleration Constants
c1 and c2 control the learning from pbest and gbest, respectively. Generally, they are set to the same value to balance

TABLE VII
R ECOMMENDED PARAMETER S ETTINGS

the influence of these two positions. Setting c1 and c2 to a
large value may make the particle more likely attracted to the
best position ever found, while a small one may make the
search far from the current optimal region. In this paper, we
set c1 = c2 = c, and vary c from 0.1 to 2.9.
Fig. 6 shows the results for different choices of acceleration
constant. Unlike other parameters, there is a consistent trend in
all five cases. In CPSO, the values larger than 0.5 all produce
good results. In DPSO, 1.3 can definitely be regarded as the
optimal value. Thus, we set 1.3 as the recommended value of
acceleration constant for both CPSO and DPSO.
5) pro1 , pro2 , and pro3 : These three parameters are new to
DPSO. Parameter pro1 determines the size of the final velocity. As those pairs whose probabilities are smaller than pro1
are removed from the final velocity set, a small value may
keep more pairs in the velocity for the next evolution, but it

586

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE VIII
S IZES (N) OF C OVERING A RRAYS FOR n FACTORS , E ACH H AVING T HREE L EVELS , W ITH C OVERING S TRENGTH t, CA(N; t, n, 3)

also requires more computation time. Parameter pro2 determines the probability of selecting each fixed level from each
schema when updating positions. A larger value may lead to
a quick construction of new position, but it may also lead
to fast convergence toward a local optimum. Parameter pro3
determines the mutation probability when updating positions.
A larger value may enhance the randomness, but it also lowers
the convergence speed. In this paper, values from 0.1 to 1.0
for these three parameters are investigated.
Fig. 7 shows the results. For pro1 , a large value is not
appropriate because it removes nearly all pairs from the final
velocity set. A medium value 0.5, which appears to lead to the
best result, may be the best choice. For pro2 , the smallest value
0.1 yields the best results. Slower construction of the new position may slow the convergence toward a local optimum, but it
also takes longer. So, in order to balance performance and execution time, we take 0.3 as the recommended value for pro2 .
For pro3 , a larger value may be a good choice. The frequent
mutation of new position may bring better results, but also
takes longer to converge. So, we take 0.7 as the recommended
value for pro3 .
In summary, the recommended parameter settings for PSO
for covering array generation are different from previously
suggested ones [13], [16]. Some parameters may significantly
impact the performance in some cases, and parameter tuning is necessary to enhance heuristic techniques for particular
applications. Naturally, the optimal settings vary for different cases of covering arrays. There may not exist a common
setting that can always lead to the best results. Because it is

impractical to tune parameters for each case in real life, we
suggest two general settings for CPSO and DPSO, as shown
in Table VII, which can typically lead to better performance
within reasonable execution time.
VI. C OMPARING A MONG PSO S
In this section, we compare the best reported array sizes
generated by PSO in [16] with our findings for CPSO, DPSO,
and four representative variants. Because the research in [16]
demonstrated that their generation results typically outperform
greedy algorithms, in this paper, we do not compare CPSO and
DPSO with greedy algorithms.
We implement both the original and discrete versions of
four variants (TVAC [24], CLPSO [25], APSO [26], and
DMS-PSO [27]) to generate covering arrays. Their discrete
versions are extended based on new representation scheme of
velocity and auxiliary strategies. We name them D-TVAC, DCLPSO, D-APSO, and D-DMS-PSO, respectively. In CLPSO,
a particle can learn from different particlesâ€™ pbest in each
dimension whereas we do not distinguish dimensions strictly
in DPSO. So in D-CLPSO, a particle can fully learn from
different particlesâ€™ pbest in all dimensions. That may weaken
the search ability of CLPSO. For the other three variants, they
can be directly extended based on DPSO.
All algorithms are compared using the same number of
fitness function evaluations. CPSO and DPSO use the recommended settings shown in Table VII. For the variants, iter,
size and pro1 , pro2 and pro3 for discrete versions are set to

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

587

TABLE IX
S IZES (N) OF C OVERING A RRAYS FOR S EVEN FACTORS , E ACH H AVING  L EVELS , W ITH C OVERING S TRENGTH t, CA(N; t, 7, )

their recommended values. Ï‰ and c are also set to recommended values unless they are adaptively adjusted during the
evolution, in which case their range is set to [0.4, 0.9] and
[0.8, 1.8], respectively. The new control parameters for these
variants follow their suggested settings.
Tables VIIIâ€“XIII give the results. Because of the execution
time, we only consider covering strengths from 2 to 4, and the
generation of each covering array is repeated 30 times. A t-test
(significance level = 0.05) is also conducted to test whether
there exists a significant difference between the mean sizes
produced by the two algorithms. In the first three columns, we
report the best and mean array sizes obtained from previous
results, CPSO and DPSO, where boldface numbers indicate
that the difference between CPSO and DPSO is significant
based on the t-test. In the last four columns, we report the mean
array sizes from the original and discrete versions of each PSO
variant (presented as meanc and meand respectively), where
boldface numbers indicate that the difference between meanc
and meand of each variant is significant.
A. Uniform Covering Arrays
Tables VIIIâ€“X present the results for uniform covering
arrays. We extend the cases considered in [16], where â€œâ€“â€
indicates the not available cases. In Tables VIII, we report
array sizes for n factors, each having three levels. In
Tables IX and X, we report array sizes for 7 and 10 factors,
each having  levels. Their covering strengths all range from
2 to 4.
Typically, CPSO can produce smaller sizes than those
reported in [16], demonstrating the effectiveness of parameter tuning. Furthermore, DPSO can produce the smallest best
and mean sizes in almost all cases, and usually the performance of DPSO is significantly better than CPSO. When the
covering strength t = 2 is adopted (Table VIII), DPSO does

not generally outperform CPSO. But when covering strength
increases, DPSO performs better. Sometimes the mean sizes
for DPSO are smaller than the best sizes for CPSO. Because
generating a covering array with higher covering strength is
more complicated, DPSO may be more appropriate for generating such covering arrays. In Tables IX and X, we also find
that DPSO is superior for covering arrays with high covering strength. Overall, these results show the effectiveness of
DPSO for uniform covering array generation.
Surprisingly, DPSO does not beat previous results for
CPSO when covering arrays have two levels for each factor
(Table IX). This appears to be a weakness of DPSO. Although
DPSO generates smaller covering arrays than previous results
and CPSO, other techniques may still yield better results than
DPSO (some best known sizes can be found in [38]).
B. VS Covering Array
Tables XIâ€“XIII give the results for VS covering arrays.
Based on CA(N; 2, 315 ), CA(N; 3, 315 ), and CA(N; 2, 42 52 63 ),
some different cases of sub covering arrays conducted in [16]
are examined. Their covering strengths are at most 4.
Generally, we can draw similar conclusions as for uniform
covering arrays. CPSO with the suggested parameter setting
can produce better results than reported sizes in some cases.
DPSO also usually beats them on the best and mean sizes.
In Table XI, often the difference between CPSO and DPSO
is not significant. In part this is because for the CA(3, 33 ),
CA(3, 33 )2 , CA(3, 33 )3 , and CA(4, 34 ), the same best results
are provably the minimum (e.g., the minimum size of the
CA(3, 33 ) is 3 Ã— 3 Ã— 3 = 27). For the other cases, although
sometimes the difference is not significant, DPSO can still generate smaller covering arrays. DPSO remains a good choice
for generating VS covering arrays. In Tables XII and XIII,
similar results can be found.

588

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE X
S IZES (N) OF C OVERING A RRAYS FOR T EN FACTORS , E ACH H AVING  L EVELS , W ITH C OVERING S TRENGTH t, CA(N; t, 10, )

TABLE XI
S IZES (N) OF VS C OVERING A RRAYS VCA(N; 2, 315 , CA)

TABLE XII
S IZES (N) OF VS C OVERING A RRAYS VCA(N; 3, 315 , CA)

For both uniform and variable cases of covering arrays,
parameter tuning can enhance CPSO to generate smaller covering arrays than previous work. DPSO can produce the smallest
covering arrays in nearly all cases, and in general the difference of performance between CPSO and DPSO is significant.
DPSO is an effective discrete version of PSO for covering
array generation.
C. PSO Variants
In order to further investigate the effectiveness of DPSO for
covering array generation, we implement the original versions
of four representative variants of PSO and extend them to their
discrete versions based on DPSO. We apply them to generate

the same cases for covering arrays. The mean sizes obtained
are shown in the last four columns in Tables VIIIâ€“XIII.
We first compare the mean sizes of original PSO variants
with those of CPSO and DPSO. In our experiments, TVACâ€™s
mean sizes are always larger than CPSOâ€™s. The linear adjustment of inertia weight and acceleration constant is not helpful
for CPSO for covering array generation. Typically, APSOâ€™s
mean sizes are also larger than CPSOâ€™s. Because APSO uses
a fuzzy system to classify different evolutionary states, its
ineffectiveness may result from inappropriate parameter settings. CLPSO and DMS-PSO can outperform CPSO typically.
Although on occasion they achieve comparable performance
with DPSO, they cannot perform as well as DPSO in most

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

589

TABLE XIII
S IZES (N) OF VS C OVERING A RRAYS VCA(N; 2, 43 53 62 , CA)

TABLE XIV
C OMPARING CPSO AND DPSO W ITH GA AND ACO

cases. That further demonstrates the effectiveness of DPSO
for covering array generation. Because these four algorithms
are representative PSO variants (as shown in Table IV), designing different neighborhood topologies and using multiswarm
techniques may have potential to improve CPSO for covering
array generation.
We next compare the original and discrete versions of each
variant. Except for CLPSO, the other three variants can be
improved using their discrete versions, and the improvement
is also significant. TVAC cannot outperform CPSO, but it
is enhanced by DPSO so that D-TVAC can produce smaller
mean sizes than CPSO in most cases. The linear adjustment
is helpful for the discrete version. For CLPSO, only in a few
cases is it improved by DPSO. Sometimes D-CLPSO even
leads to worse results (see Table X), due primarily to the
weakened search ability of its discrete version as explained in
Section VI. For APSO, DPSO can enhance its original version,
but D-APSO is still worse than DPSO. That may result from
inappropriate settings as explained before. For DMS-PSO,
sometimes DPSO does not enhance it (see Table XI).
However, in most cases D-DMS-PSO can outperform
DMS-PSO and has comparable performance with D-TVAC.
The multiswarm strategy is also helpful for the discrete
version.

In summary, the comparison study reveals that our suggested parameter settings are more suitable for covering array
generation. DPSO is an effective discrete version of PSO.
It can significantly outperform previous results and CPSO
in nearly all cases for uniform and VS covering arrays.
Furthermore, DPSOâ€™s representation scheme of a particleâ€™s
velocity and auxiliary strategies not only enhance CPSO, but
also typically enhance PSO variants. DPSO is a promising
improvement on PSO for covering array generation.
VII. C OMPARING DPSO W ITH GA AND ACO
Because GAs and ACO [8]â€“[12] have also been successfully
used for covering array generation, we compare CPSO and
DPSO with the reported array sizes in [9] and [11]. There are
no widely accepted benchmarks for the comparison of searchbased covering array generation, so we only consider these
two representative and competitive works.
Shiba et al. [9] applied both GA and ACO to generate
uniform covering arrays (CA1 to CA8 in Table XIV), and
Chen et al. [11] applied ACO to generate VS covering arrays
(VCA9 to VCA12 in Table XIV). They both set algorithm
parameters according to recommendations in related research
fields without parameter tuning, while our CPSO and DPSO

590

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

use the recommended settings shown in Table VII. To compare
these algorithms under the same number of fitness evaluations, for each case of covering array, the population size and
the number of iterations of CPSO and DPSO are modified
accordingly to satisfy the settings in [9] and [11]. Moreover,
Shiba et al. [9] and Chen et al. [11] both applied test minimization algorithms to further reduce the size of generated
covering arrays, while our CPSO and DPSO do not apply any
minimization algorithms.
Table XIV shows the comparison results, where boldface
numbers indicate the best array sizes obtained, and â€œâ€“â€ represents that the corresponding data is not available. The
generation of each case of covering array of CPSO and DPSO
is executed 30 times and the best and average results are
presented. Because we do not implement GA and ACO for
covering array generation, no statistical tests can be conducted
here. In addition, because the platforms used for collecting the
results differ, the comparison of computational time would not
be informative. We nevertheless present the execution times
of our CPSO and DPSO, which can serve as references for
practitioners.
From Table XIV, DPSO can outperform existing GA and
ACO for covering array generation, despite the latter two
applying test minimization algorithms. Because our DPSO is
a version of PSO designed and tuned for covering array generation, it suggests that tuned versions of GAs and ACOs may
be a promising area for further study. In addition, CPSO performs worse than GA and ACO in seven of 12 cases. That
may result from the improvement by minimization algorithms
in [9] and [11]. But for CA2 , CA3 , and CA6 , CPSO can still
achieve smaller covering arrays.
In summary, the results further demonstrate that DPSO is
an effective discrete version of PSO for covering array generation. Further investigations of tuned versions of GA and ACO
should be considered.
VIII. C ONCLUSION
Covering array generation is a key issue in CT. We developed a new DPSO for covering array generation, by adapting
S-PSO to generate covering arrays and incorporating two
auxiliary strategies to improve the performance. Parameter tuning was applied to both CPSO and DPSO to identify their
best parameter settings. The original and discrete versions of
four representative PSO variants were implemented and their
efficacy for covering array generation was compared. The performance of DPSO was also compared with other existing
evolutionary algorithms, GA and ACO.
DPSO can perform better than CPSO with fewer iterations, and the performance of CPSO and DPSO is significantly
impacted by their parameter settings. Different cases require
different parameter settings; there may not exist a single choice
that leads to the best results. After parameter tuning, we identified two recommended settings, which lead to relatively good
performance for CPSO and DPSO. Indeed, CPSO with our recommended parameter settings improves on previously reported
results, and our DPSO with recommended parameter settings usually beats CPSO. Typically, DPSO also significantly

enhances the performance of PSO variants. In addition, DPSO
often outperforms GA and ACO to generate covering arrays.
Consequently, DPSO is a promising improvement of PSO for
covering array generation.
Improvements on the methods here may be possible in
a number of ways. One would be to investigate further
evolution procedures and strategies proposed in PSO, such
as hybridizing with penalty approaches to handle discrete
unknowns [39], and compare the results with some exact
schemes like branch and bound method. A second would be
to examine one-column-at-a-time approaches or methods that
construct the entire array, rather than the one-row-at-a-time
approach adopted here. A third would be to incorporate
DPSO with other methods, in particular with test minimization
methods.
R EFERENCES
[1] C. Nie and H. Leung, â€œA survey of combinatorial testing,â€ ACM Comput.
Surv., vol. 43, no. 2, pp. 11.1â€“11.29, 2011.
[2] D. Kuhn and M. Reilly, â€œAn investigation of the applicability of
design of experiments to software testing,â€ in Proc. 27th Annu. NASA
Goddard/IEEE Softw. Eng. Workshop, Greenbelt, MD, USA, 2002,
pp. 91â€“95.
[3] M. Cohen, P. Gibbons, W. Mugridge, and C. Colbourn, â€œConstructing
test suites for interaction testing,â€ in Proc. 25th Int. Conf. Softw. Eng.,
Portland, OR, USA, 2003, pp. 38â€“48.
[4] M. B. Cohen, C. J. Colbourn, and A. C. Ling, â€œConstructing strength
three covering arrays with augmented annealing,â€ Discrete Math.,
vol. 308, no. 13, pp. 2709â€“2722, 2008.
[5] J. Torres-Jimenez and E. Rodriguez-Tello, â€œSimulated annealing for constructing binary covering arrays of variable strength,â€ in Proc. Congr.
Evol. Comput., Barcelona, Spain, Jul. 2010, pp. 1â€“8.
[6] B. Garvin, M. Cohen, and M. Dwyer, â€œEvaluating improvements to a
meta-heuristic search for constrained interaction testing,â€ Empir. Softw.
Eng., vol. 16, no. 1, pp. 61â€“102, 2011.
[7] J. Torres-Jimenez and E. Rodriguez-Tello, â€œNew bounds for binary
covering arrays using simulated annealing,â€ Inf. Sci., vol. 185, no. 1,
pp. 137â€“152, 2012.
[8] S. Ghazi and M. Ahmed, â€œPair-wise test coverage using genetic algorithms,â€ in Proc. Congr. Evol. Comput., vol. 2. Canberra, ACT, Australia,
2003, pp. 1420â€“1424.
[9] T. Shiba, T. Tsuchiya, and T. Kikuno, â€œUsing artificial life techniques to
generate test cases for combinatorial testing,â€ in Proc. 28th Annu. Int.
Comput. Softw. Appl. Conf., vol. 1. Hong Kong, 2004, pp. 72â€“77.
[10] J. McCaffrey, â€œAn empirical study of pairwise test set generation using
a genetic algorithm,â€ in Proc. 7th Int. Conf. Inf. Technol. New Gener.,
Las Vegas, NV, USA, 2010, pp. 992â€“997.
[11] X. Chen, Q. Gu, A. Li, and D. Chen, â€œVariable strength interaction
testing with an ant colony system approach,â€ in Proc. Asia-Pacific Softw.
Eng. Conf., Penang, Malaysia, 2009, pp. 160â€“167.
[12] X. Chen, Q. Gu, X. Zhang, and D. Chen, â€œBuilding prioritized pairwise
interaction test suites with ant colony optimization,â€ in Proc. 9th Int.
Conf. Qual. Softw., Jeju-do, Korea, 2009, pp. 347â€“352.
[13] X. Chen, Q. Gu, J. Qi, and D. Chen, â€œApplying particle swarm optimization to pairwise testing,â€ in Proc. 34th Annu. Comput. Softw. Appl.
Conf., Seoul, Korea, 2010, pp. 107â€“116.
[14] B. S. Ahmed and K. Z. Zamli, â€œPSTG: A T-way strategy adopting particle swarm optimization,â€ in Proc. 4th Asia Int. Conf. Math. Anal. Model.
Comput. Simulat., Kota Kinabalu, Malaysia, 2010, pp. 1â€“5.
[15] B. S. Ahmed and K. Z. Zamli, â€œA variable strength interaction test suites
generation strategy using particle swarm optimization,â€ J. Syst. Softw.,
vol. 84, no. 12, pp. 2171â€“2185, 2011.
[16] B. S. Ahmed, K. Z. Zamli, and C. P. Lim, â€œApplication of particle
swarm optimization to uniform and variable strength covering array
construction,â€ Appl. Soft Comput., vol. 12, no. 4, pp. 1330â€“1347, 2012.
[17] Y. del Valle, G. Venayagamoorthy, S. Mohagheghi, J.-C. Hernandez, and
R. Harley, â€œParticle swarm optimization: Basic concepts, variants and
applications in power systems,â€ IEEE Trans. Evol. Comput., vol. 12,
no. 2, pp. 171â€“195, Apr. 2008.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

[18] W.-N. Chen et al., â€œA novel set-based particle swarm optimization
method for discrete optimization problems,â€ IEEE Trans. Evol. Comput.,
vol. 14, no. 2, pp. 278â€“300, Apr. 2010.
[19] M. Clerc, Discrete Particle Swarm Optimization (New Optimization
Techniques in Engineering). New York, NY, USA: Springer, 2004.
[20] W. Pang et al., â€œModified particle swarm optimization based on space
transformation for solving traveling salesman problem,â€ in Proc. Int.
Conf. Mach. Learn. Cybern., vol. 4. Shanghai, China, Aug. 2004,
pp. 2342â€“2346.
[21] W. Pang, K.-P. Wang, C.-G. Zhou, and L.-J. Dong, â€œFuzzy discrete particle swarm optimization for solving traveling salesman problem,â€ in Proc.
4th Int. Conf. Comput. Inf. Technol., Wuhan, China, 2004, pp. 796â€“800.
[22] Y. Wang et al., â€œA novel quantum swarm evolutionary algorithm and its
applications,â€ Neurocomputing, vol. 70, nos. 4â€“6, pp. 633â€“640, 2007.
[23] E. Campana, G. Fasano, and A. Pinto, â€œDynamic analysis for the
selection of parameters and initial population, in particle swarm optimization,â€ J. Global Optim., vol. 48, no. 3, pp. 347â€“397, 2010.
[24] A. Ratnaweera, S. Halgamuge, and H. Watson, â€œSelf-organizing hierarchical particle swarm optimizer with time-varying acceleration coefficients,â€ IEEE Trans. Evol. Comput., vol. 8, no. 3, pp. 240â€“255,
Jun. 2004.
[25] J. Liang, A. Qin, P. Suganthan, and S. Baskar, â€œComprehensive learning particle swarm optimizer for global optimization of multimodal
functions,â€ IEEE Trans. Evol. Comput., vol. 10, no. 3, pp. 281â€“295,
Jun. 2006.
[26] Z.-H. Zhan, J. Zhang, Y. Li, and H.-H. Chung, â€œAdaptive particle swarm
optimization,â€ IEEE Trans. Syst., Man, Cybern., B, Cybern., vol. 39,
no. 6, pp. 1362â€“1381, Dec. 2009.
[27] J. Liang and P. Suganthan, â€œDynamic multi-swarm particle swarm optimizer,â€ in Proc. IEEE Swarm Intell. Symp., Pasadena, CA, USA, 2005,
pp. 124â€“129.
[28] D. Cohen, S. Dalal, M. Fredman, and G. Patton, â€œThe AETG system:
An approach to testing based on combinatorial design,â€ IEEE Trans.
Softw. Eng., vol. 23, no. 7, pp. 437â€“444, Jul. 1997.
[29] R. C. Bryce and C. J. Colbourn, â€œOne-test-at-a-time heuristic search for
interaction test suites,â€ in Proc. 9th Annu. Conf. Genet. Evol. Comput.,
London, U.K., 2007, pp. 1082â€“1089.
[30] J. Kennedy and R. Eberhart, â€œParticle swarm optimization,â€ in Proc. Int.
Conf. Neural Netw., vol. 4. Perth, WA, Australia, 1995, pp. 1942â€“1948.
[31] R. Eberhart and Y. Shi, â€œParticle swarm optimization: Developments,
applications and resources,â€ in Proc. Congr. Evol. Comput., vol. 1. Seoul,
Korea, 2001, pp. 81â€“86.
[32] R. Poli, J. Kennedy, and T. Blackwell, â€œParticle swarm optimization,â€
Swarm Intell., vol. 1, no. 1, pp. 33â€“57, 2007.
[33] S. Helwig, J. Branke, and S. Mostaghim, â€œExperimental analysis of
bound handling techniques in particle swarm optimization,â€ IEEE Trans.
Evol. Comput., vol. 17, no. 2, pp. 259â€“271, Apr. 2013.
[34] A. Windisch, S. Wappler, and J. Wegener, â€œApplying particle swarm
optimization to software testing,â€ in Proc. 9th Annu. Conf. Genet. Evol.
Comput., London, U.K., 2007, pp. 1121â€“1128.
[35] A. Ganjali, â€œA requirements-based partition testing framework using particle swarm optimization technique,â€ M.S. thesis, Dept. Electr. Comput.
Eng., Univ. Waterloo, Waterloo, ON, Canada, 2008.
[36] Y.-J. Gong et al., â€œOptimizing the vehicle routing problem with time
windows: A discrete particle swarm optimization approach,â€ IEEE
Trans. Syst., Man, Cybern. C, Appl. Rev., vol. 42, no. 2, pp. 254â€“267,
Mar. 2012.
[37] W.-N. Chen et al., â€œParticle swarm optimization with an aging leader and
challengers,â€ IEEE Trans. Evol. Comput., vol. 17, no. 2, pp. 241â€“258,
Apr. 2013.
[38] C. J. Colbourn. (Apr. 2013). Covering Array Tables for t=2, 3,
4, 5, 6. [Online]. Available: http://www.public.asu.edu/ ccolbou/src/
tabby/catable.html
[39] M. Corazza, G. Fasano, and R. Gusso, â€œParticle swarm optimization
with non-smooth penalty reformulation, for a complex portfolio selection
problem,â€ Appl. Math. Comput., vol. 224, pp. 611â€“624, Nov. 2013.

591

Huayao Wu received the B.S degree from Southeast
University, Nanjing, China and the M.S degree from
Nanjing University, Nanjing, China, where he is
currently working toward the Ph.D. degree from
Nanjing University, Nanjing.
His research interests include software testing,
especially on combinatorial testing and search-based
software testing.

Changhai Nie (Mâ€™12) received the B.S. and M.S.
degrees in mathematics from Harbin Institute of
Technology, Harbin, China, and the Ph.D. degree
in computer science from Southeast University,
Nanjing, China.
He is a Professor of Software Engineering
with State Key Laboratory for Novel Software
Technology, Department of Computer Science
and Technology, Nanjing University, Nanjing. His
research interests include software analysis, testing
and debugging.

Fei-Ching Kuo (Mâ€™06) received the B.Sc. (Hons.)
degree in computer science and the Ph.D. degree in
software engineering from Swinburne University of
Technology, Hawthorn, VIC, Australia.
She was a Lecturer with University of
Wollongong, Wollongong, NSW, Australia. She is
currently a Senior Lecturer with the Swinburne
University of Technology. Her research interests
include software analysis, testing, and debugging.

Hareton Leung (Mâ€™90) received the Ph.D. degree
in computer science from University of Alberta,
Edmonton, AB, Canada.
He is an Associate Professor and a Director
of the Laboratory for Software Development
and Management, Department of Computing,
Hong Kong Polytechnic University, Hong Kong. His
research interests include software testing, project
management, risk management, quality and process
improvement, and software metrics.

Charles J. Colbourn received the Ph.D. degree
from University of Toronto, Toronto, ON, Canada,
in 1980.
He is a Professor of Computer Science and
Engineering with Arizona State University, Tempe,
AZ, USA. He has authored the books The
Combinatorics of Network Reliability (Oxford) and
Triple Systems (Oxford), and also 320 refereed journal papers focussing on combinatorial designs and
graphs with applications in networking, computing,
and communications.
Prof. Colbourn received the Euler Medal for Lifetime Research
Achievement by the Institute for Combinatorics and its Applications in 2004.

Test Algebra for Combinatorial Testing
Wei-Tek Tsai , Charles J. Colbourn , Jie Luo , Guanqiu Qi , Qingyang Li , Xiaoying Bai
 School

of Computing, Informatics, and Decision Systems Engineering Arizona State University, Tempe, AZ, USA  State Key Laboratory of Software Development Environment School of Computer Science and Engineering, Beihang University, Beijing, China  Department of Computer Science and Technology, INLIST Tsinghua University, Beijing, China {wtsai, colbourn}@asu.edu, luojie@nlsde.buaa.edu.cn {guanqiuq, qingyan2b}@asu.edu, baixy@tsinghua.edu.cn

Abstract--This paper proposes a new algebraic system, Test Algebra (T A), for identifying faults in combinatorial testing for SaaS (Software-as-a-Service) applications. SaaS as a part of cloud computing is a new software delivery model, and mission-critical applications are composed, deployed, and executed in cloud platforms. Testing SaaS applications is a challenging task because new applications need to be tested when they are composed before they can be deployed for execution. Combinatorial testing algorithms can be used to identify faulty configurations and interactions from 2-way all the way to k-way where k is the number of components in the application. The T A defines rules to identify faulty configurations and interactions. Using the rules defined in the T A, a collection of configurations can be tested concurrently in different servers and in any order and the results obtained will be still same due to the algebraic constraints. Index Terms--Combinatorial testing, algebra, SaaS

I. I NTRODUCTION Software-as-a-Service (SaaS) is a new software delivery model. SaaS often supports three features: customization, multi-tenancy architecture (MTA), and scalability. MTA means using one code base to develop multiple tenant applications, and each tenant application essentially is a customization of the base code [12]. A SaaS system often also supports scalability as it can supply additional computing resources when the workload is heavy. Tenants' applications are often customized by using components stored in the SaaS database [14], [1], [11] including GUI, workflow, service, and data components. Once tenant applications are composed, they need to be tested, but a SaaS system can have millions of components, and hundreds of thousands of tenant applications. Testing tenant applications becomes a challenge as new tenant applications and components are added into the SaaS system continuously. New tenant applications are added on a daily basis while other tenant applications are running on the SaaS platform. As new tenant applications are composed, new components are added into the SaaS system. Each tenant application represents a customer for the SaaS system, and thus it needs to be tested. Combinatorial testing is a popular testing technique to test an application with different configurations. It often assumes that each component within a configuration has been tested

already, but interactions among components in the configuration may cause failures. Traditional combinatorial testing techniques often focus on test case generation to detect the presence of faults, but fault location is an active research area. Each configuration needs to be tested, as each configuration represents a tenant application. Traditional combinatorial testing such as AETG [2] can reveal the existence of faults by using few test cases to support t-way coverage for t  2. But knowing the existence of a fault does not indicate which tway interactions are faulty. When the problem size is small, an engineer can identify faults. However, when the problem is large, it can be a challenge to identify faults. As the IT industries moves to Big Data and cloud computing where hundreds of thousand processors are available, potentially, a large number of processors with distributed databases can be used to perform large-scale combinatorial testing. Furthermore, they also provide concurrent and asynchronous computing mechanisms such as MapReduce, automated redundancy and recovery management, automated resource provisioning, and automated migration for scalability. These capabilities provide significant computing power that was not available before. One simple way of performing combinatorial testing in a cloud environment is: 1) Partition the testing tasks; 2) Allocate these testing tasks to different processors in the cloud platform for test execution; 3) Collect results done by these processors. However, this is not efficient as while the number of computing and storage resources have increased significantly, the number of combinations to be considered is still too high. For example, a large SaaS system may have millions of components, and testing all of these combinations can still consume all the resources in a cloud platform. Two ways to improve this approach and both are based on learning from the previous test results:
·

·

Devise a mechanism to merge test results from different processors so that testing results can be merged quickly, and detect any inconsistency in testing; Based on the existing testing results, eliminate any con-

figurations or interactions from future testing. Due to the asynchronous and autonomous nature of cloud computing, test results may arrive asynchronously and autonomously, and this necessitates a new testing framework. This paper proposes a new algebraic system, Test Algebra (TA), to facilitate concurrent combinatorial testing. The key feature of TA is that the algebraic rules follow the combinatorial structure, and thus can track the test results obtained. The TA can then be used to determine whether a tenant application is faulty, and which interactions need to be tested. The TA is an algebraic system in which elements and operations are formally defined. Each element represents a unique component in the SaaS system, and a set of components represents a tenant application. Assuming each component has been tested by developers, testing a tenant application is equivalent to ensuring that there is no t-way interaction faults for t  2 among the elements in a set. The TA uses the principle that if a t-way interaction is faulty, every (t + 1)-way interaction that contains the t-way interaction as a subset is necessarily faulty. The TA provides guidance for the testing process based on test results so far. Each new test result may indicate if additional tests are needed to test a specific configuration. The TA is an algebraic system, primarily intended to track the test results without knowing how these results were obtained. Specifically, it does not record the execution sequence of previously executed test cases. Because of this, it is possible to allocate different configurations to different processors for execution in parallel or in any order, and the test results are merged following the TA rules. The execution order and the merge order do not affect the merged results if the merging follows the TA operation rules. This paper is structured as follows: Section II discusses the related work; Section III proposes TA and shows its details; and Section IV concludes this paper. Appendix provides proofs of TA associativity properties. II. R ELATED W ORK SaaS testing is a new research topic [14], [4], [10]. Using policies and metadata, test cases can be generated to test SaaS applications. Testing can be embedded in the cloud platform where tenant applications are run [14]. Gao proposed a framework for testing cloud applications [4], and proposed a scalability measure for testing cloud application scalability. Another scalability measure was proposed by [10]. Testing all combinations of inputs and preconditions is not feasible, even with a simple product [6], [8]. The number of defects in a software product can be large, and defects occurring infrequently are difficult to find [15]. Combinatorial test design is used to identify a small number of tests needed to get the coverage of important combinations. Combinatorial test design methods enable one to build structure variation into test cases for having greater test coverage with fewer test cases. Determining the presence of faults caused by a small number of interacting elements has been extensively studied

in component-based software testing. When interactions are to be examined, testing involves a combination-based strategy [5]. When every interaction among t or fewer elements is to be tested, methods have been developed that provide pairwise or t-way coverage. Among the early methods, AET G [2] popularized greedy one-test-at-a-time methods for constructing test suites. In the literature, the test suite is usually called a covering array, defined as follows. Suppose that there are k configurable elements, numbered from 1 to k . Suppose that for element c, there are vc valid options. A t-way interaction is a selection of t of the k configurable elements, and a valid option for each. A test selects a valid option for every element, and it covers a t-way interaction if, when one restricts the attention to the t selected elements, each has the same option in the interaction as it does in the test. A covering array of strength t is a collection of tests so that every t-way interaction is covered by at least one of the tests. Covering arrays reveal faults that arise from improper interaction of t or fewer elements [9]. There are numerous computational and mathematical approaches for construction of covering arrays with a number of tests as small as possible [3], [7]. If a t-way interaction causes a fault, then executing all tests of a covering array will reveal the presence of at least one faulty interaction. SaaS testing is interested in identifying those interactions that are faulty including their numbers and locations, as faulty configurations cannot be used in tenant applications. Furthermore, the number and location of faults keep on changing as new components can be added into the SaaS database continuously. By then executing each test, certain interactions are known not to be faulty, while others appear only in tests that reveal faults, and hence may be faulty. At this point, a classification tree analysis builds decision trees for characterizing possible sets of faults. This classification analysis is then used either to permit a system developer to focus on a small collection of possible faults, or to design additional tests to further restrict the set of possible faults. In [16], empirical results demonstrate the effectiveness of this strategy at limiting the possible faulty interactions to a manageable number. Assuming that interactions of more than t elements do not produce faults, a covering array can use few tests to certify that no fault arises from a t-way interaction. The Adaptive Reasoning algorithm (AR) is a strategy to detect faults in SaaS [13]. The algorithm uses earlier test results to generate new test cases to detect faults in tenant applications. It uses three principles:
·

·

·

Principle 1: When a tenant application (or configuration) fails the testing, there is at least one fault (but there may be more) in the tenant configuration. Principle 2: When a tenant application passes the testing, there is no fault in the tenant configuration resulting from a t-way interactions among components in the configuration. Principle 3: Whenever a configuration contains one or more faulty interactions, it is faulty.

III. T EST A LGEBRA Let C be a finite set of components. A configuration is a subset T  C . One is concerned with determining the operational status of configurations. To do this, one can execute certain tests; every test is a configuration, but there may be restrictions on which configurations can be used as tests. If a certain test can be executed, its execution results in an outcome of passed (operational) or failed (faulty). When a test execution yields result, all configurations that are subsets of the test are operational. However, when a test execution yields a faulty result, one only knows that at least one subset causes the fault, but it is unclear which of these subsets caused the failure. Among a set of configurations that may be responsible for faults, the objective is to determine, which cause faults and which do not. To do this, one must identify the set of candidates to be faulty. Because faults are expected to arise from an interaction among relatively few components, one considers t-way interactions. The t-way interactions are It = {U  C : |U | = t}. Hence the goal is to select tests, so that from the execution results of these tests, one can ascertain the status of all t-way interactions for some fixed small value of t. Because interactions and configurations are represented as subsets, one can use set-theoretic operations such as union, and their associated algebraic properties such as commutativity, associativity, and self-absorption. The structure of subsets and supersets also plays a key role. To permit this classification, one can use a valuation function V , so that for every subset S of components, V (S ) indicates the current knowledge about the operational status consistent with the components in S . The focus is on determining V (S ) whenever S is an interaction in I1  · · ·  It . These interactions can have one of five states. · Infeasible (X): For certain interactions, it may happen that no feasible test is permitted to contain this interaction. For example, it may be infeasible to select two GUI components in one configuration such that one says the wall is GREEN but the other says RED. · Faulty (F): If the interaction has been found to be faulty. · Operational (P): Among the rest, if an interaction has appeared in a test whose execution gave an operational result, the interaction cannot be faulty. · Irrelevant (N): For some feasible interactions, it may be the case that certain interactions are not expected to arise, so while it is possible to run a test containing the interaction, there is no requirement to do so. · Unknown (U): If neither of these occurs then the status of the interaction is required but not currently known. Any given stage of testing, an interaction has one of five possible status indicators. These five status indicators are ordered by X F P N U under a relation , and it has a natural interpretation to be explained in a moment. A. Learning from Previous Test Results The motivation for developing an algebra is to automate the deduction of the status of an interaction from the status

of tests and other interactions, particularly in combining the status of two interactions. Specifically, one is often interested in determining V (T1  T2 ) from V (T1 ) and V (T2 ). To do this, a binary operation  on {X, F, P, N, U} can be defined, with operation table as follows:  X F P N U X X X X X X F X F F F F P X F U N U N X F N N N U X F U N U

Using this definition, one can verify that the binary operation  has the following properties of commutativity and associativity. V (T1 )  V (T2 ) = V (T2 )  V (T1 ), V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). Using this operation, one observes that V (T1  T2 ) V (T1 )  V (T2 ). It follows that 1) Every superset of an infeasible interaction is infeasible. 2) Every superset of a failed interaction is failed or infeasible. 3) Every superset of an irrelevant interaction is irrelevant, failed, passed, or infeasible. A set S is an X-implicant if V (S ) = X but whenever S  S , V (S )  X. The X-implicants provide a compact representation for all interactions that are infeasible. Indeed for any interaction T that contains an X-implicant, V (T ) = X. Furthermore, a set S is an F-implicant if V (S ) = F but whenever S  S , V (S )  F. For any interaction T that contains an F-implicant, V (T ) F. In the same way, a set S is an N-implicant if V (S ) = N but whenever S  S , V (S ) = U. For any interaction T that contains an N-implicant, V (T ) N. An analogous statement holds for passed interactions, but here the implication is for subsets. A set S is a P-implicant if V (S ) = P but whenever S  S , V (S ) F. For any interaction T that is contained in a P-implicant, V (T ) = P. Implicants are defined with respect to the current knowledge about the status of interactions. When a t-way interaction is known to be infeasible, failed, or irrelevant, it must contain an X-, F-, or N-implicant. By repeatedly proceeding from tway to (t + 1)-way interactions, then, one avoids the need for any tests for (t + 1)-way interactions that contain any infeasible, failed, or irrelevant t-way interaction. Hence testing typically proceeds by determining the status of the 1-way interactions, then proceeding to 2-way, 3-way, and so on. The operation  is useful in determining the implied status of (t + 1)-way interactions from the computed results for t-way interactions, by examining unions of the t-way and smaller interactions and determining implications of the rule that V (T1  T2 ) V (T1 )  V (T2 ). Moreover, when adding further interactions to consider, all interactions previously tested that passed are contained in a P-implicant, and every (t + 1) interaction contained in one of these interactions can be assigned status P.

For example, V (a, b) = P, V (a, e) = X, and other 2way interactions exist as atomic interaction in TA. Based on the defined  operation, values of t-way interactions can be deduced from the atomic interactions and their contained interactions, such as V (a, b, e) V (a, b)  V (a, e) = X, i.e. V (a, b, e) = X. The 3-way interaction (a, b, c) can have inferred results from 2-way interactions (a, b), (a, c), (b, c). If any contained 2-way interaction has value F, the determining value of 3-way is F, without further testing needed. But if all values of contained 2-way interactions are P, (a, b, c) the interaction needs to be tested. In this case, U needs to be changed to non-U such as F or P, assuming the 3-way is not X or N. B. Changing Test Result Status When testing a configuration with n components, one should test individual components, 2-way interactions, 3-way interactions, all the way to n-way interactions. Since any combination of interactions is relevant in this case, the status of any interaction can be either X, F, P, or U. The status of a configuration is determined by the status of all interactions. 1) If an interaction has status X (F), the configuration has status X (F). 2) If all interactions have status P, the configuration has status P. 3) If some interactions still have status U, further tests are needed. It is important to determine when an interaction with status U can be deduced to have status F or P instead. It can never obtain status X or N once having had status U. To change U to P: An interaction is assigned status P if and only if it is a subset of a test that leads to proper operation. To change U to F: Consider the candidate T , one can conclude that V (T ) = F if there is a test containing T that yields a failed result, but for every other candidate interaction T that appears in this test, V (T ) = P. In other words, the only possible explanation for the failure is the failure of T . C. Matrix Representation Suppose that each individual component passed the testing. Then the operation table starts from 2-way interactions, then enlarges to t-way interactions step by step. During the procedure, many test results can be deduced from the existing results following TA rules. For example, all possible configurations of (a, b, c, d, e, f ) can be expressed in the form of matrix, or operation table. First, we show the operation table for 2-way interactions. The entries in the operation table are symmetric and those on the main diagonal are not necessary. So only half of the entries are shown. As shown in Figure 1, 3-way interactions can be composed by using 2-way interactions and components. Thus, following the TA implication rules, the 3-way interactions operation table is composed based on the results of 2-way combinations. Here, (a, b, c, d, e, f ) has more 3-way interactions than 2-way interactions. As seen in Figure 1, a 3-way interaction can be obtained through different combinations of 2-way interactions

and components. For example, {a, b, c} = {a}  {b, c} = {b}{a, c} = {c}{a, b} = {a, b}{a, c} = {a, b}{b, c} = {a, c}  {b, c}. V (a)  V (b, c) = V (c)  V (a, b) = V (a, b)  V (b, c) = P  P = U. But V (b)  V (a, c) = V (a, b)  V (a, c) = V (b, c)  V (a, c) = P  F = F. As TA defines the order of the five status indicators, the result should be the value with highest order. So V (a, b, c) = F.  a a b c d e f b P c d e F N X P X N F P F f U F P X U

D. Merging Concurrent Testing Results One way to achieve efficient testing is to allocate (overlapping or non-overlapping) tenant applications into different clusters, and each cluster is sent to a different set of servers for execution. Once each cluster completes its execution, the test results can be merged. The testing results of a specific interaction T in different servers should satisfy the following constraints. · If V (T ) = U in one cluster, then in other clusters, the same V (T ) can be either F, P, N, or U. · If V (T ) = N in one cluster, then in other clusters, the same V (T ) can be either F, P, N, or U. · If V (T ) = P in one cluster, then the same V (T ) can be either P, N, or U in all clusters; · If V (T ) = F in one cluster, then in other clusters, the same V (T ) can be F, N, or U. · If V (T ) = X in one cluster, then in other clusters, the same V (T ) can be X only. If these constraints are satisfied, then the testing results can be merged. Otherwise, there must be an error in the testing results. To represent this situation, a new status indicator, error (E), is introduced and E X. We define a binary operation  on {E, X, F, P, N, U}, with operation table as follows:  E X F P N U E E E E E E E X E X E E E E F E E F E F F P E E E P P P N E E F P N U U E E F P U U

 also has the properties of commutativity and associativity. See Appendix for proof of associativity. Using this operation, merging two testing results from two different servers can be defined as Vmerged (T ) = Vcluster1 (T )  Vcluster2 (T ). The merge can be performed in any order due to the commutativity and associativity of , and if the constraints of merge are satisfied and V (T ) = X, F, or P, the results cannot be changed by any further testing or merging of test results unless there are some errors in testing. If V (T ) = E, the testing

 a b c . . . f (a, b) (a, c) . . . (b, c) . . . (e, f )

a (a)

b (a, b) (b)

c (a, c) (b, c) (c)

··· ··· ··· ··· .. .

f (a, f ) (b, f ) (c, f ) . . . (f )

(a, b) (a, b) (a, b) (a, b, c) . . . (a, b, f ) (a, b)

(a, c) (a, c) (a, b, c) (a, c) . . .

··· ··· ··· ··· . . .

(b, c) (a, b, c) (b, c) (b, c) . . .

··· ··· ··· ··· . . .

(e, f ) (a, e, f ) (b, e, f ) (c, e, f ) . . . (e, f ) (a, b, e, f ) (a, c, e, f ) . . . (b, c, e, f ) . . . (e, f )

(a, c, f ) · · · (a, b, c) · · · (a, c) ··· .. .

(b, c, f ) · · · (a, b, c) · · · (a, b, c) · · · . . . . . . (b, c) ··· .. . ··· ··· ··· ··· . . . ··· ··· ··· . . . ··· .. . (e, f ) U U U . . . U U F . . . U . . .

 a b c . . . f (a, b) (a, c) . . . (b, c) . . . (e, f )

a

b P

c F P

··· ··· ··· ··· .. .

f U F P . . .

(a, b) U U U . . . U

(a, c) F F F . . . F F

··· ··· ··· ··· . . . ··· ··· ··· .. .

(b, c) U U U . . . U U F . . .

Fig. 1. 3-way interaction operation table

environment must be corrected and tests executed again after fixing the error(s) in testing. For example, Vcluster1 (a, c, e) = X and Vcluster2 (a, c, e) = F, so Vmerged (a, c, e) = X  F = E. It means that there is something wrong with the tests of interaction (a, c, e), and the problem must be fixed before doing further testing. Following the  associative rule, one can derive the following. V1 (T )  V2 (T )  V3 (T ) = (V1 (T )  V2 (T ))  V3 (T ) = V1 (T )  (V2 (T )  V3 (T )) = V1 (T )  V2 (T )  V3 (T )  V3 (T ) = (V1 (T )  V2 (T ))  (V2 (T )  V3 (T )) = ((V1 (T )  V2 (T ))  V2 (T ))  V3 (T ) = (V3 (T )  V2 (T ))  (V3 (T )  V1 (T )) Thus the  rule allows one to partition the configurations into different sets for different servers to run testing, and these sets do not need to be non-overlapping. In conventional cloud computing operations such as MapReduce, data should not overlap, otherwise incorrect data may be produced. For example, counting the items in a set can be performed by MapReduce, but data allocated to different servers cannot overlap, otherwise items may be counted more than once. In TA, this is not a concern due to the nature of the TA operations

. Once the results are available from each server, the testing results can be merged either incrementally, in parallel, or in any order. Furthermore, test results can be merged repeatedly without changing the final results. Using this approach, one can test say 10 batches of 100 tenant applications each, for a total of 1, 000 tenant applications. Once this is done, another batch of 1, 000 tenant applications can be tested with each 100 tenant application allocated to a server for execution. In this way, after running 100 batches, 100, 000 tenant applications can be evaluated completely. The following example illustrates the testing process of fifteen configurations. According to the definition of configuration testing, all feasible interactions should be tested. For simplicity, assume that only interaction (c, d, f ) is faulty, and only interaction (c, d, e) is infeasible, and all other interactions pass the testing. If one assigns configurations 1, 3, 5, 7, 9, 11, 13, 15 into Server1 , configurations 2, 4, 6, 8, 10, 12, 14 into Server2 , and 4-11 configurations into Server3 . If Server1 and Server3 do their own testing first, Server2 can reuse test results of interactions from them to eliminate interactions that need to be tested. For example, when testing 2-way interactions of configuration (b, c, d, f ) in Server2 , it can reuse the test results of (b, c), (b, d) of configuration (b, c, d, e) from Server3 , (b, f ) of configuration (a, b, c, f )

from Server1 . They are all passed, and it can reuse the test results of (b, c, d) of configuration (a, b, c, d) from Server1 , (b, c, f ) of configuration (a, b, c, f ) from Server1 , (b, d, f ) of configuration (a, b, d, f ) from Server1 , and (c, d, f ) of configuration (a, c, d, f ) from Server3 . Because (c, d, f ) is faulty, it can deduce that 4-way interaction (b, c, d, f ) is also faulty. For the sets of configuration that are overlapping, their returned test results from different servers are the same. The merged results of these results also stay the same. Not only interactions, sets of configurations, CS1 , CS2 , . . . , CSK can be allocated to different processors (or clusters) for testing, and the test results can then be merged. The sets can be non-overlapping or overlapping, and the merge process can be arbitrary. For example, say the result of CSi is RCSi , the merge process can be (· · · ((((RCS1 + RCS2 ) + RCS3 ) + RCS4 ) + · · · + RCSK ), or (· · · ((((RCSK + RCSk-1 ) + RCSk-2 ) + · · · + RCS1 ), or any other sequence that includes all RCSi , for i = 1 to K . This is true because RCS is simply a set of V (Tj ) for any intercation Tj in the configuration CSi . (a,b,c,d) (a,b,c,e) (a,b,c,f) (a,b,d,e) (a,b,d,f) (a,b,e,f) (a,c,d,e) (a,c,d,f) (a,c,e,f) (a,d,e,f) (b,c,d,e) (b,c,d,f) (b,c,e,f) (b,d,e,f) (c,d,e,f) Server1 P P P P P X F P P X F P P X P P P X F P P X Server2 P Server3

IV. C ONCLUSION This paper proposes TA to address SaaS combinatorial testing. The TA provides a foundation for concurrent combinatorial testing. The TA has two operations and test results can have five states with a priority. By using the TA operations, many combinatorial tests can be eliminated as the TA identifies those interactions that need not be tested. Also the TA defines operation rules to merge test results done by different processors, so that combinatorial tests can be done in a concurrent manner. The TA rules ensure that either merged results are consistent or a testing error has been detected so that retest is needed. In this way, large-scale combinatorial testing can be carried out in a cloud platform with a large number of processors to perform test execution in parallel to identify faulty interactions. ACKNOWLEDGMENT

This project is sponsored by U.S. National Science Foundation project DUE 0942453 and National Science Foundation M erged Results China (No.61073003), National Basic Research Program of China (No.2011CB302505), and the State Key Laboratory of P Software Development Environment (No. SKLSDE-2012ZXP 18), and Fujitsu Laboratory. P P R EFERENCES P [1] X. Bai, M. Li, B. Chen, W.-T. Tsai, and J. Gao. Cloud Testing Tools. In P Proceedings of IEEE 6th International Symposium on Service Oriented X System Engineering (SOSE), pages 1­12, Irvine, CA, USA, 2011. F [2] D. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG System: An Approach to Testing Based on Combinatorial Design. P Journal of IEEE Transactions on Software Engineering, 23:437­444, P 1997. X [3] C. J. Colbourn. Covering arrays and hash families. In D. Crnkovic and V. D. Tonchev, editors, Information Security, Coding Theory and Related F Combinatorics, volume 29 of NATO Science for Peace and Security P Series - D: Information and Communication Security. IOS Press, 2011. P [4] J. Gao, X. Bai, and W.-T. Tsai. SaaS Performance and Scalability Evaluation in Cloud. In Proceedings of The 6th IEEE International X
Symposium on Service Oriented System Engineering, SOSE '11, 2011. [5] M. Grindal, J. Offutt, and S. F. Andler. Combination Testing Strategies: A Survey. Software Testing, Verification, and Reliability, 15:167­199, 2005. [6] C. Kaner, J. Falk, and H. Q. Nguyen. Testing Computer Software, 2nd Edition. Wiley, New York, NY, USA, 1999. [7] V. V. Kuliamin and A. A. Petukhov. A Survey of Methods for Constructing Covering Arrays. Journal of Program Computer Software, 37(3):121­146, may 2011. [8] T. Muller and D. Friedenberg. Certified Tester Foundation Level Syllabus. Journal of International Software Testing Qualifications Board. [9] A. A. Porter, C. Yilmaz, A. M. Memon, D. C. Schmidt, and B. Natarajan. Skoll: A Process and Infrastructure for Distributed Continuous Quality Assurance. IEEE Transactions on Software Engineering, 33(8):510­525, 2007. [10] W.-T. Tsai, Y. Huang, X. Bai, and J. Gao. Scalable Architecture for SaaS. In Proceedings of 15th IEEE International Symposium on Object Component Service-oriented Real-time Distributed Computing, ISORC '12, Apr. 2012. [11] W.-T. Tsai, Y. Huang, and Q. Shao. Testing the Scalability of SaaS Applications. In Proceedings of IEEE International Conference on ServiceOriented Computing and Applications (SOCA), pages 1­4, Irvine, CA, USA, 2011. [12] W.-T. Tsai, Y. Huang, Q. Shao, and X. Bai. Data Partitioning and Redundancy Management for Robust Multi-Tenancy SaaS. International Journal of Software and Informatics (IJSI), 4(3):437­471, 2010.

E. Modified Testing Process Perform 2-way interaction testing first. Before going on to 3way interaction testing, use the results of 2-way testing to eliminate cases. The testing process stops when finishing testing all t-way interactions. The analysis of t-way interactions is based on the P T Rs of all (t - i)-way interactions for 1  i < t. The superset of infeasible, irrelevant, and faulty test cases do not need to be tested. The test results of the superset can be obtained by TA operations and must be infeasible, irrelevant, or faulty. But the superset of test cases with unknown indicator must be tested. In this way, a large repeating testing workload can be reduced. For n components, all t-way interactions for t  2 are composed by 2-way, 3-way, ..., t-way interactions. In n components combinatorial testing, the number of 2-way interactions is equal to n 2 . In general, the number of t-way interactions is equal to n t . More interactions are treated when n n > , which happens when t  n 2 . The total number t t-1 t of interactions examined is i=2 n . i

[13] W.-T. Tsai, Q. Li, C. J. Colbourn, and X. Bai. Adaptive Fault Detection for Testing Tenant Applications in Multi-Tenancy SaaS Systems. In Proceedings of IEEE International Conference on Cloud Engineering (IC2E), March 2013. [14] W.-T. Tsai, Q. Shao, and W. Li. OIC: Ontology-based Intelligent Customization Framework for SaaS. In Proceedings of International Conference on Service Oriented Computing and Applications(SOCA'10), Perth, Australia, Dec. 2010. [15] Wikipedia. Software Testing, 2013. [16] C. Yilmaz, M. B. Cohen, and A. Porter. Covering Arrays for Efficient Fault Characterization in Complex Configuration Spaces. In Proceedings of the 2004 ACM SIGSOFT International Symposium on Software Testing and Analysis, ISSTA '04, pages 45­54, New York, NY, USA, 2004. ACM.

A PPENDIX The associativity of binary operation . V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). Proof: We will prove this property in the following cases. (1) At least one of V (T1 ), V (T2 ), and V (T3 ) is X. Without loss of generality, suppose that V (T1 ) = X, then according to the operation table of , V (T1 )  (V (T2 )  V (T3 )) = X  (V (T2 )  V (T3 )) = X, (V (T1 )  V (T2 ))  V (T3 ) = (X  V (T2 ))  V (T3 ) = X  V (T3 ) = X. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (2) V (T1 ), V (T2 ), and V (T3 ) are not X and at least one of V (T1 ), V (T2 ), and V (T3 ) is F. Without loss of generality, suppose that V (T1 ) = F, then according to the operation table of , the value of V (T2 )  V (T3 ) can only be F, N or U. So V (T1 )  (V (T2 )  V (T3 )) = F  (V (T2 )  V (T3 )) = F, (V (T1 )  V (T2 ))  V (T3 ) = (F  V (T2 ))  V (T3 ) = F  V (T3 ) = F. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (3) V (T1 ), V (T2 ), and V (T3 ) are not X or F and at least one of V (T1 ), V (T2 ), and V (T3 ) is N. Without loss of generality, suppose that V (T1 ) = N, then according to the operation table of , the value of V (T2 )  V (T3 ) can only be N or U. So V (T1 )  (V (T2 )  V (T3 )) = N  (V (T2 )  V (T3 )) = N, (V (T1 )  V (T2 ))  V (T3 ) = (N  V (T2 ))  V (T3 ) = N  V (T3 ) = N. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). (4) V (T1 ), V (T2 ), and V (T3 ) are not X, F or N. In this case, V (T1 ), V (T2 ), and V (T3 ) can only be P or U. According to the operation table of , the value of V (T1 )  V (T2 ) and V (T2 )  V (T3 ) are U. So V (T1 )  (V (T2 )  V (T3 )) = V (T1 )  U = U, (V (T1 )  V (T2 ))  V (T3 ) = U  V (T3 ) = U. Thus, in this case, V (T1 )  (V (T2 )  V (T3 )) = (V (T1 )  V (T2 ))  V (T3 ). The associativity of binary operation . V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ). Proof: We will prove this property in the following cases. (1) One of V1 (T ), V2 (T ), and V3 (T ) is E. Without loss of generality, suppose that V1 (T ) = E, then according to the operation table of , V1 (T )(V2 (T )V3 (T )) = E(V2 (T ) V3 (T )) = E, (V1 (T )  V2 (T ))  V3 (T ) = (E  V2 (T ))  V3 (T ) = E  V3 (T ) = E. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ).

(2) V1 (T ), V2 (T ), and V3 (T ) are not E, and there is a pair of V1 (T ), V2 (T ), and V3 (T ) does not satisfy the constrains. Without loss of generality, suppose that V1 (T ) and V2 (T ) does not satisfy the constrains, then according to the operation table of , V1 (T )  V2 (T ) = E. So (V1 (T )  V2 (T ))  V3 (T ) = E  V3 (T ) = E. Since V1 (T ) and V2 (T ) does not satisfy the constrains, there can be two cases: (a) one of them is X and the other is not, or (b) one of them is P and the other is F. (a) If V1 (T ) = X, then V2 (T )  V3 (T ) cannot be X because V2 (T ) cannot be X. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. If V2 (T ) = X, then V2 (T )  V3 (T ) = X can only be E or X. Since V1 (T ) cannot be X, V1 (T )  (V2 (T )  V3 (T )) = E. (b) If V1 (T ) = P and V2 (T ) = F, then V2 (T )  V3 (T ) can only be E or F. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. If V1 (T ) = F and V2 (T ) = P, then V2 (T )  V3 (T ) can only be E or P. Thus, V1 (T )  (V2 (T )  V3 (T )) = E. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ). (3) V1 (T ), V2 (T ), and V3 (T ) are not E, and V1 (T ), V2 (T ), and V3 (T ) satisfy the constrains. (a) One of V1 (T ), V2 (T ), and V3 (T ) is X. Without loss of generality, suppose that V1 (T ) = X, then V2 (T ) = V3 (T ) = X. So V1 (T )  (V2 (T )  V3 (T )) = X  (X  X) = X  X = X and (V1 (T )  V2 (T ))  V3 (T ) = (X  X)  X = X  X = X. (b) V1 (T ), V2 (T ), and V3 (T ) are not X, and one of V1 (T ), V2 (T ), and V3 (T ) is F. Without loss of generality, suppose that V1 (T ) = F, then V2 (T ) and V3 (T ) can only be F, N, or U. According to operation table of , V2 (T )  V3 (T ) can only be F, N, or U, and V1 (T )  V2 (T ) can only be F. So V1 (T )  (V2 (T )  V3 (T )) = F  (V2 (T )  V3 (T )) = F and (V1 (T )  V2 (T ))  V3 (T ) = F  V3 (T ) = F. (c) V1 (T ), V2 (T ), and V3 (T ) are not X or F, and one of V1 (T ), V2 (T ), and V3 (T ) is P. Without loss of generality, suppose that V1 (T ) = P, then V2 (T ) and V3 (T ) can only be P, N, or U. According to operation table of , V2 (T )  V3 (T ) can only be P, N, or U, and V1 (T )  V2 (T ) can only be F. So V1 (T )  (V2 (T )  V3 (T )) = P  (V2 (T )  V3 (T )) = P and (V1 (T )  V2 (T ))  V3 (T ) = P  V3 (T ) = P. (d) V1 (T ), V2 (T ), and V3 (T ) are not X, F or P, and one of V1 (T ), V2 (T ), and V3 (T ) is U. Without loss of generality, suppose that V1 (T ) = U, then V2 (T ) and V3 (T ) can only be N, or U. According to operation table of , V2 (T )  V3 (T ) can only be N, or U, and V1 (T )  V2 (T ) can only be U. So V1 (T )  (V2 (T )  V3 (T )) = U  (V2 (T )  V3 (T )) = U and (V1 (T )  V2 (T ))  V3 (T ) = U  V3 (T ) = U. (e) V1 (T ), V2 (T ), and V3 (T ) are N. V1 (T )  (V2 (T )  V3 (T )) = N  (N  N) = N  N = N and (V1 (T )  V2 (T ))  V3 (T ) = (N  N)  N = N  N = N. Thus, in this case, V1 (T )  (V2 (T )  V3 (T )) = (V1 (T )  V2 (T ))  V3 (T ).

Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

Contents lists available at SciVerse ScienceDirect

Journal of Combinatorial Theory, Series A
www.elsevier.com/locate/jcta

Covering and packing for pairs
Yeow Meng Chee a , Charles J. Colbourn b , Alan C.H. Ling c , Richard M. Wilson d
a

Division of Mathematical Sciences, School of Physical and Mathematical Sciences, Nanyang Technological University, 21 Nanyang Link, Singapore 637371, Singapore b Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85287-8809, USA c Department of Computer Science, University of Vermont, Burlington, VT 05405, USA d Department of Mathematics, 253-37, California Institute of Technology, Pasadena, CA 91125, USA

a r t i c l e

i n f o

a b s t r a c t
When a v -set can be equipped with a set of k-subsets so that every 2-subset of the v -set appears in exactly (or at most, or at least) one of the chosen k-subsets, the result is a balanced incomplete block design (or packing, or covering, respectively). For each k, balanced incomplete block designs are known to exist for all sufficiently large values of v that meet certain divisibility conditions. When these conditions are not met, one can ask for the packing with the most blocks and/or the covering with the fewest blocks. Elementary necessary conditions furnish an upper bound on the number of blocks in a packing and a lower bound on the number of blocks in a covering. In this paper it is shown that for all sufficiently large values of v , a packing and a covering on v elements exist whose numbers of blocks differ from the basic bounds by no more than an additive constant depending only on k. © 2013 Elsevier Inc. All rights reserved.

Article history: Received 24 September 2011 Available online xxxx Keywords: Balanced incomplete block design Pair packing Pair covering Group divisible design Pairwise balanced design

1. Introduction Let v , k , and t be integers with v > k > t 2. Let  be a positive integer. A (t , )-packing of order v and blocksize k is a set V of v elements, and a collection B of k -element subsets (blocks) of V , so that every t -subset of V appears in at most  blocks. A (t , )-covering of order v and blocksize k is a set V of v elements, and a collection B of k -element subsets (blocks) of V , so that every t -subset

E-mail addresses: ymchee@ntu.edu.sg (Y.M. Chee), charles.colbourn@asu.edu (C.J. Colbourn), aling@cems.uvm.edu (A.C.H. Ling), rmw@caltech.edu (R.M. Wilson). 0097-3165/$ ­ see front matter © 2013 Elsevier Inc. All rights reserved. http://dx.doi.org/10.1016/j.jcta.2013.04.005

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1441

of V appears in at least  blocks. When  = 1, the simpler notation of t -packing or t -covering is used. When ( V , B ) is both a (t , )-packing and a (t , )-covering with blocksize k , it is a t -( v , k, ) design. v k A t -( v , k, ) design, if one exists, has  t / t blocks. When the required number of blocks is not integral, no such design can exist. Selecting all blocks containing a particular element x  V and deleting x from each forms the derived (t - 1)-( v - 1, k - 1, ) design (with respect to x). For a design v -i k -i to exist, evidently the derived design must exist; hence for a t -( v , k, ) design to exist,  t -i / t -i must be integral for every 0 i t . When these conditions are not all met, one can ask instead for the largest (t , )-packing, or for the smallest (t , )-covering, of order v and blocksize k . The Johnson bound [13] states that such a packing can have no more than

n k

···

n - t + 2 (n - t + 1) k-t +2 k-t +1

···

blocks, while the Schönheim bound [21] states that such a covering can have no fewer than

n k

···

n - t + 2 (n - t + 1) k-t +2 k-t +1

···

blocks. Our main result is that when t = 2 and  = 1, there exist packings and coverings whose sizes are within a constant of these bounds. Determining when these bounds are met exactly is a challenging question.  and Hanani [9] conjectured that, for fixed k and t , with all blocks of size k , In 1963, Erdos n a t -packing on n elements with /k (1 - o(1)) blocks and a t -covering on n elements with t t

/ t (1 + o(1)) blocks both exist. This was proved by Rödl [20], and has spawned a large literature t (for example, [10,11,14,15,23]). However, even when t = 2, all of these general constructions deviate from the Johnson and Schönheim bounds by an amount that grows as a function of the number of elements. Wilson [25] established that the necessary divisibility conditions for a 2-( v , k, ) design to exist are asymptotically sufficient (i.e., for fixed k and , and sufficiently large v ). This provides a  different means to establish the Erdos­Hanani conjecture for t = 2, but also does not immediately imply that one can find packings or coverings whose sizes are within a constant of the optimal sizes. Wilson [24] earlier considered this more challenging problem for packings, but the solution for the analogous problem for coverings has remained elusive. We focus on the case when t = 2 and  = 1 here. Caro and Yuster state stronger results for covering [3] and packing [2] than we prove here. Their approach relies in an essential manner on a strong statement by Gustavsson [12]:
Proposition 1.1. Let H be a graph with  vertices and h edges, having degree sequence (d1 , . . . , d ). Then there exist a constant N H and a constant H > 0, both depending only on H , such that for all n > N H , if G is a graph on n vertices, m edges, and degree sequence (1 , . . . , n ) so that min(1 , . . . , n ) n(1 - H ), gcd(d1 , . . . , d ) | gcd(1 , . . . , n ), and h | m, then G has an edge partition (decomposition) into graphs isomorphic to H . We have not been able to verify the proof of Proposition 1.1. Indeed, while the result has been used a number of times in the literature, no satisfactory proof of it appears there. While we expect that the statement is true, we do not think that the proof in [12] is sufficient at this time to employ the statement as a foundation for further results. Therefore we adopt a strategy that is completely independent of Proposition 1.1, and independent of the results built on it. In the remainder of the paper, we first recall relevant known results. Then in Section 3, we determine the possible structure of optimal packings and coverings, in order to determine what can remain uncovered in a packing, and what must be covered more than once in a covering. This is done in general for packings and coverings with a single hole, in order to limit any deviation from the desired bound to the manner in which a (fixed size) hole is filled. In Section 4, the most important part of the proof is established, namely that in each congruence class, one finite example can be produced. Finally in Section 5, these single examples are shown to form the required ingredients to establish asymptotic existence.

n

k

1442

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

2. Background To proceed more formally, we require a number of definitions and preliminary results from combinatorial design theory; related background material can be found in [1,22]. A balanced incomplete block design (BIBD) is a 2-( v , k, ) design. Balanced incomplete block designs have been extensively studied because of their central role in numerous applications in experimental design, coding and information theory, communications, and connections with fundamental topics in algebra, finite geometry, number theory, and combinatorics (see [5,7] for examples). The general divisibility conditions (stated for v k general t earlier) require that  2  0 mod 2 and ( v - 1)  0 (mod k - 1). A group divisible design ( V , G , B ) is a finite set V of elements or points; a partition G = {G 1 , . . . , G s } of V (groups); and a set B of subsets of V (blocks), with the property that every 2-subset of V lying within a group appears in no block, while every 2-subset of V with elements from different groups appears in exactly  blocks. When K is a set of positive integers for which | B |  K whenever B  B , the design is a ( K , )-GDD. When  = 1, we write simply K -GDD. Its order is | V |, its index is , and u u when the multiset of group sizes {|G i |: 1 i s} is the same as the multiset its type is 1 1 · · ·  j . We write (k, )-GDD (or k -GDD formed by including u j copies of  j when  j = 0, for all 1 when  = 1) when K = {k}. A transversal design TD (k, n) is a (k, )-GDD of type nk . We write TD(k, n) when  = 1. A transversal design is idempotent if its element set is {1, . . . , k} × {1, . . . , n}, and its block set contains {{(i , j ): 1 i k}: 1 j n}. A pairwise balanced design with blocksizes K and order v (( K , )-PBD of order v ) is a ( K , )-GDD of type 1 v ; we write K -PBD when  = 1. Then a balanced incomplete block design ((k, )-BIBD) is a (k, )-PBD; we write k -BIBD when  = 1. An incomplete pairwise balanced design of order v with holesize h , blocksizes K , and index  is a triple ( V , H , B ) for which | V | = v , | H | = h , H  V , B contains a set of subsets of V for which | B |  K whenever B  B , and for every pair of distinct elements x, y  V , the number of blocks in {{x, y }  B  B } is 0 if {x, y }  H and  otherwise. The notation ( K , )-IPBD( v , h) is used; we may omit  when it is 1, and write k instead of K when K = {k}. Let K be a set of positive integers, each at least 2. Then define  ( K ) = gcd{k - 1: k  K } and k ( K ) = gcd 2 : kK . Wilson establishes a crucial asymptotic existence result: Theorem 2.1. (See [25].) Let K be a set of integers, each at least 2. Let  be a positive integer. For all sufficiently n large n satisfying (n - 1)  0 (mod  ( K )) and  2  0 (mod ( K )), there exists a ( K , )-PBD of order n. In particular for K = {k}, when (n - 1)  0 (mod k - 1),  exists a (k, )-BIBD of order n. Colbourn and Rödl prove a variant that we use: Theorem 2.2. (See [6].) Let  > 0. Let K = {k1 , . . . , km } be a set of block sizes. Let { p 1 , . . . , pm } be nonm negative numbers with i =1 p i = 1. For all sufficiently large v satisfying v - 1  0 (mod  ( K )) and v  0 (mod ( K )), there is a K -PBD of order v in which, for each 1 i m, the fraction of pairs appearing 2 in blocks having size ki is in the range [ p i -  , p i +  ]. A stronger version of Theorem 2.2 is given in [26], and a variant for resolvable designs appears in [8]. Perhaps the most powerful generalization of Theorem 2.1 is due to Lamken and Wilson [16]. We in(r ,) be a complete digraph on n vertices with exactly  edges of color i joining troduce this next. Let K n (r ,) is any vertex x to any vertex y for every color i in a set of r colors. A family F of subgraphs of K n (r ,) (r ,) if every edge e  E ( K n ) belongs to exactly one member in F . Given a fama decomposition of K n (r ,) is a decomposition F such that every ily  of edge-r -colored digraphs, a  -decomposition of K n graph F  F is isomorphic to some graph G   . For a vertex x of an edge-r -colored digraph G , the degree-vector of x is the 2r -vector d(x) = (in1 (x), out1 (x), in2 (x), out2 (x), . . . , inr (x), outr (x)), where in j (x) and out j (x) denote the indegree and outdegree of vertex x in the spanning subgraph of G by
n 2

 0 mod

k 2

, and n is sufficiently large, there

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1443

edges of color j , respectively, for 1 j r . We denote by  (G ) the greatest common divisor of the integers t such that the 2r -vector (t , t , . . . , t ) is an integral linear combination of the vectors d(x) as x ranges over the vertex set V (G ) of G . Equivalently,  (G ) is the smallest positive integer t 0 such that (t 0 , t 0 , . . . , t 0 ) is an integral linear combination of the vectors {d(x)}. Let  be a family of simple edge-r -colored digraphs and let  () denote the greatest common divisor of the integers t such that the 2r -vector (t , t , . . . , t ) is an integral linear combination of the vectors {d(x)} as x ranges over all vertices of all graphs in  . For each graph G   , let (G ) = (m1 , m2 , . . . , mr ), where mi is the number of edges of color i in G . We denote by () the greatest common divisor of the integers m such that (m, m, . . . , m) is an integral linear combination of the vectors {(G ): G   }. Equivalently, () is the smallest positive integer m0 such that (m0 , m0 , . . . , m0 ) is an integral linear combination of the (r ,) vectors {(G )}. A graph G 0   is useless when it cannot occur in any  -decomposition of K n .  is admissible when no member of  is useless. Theorem 2.3. (See [16].) Let  be an admissible family of simple edge-r-colored digraphs. For all sufficiently (r ,) large n satisfying (n - 1)  0 (mod  ()) and n(n - 1)  0 (mod ()), a  -decomposition of K n exists. Theorem 2.3 has numerous consequences for the existence of various classes of combinatorial designs. Building on Theorem 2.3, Liu establishes the following: Theorem 2.4. (See [17].) Let K be a set of integers, each at least 2. Let m and  be positive integers. For all n sufficiently large n satisfying m(n - 1)  0 (mod  ( K )) and m2 2  0 (mod ( K )), there exists a ( K , )n GDD of order m . Mohácsy and Ray-Chaudhuri prove a result for a fixed number of groups when the index is 1. Theorem 2.5. (See [18,19].) Let k and u be integers with u k 2. For all sufficiently large m satisfying m(u - 1)  0 (mod k - 1) and m2 u (u - 1)  0 (mod k(k - 1)), there exists a k-GDD of type mu .  and Straus: This subsumes a classical result of Chowla, Erdos, Theorem 2.6. (See [4].) Let k 2 be an integer. For all sufficiently large m, there exists a TD(k, m).

3. Packings, coverings, and the optima We use known asymptotic existence results to treat asymptotic existence of packings and coverings in the cases that a k -BIBD does not exist. We require further definitions, to extend packings and coverings to have a `hole'. A packing with blocksize k with a hole ( V , H , B ) is a set V of elements, a subset (hole) H  V , and a set B of k -subsets of V , so that for every {x, y }  V , {x, y }  H , there is at most one B  B with {x, y }  B ; when {x, y }  H , there is no block B  B with {x, y }  B . The leave  of ( V , B ) is a graph with vertex set V ; pair {x, y } appears as an edge if and only if {x, y } H and is not a subset of any block of B . A covering with blocksize k with a hole ( V , H , B ) is a set V of elements, a subset H  V , and a set B of k -subsets of V , so that for every {x, y }  V , {x, y }  H , there is at least one B  B with {x, y }  B . The excess  of ( V , B ) is a multigraph with vertex set V ; the number of times pair {x, y } appears as an edge is exactly xy when {x, y }  H , and xy - 1 otherwise, where xy is the number of blocks of B that contain {x, y }. A packing with blocksize k ( V , B ) is a packing with blocksize k with a hole ( V , , B ), and a covering with blocksize k ( V , B ) is a covering with blocksize k with a hole ( V , , B ). A maximum packing with blocksize k is a packing with blocksize k ( V , B ) with the most blocks among all packings with blocksize k on | V | elements; equivalently, its leave has the fewest edges. A minimum covering with blocksize

1444

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

k is a covering with blocksize k ( V , B ) with fewest blocks among all coverings with blocksize k on | V | elements; equivalently, its excess has the fewest edges. Suppose that ( V , H , B ) is a packing with blocksize k with a hole, with v = | V |, h = | H |, and n = | V \ H |. Let x be a vertex in V \ H . The number of pairs on V that contain x is congruent to v - 1 modulo k - 1. The number containing x that appear in blocks of B is congruent to 0 modulo k - 1. Hence x has degree congruent to v - 1 modulo k - 1 in the leave. When the hole is nonempty, elements in the hole have degrees congruent to n modulo k - 1 in the leave. By the same token, in the excess of a covering with blocksize k with a hole, x has degree congruent to -( v - 1) modulo k - 1; elements in the hole have degrees congruent to -n modulo k - 1. We employ specific types of packings and coverings with holes in which the leave or excess has all vertices in the hole of degree 0. For an integer n  0 (mod k(k - 1)) and an integer h 1, let   h - 1 (mod k - 1) and  -(h - 1) (mod k - 1) with 0 , < k - 1. Then an optimum packing with blocksize k with a hole, k -OP(n + h, h), is a packing with blocksize k on n + h elements whose leave has degree  on each vertex not in the hole, and 0 on each vertex in the hole; and an optimum covering with blocksize k with a hole, k -OC(n + h, h), is a covering with blocksize k on n + h elements whose excess has degree on each vertex not in the hole, and 0 on each vertex in the hole. When h  1 (mod k - 1),  = = 0. In this case, a k-OP( v , h) and a k-OC( v , h) are the same, and are equivalent to a k -IPBD( v , h). In any packing with blocksize k on v = n + h elements with n  0 (mod k(k - 1)), no vertex can have degree smaller than  in the leave; and in any covering with blocksize k on v = n + h in the excess. Indeed, elements with n  0 (mod k(k - 1)), no vertex can have degree smaller than choosing and  so that v - 1 ; ,   v - 1 (mod k - 1);  - < k - 1; and =  when v v  1 (mod k - 1), every packing with blocksize k on v elements contains at most v ,k = k(k- 1)
v blocks, while every covering with blocksize k on v elements contains at least L v ,k = k( blocks. k-1) Then v ,k is at least the Johnson bound, and  v ,k is at most the Schönheim bound. The purpose of this paper is to prove the following two results.

Theorem 3.1. There is a constant pk such that for all v k, the number of blocks in a maximum packing with blocksize k on v elements is at least v ,k - pk and at most v ,k . Theorem 3.2. There is a constant ak such that for all v k, the number of blocks in a minimum covering with blocksize k on v elements is at least L v ,k and at most L v ,k + ak . We establish these results in a number of steps. Treating an arbitrary but fixed value of k , in Section 4, we show that for every c satisfying 0 c < k(k - 1), there exist positive integers nc  0 (mod k(k - 1)) and hc  c (mod k(k - 1)) so that a k -OP(nc + hc , hc ) exists; we also show that for every c satisfying 0 c < k(k - 1), there exist positive integers mc  0 (mod k(k - 1)) and c  c (mod k(k - 1)) so that a k -OC(mc + c , c ) exists. This provides a single example for optimal packings and coverings with a hole in every congruence class modulo k(k - 1). In Section 5, we use these results to establish that there exist integers k and uk , depending only on k , so that whenever v k , uk for which a k -OP( v , h) exists, and there also exists an uk for which a there exists an h k -OC( v , ) exists. From this, because uk is fixed and independent of v , we establish Theorems 3.1 and 3.2 by filling the holes. The crucial step, particularly for coverings, is producing one example in each congruence class. We treat this next. 4. One example in each congruence class In the case when h  1 (mod k - 1), a k -OP( v , h) and a k -OC( v , h) coincide with a k -IPBD( v , h), so we treat this situation first; subsequently the packing and covering cases differ. 4.1. Packing and covering: v  1 (mod k - 1) An incomplete transversal design ITD(k, n +  ; ) is a set V of k(n + ) elements, of which k form a hole H . The elements are partitioned into k groups G 1 , . . . , G k so that |G i  H | =  for 1 i k .

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1445

This set is equipped with a set of k -subsets (blocks) with the property that every pair of elements that appears in a group or appears in the hole H appears in no block, and every other pair appears in exactly one block. Lemma 4.1. Let k 2 be an integer. Let 0



k. For all sufficiently large n, an ITD(k, n +  ; ) exists.

Proof. Using Theorem 2.6, choose  so that a TD(k + 1, ), a TD(k + 1,  + 1), a TD(k + 1,  + 2), and a TD(k + 1,  + 3) all exist. Delete one group in each to form an idempotent TD(k, v ) for each v  {,  + 1,  + 2,  + 3}. For n sufficiently large, there is an { + 1,  + 2,  + 3}-PBD of order n +  + 1 containing a block of size  + 1 by Theorem 2.2. (Because  ({ + 1,  + 2,  + 3}) = 1 and ({ + 1,  + 2,  + 3}) = 1, this follows by choosing 0 <  < 1 and choosing the fraction of pairs 4 in blocks of size  + 1 to be 2 .) Delete all but  elements from a block of size  + 1, and remove the block of size  making a hole, to form an {,  + 1,  + 2,  + 3}-IPBD(n + , ). Give every element weight k , and use the idempotent TDs to inflate all blocks. The k elements arising from the  elements of hole in the IPBD form the hole of the ITD. 2 Lemma 4.2. Let h be an integer for which h  1 (mod k - 1) and k infinitely many integers  for which a k-IPBD( k(k - 1) + h, h) exists. h k(k - 1) + 1. Then there exist

h-k Proof. Let  = k -1 . Choose  so that a k -BIBD of order  (k - 1) + k and an ITD(k,  (k - 1) +  ; ) both exist. (Use Lemma 4.1 for the existence of the ITD.) Start with the ITD on the elements of V having a hole on the elements in H  V . Add k -  new elements N  . For 1 i k , let N i consist of the  elements in the i th group of the ITD that appear in H . Place on the elements of the i th group, together with N  , the blocks of a copy of the k -BIBD, omitting a block on the elements of N i  N  . On the  k(k - 1) + (k - 1) + k =  k(k - 1) + h elements of V  N  , all pairs are covered except k those within the hole on elements N   i =1 N i of size h = (k - 1) + k . 2

Corollary 4.3. Whenever c  1 (mod k - 1) and 0 c < k(k - 1), there are infinitely many integers nc and hc with nc  0 (mod k(k - 1)) and hc  c (mod k(k - 1)) so that a k-OP(nc + hc , hc ) exists. Proof. Set hc = c if c nc =  k(k - 1). 2 k , and hc = k(k - 1) + 1 if c = 1. Apply Lemma 4.2 with h = hc , and set

The same argument establishes: Corollary 4.4. Whenever c  1 (mod k - 1) and 0 c < k(k - 1), there are infinitely many integers mc and with mc  0 (mod k(k - 1)) and c  c (mod k(k - 1)) so that a k-OC(mc + c , c ) exists. 4.2. Packing: v  1 (mod k - 1) Lemma 4.5. For every integer c satisfying 0 c < k(k - 1), there exist an nc  0 (mod k(k - 1)) and an hc  c (mod k(k - 1)) for which a k-OP(nc + hc , hc ) exists. Proof. When c > 0, write c = s(k - 1) + d with 1 d < k . When c = 0, set s = d = k - 1. If d = 1, apply Lemma 4.3. Otherwise choose   1 (mod k(k - 1)) and N >  so that N   (mod k - 1); a k -GDD of type d exists (Theorem 2.4); an  -BIBD of order N exists (Theorem 2.1); and an ITD(k, d( N -  ) + s, s) exists (Lemma 4.1). Treat the  -BIBD as an  -GDD of type 1 N -  1 by removing a block, and inflate using the k GDD of type d to form a k -GDD of type d N - (d )1 . Adjoin d - s infinite elements to the
c

1446

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

ITD(k, d( N -  ) + s, s). On each group together with the infinite elements, place a copy of the k GDD of type d N - (d )1 , aligning the group of size d on the s elements in the intersection of the group and the hole of the ITD, together with the d - s infinite elements. The result is a k GDD of type dk( N - ) (d + s(k - 1))1 . Treat this as a packing. On the dk( N -  ) points not in the large hole, the leave has degree d - 1, so the result is a k -OP(nc + hc , hc ) with nc = dk( N -  ) and hc = d + s(k - 1). Because dk  0 (mod k) and N -   0 (mod k - 1), nc  0 (mod k(k - 1)). Because d  d (mod k(k - 1)), hc  c (mod k(k - 1)). 2 4.3. Covering: v  1 (mod k - 1) We employ some further, more specialized, combinatorial objects to treat coverings for the remaining congruence classes. Let V be a set of elements; B be a set of k -subsets of V ; G = {G 1 , . . . , G r } be a partition of V , j t. and H = { H 1 , . . . , H t } be a partition of V . Suppose that |G i  H j | =  for all 1 i r , 1
j i Further suppose that for every 2-subset {x, y }  V , either {x, y }  , or there  i =1 2 j =1 2 is exactly one B  B with {x, y }  B , but not both. Then ( V , G , H, B ) is a double group divisible design with blocksize k (k -DGDD) of type (r )t . A holey transversal design with blocksize k (k -HTD) of type r is a k -DGDD of type (r )k .

r

G

t

H

Theorem 4.6. Let k

2 be an integer. For all sufficiently large r, there exists a k-HTD of type 2r .

Proof. Choose K = {x1 , . . . , xs } so that  ( K ) = ( K ) = 1, and so that for each 1 i s , xi is large enough to ensure that Theorem 2.6 yields a TD(k + 1, xi ). Remove one group (and rename elements as needed) to form an idempotent TD(k, xi ). When r is large enough, Theorem 2.4 yields a K -GDD ( V , G , B ) of type 2r with groups G = {G 1 , . . . , G r }. The elements of the k-HTD to be formed are V × {0, . . . , k - 1}. For each block B  B , on the elements B × {0, . . . , k - 1}, align the k groups on { B × {i }: 0 i < k} to place the blocks of an idempotent TD(k, | B |). In the resulting design, one set of j r. 2 groups is formed by V × {i } for 0 i < k , the other by G j × {0, . . . , k - 1} for 1 Theorem 4.7. Let k 2 be an integer. For all sufficiently large integers r and t satisfying t - 1  0 (mod k - 1) t k and 2  0 mod 2 , there exists a k-DGDD of type (2r )t . Proof. Apply Theorem 2.1 to form a k -BIBD ( V , B ) with t elements. Apply Theorem 4.6 to form a k -HTD of type 2r . To form the k -DGDD, use elements V × {a, b} × {1, . . . , r }. For every B  B , place a copy of the HTD on B × {a, b} × {1, . . . , r }, aligning groups of size 2k on B × {a, b} × {i } for 1 i r , and groups of size 2r on {x} × {a, b} × {1, . . . , r } for x  B . 2 The key construction follows: Theorem 4.8. Let t , r , y be positive integers so that r  0 (mod k(k - 1)), t  1 (mod k(k - 1)), and y  2 (mod k - 1). Suppose that there exist (1) a k-DGDD of type (2r )t ; (2) a k-BIBD on 2t + k - 2 elements; (3) a k-OC(2r + y , y ). Then there is a k-OC(2rt + k - 2 + y , 2r + y + k - 2). Proof. Let V = {ai , j , b i , j : 1 i r , 1 j t } be the elements of the k -DGDD, with groups aligned j t } and H j = {ai , j , b i , j : 1 i r }. Let B be its set of blocks. Adjoin a so that G i = {ai , j , b i , j : 1 set C of k - 2 new elements. For 1 i r , on C  G i , form a k -BIBD on 2t + k - 2 elements, aligning a block on C  {ait , b it }; then delete that block, and call the resulting set of blocks Di . Adjoin a set R

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1447

with y further new elements. For 1 j < t , on R  H j place a k -OC(2r + y , y ) with the hole aligned on R , whose block set is E j . r We consider the design on the 2rt + k - 2 + y elements V  R  C with block set B  i =1 Di  All blocks have size k because each ingredient contains only blocks of size k . First we show that the design is a covering with a hole on R  C  H t . Two elements in the hole do not appear together in a block. An element from G i  H j with j < t appears in a block with each element of C  (G i  H t ) in Di ; it appears in a block with each element of R in E j ; and it appears with each element of H t \ G i in a block of B . Consider two distinct elements x  G i  H j and y  G m  H n with j , n < t . If i = m and j = n, then {x, y } = {ai , j , b i , j } appears in a block of Di (and also in at least one block of E j ). If i = m and j = n, then {x, y } appears in at least one block of E j . If i = m and j = n, then {x, y } appears in one block of Di . If i = m and j = n, then {x, y } appears in one block of B . Hence the design is a covering with a hole on R  C  H t . Secondly, we establish that it has the correct excess degrees to be an optimal covering with a hole, a k -OC. The design has 2rt + k - 2 + y elements. Because r  0 (mod k - 1), the number of elements satisfies 2rt + k - 2 + y  y - 1 (mod k - 1). The hole has 2r + k - 2 + y elements. Because r  0 (mod k - 1), the number of elements in the hole satisfies 2r + k - 2 + y  y - 1 (mod k - 1). y < k - 1. We must show that every element not in the Let y  -( y - 1) (mod k - 1) with 0 hole has degree y + 1 in the excess, and every element in the hole has degree 0 in the excess. r }. It We treat elements in the hole first. Each element of C appears only in blocks {Di : 1 i appears in r (t - 1) pairs to be covered, and appears in r (t - 1)/(k - 1) blocks, with (t - 1)/(k - 1) blocks arising in each of {Di : 1 i r } because this was constructed from a BIBD. Each element j < t }. Because elements of R have excess degree 0 in the k of R appears only in blocks {E j : 1 OC(2r + y , y ) forming E j , they have excess degree 0 in the union. Each element of H t appears only in blocks of B , and has excess degree 0. Now consider an element x  G i  H j , with j = t so that x is not in the hole. Then x appears in elements of B , Di and E j . It appears in 2(r - 1)(t - 1)/(k - 1) blocks of B , because it arises from the DGDD. It appears in (2t + k - 3)/(k - 1) blocks of Di , because it arises from a BIBD. Now in E j , x is not in the hole of the k -OC(2r + y , y ), and hence it arises 1 (2rt + k - 2 + y + y ) blocks, and in (2r - 1 + y + y )/(k - 1) blocks. So in total x appears in k- 1 because it appears in (2rt + k - 2 + y ) - 1 pairs, its excess degree is y + 1. Because 2r + y + k - 2  y - 1 (mod k - 1) and y  2 (mod k - 1), the result is the k -OC(2rt + k - 2 + y , 2r + y + k - 2). 2 Corollary 4.9. For each 0 c < k(k - 1), there exist integers mc and c  c (mod k(k - 1)) for which a k-OC(mc + c , c ) exists.
c t -1 j =1 E j .

with mc  0 (mod k(k - 1)) and

Proof. Let t 0 and r0 be integers with t 0  1 (mod k(k - 1)) and t 0 > 1 so that whenever r and t  1 (mod k(k - 1)), (1) there is a k -DGDD of type (2r )t (apply Theorem 4.7), and (2) there is a k -BIBD on 2t + k - 2 ( k (mod k(k - 1))) elements (apply Theorem 2.1).

r0 , t

t0 ,

When c  1 (mod k - 1), apply Corollary 4.4 to choose one k -OC(mc + c , c ) with mc  0 (mod k(k - 1)) and mc r0 . In general, when a k -OC(mc + c , c ) with mc  0 (mod k(k - 1)) and mc r0 exists, Theorem 4.8 produces a k -OC(mc t 0 + k - 2 + c , mc + c + k - 2). Set mc +k-2 mod k(k-1) = mc (t 0 - 1), which exceeds r0 and is a multiple of k(k - 1). Set c +k-2 mod k(k-1) = mc + c + k - 2  c + k - 2 (mod k(k - 1)). Then k - 2 applications of Theorem 4.8 handle all congruence classes. 2 5. Asymptotic existence Our next task is to handle not just one example for hole size in each congruence class modulo k(k - 1), but to extend to all sufficiently large orders. Theorem 5.1. Let k 2 be an integer. Then there are constants k and uk so that whenever v k-OP( v , h) and a k-OC( v , h) with h uk .

k , there is a

1448

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

Proof. By Corollary 4.9, for 0 c < k(k - 1) there are integers mc  0 (mod k(k - 1)) and c  c (mod k(k - 1)) for which a k -OC(mc + c , c ) exists. By Lemma 4.5, for 0 c < k(k - 1) there are integers nc  0 (mod k(k - 1)) and hc  c (mod k(k - 1)) for which a k -OP(nc + hc , hc ) exists. Set uk = max{nc + hc , mc + c : 0 c < k(k - 1)}. Using Theorem 2.4, choose an integer x for which k -GDDs of type (k(k - 1))x exist for all x  {x, x + 1, x + 2, x + 3}. Again using Theorem 2.4, choose an integer r for which an {x + 1, x + 2, x + 3}p uk and all g r . Then set k = rk(k - 1) + uk , a constant GDD of type p g exists for all 1 depending only on k . We develop the remainder of the proof for packings; that for coverings parallels it very closely. Let v k be an integer, and write v =  k(k - 1) + c with 0 c < k(k - 1). Write v - hc = gnc + d so that d  0 (mod k(k - 1)) and d < nc . Let n = nc /(k(k - 1)) and d = d/(k(k - 1)). g Construct a k -GDD of type nc d1 as follows. Form an {x + 1, x + 2, x + 3}-GDD of type (n ) g +1 . Delete all but d elements in one group to form an {x, x + 1, x + 2, x + 3}-GDD of type (n ) g (d )1 . Inflate using weight k(k - 1), employing k -GDDs of type (k(k - 1))x for x  {x, x + 1, x + 2, x + 3}, to g form a k -GDD of type nc d1 . Then add hc new elements, and place a k -OP(nc + hc , hc ) on each group of size nc together with the hc new elements, aligning the hole on these hc elements. The result is a k -OP( v , hc + d), and hc + d uk as required. 2 Proof of Theorem 3.1. When v < k , a maximum packing with blocksize k contains at least v ,k - 2k  blocks, and 2k is a constant. When v k , form a k -OP( v , h) with h uk , which has at least v ,k - uk uk blocks and is a constant. 2 2 2 Proof of Theorem 3.2. When v < k , a minimum covering with blocksize k requires at most 2k blocks, which is a constant. When v k , form a k-OC( v , h) with h uk , which has at most L v ,k blocks. A covering on h points in which every block contains some pair that is covered only once has u at most 2k blocks, which is a constant independent of v . Use this to fill the hole. 2 6. Conclusion For t = 2, our results establish that the elementary Johnson and Schönheim bounds are essentially the correct ones, in that the respective optima cannot differ from them by more than an additive constant. Unless this constant can be shown to be quite small, the specific value obtained for the constant is not of particular interest. Without recourse to Proposition 1.1 or a similar statement, we see no way at present to obtain differences from the bounds that are bounded by a quantity as small as (say) k in general, although it is plausible that such bounds hold. Acknowledgment We thank an anonymous referee for helpful comments on the presentation. References
[1] T. Beth, D. Jungnickel, H. Lenz, Design Theory, vol. I, second edition, Encyclopedia Math. Appl., vol. 69, Cambridge University Press, Cambridge, 1999. [2] Y. Caro, R. Yuster, Packing graphs: the packing problem solved, Electron. J. Combin. 4 (1) (1997), Research Paper 1, approx. 7 pp. (electronic). [3] Y. Caro, R. Yuster, Covering graphs: the covering problem solved, J. Combin. Theory Ser. A 83 (2) (1998) 273­282.  E.G. Straus, On the maximal number of pairwise orthogonal Latin squares of a given order, Canad. J. [4] S. Chowla, P. Erdos, Math. 12 (1960) 204­208. [5] C.J. Colbourn, J.H. Dinitz, D.R. Stinson, Applications of combinatorial designs to communications, cryptography, and networking, in: Surveys in Combinatorics, Canterbury, 1999, in: London Math. Soc. Lecture Note Ser., vol. 267, Cambridge Univ. Press, Cambridge, 1999, pp. 37­100. [6] C.J. Colbourn, V. Rödl, Percentages in pairwise balanced designs, Discrete Math. 77 (1­3) (1989) 57­63. [7] C.J. Colbourn, P.C. van Oorschot, Applications of combinatorial designs in computer science, ACM Comput. Surv. 21 (2) (1989) 223­250. [8] P. Dukes, A.C.H. Ling, Asymptotic existence of resolvable graph designs, Canad. Math. Bull. 50 (4) (2007) 504­518.





Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440­1449

1449

[9] [10] [11] [12] [13] [14] [15] [16] [17] [18] [19] [20] [21] [22] [23] [24]

[25] [26]

 H. Hanani, On a limit theorem in combinatorial analysis, Publ. Math. Debrecen 10 (1963) 10­13. P. Erdos, P. Frankl, V. Rödl, Near perfect coverings in graphs and hypergraphs, European J. Combin. 6 (4) (1985) 317­326. D.A. Grable, More-than-nearly-perfect packings and partial designs, Combinatorica 19 (2) (1999) 221­239. T. Gustavsson, Decompositions of large graphs and digraphs with high minimum degree, PhD thesis, Dept. of Mathematics, Univ. of Stockholm, 1991. S.M. Johnson, A new upper bound for error-correcting codes, IRE Trans. IT-8 (1962) 203­207. A.V. Kostochka, V. Rödl, Partial Steiner systems and matchings in hypergraphs, Random Structures Algorithms 13 (3­4) (1998) 335­347. N.N. Kuzjurin, On the difference between asymptotically good packings and coverings, European J. Combin. 16 (1) (1995) 35­40. E.R. Lamken, R.M. Wilson, Decompositions of edge-colored complete graphs, J. Combin. Theory Ser. A 89 (2) (2000) 149­ 200. J. Liu, Asymptotic existence theorems for frames and group divisible designs, J. Combin. Theory Ser. A 114 (3) (2007) 410­420. H. Mohácsy, The asymptotic existence of group divisible designs of large order with index one, J. Combin. Theory Ser. A 118 (7) (2011) 1915­1924. H. Mohácsy, D.K. Ray-Chaudhuri, An existence theorem for group divisible designs of large order, J. Combin. Theory Ser. A 98 (1) (2002) 163­174. V. Rödl, On a packing and covering problem, European J. Combin. 6 (1) (1985) 69­78. J. Schönheim, On coverings, Pacific J. Math. 14 (1964) 1405­1411. D.R. Stinson, Combinatorial Designs, Springer-Verlag, New York, 2004. V.H. Vu, New bounds on nearly perfect matchings in hypergraphs: higher codegrees do help, Random Structures Algorithms 17 (1) (2000) 29­63. R.M. Wilson, The construction of group divisible designs and partial planes having the maximum number of lines of a given size, in: Proc. Second Chapel Hill Conf. on Combinatorial Mathematics and its Applications, Univ. North Carolina, Chapel Hill, NC, 1970, Univ. North Carolina, Chapel Hill, NC, 1970, pp. 488­497. R.M. Wilson, An existence theory for pairwise balanced designs. III. Proof of the existence conjectures, J. Combin. Theory Ser. A 18 (1975) 71­79. R.M. Wilson, The proportion of various graphs in graph designs, in: R.A. Brualdi, S. Hedayat, H. Kharaghani, G. Khosrovshahi, S. Shahriari (Eds.), Combinatorics and Graphs: The Twentieth Anniversary Conference of IPM Combinatorics, American Mathematical Society, Providence, RI, 2010, pp. 251­255.

Test Algebra for Combinatorial Testing
Wei-Tek Tsaiâˆ— , Charles J. Colbournâˆ—â€  , Jie Luoâ€  , Guanqiu Qiâˆ— , Qingyang Liâˆ— , Xiaoying Baiâ€¡
âˆ— School

of Computing, Informatics, and Decision Systems Engineering
Arizona State University, Tempe, AZ, USA
â€  State Key Laboratory of Software Development Environment
School of Computer Science and Engineering,
Beihang University, Beijing, China
â€¡ Department of Computer Science and Technology, INLIST
Tsinghua University, Beijing, China
{wtsai, colbourn}@asu.edu, luojie@nlsde.buaa.edu.cn
{guanqiuq, qingyan2b}@asu.edu, baixy@tsinghua.edu.cn

Abstractâ€”This paper proposes a new algebraic system, Test
Algebra (T A), for identifying faults in combinatorial testing for
SaaS (Software-as-a-Service) applications. SaaS as a part of cloud
computing is a new software delivery model, and mission-critical
applications are composed, deployed, and executed in cloud
platforms. Testing SaaS applications is a challenging task because
new applications need to be tested when they are composed
before they can be deployed for execution. Combinatorial testing
algorithms can be used to identify faulty configurations and
interactions from 2-way all the way to k-way where k is the
number of components in the application. The T A defines rules
to identify faulty configurations and interactions. Using the rules
defined in the T A, a collection of configurations can be tested
concurrently in different servers and in any order and the results
obtained will be still same due to the algebraic constraints.
Index Termsâ€”Combinatorial testing, algebra, SaaS

I. I NTRODUCTION
Software-as-a-Service (SaaS) is a new software delivery
model. SaaS often supports three features: customization,
multi-tenancy architecture (MTA), and scalability. MTA means
using one code base to develop multiple tenant applications,
and each tenant application essentially is a customization
of the base code [12]. A SaaS system often also supports
scalability as it can supply additional computing resources
when the workload is heavy. Tenantsâ€™ applications are often
customized by using components stored in the SaaS database
[14], [1], [11] including GUI, workflow, service, and data
components.
Once tenant applications are composed, they need to be tested, but a SaaS system can have millions of components, and
hundreds of thousands of tenant applications. Testing tenant
applications becomes a challenge as new tenant applications
and components are added into the SaaS system continuously.
New tenant applications are added on a daily basis while other
tenant applications are running on the SaaS platform. As new
tenant applications are composed, new components are added
into the SaaS system. Each tenant application represents a
customer for the SaaS system, and thus it needs to be tested.
Combinatorial testing is a popular testing technique to test
an application with different configurations. It often assumes
that each component within a configuration has been tested

already, but interactions among components in the configuration may cause failures. Traditional combinatorial testing
techniques often focus on test case generation to detect the
presence of faults, but fault location is an active research area.
Each configuration needs to be tested, as each configuration
represents a tenant application. Traditional combinatorial testing such as AETG [2] can reveal the existence of faults by
using few test cases to support t-way coverage for t â‰¥ 2. But
knowing the existence of a fault does not indicate which tway interactions are faulty. When the problem size is small,
an engineer can identify faults. However, when the problem
is large, it can be a challenge to identify faults.
As the IT industries moves to Big Data and cloud computing where hundreds of thousand processors are available,
potentially, a large number of processors with distributed
databases can be used to perform large-scale combinatorial testing. Furthermore, they also provide concurrent and
asynchronous computing mechanisms such as MapReduce,
automated redundancy and recovery management, automated
resource provisioning, and automated migration for scalability.
These capabilities provide significant computing power that
was not available before. One simple way of performing
combinatorial testing in a cloud environment is:
1) Partition the testing tasks;
2) Allocate these testing tasks to different processors in the
cloud platform for test execution;
3) Collect results done by these processors.
However, this is not efficient as while the number of computing
and storage resources have increased significantly, the number
of combinations to be considered is still too high. For example,
a large SaaS system may have millions of components, and
testing all of these combinations can still consume all the
resources in a cloud platform. Two ways to improve this
approach and both are based on learning from the previous
test results:
â€¢

â€¢

Devise a mechanism to merge test results from different
processors so that testing results can be merged quickly,
and detect any inconsistency in testing;
Based on the existing testing results, eliminate any con-

figurations or interactions from future testing.
Due to the asynchronous and autonomous nature of cloud
computing, test results may arrive asynchronously and autonomously, and this necessitates a new testing framework.
This paper proposes a new algebraic system, Test Algebra
(TA), to facilitate concurrent combinatorial testing. The key
feature of TA is that the algebraic rules follow the combinatorial structure, and thus can track the test results obtained. The
TA can then be used to determine whether a tenant application
is faulty, and which interactions need to be tested. The TA
is an algebraic system in which elements and operations are
formally defined. Each element represents a unique component
in the SaaS system, and a set of components represents a
tenant application. Assuming each component has been tested
by developers, testing a tenant application is equivalent to
ensuring that there is no t-way interaction faults for t â‰¥ 2
among the elements in a set.
The TA uses the principle that if a t-way interaction is
faulty, every (t + 1)-way interaction that contains the t-way
interaction as a subset is necessarily faulty. The TA provides
guidance for the testing process based on test results so far.
Each new test result may indicate if additional tests are needed
to test a specific configuration. The TA is an algebraic system,
primarily intended to track the test results without knowing
how these results were obtained. Specifically, it does not
record the execution sequence of previously executed test
cases. Because of this, it is possible to allocate different
configurations to different processors for execution in parallel
or in any order, and the test results are merged following
the TA rules. The execution order and the merge order do
not affect the merged results if the merging follows the TA
operation rules.
This paper is structured as follows: Section II discusses the
related work; Section III proposes TA and shows its details;
and Section IV concludes this paper. Appendix provides proofs
of TA associativity properties.
II. R ELATED W ORK
SaaS testing is a new research topic [14], [4], [10]. Using
policies and metadata, test cases can be generated to test
SaaS applications. Testing can be embedded in the cloud
platform where tenant applications are run [14]. Gao proposed
a framework for testing cloud applications [4], and proposed
a scalability measure for testing cloud application scalability.
Another scalability measure was proposed by [10].
Testing all combinations of inputs and preconditions is not
feasible, even with a simple product [6], [8]. The number
of defects in a software product can be large, and defects
occurring infrequently are difficult to find [15]. Combinatorial
test design is used to identify a small number of tests needed
to get the coverage of important combinations. Combinatorial
test design methods enable one to build structure variation
into test cases for having greater test coverage with fewer test
cases.
Determining the presence of faults caused by a small
number of interacting elements has been extensively studied

in component-based software testing. When interactions are
to be examined, testing involves a combination-based strategy
[5]. When every interaction among t or fewer elements is to
be tested, methods have been developed that provide pairwise
or t-way coverage. Among the early methods, AET G [2]
popularized greedy one-test-at-a-time methods for constructing
test suites. In the literature, the test suite is usually called a
covering array, defined as follows. Suppose that there are k
configurable elements, numbered from 1 to k. Suppose that
for element c, there are vc valid options. A t-way interaction
is a selection of t of the k configurable elements, and a valid
option for each. A test selects a valid option for every element,
and it covers a t-way interaction if, when one restricts the
attention to the t selected elements, each has the same option
in the interaction as it does in the test.
A covering array of strength t is a collection of tests so
that every t-way interaction is covered by at least one of the
tests. Covering arrays reveal faults that arise from improper
interaction of t or fewer elements [9]. There are numerous
computational and mathematical approaches for construction
of covering arrays with a number of tests as small as possible
[3], [7].
If a t-way interaction causes a fault, then executing all
tests of a covering array will reveal the presence of at least
one faulty interaction. SaaS testing is interested in identifying
those interactions that are faulty including their numbers and
locations, as faulty configurations cannot be used in tenant
applications. Furthermore, the number and location of faults
keep on changing as new components can be added into
the SaaS database continuously. By then executing each test,
certain interactions are known not to be faulty, while others
appear only in tests that reveal faults, and hence may be faulty.
At this point, a classification tree analysis builds decision trees
for characterizing possible sets of faults. This classification
analysis is then used either to permit a system developer to
focus on a small collection of possible faults, or to design
additional tests to further restrict the set of possible faults.
In [16], empirical results demonstrate the effectiveness of
this strategy at limiting the possible faulty interactions to a
manageable number. Assuming that interactions of more than
t elements do not produce faults, a covering array can use few
tests to certify that no fault arises from a t-way interaction.
The Adaptive Reasoning algorithm (AR) is a strategy to
detect faults in SaaS [13]. The algorithm uses earlier test
results to generate new test cases to detect faults in tenant
applications. It uses three principles:
â€¢

â€¢

â€¢

Principle 1: When a tenant application (or configuration)
fails the testing, there is at least one fault (but there may
be more) in the tenant configuration.
Principle 2: When a tenant application passes the testing,
there is no fault in the tenant configuration resulting
from a t-way interactions among components in the
configuration.
Principle 3: Whenever a configuration contains one or
more faulty interactions, it is faulty.

III. T EST A LGEBRA
Let C be a finite set of components. A configuration is
a subset T âŠ† C. One is concerned with determining the
operational status of configurations. To do this, one can
execute certain tests; every test is a configuration, but there
may be restrictions on which configurations can be used as
tests. If a certain test can be executed, its execution results in
an outcome of passed (operational) or failed (faulty).
When a test execution yields result, all configurations that
are subsets of the test are operational. However, when a test
execution yields a faulty result, one only knows that at least
one subset causes the fault, but it is unclear which of these
subsets caused the failure. Among a set of configurations that
may be responsible for faults, the objective is to determine,
which cause faults and which do not. To do this, one must
identify the set of candidates to be faulty. Because faults
are expected to arise from an interaction among relatively
few components, one considers t-way interactions. The t-way
interactions are It = {U âŠ† C : |U | = t}. Hence the goal is to
select tests, so that from the execution results of these tests,
one can ascertain the status of all t-way interactions for some
fixed small value of t.
Because interactions and configurations are represented as
subsets, one can use set-theoretic operations such as union, and
their associated algebraic properties such as commutativity,
associativity, and self-absorption. The structure of subsets and
supersets also plays a key role.
To permit this classification, one can use a valuation function V , so that for every subset S of components, V (S)
indicates the current knowledge about the operational status
consistent with the components in S. The focus is on determining V (S) whenever S is an interaction in I1 âˆª Â· Â· Â· âˆª It .
These interactions can have one of five states.
â€¢ Infeasible (X): For certain interactions, it may happen
that no feasible test is permitted to contain this interaction. For example, it may be infeasible to select two GUI
components in one configuration such that one says the
wall is GREEN but the other says RED.
â€¢ Faulty (F): If the interaction has been found to be faulty.
â€¢ Operational (P): Among the rest, if an interaction has
appeared in a test whose execution gave an operational
result, the interaction cannot be faulty.
â€¢ Irrelevant (N): For some feasible interactions, it may
be the case that certain interactions are not expected to
arise, so while it is possible to run a test containing the
interaction, there is no requirement to do so.
â€¢ Unknown (U): If neither of these occurs then the status
of the interaction is required but not currently known.
Any given stage of testing, an interaction has one of five
possible status indicators. These five status indicators are
ordered by X  F  P  N  U under a relation , and
it has a natural interpretation to be explained in a moment.
A. Learning from Previous Test Results
The motivation for developing an algebra is to automate
the deduction of the status of an interaction from the status

of tests and other interactions, particularly in combining the
status of two interactions. Specifically, one is often interested
in determining V (T1 âˆª T2 ) from V (T1 ) and V (T2 ). To do this,
a binary operation âŠ— on {X, F, P, N, U} can be defined, with
operation table as follows:
âŠ—
X
F
P
N
U

X
X
X
X
X
X

F
X
F
F
F
F

P
X
F
U
N
U

N
X
F
N
N
N

U
X
F
U
N
U

Using this definition, one can verify that the binary operation âŠ— has the following properties of commutativity and
associativity.
V (T1 ) âŠ— V (T2 ) = V (T2 ) âŠ— V (T1 ),
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
Using this operation, one observes that V (T1 âˆª T2 ) 
V (T1 ) âŠ— V (T2 ). It follows that
1) Every superset of an infeasible interaction is infeasible.
2) Every superset of a failed interaction is failed or infeasible.
3) Every superset of an irrelevant interaction is irrelevant,
failed, passed, or infeasible.
A set S is an X-implicant if V (S) = X but whenever
S 0 âŠ‚ S, V (S 0 ) â‰º X. The X-implicants provide a compact
representation for all interactions that are infeasible. Indeed
for any interaction T that contains an X-implicant, V (T ) = X.
Furthermore, a set S is an F-implicant if V (S) = F but
whenever S 0 âŠ‚ S, V (S 0 ) â‰º F. For any interaction T that
contains an F-implicant, V (T )  F. In the same way, a set S is
an N-implicant if V (S) = N but whenever S 0 âŠ‚ S, V (S 0 ) = U.
For any interaction T that contains an N-implicant, V (T )  N.
An analogous statement holds for passed interactions, but here
the implication is for subsets. A set S is a P-implicant if
V (S) = P but whenever S 0 âŠƒ S, V (S 0 )  F. For any
interaction T that is contained in a P-implicant, V (T ) = P.
Implicants are defined with respect to the current knowledge
about the status of interactions. When a t-way interaction is
known to be infeasible, failed, or irrelevant, it must contain
an X-, F-, or N-implicant. By repeatedly proceeding from tway to (t + 1)-way interactions, then, one avoids the need
for any tests for (t + 1)-way interactions that contain any
infeasible, failed, or irrelevant t-way interaction. Hence testing
typically proceeds by determining the status of the 1-way
interactions, then proceeding to 2-way, 3-way, and so on.
The operation âŠ— is useful in determining the implied status
of (t + 1)-way interactions from the computed results for
t-way interactions, by examining unions of the t-way and
smaller interactions and determining implications of the rule
that V (T1 âˆª T2 )  V (T1 ) âŠ— V (T2 ). Moreover, when adding
further interactions to consider, all interactions previously
tested that passed are contained in a P-implicant, and every
(t + 1) interaction contained in one of these interactions can
be assigned status P.

For example, V (a, b) = P, V (a, e) = X, and other 2way interactions exist as atomic interaction in TA. Based
on the defined âŠ— operation, values of t-way interactions can
be deduced from the atomic interactions and their contained
interactions, such as V (a, b, e)  V (a, b) âŠ— V (a, e) = X, i.e.
V (a, b, e) = X.
The 3-way interaction (a, b, c) can have inferred results from
2-way interactions (a, b), (a, c), (b, c). If any contained 2-way
interaction has value F, the determining value of 3-way is F,
without further testing needed. But if all values of contained
2-way interactions are P, (a, b, c) the interaction needs to be
tested. In this case, U needs to be changed to non-U such as F
or P, assuming the 3-way is not X or N.
B. Changing Test Result Status
When testing a configuration with n components, one
should test individual components, 2-way interactions, 3-way
interactions, all the way to n-way interactions. Since any
combination of interactions is relevant in this case, the status
of any interaction can be either X, F, P, or U. The status of a
configuration is determined by the status of all interactions.
1) If an interaction has status X (F), the configuration has
status X (F).
2) If all interactions have status P, the configuration has
status P.
3) If some interactions still have status U, further tests are
needed.
It is important to determine when an interaction with status
U can be deduced to have status F or P instead. It can never
obtain status X or N once having had status U.
To change U to P: An interaction is assigned status P if and
only if it is a subset of a test that leads to proper operation.
To change U to F: Consider the candidate T , one can
conclude that V (T ) = F if there is a test containing T that
yields a failed result, but for every other candidate interaction
T 0 that appears in this test, V (T 0 ) = P. In other words, the
only possible explanation for the failure is the failure of T .
C. Matrix Representation
Suppose that each individual component passed the testing.
Then the operation table starts from 2-way interactions, then
enlarges to t-way interactions step by step. During the procedure, many test results can be deduced from the existing results
following TA rules. For example, all possible configurations
of (a, b, c, d, e, f ) can be expressed in the form of matrix, or
operation table. First, we show the operation table for 2-way
interactions. The entries in the operation table are symmetric
and those on the main diagonal are not necessary. So only half
of the entries are shown.
As shown in Figure 1, 3-way interactions can be composed
by using 2-way interactions and components. Thus, following
the TA implication rules, the 3-way interactions operation table
is composed based on the results of 2-way combinations. Here,
(a, b, c, d, e, f ) has more 3-way interactions than 2-way
interactions. As seen in Figure 1, a 3-way interaction can be
obtained through different combinations of 2-way interactions

and components. For example, {a, b, c} = {a} âˆª {b, c} =
{b}âˆª{a, c} = {c}âˆª{a, b} = {a, b}âˆª{a, c} = {a, b}âˆª{b, c} =
{a, c} âˆª {b, c}. V (a) âŠ— V (b, c) = V (c) âŠ— V (a, b) = V (a, b) âŠ—
V (b, c) = PâŠ—P = U. But V (b)âŠ—V (a, c) = V (a, b)âŠ—V (a, c) =
V (b, c) âŠ— V (a, c) = P âŠ— F = F. As TA defines the order of
the five status indicators, the result should be the value with
highest order. So V (a, b, c) = F.
âŠ— a
a
b
c
d
e
f

b
P

c d e
F N X
P X N
F P
F

f
U
F
P
X
U

D. Merging Concurrent Testing Results
One way to achieve efficient testing is to allocate (overlapping or non-overlapping) tenant applications into different
clusters, and each cluster is sent to a different set of servers
for execution. Once each cluster completes its execution, the
test results can be merged. The testing results of a specific
interaction T in different servers should satisfy the following
constraints.
â€¢ If V (T ) = U in one cluster, then in other clusters, the
same V (T ) can be either F, P, N, or U.
â€¢ If V (T ) = N in one cluster, then in other clusters, the
same V (T ) can be either F, P, N, or U.
â€¢ If V (T ) = P in one cluster, then the same V (T ) can be
either P, N, or U in all clusters;
â€¢ If V (T ) = F in one cluster, then in other clusters, the
same V (T ) can be F, N, or U.
â€¢ If V (T ) = X in one cluster, then in other clusters, the
same V (T ) can be X only.
If these constraints are satisfied, then the testing results can
be merged. Otherwise, there must be an error in the testing
results. To represent this situation, a new status indicator, error
(E), is introduced and E  X. We define a binary operation âŠ•
on {E, X, F, P, N, U}, with operation table as follows:
âŠ•
E
X
F
P
N
U

E
E
E
E
E
E
E

X
E
X
E
E
E
E

F
E
E
F
E
F
F

P
E
E
E
P
P
P

N
E
E
F
P
N
U

U
E
E
F
P
U
U

âŠ• also has the properties of commutativity and associativity.
See Appendix for proof of associativity.
Using this operation, merging two testing results from two
different servers can be defined as Vmerged (T ) = Vcluster1 (T ) âŠ•
Vcluster2 (T ). The merge can be performed in any order due to
the commutativity and associativity of âŠ•, and if the constraints
of merge are satisfied and V (T ) = X, F, or P, the results cannot
be changed by any further testing or merging of test results
unless there are some errors in testing. If V (T ) = E, the testing

âˆª
a
b
c
..
.

a
(a)

b
(a, b)
(b)

c
(a, c)
(b, c)
(c)

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

f
(a, f )
(b, f )
(c, f )
..
.

(a, b)
(a, b)
(a, b)
(a, b, c)
..
.

(a, c)
(a, c)
(a, b, c)
(a, c)
..
.

(f )

(a, b, f )
(a, b)

(a, c, f ) Â· Â· Â·
(a, b, c) Â· Â· Â·
(a, c)
Â·Â·Â·
..
.

f
(a, b)
(a, c)
..
.
(b, c)
..
.

(b, c)
(a, b, c)
(b, c)
(b, c)
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

(b, c, f ) Â· Â· Â·
(a, b, c) Â· Â· Â·
(a, b, c) Â· Â· Â·
..
..
.
.
(b, c)
Â·Â·Â·
..
.

(e, f )
(a, e, f )
(b, e, f )
(c, e, f )
..
.
(e, f )
(a, b, e, f )
(a, c, e, f )
..
.
(b, c, e, f )
..
.
(e, f )

(e, f )
âŠ—
a
b
c
..
.
f
(a, b)
(a, c)
..
.

a

b
P

c
F
P

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

f
U
F
P
..
.

(a, b)
U
U
U
..
.

(a, c)
F
F
F
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

(b, c)
U
U
U
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

(e, f )
U
U
U
..
.

U

F
F

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

U
U
F
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

U
U
F
..
.

Â·Â·Â·
..
.

U
..
.

(b, c)
..
.
(e, f )
Fig. 1. 3-way interaction operation table

environment must be corrected and tests executed again after
fixing the error(s) in testing. For example, Vcluster1 (a, c, e) = X
and Vcluster2 (a, c, e) = F, so Vmerged (a, c, e) = X âŠ• F = E.
It means that there is something wrong with the tests of
interaction (a, c, e), and the problem must be fixed before
doing further testing.
Following the âŠ• associative rule, one can derive the following.
V1 (T ) âŠ• V2 (T ) âŠ• V3 (T )
= (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T )
= V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T ))
= V1 (T ) âŠ• V2 (T ) âŠ• V3 (T ) âŠ• V3 (T )
= (V1 (T ) âŠ• V2 (T )) âŠ• (V2 (T ) âŠ• V3 (T ))
= ((V1 (T ) âŠ• V2 (T )) âŠ• V2 (T )) âŠ• V3 (T )
= (V3 (T ) âŠ• V2 (T )) âŠ• (V3 (T ) âŠ• V1 (T ))
Thus the âŠ• rule allows one to partition the configurations
into different sets for different servers to run testing, and
these sets do not need to be non-overlapping. In conventional
cloud computing operations such as MapReduce, data should
not overlap, otherwise incorrect data may be produced. For
example, counting the items in a set can be performed by
MapReduce, but data allocated to different servers cannot
overlap, otherwise items may be counted more than once. In
TA, this is not a concern due to the nature of the TA operations

âŠ•. Once the results are available from each server, the testing
results can be merged either incrementally, in parallel, or in
any order. Furthermore, test results can be merged repeatedly
without changing the final results.
Using this approach, one can test say 10 batches of 100 tenant applications each, for a total of 1, 000 tenant applications.
Once this is done, another batch of 1, 000 tenant applications
can be tested with each 100 tenant application allocated to a
server for execution. In this way, after running 100 batches,
100, 000 tenant applications can be evaluated completely.
The following example illustrates the testing process of
fifteen configurations. According to the definition of configuration testing, all feasible interactions should be tested. For
simplicity, assume that only interaction (c, d, f ) is faulty, and
only interaction (c, d, e) is infeasible, and all other interactions
pass the testing. If one assigns configurations 1, 3, 5, 7, 9, 11,
13, 15 into Server1 , configurations 2, 4, 6, 8, 10, 12, 14 into
Server2 , and 4-11 configurations into Server3 .
If Server1 and Server3 do their own testing first, Server2
can reuse test results of interactions from them to eliminate
interactions that need to be tested. For example, when testing
2-way interactions of configuration (b, c, d, f ) in Server2 ,
it can reuse the test results of (b, c), (b, d) of configuration
(b, c, d, e) from Server3 , (b, f ) of configuration (a, b, c, f )

from Server1 . They are all passed, and it can reuse the test
results of (b, c, d) of configuration (a, b, c, d) from Server1 ,
(b, c, f ) of configuration (a, b, c, f ) from Server1 , (b, d, f )
of configuration (a, b, d, f ) from Server1 , and (c, d, f ) of
configuration (a, c, d, f ) from Server3 . Because (c, d, f ) is
faulty, it can deduce that 4-way interaction (b, c, d, f ) is also
faulty. For the sets of configuration that are overlapping, their
returned test results from different servers are the same. The
merged results of these results also stay the same.
Not only interactions, sets of configurations, CS1 , CS2 ,
. . . , CSK can be allocated to different processors (or clusters)
for testing, and the test results can then be merged. The sets
can be non-overlapping or overlapping, and the merge process
can be arbitrary. For example, say the result of CSi is RCSi ,
the merge process can be (Â· Â· Â· ((((RCS1 + RCS2 ) + RCS3 ) +
RCS4 ) + Â· Â· Â· + RCSK ), or (Â· Â· Â· ((((RCSK + RCSkâˆ’1 ) +
RCSkâˆ’2 ) + Â· Â· Â· + RCS1 ), or any other sequence that includes
all RCSi , for i = 1 to K. This is true because RCS is simply
a set of V (Tj ) for any intercation Tj in the configuration CSi .
(a,b,c,d)
(a,b,c,e)
(a,b,c,f)
(a,b,d,e)
(a,b,d,f)
(a,b,e,f)
(a,c,d,e)
(a,c,d,f)
(a,c,e,f)
(a,d,e,f)
(b,c,d,e)
(b,c,d,f)
(b,c,e,f)
(b,d,e,f)
(c,d,e,f)

Server1
P

Server2

Server3

P
P
P
P
P
X
F
P
P
X
F
P
P
X

P
P
P
X
F
P
P
X

IV. C ONCLUSION
This paper proposes TA to address SaaS combinatorial
testing. The TA provides a foundation for concurrent combinatorial testing. The TA has two operations and test results
can have five states with a priority. By using the TA operations, many combinatorial tests can be eliminated as the
TA identifies those interactions that need not be tested. Also
the TA defines operation rules to merge test results done by
different processors, so that combinatorial tests can be done in
a concurrent manner. The TA rules ensure that either merged
results are consistent or a testing error has been detected so
that retest is needed. In this way, large-scale combinatorial
testing can be carried out in a cloud platform with a large
number of processors to perform test execution in parallel to
identify faulty interactions.
ACKNOWLEDGMENT

This project is sponsored by U.S. National Science Foundation project DUE 0942453 and National Science Foundation
M erged Results China (No.61073003), National Basic Research Program of
China (No.2011CB302505), and the State Key Laboratory of
P
Software Development Environment (No. SKLSDE-2012ZXP
18), and Fujitsu Laboratory.
P
P
R EFERENCES
P
[1] X. Bai, M. Li, B. Chen, W.-T. Tsai, and J. Gao. Cloud Testing Tools. In
P
Proceedings of IEEE 6th International Symposium on Service Oriented
X
System Engineering (SOSE), pages 1â€“12, Irvine, CA, USA, 2011.
F
[2] D. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The
AETG System: An Approach to Testing Based on Combinatorial Design.
P
Journal of IEEE Transactions on Software Engineering, 23:437â€“444,
P
1997.
X
[3] C. J. Colbourn. Covering arrays and hash families. In D. Crnkovic and
V. D. Tonchev, editors, Information Security, Coding Theory and Related
F
Combinatorics, volume 29 of NATO Science for Peace and Security
P
Series - D: Information and Communication Security. IOS Press, 2011.
P
[4] J. Gao, X. Bai, and W.-T. Tsai. SaaS Performance and Scalability
Evaluation in Cloud. In Proceedings of The 6th IEEE International
X

E. Modified Testing Process
Perform 2-way interaction testing first. Before going on to 3way interaction testing, use the results of 2-way testing to eliminate cases. The testing process stops when finishing testing all
t-way interactions. The analysis of t-way interactions is based
on the P T Rs of all (t âˆ’ i)-way interactions for 1 â‰¤ i < t.
The superset of infeasible, irrelevant, and faulty test cases do
not need to be tested. The test results of the superset can be
obtained by TA operations and must be infeasible, irrelevant,
or faulty. But the superset of test cases with unknown indicator
must be tested. In this way, a large repeating testing workload
can be reduced.
For n components, all t-way interactions for t â‰¥ 2
are composed by 2-way, 3-way, ..., t-way interactions. In
n components combinatorial
 testing, the number of 2-way
interactions is equal to n2 . In general, the number of t-way
n
interactions
are treated when

is equal to t . More interactions
n
n
n
.
The
total number
>
,
which
happens
when
t
â‰¤
t
tâˆ’1
 2
Pt
of interactions examined is i=2 ni .

Symposium on Service Oriented System Engineering, SOSE â€™11, 2011.
[5] M. Grindal, J. Offutt, and S. F. Andler. Combination Testing Strategies:
A Survey. Software Testing, Verification, and Reliability, 15:167â€“199,
2005.
[6] C. Kaner, J. Falk, and H. Q. Nguyen. Testing Computer Software, 2nd
Edition. Wiley, New York, NY, USA, 1999.
[7] V. V. Kuliamin and A. A. Petukhov. A Survey of Methods for
Constructing Covering Arrays. Journal of Program Computer Software,
37(3):121â€“146, may 2011.
[8] T. Muller and D. Friedenberg. Certified Tester Foundation Level
Syllabus. Journal of International Software Testing Qualifications
Board.
[9] A. A. Porter, C. Yilmaz, A. M. Memon, D. C. Schmidt, and B. Natarajan.
Skoll: A Process and Infrastructure for Distributed Continuous Quality
Assurance. IEEE Transactions on Software Engineering, 33(8):510â€“525,
2007.
[10] W.-T. Tsai, Y. Huang, X. Bai, and J. Gao. Scalable Architecture for
SaaS. In Proceedings of 15th IEEE International Symposium on Object
Component Service-oriented Real-time Distributed Computing, ISORC
â€™12, Apr. 2012.
[11] W.-T. Tsai, Y. Huang, and Q. Shao. Testing the Scalability of SaaS Applications. In Proceedings of IEEE International Conference on ServiceOriented Computing and Applications (SOCA), pages 1â€“4, Irvine, CA,
USA, 2011.
[12] W.-T. Tsai, Y. Huang, Q. Shao, and X. Bai. Data Partitioning and
Redundancy Management for Robust Multi-Tenancy SaaS. International
Journal of Software and Informatics (IJSI), 4(3):437â€“471, 2010.

[13] W.-T. Tsai, Q. Li, C. J. Colbourn, and X. Bai. Adaptive Fault Detection
for Testing Tenant Applications in Multi-Tenancy SaaS Systems. In
Proceedings of IEEE International Conference on Cloud Engineering
(IC2E), March 2013.
[14] W.-T. Tsai, Q. Shao, and W. Li. OIC: Ontology-based Intelligent
Customization Framework for SaaS. In Proceedings of International
Conference on Service Oriented Computing and Applications(SOCAâ€™10),
Perth, Australia, Dec. 2010.
[15] Wikipedia. Software Testing, 2013.
[16] C. Yilmaz, M. B. Cohen, and A. Porter. Covering Arrays for Efficient
Fault Characterization in Complex Configuration Spaces. In Proceedings
of the 2004 ACM SIGSOFT International Symposium on Software
Testing and Analysis, ISSTA â€™04, pages 45â€“54, New York, NY, USA,
2004. ACM.

A PPENDIX
The associativity of binary operation âŠ—.
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
Proof: We will prove this property in the following cases.
(1) At least one of V (T1 ), V (T2 ), and V (T3 ) is X. Without
loss of generality, suppose that V (T1 ) = X, then according
to the operation table of âŠ—, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) =
X âŠ— (V (T2 ) âŠ— V (T3 )) = X, (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) =
(X âŠ— V (T2 )) âŠ— V (T3 ) = X âŠ— V (T3 ) = X. Thus, in this case,
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
(2) V (T1 ), V (T2 ), and V (T3 ) are not X and at least one
of V (T1 ), V (T2 ), and V (T3 ) is F. Without loss of generality,
suppose that V (T1 ) = F, then according to the operation table
of âŠ—, the value of V (T2 ) âŠ— V (T3 ) can only be F, N or U.
So V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = F âŠ— (V (T2 ) âŠ— V (T3 )) = F,
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) = (F âŠ— V (T2 )) âŠ— V (T3 ) = F âŠ—
V (T3 ) = F. Thus, in this case, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) =
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
(3) V (T1 ), V (T2 ), and V (T3 ) are not X or F and at least one
of V (T1 ), V (T2 ), and V (T3 ) is N. Without loss of generality,
suppose that V (T1 ) = N, then according to the operation table
of âŠ—, the value of V (T2 ) âŠ— V (T3 ) can only be N or U. So
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = N âŠ— (V (T2 ) âŠ— V (T3 )) = N,
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) = (N âŠ— V (T2 )) âŠ— V (T3 ) = N âŠ—
V (T3 ) = N. Thus, in this case, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) =
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
(4) V (T1 ), V (T2 ), and V (T3 ) are not X, F or N. In this case,
V (T1 ), V (T2 ), and V (T3 ) can only be P or U. According to the
operation table of âŠ—, the value of V (T1 )âŠ—V (T2 ) and V (T2 )âŠ—
V (T3 ) are U. So V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = V (T1 ) âŠ— U = U,
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) = U âŠ— V (T3 ) = U. Thus, in this
case, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
The associativity of binary operation âŠ•.
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ).
Proof: We will prove this property in the following cases.
(1) One of V1 (T ), V2 (T ), and V3 (T ) is E. Without loss
of generality, suppose that V1 (T ) = E, then according to the
operation table of âŠ•, V1 (T )âŠ•(V2 (T )âŠ•V3 (T )) = EâŠ—(V2 (T )âŠ•
V3 (T )) = E, (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = (E âŠ• V2 (T )) âŠ•
V3 (T ) = EâŠ•V3 (T ) = E. Thus, in this case, V1 (T )âŠ•(V2 (T )âŠ•
V3 (T )) = (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ).

(2) V1 (T ), V2 (T ), and V3 (T ) are not E, and there is a pair
of V1 (T ), V2 (T ), and V3 (T ) does not satisfy the constrains.
Without loss of generality, suppose that V1 (T ) and V2 (T ) does
not satisfy the constrains, then according to the operation table
of âŠ•, V1 (T ) âŠ• V2 (T ) = E. So (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) =
E âŠ• V3 (T ) = E. Since V1 (T ) and V2 (T ) does not satisfy the
constrains, there can be two cases: (a) one of them is X and
the other is not, or (b) one of them is P and the other is F.
(a) If V1 (T ) = X, then V2 (T ) âŠ• V3 (T ) cannot be X because
V2 (T ) cannot be X. Thus, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E. If
V2 (T ) = X, then V2 (T ) âŠ• V3 (T ) 6= X can only be E or X.
Since V1 (T ) cannot be X, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E.
(b) If V1 (T ) = P and V2 (T ) = F, then V2 (T ) âŠ• V3 (T )
can only be E or F. Thus, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E. If
V1 (T ) = F and V2 (T ) = P, then V2 (T ) âŠ• V3 (T ) can only be
E or P. Thus, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E.
Thus, in this case, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = (V1 (T ) âŠ•
V2 (T )) âŠ• V3 (T ).
(3) V1 (T ), V2 (T ), and V3 (T ) are not E, and V1 (T ), V2 (T ),
and V3 (T ) satisfy the constrains.
(a) One of V1 (T ), V2 (T ), and V3 (T ) is X. Without loss of
generality, suppose that V1 (T ) = X, then V2 (T ) = V3 (T ) = X.
So V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = X âŠ• (X âŠ• X) = X âŠ• X = X and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = (X âŠ• X) âŠ• X = X âŠ• X = X.
(b) V1 (T ), V2 (T ), and V3 (T ) are not X, and one of V1 (T ),
V2 (T ), and V3 (T ) is F. Without loss of generality, suppose
that V1 (T ) = F, then V2 (T ) and V3 (T ) can only be F, N,
or U. According to operation table of âŠ•, V2 (T ) âŠ• V3 (T ) can
only be F, N, or U, and V1 (T ) âŠ• V2 (T ) can only be F. So
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = F âŠ• (V2 (T ) âŠ• V3 (T )) = F and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = F âŠ• V3 (T ) = F.
(c) V1 (T ), V2 (T ), and V3 (T ) are not X or F, and one of
V1 (T ), V2 (T ), and V3 (T ) is P. Without loss of generality,
suppose that V1 (T ) = P, then V2 (T ) and V3 (T ) can only be
P, N, or U. According to operation table of âŠ•, V2 (T ) âŠ• V3 (T )
can only be P, N, or U, and V1 (T ) âŠ• V2 (T ) can only be F. So
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = P âŠ• (V2 (T ) âŠ• V3 (T )) = P and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = P âŠ• V3 (T ) = P.
(d) V1 (T ), V2 (T ), and V3 (T ) are not X, F or P, and one
of V1 (T ), V2 (T ), and V3 (T ) is U. Without loss of generality,
suppose that V1 (T ) = U, then V2 (T ) and V3 (T ) can only be
N, or U. According to operation table of âŠ•, V2 (T ) âŠ• V3 (T )
can only be N, or U, and V1 (T ) âŠ• V2 (T ) can only be U. So
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = U âŠ• (V2 (T ) âŠ• V3 (T )) = U and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = U âŠ• V3 (T ) = U.
(e) V1 (T ), V2 (T ), and V3 (T ) are N. V1 (T ) âŠ• (V2 (T ) âŠ•
V3 (T )) = N âŠ• (N âŠ• N) = N âŠ• N = N and (V1 (T ) âŠ• V2 (T )) âŠ•
V3 (T ) = (N âŠ• N) âŠ• N = N âŠ• N = N.
Thus, in this case, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = (V1 (T ) âŠ•
V2 (T )) âŠ• V3 (T ).

Gao SW, Lv JH, Du BL et al. Balancing frequencies and fault detection in the in-parameter-order algorithm. JOURNAL OF COMPUTER SCIENCE AND TECHNOLOGY 30(5): 957­968 Sept. 2015. DOI 10.1007/s11390-015-1574-6

Balancing Frequencies and Fault Detection in the In-Parameter-Order Algorithm
Shi-Wei Gao 1 ( ), Jiang-Hua Lv 1, ( 1 ) and Shi-Long Ma (
1 2

Ô å

ù×), Bing-Lei Du

1

(

), Charles J. Colbourn 2

State Key Laboratory of Software Development Environment, Beihang University, Beijing 100191, China School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe AZ 85287-8809, U.S.A.

E-mail: ge89@163.com; jhlv@nlsde.buaa.edu.cn; binglei.du@gmail.com; Charles.Colbourn@asu.edu E-mail: slma@nlsde.buaa.edu.cn Received March 16, 2015; revised June 29, 2015.
Abstract The In-Parameter-Order (IPO) algorithm is a widely used strategy for the construction of software test suites for combinatorial testing (CT) whose goal is to reveal faults triggered by interactions among parameters. Variants of IPO have been shown to produce test suites within reasonable amounts of time that are often not much larger than the smallest test suites known. When an entire test suite is executed, all faults that arise from t-way interactions for some fixed t are surely found. However, when tests are executed one at a time, it is desirable to detect a fault as early as possible so that it can be repaired. The basic IPO strategies of horizontal and vertical growth address test suite size, but not the early detection of faults. In this paper, the growth strategies in IPO are modified to attempt to evenly distribute the values of each parameter across the tests. Together with a reordering strategy that we add, this modification to IPO improves the rate of fault detection dramatically (improved by 31% on average). Moreover, our modifications always reduce generation time (2 times faster on average) and in some cases also reduce test suite size. Keywords combinatorial testing, IPO, test suite generation, expected time to fault detection, software under test

1

Introduction

Modern software systems are highly configurable. Their behavior is controlled by many parameters. Interactions among these parameters may cause severe failures, resulting in poor reliability. Therefore, software testing and reliability assessment are crucial in the design of effective software, as discussed in [1-3] for reliability and in [4-15] for software testing. Software testing serves two main purposes: 1) to ensure that software has as few errors as possible prior to release, and 2) to detect and isolate faults in the software. A generic model of such a software system identifies a finite set of parameters, and a finite set of possible values

for each parameter. Faults may arise due to a choice of a value for a single parameter, interactions among the values of a subset of the parameters, or a result of environmental conditions not included in the software model. We focus on the faults that arise from the parameters identified and the interactions among them. It is nearly always impractical to exhaustively test all combinations of parameter values because of resource constraints. Fortunately, this is not necessary in general: in some real software systems, more than 70 percent of faults are caused by interactions between two parameters[16], and all known faults are caused by interactions among six or fewer parameters[17-18] .

Regular Paper Special Section on Software Systems This work was supported by the National Natural Science Foundation of China under Grant Nos. 61300007 and 61305054, the Fundamental Research Funds for the Central Universities of China under Grant Nos. YWF-15-GJSYS-106 and YWF-14-JSJXY-007, and the Project of the State Key Laboratory of Software Development Environment of China under Grant Nos. SKLSDE-2015ZX-09 and SKLSDE-2014ZX-06.  Corresponding Author ©2015 Springer Science + Business Media, LLC & Science Press, China

958

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

For these reasons, combinatorial testing (CT) or tway testing chooses a strength t (the largest number of parameters interacting to cause a fault), and forms a software interaction test suite as follows. Every row of the test suite is a test or a test case. For each parameter in the system, each test specifies an admissible value for the parameter. The defining property is that, no matter how one chooses t parameters and an admissible value for each (a t-way interaction), at least one test has the specified parameters set to the indicated values. This coverage property ensures that every possible interaction among t or fewer parameter values must arise in at least one of the test cases. CT has proved to be an efficient testing technique for software[6,9,19]. Indeed, empirical studies have shown that t-way testing can effectively detect faults in various applications[17-18,20-22] . A primary objective in producing a test suite is to minimize the cost of executing the tests; hence minimizing the number of tests is desired. At the same time, however, the time to produce the test suite is also crucial. Hence the most effort has been invested in finding a variety of test suite generation algorithms. Some invest additional computational resources in minimizing the size of the test suite, while others focus on fast generation methods for test suites of acceptable but not minimum size. General methods providing fast generation have primarily involved greedy algorithms[9]. One-test-at-a-time methods start with an empty test suite, and keep track of the as-yetuncovered t-way interactions. Then repeatedly a test is selected, which attempts to maximize the number of such interactions that are covered by the test, until all interactions are covered. This strategy was pioneered in AETG[23] , and later proved to be within a constant factor of the optimal size[24-25] . In practice, maintaining a list of all t-way interactions can be prohibitive when the number of parameters is large. One-parameter-ata-time methods instead construct a test suite for t of the parameters (this contains all of the possible tests). Then it repeatedly adds a new parameter, and chooses a value for this parameter in each of the existing tests (horizontal growth). Because it is possible that some t-way interactions involving the new parameter have not been covered yet, further tests are selected to cover all such interactions (vertical growth). This requires maintaining a list of (t - 1)-way interactions, and hence can involve less bookkeeping. The pioneering example here is IPO[26] and its extensions, IPOG[27] , and IPOG-F and IPOG-F2[28], which will be discussed in more detail in Section 2. Both strategies typically pro-

duce test suites of acceptable size[26,29] . It has been observed that one-test-at-a-time methods produce slightly smaller test suites in general, while one-parameter-ata-time methods are somewhat faster at generation[26]. As mentioned earlier, software interaction test suites serve as two complementary roles[30]: to verify that no t-way interaction of SUT (software under test) causes a fault, or to locate such a fault. These two roles are different: certifying absence of a fault requires running the whole test suite, while locating a fault may not. Indeed in [30], it is shown that minimum test suite size is not the correct objective for fault location; the structure of the test suite can be more important than its size alone. An improved rate of fault detection can provide faster feedback to testers[31] . Recent studies have shown that CT is an effective fault detection technique and that early fault detection can be improved by reordering the generated test suites using interaction-based prioritization approaches[32-34] . Many strategies have been proposed to guide prioritization using evaluation measures such as interaction coverage based prioritization[30,35-39] and incremental interaction coverage based prioritization[40-41] . In [30], an evaluation measure of the expected time to fault detection is given. Test case prioritization techniques have been explored for the one-test-at-a-time methods, but little is known for the one-parameter-at-a-time methods. Bryce et al.[35-36,42] presented techniques that combine generation and prioritization. Pure prioritization[32-34,39] instead reorders an existing interaction test suite, using the metric of normalized average percentage of faults detected (NAPFD). However, existing pure prioritization techniques use explicit fault measurements of real systems, and hence are not directly suitable for the IPO algorithm. The main contributions of our work are: 1) We modify the IPO algorithm in order to accelerate the method and make it effective for fault detection. Our modifications attempt to make the values of each parameter more evenly distributed during generation. We focus on choosing values for the extension to an additional parameter during the horizontal growth of the algorithm and filling values for don't care positions. (See Section 3.) 2) We develop a pure prioritization technique (a reordering strategy) for the IPO algorithm based on the evaluation measure presented in [30]. Our method can reduce the expected time to fault detection effectively. (See Section 4.)

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO

959

3) We conduct experiments to demonstrate the effectiveness of the modifications (see Section 5). We conclude that the modifications to the IPO strategy result in faster generation (2 times faster on average according to the experimental results in Subsection 5.1), sometimes in smaller test suites, and together with the pure prioritization, in less time to detect the first fault (improved by 31% on average according to the experimental results in Subsection 5.2). 2 Framework of the IPO Algorithm

IPO comprises a family of methods of the oneparameter-at-a-time type. We focus on IPOG as a representative implementation. The basic operation is to add a new parameter to an existing interaction test suite of strength t. To initialize the method, whenever the number of parameters is at most t, all possible rows are included, which is necessary and sufficient to obtain a test suite. Thereafter, to introduce a new parameter, the set  of all t-way interactions involving the new parameter is computed. Horizontal growth adds a value of the new parameter to each existing row so that this extended row covers the most interactions in  ; the interactions covered are removed from  . Then if  still contains uncovered interactions, vertical growth adds new rows to cover them. This process is outlined in the flowchart in Fig.1. Existing variants of the IPO strategy alter the selection of values for the new parameter during horizontal growth and the selection of additional rows during

vertical growth. During both horizontal and vertical growth, it frequently happens that the value for one or more parameters in a row can be chosen arbitrarily without affecting the coverage of the row. Such entries are don't care positions[26] in the test suite. The IPO methods exploit the fact that selecting values for don't care positions can be deferred; then they can be filled during horizontal growth when the next parameter is introduced. Every variant of IPO must therefore deal with two basic problems: · choose values for the new parameter to maximize the number of uncovered interactions covered during horizontal growth; · assign values for don't care positions that arise. In the next section, we explore an implementation of this IPO framework in which the objective is not just to ensure coverage, but also to attempt to make each value appear as equally often as possible for each parameter. The latter is a balance condition. 3 Balance in the IPO Algorithm

A test suite must cover all t-way interactions. Consider a specific parameter and the t-way interactions that contain it. For each value of the parameter, the numbers of these t-way interactions with each different value of the parameter are the same. Now consider the frequencies of values of the parameter within the tests of a test suite. Because each value must provide the coverage of the same number of interactions, it appears to be reasonable to attempt to make the frequencies

Create Set  of Uncovered t-Way Combinations of Values Involving the Next Parameter

START

Horizontal Growth (Remove the Covered Combinatons from )

Build a t-Way Test Set for the First t Parameters

Yes  Is Empty?

All the Parameters Are Included in the Set?

No

No Vertical Growth (Remove the Covered Combinations from )

Yes END

Fig.1. Flowchart of IPOG algorithm.

960

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

close to equal. The same argument applies to fault detection. Two issues arise. First, current IPO algorithms do not make any explicit effort to balance the frequencies of values. Second, it is not at all clear how such an objective might affect the sizes of test suites produced, or the time to generate them, or their rates of fault detection. In this section, we develop modifications of IPO to address frequencies of values. Subsequent sections treat their impacts. 3.1 Choosing a New Parameter's Values

While shown in Algorithm 1 for IPOG, this simple strategy can also be used in IPOG-F and IPOG-F2. We show the modification for IPOG-F. The IPOG-F algorithm greedily selects over both the row and the value with which the covering array is extended, and the extended row/value pair (i; a) is greedily selected by the following formula[28] : tn = n-1 - T c [i ; a ], t-1

During horizontal growth, the IPOG algorithm chooses to add a value of the new parameter to cover the greatest number of interactions in  . In many situations, more than one value achieves this goal, and we must choose one. A naive strategy treats the values as ordered, and selects the smallest value that covers the most interactions in  . This introduces a bias towards the smaller values of each parameter, sometimes resulting in smaller values appearing much more frequently than larger ones. Here a different strategy, shown in Algorithm 1, is proposed. The essential change is to treat the values as being cyclically ordered, recording the value selected for the previous row. Then possible values for this row are considered by starting from the value following the previous one selected. For this modification, vertical growth remains unchanged.
Algorithm 1. Modified Horizontal Growth 1. Cov[r ; v] is the number of interactions that the extended row (r ; v) covers 2. q  |P | 3. prev  q 4. for each row r in the covering array ca do 5. max  (prev + 1) mod q 6. j  (max + 1) mod q 7. while j = ((prev + 1) mod q ) do 8. if Cov[r, vj ] > Cov[r, vmax ] then 9. max  j 10. end if 11. j  (j + 1) mod q 12. end while 13. r  (r, vmax ) 14. prev  max 15. end for

where n is the number of parameters, Tc [i; a] denotes the t-tuples that have previously been covered by already extended rows, and tn denotes the number of new t-tuples the row/value pair would cover if we extend row i with value a. The metric of optimal selection for the extended row (i; a) is that the extended row (i; a) would maximize tn . The original pseudo-code for horizontal growth in IPOG-F is shown in Algorithm 2. The modification replaces line 6 to line 10 of Algorithm 2 as shown in Algorithm 3. Similar modifications can be applied to IPOG-F2.
Algorithm 2. Horizontal Growth of IPOG-F 1. Tc [r ; a] is the number of t-tuples covered by (r ; a) 2. Cov[, v] is true if the interaction with column tuple  and value tuple v is covered false otherwise 3. Tc [i; a]  0, i, a 4. Cov[, v]  false, , a 5. while some row is non-extended do 6. Find non-extended row i and value a -1 7. so that tn = k - Tc [i; a] is maximum t -1 8. if tn = 0 then 9. Stop horizontal growth 10. end if 11. Extend row i with value a 12. for all non-extended row j do 13. S  set of columns where rows i and j have identical entries 14. for all column tuples   S do 15. v  the value tuple in row i and column tuple  16. if Cov[, v] = false then 17. Tc [j ; a]  Tc [j ; a] + 1 18. end if 19. end for 20. end for 21. for all column tuples  do 22. v  the value tuple in row r and column tuple  23. if Cov[, v] = false then 24. Cov[, v]  true 25. end if 26. end for 27. end while

Algorithm 1 incurs additional time to track the previous value selected, but this small addition is dominated by the computation of coverage, and hence makes no change in the complexity of the method.

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO
Algorithm 3. Modification (Lines 610) 1. max  (prev + 1) mod q 2. j  (max + 1) mod q 3. while j = ((prev + 1) mod q ) do 4. if Tc [i; vj ] < Tc [i, vmax ] then 5. max  j 6. j  (j + 1) mod q 7. end if 8. end while 9. a  vmax -1 10. tn  k - Tc [, a] t -1 11. if tn = 0 then 12. Stop horizontal growth 13. end if 14. Extend row i with value a 15. prev  max Algorithm 4. Addressing don't care Positions 1. Number the values of Pi as v1 , v2 , . . . , v|Pi | 2. f req [Pi , j ] is the frequency of value vj of Pi appears in the existing test set 3. e is an entry in column i 4. if e is a don't care position then 5. Find min that f req [Pi , min] is minimum in f req [Pi , 1], . . . , f req [Pi , |Pi |] 6. Assign e with vmin 7. end if

961

3.2

Addressing don't care Positions

our balance strategy only examines frequencies. Savings are only incurred with the balance strategy when don't care positions arise during vertical growth. In both cases, the worst-case complexity is dominated by the cost of horizontal growth, so in principle the two methods have the same asymptotic complexity. However, in practice, every don't care position results in a saving in computation time for the balance strategy. 4 Reducing the Expected Time to Fault Detection

In horizontal growth, when the maximum number of interactions that the extended row (r; v ) can cover is 0, the value at this position is a don't care. The don't care positions can be addressed using the method of Subsection 3.1. In vertical growth, new rows that are created to cover the t-way combinations in  not covered by horizontal growth can leave positions not needed to cover interactions in  as don't care. The selection of these values can influence the extension for the remaining parameters. To exploit these don't care positions, one strategy focuses on coverage, and the other on balance. The balance strategy attempts to make values of all parameters distributed evenly: as each don't care arises, it is filled with a value for this parameter that currently appears the least often; ties are handled by taking the next in the cyclic order of values after the previous selection. The coverage strategy is greedy. Don't care positions produced in vertical growth are left unassigned until the next horizontal growth. Then a value is chosen so that the row covers the most uncovered interactions, using the method described in Subsection 3.1. Focusing on coverage is generally slightly superior in reducing the size of test suites. However, the balance strategy reduces the time to generate the test suite. Because of our interest in fault detection, and the fact that existing IPO variants use a coverage strategy, we adopt the balance strategy here. The pseudo-code for the balance strategy is shown in Algorithm 4. Vertical growth treating don't care positions using a coverage strategy examines all t-way interactions, while

In [30], a measurement of the goodness of a test suite at detecting a fault is defined. Suppose that every test takes the same time to run. Further suppose that faults are randomly distributed among the t-way interactions, and that there is no a priori information about their location. For a system with s faults, the expected time to fault detection is determined by the expected number of tests to detect the presence of a fault. s denotes the expected number of tests to detect the first fault in a system with s faults. s =
N ui i=1 s  s

.

Here ui is the number of uncovered interactions before executing the i-th row, N is the number of rows of the test suite, and  is the total number of t-way interactions. This measure applies to any test suite when faults arise randomly, and is not intended to examine particular patterns of faults in specific systems. As such, it can serve as a means to evaluate test suites for use in an as-yet-unknown application. Minimizing the expected time to fault detection means constructing a test suite to minimize s given s. Rather than constructing a test suite to minimize s directly, we can reorder the rows of a test suite to reduce s . Because all faults of interest are caused by parameter interactions, the more uncovered interactions con-

962

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

tained in the test, the more likely a fault is to be revealed. Hence placing the tests that cover the greatest number of the uncovered interactions early can increase the probability of detecting a fault. To see this, we rewrite the formula as follows: s =
N ui i=1 s  s N

=
i=1

ui s  s

.

Then the problem becomes minimizing the average i (u s ) value of  , the likelihood that all faults remain unde(s) tected after running i tests. The method for reordering the test suite is Algorithm 5. There may be a tie for row rj where Tc [rj ] is the largest -- if there is, the tie would be broken randomly.
Algorithm 5. Reordering Test Suites 1. n  N 2. for j from 1 to n do 3. 4. for each row r1 , . . . , rn Determine the number Tc [ri ] of t-way interactions covered in ri but not covered in r1 , . . . , ri-1 5. 6. 7. 8. 9. 10. 11. 12. 13. end for end for Choose a row rj from ri , . . . , rn for which Tc [rj ] is the largest if Tc [rj ] = 0 then Remove all rows ri , . . . , rn from the suite n i-1 else Swap ri and rj in the suite end if

5

Experiments

We employ the tool ACTS-2.8 (Advanced Combinatorial Testing System)[43] , including implementations of IPOG, IPOG-F and IPOG-F2, etc. We compare the tool ACTS-2.8 with our variants of IPOG, IPOG-F and IPOG-F2 in which the handling of don't care positions attempts to balance frequencies of values; our versions are coded in C++. All of the experimental results reported here are performed on a laptop with CoreTM 2 Duo Intel processor clocked at 2.60 GHz and 4 GB memory. 5.1 Test Suite Size and Execution Time

First we examine the relative performance for different numbers of values for the parameters. The notation dt indicates that there are t parameters, each with

d values. To start, we vary the number of values. Table 1 shows execution time and test suite sizes when the strength is 4, and there are five parameters whose number of values is 5, 10, 15, or 20. As expected, the execution time for our methods is substantially smaller (see Fig.2). What is more surprising is that our methods consistently produce test suites no larger than the original methods, and sometimes produce much smaller ones. Now we vary the number of parameters. Table 2 shows results when the strength is 4, the number of parameters is 10, 15, 20, or 25, and the number of values is 5. Again the execution time for our methods shows improvements (see Fig.3). However, as the number of parameters increases, the deferral in filling don't care positions by the original methods generally produces smaller test suite sizes. Now we vary the strength. Table 3 presents results for 106 when the strength is 2, 3, 4, or 5. Once again, the execution time for our methods is substantially lower (see Fig.4). Our methods do not fare as well with respect to test suite size, but appear to be very effective when the strength is larger. Our methods appear to improve execution time consistently as expected. Nevertheless, they also improve on test suite sizes in some cases, especially when the strength is large or the number of values is large. Real systems rarely have the same number of values for each parameter, so we also consider situations in which different parameters can have different numbers of values. Table 4 presents results with strength 4 for five different sets of numbers of values for 10 parameters. Execution time improvements again arise for our algorithms. Moreover, a pattern for test suite sizes is clear: our methods improve when there is more variation in numbers of values. Next we examine the relative performance using the Traffic Collision Avoidance System (TCAS), which has been utilized in several other studies of software testing[27,44-46] . TCAS has 12 parameters: seven parameters have two values, two parameters have three values, one parameter has four values, and two parameters have 10 values. Table 5 gives the results. (In [46], similar results for the original IPOG versions are given for the TCAS system.) While our improvements in execution time are evident, no obvious pattern indicates which method produces the smallest test suite. Our methods have simplified the manner in which don't care positions are treated in order to balance the frequencies of values. Our experimental results all con-

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO
Table 1. Results for Five Parameters with 5 to 20 Values for 4-Way Testing Parameter Config. 55 105 155 205 Our IPOG Size 000 745 011 990 058 410 184 680 Time (s) 0.001 0.078 1.101 9.666 IPOG(ACTS) Size Time (s) 000 790 000.015 012 298 000.827 061 945 016.329 191 652 120.220 Our IPOG-F Size Time (s) 000 625 000.000 010 000 000.673 050 625 018.469 160 000 200.020 IPOG-F(ACTS) Size Time (s) 000625 1 000.047 010 000 1 006.109 050 625 1 146.730 160 000 1 376.000 Our IPOG-F2 Size Time (s) 000 625 000.000 010 000 000.500 050 625 012.782 160 000 209.290

963

IPOG-F2(ACTS) Size Time (s) 100 788 1 000.031 112 394 1 004.859 161 615 1 184.450 192 082 1 966.200

140 120 100 Time (s) Time (s) 80 60 40 20 0 5 10 15 Domain Size (a) 20
Our IPOG IPOG(ACTS)

1400 1200 1000 800 600 400 200 0 5 10 15 Domain Size (b) 20 Time (s)
Our IPOG-F IPOG-F(ACTS)

2000
Our IPOG-F2 IPOG-F2(ACTS)

1500

1000

500

0

5

10 15 Domain Size (c)

20

Fig.2. Execution time, varying the number of values (4-way). (a) IPOG. (b) IPOG-F. (c) IPOG-F2. Table 2. Results for 10 to 25 5-Value Parameters for 4-Way Testing Parameter Config. 510 515 520 525 Our IPOG Size 1 890 2 584 3 114 3 540 Time (s) 0.056 0.517 2.140 7.012 IPOG(ACTS) Size 1 859 2 534 3 032 3 434 Time (s) 00.188 00.954 04.094 16.049 Our IPOG-F Size 1 833 2 461 2 951 3 338 Time (s) 000.625 007.109 034.361 111.150 IPOG-F(ACTS) Size 1 882 2 454 2 898 3 279 Time (s) 001.750 014.579 060.987 176.340 Our IPOG-F2 Size 1 965 2 736 3 308 3 763 Time (s) 0.187 1.282 4.329 8.752 IPOG-F2(ACTS) Size 1 905 2 644 3 180 3 589 Time (s) 0.297 1.421 4.344 9.188

10 15 Time (s) Our IPOG IPOG(ACTS) Time (s) 150 Our IPOG-F IPOG-F(ACTS) Time (s) 8 6 4 2 0 10 0 0 Our IPOG-F2 IPOG-F2(ACTS)

10

100

5

50

15 20 25 Number of Parameters (a)

10

15 20 25 Number of Parameters (b)

10

15 20 25 Number of Parameters (c)

Fig.3. Execution time, increasing the number of parameters (4-way). (a) IPOG. (b) IPOG-F. (c) IPOG-F2. Table 3. Results for Six 10-Value Parameters for 25-Way Testing t Our IPOG Size 2 000 149 3 001 633 4 016 293 5 123 060 Time (s) 0.000 0.010 0.195 5.139 IPOG(ACTS) Size 000 130 001 633 016 496 130 728 Time (s) 000.005 000.059 004.276 116.470 Our IPOG-F Size 000 133 001 577 015 594 100 000 Time (s) 00.000 00.047 02.704 88.692 IPOG-F(ACTS) Size 000 134 001 553 015 467 100 000 Time (s) 000.031 000.266 018.126 575.150 Our IPOG-F2 Size 000 135 001 629 015 631 100 000 Time (s) 00.000 00.032 01.594 54.971 IPOG-F2(ACTS) Size 000 134 001 625 016 347 132 428 Time (s) 000.016 000.140 009.297 449.330

964

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5
Table 4. Results for Five Systems with Different Numbers of Values in 4-Way Testing

Parameter Config. 1010 105 95 153 104 53 161 152 104 52 41

Our IPOG Size 29 915 23 878 41 128 42 913 Time (s) 1.942 1.295 1.734 1.750 1.844

IPOG(ACTS) Size 29 466 23 961 45 128 47 591 52 991 Time (s) 28.040 14.583 13.689 14.532 14.860
600

Our IPOG-F Size 28 437 22 521 41 505 43 774 48 847 Time (s) 129.57 094.27 236.87 249.37 235.95

IPOG-F(ACTS) Size 28 079 22 726 43 306 45 693 50 287 Time (s) 359.17 248.04 757.68 289.72 333.89

Our IPOG-F2 Size Time (s) 31 744 045.237 25 222 031.611 46 509 162.850 48 660 148.510 54 099 189.290

IPOG-F2(ACTS) Size 30 986 24 741 48 295 51 147 57 634 Time (s) 053.440 039.736 262.170 149.510 199.810

171 161 151 104 51 42 47 248
120 100 Time (s) 80 60 40 20 0 2

Our IPOG IPOG(ACTS)

500 Time (s) 400 300 200 100 0

Our IPOG-F IPOG-F(ACTS)

400 300 200 100 0

Our IPOG-F2 IPOG-F2(ACTS)

3 4 Strength of Coverage (a)

5

2

3 4 Strength of Coverage (b)

5

Time (s)

2

3 4 Strength of Coverage (c)

5

Fig.4. Execution time, increasing the test strength. (a) IPOG. (b) IPOG-F. (c) IPOG-F2. Table 5. Results for TCAS t 2 3 4 5 6 Our IPOG Size 00 100 00 404 01 306 04 464 11 774 Time (s) 0.001 0.009 0.065 0.411 1.463 IPOG(ACTS) Size 00 100 00 400 01 359 04 233 11 021 Time (s) 0.002 0.007 0.031 0.219 3.233 Our IPOG-F Size 00 100 00 400 01 269 04 068 11 381 Time (s) 00.002 00.025 00.323 04.104 32.870 IPOG-F(ACTS) Size 00 100 00 402 01 349 04 245 11 257 Time (s) 000.015 000.087 001.117 013.405 101.330 Our IPOG-F2 Size 00 100 00 431 01 639 05 129 13 323 Time (s) 00.004 00.044 00.489 04.133 18.030 IPOG-F2(ACTS) Size 00 100 00 438 01 653 05 034 13 379 Time (s) 00.017 00.061 00.572 04.379 20.959

firm that this can dramatically reduce the execution time. One might have expected a substantial degradation in the test suite sizes produced. However, our results indicate not only that the balancing strategy is competitive, but also that it can improve test suite sizes. Fast methods such as IPO do not generally produce the smallest test suites possible. To illustrate this, we apply a post-optimization method from [4748] to some of the TCAS results. For strength 4, we treat the solutions for IPOG-F2; within 10 minutes of computation, post-optimization reduces the solution by our method from 1 639 to 1 201 rows, and the solution by the original method from 1 653 to 1 205 rows. For strength 5, we treat the solutions for IPOG-F; within one hour of computation, post-optimization reduces the solution by our method from 4 068 to 3 600 rows, and the solution by the original method from 4 245 also to 3 600 rows. For strength 6, we treat the solutions

for IPOG; within 10 hours, post-optimization reduces the solution by our method from 11 774 to 9 794 rows, and the solution by the original method from 11 021 to 9 798 rows. By contrast, in a comparison of six different one-parameter-at-a-time methods[46] , the best result has 10 851 rows. While the test suites from oneparameter-at-a-time methods are therefore definitely not the smallest, post-optimization is much more timeconsuming and it requires a test suite as input. As the number of parameters increases, the speed with which an initial test suite can be constructed is crucial. 5.2 Expected Time to Fault Detection

Accelerating the IPO methods, even with a possible loss of accuracy in test suite size, can be worthwhile. However, a second concern is with potential performance in revealing faults. We examine the TCAS system, using our and the original versions of the three IPO variants. We examine the time to find the first

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO

965

fault when 1, 2, or 3 faults are present and when the strength is between 2 and 6. In our model, the time to execute each test is the same, so the expected time is directly proportional to the expected number of tests or rows needed. We consider test suites before and after our reordering. Table 6 gives the results. To assess the efficacy of our modifications, we report two lines for each method and each strength; the first reports results for our methods, and the second for the original methods. 1 , 2 and 3 denote the expected number of tests to detect the first fault when there are one, two or three faults that are randomly chosen. These results indicate that reordering is effective in reducing the time to fault detection, both for our met-

hods and for the original ones. Fig.5 shows 2 for each strength before and after the reordering for our methods, showing a substantial reduction from reordering. Fig.6 instead shows the expected number of tests when zero, one, two, or three faults are present. It appears that the reordering method is the most effective when the number of faults is small. This should be expected, because the presence of many faults ensures that one will be found early no matter what ordering is used. Our methods, despite often producing larger test suites, fare well with respect to expected time to fault detection. Comparing the performance of ours and the original IPOG when t = 6, for example, although our test suite is larger, it would yield smaller expected time to detect faults once reordered. Evidently the size of the

Table 6. Expected Time to Fault Detection for TCAS Before and After Reordering Algorithm t 1 Before IPOG 2 3 4 5 6 IPOG-F 2 3 4 5 6 IPOG-F2 2 3 4 5 6 00 24.80 00 24.81 0 117.90 0 117.68 0 407.20 0 408.42 1 348.26 1 348.14 3 015.32 3 007.69 00 28.36 00 27.19 0 120.96 0 120.94 0 411.37 0 411.97 1 353.42 1 354.28 3 076.17 3 017.29 00 26.44 00 26.27 0 120.61 0 121.04 0 419.07 0 421.75 1 378.15 1 377.84 3 129.21 3 138.02 After 00 19.65 00 19.65 00 82.15 00 82.12 0 275.38 0 276.34 0 850.74 0 848.20 2 127.94 2 140.04 00 20.43 00 20.67 00 81.47 00 81.34 0 272.86 0 269.36 0 828.83 0 822.73 2 090.57 2 059.33 00 20.52 00 20.19 00 82.75 00 81.63 0 275.05 0 278.10 0 844.54 0 838.77 2 127.62 2 121.97 Before 00 10.72 00 10.73 00 53.30 00 53.18 0 200.45 0 201.60 0 707.82 0 708.24 1 682.31 1 680.95 00 12.73 00 12.18 00 55.81 00 55.99 0 204.59 0 204.73 0 716.08 0 715.52 1 722.82 1 693.94 00 11.90 00 11.67 00 55.26 00 55.61 0 207.11 0 208.20 0 724.94 0 725.02 1 732.44 1 736.29 Number of Faults 2 After 000 9.26 000 9.26 00 38.57 00 38.47 0 131.93 0 132.73 0 421.49 0 421.30 1 095.54 1 106.03 000 9.58 000 9.67 00 38.33 00 38.18 0 132.18 0 129.68 0 411.92 0 410.22 1 065.49 1 063.99 000 9.75 000 9.56 00 38.36 00 38.15 0 130.39 0 131.82 0 412.79 0 409.71 1 068.73 1 062.64 Before 000 6.27 000 6.27 00 30.08 00 30.03 0 118.33 0 119.08 0 436.03 0 436.40 1 097.27 1 096.56 000 7.29 000 7.08 00 31.84 00 32.13 0 121.66 0 121.75 0 444.08 0 443.26 1 129.80 1 109.05 000 6.98 000 6.80 00 31.49 00 31.76 0 123.20 0 123.79 0 449.34 0 449.50 1 133.78 1 136.22 3 After 005.83 005.85 023.69 023.56 082.38 082.77 268.03 268.37 719.43 725.45 006.01 006.02 023.71 023.72 082.83 081.77 263.23 261.36 698.08 700.68 006.13 006.00 023.84 023.55 081.95 082.43 263.65 261.35 699.08 695.18

966
2000 Expected Number of Tests Expected Number of Tests 1500 1000 500 0 Before Reorder After Reorder 2000

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5
2000 Expected Number of Tests Before Reorder After Reorder 1500 1000 500 0 Before Reorder After Reorder

1500 1000 500 0

2

3 4 5 Test Strength (a)

6

2

3 4 5 Test Strength (b)

6

2

3 4 5 Test Strength (c)

6

Fig.5. Expected number of tests for 2 . (a) IPOG. (b) IPOG-F. (c) IPOG-F2.

Expected Number of Tests

Expected Number of Tests

Before Reorder After Reorder 1000

Before Reorder After Reorder 1000

Expected Number of Tests

1500

1500

2000 Before Reorder After Reorder

1500

1000

500

500

500

0

0

1 2 3 Number of Faults (a)

4

0

0

1 2 3 Number of Faults (b)

4

0

0

1 2 3 Number of Faults (c)

4

Fig.6. Expected number of tests, increasing the number of faults in TCAS (4-way). (a) IPOG. (b) IPOG-F. (c) IPOG-F2.

test suite, while relevant, is not the only factor affecting the expected time. Our results suggest that faster IPO implementations remain competitive, and hence that the objective of balancing frequencies of values is a reasonable one to pursue. 6 Conclusions

We identified three main goals in generating a test suite: time to generate the test suite, time to execute the test suite (test suite size), and the rate of fault detection. Our methods focus on reducing the time for generation, without severe negative impact on test suite size and fault detection. We accelerated variants of the IPO method by simplifying the manner in which don't care positions are filled. This results in a consistent improvement in the execution time to construct a test suite, but sacrifices to some extent the algorithm's ability to exploit such positions in repeated horizontal growth phases. This is reflected in our experimental results. While in numerous cases, our modifications find smaller test suites, in the others they do not. This occurs particularly when the number of parameters is large.

Any method to fill don't care positions immediately would be expected to accelerate the methods; however we devised a simple method that strives to balance the frequency of values for each parameter. We argued that such an objective can result in more effective horizontal growth, and that it can permit us to retain effective rates of fault detection. Both of these motivations are borne out by the experimental data. One-test-at-a-time generation methods explicitly aim for good rates of fault detection by covering interactions early in the test suite, while one-parameterat-a-time methods like IPO do not. Nevertheless, we showed that a reordering strategy can be applied to make dramatic improvement on the rate of fault detection. If test suite size is a primary objective, using our methods together with randomized postoptimization[47-48] appears to be worthwhile. If expected time to fault detection is paramount, extending reordering to discover and replace don't care positions appears to be viable. Both merit further study. We suggest that both can benefit from balancing frequencies of values, a fast and simple way to generate useful test suites.

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO

967

References
[1] Birnbaum Z W. On the importance of different components in a multicomponent system. In Multivariate Analysis, Krishnaiah P R (ed.), New York: Academic Press, 1969, pp.591-592. [2] Kuo W, Zhu X. Relations and generalizations of importance measures in reliability. IEEE Trans. Rel., 2012, 61(3): 659674. [3] Kuo W, Zhu X. Some recent advances on importance measures in reliability. IEEE Trans. Rel., 2012, 61(2): 344-360. [4] Anand S, Burke E K, Chen T Y et al. An orchestrated survey of methodologies for automated software test case generation. J. Sys. Software, 2013, 86(8): 1978-2001. [5] Chen T Y, Kuo F C, Liu H et al. Code coverage of adaptive random testing. IEEE Trans. Rel., 2013, 62(1): 226-237. [6] Grindal M, Offutt J, Andler S F. Combination testing strategies: A survey. Softw. Test. Verif. Rel., 2005, 15(3): 167-199. [7] Hao D, Zhang L M, Zhang L et al. A unified test-case prioritization approach. ACM Trans. Soft. Eng. Method, 2014, 24(2): 10:1-10:31. [8] Harman M, McMinn P. A theoretical and empirical study of search-based testing: Local, global, and hybrid search. IEEE Trans. Software Eng., 2010, 36(2): 226-247. [9] Nie C H, Leung H. A survey of combinatorial testing. ACM Comput. Surv., 2011, 43(2): 11:1-11:29. [10] Nebut C, Fleurey F, Le Traon Y et al. Automatic test generation: A use case driven approach. IEEE. Trans. Software Eng., 2006, 32(3): 140-155. [11] Perrouin G, Oster S, Sen S et al. Pairwise testing for software product lines: Comparison of two approaches. Software. Qual. J., 2012, 20(3/4): 605-643. [12] Xie T, Zhang L, Xiao X et al. Cooperative software testing and analysis: Advances and challenges. Journal of Computer Science and Technology, 2014, 29(4): 713-723. [13] Yoo S, Harman M. Regression testing minimization, selection and prioritization: A survey. Softw. Test. Verif. Rel., 2012, 22(2): 67-120. [14] Zhang D M, Xie T. Software analytics: Achievements and challenges. In Proc. the 35th Int. Conf. Software Eng., May 2013, p.1487. [15] Yu K, Lin M, Chen J et al. Towards automated debugging in software evolution: Evaluating delta debugging on real regression bugs from the developers' perspectives. J. Sys. Software, 2012, 85(10): 2305-2317. [16] Bryce R C, Colbourn C J. One-test-at-a-time heuristic search for interaction test suites. In Proc. the 9th Annu. Conf. Genetic and Evolutionary Computation, Jul. 2007, pp.1082-1089. [17] Kuhn D R, Reilly M J. An investigation of the applicability of design of experiments to software testing. In Proc. the 27th Annu. NASA Goddard Workshop on Software Eng., Dec. 2002, pp.91-95. [18] Kuhn D R, Wallace D R, Gallo Jr J A. Software fault interactions and implications for software testing. IEEE Trans. Software Eng., 2004, 30(6): 418-421. [19] Cohen M B, Dwyer M B, Shi J. Constructing interaction test suites for highly-configurable systems in the presence of constraints: A greedy approach. IEEE Trans. Software Eng., 2008, 34(5): 633-650.

[20] Lei Y, Kacker R, Kuhn D R et al. IPOG/IPOG-D: Efficient test generation for multi-way combinatorial testing. Softw. Test. Verif. Rel., 2008, 18(3): 125-148. [21] Tung Y W, Aldiwan W S. Automating test case generation for the new generation mission software system. In Proc. IEEE Aerospace Con., March 2000, pp.431-437. [22] Wallace D R, Kuhn D R. Failure modes in medical device software: An analysis of 15 years of recall data. Int. J. Rel., Quality and Safety Eng., 2001, 8(4): 351-371. [23] Cohen D M, Dalal S R, Kajla A et al. The automatic efficient tests generator (AETG) system. In Proc. the 5th Int. Sympo. Software Rel. Eng., Nov. 1994, pp.303-309. [24] Bryce R C, Colbourn C J. The density algorithm for pairwise interaction testing. Softw. Test. Verif. Rel., 2007, 17(3): 159-182. [25] Bryce R C, Colbourn C J. A density-based greedy algorithm for higher strength covering arrays. Softw. Test. Verif. Rel., 2009, 19(1): 37-53. [26] Lei Y, Tai K C. In-parameter-order: A test generation strategy for pairwise testing. In Proc. the 3rd Int. Symp. HighAssurance Sys. Eng., Nov. 1998, pp.254-261. [27] Lei Y, Kacker R, Kuhn D R et al. IPOG: A general strategy for t-way software testing. In Proc. the 14th Annu. Int. Conf. Worshop. Eng. Computer-Based Sys., March 2007, pp.549-556. [28] Forbes M, Lawrence J, Lei Y, Kacker R N, Kuhn D R. Refining the in-parameter-order strategy for constructing covering arrays. Journal of Research of the National Institute of Standards and Technology, 2008, 113(5): 287-297. [29] Cohen M B, Gibbons P B, Mugridge W B et al. Constructing test cases for interaction testing. In Proc. the 25th Int. Conf. Software Eng., May 2003, pp.38-48. [30] Bryce R C, Colbourn C J. Expected time to detection of interaction faults. J. Combin. Mathematics and Combin. Comput., 2013, 86: 87-110. [31] Rothermel G, Untch R H, Chu C et al. Prioritizing test cases for regression testing. IEEE Trans. Software Eng., 2001, 27(10): 929-948. [32] Qu X, Cohen M B. A study in prioritization for higher strength combinatorial testing. In Proc. the 6th Int. Con. Software Testing, Verification and Validation, the 2nd Int. Workshops on Combinatorial Testing, March 2013, pp.285294. [33] Qu X. Configuration aware prioritization techniques in regression testing. In Proc. the 31st Int. Conf. Software Engineering, Companion Volume, May 2009, pp.375-378. [34] Qu X, Cohen M B, Rothermel G. Configuration-aware regression testing: An empirical study of sampling and prioritization. In Proc. Int. Symp. Software Tesing and Analysis, July 2008, pp.75-86. [35] Bryce R C, Colbourn C J. Test prioritization for pairwise interaction coverage. In Proc. the 1st Int. Workshop on Advances in Model-Based Testing, May 2005. [36] Bryce R C, Colbourn C J. Prioritized interaction testing for pair-wise coverage with seeding and constraints. Inform. Software Tech., 2006, 48(10): 960-970. [37] Huang R, Chen J, Li Z, Wang R, Lu Y. Adaptive random prioritization for interaction test suites. In Proc. the 29th Symp. Appl. Comput., March 2014, pp.1058-1063.

968
[38] Petke J, Yoo S, Cohen M B, Harman M. Efficiency and early fault detection with lower and higher strength combinatorial interaction testing. In Proc. the 12th Joint Meeting on European Software Engineering Conf. and the ACM SIGSOFT Symp. the Foundations of Software Eng. (ESEC/FSE 2013), August 2013, pp.26-36. [39] Qu X, Cohen M B, Woolf K M. Combinatorial interaction regression testing: A study of test case generation and prioritization. In Proc. the 23rd Int. Conf. Software Maintenance, Oct. 2007, pp.255-264. [40] Huang R, Chen J, Zhang T, Wang R, Lu Y. Prioritizing variable-strength covering array. In Proc. the 37th IEEE Annu. Computer Software and Applications Conf., July 2013, pp.502-511. [41] Huang R, Xie X, Towey D, Chen T Y, Lu Y, Chen J. Prioritization of combinatorial test cases by incremental interaction coverage. Int. J. Softw. Eng. Know., 2014, 23(10): 1427-1457. [42] Bryce R C, Memon A M. Test suite prioritization by interaction coverage. In Proc. Workshop on Domain Specific Approaches to Software Test Automation, September 2007, pp.1-7. [43] Lei Y, Kuhn D R. Advanced combinatorial testing suite (ACTS). http://csrc.nist.gov/groups/SNS/acts/index.html, Aug. 2015. [44] Hutchins M, Foster H, Goradia T et al. Experiments of the effectiveness of dataflow and control-flow-based test adequacy criteria. In Proc. the 16th Int. Conf. Software Eng., May 1994, pp.191-200. [45] Kuhn D R, Okun V. Pseudo-exhaustive testing for software. In Proc. the 30th Annu. IEEE/NASA Software Engineering Workshop, April 2006, pp.153-158. [46] Soh Z H C, Abdullah S A C, Zamil K Z. A distributed tway test suite generation using "One-Parameter-at-a-Time" approach. Int. J. Advance Soft Compu. Appl., 2013, 5(3): 91-103. [47] Li X, Dong Z, Wu H et al. Refining a randomized postoptimization method for covering arrays. In Proc. the 7th IEEE Int. Conf. Software Testing, Verification and Validation Workshops (ICSTW), March 31-April 4, 2014, pp.143152. [48] Nayeri P, Colbourn C J, Konjevod G. Randomized postoptimization of covering arrays. Eur. J. Combin., 2013, 34(1): 91-103.

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

Jiang-Hua Lv received her B.S. and Ph.D. degrees in computer science from Jilin University, Changchun, in 1998 and 2003, respectively. Currently she is an assistant professor in the School of Computer Science and Engineering of Beihang University, Beijing. She is a member of the State Key Laboratory of Software Development Environment of Beihang University. Her research focuses on formal theory and technology of software, theory and technology of testing, automatic testing of safety critical systems, and device collaboration. Bing-Lei Du is currently an undergraduate in the School of Computer Science and Engineering of Beihang University, Beijing, and has been an intern in the State Key Laboratory of Software Development Environment of Beihang University since 2013. His research interest is software testing. Charles J. Colbourn earned his Ph.D. degree in computer science from the University of Toronto in 1980, and is a professor of computer science and engineering at Arizona State University. He is the author of The Combinatorics of Network Reliability (Oxford), Triple Systems (Oxford), and 320 refereed journal papers focusing on combinatorial designs and graphs with applications in networking, computing, and communications. In 2004, he was awarded the Euler Medal for Lifetime Research Achievement by the Institute for Combinatorics and its Applications. Shi-Long Ma is currently a professor and doctor tutor of the School of Computer Science and Engineering of Beihang University, Beijing. He is a member of the State Key Laboratory of Software Development Environment of Beihang University. His main research focus is on computation models in networks, logic reasoning and behaviors in network computing, and the theory of automatic testing.

Shi-Wei Gao received his B.S. degree in computer science and technology from Dezhou University, Dezhou, in 2007, and M.S. degree in information science and engineering from Yanshan University, Qinhuangdao, in 2010. He is currently a Ph.D. candidate in the School of Computer Science and Engineering of Beihang University, Beijing. He is a member of the State Key Laboratory of Software Development Environment of Beihang University. His research interests include software testing, software reliability theory, and formal methods.

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

575

A Discrete Particle Swarm Optimization for Covering Array Generation
Huayao Wu, Changhai Nie, Member, IEEE, Fei-Ching Kuo, Member, IEEE, Hareton Leung, Member, IEEE, and Charles J. Colbourn

Abstract--Software behavior depends on many factors. Combinatorial testing (CT) aims to generate small sets of test cases to uncover defects caused by those factors and their interactions. Covering array generation, a discrete optimization problem, is the most popular research area in the field of CT. Particle swarm optimization (PSO), an evolutionary search-based heuristic technique, has succeeded in generating covering arrays that are competitive in size. However, current PSO methods for covering array generation simply round the particle's position to an integer to handle the discrete search space. Moreover, no guidelines are available to effectively set PSOs parameters for this problem. In this paper, we extend the set-based PSO, an existing discrete PSO (DPSO) method, to covering array generation. Two auxiliary strategies (particle reinitialization and additional evaluation of gbest) are proposed to improve performance, and thus a novel DPSO for covering array generation is developed. Guidelines for parameter settings both for conventional PSO (CPSO) and for DPSO are developed systematically here. Discrete extensions of four existing PSO variants are developed, in order to further investigate the effectiveness of DPSO for covering array generation. Experiments show that CPSO can produce better results using the guidelines for parameter settings, and that DPSO can generate smaller covering arrays than CPSO and other existing evolutionary algorithms. DPSO is a promising improvement on PSO for covering array generation. Index Terms--Combinatorial testing (CT), covering array generation, particle swarm optimization (PSO).

I. I NTRODUCTION S SOFTWARE functions and run-time environments become more complex, testing of modern software systems is becoming more expensive. Effective detection of

A

Manuscript received May 12, 2013; revised December 29, 2013 and May 18, 2014; accepted September 28, 2014. Date of publication October 9, 2014; date of current version July 28, 2015. This work was supported in part by the National Natural Science Foundation of China under Grant 61272079, in part by the Research Fund for the Doctoral Program of Higher Education of China under Grant 20130091110032, in part by the Science Fund for Creative Research Groups of the National Natural Science Foundation of China under Grant 61321491, in part by the Major Program of National Natural Science Foundation of China under Grant 91318301, and in part by the Australian Research Council Linkage under Grant LP100200208. (Corresponding author: Changhai Nie.) H. Wu and C. Nie are with the State Key Laboratory for Novel Software Technology, Nanjing University, Nanjing 210023, China (e-mail: hywu@outlook.com; changhainie@nju.edu.cn). F.-C. Kuo is with the Faculty of Information and Communication Technologies, Swinburne University of Technology, Hawthorn, VIC 3122, Australia (e-mail: dkuo@swin.edu.au). H. Leung is with the Department of Computing, Hong Kong Polytechnic University, Hong Kong (e-mail: hareton.leung@polyu.edu.hk). C. J. Colbourn is with Arizona State University, Tempe, AZ 85287-8809, USA (e-mail: colbourn@asu.edu). Digital Object Identifier 10.1109/TEVC.2014.2362532

failures at a low cost is a key issue for test case generation. Combinatorial testing (CT) is a popular testing method to detect failures triggered by various factors and their interactions [1]. By employing covering arrays as test suites, the CT method aims to sample the large combination space with few test cases to cover different interactions among a fixed number of factors. Kuhn and Reilly [2] shows that more than 70% of the failures in certain software were caused by the interactions of one or two factors, and almost all the failures could be detected by checking the interactions among six factors. Therefore, CT can be an effective method in practice. Generating a covering array with fewest tests (minimum size) is a major challenge in CT. In general, the minimum size of a covering array is unknown; hence, methods have focused on finding covering arrays that have as few tests as possible at reasonable search cost. The many methods that have been proposed can be classified into two main groups: 1) mathematical methods and 2) computational methods [1]. Mathematical (algebraic or combinatorial) methods typically exploit some known combinatorial structure. Computational methods primarily use greedy strategies or heuristic-search techniques to generate covering arrays, due to the size of the search space. Mathematical methods yield the best possible covering arrays in certain cases. For example, orthogonal arrays used in the design of experiments provide covering arrays with a number of tests that is provably minimum. However, all known mathematical methods can be applied only for restrictive sets of factors. This limitation has led to an emphasis on computational methods. Greedy algorithms have been quite effective in generating covering arrays, but their accuracy suffers from becoming trapped in local optima. In recent years, search-based software engineering (SBSE) has focused on using search-based optimization algorithms to find high-quality solutions for software engineering problems. Inspired by SBSE, many artificial intelligence-based heuristic-search techniques have been applied to software testing. For example, simulated annealing (SA) [3]­[7], genetic algorithm (GA) [8]­[10], and ant colony optimization (ACO) [9], [11], [12] have all been applied to covering array generation. These techniques can generate any types of covering arrays, and the constraint solving and prioritization techniques can be easily integrated. Their applications have been shown to be effective, producing relatively small covering arrays in many cases. Particle swarm optimization (PSO), a relatively new evolutionary algorithm,

1089-778X c 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission. See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

576

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

has also been used in this area [13]­[16]. It is easy to implement and has fast initial progress. The conventional PSO (CPSO) algorithm was originally designed to find optimal or near optimal solutions in a continuous space. Nevertheless, many discrete PSO (DPSO) algorithms and frameworks have been developed to solve discrete problems [17]­[22]. For covering array generation, current discrete methods [13]­[16] simply round the particle's position to an integer while keeping the velocity as a real number. They suffer from two main shortcomings. First, the performance of PSO is significantly impacted by its parameter settings. In [23], effects of the general parameter selection and initial population of PSO have been analyzed, but no guidelines on parameter settings have been reported for covering array generation. Hence, a clear understanding of how to set these execution parameters is needed. Second, simple rounding fractional positions to integers introduces a substantial source of errors in the search. Instead, a specialized DPSO version is needed. Because current PSO methods show promise for generating covering arrays, these two main shortcomings should be addressed. In this paper, we adapt set-based PSO (S-PSO) [18] to generate covering arrays. S-PSO utilizes set and probability theories to develop a new representation scheme for combinatorial optimization problems as well as refining the related evolution operators. In our adaptation of S-PSO, two auxiliary strategies are proposed to enhance performance, and a novel DPSO algorithm is thus proposed. DPSO has the same conceptual basis and exhibits similar search behavior to CPSO, with parameters playing similar roles. Then, we explore the optimal parameter settings for both CPSO and DPSO to improve their performance, and identify recommended settings. Furthermore, because many CPSO variants [24]­[27] can be easily extended to discrete versions based on our DPSO, the performance of these discrete versions is also compared with their original ones. Finally, we compare CPSO and DPSO with existing GA and ACO [9], [11] algorithms to generate covering arrays. The main contributions of this paper are as follows. 1) Based on the set-based representation, we design a version of S-PSO [18] for covering array generation. 2) We propose two auxiliary strategies, particle reinitialization, and additional evaluation of gbest, to enhance the performance of PSO. A novel DPSO for covering array generation is proposed. 3) We design experiments to explore the optimal parameter settings for CPSO and DPSO for covering array generation. 4) We implement original and discrete versions of four representative PSO variants (TVAC [24], CLPSO [25], APSO [26], and DMS-PSO [27]) to compare their efficacy for covering array generation. The rest of this paper is organized as follows. Section II gives background on CT, covering array generation, and the CPSO algorithm. Section III summarizes related work. Section IV presents our DPSO algorithm, including the representation scheme, related operators, and two auxiliary strategies. Section V evaluates the performance of CPSO and

TABLE I E-C OMMERCE S OFTWARE S YSTEM

DPSO, and explores optimal parameter settings. Section VI gives a comparison among CPSO, DPSO, and original and discrete variants. Section VII compares CPSO and DPSO with GA and ACO. Section VIII concludes this paper and outlines future work. II. BACKGROUND A. CT Suppose that the behavior of the software under test (SUT) is controlled by n independent factors, which may represent configuration parameters, internal or external events, user inputs, and the like. The ith factor has i discrete levels, chosen from the finite set Vi of size i . An n-tuple (x1 , x2 , . . . , xn ) forms a test case, where xi  Vi for 1  i  n. Consider a simple e-commerce software system [15]. This system consists of five different components. Each of these five components can be regarded as a factor, and its configurations can be regarded as different levels. Table I shows these five factors and their corresponding levels. In this example, n = 5, 1 = 2 = 3 = 2, 4 = 5 = 3. System failures are often triggered by interactions among some factors, which can be represented by the combinations of factor levels. In order to detect these failures, their combinations should be covered at least once by the test suite. A t-way schema can be used to represent them. Definition 1 (t-way schema): The n-tuple (-, y1 , . . . , yt , . . . ) is a t-way schema (t > 0) when some t factors have fixed levels and the others can take any valid levels, represented as "-." For example, suppose that when factor Payment Server takes the level Master and factor Web Server takes the level Apache, a system failure occurs. To detect this failure, the 2-way schema (Master, ­, Apache, ­, ­) must be covered at least once by the test suite. To simplify later discussion, we use the index in the level set of each factor to present a schema. For example, (0, ­, 1, ­, ­) is used to represent (Master, ­, Apache, ­, ­). Exhaustive testing covers all n-way schemas of the system; in our example, it uses 2 × 2 × 2 × 3 × 3 = 72 test cases. Testing cost becomes prohibitive as the numbers of factors and levels increase. Moreover, because only interactions among few factors are likely to trigger failures [2], testing high-way schemas can lead to many uninformative test cases. At the other extreme, if we only guarantee to cover each 1-way schema once, only three test cases are needed (a single test case can cover five 1-way schemas at most). But we may fail to detect some interaction triggered failures involving two factors.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

577

TABLE II C OVERING A RRAY CA(9; 2, 23 32 )

TABLE III A DDING T HREE T EST C ASES TO C ONSTRUCT VCA(12; 2, 23 32 , CA(3, 22 31 ))

Instead, CT covers all t-way schemas. Such a test suite is a t-way covering array, with t being the covering strength. The value of t determines the depth of coverage. It is a key setting of CT, and should be decided by the testers. We give a precise definition. Definition 2 (Covering Array): If an N ×n array, where N is the number of test cases, has the following properties: 1) each column i (1  i  n) contains only elements from the set Vi with i = |Vi | and 2) the rows of each N × t sub array cover all |Vk1 | × |Vk2 | × . . . × |Vkt | combinations of the t columns at least once, where t  n and 1  k1 < . . . < kt  n, then it is a t-way covering array, denoted by CA(N ; t, n, ( 1 , 2 , . . . , n )). When 1 = 2 = . . . = n = , it is denoted by CA(N ; t, n, ). Reference [2] demonstrated that more than 70% failures can be detected by a 2-way covering array, and almost all failures can be detected by a 6-way covering array. Hence, using CT, we can detect many failures of the system by applying relatively low strength covering arrays. In other words, CT can greatly reduce the size of test suite while maintaining high fault detection ability. In the example of Table I, if we only consider the interactions between any two factors, only nine test cases are required to construct a 2-way covering array instead of 72 for exhaustive testing. Table II shows a covering array, where each row represents one test case. This table covers all possible 2-way schemas for any two factors corresponding to the columns. For example, consider factor Payment Server and User Browser, all 2 × 3 = 6 schemas, (Master, ­, ­, Firefox, ­) (Master, ­, ­, Explorer, ­), (Master, ­, ­, Chrome, ­), (VISA, ­, ­, Firefox, ­), (VISA, ­, ­, Explorer, ­), (VISA, ­, ­, Chrome, ­), can be found in the table. For convenience, if several groups of gi factors (gi < n) have g the same number of levels ak , ak i can be used to represent these factors and their levels. Thus, the covering array can g g g gi = n, be denoted by CA(N ; t, a11 , a22 , . . . , ak k ) where n or CA(N ; t, a ) when g1 = n and a1 = a. For example, the covering array in Table II is a CA(9; 2, 23 32 ). In many software systems, the impacts of the interactions among factors are not uniform. Some interactions may be more prone to trigger system failures, while other may have little or no impact on the system. To effectively detect these different interactions, variable strength (VS) covering arrays can be applied. This can offer different covering strengths

to different groups of factors, and can therefore provide a practical approach to test real applications. Definition 3 (VS Covering Array): A VS covering array, m 1 denoted by VCA(N ; t, a11 . . . akk , CA1 (t1 , bm . . . bp p ), . . . , 1 nq 1 CAj (tj , cn 1 . . . cq )), is an N × n covering array of covering strength t containing one or more sub covering arrays, namely CA1 , . . . , CAj , each of which has covering strength t1 , . . . , tj all larger than t. Consider the e-commerce system shown in Table I. If the interactions of three factors, Payment Server, Web Server, and Business Database, have a higher probability to trigger system failures, then a VS covering array can be constructed. As in Table II, only three more test cases (Table III) are needed to construct the VCA(12; 2, 23 32 , CA(3, 22 31 )). With these 12 test cases, not only are all 2-way schemas of all five factors covered, but also all 3-way schemas of these three factors (Payment Server, Web Server, and Business Database) are covered. B. Covering Array Generation Covering arrays are used as test suites in CT. Covering array generation is the process of test suite construction. It is the most active area in CT with more than 50% of research papers focusing on this field [1]. Due to limited testing resources, all aim to construct a minimal covering array while still maintaining full coverage of combinations. Computational methods have been used widely for covering array generation because they can be applied to any systems. In general, these methods generate all possible combinations first. Then they generate test cases to cover these combinations one-by-one. One-testat-a-time is the most widely used strategy among evolutionary algorithms to generate a covering array. The one-test-at-a-time strategy was popularized by AETG [28] and was further used by Bryce and Colbourn [29]. This strategy takes the model of SUT(n, 1 , . . . , n ) where n is the number of factors and i is the number of valid levels of factor i, and the covering strength t as input. At first, an empty test suite TS and a set S of t-way schemas to be covered are initialized. In each iteration, a test case is generated with the highest fitness value according to some heuristic techniques. Then it is added to TS and the t-way schemas covered by it are removed. When all the t-way schemas have been covered, the final test suite TS is returned. This process is shown in Algorithm 1. In this strategy, a fitness function must be used to evaluate the quality of a candidate test case (line 6 in Algorithm 1). It is an important part of all heuristic techniques. In covering array generation, the fitness function takes the test case as the input and then outputs a fitness value representing its "goodness." It is defined as follows.

578

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

Algorithm 1 One-Test-at-a-Time Strategy 1: Input: SUT(n, 1 , . . . , n ) and covering strength t 2: Output: covering array TS 3: TS =  4: Construct S (all the t-way schemas to be covered) based on n, 1 , . . . , n and t 5: while S =  do 6: Generate a test case p with the highest fitness value according to some heuristics 7: Add p to the test suite TS 8: Remove the t-way schemas covered by p from S 9: end while 10: return TS

its corresponding position: vi,j (k + 1) =  × vi,j (k) + c1 × r1,j × (pbesti,j - xi,j (k)) (2) + c2 × r2,j × (gbestj - xi,j (k)) xi,j (k + 1) = xi,j (k) + vi,j (k + 1). (3) The best position of particle i in its history is pbesti , and gbest is the best position among all particles. The velocityupdate equation (2) captures the three basic concepts of PSO. The first is inertia, the tendency of the particle to continue in the direction it has been moving. The second is memory of the best position ever found by itself. The third is cooperation using the best position found by other particles. The parameter  is inertia weight. It controls the balance between exploration (global search state) and exploitation (local search state). Two positive real numbers c1 and c2 are acceleration constants that control the movement tendency toward the individual and global best position. Most studies set  = 0.9, and c1 = c2 = 2 to get the best balance [17], [31], [32]. In addition, r1,j and r2,j are two uniformly distributed random numbers in the range of [0.0, 1.0], used to ensure the diversity of the population. If the problem domain (the search space of particles) has bounds, a bound handling strategy is adopted to keep the particles inside the search space. Many different strategies have been proposed [33]. In the reflecting strategy, when a particle exceeds the bound of the search space in any dimension, the particle direction is reversed in this dimension to get back to the search space. For example, in case of a dimension with a range of values from 0 to 2, if a particle moves to 3, its position is reversed to 1. In addition, as the velocity can increase over time, a limit is set on velocity to prevent an infinite velocity or invalid position for the particle. Setting a maximum velocity, which determines the distance of movement from the current position to the possible target position, can reduce the likelihood of explosion of the swarm traveling distance [31]. Generally, the value of the maximum velocity is selected as i /2, where i is the range of dimension i. The pseudocode in Algorithm 2 presents the process of generating a test case by PSO. This algorithm can be invoked by line 6 of Algorithm 1 to generate a test case for t-way schemas. The n factors of the test case can be treated as an n-dimensional hyperspace. A particle pi = (x1 , x2 , . . . , xn ) can be regarded as a candidate test case. The fitness function is that of Definition 4, the number of uncovered t-way schemas in the generated test suite that are covered by particle pi . PSO employs real numbers but the valid values are integers for covering array generation, so each dimension of particle's position can be rounded to an integer while maintaining the velocity as a real number. This method is used in all prior research applying PSO to covering array generation [13]­[16]. III. R ELATED W ORK In this section, three different but related aspects are discussed. We first summarize search-based CT, especially the current applications of PSO for covering array generation. Then, we introduce prior research on discrete versions of PSO,

Definition 4 (Fitness Function): Let TS be the generated test set, and p be a test case. Then fitness(p) is the number of uncovered t-way schemas in TS that are covered by p. The fitness function can be formulated as fitness(p) = |schemat ({p}) - schemat (TS)| (1)

where schemat (TS) represents the set of all t-way schemas covered by test set TS, and | · | stands for cardinality. When t t-way schemas covered by p are not covered by TS, all Cn the fitness function reaches the maximum value fitness(p) = t. |schemat ({p})| = Cn For example, consider the 2-way covering array generation of the e-commerce system shown in Table II. Suppose that TS consists of test cases (0, 0, 0, 0, 0) and (0, 0, 1, 1, 1). The fitness function computes the fitness value for a candidate test case p = (1, 0, 1, 2, 2) as follows: because p covers ten 2-way schemas, namely schema2 ({p}) = {(1, 0, ­, ­, ­), (1, ­, 1, ­, ­,), (1, ­, ­, 2, ­), (1, ­, ­, ­, 2), (­, 0, 1, ­, ­), (­, 0, ­, 2, ­), (­, 0, ­, ­, 2), (­, ­, 1, 2, ­), (­, ­, 1, ­, 2), (­, ­, ­, 2, 2)} and TS only covers (­, 0, 1, ­, ­) in p, the function returns fitness(p) = 9. C. PSO PSO is a swarm-based evolutionary computation technique. It was developed by Kennedy et al. [30], inspired by the social behavior of bird flocking and fish schooling. PSO utilizes a population of particles as a set of candidate solutions. Each of the particles represents a certain position in the problem hyperspace with a given velocity. A fitness function is used to evaluate the quality of each particle. Initially, particles are distributed in the hyperspace uniformly. Then each particle repeatedly updates its state according to the individual best position in its history (pbest) and the current global best position (gbest). Eventually, each particle possibly moves toward the direction of the individual optimum and global optimum, and finds an optimal or near optimal solution. Suppose that the problem domain is a D-dimensional hyperspace. Then the position and velocity of particle i can be represented by xi  RD and vi  RD respectively. CPSO uses the following equations to update a particle's velocity and position, where vi,j (k) represents the jth component of the velocity of particle i at the kth iteration, and xi,j (k) represents

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

579

Algorithm 2 Generate Test Case by PSO 1: Input: SUT(n, 1 , . . . , n ), covering strength t and the related parameters of PSO 2: Output: the best test case gbest 3: it = 0, gbest = NULL 4: for each particle pi do 5: Initialize the position xi and velocity vi randomly 6: end for 7: while it < maximum iteration do 8: for each particle pi do 9: Compute the fitness value fitness(pi ) 10: if fitness(pi ) > fitness(pbesti ) then 11: pbesti = pi 12: end if 13: if fitness(pi ) > fitness(gbest) then 14: gbest = pi 15: end if 16: end for 17: for each particle pi do 18: Update the velocity and position according to Equations 2 and 3 19: Apply maximum velocity limitation and bound handling strategy 20: end for 21: it = it + 1 22: end while 23: return gbest

applied PSO to 2-way covering array generation. They further used a test suite minimization algorithm to reduce the size of the generated covering array. Current applications of PSO for covering array generation can yield smaller covering arrays than most greedy algorithms, but they all apply the same rounding operator to the particle's position, and they lack guidelines on the parameter settings. B. Discrete Versions of PSO PSO was initially developed to solve problems in continuous space, but PSO can also be applied to some discrete optimization problems including binary or general integer variables. Many discrete versions of PSO have been proposed [17]­[22]. Chen et al. [18] classify existing algorithms into four types. 1) Swap operator-based PSO [19] uses a permutation of numbers as position and a set of swaps as velocity. 2) Space transformation-based PSO [20] uses various techniques to transform the position, defined as a vector of real numbers, to the corresponding solution in discrete space. 3) Fuzzy matrix-based PSO [21] defines the position and velocity as a fuzzy matrix, and decode it to a feasible solution. 4) Incorporating-based PSO [22] consists of hybrid methods incorporating some heuristic or problem dependent techniques. Chen et al. [18] also propose a S-PSO method based on sets with probabilities, which we later adapt to represent a particle's velocity. S-PSO uses a set-based scheme to represent the discrete search space. The velocity is defined as a set with probabilities, and the operators are all replaced by procedures defined on the set. They extend some PSO variants to discrete versions and test them on the traveling salesman problem and the multidimensional knapsack problem. They show that the discrete version of CLPSO [25] can perform better than other variants. S-PSO was found to characterize the combinatorial optimization problems very well. Gong et al. [36] then employed such a set-based representation scheme for solving vehicle routing problems to obtain a new method, S-PSO-VRPTW. C. PSO Variants The original PSO may become trapped in a local optimum. In order to improve the performance, many variants have been proposed [17], [24]­[27]. Chen et al. [37] classify these variants into four different groups. The first group of variants aims to adjust the control parameters during the evolution, for example by decreasing inertia weight  linearly from 0.9 to 0.4 over the search process. The second group employs different neighborhood topologies. The basic local version of PSO makes the particle learn from the local best position lbesti found by particle i's neighborhood instead of the global best position gbest. RPSO and VPSO are two common versions which use a ring topology and a Von Neumann topology, respectively. The third group uses hybrid strategies with other search techniques. Many types of operators, such as genetic operators

to improve CPSO for discrete problems. Finally, some PSO variants are introduced. These try to enhance PSO for conventional continuous problems by adopting strategies that can further improve covering array generation. A. PSO in Search-Based CT SBSE has grown quickly in recent years. Many problems in software engineering have been formulated into searchbased optimization problems, and heuristic techniques have been used to find solutions. Software testing is a major topic in software engineering. Many heuristic techniques have also been applied to this field, including functional testing, mutation testing, stress testing, temporal testing, CT, and regression testing. Currently, many classic heuristic techniques, such as SA [3]­[7], GA [8]­[10], and ACO [9], [11], [12] have been applied to generate uniform and VS covering arrays successfully. PSO has also been applied to software testing. Windisch et al. [34] applied PSO to structural testing, and compared its performance with GA. They showed that PSO outperformed GA for most cases in terms of effectiveness and efficiency. Ganjali [35] proposed a framework for automatic test partitioning based on PSO, observing that PSO performed better than other existing heuristic techniques. PSO has been applied to covering array generation. Ahmed and Zamli [15] proposed frameworks of PSO to generate different kinds of covering arrays, including t-way (PSTG [14]) and VS-PSTG. Then they extended PSTG to support VS covering array generation [16]. Chen et al. [18] also

580

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE IV D IFFERENT G ROUPS OF DPSO AND PSO VARIANTS

(selection, crossover, and mutation), evolutionary computation paradigms and biology inspired operators have been used. The fourth uses multiswarm techniques. Several sets of swarms optimize different components of the solution concurrently or cooperatively. In our experiments, four representative PSO variants are included, as follows. 1) Ratnaweera et al. [24] proposed a typical variant of the first group, TVAC, which uses a time varying inertia weight and acceleration constant to adjust the parameters. 2) Liang et al. [25] proposed the CLPSO method, a wellknown variant of the second group, which allows the particle to learn from other particles' individual best positions in different dimensions. 3) Zhan et al. [26] proposed an adaptive PSO (APSO) that can be seen as a variant of the third group. They developed an evolutionary state estimation to adaptively control the parameter and used an elitist learning strategy. 4) DMS-PSO [27], a variant of the fourth group, was proposed by Liang and Suganthan [27]. It is characterized by a set of swarms with small sizes and these swarms are frequently regrouped. In summary, Table IV lists the different groups of discrete versions of PSO and PSO variants. IV. DPSO In this section, a new DPSO for covering array generation is presented. We firstly illustrate the weakness of CPSO with a simple example. Then the representation scheme of a particle's velocity and the corresponding redefined operators are introduced. Finally, we give two auxiliary strategies to enhance the performance of DPSO. A. Example In CPSO, a particle's position represents a candidate test case; its velocity, a real vector, represents the movement tendency of this particle in each dimension. This scheme is meaningful in a continuous optimization problem, because an optimal solution may exist near the current best particle's position. So it is desirable to move the particle to this area for further search. This may not hold for covering array generation.

Here, we use CA(N ; 2, 34 ) as an example. Suppose that three test cases (0, 1, 0, 1), (2, 1, 1, 2), and (1, 2, 0, 0) have been generated and added to TS in Algorithm 1, and the fourth one is to be generated according to CPSO. A possible candidate particle pi may have the position (0, 0, 0, 0) and velocity (0.5, 0.6, -0.4, 0.2) with fitness value 4, and its individual best position pbesti may be (0, 0, 1, 1) with fitness value 5. The global best position gbest may be (0, 2, 2, 2) with fitness value 6. According to the update (2), if we take  = 0.9 and c1 × r1 = c2 × r2  0.65, in the next iteration, pi 's velocity may be transformed to (0.45, 1.84, 1.59, 2.13). Thus, pi moves to the new position (0, 1, 1, 1) with a limitation on the velocity (the velocity is kept in [- i /2, i /2]), or (0, 1, 1, 2) with no limitation. In both cases, pi only has fitness value 2 after updating. When this occurs, pi evolves to a worse situation, although its position is closer to the pbesti and gbest than its original one. Analyzing the fitness measurement, the main contribution to the fitness value is the combinations that the test case can cover, not the concrete "position" at which it is located. For example, test case (2, 1, 1, 2) has a larger fitness value than (0, 0, 0, 0) because it covers six new schemas [(2, 1, ­, ­), (2, ­, 1, ­), (2, ­, ­, 2) etc.], not because of its relative distance to other particles. B. DPSO To overcome this weakness, the movement of particles should be modified. Inspired by the set-based representation scheme of velocity, DPSO is designed as a version of S-PSO [18] to make the particle learn from the individual and global best more effectively when generating covering arrays. Unlike S-PSO, the element of the velocity set in DPSO is designed for covering array generation, and DPSO does not classify the velocity set into different dimensions to avoid the inconsistency of different dimensions when updating velocities in S-PSO. In DPSO, a particle's position represents a candidate test case, while its velocity is changed to a set of t-way schemas with probabilities. Other than the velocity's representation and the newly defined operators, the evolution procedure of DPSO is the same as CPSO (Algorithm 2). Definition 5 (Velocity): The velocity is a set of pairs (s, p(s)), where s is a possible t-way schema of the covering array and p(s) is a real number between 0 and 1 representing the probability of the selection of schema s to update the current position. In the initialization of the swarm, the particle's position is t possible different schemas are randomly assigned. Then Cn selected randomly and each of them is assigned a random t pairs form the initial velocprobability p(s)  (0, 1). These Cn ity set of this particle; the size of this set changes dynamically during the evaluation. We consider the same example CA(N ; 2, 34 ). In DPSO, when particle pi is initialized, its position xi (k) may be (0, 0, 0, 0) representing a candidate test case as before, and its velocity vi (k) may be such a set {((1, 1, ­, ­), 0.7), ((0, ­, 0, ­), 0.3), ((­, 0, ­, 1), 0.8), ((0, ­, ­, 2), 0.9),

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

581

(a)

(b)

(c)

(d)

(e)

Fig. 1. Example of pi 's velocity updating. (a) 0.9 × vi (k). (b) 2 × r1 × (pbesti - xi (k)). (c) 2 × r2 × (gbest - xi (k)). (d) vi (k + 1). (e) Final vi (k + 1) where pro1 = 0.5.
2 = 6 ((­, 2, 1, ­), 0.5), ((­, ­, 0, 1), 0.2)} which contains C4 pairs. DPSO follows the conventional evolution procedure (Algorithm 2) that uses (2) and (3) to update the velocity and position of a particle. To adapt the new scheme for velocity in Definition 5, the related operators in these equations must be redefined. 1) Coefficient × Velocity: The coefficient is a real number which may be a parameter or a random number. It modifies all the probabilities in the velocity. Definition 6 (Coefficient × Velocity): Let a be a nonnegative real number and v be a velocity, a × v = {(s, p(s) × a)|(s, p(s))  v}. (If p(s) × a > 1, p(s) × a = 1.) For example, Fig. 1(a) shows the result for  × vi (k) where  = 0.9. 2) Position­Position: The difference of two positions gives the direction on which a particle moves. The results of the minus operator is a set of (s, p(s)) pairs, as velocity. Definition 7 (Position­Position): Let x1 and x2 be two positions. Then x1 - x2 = {(s, 0.5)|s is a schema that exists in x1 but not in x2 }. In the newly generated schema, probability p(s) for s is set to 0.5 so that the acceleration constants take similar values in both CPSO and DPSO. As in (2), the result of position­position is multiplied by ci × ri . In CPSO, ci is often set to 2 and ri is a random number between 0 and 1 (recall Section II-C). In DPSO, we want the value of final probability to have a range between 0 and 1 after multiplying by ci . Setting p(s) to 0.5 puts the result in [0, 1] when ci = 2. For example, suppose that xi (k) = (0, 0, 0, 0), pbesti = (0, 0, 1, 1), and gbest = (0, 2, 2, 2) as before. We can get pbesti - xi (k) = {((0, ­, 1, ­, ­), 0.5), ((0, ­, ­, 1), 0.5), ((­, 0, 1, ­), 0.5), ((­, 0, ­, 1), 0.5), ((­, ­, 1, 1), 0.5)}. Fig. 1(b) and (c) shows the results for 2 × r1 × (pbesti - xi (k)) and 2 × r2 × (gbest - xi (k)) respectively. 3) Velocity + Velocity: The addition of velocities gives a particle's movement path. The plus operator results in the union of two velocities. Definition 8 (Velocity + Velocity): Let v1 and v2 be two velocities. Then v1 + v2 = {(s, p(s))| if (s, p1 (s))  v1 and (s, p2 (s))  v2 , p(s) = p1 (s) + p2 (s); if (s, pi (s))  v1 and (s, pi (s))  / v2 or (s, pi (s))  v2 and (s, pi (s))  / v1 , p(s) = pi (s)}. (If p1 (s) + p2 (s) > 1, p1 (s) + p2 (s) = 1.) For example, Fig. 1(d) shows the results for pi 's new velocity vi (k + 1) after updating velocity.

Algorithm 3 Position Updating 1: Input: position xi (k), velocity vi (k + 1), pro2 and pro3 2: Output: new position xi (k + 1) 3: xi (k + 1) = (­, ­, . . . , ­) 4: Sort vi (k + 1) in descending order of p(s) 5: for each pair (si ,p(si )) in vi (k + 1) do 6: Generate a random number   [0, 1] 7: if  < p(si ) then 8: for each fixed level in si do 9: Generate a random number   [0, 1] 10: if  < pro2 and the corresponding factor of has not been fixed in xi (k + 1) then 11: Update xi (k + 1) with 12: end if 13: end for 14: end if 15: end for 16: if xi (k + 1) has unfixed factors then 17: Fill these factors by the same levels of previous position xi (k) 18: end if 19: Generate a random number   [0, 1] 20: if  < pro3 then 21: randomly change the level of one factor of xi (k + 1) 22: end if 23: return xi (k + 1)

In the velocity-updating phase of DPSO (2), we introduce a new parameter pro1 to control the size of the final velocity set. If p(si ) < pro1 , the pair (si , p(si )) is removed from the final velocity. For example, if we set pro1 = 0.5, vi (k + 1) is removed as shown in Fig. 1(e). Here, the velocity has been sorted in descending order of p(s), where if several p(s) have the same value, they are in an arbitrary order. If vi (k + 1) becomes empty, it stays empty until new pairs are added to it. As long as the velocity is empty, the particle's position is not updated and no better solutions can be found from this particle. In Section IV-C1, we discuss how to reinitialize this particle. 4) Position + Velocity: Position plus velocity is the position updating phase. Algorithm 3 gives the pseudo code of this procedure. Here, two new parameters, pro2 and pro3 , numbers in the range [0, 1], are introduced. pro2 is used to determine

582

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

the probability of selecting each fixed level in schema s and pro3 is a mutation probability to make the particle mutate randomly. An example helps to describe this procedure. We already have pi 's current position xi (k) = (0, 0, 0, 0), and its updated velocity vi (k + 1) in descending order of p(s) as shown in Fig. 1(e). We also assume that pro2 and pro3 are both set to 0.5. Each schema si here is selected to update the position with probability p(si ). For the first pair ((0, ­, ­, 2), 1.0), suppose that the random number  satisfies  < 1.0. Then, for each fixed level of this pair, namely level 0 of the first factor and level 2 of the fourth factor, its corresponding factor has not been fixed in xi (k + 1). Suppose that we have the first  < 0.5 but the second  > 0.5, the first factor will be selected to update the position and the second factor will not. So the new position becomes (0, ­, ­, ­). For the second pair, we regenerate the random number  , and compare it with the probability 0.9. If  < 0.9, the second pair is selected. If we generate  < 0.5 in two rounds, the new position becomes (0, 2, 2, ­). Accordingly, if the third pair is selected, and its second factor's level 0 is chosen to update, it does not change position because this factor has been set to a fixed level 2. This procedure is repeated until all factors in the new position xi (k + 1) are set to fixed levels. If all pairs in velocity have been considered, unfixed factors of xi (k + 1) are filled by the same levels of previous position xi (k). For example, after finishing the For loop in line 15, if the fourth factor of xi (k + 1) has not been given any level, the fourth factor of xi (k) is used to update it. Then xi (k + 1) becomes (0, 2, 2, 0). C. Auxiliary Strategies Auxiliary strategies can be introduced to improve the performance of PSO. Two of them are added to DPSO: 1) particle reinitialization to make the search more effective and 2) additional evaluation of gbest to improve the selection of the global best test case. 1) Particle Reinitialization: The PSO algorithm starts with a random distribution of particles, which finally converge. Then the best position that has been found is returned. The swarm may jump out of a local optimum, but this can not be guaranteed because CPSO lacks specific strategies for this. When applying PSO for covering array generation, increasing the number of iterations does not improve the ability to escape a local optimum. Hence, particle reinitialization, a widely used method, is employed to help DPSO to jump out of the local optimum. The main issue with particle reinitialization is when to reinitialize the particle. We may select a threshold, so that the reinitialization is done when the number of iterations exceeds the threshold. In DPSO, a better method can be applied. Using the new representation for velocity, when the current particle pi 's position equals its individual best position pbesti and global best position gbest, the size (norm) of pi 's velocity reduces gradually, because no pairs are generated from (pbesti - xi ) and (gbest - xi ), and the original pairs in velocity are removed gradually under the influence of  × v (reduce the p(s) of original pairs) and parameter pro1 . After a few fluctuations around gbest, the particle may stay at gbest, and

TABLE V T WO D IFFERENT C ONSTRUCTIONS OF CA(N ; 2, 34 )

its size (norm) of velocity is zero. We can use this scenario to trigger reinitialization of the particle. When the reinitialization is done, each dimension of a particle's position is randomly assigned a valid value, and its velocity is regenerated as in the initialization of the swarm. 2) Additional Evaluation of gbest: Current PSO methods to generate covering arrays generally employ the fitness function in Definition 4. This fitness function only focuses on the current candidate, and does not consider the partial test suite TS. Consider the generation of CA(N ; 2, 34 ). Table V shows two different construction processes. Both constructions generate (0, 0, 0, 0) as the first test case. Then they choose different test cases, but each of the first three reaches the largest number of 2 = 6. The difference between newly covered combinations, C4 these two constructions emerges when generating the fourth test case. In Construction 1, because the combinations with the same level between any two factors have all been covered, we cannot find a new test case that can still cover six combinations. However, in Construction 2, such a new test case can be found, (1, 0, 1, 2). Because the minimum size of CA(N ; 2, 34 ) is 9, each test case is required to cover six new combinations. Thus, Construction 1 cannot generate the minimum test suite, but Construction 2 can. In general, there may exist multiple test cases with the same highest fitness value, which make them equally qualified to be gbest in Algorithm 2. Instead of arbitrarily selecting one as gbest, it is better to apply additional distance metric to select one among them. As shown in Table V, if the new test case is similar to the existing tests [as (0, 1, 1, 1) is closer to (0, 0, 0, 0) than (1, 1, 1, 1)], there may be a better chance to find test cases with larger fitness subsequently. In order to measure the "similarity" between a test case t and an existing test suite TS, we use the average Hamming distance. The Hamming distance d12 indicates the number of factors that have different levels between two test cases t1 and t2 . Hence, the similarity between t and TS can be defined by the average Hamming distance H (t, TS) = 1 |TS| dtk .
kTS

(4)

The new gbest selection strategy selects the best candidate gbest based on two metrics. A test case that has the minimum average Hamming distance (i.e., the maximum similarity) from the set of test cases that all have the same highest fitness value is selected as gbest in Algorithm 2. For example, when we generate the second test case for CA(N ; 2, 34 ) in Table V, although (1, 1, 1, 1) and (0, 1, 1, 1) both have the highest fitness value 6, (0, 1, 1, 1) is selected as gbest

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

583

TABLE VI C OMPARING P ERFORMANCE B ETWEEN CPSO AND DPSO

because its average Hamming distance, 3, is smaller than that of (1, 1, 1, 1), 4. We expect that this additional evaluation can enhance the probability of generating a smaller test suite. In addition, because we still want to make the particle follow the conventional search behavior on the individual best direction, this additional distance metric will not be used in updating the pbest of DPSO. V. E VALUATION AND PARAMETER T UNING In this section, we first evaluate the effectiveness of DPSO in some representative cases, and compare the results against CPSO. Then the optimal parameter settings for both CPSO and DPSO are explored. The goal of evaluation and parameter tuning is to make the size of generated covering array as small as possible. Five representative cases of covering arrays, listed below, are selected for our experiments CA1 (N ; 2, 610 ) CA2 (N ; 3, 57 ) CA3 (N ; 4, 39 ) CA4 (N ; 2, 43 53 62 ) VCA(N ; 2, 315 , CA(3, 34 )). We consider four independent parameters, iteration number (iter), population size (size), inertia weight (), and acceleration constant (c), which play similar roles in both CPSO and DPSO, and three new parameters for DPSO, pro1 , pro2 , and pro3 . We carry out a base choice experiment to study the impact of various values of these parameters on CPSO and DPSO's performance and find the recommended settings for them. First, a base setting is chosen as the basic configuration. Then the value of each parameter is changed to create a series of configurations, while leaving the other parameters unchanged. Initially, we set iter = 50, size = 20,  = 0.9, c = 1.3, and pro1 = pro2 = pro3 = 0.5 as a basic configuration based on the previous studies [13], [16] and our empirical experience. To obtain statistically significant results, the generation of each case of covering array is executed 30 times. A. Evaluation of DPSO We compare the performance between CPSO and DPSO with the basic configuration. Five classes of covering arrays are generated by these two algorithms. The sizes obtained and average execution time per test case are shown as CPSO1 and DPSO in Table VI. The best and mean array sizes of DPSO are all better than those of CPSO1 . The new discrete representation scheme and auxiliary strategies improve PSO for covering array generation. DPSO can produce smaller covering arrays than CPSO with the basic configuration. However, DPSO spends more time

during the evolution, because its new operators for updating are more intricate than the conventional ones. DPSO needs to deal with many elements of the velocity set, whereas the conventional scheme only needs simple arithmetic operations. To compare the performance between CPSO and DPSO given the same execution time, for each case we let the execution time per test case for CPSO equal to that for DPSO, so that CPSO can spend more time in searching. We refer to this version of CPSO as CPSO2 . In addition, a t-test between CPSO2 and DPSO is conducted and the corresponding p-value is presented. A p-value smaller than 0.05 indicates that the performance of these two algorithms is statistically different with 95% confidence. From CPSO1 , CPSO2 , and DPSO in Table VI, the performance of CPSO is improved with more search time. DPSO, which must use fewer iterations, still works better than CPSO. The effectiveness of DPSO comes from the essential improvement of its new representation scheme for velocity and the auxiliary strategies. The results of the t-test demonstrate the significance of these differences. Therefore, we can conclude that DPSO performs better than CPSO with fewer iterations for covering array generation. B. Parameter Tuning In the base choice experiment, because the sizes of the various covering arrays differ, the mean size of each case (si ) obtained is normalized using si = si - smin . smax - smin

This normalization enables the graphical representation of different cases on a common scale. Because some parameters may not significantly impact the performance, we use ANOVA (significance level = 0.05) to test whether there exist significant differences among the mean results obtained by different parameter settings. When changing the parameter settings have no significant impact on the generation results, these results will be presented as dotted lines in the corresponding figures. For example, CA4 is presented as a dotted line in Fig. 2(b). It means that the iteration number does not significantly impact the generation of this case of covering array, and so this case will not be further considered when identifying the optimal settings. 1) Iteration Number (iter): Iteration number determines the number of updates. PSO typically requires thousands of iterations to evolve. Nevertheless, only a few iterations are required to generate covering array according to [14]­[16]. So we

584

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

(a)
Fig. 2. Comparing array sizes under different iteration numbers. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 3. Comparing array sizes under different population sizes. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 4.

(b)

Comparing array sizes under different settings of population size and iteration number. (a) CPSO. (b) DPSO.

change its value from 50, the base setting, to 1950 incrementally. Much larger values are not used because the execution time may increase markedly without a commensurate increase in the quality of the results. Fig. 2 shows the results for different choices of iteration numbers, where each line represents a covering array. Performance is improved with increasing iterations for both CPSO and DPSO. A small number of iterations may not be appropriate due to insufficient searching. Because the optimal settings are different among different cases, and several settings can be chosen to generate the minimum covering arrays, it is open to debate which setting is the "best" one. Given time constraints, for a population size of 20, a good setting of iteration number could be approximately 1000 for both CPSO and DPSO. 2) Population Size (Size): Population size determines the initial search space. Generally a small value, e.g., 20, is satisfactory for most cases. A large population size may bring a higher diversity and find a better solution, but it also increases the evolving time. For the same reason as before, we change the value of population size from 10 to 100.

Fig. 3 shows the results for different choices of population size. There is no doubt that the mean array size obtained decreases as population size increases. A large population size can have more chances to generate smaller covering arrays, but its execution time can become prohibitive. In addition, because iteration number and population size together determine the search effort of PSO, we explore the combinations between these two parameters. We let iter × size be a constant 20 000, and generate each case under different settings of these two parameters. Fig. 4 shows the results, where PSO prefers a relatively larger population size. In CPSO, ten particles with 2000 iterations is the worst setting, and most cases produce good results with 60 or 80 particles. In DPSO, the best choice of population size is still 80. In both CPSO and DPSO, the largest population size 100 cannot produce as good results as that of 80 due to having fewer iterations. So achieving smallest covering arrays requires a good balance between these two parameters, and a moderately large population size is necessary for both CPSO and DPSO. Thus, we can set iteration number to 250, and population size to 80, as the recommended settings for both CPSO and DPSO.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

585

(a)
Fig. 5. Comparing array sizes under different inertia weights. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 6. Comparing array sizes under different acceleration constants. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 7.

(b)

(c)

Comparing array sizes under different settings of (a) pro1 , (b) pro2 , and (c) pro3 . TABLE VII R ECOMMENDED PARAMETER S ETTINGS

3) Inertia Weight (): Inertia weight determines the tendency of the particle to continue in the same direction. A small value help the particle move primarily toward the best position, while a large one is helpful to continue its previous movement. A linearly decreasing value is also used as it can make the swarm gradually narrow the search space. Here, we investigate both fixed values and a linearly decreasing value from 0.9 to 0.4 over the whole evolution (presented as "dec"). Fig. 5 shows the results for different choices of inertia weight. In CPSO, most of the smallest covering arrays are generated by large fixed inertia weights. The decreasing value does not perform as well as the large values, such as 0.9 for CA1 , CA2 , and CA4 . So 0.9 can be the recommended choice for CPSO. In DPSO, 1.1 is the worst choice because the particle may update its position based on its own velocity and thus fail to learn from individual and global best positions. The decreasing value can perform reasonably well, but a fixed value 0.5 may be a better choice in that it keeps the effort of global search moderate. Thus, we can recommend the fixed inertia weight of 0.9 for CPSO, and 0.5 for DPSO. 4) Acceleration Constant (c): Acceleration Constants c1 and c2 control the learning from pbest and gbest, respectively. Generally, they are set to the same value to balance

the influence of these two positions. Setting c1 and c2 to a large value may make the particle more likely attracted to the best position ever found, while a small one may make the search far from the current optimal region. In this paper, we set c1 = c2 = c, and vary c from 0.1 to 2.9. Fig. 6 shows the results for different choices of acceleration constant. Unlike other parameters, there is a consistent trend in all five cases. In CPSO, the values larger than 0.5 all produce good results. In DPSO, 1.3 can definitely be regarded as the optimal value. Thus, we set 1.3 as the recommended value of acceleration constant for both CPSO and DPSO. 5) pro1 , pro2 , and pro3 : These three parameters are new to DPSO. Parameter pro1 determines the size of the final velocity. As those pairs whose probabilities are smaller than pro1 are removed from the final velocity set, a small value may keep more pairs in the velocity for the next evolution, but it

586

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE VIII S IZES (N ) OF C OVERING A RRAYS FOR n FACTORS , E ACH H AVING T HREE L EVELS , W ITH C OVERING S TRENGTH t , CA(N ; t, n, 3)

also requires more computation time. Parameter pro2 determines the probability of selecting each fixed level from each schema when updating positions. A larger value may lead to a quick construction of new position, but it may also lead to fast convergence toward a local optimum. Parameter pro3 determines the mutation probability when updating positions. A larger value may enhance the randomness, but it also lowers the convergence speed. In this paper, values from 0.1 to 1.0 for these three parameters are investigated. Fig. 7 shows the results. For pro1 , a large value is not appropriate because it removes nearly all pairs from the final velocity set. A medium value 0.5, which appears to lead to the best result, may be the best choice. For pro2 , the smallest value 0.1 yields the best results. Slower construction of the new position may slow the convergence toward a local optimum, but it also takes longer. So, in order to balance performance and execution time, we take 0.3 as the recommended value for pro2 . For pro3 , a larger value may be a good choice. The frequent mutation of new position may bring better results, but also takes longer to converge. So, we take 0.7 as the recommended value for pro3 . In summary, the recommended parameter settings for PSO for covering array generation are different from previously suggested ones [13], [16]. Some parameters may significantly impact the performance in some cases, and parameter tuning is necessary to enhance heuristic techniques for particular applications. Naturally, the optimal settings vary for different cases of covering arrays. There may not exist a common setting that can always lead to the best results. Because it is

impractical to tune parameters for each case in real life, we suggest two general settings for CPSO and DPSO, as shown in Table VII, which can typically lead to better performance within reasonable execution time. VI. C OMPARING A MONG PSO S In this section, we compare the best reported array sizes generated by PSO in [16] with our findings for CPSO, DPSO, and four representative variants. Because the research in [16] demonstrated that their generation results typically outperform greedy algorithms, in this paper, we do not compare CPSO and DPSO with greedy algorithms. We implement both the original and discrete versions of four variants (TVAC [24], CLPSO [25], APSO [26], and DMS-PSO [27]) to generate covering arrays. Their discrete versions are extended based on new representation scheme of velocity and auxiliary strategies. We name them D-TVAC, DCLPSO, D-APSO, and D-DMS-PSO, respectively. In CLPSO, a particle can learn from different particles' pbest in each dimension whereas we do not distinguish dimensions strictly in DPSO. So in D-CLPSO, a particle can fully learn from different particles' pbest in all dimensions. That may weaken the search ability of CLPSO. For the other three variants, they can be directly extended based on DPSO. All algorithms are compared using the same number of fitness function evaluations. CPSO and DPSO use the recommended settings shown in Table VII. For the variants, iter, size and pro1 , pro2 and pro3 for discrete versions are set to

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

587

TABLE IX S IZES (N ) OF C OVERING A RRAYS FOR S EVEN FACTORS , E ACH H AVING

L EVELS , W ITH C OVERING S TRENGTH t , CA(N ; t, 7, )

their recommended values.  and c are also set to recommended values unless they are adaptively adjusted during the evolution, in which case their range is set to [0.4, 0.9] and [0.8, 1.8], respectively. The new control parameters for these variants follow their suggested settings. Tables VIII­XIII give the results. Because of the execution time, we only consider covering strengths from 2 to 4, and the generation of each covering array is repeated 30 times. A t-test (significance level = 0.05) is also conducted to test whether there exists a significant difference between the mean sizes produced by the two algorithms. In the first three columns, we report the best and mean array sizes obtained from previous results, CPSO and DPSO, where boldface numbers indicate that the difference between CPSO and DPSO is significant based on the t-test. In the last four columns, we report the mean array sizes from the original and discrete versions of each PSO variant (presented as meanc and meand respectively), where boldface numbers indicate that the difference between meanc and meand of each variant is significant. A. Uniform Covering Arrays Tables VIII­X present the results for uniform covering arrays. We extend the cases considered in [16], where "­" indicates the not available cases. In Tables VIII, we report array sizes for n factors, each having three levels. In Tables IX and X, we report array sizes for 7 and 10 factors, each having levels. Their covering strengths all range from 2 to 4. Typically, CPSO can produce smaller sizes than those reported in [16], demonstrating the effectiveness of parameter tuning. Furthermore, DPSO can produce the smallest best and mean sizes in almost all cases, and usually the performance of DPSO is significantly better than CPSO. When the covering strength t = 2 is adopted (Table VIII), DPSO does

not generally outperform CPSO. But when covering strength increases, DPSO performs better. Sometimes the mean sizes for DPSO are smaller than the best sizes for CPSO. Because generating a covering array with higher covering strength is more complicated, DPSO may be more appropriate for generating such covering arrays. In Tables IX and X, we also find that DPSO is superior for covering arrays with high covering strength. Overall, these results show the effectiveness of DPSO for uniform covering array generation. Surprisingly, DPSO does not beat previous results for CPSO when covering arrays have two levels for each factor (Table IX). This appears to be a weakness of DPSO. Although DPSO generates smaller covering arrays than previous results and CPSO, other techniques may still yield better results than DPSO (some best known sizes can be found in [38]). B. VS Covering Array Tables XI­XIII give the results for VS covering arrays. Based on CA(N ; 2, 315 ), CA(N ; 3, 315 ), and CA(N ; 2, 42 52 63 ), some different cases of sub covering arrays conducted in [16] are examined. Their covering strengths are at most 4. Generally, we can draw similar conclusions as for uniform covering arrays. CPSO with the suggested parameter setting can produce better results than reported sizes in some cases. DPSO also usually beats them on the best and mean sizes. In Table XI, often the difference between CPSO and DPSO is not significant. In part this is because for the CA(3, 33 ), CA(3, 33 )2 , CA(3, 33 )3 , and CA(4, 34 ), the same best results are provably the minimum (e.g., the minimum size of the CA(3, 33 ) is 3 × 3 × 3 = 27). For the other cases, although sometimes the difference is not significant, DPSO can still generate smaller covering arrays. DPSO remains a good choice for generating VS covering arrays. In Tables XII and XIII, similar results can be found.

588

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE X S IZES (N ) OF C OVERING A RRAYS FOR T EN FACTORS , E ACH H AVING

L EVELS , W ITH C OVERING S TRENGTH t , CA(N ; t, 10, )

TABLE XI S IZES (N ) OF VS C OVERING A RRAYS VCA(N ; 2, 315 , CA)

TABLE XII S IZES (N ) OF VS C OVERING A RRAYS VCA(N ; 3, 315 , CA)

For both uniform and variable cases of covering arrays, parameter tuning can enhance CPSO to generate smaller covering arrays than previous work. DPSO can produce the smallest covering arrays in nearly all cases, and in general the difference of performance between CPSO and DPSO is significant. DPSO is an effective discrete version of PSO for covering array generation. C. PSO Variants In order to further investigate the effectiveness of DPSO for covering array generation, we implement the original versions of four representative variants of PSO and extend them to their discrete versions based on DPSO. We apply them to generate

the same cases for covering arrays. The mean sizes obtained are shown in the last four columns in Tables VIII­XIII. We first compare the mean sizes of original PSO variants with those of CPSO and DPSO. In our experiments, TVAC's mean sizes are always larger than CPSO's. The linear adjustment of inertia weight and acceleration constant is not helpful for CPSO for covering array generation. Typically, APSO's mean sizes are also larger than CPSO's. Because APSO uses a fuzzy system to classify different evolutionary states, its ineffectiveness may result from inappropriate parameter settings. CLPSO and DMS-PSO can outperform CPSO typically. Although on occasion they achieve comparable performance with DPSO, they cannot perform as well as DPSO in most

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

589

TABLE XIII S IZES (N ) OF VS C OVERING A RRAYS VCA(N ; 2, 43 53 62 , CA)

TABLE XIV C OMPARING CPSO AND DPSO W ITH GA AND ACO

cases. That further demonstrates the effectiveness of DPSO for covering array generation. Because these four algorithms are representative PSO variants (as shown in Table IV), designing different neighborhood topologies and using multiswarm techniques may have potential to improve CPSO for covering array generation. We next compare the original and discrete versions of each variant. Except for CLPSO, the other three variants can be improved using their discrete versions, and the improvement is also significant. TVAC cannot outperform CPSO, but it is enhanced by DPSO so that D-TVAC can produce smaller mean sizes than CPSO in most cases. The linear adjustment is helpful for the discrete version. For CLPSO, only in a few cases is it improved by DPSO. Sometimes D-CLPSO even leads to worse results (see Table X), due primarily to the weakened search ability of its discrete version as explained in Section VI. For APSO, DPSO can enhance its original version, but D-APSO is still worse than DPSO. That may result from inappropriate settings as explained before. For DMS-PSO, sometimes DPSO does not enhance it (see Table XI). However, in most cases D-DMS-PSO can outperform DMS-PSO and has comparable performance with D-TVAC. The multiswarm strategy is also helpful for the discrete version.

In summary, the comparison study reveals that our suggested parameter settings are more suitable for covering array generation. DPSO is an effective discrete version of PSO. It can significantly outperform previous results and CPSO in nearly all cases for uniform and VS covering arrays. Furthermore, DPSO's representation scheme of a particle's velocity and auxiliary strategies not only enhance CPSO, but also typically enhance PSO variants. DPSO is a promising improvement on PSO for covering array generation. VII. C OMPARING DPSO W ITH GA AND ACO Because GAs and ACO [8]­[12] have also been successfully used for covering array generation, we compare CPSO and DPSO with the reported array sizes in [9] and [11]. There are no widely accepted benchmarks for the comparison of searchbased covering array generation, so we only consider these two representative and competitive works. Shiba et al. [9] applied both GA and ACO to generate uniform covering arrays (CA1 to CA8 in Table XIV), and Chen et al. [11] applied ACO to generate VS covering arrays (VCA9 to VCA12 in Table XIV). They both set algorithm parameters according to recommendations in related research fields without parameter tuning, while our CPSO and DPSO

590

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

use the recommended settings shown in Table VII. To compare these algorithms under the same number of fitness evaluations, for each case of covering array, the population size and the number of iterations of CPSO and DPSO are modified accordingly to satisfy the settings in [9] and [11]. Moreover, Shiba et al. [9] and Chen et al. [11] both applied test minimization algorithms to further reduce the size of generated covering arrays, while our CPSO and DPSO do not apply any minimization algorithms. Table XIV shows the comparison results, where boldface numbers indicate the best array sizes obtained, and "­" represents that the corresponding data is not available. The generation of each case of covering array of CPSO and DPSO is executed 30 times and the best and average results are presented. Because we do not implement GA and ACO for covering array generation, no statistical tests can be conducted here. In addition, because the platforms used for collecting the results differ, the comparison of computational time would not be informative. We nevertheless present the execution times of our CPSO and DPSO, which can serve as references for practitioners. From Table XIV, DPSO can outperform existing GA and ACO for covering array generation, despite the latter two applying test minimization algorithms. Because our DPSO is a version of PSO designed and tuned for covering array generation, it suggests that tuned versions of GAs and ACOs may be a promising area for further study. In addition, CPSO performs worse than GA and ACO in seven of 12 cases. That may result from the improvement by minimization algorithms in [9] and [11]. But for CA2 , CA3 , and CA6 , CPSO can still achieve smaller covering arrays. In summary, the results further demonstrate that DPSO is an effective discrete version of PSO for covering array generation. Further investigations of tuned versions of GA and ACO should be considered. VIII. C ONCLUSION Covering array generation is a key issue in CT. We developed a new DPSO for covering array generation, by adapting S-PSO to generate covering arrays and incorporating two auxiliary strategies to improve the performance. Parameter tuning was applied to both CPSO and DPSO to identify their best parameter settings. The original and discrete versions of four representative PSO variants were implemented and their efficacy for covering array generation was compared. The performance of DPSO was also compared with other existing evolutionary algorithms, GA and ACO. DPSO can perform better than CPSO with fewer iterations, and the performance of CPSO and DPSO is significantly impacted by their parameter settings. Different cases require different parameter settings; there may not exist a single choice that leads to the best results. After parameter tuning, we identified two recommended settings, which lead to relatively good performance for CPSO and DPSO. Indeed, CPSO with our recommended parameter settings improves on previously reported results, and our DPSO with recommended parameter settings usually beats CPSO. Typically, DPSO also significantly

enhances the performance of PSO variants. In addition, DPSO often outperforms GA and ACO to generate covering arrays. Consequently, DPSO is a promising improvement of PSO for covering array generation. Improvements on the methods here may be possible in a number of ways. One would be to investigate further evolution procedures and strategies proposed in PSO, such as hybridizing with penalty approaches to handle discrete unknowns [39], and compare the results with some exact schemes like branch and bound method. A second would be to examine one-column-at-a-time approaches or methods that construct the entire array, rather than the one-row-at-a-time approach adopted here. A third would be to incorporate DPSO with other methods, in particular with test minimization methods. R EFERENCES
[1] C. Nie and H. Leung, "A survey of combinatorial testing," ACM Comput. Surv., vol. 43, no. 2, pp. 11.1­11.29, 2011. [2] D. Kuhn and M. Reilly, "An investigation of the applicability of design of experiments to software testing," in Proc. 27th Annu. NASA Goddard/IEEE Softw. Eng. Workshop, Greenbelt, MD, USA, 2002, pp. 91­95. [3] M. Cohen, P. Gibbons, W. Mugridge, and C. Colbourn, "Constructing test suites for interaction testing," in Proc. 25th Int. Conf. Softw. Eng., Portland, OR, USA, 2003, pp. 38­48. [4] M. B. Cohen, C. J. Colbourn, and A. C. Ling, "Constructing strength three covering arrays with augmented annealing," Discrete Math., vol. 308, no. 13, pp. 2709­2722, 2008. [5] J. Torres-Jimenez and E. Rodriguez-Tello, "Simulated annealing for constructing binary covering arrays of variable strength," in Proc. Congr. Evol. Comput., Barcelona, Spain, Jul. 2010, pp. 1­8. [6] B. Garvin, M. Cohen, and M. Dwyer, "Evaluating improvements to a meta-heuristic search for constrained interaction testing," Empir. Softw. Eng., vol. 16, no. 1, pp. 61­102, 2011. [7] J. Torres-Jimenez and E. Rodriguez-Tello, "New bounds for binary covering arrays using simulated annealing," Inf. Sci., vol. 185, no. 1, pp. 137­152, 2012. [8] S. Ghazi and M. Ahmed, "Pair-wise test coverage using genetic algorithms," in Proc. Congr. Evol. Comput., vol. 2. Canberra, ACT, Australia, 2003, pp. 1420­1424. [9] T. Shiba, T. Tsuchiya, and T. Kikuno, "Using artificial life techniques to generate test cases for combinatorial testing," in Proc. 28th Annu. Int. Comput. Softw. Appl. Conf., vol. 1. Hong Kong, 2004, pp. 72­77. [10] J. McCaffrey, "An empirical study of pairwise test set generation using a genetic algorithm," in Proc. 7th Int. Conf. Inf. Technol. New Gener., Las Vegas, NV, USA, 2010, pp. 992­997. [11] X. Chen, Q. Gu, A. Li, and D. Chen, "Variable strength interaction testing with an ant colony system approach," in Proc. Asia-Pacific Softw. Eng. Conf., Penang, Malaysia, 2009, pp. 160­167. [12] X. Chen, Q. Gu, X. Zhang, and D. Chen, "Building prioritized pairwise interaction test suites with ant colony optimization," in Proc. 9th Int. Conf. Qual. Softw., Jeju-do, Korea, 2009, pp. 347­352. [13] X. Chen, Q. Gu, J. Qi, and D. Chen, "Applying particle swarm optimization to pairwise testing," in Proc. 34th Annu. Comput. Softw. Appl. Conf., Seoul, Korea, 2010, pp. 107­116. [14] B. S. Ahmed and K. Z. Zamli, "PSTG: A T-way strategy adopting particle swarm optimization," in Proc. 4th Asia Int. Conf. Math. Anal. Model. Comput. Simulat., Kota Kinabalu, Malaysia, 2010, pp. 1­5. [15] B. S. Ahmed and K. Z. Zamli, "A variable strength interaction test suites generation strategy using particle swarm optimization," J. Syst. Softw., vol. 84, no. 12, pp. 2171­2185, 2011. [16] B. S. Ahmed, K. Z. Zamli, and C. P. Lim, "Application of particle swarm optimization to uniform and variable strength covering array construction," Appl. Soft Comput., vol. 12, no. 4, pp. 1330­1347, 2012. [17] Y. del Valle, G. Venayagamoorthy, S. Mohagheghi, J.-C. Hernandez, and R. Harley, "Particle swarm optimization: Basic concepts, variants and applications in power systems," IEEE Trans. Evol. Comput., vol. 12, no. 2, pp. 171­195, Apr. 2008.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

591

[18] W.-N. Chen et al., "A novel set-based particle swarm optimization method for discrete optimization problems," IEEE Trans. Evol. Comput., vol. 14, no. 2, pp. 278­300, Apr. 2010. [19] M. Clerc, Discrete Particle Swarm Optimization (New Optimization Techniques in Engineering). New York, NY, USA: Springer, 2004. [20] W. Pang et al., "Modified particle swarm optimization based on space transformation for solving traveling salesman problem," in Proc. Int. Conf. Mach. Learn. Cybern., vol. 4. Shanghai, China, Aug. 2004, pp. 2342­2346. [21] W. Pang, K.-P. Wang, C.-G. Zhou, and L.-J. Dong, "Fuzzy discrete particle swarm optimization for solving traveling salesman problem," in Proc. 4th Int. Conf. Comput. Inf. Technol., Wuhan, China, 2004, pp. 796­800. [22] Y. Wang et al., "A novel quantum swarm evolutionary algorithm and its applications," Neurocomputing, vol. 70, nos. 4­6, pp. 633­640, 2007. [23] E. Campana, G. Fasano, and A. Pinto, "Dynamic analysis for the selection of parameters and initial population, in particle swarm optimization," J. Global Optim., vol. 48, no. 3, pp. 347­397, 2010. [24] A. Ratnaweera, S. Halgamuge, and H. Watson, "Self-organizing hierarchical particle swarm optimizer with time-varying acceleration coefficients," IEEE Trans. Evol. Comput., vol. 8, no. 3, pp. 240­255, Jun. 2004. [25] J. Liang, A. Qin, P. Suganthan, and S. Baskar, "Comprehensive learning particle swarm optimizer for global optimization of multimodal functions," IEEE Trans. Evol. Comput., vol. 10, no. 3, pp. 281­295, Jun. 2006. [26] Z.-H. Zhan, J. Zhang, Y. Li, and H.-H. Chung, "Adaptive particle swarm optimization," IEEE Trans. Syst., Man, Cybern., B, Cybern., vol. 39, no. 6, pp. 1362­1381, Dec. 2009. [27] J. Liang and P. Suganthan, "Dynamic multi-swarm particle swarm optimizer," in Proc. IEEE Swarm Intell. Symp., Pasadena, CA, USA, 2005, pp. 124­129. [28] D. Cohen, S. Dalal, M. Fredman, and G. Patton, "The AETG system: An approach to testing based on combinatorial design," IEEE Trans. Softw. Eng., vol. 23, no. 7, pp. 437­444, Jul. 1997. [29] R. C. Bryce and C. J. Colbourn, "One-test-at-a-time heuristic search for interaction test suites," in Proc. 9th Annu. Conf. Genet. Evol. Comput., London, U.K., 2007, pp. 1082­1089. [30] J. Kennedy and R. Eberhart, "Particle swarm optimization," in Proc. Int. Conf. Neural Netw., vol. 4. Perth, WA, Australia, 1995, pp. 1942­1948. [31] R. Eberhart and Y. Shi, "Particle swarm optimization: Developments, applications and resources," in Proc. Congr. Evol. Comput., vol. 1. Seoul, Korea, 2001, pp. 81­86. [32] R. Poli, J. Kennedy, and T. Blackwell, "Particle swarm optimization," Swarm Intell., vol. 1, no. 1, pp. 33­57, 2007. [33] S. Helwig, J. Branke, and S. Mostaghim, "Experimental analysis of bound handling techniques in particle swarm optimization," IEEE Trans. Evol. Comput., vol. 17, no. 2, pp. 259­271, Apr. 2013. [34] A. Windisch, S. Wappler, and J. Wegener, "Applying particle swarm optimization to software testing," in Proc. 9th Annu. Conf. Genet. Evol. Comput., London, U.K., 2007, pp. 1121­1128. [35] A. Ganjali, "A requirements-based partition testing framework using particle swarm optimization technique," M.S. thesis, Dept. Electr. Comput. Eng., Univ. Waterloo, Waterloo, ON, Canada, 2008. [36] Y.-J. Gong et al., "Optimizing the vehicle routing problem with time windows: A discrete particle swarm optimization approach," IEEE Trans. Syst., Man, Cybern. C, Appl. Rev., vol. 42, no. 2, pp. 254­267, Mar. 2012. [37] W.-N. Chen et al., "Particle swarm optimization with an aging leader and challengers," IEEE Trans. Evol. Comput., vol. 17, no. 2, pp. 241­258, Apr. 2013. [38] C. J. Colbourn. (Apr. 2013). Covering Array Tables for t=2, 3, 4, 5, 6. [Online]. Available: http://www.public.asu.edu/ ccolbou/src/ tabby/catable.html [39] M. Corazza, G. Fasano, and R. Gusso, "Particle swarm optimization with non-smooth penalty reformulation, for a complex portfolio selection problem," Appl. Math. Comput., vol. 224, pp. 611­624, Nov. 2013.

Huayao Wu received the B.S degree from Southeast University, Nanjing, China and the M.S degree from Nanjing University, Nanjing, China, where he is currently working toward the Ph.D. degree from Nanjing University, Nanjing. His research interests include software testing, especially on combinatorial testing and search-based software testing.

Changhai Nie (M'12) received the B.S. and M.S. degrees in mathematics from Harbin Institute of Technology, Harbin, China, and the Ph.D. degree in computer science from Southeast University, Nanjing, China. He is a Professor of Software Engineering with State Key Laboratory for Novel Software Technology, Department of Computer Science and Technology, Nanjing University, Nanjing. His research interests include software analysis, testing and debugging.

Fei-Ching Kuo (M'06) received the B.Sc. (Hons.) degree in computer science and the Ph.D. degree in software engineering from Swinburne University of Technology, Hawthorn, VIC, Australia. She was a Lecturer with University of Wollongong, Wollongong, NSW, Australia. She is currently a Senior Lecturer with the Swinburne University of Technology. Her research interests include software analysis, testing, and debugging.

Hareton Leung (M'90) received the Ph.D. degree in computer science from University of Alberta, Edmonton, AB, Canada. He is an Associate Professor and a Director of the Laboratory for Software Development and Management, Department of Computing, Hong Kong Polytechnic University, Hong Kong. His research interests include software testing, project management, risk management, quality and process improvement, and software metrics.

Charles J. Colbourn received the Ph.D. degree from University of Toronto, Toronto, ON, Canada, in 1980. He is a Professor of Computer Science and Engineering with Arizona State University, Tempe, AZ, USA. He has authored the books The Combinatorics of Network Reliability (Oxford) and Triple Systems (Oxford), and also 320 refereed journal papers focussing on combinatorial designs and graphs with applications in networking, computing, and communications. Prof. Colbourn received the Euler Medal for Lifetime Research Achievement by the Institute for Combinatorics and its Applications in 2004.

Given v, t, and m, does there exist a partial Steiner triple system of order v with t triples whose triples can be ordered so that any m consecutive triples are pairwise disjoint? Given v, t, and m1, m2, . . . , ms with  t=âˆ‘si=1mit=âˆ‘i=1smi  , does there exist a partial Steiner triple system with t triples whose triples can be partitioned into partial parallel classes of sizes m1, . . . , ms? An affirmative answer to the first question gives an affirmative answer to the second when mi â‰¤ m for each  iâˆˆ{1,2,â€¦,s}iâˆˆ{1,2,â€¦,s}  . These questions arise in the analysis of erasure codes for disk arrays and that of codes for unipolar communication, respectively. A complete solution for the first problem is given when m is at most  13(vâˆ’(9v)2/3)+O(v1/3)13(vâˆ’(9v)2/3)+O(v1/3)  .Electronic Journal of Differential Equations, Vol. 2012 (2012), No. 96, pp. 1­11. ISSN: 1072-6691. URL: http://ejde.math.txstate.edu or http://ejde.math.unt.edu ftp ejde.math.txstate.edu

EXISTENCE AND ASYMPTOTIC BEHAVIOR OF SOLUTIONS TO THE GENERALIZED DAMPED BOUSSINESQ EQUATION
YINXIA WANG

Abstract. We consider the Cauchy problem for the n-dimensional generalized damped Boussinesq equation. Based on decay estimates of solutions to the corresponding linear equation, we define a solution space with time weighted norms. Under small condition on the initial value, the existence and asymptotic behavior of global solutions in the corresponding Sobolev spaces are established by the contraction mapping principle.

1. Introduction We study the Cauchy problem of the generalized damped Boussinesq equation in n space dimensions utt - autt - 2but - 3 u +  2 u - u = f (u) with the initial value t=0: u = u0 (x), ut = u1 (x). (1.2) (1.1)

Here u = u(x, t) is the unknown function of x = (x1 , · · · , xn )  Rn and t > 0, a, b, ,  are positive constants. The nonlinear term f (u) = O(u1+ ) and  is a positive integer. The first initial boundary value problem for utt - autt - 2but - 3 u +  2 u - u =  (u2 ) (1.3)

in a unit circle was investigated in [16], where a, b, ,  are positive constants and  is a constant. The existence and the uniqueness of strong solution was established and the solutions were constructed in the form of series in the small parameter present in the initial conditions. The long-time asymptotics was also obtained in the explicit form. In [1], the authors considered the initial-boundary value problem for (1.3) in the unit ball B  R3 , similar results were established. It is well-known that the equation (1.3) is closely contacted with many wave equations. For example, the equation (which we call the Bq equation) utt - uxx + uxxxx = (u2 )xx ,
2000 Mathematics Subject Classification. 35L30, 35L75. Key words and phrases. Generalized damped equation; global solution; asymptotic behavior. c 2012 Texas State University - San Marcos. Submitted May 31, 2012. Published June 10, 2012.
1

2

Y. WANG

EJDE-2012/96

which was derived by Boussinesq in 1872 to describe shallow water waves. The improved Bq equation(which we call IBq equation) is utt - uxx - uxxtt = (u2 )xx . A modification of the IBq equation analogous of the MKdV equation yields utt - uxx - uxxtt = (u3 )xx , which we call the IMBq equation (see [5]). (1.1) is a higher order wave equation. In [8], we considered the Cauchy problem for the Cahn-Hilliard equation with inertial term. Combining high frequency, low frequency technique and energy methods, we obtained global existence and asymptotic behavior of solutions. Wang, Liu and Zhang [13] investigated a fourth wave equation that is of the regularity-loss type. Based on the decay property of the solution operators, global existence and asymptotic behavior of solutions are obtained. For global existence and asymptotic behavior of solutions to higher order wave equations, we refer to [2]-[3] and [6]-[15] and references therein. The main purpose of this paper is to establish global existence and asymptotic behavior of solutions to (1.1), (1.2) by using the contraction mapping principle. Firstly, we consider the decay property of the following linear equation utt - autt - 2but - 3 u +  2 u - u = 0. (1.4)

We obtain the following decay estimate of solutions to (1.4) associated with initial condition (1.2),
k x u(t) L2

 C (1 + t)- 4 - 2 - 2 ( u0

n

k

1

 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

(1.5)

(k  s + 2),
h x ut (t) L2

 C (1 + t)- 4 - 2 -1 ( u0

n

h

 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

(1.6)

(h  s) Based on the estimates (1.5) and (1.6), we define a solution space with time weighted norms. Then global existence and asymptotic behavior of classical solutions to (1.1), (1.2) are obtained by using the contraction mapping principle. We give notation which is used in this paper. Let F [u] denote the Fourier transform of u defined by u ^( ) = F [u] =
Rn

e-i·x u(x)dx,

and we denote its inverse transform by F -1 . For 1  p  , Lp = Lp (Rn ) denotes the usual Lebesgue space with the s norm · Lp . The usual Sobolev space of s is defined by Hp = (I - )-s/2 Lp s = with the norm f Hp (I - )s/2 f Lp ; the homogeneous Sobolev space of s is s - s/  p = (-) 2 Lp with the norm f H s = (-)s/2 f Lp ; especially defined by H p s s s  s . Moreover, we know that H s = Lp  H p H s = H2 ,H = H for s  0. p 2 Finally, in this paper, we denote every positive constant by the same symbol C or c without confusion. [·] is the Gauss symbol. The article is organized as follows. In Section 2 we derive the solution formula of our semi-linear problem. We study the decay property of the solution operators appearing in the solution formula in section 3. Then, in Section 4, we discuss the linear problem and show the decay estimates. Finally, we prove global existence

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

3

and asymptotic behavior of solutions for the Cauchy problem (1.1), (1.2) in Section 5. 2. Solution formula The aim of this section is to derive the solution formula for problem (1.1), (1.2). We first investigate the equation (1.4). Taking the Fourier transform, we have (1 + a| |2 )^ utt + 2b| |2 u ^t + (| |6 +  | |4 + | |2 )^ u = 0. The corresponding initial value are t=0: u ^=u ^0 ( ), u ^t = u ^1 ( ). (2.2) (2.1)

The characteristic equation of (2.1) is (1 + a| |2 )2 + 2b| |2  + | |6 +  | |4 + | |2 = 0. Let  = ± ( ) be the corresponding eigenvalues of (2.3), we obtain ± ( ) = -b| |2 ± | | -1 - (a +  - b2 )| |2 - ( + a )| |4 - a| |6 . 1 + a| |2 ^ (, t)^ ^ (, t)^ u1 ( ) + H u0 ( ), u ^(, t) = G where ^ (, t) = G and 1 (+ ( )e- ()t - - ( )e+ ()t ). + ( ) - - ( ) We define G(x, t) and H (x, t) by ^ (, t) = H ^ (, t)](x), G(x, t) = F -1 [G ^ (, t)](x), H (x, t) = F -1 [H (2.7) 1 (e+ ()t - e- ()t ) + ( ) - - ( ) (2.6) (2.4) (2.3)

The solution to the problem (2.1)-(2.2) is given in the form (2.5)

respectively, where F -1 denotes the inverse Fourier transform. Then, applying F -1 to (2.5), we obtain u(t) = G(t)  u1 + H (t)  u0 . By the Duhamel principle, we obtain the solution formula to (1.1), (1.2),
t

(2.8)

u(t) = G(t)  u1 + H (t)  u0 +
0

G(t -  )  (I - a)-1 f (u)( )d.

(2.9)

3. Decay Property The aim of this section is to establish decay estimates of the solution operators G(t) and H (t) appearing in the solution formula (2.8). Lemma 3.1. The solution of problem (2.1), (2.2) satisfies | |2 (1+ | |2 )|u ^(, t)|2 + |u ^t (, t)|2  Ce-c()t (| |2 (1+ | |2 )|u ^0 ( )|2 + |u ^1 ( )|2 ), (3.1) for   Rn and t  0, where  ( ) =
| |2 1+| |2 .

4

Y. WANG

EJDE-2012/96

¯ Proof. Multiplying (2.1) by u ^t and taking the real part yields 1 d {(1 + a| |2 )|u ^t |2 + (| |6 +  | |4 + | |2 )|u ^|2 } + 2b| |2 |u ^t |2 = 0. 2 dt ¯ Multiplying (2.1) by u ^ and taking the real part, we obtain (3.2)

1 d ¯ {b| |2 |u ^|2 + 2(1 + a| |2 )Re(^ ut u ^)} + (| |6 +  | |4 + | |2 )|u ^|2 - (1 + a| |2 )|u ^t |2 = 0. 2 dt (3.3) Multiplying both sides of (3.2) and (3.3) by (1 + a| |2 ) and b| |2 respectively, summing up the products yields d E + F = 0, (3.4) dt where 1 E = (1 + a| |2 )2 |u ^t |2 + (1 + a| |2 )(| |6 +  | |4 + | |2 )|u ^|2 + b2 | |4 |u ^|2 2 ¯ + b| |2 (1 + a| |2 ) Re(^ ut u ^) and F = b| |2 (| |6 +  | |4 + | |2 )|u ^|2 + b| |2 (1 + a| |2 )|u ^t |2 . A simple computation implies that C (1 + | |2 )2 E0  E  C (1 + | |2 )2 E0 , where E0 = | |2 (1 + | |2 )|u ^|2 + |u ^t |2 . Note that F  c| |2 E0 . It follows from (3.5) that F  c ( )E, where  ( ) = Using (3.4) and (3.6), we obtain d E + c ( )E  0. dt Thus E (, t)  e-cw()t E (, 0), which together with (3.5) proves the desired estimates (3.1). Then the proof is complete. ^ (, t) and H ^ (, t) be the fundamental solution of (1.4) in the Lemma 3.2. Let G Fourier space, which are given in (2.6) and (2.7), respectively. Then we have the estimates ^ (, t)|2 + |G ^ t (, t)|2  Ce-c()t | |2 (1 + | |2 )|G (3.7) and ^ (, t)|2 + |H ^ t (, t)|2  C | |2 (1 + | |2 )e-c()t | |2 (1 + | |2 )|H for   Rn and t  0, where  ( ) =
| |2 1+| |2 .

(3.5)

(3.6)

| |2 . 1 + | |2

(3.8)

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

5

Proof. If u ^0 ( ) = 0, from (2.5), we obtain ^ (, t)^ u ^(, t) = G u1 ( ), ^ t (, t)^ u ^t (, t) = G u1 ( ).

Substituting the equalities into (3.1) with u ^0 ( ) = 0, we obtain (3.7). In what follows, we consider u ^1 ( ) = 0, it follows from (2.5) that ^ (, t)^ ^ t (, t)^ u ^(, t) = H u0 ( ), u ^t (, t) = H u0 ( ). Substituting the equalities into (3.1) with u ^1 ( ) = 0, we obtain the desired estimate (3.8). The Lemma is proved. Lemma 3.3. Let k  0 and 1  p  2. Then we have
k x G(t)   k H (t) x k x Gt (t) L2

 C (1 + t)-( 2 ( p - 2 )+ 2 + 2 - 2 ) 
L2 L2

n

1

1

k

l

1

-l p H

(k-2)+ + Ce-ct x 

L2 ,

(3.9) (3.10) (3.11) (3.12) (3.13) (3.14)

 
L2

 C (1 + t)  C (1 + t)

1 1 k l -( n 2 ( p - 2 )+ 2 + 2 )

 

-l p H -l p H -l p H

+ Ce + Ce

-ct

k x  L2 k x  L2 , L2 L2 , L2 ,

1 1 k l -( n 2 ( p - 2 )+ 2 + 2 ) n 1 1 k l 1

-ct

k x Ht (t)  

 C (1 + t)-( 2 ( p - 2 )+ 2 + 2 + 2 ) 
n k 1

k+2 + Ce-ct x  L1 L1 k + Ce-ct x g k + Ce-ct x g

k x G(t)  (I - a)-1 g k x Gt (t)  (I - a)-1 g

L2 L2

 C (1 + t)-( 4 + 2 + 2 ) g  C (1 + t)
k -( n 4 + 2 +1)

g

where (k - 2)+ = max{0, k - 2}. Proof. Firstly, we prove (3.9). By the Plancherel theorem and (3.7), we obtain
k x G(t)   2 L2

=
| |R0

^( )|2 d + ^ (, t)|2 | | |2k |G
| |R0

^( )|2 d ^ (, t)|2 | | |2k |G

C
| |R0

| |
-ct

2k-2 -c| |2 t

e

^( )| d |
2

(3.15) | | (| | (1 + | | ))
| |R0 2 Lp 2k 2 2 -1

+ Ce

^( )|2 d |
2

^( )  C | |-l  + Ce
-ct

| |(2k-2+2l)q e-cq|| t d
| |R0

1/q

(k-2)+ x  2 L2 , 1 p

where R0 is a small positive constant and Hausdorff-Young inequality that ^( ) | |-l 
Lp

+

1 p

= 1,
l

2 p

+

1 q

= 1. It follows from (3.16)

 C (-)- 2 

Lp .

By a straight computation, we obtain | |(2k-2+2l)q e-cq|| t d
| |R0
2

1/q

 C (1 + t)-( 2q +k-1+l) (3.17)  C (1 + t)-(n( p - 2 )+k-1+l) .
1 1

n

Combining (3.15), (3.16) and (3.17) yields (3.9). Similarly, using (3.7) and (3.8), respectively, we can prove (3.10)-(3.12).

6

Y. WANG

EJDE-2012/96

In what follows, we prove (3.13). By the Plancherel theorem, (3.7), and Hausdorff-Young inequality, we have
k x G(t)  (I - a)-1 g 2 L2

=
| |R0

^ (, t)|2 | |4 (1 + a| |2 )-2 |g | |2k |G ^( )|2 d ^ (, t)|2 | |4 (1 + | |2 )-2 |g | |2k |G ^( )|2 d
| |R0

+ C

| |2k+2 e-c|| t |g ^( )|2 d + Ce-ct
| |R0 | |R0 k | |2k+2 e-c|| t d + Ce-ct x g | |R0 2 L1 k + Ce-ct x g 2 L2 .
2

2

| |2k |g ^( )|2 d
2 L2

C g ^( )

2 L
n

 C (1 + t)-( 2 +k+1) g

where R0 is a small positive constant. Thus (3.13) follows. Similarly, we can prove (3.14). Thus we have completed the proof of lemma. 4. Decay estimate for solutions to the linear equation  -1 (Rn ), u1  H s (Rn )  H  -2 (Rn ) Theorem 4.1. Assume that u0  H s+2 (Rn )  H 1 1 n (s  [ 2 ] + 5). Then the classical solution u(x, t) to (1.4) associated with initial condition (1.2), which is given by the formula (2.8), satisfies the decay estimates
k x u(t) L2

 C (1 + t)- 4 - 2 - 2 ( u0
n h

n

k

1

 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

(4.1)

for k  s + 2,
h x ut (t) L2

 C (1 + t)- 4 - 2 -1 ( u0
n m 1

 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

(4.2)

for h  s,
m x u(t) L

 C (1 + t)- 2 - 2 - 2 ( u0

 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

(4.3)

for m  s + 1 - [ n 2 ]. Proof. Firstly, we prove (4.1). Using (3.9) and (3.10), we obtain
k x u(t) L2 k x G(t) h + C x H (t)  u0



 u1

L2

L2  -2 ) H 1  -2 H 1

 C (1 + t)  C (1 + t)

k 1 -n 4 -2-2

( u0 ( u0

 -1 H 1  -1 H 1

+ u1 + u1

+ Ce-ct ( u0
H s+2

H s+2

+ u1

Hs )

k 1 -n 4 -2-2

+ u0

+ u1

H s ).

Similar to the proof of (4.1), using (3.11) and (3.12), we can prove (4.2). In what follows, we prove (4.3). Using (4.1) and Gagliardo-Nirenberg inequality, it is not difficult to get (4.3). The Lemma is proved. 5. Existence of global solution and asymptotic behavior The purpose of this section is to prove the existence and asymptotic behavior of global solutions to the Cauchy problem (1.1), (1.2). We need the following Lemma, which come from [4] (see also [17]).

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

7

Lemma 5.1. Let s and  be positive integers,  > 0, p, q, r  [1, ] satisfy 1 r = 1 1 s + , and let k  { 0 , 1 , 2 , · · · , s } . Assume that F ( v ) is a class of C and satisfies p r
l |v F (v )|  Cl, |v |+1-l ,

|v |  ,

0  l  s, l <  + 1

and
l |v F (v )|  Cl, , |v |  , l  s,  + 1  l.

If v  Lp  W k,q  L and v F (v )
 x F (v ) Lr

L

  , then
W k,q

W k,r

 Ck, v
Lq

v v

Lp

v

 -1 L ,

  Ck, x v

v

Lp

 -1 L ,

||  k.

1 Lemma 5.2. Let s and  be positive integers,  > 0, p, q, r  [1, ] satisfy 1 r = p+ 1 r , and let k  {0, 1, 2, · · · , s}. Let F (v ) be a function that satisfies the assumptions of Lemma 5.1. Moreover, assume that s s |v F (v1 ) - v F (v2 )|  C (|v1 | + |v2 |)max{-s,} |v1 - v2 |,

|v1 |  ,

|v2 |  .

If v1 , v2  L  W

p

k,q

L



and v1
Lr

L

 , v2

L

  , then for ||  k , we have

 x (F (v1 ) - F (v2 ))

 Ck, {( + ( v1

 x v1 Lq Lp

 + x v2 Lp )

Lq )

v1 - v2

Lp

+ v2

 x (v1

- v2 )

Lq }(

v1

L

+ v2

 -1 . L )

Based on the estimates (4.1)-(4.3) of solutions to (1.4) associated with initial condition (1.2), we define the following solution space X = {u  C ([0, ); H s+2 (Rn ))  C 1 ([0, ); H s (Rn )) : u where u
X k u(t) (1 + t) 4 + 2 + 2 x ks+2 X
n k 1 n h

X

< },

= sup
t0

L2

+
hs

h ut (t) (1 + t) 4 + 2 +1 x

L2 },

For R > 0, we define XR = {u  X : u Gagliardo-Nirenberg inequality, we obtain
m x u(t) L

 R}. For m  s + 1 - [ n 2 ], using
n m 1

 C (1 + t)-( 2 + 2 + 2 ) u

X.

(5.1)

 -1 (Rn ), u1  H s (Rn )  H  -2 (Rn ) Theorem 5.3. Assume that u0  H s+2 (Rn )  H 1 1 n s+2 (s  [ 2 ] + 5) and integer   2. Let f (u) be a function of class C and satisfy Lemmas 5.1 and 5.2. Put E0 = u 0
 -1 H 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs .

If E0 is suitably small, the Cauchy problem (1.1)-(1.2) has a unique global classical solution u(x, t) satisfying u  C ([0, ); H s+2 (Rn )), ut  C ([0, ); H s (Rn )), utt  L ([0, ); H s-2 (Rn )). Moreover, the solution satisfies the decay estimate
k x u(t) h x ut (t) L2

 CE0 (1 + t)- 4 - 2 - 2 ,  CE0 (1 + t)
h -n 4 - 2 -1

n

k

1

(5.2) (5.3)

L2

for k  s + 2 and h  s.

8

Y. WANG

EJDE-2012/96

Proof. Define the mapping
t

(u) = G(t)  u1 + H (t)  u0 +
0

G(t -  )  (I - a)-1 f (u( ))d.

(5.4)

Using (3.9)-(3.10), (3.13), Lemma 5.1 and (5.1), for k  s + 2 we obtain
k x (u) L2 L2 k + C x H (t)  u0 L2 L2 d H s+2 + u1
Hs )

k  C x G(t)  u1 t

+C
0

k x G(t -  )  (I - a)-1 f (u( ))
n k 1

 C (1 + t)- 4 - 2 - 2 ( u0
t/2

 -1 H 1
k

+ u1
1

 -2 ) H 1 L1 d

+ Ce-ct ( u0

+C
0 t

(1 + t -  )- 4 - 2 - 2 f (u)
k (1 + t -  )- 4 - 2 x f (u) t/2 t k e-c(t- ) x f (u) 0
n k 1 n 1

n

+C +C

L1 d

L2 d

 C (1 + t)- 4 - 2 - 2 ( u0
t/2

 -1 H 1
k

+ u1
1

 -2 ) H 1

+ Ce-ct ( u0

H s+2 + u1

Hs )

+C
0 t

(1 + t -  )- 4 - 2 - 2 u
k (1 + t -  )- 4 - 2 x u t/2
n k 1 n 1

n

2 L2

u

 -1 L d t k e-c(t- ) x u 0 H s+2 + u1
n 1 Hs )

+C

2 L2

u

 -1 L d +C

L2

u

 L d

 C (1 + t)- 4 - 2 - 2 ( u0
t/2

 -1 H 1
n

+ u1
k 1

 -2 ) H 1

+ Ce-ct ( u0
n

+CR+1
0 t

(1 + t -  )- 4 - 2 - 2 (1 +  )-( 2 +1) (1 +  )-( 2 + 2 )(-1) d (1 + t -  )- 4 - 2 (1 +  )- 2 -k-1 (1 +  )-( 2 + 2 )(-1) d
t/2 t
n 1 n n 1

+CR+1 +CR+1  C (1 + t) Thus

e-c(t- ) (1 +  )- 4 - 2 - 2 (1 +  )-( 2 + 2 ) d
0 k 1 -n 4 -2-2

n

k

1

n

1

{( u 0
n

 -1 H 1
k 1

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

+ R+1 }. (5.5)

k (1 + t) 4 + 2 + 2 x (u) It follows from (5.4) that t

L2

 CE0 + CR+1 .

(u)t = Gt (t)  u1 + Ht (t)  u0 +
0

Gt (t -  )  (I - a)-1 f (u( ))d.

(5.6)

Using (3.11)-(3.12), (3.14) Lemma 5.1 and (5.1), for h  s we have
h x (u)t L2 h x Ht (t)  u0 L2 L2 d h  C x Gt (t)  u1 L2 + C t h +C x Gt (t -  )  (I 0

- a)-1 f (u( ))

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR
n h

9 Hs )

 C (1 + t)- 4 - 2 -1 ( u0
t/2

 -1 H 1
h

+ u1

 -2 ) H 1 L1 d

+ Ce-ct ( u0

H s+2

+ u1

+C
0 t

(1 + t -  )- 4 - 2 -1 f (u)
h f (u) (1 + t -  )- 4 -1 x t/2
n h n

n

t L1 d

+C

+C
0

h e-c(t- ) x f (u)

L2 d Hs )

 C (1 + t)- 4 - 2 -1 ( u0
t/2

 -1 H 1
h

+ u1
2 L2

 -2 ) H 1

+ Ce-ct ( u0

H s+2

+ u1

+C
0 t

(1 + t -  )- 4 - 2 -1 u
h u (1 + t -  )- 4 -1 x t/2
h -n 4 - 2 -1 n

n

u

 -1 L d t

+C

2 L2

u

 -1 L d

+C
0 -ct

h e-c(t- ) x u

L2

u

 L d

 C (1 + t) +CR+1

( u0

 -1 H 1
n

+ u1
h

 -2 ) H 1

+ Ce
n

( u0

H s+2
n

+ u1
1

Hs )

t/2 0 t

(1 + t -  )- 4 - 2 -1 (1 +  )-( 2 +1) (1 +  )-( 2 + 2 )(-1) d (1 + t -  )- 4 -1 (1 +  )- 2 -h-1 (1 +  )-( 2 + 2 )(-1) d
t/2 t
n n n 1

+CR+1 +CR+1  C (1 + t) Thus

e-c(t- ) (1 +  )- 4 - 2 -1 (1 +  )-( 2 + 2 ) d
0 h -n 4 - 2 -1

n

h

n

1

{( u 0
n

 -1 H 1
h

+ u1

 -2 H 1

+ u0

H s+2

+ u1

Hs )

+ R+1 }. (5.7) (5.8)

h (1 + t) 4 + 2 +1 x (u)t L2  CE0 + CR+1 . Combining (5.5), (5.7) and taking E0 and R suitably small yields

(u) For u ~, u ¯  XR , by using (5.4), we have
t

X

 R.

(~ u) - (¯ u) =
0

G(t -  )  (I - a)-1 [f (~ u) - f (¯ u)]d.

(5.9)

Using (5.9), (3.13) and Lemma 5.2, (5.1), for k  s + 2 we obtain
k x (~ u) - (¯ u)) t L2 L2 d


0

k x G(t -  )  (I - a)-1 [f (~ u) - f (¯ u)] t/2

C
0 t

(1 + t -  )- 4 - 2 - 2 (f (~ u) - f (¯ u))
k (1 + t -  )- 4 - 2 x (f (~ u) - f (¯ u)) t/2 t k e-c(t- ) x (f (~ u) - f (¯ u)) 0 t/2
n 1

n

k

1

L1 d

+C +C C
0

L1 d

L2 d

(1 + t -  )- 4 - 2 - 2 ( u ~
L

n

k

1

L2

+ u ¯

L2 )

u ~-u ¯

L2

×( u ~

+ u ¯

 -1 d L )

10 t

Y. WANG

EJDE-2012/96

+C
t/2

k u ~ (1 + t -  )- 4 - 2 {( x L2 t

n

1

L2

k + x u ~

L2 )

u ~-u ¯

L2

+( u ~ +C
0

+ u ¯

L2 )

k x (~ u-u ¯) L2

L2 }(

u ~
L2 )

L

+ u ¯

 -1 d L )

k e-c(t- ) {( x u ~ L

k + x u ~

u ~-u ¯
L

L  -1 d L )
n 1

+( u ~

+ u ¯
X

L ) t/2

k x (~ u-u ¯)

L2 }(
n k

u ~
1

+ u ¯

 CR u ~-u ¯ +CR u ~-u ¯

(1 + t -  )- 4 - 2 - 2 (1 +  )-( 2 + 2 ) d
0 t

X t/2

(1 + t -  )- 4 - 2 (1 +  )-( 2 (n+1)+
t

n

1



k+1 2 )

d

~-u ¯ +CCR u
n

X 0
k 1

e-c(t- ) (1 +  )-( 4 + 2 + 2 + 2 ) d
X,

n

n

k

1

 CR (1 + t)- 4 - 2 - 2 u ~-u ¯ which implies

k (1 + t) 4 + 2 + 2 x ((~ u) - (¯ u))

n

k

1

L2

 CR u ~-u ¯

X.

(5.10)

Similarly for h  s, from (5.6), (3.14) and (5.1), we have
t h x ((~ u) - (¯ u))t L2


0

h x Gt (t -  )  (I - a)-1 [f (~ u) - f (¯ u)] t/2

L2 d

C
0 t

u) - f (¯ u)) (1 + t -  )- 4 - 2 -1 (f (~
h (1 + t -  )- 4 -1 x (f (~ u) - f (¯ u)) t/2 t h e-c(t- ) x (f (~ u) - f (¯ u)) 0
n h n

n

h

L1 d

+C +C

L1 d

L2 d

 CR (1 + t)- 4 - 2 -1 u ~-u ¯ which implies
h (1 + t) 4 + 2 +1 x ((~ u) - (¯ u))t
n h

X,

L2

 CR u ~-u ¯

X.

(5.11)

Using (5.10), (5.11) and taking R suitably small yields (~ u) - (¯ u)
X



1 u ~-u ¯ 2

X.

(5.12)

From (5.8) and (5.12), we know that  is strictly contracting mapping. Consequently, we conclude that there exists a fixed point u  XR of the mapping , which is a classical solution to (1.1), (1.2). This completes the proof. Acknowledgements. The author would like to thank the anonymous referee for his/her comments and suggestions. This work was supported in part by grants 11101144 from the NNSF of China, and 201031 from the Research Initiation Project for High-level Talents of North China University of Water Resources and Electric Power.

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

11

References
[1] S. Lai, Y. Wang, Y. Wu and Q. Lin; An initial-boundary value problem for a generalized Boussinesq water system in a ball, Int. J. Appl. Math. Sci. 3 (2006), 117-133. [2] Y. Liu and S. Kawashima; Global existence and asymptotic behavior of solutions for quasilinear dissipative plate equation, Discrete Contin. Dyn. Syst. 29 (2011) 1113-1139. [3] Y. Liu and S. Kawashima; Global existence and decay of solutions for a quasi-linear dissipative plate equation, J. Hyperbolic Differential Equations 8 (2011) 591-614. [4] T. T. Li and Y. M. Chen; Nonlinear Evolution Equations, Scientific Press, 1989, (in Chinese). [5] V. G. Makhankov; Dynamics of classical solitons(in nonintegrable systems), Physics Reports, A review section of Phys. Lett.(Section C) 35(1) (1978), 1-128. [6] S. Wang and H. Xu; On the asymptotic behavior of solution for the generalized IBq equation with hydrodynamical damped term, J. Differ. Equ.252 (2012) 4243-4258. [7] S. Wang and F. Da; On the asymptotic behavior of solution for the generalized double dipersion equation, Appl. Anal. in press. [8] Y. Wang and Z. Wei; Global existence and asymptotic behavior of solutions to Cahn-Hilliard equation with inertial term, Internationl Journal of mathematics, accepted. [9] Y. Wang; Global existence and asymptotic behaviour of solutions for the generalized Boussinesq equation, Nonlinear Anal. 70 (2009) 465-482. [10] Y. Wang; Global existence of classical solutions to the minimal surface equation in two space dimensions with slow decay initial value, J. Math. Phys. 50 (2009) 103506-01-103506-01-14. [11] Y. Wang; Existence and nonexistence of global solutions for a class of nonlinear wave equations of higher order, Nonlinear Anal. 72 (2010) 4500-4507. [12] Y. Wang and Y. Wang; Global existence of classical solutions to the minimal surface equation with slow decay initial value, Appl. Math. Comput. 216 (2010) 576-583. [13] Y. Wang, F. Liu and Y. Zhang; Global existence and asymptotic of solutions for a semi-linear wave equation, J. Math. Anal. Appl. 385 (2012) 836-853. [14] Y. Wang and Y. Wang; Global existence and asymptotic behavior of solutions to a nonlinear wave equation of fourth-order, J. Math. Phys. 53 (2012) 013512-01-013512-13. [15] Z. Yang; Longtime behavior of the Kirchhoff type equation with strong damping on Rn , J. Differ. Equ. 242 (2007) 269-286. [16] Y. Zhang, Q. Lin and S. Lai; Long time asymptotic for the damped Boussinesq equation in a circle, J. Partial Dif. Eqs. 18(2005), 97-113. [17] S. M. Zheng; Nonlinear Evolution Equations, Monographs and Surveys in Pure and Applied Mathematics, 133, Chapan Hall/CRC, 2004. Yinxia Wang School of Mathematics and Information Sciences, North China University of Water Resources and Electric Power, Zhengzhou 450011, China E-mail address : yinxia117@126.com

Electronic Journal of Differential Equations, Vol. 2012 (2012), No. 96, pp. 1â€“11.
ISSN: 1072-6691. URL: http://ejde.math.txstate.edu or http://ejde.math.unt.edu
ftp ejde.math.txstate.edu

EXISTENCE AND ASYMPTOTIC BEHAVIOR OF SOLUTIONS
TO THE GENERALIZED DAMPED BOUSSINESQ EQUATION
YINXIA WANG

Abstract. We consider the Cauchy problem for the n-dimensional generalized damped Boussinesq equation. Based on decay estimates of solutions
to the corresponding linear equation, we define a solution space with time
weighted norms. Under small condition on the initial value, the existence and
asymptotic behavior of global solutions in the corresponding Sobolev spaces
are established by the contraction mapping principle.

1. Introduction
We study the Cauchy problem of the generalized damped Boussinesq equation
in n space dimensions
utt âˆ’ aâˆ†utt âˆ’ 2bâˆ†ut âˆ’ Î±âˆ†3 u + Î²âˆ†2 u âˆ’ âˆ†u = âˆ†f (u)

(1.1)

with the initial value
t=0:

u = u0 (x),

ut = u1 (x).

(1.2)

Here u = u(x, t) is the unknown function of x = (x1 , Â· Â· Â· , xn ) âˆˆ Rn and t > 0,
a, b, Î±, Î² are positive constants. The nonlinear term f (u) = O(u1+Î¸ ) and Î¸ is a
positive integer.
The first initial boundary value problem for
utt âˆ’ aâˆ†utt âˆ’ 2bâˆ†ut âˆ’ Î±âˆ†3 u + Î²âˆ†2 u âˆ’ âˆ†u = Î³âˆ†(u2 )

(1.3)

in a unit circle was investigated in [16], where a, b, Î±, Î² are positive constants and Î³
is a constant. The existence and the uniqueness of strong solution was established
and the solutions were constructed in the form of series in the small parameter
present in the initial conditions. The long-time asymptotics was also obtained in
the explicit form. In [1], the authors considered the initial-boundary value problem
for (1.3) in the unit ball B âŠ‚ R3 , similar results were established. It is well-known
that the equation (1.3) is closely contacted with many wave equations. For example,
the equation (which we call the Bq equation)
utt âˆ’ uxx + uxxxx = (u2 )xx ,
2000 Mathematics Subject Classification. 35L30, 35L75.
Key words and phrases. Generalized damped equation; global solution; asymptotic behavior.
c
2012
Texas State University - San Marcos.
Submitted May 31, 2012. Published June 10, 2012.
1

2

Y. WANG

EJDE-2012/96

which was derived by Boussinesq in 1872 to describe shallow water waves. The
improved Bq equation(which we call IBq equation) is
utt âˆ’ uxx âˆ’ uxxtt = (u2 )xx .
A modification of the IBq equation analogous of the MKdV equation yields
utt âˆ’ uxx âˆ’ uxxtt = (u3 )xx ,
which we call the IMBq equation (see [5]). (1.1) is a higher order wave equation. In
[8], we considered the Cauchy problem for the Cahn-Hilliard equation with inertial
term. Combining high frequency, low frequency technique and energy methods,
we obtained global existence and asymptotic behavior of solutions. Wang, Liu
and Zhang [13] investigated a fourth wave equation that is of the regularity-loss
type. Based on the decay property of the solution operators, global existence and
asymptotic behavior of solutions are obtained. For global existence and asymptotic
behavior of solutions to higher order wave equations, we refer to [2]-[3] and [6]-[15]
and references therein.
The main purpose of this paper is to establish global existence and asymptotic
behavior of solutions to (1.1), (1.2) by using the contraction mapping principle.
Firstly, we consider the decay property of the following linear equation
utt âˆ’ aâˆ†utt âˆ’ 2bâˆ†ut âˆ’ Î±âˆ†3 u + Î²âˆ†2 u âˆ’ âˆ†u = 0.

(1.4)

We obtain the following decay estimate of solutions to (1.4) associated with initial
condition (1.2),
n

k

1

kâˆ‚xk u(t)kL2 â‰¤ C(1 + t)âˆ’ 4 âˆ’ 2 âˆ’ 2 (ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 + ku0 kH s+2 + ku1 kH s ) (1.5)
1

1

(k â‰¤ s + 2),
n

h

kâˆ‚xh ut (t)kL2 â‰¤ C(1 + t)âˆ’ 4 âˆ’ 2 âˆ’1 (ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 + ku0 kH s+2 + ku1 kH s ) (1.6)
1

1

(h â‰¤ s) Based on the estimates (1.5) and (1.6), we define a solution space with
time weighted norms. Then global existence and asymptotic behavior of classical
solutions to (1.1), (1.2) are obtained by using the contraction mapping principle.
We give notation which is used in this paper. Let F [u] denote the Fourier
transform of u defined by
Z
uÌ‚(Î¾) = F [u] =
eâˆ’iÎ¾Â·x u(x)dx,
Rn

and we denote its inverse transform by F âˆ’1 .
For 1 â‰¤ p â‰¤ âˆ, Lp = Lp (Rn ) denotes the usual Lebesgue space with the
norm k Â· kLp . The usual Sobolev space of s is defined by Hps = (I âˆ’ âˆ†)âˆ’s/2 Lp
with the norm kf kHps = k(I âˆ’ âˆ†)s/2 f kLp ; the homogeneous Sobolev space of s is
defined by HÌ‡ps = (âˆ’âˆ†)âˆ’s/2 Lp with the norm kf kHps = k(âˆ’âˆ†)s/2 f kLp ; especially
H s = H2s , HÌ‡ s = HÌ‡2s . Moreover, we know that Hps = Lp âˆ© HÌ‡ps for s â‰¥ 0.
Finally, in this paper, we denote every positive constant by the same symbol C
or c without confusion. [Â·] is the Gauss symbol.
The article is organized as follows. In Section 2 we derive the solution formula
of our semi-linear problem. We study the decay property of the solution operators
appearing in the solution formula in section 3. Then, in Section 4, we discuss the
linear problem and show the decay estimates. Finally, we prove global existence

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

3

and asymptotic behavior of solutions for the Cauchy problem (1.1), (1.2) in Section
5.
2. Solution formula
The aim of this section is to derive the solution formula for problem (1.1), (1.2).
We first investigate the equation (1.4). Taking the Fourier transform, we have
(1 + a|Î¾|2 )uÌ‚tt + 2b|Î¾|2 uÌ‚t + (Î±|Î¾|6 + Î²|Î¾|4 + |Î¾|2 )uÌ‚ = 0.

(2.1)

The corresponding initial value are
t=0:

uÌ‚ = uÌ‚0 (Î¾),

uÌ‚t = uÌ‚1 (Î¾).

(2.2)

The characteristic equation of (2.1) is
(1 + a|Î¾|2 )Î»2 + 2b|Î¾|2 Î» + Î±|Î¾|6 + Î²|Î¾|4 + |Î¾|2 = 0.
Let Î» = Î»Â± (Î¾) be the corresponding eigenvalues of (2.3), we obtain
p
âˆ’b|Î¾|2 Â± |Î¾| âˆ’1 âˆ’ (a + Î² âˆ’ b2 )|Î¾|2 âˆ’ (Î± + aÎ²)|Î¾|4 âˆ’ aÎ±|Î¾|6
Î»Â± (Î¾) =
.
1 + a|Î¾|2

(2.3)

(2.4)

The solution to the problem (2.1)-(2.2) is given in the form
uÌ‚(Î¾, t) = GÌ‚(Î¾, t)uÌ‚1 (Î¾) + HÌ‚(Î¾, t)uÌ‚0 (Î¾),

(2.5)

where
GÌ‚(Î¾, t) =

1
(eÎ»+ (Î¾)t âˆ’ eÎ»âˆ’ (Î¾)t )
Î»+ (Î¾) âˆ’ Î»âˆ’ (Î¾)

(2.6)

and
1
(Î»+ (Î¾)eÎ»âˆ’ (Î¾)t âˆ’ Î»âˆ’ (Î¾)eÎ»+ (Î¾)t ).
Î»+ (Î¾) âˆ’ Î»âˆ’ (Î¾)
We define G(x, t) and H(x, t) by
HÌ‚(Î¾, t) =

G(x, t) = F âˆ’1 [GÌ‚(Î¾, t)](x),

(2.7)

H(x, t) = F âˆ’1 [HÌ‚(Î¾, t)](x),

respectively, where F âˆ’1 denotes the inverse Fourier transform. Then, applying
F âˆ’1 to (2.5), we obtain
u(t) = G(t) âˆ— u1 + H(t) âˆ— u0 .
By the Duhamel principle, we obtain the solution formula to (1.1), (1.2),
Z t
u(t) = G(t) âˆ— u1 + H(t) âˆ— u0 +
G(t âˆ’ Ï„ ) âˆ— (I âˆ’ aâˆ†)âˆ’1 âˆ†f (u)(Ï„ )dÏ„.

(2.8)

(2.9)

0

3. Decay Property
The aim of this section is to establish decay estimates of the solution operators
G(t) and H(t) appearing in the solution formula (2.8).
Lemma 3.1. The solution of problem (2.1), (2.2) satisfies
|Î¾|2 (1+|Î¾|2 )|uÌ‚(Î¾, t)|2 +|uÌ‚t (Î¾, t)|2 â‰¤ Ceâˆ’cÏ‰(Î¾)t (|Î¾|2 (1+|Î¾|2 )|uÌ‚0 (Î¾)|2 +|uÌ‚1 (Î¾)|2 ), (3.1)
for Î¾ âˆˆ Rn and t â‰¥ 0, where Ï‰(Î¾) =

|Î¾|2
1+|Î¾|2 .

4

Y. WANG

EJDE-2012/96

Â¯t and taking the real part yields
Proof. Multiplying (2.1) by uÌ‚
1 d
{(1 + a|Î¾|2 )|uÌ‚t |2 + (Î±|Î¾|6 + Î²|Î¾|4 + |Î¾|2 )|uÌ‚|2 } + 2b|Î¾|2 |uÌ‚t |2 = 0.
2 dt
Â¯ and taking the real part, we obtain
Multiplying (2.1) by uÌ‚

(3.2)

1 d
Â¯ + (Î±|Î¾|6 + Î²|Î¾|4 + |Î¾|2 )|uÌ‚|2 âˆ’ (1 + a|Î¾|2 )|uÌ‚t |2 = 0.
{b|Î¾|2 |uÌ‚|2 + 2(1 + a|Î¾|2 )Re(uÌ‚t uÌ‚)}
2 dt
(3.3)
Multiplying both sides of (3.2) and (3.3) by (1 + a|Î¾|2 ) and b|Î¾|2 respectively, summing up the products yields
d
E + F = 0,
(3.4)
dt
where
1
E = (1 + a|Î¾|2 )2 |uÌ‚t |2 + (1 + a|Î¾|2 )(Î±|Î¾|6 + Î²|Î¾|4 + |Î¾|2 )|uÌ‚|2 + b2 |Î¾|4 |uÌ‚|2
2
Â¯
+ b|Î¾|2 (1 + a|Î¾|2 ) Re(uÌ‚t uÌ‚)
and
F = b|Î¾|2 (Î±|Î¾|6 + Î²|Î¾|4 + |Î¾|2 )|uÌ‚|2 + b|Î¾|2 (1 + a|Î¾|2 )|uÌ‚t |2 .
A simple computation implies that
C(1 + |Î¾|2 )2 E0 â‰¤ E â‰¤ C(1 + |Î¾|2 )2 E0 ,

(3.5)

where
E0 = |Î¾|2 (1 + |Î¾|2 )|uÌ‚|2 + |uÌ‚t |2 .
Note that F â‰¥ c|Î¾|2 E0 . It follows from (3.5) that
F â‰¥ cÏ‰(Î¾)E,

(3.6)

where
Ï‰(Î¾) =

|Î¾|2
.
1 + |Î¾|2

Using (3.4) and (3.6), we obtain
d
E + cÏ‰(Î¾)E â‰¤ 0.
dt
Thus E(Î¾, t) â‰¤ eâˆ’cw(Î¾)t E(Î¾, 0), which together with (3.5) proves the desired estimates (3.1). Then the proof is complete.

Lemma 3.2. Let GÌ‚(Î¾, t) and HÌ‚(Î¾, t) be the fundamental solution of (1.4) in the
Fourier space, which are given in (2.6) and (2.7), respectively. Then we have the
estimates
|Î¾|2 (1 + |Î¾|2 )|GÌ‚(Î¾, t)|2 + |GÌ‚t (Î¾, t)|2 â‰¤ Ceâˆ’cÏ‰(Î¾)t

(3.7)

|Î¾|2 (1 + |Î¾|2 )|HÌ‚(Î¾, t)|2 + |HÌ‚t (Î¾, t)|2 â‰¤ C|Î¾|2 (1 + |Î¾|2 )eâˆ’cÏ‰(Î¾)t

(3.8)

and
for Î¾ âˆˆ Rn and t â‰¥ 0, where Ï‰(Î¾) =

|Î¾|2
1+|Î¾|2 .

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

5

Proof. If uÌ‚0 (Î¾) = 0, from (2.5), we obtain
uÌ‚(Î¾, t) = GÌ‚(Î¾, t)uÌ‚1 (Î¾),

uÌ‚t (Î¾, t) = GÌ‚t (Î¾, t)uÌ‚1 (Î¾).

Substituting the equalities into (3.1) with uÌ‚0 (Î¾) = 0, we obtain (3.7). In what
follows, we consider uÌ‚1 (Î¾) = 0, it follows from (2.5) that
uÌ‚(Î¾, t) = HÌ‚(Î¾, t)uÌ‚0 (Î¾), uÌ‚t (Î¾, t) = HÌ‚t (Î¾, t)uÌ‚0 (Î¾).
Substituting the equalities into (3.1) with uÌ‚1 (Î¾) = 0, we obtain the desired estimate
(3.8). The Lemma is proved.

Lemma 3.3. Let k â‰¥ 0 and 1 â‰¤ p â‰¤ 2. Then we have
n

1

1

k

l

1

kâˆ‚xk G(t) âˆ— Ï†kL2 â‰¤ C(1 + t)âˆ’( 2 ( p âˆ’ 2 )+ 2 + 2 âˆ’ 2 ) kÏ†kHÌ‡pâˆ’l + Ceâˆ’ct kâˆ‚x(kâˆ’2)+ Ï†kL2 , (3.9)
n

1

1

k

l

kâˆ‚xk H(t) âˆ— Ï†kL2 â‰¤ C(1 + t)âˆ’( 2 ( p âˆ’ 2 )+ 2 + 2 ) kÏ†kHÌ‡pâˆ’l + Ceâˆ’ct kâˆ‚xk Ï†kL2
kâˆ‚xk Gt (t)

1
1
k
l
âˆ’( n
2 ( p âˆ’ 2 )+ 2 + 2 )

âˆ— Ï†kL2 â‰¤ C(1 + t)

n

1

1

k

l

âˆ’ct

kÏ†kHÌ‡pâˆ’l + Ce

kâˆ‚xk Ï†kL2 ,

1

kâˆ‚xk Ht (t) âˆ— Ï†kL2 â‰¤ C(1 + t)âˆ’( 2 ( p âˆ’ 2 )+ 2 + 2 + 2 ) kÏ†kHÌ‡pâˆ’l + Ceâˆ’ct kâˆ‚xk+2 Ï†kL2
n

k

(3.10)
(3.11)
(3.12)

1

kâˆ‚xk G(t) âˆ— (I âˆ’ aâˆ†)âˆ’1 âˆ†gkL2 â‰¤ C(1 + t)âˆ’( 4 + 2 + 2 ) kgkL1 + Ceâˆ’ct kâˆ‚xk gkL2 , (3.13)
n

k

kâˆ‚xk Gt (t) âˆ— (I âˆ’ aâˆ†)âˆ’1 âˆ†gkL2 â‰¤ C(1 + t)âˆ’( 4 + 2 +1) kgkL1 + Ceâˆ’ct kâˆ‚xk gkL2 , (3.14)
where (k âˆ’ 2)+ = max{0, k âˆ’ 2}.
Proof. Firstly, we prove (3.9). By the Plancherel theorem and (3.7), we obtain
kâˆ‚xk G(t) âˆ— Ï†k2L2
Z
Z
=
|Î¾|2k |GÌ‚(Î¾, t)|2 |Ï†Ì‚(Î¾)|2 dÎ¾ +
|Î¾|2k |GÌ‚(Î¾, t)|2 |Ï†Ì‚(Î¾)|2 dÎ¾
|Î¾|â‰¤R0
|Î¾|â‰¥R0
Z
2kâˆ’2 âˆ’c|Î¾|2 t
2
â‰¤C
|Î¾|
e
|Ï†Ì‚(Î¾)| dÎ¾
|Î¾|â‰¤R0
Z
âˆ’ct
+ Ce
|Î¾|2k (|Î¾|2 (1 + |Î¾|2 ))âˆ’1 |Ï†Ì‚(Î¾)|2 dÎ¾

(3.15)

|Î¾|â‰¥R0
âˆ’l

â‰¤ Ck|Î¾|

Ï†Ì‚(Î¾)k2Lp0

âˆ’ct

+ Ce

Z

2

|Î¾|(2kâˆ’2+2l)q eâˆ’cq|Î¾| t dÎ¾

1/q

|Î¾|â‰¤R0

kâˆ‚x(kâˆ’2)+ Ï†k2L2 ,

where R0 is a small positive constant and
Hausdorff-Young inequality that

1
p

+

1
p0

= 1,

2
p0

+

1
q

= 1. It follows from

l

k |Î¾|âˆ’l Ï†Ì‚(Î¾)kLp0 â‰¤ Ck(âˆ’âˆ†)âˆ’ 2 Ï†kLp .

(3.16)

By a straight computation, we obtain
Z
1/q
2
n
|Î¾|(2kâˆ’2+2l)q eâˆ’cq|Î¾| t dÎ¾
â‰¤ C(1 + t)âˆ’( 2q +kâˆ’1+l)
(3.17)

|Î¾|â‰¤R0
1

1

â‰¤ C(1 + t)âˆ’(n( p âˆ’ 2 )+kâˆ’1+l) .
Combining (3.15), (3.16) and (3.17) yields (3.9).
Similarly, using (3.7) and (3.8), respectively, we can prove (3.10)-(3.12).

6

Y. WANG

EJDE-2012/96

In what follows, we prove (3.13). By the Plancherel theorem, (3.7), and
Hausdorff-Young inequality, we have
kâˆ‚xk G(t) âˆ— (I âˆ’ aâˆ†)âˆ’1 âˆ†gk2L2
Z
=
|Î¾|2k |GÌ‚(Î¾, t)|2 |Î¾|4 (1 + a|Î¾|2 )âˆ’2 |gÌ‚(Î¾)|2 dÎ¾
|Î¾|â‰¤R0
Z
+
|Î¾|2k |GÌ‚(Î¾, t)|2 |Î¾|4 (1 + |Î¾|2 )âˆ’2 |gÌ‚(Î¾)|2 dÎ¾
|Î¾|â‰¥R0
Z
Z
2
â‰¤C
|Î¾|2k+2 eâˆ’c|Î¾| t |gÌ‚(Î¾)|2 dÎ¾ + Ceâˆ’ct
|Î¾|2k |gÌ‚(Î¾)|2 dÎ¾
|Î¾|â‰¤R0
|Î¾|â‰¥R0
Z
2
â‰¤ CkgÌ‚(Î¾)k2Lâˆ
|Î¾|2k+2 eâˆ’c|Î¾| t dÎ¾ + Ceâˆ’ct kâˆ‚xk gk2L2
|Î¾|â‰¤R0

âˆ’( n
2 +k+1)

â‰¤ C(1 + t)

kgk2L1 + Ceâˆ’ct kâˆ‚xk gk2L2 .

where R0 is a small positive constant. Thus (3.13) follows. Similarly, we can prove
(3.14). Thus we have completed the proof of lemma.

4. Decay estimate for solutions to the linear equation
Theorem 4.1. Assume that u0 âˆˆ H s+2 (Rn ) âˆ© HÌ‡1âˆ’1 (Rn ), u1 âˆˆ H s (Rn ) âˆ© HÌ‡1âˆ’2 (Rn )
(s â‰¥ [ n2 ] + 5). Then the classical solution u(x, t) to (1.4) associated with initial
condition (1.2), which is given by the formula (2.8), satisfies the decay estimates
n

k

1

kâˆ‚xk u(t)kL2 â‰¤ C(1 + t)âˆ’ 4 âˆ’ 2 âˆ’ 2 (ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 + ku0 kH s+2 + ku1 kH s ) (4.1)
1

1

for k â‰¤ s + 2,
n

h

kâˆ‚xh ut (t)kL2 â‰¤ C(1 + t)âˆ’ 4 âˆ’ 2 âˆ’1 (ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 + ku0 kH s+2 + ku1 kH s ) (4.2)
1

1

for h â‰¤ s,
n

m

1

kâˆ‚xm u(t)kLâˆ â‰¤ C(1 + t)âˆ’ 2 âˆ’ 2 âˆ’ 2 (ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 + ku0 kH s+2 + ku1 kH s ) (4.3)
1

for m â‰¤ s + 1 âˆ’

1

[ n2 ].

Proof. Firstly, we prove (4.1). Using (3.9) and (3.10), we obtain
kâˆ‚xk u(t)kL2
â‰¤ kâˆ‚xk G(t) âˆ— u1 kL2 + Ckâˆ‚xh H(t) âˆ— u0 kL2
n

k

1

â‰¤ C(1 + t)âˆ’ 4 âˆ’ 2 âˆ’ 2 (ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 ) + Ceâˆ’ct (ku0 kH s+2 + ku1 kH s )
1

k
1
âˆ’n
4 âˆ’2âˆ’2

â‰¤ C(1 + t)

1

(ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 + ku0 kH s+2 + ku1 kH s ).
1

1

Similar to the proof of (4.1), using (3.11) and (3.12), we can prove (4.2). In what
follows, we prove (4.3). Using (4.1) and Gagliardo-Nirenberg inequality, it is not
difficult to get (4.3). The Lemma is proved.

5. Existence of global solution and asymptotic behavior
The purpose of this section is to prove the existence and asymptotic behavior of
global solutions to the Cauchy problem (1.1), (1.2). We need the following Lemma,
which come from [4] (see also [17]).

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

7

Lemma 5.1. Let s and Î¸ be positive integers, Î´ > 0, p, q, r âˆˆ [1, âˆ] satisfy 1r =
1
1
s
p + r , and let k âˆˆ {0, 1, 2, Â· Â· Â· , s}. Assume that F (v) is a class of C and satisfies
|âˆ‚vl F (v)| â‰¤ Cl,Î´ |v|Î¸+1âˆ’l ,

|v| â‰¤ Î´,

0 â‰¤ l â‰¤ s, l < Î¸ + 1

and
|âˆ‚vl F (v)| â‰¤ Cl,Î´ , |v| â‰¤ Î´, l â‰¤ s, Î¸ + 1 â‰¤ l.
If v âˆˆ Lp âˆ© W k,q âˆ© Lâˆ and kvkLâˆ â‰¤ Î´, then
kF (v)kW k,r â‰¤ Ck,Î´ kvkW k,q kvkLp kvkÎ¸âˆ’1
Lâˆ ,
kâˆ‚xÎ± F (v)kLr â‰¤ Ck,Î´ kâˆ‚xÎ± vkLq kvkLp kvkÎ¸âˆ’1
Lâˆ ,

|Î±| â‰¤ k.

Lemma 5.2. Let s and Î¸ be positive integers, Î´ > 0, p, q, r âˆˆ [1, âˆ] satisfy 1r = p1 +
1
r , and let k âˆˆ {0, 1, 2, Â· Â· Â· , s}. Let F (v) be a function that satisfies the assumptions
of Lemma 5.1. Moreover, assume that
|âˆ‚vs F (v1 ) âˆ’ âˆ‚vs F (v2 )| â‰¤ CÎ´ (|v1 | + |v2 |)max{Î¸âˆ’s,Î¸} |v1 âˆ’ v2 |,
p

If v1 , v2 âˆˆ L âˆ© W

k,q

âˆ

âˆ©L

|v1 | â‰¤ Î´,

|v2 | â‰¤ Î´.

and kv1 kLâˆ â‰¤ Î´, kv2 kLâˆ â‰¤ Î´, then for |Î±| â‰¤ k, we have

kâˆ‚xÎ± (F (v1 ) âˆ’ F (v2 ))kLr
â‰¤ Ck,Î´ {(kâˆ‚xÎ± v1 kLq + kâˆ‚xÎ± v2 kLq )kv1 âˆ’ v2 kLp
+ (kv1 kLp + kv2 kLp )kâˆ‚xÎ± (v1 âˆ’ v2 )kLq }(kv1 kLâˆ + kv2 kLâˆ )Î¸âˆ’1 .
Based on the estimates (4.1)-(4.3) of solutions to (1.4) associated with initial
condition (1.2), we define the following solution space
X = {u âˆˆ C([0, âˆ); H s+2 (Rn )) âˆ© C 1 ([0, âˆ); H s (Rn )) : kukX < âˆ},
where
kukX = sup
tâ‰¥0

n

 X

k

1

(1 + t) 4 + 2 + 2 kâˆ‚xk u(t)kL2 +

X

n

h

(1 + t) 4 + 2 +1 kâˆ‚xh ut (t)kL2 },

hâ‰¤s

kâ‰¤s+2

For R > 0, we define XR = {u âˆˆ X : kukX â‰¤ R}. For m â‰¤ s + 1 âˆ’ [ n2 ], using
Gagliardo-Nirenberg inequality, we obtain
m

n

1

kâˆ‚xm u(t)kLâˆ â‰¤ C(1 + t)âˆ’( 2 + 2 + 2 ) kukX .

(5.1)

Theorem 5.3. Assume that u0 âˆˆ H s+2 (Rn ) âˆ© HÌ‡1âˆ’1 (Rn ), u1 âˆˆ H s (Rn ) âˆ© HÌ‡1âˆ’2 (Rn )
(s â‰¥ [ n2 ] + 5) and integer Î¸ â‰¥ 2. Let f (u) be a function of class C s+2 and satisfy
Lemmas 5.1 and 5.2. Put
E0 = ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 + ku0 kH s+2 + ku1 kH s .
1

1

If E0 is suitably small, the Cauchy problem (1.1)-(1.2) has a unique global classical
solution u(x, t) satisfying u âˆˆ C([0, âˆ); H s+2 (Rn )), ut âˆˆ C([0, âˆ); H s (Rn )), utt âˆˆ
Lâˆ ([0, âˆ); H sâˆ’2 (Rn )). Moreover, the solution satisfies the decay estimate
n

k

1

kâˆ‚xk u(t)kL2 â‰¤ CE0 (1 + t)âˆ’ 4 âˆ’ 2 âˆ’ 2 ,
h
âˆ’n
4 âˆ’ 2 âˆ’1

kâˆ‚xh ut (t)kL2 â‰¤ CE0 (1 + t)
for k â‰¤ s + 2 and h â‰¤ s.

(5.2)
(5.3)

8

Y. WANG

EJDE-2012/96

Proof. Define the mapping
t

Z

G(t âˆ’ Ï„ ) âˆ— (I âˆ’ aâˆ†)âˆ’1 âˆ†f (u(Ï„ ))dÏ„.

Î¨(u) = G(t) âˆ— u1 + H(t) âˆ— u0 +

(5.4)

0

Using (3.9)-(3.10), (3.13), Lemma 5.1 and (5.1), for k â‰¤ s + 2 we obtain
kâˆ‚xk Î¨(u)kL2
â‰¤ Ckâˆ‚xk G(t) âˆ— u1 kL2 + Ckâˆ‚xk H(t) âˆ— u0 kL2
Z t
+C
kâˆ‚xk G(t âˆ’ Ï„ ) âˆ— (I âˆ’ aâˆ†)âˆ’1 âˆ†f (u(Ï„ ))kL2 dÏ„
0
k

n

1

â‰¤ C(1 + t)âˆ’ 4 âˆ’ 2 âˆ’ 2 (ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 ) + Ceâˆ’ct (ku0 kH s+2 +ku1 kH s )
1
1
Z t/2
n
k
1
+C
(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’ 2 âˆ’ 2 kf (u)kL1 dÏ„
0
Z t
n
1
+C
(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’ 2 kâˆ‚xk f (u)kL1 dÏ„
t/2
t

Z
+C

eâˆ’c(tâˆ’Ï„ ) kâˆ‚xk f (u)kL2 dÏ„

0
k

n

1

â‰¤ C(1 + t)âˆ’ 4 âˆ’ 2 âˆ’ 2 (ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 ) + Ceâˆ’ct (ku0 kH s+2 +ku1 kH s )
1
1
Z t/2
n
k
1
+C
(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’ 2 âˆ’ 2 kuk2L2 kukÎ¸âˆ’1
Lâˆ dÏ„
0
Z t
Z t
Î¸âˆ’1
âˆ’ 21
âˆ’n
k
2
4
+C
(1 + t âˆ’ Ï„ )
kâˆ‚x ukL2 kukLâˆ dÏ„ +C
eâˆ’c(tâˆ’Ï„ ) kâˆ‚xk ukL2 kukÎ¸Lâˆ dÏ„
t/2

0

k
1
âˆ’n
4 âˆ’2âˆ’2

âˆ’ct

â‰¤ C(1 + t)
(ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 ) + Ce (ku0 kH s+2 +ku1 kH s )
1
1
Z t/2
k
1
n
1
n
n
+CRÎ¸+1
(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’ 2 âˆ’ 2 (1 + Ï„ )âˆ’( 2 +1) (1 + Ï„ )âˆ’( 2 + 2 )(Î¸âˆ’1) dÏ„
0
Z t
n
1
n
1
n
+CRÎ¸+1
(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’ 2 (1 + Ï„ )âˆ’ 2 âˆ’kâˆ’1 (1 + Ï„ )âˆ’( 2 + 2 )(Î¸âˆ’1) dÏ„
t/2
t

+CRÎ¸+1

Z

0
k
1
âˆ’n
4 âˆ’2âˆ’2

â‰¤ C(1 + t)

n

k

1

n

1

eâˆ’c(tâˆ’Ï„ ) (1 + Ï„ )âˆ’ 4 âˆ’ 2 âˆ’ 2 (1 + Ï„ )âˆ’( 2 + 2 )Î¸ dÏ„
{(ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 + ku0 kH s+2 + ku1 kH s ) + RÎ¸+1 }.
1

1

Thus
n

k

1

(1 + t) 4 + 2 + 2 kâˆ‚xk Î¨(u)kL2 â‰¤ CE0 + CRÎ¸+1 .
It follows from (5.4) that
Z t
Î¨(u)t = Gt (t) âˆ— u1 + Ht (t) âˆ— u0 +
Gt (t âˆ’ Ï„ ) âˆ— (I âˆ’ aâˆ†)âˆ’1 âˆ†f (u(Ï„ ))dÏ„.
0

Using (3.11)-(3.12), (3.14) Lemma 5.1 and (5.1), for h â‰¤ s we have
kâˆ‚xh Î¨(u)t kL2
â‰¤ Ckâˆ‚xh Gt (t) âˆ— u1 kL2 + Ckâˆ‚xh Ht (t) âˆ— u0 kL2
Z t
+C
kâˆ‚xh Gt (t âˆ’ Ï„ ) âˆ— (I âˆ’ aâˆ†)âˆ’1 âˆ†f (u(Ï„ ))kL2 dÏ„
0

(5.5)

(5.6)

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR
n

9

h

â‰¤ C(1 + t)âˆ’ 4 âˆ’ 2 âˆ’1 (ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 ) + Ceâˆ’ct (ku0 kH s+2 + ku1 kH s )
1
1
Z t/2
h
n
+C
(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’ 2 âˆ’1 kf (u)kL1 dÏ„
0
Z t
Z t
âˆ’1
h
âˆ’n
4
+C
kâˆ‚x f (u)kL1 dÏ„ + C
(1 + t âˆ’ Ï„ )
eâˆ’c(tâˆ’Ï„ ) kâˆ‚xh f (u)kL2 dÏ„
t/2

0
h
âˆ’n
4 âˆ’ 2 âˆ’1

(ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 ) + Ceâˆ’ct (ku0 kH s+2 + ku1 kH s )
â‰¤ C(1 + t)
1
1
Z t/2
h
n
+C
(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’ 2 âˆ’1 kuk2L2 kukÎ¸âˆ’1
Lâˆ dÏ„
0
Z t
Z t
n
+C
dÏ„
+
C
(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’1 kâˆ‚xh uk2L2 kukÎ¸âˆ’1
eâˆ’c(tâˆ’Ï„ ) kâˆ‚xh ukL2 kukÎ¸Lâˆ dÏ„
Lâˆ
t/2

0

h
âˆ’n
4 âˆ’ 2 âˆ’1

âˆ’ct

(ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 ) + Ce (ku0 kH s+2 + ku1 kH s )
â‰¤ C(1 + t)
1
1
Z t/2
n
h
n
n
1
+CRÎ¸+1
(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’ 2 âˆ’1 (1 + Ï„ )âˆ’( 2 +1) (1 + Ï„ )âˆ’( 2 + 2 )(Î¸âˆ’1) dÏ„
0
Z t
n
n
1
n
+CRÎ¸+1
(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’1 (1 + Ï„ )âˆ’ 2 âˆ’hâˆ’1 (1 + Ï„ )âˆ’( 2 + 2 )(Î¸âˆ’1) dÏ„
t/2
t

+CRÎ¸+1

Z

n

h

n

1

eâˆ’c(tâˆ’Ï„ ) (1 + Ï„ )âˆ’ 4 âˆ’ 2 âˆ’1 (1 + Ï„ )âˆ’( 2 + 2 )Î¸ dÏ„

0
h
âˆ’n
4 âˆ’ 2 âˆ’1

â‰¤ C(1 + t)

{(ku0 kHÌ‡ âˆ’1 + ku1 kHÌ‡ âˆ’2 + ku0 kH s+2 + ku1 kH s ) + RÎ¸+1 }.
1

1

Thus
n

h

(1 + t) 4 + 2 +1 kâˆ‚xh Î¨(u)t kL2 â‰¤ CE0 + CRÎ¸+1 .
Combining (5.5), (5.7) and taking E0 and R suitably small yields
kÎ¨(u)kX â‰¤ R.
For uÌƒ, uÌ„ âˆˆ XR , by using (5.4), we have
Z t
Î¨(uÌƒ) âˆ’ Î¨(uÌ„) =
G(t âˆ’ Ï„ ) âˆ— (I âˆ’ aâˆ†)âˆ’1 âˆ†[f (uÌƒ) âˆ’ f (uÌ„)]dÏ„.
0

Using (5.9), (3.13) and Lemma 5.2, (5.1), for k â‰¤ s + 2 we obtain
kâˆ‚xk Î¨(uÌƒ) âˆ’ Î¨(uÌ„))kL2
Z t
â‰¤
kâˆ‚xk G(t âˆ’ Ï„ ) âˆ— (I âˆ’ aâˆ†)âˆ’1 âˆ†[f (uÌƒ) âˆ’ f (uÌ„)]kL2 dÏ„
0

Z

t/2

n

k

n

1

1

(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’ 2 âˆ’ 2 k(f (uÌƒ) âˆ’ f (uÌ„))kL1 dÏ„

â‰¤C
0

Z

t

(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’ 2 kâˆ‚xk (f (uÌƒ) âˆ’ f (uÌ„))kL1 dÏ„

+C
t/2
t

Z
+C
Z
â‰¤C

eâˆ’c(tâˆ’Ï„ ) kâˆ‚xk (f (uÌƒ) âˆ’ f (uÌ„))kL2 dÏ„

0
t/2

n

k

1

(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’ 2 âˆ’ 2 (kuÌƒkL2 + kuÌ„kL2 )kuÌƒ âˆ’ uÌ„kL2

0

Ã—(kuÌƒkLâˆ + kuÌ„kLâˆ )Î¸âˆ’1 dÏ„

(5.7)
(5.8)

(5.9)

10

Y. WANG

Z

t

+C

EJDE-2012/96

1

n

(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’ 2 {(kâˆ‚xk uÌƒkL2 + kâˆ‚xk uÌƒkL2 )kuÌƒ âˆ’ uÌ„kL2

t/2

+(kuÌƒkL2 + kuÌ„kL2 )kâˆ‚xk (uÌƒ âˆ’ uÌ„)kL2 }(kuÌƒkLâˆ + kuÌ„kLâˆ )Î¸âˆ’1 dÏ„
Z t
eâˆ’c(tâˆ’Ï„ ) {(kâˆ‚xk uÌƒkL2 + kâˆ‚xk uÌƒkL2 )kuÌƒ âˆ’ uÌ„kLâˆ
+C
0

+(kuÌƒkLâˆ + kuÌ„kLâˆ )kâˆ‚xk (uÌƒ âˆ’ uÌ„)kL2 }(kuÌƒkLâˆ + kuÌ„kLâˆ )Î¸âˆ’1 dÏ„
Z t/2
n
k
1
1
n
Î¸
â‰¤ CR kuÌƒ âˆ’ uÌ„kX
(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’ 2 âˆ’ 2 (1 + Ï„ )âˆ’( 2 + 2 )Î¸ dÏ„
0
Z t
k+1
n
Î¸
1
+CRÎ¸ kuÌƒ âˆ’ uÌ„kX
(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’ 2 (1 + Ï„ )âˆ’( 2 (n+1)+ 2 ) dÏ„
t/2
t

Z

+CCRÎ¸ kuÌƒ âˆ’ uÌ„kX

n

n

k

1

eâˆ’c(tâˆ’Ï„ ) (1 + Ï„ )âˆ’( 4 + 2 Î¸+ 2 + 2 ) dÏ„

0
k

n

1

â‰¤ CRÎ¸ (1 + t)âˆ’ 4 âˆ’ 2 âˆ’ 2 kuÌƒ âˆ’ uÌ„kX ,
which implies
n

k

1

(1 + t) 4 + 2 + 2 kâˆ‚xk (Î¨(uÌƒ) âˆ’ Î¨(uÌ„))kL2 â‰¤ CRÎ¸ kuÌƒ âˆ’ uÌ„kX .

(5.10)

Similarly for h â‰¤ s, from (5.6), (3.14) and (5.1), we have
Z t
kâˆ‚xh (Î¨(uÌƒ) âˆ’ Î¨(uÌ„))t kL2 â‰¤
kâˆ‚xh Gt (t âˆ’ Ï„ ) âˆ— (I âˆ’ aâˆ†)âˆ’1 âˆ†[f (uÌƒ) âˆ’ f (uÌ„)]kL2 dÏ„
0

Z

t/2

n

h

(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’ 2 âˆ’1 k(f (uÌƒ) âˆ’ f (uÌ„))kL1 dÏ„

â‰¤C
0

Z

t

n

(1 + t âˆ’ Ï„ )âˆ’ 4 âˆ’1 kâˆ‚xh (f (uÌƒ) âˆ’ f (uÌ„))kL1 dÏ„

+C
t/2
t

Z
+C

eâˆ’c(tâˆ’Ï„ ) kâˆ‚xh (f (uÌƒ) âˆ’ f (uÌ„))kL2 dÏ„

0
n

h

â‰¤ CRÎ¸ (1 + t)âˆ’ 4 âˆ’ 2 âˆ’1 kuÌƒ âˆ’ uÌ„kX ,
which implies
n

h

(1 + t) 4 + 2 +1 kâˆ‚xh (Î¨(uÌƒ) âˆ’ Î¨(uÌ„))t kL2 â‰¤ CRÎ¸ kuÌƒ âˆ’ uÌ„kX .

(5.11)

Using (5.10), (5.11) and taking R suitably small yields
kÎ¨(uÌƒ) âˆ’ Î¨(uÌ„)kX â‰¤

1
kuÌƒ âˆ’ uÌ„kX .
2

(5.12)

From (5.8) and (5.12), we know that Î¨ is strictly contracting mapping. Consequently, we conclude that there exists a fixed point u âˆˆ XR of the mapping Î¨,
which is a classical solution to (1.1), (1.2). This completes the proof.

Acknowledgements. The author would like to thank the anonymous referee for
his/her comments and suggestions. This work was supported in part by grants
11101144 from the NNSF of China, and 201031 from the Research Initiation Project
for High-level Talents of North China University of Water Resources and Electric
Power.

EJDE-2012/96

EXISTENCE AND ASYMPTOTIC BEHAVIOR

11

References
[1] S. Lai, Y. Wang, Y. Wu and Q. Lin; An initial-boundary value problem for a generalized
Boussinesq water system in a ball, Int. J. Appl. Math. Sci. 3 (2006), 117-133.
[2] Y. Liu and S. Kawashima; Global existence and asymptotic behavior of solutions for quasilinear dissipative plate equation, Discrete Contin. Dyn. Syst. 29 (2011) 1113-1139.
[3] Y. Liu and S. Kawashima; Global existence and decay of solutions for a quasi-linear dissipative plate equation, J. Hyperbolic Differential Equations 8 (2011) 591-614.
[4] T. T. Li and Y. M. Chen; Nonlinear Evolution Equations, Scientific Press, 1989, (in Chinese).
[5] V. G. Makhankov; Dynamics of classical solitons(in nonintegrable systems), Physics Reports,
A review section of Phys. Lett.(Section C) 35(1) (1978), 1-128.
[6] S. Wang and H. Xu; On the asymptotic behavior of solution for the generalized IBq equation
with hydrodynamical damped term, J. Differ. Equ.252 (2012) 4243-4258.
[7] S. Wang and F. Da; On the asymptotic behavior of solution for the generalized double dipersion equation, Appl. Anal. in press.
[8] Y. Wang and Z. Wei; Global existence and asymptotic behavior of solutions to Cahn-Hilliard
equation with inertial term, Internationl Journal of mathematics, accepted.
[9] Y. Wang; Global existence and asymptotic behaviour of solutions for the generalized Boussinesq equation, Nonlinear Anal. 70 (2009) 465-482.
[10] Y. Wang; Global existence of classical solutions to the minimal surface equation in two space
dimensions with slow decay initial value, J. Math. Phys. 50 (2009) 103506-01-103506-01-14.
[11] Y. Wang; Existence and nonexistence of global solutions for a class of nonlinear wave equations of higher order, Nonlinear Anal. 72 (2010) 4500-4507.
[12] Y. Wang and Y. Wang; Global existence of classical solutions to the minimal surface equation
with slow decay initial value, Appl. Math. Comput. 216 (2010) 576-583.
[13] Y. Wang, F. Liu and Y. Zhang; Global existence and asymptotic of solutions for a semi-linear
wave equation, J. Math. Anal. Appl. 385 (2012) 836-853.
[14] Y. Wang and Y. Wang; Global existence and asymptotic behavior of solutions to a nonlinear
wave equation of fourth-order, J. Math. Phys. 53 (2012) 013512-01-013512-13.
[15] Z. Yang; Longtime behavior of the Kirchhoff type equation with strong damping on Rn , J.
Differ. Equ. 242 (2007) 269-286.
[16] Y. Zhang, Q. Lin and S. Lai; Long time asymptotic for the damped Boussinesq equation in
a circle, J. Partial Dif. Eqs. 18(2005), 97-113.
[17] S. M. Zheng; Nonlinear Evolution Equations, Monographs and Surveys in Pure and Applied
Mathematics, 133, Chapan Hall/CRC, 2004.
Yinxia Wang
School of Mathematics and Information Sciences, North China University of Water
Resources and Electric Power, Zhengzhou 450011, China
E-mail address: yinxia117@126.com

Two-stage algorithms for covering array construction

arXiv:1606.06730v1 [cs.DS] 21 Jun 2016

Kaushik Sarkar and Charles J. Colbourn
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, PO Box 878809
Tempe, Arizona, 85287-8809, U.S.A.
June 23, 2016
Abstract
Modern software systems often consist of many different components, each with a number of options.
Although unit tests may reveal faulty options for individual components, functionally correct components
may interact in unforeseen ways to cause a fault. Covering arrays are used to test for interactions
among components systematically. A two-stage framework, providing a number of concrete algorithms,
is developed for the efficient construction of covering arrays. In the first stage, a time and memory
efficient randomized algorithm covers most of the interactions. In the second stage, a more sophisticated
search covers the remainder in relatively few tests. In this way, the storage limitations of the sophisticated
search algorithms are avoided; hence the range of the number of components for which the algorithm can
be applied is extended, without increasing the number of tests. Many of the framework instantiations
can be tuned to optimize a memory-quality trade-off, so that fewer tests can be achieved using more
memory. The algorithms developed outperform the currently best known methods when the number
of components ranges from 20 to 60, the number of options for each ranges from 3 to 6, and t-way
interactions are covered for t âˆˆ {5, 6}. In some cases a reduction in the number of tests by more than
50% is achieved.

Keywords: Covering array, Software interaction testing, Combinatorial construction algorithm

1

Introduction

Real world software and engineered systems are composed of many different components, each with a number
of options, that are required to work together in a variety of circumstances. Components are factors, and
options for a component form the levels of its factor. Although each level for an individual factor can be tested
in isolation, faults in deployed software can arise from interactions among levels of different factors. When
an interaction involves levels of t different factors, it is a t-way interaction. Testing for faults caused by t-way
interactions for every t is generally infeasible, as a result of a combinatorial explosion. However, empirical
research on real world software systems indicates that testing all possible 2-way (or 3-way) interactions
would detect 70% (or 90%) of all faults [25]. Moreover, testing all possible 6-way interactions is sufficient
for detection of 100% of all faults in the systems examined in [25]. Testing all possible t-way interactions for
some 2 â‰¤ t â‰¤ 6 is pseudo-exhaustive testing [24], and is accomplished with a combinatorial array known as
a covering array.
Formally, let N, t, k, and v be integers with k â‰¥ t â‰¥ 2 and v â‰¥ 2. A covering array CA(N ; t, k, v) is an
N Ã— k array A in which each entry is from a v-ary alphabet Î£, and for every N Ã— t sub-array B of A and
every x âˆˆ Î£t , there is a row of B that equals x. Then t is the strength of the covering array, k is the number
of factors, and v is the number of levels.
When k is a positive integer, [k] denotes the set {1, . . . , k}. A t-way interaction is {(ci , ai ) : 1 â‰¤ i â‰¤
t, ci âˆˆ [k], ci 6= cj for i 6= j, and ai âˆˆ Î£}. So an interaction is an assignment of levels from Î£ to t of the k

factors. It,k,v denotes the set of all kt v t interactions for given t, k and v. An N Ã— k array A covers the
1

interaction Î¹ = {(ci , ai ) : 1 â‰¤ i â‰¤ t, ci âˆˆ [k], ci 6= cj for i 6= j, and ai âˆˆ Î£} if there is a row r in A such that
A(r, ci ) = ai for 1 â‰¤ i â‰¤ t. When there is no such row in A, Î¹ is not covered in A. Hence a CA(N ; t, k, v)
covers all interactions in It,k,v .
Covering arrays are used extensively for interaction testing in complex engineered systems. To ensure
that all possible combinations of options of t components function together correctly, one needs examine
all possible t-way interactions. When the number of components is k, and the number of different options
available for each component is v, each row of CA(N ; t, k, v) represents a test case. The N test cases
collectively test all t-way interactions. For this reason, covering arrays have been used in combinatorial
interaction testing in varied fields like software and hardware engineering, design of composite materials,
and biological networks [8, 24, 26, 32, 34].
The cost of testing is directly related to the number of test cases. Therefore, one is interested in covering
arrays with the fewest rows. The smallest value of N for which CA(N ; t, k, v) exists is denoted by CAN(t, k, v).
Efforts to determine or bound CAN(t, k, v) have been extensive; see [12, 14, 24, 31] for example. Naturally one
would prefer to determine CAN(t, k, v) exactly. Katona [22] and Kleitman and Spencer [23] independently
showed that
 for t = v = 2, the minimum number of rows N in a CA(N ; 2, k, 2) is the smallest N for which
âˆ’1
. Exact determination of CAN(t, k, v) for other values of t and v has remained open. However,
kâ‰¤ N
N
d2e
some progress has been made in determining upper bounds for CAN(t, k, v) in the general case; for recent
results, see [33].
For practical applications such bounds are often unhelpful, because one needs explicit covering arrays to
use as test suites. Explicit constructions can be recursive, producing larger covering arrays using smaller ones
as ingredients (see [14] for a survey), or direct. Direct methods for some specific cases arise from algebraic,
geometric, or number-theoretic techniques; general direct methods are computational in nature. Indeed
when k is relatively small, the best known results arise from computational techniques [13], and these are
in turn essential for the successes of recursive methods. Unfortunately, the existing computational methods
encounter difficulties as k increases, but is still within the range needed for practical applications. Typically
such difficulties arise either as a result of storage or time limitations or by producing covering arrays that
are too big to compete with those arising from simpler recursive methods.
Cohen [11] discusses commercial software where the number of factors often exceeds 50. Aldaco et al. [1]
analyze a complex engineered system having 75 factors, using a variant of covering arrays. Android [3] uses
a Configuration class to describe the device configuration; there are 17 different configuration parameters
with 3 âˆ’ 20 different levels. In each of these cases, while existing techniques are effective when the strength
is small, these moderately large values of k pose concerns for larger strengths.
In this paper, we focus on situations in which every factor has the same number of levels. These cases
have been most extensively studied, and hence provide a basis for making comparisons. In practice, however,
often different components have different number of levels, which is captured by extending the notion of a
covering array. A mixed covering array MCA(N ; t, k, (v1 , v2 , . . . , vk )) is an N Ã— k array in which the ith
column contains vi symbols for 1 â‰¤ i â‰¤ k. When {i1 , . . . , it } âŠ† {1, . . . , k} is
Qat set of t columns, in the N Ã— t
subarray obtained by selecting columns i1 , . . . , it of the MCA, each of the j=1 vij distinct t-tuples appears
as a row at least once. Although we examine the uniform case in which v1 = Â· Â· Â· = vk , the methods developed
here can all be directly applied to mixed covering arrays as well.
Inevitably, when k > max(t + 1, v + 2), a covering array must cover some interactions more than once,
for if not they are orthogonal arrays [20]. Treating the rows of a covering array in a fixed order, each row
covers some number of interactions not covered by any earlier row. For a variety of known constructions,
the initial rows cover many new interactions, while the later ones cover very few [7]. Comparing this rate
of coverage for a purely random method and for one of the sophisticated search techniques, one finds little
difference in the initial rows, but very substantial differences in the final ones. This suggests strategies to
build the covering array in stages, investing more effort as the number of remaining uncovered interactions
declines.
In this paper we propose a new algorithmic framework for covering array construction, the two-stage
framework. In the first stage, a randomized row construction method builds a specified number of rows to
cover all but at most a specified, small number of interactions. As we see later, by dint of being randomized

2

this uses very little memory. The second stage is a more sophisticated search that adds few rows to cover the
remaining uncovered interactions. We choose search algorithms whose requirements depend on the number
of interactions to be covered, to profit from the fact that few interactions remain. By mixing randomized and
deterministic methods, we hope to retain the fast execution and small storage of the randomized methods,
along with the accuracy of the deterministic search techniques.
We introduce a number of algorithms within the two-stage framework. Some improve upon best known
bounds on CAN(t, k, v) (see [33]) in principle. But our focus is on the practical consequences: The twostage algorithms are indeed quite efficient for higher strength (t âˆˆ {5, 6}) and moderate number of levels
(v âˆˆ {3, 4, 5, 6}), when the number of factors k is moderately high (approximately in the range of 20 âˆ’ 80
depending on value of t and v). In fact, for many combination of t, k and v values the two-stage algorithms
beat the previously best known bounds.
Torres-Jimenez et al. [36] explore a related two-stage strategy. In their first stage, an in-parameter-order
greedy strategy (as used in ACTS [24]) adds a column to an existing array; in their second stage, simulated
annealing is applied to cover the remaining interactions. They apply their methods when t = v = 3, when
the storage and time requirements for both stages remain acceptable. In addition to the issues in handling
larger strengths, their methods provide no a priori bound on the size of the resulting array. In contrast with
their methods, ours provide a guarantee prior to execution with much more modest storage and time.
The rest of the paper is organized as follows. Section 2 reviews algorithmic methods of covering array
construction, specifically the randomized algorithm and the density algorithm. This section contrasts these
two methods and points out their limitations. Then it gives an intuitive answer to the question of why a two
stage based strategy might work and introduces the general two-stage framework. Section 3 introduces some
specific two-stage algorithms. Section 3.1 analyzes and evaluates the naÄ±Ìˆve strategy. Section 3.2 describes a
two-stage algorithm that combines the randomized and the density algorithm. Section 3.3 introduces graph
coloring based techniques in the second stage. Section 3.4 examines the effect of group action on the size
of the constructed covering arrays. Section 4 compares the results of various two-stage algorithms with the
presently best known sizes. In Section 5 we discuss the LovaÌsz local lemma (LLL) bounds on CAN(t, k, v)
and provide a Moser-Tardos type randomized algorithm for covering array construction that matches the
bound. Although the bound was known [18], the proof was non-constructive, and a constructive algorithm to
match this bound seems to be absent in the literature. We explore potentially better randomized algorithms
for the first stage using LLL based techniques, We also obtain a two-stage bound that improves the LLL
bound for CAN(t, k, v). We conclude the paper in Section 6.

2

Algorithmic construction of covering arrays

Available algorithms for the construction of covering arrays are primarily heuristic in nature; indeed exact
algorithms have succeeded for very few cases. Computationally intensive metaheuristic search methods such
as simulated annealing, tabu search, constraint programming, and genetic algorithms have been employed
when the strength is relatively small or the number of factors and levels is small. These methods have
established many of the best known bounds on sizes of covering arrays [13], but for many problems of
practical size their time and storage requirements are prohibitive. For larger problems, the best available
methods are greedy. The IPO family of algorithms [24] repeatedly adds one column at a time, and
 then adds
new rows to ensure complete coverage. In this way, at any point in time, the status of v t kâˆ’1
tâˆ’1 interactions
may be stored. AETG [10] pioneered a different method, which greedily selects one row at a time to cover
a large number of as-yet-uncovered interactions. They establish that if a row can be chosen that covers the
maximum number, a good a priori bound on the size of the covering array can be computed. Unfortunately
selecting the maximum is NP-hard, and even if one selects the maximum there is no guarantee that the
covering array is the smallest possible
 [7], so AETG resorts to a good heuristic selection of the next row by
examining the stored status of v t kt interactions. None of the methods so far mentioned therefore guarantee
to reach an a priori bound. An
 extension of the AETG strategy, the density algorithm [5, 6, 15], stores
additional statistics for all v t kt interactions in order to ensure the selection of a good next row, and hence
guarantees to produce an array with at most the precomputed number of rows. Variants of the density
3

Algorithm 1: A randomized algorithm for covering array construction.
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; 
t, k, v)
log (kt)+t log v


1 Set N :=
;
vt
log

2
3

4
5
6
7
8
9
10
11
12

v t âˆ’1

repeat
Construct an N Ã— k array A where each entry is chosen independently and uniformly at random
from a v-ary alphabet;
Set covered := true;
for each interaction Î¹ âˆˆ It,k,v do
if Î¹ is not covered in A then
Set covered := false;
break;
end
end
until covered = true;
Output A;

algorithm have proved to be most effective for problems of moderately large size. For even larger problems,
pure random approaches have been applied.
To produce methods that provide a guarantee on size, it is natural to focus on the density algorithm
in order to understand its strengths and weaknesses. To do this, we contrast it with a basic randomized
algorithm. Algorithm 1 shows a simple randomized algorithm for covering array construction. The algorithm
constructs an array of a particular size randomly and checks whether all the interactions are covered. It
repeats until it finds an array that covers all the interactions.
log (kt)+t log v

 is guaranteed to exist:
A CA(N ; t, k, v) with N =
vt
log

v t âˆ’1

Theorem 1. [21, 27, 35] (Stein-LovaÌsz-Johnson (SLJ) bound): Let t, k, v be integers with k â‰¥ t â‰¥ 2, and
v â‰¥ 2. Then as k â†’ âˆ,

log kt + t log v


CAN(t, k, v) â‰¤
t
log vtvâˆ’1
In fact, the probability that the N Ã— k array constructed in line 3 of Algorithm 1 is a valid covering array
is high enough that the expected number of times the loop in line 2 is repeated is a small constant.
An alternative strategy is to add rows one by one instead of constructing the full array at the outset. We
start with an empty array, and whenever we add a new row we ensure that it covers at least the expected
number of previously uncovered interactions for a randomly chosen row. The probability that an uncovered
interaction is covered by a random row is 1/v t . If the number of uncovered interactions is u, then by linearity
of expectation, the expected number of newly covered interactions in a randomly chosen row is uv âˆ’t . If each
row added covers exactly this expected number, we obtain the same number of rows as the SLJ bound,
realized in Algorithm 1. But because the actual number of newly covered interactions is always an integer,
each added row covers at least duv âˆ’t e interactions. This is especially helpful towards the end when the
expected number is a small fraction.
Algorithm 2 follows this strategy. Again the probability that a randomly chosen row covers at least the
expected number of previously uncovered interactions is high enough that the expected number of times the
row selection loop in line 6 of Algorithm 2 is repeated is bounded by a small constant.
We can obtain an upper bound on the size produced by Algorithm 2 by assuming that each new row
added covers exactly duv âˆ’t e previously uncovered interactions. This bound is the discrete Stein-LovaÌsz4

Algorithm 2: A randomized algorithm for covering array construction using the discrete SLJ strategy.
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; t, k, v)
1 Let A be an empty array;

k t
2 Initialize a table T indexed by all t v interactions, marking every interaction â€œuncoveredâ€;
3 while there is an interaction marked â€œuncoveredâ€ in T do
4
Let u be the number of interactions marked â€œuncoveredâ€ in T ;
5
Set expectedCoverage := d vut e;
6
repeat
7
Let r be a row of length k where each entry is chosen independently and uniformly at random
from a v-ary alphabet;
8
Let coverage be the number of â€œuncoveredâ€ interactions in T that are covered in row r;
9
until coverage > expectedCoverage;
10
Add r to A;
11
Mark all interactions covered by r as â€œcoveredâ€ in T ;
12 end
13 Output A;

Johnson (discrete SLJ) bound. Figure 1 compares the sizes of covering arrays obtained from the SLJ and
the discrete SLJ bounds for different values of k when t = 6 and v = 3. Consider a concrete example, when
t = 5, k = 20, and v = 3. The SLJ bound guarantees the existence of a covering array with 12499 rows,
whereas the discrete SLJ bound guarantees the existence of a covering array with only 8117 rows.
The density algorithm replaces the loop at line 6 of Algorithm 2 by a conditional expectation derandomized method. For fixed v and t the density algorithm selects a row efficiently (time polynomial in k) and
deterministically that is guaranteed to cover at least duv âˆ’t e previously uncovered interactions. In practice,
for small values of k the density algorithm works quite well, often covering many more interactions than
the minimum. Many of the currently best known CAN(t, k, v) upper bounds are obtained by the density
algorithm in combination with various post-optimization techniques [13].
However, the practical applicability of Algorithm 2 and the density algorithm is limited by the storage
of the table T , representing each of the kt v t interactions. Even when t = 6, v = 3, and k = 54, there are
18,828,003,285 6-way interactions. This huge memory requirement renders the density algorithm impractical
for rather small values of k when t âˆˆ {5, 6} and v â‰¥ 3. We present an idea to circumvent this large
requirement for memory, and develop it in full in Section 3.

2.1

Why does a two stage based strategy make sense?

Compare the two extremes, the density algorithm and Algorithm 1. On one hand, Algorithm 1 does not suffer
from any substantial storage restriction, but appears to generate many more rows than the density algorithm.
On the other hand, the density algorithm constructs fewer rows for small values of k, but becomes impractical
when k is moderately large. One wants algorithms that behave like Algorithm 1 in terms of memory, but
yield a number of rows competitive with the density algorithm.
For t = 6, k = 16, and v = 3, Figure 2 compares the coverage profile for the density algorithm and
Algorithm 1. We plot the number of newly covered interactions for each row in the density algorithm, and
the expected number of newly covered interactions for each row for Algorithm 1. The qualitative features
exhibited by this plot are representative of the rates of coverage for other parameters.
Two key observations are suggested by Figure 2. First, the expected coverage in the initial random rows
is similar to the rows chosen by the density algorithm. In this example, the partial arrays consisting of the
first 1000 rows exhibit similar coverage, yet the randomized algorithm needed no extensive bookkeeping.
Secondly, as later rows are added, the judicious selections of the density algorithm produce much larger
coverage per row than Algorithm 1. Consequently it appears sensible to invest few computational resources
5

4

3.5

x 10

SLJ bound
Discrete SLJ bound

N âˆ’ number of rows

3

2.5

2

1.5

1

0.5
0

100

200

300

400

500
k

600

700

800

900

1000

Figure 1: Comparison of covering array sizes obtained from SLJ bound and discrete SLJ bound for different
values of k, when t = 6 and v = 3.

9000
Density
Basic Random

Number of newly covered interactions

8000
7000
6000
5000
4000
3000
2000
1000
0

0

2000

4000

6000
Row number

8000

10000

12000

Figure 2: For t = 6, k = 16 and v = 3, the actual number of newly covered interactions of the density
algorithm and the expected number of newly covered interactions in a random array.

6

on the initial rows, while making more careful selections in the later ones. This forms the blueprint of our
general two-stage algorithmic framework shown in Algorithm 3.
Algorithm 3: The general two-stage framework for covering array construction.
Input: t : strength of the required covering array, k : number of factors, v : number of levels for each
factor
Output: A : a CA(N ; t, k, v)
1 Choose a number n of rows and a number Ï of interactions;
// First Stage
0
2 Use a randomized algorithm to construct an n Ã— k array A ;
0
3 Ensure that A covers all but at most Ï interactions;
0
4 Make a list L of interactions that are not covered in A (L contains at most Ï interactions);
// Second Stage
0
5 Use a deterministic procedure to add N âˆ’ n rows to A to cover all the interactions in L;
6 Output A;
A specific covering array construction algorithm results by specifying the randomized method in the first
stage, the deterministic method in the second stage, and the computation of n and Ï. Any such algorithm
produces a covering array, but we wish to make selections so that the resulting algorithms are practical while
still providing a guarantee on the size of the array. In Section 3 we describe different algorithms from the
two-stage family, determine the size of the partial array to be constructed in the first stage, and establish
upper bound guarantees. In Section 4 we explore how good the algorithms are in practice.

3

Two-stage framework

For the first stage we consider two methods:
Rand
MT

the basic randomized algorithm
the Moser-Tardos type algorithm

We defer the development of method MT until Section 5. Method Rand uses a simple variant of Algorithm
1, choosing a random n Ã— k array.
For the second stage we consider four methods:
Naive
Greedy
Den
Col

the
the
the
the

naÄ±Ìˆve strategy, one row per uncovered interaction
online greedy coloring strategy
density algorithm
graph coloring algorithm

Using these abbreviations, we adopt a uniform naming convention for the algorithms: TS hA, Bi is the algorithm in which A is used in the first stage, and B is used in the second stage. For example, TS hMT, Greedyi
denotes a two-stage algorithm where the first stage is a Moser-Tardos type randomized algorithm and the
second stage is a greedy coloring algorithm. Later when the need arises we refine these algorithm names.

3.1

One row per uncovered interaction in the second stage (TS hRand, Naivei)

In the second stage each of the uncovered interactions after the first stage is covered using a new row.
Algorithm 4 describes the method in more detail.
This simple strategy improves on the basic randomized strategy when n is chosen judiciously. For
example, when t = 6, k = 54 and v = 3, Algorithm 1 constructs a covering array with 17, 236 rows. Figure
3 plots an upper bound on the size of the covering array against the number n of rows in the partial array.

7

Algorithm 4: NaÄ±Ìˆve two-stage algorithm (TS hRand, Naivei).
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; t, k, v)
 t 
log (kt)+t log v+log log vtvâˆ’1


;
1 Let n :=
vt
log

1

v t âˆ’1

2

Let Ï =

3

repeat
Let A be an n Ã— k array where each entry is chosen independently and uniformly at random from
a v-ary alphabet;
Let uncovNum := 0 and unCovList be an empty list of interactions;
Set covered := true;
for each interaction Î¹ âˆˆ It,k,v do
if Î¹ is not covered in A then
Set uncovNum :=uncovNum+1;
Add Î¹ to unCovList;
if uncovNum > Ï then
Set covered := false;
break;
end
end
end
until covered= true;
for each interaction Î¹ âˆˆuncovList do
Add a row to A that covers Î¹;
end
Output A;

4

5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

log

vt
v t âˆ’1

;

8

4

Total number of rows in the Covering array

1.75

x 10

1.7
1.65
1.6
1.55
1.5
1.45
1.4
1.35
1.3

1

1.1
1.2
1.3
1.4
1.5
1.6
1.7
n âˆ’âˆ’ number of rows in the partial array of the first stage

1.8
4

x 10

Figure 3: An upper bound on the size of the covering array against n, the size of the partial array constructed
in the first stage when t = 6, k = 54, and v = 3, with one new row added per uncovered interaction in the
second stage. The minimum size of 13, 162 is obtained when n = 12, 402. Algorithm 1 requires 17, 236 rows,
and the currently best known covering array has 17, 197 rows.

9

The smallest covering array is obtained when n = 12, 402 which, when completed, yields a covering array
with at most 13, 162 rowsâ€”a big improvement over Algorithm 1.
A theorem from [33] tells us the optimal value of n in general:
Theorem 2. [33] Let t, k, v be integers with k â‰¥ t â‰¥ 2, and v â‰¥ 2. Then
 t 

log kt + t log v + log log vtvâˆ’1 + 1


.
CAN(t, k, v) â‰¤
t
log vtvâˆ’1
log (kt)+t log v+log log


t
log vtvâˆ’1



vt
v t âˆ’1



. The expected number of uncovered
The bound is obtained by setting n =
 t 
interactions is exactly Ï = 1/ log vtvâˆ’1 .
Figure 4 compares SLJ, discrete SLJ and two-stage bounds for k â‰¤ 100, when t = 6 and v = 3. The
two-stage bound does not deteriorate in comparison to discrete SLJ bound as k increases; it consistently
takes only 307-309 more rows. Thus when k = 12 the two-stage bound requires only 6% more rows and
when k = 100 only 2% more rows than the discrete SLJ bound.
4

2.2

x 10

SLJ bound
Discrete SLJ bound
Twoâˆ’stage bound

2

N âˆ’ number of rows

1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
10

20

30

40

50

60

70

80

90

100

k

Figure 4: Comparison of covering array sizes obtained from SLJ bound, discrete SLJ bound and two-stage
bound for k â‰¤ 100, when t = 6 and v = 3. In this range of k values the two-stage bound requires 307-309
more rows than the discrete SLJ bound, that is, 2-6% more rows.
To ensure that the loop in line 7 of Algorithm 4 does not repeat too many times we need to know the
probability with which a random n Ã— k array leaves at most Ï interactions uncovered. Using Chebyshevâ€™s
inequality and the second moment method developed in [2, Chapter 4], we next show that in a random n Ã—
nk
array the number of uncovered interactions is almost always close to its expectation, i.e. kt v t 1 âˆ’ v1t .
Substituting the value of n from line 1, this expected value is equal to Âµ, as in line 2. Therefore, the probability
that a random n Ã— k array covers the desired number of interactions is constant, and the expected number
of times the loop in line 7 is repeated is also a constant (around 2 in practice).

10

Because the theory of the second moment method is developed
considerable detail in [2], here we briefly
Pin
m
mention the relevant concepts and results. Suppose that X = i=1 Xi , where Xi is the indicator random
variable for event Ai for 1 â‰¤ i â‰¤ m. For indices i, j, we write i âˆ¼ j if i 6= j and the events Ai , Aj are not
independent. Also suppose that X1 , . . . , Xm are symmetric, i.e. for every i 6= j there is a measure
preserving
P
mapping of the underlying probability space that sends event Ai to event Aj . Define âˆ†âˆ— = jâˆ¼i Pr [Aj |Ai ].
Then by [2, Corollary 4.3.4]:
Lemma 3. [2] If E[X] â†’ âˆ and âˆ†âˆ— = o(E[X]) then X âˆ¼ E[X] almost always.
In our case, Ai denotes the event that the ith interaction is not covered in a n Ã— k array where each entry
n
is chosen independently and uniformly at random from a v-ary alphabet. Then Pr[Xi ] = 1 âˆ’ v1t . Because



n
there are kt v t interactions in total, by linearity of expectation, E[X] = kt v t 1 âˆ’ v1t , and E[X] â†’ âˆ as
k â†’ âˆ.
Distinct events Ai and Aj are independent if the ith and jth interactions share no column. Therefore,

P
P
k
the event Ai is not independent of at most t tâˆ’1
other events Aj . So âˆ†âˆ— = jâˆ¼i Pr [Aj |Ai ] â‰¤ jâˆ¼i 1 â‰¤

k
t tâˆ’1
= o(E[X]) when v and t are constants. By Lemma 3, the number of uncovered interactions in a random
n Ã— k array is close to the expected number of uncovered interactions. This guarantees that Algorithm 4 is
an efficient randomized algorithm for constructing covering arrays with a number of rows upper bounded by
Theorem 2.
In keeping with the general two-stage framework, Algorithm 4 does not store the coverage status of
each interaction. We only need store the interactions that are uncovered in A, of which there are at most
1
 â‰ˆ v t . This quantity depends only on v and t and is independent of k, so is effectively a
Ï =
t
log vtvâˆ’1

constant that is much smaller than kt v t , the storage requirement for the density algorithm. Hence the
algorithm can be applied to a higher range of k values.
Although Theorem 5 provides asymptotically tighter bounds than Theorem 2, in a range of k values that
are relevant for practical application, Theorem 2 provides better results. Figure 5 compares the bounds on
CAN(t, k, v) with the currently best known results.
4

3

6

x 10

2.5

Best known
Twoâˆ’stage (simple)
GSS bound
2

2

N âˆ’ number of rows

N âˆ’ number of rows

2.5

1.5

1

1.5

1

0.5

0.5

0
0

x 10

Best known
Twoâˆ’stage (simple)
GSS bound

10

20

30

40

50

60

70

80

90

k

0

5

10

15

20

25

30

35

40

45

50

k

(a) t = 6, v = 3

(b) t = 6, v = 6

Figure 5: Comparison of GSS bound and two-stage bound with the currently best known results

3.2

The density algorithm in the second stage (TS hRand, Deni)

Next we apply the density algorithm in the second stage. Figure 6 plots an upper bound on the size of the
covering array against the size of the partial array constructed in the first stage when the density algorithm
is used in the second stage, and compares it with TS hRand, Naivei. The size of the covering array decreases
11

as n decreases. This is expected because with smaller partial arrays, more interactions remain for the second
stage to be covered by the density algorithm. In fact if we cover all the interactions using the density
algorithm (as when n = 0) we would get an even smaller covering array. However, our motivation was
precisely to avoid doing that. Therefore, we need a â€cut-offâ€ for the first stage.
4

Total number of rows in the Covering array

1.9

x 10

Basic Twoâˆ’stage
Twoâˆ’stage with density in second stage

1.8

1.7

1.6

1.5

1.4

1.3

1.2

1

1.1
1.2
1.3
1.4
1.5
1.6
1.7
n âˆ’âˆ’ number of rows in the partial array of the first stage

1.8
4

x 10

Figure 6: Comparison of covering array sizes from two-stage algorithms with Den and Naive in the second
stage. With Den there is no minimum point in the curve; the size of the covering array keeps decreasing as
we leave more uncovered interactions for the second stage.
We are presented with a trade-off. If we construct a smaller partial array in the first stage, we obtain a
smaller covering array overall. But we then pay for more storage and computation time for the second stage.
To appreciate the nature of this trade-off, look at Figure 7, which plots an upper bound on the covering
array size and the number of uncovered interactions in the first stage against n. The improvement in the
covering array size plateaus after a certain point. The three horizontal lines indicate Ï (â‰ˆ v t ), 2Ï and 3Ï
uncovered interactions in the first stage. (In the naÄ±Ìˆve method of Section 3.1, the partial array after the first
stage leaves at most Ï uncovered interactions.) In Figure 7 the final covering array size appears to plateau
when the number of uncovered interactions left by the first stage is around 2Ï. After that we see diminishing
returns â€” the density algorithm needs to cover more interactions in return for a smaller improvement in the
covering array size.
Let r be the maximum number of interactions allowed to remain uncovered after the first stage. Then r
can be specified in the two-stage algorithm. To accommodate this, we denote by TS hA, B; ri the two-stage
algorithm where A is the first stage strategy, B is the second stage strategy, and r is the maximum number
of uncovered interactions after the first stage. For example, TS hRand, Den; 2Ïi applies the basic randomized
algorithm in the first stage to cover all but at most 2Ï interactions, and the density algorithm to cover the
remaining interactions in the second stage.

3.3

Coloring in the second stage (TS hRand, Coli and TS hRand, Greedyi)

Now we describe strategies using graph coloring in the second stage. Construct a graph G = (V, E), the
incompatibility graph, in which V is the set of uncovered interactions, and there is an edge between two
12

Number of rows / Number of uncovered interactions

18000
16000

Num. of rows in the completed CA
Num. of uncovered interaction in first stage

14000
12000
10000
8000
6000
4000
2000
0
0.8

0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
4
n âˆ’âˆ’ number of rows in the partial array of the first stage
x 10

Figure 7: Final covering array size against the number of uncovered interactions after the first stage. As the
size n of the partial array decreases, the number of uncovered interactions in the first stage increases. Den is
used in the second stage. From bottom to top, the green lines denote Ï, 2Ï, and 3Ï uncovered interactions.
interactions exactly when they share a column in which they have different symbols. A single row can cover
a set of interactions if and only if it forms an independent set in G. Hence the minimum number of rows
required to cover all interactions of G is exactly its chromatic number Ï‡(G), the minimum number of colors
in a proper coloring of G. Graph coloring is an NP-hard problem, so we employ heuristics to bound the
chromatic number. Moreover, G only has vertices for the uncovered interactions after the first stage, so is
size is small relative to the total number of interactions.
The expected number of edges in the incompatibility
graph after
n choosing n rows uniformly at random
 k  t Pt

 t
 
t kâˆ’t
1
1 n
1
tâˆ’i
is Î³ = 2 t v
) 1 âˆ’ vt
1 âˆ’ (vt âˆ’vtâˆ’i ) . Using the elementary upper bound on
i=1 i tâˆ’i (v âˆ’ v
q
the chromatic number Ï‡ â‰¤ 12 + 2m + 14 , where m is the number of edges [16, Chapter 5.2], we can surely
q
cover the remaining interactions with at most 12 + 2m + 14 rows.
The actual number of edges m that remain after the first stage is a random variable with mean Î³. In
principle, the first stage could be repeatedly applied until m â‰¤ Î³, so we call m = Î³ the optimistic estimate.
To ensure that the first stage is expected to be run a small constant number of times, we increase the
estimate. With probability more than 1/2 the incompatibility graph has m â‰¤ 2Î³ edges, so m = 2Î³ is the
conservative estimate.
For t = 6, k = 56, and v = 3, Figure 8 shows the effect on the minimum number of rows when the bound
on the chromatic number in the second stage is used, for the conservative or optimistic estimates. The NaÄ±Ìˆve
method is plotted for comparison. Better coloring bounds shift the minima leftward, reducing the number
of rows produced in both stages.
Thus far we have considered bounds on the chromatic number. Better estimation of Ï‡(G) is complicated
by the fact that we do not have much information about the structure of G until the first stage is run. In
practice, however, G is known after the first stage and hence an algorithmic method to bound its chromatic
number can be applied. Because the number of vertices in G equals the number of uncovered interactions

13

4

2.2

x 10

Conservative estimate
Optimistic estimate
Simple

Number of rows required

2

1.8

1.6

1.4

1.2

1

1

1.1

1.2

1.3

1.4
N

1.5

1.6

1.7

1.8
4

x 10

Figure 8: Size of the partial array vs. size of the complete CA. t = 6, k = 56, v = 3. Stein-LovaÌsz-Johnson
bound requires 17, 403 rows, discrete Stein-LovaÌsz-Johnson bound requires 13, 021 rows. Simple estimate
for the two stage algorithm is 13, 328 rows, conservative estimate assuming m = 2Î³ is 12, 159 rows, and
optimistic estimate assuming m = Î³ is 11, 919 rows. Even the conservative estimate beats the discrete
Stein-LovaÌsz-Johnson bound.

14

after the first stage, we encounter the same trade-off between time and storage, and final array size, as seen
earlier for density. Hence we again parametrize by the expected number of uncovered interactions in the
first stage.
We employ two different greedy algorithms to color the incompatibility graph. In method Col we first
construct the incompatibility graph G after the first stage. Then we apply the commonly used smallest last
order heuristic to order the vertices for greedy coloring: At each stage, find a vertex vi of minimum degree
in Gi , order the vertices of Gi âˆ’ vi , and then place vi at the end. More precisely, we order the vertices of G
as v1 , v2 , . . . , vn , such that vi is a vertex of minimum degree in Gi , where Gi = G âˆ’ {vi+1 , . . . , vn }. A graph
is d-degenerate if all of its subgraphs have a vertex with degree at most d. When G is d-degenerate but not
(d âˆ’ 1)-degenerate, the Coloring number col(G) is d + 1. If we then greedily color the vertices with the first
available color, at most col(G) colors are used.
In method Greedy we employ an on-line, greedy approach that colors the interactions as they are discovered in the first stage. In this way, the incompatibility graph is never constructed. We instead maintain a set
of rows. Some entries in rows are fixed to a specific value; others are flexible to take on any value. Whenever
a new interaction is found to be uncovered in the first stage, we check if any of the rows is compatible with
this interaction. If such a row is found then entries in the row are fixed so that the row now covers the
interaction. If no such row exists, a new row with exactly t fixed entries corresponding to the interaction is
added to the set of rows. This method is much faster than method Col in practice.

3.4

Using group action

Covering arrays that are invariant under the action of a permutation group on their symbols can be easier
to construct and are often smaller [15]. Direct and computational constructions using group actions are
explored in [9, 28]. Sarkar et al. [33] establish the asymptotically tightest known bounds on CAN(t, k, v)
using group actions. In this section we explore the implications of group actions on two-stage algorithms.
Let Î“ be a permutation group on the set of symbols. The action of this group partitions the set of
t-way interactions into orbits. We construct an array A such that for every orbit, at least one row covers
an interaction from that orbit. Then we develop the rows of A over Î“ to obtain a covering array that is
invariant under the action of Î“. Effort then focuses on covering all the orbits of t-way interactions, instead
of the individual interactions.
If Î“ acts sharply transitively
on the set of symbols

 (for example, if Î“ is a cyclic group of order v) then
the action of Î“ partitions kt v t interactions into kt v tâˆ’1 orbits of length v each. Following
the lines of the

v tâˆ’1
+1
log (kt)+(tâˆ’1) log v+log log tâˆ’1
âˆ’1

 v
that covers at
proof of Theorem 2, there exists an n Ã— k array with n =
v tâˆ’1
log

v tâˆ’1 âˆ’1

least one interaction from each orbit. Therefore,
log
CAN(t, k, v) â‰¤ v

k
t



+ (t âˆ’ 1) log v + log log


v tâˆ’1
log vtâˆ’1
âˆ’1



v tâˆ’1
v tâˆ’1 âˆ’1



+1
.

(1)

Similarly, we can employ a Frobenius group. When v is a prime power, the Frobenius group is the group
of permutations of Fv of the form {x 7â†’ ax + b : a, b âˆˆ Fv , a 6= 0}. The action of the Frobenius group
tâˆ’1
partitions the set of t-tuples on v symbols into v vâˆ’1âˆ’1 orbits of length v(v âˆ’ 1) (full orbits) each and 1 orbit
of length v (a short orbit). The short orbit consists of tuples of the form (x1 , . . . , xt ) where x1 = . . . = xt .
Therefore, we can obtain a covering array by first constructing an array that covers all the full orbits, and
then developing all the rows over the Frobenius group and adding v constant rows. Using the two stage
strategy in conjunction with the Frobenius group action we obtain:


 tâˆ’1 

v tâˆ’1
log kt + log v vâˆ’1âˆ’1 + log log vtâˆ’1
âˆ’v+1 + 1


CAN(t, k, v) â‰¤ v(v âˆ’ 1)
+ v.
(2)
tâˆ’1
v
log vtâˆ’1
âˆ’v+1

15

4

1.5

x 10

N âˆ’ number of rows

1.45

Twoâˆ’stage (simple)
Twoâˆ’stage (cyclic group action)
Twoâˆ’stage (Frobenius group action)

1.4

1.35

1.3

1.25
50

55

60

65

70

75

k

Figure 9: Comparison of the simple two-stage bound with the cyclic and the Frobenius two-stage bounds.
t = 6, v = 3 and 50 â‰¤ k â‰¤ 75. Group action reduces the required number of rows slightly.
Figure 9 compares the simple two-stage bound with the cyclic and Frobenius two-stage bounds. For
t = 6, v = 3 and 12 â‰¤ k â‰¤ 100, the cyclic bound requires 7-21 (on average 16) fewer rows than the simple
bound. In the same range the Frobenius bound requires 17 âˆ’ 51 (on average 40) fewer rows.
Group action can be applied in other methods for the second stage as well. Colbourn [15] incorporates
group action into the density algorithm, allowing us to apply method Den in the second stage.
Greedy extends easily to use group action, as we do not construct an explicit incompatibility graph.
Whenever we fix entries in a row to cover an uncovered orbit, we commit to a specific orbit representative.
However, applying group action to the incompatibility graph coloring for Col is more complicated. We
need to modify the definition of the incompatibility graph for two reasons. First the vertices no longer
represent uncovered interactions, but rather uncovered orbits of interaction. Secondly, and perhaps more
importantly, pairwise compatibility between every two orbits in a set no longer implies mutual compatibility
among all orbits in the set.
One approach is to form a vertex for each uncovered orbit, placing an edge between two when they share
a column. Rather than the usual coloring, however, one asks for a partition of the vertex set into classes so
that every class induces an acyclic subgraph. Problems of this type are generalized graph coloring problems
[4]. Within each class of such a vertex partition, consistent representatives of each orbit can be selected to
form a row; when a cycle is present, this may not be possible. Unfortunately, heuristics for solving these
types of problems appear to be weak, so we adopt a second approach. As we build the incompatibility
graph, we commit to specific orbit representatives. When a vertex for an uncovered orbit is added, we check
its compatibility with the orbit representatives chosen for the orbits already handled with which it shares
columns; we commit to an orbit representative and add edges to those with which it is now incompatible.
Once completed, we have a (standard) coloring problem for the resulting graph.
Because group action can be applied using each of the methods for the two stages, we extend our naming
to TS hA, B; r, Î“i, where Î“ can be Trivial (i.e. no group action), Cyclic, or Frobenius.

16

4

Computational results

Figure 5 indicates that even a simple two-stage bound can improve on best known covering array numbers.
Therefore we investigate the actual performance of our two-stage algorithms for covering arrays of strength
5 and 6.
First we present results for t = 6, when v âˆˆ {3, 4, 5, 6} and no group action is assumed. Table 1 shows the
results for different v values. In each case we select the range of k values where the two-stage bound predicts
smaller covering arrays than
 previously known best ones, setting the maximum number of uncovered
 tthe
v
interactions as Ï = 1/ log vt âˆ’1 â‰ˆ v t . For each value of k we construct a single partial array and then run
the different second stage algorithms on it consecutively. In this way all the second stage algorithms cover
the same set of uncovered interactions.
The column tab lists the best known CAN(t, k, v) upper bounds from [13]. The column bound shows the
upper bounds obtained from the two-stage bound (2). The columns naÄ±Ìˆve, greedy, col and den show results
obtained from running the TS hRand, Naive; Ï, Triviali, TS hRand, Greedy; Ï, Triviali, TS hRand, Col; Ï, Triviali
and TS hRand, Den; Ï, Triviali algorithms, respectively.
The naÄ±Ìˆve method always finds a covering array that is smaller than the two-stage bound. This happens
because we repeat the first stage of Algorithm 4 until the array has fewer than v t uncovered interactions.
(If the first stage were not repeated, the algorithm still produce covering arrays that are not too far from
the bound.) For v = 3 Greedy and Den have comparable performance. Method Col produces covering arrays
that are smaller. However, for v âˆˆ {4, 5, 6} Den and Col are competitive.
Table 2 shows the results obtained by the different second stage algorithms when the maximum number
of uncovered interactions in the first stage is set to 2Ï and 3Ï respectively. When more interactions are
covered in the second stage, we obtain smaller arrays as expected. However, the improvement in size does
not approach 50%. There is no clear winner.
Next we investigate the covering arrays that are invariant under the action of a cyclic group. In Table 3 the
column bound shows the upper bounds from Equation (1). The columns naÄ±Ìˆve, greedy, col and den show
results obtained from running TS hRand, Naive; Ï, Cyclici, TS hRand, Greedy; Ï, Cyclici, TS hRand, Col; Ï, Cyclici
and TS hRand, Den; Ï, Cyclici, respectively.
Table 4 presents results for cyclic group action based algorithms when the number of maximum uncovered
interactions in the first stage is set to 2Ï and 3Ï respectively.
For the Frobenius group action, we show results only for v âˆˆ {3, 5} in Table 5. The column bound shows
the upper bounds obtained from Equation (2).
Table 6 presents results for Frobenius group action algorithms when the number of maximum uncovered
interactions in the first stage is 2Ï or 3Ï.
Next we present a handful of results when t = 5. In the cases examined, using the trivial group action
is too time consuming to be practical. However, the cyclic or Frobenius cases are feasible. Tables 7 and 8
compare two stage algorithms when the number of uncovered interactions in the first stage is at most 2Ï.
In almost all cases there is no clear winner among the three second stage methods. Methods Den and
Greedy are, however, substantially faster and use less memory than method Col; for practical purposes they
would be preferred.
All code used in this experimentation is available from the github repository
https://github.com/ksarkar/CoveringArray
under an open source GPLv3 license.

5

Limited dependence and the Moser-Tardos algorithm

Here we explore a different randomized algorithm that produces smaller covering arrays than Algorithm 1.
When k > 2t, there are interactions that share no column. The events of coverage of such interactions are
independent. Moser et al. [29, 30] provide an efficient randomized construction method that exploits this

17

k

tab

bound

53
54
55
56
57

13021
14155
17161
19033
20185

13076
13162
13246
13329
13410

39
40
41
42
43
44

68314
71386
86554
94042
99994
104794

65520
66186
66834
67465
68081
68681

31
32
33
34
35
36

233945
258845
281345
293845
306345
356045

226700
229950
233080
236120
239050
241900

17
18
19
20
21
22
23
24
25
26
27

506713
583823
653756
694048
783784
844834
985702
1035310
1112436
1146173
1184697

486310
505230
522940
539580
555280
570130
584240
597660
610460
622700
634430

naÄ±Ìˆve
greedy
t = 6, v = 3
13056
12421
13160
12510
13192
12590
13304
12671
13395
12752
t = 6, v = 4
65452
61913
66125
62573
66740
63209
67408
63819
68064
64438
68556
65021
t = 6, v = 5
226503 213244
229829 216444
232929 219514
235933 222516
238981 225410
241831 228205
t = 6, v = 6
486302 449950
505197 468449
522596 485694
539532 502023
555254 517346
569934 531910
584194 545763
597152 558898
610389 571389
622589 583473
634139 594933

col

den

12415
12503
12581
12665
12748

12423
12512
12591
12674
12757

61862
62826
63160
64077
64935
65739

61886
62835
63186
64082
64907
65703

212942
217479
219215
222242
226379
230202

212940
217326
219241
222244
226270
229942

448922
467206
484434
500788
516083
530728
544547
557917
570316
582333
593857

447864
466438
483820
500194
515584
530242
548307
557316
569911
582028
593546

Table 1: Comparison of different TS hRand, âˆ’; Ï, Triviali algorithms.

18

k
greedy

2Ï
col

53
54
55
56
57

11968
12135
12286
12429
12562

11958
12126
12129
12204
12290

39
40
41
42
43
44

59433
60090
60715
61330
61936
62530

59323
60479
61527
62488
61839
62899

31
32
33
34
35
36

204105
207243
210308
213267
216082
218884

203500
206659
209716
212675
215521
218314

17
18
19
20
21
22
23
24
25
26
27

425053
443236
460315
476456
491570
505966
519611
532612
544967
556821
568135

-

den
t = 6, v
11968
12050
12131
12218
12296
t = 6, v
59326
59976
60615
61242
61836
62428
t = 6, v
203302
206440
209554
212508
215389
218172
t = 6, v
420333
438754
455941
472198
487501
502009
515774
528868
541353
553377
564827

greedy
=3
11716
11804
11877
11961
12044
=4
58095
58742
59369
59974
60575
61158
=5
199230
202342
205386
208285
211118
213872
=6
412275
430402
447198
463071
478269
492425
505980
518746
531042
542788
554052

3Ï
col

den

11705
11787
11875
12055
12211

11708
11790
11872
11950
12034

57951
58583
59867
61000
60407
61004

57888
58544
59187
59796
60393
60978

198361
201490
204548
-

197889
201068
204107
207060
209936
212707

-

405093
423493
440532
456725
471946
486306
500038
513047
525536
537418
548781

Table 2: Comparison of TS hRand, âˆ’; 2Ï, Triviali and TS hRand, âˆ’; 3Ï, Triviali algorithms.

19

k

tab

bound

53
54
55
56
57

13021
14155
17161
19033
20185

13059
13145
13229
13312
13393

k
39
40
41
42
43
44

tab
68314
71386
86554
94042
99994
104794

bound
65498
66163
66811
67442
68057
68658

31
32
33
34
35
36

226000
244715
263145
235835
238705
256935

226680
229920
233050
236090
239020
241870

17
18
19
20
21
22
23
24
25
26
27

506713
583823
653756
694048
783784
844834
985702
1035310
1112436
1146173
1184697

486290
505210
522910
539550
555250
570110
584210
597630
610430
622670
624400

naÄ±Ìˆve
greedy
t = 6, v = 3
13053
12405
13119
12489
13209
12573
13284
12660
13368
12744
t = 6, v = 4
naÄ±Ìˆve
greedy
65452
61896
66080
62516
66740
63184
67408
63800
68032
64408
68556
64988
t = 6, v = 5
226000 213165
229695 216440
233015 219450
235835 222450
238705 225330
241470 228140
t = 6, v = 6
485616 449778
504546 468156
522258 485586
539280 501972
554082 517236
569706 531852
583716 545562
597378 558888
610026 571380
622290 583320
633294 594786

col

den

12405
12543
12663
12651
12744

12411
12546
12663
12663
12750

col
61860
62820
63144
63780
64692
64964

den
61864
62784
63152
63784
64680
64976

212945
217585
221770
222300
225130
229235

212890
217270
221290
222210
225120
229020

448530
467232
490488
500880
521730
530832
549660
557790
575010
582546
598620

447732
466326
488454
500172
519966
530178
548196
557280
573882
582030
597246

Table 3: Comparison of TS hRand, âˆ’; Ï, Cyclicialgorithms.

20

k
greedy

2Ï
col

53
54
55
56
57

11958
12039
12120
12204
12276

11955
12027
12183
12342
12474

39
40
41
42
43
44

59412
60040
60700
61320
61908
62512

59336
59996
61156
62196
63192
64096

31
32
33
34
35
36

204060
207165
207165
213225
216050
218835

203650
209110
209865
212830
217795
218480

17
18
19
20
21
22
23
24
25
26
27

424842
443118
460014
476328
491514
505884
519498
532368
544842
543684
568050

422736
440922
457944
474252
489270
503580
517458
530340
542688
543684
566244

den
t = 6, v
11958
12036
12195
12324
12450
t = 6, v
59304
59964
61032
61976
62852
63672
t = 6, v
203265
208225
209540
212510
217070
218155
t = 6, v
420252
438762
455994
472158
487500
501852
515718
528828
541332
543684
564756

greedy
=3
11700
11790
11862
11949
12027
=4
58076
58716
59356
59932
60568
61152
=5
199180
202255
205380
208225
211080
213770
=6
411954
430506
447186
463062
478038
492372
505824
518700
530754
542664
553704

3Ï
col

den

11691
11874
12057
11937
12021

11694
11868
12027
11943
12024

57976
58616
59252
59840
61124
61048

57864
58520
59160
59760
60904
60988

198455
204495
204720
207790
213425
213185

197870
203250
204080
207025
212040
212695

409158
427638
456468
460164
486180
489336
502806
515754
538056
539922
560820

405018
423468
449148
456630
479970
486264
500040
512940
532662
537396
555756

Table 4: Comparison of TS hRand, âˆ’; 2Ï, Cyclici and TS hRand, âˆ’; 3Ï, Cyclici algorithms.

21

k

tab

bound

53
54
55
56
57

13021
14155
17161
19033
20185

13034
13120
13203
13286
13366

31
32
33
34
35
36

233945
258845
281345
293845
306345
356045

226570
229820
232950
235980
238920
241760

naÄ±Ìˆve
greedy
t = 6, v = 3
13029
12393
13071
12465
13179
12561
13245
12633
13365
12723
t = 6, v = 5
226425 213025
229585 216225
232725 219285
234905 222265
238185 225205
241525 227925

col

den

12387
12513
12549
12627
12717

12393
12531
12567
12639
12735

212865
216085
219205
223445
227445
231145

212865
216065
219145
223265
227065
230645

Table 5: Comparison of TS hRand, âˆ’; Ï, Frobeniusi algorithms.

k
greedy

2Ï
col

53
54
55
56
57
70
75
80
85
90

11931
12021
12105
12171
12255
13167
13473
13773
14031
14289

11919
12087
12237
12171
12249
13155
13473
13767
14025
14283

31
32
33
34
35
36
50
55
60
65

203785
206965
209985
213005
215765
218605
250625
259785
268185
275785

203485
208965
209645
214825
215545
218285
250365
259625
268025
275665

den
greedy
t = 6, v = 3
11931
11700
12087
11790
12231
11862
12183
11949
12255
12027
13179
13479
13779
14037
14301
t = 6, v = 5
203225 198945
208065 201845
209405 205045
214145 208065
215265 210705
218025 213525
250325
259565
267945
275665
-

3Ï
col

den

11691
11874
12057
11937
12021
-

11694
11868
12027
11943
12024
-

198445
204505
209845
207545
210365
213105
-

197825
203105
207865
206985
209885
212645
-

Table 6: Comparison of TS hRand, âˆ’; 2Ï, Frobeniusi and TS hRand, âˆ’; 3Ï, Frobeniusi algorithms.

22

k
67
68
69
70
71

tab
59110
60991
60991
60991
60991

greedy
48325
48565
48765
49005
49245

col
48285
48565
49005
48985
49205

den
48305
48585
48985
49025
49245

Table 7: Comparison of TS hRand, âˆ’; 2Ï, Frobeniusi algorithms. t = 5, v = 5
k
49
50
51
52
53

tab
122718
125520
128637
135745
137713

greedy
108210
109014
109734
110556
111306

col
108072
108894
110394
110436
111180

den
107988
108822
110166
110364
111120

Table 8: Comparison of TS hRand, âˆ’; 2Ï, Cyclici algorithms. t = 5, v = 6
limited dependence. Specializing their method to covering arrays, we obtain Algorithm 5. For the specified
value of N in the algorithm it is guaranteed that the expected number of times the loop in line 3 of Algorithm
5 is repeated is linearly bounded in k (See Theorem 1.2 of [30]).
The upper bound on CAN(t, k, v) guaranteed by Algorithm 5 is obtained by applying the LovaÌsz local
lemma (LLL).
Lemma 4. (LovaÌsz local lemma; symmetric case) (see [2]) Let A1 , A2 , . . . , An events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at
most d, and that Pr[Ai ] â‰¤ p for all 1 â‰¤ i â‰¤ n. If ep(d + 1) â‰¤ 1, then Pr[âˆ©ni=1 AÌ„i ] > 0.
The symmetric version of LovaÌsz local lemma provides an upper bound on the probability of a â€œbadâ€
event in terms of the maximum degree of a bad event in a dependence graph, so that the probability that
all the bad events are avoided is non zero. Godbole, Skipper, and Sunley [18] apply Lemma 4 essentially to
obtain the bound on CAN(t, k, v) in line 1 of Algorithm 5.
Theorem 5. [18] Let t, v and k â‰¥ 2t be integers with t, v â‰¥ 2. Then
n 
o
+ t log v + 1
log kt âˆ’ kâˆ’t
t


CAN (t, k, v) â‰¤
t
log vtvâˆ’1
The bound on the size of covering arrays obtained from Theorem 5 is asymptotically tighter than the
one obtained from Theorem 1. Figure 10 compares the bounds for t = 6 and v = 3.
The original proof of LLL is essentially non-constructive and does not immediately lead to a polynomial
time construction algorithm for covering arrays satisfying the bound of Theorem 5. Indeed no previous
construction algorithms appear to be based on it. However the Moser-Tardos method of Algorithm 5 does
provide a construction algorithm running in expected polynomial time. For sufficiently large values of k
Algorithm 5 produces smaller covering arrays than the Algorithm 1.
But the question remains: Does Algorithm 5 produce smaller covering arrays than the currently best
known results within the range that it can be effectively computed? Perhaps surprisingly, we show that
the answer is affirmative. In Algorithm 5 we do not need to store the coverage information of individual
interactions in memory because each time an uncovered interaction is encountered we re-sample the columns
involved in that interaction and start the check afresh (checking the coverage in interactions in the same
order each time). Consequently, Algorithm 5 can be applied for larger values of k than the density algorithm.
23

Algorithm 5: Moser-Tardos type algorithm for covering array construction.
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; t, k, v)
log{(kt)âˆ’(kâˆ’t
log v+1
)}+t.
t

;
1 Let N :=
vt
log

2

3
4
5
6
7
8
9
10
11
12
13

14
15
16

v t âˆ’1

Construct an N Ã— k array A where each entry is chosen independently and uniformly at random from
a v-ary alphabet;
repeat
Set covered := true;
for each interaction Î¹ âˆˆ It,k,v do
if Î¹ is not covered in A then
Set covered := false;
Set missing-interaction := Î¹;
break;
end
end
if covered = false then
Choose all the entries in the t columns involved in missing-interaction independently and
uniformly at random from the v-ary alphabet;
end
until covered = true;
Output A;

Smaller covering arrays can be obtained by exploiting a group action using LLL, as shown in [33]. Table
9 shows the sizes of the covering arrays constructed by a variant of Algorithm 5 that employs cyclic and
Frobenius group actions. While this single stage algorithm produces smaller arrays than the currently best
known [13], these are already superseded by the two-stage based algorithms.
k
56
57
58
59
60

tab
19033
20185
23299
23563
23563

MT
16281
16353
16425
16491
16557

(a) Frobenius. t = 6, v = 3

k
44
45
46
47
48

tab
411373
417581
417581
423523
423523

MT
358125
360125
362065
363965
365805

(b) Frobenius. t = 6, v = 5

k
25
26
27
28
29

tab
1006326
1040063
1082766
1105985
1149037

MT
1020630
1032030
1042902
1053306
1063272

(c) Cyclic. t = 6, v = 6

Table 9: Comparison of covering array size from Algorithm 5 (MT) with the best known results [13] (tab).

5.1

Moser-Tardos type algorithm for the first stage

The linearity of expectation arguments used in the SLJ bounds permit one to consider situations in which
a few of the â€œbadâ€ events are allowed to occur, a fact that we exploited in the first stage of the algorithms
thus far. However, the LovaÌsz local lemma does not address this situation directly. The conditional LovaÌsz
local lemma (LLL) distribution, introduced in [19], is a very useful tool.
Lemma 6. (Conditional LLL distribution; symmetric case) (see [2, 33]) Let A = {A1 , A2 , . . . , Al } be a set
of l events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of
all other events Aj except for at most d, and that Pr[Ai ] â‰¤ p for all 1 â‰¤ i â‰¤ l. Also suppose that ep(d+1) â‰¤ 1
(Therefore, by LLL (Lemma 4) Pr[âˆ©li=1 AÌ„i ] > 0). Let B âˆˆ
/ A be another event in the same probability space
24

5

10

N âˆ’ number of rows

SLJ bound
GSS bound

4

10

3

10
1
10

2

3

10

10
k

4

10

5

10

Figure 10: Comparison of SLJ (Theorem 1) and GSS (Theorem 5) bounds for t = 6 and v = 3. The graph
is plotted in log-log scale to highlight the asymptotic difference between the two bounds.
with Pr[B] â‰¤ q, such that B is also mutually independent of a set of all other events Aj âˆˆ A except for at
most d. Then Pr[B| âˆ©li=1 AÌ„i ] â‰¤ eq.
We apply the conditional
LLL distribution to obtain an upper bound on the size of partial array that
 t 
v
leaves at most log vt âˆ’1 â‰ˆ v t interactions uncovered. For a positive integer k, let I = {j1 , . . . , jÏ } âŠ† [k]
where j1 < . . . < jÏ . Let A be an n Ã— k array where each entry is from the set [v]. Let AI denote the n Ã— Ï
array in which AI (i, `) = A(i, j` ) for 1 â‰¤ i â‰¤ N and 1 â‰¤ ` â‰¤ Ï; AI is the projection of A onto the columns
in I.

Let M âŠ† [v]t be a set of m t-tuples of symbols, and C âˆˆ [k]
be a set of t columns. Suppose the
t
entries in the array A are chosen independently from [v] with uniform probability.
 Let BC denote the event
that at least one of the tuples in M is not covered in AC . There are Î· = kt such events, and for all of
n
them Pr[BC ] â‰¤ m 1 âˆ’ v1t . Moreover, when k â‰¥ 2t, each of the events is mutually independent of all



k
other events except for at most Ï = kt âˆ’ kâˆ’t
âˆ’ 1 < t tâˆ’1
. Therefore, by the LovaÌsz local lemma, when
t

1 n
eÏm 1 âˆ’ vt â‰¤ 1, none of the events BC occur. Solving for n, when
nâ‰¥

log(eÏm)


t
log vtvâˆ’1

(3)


there exists an n Ã— k array A over [v] such that for all C âˆˆ [k]
t , AC covers all the m tuples in M . In fact
we can use a Moser-Tardos type algorithm to construct such an array.
Let Î¹ be an interaction whose t-tuple
n of symbols is not in M . Then the probability that Î¹ is not covered
in an n Ã— k array is at most 1 âˆ’ v1t
when each entry of the array is chosen independently from [v] with
uniform probability. Therefore, by the
 conditional LLL distribution the probability that Î¹1 is
n not covered
in the array A where for all C âˆˆ [k]
. Moreover,
t , AC covers all the m tuples in M is at most e 1 âˆ’ v t
there are Î·(v t âˆ’ m) such interactions Î¹. By the linearity of expectation, the expected number of uncovered
25

n
interactions in A is less than v t when Î·(v t âˆ’ m)e 1 âˆ’ v1t â‰¤ v t . Solving for n, we obtain
	

log Î·e 1 âˆ’ vmt

 .
nâ‰¥
t
log vtvâˆ’1

Therefore, there exists an n Ã— k array with n = max

log{Î·e(1âˆ’ vmt )}
log(eÏm)


,

t
t
log vtvâˆ’1
log vtvâˆ’1



(4)

that has at most v t

uncovered interactions. To compute n explicitly, we must choose m. We can select a value of m to minimize
n graphically for given values of t, k and v. For example, Figure 11 plots Equations 3 and 4 against m for
t = 3, k = 350, v = 3, and finds the minimum value of n.
460

445
max(Equation (3), Equation (4))

440

n âˆ’ number of rows in the partial array

n âˆ’ number of rows in the partial array

Equation (3)
Equation (4)

420

400

380

360

340
0

5

10

15
m

20

25

30

(a) Equations 3 and 4 against m.

440

435

430

425

420
0

5

10

15
m

20

25

30

(b) Maximum of the two sizes against m.

Figure 11: The minimum is at n = 422, when m = 16. t = 3, k = 350, v = 3
We compare the size of the partial array from the naÄ±Ìˆve two-stage method (Algorithm 4) with the size
obtained by the graphical methods in Figure 12. The LovaÌsz local lemma based method is asymptotically
better than the simple randomized method. However, except for the small values of t and v, in the range
of k values relevant for practical applications the simple randomized algorithm requires fewer rows than the
LovaÌsz local lemma based method.

5.2

LovaÌsz local lemma based two-stage bound

We can apply the techniques from Section 5.1 to obtain a two-stage bound similar to Theorem 2 using the
LovaÌsz local lemma and conditional LLL distribution. First we extend a result from [33].



Theorem
7. Let t, k, v be integers with k â‰¥ t â‰¥ 2, v â‰¥ 2 and let Î· = kt , and Ï = kt âˆ’ kâˆ’t
t . If


vt
v t âˆ’1

Î·v t log

Ï

â‰¤ v t Then
log
CAN(t, k, v) â‰¤

k
t



+ t log v + log log


t
log vtvâˆ’1



vt
v t âˆ’1



+2

Î·
âˆ’ .
Ï

Proof. Let M âŠ† [v]t be a set of m t-tuples of symbols. Following the arguments of Section 5.1, when


 there exists an n Ã— k array A over [v] such that for all C âˆˆ [k] , AC covers all m tuples in M .
n â‰¥ log(eÏm)
t
vt
log

v t âˆ’1

At most Î·(v t âˆ’ m) interactions are uncovered in such an array. Using the conditional
n LLL distribution,
the probability that one such interaction is not covered in A is at most e 1 âˆ’ v1t . Therefore, by the
26

n âˆ’ number of rows in partial array with vt missing interactions

n âˆ’ number of rows in partial array with vt missing interactions

550
500
450
400
350
300
250
200
150
Randomized (Algorithm 4)
LLL based

100
50

0

200

400

600
k

800

1000

1200

2500

2000

1500

1000

500

0

0

Randomized (Algorithm 4)
LLL based

500

1000

1500

2000

2500

3000

3500

4000

4500

k

(a) t = 3, v = 3.

(b) t = 4, v = 3.

Figure 12: Comparison of the size of the partial array constructed in the first stage. Compares the size of
the partial array specified in Algorithm 4 in Section 3.1, and the size derived in Section 5.1.
 t

n
linearity of expectation, we can find one such array A that leaves at most eÎ·(v t âˆ’ m) 1 âˆ’ v1t = Î·Ï vm âˆ’ 1
interactions uncovered. Adding one row per uncovered interactions to A, we obtain a covering array with at
most N rows, where


log(eÏm)
Î· vt

+
N=
âˆ’
1
t
Ï m
log tv
The value of N is minimized when m =

v âˆ’1
 t 
Î·v t log vtvâˆ’1
Ï

. Because m â‰¤ v t , we obtain the desired bound.

When m = v t this recaptures the bound of Theorem 5.
Figure 13 compares the LLL based two-stage bound from Theorem 7 to the standard two-stage bound
from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1. Although the
LLL based two-stage bound is tighter than the LLL based Godbole et al. bound, even for quite large values
of k the standard two-stage bound is tighter than the LLL based two-stage bound. In practical terms, this
specific LLL based two-stage method does not look very promising, unless the parameters are quite large.

6

Conclusion and open problems

Many concrete algorithms within a two-stage framework for covering array construction have been introduced
and evaluated. The two-stage approach extends the range of parameters for which competitive covering arrays
can be constructed, each meeting an a priori guarantee on its size. Indeed as a consequence a number of
best known covering arrays have been improved upon. Although each of the methods proposed has useful
features, our experimental evaluation suggests that TS hRand, Greedy; 2Ï, Î“i and TS hRand, Den; 2Ï.Î“i with
Î“ âˆˆ {Cyclic, Frobenius} realize a good trade-off between running time and size of the constructed covering
array.
Improvements in the bounds, or in the algorithms that realize them, are certainly of interest. We
mention some specific directions. Establishing tighter bounds on the coloring based methods of Section 3.3
is a challenging problem. Either better estimates of the chromatic number of the incompatibility graph after
a random first stage, or a first stage designed to limit the chromatic number, could lead to improvements in
the bounds.
In Section 5.1 and 5.2 we explored using a Moser-Tardos type algorithm for the first stage. Although this
is useful for asymptotic bounds, practical improvements appear to be limited. Perhaps a different approach of
27

10000

500

9000

450

8000

400

7000

N âˆ’ number of rows

N âˆ’ number of rows

550

350
300
250

SLJ bound
Godbole
LLLâˆ’2âˆ’stage
2âˆ’stage

200
150
100

0

50

100

150

200

250

300

350

400

6000
5000
4000

SLJ bound
Godbole
LLLâˆ’2âˆ’stage
2âˆ’stage

3000
2000

450

k

(a) t = 3, v = 3.

1000
0

1000

2000

3000

4000
k

5000

6000

7000

8000

(b) t = 4, v = 4.

Figure 13: Comparison among the LLL based two-stage bound from Theorem 7, the standard two-stage
bound from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1.
reducing the number of bad events to be avoided explicitly by the algorithm may lead to a better algorithm.
A potential approach may look like following: â€œBadâ€ events would denote non-coverage of an interaction
over a t-set of columns. We would select a set of column t-sets such that the dependency graph of the
corresponding bad events have a bounded maximum degree (less than the original dependency graph). We
would devise a Moser-Tardos type algorithm for covering all the interactions on our chosen column t-sets,
and then apply the conditional LLL distribution to obtain an upper bound on the number of uncovered
interactions. However, the difficulty lies in the fact that â€œall vertices have degree â‰¤ Ïâ€ is a non-trivial,
â€œhereditaryâ€ property for induced subgraphs, and for such properties finding a maximum induced subgraph
with the property is an NP-hard optimization problem [17]. There is still hope for a randomized or â€œnibbleâ€
like strategy that may find a reasonably good induced subgraph with a bounded maximum degree. Further
exploration of this idea seems to be a promising research avenue.
In general, one could consider more than two stages. Establishing the benefit (or not) of having more
than two stages is also an interesting open problem. Finally, the application of the methods developed to
mixed covering arrays appears to provide useful techniques for higher strengths; this merits further study as
well.

Acknowledgments
The research was supported in part by the National Science Foundation under Grant No. 1421058.

References
[1] A. N. Aldaco, C. J. Colbourn, and V. R. Syrotiuk. Locating arrays: A new experimental design for
screening complex engineered systems. SIGOPS Oper. Syst. Rev., 49(1):31â€“40, Jan. 2015.
[2] N. Alon and J. H. Spencer. The probabilistic method. Wiley-Interscience Series in Discrete Mathematics
and Optimization. John Wiley & Sons, Inc., Hoboken, NJ, third edition, 2008. With an appendix on
the life and work of Paul ErdoÌ‹s.
[3] Android. Android configuration class, 2016. http://developer.android.com/reference/android/content/
res/Configuration.html.
[4] J. I. Brown. The complexity of generalized graph colorings. Discrete Appl. Math., 69(3):257â€“270, 1996.
28

[5] R. C. Bryce and C. J. Colbourn. The density algorithm for pairwise interaction testing. Software
Testing, Verification, and Reliability, 17:159â€“182, 2007.
[6] R. C. Bryce and C. J. Colbourn. A density-based greedy algorithm for higher strength covering arrays.
Software Testing, Verification, and Reliability, 19:37â€“53, 2009.
[7] R. C. Bryce and C. J. Colbourn. Expected time to detection of interaction faults. Journal of Combinatorial Mathematics and Combinatorial Computing, 86:87â€“110, 2013.
[8] J. N. Cawse. Experimental design for combinatorial and high throughput materials development. GE
Global Research Technical Report, 29:769â€“781, 2002.
[9] M. A. Chateauneuf, C. J. Colbourn, and D. L. Kreher. Covering arrays of strength 3. Des. Codes
Crypt., 16:235â€“242, 1999.
[10] D. M. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG system: An approach to
testing based on combinatorial design. IEEE Transactions on Software Engineering, 23:437â€“44, 1997.
[11] M. B. Cohen. Designing test suites for software interaction testing. PhD thesis, The University of
Auckland, Department of Computer Science, 2004.
[12] C. J. Colbourn. Combinatorial aspects of covering arrays. Le Matematiche (Catania), 58:121â€“167, 2004.
[13] C. J. Colbourn. Covering array tables, 2005-2015. http://www.public.asu.edu/âˆ¼ccolbou/src/tabby.
[14] C. J. Colbourn. Covering arrays and hash families. In Information Security and Related Combinatorics,
NATO Peace and Information Security, pages 99â€“136. IOS Press, 2011.
[15] C. J. Colbourn. Conditional expectation algorithms for covering arrays. Journal of Combinatorial
Mathematics and Combinatorial Computing, 90:97â€“115, 2014.
[16] R. Diestel. Graph Theory. Graduate Texts in Mathematics. Springer, fourth edition, 2010.
[17] M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NPCompleteness. W. H. Freeman & Co., New York, NY, USA, 1979.
[18] A. P. Godbole, D. E. Skipper, and R. A. Sunley. t-covering arrays: upper bounds and Poisson approximations. Combinatorics, Probability and Computing, 5:105â€“118, 1996.
[19] B. Haeupler, B. Saha, and A. Srinivasan. New constructive aspects of the LovaÌsz local lemma. J. ACM,
58(6):Art. 28, 28, 2011.
[20] A. S. Hedayat, N. J. A. Sloane, and J. Stufken. Orthogonal Arrays. Springer-Verlag, New York, 1999.
[21] D. S. Johnson. Approximation algorithms for combinatorial problems. J. Comput. System Sci., 9:256â€“
278, 1974.
[22] G. O. H. Katona. Two applications (for search theory and truth functions) of Sperner type theorems.
Periodica Math., 3:19â€“26, 1973.
[23] D. Kleitman and J. Spencer. Families of k-independent sets. Discrete Math., 6:255â€“262, 1973.
[24] D. R. Kuhn, R. Kacker, and Y. Lei. Introduction to Combinatorial Testing. CRC Press, 2013.
[25] D. R. Kuhn and M. Reilly. An investigation of the applicability of design of experiments to software
testing. In Proc. 27th Annual NASA Goddard/IEEE Software Engineering Workshop, pages 91â€“95, Los
Alamitos, CA, 2002. IEEE.

29

[26] D. R. Kuhn, D. R. Wallace, and A. M. Gallo. Software fault interactions and implications for software
testing. IEEE Trans. Software Engineering, 30:418â€“421, 2004.
[27] L. LovaÌsz. On the ratio of optimal integral and fractional covers. Discrete Math., 13(4):383â€“390, 1975.
[28] K. Meagher and B. Stevens. Group construction of covering arrays. J. Combin. Des., 13:70â€“77, 2005.
[29] R. A. Moser. A constructive proof of the LovaÌsz local lemma. In STOCâ€™09â€”Proceedings of the 2009
ACM International Symposium on Theory of Computing, pages 343â€“350. ACM, New York, 2009.
[30] R. A. Moser and G. Tardos. A constructive proof of the general LovaÌsz local lemma. J. ACM, 57(2):Art.
11, 15, 2010.
[31] C. Nie and H. Leung. A survey of combinatorial testing. ACM Computing Surveys, 43(2):#11, 2011.
[32] A. H. Ronneseth and C. J. Colbourn. Merging covering arrays and compressing multiple sequence
alignments. Discrete Appl. Math., 157:2177â€“2190, 2009.
[33] K. Sarkar and C. J. Colbourn. Upper bounds on the size of covering arrays. ArXiv e-prints.
[34] G. Seroussi and N. H. Bshouty. Vector sets for exhaustive testing of logic circuits. IEEE Trans. Inform.
Theory, 34:513â€“522, 1988.
[35] S. K. Stein. Two combinatorial covering theorems. J. Combinatorial Theory Ser. A, 16:391â€“397, 1974.
[36] J. Torres-Jimenez, H. Avila-George, and I. Izquierdo-Marquez. A two-stage algorithm for combinatorial
testing. Optimization Letters, pages 1â€“13, 2016.

30

Partial Covering Arrays: Algorithms and Asymptotics
Kaushik Sarkar1 , Charles J. Colbourn1 , Annalisa De Bonis2 , and Ugo Vaccaro2

arXiv:1605.02131v1 [math.CO] 7 May 2016

2

CIDSE, Arizona State University, U.S.A. Dipartimento di Informatica, University of Salerno, Italy

1

Abstract. A covering array CA(N ; t, k, v ) is an N × k array with entries in {1, 2, . . . , v }, for which every N × t subarray contains each ttuple of {1, 2, . . . , v }t among its rows. Covering arrays find application in interaction testing, including software and hardware testing, advanced materials development, and biological systems. A central question is to determine or bound CAN(t, k, v ), the minimum number N of rows of a CA(N ; t, k, v ). The well known bound CAN(t, k, v ) = O((t - 1)v t log k) is not too far from being asymptotically optimal. Sensible relaxations of the covering requirement arise when (1) the set {1, 2, . . . , v }t need only be contained among the rows of at least (1 - ) k of the N × t subarrays t and (2) the rows of every N × t subarray need only contain a (large) subset of {1, 2, . . . , v }t . In this paper, using probabilistic methods, significant improvements on the covering array upper bound are established for both relaxations, and for the conjunction of the two. In each case, a randomized algorithm constructs such arrays in expected polynomial time.

1

Introduction

Let [n] denote the set {1, 2, . . . , n}. Let N, t, k, and v be integers such that k  t  2 and v  2. Let A be an N × k array where each entry is from the set [v ]. For I = {j1 , . . . , j }  [k ] where j1 < . . . < j , let AI denote the N ×  array in which AI (i, ) = A(i, j ) for 1  i  N and 1   ; AI is the projection of A onto the columns in I . A covering array CA(N ; t, k, v ) is an N × k array A with each entry from ] t [v ] so that for each t-set of columns C  [k t , each t-tuple x  [v ] appears as a row in AC . The smallest N for which a CA(N ; t, k, v ) exists is denoted by CAN(t, k, v ). Covering arrays find important application in software and hardware testing (see [22] and references therein). Applications of covering arrays also arise in experimental testing for advanced materials [4], inference of interactions that regulate gene expression [29], fault-tolerance of parallel architectures [15], synchronization of robot behavior [17], drug screening [30], and learning of boolean functions [11]. Covering arrays have been studied using different nomenclature, as qualitatively independent partitions [13], t-surjective arrays [5], and (k, t)universal sets [19], among others. Covering arrays are closely related to hash families [10] and orthogonal arrays [8].

2

Background and Motivation

The exact or approximate determination of CAN(t, k, v ) is central in applications of covering arrays, but remains an open problem. For fixed t and v , only when t = v = 2 is CAN(t, k, v ) known precisely for infinitely many values of k . Kleitman and Spencer [21] and Katona [20] independently proved that the largest k for -1 which a CA(N ; 2, k, 2) exists satisfies k = N orner, N/2 . When t = 2, Gargano, K and Vaccaro [13] establish that CAN(2, k, v ) = v log k (1 + o(1)). 2 (1)

(We write log for logarithms base 2, and ln for natural logarithms.) Several researchers [2,5,14,16] establish a general asymptotic upper bound on CAN(t, k, v ): CAN(t, k, v )  t-1 log k (1 + o(1)). t log vtv-1 (2)

A slight improvement on (2) has recently been proved [12,28]. An (essentially) equivalent but more convenient form of (2) is: CAN(t, k, v )  (t - 1)v t log k (1 + o(1)). (3)

A lower bound on CAN(t, k, v ) results from the inequality CAN(t, k, v )  v · CAN(t - 1, k - 1, v ) obtained by derivation, together with (1), to establish that CAN(t, k, v )  v t-2 · CAN(2, k - t + 2, v ) = v t-2 · v 2 log(k - t + 2)(1 + o(1)). When t < 1, we obtain: k CAN(t, k, v ) =  (v t-1 log k ). (4) Because (4) ensures that the number of rows in covering arrays can be considerable, researchers have suggested the need for relaxations in which not all interactions must be covered [7,18,23,24] in order to reduce the number of rows. The practical relevance is that each row corresponds to a test to be performed, adding to the cost of testing. For example, an array covers a t-set of columns when it covers each of the v t interactions on this t-set. Hartman and Raskin [18] consider arrays with a fixed number of rows that cover the maximum number of t-sets of columns. A similar question was also considered in [24]. In [23,24] a more refined measure of the (partial) coverage of an N × k array A is introduced. For a given q  [0, 1], let (A, q ) be the number of N × t submatrices of A with the property that at least qv t elements of [v ]t appear in their set of rows; the (q, t)-completeness of A is (A, q )/ k t . Then for practical purposes one wants "high" (q, t)-completeness with few rows. In these works, no theoretical results on partial coverage appear to have been stated; earlier contributions focus on experimental investigations of heuristic construction methods. Our purpose is to initiate a mathematical investigation of arrays offering "partial" coverage. More precisely, we address:

­ Can one obtain a significant improvement on the upper bound (3) if the set [v ]t is only required to be contained among the rows of at least (1 - ) k t subarrays of A of dimension N × t? ­ Can one obtain a significant improvement if, among the rows of every N × t subarray of A, only a (large) subset of [v ]t is required to be contained? ­ Can one obtain a significant improvement if the set [v ]t is only required to be contained among the rows of at least (1 - ) k t subarrays of A of dimension N × t, and among the rows of each of the k t subarrays that remain, a (large) subset of [v ]t is required to be contained? We answer these questions both theoretically and algorithmically in the following sections.

3

Partial Covering Arrays

When 1  m  v t , a partial m-covering array, PCA(N ; t, k, v, m), is an N × k ] array A with each entry from [v ] so that for each t-set of columns C  [k t , at t least m distinct tuples x  [v ] appear as rows in AC . Hence a covering array CA(N ; t, k, v ) is precisely a partial v t -covering array PCA(N ; t, k, v, v t ). Theorem 1. For integers t, k, v , and m where k  t  2, v  2 and 1  m  v t there exists a PCA(N ; t, k, v, m) with ln N ln . Proof. Let r = v t -m+1, and A be a random N ×k array where each entry is cho] sen independently from [v ] with uniform probability. For C  [k t , let BC denote t the event that at least r tuples from [v ] are missing in AC . The probability that r N a particular r-set of tuples from [v ]t is missing in AC is 1 - v . Applying the t
r . union bound to all r-sets of tuples from [v ]t , we obtain Pr[BC ]  v 1- v t r By linearity of expectation, the expected number of t-sets C for which AC misses vt r N at least r tuples from [v ]t is at most k 1- v . When A has at least t t r vt ln (k )( ) t m-1 rows this expected number is less than 1. Therefore, an array A vt ln( m -1 ) ] exists with the required number of rows such that for all C  [k t , AC misses t at most r - 1 tuples from [v ] , i.e. AC covers at least m tuples from [v ]t .
t

k t

vt m-1 vt m-1

.

(5)

N

Theorem 1 can be improved upon using the Lov´ asz local lemma. Lemma 1. (Lov´ asz local lemma; symmetric case) (see [1]) Let A1 , A2 , . . . , An events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at most d, and that Pr[Ai ]  ¯ p for all 1  i  n. If ep(d + 1)  1, then Pr[n i=1 Ai ] > 0.

Lemma 1 provides an upper bound on the probability of a "bad" event in terms of the dependence structure among such bad events, so that there is a guaranteed outcome in which all "bad" events are avoided. This lemma is most useful when there is limited dependence among the "bad" events, as in the following: Theorem 2. For integers t, k, v and m where v, t  2, k  2t and 1  m  v t there exists a PCA(N ; t, k, v, m) with 1 + ln t N ln
k t-1 vt m-1 vt m-1

.

(6)

] Proof. When k  2t, each event BC with C  [k (that is, at least v t - m + 1 t t k-1 k tuples are missing in AC ) is independent of all but at most 1 t-1 < t t-1 [k ] events in {BC : C  t \ {C }}. Applying Lemma 1, Pr[C ([k]) BC ] > 0 when
t

e

vt r

1-

r vt

N

t

k t-1

 1.

(7)

Solve (7) to obtain the required upper bound on N . When m = v t , apply the Taylor series expansion to obtain ln and thereby recover the upper bound (3). Theorem 2 implies: Corollary 1. Given q  [0, 1] and integers 2  t  k , v  2, there exists an N × k array on [v ] with (q, t)-completeness equal to 1 (i.e., maximal), whose number N of rows satisfies 1 + ln t N ln
k t-1 vt qv t -1 vt m-1



1 vt ,

vt qv t -1

.

Rewriting (6), setting r = v t - m + 1, and using the Taylor series expansion r of ln 1 - v t , we get 1 + ln t N ln
k t-1 vt v t -r vt r



v t (t - 1) ln k r

1-

ln r + o(1) . ln k

(8)

Hence when r = v (t - 1) (or equivalently, m = v t - v (t - 1) + 1), there is a partial m-covering array with (v t-1 ln k ) rows. This matches the lower bound (4) asymptotically for covering arrays by missing, in each t-set of columns, no more than v (t - 1) - 1 of the v t possible rows. The dependence of the bound (6) on the number of v -ary t-vectors that must appear in the t-tuples of columns is particularly of interest when test suites are run sequentially until a fault is revealed, as in [3]. Indeed the arguments here may have useful consequences for the rate of fault detection.

Algorithm 1: Moser-Tardos type algorithm for partial m-covering arrays.
Input: Integers N, t, k, v and m where v, t  2, k  2t and 1  m  v t Output: A : a PCA(N ; t, k, v, m) k vt 1+ln t(t- 1)(m-1) Let N := ; t v
ln m-1

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

Construct an N × k array A where each entry is chosen independently and uniformly at random from [v ]; repeat Set covered := true; ] for each column t-set C  [k do t if AC does not cover at least m distinct t-tuples x  [v ]t then Set covered := false; Set missing-column-set := C ; break; end end if covered = false then Choose all the entries in the t columns of missing-column-set independently and uniformly at random from [v ]; end until covered = true ; Output A;

Lemma 1 and hence Theorem 2 have proofs that are non-constructive in nature. Nevertheless, Moser and Tardos [26] provide a randomized algorithm with the same guarantee. Patterned on their method, Algorithm 1 constructs a partial m-covering array with exactly the same number of rows as (6) in expected polynomial time. Indeed, for fixed t, the expected number of times the resampling step (line 13) is repeated is linear in k (see [26] for more details).

4

Almost Partial Covering Arrays

For 0 < < 1, an -almost partial m-covering array, APCA(N ; t, k, v, m, ), is an N × k array A with each entry from [v ] so that for at least (1 - ) k t column [k] t t-sets C  t , AC covers at least m distinct tuples x  [v ] . Again, a covering array CA(N ; t, k, v ) is precisely an APCA(N ; t, k, v, v t , ) when < 1/ k t . Our first result on -almost partial m-covering arrays is the following. Theorem 3. For integers t, k, v, m and real where k  t  2, v  2, 1  m  v t and 0   1, there exists an APCA(N ; t, k, v, m, ) with ln N ln
vt m-1 vt m-1

/ . (9)

Proof. Parallelling the proof of Theorem 1 we compute an upper bound on the ] t expected number of t-sets C  [k t for which AC misses at least r tuples x  [v ] . k When this expected number is at most t , an array A is guaranteed to exist [k] with at least (1 - ) k such that AC misses at most t t-sets of columns C  t t r - 1 distinct tuples x  [v ] . Thus A is an APCA(N ; t, k, v, m, ). To establish the theorem, solve the following for N : k t vt r 1- r vt
N



k . t

When < 1/ k t we recover the bound from Theorem 1 for partial m-covering arrays. In terms of (q, t)-completeness, Theorem 3 yields the following. Corollary 2. For q  [0, 1] and integers 2  t  k , v  2, there exists an N × k array on [v ] with (q, t)-completeness equal to 1 - , with ln N ln
vt m-1 vt m-1

/ .

When m = v t , an -almost covering array exists with N  v t ln v rows. Improvements result by focussing on covering arrays in which the symbols are acted on by a finite group. In this setting, one chooses orbit representatives of rows that collectively cover orbit representatives of t-way interactions under the group action; see [9], for example. Such group actions have been used in direct and computational methods for covering arrays [6,25], and in randomized and derandomized methods [9,27,28]. We employ the sharply transitive action of the cyclic group of order v , adapting the earlier arguments using methods from [28]: Theorem 4. For integers t, k, v and real where k  t  2, v  2 and 0   1 there exists an APCA(N ; t, k, v, v t , ) with N  v t ln v t-1 . (10)

t

Proof. The action of the cyclic group of order v partitions [v ]t into v t-1 orbits, each of length v . Let n = N and let A be an n × k random array v where each entry is chosen independently from the set [v ] with uniform prob] ability. For C  [k t , AC covers the orbit X if at least one tuple x  X is present in AC . The probability that the orbit X is not covered in A is n v n 1- v = 1 - vt1 . Let DC denote the event that AC does not cover t -1 n at least one orbit. Applying the union bound, Pr[DC ]  v t-1 1 - vt1 . By -1 linearity of expectation, the expected number of column t-sets C for which DC n t-1 occurs is at most k 1 - vt1 . As earlier, set this expected value to be -1 t v

at most k t and solve for n. An array exists that covers all orbits in at least (1 - ) k column t-sets. Develop this array over the cyclic group to obtain the t desired array. As in [28], further improvements result by considering a group, like the Frobenius group, that acts sharply 2-transitively on [v ]. When v is a prime power, the Frobenius group is the group of permutations of Fv of the form {x  ax + b : a, b  Fv , a = 0}. Theorem 5. For integers t, k, v and real where k  t  2, v  2, v is a prime power and 0   1 there exists an APCA(N ; t, k, v, v t , ) with N  v t ln 2v t-2 + v.
t- 1

(11)

1 Proof. The action of the Frobenius group partitions [v ]t into v v-- orbits of 1 length v (v - 1) (full orbits) each and 1 orbit of length v (a short orbit). The short orbit consists of tuples of the form (x1 , . . . , xt )  [v ]t where x1 = . . . = xt . -v Let n = vN (v -1) and let A be an n × k random array where each entry is chosen independently from the set [v ] with uniform probability. Our strategy is to construct A so that it covers all full orbits for the required number of arrays ] {AC : C  [k t }. Develop A over the Frobenius group and add v rows of the form (x1 , . . . , xk )  [v ]t with x1 = . . . = xk to obtain an APCA(N ; t, k, v, v t , ) with the desired value of N . Following the lines of the proof of Theorem 4, A covers all full orbits in at least (1 - ) k t column t-sets C when

k v t-1 - 1 t v-1 Because
v t-1 -1 v -1

1-

v-1 v t-1

n



k . t

 2v t-2 for v  2, we obtain the desired bound.

Using group action when m = v t affords useful improvements. Does this improvement extend to cases when m < v t ? Unfortunately, the answer appears to be no. Consider the case for PCA(N ; t, k, v, m) when m  v t using the action of the cyclic group of order v on [v ]t . Let A be a random n × k array over [v ]. When v t - vs + 1  m  v t - v (s - 1) for 1  s  v t-1 , this implies that ] t for all C  [k t , AC misses at most s - 1 orbits of [v ] . Then we obtain that n  1 + ln t
k t-1 v t-1 s

/ ln

v t-1 v t-1 -s

. Developing A over the cyclic group

we obtain a PCA(N ; t, k, v, m) with 1 + ln N v ln
k t-1 v t-1 s

v t-1 v t-1 -s

(12)

Figure 1 compares (12) and (6). In Figure 1a we plot the size of the partial m-covering array as obtained by (12) and (6) for v t - 6v + 1  m  v t and

9

x 10

4

10 Eq. (12) Eq. (6)

6

8

Eq. (12) Eq. (6)

7 N - number of rows N - number of rows 4075 4080 4085 m 4090 4095 4100

6

10

5

5

4

3
4

2 4070

10 1 10

10

2

10 k

3

10

4

(a) t = 6, k = 20, v = 4

(b) t = 6, v = 4, m = v t - v

Fig. 1: Comparison of (12) and (6). Figure (a) compares the sizes of the partial m-covering arrays when v t - 6v + 1  m  v t . Except for m = v t = 4096 the bound from (6) outperforms the bound obtained by assuming group action. Figure (b) shows that for m = v t - v = 4092, (6) outperforms (12) for all values of k . t = 6, k = 20, v = 4. Except when m = v t = 4096, the covering array case, (6) outperforms (12). Similarly, Figure 1b shows that for m = v t - v = 4092, (6) consistently outperforms (12) for all values of k when t = 6, v = 4. We observe similar behavior for different values of t and v . Next we consider even stricter coverage restrictions, combining Theorems 2 and 4. Theorem 6. For integers t, k, v, m and real where k  t  2, v  2, 0   1 k and m  v t + 1 - ln(v/ln 1/(t-1) ) there exists an N × k array A with entries from [v ] such that
] t 1. for each C  [k t , AC covers at least m tuples x  [v ] , t 2. for at least (1 - ) k t column t-sets C , AC covers all tuples x  [v ] ,

3. N = O(v t ln

v t-1

).

Proof. We vertically juxtapose a partial m-covering array and an -almost v t k t covering array. For r = ln(v/ln 1/(t-1) ) and m = v - r + 1, (8) guarantees the existence of a partial m-covering array with v t ln
t v t-1

{1 + o(1)} rows. The-

orem 4 guarantees the existence of an -almost v -covering array with at most t-1 v t ln v rows. Corollary 3. There exists an N × k array A such that: 1. for any t-set of columns C  distinct t-tuples x  [v ]t ,
[k] t

, AC covers at least m  v t + 1 - v (t - 1)

2. for at least 1 -
t

v t- 1 k1/v

k t

column t-sets C , AC covers all the distinct t-tuples

x  [v ] . 3. N = O(v t-1 ln k ). Proof. Apply Theorem 6 with m = v t + 1 -
ln k t ln(v/
1/(t-1) )

ln k ln(v/
1/(t-1) )

. There are at most

- 1 missing t-tuples x  [v ] in the AC for each of the at most k t column t-sets C that do not satisfy the second condition of Theorem 6. To bound from above the number of missing tuples to a certain small function f (t) of t, it
1 f (t)+1 is sufficient that  v t-1 k . Then the number of missing t-tuples x  [v ]t in AC is bounded from above by f (t) whenever is not larger than
t- 1

v

t-1

1 k

t- 1 f (t)+1

(13)
v t-1

On the other hand, in order for the number N = O v t-1 ln

of rows is not (14)

of A to be asymptotically equal to the lower bound (4), it suffices that smaller than v t-1 1 . kv

When f (t) = v (t - 1) - 1, (13) and (14) agree asymptotically, completing the proof. Once again we obtain a size that is O(v t-1 log k ), a goal that has not been reached for covering arrays. This is evidence that even a small relaxation of covering arrays provides arrays of the best sizes one can hope for. Next we consider the efficient construction of the arrays whose existence is ensured by Theorem 6. Algorithm 2 is a randomized method to construct an APCA(N ; t, k, v, m, ) of a size N that is very close to the bound of Theorem 3. By Markov's inequality the condition in line 9 of Algorithm 2 is met with probability at most 1/2. Therefore, the expected number of times the loop in line 2 repeats is at most 2. To prove Theorem 3, t-wise independence among the variables is sufficient. Hence, Algorithm 2 can be derandomized using t-wise independent random variables. We can also derandomize the algorithm using the method of conditional expectation. In this method we construct A by considering the k columns one by one and fixing all N entries of a column. Given a set of already fixed columns, to fix the entries of the next column we consider all possible v N choices, and choose one that provides the maximum conditional expectation of the number of ] column t-sets C  [k such that AC covers at least m tuples x  [v ]t . Because t N v = O(poly(1/ )), this derandomized algorithm constructs the desired array in polynomial time. Similar randomized and derandomized strategies can be applied to construct the array guaranteed by Theorem 4. Together with Algorithm 1 this implies that the array in Theorem 6 is also efficiently constructible.

Algorithm 2: Randomized algorithm for -almost partial m-covering arrays.
Input: Integers N, t, k, v and m where v, t  2, k  2t and 1  m  v t , and real 0< <1 Output: A : an APCA(N ; t, k, v, m, ) vt ln 2(m / -1) Let N := ; vt
ln m-1

1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

repeat Construct an N × k array A where each entry is chosen independently and uniformly at random from [v ]; Set isAPCA:= true; Set defectiveCount := 0; ] for each column t-set C  [k do t if AC does not cover at least m distinct t-tuples x  [v ]t then Set defectiveCount := defectiveCount + 1; k if defectiveCount > then t Set isAPCA:= false; break; end end end until isAPCA = true ; Output A;

5

Final Remarks

We have shown that by relaxing the coverage requirement of a covering array somewhat, powerful upper bounds on the sizes of the arrays can be established. Indeed the upper bounds are substantially smaller than the best known bounds for a covering array; they are of the same order as the lower bound for CAN(t, k, v ). As importantly, the techniques not only provide asymptotic bounds but also randomized polynomial time construction algorithms for such arrays. Our approach seems flexible enough to handle variations of these problems. For instance, some applications require arrays that satisfy, for different subsets of columns, different coverage or separation requirements [8]. In [16] several interesting examples of combinatorial problems are presented that can be unified and expressed in the framework of S -constrained matrices. Given a set of vectors ] S each of length t, an N × k matrix M is S -constrained if for every t-set C  [k t , MC contains as a row each of the vectors in S . The parameter to optimize is, as usual, the number of rows of M . One potential direction is to ask for arrays that, in every t-tuple of columns, cover at least m of the vectors in S , or that all vectors in S are covered by all but a small number of t-tuples of columns. Exploiting the structure of the members of S appears to require an extension of the results developed here.

Acknowledgements
Research of KS and CJC was supported in part by the National Science Foundation under Grant No. 1421058.

References
1. Noga Alon and Joel H. Spencer. The probabilistic method. Wiley-Interscience Series in Discrete Mathematics and Optimization. John Wiley & Sons, Inc., Hoboken, NJ, third edition, 2008. 2. B. Becker and H.-U. Simon. How robust is the n-cube? Inform. and Comput., 77:162­178, 1988. 3. Ren´ ee C. Bryce, Yinong Chen, and Charles J. Colbourn. Biased covering arrays for progressive ranking and composition of web services. Int. J. Simulation Process Modelling, 3(1/2):80­87, 2007. 4. J. N. Cawse. Experimental design for combinatorial and high throughput materials development. GE Global Research Technical Report, 29:769­781, 2002. 5. Ashok K. Chandra, Lawrence T. Kou, George Markowsky, and Shmuel Zaks. On sets of boolean n-vectors with all k-projections surjective. Acta Informatica, 20(1):103­111, 1983. 6. M. A. Chateauneuf, C. J. Colbourn, and D. L. Kreher. Covering arrays of strength 3. Des. Codes Crypt., 16:235­242, 1999. 7. Baiqiang Chen and Jian Zhang. Tuple density: a new metric for combinatorial test suites. In Proceedings of the 33rd International Conference on Software Engineering, ICSE 2011, Waikiki, Honolulu , HI, USA, May 21-28, 2011, pages 876­879, 2011. 8. C. J. Colbourn. Combinatorial aspects of covering arrays. Le Matematiche (Catania), 58:121­167, 2004. 9. C. J. Colbourn. Conditional expectation algorithms for covering arrays. Journal of Combinatorial Mathematics and Combinatorial Computing, 90:97­115, 2014. 10. Charles J. Colbourn. Covering arrays and hash families. In D. Crnkovi c and V. Tonchev, editors, Information Security, Coding Theory, and Related Combinatorics, NATO Science for Peace and Security Series, pages 99­135. IOS Press, 2011. 11. Peter Damaschke. Adaptive versus nonadaptive attribute-efficient learning. Machine Learning, 41(2):197­215, 2000. 12. N. Franceti´ c and B. Stevens. Asymptotic size of covering arrays: an application of entropy compression. ArXiv e-prints, March 2015. 13. L. Gargano, J. K¨ orner, and U. Vaccaro. Sperner capacities. Graphs and Combinatorics, 9:31­46, 1993. 14. A. P. Godbole, D. E. Skipper, and R. A. Sunley. t-covering arrays: upper bounds and Poisson approximations. Combinatorics, Probability and Computing, 5:105­ 118, 1996. 15. N. Graham, F. Harary, M. Livingston, and Q.F. Stout. Subcube fault-tolerance in hypercubes. Information and Computation, 102(2):280 ­ 314, 1993. 16. Sylvain Gravier and Bernard Ycart. S-constrained random matrices. DMTCS Proceedings, 0(1), 2006.

17. A. Hartman. Software and hardware testing using combinatorial covering suites. In M. C. Golumbic and I. B.-A. Hartman, editors, Interdisciplinary Applications of Graph Theory, Combinatorics, and Algorithms, pages 237­266. Springer, Norwell, MA, 2005. 18. Alan Hartman and Leonid Raskin. Problems and algorithms for covering arrays. Discrete Mathematics, 284(13):149 ­ 156, 2004. 19. Stasys Jukna. Extremal Combinatorics: With Applications in Computer Science. Springer Publishing Company, Incorporated, 1st edition, 2010. 20. G. O. H. Katona. Two applications (for search theory and truth functions) of Sperner type theorems. Periodica Math., 3:19­26, 1973. 21. D. Kleitman and J. Spencer. Families of k-independent sets. Discrete Math., 6:255­262, 1973. 22. D. R. Kuhn, R. Kacker, and Y. Lei. Introduction to Combinatorial Testing. CRC Press, 2013. 23. D. R. Kuhn, I. D. Mendoza, R. N. Kacker, and Y. Lei. Combinatorial coverage measurement concepts and applications. In Software Testing, Verification and Validation Workshops (ICSTW), 2013 IEEE Sixth International Conference on, pages 352­361, March 2013. 24. J. R. Maximoff, M. D. Trela, D. R. Kuhn, and R. Kacker. A method for analyzing system state-space coverage within a t-wise testing framework. In 4th Annual IEEE Systems Conference, pages 598­603, 2010. 25. K. Meagher and B. Stevens. Group construction of covering arrays. J. Combin. Des., 13:70­77, 2005. 26. Robin A. Moser and G´ abor Tardos. A constructive proof of the general Lov´ asz local lemma. J. ACM, 57(2):Art. 11, 15, 2010. 27. K. Sarkar and C. J. Colbourn. Two-stage algorithms for covering array construction. submitted for publication. 28. K. Sarkar and C. J. Colbourn. Upper bounds on the size of covering arrays. ArXiv e-prints, March 2016. 29. D. E. Shasha, A. Y. Kouranov, L. V. Lejay, M. F. Chou, and G. M. Coruzzi. Using combinatorial design to study regulation by multiple input signals: A tool for parsimony in the post-genomics era. Plant Physiol., 127:1590­2594, 2001. 30. A. J. Tong, Y. G. Wu, and L. D. Li. Room-temperature phosphorimetry studies of some addictive drugs following dansyl chloride labelling. Talanta, 43(9):14291436, September 1996.

Search, test, and measurement problems in sparse domains often require the construction of arrays in which every t or fewer columns satisfy a simply stated combinatorial condition. Such t-restriction problems often ask for the construction of an array satisfying the t-restriction while having as few rows as possible. Combinatorial, algebraic, and probabilistic methods have been brought to bear for specific t-restriction problems; yet in most cases they do not succeed in constructing arrays with a number of rows near the minimum, at least when the number of columns is small. To address this, an algorithmic method is proposed that, given an array satisfying a t-restriction, attempts to improve the array by removing rows. The key idea is to determine the necessity of the entry in each cell of the array in meeting the t-restriction, and repeatedly replacing unnecessary entries, with the goal of producing an entire row of unnecessary entries. Such a row can then be deleted, improving the array, and the process can be iterated. For certain t-restrictions, it is shown that by determining conflict graphs, entries that are necessary can nonetheless be changed without violating the t-restriction. This permits a richer set of ways to improve the arrays. The efficacy of these methods is demonstrated via computational results.Locating Arrays: A New Experimental Design for
Screening Complex Engineered Systems
Abraham N. Aldaco, Charles J. Colbourn, and Violet R. Syrotiuk
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, Tempe, AZ, U.S.A. 85287-8809

{aaldacog, colbourn, syrotiuk}@asu.edu
ABSTRACT

Screening: Which factors and interactions are most inï¬‚uential on
a response?

The purpose of a screening experiment is to identify signiï¬cant factors and interactions on a response for a system. Engineered systems are complex in part due to their size. To apply traditional
experimental designs for screening in complex engineered systems
requires either restricting the factors considered, which automatically restricts the interactions to those in the set, or restricting interest to main effects, which fails to consider any possible interactions. To address this problem we propose a locating array (LA) as
a screening design. Locating arrays exhibit logarithmic growth in
the number of factors because their focus is on identiï¬cation rather
than on measurement. This makes practical the consideration of an
order of magnitude more factors in experimentation than traditional
screening designs. We present preliminary results applying an LA
for screening the response of TCP throughput in a simulation model
of a mobile wireless network. The full-factorial design for this system is infeasible (over 1043 design points!) yet an LA has only 421
design points. We validate the signiï¬cance of the identiï¬ed factors
and interactions independently using the statistical software JMP.
Screening using locating arrays is viable and yields useful models.

Conï¬rmation: Is the system currently performing in the same way
as it did in the past?
Discovery: What happens when new operating conditions, materials, factors, etc., are explored?
Robustness: Under what conditions does a response degrade?
Stability: How can variability in a response be reduced?
Our focus is on screening using techniques from statistical design
of experiments (DoE). DoE refers to the process of planning an
experiment so that appropriate data are collected and analyzed by
statistical methods, in order to result in valid and objective conclusions. Hence any experimental problem includes both the design of
the experiment and the statistical analysis of the data.
Suppose that there are k factors, F1 , . . . , Fk , and that each factor
Fj has a set Lj = {vj,1 , . . . , vj,j }, of j possible levels (or values). A design point is an assignment of a level from Lj to Fj ,
for each factor j = 1, . . . , k. An experimental design is a collection of design points. When a design has N design points, it
can be represented by an N Ã— k array A = (ai,j ) in which each
row i corresponds to a design point and each column j to a factor;
the entry ai,j gives the level assigned to factor j in the ith design
point. When run, a design point results in one or more observable
responses.

Categories and Subject Descriptors
General and reference [Cross-computing tools and techniques]:
Experimentation; Mathematics of computing [Discrete mathematics]: Combinatorics

General Terms
Experimentation

A t-way interaction (or interaction of strength t) in A is a choice
of t columns i1 , . . . , it , and the selection of a level Î½ij âˆˆ Lij for
1 â‰¤ j â‰¤ t, represented as T = {(ij , Î½ij ) : 1 â‰¤ j â‰¤ t}. Every
 
design point in A covers kt interactions of strength t.

Keywords
Screening experiments, Locating arrays

1.

INTRODUCTION

Computer and networked systems are examples of complex engineered systems (CESs). The complexity of an engineered system is
not just due to its size, but also arises from its structure, operation
(including control and management), evolution over time, and that
people are involved in its design and operation [35].

When the objective of experimentation is screening, it is often recommended to keep the number of factors low. It has been considered impractical to experiment with â€œmanyâ€ factors; about ten
factors is a suggested maximum [23, 31]. Generally, two levels for
each factor is considered to work well in screening experiments.

Experimentation is often used to study the performance of CESs.
At its most basic, a system may be viewed as transforming some
input variables, or factors, into one or more observable output variables, or responses. Some factors of a system are controllable,
whereas others are not.

Methods for screening seek to reduce the number of design points
required because the exhaustive full-factorial design [9, 31] is too
large. For k factors each with two levels it has 2k design points.
An analysis of variance (ANOVA) allows the signiï¬cant factors
and interactions on the response to be identiï¬ed.

Objectives of experimentation include:

A fractional factorial design 2kâˆ’p
is a 21p fraction of a full factoR
rial design with k two-level factors. The design is described by p

Copyright is held by the authors.

31

tential to transform experimentation in huge factor spaces such as
those found in CESs.

generators, expressions of factors that are confounded; the generators determine the alias structure. A design is of resolution R if no
m-factor effect is aliased with another effect containing fewer than
R âˆ’ m factors.

The rest of this paper is organized as follows. Â§2 deï¬nes a locating
array, and gives an example of how a design is used for location.
Â§3 presents preliminary results applying an LA for screening the response of TCP throughput in a simulation model of a mobile wireless network. The full-factorial design for this system is infeasible
â€” it has over 1043 design points! Yet there is an LA with only 421
design points. We develop an algorithm using the LA to identify
the signiï¬cant factors and interactions from the data collected, providing a small example. In Â§4 we validate the signiï¬cance of the
identiï¬ed factors and interactions independently using the statistical software JMP. Finally, in Â§5 we summarize, discuss potential
threats to our approach, directions for this research, and conclude.

A D-optimal design is a popular experimental design among those
using optimality criteria. A model to ï¬t, and a bound N on the
number of design points, must be speciï¬ed a priori; this restricts
the factors to be analyzed to those in the model. The size of a Doptimal design is bounded by the size of a full-factorial design.
Some designs aggregate the factors into groups, e.g., sequential bifurcation [24], to improve design efï¬ciency. Grouping requires care
to ensure that factor effects do not cancel. This presents a â€œchicken
and eggâ€ problem: we need to know how to group in order to group.
Often, a domain expert is expected to make such grouping decisions. While such experts may have considerable knowledge, it
is doubtful whether an expert knows the importance of a speciï¬c
factor or interaction in a CES.

2.

An interaction graph depicts how a change in the level of one factor
affects the other factor with respect to a response. Figure 1 shows
an interaction graph for the factors of routing and medium access
control (MAC) protocol on average delay in a network. The choice
of MAC protocol (EDCF or IEEE 802.11) has little impact on the
average delay in the AODV routing protocol, while for the DSR
routing protocol the impact is very large; see [53]. If MAC protocols were aggregated, this signiï¬cant interaction would be lost.

LAs differ from standard designed experiments, which are used to
measure interactions and to develop a model for the response as a
function of these [31]. â€œSearch designsâ€ [17, 48, 49] also attempt
to locate interactions of higher strength, but their focus remains on
measurement and hence on balanced designs. Rao [20] shows that
the number of design points in a balanced design must be at least
as large as the number of interactions considered. Thus if t-way
interactions among k factors each having v levels are to be examined, balanced designs only reduce the v k exhaustive design points
to O(kt ). The selection of few factors from hundreds of candidates by this reduction is not viable. By lessening the requirement
from measurement to identiï¬cation, LAs are not subject to the Rao
bound.

Log10(Average delay)

-0.2
-0.3
-0.4
-0.5
-0.6
-0.7
-0.8
-0.9

LOCATING ARRAYS

Reducing the number of design points required relies on a sparsity
of effects assumption, that interactions of interest involve at most
a small, known number t of interacting factors. As one means of
reduction, we deï¬ne locating arrays (LAs) [8]. For a set of factors
each taking on a number of levels, an LA permits the identiï¬cation
of a small number of signiï¬cant interactions among small sets of
(factor, level) combinations.

EDCF
IEEE 802.11

Fortunately LAs behave more like covering arrays, experimental
designs in which every t-way interaction among factors appears in
at least one design point. Unlike designed experiments, the number
of design points in a covering array for k factors grows as a logarithmic function of k (see [43], for example). In [8], a construction
of LAs using covering arrays of higher strength is given, and hence
LAs also exhibit this logarithmic growth, making them asymptotically much more efï¬cient than balanced designs. This motivates
the consideration of covering arrays, which have been the subject
of extensive study [4, 5, 19, 36]. They are used in testing software
[10,13,25,26], hardware [46,50], composite materials [3], biological networks [44, 47], and others. Their use to facilitate location of
interactions is examined in [29, 56], and measurement in [21, 22].
Covering arrays form the basis for combinatorial methods to learn
an unknown classiï¬cation function using few evaluations â€” these
arise in computational learning and classiï¬cation, and hinge on locating the relevant attributes (factors) [11]. Algorithms for generating covering arrays range from greedy (e.g., [2, 16]) through
heuristic search (e.g., [38, 52]). However, combinatorial constructions (see [5]) provide the only available deterministic means of
producing covering arrays with more than a few hundred factors.

-1
AODV
DSR
Routing protocol

Figure 1: Interaction of routing and MAC protocols on delay [53].
A fractional factorial design is saturated when it investigates k =
N âˆ’ 1 factors in N design points [31]. In a supersaturated design,
the number of factors k > N âˆ’ 1; such designs contain more factors than design points. These designs are only able to estimate a
main effects model [27, 31]. Thus they cannot consider possible
interactions at all.
Even with substantial and detailed domain knowledge, it is imperative not to eliminate or aggregate factors a priori. Our goal,
therefore, is an automatic and objective approach to screening. To
address this problem we have formulated the deï¬nition of a locating array (LA) [8]. Locating arrays exhibit logarithmic growth in
the number of factors because their focus is on identiï¬cation rather
than on measurement. This makes practical the consideration of an
order of magnitude more factors in experimentation, removing the
need for the elimination of factors. As a result, LAs have the po-

A design point, when run, yields one or more responses. For ease
of exposition, we classify the responses in two groups, those that

32

exceed a speciï¬ed threshold and those that do not. So we suppose
that the outcome of a run of a design point is a single binary response (â€œpassâ€ or â€œfailâ€). A fault is caused by one or more t-way
interactions, and is evidenced by a run failing.

uous, we can select a threshold on the responses so as to limit the
number of design points yielding a â€œfailâ€ outcome to locate those
that make the most substantial contribution to the response. We
exploit this fact later in Â§3.2.

Given an experimental design and the set of interactions that cause
faults, the outcomes can be easily calculated: A run fails exactly
when it contains one or more of the faulty interactions, and does
not fail otherwise. In order to observe a fault, the interaction must
be covered by at least one design point. With no restriction on the
interactions that can cause faults, every interaction
 must be covered. Then the best one can do is to form all kj=1 j possible
design points, the exhaustive design. Using sparsity of effects, an
upper bound t is placed on the strength of interactions that may be
faulty. Then we require that every t-way interaction be covered; in
other words, the design is a covering array of strength t.

2.1

A Small Example

An example is provided to demonstrate fault location, and show
the limitations of covering arrays for this purpose. Suppose that
we use the experimental design for ï¬ve binaryfactors
in Table 1.

It is a covering array in which each of the 22 52 = 40 two-way
interactions is covered. A response for each design point run is
listed in the adjacent column.
Table 1: Experimental design and response for each run.

Let A = (ai,j ) be an experimental design, an N Ã— k array where in
each row i, levels in the jth column are chosen from a set Lj of size
j . For array A and t-way interaction T = {(ij , Î½ij ) : 1 â‰¤ j â‰¤ t},
deï¬ne Ï(A, T ) = {r : ar,ij = Î½ij , 1 â‰¤ j â‰¤ t} as the set of
rows of A in which T is covered. For a set T of interactions,
Ï(A, T ) = âˆªT âˆˆT Ï(A, T ). Locating faults requires that T be
recovered from Ï(A, T ), whenever T is a possible set of faults.

Design Points

1
2
3
4
5
6

1
0
1
0
1
0
1

2
1
0
1
0
0
1

Factors
3 4
1 1
1 0
0 0
0 1
0 0
0 1

5
1
0
0
1
1
0

Response
Fail
Pass
Fail
Pass
Pass
Pass

First, let us locate faults due to main effects (i.e., the individual factors or one-way interactions). The second design point run passes,
so all (factor, level) pairs in it are known not to be faulty. Therefore
in Table 2(a), that considers only the second design point, when
factor 1 is set to one, the run is not faulty. Similarly, for factors
2, 3, 4, and 5 set to zero, one, zero, and zero, respectively. This
is indicated by a check-mark () in the table. Repeating to check
coverage of each one-way interaction for each successful run, no
single (factor, level) error accounts for the faults; see Table 2(b).

Let It be the set of all t-way interactions for an array, and let It
be the set of all interactions of strength at most t. Consider an
interaction T âˆˆ It of strength less than t. Any interaction T 
of strength t that contains T necessarily has Ï(A, T  ) âŠ† Ï(A, T ).
In this case, when T is faulty we are unable to determine whether
or not T  is also faulty. Call a subset T  of interactions in It
independent if there do not exist T, T  âˆˆ T  with T âŠ† T  . In
general, some interactions in It (or perhaps It ) are believed to
be faulty, but their number and identity are unknown. The faulty
interactions cannot be identiï¬ed precisely from the outcomes, even
if the full factorial design is employed, without some restriction
on their number. (Consider the situation in which every design
point run fails.) We therefore suppose that a maximum number d
of faulty interactions is speciï¬ed.

Table 2: Locating faults due to main effects.
(a) Run 2
Factors 0
1
1

2

3

4

5


D EFINITION 2.1 ( [8]). An array A is (d, t)-locating if whenever T1 , T2 âŠ† It and T1 âˆª T2 is independent, |T1 | â‰¤ d, and
|T2 | â‰¤ d, it holds that Ï(A, T1 ) = Ï(A, T2 ) â‡” T1 = T2 .

(b) All Runs
Factors 0
1
1
 
2
 
3
 
4
 
5
 

Computing Ï(T ) for every one-way interaction, we obtain the sets
in Table 3. Because no two sets are equal, the array is (1, 1)locating and when there is a single faulty one-way interaction it can
be located. However, because {1, 3, 5} âˆª {2, 3, 5} = {1, 3, 5} âˆª
{1, 2}, when rows 1, 3, and 5 fail and 2, 4, and 6 pass, we cannot determine the two faulty interactions â€” the array is not (2, 1)locating.

If there is any set of d interactions of strength t that produce exactly the outcomes obtained when using a (d, t)-locating array A
to conduct experiments, then there is exactly one such set of interactions. To avoid enumeration of all sets of d interactions of
strength t, one can employ a stronger condition that for every interaction T of strength at most T and every set T1 âŠ† It that does
not contain T and for which T1 âˆª {T } is independent, it holds that
Ï(A, T ) = Ï(A, T1 ) â‡” T âˆˆ T1 . A locating array meeting this
stronger condition is termed a detecting array in [8]. When using
a detecting array, if there are at most d independent faulty interactions each of strength at most t, they are characterized precisely
as the interactions that appear in no run that passes. We typically
employ the term locating array to refer to both, but for reasons of
computational efï¬ciency the locating arrays that we use are, in fact,
detecting arrays.

Table 3: Ï(T ) for one-way interactions T = {(c, Î½)}.
Î½â†“câ†’
0
1

1
{1,3,5}
{2,4,6}

2
{2,4,5}
{1,3,6}

3
{3,4,5,6}
{1,2}

4
{2,3,5}
{1,4,6}

5
{2,3,6}
{1,4,6}

Now, let us try to locate faults due to two-way interactions. Because the second design point run passes, all two-way interactions
in it are known not to be faulty; Table 4(a) records the results. Repeating to check for coverage of each two-way interaction for each
successful run, those interactions not found to pass in this way in

In practice, one does not know a priori how many interactions are
faulty, or their strengths. Nevertheless, when responses are contin-

33

Table 4: Locating faults due to two-way interactions.

Factors
1, 2
1, 3
1, 4
1, 5
2, 3
2, 4
2, 5
3, 4
3, 5
4, 5

(a) Run 2
00 01 10














11


Factors
1, 2
1, 3
1, 4
1, 5
2, 3
2, 4
2, 5
3, 4
3, 5
4, 5

(b) All Runs
00 01 10






 
  
 
  
  
  
  

of levels for each factor to the desired number, eliminating rows in
the process and forming an array C with 143 design points. The
resulting array provides coverage of two-way interactions but does
not support location. When T and T  are interactions, to distinguish them we require that Ï(T ) = Ï(T  ), but we ask for more,
namely that |Ï(T ) \ Ï(T  )| â‰¥ 2 and |Ï(T  ) \ Ï(T )| â‰¥ 2; this ensures that for every two interactions of interest, there are at least
two design points containing one but not the other. To accomplish
this, we formed three copies of C, randomly permuted their symbols within each column, and formed their union (so that every
two-way interaction is covered at least three times). The resulting
array B with 429 rows turned out to be (1, 2)-detecting. Three rows
were selected by a greedy method to ensure the stronger condition
that |Ï(T ) \ Ï(T  )| â‰¥ 2 for every pair T, T  of interactions; then
eleven rows were deleted by a greedy algorithm to remove redundant rows, ultimately producing a design with 421 rows. Appendix
A gives a pointer to the locating array used as the experimental
design. Our objective was not to ï¬nd the smallest possible array,
because a fair evaluation of the efï¬cacy of locating arrays should
not rely on substantial additional structure being present.

11








Table 4(b) form a set of candidate faults. In this example, there are
nine interactions in the set of candidate faults. Now for the twoway interaction {(1, 0), (2, 1)}, Ï({(1, 0), (2, 1)}) = {1, 3}, and
it is the only two-way interaction for which this holds; and, no oneway interaction T has Ï(T ) = {1, 3}. Hence if there is a single
fault, it must be {(1, 0), (2, 1)}, and we have located the fault.

Ten replicates of each design point in the LA are run in ns-2;
for each a response of TCP throughput is measured. These are
averaged for each design point resulting in a vector with 421 entries
of observed average TCP throughput obsT h.

Our success for one response is not sufï¬cient, however. Because
Ï({(1, 0), (2, 1)}) = {1} = Ï({(2, 1), (3, 1)}), if only run 1 fails,
there are at least two equally plausible explanations using only a
single two-way interaction. Indeed A is not (1, 2)-locating. Thus
the ability to locate is more than simply coverage!

3.

3.2

SCREENING AN ENGINEERED SYSTEM

We now apply locating arrays for screening in a complex engineered system. One example of a CES for which it has been particularly difï¬cult to develop models is a mobile ad hoc network
(MANET). A MANET is a collection of mobile wireless nodes that
self-organize without the use of any ï¬xed infrastructure or centralized control. We seek to use a locating array to screen for the inï¬‚uential factors and interactions on average transport control protocol
(TCP) throughput in a simulation model of a MANET.

3.1

Screening Algorithm

We describe an algorithm for screening at a high level to facilitate
understanding. In each iteration of the algorithm the most signiï¬cant main effect or two-way interaction is identiï¬ed. These terms
are accumulated in a screening model of average TCP throughput. However, this screening model is not intended as a predictive
model; the quality of its current estimate allows the algorithm to
select the next most signiï¬cant term. The screening model is used
only to identify inï¬‚uential main effects and two-way interactions.
With its output, a predictive model can be built; see Â§4.
Initially, the screening model has no terms. With no other information, it should estimate the average TCP throughput to be the
average of the vector of observed average throughput. This is unlikely to be a very good estimation!

Designing the Experiment

We use the ns-2 simulator [37], version 2.34, for our experimentation. Since our response of interest is average TCP throughput,
we select the ï¬le transfer protocol (FTP) as our application because
it uses TCP for reliability. We select the internet protocol (IP), the
Ad hoc On-demand Distance Vector routing protocol (AODV) [42],
and IEEE 802.11b direct sequence spread spectrum (DSSS) as protocols at the network, data link, and physical layers of the protocol
stack. We also use the mobility, energy, error, and propagation
models in ns-2. From these protocols and models we identify
75 controllable factors. The region of interest for each factor, i.e.,
the range over which the factor is varied, ranges from two to ten
levels, with some set according to recommendations in [33]. See
Appendix A for a pointer to details of the factors and their levels.

Our strategy to identify the most signiï¬cant factor or interaction
as the term to add to the screening model is as follows. Suppose
that factor Fj , 1 â‰¤ k â‰¤ 75, has j levels Lj = {vj,1 , . . . vj,j }.
For each level , 1 â‰¤  â‰¤ j , of factor Fj iterate through each
of the 421 design points of the locating array A. For each design
point i, 1 â‰¤ i â‰¤ 421, partition the contribution of the (factor Fj ,
level vj, ) combination into one of two sets: S or S. If the design
point has the factor Fj set to level , i.e., ai,j = vj, , then add
the throughput measured for design point i, obsT h[i], to S; otherwise add obsT h[i] to S. Then, compute the (absolute) difference
of the average of sets S and S. (Of course, metrics other than the
difference of averages could be used.) Either the difference is zero
(i.e., the average TCP throughput collected in the sets S and S is
the same), or it is non-zero. If the difference is non-zero, then one
possible explanation is that the (factor Fj , level vj, ) combination
is responsible for the difference.

The full-factorial design for this factor space is infeasible; it has
over 1043 design points! In contrast, the locating array constructed
and checked manually has only 421 design points. Except for small
locating arrays [51], no general construction methods have been
published. We adopted a heuristic approach to construct the LA.

Our hypothesis is that the (factor Fj , level vj, ) combination over
all combinations for which the difference between the sets is the
greatest is the most signiï¬cant one. If this is correct, then a term
of the form c Â· (Fj , vj, ) is added to the screening model. The

Initially we selected a covering array with 75 factors and 10 levels
per factor, constructed using a standard product construction [7].
We applied a post-optimization method [34] to reduce the number

34

coefï¬cient c is equal to the difference in average TCP throughput
of each set. When this term is added to the screening model, it
makes the same estimation for average TCP throughput for sets S
and S.

design point with this initial ï¬tted value.
Now, we iterate over each (factor,level) combination. Factor
1 is set

to its low level in design points 1â€“4. Therefore S = 14 41 resT h[i] =

âˆ’134347
= âˆ’33586 and S = 14 85 resT h[i] = 134344
= 33586.
4
4
The absolute difference, |S âˆ’ S| = |-33586 âˆ’ 33586| = 67172.

In the ï¬rst iteration of this algorithm, the estimate (i.e., the average
of the vector of observed average TCP throughput) is used to determine deviations from each entry in the vector obsT h. We now
have a screening model that apparently includes the most signiï¬cant factor. It is now used to produce a new estimate of average
TCP throughput and update the vector of residual throughput. The
algorithm can be applied repeatedly to the residuals to identify the
next most important factor or interaction.

Repeating for each (factor, level) combination, as well as all twoway interactions, we ï¬nd that it is a main effect that has highest
absolute difference with a value of 131255. It occurs when factor
3 is set to its lowest level, namely when the number of ï¬‚ows at
the application layer is only one. Hence we attribute this as the
explanation for the largest difference and add the term c Â· (F3 , v3,0 )
to the model. The method of ordinary least squares (OLS) is used
to ï¬t the intercept and coefï¬cient c of the new term. This results
in an updated model of T = 12410 + 131255 Â· (F3 , v3,0 ). Its
coefï¬cient of determination is R2 = 0.33.

While this algorithm is described for (factor, level) combinations,
we actually iterate over all one-way (i.e., all (factor, level) combinations) and all two-way interactions (i.e., all pairs of (factor, level)
combinations) to identify the main effect or two-way interaction of
highest signiï¬cance. Any number of stopping conditions may be
used to decide when to terminate the model development. We use
the R2 , the coefï¬cient of determination, indicating how well data
ï¬ts a line or curve; when it shows marginal improvement, we stop.

Using this updated model, the residuals can be recomputed as input
to the next iteration of the algorithm.
Next, we describe some of the obstacles arising in the practical
application of the screening algorithm.

The locating array constructed for our CES is a (d = 1, t = 2)locating array, meaning it only guarantees to be able to locate (identify) at most one (d = 1) main effect or two-way (i.e., up to t = 2way) interaction. It is interesting that the LA may be used iteratively to identify subsequent signiï¬cant main effects or interactions. In this sense, the algorithm uses a â€œheavy-hittersâ€ approach
as in compressive sensing [6].

Applying the Screening Algorithm

In applying the screening algorithm to our CES, several obstacles
arose. The ï¬rst is that the measured average TCP throughput is not
normally distributed, as Figure 2 shows; this is not uncommon in
systems experimentation [12]. The best transformation of the data
is a natural logarithm (Figure 3a). From the normal probability plot
(Figure 3b), we ï¬nd that the transformed data are still not normally
distributed; nevertheless, we work with this transformation of the
data.

Example of the Screening Algorithm

A small example is provided to step through one iteration of the
screening algorithm. Suppose that we use the experimental design for four binary factors in Table 5. It is a covering array of
strength three and therefore also a (2, 1)-detecting array. Factor 1
corresponds to the distribution function used for introducing errors
(uniformly or exponentially distributed), factor 2 to the error rate
(10âˆ’7 or 10âˆ’5 ), factor 3 to the number of ï¬‚ows at the application
layer (1 or 18), and factor 4 to the TCP packet size (64 or 2048);
the levels are taken as â€œbinaryâ€ for this example. All remaining factors are set to their default levels for experimentation. A response
of observed TCP throughput for each design point, averaged over
ten replicates, is listed in the column obsT h. (All measures are
truncated to integers for simplicity.)

72%

0.985

300

0.94
250

0.88
Normal Probability

3.3

3.4

Frequency

200

150

0.75

0.5

0.25
0.12

100

0.06
18%

0.015

50
5%
2%

0

50,000

0.003
1%

100,000
obsTH1

1%

0%

150,000

0%

0%

200,000

(a) Throughput distribution.

0%

0

50,000

100,000

150,000

200,000

250,000

250,00

obsTH1

(b) Normal probability plot.

Design Points

Table 5: Experimental design and average TCP throughput.

1
2
3
4
5
6
7
8

1
0
0
0
0
1
1
1
1

Factors
2 3
0 0
0 1
1 0
1 1
0 0
0 1
1 0
1 1

Figure 2: Distribution of the original observed average throughput,
and corresponding normal probability plot.
4
0
1
1
0
1
0
0
1

obsT h
63339
29860
80801
3804
373866
3879
56656
12095

resT h
-14699
-48178
2764
-74234
295828
-74159
-21382
-65943

A much larger problem arises from the fact that the LA does not
cover each main effect and two-way interaction the same number
of times. Indeed, binary factors are covered much more frequently
(some as many as two hundred times in the 421 row LA) compared
to two-way interactions of factors with ten levels (only a handful
of times). This is unavoidable when one-way and two-way interactions are compared, and when factors have a different numbers of
levels.

The overall mean of the obsT h is 78038. Therefore, the screening
model initially estimates this value for average TCP throughput,
i.e., T = 78038. The residuals (resT h) are computed in Table 5 by
taking the difference of the observed average throughput for each

Consider the behaviour of the screening algorithm. For a binary
factor the sets S and S have the same or nearly the same size and,
as a result, the average of each set has small variance. In the example in Â§3.3, each (factor, level) combination is covered four times

35

0.985

26%

0.985

24%

100

100

0.94

0.94
21%

0.88

0.88
80

Normal Probability

14%

18%

0.75
Frequency

60

Normal Probability

Frequency

80

0.5

60
13%
12%

0.25
40

40

0.12

8%

0.75

0.5

0.25
0.12

9%

8%
7%

0.06
5%

20

5%

20

0.015

4%

1%

1%

1%

4

6
8
ln(obsTH1)

10

(a) ln transformation.

0.015

0.003

1%

2
2

0.06

6%

6%
5%

4

6

8

10

1%

1%

0%

12

ln(obsTH1

-4
-2
0
2
Residuals of TCP throughput after iteration 1

-4

-2

0

2

4

4

(a) Distribution of ï¬rst residual.

(b) Normal probability plot.

0.003
-6

-6

12

Residuals of TCP throughput after iteration 1

(b) Normal probability plot.

Figure 3: Natural logarithm transformation of the original observed
throughput, and corresponding normal probability plot.

Figure 4: Distribution of residuals after the ï¬rst iteration of the
screening algorithm, and corresponding normal probability plot.

(each column of the array has four zeros and four ones). However
in general, as the number of levels for a factor increases, the size of
the sets S and S may become markedly different, and the variance
of the average of each set may increase greatly. Returning to the
example in Â§3.3, the two-way interactions are not covered equally.
Consider the two-way interaction {(1, 0), (2, 0)}. It is covered in
only two rows of the array, namely |Ï({(1, 0), (2, 0)})| = |{1, 2}| =
2 (this is true for all two-way interactions in this example). Even
in this small array, the coverage of two-way interactions is unbalanced resulting in S accumulating two values and S accumulating
six values. This makes any direct comparison among (factor, level)
combinations and/or two-way interactions impossible.

intercept and Î²i is the coefï¬cient of term i, 1 â‰¤ i â‰¤ 12.
Table 6: Screening model with twelve terms.

To address this problem, factors are grouped according to the number of times each level is covered in the LA; see Appendix A for
a pointer to the details on how groups are formed. Now, in each
iteration of the screening algorithm, the ï¬rst step is to select the
most signiï¬cant factor or interaction from each group. Then from
these candidates, the most signiï¬cant factor or interaction overall
is selected.
The Figure 4 shows the graphical tests for normality of the residuals after the ï¬rst iteration of the screening algorithm. (Similar behaviour of the residuals is observed after each iteration.) While the
ï¬gures indicate that the residuals are close to normally distributed,
we check using the non-parametric Shapiro-Wilk test. This test indicates that the residuals are still not normally distributed. Hence,
we use the Wilcoxon rank sum test and the Mann-Whitney U test [14, 28, 54] to select the most signiï¬cant factor or two-way
interaction within each group. Then, to select the most signiï¬cant factor or interaction over all groups, the Akaike information
criterion (AICC ) [1] is used.

Î²i
5.6
4.4
4.0
-4.7

-11.8
-12.1
-9.3
6.5

-1.6
-1.5
-1.2
0.9

6.6
8.4

0.7
1.1

6.3

1.1

5.5
5.2

0.7
0.5

Factor or interaction, and level(s)
ErrorModel_ranvar_ U nif orm
ErrorModel_unit_ pkt)
(ErrorModel_ranvar_ U nif orm) *
(ErrorModel_unit_ pkt)
TCP_packetSize_ 64
MAC_RTSThreshold_ 0
TCP_packetSize_ 128
(TCP_RTTvar_exp_ 2) *
(TCP_min_max_RTO_ 0.1)
TCP_min_max_RTO_ 0.2
(ErrorModel_unit_ pkt ) *
(ErrorModel_rate_ 1.0E-07)
(ErrorModel_ranvar_ U nif orm) *
(MAC_RTSThreshold_ 0)
APP_flows_ 1
RWP_Area_ 8

The ï¬rst notable observation about this screening model is that it
contains both main effects and two-way interactions. Moreover, it
contains factors from across the layers of the protocol stack (application, transport, and MAC) and not just the transport layer; in
addition, it includes factors from the error model and the mobility model. Aside from these differences with other models of TCP
throughput (such as [15, 18, 30, 39â€“41, 55, 57, 58]), the screening
model includes not just which factors or two-way interactions are
signiï¬cant, but the level at which each is signiï¬cant.

We still need to ï¬t the intercept and the coefï¬cients of the terms.
For a linear model with the assumptions of expected error of zero
and expected variance in the error to be equal, the method of ordinary least squares (OLS) is used. However, if the expected variance
in the error is unequal, OLS is no longer appropriate [32]. In this
case, the method of weighted least squares (WLS) is used to ï¬t the
intercept and coefï¬cients of the terms in the screening model.

3.4.1

t-Test
52.6
34.5
32.8
-29.1

From the statistical point of view, Table 7 shows a strong correlation among the regressors and the response of average TCP
throughput. The F statistic indicates that the model is signiï¬cant
to the response.
Table 7: Summary statistics of the screening model in Table 6.
R2 and Adjusted R2 : 0.84
Standard deviation: 0.92
F statistic: 180.6 on 12 and 408 df, p-value < 7.89e-155

The Resulting Screening Model

Table 6 gives the screening model for average TCP throughput developed in twelve iterations of the screening algorithm; Table 8 lists
its unique factors. A Studentâ€™s t-test was run on each term in the
screening model and each was found to be signiï¬cant; Î²0 is the

We are encouraged by the factors and interactions identiï¬ed. This
includes how and into what unit errors are introduced (using a uni-

36

Table 9: Partial results of a 29 full-factorial screening experiment
using JMP 11.0 on the nine factors in Table 8.

form distribution into packets rather than bit errors), and their interaction. Smaller sized packets (64 and 128 bytes) tend to reduce
throughput. When RTS/CTS is always on (i.e., the threshold is
zero bytes), there is a negative impact on throughput compared to
when it is conï¬gured to 1500 or 3000 bytes (always off). The retransmission timeout (RTO) and round trip time (RTT) are part of
TCPâ€™s congestion control mechanism; the RTO infers packet loss
by observing duplicate acknowledgements and the RTT is related
to the propagation delay. The RTO is signiï¬cant by itself, and in
its interaction with the RTT as they work to correct and prevent
network congestion. The synthetic error model of the simulator
drops packets comparing them with data from an uniform distribution at a steady-state loss event rate of 1.0E-07; this is the lowest
error rate used and naturally it corresponds with higher throughput. Smaller simulation areas also result in higher throughput; a
larger area has longer average shortest-hop path lengths and average higher network partition rates both of which negatively affect
throughput. The throughput response is higher with fewer ï¬‚ows
because increasing the number of ï¬‚ows not only may overload the
network but more ï¬‚ows are more challenging to route in a MANET.

4.

Term
ErrorModel_ranvar_*ErrorModel_unit_
ErrorModel_ranvar_
ErrorModel_unit_
TCP_packetSize_
APP_ï¬‚ows_
TCP_min_max_RTO_
RWP_Area_
MAC_RTSThreshold_
ErrorModel_unit_*TCP_packetSize_
ErrorModel_rate_
ErrorModel_ranvar_*MAC_RTSThreshold_
APP_ï¬‚ows_*RWP_Area_
ErrorModel_unit_*ErrorModel_rate_
TCP_packetSize_*ErrorModel_rate_
ErrorModel_unit_*MAC_RTSThreshold_
ErrorModel_ranvar_*APP_ï¬‚ows_
APP_ï¬‚ows_*TCP_min_max_RTO_
ErrorModel_unit_*APP_ï¬‚ows_
ErrorModel_ranvar_*TCP_min_max_RTO_
ErrorModel_ranvar_*TCP_packetSize_
TCP_packetSize_*APP_ï¬‚ows_
TCP_min_max_RTO_*RWP_Area_
ErrorModel_ranvar_*RWP_Area_
MAC_RTSThreshold_*ErrorModel_rate_
TCP_min_max_RTO_*ErrorModel_rate_
TCP_min_max_RTO_*TCP_rttvar_exp_
ErrorModel_unit_*TCP_min_max_RTO_
APP_ï¬‚ows_*ErrorModel_rate_
RWP_Area_*MAC_RTSThreshold_
ErrorModel_unit_*RWP_Area_
TCP_rttvar_exp_
TCP_packetSize_*RWP_Area_
APP_ï¬‚ows_*MAC_RTSThreshold_
RWP_Area_*ErrorModel_rate_
ErrorModel_ranvar_*TCP_rttvar_exp_

VALIDATION AND VERIFICATION

From the 75 controllable factors used in experimentation, nine unique
factors are present in the twelve terms in the screening model in Table 6; these are listed in Table 8.
Table 8: Unique factors in the screening model in Table 6.
Level
Factor
TCP_RTTvar_exp_
ErrorModel_ranvar_
ErrorModel_unit_
MAC_RTSThreshold_
ErrorModel_rate_
RWP_Area_
TCP_min_max_RTO_
APP_flows_
TCP_packetSize_

Minimum
2
U nif orm
pkt
0
1.0E-07
8
0.1
1
64

Maximum
4
Exponential
bit
3000
1.0E-05
40
40
18
2048

In order to validate the factors and interactions identiï¬ed, we ï¬rst
conduct a full-factorial experiment for these nine factors using the
extremes of their region of interest, using the statistical software
JMP to analyze the results. From this, we produce a predictive
model of average TCP throughput. We then examine the quality
of this predictive model by comparing how it performs on random
design points (i.e., a design point in which the level of each factor
is selected at random).

in Table 6. Indeed, both models have the same four most signiï¬cant
terms (though in a different order), and all factors and interactions
in Table 6 are a subset of the terms in Table 9. Appendix A gives
a pointer to the details of the predictive model for average TCP
throughput that was ï¬t using a subset of the signiï¬cant terms in
Table 9.
Figure 5 shows the results of evaluating the JMP predictive model
as a function of the TCP packet size, for the three levels of error
rate. As in the experimentation, all remaining factors are ï¬xed at
their default levels. As expected, the results show that the highest
TCP throughput is achieved when the error rate is at the lowest level
(1.0E-07). For a given error rate the TCP throughput increases as
a function of packet size, after which it decreases. An exception
is for packet size 1024. Aside from this exception, these results
also conï¬rm our intuition of TCP throughput behaviour. The reason for this exception deserves further study but may be related to
the default settings used for the other 66 factors not varied in this
screening experiment.

We present our validation results next.

4.1

p-Value
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
0.0001
0.0001
0.0003
0.0006
0.001
0.0012
0.002
0.0116
0.0444
0.0515

Full-Factorial Screening in JMP

We conduct an independent 29 full-factorial experiment on the nine
factors in Table 8. All remaining 75 âˆ’ 9 = 66 factors are ï¬xed to
their default levels. Ten replicates of each of the 29 design points is
run, and TCP throughput measured. The results of the experimentation are input to the JMP statistical software, version 11.0 [45].
The results from the full-factorial screening experiment are given in
Table 9. It includes only the main effects and two-way interactions
sorted in increasing order by the p-value. The results indicate high
commonality with the main effects and two-factor interactions selected by the screening algorithm that formed the screening model

We now examine the predictive accuracy of the JMP model for random design points.

37

120000

TCP throughput (bps)

factors and two-way interactions, the screening model developed
also reï¬‚ects the actual behaviour well.

JMP 1.0e-7
JMP 1.0e-6
JMP 1.0e-5

100000

80000

Despite this, the method aims only to deal with many factors and
their interactions to identify the signiï¬cant ones. We advocate that
further experimentation is necessary after the screening is completed, both to conï¬rm the screening results and to build a predictive model. One must be cautious not to over-ï¬t the experimental results and claim unwarranted conï¬dence; conï¬rmation is
needed. This is particularly a concern if the stopping criterion chosen locates too many or too few signiï¬cant interactions; while our
choice of R2 appears to have worked well, future effort should address the impact of different stopping criteria. A second concern is
the selection criterion for the next factor or interaction to include.
Subsequent selections depend upon selections already made, so our
method could in principle be misdirected by a bad selection. Our
criterion of using the differences between responses for S and those
for S has also worked well, but we cannot be certain that such a
simple selection sufï¬ces in general. Finally, we have employed
only a few locating arrays; while they have worked well in our
analyses, constructing a suitable locating array remains a challenging problem that merits further research.

60000

40000

20000

0
64 128

256

512

768

1024

1280

1536

1792

2048

Packet size (bytes)

Figure 5: TCP throughput as a function of packet size as predicted
the by JMP model; all other factors are at their default levels.

4.2

Predictive Accuracy of JMP Model

In order to test the predictive accuracy of the JMP model, a new
experimental design of one hundred random design points is constructed. In constructing each design point, for each of factor Fj ,
1 â‰¤ j â‰¤ 75, a random level from Lj is selected. New mobility
scenarios are also generated. Ten replicates of each of the random
design points are run in the ns-2 simulator, and the TCP throughput measured. In addition, for each experiment in the design, the
JMP model is evaluated generating a new data set of ï¬tted TCP
throughput.

Certainly further experimentation is needed to assess the merit of
screening using LAs, in particular on physical not just simulated
complex engineered systems, and draw ï¬rm conclusions. What we
can conclude is that in a challenging CES arising from a MANET,
screening using locating arrays is viable and yields useful models.

Figure 6 shows the average TCP throughput from simulation, and
the ï¬tted throughput from the JMP model corresponding to this
random design. The mean TCP throughput from the simulations is
20,892 bps whereas the mean from the JMP model is lower, only
13,946 bps. However, the standard deviation of the results from the
JMP model is smaller than the standard deviation from the simulations. Both models exhibit a few outliers. Approximately 94%
of the results predicted for TCP throughput from the JMP model
are in one standard deviation of the simulation results. Considering
the size of the factor space, we conclude that the predicted average
TCP throughput of the JMP model is similar to the average TCP
throughput measured in simulation.

4.3

Acknowledgment
Thanks to Doug Montgomery for his advice on all things statistical.
This material is based in part upon work supported by the National
Science Foundation under Grant No. 1421058.

6.

Predictive Accuracy of Screening Model

While the model developed in applying the screening algorithm
based on the LA (Table 6) is not intended to be used as a predictive
model, we were curious about its predictive accuracy. Appendix
A gives a pointer to a summary of results similar to those in this
section for the screening model. To our surprise, the predictive accuracy of the screening model is reasonably good. The screening
model does appear to have more variability than the model developed in JMP.

5.

REFERENCES

[1] H. Akaike. A new look at the statistical model identiï¬cation.
IEEE Transactions on Automatic Control, 19(6):716â€“723,
1974.
[2] R. C. Bryce and C. J. Colbourn. A density-based greedy
algorithm for higher strength covering arrays. Software
Testing, Veriï¬cation, and Reliability, 19:37â€“53, 2009.
[3] J. N. Cawse. Experimental design for combinatorial and high
throughput materials development. GE Global Research
Technical Report, 29(9):769â€“781, 2002.
[4] C. J. Colbourn. Combinatorial aspects of covering arrays. Le
Matematiche (Catania), 58:121â€“167, 2004.
[5] C. J. Colbourn. Covering arrays and hash families. In
Information Security and Related Combinatorics, NATO
Peace and Information Security, pages 99â€“136. IOS Press,
2011.
[6] C. J. Colbourn, D. Horsley, and V. R. Syrotiuk. Frameproof
codes and compressive sensing. In Proc. 48th Annual
Allerton Conference on Communication, Control, and
Computing, 2010.
[7] C. J. Colbourn, S. S. Martirosyan, G. L. Mullen, D. E.
Shasha, G. B. Sherwood, and J. L. Yucas. Products of mixed
covering arrays of strength two. Journal of Combinatorial
Designs, 14(2):124â€“138, 2006.
[8] C. J. Colbourn and D. W. McClary. Locating and detecting
arrays for interaction faults. Journal of Combinatorial
Optimization, 15:17â€“48, 2008.
[9] C. Croarkin, P. Tobias, J. J. Filliben, B. Hembree,

CONCLUSIONS

Locating arrays capture the intuition that in order to see the effect of
a main effect or interaction, some design point must cover it; and in
order to distinguish it, the responses for the set of design points that
cover it must not be equally explained by another small set of main
effects or interactions. In a complex engineered system, many main
effects and interactions may be signiï¬cant, but our method identiï¬es them one at a time, iteratively improving a screening model. In
this way, an experimental design must be able to repeatedly locate
a single â€œmost signiï¬cantâ€ main effect or interaction. Our results
show that using locating arrays for screening appears promising.
Indeed while the screening targeted the identiï¬cation of signiï¬cant

38

160000
Fitted JMP model
Mean fitted JMP model

140000

TCP throughput (bps)

120000
100000
80000
60000
40000
+StDev Fitted JMP model

20000
0

-StDev Fitted JMP model

0

10

20

30

40

50

60

70

80

90

100

60

70

80

90

100

Random tests

(a) Predictions by JMP.
160000
Simulated
Mean Simulated

140000

TCP throughput (bps)

120000
100000
80000
60000
+StDev Simulated

40000
20000
0
-StDev Simulated

0

10

20

30

40

50
Random tests

(b) Simulation results.

Figure 6: Predictions by the JMP model and simulation results for random design points.

[10]

[11]

[12]

[13]

[14]

[15]

[16]

W. Guthrie, L. Trutna, and J. Prins, editors.
NIST/SEMATECH e-Handbook of Statistical Methods.
NIST/SEMATECH, 2012.
S. R. Dalal, A. J. N. Karunanithi, J. M. L. Leaton, G. C. P.
Patton, and B. M. Horowitz. Model-based testing in practice.
In Proc. Intl. Conf. on Software Engineering (ICSE â€™99),
pages 285â€“294, 1999.
P. Damaschke. Adaptive versus nonadaptive
attribute-efï¬cient learning. Machine Learning, 41:197â€“215,
2000.
A. B. de Oliveira, S. Fischmeister, A. Diwan, M. Hauswirth,
and P. F. Sweeney. Why you should care about quantile
regression. In Proc. of the ACM Conf. on Architectural
Support for Programming Languages and Operating Systems
(ASPLOS), March 2013.
S. Dunietz, W. K. Ehrlich, B. D. Szablak, C. L. Mallows, and
A. Iannino. Applying design of experiments to software
testing. In Proc. Intl. Conf. on Software Engineering (ICSE
â€™97), pages 205â€“215, Los Alamitos, CA, 1997. IEEE.
M. Fay and M. Proschan. Wilcoxon-Mann-Whitney or t-test?
On assumptions for hypothesis tests and multiple
interpretations of decision rules. Statistics Surveys, 4:1â€“39,
2010.
S. Floyd, M. Handley, J. Padhye, and J. Widmer.
Equation-based congestion control for unicast applications:
The extended version. SIGCOMM Computing
Communications Review, 30:43â€“56, 2000.
M. Forbes, J. Lawrence, Y. Lei, R. N. Kacker, and D. R.

[17]

[18]

[19]

[20]
[21]

[22]

[23]

[24]

39

Kuhn. Reï¬ning the in-parameter-order strategy for
constructing covering arrays. J. Res. Nat. Inst. Stand. Tech.,
113:287â€“297, 2008.
S. Ghosh and C. Burns. Comparison of four new general
classes of search designs. Austral. New Zealand J. Stat.,
44:357â€“366, 2002.
K.-J. Grinnemo and A. Brunstrom. A simulation based
performance analysis of a TCP extension for best-effort
multimedia applications. In Proceedings of the 35th Annual
Simulation Symposium, 2002.
A. Hartman. Software and hardware testing using
combinatorial covering suites. In M. C. Golumbic and
I. B.-A. Hartman, editors, Interdisciplinary Applications of
Graph Theory, Combinatorics, and Algorithms, pages
237â€“266. Springer, Norwell, MA, 2005.
A. S. Hedayat, N. J. A. Sloane, and J. Stufken. Orthogonal
Arrays. Springer-Verlag, New York, 1999.
D. S. Hoskins, C. J. Colbourn, and M. Kulahci. Truncated
D-optimal designs for screening experiments. American
Journal of Mathematical and Management Sciences,
28:359â€“383, 2008.
D. S. Hoskins, C. J. Colbourn, and D. C. Montgomery.
D-optimal designs with interaction coverage. Journal of
Statistical Theory and Practice, 3:817â€“830, 2009.
J. P. C. Kleijnen. An overview of the design and analysis of
simulation experiments for sensitivity analysis. European
Journal of Operational Research, 164:287â€“300, 2005.
J. P. C. Kleijnen, B. Bettonvil, and F. Persson. Screening for

[25]

[26]

[27]

[28]

[29]

[30]

[31]
[32]

[33]

[34]

[35]

[36]
[37]
[38]

[39]

[40]

[41]

[42]

[43]

the important factors in large discrete-even simulation
models: Sequential bifurcation and its applications. In A. M.
Dean and S. M. Lewis, editors, Screening: Methods for
Experimentation in Industry, Drug Discovery and Genetics,
chapter 13, pages 287â€“307. Springer-Verlag, 2006.
D. Kuhn and M. Reilly. An investigation of the applicability
of design of experiments to software testing. In Proc. 27th
Annual NASA Goddard/IEEE Software Engineering
Workshop, pages 91â€“95, Los Alamitos, CA, 2002. IEEE.
D. R. Kuhn, D. R. Wallace, and A. M. Gallo. Software fault
interactions and implications for software testing. IEEE
Trans. Software Engineering, 30(6):418â€“421, 2004.
R. Li and D. K. J. Lin. Analysis methods for supersaturated
designs: Some comparisons. Journal of Data Science, pages
249â€“260, 2003.
H. B. Mann and D. R. Whitney. On a test of whether one of
two random variables is stochastically larger than the other.
Annals of Mathematical Statistics, 18:50â€“60, 1947.
C. MartÃ­nez, L. Moura, D. Panario, and B. Stevens. Locating
errors using ELAs, covering arrays, and adaptive testing
algorithms. SIAM J. Discrete Math., 23:1776â€“1799, 2009/10.
M. Mathis, J. Semke, J. Mahdavi, and T. Ott. The
macroscopic behavior of the TCP congestion avoidance
algorithm. SIGCOMM Comput. Commun. Rev., 27:67â€“82,
1997.
D. C. Montgomery. Design and Analysis of Experiments.
John Wiley & Sons, Inc., 8 edition, 2012.
D. C. Montgomery, E. A. Peck, and C. G. Vining.
Introduction to Linear Regression Analysis. John Wiley &
Sons, Inc., 4th edition, 2006.
A. Munjal, T. Camp, and W. Navidi. Constructing rigorous
MANET simulation scenarios with realistic mobility. In
European Wireless Conference (EW), pages 817â€“824, 2010.
P. Nayeri, C. J. Colbourn, and G. Konjevod. Randomized
postoptimization of covering arrays. European Journal of
Combinatorics, 34:91â€“103, 2013.
Networking and information technology research and
development (NITRD) large scale networking (LSN)
workshop report on complex engineered networks, 2012.
C. Nie and H. Leung. A survey of combinatorial testing.
ACM Computing Surveys, 43, 2011.
The Network Simulator - ns-2.
http://www.isi.edu/nsnam/ns.
K. Nurmela. Upper bounds for covering arrays by tabu
search. Discrete Applied Mathematics, 138(9):143â€“152,
2004.
J. Padhye, V. Firoiu, D. Towsley, and J. Kurose. Modeling
TCP throughput: a simple model and its empirical validation.
SIGCOMM Computing Communications Review,
28:303â€“314, 1998.
J. Padhye, V. Firoiu, D. F. Towsley, and J. F. Kurose.
Modeling TCP Reno performance: A simple model and its
empirical validation. IEEE/ACM Transactions on
Networking, 8:133â€“145, 2000.
N. Parvez, A. Mahanti, and C. Williamson. An analytic
throughput model for TCP NewReno. IEEE/ACM
Transactions on Networking, 18:448â€“461, 2010.
C. E. Perkins and E. M. Royer. Ad hoc on-demand distance
vector routing. In Proc. Second IEEE Workshop on Mobile
Computing Systems and Applications, pages 90â€“100, 1999.
S. Poljak, A. Pultr, and V. RÃ¶dl. On qualitatively independent

[44]

[45]
[46]

[47]

[48]

[49]

[50]

[51]

[52]

[53]

[54]
[55]

[56]

[57]

[58]

partitions and related problems. Discrete Applied Math.,
6:193â€“205, 1983.
A. H. Ronneseth and C. J. Colbourn. Merging covering
arrays and compressing multiple sequence alignments.
Discrete Applied Mathematics, 157:2177â€“2190, 2009.
JMP statistical software from SAS.
http://www.jmp.com.
G. Seroussi and N. H. Bshouty. Vector sets for exhaustive
testing of logic circuits. IEEE Transactions on Information
Theory, 34:513â€“522, 1988.
D. E. Shasha, A. Y. Kouranov, L. V. Lejay, M. F. Chou, and
G. M. Coruzzi. Using combinatorial design to study
regulation by multiple input signals: A tool for parsimony in
the post-genomics era. Plant Physiology, 127:1590â€“1594,
2001.
T. Shirakura, T. Takahashi, and J. N. Srivastava. Searching
probabilities for nonzero effects in search designs for the
noisy case. Ann. Statist., 24:2560â€“2568, 1996.
J. N. Srivastava. Designs for searching non-negligible effects.
In J. N. Srivastava, editor, A Survey of Statistical Design and
Linear Models, pages 507â€“519. Northâ€“Holland, 1975.
D. T. Tang and C. L. Chen. Iterative exhaustive pattern
generation for logic testing. IBM Journal Research and
Development, 28:212â€“219, 1984.
Y. Tang, C. J. Colbourn, and J. Yin. Optimality and
constructions of locating arrays. J. Stat. Theory Pract.,
6(1):20â€“29, 2012.
J. Torres-Jimenez and E. Rodriguez-Tello. New upper
bounds for binary covering arrays using simulated annealing.
Information Sciences, 185:137â€“152, 2012.
K. K. Vadde and V. R. Syrotiuk. Factor interaction on service
delivery in mobile ad hoc networks. IEEE Journal on
Selected Areas in Communications, 22:1335â€“1346, 2004.
F. Wilcoxon. Individual comparisons by ranking methods.
Biometrics Bulletin, 1:80â€“83, 1945.
I. Yeom and A. L. N. Reddy. Modeling TCP behavior in a
differentiated services network. IEEE/ACM Transactions on
Networking, 9:31â€“46, 1999.
C. Yilmaz, M. B. Cohen, and A. Porter. Covering arrays for
efï¬cient fault characterization in complex conï¬guration
spaces. IEEE Transactions on Software Engineering,
31:20â€“34, 2006.
B. Zhou, C. P. Fu, D.-M. Chiu, C. T. Lau, and L. H. Ngoh. A
simple throughput model for TCP Reno. In Proceedings of
the IEEE International Communications Conference
(ICCâ€™06), 2006.
M. Zorzi, A. Chockalingam, and R. R. Rao. Throughput
analysis of TCP on channels with memory. IEEE Journal on
Selected Areas in Communications, 18:1289â€“1300, 2000.

APPENDIX
A. GITHUB REPOSITORY
A GitHub repository provides supplementary material at:
https://github.com/locatingarray/screening.git
Speciï¬cally, it includes the 75 controllable factors in the ns-2 simulator used in experimentation and their levels (Â§3.1), the 421 Ã— 75
LA used as the experimental design (Â§3.1), a description of how
factors are grouped (Â§3.4), the JMP model for TCP throughput,
along with some statistical analysis (Â§4.1), and some analysis of
the predictive capability of the screening model (Â§4.3).

40

Two-stage algorithms for covering array construction

arXiv:1606.06730v1 [cs.DS] 21 Jun 2016

Kaushik Sarkar and Charles J. Colbourn
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, PO Box 878809
Tempe, Arizona, 85287-8809, U.S.A.
June 23, 2016
Abstract
Modern software systems often consist of many different components, each with a number of options.
Although unit tests may reveal faulty options for individual components, functionally correct components
may interact in unforeseen ways to cause a fault. Covering arrays are used to test for interactions
among components systematically. A two-stage framework, providing a number of concrete algorithms,
is developed for the efficient construction of covering arrays. In the first stage, a time and memory
efficient randomized algorithm covers most of the interactions. In the second stage, a more sophisticated
search covers the remainder in relatively few tests. In this way, the storage limitations of the sophisticated
search algorithms are avoided; hence the range of the number of components for which the algorithm can
be applied is extended, without increasing the number of tests. Many of the framework instantiations
can be tuned to optimize a memory-quality trade-off, so that fewer tests can be achieved using more
memory. The algorithms developed outperform the currently best known methods when the number
of components ranges from 20 to 60, the number of options for each ranges from 3 to 6, and t-way
interactions are covered for t âˆˆ {5, 6}. In some cases a reduction in the number of tests by more than
50% is achieved.

Keywords: Covering array, Software interaction testing, Combinatorial construction algorithm

1

Introduction

Real world software and engineered systems are composed of many different components, each with a number
of options, that are required to work together in a variety of circumstances. Components are factors, and
options for a component form the levels of its factor. Although each level for an individual factor can be tested
in isolation, faults in deployed software can arise from interactions among levels of different factors. When
an interaction involves levels of t different factors, it is a t-way interaction. Testing for faults caused by t-way
interactions for every t is generally infeasible, as a result of a combinatorial explosion. However, empirical
research on real world software systems indicates that testing all possible 2-way (or 3-way) interactions
would detect 70% (or 90%) of all faults [25]. Moreover, testing all possible 6-way interactions is sufficient
for detection of 100% of all faults in the systems examined in [25]. Testing all possible t-way interactions for
some 2 â‰¤ t â‰¤ 6 is pseudo-exhaustive testing [24], and is accomplished with a combinatorial array known as
a covering array.
Formally, let N, t, k, and v be integers with k â‰¥ t â‰¥ 2 and v â‰¥ 2. A covering array CA(N ; t, k, v) is an
N Ã— k array A in which each entry is from a v-ary alphabet Î£, and for every N Ã— t sub-array B of A and
every x âˆˆ Î£t , there is a row of B that equals x. Then t is the strength of the covering array, k is the number
of factors, and v is the number of levels.
When k is a positive integer, [k] denotes the set {1, . . . , k}. A t-way interaction is {(ci , ai ) : 1 â‰¤ i â‰¤
t, ci âˆˆ [k], ci 6= cj for i 6= j, and ai âˆˆ Î£}. So an interaction is an assignment of levels from Î£ to t of the k

factors. It,k,v denotes the set of all kt v t interactions for given t, k and v. An N Ã— k array A covers the
1

interaction Î¹ = {(ci , ai ) : 1 â‰¤ i â‰¤ t, ci âˆˆ [k], ci 6= cj for i 6= j, and ai âˆˆ Î£} if there is a row r in A such that
A(r, ci ) = ai for 1 â‰¤ i â‰¤ t. When there is no such row in A, Î¹ is not covered in A. Hence a CA(N ; t, k, v)
covers all interactions in It,k,v .
Covering arrays are used extensively for interaction testing in complex engineered systems. To ensure
that all possible combinations of options of t components function together correctly, one needs examine
all possible t-way interactions. When the number of components is k, and the number of different options
available for each component is v, each row of CA(N ; t, k, v) represents a test case. The N test cases
collectively test all t-way interactions. For this reason, covering arrays have been used in combinatorial
interaction testing in varied fields like software and hardware engineering, design of composite materials,
and biological networks [8, 24, 26, 32, 34].
The cost of testing is directly related to the number of test cases. Therefore, one is interested in covering
arrays with the fewest rows. The smallest value of N for which CA(N ; t, k, v) exists is denoted by CAN(t, k, v).
Efforts to determine or bound CAN(t, k, v) have been extensive; see [12, 14, 24, 31] for example. Naturally one
would prefer to determine CAN(t, k, v) exactly. Katona [22] and Kleitman and Spencer [23] independently
showed that
 for t = v = 2, the minimum number of rows N in a CA(N ; 2, k, 2) is the smallest N for which
âˆ’1
. Exact determination of CAN(t, k, v) for other values of t and v has remained open. However,
kâ‰¤ N
N
d2e
some progress has been made in determining upper bounds for CAN(t, k, v) in the general case; for recent
results, see [33].
For practical applications such bounds are often unhelpful, because one needs explicit covering arrays to
use as test suites. Explicit constructions can be recursive, producing larger covering arrays using smaller ones
as ingredients (see [14] for a survey), or direct. Direct methods for some specific cases arise from algebraic,
geometric, or number-theoretic techniques; general direct methods are computational in nature. Indeed
when k is relatively small, the best known results arise from computational techniques [13], and these are
in turn essential for the successes of recursive methods. Unfortunately, the existing computational methods
encounter difficulties as k increases, but is still within the range needed for practical applications. Typically
such difficulties arise either as a result of storage or time limitations or by producing covering arrays that
are too big to compete with those arising from simpler recursive methods.
Cohen [11] discusses commercial software where the number of factors often exceeds 50. Aldaco et al. [1]
analyze a complex engineered system having 75 factors, using a variant of covering arrays. Android [3] uses
a Configuration class to describe the device configuration; there are 17 different configuration parameters
with 3 âˆ’ 20 different levels. In each of these cases, while existing techniques are effective when the strength
is small, these moderately large values of k pose concerns for larger strengths.
In this paper, we focus on situations in which every factor has the same number of levels. These cases
have been most extensively studied, and hence provide a basis for making comparisons. In practice, however,
often different components have different number of levels, which is captured by extending the notion of a
covering array. A mixed covering array MCA(N ; t, k, (v1 , v2 , . . . , vk )) is an N Ã— k array in which the ith
column contains vi symbols for 1 â‰¤ i â‰¤ k. When {i1 , . . . , it } âŠ† {1, . . . , k} is
Qat set of t columns, in the N Ã— t
subarray obtained by selecting columns i1 , . . . , it of the MCA, each of the j=1 vij distinct t-tuples appears
as a row at least once. Although we examine the uniform case in which v1 = Â· Â· Â· = vk , the methods developed
here can all be directly applied to mixed covering arrays as well.
Inevitably, when k > max(t + 1, v + 2), a covering array must cover some interactions more than once,
for if not they are orthogonal arrays [20]. Treating the rows of a covering array in a fixed order, each row
covers some number of interactions not covered by any earlier row. For a variety of known constructions,
the initial rows cover many new interactions, while the later ones cover very few [7]. Comparing this rate
of coverage for a purely random method and for one of the sophisticated search techniques, one finds little
difference in the initial rows, but very substantial differences in the final ones. This suggests strategies to
build the covering array in stages, investing more effort as the number of remaining uncovered interactions
declines.
In this paper we propose a new algorithmic framework for covering array construction, the two-stage
framework. In the first stage, a randomized row construction method builds a specified number of rows to
cover all but at most a specified, small number of interactions. As we see later, by dint of being randomized

2

this uses very little memory. The second stage is a more sophisticated search that adds few rows to cover the
remaining uncovered interactions. We choose search algorithms whose requirements depend on the number
of interactions to be covered, to profit from the fact that few interactions remain. By mixing randomized and
deterministic methods, we hope to retain the fast execution and small storage of the randomized methods,
along with the accuracy of the deterministic search techniques.
We introduce a number of algorithms within the two-stage framework. Some improve upon best known
bounds on CAN(t, k, v) (see [33]) in principle. But our focus is on the practical consequences: The twostage algorithms are indeed quite efficient for higher strength (t âˆˆ {5, 6}) and moderate number of levels
(v âˆˆ {3, 4, 5, 6}), when the number of factors k is moderately high (approximately in the range of 20 âˆ’ 80
depending on value of t and v). In fact, for many combination of t, k and v values the two-stage algorithms
beat the previously best known bounds.
Torres-Jimenez et al. [36] explore a related two-stage strategy. In their first stage, an in-parameter-order
greedy strategy (as used in ACTS [24]) adds a column to an existing array; in their second stage, simulated
annealing is applied to cover the remaining interactions. They apply their methods when t = v = 3, when
the storage and time requirements for both stages remain acceptable. In addition to the issues in handling
larger strengths, their methods provide no a priori bound on the size of the resulting array. In contrast with
their methods, ours provide a guarantee prior to execution with much more modest storage and time.
The rest of the paper is organized as follows. Section 2 reviews algorithmic methods of covering array
construction, specifically the randomized algorithm and the density algorithm. This section contrasts these
two methods and points out their limitations. Then it gives an intuitive answer to the question of why a two
stage based strategy might work and introduces the general two-stage framework. Section 3 introduces some
specific two-stage algorithms. Section 3.1 analyzes and evaluates the naÄ±Ìˆve strategy. Section 3.2 describes a
two-stage algorithm that combines the randomized and the density algorithm. Section 3.3 introduces graph
coloring based techniques in the second stage. Section 3.4 examines the effect of group action on the size
of the constructed covering arrays. Section 4 compares the results of various two-stage algorithms with the
presently best known sizes. In Section 5 we discuss the LovaÌsz local lemma (LLL) bounds on CAN(t, k, v)
and provide a Moser-Tardos type randomized algorithm for covering array construction that matches the
bound. Although the bound was known [18], the proof was non-constructive, and a constructive algorithm to
match this bound seems to be absent in the literature. We explore potentially better randomized algorithms
for the first stage using LLL based techniques, We also obtain a two-stage bound that improves the LLL
bound for CAN(t, k, v). We conclude the paper in Section 6.

2

Algorithmic construction of covering arrays

Available algorithms for the construction of covering arrays are primarily heuristic in nature; indeed exact
algorithms have succeeded for very few cases. Computationally intensive metaheuristic search methods such
as simulated annealing, tabu search, constraint programming, and genetic algorithms have been employed
when the strength is relatively small or the number of factors and levels is small. These methods have
established many of the best known bounds on sizes of covering arrays [13], but for many problems of
practical size their time and storage requirements are prohibitive. For larger problems, the best available
methods are greedy. The IPO family of algorithms [24] repeatedly adds one column at a time, and
 then adds
new rows to ensure complete coverage. In this way, at any point in time, the status of v t kâˆ’1
tâˆ’1 interactions
may be stored. AETG [10] pioneered a different method, which greedily selects one row at a time to cover
a large number of as-yet-uncovered interactions. They establish that if a row can be chosen that covers the
maximum number, a good a priori bound on the size of the covering array can be computed. Unfortunately
selecting the maximum is NP-hard, and even if one selects the maximum there is no guarantee that the
covering array is the smallest possible
 [7], so AETG resorts to a good heuristic selection of the next row by
examining the stored status of v t kt interactions. None of the methods so far mentioned therefore guarantee
to reach an a priori bound. An
 extension of the AETG strategy, the density algorithm [5, 6, 15], stores
additional statistics for all v t kt interactions in order to ensure the selection of a good next row, and hence
guarantees to produce an array with at most the precomputed number of rows. Variants of the density
3

Algorithm 1: A randomized algorithm for covering array construction.
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; 
t, k, v)
log (kt)+t log v


1 Set N :=
;
vt
log

2
3

4
5
6
7
8
9
10
11
12

v t âˆ’1

repeat
Construct an N Ã— k array A where each entry is chosen independently and uniformly at random
from a v-ary alphabet;
Set covered := true;
for each interaction Î¹ âˆˆ It,k,v do
if Î¹ is not covered in A then
Set covered := false;
break;
end
end
until covered = true;
Output A;

algorithm have proved to be most effective for problems of moderately large size. For even larger problems,
pure random approaches have been applied.
To produce methods that provide a guarantee on size, it is natural to focus on the density algorithm
in order to understand its strengths and weaknesses. To do this, we contrast it with a basic randomized
algorithm. Algorithm 1 shows a simple randomized algorithm for covering array construction. The algorithm
constructs an array of a particular size randomly and checks whether all the interactions are covered. It
repeats until it finds an array that covers all the interactions.
log (kt)+t log v

 is guaranteed to exist:
A CA(N ; t, k, v) with N =
vt
log

v t âˆ’1

Theorem 1. [21, 27, 35] (Stein-LovaÌsz-Johnson (SLJ) bound): Let t, k, v be integers with k â‰¥ t â‰¥ 2, and
v â‰¥ 2. Then as k â†’ âˆ,

log kt + t log v


CAN(t, k, v) â‰¤
t
log vtvâˆ’1
In fact, the probability that the N Ã— k array constructed in line 3 of Algorithm 1 is a valid covering array
is high enough that the expected number of times the loop in line 2 is repeated is a small constant.
An alternative strategy is to add rows one by one instead of constructing the full array at the outset. We
start with an empty array, and whenever we add a new row we ensure that it covers at least the expected
number of previously uncovered interactions for a randomly chosen row. The probability that an uncovered
interaction is covered by a random row is 1/v t . If the number of uncovered interactions is u, then by linearity
of expectation, the expected number of newly covered interactions in a randomly chosen row is uv âˆ’t . If each
row added covers exactly this expected number, we obtain the same number of rows as the SLJ bound,
realized in Algorithm 1. But because the actual number of newly covered interactions is always an integer,
each added row covers at least duv âˆ’t e interactions. This is especially helpful towards the end when the
expected number is a small fraction.
Algorithm 2 follows this strategy. Again the probability that a randomly chosen row covers at least the
expected number of previously uncovered interactions is high enough that the expected number of times the
row selection loop in line 6 of Algorithm 2 is repeated is bounded by a small constant.
We can obtain an upper bound on the size produced by Algorithm 2 by assuming that each new row
added covers exactly duv âˆ’t e previously uncovered interactions. This bound is the discrete Stein-LovaÌsz4

Algorithm 2: A randomized algorithm for covering array construction using the discrete SLJ strategy.
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; t, k, v)
1 Let A be an empty array;

k t
2 Initialize a table T indexed by all t v interactions, marking every interaction â€œuncoveredâ€;
3 while there is an interaction marked â€œuncoveredâ€ in T do
4
Let u be the number of interactions marked â€œuncoveredâ€ in T ;
5
Set expectedCoverage := d vut e;
6
repeat
7
Let r be a row of length k where each entry is chosen independently and uniformly at random
from a v-ary alphabet;
8
Let coverage be the number of â€œuncoveredâ€ interactions in T that are covered in row r;
9
until coverage > expectedCoverage;
10
Add r to A;
11
Mark all interactions covered by r as â€œcoveredâ€ in T ;
12 end
13 Output A;

Johnson (discrete SLJ) bound. Figure 1 compares the sizes of covering arrays obtained from the SLJ and
the discrete SLJ bounds for different values of k when t = 6 and v = 3. Consider a concrete example, when
t = 5, k = 20, and v = 3. The SLJ bound guarantees the existence of a covering array with 12499 rows,
whereas the discrete SLJ bound guarantees the existence of a covering array with only 8117 rows.
The density algorithm replaces the loop at line 6 of Algorithm 2 by a conditional expectation derandomized method. For fixed v and t the density algorithm selects a row efficiently (time polynomial in k) and
deterministically that is guaranteed to cover at least duv âˆ’t e previously uncovered interactions. In practice,
for small values of k the density algorithm works quite well, often covering many more interactions than
the minimum. Many of the currently best known CAN(t, k, v) upper bounds are obtained by the density
algorithm in combination with various post-optimization techniques [13].
However, the practical applicability of Algorithm 2 and the density algorithm is limited by the storage
of the table T , representing each of the kt v t interactions. Even when t = 6, v = 3, and k = 54, there are
18,828,003,285 6-way interactions. This huge memory requirement renders the density algorithm impractical
for rather small values of k when t âˆˆ {5, 6} and v â‰¥ 3. We present an idea to circumvent this large
requirement for memory, and develop it in full in Section 3.

2.1

Why does a two stage based strategy make sense?

Compare the two extremes, the density algorithm and Algorithm 1. On one hand, Algorithm 1 does not suffer
from any substantial storage restriction, but appears to generate many more rows than the density algorithm.
On the other hand, the density algorithm constructs fewer rows for small values of k, but becomes impractical
when k is moderately large. One wants algorithms that behave like Algorithm 1 in terms of memory, but
yield a number of rows competitive with the density algorithm.
For t = 6, k = 16, and v = 3, Figure 2 compares the coverage profile for the density algorithm and
Algorithm 1. We plot the number of newly covered interactions for each row in the density algorithm, and
the expected number of newly covered interactions for each row for Algorithm 1. The qualitative features
exhibited by this plot are representative of the rates of coverage for other parameters.
Two key observations are suggested by Figure 2. First, the expected coverage in the initial random rows
is similar to the rows chosen by the density algorithm. In this example, the partial arrays consisting of the
first 1000 rows exhibit similar coverage, yet the randomized algorithm needed no extensive bookkeeping.
Secondly, as later rows are added, the judicious selections of the density algorithm produce much larger
coverage per row than Algorithm 1. Consequently it appears sensible to invest few computational resources
5

4

3.5

x 10

SLJ bound
Discrete SLJ bound

N âˆ’ number of rows

3

2.5

2

1.5

1

0.5
0

100

200

300

400

500
k

600

700

800

900

1000

Figure 1: Comparison of covering array sizes obtained from SLJ bound and discrete SLJ bound for different
values of k, when t = 6 and v = 3.

9000
Density
Basic Random

Number of newly covered interactions

8000
7000
6000
5000
4000
3000
2000
1000
0

0

2000

4000

6000
Row number

8000

10000

12000

Figure 2: For t = 6, k = 16 and v = 3, the actual number of newly covered interactions of the density
algorithm and the expected number of newly covered interactions in a random array.

6

on the initial rows, while making more careful selections in the later ones. This forms the blueprint of our
general two-stage algorithmic framework shown in Algorithm 3.
Algorithm 3: The general two-stage framework for covering array construction.
Input: t : strength of the required covering array, k : number of factors, v : number of levels for each
factor
Output: A : a CA(N ; t, k, v)
1 Choose a number n of rows and a number Ï of interactions;
// First Stage
0
2 Use a randomized algorithm to construct an n Ã— k array A ;
0
3 Ensure that A covers all but at most Ï interactions;
0
4 Make a list L of interactions that are not covered in A (L contains at most Ï interactions);
// Second Stage
0
5 Use a deterministic procedure to add N âˆ’ n rows to A to cover all the interactions in L;
6 Output A;
A specific covering array construction algorithm results by specifying the randomized method in the first
stage, the deterministic method in the second stage, and the computation of n and Ï. Any such algorithm
produces a covering array, but we wish to make selections so that the resulting algorithms are practical while
still providing a guarantee on the size of the array. In Section 3 we describe different algorithms from the
two-stage family, determine the size of the partial array to be constructed in the first stage, and establish
upper bound guarantees. In Section 4 we explore how good the algorithms are in practice.

3

Two-stage framework

For the first stage we consider two methods:
Rand
MT

the basic randomized algorithm
the Moser-Tardos type algorithm

We defer the development of method MT until Section 5. Method Rand uses a simple variant of Algorithm
1, choosing a random n Ã— k array.
For the second stage we consider four methods:
Naive
Greedy
Den
Col

the
the
the
the

naÄ±Ìˆve strategy, one row per uncovered interaction
online greedy coloring strategy
density algorithm
graph coloring algorithm

Using these abbreviations, we adopt a uniform naming convention for the algorithms: TS hA, Bi is the algorithm in which A is used in the first stage, and B is used in the second stage. For example, TS hMT, Greedyi
denotes a two-stage algorithm where the first stage is a Moser-Tardos type randomized algorithm and the
second stage is a greedy coloring algorithm. Later when the need arises we refine these algorithm names.

3.1

One row per uncovered interaction in the second stage (TS hRand, Naivei)

In the second stage each of the uncovered interactions after the first stage is covered using a new row.
Algorithm 4 describes the method in more detail.
This simple strategy improves on the basic randomized strategy when n is chosen judiciously. For
example, when t = 6, k = 54 and v = 3, Algorithm 1 constructs a covering array with 17, 236 rows. Figure
3 plots an upper bound on the size of the covering array against the number n of rows in the partial array.

7

Algorithm 4: NaÄ±Ìˆve two-stage algorithm (TS hRand, Naivei).
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; t, k, v)
 t 
log (kt)+t log v+log log vtvâˆ’1


;
1 Let n :=
vt
log

1

v t âˆ’1

2

Let Ï =

3

repeat
Let A be an n Ã— k array where each entry is chosen independently and uniformly at random from
a v-ary alphabet;
Let uncovNum := 0 and unCovList be an empty list of interactions;
Set covered := true;
for each interaction Î¹ âˆˆ It,k,v do
if Î¹ is not covered in A then
Set uncovNum :=uncovNum+1;
Add Î¹ to unCovList;
if uncovNum > Ï then
Set covered := false;
break;
end
end
end
until covered= true;
for each interaction Î¹ âˆˆuncovList do
Add a row to A that covers Î¹;
end
Output A;

4

5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

log

vt
v t âˆ’1

;

8

4

Total number of rows in the Covering array

1.75

x 10

1.7
1.65
1.6
1.55
1.5
1.45
1.4
1.35
1.3

1

1.1
1.2
1.3
1.4
1.5
1.6
1.7
n âˆ’âˆ’ number of rows in the partial array of the first stage

1.8
4

x 10

Figure 3: An upper bound on the size of the covering array against n, the size of the partial array constructed
in the first stage when t = 6, k = 54, and v = 3, with one new row added per uncovered interaction in the
second stage. The minimum size of 13, 162 is obtained when n = 12, 402. Algorithm 1 requires 17, 236 rows,
and the currently best known covering array has 17, 197 rows.

9

The smallest covering array is obtained when n = 12, 402 which, when completed, yields a covering array
with at most 13, 162 rowsâ€”a big improvement over Algorithm 1.
A theorem from [33] tells us the optimal value of n in general:
Theorem 2. [33] Let t, k, v be integers with k â‰¥ t â‰¥ 2, and v â‰¥ 2. Then
 t 

log kt + t log v + log log vtvâˆ’1 + 1


.
CAN(t, k, v) â‰¤
t
log vtvâˆ’1
log (kt)+t log v+log log


t
log vtvâˆ’1



vt
v t âˆ’1



. The expected number of uncovered
The bound is obtained by setting n =
 t 
interactions is exactly Ï = 1/ log vtvâˆ’1 .
Figure 4 compares SLJ, discrete SLJ and two-stage bounds for k â‰¤ 100, when t = 6 and v = 3. The
two-stage bound does not deteriorate in comparison to discrete SLJ bound as k increases; it consistently
takes only 307-309 more rows. Thus when k = 12 the two-stage bound requires only 6% more rows and
when k = 100 only 2% more rows than the discrete SLJ bound.
4

2.2

x 10

SLJ bound
Discrete SLJ bound
Twoâˆ’stage bound

2

N âˆ’ number of rows

1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
10

20

30

40

50

60

70

80

90

100

k

Figure 4: Comparison of covering array sizes obtained from SLJ bound, discrete SLJ bound and two-stage
bound for k â‰¤ 100, when t = 6 and v = 3. In this range of k values the two-stage bound requires 307-309
more rows than the discrete SLJ bound, that is, 2-6% more rows.
To ensure that the loop in line 7 of Algorithm 4 does not repeat too many times we need to know the
probability with which a random n Ã— k array leaves at most Ï interactions uncovered. Using Chebyshevâ€™s
inequality and the second moment method developed in [2, Chapter 4], we next show that in a random n Ã—
nk
array the number of uncovered interactions is almost always close to its expectation, i.e. kt v t 1 âˆ’ v1t .
Substituting the value of n from line 1, this expected value is equal to Âµ, as in line 2. Therefore, the probability
that a random n Ã— k array covers the desired number of interactions is constant, and the expected number
of times the loop in line 7 is repeated is also a constant (around 2 in practice).

10

Because the theory of the second moment method is developed
considerable detail in [2], here we briefly
Pin
m
mention the relevant concepts and results. Suppose that X = i=1 Xi , where Xi is the indicator random
variable for event Ai for 1 â‰¤ i â‰¤ m. For indices i, j, we write i âˆ¼ j if i 6= j and the events Ai , Aj are not
independent. Also suppose that X1 , . . . , Xm are symmetric, i.e. for every i 6= j there is a measure
preserving
P
mapping of the underlying probability space that sends event Ai to event Aj . Define âˆ†âˆ— = jâˆ¼i Pr [Aj |Ai ].
Then by [2, Corollary 4.3.4]:
Lemma 3. [2] If E[X] â†’ âˆ and âˆ†âˆ— = o(E[X]) then X âˆ¼ E[X] almost always.
In our case, Ai denotes the event that the ith interaction is not covered in a n Ã— k array where each entry
n
is chosen independently and uniformly at random from a v-ary alphabet. Then Pr[Xi ] = 1 âˆ’ v1t . Because



n
there are kt v t interactions in total, by linearity of expectation, E[X] = kt v t 1 âˆ’ v1t , and E[X] â†’ âˆ as
k â†’ âˆ.
Distinct events Ai and Aj are independent if the ith and jth interactions share no column. Therefore,

P
P
k
the event Ai is not independent of at most t tâˆ’1
other events Aj . So âˆ†âˆ— = jâˆ¼i Pr [Aj |Ai ] â‰¤ jâˆ¼i 1 â‰¤

k
t tâˆ’1
= o(E[X]) when v and t are constants. By Lemma 3, the number of uncovered interactions in a random
n Ã— k array is close to the expected number of uncovered interactions. This guarantees that Algorithm 4 is
an efficient randomized algorithm for constructing covering arrays with a number of rows upper bounded by
Theorem 2.
In keeping with the general two-stage framework, Algorithm 4 does not store the coverage status of
each interaction. We only need store the interactions that are uncovered in A, of which there are at most
1
 â‰ˆ v t . This quantity depends only on v and t and is independent of k, so is effectively a
Ï =
t
log vtvâˆ’1

constant that is much smaller than kt v t , the storage requirement for the density algorithm. Hence the
algorithm can be applied to a higher range of k values.
Although Theorem 5 provides asymptotically tighter bounds than Theorem 2, in a range of k values that
are relevant for practical application, Theorem 2 provides better results. Figure 5 compares the bounds on
CAN(t, k, v) with the currently best known results.
4

3

6

x 10

2.5

Best known
Twoâˆ’stage (simple)
GSS bound
2

2

N âˆ’ number of rows

N âˆ’ number of rows

2.5

1.5

1

1.5

1

0.5

0.5

0
0

x 10

Best known
Twoâˆ’stage (simple)
GSS bound

10

20

30

40

50

60

70

80

90

k

0

5

10

15

20

25

30

35

40

45

50

k

(a) t = 6, v = 3

(b) t = 6, v = 6

Figure 5: Comparison of GSS bound and two-stage bound with the currently best known results

3.2

The density algorithm in the second stage (TS hRand, Deni)

Next we apply the density algorithm in the second stage. Figure 6 plots an upper bound on the size of the
covering array against the size of the partial array constructed in the first stage when the density algorithm
is used in the second stage, and compares it with TS hRand, Naivei. The size of the covering array decreases
11

as n decreases. This is expected because with smaller partial arrays, more interactions remain for the second
stage to be covered by the density algorithm. In fact if we cover all the interactions using the density
algorithm (as when n = 0) we would get an even smaller covering array. However, our motivation was
precisely to avoid doing that. Therefore, we need a â€cut-offâ€ for the first stage.
4

Total number of rows in the Covering array

1.9

x 10

Basic Twoâˆ’stage
Twoâˆ’stage with density in second stage

1.8

1.7

1.6

1.5

1.4

1.3

1.2

1

1.1
1.2
1.3
1.4
1.5
1.6
1.7
n âˆ’âˆ’ number of rows in the partial array of the first stage

1.8
4

x 10

Figure 6: Comparison of covering array sizes from two-stage algorithms with Den and Naive in the second
stage. With Den there is no minimum point in the curve; the size of the covering array keeps decreasing as
we leave more uncovered interactions for the second stage.
We are presented with a trade-off. If we construct a smaller partial array in the first stage, we obtain a
smaller covering array overall. But we then pay for more storage and computation time for the second stage.
To appreciate the nature of this trade-off, look at Figure 7, which plots an upper bound on the covering
array size and the number of uncovered interactions in the first stage against n. The improvement in the
covering array size plateaus after a certain point. The three horizontal lines indicate Ï (â‰ˆ v t ), 2Ï and 3Ï
uncovered interactions in the first stage. (In the naÄ±Ìˆve method of Section 3.1, the partial array after the first
stage leaves at most Ï uncovered interactions.) In Figure 7 the final covering array size appears to plateau
when the number of uncovered interactions left by the first stage is around 2Ï. After that we see diminishing
returns â€” the density algorithm needs to cover more interactions in return for a smaller improvement in the
covering array size.
Let r be the maximum number of interactions allowed to remain uncovered after the first stage. Then r
can be specified in the two-stage algorithm. To accommodate this, we denote by TS hA, B; ri the two-stage
algorithm where A is the first stage strategy, B is the second stage strategy, and r is the maximum number
of uncovered interactions after the first stage. For example, TS hRand, Den; 2Ïi applies the basic randomized
algorithm in the first stage to cover all but at most 2Ï interactions, and the density algorithm to cover the
remaining interactions in the second stage.

3.3

Coloring in the second stage (TS hRand, Coli and TS hRand, Greedyi)

Now we describe strategies using graph coloring in the second stage. Construct a graph G = (V, E), the
incompatibility graph, in which V is the set of uncovered interactions, and there is an edge between two
12

Number of rows / Number of uncovered interactions

18000
16000

Num. of rows in the completed CA
Num. of uncovered interaction in first stage

14000
12000
10000
8000
6000
4000
2000
0
0.8

0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
4
n âˆ’âˆ’ number of rows in the partial array of the first stage
x 10

Figure 7: Final covering array size against the number of uncovered interactions after the first stage. As the
size n of the partial array decreases, the number of uncovered interactions in the first stage increases. Den is
used in the second stage. From bottom to top, the green lines denote Ï, 2Ï, and 3Ï uncovered interactions.
interactions exactly when they share a column in which they have different symbols. A single row can cover
a set of interactions if and only if it forms an independent set in G. Hence the minimum number of rows
required to cover all interactions of G is exactly its chromatic number Ï‡(G), the minimum number of colors
in a proper coloring of G. Graph coloring is an NP-hard problem, so we employ heuristics to bound the
chromatic number. Moreover, G only has vertices for the uncovered interactions after the first stage, so is
size is small relative to the total number of interactions.
The expected number of edges in the incompatibility
graph after
n choosing n rows uniformly at random
 k  t Pt

 t
 
t kâˆ’t
1
1 n
1
tâˆ’i
is Î³ = 2 t v
) 1 âˆ’ vt
1 âˆ’ (vt âˆ’vtâˆ’i ) . Using the elementary upper bound on
i=1 i tâˆ’i (v âˆ’ v
q
the chromatic number Ï‡ â‰¤ 12 + 2m + 14 , where m is the number of edges [16, Chapter 5.2], we can surely
q
cover the remaining interactions with at most 12 + 2m + 14 rows.
The actual number of edges m that remain after the first stage is a random variable with mean Î³. In
principle, the first stage could be repeatedly applied until m â‰¤ Î³, so we call m = Î³ the optimistic estimate.
To ensure that the first stage is expected to be run a small constant number of times, we increase the
estimate. With probability more than 1/2 the incompatibility graph has m â‰¤ 2Î³ edges, so m = 2Î³ is the
conservative estimate.
For t = 6, k = 56, and v = 3, Figure 8 shows the effect on the minimum number of rows when the bound
on the chromatic number in the second stage is used, for the conservative or optimistic estimates. The NaÄ±Ìˆve
method is plotted for comparison. Better coloring bounds shift the minima leftward, reducing the number
of rows produced in both stages.
Thus far we have considered bounds on the chromatic number. Better estimation of Ï‡(G) is complicated
by the fact that we do not have much information about the structure of G until the first stage is run. In
practice, however, G is known after the first stage and hence an algorithmic method to bound its chromatic
number can be applied. Because the number of vertices in G equals the number of uncovered interactions

13

4

2.2

x 10

Conservative estimate
Optimistic estimate
Simple

Number of rows required

2

1.8

1.6

1.4

1.2

1

1

1.1

1.2

1.3

1.4
N

1.5

1.6

1.7

1.8
4

x 10

Figure 8: Size of the partial array vs. size of the complete CA. t = 6, k = 56, v = 3. Stein-LovaÌsz-Johnson
bound requires 17, 403 rows, discrete Stein-LovaÌsz-Johnson bound requires 13, 021 rows. Simple estimate
for the two stage algorithm is 13, 328 rows, conservative estimate assuming m = 2Î³ is 12, 159 rows, and
optimistic estimate assuming m = Î³ is 11, 919 rows. Even the conservative estimate beats the discrete
Stein-LovaÌsz-Johnson bound.

14

after the first stage, we encounter the same trade-off between time and storage, and final array size, as seen
earlier for density. Hence we again parametrize by the expected number of uncovered interactions in the
first stage.
We employ two different greedy algorithms to color the incompatibility graph. In method Col we first
construct the incompatibility graph G after the first stage. Then we apply the commonly used smallest last
order heuristic to order the vertices for greedy coloring: At each stage, find a vertex vi of minimum degree
in Gi , order the vertices of Gi âˆ’ vi , and then place vi at the end. More precisely, we order the vertices of G
as v1 , v2 , . . . , vn , such that vi is a vertex of minimum degree in Gi , where Gi = G âˆ’ {vi+1 , . . . , vn }. A graph
is d-degenerate if all of its subgraphs have a vertex with degree at most d. When G is d-degenerate but not
(d âˆ’ 1)-degenerate, the Coloring number col(G) is d + 1. If we then greedily color the vertices with the first
available color, at most col(G) colors are used.
In method Greedy we employ an on-line, greedy approach that colors the interactions as they are discovered in the first stage. In this way, the incompatibility graph is never constructed. We instead maintain a set
of rows. Some entries in rows are fixed to a specific value; others are flexible to take on any value. Whenever
a new interaction is found to be uncovered in the first stage, we check if any of the rows is compatible with
this interaction. If such a row is found then entries in the row are fixed so that the row now covers the
interaction. If no such row exists, a new row with exactly t fixed entries corresponding to the interaction is
added to the set of rows. This method is much faster than method Col in practice.

3.4

Using group action

Covering arrays that are invariant under the action of a permutation group on their symbols can be easier
to construct and are often smaller [15]. Direct and computational constructions using group actions are
explored in [9, 28]. Sarkar et al. [33] establish the asymptotically tightest known bounds on CAN(t, k, v)
using group actions. In this section we explore the implications of group actions on two-stage algorithms.
Let Î“ be a permutation group on the set of symbols. The action of this group partitions the set of
t-way interactions into orbits. We construct an array A such that for every orbit, at least one row covers
an interaction from that orbit. Then we develop the rows of A over Î“ to obtain a covering array that is
invariant under the action of Î“. Effort then focuses on covering all the orbits of t-way interactions, instead
of the individual interactions.
If Î“ acts sharply transitively
on the set of symbols

 (for example, if Î“ is a cyclic group of order v) then
the action of Î“ partitions kt v t interactions into kt v tâˆ’1 orbits of length v each. Following
the lines of the

v tâˆ’1
+1
log (kt)+(tâˆ’1) log v+log log tâˆ’1
âˆ’1

 v
that covers at
proof of Theorem 2, there exists an n Ã— k array with n =
v tâˆ’1
log

v tâˆ’1 âˆ’1

least one interaction from each orbit. Therefore,
log
CAN(t, k, v) â‰¤ v

k
t



+ (t âˆ’ 1) log v + log log


v tâˆ’1
log vtâˆ’1
âˆ’1



v tâˆ’1
v tâˆ’1 âˆ’1



+1
.

(1)

Similarly, we can employ a Frobenius group. When v is a prime power, the Frobenius group is the group
of permutations of Fv of the form {x 7â†’ ax + b : a, b âˆˆ Fv , a 6= 0}. The action of the Frobenius group
tâˆ’1
partitions the set of t-tuples on v symbols into v vâˆ’1âˆ’1 orbits of length v(v âˆ’ 1) (full orbits) each and 1 orbit
of length v (a short orbit). The short orbit consists of tuples of the form (x1 , . . . , xt ) where x1 = . . . = xt .
Therefore, we can obtain a covering array by first constructing an array that covers all the full orbits, and
then developing all the rows over the Frobenius group and adding v constant rows. Using the two stage
strategy in conjunction with the Frobenius group action we obtain:


 tâˆ’1 

v tâˆ’1
log kt + log v vâˆ’1âˆ’1 + log log vtâˆ’1
âˆ’v+1 + 1


CAN(t, k, v) â‰¤ v(v âˆ’ 1)
+ v.
(2)
tâˆ’1
v
log vtâˆ’1
âˆ’v+1

15

4

1.5

x 10

N âˆ’ number of rows

1.45

Twoâˆ’stage (simple)
Twoâˆ’stage (cyclic group action)
Twoâˆ’stage (Frobenius group action)

1.4

1.35

1.3

1.25
50

55

60

65

70

75

k

Figure 9: Comparison of the simple two-stage bound with the cyclic and the Frobenius two-stage bounds.
t = 6, v = 3 and 50 â‰¤ k â‰¤ 75. Group action reduces the required number of rows slightly.
Figure 9 compares the simple two-stage bound with the cyclic and Frobenius two-stage bounds. For
t = 6, v = 3 and 12 â‰¤ k â‰¤ 100, the cyclic bound requires 7-21 (on average 16) fewer rows than the simple
bound. In the same range the Frobenius bound requires 17 âˆ’ 51 (on average 40) fewer rows.
Group action can be applied in other methods for the second stage as well. Colbourn [15] incorporates
group action into the density algorithm, allowing us to apply method Den in the second stage.
Greedy extends easily to use group action, as we do not construct an explicit incompatibility graph.
Whenever we fix entries in a row to cover an uncovered orbit, we commit to a specific orbit representative.
However, applying group action to the incompatibility graph coloring for Col is more complicated. We
need to modify the definition of the incompatibility graph for two reasons. First the vertices no longer
represent uncovered interactions, but rather uncovered orbits of interaction. Secondly, and perhaps more
importantly, pairwise compatibility between every two orbits in a set no longer implies mutual compatibility
among all orbits in the set.
One approach is to form a vertex for each uncovered orbit, placing an edge between two when they share
a column. Rather than the usual coloring, however, one asks for a partition of the vertex set into classes so
that every class induces an acyclic subgraph. Problems of this type are generalized graph coloring problems
[4]. Within each class of such a vertex partition, consistent representatives of each orbit can be selected to
form a row; when a cycle is present, this may not be possible. Unfortunately, heuristics for solving these
types of problems appear to be weak, so we adopt a second approach. As we build the incompatibility
graph, we commit to specific orbit representatives. When a vertex for an uncovered orbit is added, we check
its compatibility with the orbit representatives chosen for the orbits already handled with which it shares
columns; we commit to an orbit representative and add edges to those with which it is now incompatible.
Once completed, we have a (standard) coloring problem for the resulting graph.
Because group action can be applied using each of the methods for the two stages, we extend our naming
to TS hA, B; r, Î“i, where Î“ can be Trivial (i.e. no group action), Cyclic, or Frobenius.

16

4

Computational results

Figure 5 indicates that even a simple two-stage bound can improve on best known covering array numbers.
Therefore we investigate the actual performance of our two-stage algorithms for covering arrays of strength
5 and 6.
First we present results for t = 6, when v âˆˆ {3, 4, 5, 6} and no group action is assumed. Table 1 shows the
results for different v values. In each case we select the range of k values where the two-stage bound predicts
smaller covering arrays than
 previously known best ones, setting the maximum number of uncovered
 tthe
v
interactions as Ï = 1/ log vt âˆ’1 â‰ˆ v t . For each value of k we construct a single partial array and then run
the different second stage algorithms on it consecutively. In this way all the second stage algorithms cover
the same set of uncovered interactions.
The column tab lists the best known CAN(t, k, v) upper bounds from [13]. The column bound shows the
upper bounds obtained from the two-stage bound (2). The columns naÄ±Ìˆve, greedy, col and den show results
obtained from running the TS hRand, Naive; Ï, Triviali, TS hRand, Greedy; Ï, Triviali, TS hRand, Col; Ï, Triviali
and TS hRand, Den; Ï, Triviali algorithms, respectively.
The naÄ±Ìˆve method always finds a covering array that is smaller than the two-stage bound. This happens
because we repeat the first stage of Algorithm 4 until the array has fewer than v t uncovered interactions.
(If the first stage were not repeated, the algorithm still produce covering arrays that are not too far from
the bound.) For v = 3 Greedy and Den have comparable performance. Method Col produces covering arrays
that are smaller. However, for v âˆˆ {4, 5, 6} Den and Col are competitive.
Table 2 shows the results obtained by the different second stage algorithms when the maximum number
of uncovered interactions in the first stage is set to 2Ï and 3Ï respectively. When more interactions are
covered in the second stage, we obtain smaller arrays as expected. However, the improvement in size does
not approach 50%. There is no clear winner.
Next we investigate the covering arrays that are invariant under the action of a cyclic group. In Table 3 the
column bound shows the upper bounds from Equation (1). The columns naÄ±Ìˆve, greedy, col and den show
results obtained from running TS hRand, Naive; Ï, Cyclici, TS hRand, Greedy; Ï, Cyclici, TS hRand, Col; Ï, Cyclici
and TS hRand, Den; Ï, Cyclici, respectively.
Table 4 presents results for cyclic group action based algorithms when the number of maximum uncovered
interactions in the first stage is set to 2Ï and 3Ï respectively.
For the Frobenius group action, we show results only for v âˆˆ {3, 5} in Table 5. The column bound shows
the upper bounds obtained from Equation (2).
Table 6 presents results for Frobenius group action algorithms when the number of maximum uncovered
interactions in the first stage is 2Ï or 3Ï.
Next we present a handful of results when t = 5. In the cases examined, using the trivial group action
is too time consuming to be practical. However, the cyclic or Frobenius cases are feasible. Tables 7 and 8
compare two stage algorithms when the number of uncovered interactions in the first stage is at most 2Ï.
In almost all cases there is no clear winner among the three second stage methods. Methods Den and
Greedy are, however, substantially faster and use less memory than method Col; for practical purposes they
would be preferred.
All code used in this experimentation is available from the github repository
https://github.com/ksarkar/CoveringArray
under an open source GPLv3 license.

5

Limited dependence and the Moser-Tardos algorithm

Here we explore a different randomized algorithm that produces smaller covering arrays than Algorithm 1.
When k > 2t, there are interactions that share no column. The events of coverage of such interactions are
independent. Moser et al. [29, 30] provide an efficient randomized construction method that exploits this

17

k

tab

bound

53
54
55
56
57

13021
14155
17161
19033
20185

13076
13162
13246
13329
13410

39
40
41
42
43
44

68314
71386
86554
94042
99994
104794

65520
66186
66834
67465
68081
68681

31
32
33
34
35
36

233945
258845
281345
293845
306345
356045

226700
229950
233080
236120
239050
241900

17
18
19
20
21
22
23
24
25
26
27

506713
583823
653756
694048
783784
844834
985702
1035310
1112436
1146173
1184697

486310
505230
522940
539580
555280
570130
584240
597660
610460
622700
634430

naÄ±Ìˆve
greedy
t = 6, v = 3
13056
12421
13160
12510
13192
12590
13304
12671
13395
12752
t = 6, v = 4
65452
61913
66125
62573
66740
63209
67408
63819
68064
64438
68556
65021
t = 6, v = 5
226503 213244
229829 216444
232929 219514
235933 222516
238981 225410
241831 228205
t = 6, v = 6
486302 449950
505197 468449
522596 485694
539532 502023
555254 517346
569934 531910
584194 545763
597152 558898
610389 571389
622589 583473
634139 594933

col

den

12415
12503
12581
12665
12748

12423
12512
12591
12674
12757

61862
62826
63160
64077
64935
65739

61886
62835
63186
64082
64907
65703

212942
217479
219215
222242
226379
230202

212940
217326
219241
222244
226270
229942

448922
467206
484434
500788
516083
530728
544547
557917
570316
582333
593857

447864
466438
483820
500194
515584
530242
548307
557316
569911
582028
593546

Table 1: Comparison of different TS hRand, âˆ’; Ï, Triviali algorithms.

18

k
greedy

2Ï
col

53
54
55
56
57

11968
12135
12286
12429
12562

11958
12126
12129
12204
12290

39
40
41
42
43
44

59433
60090
60715
61330
61936
62530

59323
60479
61527
62488
61839
62899

31
32
33
34
35
36

204105
207243
210308
213267
216082
218884

203500
206659
209716
212675
215521
218314

17
18
19
20
21
22
23
24
25
26
27

425053
443236
460315
476456
491570
505966
519611
532612
544967
556821
568135

-

den
t = 6, v
11968
12050
12131
12218
12296
t = 6, v
59326
59976
60615
61242
61836
62428
t = 6, v
203302
206440
209554
212508
215389
218172
t = 6, v
420333
438754
455941
472198
487501
502009
515774
528868
541353
553377
564827

greedy
=3
11716
11804
11877
11961
12044
=4
58095
58742
59369
59974
60575
61158
=5
199230
202342
205386
208285
211118
213872
=6
412275
430402
447198
463071
478269
492425
505980
518746
531042
542788
554052

3Ï
col

den

11705
11787
11875
12055
12211

11708
11790
11872
11950
12034

57951
58583
59867
61000
60407
61004

57888
58544
59187
59796
60393
60978

198361
201490
204548
-

197889
201068
204107
207060
209936
212707

-

405093
423493
440532
456725
471946
486306
500038
513047
525536
537418
548781

Table 2: Comparison of TS hRand, âˆ’; 2Ï, Triviali and TS hRand, âˆ’; 3Ï, Triviali algorithms.

19

k

tab

bound

53
54
55
56
57

13021
14155
17161
19033
20185

13059
13145
13229
13312
13393

k
39
40
41
42
43
44

tab
68314
71386
86554
94042
99994
104794

bound
65498
66163
66811
67442
68057
68658

31
32
33
34
35
36

226000
244715
263145
235835
238705
256935

226680
229920
233050
236090
239020
241870

17
18
19
20
21
22
23
24
25
26
27

506713
583823
653756
694048
783784
844834
985702
1035310
1112436
1146173
1184697

486290
505210
522910
539550
555250
570110
584210
597630
610430
622670
624400

naÄ±Ìˆve
greedy
t = 6, v = 3
13053
12405
13119
12489
13209
12573
13284
12660
13368
12744
t = 6, v = 4
naÄ±Ìˆve
greedy
65452
61896
66080
62516
66740
63184
67408
63800
68032
64408
68556
64988
t = 6, v = 5
226000 213165
229695 216440
233015 219450
235835 222450
238705 225330
241470 228140
t = 6, v = 6
485616 449778
504546 468156
522258 485586
539280 501972
554082 517236
569706 531852
583716 545562
597378 558888
610026 571380
622290 583320
633294 594786

col

den

12405
12543
12663
12651
12744

12411
12546
12663
12663
12750

col
61860
62820
63144
63780
64692
64964

den
61864
62784
63152
63784
64680
64976

212945
217585
221770
222300
225130
229235

212890
217270
221290
222210
225120
229020

448530
467232
490488
500880
521730
530832
549660
557790
575010
582546
598620

447732
466326
488454
500172
519966
530178
548196
557280
573882
582030
597246

Table 3: Comparison of TS hRand, âˆ’; Ï, Cyclicialgorithms.

20

k
greedy

2Ï
col

53
54
55
56
57

11958
12039
12120
12204
12276

11955
12027
12183
12342
12474

39
40
41
42
43
44

59412
60040
60700
61320
61908
62512

59336
59996
61156
62196
63192
64096

31
32
33
34
35
36

204060
207165
207165
213225
216050
218835

203650
209110
209865
212830
217795
218480

17
18
19
20
21
22
23
24
25
26
27

424842
443118
460014
476328
491514
505884
519498
532368
544842
543684
568050

422736
440922
457944
474252
489270
503580
517458
530340
542688
543684
566244

den
t = 6, v
11958
12036
12195
12324
12450
t = 6, v
59304
59964
61032
61976
62852
63672
t = 6, v
203265
208225
209540
212510
217070
218155
t = 6, v
420252
438762
455994
472158
487500
501852
515718
528828
541332
543684
564756

greedy
=3
11700
11790
11862
11949
12027
=4
58076
58716
59356
59932
60568
61152
=5
199180
202255
205380
208225
211080
213770
=6
411954
430506
447186
463062
478038
492372
505824
518700
530754
542664
553704

3Ï
col

den

11691
11874
12057
11937
12021

11694
11868
12027
11943
12024

57976
58616
59252
59840
61124
61048

57864
58520
59160
59760
60904
60988

198455
204495
204720
207790
213425
213185

197870
203250
204080
207025
212040
212695

409158
427638
456468
460164
486180
489336
502806
515754
538056
539922
560820

405018
423468
449148
456630
479970
486264
500040
512940
532662
537396
555756

Table 4: Comparison of TS hRand, âˆ’; 2Ï, Cyclici and TS hRand, âˆ’; 3Ï, Cyclici algorithms.

21

k

tab

bound

53
54
55
56
57

13021
14155
17161
19033
20185

13034
13120
13203
13286
13366

31
32
33
34
35
36

233945
258845
281345
293845
306345
356045

226570
229820
232950
235980
238920
241760

naÄ±Ìˆve
greedy
t = 6, v = 3
13029
12393
13071
12465
13179
12561
13245
12633
13365
12723
t = 6, v = 5
226425 213025
229585 216225
232725 219285
234905 222265
238185 225205
241525 227925

col

den

12387
12513
12549
12627
12717

12393
12531
12567
12639
12735

212865
216085
219205
223445
227445
231145

212865
216065
219145
223265
227065
230645

Table 5: Comparison of TS hRand, âˆ’; Ï, Frobeniusi algorithms.

k
greedy

2Ï
col

53
54
55
56
57
70
75
80
85
90

11931
12021
12105
12171
12255
13167
13473
13773
14031
14289

11919
12087
12237
12171
12249
13155
13473
13767
14025
14283

31
32
33
34
35
36
50
55
60
65

203785
206965
209985
213005
215765
218605
250625
259785
268185
275785

203485
208965
209645
214825
215545
218285
250365
259625
268025
275665

den
greedy
t = 6, v = 3
11931
11700
12087
11790
12231
11862
12183
11949
12255
12027
13179
13479
13779
14037
14301
t = 6, v = 5
203225 198945
208065 201845
209405 205045
214145 208065
215265 210705
218025 213525
250325
259565
267945
275665
-

3Ï
col

den

11691
11874
12057
11937
12021
-

11694
11868
12027
11943
12024
-

198445
204505
209845
207545
210365
213105
-

197825
203105
207865
206985
209885
212645
-

Table 6: Comparison of TS hRand, âˆ’; 2Ï, Frobeniusi and TS hRand, âˆ’; 3Ï, Frobeniusi algorithms.

22

k
67
68
69
70
71

tab
59110
60991
60991
60991
60991

greedy
48325
48565
48765
49005
49245

col
48285
48565
49005
48985
49205

den
48305
48585
48985
49025
49245

Table 7: Comparison of TS hRand, âˆ’; 2Ï, Frobeniusi algorithms. t = 5, v = 5
k
49
50
51
52
53

tab
122718
125520
128637
135745
137713

greedy
108210
109014
109734
110556
111306

col
108072
108894
110394
110436
111180

den
107988
108822
110166
110364
111120

Table 8: Comparison of TS hRand, âˆ’; 2Ï, Cyclici algorithms. t = 5, v = 6
limited dependence. Specializing their method to covering arrays, we obtain Algorithm 5. For the specified
value of N in the algorithm it is guaranteed that the expected number of times the loop in line 3 of Algorithm
5 is repeated is linearly bounded in k (See Theorem 1.2 of [30]).
The upper bound on CAN(t, k, v) guaranteed by Algorithm 5 is obtained by applying the LovaÌsz local
lemma (LLL).
Lemma 4. (LovaÌsz local lemma; symmetric case) (see [2]) Let A1 , A2 , . . . , An events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at
most d, and that Pr[Ai ] â‰¤ p for all 1 â‰¤ i â‰¤ n. If ep(d + 1) â‰¤ 1, then Pr[âˆ©ni=1 AÌ„i ] > 0.
The symmetric version of LovaÌsz local lemma provides an upper bound on the probability of a â€œbadâ€
event in terms of the maximum degree of a bad event in a dependence graph, so that the probability that
all the bad events are avoided is non zero. Godbole, Skipper, and Sunley [18] apply Lemma 4 essentially to
obtain the bound on CAN(t, k, v) in line 1 of Algorithm 5.
Theorem 5. [18] Let t, v and k â‰¥ 2t be integers with t, v â‰¥ 2. Then
n 
o
+ t log v + 1
log kt âˆ’ kâˆ’t
t


CAN (t, k, v) â‰¤
t
log vtvâˆ’1
The bound on the size of covering arrays obtained from Theorem 5 is asymptotically tighter than the
one obtained from Theorem 1. Figure 10 compares the bounds for t = 6 and v = 3.
The original proof of LLL is essentially non-constructive and does not immediately lead to a polynomial
time construction algorithm for covering arrays satisfying the bound of Theorem 5. Indeed no previous
construction algorithms appear to be based on it. However the Moser-Tardos method of Algorithm 5 does
provide a construction algorithm running in expected polynomial time. For sufficiently large values of k
Algorithm 5 produces smaller covering arrays than the Algorithm 1.
But the question remains: Does Algorithm 5 produce smaller covering arrays than the currently best
known results within the range that it can be effectively computed? Perhaps surprisingly, we show that
the answer is affirmative. In Algorithm 5 we do not need to store the coverage information of individual
interactions in memory because each time an uncovered interaction is encountered we re-sample the columns
involved in that interaction and start the check afresh (checking the coverage in interactions in the same
order each time). Consequently, Algorithm 5 can be applied for larger values of k than the density algorithm.
23

Algorithm 5: Moser-Tardos type algorithm for covering array construction.
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; t, k, v)
log{(kt)âˆ’(kâˆ’t
log v+1
)}+t.
t

;
1 Let N :=
vt
log

2

3
4
5
6
7
8
9
10
11
12
13

14
15
16

v t âˆ’1

Construct an N Ã— k array A where each entry is chosen independently and uniformly at random from
a v-ary alphabet;
repeat
Set covered := true;
for each interaction Î¹ âˆˆ It,k,v do
if Î¹ is not covered in A then
Set covered := false;
Set missing-interaction := Î¹;
break;
end
end
if covered = false then
Choose all the entries in the t columns involved in missing-interaction independently and
uniformly at random from the v-ary alphabet;
end
until covered = true;
Output A;

Smaller covering arrays can be obtained by exploiting a group action using LLL, as shown in [33]. Table
9 shows the sizes of the covering arrays constructed by a variant of Algorithm 5 that employs cyclic and
Frobenius group actions. While this single stage algorithm produces smaller arrays than the currently best
known [13], these are already superseded by the two-stage based algorithms.
k
56
57
58
59
60

tab
19033
20185
23299
23563
23563

MT
16281
16353
16425
16491
16557

(a) Frobenius. t = 6, v = 3

k
44
45
46
47
48

tab
411373
417581
417581
423523
423523

MT
358125
360125
362065
363965
365805

(b) Frobenius. t = 6, v = 5

k
25
26
27
28
29

tab
1006326
1040063
1082766
1105985
1149037

MT
1020630
1032030
1042902
1053306
1063272

(c) Cyclic. t = 6, v = 6

Table 9: Comparison of covering array size from Algorithm 5 (MT) with the best known results [13] (tab).

5.1

Moser-Tardos type algorithm for the first stage

The linearity of expectation arguments used in the SLJ bounds permit one to consider situations in which
a few of the â€œbadâ€ events are allowed to occur, a fact that we exploited in the first stage of the algorithms
thus far. However, the LovaÌsz local lemma does not address this situation directly. The conditional LovaÌsz
local lemma (LLL) distribution, introduced in [19], is a very useful tool.
Lemma 6. (Conditional LLL distribution; symmetric case) (see [2, 33]) Let A = {A1 , A2 , . . . , Al } be a set
of l events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of
all other events Aj except for at most d, and that Pr[Ai ] â‰¤ p for all 1 â‰¤ i â‰¤ l. Also suppose that ep(d+1) â‰¤ 1
(Therefore, by LLL (Lemma 4) Pr[âˆ©li=1 AÌ„i ] > 0). Let B âˆˆ
/ A be another event in the same probability space
24

5

10

N âˆ’ number of rows

SLJ bound
GSS bound

4

10

3

10
1
10

2

3

10

10
k

4

10

5

10

Figure 10: Comparison of SLJ (Theorem 1) and GSS (Theorem 5) bounds for t = 6 and v = 3. The graph
is plotted in log-log scale to highlight the asymptotic difference between the two bounds.
with Pr[B] â‰¤ q, such that B is also mutually independent of a set of all other events Aj âˆˆ A except for at
most d. Then Pr[B| âˆ©li=1 AÌ„i ] â‰¤ eq.
We apply the conditional
LLL distribution to obtain an upper bound on the size of partial array that
 t 
v
leaves at most log vt âˆ’1 â‰ˆ v t interactions uncovered. For a positive integer k, let I = {j1 , . . . , jÏ } âŠ† [k]
where j1 < . . . < jÏ . Let A be an n Ã— k array where each entry is from the set [v]. Let AI denote the n Ã— Ï
array in which AI (i, `) = A(i, j` ) for 1 â‰¤ i â‰¤ N and 1 â‰¤ ` â‰¤ Ï; AI is the projection of A onto the columns
in I.

Let M âŠ† [v]t be a set of m t-tuples of symbols, and C âˆˆ [k]
be a set of t columns. Suppose the
t
entries in the array A are chosen independently from [v] with uniform probability.
 Let BC denote the event
that at least one of the tuples in M is not covered in AC . There are Î· = kt such events, and for all of
n
them Pr[BC ] â‰¤ m 1 âˆ’ v1t . Moreover, when k â‰¥ 2t, each of the events is mutually independent of all



k
other events except for at most Ï = kt âˆ’ kâˆ’t
âˆ’ 1 < t tâˆ’1
. Therefore, by the LovaÌsz local lemma, when
t

1 n
eÏm 1 âˆ’ vt â‰¤ 1, none of the events BC occur. Solving for n, when
nâ‰¥

log(eÏm)


t
log vtvâˆ’1

(3)


there exists an n Ã— k array A over [v] such that for all C âˆˆ [k]
t , AC covers all the m tuples in M . In fact
we can use a Moser-Tardos type algorithm to construct such an array.
Let Î¹ be an interaction whose t-tuple
n of symbols is not in M . Then the probability that Î¹ is not covered
in an n Ã— k array is at most 1 âˆ’ v1t
when each entry of the array is chosen independently from [v] with
uniform probability. Therefore, by the
 conditional LLL distribution the probability that Î¹1 is
n not covered
in the array A where for all C âˆˆ [k]
. Moreover,
t , AC covers all the m tuples in M is at most e 1 âˆ’ v t
there are Î·(v t âˆ’ m) such interactions Î¹. By the linearity of expectation, the expected number of uncovered
25

n
interactions in A is less than v t when Î·(v t âˆ’ m)e 1 âˆ’ v1t â‰¤ v t . Solving for n, we obtain
	

log Î·e 1 âˆ’ vmt

 .
nâ‰¥
t
log vtvâˆ’1

Therefore, there exists an n Ã— k array with n = max

log{Î·e(1âˆ’ vmt )}
log(eÏm)


,

t
t
log vtvâˆ’1
log vtvâˆ’1



(4)

that has at most v t

uncovered interactions. To compute n explicitly, we must choose m. We can select a value of m to minimize
n graphically for given values of t, k and v. For example, Figure 11 plots Equations 3 and 4 against m for
t = 3, k = 350, v = 3, and finds the minimum value of n.
460

445
max(Equation (3), Equation (4))

440

n âˆ’ number of rows in the partial array

n âˆ’ number of rows in the partial array

Equation (3)
Equation (4)

420

400

380

360

340
0

5

10

15
m

20

25

30

(a) Equations 3 and 4 against m.

440

435

430

425

420
0

5

10

15
m

20

25

30

(b) Maximum of the two sizes against m.

Figure 11: The minimum is at n = 422, when m = 16. t = 3, k = 350, v = 3
We compare the size of the partial array from the naÄ±Ìˆve two-stage method (Algorithm 4) with the size
obtained by the graphical methods in Figure 12. The LovaÌsz local lemma based method is asymptotically
better than the simple randomized method. However, except for the small values of t and v, in the range
of k values relevant for practical applications the simple randomized algorithm requires fewer rows than the
LovaÌsz local lemma based method.

5.2

LovaÌsz local lemma based two-stage bound

We can apply the techniques from Section 5.1 to obtain a two-stage bound similar to Theorem 2 using the
LovaÌsz local lemma and conditional LLL distribution. First we extend a result from [33].



Theorem
7. Let t, k, v be integers with k â‰¥ t â‰¥ 2, v â‰¥ 2 and let Î· = kt , and Ï = kt âˆ’ kâˆ’t
t . If


vt
v t âˆ’1

Î·v t log

Ï

â‰¤ v t Then
log
CAN(t, k, v) â‰¤

k
t



+ t log v + log log


t
log vtvâˆ’1



vt
v t âˆ’1



+2

Î·
âˆ’ .
Ï

Proof. Let M âŠ† [v]t be a set of m t-tuples of symbols. Following the arguments of Section 5.1, when


 there exists an n Ã— k array A over [v] such that for all C âˆˆ [k] , AC covers all m tuples in M .
n â‰¥ log(eÏm)
t
vt
log

v t âˆ’1

At most Î·(v t âˆ’ m) interactions are uncovered in such an array. Using the conditional
n LLL distribution,
the probability that one such interaction is not covered in A is at most e 1 âˆ’ v1t . Therefore, by the
26

n âˆ’ number of rows in partial array with vt missing interactions

n âˆ’ number of rows in partial array with vt missing interactions

550
500
450
400
350
300
250
200
150
Randomized (Algorithm 4)
LLL based

100
50

0

200

400

600
k

800

1000

1200

2500

2000

1500

1000

500

0

0

Randomized (Algorithm 4)
LLL based

500

1000

1500

2000

2500

3000

3500

4000

4500

k

(a) t = 3, v = 3.

(b) t = 4, v = 3.

Figure 12: Comparison of the size of the partial array constructed in the first stage. Compares the size of
the partial array specified in Algorithm 4 in Section 3.1, and the size derived in Section 5.1.
 t

n
linearity of expectation, we can find one such array A that leaves at most eÎ·(v t âˆ’ m) 1 âˆ’ v1t = Î·Ï vm âˆ’ 1
interactions uncovered. Adding one row per uncovered interactions to A, we obtain a covering array with at
most N rows, where


log(eÏm)
Î· vt

+
N=
âˆ’
1
t
Ï m
log tv
The value of N is minimized when m =

v âˆ’1
 t 
Î·v t log vtvâˆ’1
Ï

. Because m â‰¤ v t , we obtain the desired bound.

When m = v t this recaptures the bound of Theorem 5.
Figure 13 compares the LLL based two-stage bound from Theorem 7 to the standard two-stage bound
from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1. Although the
LLL based two-stage bound is tighter than the LLL based Godbole et al. bound, even for quite large values
of k the standard two-stage bound is tighter than the LLL based two-stage bound. In practical terms, this
specific LLL based two-stage method does not look very promising, unless the parameters are quite large.

6

Conclusion and open problems

Many concrete algorithms within a two-stage framework for covering array construction have been introduced
and evaluated. The two-stage approach extends the range of parameters for which competitive covering arrays
can be constructed, each meeting an a priori guarantee on its size. Indeed as a consequence a number of
best known covering arrays have been improved upon. Although each of the methods proposed has useful
features, our experimental evaluation suggests that TS hRand, Greedy; 2Ï, Î“i and TS hRand, Den; 2Ï.Î“i with
Î“ âˆˆ {Cyclic, Frobenius} realize a good trade-off between running time and size of the constructed covering
array.
Improvements in the bounds, or in the algorithms that realize them, are certainly of interest. We
mention some specific directions. Establishing tighter bounds on the coloring based methods of Section 3.3
is a challenging problem. Either better estimates of the chromatic number of the incompatibility graph after
a random first stage, or a first stage designed to limit the chromatic number, could lead to improvements in
the bounds.
In Section 5.1 and 5.2 we explored using a Moser-Tardos type algorithm for the first stage. Although this
is useful for asymptotic bounds, practical improvements appear to be limited. Perhaps a different approach of
27

10000

500

9000

450

8000

400

7000

N âˆ’ number of rows

N âˆ’ number of rows

550

350
300
250

SLJ bound
Godbole
LLLâˆ’2âˆ’stage
2âˆ’stage

200
150
100

0

50

100

150

200

250

300

350

400

6000
5000
4000

SLJ bound
Godbole
LLLâˆ’2âˆ’stage
2âˆ’stage

3000
2000

450

k

(a) t = 3, v = 3.

1000
0

1000

2000

3000

4000
k

5000

6000

7000

8000

(b) t = 4, v = 4.

Figure 13: Comparison among the LLL based two-stage bound from Theorem 7, the standard two-stage
bound from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1.
reducing the number of bad events to be avoided explicitly by the algorithm may lead to a better algorithm.
A potential approach may look like following: â€œBadâ€ events would denote non-coverage of an interaction
over a t-set of columns. We would select a set of column t-sets such that the dependency graph of the
corresponding bad events have a bounded maximum degree (less than the original dependency graph). We
would devise a Moser-Tardos type algorithm for covering all the interactions on our chosen column t-sets,
and then apply the conditional LLL distribution to obtain an upper bound on the number of uncovered
interactions. However, the difficulty lies in the fact that â€œall vertices have degree â‰¤ Ïâ€ is a non-trivial,
â€œhereditaryâ€ property for induced subgraphs, and for such properties finding a maximum induced subgraph
with the property is an NP-hard optimization problem [17]. There is still hope for a randomized or â€œnibbleâ€
like strategy that may find a reasonably good induced subgraph with a bounded maximum degree. Further
exploration of this idea seems to be a promising research avenue.
In general, one could consider more than two stages. Establishing the benefit (or not) of having more
than two stages is also an interesting open problem. Finally, the application of the methods developed to
mixed covering arrays appears to provide useful techniques for higher strengths; this merits further study as
well.

Acknowledgments
The research was supported in part by the National Science Foundation under Grant No. 1421058.

References
[1] A. N. Aldaco, C. J. Colbourn, and V. R. Syrotiuk. Locating arrays: A new experimental design for
screening complex engineered systems. SIGOPS Oper. Syst. Rev., 49(1):31â€“40, Jan. 2015.
[2] N. Alon and J. H. Spencer. The probabilistic method. Wiley-Interscience Series in Discrete Mathematics
and Optimization. John Wiley & Sons, Inc., Hoboken, NJ, third edition, 2008. With an appendix on
the life and work of Paul ErdoÌ‹s.
[3] Android. Android configuration class, 2016. http://developer.android.com/reference/android/content/
res/Configuration.html.
[4] J. I. Brown. The complexity of generalized graph colorings. Discrete Appl. Math., 69(3):257â€“270, 1996.
28

[5] R. C. Bryce and C. J. Colbourn. The density algorithm for pairwise interaction testing. Software
Testing, Verification, and Reliability, 17:159â€“182, 2007.
[6] R. C. Bryce and C. J. Colbourn. A density-based greedy algorithm for higher strength covering arrays.
Software Testing, Verification, and Reliability, 19:37â€“53, 2009.
[7] R. C. Bryce and C. J. Colbourn. Expected time to detection of interaction faults. Journal of Combinatorial Mathematics and Combinatorial Computing, 86:87â€“110, 2013.
[8] J. N. Cawse. Experimental design for combinatorial and high throughput materials development. GE
Global Research Technical Report, 29:769â€“781, 2002.
[9] M. A. Chateauneuf, C. J. Colbourn, and D. L. Kreher. Covering arrays of strength 3. Des. Codes
Crypt., 16:235â€“242, 1999.
[10] D. M. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG system: An approach to
testing based on combinatorial design. IEEE Transactions on Software Engineering, 23:437â€“44, 1997.
[11] M. B. Cohen. Designing test suites for software interaction testing. PhD thesis, The University of
Auckland, Department of Computer Science, 2004.
[12] C. J. Colbourn. Combinatorial aspects of covering arrays. Le Matematiche (Catania), 58:121â€“167, 2004.
[13] C. J. Colbourn. Covering array tables, 2005-2015. http://www.public.asu.edu/âˆ¼ccolbou/src/tabby.
[14] C. J. Colbourn. Covering arrays and hash families. In Information Security and Related Combinatorics,
NATO Peace and Information Security, pages 99â€“136. IOS Press, 2011.
[15] C. J. Colbourn. Conditional expectation algorithms for covering arrays. Journal of Combinatorial
Mathematics and Combinatorial Computing, 90:97â€“115, 2014.
[16] R. Diestel. Graph Theory. Graduate Texts in Mathematics. Springer, fourth edition, 2010.
[17] M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NPCompleteness. W. H. Freeman & Co., New York, NY, USA, 1979.
[18] A. P. Godbole, D. E. Skipper, and R. A. Sunley. t-covering arrays: upper bounds and Poisson approximations. Combinatorics, Probability and Computing, 5:105â€“118, 1996.
[19] B. Haeupler, B. Saha, and A. Srinivasan. New constructive aspects of the LovaÌsz local lemma. J. ACM,
58(6):Art. 28, 28, 2011.
[20] A. S. Hedayat, N. J. A. Sloane, and J. Stufken. Orthogonal Arrays. Springer-Verlag, New York, 1999.
[21] D. S. Johnson. Approximation algorithms for combinatorial problems. J. Comput. System Sci., 9:256â€“
278, 1974.
[22] G. O. H. Katona. Two applications (for search theory and truth functions) of Sperner type theorems.
Periodica Math., 3:19â€“26, 1973.
[23] D. Kleitman and J. Spencer. Families of k-independent sets. Discrete Math., 6:255â€“262, 1973.
[24] D. R. Kuhn, R. Kacker, and Y. Lei. Introduction to Combinatorial Testing. CRC Press, 2013.
[25] D. R. Kuhn and M. Reilly. An investigation of the applicability of design of experiments to software
testing. In Proc. 27th Annual NASA Goddard/IEEE Software Engineering Workshop, pages 91â€“95, Los
Alamitos, CA, 2002. IEEE.

29

[26] D. R. Kuhn, D. R. Wallace, and A. M. Gallo. Software fault interactions and implications for software
testing. IEEE Trans. Software Engineering, 30:418â€“421, 2004.
[27] L. LovaÌsz. On the ratio of optimal integral and fractional covers. Discrete Math., 13(4):383â€“390, 1975.
[28] K. Meagher and B. Stevens. Group construction of covering arrays. J. Combin. Des., 13:70â€“77, 2005.
[29] R. A. Moser. A constructive proof of the LovaÌsz local lemma. In STOCâ€™09â€”Proceedings of the 2009
ACM International Symposium on Theory of Computing, pages 343â€“350. ACM, New York, 2009.
[30] R. A. Moser and G. Tardos. A constructive proof of the general LovaÌsz local lemma. J. ACM, 57(2):Art.
11, 15, 2010.
[31] C. Nie and H. Leung. A survey of combinatorial testing. ACM Computing Surveys, 43(2):#11, 2011.
[32] A. H. Ronneseth and C. J. Colbourn. Merging covering arrays and compressing multiple sequence
alignments. Discrete Appl. Math., 157:2177â€“2190, 2009.
[33] K. Sarkar and C. J. Colbourn. Upper bounds on the size of covering arrays. ArXiv e-prints.
[34] G. Seroussi and N. H. Bshouty. Vector sets for exhaustive testing of logic circuits. IEEE Trans. Inform.
Theory, 34:513â€“522, 1988.
[35] S. K. Stein. Two combinatorial covering theorems. J. Combinatorial Theory Ser. A, 16:391â€“397, 1974.
[36] J. Torres-Jimenez, H. Avila-George, and I. Izquierdo-Marquez. A two-stage algorithm for combinatorial
testing. Optimization Letters, pages 1â€“13, 2016.

30

Abstract:
Covering array generation is a key issue in combinatorial testing. A number of researchers have been applying greedy algorithms for covering array construction. A greedy framework has been built to integrate most greedy algorithms and evaluate new approaches derived from this framework. However, this framework is affected by multiple factors, which makes its deployment and optimization very challenging. In order to identify the best configuration, we propose a search method that combines pairwise coverage with either base choice or hill climbing techniques. We conduct three different groups of experiments based on six decisions of the greedy framework. The influence of these decisions and their interactions are studied systematically, and the selected greedy algorithm for covering array generation is shown to be better than the existing greedy algorithms.Test Algebra for Combinatorial Testing
Wei-Tek Tsaiâˆ— , Charles J. Colbournâˆ—â€  , Jie Luoâ€  , Guanqiu Qiâˆ— , Qingyang Liâˆ— , Xiaoying Baiâ€¡
âˆ— School

of Computing, Informatics, and Decision Systems Engineering
Arizona State University, Tempe, AZ, USA
â€  State Key Laboratory of Software Development Environment
School of Computer Science and Engineering,
Beihang University, Beijing, China
â€¡ Department of Computer Science and Technology, INLIST
Tsinghua University, Beijing, China
{wtsai, colbourn}@asu.edu, luojie@nlsde.buaa.edu.cn
{guanqiuq, qingyan2b}@asu.edu, baixy@tsinghua.edu.cn

Abstractâ€”This paper proposes a new algebraic system, Test
Algebra (T A), for identifying faults in combinatorial testing for
SaaS (Software-as-a-Service) applications. SaaS as a part of cloud
computing is a new software delivery model, and mission-critical
applications are composed, deployed, and executed in cloud
platforms. Testing SaaS applications is a challenging task because
new applications need to be tested when they are composed
before they can be deployed for execution. Combinatorial testing
algorithms can be used to identify faulty configurations and
interactions from 2-way all the way to k-way where k is the
number of components in the application. The T A defines rules
to identify faulty configurations and interactions. Using the rules
defined in the T A, a collection of configurations can be tested
concurrently in different servers and in any order and the results
obtained will be still same due to the algebraic constraints.
Index Termsâ€”Combinatorial testing, algebra, SaaS

I. I NTRODUCTION
Software-as-a-Service (SaaS) is a new software delivery
model. SaaS often supports three features: customization,
multi-tenancy architecture (MTA), and scalability. MTA means
using one code base to develop multiple tenant applications,
and each tenant application essentially is a customization
of the base code [12]. A SaaS system often also supports
scalability as it can supply additional computing resources
when the workload is heavy. Tenantsâ€™ applications are often
customized by using components stored in the SaaS database
[14], [1], [11] including GUI, workflow, service, and data
components.
Once tenant applications are composed, they need to be tested, but a SaaS system can have millions of components, and
hundreds of thousands of tenant applications. Testing tenant
applications becomes a challenge as new tenant applications
and components are added into the SaaS system continuously.
New tenant applications are added on a daily basis while other
tenant applications are running on the SaaS platform. As new
tenant applications are composed, new components are added
into the SaaS system. Each tenant application represents a
customer for the SaaS system, and thus it needs to be tested.
Combinatorial testing is a popular testing technique to test
an application with different configurations. It often assumes
that each component within a configuration has been tested

already, but interactions among components in the configuration may cause failures. Traditional combinatorial testing
techniques often focus on test case generation to detect the
presence of faults, but fault location is an active research area.
Each configuration needs to be tested, as each configuration
represents a tenant application. Traditional combinatorial testing such as AETG [2] can reveal the existence of faults by
using few test cases to support t-way coverage for t â‰¥ 2. But
knowing the existence of a fault does not indicate which tway interactions are faulty. When the problem size is small,
an engineer can identify faults. However, when the problem
is large, it can be a challenge to identify faults.
As the IT industries moves to Big Data and cloud computing where hundreds of thousand processors are available,
potentially, a large number of processors with distributed
databases can be used to perform large-scale combinatorial testing. Furthermore, they also provide concurrent and
asynchronous computing mechanisms such as MapReduce,
automated redundancy and recovery management, automated
resource provisioning, and automated migration for scalability.
These capabilities provide significant computing power that
was not available before. One simple way of performing
combinatorial testing in a cloud environment is:
1) Partition the testing tasks;
2) Allocate these testing tasks to different processors in the
cloud platform for test execution;
3) Collect results done by these processors.
However, this is not efficient as while the number of computing
and storage resources have increased significantly, the number
of combinations to be considered is still too high. For example,
a large SaaS system may have millions of components, and
testing all of these combinations can still consume all the
resources in a cloud platform. Two ways to improve this
approach and both are based on learning from the previous
test results:
â€¢

â€¢

Devise a mechanism to merge test results from different
processors so that testing results can be merged quickly,
and detect any inconsistency in testing;
Based on the existing testing results, eliminate any con-

figurations or interactions from future testing.
Due to the asynchronous and autonomous nature of cloud
computing, test results may arrive asynchronously and autonomously, and this necessitates a new testing framework.
This paper proposes a new algebraic system, Test Algebra
(TA), to facilitate concurrent combinatorial testing. The key
feature of TA is that the algebraic rules follow the combinatorial structure, and thus can track the test results obtained. The
TA can then be used to determine whether a tenant application
is faulty, and which interactions need to be tested. The TA
is an algebraic system in which elements and operations are
formally defined. Each element represents a unique component
in the SaaS system, and a set of components represents a
tenant application. Assuming each component has been tested
by developers, testing a tenant application is equivalent to
ensuring that there is no t-way interaction faults for t â‰¥ 2
among the elements in a set.
The TA uses the principle that if a t-way interaction is
faulty, every (t + 1)-way interaction that contains the t-way
interaction as a subset is necessarily faulty. The TA provides
guidance for the testing process based on test results so far.
Each new test result may indicate if additional tests are needed
to test a specific configuration. The TA is an algebraic system,
primarily intended to track the test results without knowing
how these results were obtained. Specifically, it does not
record the execution sequence of previously executed test
cases. Because of this, it is possible to allocate different
configurations to different processors for execution in parallel
or in any order, and the test results are merged following
the TA rules. The execution order and the merge order do
not affect the merged results if the merging follows the TA
operation rules.
This paper is structured as follows: Section II discusses the
related work; Section III proposes TA and shows its details;
and Section IV concludes this paper. Appendix provides proofs
of TA associativity properties.
II. R ELATED W ORK
SaaS testing is a new research topic [14], [4], [10]. Using
policies and metadata, test cases can be generated to test
SaaS applications. Testing can be embedded in the cloud
platform where tenant applications are run [14]. Gao proposed
a framework for testing cloud applications [4], and proposed
a scalability measure for testing cloud application scalability.
Another scalability measure was proposed by [10].
Testing all combinations of inputs and preconditions is not
feasible, even with a simple product [6], [8]. The number
of defects in a software product can be large, and defects
occurring infrequently are difficult to find [15]. Combinatorial
test design is used to identify a small number of tests needed
to get the coverage of important combinations. Combinatorial
test design methods enable one to build structure variation
into test cases for having greater test coverage with fewer test
cases.
Determining the presence of faults caused by a small
number of interacting elements has been extensively studied

in component-based software testing. When interactions are
to be examined, testing involves a combination-based strategy
[5]. When every interaction among t or fewer elements is to
be tested, methods have been developed that provide pairwise
or t-way coverage. Among the early methods, AET G [2]
popularized greedy one-test-at-a-time methods for constructing
test suites. In the literature, the test suite is usually called a
covering array, defined as follows. Suppose that there are k
configurable elements, numbered from 1 to k. Suppose that
for element c, there are vc valid options. A t-way interaction
is a selection of t of the k configurable elements, and a valid
option for each. A test selects a valid option for every element,
and it covers a t-way interaction if, when one restricts the
attention to the t selected elements, each has the same option
in the interaction as it does in the test.
A covering array of strength t is a collection of tests so
that every t-way interaction is covered by at least one of the
tests. Covering arrays reveal faults that arise from improper
interaction of t or fewer elements [9]. There are numerous
computational and mathematical approaches for construction
of covering arrays with a number of tests as small as possible
[3], [7].
If a t-way interaction causes a fault, then executing all
tests of a covering array will reveal the presence of at least
one faulty interaction. SaaS testing is interested in identifying
those interactions that are faulty including their numbers and
locations, as faulty configurations cannot be used in tenant
applications. Furthermore, the number and location of faults
keep on changing as new components can be added into
the SaaS database continuously. By then executing each test,
certain interactions are known not to be faulty, while others
appear only in tests that reveal faults, and hence may be faulty.
At this point, a classification tree analysis builds decision trees
for characterizing possible sets of faults. This classification
analysis is then used either to permit a system developer to
focus on a small collection of possible faults, or to design
additional tests to further restrict the set of possible faults.
In [16], empirical results demonstrate the effectiveness of
this strategy at limiting the possible faulty interactions to a
manageable number. Assuming that interactions of more than
t elements do not produce faults, a covering array can use few
tests to certify that no fault arises from a t-way interaction.
The Adaptive Reasoning algorithm (AR) is a strategy to
detect faults in SaaS [13]. The algorithm uses earlier test
results to generate new test cases to detect faults in tenant
applications. It uses three principles:
â€¢

â€¢

â€¢

Principle 1: When a tenant application (or configuration)
fails the testing, there is at least one fault (but there may
be more) in the tenant configuration.
Principle 2: When a tenant application passes the testing,
there is no fault in the tenant configuration resulting
from a t-way interactions among components in the
configuration.
Principle 3: Whenever a configuration contains one or
more faulty interactions, it is faulty.

III. T EST A LGEBRA
Let C be a finite set of components. A configuration is
a subset T âŠ† C. One is concerned with determining the
operational status of configurations. To do this, one can
execute certain tests; every test is a configuration, but there
may be restrictions on which configurations can be used as
tests. If a certain test can be executed, its execution results in
an outcome of passed (operational) or failed (faulty).
When a test execution yields result, all configurations that
are subsets of the test are operational. However, when a test
execution yields a faulty result, one only knows that at least
one subset causes the fault, but it is unclear which of these
subsets caused the failure. Among a set of configurations that
may be responsible for faults, the objective is to determine,
which cause faults and which do not. To do this, one must
identify the set of candidates to be faulty. Because faults
are expected to arise from an interaction among relatively
few components, one considers t-way interactions. The t-way
interactions are It = {U âŠ† C : |U | = t}. Hence the goal is to
select tests, so that from the execution results of these tests,
one can ascertain the status of all t-way interactions for some
fixed small value of t.
Because interactions and configurations are represented as
subsets, one can use set-theoretic operations such as union, and
their associated algebraic properties such as commutativity,
associativity, and self-absorption. The structure of subsets and
supersets also plays a key role.
To permit this classification, one can use a valuation function V , so that for every subset S of components, V (S)
indicates the current knowledge about the operational status
consistent with the components in S. The focus is on determining V (S) whenever S is an interaction in I1 âˆª Â· Â· Â· âˆª It .
These interactions can have one of five states.
â€¢ Infeasible (X): For certain interactions, it may happen
that no feasible test is permitted to contain this interaction. For example, it may be infeasible to select two GUI
components in one configuration such that one says the
wall is GREEN but the other says RED.
â€¢ Faulty (F): If the interaction has been found to be faulty.
â€¢ Operational (P): Among the rest, if an interaction has
appeared in a test whose execution gave an operational
result, the interaction cannot be faulty.
â€¢ Irrelevant (N): For some feasible interactions, it may
be the case that certain interactions are not expected to
arise, so while it is possible to run a test containing the
interaction, there is no requirement to do so.
â€¢ Unknown (U): If neither of these occurs then the status
of the interaction is required but not currently known.
Any given stage of testing, an interaction has one of five
possible status indicators. These five status indicators are
ordered by X  F  P  N  U under a relation , and
it has a natural interpretation to be explained in a moment.
A. Learning from Previous Test Results
The motivation for developing an algebra is to automate
the deduction of the status of an interaction from the status

of tests and other interactions, particularly in combining the
status of two interactions. Specifically, one is often interested
in determining V (T1 âˆª T2 ) from V (T1 ) and V (T2 ). To do this,
a binary operation âŠ— on {X, F, P, N, U} can be defined, with
operation table as follows:
âŠ—
X
F
P
N
U

X
X
X
X
X
X

F
X
F
F
F
F

P
X
F
U
N
U

N
X
F
N
N
N

U
X
F
U
N
U

Using this definition, one can verify that the binary operation âŠ— has the following properties of commutativity and
associativity.
V (T1 ) âŠ— V (T2 ) = V (T2 ) âŠ— V (T1 ),
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
Using this operation, one observes that V (T1 âˆª T2 ) 
V (T1 ) âŠ— V (T2 ). It follows that
1) Every superset of an infeasible interaction is infeasible.
2) Every superset of a failed interaction is failed or infeasible.
3) Every superset of an irrelevant interaction is irrelevant,
failed, passed, or infeasible.
A set S is an X-implicant if V (S) = X but whenever
S 0 âŠ‚ S, V (S 0 ) â‰º X. The X-implicants provide a compact
representation for all interactions that are infeasible. Indeed
for any interaction T that contains an X-implicant, V (T ) = X.
Furthermore, a set S is an F-implicant if V (S) = F but
whenever S 0 âŠ‚ S, V (S 0 ) â‰º F. For any interaction T that
contains an F-implicant, V (T )  F. In the same way, a set S is
an N-implicant if V (S) = N but whenever S 0 âŠ‚ S, V (S 0 ) = U.
For any interaction T that contains an N-implicant, V (T )  N.
An analogous statement holds for passed interactions, but here
the implication is for subsets. A set S is a P-implicant if
V (S) = P but whenever S 0 âŠƒ S, V (S 0 )  F. For any
interaction T that is contained in a P-implicant, V (T ) = P.
Implicants are defined with respect to the current knowledge
about the status of interactions. When a t-way interaction is
known to be infeasible, failed, or irrelevant, it must contain
an X-, F-, or N-implicant. By repeatedly proceeding from tway to (t + 1)-way interactions, then, one avoids the need
for any tests for (t + 1)-way interactions that contain any
infeasible, failed, or irrelevant t-way interaction. Hence testing
typically proceeds by determining the status of the 1-way
interactions, then proceeding to 2-way, 3-way, and so on.
The operation âŠ— is useful in determining the implied status
of (t + 1)-way interactions from the computed results for
t-way interactions, by examining unions of the t-way and
smaller interactions and determining implications of the rule
that V (T1 âˆª T2 )  V (T1 ) âŠ— V (T2 ). Moreover, when adding
further interactions to consider, all interactions previously
tested that passed are contained in a P-implicant, and every
(t + 1) interaction contained in one of these interactions can
be assigned status P.

For example, V (a, b) = P, V (a, e) = X, and other 2way interactions exist as atomic interaction in TA. Based
on the defined âŠ— operation, values of t-way interactions can
be deduced from the atomic interactions and their contained
interactions, such as V (a, b, e)  V (a, b) âŠ— V (a, e) = X, i.e.
V (a, b, e) = X.
The 3-way interaction (a, b, c) can have inferred results from
2-way interactions (a, b), (a, c), (b, c). If any contained 2-way
interaction has value F, the determining value of 3-way is F,
without further testing needed. But if all values of contained
2-way interactions are P, (a, b, c) the interaction needs to be
tested. In this case, U needs to be changed to non-U such as F
or P, assuming the 3-way is not X or N.
B. Changing Test Result Status
When testing a configuration with n components, one
should test individual components, 2-way interactions, 3-way
interactions, all the way to n-way interactions. Since any
combination of interactions is relevant in this case, the status
of any interaction can be either X, F, P, or U. The status of a
configuration is determined by the status of all interactions.
1) If an interaction has status X (F), the configuration has
status X (F).
2) If all interactions have status P, the configuration has
status P.
3) If some interactions still have status U, further tests are
needed.
It is important to determine when an interaction with status
U can be deduced to have status F or P instead. It can never
obtain status X or N once having had status U.
To change U to P: An interaction is assigned status P if and
only if it is a subset of a test that leads to proper operation.
To change U to F: Consider the candidate T , one can
conclude that V (T ) = F if there is a test containing T that
yields a failed result, but for every other candidate interaction
T 0 that appears in this test, V (T 0 ) = P. In other words, the
only possible explanation for the failure is the failure of T .
C. Matrix Representation
Suppose that each individual component passed the testing.
Then the operation table starts from 2-way interactions, then
enlarges to t-way interactions step by step. During the procedure, many test results can be deduced from the existing results
following TA rules. For example, all possible configurations
of (a, b, c, d, e, f ) can be expressed in the form of matrix, or
operation table. First, we show the operation table for 2-way
interactions. The entries in the operation table are symmetric
and those on the main diagonal are not necessary. So only half
of the entries are shown.
As shown in Figure 1, 3-way interactions can be composed
by using 2-way interactions and components. Thus, following
the TA implication rules, the 3-way interactions operation table
is composed based on the results of 2-way combinations. Here,
(a, b, c, d, e, f ) has more 3-way interactions than 2-way
interactions. As seen in Figure 1, a 3-way interaction can be
obtained through different combinations of 2-way interactions

and components. For example, {a, b, c} = {a} âˆª {b, c} =
{b}âˆª{a, c} = {c}âˆª{a, b} = {a, b}âˆª{a, c} = {a, b}âˆª{b, c} =
{a, c} âˆª {b, c}. V (a) âŠ— V (b, c) = V (c) âŠ— V (a, b) = V (a, b) âŠ—
V (b, c) = PâŠ—P = U. But V (b)âŠ—V (a, c) = V (a, b)âŠ—V (a, c) =
V (b, c) âŠ— V (a, c) = P âŠ— F = F. As TA defines the order of
the five status indicators, the result should be the value with
highest order. So V (a, b, c) = F.
âŠ— a
a
b
c
d
e
f

b
P

c d e
F N X
P X N
F P
F

f
U
F
P
X
U

D. Merging Concurrent Testing Results
One way to achieve efficient testing is to allocate (overlapping or non-overlapping) tenant applications into different
clusters, and each cluster is sent to a different set of servers
for execution. Once each cluster completes its execution, the
test results can be merged. The testing results of a specific
interaction T in different servers should satisfy the following
constraints.
â€¢ If V (T ) = U in one cluster, then in other clusters, the
same V (T ) can be either F, P, N, or U.
â€¢ If V (T ) = N in one cluster, then in other clusters, the
same V (T ) can be either F, P, N, or U.
â€¢ If V (T ) = P in one cluster, then the same V (T ) can be
either P, N, or U in all clusters;
â€¢ If V (T ) = F in one cluster, then in other clusters, the
same V (T ) can be F, N, or U.
â€¢ If V (T ) = X in one cluster, then in other clusters, the
same V (T ) can be X only.
If these constraints are satisfied, then the testing results can
be merged. Otherwise, there must be an error in the testing
results. To represent this situation, a new status indicator, error
(E), is introduced and E  X. We define a binary operation âŠ•
on {E, X, F, P, N, U}, with operation table as follows:
âŠ•
E
X
F
P
N
U

E
E
E
E
E
E
E

X
E
X
E
E
E
E

F
E
E
F
E
F
F

P
E
E
E
P
P
P

N
E
E
F
P
N
U

U
E
E
F
P
U
U

âŠ• also has the properties of commutativity and associativity.
See Appendix for proof of associativity.
Using this operation, merging two testing results from two
different servers can be defined as Vmerged (T ) = Vcluster1 (T ) âŠ•
Vcluster2 (T ). The merge can be performed in any order due to
the commutativity and associativity of âŠ•, and if the constraints
of merge are satisfied and V (T ) = X, F, or P, the results cannot
be changed by any further testing or merging of test results
unless there are some errors in testing. If V (T ) = E, the testing

âˆª
a
b
c
..
.

a
(a)

b
(a, b)
(b)

c
(a, c)
(b, c)
(c)

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

f
(a, f )
(b, f )
(c, f )
..
.

(a, b)
(a, b)
(a, b)
(a, b, c)
..
.

(a, c)
(a, c)
(a, b, c)
(a, c)
..
.

(f )

(a, b, f )
(a, b)

(a, c, f ) Â· Â· Â·
(a, b, c) Â· Â· Â·
(a, c)
Â·Â·Â·
..
.

f
(a, b)
(a, c)
..
.
(b, c)
..
.

(b, c)
(a, b, c)
(b, c)
(b, c)
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

(b, c, f ) Â· Â· Â·
(a, b, c) Â· Â· Â·
(a, b, c) Â· Â· Â·
..
..
.
.
(b, c)
Â·Â·Â·
..
.

(e, f )
(a, e, f )
(b, e, f )
(c, e, f )
..
.
(e, f )
(a, b, e, f )
(a, c, e, f )
..
.
(b, c, e, f )
..
.
(e, f )

(e, f )
âŠ—
a
b
c
..
.
f
(a, b)
(a, c)
..
.

a

b
P

c
F
P

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

f
U
F
P
..
.

(a, b)
U
U
U
..
.

(a, c)
F
F
F
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

(b, c)
U
U
U
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

(e, f )
U
U
U
..
.

U

F
F

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

U
U
F
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

U
U
F
..
.

Â·Â·Â·
..
.

U
..
.

(b, c)
..
.
(e, f )
Fig. 1. 3-way interaction operation table

environment must be corrected and tests executed again after
fixing the error(s) in testing. For example, Vcluster1 (a, c, e) = X
and Vcluster2 (a, c, e) = F, so Vmerged (a, c, e) = X âŠ• F = E.
It means that there is something wrong with the tests of
interaction (a, c, e), and the problem must be fixed before
doing further testing.
Following the âŠ• associative rule, one can derive the following.
V1 (T ) âŠ• V2 (T ) âŠ• V3 (T )
= (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T )
= V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T ))
= V1 (T ) âŠ• V2 (T ) âŠ• V3 (T ) âŠ• V3 (T )
= (V1 (T ) âŠ• V2 (T )) âŠ• (V2 (T ) âŠ• V3 (T ))
= ((V1 (T ) âŠ• V2 (T )) âŠ• V2 (T )) âŠ• V3 (T )
= (V3 (T ) âŠ• V2 (T )) âŠ• (V3 (T ) âŠ• V1 (T ))
Thus the âŠ• rule allows one to partition the configurations
into different sets for different servers to run testing, and
these sets do not need to be non-overlapping. In conventional
cloud computing operations such as MapReduce, data should
not overlap, otherwise incorrect data may be produced. For
example, counting the items in a set can be performed by
MapReduce, but data allocated to different servers cannot
overlap, otherwise items may be counted more than once. In
TA, this is not a concern due to the nature of the TA operations

âŠ•. Once the results are available from each server, the testing
results can be merged either incrementally, in parallel, or in
any order. Furthermore, test results can be merged repeatedly
without changing the final results.
Using this approach, one can test say 10 batches of 100 tenant applications each, for a total of 1, 000 tenant applications.
Once this is done, another batch of 1, 000 tenant applications
can be tested with each 100 tenant application allocated to a
server for execution. In this way, after running 100 batches,
100, 000 tenant applications can be evaluated completely.
The following example illustrates the testing process of
fifteen configurations. According to the definition of configuration testing, all feasible interactions should be tested. For
simplicity, assume that only interaction (c, d, f ) is faulty, and
only interaction (c, d, e) is infeasible, and all other interactions
pass the testing. If one assigns configurations 1, 3, 5, 7, 9, 11,
13, 15 into Server1 , configurations 2, 4, 6, 8, 10, 12, 14 into
Server2 , and 4-11 configurations into Server3 .
If Server1 and Server3 do their own testing first, Server2
can reuse test results of interactions from them to eliminate
interactions that need to be tested. For example, when testing
2-way interactions of configuration (b, c, d, f ) in Server2 ,
it can reuse the test results of (b, c), (b, d) of configuration
(b, c, d, e) from Server3 , (b, f ) of configuration (a, b, c, f )

from Server1 . They are all passed, and it can reuse the test
results of (b, c, d) of configuration (a, b, c, d) from Server1 ,
(b, c, f ) of configuration (a, b, c, f ) from Server1 , (b, d, f )
of configuration (a, b, d, f ) from Server1 , and (c, d, f ) of
configuration (a, c, d, f ) from Server3 . Because (c, d, f ) is
faulty, it can deduce that 4-way interaction (b, c, d, f ) is also
faulty. For the sets of configuration that are overlapping, their
returned test results from different servers are the same. The
merged results of these results also stay the same.
Not only interactions, sets of configurations, CS1 , CS2 ,
. . . , CSK can be allocated to different processors (or clusters)
for testing, and the test results can then be merged. The sets
can be non-overlapping or overlapping, and the merge process
can be arbitrary. For example, say the result of CSi is RCSi ,
the merge process can be (Â· Â· Â· ((((RCS1 + RCS2 ) + RCS3 ) +
RCS4 ) + Â· Â· Â· + RCSK ), or (Â· Â· Â· ((((RCSK + RCSkâˆ’1 ) +
RCSkâˆ’2 ) + Â· Â· Â· + RCS1 ), or any other sequence that includes
all RCSi , for i = 1 to K. This is true because RCS is simply
a set of V (Tj ) for any intercation Tj in the configuration CSi .
(a,b,c,d)
(a,b,c,e)
(a,b,c,f)
(a,b,d,e)
(a,b,d,f)
(a,b,e,f)
(a,c,d,e)
(a,c,d,f)
(a,c,e,f)
(a,d,e,f)
(b,c,d,e)
(b,c,d,f)
(b,c,e,f)
(b,d,e,f)
(c,d,e,f)

Server1
P

Server2

Server3

P
P
P
P
P
X
F
P
P
X
F
P
P
X

P
P
P
X
F
P
P
X

IV. C ONCLUSION
This paper proposes TA to address SaaS combinatorial
testing. The TA provides a foundation for concurrent combinatorial testing. The TA has two operations and test results
can have five states with a priority. By using the TA operations, many combinatorial tests can be eliminated as the
TA identifies those interactions that need not be tested. Also
the TA defines operation rules to merge test results done by
different processors, so that combinatorial tests can be done in
a concurrent manner. The TA rules ensure that either merged
results are consistent or a testing error has been detected so
that retest is needed. In this way, large-scale combinatorial
testing can be carried out in a cloud platform with a large
number of processors to perform test execution in parallel to
identify faulty interactions.
ACKNOWLEDGMENT

This project is sponsored by U.S. National Science Foundation project DUE 0942453 and National Science Foundation
M erged Results China (No.61073003), National Basic Research Program of
China (No.2011CB302505), and the State Key Laboratory of
P
Software Development Environment (No. SKLSDE-2012ZXP
18), and Fujitsu Laboratory.
P
P
R EFERENCES
P
[1] X. Bai, M. Li, B. Chen, W.-T. Tsai, and J. Gao. Cloud Testing Tools. In
P
Proceedings of IEEE 6th International Symposium on Service Oriented
X
System Engineering (SOSE), pages 1â€“12, Irvine, CA, USA, 2011.
F
[2] D. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The
AETG System: An Approach to Testing Based on Combinatorial Design.
P
Journal of IEEE Transactions on Software Engineering, 23:437â€“444,
P
1997.
X
[3] C. J. Colbourn. Covering arrays and hash families. In D. Crnkovic and
V. D. Tonchev, editors, Information Security, Coding Theory and Related
F
Combinatorics, volume 29 of NATO Science for Peace and Security
P
Series - D: Information and Communication Security. IOS Press, 2011.
P
[4] J. Gao, X. Bai, and W.-T. Tsai. SaaS Performance and Scalability
Evaluation in Cloud. In Proceedings of The 6th IEEE International
X

E. Modified Testing Process
Perform 2-way interaction testing first. Before going on to 3way interaction testing, use the results of 2-way testing to eliminate cases. The testing process stops when finishing testing all
t-way interactions. The analysis of t-way interactions is based
on the P T Rs of all (t âˆ’ i)-way interactions for 1 â‰¤ i < t.
The superset of infeasible, irrelevant, and faulty test cases do
not need to be tested. The test results of the superset can be
obtained by TA operations and must be infeasible, irrelevant,
or faulty. But the superset of test cases with unknown indicator
must be tested. In this way, a large repeating testing workload
can be reduced.
For n components, all t-way interactions for t â‰¥ 2
are composed by 2-way, 3-way, ..., t-way interactions. In
n components combinatorial
 testing, the number of 2-way
interactions is equal to n2 . In general, the number of t-way
n
interactions
are treated when

is equal to t . More interactions
n
n
n
.
The
total number
>
,
which
happens
when
t
â‰¤
t
tâˆ’1
 2
Pt
of interactions examined is i=2 ni .

Symposium on Service Oriented System Engineering, SOSE â€™11, 2011.
[5] M. Grindal, J. Offutt, and S. F. Andler. Combination Testing Strategies:
A Survey. Software Testing, Verification, and Reliability, 15:167â€“199,
2005.
[6] C. Kaner, J. Falk, and H. Q. Nguyen. Testing Computer Software, 2nd
Edition. Wiley, New York, NY, USA, 1999.
[7] V. V. Kuliamin and A. A. Petukhov. A Survey of Methods for
Constructing Covering Arrays. Journal of Program Computer Software,
37(3):121â€“146, may 2011.
[8] T. Muller and D. Friedenberg. Certified Tester Foundation Level
Syllabus. Journal of International Software Testing Qualifications
Board.
[9] A. A. Porter, C. Yilmaz, A. M. Memon, D. C. Schmidt, and B. Natarajan.
Skoll: A Process and Infrastructure for Distributed Continuous Quality
Assurance. IEEE Transactions on Software Engineering, 33(8):510â€“525,
2007.
[10] W.-T. Tsai, Y. Huang, X. Bai, and J. Gao. Scalable Architecture for
SaaS. In Proceedings of 15th IEEE International Symposium on Object
Component Service-oriented Real-time Distributed Computing, ISORC
â€™12, Apr. 2012.
[11] W.-T. Tsai, Y. Huang, and Q. Shao. Testing the Scalability of SaaS Applications. In Proceedings of IEEE International Conference on ServiceOriented Computing and Applications (SOCA), pages 1â€“4, Irvine, CA,
USA, 2011.
[12] W.-T. Tsai, Y. Huang, Q. Shao, and X. Bai. Data Partitioning and
Redundancy Management for Robust Multi-Tenancy SaaS. International
Journal of Software and Informatics (IJSI), 4(3):437â€“471, 2010.

[13] W.-T. Tsai, Q. Li, C. J. Colbourn, and X. Bai. Adaptive Fault Detection
for Testing Tenant Applications in Multi-Tenancy SaaS Systems. In
Proceedings of IEEE International Conference on Cloud Engineering
(IC2E), March 2013.
[14] W.-T. Tsai, Q. Shao, and W. Li. OIC: Ontology-based Intelligent
Customization Framework for SaaS. In Proceedings of International
Conference on Service Oriented Computing and Applications(SOCAâ€™10),
Perth, Australia, Dec. 2010.
[15] Wikipedia. Software Testing, 2013.
[16] C. Yilmaz, M. B. Cohen, and A. Porter. Covering Arrays for Efficient
Fault Characterization in Complex Configuration Spaces. In Proceedings
of the 2004 ACM SIGSOFT International Symposium on Software
Testing and Analysis, ISSTA â€™04, pages 45â€“54, New York, NY, USA,
2004. ACM.

A PPENDIX
The associativity of binary operation âŠ—.
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
Proof: We will prove this property in the following cases.
(1) At least one of V (T1 ), V (T2 ), and V (T3 ) is X. Without
loss of generality, suppose that V (T1 ) = X, then according
to the operation table of âŠ—, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) =
X âŠ— (V (T2 ) âŠ— V (T3 )) = X, (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) =
(X âŠ— V (T2 )) âŠ— V (T3 ) = X âŠ— V (T3 ) = X. Thus, in this case,
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
(2) V (T1 ), V (T2 ), and V (T3 ) are not X and at least one
of V (T1 ), V (T2 ), and V (T3 ) is F. Without loss of generality,
suppose that V (T1 ) = F, then according to the operation table
of âŠ—, the value of V (T2 ) âŠ— V (T3 ) can only be F, N or U.
So V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = F âŠ— (V (T2 ) âŠ— V (T3 )) = F,
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) = (F âŠ— V (T2 )) âŠ— V (T3 ) = F âŠ—
V (T3 ) = F. Thus, in this case, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) =
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
(3) V (T1 ), V (T2 ), and V (T3 ) are not X or F and at least one
of V (T1 ), V (T2 ), and V (T3 ) is N. Without loss of generality,
suppose that V (T1 ) = N, then according to the operation table
of âŠ—, the value of V (T2 ) âŠ— V (T3 ) can only be N or U. So
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = N âŠ— (V (T2 ) âŠ— V (T3 )) = N,
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) = (N âŠ— V (T2 )) âŠ— V (T3 ) = N âŠ—
V (T3 ) = N. Thus, in this case, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) =
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
(4) V (T1 ), V (T2 ), and V (T3 ) are not X, F or N. In this case,
V (T1 ), V (T2 ), and V (T3 ) can only be P or U. According to the
operation table of âŠ—, the value of V (T1 )âŠ—V (T2 ) and V (T2 )âŠ—
V (T3 ) are U. So V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = V (T1 ) âŠ— U = U,
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) = U âŠ— V (T3 ) = U. Thus, in this
case, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
The associativity of binary operation âŠ•.
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ).
Proof: We will prove this property in the following cases.
(1) One of V1 (T ), V2 (T ), and V3 (T ) is E. Without loss
of generality, suppose that V1 (T ) = E, then according to the
operation table of âŠ•, V1 (T )âŠ•(V2 (T )âŠ•V3 (T )) = EâŠ—(V2 (T )âŠ•
V3 (T )) = E, (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = (E âŠ• V2 (T )) âŠ•
V3 (T ) = EâŠ•V3 (T ) = E. Thus, in this case, V1 (T )âŠ•(V2 (T )âŠ•
V3 (T )) = (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ).

(2) V1 (T ), V2 (T ), and V3 (T ) are not E, and there is a pair
of V1 (T ), V2 (T ), and V3 (T ) does not satisfy the constrains.
Without loss of generality, suppose that V1 (T ) and V2 (T ) does
not satisfy the constrains, then according to the operation table
of âŠ•, V1 (T ) âŠ• V2 (T ) = E. So (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) =
E âŠ• V3 (T ) = E. Since V1 (T ) and V2 (T ) does not satisfy the
constrains, there can be two cases: (a) one of them is X and
the other is not, or (b) one of them is P and the other is F.
(a) If V1 (T ) = X, then V2 (T ) âŠ• V3 (T ) cannot be X because
V2 (T ) cannot be X. Thus, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E. If
V2 (T ) = X, then V2 (T ) âŠ• V3 (T ) 6= X can only be E or X.
Since V1 (T ) cannot be X, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E.
(b) If V1 (T ) = P and V2 (T ) = F, then V2 (T ) âŠ• V3 (T )
can only be E or F. Thus, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E. If
V1 (T ) = F and V2 (T ) = P, then V2 (T ) âŠ• V3 (T ) can only be
E or P. Thus, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E.
Thus, in this case, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = (V1 (T ) âŠ•
V2 (T )) âŠ• V3 (T ).
(3) V1 (T ), V2 (T ), and V3 (T ) are not E, and V1 (T ), V2 (T ),
and V3 (T ) satisfy the constrains.
(a) One of V1 (T ), V2 (T ), and V3 (T ) is X. Without loss of
generality, suppose that V1 (T ) = X, then V2 (T ) = V3 (T ) = X.
So V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = X âŠ• (X âŠ• X) = X âŠ• X = X and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = (X âŠ• X) âŠ• X = X âŠ• X = X.
(b) V1 (T ), V2 (T ), and V3 (T ) are not X, and one of V1 (T ),
V2 (T ), and V3 (T ) is F. Without loss of generality, suppose
that V1 (T ) = F, then V2 (T ) and V3 (T ) can only be F, N,
or U. According to operation table of âŠ•, V2 (T ) âŠ• V3 (T ) can
only be F, N, or U, and V1 (T ) âŠ• V2 (T ) can only be F. So
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = F âŠ• (V2 (T ) âŠ• V3 (T )) = F and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = F âŠ• V3 (T ) = F.
(c) V1 (T ), V2 (T ), and V3 (T ) are not X or F, and one of
V1 (T ), V2 (T ), and V3 (T ) is P. Without loss of generality,
suppose that V1 (T ) = P, then V2 (T ) and V3 (T ) can only be
P, N, or U. According to operation table of âŠ•, V2 (T ) âŠ• V3 (T )
can only be P, N, or U, and V1 (T ) âŠ• V2 (T ) can only be F. So
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = P âŠ• (V2 (T ) âŠ• V3 (T )) = P and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = P âŠ• V3 (T ) = P.
(d) V1 (T ), V2 (T ), and V3 (T ) are not X, F or P, and one
of V1 (T ), V2 (T ), and V3 (T ) is U. Without loss of generality,
suppose that V1 (T ) = U, then V2 (T ) and V3 (T ) can only be
N, or U. According to operation table of âŠ•, V2 (T ) âŠ• V3 (T )
can only be N, or U, and V1 (T ) âŠ• V2 (T ) can only be U. So
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = U âŠ• (V2 (T ) âŠ• V3 (T )) = U and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = U âŠ• V3 (T ) = U.
(e) V1 (T ), V2 (T ), and V3 (T ) are N. V1 (T ) âŠ• (V2 (T ) âŠ•
V3 (T )) = N âŠ• (N âŠ• N) = N âŠ• N = N and (V1 (T ) âŠ• V2 (T )) âŠ•
V3 (T ) = (N âŠ• N) âŠ• N = N âŠ• N = N.
Thus, in this case, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = (V1 (T ) âŠ•
V2 (T )) âŠ• V3 (T ).

Partial Covering Arrays: Algorithms and
Asymptotics

arXiv:1605.02131v1 [math.CO] 7 May 2016

Kaushik Sarkar1 , Charles J. Colbourn1 , Annalisa De Bonis2 , and Ugo Vaccaro2
1

2

CIDSE, Arizona State University, U.S.A.
Dipartimento di Informatica, University of Salerno, Italy

Abstract. A covering array CA(N ; t, k, v) is an N Ã— k array with entries in {1, 2, . . . , v}, for which every N Ã— t subarray contains each ttuple of {1, 2, . . . , v}t among its rows. Covering arrays find application
in interaction testing, including software and hardware testing, advanced
materials development, and biological systems. A central question is to
determine or bound CAN(t, k, v), the minimum number N of rows of a
CA(N ; t, k, v). The well known bound CAN(t, k, v) = O((t âˆ’ 1)v t log k)
is not too far from being asymptotically optimal. Sensible relaxations of
the covering requirement arise when (1) the set {1, 2, . . . , v}t need only
be contained among the rows of at least (1 âˆ’ ) kt of the N Ã— t subarrays
and (2) the rows of every N Ã— t subarray need only contain a (large)
subset of {1, 2, . . . , v}t . In this paper, using probabilistic methods, significant improvements on the covering array upper bound are established
for both relaxations, and for the conjunction of the two. In each case,
a randomized algorithm constructs such arrays in expected polynomial
time.

1

Introduction

Let [n] denote the set {1, 2, . . . , n}. Let N, t, k, and v be integers such that
k â‰¥ t â‰¥ 2 and v â‰¥ 2. Let A be an N Ã— k array where each entry is from the set
[v]. For I = {j1 , . . . , jÏ } âŠ† [k] where j1 < . . . < jÏ , let AI denote the N Ã— Ï array
in which AI (i, `) = A(i, j` ) for 1 â‰¤ i â‰¤ N and 1 â‰¤ ` â‰¤ Ï; AI is the projection of
A onto the columns in I.
A covering array CA(N ; t, k, v) is an N Ã— k array A with each entry from

t
[v] so that for each t-set of columns C âˆˆ [k]
t , each t-tuple x âˆˆ [v] appears
as a row in AC . The smallest N for which a CA(N ; t, k, v) exists is denoted by
CAN(t, k, v).
Covering arrays find important application in software and hardware testing
(see [22] and references therein). Applications of covering arrays also arise in
experimental testing for advanced materials [4], inference of interactions that
regulate gene expression [29], fault-tolerance of parallel architectures [15], synchronization of robot behavior [17], drug screening [30], and learning of boolean
functions [11]. Covering arrays have been studied using different nomenclature,
as qualitatively independent partitions [13], t-surjective arrays [5], and (k, t)universal sets [19], among others. Covering arrays are closely related to hash
families [10] and orthogonal arrays [8].

2

Background and Motivation

The exact or approximate determination of CAN(t, k, v) is central in applications
of covering arrays, but remains an open problem. For fixed t and v, only when
t = v = 2 is CAN(t, k, v) known precisely for infinitely many values of k. Kleitman
and Spencer [21] and Katona [20] independently
 proved that the largest k for
N âˆ’1
which a CA(N ; 2, k, 2) exists satisfies k = dN/2e
. When t = 2, Gargano, KoÌ‹rner,
and Vaccaro [13] establish that
CAN(2, k, v) =

v
log k(1 + o(1)).
2

(1)

(We write log for logarithms base 2, and ln for natural logarithms.) Several researchers [2,5,14,16] establish a general asymptotic upper bound on CAN(t, k, v):
CAN(t, k, v) â‰¤

tâˆ’1
log k(1 + o(1)).
t
log vtvâˆ’1

(2)

A slight improvement on (2) has recently been proved [12,28]. An (essentially)
equivalent but more convenient form of (2) is:
CAN(t, k, v) â‰¤ (t âˆ’ 1)v t log k(1 + o(1)).

(3)

A lower bound on CAN(t, k, v) results from the inequality CAN(t, k, v) â‰¥ v Â·
CAN(t âˆ’ 1, k âˆ’ 1, v) obtained by derivation, together with (1), to establish that
CAN(t, k, v) â‰¥ v tâˆ’2 Â· CAN(2, k âˆ’ t + 2, v) = v tâˆ’2 Â· v2 log(k âˆ’ t + 2)(1 + o(1)). When
t
k < 1, we obtain:
CAN(t, k, v) = â„¦(v tâˆ’1 log k).
(4)
Because (4) ensures that the number of rows in covering arrays can be considerable, researchers have suggested the need for relaxations in which not all
interactions must be covered [7,18,23,24] in order to reduce the number of rows.
The practical relevance is that each row corresponds to a test to be performed,
adding to the cost of testing.
For example, an array covers a t-set of columns when it covers each of the
v t interactions on this t-set. Hartman and Raskin [18] consider arrays with a
fixed number of rows that cover the maximum number of t-sets of columns. A
similar question was also considered in [24]. In [23,24] a more refined measure of
the (partial) coverage of an N Ã— k array A is introduced. For a given q âˆˆ [0, 1],
let Î±(A, q) be the number of N Ã— t submatrices of A with the property that at
least qv t elements
of [v]t appear in their set of rows; the (q, t)-completeness of A

k
is Î±(A, q)/ t . Then for practical purposes one wants â€œhighâ€ (q, t)-completeness
with few rows.
In these works, no theoretical results on partial coverage appear to have been
stated; earlier contributions focus on experimental investigations of heuristic
construction methods. Our purpose is to initiate a mathematical investigation
of arrays offering â€œpartialâ€ coverage. More precisely, we address:

â€“ Can one obtain a significant improvement on the upper bound (3) if the set
[v]t is only required to be contained among the rows of at least (1 âˆ’ ) kt
subarrays of A of dimension N Ã— t?
â€“ Can one obtain a significant improvement if, among the rows of every N Ã— t
subarray of A, only a (large) subset of [v]t is required to be contained?
â€“ Can one obtain a significant improvement if the set [v]t is only required to be
contained among the rows of at least (1 âˆ’ ) kt subarrays of A of dimension

N Ã— t, and among the rows of each of the  kt subarrays that remain, a
(large) subset of [v]t is required to be contained?
We answer these questions both theoretically and algorithmically in the following
sections.

3

Partial Covering Arrays

When 1 â‰¤ m â‰¤ v t , a partial m-covering array, PCA(N ; t, k, v, m), is an N Ã— k

array A with each entry from [v] so that for each t-set of columns C âˆˆ [k]
t , at
least m distinct tuples x âˆˆ [v]t appear as rows in AC . Hence a covering array
CA(N ; t, k, v) is precisely a partial v t -covering array PCA(N ; t, k, v, v t ).
Theorem 1. For integers t, k, v, and m where k â‰¥ t â‰¥ 2, v â‰¥ 2 and 1 â‰¤ m â‰¤ v t
there exists a PCA(N ; t, k, v, m) with
n  t o
v
ln kt mâˆ’1
 .

(5)
Nâ‰¤
vt
ln mâˆ’1
.
Proof. Let r = v t âˆ’m+1, and A be a random N Ã—k array where each
 entry is chosen independently from [v] with uniform probability. For C âˆˆ [k]
t , let BC denote
the event that at least r tuples from [v]t are missing in AC . The probability that
N
a particular r-set of tuples from [v]t is missing in AC is 1 âˆ’ vrt . Applying the
N
t
union bound to all r-sets of tuples from [v]t , we obtain Pr[BC ] â‰¤ vr 1 âˆ’ vrt .
By linearity of expectation, the expected number of t-sets C for which AC misses
 t
N
at least r tuples from [v]t is at most kt vr 1 âˆ’ vrt . When A has at least
n
o
vt
ln (kt)(mâˆ’1
)
rows this expected number is less than 1. Therefore, an array A
t
v
ln( mâˆ’1
)

exists with the required number of rows such that for all C âˆˆ [k]
t , AC misses
at most r âˆ’ 1 tuples from [v]t , i.e. AC covers at least m tuples from [v]t .
t
u
Theorem 1 can be improved upon using the LovaÌsz local lemma.
Lemma 1. (LovaÌsz local lemma; symmetric case) (see [1]) Let A1 , A2 , . . . , An
events in an arbitrary probability space. Suppose that each event Ai is mutually
independent of a set of all other events Aj except for at most d, and that Pr[Ai ] â‰¤
p for all 1 â‰¤ i â‰¤ n. If ep(d + 1) â‰¤ 1, then Pr[âˆ©ni=1 AÌ„i ] > 0.

Lemma 1 provides an upper bound on the probability of a â€œbadâ€ event in terms
of the dependence structure among such bad events, so that there is a guaranteed
outcome in which all â€œbadâ€ events are avoided. This lemma is most useful when
there is limited dependence among the â€œbadâ€ events, as in the following:
Theorem 2. For integers t, k, v and m where v, t â‰¥ 2, k â‰¥ 2t and 1 â‰¤ m â‰¤ v t
there exists a PCA(N ; t, k, v, m) with
n
 v t o
k
1 + ln t tâˆ’1
mâˆ’1


.
(6)
Nâ‰¤
vt
ln mâˆ’1

Proof. When k â‰¥ 2t, each event BC with C âˆˆ [k]
(that is, at least v t âˆ’ m + 1
t



k
tuples are missing in AC ) is independent of all but at most 1t kâˆ’1
tâˆ’1 < t tâˆ’1

events in {BC 0 : C 0 âˆˆ [k]
t \ {C}}. Applying Lemma 1, Pr[âˆ§Câˆˆ([k]) BC ] > 0 when
t

 t 


v
r N
k
t
â‰¤ 1.
e
1âˆ’ t
r
v
tâˆ’1

(7)
t
u

Solve (7) to obtain the required upper bound on N .
When m = v t , apply the Taylor series expansion to obtain ln



vt
mâˆ’1



â‰¥

1
vt ,

and thereby recover the upper bound (3). Theorem 2 implies:
Corollary 1. Given q âˆˆ [0, 1] and integers 2 â‰¤ t â‰¤ k, v â‰¥ 2, there exists an
N Ã— k array on [v] with (q, t)-completeness equal to 1 (i.e., maximal), whose
number N of rows satisfies
n
 v t o
k
1 + ln t tâˆ’1
qv t âˆ’1


.
Nâ‰¤
t
ln qvvt âˆ’1
Rewriting (6), setting r = v t âˆ’ m + 1, and using the Taylor series expansion
of ln 1 âˆ’ vrt , we get
n
 v t o
k


1 + ln t tâˆ’1
r
v t (t âˆ’ 1) ln k
ln r


Nâ‰¤
â‰¤
1âˆ’
+ o(1) .
(8)
t
r
ln k
ln vtvâˆ’r
Hence when r = v(t âˆ’ 1) (or equivalently, m = v t âˆ’ v(t âˆ’ 1) + 1), there is a
partial m-covering array with Î˜(v tâˆ’1 ln k) rows. This matches the lower bound
(4) asymptotically for covering arrays by missing, in each t-set of columns, no
more than v(t âˆ’ 1) âˆ’ 1 of the v t possible rows.
The dependence of the bound (6) on the number of v-ary t-vectors that must
appear in the t-tuples of columns is particularly of interest when test suites are
run sequentially until a fault is revealed, as in [3]. Indeed the arguments here
may have useful consequences for the rate of fault detection.

Algorithm 1: Moser-Tardos type algorithm for partial m-covering arrays.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

Input: Integers N, t, k, v and m where v, t â‰¥ 2, k â‰¥ 2t and 1 â‰¤ m â‰¤ v t
Output: A : a PCA(N ; t, k,
v, m)

k
vt
1+ln t(tâˆ’1
)(mâˆ’1
)
 t 
Let N :=
;
v
ln mâˆ’1

Construct an N Ã— k array A where each entry is chosen independently and
uniformly at random from [v];
repeat
Set covered := true;

for each column t-set C âˆˆ [k]
do
t
if AC does not cover at least m distinct t-tuples x âˆˆ [v]t then
Set covered := false;
Set missing-column-set := C;
break;
end
end
if covered = false then
Choose all the entries in the t columns of missing-column-set
independently and uniformly at random from [v];
end
until covered = true;
Output A;

Lemma 1 and hence Theorem 2 have proofs that are non-constructive in
nature. Nevertheless, Moser and Tardos [26] provide a randomized algorithm
with the same guarantee. Patterned on their method, Algorithm 1 constructs a
partial m-covering array with exactly the same number of rows as (6) in expected
polynomial time. Indeed, for fixed t, the expected number of times the resampling
step (line 13) is repeated is linear in k (see [26] for more details).

4

Almost Partial Covering Arrays

For 0 <  < 1, an -almost partial m-covering array, APCA(N ; t, k, v, m, ), is an
N Ã— k array A with each entry from [v] so that for at least (1 âˆ’ ) kt column

t
t-sets C âˆˆ [k]
t , AC covers at least m distinct tuples x âˆˆ [v] . Again, a covering

array CA(N ; t, k, v) is precisely an APCA(N ; t, k, v, v t , ) when  < 1/ kt . Our
first result on -almost partial m-covering arrays is the following.
Theorem 3. For integers t, k, v, m and real  where k â‰¥ t â‰¥ 2, v â‰¥ 2, 1 â‰¤ m â‰¤
v t and 0 â‰¤  â‰¤ 1, there exists an APCA(N ; t, k, v, m, ) with
n t  o
v
ln mâˆ’1
/

 .
Nâ‰¤
(9)
vt
ln mâˆ’1

Proof. Parallelling the proof of Theorem
1 we compute an upper bound on the

expected number of t-sets C âˆˆ [k]
for
which
A misses at least r tuples x âˆˆ [v]t .
t
 C
k
When this expected number is at most  t , an array A is guaranteed to exist


with at least (1 âˆ’ ) kt t-sets of columns C âˆˆ [k]
such that AC misses at most
t
r âˆ’ 1 distinct tuples x âˆˆ [v]t . Thus A is an APCA(N ; t, k, v, m, ). To establish
the theorem, solve the following for N :
  t  
 
k
v
r N
k
1âˆ’ t
â‰¤
.
t
r
v
t
t
u

k

When  < 1/ t we recover the bound from Theorem 1 for partial m-covering
arrays. In terms of (q, t)-completeness, Theorem 3 yields the following.
Corollary 2. For q âˆˆ [0, 1] and integers 2 â‰¤ t â‰¤ k, v â‰¥ 2, there exists an N Ã— k
array on [v] with (q, t)-completeness equal to 1 âˆ’ , with
n t  o
v
/
ln mâˆ’1

 .
Nâ‰¤
vt
ln mâˆ’1
 t
When m = v t , an -almost covering array exists with N â‰¤ v t ln v rows.
Improvements result by focussing on covering arrays in which the symbols are
acted on by a finite group. In this setting, one chooses orbit representatives of
rows that collectively cover orbit representatives of t-way interactions under the
group action; see [9], for example. Such group actions have been used in direct
and computational methods for covering arrays [6,25], and in randomized and
derandomized methods [9,27,28].
We employ the sharply transitive action of the cyclic group of order v, adapting the earlier arguments using methods from [28]:
Theorem 4. For integers t, k, v and real  where k â‰¥ t â‰¥ 2, v â‰¥ 2 and 0 â‰¤  â‰¤ 1
there exists an APCA(N ; t, k, v, v t , ) with
 tâˆ’1 
v
t
N â‰¤ v ln
.
(10)

Proof. The action of the cyclic group of order v partitions [v]t into v tâˆ’1 orbits, each of length v. Let n = b Nv c and let A be an n Ã— k random array
where each entry is chosen independently from the set [v] with uniform prob
ability. For C âˆˆ [k]
t , AC covers the orbit X if at least one tuple x âˆˆ X
is present
n in AC . The
nprobability that the orbit X is not covered in A is
1
1 âˆ’ vvt
= 1 âˆ’ vtâˆ’1
. Let DC denote the event that AC does not cover
n
1
at least one orbit. Applying the union bound, Pr[DC ] â‰¤ v tâˆ’1 1 âˆ’ vtâˆ’1
. By
linearity of expectation,
the
expected
number
of
column
t-sets
C
for
which
DC

n
1
occurs is at most kt v tâˆ’1 1 âˆ’ vtâˆ’1
. As earlier, set this expected value to be


at most  kt and solve for n. An array exists that covers all orbits in at least

(1 âˆ’ ) kt column t-sets. Develop this array over the cyclic group to obtain the
desired array.
t
u
As in [28], further improvements result by considering a group, like the
Frobenius group, that acts sharply 2-transitively on [v]. When v is a prime
power, the Frobenius group is the group of permutations of Fv of the form
{x 7â†’ ax + b : a, b âˆˆ Fv , a 6= 0}.
Theorem 5. For integers t, k, v and real  where k â‰¥ t â‰¥ 2, v â‰¥ 2, v is a prime
power and 0 â‰¤  â‰¤ 1 there exists an APCA(N ; t, k, v, v t , ) with
 tâˆ’2 
2v
t
+ v.
(11)
N â‰¤ v ln

tâˆ’1

Proof. The action of the Frobenius group partitions [v]t into v vâˆ’1âˆ’1 orbits of
length v(v âˆ’ 1) (full orbits) each and 1 orbit of length v (a short orbit). The
short orbit consists of tuples of the form (x1 , . . . , xt ) âˆˆ [v]t where x1 = . . . = xt .
N âˆ’v
Let n = b v(vâˆ’1)
c and let A be an n Ã— k random array where each entry is
chosen independently from the set [v] with uniform probability. Our strategy is
to construct A so that it covers all full orbits for the required number of arrays
{AC : C âˆˆ [k]
t }. Develop A over the Frobenius group and add v rows of the
form (x1 , . . . , xk ) âˆˆ [v]t with x1 = . . . = xk to obtain an APCA(N ; t, k, v, v t , )
with the desired value of N . Following the lines of the proof of Theorem 4, A
covers all full orbits in at least (1 âˆ’ ) kt column t-sets C when
  tâˆ’1

n
 
vâˆ’1
k
âˆ’1
k v
1 âˆ’ tâˆ’1
â‰¤
.
t
vâˆ’1
v
t
Because

v tâˆ’1 âˆ’1
vâˆ’1

â‰¤ 2v tâˆ’2 for v â‰¥ 2, we obtain the desired bound.

t
u

Using group action when m = v t affords useful improvements. Does this
improvement extend to cases when m < v t ? Unfortunately, the answer appears
to be no. Consider the case for PCA(N ; t, k, v, m) when m â‰¤ v t using the action
of the cyclic group of order v on [v]t . Let A be a random n Ã— k array over [v].
When v t âˆ’ vs + 1 â‰¤ m â‰¤ v t âˆ’ v(s âˆ’ 1) for 1 â‰¤ s â‰¤ v tâˆ’1 , this implies that

for all C âˆˆ [k]
s âˆ’1 orbits of [v]t . Then we obtain that
t , AC misses
 at most
 tâˆ’1

tâˆ’1 
k
v
v
n â‰¤ 1 + ln t tâˆ’1
/ ln vtâˆ’1
s
âˆ’s . Developing A over the cyclic group
we obtain a PCA(N ; t, k, v, m) with
1 + ln

n

ln



N â‰¤v

k
tâˆ’1



v tâˆ’1
s

v tâˆ’1
v tâˆ’1 âˆ’s



o
(12)

Figure 1 compares (12) and (6). In Figure 1a we plot the size of the partial
m-covering array as obtained by (12) and (6) for v t âˆ’ 6v + 1 â‰¤ m â‰¤ v t and

4

9

x 10

8

6

10

Eq. (12)
Eq. (6)

Eq. (12)
Eq. (6)

N âˆ’ number of rows

N âˆ’ number of rows

7

6

5

5

10

4

3

2
4070

4

4075

4080

4085
m

4090

4095

(a) t = 6, k = 20, v = 4

4100

10
1
10

2

3

10

10

4

10

k

(b) t = 6, v = 4, m = v t âˆ’ v

Fig. 1: Comparison of (12) and (6). Figure (a) compares the sizes of the partial
m-covering arrays when v t âˆ’ 6v + 1 â‰¤ m â‰¤ v t . Except for m = v t = 4096
the bound from (6) outperforms the bound obtained by assuming group action.
Figure (b) shows that for m = v t âˆ’ v = 4092, (6) outperforms (12) for all values
of k.
t = 6, k = 20, v = 4. Except when m = v t = 4096, the covering array case, (6)
outperforms (12). Similarly, Figure 1b shows that for m = v t âˆ’ v = 4092, (6)
consistently outperforms (12) for all values of k when t = 6, v = 4. We observe
similar behavior for different values of t and v.
Next we consider even stricter coverage restrictions, combining Theorems 2
and 4.
Theorem 6. For integers t, k, v, m and real  where k â‰¥ t â‰¥ 2, v â‰¥ 2, 0 â‰¤  â‰¤ 1
k
and m â‰¤ v t + 1 âˆ’ ln(v/ln1/(tâˆ’1)
there exists an N Ã— k array A with entries from
)
[v] such that

t
1. for each C âˆˆ [k]
t , AC covers at least m tuples x âˆˆ [v] ,
2. for at least (1 âˆ’ )kt column t-sets C, AC covers all tuples x âˆˆ [v]t ,
tâˆ’1
3. N = O(v t ln v  ).
Proof. We vertically juxtapose a partial m-covering array and an -almost v t k
covering array. For r = ln(v/ln1/(tâˆ’1)
and m = v t âˆ’ r + 1, (8) guarantees the
)
 tâˆ’1 
existence of a partial m-covering array with v t ln v 
{1 + o(1)} rows. Theorem4 guarantees
the existence of an -almost v t -covering array with at most

v tâˆ’1
t
v ln
rows.
t
u

Corollary 3. There exists an N Ã— k array A such that:

t
1. for any t-set of columns C âˆˆ [k]
t , AC covers at least m â‰¤ v + 1 âˆ’ v(t âˆ’ 1)
t
distinct t-tuples x âˆˆ [v] ,


2. for at least 1 âˆ’

v tâˆ’1
k1/v



k
t



column t-sets C, AC covers all the distinct t-tuples

t

x âˆˆ [v] .
3. N = O(v tâˆ’1 ln k).
Proof. Apply Theorem 6 with m = v t + 1 âˆ’

ln k
.
ln(v/1/(tâˆ’1) )

There are at most

âˆ’ 1 missing t-tuples x âˆˆ [v] in the AC for each of the at most  kt
column t-sets C that do not satisfy the second condition of Theorem 6. To bound
from above the number of missing tuples to a certain small function f (t) of t, it
 tâˆ’1
is sufficient that  â‰¤ v tâˆ’1 k1 f (t)+1 . Then the number of missing t-tuples x âˆˆ [v]t
in AC is bounded from above by f (t) whenever  is not larger than
ln k
ln(v/1/(tâˆ’1) )

t

v

tâˆ’1

tâˆ’1
  f (t)+1
1
k

(13)


 tâˆ’1 
of rows
On the other hand, in order for the number N = O v tâˆ’1 ln v 
of A to be asymptotically equal to the lower bound (4), it suffices that  is not
smaller than
v tâˆ’1
(14)
1 .
kv
When f (t) = v(t âˆ’ 1) âˆ’ 1, (13) and (14) agree asymptotically, completing the
proof.
t
u
Once again we obtain a size that is O(v tâˆ’1 log k), a goal that has not been
reached for covering arrays. This is evidence that even a small relaxation of
covering arrays provides arrays of the best sizes one can hope for.
Next we consider the efficient construction of the arrays whose existence is
ensured by Theorem 6. Algorithm 2 is a randomized method to construct an
APCA(N ; t, k, v, m, ) of a size N that is very close to the bound of Theorem
3. By Markovâ€™s inequality the condition in line 9 of Algorithm 2 is met with
probability at most 1/2. Therefore, the expected number of times the loop in
line 2 repeats is at most 2.
To prove Theorem 3, t-wise independence among the variables is sufficient.
Hence, Algorithm 2 can be derandomized using t-wise independent random variables. We can also derandomize the algorithm using the method of conditional
expectation. In this method we construct A by considering the k columns one by
one and fixing all N entries of a column. Given a set of already fixed columns,
to fix the entries of the next column we consider all possible v N choices, and
choose one that provides
 the maximum conditional expectation of the tnumber of
column t-sets C âˆˆ [k]
such that AC covers at least m tuples x âˆˆ [v] . Because
t
v N = O(poly(1/)), this derandomized algorithm constructs the desired array
in polynomial time. Similar randomized and derandomized strategies can be applied to construct the array guaranteed by Theorem 4. Together with Algorithm
1 this implies that the array in Theorem 6 is also efficiently constructible.

Algorithm 2: Randomized algorithm for -almost partial m-covering arrays.

1
2
3
4
5
6
7
8
9
10
11
12
13
14
15
16

5

Input: Integers N, t, k, v and m where v, t â‰¥ 2, k â‰¥ 2t and 1 â‰¤ m â‰¤ v t , and real
0<<1
Output: A : an APCA(N
; t, k, v, m, )

vt
ln 2(mâˆ’1
)/
 t 
Let N :=
;
v
ln mâˆ’1

repeat
Construct an N Ã— k array A where each entry is chosen independently and
uniformly at random from [v];
Set isAPCA:= true;
Set defectiveCount:= 0;

for each column t-set C âˆˆ [k]
do
t
if AC does not cover at least m distinct t-tuples x âˆˆ [v]t then
Set defectiveCount:= defectiveCount + 1;

if defectiveCount > b kt c then
Set isAPCA:= false;
break;
end
end
end
until isAPCA = true;
Output A;

Final Remarks

We have shown that by relaxing the coverage requirement of a covering array somewhat, powerful upper bounds on the sizes of the arrays can be established. Indeed the upper bounds are substantially smaller than the best known
bounds for a covering array; they are of the same order as the lower bound for
CAN(t, k, v). As importantly, the techniques not only provide asymptotic bounds
but also randomized polynomial time construction algorithms for such arrays.
Our approach seems flexible enough to handle variations of these problems.
For instance, some applications require arrays that satisfy, for different subsets
of columns, different coverage or separation requirements [8]. In [16] several
interesting examples of combinatorial problems are presented that can be unified
and expressed in the framework of S-constrained matrices. Given a set of vectors

S each of length t, an N Ã—k matrix M is S-constrained if for every t-set C âˆˆ [k]
t ,
MC contains as a row each of the vectors in S. The parameter to optimize is,
as usual, the number of rows of M . One potential direction is to ask for arrays
that, in every t-tuple of columns, cover at least m of the vectors in S, or that
all vectors in S are covered by all but a small number of t-tuples of columns.
Exploiting the structure of the members of S appears to require an extension of
the results developed here.

Acknowledgements
Research of KS and CJC was supported in part by the National Science Foundation under Grant No. 1421058.

References
1. Noga Alon and Joel H. Spencer. The probabilistic method. Wiley-Interscience Series
in Discrete Mathematics and Optimization. John Wiley & Sons, Inc., Hoboken, NJ,
third edition, 2008.
2. B. Becker and H.-U. Simon. How robust is the n-cube? Inform. and Comput.,
77:162â€“178, 1988.
3. ReneÌe C. Bryce, Yinong Chen, and Charles J. Colbourn. Biased covering arrays
for progressive ranking and composition of web services. Int. J. Simulation Process
Modelling, 3(1/2):80â€“87, 2007.
4. J. N. Cawse. Experimental design for combinatorial and high throughput materials
development. GE Global Research Technical Report, 29:769â€“781, 2002.
5. Ashok K. Chandra, Lawrence T. Kou, George Markowsky, and Shmuel Zaks.
On sets of boolean n-vectors with all k-projections surjective. Acta Informatica,
20(1):103â€“111, 1983.
6. M. A. Chateauneuf, C. J. Colbourn, and D. L. Kreher. Covering arrays of strength
3. Des. Codes Crypt., 16:235â€“242, 1999.
7. Baiqiang Chen and Jian Zhang. Tuple density: a new metric for combinatorial test
suites. In Proceedings of the 33rd International Conference on Software Engineering, ICSE 2011, Waikiki, Honolulu , HI, USA, May 21-28, 2011, pages 876â€“879,
2011.
8. C. J. Colbourn. Combinatorial aspects of covering arrays. Le Matematiche (Catania), 58:121â€“167, 2004.
9. C. J. Colbourn. Conditional expectation algorithms for covering arrays. Journal
of Combinatorial Mathematics and Combinatorial Computing, 90:97â€“115, 2014.
10. Charles J. Colbourn. Covering arrays and hash families. In D. CrnkovicÌŒ and
V. Tonchev, editors, Information Security, Coding Theory, and Related Combinatorics, NATO Science for Peace and Security Series, pages 99â€“135. IOS Press,
2011.
11. Peter Damaschke. Adaptive versus nonadaptive attribute-efficient learning. Machine Learning, 41(2):197â€“215, 2000.
12. N. FranceticÌ and B. Stevens. Asymptotic size of covering arrays: an application of
entropy compression. ArXiv e-prints, March 2015.
13. L. Gargano, J. KoÌˆrner, and U. Vaccaro. Sperner capacities. Graphs and Combinatorics, 9:31â€“46, 1993.
14. A. P. Godbole, D. E. Skipper, and R. A. Sunley. t-covering arrays: upper bounds
and Poisson approximations. Combinatorics, Probability and Computing, 5:105â€“
118, 1996.
15. N. Graham, F. Harary, M. Livingston, and Q.F. Stout. Subcube fault-tolerance in
hypercubes. Information and Computation, 102(2):280 â€“ 314, 1993.
16. Sylvain Gravier and Bernard Ycart. S-constrained random matrices. DMTCS
Proceedings, 0(1), 2006.

17. A. Hartman. Software and hardware testing using combinatorial covering suites.
In M. C. Golumbic and I. B.-A. Hartman, editors, Interdisciplinary Applications of
Graph Theory, Combinatorics, and Algorithms, pages 237â€“266. Springer, Norwell,
MA, 2005.
18. Alan Hartman and Leonid Raskin. Problems and algorithms for covering arrays.
Discrete Mathematics, 284(13):149 â€“ 156, 2004.
19. Stasys Jukna. Extremal Combinatorics: With Applications in Computer Science.
Springer Publishing Company, Incorporated, 1st edition, 2010.
20. G. O. H. Katona. Two applications (for search theory and truth functions) of
Sperner type theorems. Periodica Math., 3:19â€“26, 1973.
21. D. Kleitman and J. Spencer. Families of k-independent sets. Discrete Math.,
6:255â€“262, 1973.
22. D. R. Kuhn, R. Kacker, and Y. Lei. Introduction to Combinatorial Testing. CRC
Press, 2013.
23. D. R. Kuhn, I. D. Mendoza, R. N. Kacker, and Y. Lei. Combinatorial coverage
measurement concepts and applications. In Software Testing, Verification and
Validation Workshops (ICSTW), 2013 IEEE Sixth International Conference on,
pages 352â€“361, March 2013.
24. J. R. Maximoff, M. D. Trela, D. R. Kuhn, and R. Kacker. A method for analyzing
system state-space coverage within a t-wise testing framework. In 4th Annual IEEE
Systems Conference, pages 598â€“603, 2010.
25. K. Meagher and B. Stevens. Group construction of covering arrays. J. Combin.
Des., 13:70â€“77, 2005.
26. Robin A. Moser and GaÌbor Tardos. A constructive proof of the general LovaÌsz
local lemma. J. ACM, 57(2):Art. 11, 15, 2010.
27. K. Sarkar and C. J. Colbourn. Two-stage algorithms for covering array construction. submitted for publication.
28. K. Sarkar and C. J. Colbourn. Upper bounds on the size of covering arrays. ArXiv
e-prints, March 2016.
29. D. E. Shasha, A. Y. Kouranov, L. V. Lejay, M. F. Chou, and G. M. Coruzzi.
Using combinatorial design to study regulation by multiple input signals: A tool
for parsimony in the post-genomics era. Plant Physiol., 127:1590â€“2594, 2001.
30. A. J. Tong, Y. G. Wu, and L. D. Li. Room-temperature phosphorimetry studies of
some addictive drugs following dansyl chloride labelling. Talanta, 43(9):14291436,
September 1996.

Two-stage algorithms for covering array construction

arXiv:1606.06730v1 [cs.DS] 21 Jun 2016

Kaushik Sarkar and Charles J. Colbourn
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, PO Box 878809
Tempe, Arizona, 85287-8809, U.S.A.
June 23, 2016
Abstract
Modern software systems often consist of many different components, each with a number of options.
Although unit tests may reveal faulty options for individual components, functionally correct components
may interact in unforeseen ways to cause a fault. Covering arrays are used to test for interactions
among components systematically. A two-stage framework, providing a number of concrete algorithms,
is developed for the efficient construction of covering arrays. In the first stage, a time and memory
efficient randomized algorithm covers most of the interactions. In the second stage, a more sophisticated
search covers the remainder in relatively few tests. In this way, the storage limitations of the sophisticated
search algorithms are avoided; hence the range of the number of components for which the algorithm can
be applied is extended, without increasing the number of tests. Many of the framework instantiations
can be tuned to optimize a memory-quality trade-off, so that fewer tests can be achieved using more
memory. The algorithms developed outperform the currently best known methods when the number
of components ranges from 20 to 60, the number of options for each ranges from 3 to 6, and t-way
interactions are covered for t âˆˆ {5, 6}. In some cases a reduction in the number of tests by more than
50% is achieved.

Keywords: Covering array, Software interaction testing, Combinatorial construction algorithm

1

Introduction

Real world software and engineered systems are composed of many different components, each with a number
of options, that are required to work together in a variety of circumstances. Components are factors, and
options for a component form the levels of its factor. Although each level for an individual factor can be tested
in isolation, faults in deployed software can arise from interactions among levels of different factors. When
an interaction involves levels of t different factors, it is a t-way interaction. Testing for faults caused by t-way
interactions for every t is generally infeasible, as a result of a combinatorial explosion. However, empirical
research on real world software systems indicates that testing all possible 2-way (or 3-way) interactions
would detect 70% (or 90%) of all faults [25]. Moreover, testing all possible 6-way interactions is sufficient
for detection of 100% of all faults in the systems examined in [25]. Testing all possible t-way interactions for
some 2 â‰¤ t â‰¤ 6 is pseudo-exhaustive testing [24], and is accomplished with a combinatorial array known as
a covering array.
Formally, let N, t, k, and v be integers with k â‰¥ t â‰¥ 2 and v â‰¥ 2. A covering array CA(N ; t, k, v) is an
N Ã— k array A in which each entry is from a v-ary alphabet Î£, and for every N Ã— t sub-array B of A and
every x âˆˆ Î£t , there is a row of B that equals x. Then t is the strength of the covering array, k is the number
of factors, and v is the number of levels.
When k is a positive integer, [k] denotes the set {1, . . . , k}. A t-way interaction is {(ci , ai ) : 1 â‰¤ i â‰¤
t, ci âˆˆ [k], ci 6= cj for i 6= j, and ai âˆˆ Î£}. So an interaction is an assignment of levels from Î£ to t of the k

factors. It,k,v denotes the set of all kt v t interactions for given t, k and v. An N Ã— k array A covers the
1

interaction Î¹ = {(ci , ai ) : 1 â‰¤ i â‰¤ t, ci âˆˆ [k], ci 6= cj for i 6= j, and ai âˆˆ Î£} if there is a row r in A such that
A(r, ci ) = ai for 1 â‰¤ i â‰¤ t. When there is no such row in A, Î¹ is not covered in A. Hence a CA(N ; t, k, v)
covers all interactions in It,k,v .
Covering arrays are used extensively for interaction testing in complex engineered systems. To ensure
that all possible combinations of options of t components function together correctly, one needs examine
all possible t-way interactions. When the number of components is k, and the number of different options
available for each component is v, each row of CA(N ; t, k, v) represents a test case. The N test cases
collectively test all t-way interactions. For this reason, covering arrays have been used in combinatorial
interaction testing in varied fields like software and hardware engineering, design of composite materials,
and biological networks [8, 24, 26, 32, 34].
The cost of testing is directly related to the number of test cases. Therefore, one is interested in covering
arrays with the fewest rows. The smallest value of N for which CA(N ; t, k, v) exists is denoted by CAN(t, k, v).
Efforts to determine or bound CAN(t, k, v) have been extensive; see [12, 14, 24, 31] for example. Naturally one
would prefer to determine CAN(t, k, v) exactly. Katona [22] and Kleitman and Spencer [23] independently
showed that
 for t = v = 2, the minimum number of rows N in a CA(N ; 2, k, 2) is the smallest N for which
âˆ’1
. Exact determination of CAN(t, k, v) for other values of t and v has remained open. However,
kâ‰¤ N
N
d2e
some progress has been made in determining upper bounds for CAN(t, k, v) in the general case; for recent
results, see [33].
For practical applications such bounds are often unhelpful, because one needs explicit covering arrays to
use as test suites. Explicit constructions can be recursive, producing larger covering arrays using smaller ones
as ingredients (see [14] for a survey), or direct. Direct methods for some specific cases arise from algebraic,
geometric, or number-theoretic techniques; general direct methods are computational in nature. Indeed
when k is relatively small, the best known results arise from computational techniques [13], and these are
in turn essential for the successes of recursive methods. Unfortunately, the existing computational methods
encounter difficulties as k increases, but is still within the range needed for practical applications. Typically
such difficulties arise either as a result of storage or time limitations or by producing covering arrays that
are too big to compete with those arising from simpler recursive methods.
Cohen [11] discusses commercial software where the number of factors often exceeds 50. Aldaco et al. [1]
analyze a complex engineered system having 75 factors, using a variant of covering arrays. Android [3] uses
a Configuration class to describe the device configuration; there are 17 different configuration parameters
with 3 âˆ’ 20 different levels. In each of these cases, while existing techniques are effective when the strength
is small, these moderately large values of k pose concerns for larger strengths.
In this paper, we focus on situations in which every factor has the same number of levels. These cases
have been most extensively studied, and hence provide a basis for making comparisons. In practice, however,
often different components have different number of levels, which is captured by extending the notion of a
covering array. A mixed covering array MCA(N ; t, k, (v1 , v2 , . . . , vk )) is an N Ã— k array in which the ith
column contains vi symbols for 1 â‰¤ i â‰¤ k. When {i1 , . . . , it } âŠ† {1, . . . , k} is
Qat set of t columns, in the N Ã— t
subarray obtained by selecting columns i1 , . . . , it of the MCA, each of the j=1 vij distinct t-tuples appears
as a row at least once. Although we examine the uniform case in which v1 = Â· Â· Â· = vk , the methods developed
here can all be directly applied to mixed covering arrays as well.
Inevitably, when k > max(t + 1, v + 2), a covering array must cover some interactions more than once,
for if not they are orthogonal arrays [20]. Treating the rows of a covering array in a fixed order, each row
covers some number of interactions not covered by any earlier row. For a variety of known constructions,
the initial rows cover many new interactions, while the later ones cover very few [7]. Comparing this rate
of coverage for a purely random method and for one of the sophisticated search techniques, one finds little
difference in the initial rows, but very substantial differences in the final ones. This suggests strategies to
build the covering array in stages, investing more effort as the number of remaining uncovered interactions
declines.
In this paper we propose a new algorithmic framework for covering array construction, the two-stage
framework. In the first stage, a randomized row construction method builds a specified number of rows to
cover all but at most a specified, small number of interactions. As we see later, by dint of being randomized

2

this uses very little memory. The second stage is a more sophisticated search that adds few rows to cover the
remaining uncovered interactions. We choose search algorithms whose requirements depend on the number
of interactions to be covered, to profit from the fact that few interactions remain. By mixing randomized and
deterministic methods, we hope to retain the fast execution and small storage of the randomized methods,
along with the accuracy of the deterministic search techniques.
We introduce a number of algorithms within the two-stage framework. Some improve upon best known
bounds on CAN(t, k, v) (see [33]) in principle. But our focus is on the practical consequences: The twostage algorithms are indeed quite efficient for higher strength (t âˆˆ {5, 6}) and moderate number of levels
(v âˆˆ {3, 4, 5, 6}), when the number of factors k is moderately high (approximately in the range of 20 âˆ’ 80
depending on value of t and v). In fact, for many combination of t, k and v values the two-stage algorithms
beat the previously best known bounds.
Torres-Jimenez et al. [36] explore a related two-stage strategy. In their first stage, an in-parameter-order
greedy strategy (as used in ACTS [24]) adds a column to an existing array; in their second stage, simulated
annealing is applied to cover the remaining interactions. They apply their methods when t = v = 3, when
the storage and time requirements for both stages remain acceptable. In addition to the issues in handling
larger strengths, their methods provide no a priori bound on the size of the resulting array. In contrast with
their methods, ours provide a guarantee prior to execution with much more modest storage and time.
The rest of the paper is organized as follows. Section 2 reviews algorithmic methods of covering array
construction, specifically the randomized algorithm and the density algorithm. This section contrasts these
two methods and points out their limitations. Then it gives an intuitive answer to the question of why a two
stage based strategy might work and introduces the general two-stage framework. Section 3 introduces some
specific two-stage algorithms. Section 3.1 analyzes and evaluates the naÄ±Ìˆve strategy. Section 3.2 describes a
two-stage algorithm that combines the randomized and the density algorithm. Section 3.3 introduces graph
coloring based techniques in the second stage. Section 3.4 examines the effect of group action on the size
of the constructed covering arrays. Section 4 compares the results of various two-stage algorithms with the
presently best known sizes. In Section 5 we discuss the LovaÌsz local lemma (LLL) bounds on CAN(t, k, v)
and provide a Moser-Tardos type randomized algorithm for covering array construction that matches the
bound. Although the bound was known [18], the proof was non-constructive, and a constructive algorithm to
match this bound seems to be absent in the literature. We explore potentially better randomized algorithms
for the first stage using LLL based techniques, We also obtain a two-stage bound that improves the LLL
bound for CAN(t, k, v). We conclude the paper in Section 6.

2

Algorithmic construction of covering arrays

Available algorithms for the construction of covering arrays are primarily heuristic in nature; indeed exact
algorithms have succeeded for very few cases. Computationally intensive metaheuristic search methods such
as simulated annealing, tabu search, constraint programming, and genetic algorithms have been employed
when the strength is relatively small or the number of factors and levels is small. These methods have
established many of the best known bounds on sizes of covering arrays [13], but for many problems of
practical size their time and storage requirements are prohibitive. For larger problems, the best available
methods are greedy. The IPO family of algorithms [24] repeatedly adds one column at a time, and
 then adds
new rows to ensure complete coverage. In this way, at any point in time, the status of v t kâˆ’1
tâˆ’1 interactions
may be stored. AETG [10] pioneered a different method, which greedily selects one row at a time to cover
a large number of as-yet-uncovered interactions. They establish that if a row can be chosen that covers the
maximum number, a good a priori bound on the size of the covering array can be computed. Unfortunately
selecting the maximum is NP-hard, and even if one selects the maximum there is no guarantee that the
covering array is the smallest possible
 [7], so AETG resorts to a good heuristic selection of the next row by
examining the stored status of v t kt interactions. None of the methods so far mentioned therefore guarantee
to reach an a priori bound. An
 extension of the AETG strategy, the density algorithm [5, 6, 15], stores
additional statistics for all v t kt interactions in order to ensure the selection of a good next row, and hence
guarantees to produce an array with at most the precomputed number of rows. Variants of the density
3

Algorithm 1: A randomized algorithm for covering array construction.
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; 
t, k, v)
log (kt)+t log v


1 Set N :=
;
vt
log

2
3

4
5
6
7
8
9
10
11
12

v t âˆ’1

repeat
Construct an N Ã— k array A where each entry is chosen independently and uniformly at random
from a v-ary alphabet;
Set covered := true;
for each interaction Î¹ âˆˆ It,k,v do
if Î¹ is not covered in A then
Set covered := false;
break;
end
end
until covered = true;
Output A;

algorithm have proved to be most effective for problems of moderately large size. For even larger problems,
pure random approaches have been applied.
To produce methods that provide a guarantee on size, it is natural to focus on the density algorithm
in order to understand its strengths and weaknesses. To do this, we contrast it with a basic randomized
algorithm. Algorithm 1 shows a simple randomized algorithm for covering array construction. The algorithm
constructs an array of a particular size randomly and checks whether all the interactions are covered. It
repeats until it finds an array that covers all the interactions.
log (kt)+t log v

 is guaranteed to exist:
A CA(N ; t, k, v) with N =
vt
log

v t âˆ’1

Theorem 1. [21, 27, 35] (Stein-LovaÌsz-Johnson (SLJ) bound): Let t, k, v be integers with k â‰¥ t â‰¥ 2, and
v â‰¥ 2. Then as k â†’ âˆ,

log kt + t log v


CAN(t, k, v) â‰¤
t
log vtvâˆ’1
In fact, the probability that the N Ã— k array constructed in line 3 of Algorithm 1 is a valid covering array
is high enough that the expected number of times the loop in line 2 is repeated is a small constant.
An alternative strategy is to add rows one by one instead of constructing the full array at the outset. We
start with an empty array, and whenever we add a new row we ensure that it covers at least the expected
number of previously uncovered interactions for a randomly chosen row. The probability that an uncovered
interaction is covered by a random row is 1/v t . If the number of uncovered interactions is u, then by linearity
of expectation, the expected number of newly covered interactions in a randomly chosen row is uv âˆ’t . If each
row added covers exactly this expected number, we obtain the same number of rows as the SLJ bound,
realized in Algorithm 1. But because the actual number of newly covered interactions is always an integer,
each added row covers at least duv âˆ’t e interactions. This is especially helpful towards the end when the
expected number is a small fraction.
Algorithm 2 follows this strategy. Again the probability that a randomly chosen row covers at least the
expected number of previously uncovered interactions is high enough that the expected number of times the
row selection loop in line 6 of Algorithm 2 is repeated is bounded by a small constant.
We can obtain an upper bound on the size produced by Algorithm 2 by assuming that each new row
added covers exactly duv âˆ’t e previously uncovered interactions. This bound is the discrete Stein-LovaÌsz4

Algorithm 2: A randomized algorithm for covering array construction using the discrete SLJ strategy.
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; t, k, v)
1 Let A be an empty array;

k t
2 Initialize a table T indexed by all t v interactions, marking every interaction â€œuncoveredâ€;
3 while there is an interaction marked â€œuncoveredâ€ in T do
4
Let u be the number of interactions marked â€œuncoveredâ€ in T ;
5
Set expectedCoverage := d vut e;
6
repeat
7
Let r be a row of length k where each entry is chosen independently and uniformly at random
from a v-ary alphabet;
8
Let coverage be the number of â€œuncoveredâ€ interactions in T that are covered in row r;
9
until coverage > expectedCoverage;
10
Add r to A;
11
Mark all interactions covered by r as â€œcoveredâ€ in T ;
12 end
13 Output A;

Johnson (discrete SLJ) bound. Figure 1 compares the sizes of covering arrays obtained from the SLJ and
the discrete SLJ bounds for different values of k when t = 6 and v = 3. Consider a concrete example, when
t = 5, k = 20, and v = 3. The SLJ bound guarantees the existence of a covering array with 12499 rows,
whereas the discrete SLJ bound guarantees the existence of a covering array with only 8117 rows.
The density algorithm replaces the loop at line 6 of Algorithm 2 by a conditional expectation derandomized method. For fixed v and t the density algorithm selects a row efficiently (time polynomial in k) and
deterministically that is guaranteed to cover at least duv âˆ’t e previously uncovered interactions. In practice,
for small values of k the density algorithm works quite well, often covering many more interactions than
the minimum. Many of the currently best known CAN(t, k, v) upper bounds are obtained by the density
algorithm in combination with various post-optimization techniques [13].
However, the practical applicability of Algorithm 2 and the density algorithm is limited by the storage
of the table T , representing each of the kt v t interactions. Even when t = 6, v = 3, and k = 54, there are
18,828,003,285 6-way interactions. This huge memory requirement renders the density algorithm impractical
for rather small values of k when t âˆˆ {5, 6} and v â‰¥ 3. We present an idea to circumvent this large
requirement for memory, and develop it in full in Section 3.

2.1

Why does a two stage based strategy make sense?

Compare the two extremes, the density algorithm and Algorithm 1. On one hand, Algorithm 1 does not suffer
from any substantial storage restriction, but appears to generate many more rows than the density algorithm.
On the other hand, the density algorithm constructs fewer rows for small values of k, but becomes impractical
when k is moderately large. One wants algorithms that behave like Algorithm 1 in terms of memory, but
yield a number of rows competitive with the density algorithm.
For t = 6, k = 16, and v = 3, Figure 2 compares the coverage profile for the density algorithm and
Algorithm 1. We plot the number of newly covered interactions for each row in the density algorithm, and
the expected number of newly covered interactions for each row for Algorithm 1. The qualitative features
exhibited by this plot are representative of the rates of coverage for other parameters.
Two key observations are suggested by Figure 2. First, the expected coverage in the initial random rows
is similar to the rows chosen by the density algorithm. In this example, the partial arrays consisting of the
first 1000 rows exhibit similar coverage, yet the randomized algorithm needed no extensive bookkeeping.
Secondly, as later rows are added, the judicious selections of the density algorithm produce much larger
coverage per row than Algorithm 1. Consequently it appears sensible to invest few computational resources
5

4

3.5

x 10

SLJ bound
Discrete SLJ bound

N âˆ’ number of rows

3

2.5

2

1.5

1

0.5
0

100

200

300

400

500
k

600

700

800

900

1000

Figure 1: Comparison of covering array sizes obtained from SLJ bound and discrete SLJ bound for different
values of k, when t = 6 and v = 3.

9000
Density
Basic Random

Number of newly covered interactions

8000
7000
6000
5000
4000
3000
2000
1000
0

0

2000

4000

6000
Row number

8000

10000

12000

Figure 2: For t = 6, k = 16 and v = 3, the actual number of newly covered interactions of the density
algorithm and the expected number of newly covered interactions in a random array.

6

on the initial rows, while making more careful selections in the later ones. This forms the blueprint of our
general two-stage algorithmic framework shown in Algorithm 3.
Algorithm 3: The general two-stage framework for covering array construction.
Input: t : strength of the required covering array, k : number of factors, v : number of levels for each
factor
Output: A : a CA(N ; t, k, v)
1 Choose a number n of rows and a number Ï of interactions;
// First Stage
0
2 Use a randomized algorithm to construct an n Ã— k array A ;
0
3 Ensure that A covers all but at most Ï interactions;
0
4 Make a list L of interactions that are not covered in A (L contains at most Ï interactions);
// Second Stage
0
5 Use a deterministic procedure to add N âˆ’ n rows to A to cover all the interactions in L;
6 Output A;
A specific covering array construction algorithm results by specifying the randomized method in the first
stage, the deterministic method in the second stage, and the computation of n and Ï. Any such algorithm
produces a covering array, but we wish to make selections so that the resulting algorithms are practical while
still providing a guarantee on the size of the array. In Section 3 we describe different algorithms from the
two-stage family, determine the size of the partial array to be constructed in the first stage, and establish
upper bound guarantees. In Section 4 we explore how good the algorithms are in practice.

3

Two-stage framework

For the first stage we consider two methods:
Rand
MT

the basic randomized algorithm
the Moser-Tardos type algorithm

We defer the development of method MT until Section 5. Method Rand uses a simple variant of Algorithm
1, choosing a random n Ã— k array.
For the second stage we consider four methods:
Naive
Greedy
Den
Col

the
the
the
the

naÄ±Ìˆve strategy, one row per uncovered interaction
online greedy coloring strategy
density algorithm
graph coloring algorithm

Using these abbreviations, we adopt a uniform naming convention for the algorithms: TS hA, Bi is the algorithm in which A is used in the first stage, and B is used in the second stage. For example, TS hMT, Greedyi
denotes a two-stage algorithm where the first stage is a Moser-Tardos type randomized algorithm and the
second stage is a greedy coloring algorithm. Later when the need arises we refine these algorithm names.

3.1

One row per uncovered interaction in the second stage (TS hRand, Naivei)

In the second stage each of the uncovered interactions after the first stage is covered using a new row.
Algorithm 4 describes the method in more detail.
This simple strategy improves on the basic randomized strategy when n is chosen judiciously. For
example, when t = 6, k = 54 and v = 3, Algorithm 1 constructs a covering array with 17, 236 rows. Figure
3 plots an upper bound on the size of the covering array against the number n of rows in the partial array.

7

Algorithm 4: NaÄ±Ìˆve two-stage algorithm (TS hRand, Naivei).
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; t, k, v)
 t 
log (kt)+t log v+log log vtvâˆ’1


;
1 Let n :=
vt
log

1

v t âˆ’1

2

Let Ï =

3

repeat
Let A be an n Ã— k array where each entry is chosen independently and uniformly at random from
a v-ary alphabet;
Let uncovNum := 0 and unCovList be an empty list of interactions;
Set covered := true;
for each interaction Î¹ âˆˆ It,k,v do
if Î¹ is not covered in A then
Set uncovNum :=uncovNum+1;
Add Î¹ to unCovList;
if uncovNum > Ï then
Set covered := false;
break;
end
end
end
until covered= true;
for each interaction Î¹ âˆˆuncovList do
Add a row to A that covers Î¹;
end
Output A;

4

5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

log

vt
v t âˆ’1

;

8

4

Total number of rows in the Covering array

1.75

x 10

1.7
1.65
1.6
1.55
1.5
1.45
1.4
1.35
1.3

1

1.1
1.2
1.3
1.4
1.5
1.6
1.7
n âˆ’âˆ’ number of rows in the partial array of the first stage

1.8
4

x 10

Figure 3: An upper bound on the size of the covering array against n, the size of the partial array constructed
in the first stage when t = 6, k = 54, and v = 3, with one new row added per uncovered interaction in the
second stage. The minimum size of 13, 162 is obtained when n = 12, 402. Algorithm 1 requires 17, 236 rows,
and the currently best known covering array has 17, 197 rows.

9

The smallest covering array is obtained when n = 12, 402 which, when completed, yields a covering array
with at most 13, 162 rowsâ€”a big improvement over Algorithm 1.
A theorem from [33] tells us the optimal value of n in general:
Theorem 2. [33] Let t, k, v be integers with k â‰¥ t â‰¥ 2, and v â‰¥ 2. Then
 t 

log kt + t log v + log log vtvâˆ’1 + 1


.
CAN(t, k, v) â‰¤
t
log vtvâˆ’1
log (kt)+t log v+log log


t
log vtvâˆ’1



vt
v t âˆ’1



. The expected number of uncovered
The bound is obtained by setting n =
 t 
interactions is exactly Ï = 1/ log vtvâˆ’1 .
Figure 4 compares SLJ, discrete SLJ and two-stage bounds for k â‰¤ 100, when t = 6 and v = 3. The
two-stage bound does not deteriorate in comparison to discrete SLJ bound as k increases; it consistently
takes only 307-309 more rows. Thus when k = 12 the two-stage bound requires only 6% more rows and
when k = 100 only 2% more rows than the discrete SLJ bound.
4

2.2

x 10

SLJ bound
Discrete SLJ bound
Twoâˆ’stage bound

2

N âˆ’ number of rows

1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
10

20

30

40

50

60

70

80

90

100

k

Figure 4: Comparison of covering array sizes obtained from SLJ bound, discrete SLJ bound and two-stage
bound for k â‰¤ 100, when t = 6 and v = 3. In this range of k values the two-stage bound requires 307-309
more rows than the discrete SLJ bound, that is, 2-6% more rows.
To ensure that the loop in line 7 of Algorithm 4 does not repeat too many times we need to know the
probability with which a random n Ã— k array leaves at most Ï interactions uncovered. Using Chebyshevâ€™s
inequality and the second moment method developed in [2, Chapter 4], we next show that in a random n Ã—
nk
array the number of uncovered interactions is almost always close to its expectation, i.e. kt v t 1 âˆ’ v1t .
Substituting the value of n from line 1, this expected value is equal to Âµ, as in line 2. Therefore, the probability
that a random n Ã— k array covers the desired number of interactions is constant, and the expected number
of times the loop in line 7 is repeated is also a constant (around 2 in practice).

10

Because the theory of the second moment method is developed
considerable detail in [2], here we briefly
Pin
m
mention the relevant concepts and results. Suppose that X = i=1 Xi , where Xi is the indicator random
variable for event Ai for 1 â‰¤ i â‰¤ m. For indices i, j, we write i âˆ¼ j if i 6= j and the events Ai , Aj are not
independent. Also suppose that X1 , . . . , Xm are symmetric, i.e. for every i 6= j there is a measure
preserving
P
mapping of the underlying probability space that sends event Ai to event Aj . Define âˆ†âˆ— = jâˆ¼i Pr [Aj |Ai ].
Then by [2, Corollary 4.3.4]:
Lemma 3. [2] If E[X] â†’ âˆ and âˆ†âˆ— = o(E[X]) then X âˆ¼ E[X] almost always.
In our case, Ai denotes the event that the ith interaction is not covered in a n Ã— k array where each entry
n
is chosen independently and uniformly at random from a v-ary alphabet. Then Pr[Xi ] = 1 âˆ’ v1t . Because



n
there are kt v t interactions in total, by linearity of expectation, E[X] = kt v t 1 âˆ’ v1t , and E[X] â†’ âˆ as
k â†’ âˆ.
Distinct events Ai and Aj are independent if the ith and jth interactions share no column. Therefore,

P
P
k
the event Ai is not independent of at most t tâˆ’1
other events Aj . So âˆ†âˆ— = jâˆ¼i Pr [Aj |Ai ] â‰¤ jâˆ¼i 1 â‰¤

k
t tâˆ’1
= o(E[X]) when v and t are constants. By Lemma 3, the number of uncovered interactions in a random
n Ã— k array is close to the expected number of uncovered interactions. This guarantees that Algorithm 4 is
an efficient randomized algorithm for constructing covering arrays with a number of rows upper bounded by
Theorem 2.
In keeping with the general two-stage framework, Algorithm 4 does not store the coverage status of
each interaction. We only need store the interactions that are uncovered in A, of which there are at most
1
 â‰ˆ v t . This quantity depends only on v and t and is independent of k, so is effectively a
Ï =
t
log vtvâˆ’1

constant that is much smaller than kt v t , the storage requirement for the density algorithm. Hence the
algorithm can be applied to a higher range of k values.
Although Theorem 5 provides asymptotically tighter bounds than Theorem 2, in a range of k values that
are relevant for practical application, Theorem 2 provides better results. Figure 5 compares the bounds on
CAN(t, k, v) with the currently best known results.
4

3

6

x 10

2.5

Best known
Twoâˆ’stage (simple)
GSS bound
2

2

N âˆ’ number of rows

N âˆ’ number of rows

2.5

1.5

1

1.5

1

0.5

0.5

0
0

x 10

Best known
Twoâˆ’stage (simple)
GSS bound

10

20

30

40

50

60

70

80

90

k

0

5

10

15

20

25

30

35

40

45

50

k

(a) t = 6, v = 3

(b) t = 6, v = 6

Figure 5: Comparison of GSS bound and two-stage bound with the currently best known results

3.2

The density algorithm in the second stage (TS hRand, Deni)

Next we apply the density algorithm in the second stage. Figure 6 plots an upper bound on the size of the
covering array against the size of the partial array constructed in the first stage when the density algorithm
is used in the second stage, and compares it with TS hRand, Naivei. The size of the covering array decreases
11

as n decreases. This is expected because with smaller partial arrays, more interactions remain for the second
stage to be covered by the density algorithm. In fact if we cover all the interactions using the density
algorithm (as when n = 0) we would get an even smaller covering array. However, our motivation was
precisely to avoid doing that. Therefore, we need a â€cut-offâ€ for the first stage.
4

Total number of rows in the Covering array

1.9

x 10

Basic Twoâˆ’stage
Twoâˆ’stage with density in second stage

1.8

1.7

1.6

1.5

1.4

1.3

1.2

1

1.1
1.2
1.3
1.4
1.5
1.6
1.7
n âˆ’âˆ’ number of rows in the partial array of the first stage

1.8
4

x 10

Figure 6: Comparison of covering array sizes from two-stage algorithms with Den and Naive in the second
stage. With Den there is no minimum point in the curve; the size of the covering array keeps decreasing as
we leave more uncovered interactions for the second stage.
We are presented with a trade-off. If we construct a smaller partial array in the first stage, we obtain a
smaller covering array overall. But we then pay for more storage and computation time for the second stage.
To appreciate the nature of this trade-off, look at Figure 7, which plots an upper bound on the covering
array size and the number of uncovered interactions in the first stage against n. The improvement in the
covering array size plateaus after a certain point. The three horizontal lines indicate Ï (â‰ˆ v t ), 2Ï and 3Ï
uncovered interactions in the first stage. (In the naÄ±Ìˆve method of Section 3.1, the partial array after the first
stage leaves at most Ï uncovered interactions.) In Figure 7 the final covering array size appears to plateau
when the number of uncovered interactions left by the first stage is around 2Ï. After that we see diminishing
returns â€” the density algorithm needs to cover more interactions in return for a smaller improvement in the
covering array size.
Let r be the maximum number of interactions allowed to remain uncovered after the first stage. Then r
can be specified in the two-stage algorithm. To accommodate this, we denote by TS hA, B; ri the two-stage
algorithm where A is the first stage strategy, B is the second stage strategy, and r is the maximum number
of uncovered interactions after the first stage. For example, TS hRand, Den; 2Ïi applies the basic randomized
algorithm in the first stage to cover all but at most 2Ï interactions, and the density algorithm to cover the
remaining interactions in the second stage.

3.3

Coloring in the second stage (TS hRand, Coli and TS hRand, Greedyi)

Now we describe strategies using graph coloring in the second stage. Construct a graph G = (V, E), the
incompatibility graph, in which V is the set of uncovered interactions, and there is an edge between two
12

Number of rows / Number of uncovered interactions

18000
16000

Num. of rows in the completed CA
Num. of uncovered interaction in first stage

14000
12000
10000
8000
6000
4000
2000
0
0.8

0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
4
n âˆ’âˆ’ number of rows in the partial array of the first stage
x 10

Figure 7: Final covering array size against the number of uncovered interactions after the first stage. As the
size n of the partial array decreases, the number of uncovered interactions in the first stage increases. Den is
used in the second stage. From bottom to top, the green lines denote Ï, 2Ï, and 3Ï uncovered interactions.
interactions exactly when they share a column in which they have different symbols. A single row can cover
a set of interactions if and only if it forms an independent set in G. Hence the minimum number of rows
required to cover all interactions of G is exactly its chromatic number Ï‡(G), the minimum number of colors
in a proper coloring of G. Graph coloring is an NP-hard problem, so we employ heuristics to bound the
chromatic number. Moreover, G only has vertices for the uncovered interactions after the first stage, so is
size is small relative to the total number of interactions.
The expected number of edges in the incompatibility
graph after
n choosing n rows uniformly at random
 k  t Pt

 t
 
t kâˆ’t
1
1 n
1
tâˆ’i
is Î³ = 2 t v
) 1 âˆ’ vt
1 âˆ’ (vt âˆ’vtâˆ’i ) . Using the elementary upper bound on
i=1 i tâˆ’i (v âˆ’ v
q
the chromatic number Ï‡ â‰¤ 12 + 2m + 14 , where m is the number of edges [16, Chapter 5.2], we can surely
q
cover the remaining interactions with at most 12 + 2m + 14 rows.
The actual number of edges m that remain after the first stage is a random variable with mean Î³. In
principle, the first stage could be repeatedly applied until m â‰¤ Î³, so we call m = Î³ the optimistic estimate.
To ensure that the first stage is expected to be run a small constant number of times, we increase the
estimate. With probability more than 1/2 the incompatibility graph has m â‰¤ 2Î³ edges, so m = 2Î³ is the
conservative estimate.
For t = 6, k = 56, and v = 3, Figure 8 shows the effect on the minimum number of rows when the bound
on the chromatic number in the second stage is used, for the conservative or optimistic estimates. The NaÄ±Ìˆve
method is plotted for comparison. Better coloring bounds shift the minima leftward, reducing the number
of rows produced in both stages.
Thus far we have considered bounds on the chromatic number. Better estimation of Ï‡(G) is complicated
by the fact that we do not have much information about the structure of G until the first stage is run. In
practice, however, G is known after the first stage and hence an algorithmic method to bound its chromatic
number can be applied. Because the number of vertices in G equals the number of uncovered interactions

13

4

2.2

x 10

Conservative estimate
Optimistic estimate
Simple

Number of rows required

2

1.8

1.6

1.4

1.2

1

1

1.1

1.2

1.3

1.4
N

1.5

1.6

1.7

1.8
4

x 10

Figure 8: Size of the partial array vs. size of the complete CA. t = 6, k = 56, v = 3. Stein-LovaÌsz-Johnson
bound requires 17, 403 rows, discrete Stein-LovaÌsz-Johnson bound requires 13, 021 rows. Simple estimate
for the two stage algorithm is 13, 328 rows, conservative estimate assuming m = 2Î³ is 12, 159 rows, and
optimistic estimate assuming m = Î³ is 11, 919 rows. Even the conservative estimate beats the discrete
Stein-LovaÌsz-Johnson bound.

14

after the first stage, we encounter the same trade-off between time and storage, and final array size, as seen
earlier for density. Hence we again parametrize by the expected number of uncovered interactions in the
first stage.
We employ two different greedy algorithms to color the incompatibility graph. In method Col we first
construct the incompatibility graph G after the first stage. Then we apply the commonly used smallest last
order heuristic to order the vertices for greedy coloring: At each stage, find a vertex vi of minimum degree
in Gi , order the vertices of Gi âˆ’ vi , and then place vi at the end. More precisely, we order the vertices of G
as v1 , v2 , . . . , vn , such that vi is a vertex of minimum degree in Gi , where Gi = G âˆ’ {vi+1 , . . . , vn }. A graph
is d-degenerate if all of its subgraphs have a vertex with degree at most d. When G is d-degenerate but not
(d âˆ’ 1)-degenerate, the Coloring number col(G) is d + 1. If we then greedily color the vertices with the first
available color, at most col(G) colors are used.
In method Greedy we employ an on-line, greedy approach that colors the interactions as they are discovered in the first stage. In this way, the incompatibility graph is never constructed. We instead maintain a set
of rows. Some entries in rows are fixed to a specific value; others are flexible to take on any value. Whenever
a new interaction is found to be uncovered in the first stage, we check if any of the rows is compatible with
this interaction. If such a row is found then entries in the row are fixed so that the row now covers the
interaction. If no such row exists, a new row with exactly t fixed entries corresponding to the interaction is
added to the set of rows. This method is much faster than method Col in practice.

3.4

Using group action

Covering arrays that are invariant under the action of a permutation group on their symbols can be easier
to construct and are often smaller [15]. Direct and computational constructions using group actions are
explored in [9, 28]. Sarkar et al. [33] establish the asymptotically tightest known bounds on CAN(t, k, v)
using group actions. In this section we explore the implications of group actions on two-stage algorithms.
Let Î“ be a permutation group on the set of symbols. The action of this group partitions the set of
t-way interactions into orbits. We construct an array A such that for every orbit, at least one row covers
an interaction from that orbit. Then we develop the rows of A over Î“ to obtain a covering array that is
invariant under the action of Î“. Effort then focuses on covering all the orbits of t-way interactions, instead
of the individual interactions.
If Î“ acts sharply transitively
on the set of symbols

 (for example, if Î“ is a cyclic group of order v) then
the action of Î“ partitions kt v t interactions into kt v tâˆ’1 orbits of length v each. Following
the lines of the

v tâˆ’1
+1
log (kt)+(tâˆ’1) log v+log log tâˆ’1
âˆ’1

 v
that covers at
proof of Theorem 2, there exists an n Ã— k array with n =
v tâˆ’1
log

v tâˆ’1 âˆ’1

least one interaction from each orbit. Therefore,
log
CAN(t, k, v) â‰¤ v

k
t



+ (t âˆ’ 1) log v + log log


v tâˆ’1
log vtâˆ’1
âˆ’1



v tâˆ’1
v tâˆ’1 âˆ’1



+1
.

(1)

Similarly, we can employ a Frobenius group. When v is a prime power, the Frobenius group is the group
of permutations of Fv of the form {x 7â†’ ax + b : a, b âˆˆ Fv , a 6= 0}. The action of the Frobenius group
tâˆ’1
partitions the set of t-tuples on v symbols into v vâˆ’1âˆ’1 orbits of length v(v âˆ’ 1) (full orbits) each and 1 orbit
of length v (a short orbit). The short orbit consists of tuples of the form (x1 , . . . , xt ) where x1 = . . . = xt .
Therefore, we can obtain a covering array by first constructing an array that covers all the full orbits, and
then developing all the rows over the Frobenius group and adding v constant rows. Using the two stage
strategy in conjunction with the Frobenius group action we obtain:


 tâˆ’1 

v tâˆ’1
log kt + log v vâˆ’1âˆ’1 + log log vtâˆ’1
âˆ’v+1 + 1


CAN(t, k, v) â‰¤ v(v âˆ’ 1)
+ v.
(2)
tâˆ’1
v
log vtâˆ’1
âˆ’v+1

15

4

1.5

x 10

N âˆ’ number of rows

1.45

Twoâˆ’stage (simple)
Twoâˆ’stage (cyclic group action)
Twoâˆ’stage (Frobenius group action)

1.4

1.35

1.3

1.25
50

55

60

65

70

75

k

Figure 9: Comparison of the simple two-stage bound with the cyclic and the Frobenius two-stage bounds.
t = 6, v = 3 and 50 â‰¤ k â‰¤ 75. Group action reduces the required number of rows slightly.
Figure 9 compares the simple two-stage bound with the cyclic and Frobenius two-stage bounds. For
t = 6, v = 3 and 12 â‰¤ k â‰¤ 100, the cyclic bound requires 7-21 (on average 16) fewer rows than the simple
bound. In the same range the Frobenius bound requires 17 âˆ’ 51 (on average 40) fewer rows.
Group action can be applied in other methods for the second stage as well. Colbourn [15] incorporates
group action into the density algorithm, allowing us to apply method Den in the second stage.
Greedy extends easily to use group action, as we do not construct an explicit incompatibility graph.
Whenever we fix entries in a row to cover an uncovered orbit, we commit to a specific orbit representative.
However, applying group action to the incompatibility graph coloring for Col is more complicated. We
need to modify the definition of the incompatibility graph for two reasons. First the vertices no longer
represent uncovered interactions, but rather uncovered orbits of interaction. Secondly, and perhaps more
importantly, pairwise compatibility between every two orbits in a set no longer implies mutual compatibility
among all orbits in the set.
One approach is to form a vertex for each uncovered orbit, placing an edge between two when they share
a column. Rather than the usual coloring, however, one asks for a partition of the vertex set into classes so
that every class induces an acyclic subgraph. Problems of this type are generalized graph coloring problems
[4]. Within each class of such a vertex partition, consistent representatives of each orbit can be selected to
form a row; when a cycle is present, this may not be possible. Unfortunately, heuristics for solving these
types of problems appear to be weak, so we adopt a second approach. As we build the incompatibility
graph, we commit to specific orbit representatives. When a vertex for an uncovered orbit is added, we check
its compatibility with the orbit representatives chosen for the orbits already handled with which it shares
columns; we commit to an orbit representative and add edges to those with which it is now incompatible.
Once completed, we have a (standard) coloring problem for the resulting graph.
Because group action can be applied using each of the methods for the two stages, we extend our naming
to TS hA, B; r, Î“i, where Î“ can be Trivial (i.e. no group action), Cyclic, or Frobenius.

16

4

Computational results

Figure 5 indicates that even a simple two-stage bound can improve on best known covering array numbers.
Therefore we investigate the actual performance of our two-stage algorithms for covering arrays of strength
5 and 6.
First we present results for t = 6, when v âˆˆ {3, 4, 5, 6} and no group action is assumed. Table 1 shows the
results for different v values. In each case we select the range of k values where the two-stage bound predicts
smaller covering arrays than
 previously known best ones, setting the maximum number of uncovered
 tthe
v
interactions as Ï = 1/ log vt âˆ’1 â‰ˆ v t . For each value of k we construct a single partial array and then run
the different second stage algorithms on it consecutively. In this way all the second stage algorithms cover
the same set of uncovered interactions.
The column tab lists the best known CAN(t, k, v) upper bounds from [13]. The column bound shows the
upper bounds obtained from the two-stage bound (2). The columns naÄ±Ìˆve, greedy, col and den show results
obtained from running the TS hRand, Naive; Ï, Triviali, TS hRand, Greedy; Ï, Triviali, TS hRand, Col; Ï, Triviali
and TS hRand, Den; Ï, Triviali algorithms, respectively.
The naÄ±Ìˆve method always finds a covering array that is smaller than the two-stage bound. This happens
because we repeat the first stage of Algorithm 4 until the array has fewer than v t uncovered interactions.
(If the first stage were not repeated, the algorithm still produce covering arrays that are not too far from
the bound.) For v = 3 Greedy and Den have comparable performance. Method Col produces covering arrays
that are smaller. However, for v âˆˆ {4, 5, 6} Den and Col are competitive.
Table 2 shows the results obtained by the different second stage algorithms when the maximum number
of uncovered interactions in the first stage is set to 2Ï and 3Ï respectively. When more interactions are
covered in the second stage, we obtain smaller arrays as expected. However, the improvement in size does
not approach 50%. There is no clear winner.
Next we investigate the covering arrays that are invariant under the action of a cyclic group. In Table 3 the
column bound shows the upper bounds from Equation (1). The columns naÄ±Ìˆve, greedy, col and den show
results obtained from running TS hRand, Naive; Ï, Cyclici, TS hRand, Greedy; Ï, Cyclici, TS hRand, Col; Ï, Cyclici
and TS hRand, Den; Ï, Cyclici, respectively.
Table 4 presents results for cyclic group action based algorithms when the number of maximum uncovered
interactions in the first stage is set to 2Ï and 3Ï respectively.
For the Frobenius group action, we show results only for v âˆˆ {3, 5} in Table 5. The column bound shows
the upper bounds obtained from Equation (2).
Table 6 presents results for Frobenius group action algorithms when the number of maximum uncovered
interactions in the first stage is 2Ï or 3Ï.
Next we present a handful of results when t = 5. In the cases examined, using the trivial group action
is too time consuming to be practical. However, the cyclic or Frobenius cases are feasible. Tables 7 and 8
compare two stage algorithms when the number of uncovered interactions in the first stage is at most 2Ï.
In almost all cases there is no clear winner among the three second stage methods. Methods Den and
Greedy are, however, substantially faster and use less memory than method Col; for practical purposes they
would be preferred.
All code used in this experimentation is available from the github repository
https://github.com/ksarkar/CoveringArray
under an open source GPLv3 license.

5

Limited dependence and the Moser-Tardos algorithm

Here we explore a different randomized algorithm that produces smaller covering arrays than Algorithm 1.
When k > 2t, there are interactions that share no column. The events of coverage of such interactions are
independent. Moser et al. [29, 30] provide an efficient randomized construction method that exploits this

17

k

tab

bound

53
54
55
56
57

13021
14155
17161
19033
20185

13076
13162
13246
13329
13410

39
40
41
42
43
44

68314
71386
86554
94042
99994
104794

65520
66186
66834
67465
68081
68681

31
32
33
34
35
36

233945
258845
281345
293845
306345
356045

226700
229950
233080
236120
239050
241900

17
18
19
20
21
22
23
24
25
26
27

506713
583823
653756
694048
783784
844834
985702
1035310
1112436
1146173
1184697

486310
505230
522940
539580
555280
570130
584240
597660
610460
622700
634430

naÄ±Ìˆve
greedy
t = 6, v = 3
13056
12421
13160
12510
13192
12590
13304
12671
13395
12752
t = 6, v = 4
65452
61913
66125
62573
66740
63209
67408
63819
68064
64438
68556
65021
t = 6, v = 5
226503 213244
229829 216444
232929 219514
235933 222516
238981 225410
241831 228205
t = 6, v = 6
486302 449950
505197 468449
522596 485694
539532 502023
555254 517346
569934 531910
584194 545763
597152 558898
610389 571389
622589 583473
634139 594933

col

den

12415
12503
12581
12665
12748

12423
12512
12591
12674
12757

61862
62826
63160
64077
64935
65739

61886
62835
63186
64082
64907
65703

212942
217479
219215
222242
226379
230202

212940
217326
219241
222244
226270
229942

448922
467206
484434
500788
516083
530728
544547
557917
570316
582333
593857

447864
466438
483820
500194
515584
530242
548307
557316
569911
582028
593546

Table 1: Comparison of different TS hRand, âˆ’; Ï, Triviali algorithms.

18

k
greedy

2Ï
col

53
54
55
56
57

11968
12135
12286
12429
12562

11958
12126
12129
12204
12290

39
40
41
42
43
44

59433
60090
60715
61330
61936
62530

59323
60479
61527
62488
61839
62899

31
32
33
34
35
36

204105
207243
210308
213267
216082
218884

203500
206659
209716
212675
215521
218314

17
18
19
20
21
22
23
24
25
26
27

425053
443236
460315
476456
491570
505966
519611
532612
544967
556821
568135

-

den
t = 6, v
11968
12050
12131
12218
12296
t = 6, v
59326
59976
60615
61242
61836
62428
t = 6, v
203302
206440
209554
212508
215389
218172
t = 6, v
420333
438754
455941
472198
487501
502009
515774
528868
541353
553377
564827

greedy
=3
11716
11804
11877
11961
12044
=4
58095
58742
59369
59974
60575
61158
=5
199230
202342
205386
208285
211118
213872
=6
412275
430402
447198
463071
478269
492425
505980
518746
531042
542788
554052

3Ï
col

den

11705
11787
11875
12055
12211

11708
11790
11872
11950
12034

57951
58583
59867
61000
60407
61004

57888
58544
59187
59796
60393
60978

198361
201490
204548
-

197889
201068
204107
207060
209936
212707

-

405093
423493
440532
456725
471946
486306
500038
513047
525536
537418
548781

Table 2: Comparison of TS hRand, âˆ’; 2Ï, Triviali and TS hRand, âˆ’; 3Ï, Triviali algorithms.

19

k

tab

bound

53
54
55
56
57

13021
14155
17161
19033
20185

13059
13145
13229
13312
13393

k
39
40
41
42
43
44

tab
68314
71386
86554
94042
99994
104794

bound
65498
66163
66811
67442
68057
68658

31
32
33
34
35
36

226000
244715
263145
235835
238705
256935

226680
229920
233050
236090
239020
241870

17
18
19
20
21
22
23
24
25
26
27

506713
583823
653756
694048
783784
844834
985702
1035310
1112436
1146173
1184697

486290
505210
522910
539550
555250
570110
584210
597630
610430
622670
624400

naÄ±Ìˆve
greedy
t = 6, v = 3
13053
12405
13119
12489
13209
12573
13284
12660
13368
12744
t = 6, v = 4
naÄ±Ìˆve
greedy
65452
61896
66080
62516
66740
63184
67408
63800
68032
64408
68556
64988
t = 6, v = 5
226000 213165
229695 216440
233015 219450
235835 222450
238705 225330
241470 228140
t = 6, v = 6
485616 449778
504546 468156
522258 485586
539280 501972
554082 517236
569706 531852
583716 545562
597378 558888
610026 571380
622290 583320
633294 594786

col

den

12405
12543
12663
12651
12744

12411
12546
12663
12663
12750

col
61860
62820
63144
63780
64692
64964

den
61864
62784
63152
63784
64680
64976

212945
217585
221770
222300
225130
229235

212890
217270
221290
222210
225120
229020

448530
467232
490488
500880
521730
530832
549660
557790
575010
582546
598620

447732
466326
488454
500172
519966
530178
548196
557280
573882
582030
597246

Table 3: Comparison of TS hRand, âˆ’; Ï, Cyclicialgorithms.

20

k
greedy

2Ï
col

53
54
55
56
57

11958
12039
12120
12204
12276

11955
12027
12183
12342
12474

39
40
41
42
43
44

59412
60040
60700
61320
61908
62512

59336
59996
61156
62196
63192
64096

31
32
33
34
35
36

204060
207165
207165
213225
216050
218835

203650
209110
209865
212830
217795
218480

17
18
19
20
21
22
23
24
25
26
27

424842
443118
460014
476328
491514
505884
519498
532368
544842
543684
568050

422736
440922
457944
474252
489270
503580
517458
530340
542688
543684
566244

den
t = 6, v
11958
12036
12195
12324
12450
t = 6, v
59304
59964
61032
61976
62852
63672
t = 6, v
203265
208225
209540
212510
217070
218155
t = 6, v
420252
438762
455994
472158
487500
501852
515718
528828
541332
543684
564756

greedy
=3
11700
11790
11862
11949
12027
=4
58076
58716
59356
59932
60568
61152
=5
199180
202255
205380
208225
211080
213770
=6
411954
430506
447186
463062
478038
492372
505824
518700
530754
542664
553704

3Ï
col

den

11691
11874
12057
11937
12021

11694
11868
12027
11943
12024

57976
58616
59252
59840
61124
61048

57864
58520
59160
59760
60904
60988

198455
204495
204720
207790
213425
213185

197870
203250
204080
207025
212040
212695

409158
427638
456468
460164
486180
489336
502806
515754
538056
539922
560820

405018
423468
449148
456630
479970
486264
500040
512940
532662
537396
555756

Table 4: Comparison of TS hRand, âˆ’; 2Ï, Cyclici and TS hRand, âˆ’; 3Ï, Cyclici algorithms.

21

k

tab

bound

53
54
55
56
57

13021
14155
17161
19033
20185

13034
13120
13203
13286
13366

31
32
33
34
35
36

233945
258845
281345
293845
306345
356045

226570
229820
232950
235980
238920
241760

naÄ±Ìˆve
greedy
t = 6, v = 3
13029
12393
13071
12465
13179
12561
13245
12633
13365
12723
t = 6, v = 5
226425 213025
229585 216225
232725 219285
234905 222265
238185 225205
241525 227925

col

den

12387
12513
12549
12627
12717

12393
12531
12567
12639
12735

212865
216085
219205
223445
227445
231145

212865
216065
219145
223265
227065
230645

Table 5: Comparison of TS hRand, âˆ’; Ï, Frobeniusi algorithms.

k
greedy

2Ï
col

53
54
55
56
57
70
75
80
85
90

11931
12021
12105
12171
12255
13167
13473
13773
14031
14289

11919
12087
12237
12171
12249
13155
13473
13767
14025
14283

31
32
33
34
35
36
50
55
60
65

203785
206965
209985
213005
215765
218605
250625
259785
268185
275785

203485
208965
209645
214825
215545
218285
250365
259625
268025
275665

den
greedy
t = 6, v = 3
11931
11700
12087
11790
12231
11862
12183
11949
12255
12027
13179
13479
13779
14037
14301
t = 6, v = 5
203225 198945
208065 201845
209405 205045
214145 208065
215265 210705
218025 213525
250325
259565
267945
275665
-

3Ï
col

den

11691
11874
12057
11937
12021
-

11694
11868
12027
11943
12024
-

198445
204505
209845
207545
210365
213105
-

197825
203105
207865
206985
209885
212645
-

Table 6: Comparison of TS hRand, âˆ’; 2Ï, Frobeniusi and TS hRand, âˆ’; 3Ï, Frobeniusi algorithms.

22

k
67
68
69
70
71

tab
59110
60991
60991
60991
60991

greedy
48325
48565
48765
49005
49245

col
48285
48565
49005
48985
49205

den
48305
48585
48985
49025
49245

Table 7: Comparison of TS hRand, âˆ’; 2Ï, Frobeniusi algorithms. t = 5, v = 5
k
49
50
51
52
53

tab
122718
125520
128637
135745
137713

greedy
108210
109014
109734
110556
111306

col
108072
108894
110394
110436
111180

den
107988
108822
110166
110364
111120

Table 8: Comparison of TS hRand, âˆ’; 2Ï, Cyclici algorithms. t = 5, v = 6
limited dependence. Specializing their method to covering arrays, we obtain Algorithm 5. For the specified
value of N in the algorithm it is guaranteed that the expected number of times the loop in line 3 of Algorithm
5 is repeated is linearly bounded in k (See Theorem 1.2 of [30]).
The upper bound on CAN(t, k, v) guaranteed by Algorithm 5 is obtained by applying the LovaÌsz local
lemma (LLL).
Lemma 4. (LovaÌsz local lemma; symmetric case) (see [2]) Let A1 , A2 , . . . , An events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at
most d, and that Pr[Ai ] â‰¤ p for all 1 â‰¤ i â‰¤ n. If ep(d + 1) â‰¤ 1, then Pr[âˆ©ni=1 AÌ„i ] > 0.
The symmetric version of LovaÌsz local lemma provides an upper bound on the probability of a â€œbadâ€
event in terms of the maximum degree of a bad event in a dependence graph, so that the probability that
all the bad events are avoided is non zero. Godbole, Skipper, and Sunley [18] apply Lemma 4 essentially to
obtain the bound on CAN(t, k, v) in line 1 of Algorithm 5.
Theorem 5. [18] Let t, v and k â‰¥ 2t be integers with t, v â‰¥ 2. Then
n 
o
+ t log v + 1
log kt âˆ’ kâˆ’t
t


CAN (t, k, v) â‰¤
t
log vtvâˆ’1
The bound on the size of covering arrays obtained from Theorem 5 is asymptotically tighter than the
one obtained from Theorem 1. Figure 10 compares the bounds for t = 6 and v = 3.
The original proof of LLL is essentially non-constructive and does not immediately lead to a polynomial
time construction algorithm for covering arrays satisfying the bound of Theorem 5. Indeed no previous
construction algorithms appear to be based on it. However the Moser-Tardos method of Algorithm 5 does
provide a construction algorithm running in expected polynomial time. For sufficiently large values of k
Algorithm 5 produces smaller covering arrays than the Algorithm 1.
But the question remains: Does Algorithm 5 produce smaller covering arrays than the currently best
known results within the range that it can be effectively computed? Perhaps surprisingly, we show that
the answer is affirmative. In Algorithm 5 we do not need to store the coverage information of individual
interactions in memory because each time an uncovered interaction is encountered we re-sample the columns
involved in that interaction and start the check afresh (checking the coverage in interactions in the same
order each time). Consequently, Algorithm 5 can be applied for larger values of k than the density algorithm.
23

Algorithm 5: Moser-Tardos type algorithm for covering array construction.
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; t, k, v)
log{(kt)âˆ’(kâˆ’t
log v+1
)}+t.
t

;
1 Let N :=
vt
log

2

3
4
5
6
7
8
9
10
11
12
13

14
15
16

v t âˆ’1

Construct an N Ã— k array A where each entry is chosen independently and uniformly at random from
a v-ary alphabet;
repeat
Set covered := true;
for each interaction Î¹ âˆˆ It,k,v do
if Î¹ is not covered in A then
Set covered := false;
Set missing-interaction := Î¹;
break;
end
end
if covered = false then
Choose all the entries in the t columns involved in missing-interaction independently and
uniformly at random from the v-ary alphabet;
end
until covered = true;
Output A;

Smaller covering arrays can be obtained by exploiting a group action using LLL, as shown in [33]. Table
9 shows the sizes of the covering arrays constructed by a variant of Algorithm 5 that employs cyclic and
Frobenius group actions. While this single stage algorithm produces smaller arrays than the currently best
known [13], these are already superseded by the two-stage based algorithms.
k
56
57
58
59
60

tab
19033
20185
23299
23563
23563

MT
16281
16353
16425
16491
16557

(a) Frobenius. t = 6, v = 3

k
44
45
46
47
48

tab
411373
417581
417581
423523
423523

MT
358125
360125
362065
363965
365805

(b) Frobenius. t = 6, v = 5

k
25
26
27
28
29

tab
1006326
1040063
1082766
1105985
1149037

MT
1020630
1032030
1042902
1053306
1063272

(c) Cyclic. t = 6, v = 6

Table 9: Comparison of covering array size from Algorithm 5 (MT) with the best known results [13] (tab).

5.1

Moser-Tardos type algorithm for the first stage

The linearity of expectation arguments used in the SLJ bounds permit one to consider situations in which
a few of the â€œbadâ€ events are allowed to occur, a fact that we exploited in the first stage of the algorithms
thus far. However, the LovaÌsz local lemma does not address this situation directly. The conditional LovaÌsz
local lemma (LLL) distribution, introduced in [19], is a very useful tool.
Lemma 6. (Conditional LLL distribution; symmetric case) (see [2, 33]) Let A = {A1 , A2 , . . . , Al } be a set
of l events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of
all other events Aj except for at most d, and that Pr[Ai ] â‰¤ p for all 1 â‰¤ i â‰¤ l. Also suppose that ep(d+1) â‰¤ 1
(Therefore, by LLL (Lemma 4) Pr[âˆ©li=1 AÌ„i ] > 0). Let B âˆˆ
/ A be another event in the same probability space
24

5

10

N âˆ’ number of rows

SLJ bound
GSS bound

4

10

3

10
1
10

2

3

10

10
k

4

10

5

10

Figure 10: Comparison of SLJ (Theorem 1) and GSS (Theorem 5) bounds for t = 6 and v = 3. The graph
is plotted in log-log scale to highlight the asymptotic difference between the two bounds.
with Pr[B] â‰¤ q, such that B is also mutually independent of a set of all other events Aj âˆˆ A except for at
most d. Then Pr[B| âˆ©li=1 AÌ„i ] â‰¤ eq.
We apply the conditional
LLL distribution to obtain an upper bound on the size of partial array that
 t 
v
leaves at most log vt âˆ’1 â‰ˆ v t interactions uncovered. For a positive integer k, let I = {j1 , . . . , jÏ } âŠ† [k]
where j1 < . . . < jÏ . Let A be an n Ã— k array where each entry is from the set [v]. Let AI denote the n Ã— Ï
array in which AI (i, `) = A(i, j` ) for 1 â‰¤ i â‰¤ N and 1 â‰¤ ` â‰¤ Ï; AI is the projection of A onto the columns
in I.

Let M âŠ† [v]t be a set of m t-tuples of symbols, and C âˆˆ [k]
be a set of t columns. Suppose the
t
entries in the array A are chosen independently from [v] with uniform probability.
 Let BC denote the event
that at least one of the tuples in M is not covered in AC . There are Î· = kt such events, and for all of
n
them Pr[BC ] â‰¤ m 1 âˆ’ v1t . Moreover, when k â‰¥ 2t, each of the events is mutually independent of all



k
other events except for at most Ï = kt âˆ’ kâˆ’t
âˆ’ 1 < t tâˆ’1
. Therefore, by the LovaÌsz local lemma, when
t

1 n
eÏm 1 âˆ’ vt â‰¤ 1, none of the events BC occur. Solving for n, when
nâ‰¥

log(eÏm)


t
log vtvâˆ’1

(3)


there exists an n Ã— k array A over [v] such that for all C âˆˆ [k]
t , AC covers all the m tuples in M . In fact
we can use a Moser-Tardos type algorithm to construct such an array.
Let Î¹ be an interaction whose t-tuple
n of symbols is not in M . Then the probability that Î¹ is not covered
in an n Ã— k array is at most 1 âˆ’ v1t
when each entry of the array is chosen independently from [v] with
uniform probability. Therefore, by the
 conditional LLL distribution the probability that Î¹1 is
n not covered
in the array A where for all C âˆˆ [k]
. Moreover,
t , AC covers all the m tuples in M is at most e 1 âˆ’ v t
there are Î·(v t âˆ’ m) such interactions Î¹. By the linearity of expectation, the expected number of uncovered
25

n
interactions in A is less than v t when Î·(v t âˆ’ m)e 1 âˆ’ v1t â‰¤ v t . Solving for n, we obtain
	

log Î·e 1 âˆ’ vmt

 .
nâ‰¥
t
log vtvâˆ’1

Therefore, there exists an n Ã— k array with n = max

log{Î·e(1âˆ’ vmt )}
log(eÏm)


,

t
t
log vtvâˆ’1
log vtvâˆ’1



(4)

that has at most v t

uncovered interactions. To compute n explicitly, we must choose m. We can select a value of m to minimize
n graphically for given values of t, k and v. For example, Figure 11 plots Equations 3 and 4 against m for
t = 3, k = 350, v = 3, and finds the minimum value of n.
460

445
max(Equation (3), Equation (4))

440

n âˆ’ number of rows in the partial array

n âˆ’ number of rows in the partial array

Equation (3)
Equation (4)

420

400

380

360

340
0

5

10

15
m

20

25

30

(a) Equations 3 and 4 against m.

440

435

430

425

420
0

5

10

15
m

20

25

30

(b) Maximum of the two sizes against m.

Figure 11: The minimum is at n = 422, when m = 16. t = 3, k = 350, v = 3
We compare the size of the partial array from the naÄ±Ìˆve two-stage method (Algorithm 4) with the size
obtained by the graphical methods in Figure 12. The LovaÌsz local lemma based method is asymptotically
better than the simple randomized method. However, except for the small values of t and v, in the range
of k values relevant for practical applications the simple randomized algorithm requires fewer rows than the
LovaÌsz local lemma based method.

5.2

LovaÌsz local lemma based two-stage bound

We can apply the techniques from Section 5.1 to obtain a two-stage bound similar to Theorem 2 using the
LovaÌsz local lemma and conditional LLL distribution. First we extend a result from [33].



Theorem
7. Let t, k, v be integers with k â‰¥ t â‰¥ 2, v â‰¥ 2 and let Î· = kt , and Ï = kt âˆ’ kâˆ’t
t . If


vt
v t âˆ’1

Î·v t log

Ï

â‰¤ v t Then
log
CAN(t, k, v) â‰¤

k
t



+ t log v + log log


t
log vtvâˆ’1



vt
v t âˆ’1



+2

Î·
âˆ’ .
Ï

Proof. Let M âŠ† [v]t be a set of m t-tuples of symbols. Following the arguments of Section 5.1, when


 there exists an n Ã— k array A over [v] such that for all C âˆˆ [k] , AC covers all m tuples in M .
n â‰¥ log(eÏm)
t
vt
log

v t âˆ’1

At most Î·(v t âˆ’ m) interactions are uncovered in such an array. Using the conditional
n LLL distribution,
the probability that one such interaction is not covered in A is at most e 1 âˆ’ v1t . Therefore, by the
26

n âˆ’ number of rows in partial array with vt missing interactions

n âˆ’ number of rows in partial array with vt missing interactions

550
500
450
400
350
300
250
200
150
Randomized (Algorithm 4)
LLL based

100
50

0

200

400

600
k

800

1000

1200

2500

2000

1500

1000

500

0

0

Randomized (Algorithm 4)
LLL based

500

1000

1500

2000

2500

3000

3500

4000

4500

k

(a) t = 3, v = 3.

(b) t = 4, v = 3.

Figure 12: Comparison of the size of the partial array constructed in the first stage. Compares the size of
the partial array specified in Algorithm 4 in Section 3.1, and the size derived in Section 5.1.
 t

n
linearity of expectation, we can find one such array A that leaves at most eÎ·(v t âˆ’ m) 1 âˆ’ v1t = Î·Ï vm âˆ’ 1
interactions uncovered. Adding one row per uncovered interactions to A, we obtain a covering array with at
most N rows, where


log(eÏm)
Î· vt

+
N=
âˆ’
1
t
Ï m
log tv
The value of N is minimized when m =

v âˆ’1
 t 
Î·v t log vtvâˆ’1
Ï

. Because m â‰¤ v t , we obtain the desired bound.

When m = v t this recaptures the bound of Theorem 5.
Figure 13 compares the LLL based two-stage bound from Theorem 7 to the standard two-stage bound
from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1. Although the
LLL based two-stage bound is tighter than the LLL based Godbole et al. bound, even for quite large values
of k the standard two-stage bound is tighter than the LLL based two-stage bound. In practical terms, this
specific LLL based two-stage method does not look very promising, unless the parameters are quite large.

6

Conclusion and open problems

Many concrete algorithms within a two-stage framework for covering array construction have been introduced
and evaluated. The two-stage approach extends the range of parameters for which competitive covering arrays
can be constructed, each meeting an a priori guarantee on its size. Indeed as a consequence a number of
best known covering arrays have been improved upon. Although each of the methods proposed has useful
features, our experimental evaluation suggests that TS hRand, Greedy; 2Ï, Î“i and TS hRand, Den; 2Ï.Î“i with
Î“ âˆˆ {Cyclic, Frobenius} realize a good trade-off between running time and size of the constructed covering
array.
Improvements in the bounds, or in the algorithms that realize them, are certainly of interest. We
mention some specific directions. Establishing tighter bounds on the coloring based methods of Section 3.3
is a challenging problem. Either better estimates of the chromatic number of the incompatibility graph after
a random first stage, or a first stage designed to limit the chromatic number, could lead to improvements in
the bounds.
In Section 5.1 and 5.2 we explored using a Moser-Tardos type algorithm for the first stage. Although this
is useful for asymptotic bounds, practical improvements appear to be limited. Perhaps a different approach of
27

10000

500

9000

450

8000

400

7000

N âˆ’ number of rows

N âˆ’ number of rows

550

350
300
250

SLJ bound
Godbole
LLLâˆ’2âˆ’stage
2âˆ’stage

200
150
100

0

50

100

150

200

250

300

350

400

6000
5000
4000

SLJ bound
Godbole
LLLâˆ’2âˆ’stage
2âˆ’stage

3000
2000

450

k

(a) t = 3, v = 3.

1000
0

1000

2000

3000

4000
k

5000

6000

7000

8000

(b) t = 4, v = 4.

Figure 13: Comparison among the LLL based two-stage bound from Theorem 7, the standard two-stage
bound from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1.
reducing the number of bad events to be avoided explicitly by the algorithm may lead to a better algorithm.
A potential approach may look like following: â€œBadâ€ events would denote non-coverage of an interaction
over a t-set of columns. We would select a set of column t-sets such that the dependency graph of the
corresponding bad events have a bounded maximum degree (less than the original dependency graph). We
would devise a Moser-Tardos type algorithm for covering all the interactions on our chosen column t-sets,
and then apply the conditional LLL distribution to obtain an upper bound on the number of uncovered
interactions. However, the difficulty lies in the fact that â€œall vertices have degree â‰¤ Ïâ€ is a non-trivial,
â€œhereditaryâ€ property for induced subgraphs, and for such properties finding a maximum induced subgraph
with the property is an NP-hard optimization problem [17]. There is still hope for a randomized or â€œnibbleâ€
like strategy that may find a reasonably good induced subgraph with a bounded maximum degree. Further
exploration of this idea seems to be a promising research avenue.
In general, one could consider more than two stages. Establishing the benefit (or not) of having more
than two stages is also an interesting open problem. Finally, the application of the methods developed to
mixed covering arrays appears to provide useful techniques for higher strengths; this merits further study as
well.

Acknowledgments
The research was supported in part by the National Science Foundation under Grant No. 1421058.

References
[1] A. N. Aldaco, C. J. Colbourn, and V. R. Syrotiuk. Locating arrays: A new experimental design for
screening complex engineered systems. SIGOPS Oper. Syst. Rev., 49(1):31â€“40, Jan. 2015.
[2] N. Alon and J. H. Spencer. The probabilistic method. Wiley-Interscience Series in Discrete Mathematics
and Optimization. John Wiley & Sons, Inc., Hoboken, NJ, third edition, 2008. With an appendix on
the life and work of Paul ErdoÌ‹s.
[3] Android. Android configuration class, 2016. http://developer.android.com/reference/android/content/
res/Configuration.html.
[4] J. I. Brown. The complexity of generalized graph colorings. Discrete Appl. Math., 69(3):257â€“270, 1996.
28

[5] R. C. Bryce and C. J. Colbourn. The density algorithm for pairwise interaction testing. Software
Testing, Verification, and Reliability, 17:159â€“182, 2007.
[6] R. C. Bryce and C. J. Colbourn. A density-based greedy algorithm for higher strength covering arrays.
Software Testing, Verification, and Reliability, 19:37â€“53, 2009.
[7] R. C. Bryce and C. J. Colbourn. Expected time to detection of interaction faults. Journal of Combinatorial Mathematics and Combinatorial Computing, 86:87â€“110, 2013.
[8] J. N. Cawse. Experimental design for combinatorial and high throughput materials development. GE
Global Research Technical Report, 29:769â€“781, 2002.
[9] M. A. Chateauneuf, C. J. Colbourn, and D. L. Kreher. Covering arrays of strength 3. Des. Codes
Crypt., 16:235â€“242, 1999.
[10] D. M. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG system: An approach to
testing based on combinatorial design. IEEE Transactions on Software Engineering, 23:437â€“44, 1997.
[11] M. B. Cohen. Designing test suites for software interaction testing. PhD thesis, The University of
Auckland, Department of Computer Science, 2004.
[12] C. J. Colbourn. Combinatorial aspects of covering arrays. Le Matematiche (Catania), 58:121â€“167, 2004.
[13] C. J. Colbourn. Covering array tables, 2005-2015. http://www.public.asu.edu/âˆ¼ccolbou/src/tabby.
[14] C. J. Colbourn. Covering arrays and hash families. In Information Security and Related Combinatorics,
NATO Peace and Information Security, pages 99â€“136. IOS Press, 2011.
[15] C. J. Colbourn. Conditional expectation algorithms for covering arrays. Journal of Combinatorial
Mathematics and Combinatorial Computing, 90:97â€“115, 2014.
[16] R. Diestel. Graph Theory. Graduate Texts in Mathematics. Springer, fourth edition, 2010.
[17] M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NPCompleteness. W. H. Freeman & Co., New York, NY, USA, 1979.
[18] A. P. Godbole, D. E. Skipper, and R. A. Sunley. t-covering arrays: upper bounds and Poisson approximations. Combinatorics, Probability and Computing, 5:105â€“118, 1996.
[19] B. Haeupler, B. Saha, and A. Srinivasan. New constructive aspects of the LovaÌsz local lemma. J. ACM,
58(6):Art. 28, 28, 2011.
[20] A. S. Hedayat, N. J. A. Sloane, and J. Stufken. Orthogonal Arrays. Springer-Verlag, New York, 1999.
[21] D. S. Johnson. Approximation algorithms for combinatorial problems. J. Comput. System Sci., 9:256â€“
278, 1974.
[22] G. O. H. Katona. Two applications (for search theory and truth functions) of Sperner type theorems.
Periodica Math., 3:19â€“26, 1973.
[23] D. Kleitman and J. Spencer. Families of k-independent sets. Discrete Math., 6:255â€“262, 1973.
[24] D. R. Kuhn, R. Kacker, and Y. Lei. Introduction to Combinatorial Testing. CRC Press, 2013.
[25] D. R. Kuhn and M. Reilly. An investigation of the applicability of design of experiments to software
testing. In Proc. 27th Annual NASA Goddard/IEEE Software Engineering Workshop, pages 91â€“95, Los
Alamitos, CA, 2002. IEEE.

29

[26] D. R. Kuhn, D. R. Wallace, and A. M. Gallo. Software fault interactions and implications for software
testing. IEEE Trans. Software Engineering, 30:418â€“421, 2004.
[27] L. LovaÌsz. On the ratio of optimal integral and fractional covers. Discrete Math., 13(4):383â€“390, 1975.
[28] K. Meagher and B. Stevens. Group construction of covering arrays. J. Combin. Des., 13:70â€“77, 2005.
[29] R. A. Moser. A constructive proof of the LovaÌsz local lemma. In STOCâ€™09â€”Proceedings of the 2009
ACM International Symposium on Theory of Computing, pages 343â€“350. ACM, New York, 2009.
[30] R. A. Moser and G. Tardos. A constructive proof of the general LovaÌsz local lemma. J. ACM, 57(2):Art.
11, 15, 2010.
[31] C. Nie and H. Leung. A survey of combinatorial testing. ACM Computing Surveys, 43(2):#11, 2011.
[32] A. H. Ronneseth and C. J. Colbourn. Merging covering arrays and compressing multiple sequence
alignments. Discrete Appl. Math., 157:2177â€“2190, 2009.
[33] K. Sarkar and C. J. Colbourn. Upper bounds on the size of covering arrays. ArXiv e-prints.
[34] G. Seroussi and N. H. Bshouty. Vector sets for exhaustive testing of logic circuits. IEEE Trans. Inform.
Theory, 34:513â€“522, 1988.
[35] S. K. Stein. Two combinatorial covering theorems. J. Combinatorial Theory Ser. A, 16:391â€“397, 1974.
[36] J. Torres-Jimenez, H. Avila-George, and I. Izquierdo-Marquez. A two-stage algorithm for combinatorial
testing. Optimization Letters, pages 1â€“13, 2016.

30

Perspectives

The BioIntelligence Framework: a new computational platform for biomedical knowledge computing
Toni Farley,1 Jeff Kiefer,1 Preston Lee,1 Daniel Von Hoff,1 Jeffrey M Trent,1 Charles Colbourn,2 Spyro Mousses1
< Additional material are

published online only. To view these files please visit the journal online (http://dx.doi.org/ 10.1136/amiajnl-2011-000646).
1

The Translational Genomics Research Institute (TGen), Center for BioIntelligence, Phoenix, Arizona, USA 2 School of Computing, Informatics, Decision Systems Engineering, Arizona State University, Tempe, Arizona, USA Correspondence to Dr Spyro Mousses, The Translational Genomics Research Institute (TGen), Center for BioIntelligence, 445 N. Fifth Street, Phoenix, AZ 85004, USA; smousses@tgen.org Received 19 October 2011 Accepted 7 July 2012 Published Online First 2 August 2012

ABSTRACT Breakthroughs in molecular profiling technologies are enabling a new data-intensive approach to biomedical research, with the potential to revolutionize how we study, manage, and treat complex diseases. The next great challenge for clinical applications of these innovations will be to create scalable computational solutions for intelligently linking complex biomedical patient data to clinically actionable knowledge. Traditional database management systems (DBMS) are not well suited to representing complex syntactic and semantic relationships in unstructured biomedical information, introducing barriers to realizing such solutions. We propose a scalable computational framework for addressing this need, which leverages a hypergraph-based data model and query language that may be better suited for representing complex multilateral, multi-scalar, and multi-dimensional relationships. We also discuss how this framework can be used to create rapid learning knowledge base systems to intelligently capture and relate complex patient data to biomedical knowledge in order to automate the recovery of clinically actionable information.

genomic interpretation require: (a) a fundamentally different computational framework for storing and representing disparate data types with complex relationships, and (b) advanced software applications that leverage this framework to structure the representation of prior knowledge so that it can be intelligently linked to patient data. We propose a framework conceptually based on requirements and cognitive strategies for knowledge computing, previously introduced as the BioIntelligence Framework.2 Our framework is compatible with future directions toward computational intelligence. Since it can support the capturing and querying of multilateral and multi-scalar relations among genomes, phenotype, environment, lifestyle, medical history, and clinical outcome data, our platform can support systems with higher order functions such as inference and learning. This will ultimately allow genomic data to be intelligently repurposed beyond personalized medicine to support more sophisticated translational research and highly iterative knowledge discovery.

BIOINTELLIGENCE FRAMEWORK INTRODUCTION
Next generation genomic profiling technologies are generating deep and detailed characterizations of patients and disease states. This data-intensive approach is providing unprecedented insights that can be used to resolve mechanistic complexity and clinical heterogeneity, thereby revolutionizing how we study, manage, and treat complex diseases. To support this revolution, bioinformatics tools are rapidly emerging to process and analyze large-scale complex molecular data sets for discovery research applications. Unfortunately, when it comes to clinical (n¼1) applications of genomics, the data deluge is rapidly outpacing our capacity to interpret rich data sets to extract medically useful and meaningful knowledge. The next great challenge will be to address the manual interpretation bottleneck through the development of computational solutions for intelligently linking complex patient data to actionable biomedical knowledge. This illuminates a need to represent and query large-scale complex relationships distributed across disparate types of biomedical knowledge. A recent report states a goal for the community is to transition from traditional database management to managing potentially unstructured data across many repositories.1 We propose the key challenges for intelligently linking prior knowledge to partially automate
128

Systems biology is concerned with emergent properties in complex interactions of systems of systems involving disparate data elements. Extracting useful information requires syntactic and semantic linking of data within and across large data sets. Systems modeled as networks based on binary graphs (where edges connect node pairs) are suited to capturing bilateral relationships and interactions. To represent multilateral relationships requires a fundamental change in how we model systems. We generalize the binary graph model to a hypergraph model, an approach which has been previously suggested,3 and introduce a hypergraphbased solution for representing multilateral relations and multi-scalar networks. Biological systems may benefit from a flexible data model that supports nesting of data elements and concept abstraction in a more natural manner than functionally equivalent relational counterparts, and the ability to readily query across multiple systems and abstraction layers representing complex relationships, leading to systems compatible with learning, reasoning, and inferencing. Following a model for human intelligence, information lives in different levels of the neocortex: from highly variable data inputs, to patterns, to patterns of patterns, to invariant concepts.4 Inspired by this model of intelligence, we extend the notion of a hypergraph to allow

J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646

Perspectives
links among edges to capture relationships that cross bounds of scale and dimension, and develop a novel generic framework for capturing information that can benefit systems biology and other areas. We desire a solution that is flexible to include various types of data from disparate sources, extensible to scale to massive stores of information, and accessible to permit the efficient extraction of patient-centric knowledge. Figure 1 outlines the architecture of our BioIntelligence Framework, the components of which are: 1. A public hypergraph-based network for representing knowledge, including a. A scalable hypergraph-like model for representing a knowledge network b. Processes to automate populating and updating the network with public domain knowledge from multiple sources c. An efficient database solution for storing the network platform 2. A Patient Data Locker application built on top of the knowledge network, including: d. An accessible web-based solution for storing patient-centric knowledge e. Processes for structuring and formatting patient genomic and health data, inducing patient-centric subgraphs on the public hypergraph, and stratifying patients based on information in their lockers 3. A process for structuring and formatting analyst interpretation to facilitate feedback and rapid automated learning in the system.

Public hypergraph
A graph is defined G(V,E) where V is a set of vertices (nodes) and E is a set of edges (links) between two vertices. A hypergraph is a generalization of a graph in which an edge can connect any number of vertices. Biological networks have traditionally been modeled as graphs/networks. These graph models capture bilateral relationships among node pairs. Using hypergraphs as a modeling paradigm supports the characterization of multilateral relationships and processes.3 For example, in a general graph,

Figure 1 A BioIntelligence Framework for creating a hypergraph-like store of public knowledge and using this, along with an individual's genomic and other patient information, to derive a personalized genome-based knowledge store for clinical translation and discovery research.
J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646 129

Perspectives
an edge might represent a relationship between a gene and disease state. That relationship may change in the context of a drug, and a hypergraph can represent this contextual knowledge with an edge containing all three elements. Hypergraphs are proving useful for capturing semantic and biomedical information in semantic web technologies for biological knowledge management and semantic knowledge networks.5 6 Approaches to extracting knowledge from biological research literature to store in a hypergraph have been proposed,7 8 with similar techniques used for population stratification.9 To support a data intelligent system, we wish for information to be stored not only explicitly in the data itself, but implicitly in how the data are linked, and capture this in our model. Abstraction is permitted by allowing hyperedges to contain other edges, forming a nested graph structure. A machine learning model called hierarchical temporal memory (HTM) mimics the human neocortex.4 10 Inspired by this design, we store knowledge at different levels of granularity, from single points of data, to collections of points, abstracting out to collections of collections. Thus, we perceive edges in lower levels of abstraction as nodes in higher levels, thereby permitting the network to be viewed and operated on within and across different scales of abstraction. Information is stored on the nodes and edges of the network and accessed via processes. Populating the network platform requires processes to pull different types of information from multiple sources, such as the Cancer Biomedical Informatics Grid (caBIG) and BioWarehouse,11 12 and other tools and techniques.13e15 The processes are represented as S0. Sm in figure 1, and take unstructured, public domain knowledge as input and structures it as input to the public network. The most common database management system (DBMS) is based on the relational data model, which is best suited to capturing data structured in a predefined schema, and presents limitations in handling complexity and scalability.16 Our solution handles unstructured and semi-structured data, and is in the scope of non-relational (NoSQL) databases, which do not require pre-defined schemas.16 17 Such databases include those based on graph models and triplestores (eg, RDF), and are best suited to capturing binary relationships between two elements (a triplestore is even more restrictive as it is essentially a `directed' binary graph). To effectively represent multilateral relationships and interactions present in biomedical data, hypergraph-based approaches have been suggested.3 18e21 The flexibility granted by allowing elements to contain elements allows processing knowledge at different levels of abstraction, in a less restrictive way than hierarchical models that restrict the network's topology. HyperGraphDB resembles our model, but uses a directed hypergraph and is built on a traditional database.22 Our model is more general, based on less restrictive undirected edges, and intended to be implemented natively, although a DBMS using our model may use other DB solutions as a persistent data store. In fact, other solutions can be built on our model as it is a generalization of other models. found in the online supplementary material, and demonstrates the added effort involved in capturing these complex relationships in a SQL database. The entities in figure 2 are represented as elements in our database. An attribute is a key/value pair, and a list of attributes (Attribute Set) is stored with each element. In this way, we can arbitrarily add any type of attribute to an element without changing the database structure. In the relational model, each entity type requires its own table, with pre-defined fields for attributes, and adding an attribute requires adding a field to the table, and migrating the database. A characteristic of the relational model is that all entities of the same type are stored in the same table. Our model is flexible in that it does not require prestructuring data, but we can certainly mimic this behavior if desired by enforcing a rule that requires all elements to have an attribute with key¼`type.' Using our model, this decision is left to the database designer, and not enforced by the model itself. An element in our model can contain an arbitrary number of other elements (allowing `has-a' and `has-many ' relationships). These elements are referenced in the `Internal Element Set' of the element. For example, in figure 2, element g is a gene and contains three gene variant elements (v1, v2, v3) in its internal element set. This is an example of a multi-lateral relationship as g can be viewed as a `hyperedge' in a hypergraph, connecting three `nodes.' While this behavior is easy to model in a relational database, using a one-to-many relation, it becomes more complex when an element contains an arbitrary number of arbitrary types of elements. For example, the element R2 represents a molecular state, which in this case is a protein p associated with a protein state ps1. In a relational database, we can capture these relationships in a `molecular state' table with foreign key fields pointing to a `protein' table and a `protein state' table. Now consider a molecular state that exists in the context of a modifier drug (ie, the state of the protein is perturbed by a modifier drug in a laboratory experiment). To capture this in our model, we can simply create a new element with three internal elements: p, ps1 and the element representing the modifier drug. To capture this in the relational database, we would need to either add a field to the `molecular state' table, which may be blank for many records, or create a new table with the three related fields. Both of the later options require a change to the underlying data structure, and migrating the database. The elements shown in figure 2C represent higher-level concepts and recursive nesting of elements at different levels, illustrating the ability to flexibly and efficiently capture multiscalar relationships among elements, the motivation behind our data model. For example, note that R1 captures the relationship that gene g codes for protein p. C1 captures the genetic event concept, where gene variant v2 changes the state of the protein (the molecular state represented by R3). C2 is a drug response concept representing the higher-level concept that C1, in the context of the pharmaceutical ph, leads to a changed molecular state, R2. These combined biological and pharmacologic effects lead to a change in disease state to ds2, captured by the clinical response concept C3. The internal (nested) element sets contain the topology of the network, that is, they define how entities are related, and capture meaning in those relationships. Further details to describe the nature of the relationships can always be stored in attributes of the elements. The `External Element Set' of an element is the inverse of internal element relations. For example, v1, v2, and v3 all have g in their external element set. While it is not necessary to store this information (the network of relationships among elements
J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646

Data model
An example use of our model is shown in figure 2. This example captures multilateral, multiscalar, and multidimensional relationships using a general model (A), and viewing the network at different levels of abstraction (B and C). The elements shown in this solution are described in the legend of the image, and based on a real-world problem we are exploring. A possible schema model for a relational database capturing these same data can be
130

Perspectives
Figure 2 An illustrative example of storing biomedical information in our proposed knowledge base: a component of the BioIntelligence Framework. A shows our data model, and describes its components. B and C show the elements described in the legend (at the bottom of the figure) at two different levels of abstraction.

can be constructed via the internal element sets alone), it does aid in querying the database. We have defined three new types of queries associated with our model: recover, context, and expand. The expand and context queries retrieve all internal and external elements of an element, respectively, optionally limited by modifiers presented with the query. Recover is a combination of these, and returns both internal and external elements. All three query actions have an optional level constraint, defining how deep to traverse the graph when retrieving related elements. For instance, expand n will retrieve all internal elements, and their internal elements, recursively up to n times. For instance, we can view C3 as an abstraction; a clinical effect that we can relate to patients and other concepts. Expanding C3 by one level shows us that it represents a disease state ds2, triggered by a pharmacologic effect C2. Expanding C3 by two levels shows us the
J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646

details of the pharmacologic effect, and so on. In this way, we can choose which level of abstraction we wish to view and compute over, and we can create new concepts that cross these layers of abstraction (multi-scalar relationships). By the definition of the internal and external element sets, it follows that our model naturally handles many-to-many relationships as well. In summary, we do not argue that a relational database is incapable of capturing the types of relationships we discuss here, rather that it requires more work, and added layers of complexity to the underlying structure of the database, which makes capturing and querying complex biomedical relationships more difficult. Our model is an abstraction of other models, including relational, graph, hierarchical, and object-oriented, and can therefore be used to model data represented using any and all of these models at once. The potential benefit of our model is
131

Perspectives
Figure 3 The network on the left is an example knowledge network platform. The darkened nodes represent gene variants present in an individual genome. The network on the top right is a genome-induced subgraph of the network. The network on the bottom right is a genome-induced subgraph, expanded out to include additional knowledge stored on edges in the connected-component each data element is contained in.

that it provides levels of scalability and flexibility that are difficult to achieve with existing models. We are currently developing a solution based on this model and will present additional details of the model and related query language in future publications.

Patient data locker
Given a patient's data (genomic, health, etc), we wish to recover related knowledge from our network using BioIntelligence Tools (BIT). The first step (BIT1 in figure 1) is a process to structure data as input to a process for inducing patient-relevant subgraphs of the knowledge network (BIT2 in figure 1). BIT1 integrates many types of data sets across multiple databases to support electronic medical and health records (EMR/EHRs), and is designed as a modular based system to provide metadata and indexing for queries. The next step (BIT2 in figure 1) is a process to extract relevant knowledge from the network based on individual patient information. An induced subgraph H(S,T) of network G(V,E) has the properties S3V, and for every vertex set Si of H, the set Si is an edge of H if and only if it is an edge in G. That is, H has the same edges that appear in G over the same set of nodes. We say that H is an induced subgraph of G, and H is induced by S. In our system, V is the set of nodes in the platform network G, and S is the set of nodes that map to an individual's genomic and health information. Thus, a subgraph is induced by an individual genome. The architecture in figure 1 shows an example public hypergraph, and private subgraphs stored in a data locker. These networks are detailed in figure 3, where the network on the left is a public knowledge network, and the darkened nodes are elements relevant to an individual's information (based on input patient data). The network on the top right is induced by this information, and contains all of the darkened nodes, and edges incident to them. Alternately, we can expand the information retrieved to include all connected components of the induced subgraph. An example of a connected-component induced subgraph is shown on the bottom right of figure 3. The most important characteristic of the data locker is that it contains all relevant knowledge to facilitate clinical translation. Induced subgraphs can be used to transform a large set of patient-relevant data to smaller, task-tailored formats void of extraneous detail. The patient data locker is linked to the public
132

knowledge store, and automatically updated to contain only a subset of information related to the patient. Thus, an expert need not develop their own intricate search queries and perform the tedious task of progressively reducing the amount of irrelevant data returned by the query. Any query that can be run on the entire knowledge network, can be run on the subgraph in a patient's locker, leading to a more precise subset of knowledge returned, and potentially faster querying speeds as the search space is reduced. The expert analyst is provided with knowledge tailored to a particular patient, partially automating the interpretation process, and a process (BIT3 in figure 1) allows the analyst to input new interpretation knowledge into the public network. We envision this type of feedback mechanism will support the inclusion of a learning model for our system, and allow the community to contribute to its growth. The system is diverse, providing framework and template libraries, allowing users to integrate their own tools for analysis, data collection, and beyond.

CONCLUSION
A deluge of biomedical data generated from next-generation sequencing (NGS) and clinical applications is overwhelming our ability to efficiently extract value from it. Existing bioinformatics tools were not developed to support clinical translation for an individual patient, causing an n¼1 translation bottleneck. A new architecture for managing biomedical data is desired, and we present the BioIntelligence Framework as a genomecompatible biomedical knowledge representation platform. Our future efforts to achieve the goals outlined in this paper include ensuring that we develop algorithms on this framework that minimally meet the performance expectations of existing solutions in practice.
Contributors All authors contributed to the ideas behind the framework. TF, CC, and PL contributed computer science expertise. JK, DVH, JMT, and SM contributed expertise in clinical genomics and translational research. Competing interests None. Provenance and peer review Commissioned; externally peer reviewed.

Open Access This is an Open Access article distributed in accordance with the Creative Commons Attribution Non Commercial (CC BY-NC 3.0) license, which permits others to distribute, remix, adapt, build upon this work non-commercially, and license their derivative works on different terms, provided the original work is properly cited and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/3.0/ J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646

Perspectives
REFERENCES
1. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. Agrawal R, Ailamaki A, Bernstein PA, et al. The Claremont report on database research. ACM SIGMOD Record 2008;37:9e19. Mousses S, Kiefer J, Von Hoff D, et al. Using biointelligence to search the cancer genome: an epistemological perspective on knowledge recovery strategies to enable precision medical genomics. Oncogene 2008;27:S58e66. Klamt S, Haus U, Theis F. Hypergraphs and cellular networks. PLoS Comput Biol 2009;5:e1000385. Hawkins J, Blakeslee S. On Intelligence. New York: Times Books, 2004. Antezana E, Kuiper M, Mironov V. Biological knowledge management: the emerging role of the semantic web technologies. Brief Bioinform 2009;10:392e407. Zhen L, Jiang Z. Hy-SN: hyper-graph based semantic network. Knowledge-Based Systems 2010;23:809e16. Vailaya A, Bluvas P, Kincaid R, et al. An architecture for biological information extraction and representation. Bioinformatics 2005;21:430e8. Mukhopadhyay S, Palakal M, Maddu K. Multi-way association extraction and visualization from biological text documents using hyper-graphs: applications to genetic association studies for diseases. Artif Intell Med 2010;49:145e54. Vazquez A. Population stratification using a statistical model on hypergraphs. Phys Rev E Stat Nonlin Soft Matter Phys 2008;77:1e7. George D. How the Brain Might Work: a Hierarchical and Temporal Model for Learning and Recognition [dissertation]. Palo Alto, California: Stanford University, 2008. NCI. Cancer Biomedical Informatics Grid (caBIG). https://cabig.nci.nih.gov/ (accessed 16 Sep 2011). 12. 13. 14. 15. 16. 17. 18. 19. 20. 21. 22. Karp P. Biowarehouse database integration for bioinformatics. http://biowarehouse ai.sri.com/ (accessed 16 Sept 2011). Chen H, Ding L, Wu Z, et al. Semantic web for integrated network analysis in biomedicine. Components 2009;10:177e92. Tudor CO, Schmidt CJ, Vijay-Shanker K. eGIFT: mining gene information from the literature. BMC Bioinformatics 2010;11:418. Valentin F, Squizzato S, Goujon M, et al. Fast and efficient searching of biological data resourceseusing EB-eye. Brief Bioinform 2010;11:375e84. Leavitt N. Will NoSQL databases live up to their promise? Computer 2010;43:12e14. Angles R, Gutierrez C. Survey of graph database models. ACM Computing Surveys 2008;40:1e39. Olken F. Graph data management for molecular biology. OMICS 2003;7:75e8. Hu Z, Mellor J, Wu J, et al. Towards zoomable multidimensional maps of the cell. Nat Biotechnol 2007;25:547e55. Spreckelsen C, Spitzer K. Formalising and acquiring model-based hypertext in medicine: an integrative approach. Methods Inform Med 1998;37:239e46. Wu G, Li J, Hu J, et al. System: a native RDF repository based on the hypergraph representation for RDF data model. J Comput Sci Technol 2009;24:652e64. Iordanov B. HyperGraphDB: a generalized graph database. Proceedings of the 2010 International Conference on Web-age Information Management 2010:25e36.

PAGE fraction trail=5.5

J Am Med Inform Assoc 2013;20:128­133. doi:10.1136/amiajnl-2011-000646

133

ATLAS: Adaptive Topology- and Load-Aware Scheduling
Jonathan Lutz, Charles J. Colbourn, and Violet R. Syrotiuk CIDSE, Arizona State University, Tempe, AZ 85287-8809 Email: {jlutz, colbourn, syrotiuk}@asu.edu

arXiv:1305.4897v2 [cs.NI] 4 Nov 2013

Abstract--The largest strength of contention-based MAC protocols is simultaneously the largest weakness of their scheduled counterparts: the ability to adapt to changes in network conditions. For scheduling to be competitive in mobile wireless networks, continuous adaptation must be addressed. We propose ATLAS, an Adaptive Topology- and Load-Aware Scheduling protocol to address this problem. In ATLAS, each node employs a random schedule achieving its persistence, the fraction of time a node is permitted to transmit, that is computed in a topology and load dependent manner. A distributed auction (REACT) piggybacks offers and claims onto existing network traffic to compute a lexicographic max-min channel allocation. A node's persistence p is related to its allocation. Its schedule achieving p is updated where and when needed, without waiting for a frame boundary. We study how ATLAS adapts to controlled changes in topology and load. Our results show that ATLAS adapts to most network changes in less than 0.1s, with about 20% relative error, scaling with network size. We further study ATLAS in more dynamic networks showing that it keeps up with changes in topology and load sufficient for TCP to sustain multi-hop flows, a struggle in IEEE 802.11 networks. The stable performance of ATLAS supports the design of higher-layer services that inform, and are informed by, the underlying communication network. Index Terms--Wireless networks, medium access control, adaptation.

I. I NTRODUCTION Despite the well known shortcomings of IEEE 802.11 and other contention-based MAC protocols for mobile wireless networks--such as probabilistic delay guarantees, severe short-term unfairness, and poor performance at high load-- they remain the access method of choice. The primary reason is their ease in adapting to changes in network conditions, specifically to changes in topology and in load. The lack of timely adaptation is the most serious limitation facing scheduled MAC protocols. For scheduling to be competitive, continuous adaptation is required. Topology-dependent approaches to adaptation in scheduling alternate a contention phase with a scheduled phase. In the contention phase, nodes exchange topology information used to compute a conflict-free schedule that is followed in the subsequent scheduled phase (see, as examples, [5], [30]). However, changes in topology and load do not always align with the phases of the algorithm resulting in a schedule that often lags behind the network state. In contrast, the idea behind topology-transparent scheduling is to design schedules independent of the detailed network topology [3], [15]. Specifically, the schedules do not depend on the identity of a node's neighbours, but rather on how many

of them are transmitting. Even if a node's neighbours change, its schedule does not; if the number of neighbours does not exceed the designed bound then the schedule guarantees success. Though such schedules are robust to network conditions that deviate from the design parameters [27], because the schedules do not adapt, the technique remains a theoretical curiosity. In contention-based schemes, such as IEEE 802.11, a node computes implicitly when to access the channel, basing its decisions on perceived channel contention. We instead compute a node's persistence--the fraction of time it is permitted to transmit--explicitly in a way that tracks the current topology and load. To achieve this, we propose ATLAS, an Adaptive Topology- and Load-Aware Scheduling protocol. Channel allocation is a resource allocation problem where the demands correspond to transmitters, and the resources to receivers. ATLAS implements the REsource AlloCaTion computed by REACT, a distributed auction that runs continuously. REACT piggybacks offers and claims onto existing network traffic to compute the lexicographic max-min allocation to transmitters which we call the TLA allocation, emphasizing that it is both topology- and load-aware. Each node's random schedule, achieving a persistence informed by its allocation, is updated whenever a change in topology or load results in a change in allocation. While the slots of the schedule are grouped into frames, this is done only to reduce the variance in delay [6]; there is no need to wait for a frame boundary to update the schedule. Even though the random schedules may not be conflict-free, ATLAS is not contention-based; it does not select persistences or make scheduling decisions based on perceived channel contention--its decisions are based solely on topology and load. We study how ATLAS adapts to controlled changes in topology and load, measuring convergence time, relative error, and scalability. We also assess the ability of ATLAS to adapt in more dynamic network conditions. To the best of our knowledge, ATLAS is the first scheduled MAC protocol able to adapt to changes in topology and load that is competitive with contention-based protocols in throughput and delay while realizing superior delay variance. It achieves this through the continuous computation of the TLA allocation, and updating the schedule on-the-fly. These updates occur only where and when needed. By not requiring phases of execution and by computing persistences rather than conflict-free schedules, ATLAS eliminates the complexity of, and lag inherent in, topology-dependent approaches. By not being dependent on the identity of neighbours, ATLAS shares the best of topology-transparent schemes (and also their

Submitted to IEEE Transactions on Mobile Computing ­ c 2013 IEEE

2

potential for collisions) yet overcomes its weakness by being adaptive. By not forcing updates to be frame synchronized, ATLAS shares the critical features of continuous adaptation with contention-based protocols. As a result, ATLAS achieves predictable throughput and delay characteristics. Such characteristics and information about localized capacity at the MAC layer may be used to inform higher layers, while end-toend characteristics at higher layers may be used to inform ATLAS. This may support the development of an agile, higher performing protocol stack. The primary contributions of this paper are twofold: (1) The REACT algorithm, an asynchronous, adaptive, and distributed auction that solves a general resource allocation problem to produce the TLA allocation. (2) ATLAS, a MAC protocol that uses REACT to solve the specific problem of channel allocation in a wireless network where each node produces a random schedule with the number of transmission slots determined by its allocation. The sequel is organized as follows: Section II defines a general resource allocation problem and presents the REACT algorithm, proving its correctness. Section III expresses channel allocation as a resource allocation problem and defines ATLAS. Related work is described in Section IV. After describing the simulation set-up in Section V, Section VI studies how ATLAS adapts to controlled changes in topology and load, and to dynamic network conditions. In Section VII, we discuss open issues and potential applications of REACT, including the design of higher-layer services that inform, and are informed by, the underlying communication channel. II. D ISTRIBUTED R ESOURCE A LLOCATION -- REACT We consider a general resource allocation problem. Let R be a set of N resources with capacity c = (c1 , . . . , cN ). Let D be a set of M demands with magnitudes w = (w1 , . . . , wM ). Resource j  R is required by demands Dj  D. Demand i  D consumes capacity at all resources in Ri  R simultaneously. The resource allocation s = (s1 , . . . , sM ), si  0 defines the capacity reserved for the demands. Resource allocation s is feasible if iDj si  cj for all j  R and si  wi for all i  D. Demand i is satisfied if si  wi . Resource j is saturated if iDj si  cj . Throughout, capacity refers to the magnitude of a resource. Definition 1: [22] A feasible allocation s is lexicographically max-min if, for every demand i  D, either i is satisfied, or there exists a saturated resource j with i  Dj where si = max(sk : k  Dj ). We now describe REACT, a distributed auction that computes the lexicographic max-min allocation. In it, resources are represented by auctioneers and demands by bidders. Each auctioneer maintains an offer--the maximum capacity consumed by any adjacent bidder--and each bidder maintains a claim--the capacity the bidder intends to consume at adjacent auctions. The final claim of bidder i defines allocation si . Auctioneer j satisfies Def. 1 locally by increasing its offer in an attempt to become saturated while maintaining a feasible allocation. Bidder i satisfies Def. 1 locally for demand i by increasing its claim until it is satisfied or has a maximal claim

Algorithm 1 REACT Bidder for Demand i.
1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: upon initialization Ri   wi  0 U PDATE C LAIM () end upon upon receiving a new demand magnitude wi U PDATE C LAIM () end upon upon receiving offer from auctioneer j offers[j ]  offer // Remember the offer of auctioneer j . U PDATE C LAIM () end upon upon bidder i joining auction j Ri  Ri  j // Resource j is now required by demand i. U PDATE C LAIM () end upon upon bidder i leaving auction j Ri  Ri \ j // Resource j is no longer required by demand i. U PDATE C LAIM () end upon procedure U PDATE C LAIM () // Select the claim to be no larger than the smallest offer or wi . claim  min ({offers[j ] : j  Ri }, wi ) send claim to all auctions in Ri end procedure

at an adjacent auction. Through continuous updates of offers and claims, the auctioneers and bidders eventually converge on the lexicographic max-min allocation. We give precise definitions of auction and bidder behaviour next. Bidder i knows wi and maintains set Ri . Offers are stored in offers[]; offers[j ] holds the offer last received from auctioneer j . Bidder i constrains its claim to be no larger than wi or the smallest offer from auctioneers in Ri , claim = min ({offers[j ] : j  Ri }, wi ) . (1)

Auctioneer j knows cj and maintains set Dj . Bidder claims are stored in claims[]; claims[i] holds the claim last received  from bidder i. Auctioneer j identifies set Dj  Dj containing bidders with claims strictly smaller than its offer,
 Dj = {b : b  Dj , claims[b] < offer}.

(2)

 Bidders in Dj are either satisfied or are constrained by another auction and cannot increase their claims in response to a larger  offer from auctioneer j . Bidders in Dj \ Dj are constrained by auction j . They may increase their claims in response to a  larger offer. Resources left unclaimed by bidders in Dj ,

Aj = cj -

 iDj

claims[i] ,

(3)

remain available to be offered in equal portions to bidders in  Dj \Dj . If claims of all bidders in Dj are smaller than the offer  (i.e., Dj = Dj ), there are no bidders to share the available resources in Aj . The auctioneer sets its offer to Aj plus the largest claim, ensuring that any bidder in Dj can increase its claim to consume resources in Aj : offer =
  Aj /|Dj \ Dj |, if Dj = Dj , (4) Aj + max (claims[i] : i  Dj ) , otherwise.

Alg. 1 and Alg. 2 describe actions taken by the bidders and auctioneers of REACT in response to externally triggered

3

Algorithm 2 REACT Auctioneer for Resource j .
1: 2: 3: 4: 5: 6: 7: 8: 9: 10: 11: 12: 13: 14: 15: 16: 17: 18: 19: 20: 21: 22: 23: 24: 25: 26: 27: 28: 29: 30: 31: 32: 33: 34: 35: 36: 37: 38: 39: 40: 41: 42: 43: 44: upon initialization Dj   cj  0 U PDATE O FFER () end upon upon receiving a new capacity of cj U PDATE O FFER () end upon upon receiving claim from bidder i claims[i]  claim // Remember the claim of bidder i. U PDATE O FFER () end upon upon bidder i joining auction j Dj  Dj  i // Demand i now requires resource j . U PDATE O FFER () end upon upon bidder i leaving auction j Dj  Dj \ i // Demand i no longer requires resource j . U PDATE O FFER () end upon procedure U PDATE O FFER ()   Dj Aj  cj done  False while ( done = False ) do  contains all bidders in D , then auction j does not // If Dj j // constrain any of the bidders in Dj .  = D ) then if ( Dj j done  True offer  Aj + max ({claims[i] : i  Dj }) // Otherwise, auction j constrains at least one bidder in Dj . else done  True // What remains available is offered in equal portions to the // bidders constrained by auction j . | offer  Aj /|Dj \ Dj  and compute A for the new offer. // Construct Dj j  } do for all b  {Dj \ Dj if ( claims[b] < offer ) then   D  b Dj j Aj  Aj - claims[b] done  False send offer to all bidders in Dj end procedure

events. Collectively, auctioneers and bidders know the inputs to the allocation problem and bidder claims converge on the lexicographic max-min allocation; the claim of bidder i converges on si . The correctness of Alg. 1 and Alg. 2 is established in two steps: Lemma 1 establishes forward progress on the number of auctioneers to have converged on their final offer. Theorem 1 employs Lemma 1 to show eventual convergence to the lexicographic max-min allocation. Let claimi denote the claim of bidder i and offerj the offer of auctioneer j . Assume that the resource allocation remains constant for the period of analysis, that bidder i knows Ri and wi , and that auctioneer j knows Dj and cj . Further assume communication between adjacent auctioneers and bidders is not delayed indefinitely. A claim or offer is stable if it has converged on its final value. Denote by Astable the set of auctioneers whose offers are stable and remain the smallest among all offers. Lemma 1: Suppose Astable contains k auctioneers, 0  k < N . Then, within finite time, at least one auctioneer converges

on the next smallest offer omin . Offers equal to omin are stable and remain smaller than all other offers not in Astable . Proof: Wait sufficient time for every bidder i to send a new claim to auctioneers in Ri and for every auctioneer j to send a new offer to bidders in Dj . Let omin be the smallest offer of an auctioneer not in Astable . Assume to the contrary that offerx for some x  / Astable is the first to become smaller than omin . By Eq. 2 and 4, a decrease to offerx can only occur after a bidder y at auction x with claimy < offerx increases its claim. By Eq. 1, claimy can increase only after its limiting constraint starts out smaller than offerx and increases. Constraints in the system smaller than offerx are maximum claims, offers from Astable , and offers equal to omin . Maximum claims and offers from Astable do not change, leaving some x with offerx = omin as the only potential limiting constraint for claimy . By Eq. 2 and 4, offerx can increase only after one of its bidders y reduces its claim to be smaller than offerx . By Eq. 1, claimy can get smaller only after one of its auctioneers, say x , reduces its offer to be offerx < omin = offerx contradicting the assumption that offerx is the first to become smaller than omin . Therefore, offers equal to omin remain smaller than offers not from Astable . By Eq. 2 and 4, any j offering omin can change only after a bidder i at auction j with claimi  omin changes. By Eq. 1, claimi only changes if its limiting constraint changes. Potential limiting constraints include wi , offers from Astable , and offers equal to omin . These constraints are stable; therefore, offers equal to omin are stable. Theorem 1: Bidders and auctioneers of Alg. 1 and Alg. 2 compute the lexicographic max-min allocation. Proof: We apply Lemma 1 to show by induction that every auctioneer eventually computes a stable offer. Base Case: Consider an allocation problem with arbitrary wi , cj , Ri , and Dj for 1  i  M , 1  j  N . Let |Astable | = 0. By Lemma 1, at least one auctioneer eventually converges on a smallest offer omin . Offers equal to omin are stable and remain smallest among all offers. Add auctioneers offering omin to Astable ; |Astable |  1. Inductive Step: Let |Astable | = k , 1  k < N . Then, by Lemma 1 a non-empty set of auctioneers A+ with A+  Astable =  eventually converge on the next smallest offer. Offers from A+ remain smaller than offers not from A+ or Astable and are stable. Add A+ to Astable ; |Astable |  k + 1. By induction, all auctioneers are eventually added to Astable . Wait for auctioneers to send their offers to adjacent bidders. Bidder claims are now stable. By Eq. 1, bidder i is either satisfied with its claim (claimi = wi ) or its claim is maximal at an auction in Ri . By Definition 1, the claims are lexicographic max-min. III. T HE ATLAS MAC P ROTOCOL Channel allocation in wireless networks can be expressed as a resource allocation problem. In this context, transmitters correspond to the demands in D and receivers to the resources in R. Label transmitters {1, . . . , M } and receivers {1, . . . , N }. A transmitter with a non-zero demand magnitude is active. Receiver j is in Ri if it is within transmission range of

4

Slot x Node A Node B Node C
Header Fields pkt #1 collision pkt #2 claim offer

Slot x + 1

Slot x + 2
pkt #1 pkt #1 pkt #1 ack #1 ack #1 ack #1

Slot x + 3
pkt #2 pkt #2 pkt #2 ack #2 ack #2 ack #2
Node # 1 w1 = 0.45 s1 = 0.25 s 1 = 0.20

Node # 7 w7 = 0.30 s7 = 0.30 s 7 = 0.20

MAC Payload

Header Fields

ACK Fields

MAC Header

MAC Header

Node # 3 w3 = 0.50 s3 = 0.25 s 3 = 0.20 Node # 4 w4 = 0.40 s4 = 0.25 s 4 = 0.20 Bidirectional Link New Bidirectional Link

claim offer

Node # 5 w5 = 0.75 s5 = 0.45 s 5 = 0.55

Fig. 1. Example transmissions in ATLAS of two packets in a network of three fully connected nodes. The first packet is sent from node A to node C . The second packet is sent from node C to node B . Transmissions are coloured white and receptions are shaded grey. The frame structure is shown for a data packet and an acknowledgement.

Node # 2 w2 = 0.55 s2 = 0.25 s 2 = 0.20

Node # 6 w6 = 0.05 s6 = 0.05 s 6 = 0.05

transmitter i and transmitter i is active. Dj contains the active transmitters for which receiver j is within transmission range. Receiver j is adjacent to transmitter i if j  Ri and i  Dj . The sets Dj and Ri capture the network topology for active transmitters. For load, wi is set to the percentage of slots required to support the demand at transmitter i. Transmitters with no demand (i.e., wi = 0) receive an allocation of zero slots: they are not active. Receiver capacities are set to one, targeting 100% channel allocation. The lexicographic max-min solution s = (s1 , . . . , sM ) for a given topology and traffic load is the TLA allocation. To apply REACT to channel allocation, we integrate it into ATLAS, a simple random scheduled MAC protocol. Although REACT could instead augment contention-based schemes, we choose to work within a scheduled environment, a traditionally difficult setting for adaptation. In ATLAS, each node runs a REACT bidder (Alg. 1) and a REACT auctioneer (Alg. 2) continuously. Auctioneers and bidders discover each other as they hear from one another and rely on the host node to detect lost adjacencies. The network topology is implicit in the sets Ri and Dj . Each node updates its bidder's demand magnitude to accurately reflect its traffic load. Offers and claims are encoded using eight bits each and are embedded within the MAC header of all transmissions to be piggybacked on existing network traffic. The encoding supports a total of 256 values for offers, claims, and persistences uniformly distributed between 0 and 1; the error in the representation does not exceed 0.004. Adding fields for an offer and claim to data packets and acknowledgements results in a communication overhead of four bytes per packet. For the slot size and data rate simulated in Section VI, the overhead is 0.36%. A node's offer and claim are eventually received by all single-hop neighbours reaching the bidders and auctioneers that need to know the offer and claim. In time, the bidder claims in REACT converge on the TLA allocation s. Packets are acknowledged within the slot they are transmitted and slots are sized accordingly. Unacknowledged MAC packets are retransmitted up to ten times before they are dropped by the sender. Fig. 1 shows that collisions are possible in ATLAS, and that successful transmissions are acknowledged in the same slot. The transmissions collide in slot x; they are repeated (successfully) in slots x + 2 and x + 3. Fig. 1 also shows the frame structure. The TLA allocation can be interpreted directly as a set of

Fig. 2. Example network showing the TLA allocation computed by REACT before and after an added link in the topology. wi identifies a node's demand, si its initial TLA allocation, and s i its TLA allocation after the added link. Resource capacities are set to one. Double-lined circles identify nodes with saturated resources.

persistences in a p-persistent MAC [28]. However, we achieve lower variation in delay by introducing the notion of a frame [6]. Specifically, ATLAS divides time into slots which are organized into frames of v slots. Node i operates at persistence pi = si . At the start of every frame and upon any change to pi , node i computes ki = pi v + 1 with probability i and ki = pi v with probability 1 - i where i = pi v - pi v . Node i constructs a transmission schedule of ki slots selected uniformly at random. Over many frames, E [ki ]/v equals pi where E [ki ] is the expectation for ki . Fig. 2 shows the TLA allocation in a small example network before and after a change in topology. Node 7 starts out disconnected from the other nodes and moves within range of node 3. In REACT, node 3 starts out offering 0.25 which is claimed by the bidders of nodes 1, 2, 3, and 4. With the claims of node 3 and 4 limited by the offer of node 3 and the claim of node 6 limited by its demand, the auctioneer at node 4 is free to offer 0.45, which is claimed by node 5. Upon detecting node 7 as a neighbour, the auctioneer at node 3 decreases its offer to 0.20. The bidders at nodes 1, 2, 3, 4, and 7 respond by reducing their claims accordingly. The smaller claims of the bidders at nodes 3 and 4 allow the auctioneer at node 4 to increase its offer to 0.55. The bidder at node 5 responds by increasing its claim to 0.55. It can be verified that, before and after the topology change, the claims of the bidders (i.e., the values of si and s i ) are lexicographically max-min; that is, every claim is satisfied or is maximal at an adjacent auction. Consider the topology with node 3 and node 7 connected. The bidder at node 6 is satisfied. The bidders at nodes 1, 2, 3, 4, and 7 are maximal at the auction of node 3. The bidder at node 5 is maximal at the auction of node 4. There are many implementation choices to be made in applying REACT to channel allocation. We identify three binary choices--lazy or eager persistences, physical layer or MAC layer receivers, and weighted or non-weighted bidders--and three configurable parameters--pmin , pdefault , and tlostNbr . The choices are described here; they are evaluated in Section VI.

5

A. Lazy or Eager Persistences A lazy approach sets persistence pi equal to the claim of bidder i. Once converged, pi matches the TLA allocation interpreted as a persistence. There is a potential disadvantage with being lazy. For many applications, nodes cannot predict future demand for the channel; they can only estimate demand based on past events, i.e., packet arrival rate or queue depth. As a consequence, wi lags the true magnitude of the demand at node i. If wi is the limiting constraint for the claim of bidder i, pi can be sluggish in response to increases in demand. Alternatively, an eager approach sets persistence pi = min (offers[j ] : j  Ri ), breaking the direct dependence on wi . Under stable conditions, a node's channel occupancy, the fraction of time it spends transmitting, matches its TLA allocation; its occupancy is limited by the availability of packets to transmit which is no larger than wi , even when pi > wi . By allowing pi > wi , the persistence is made more responsive to sudden increases in demand. B. Physical Layer or MAC Layer Receivers A central objective of the TLA allocation is to ensure that no receiver is overrun. In a wireless network, receivers can be defined in terms of physical layer or MAC layer communication. At the physical layer, every node is a receiver. At the MAC layer, packets are filtered by destination address; a node is only a receiver if one of its neighbours has MAC packets destined to it. MAC layer receivers can increase channel allocation by over-allocating at non-receiving nodes. However, the overallocation can slow detection of new receivers. Physical receivers prevent overallocation at any receiver, making the allocation more responsive to changes in traffic where nodes become receivers. C. Weighted or Non-Weighted Bidders We have described a MAC protocol where transmitters are represented by equally weighted bidders. For applications requiring multiple demands per transmitter, i.e., nodes servicing more than one traffic flow, we propose the weighted TLA allocation. The demands of weighted bidders are comprised of one or more demand fragments; the number of fragments accumulated into a demand is the demand's weight. Let i be the weight for demand i. Demand fragments in demand i have magnitude wi /i . The weighted TLA allocation defines the lexicographically max-min vector u = (u1 , . . . , uN ) where ui is the allocation to each demand fragment in demand i for a total allocation of ui i to demand i. REACT can be extended to compute the weighted TLA allocation. To do this, each bidder must inform adjacent auctions of its weight. Sixteen unique weights (with a four-bit representation) may be sufficient for many applications. D. Minimum Persistence pmin A node can maintain a persistence of zero without impacting the communication requirements of its bidder. For auctioneers, a persistence of zero is problematic. If a receiver becomes

overwhelmed by neighbouring transmitters, a non-zero persistence is needed to quiet the neighbours. To accomplish this, the node enforces a minimum persistence pmin , creating dummy packets if necessary, whenever the sum of claims from adjacent bidders exceeds the auction capacity. E. Overriding the TLA Allocation with pdefault There are two conditions where a node constrains its persistence to be no larger than pdefault . The first is when it has no neighbours. While the TLA allocation permits an isolated node to consume 100% of the channel, it cannot discover new neighbours if it does so. The second time a node employs pdefault is for a short period after the discovery of a new neighbour. It is possible for several nodes operating with large persistences to join a neighbourhood at about the same time. If the persistences are large enough, neighbour discovery can be hindered. For both scenarios, limiting the persistence to pdefault facilitates efficient neighbour discovery. F. Adaptation to Topology Changes and tlostNbr Changes in network topology are detected externally to REACT. In ATLAS, neighbour discovery is performed independently by each node. If a node hears from a new neighbour, then the node notifies its bidder of the new auction and its auctioneer of the new bidder. Conversely, if a node has not heard from a neighbour in more than tlostNbr seconds, it presumes the node is no longer a neighbour and informs its auctioneer and bidder accordingly. IV. R ELATED W ORK This paper focuses on the TLA allocation, its continuous distributed computation, and its application to setting transmitter persistences. In this section, we review a representative set of scheduled MAC protocols, observing how each selects a node's persistence and adapts to topology and load. Any finite schedule used in a cyclically repeated way can be generalized as a (k, v )-schedule with k transmission slots per frame of v slots, producing an effective persistence of p = k/v . Examples include the random schedules of [6], [18] where each node selects its k transmission slots randomly from the set of v slots in the frame. Topology transparent schemes [3], [15], [27] also implement (k, v )-schedules. These schedules rely on only two design parameters: N , the number of nodes in the network, and Dmax , the maximum supported neighbourhood size. These schedules guarantee each node a collision-free transmission opportunity from each of its neighbours at least once per frame, provided the node's neighbourhood size does not exceed Dmax . (k, v )-schedules do not adapt to variations in neighbourhood size or traffic load. The combinatorial requirements for variable-weight topology transparent schedules (variable k ) are explored in [19], but no construction nor protocol using them is given. A class of topology-dependent scheduled protocols compute distance-2 vertex colourings of the network graph to achieve TDMA schedules with spatial reuse. The colourings assign one transmission slot to each node and do not adapt to

6

TABLE I ATLAS CONFIGURATIONS SELECTED FOR SIMULATION . Configuration Name Nominal Lazy Persistences Physical Receivers Weighted Bidders Eager (0) or Lazy (1) 0 1 0 0 MAC (0) or Physical (1) 0 0 1 0 Unweighted (0) or Weighted (1) 0 0 0 1

A. Scenario Details Unless otherwise noted, all four configurations run with pdefault = 0.05, tlostNbr = 0.5s, and pmin = 0.01. The selection of pdefault and tlostNbr are justified by results in Figs. 5, 11a, and 11b. The selection of pmin is based on [20]. Frames contain v = 100 slots of length 800µs (1100 bytes per slot). Simulations are run using the ns-2 simulator [21]. Each wireless node is equipped with a single half-duplex transceiver and omni-directional antenna whose physical properties match those of the 914 MHz Lucent WaveLAN DSS radio. The data rate for all simulations is 11 Mbps. The transmission and carrier sense ranges are 250m. Each simulation runs a network scenario composed of a randomly generated topology and a randomly generated traffic load. Unless specified otherwise, topologies contain 50 randomly placed nodes constrained to a 300 × 1500m2 area. With the exception of the multi-hop TCP flows in Section VI-F, each traffic load consists of single-hop constant rate traffic. Four traffic loads are simulated: 20% and 80% of nodes loaded with small demands (75 ± 50 pkts/s), 20% and 80% of nodes loaded with large demands (500 ± 50 pkts/s). Nodes loaded with traffic are selected at random and the demand magnitudes are selected uniformly at random from the specified range. The packet destination is selected dynamically from the set of neighbouring nodes as the packet is passed down to the MAC layer. For the Weighted Bidders configuration, each demand is assigned a random integer weight between one and five. Traffic is generated by constant bit rate generators and transported over UDP; packets are 900 bytes in length, leaving room in each slot for header bytes and a MAC layer acknowledgement. Combined with the random placement of nodes and the addition of mobility, these four traffic loads enable simulation of a wide variety of network conditions. B. Relative Error A metric of interest is the average relative error for a node's persistence with respect to the TLA allocation. Error is reported in two parts: relative excess and deficit persistence error. Errors are measured per node over 80ms consecutive intervals in time (equal to the length of one MAC frame). We compute the average relative excess error and average relative deficit error for a given sample set of persistence measurements. The relative errors are ratios, requiring use of the geometric rather than arithmetic mean. But, the errors are often zero, preventing direct use of their mean. Instead, we convert errors into accuracies eliminating zeros from the data set for a more meaningful geometric average. The average relative accuracies are converted back to relative errors. VI. E VALUATION OF ATLAS Results from [20] show the TLA allocation applied in a static network to maintain expected delay and throughput compared to IEEE 802.11, while reducing the variance for both metrics. The TLA allocation nearly eliminates packets dropped by the MAC layer. In this section, we build on these results, focusing on the efficient distributed computation of the

traffic load. One of the first distributed protocols to bound the number of colours is proposed in [5]. Distributed-RAND (DRAND) [25] is a distributed implementation of RAND (a centralized algorithm for distance-2 colouring [23]). DRAND runs a series of loosely synchronized rounds. A colour is assigned in each round to one or more nodes in different two-hop neighbourhoods. DRAND is employed by ZebraMAC (Z-MAC) [24] to compute schedules over which to run CSMA/CA. Nodes are given priority access to their own slot, but also allowed to contend for access in other unused slots, as is done in [4]. Due to the complexity of DRAND, schedules are only computed once during network initialization. Other topology-dependent schemes support variable persistences. The periodic slot chains proposed in [14] are not limited to the structure of a fixed length frame and can support variable and arbitrarily precise persistences. A slot chain is defined by its starting transmission slot and period between its consecutive transmission slots. By combining multiple slot chains with different periods, schedules are constructed targeting any rational persistence in the range [0, 1]. The computation of slot chains provided in [14] is centralized; a distributed mechanism to adaptively compute the slot chains remains an open problem. In [30], a five phase reservation protocol (FPRP) computes conflict-free schedules where a node can reserve one or more transmission slots in the frame to achieve variable persistences. Reservation frames are run periodically rather than on a demand basis and, therefore, may not accommodate the current topology and traffic load. In SEEDEX [26], nodes do not attempt to derive conflictfree schedules. They learn the identities of their two-hop neighbours and adjust transmission probabilities (i.e., persistences) to improve the likelihood of collision-free transmissions. The transmission probabilities accommodate the number and identity of neighbours, but not traffic load. In our earlier work [20], a distributed algorithm for computing the TLA allocation is provided; however, the algorithm assumes a fixed topology and does not adapt to changes in the network. REACT solves these limitations by asynchronously adapting to changes in both topology and traffic demand. V. S IMULATION S ET- UP We now describe the simulations used to produce the experimental results presented in Section VI. Table I lists the four ATLAS configurations simulated. The Nominal configuration employs eager persistences, defines receivers in terms of MAC layer communication, and operates with unweighted bidders. The other three configurations differ from the Nominal case by a single choice and are named accordingly.

7

Fig. 3.

Convergence time following network initialization.

simulations of 1000 network scenarios, 250 of each traffic load. The scenarios are simulated eight times each, once per default persistence: 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, and 0.4. Small default persistences (pdefault  0.01) limit a node's ability to communicate during neighbour discovery, slowing convergence. Large default persistences (pdefault  0.3) permit nodes to transmit with large persistences before they discover their neighbours. In networks with 40 large demands, the large persistences can overwhelm the channel preventing neighbour discovery and delaying convergence. ATLAS is robust to the selection of pdefault with a suitable range of [0.05­0.2]. For the remaining simulations, pdefault is 0.05. B. Convergence after a Change in Demand

TLA allocation in the face of changes in topology and load. The results presented here work to answer four questions: 1) Can ATLAS converge quickly on the TLA allocation? 2) Can ATLAS scale to larger networks? 3) Can ATLAS keep up with changes in a mobile network? 4) Can ATLAS adapt to multi-hop traffic flows? The first question is addressed Sections VI-A, VI-B, and VI-C. The second is addressed in Section VI-D. The third and fourth questions are addressed in Sections VI-E and VI-F, respectively. Continuing the focus on adaptation, Section VI-G provides comparisons with several scheduled protocols. A. Convergence after Network Initialization Fig. 3 reports average convergence times for all four ATLAS configurations. Error bars denote the arithmetic standard deviation from the mean for each sample set. Convergence is measured from network initialization (time = 0) to the time ATLAS converges on the TLA allocation. Times are collected from simulations of 1000 network scenarios simulated four times each, once per configuration. There are 250 scenarios for each traffic load. The Physical Receivers configuration converges fastest in less than 0.4s on average for networks with 40 large demands and faster for other traffic loads. The extra step of detecting MAC receivers slows convergence. The Lazy Persistences configuration is the slowest with an average convergence time of 0.67s for networks with 40 large demands. The strict limit on persistences enforced by this configuration slows convergence compared to the others. Fig. 4a shows average excess and deficit relative persistence errors for all four configurations. The averages are computed for nodes with a non-zero TLA allocation and only during convergence. Nodes are observed to operate within approximately 20% of their TLA allocation regardless of configuration. Deficit errors are larger than excess errors reflecting a tendency to converge from below, rather than above, the TLA allocation. In Fig. 4b, each data point reflects the convergence time (xcoordinate) and total relative persistence error (y -coordinate) for one simulation of the nominal configuration. The data shows relative persistence error to be fairly consistent from network to network with a maximum observed error of 27%. Fig. 5 reports convergence time for the Nominal configuration while varying pdefault . Convergence is measured for

Fig. 6 reports convergence times and relative persistence errors for the Nominal configuration following a change to a single demand magnitude. Four types of demand are simulated: a new small demand, a new large demand, a removed small demand, and a removed large demand. New demands start with magnitude zero and change to 75 ± 50 pkts/s for small demands and to 500 ± 50 pkts/s for large demands. Removed small demands and removed large demands start at 75 ± 50 pkts/s and at 500 ± 50 pkts/s, respectively; both change to zero. The four demand change types are simulated under the four traffic loads. REACT is allowed to converge on the initial TLA allocation prior to the demand change. Convergence times and error measurements are taken from simulations of 4000 network scenarios, 250 for each of the 16 demand change and traffic load combinations. Fig. 6a reports convergence times measured from the time of the change to the time of convergence on the new TLA allocation. The largest convergence times of approximately 0.175s are found in networks loaded with 40 demands. The average convergence time for the other scenarios is 0.125s or smaller. Fig. 6b shows relative persistence errors measured during convergence at nodes whose TLA allocation are affected by the demand change. Persistences are observed to be within 10% of the TLA allocation. C. Convergence after a Change in Topology Fig. 7 reports convergence time and relative persistence error following two types of topology change: the creation of a link and the removal of a link between a pair of nodes. Simulations are run on 2000 network scenarios, 250 for each topology change type and traffic load combination. Networks that lose a link are simulated once per neighbour timeout tlostNbr of 0.5s, 2.0s and 5.0s. Network topologies are generated as follows. A first node is placed at a random location in the simulation area. For topologies gaining a link, a second node is placed just outside the transmission range of the first node with a trajectory toward the first node. For topologies losing a link, the second node is placed just inside the transmission range of the first node with a trajectory away from the first node. The remaining 48 nodes are placed at random locations in the simulation area. The distance travelled by the second node is constrained to avoid unintentional topology changes.

8

(a) Relative excess and deficit persistence errors. Fig. 4. Relative persistence error for ATLAS.

(b) Convergence time vs. error for the Nominal configuration.

Fig. 5.

Convergence times when run with varying default persistences.

convergence is reached in an average of 0.89s, a mere 40% increase compared to networks spanning 4.8 hops. The impressive convergence times, particularly those of networks spanning 12 or more hops, suggest that convergence happens locally, allowing distant neighbourhoods to converge in parallel. This local behaviour is captured in Fig. 9 which reports the average distance between a network change and a node whose bidder changes its claim in response. Distances are reported in hops. A node that changes its demand or gains/loses a neighbour has distance zero. Neighbours of this node have distance one, and so on. Range of impact is reported for the six types of change evaluated in Sections VI-B and VI-C. Each type of change is simulated in 1000 network scenarios, 250 of each traffic load. The range of impact is less than 1.75 hops on average. E. Performance with Node Mobility Section VI-C addresses the robustness of ATLAS to single topology changes. We now evaluate its performance in networks with continuous mobility which may not have the opportunity to converge on the TLA allocation. Fig. 10a reports persistence error for node speeds ranging from 0 m/s to 120 m/s with 200 scenarios simulated for each node speed, 50 of each traffic load. Node movements are generated using the steady-state mobility model generator of [13] with a pause time of zero. Simulations are run for 20s. As node speeds increase, so do deficit persistence errors. The larger deficit errors are an artifact of lost neighbour detection which is delayed by tlostNbr = 0.5s. As a result, nodes tend to think their neighbourhoods are more crowded than they are, a tendency that gets worse as node speeds increase. In terms of REACT, auctioneers and bidders unnecessarily constrain their offers and claims to accommodate lost neighbours. The deficit persistences translate to degraded throughput. Fig. 10b reports MAC throughput for the simulations of Fig. 10a. Even with node speeds of 120 m/s where a node travels its transmission range in 2.1s, throughput degrades modestly, decreasing by less than 20% compared to static networks. Fig. 11 shows that a large tlostNbr exacerbates deficit persistence error and further degrades throughput. Data is collected from 200 scenarios, 50 of each traffic load. Each scenario is simulated five times with neighbour timeouts ranging from

The expected convergence time following the addition of a new link is 0.025s. For tlostNbr =0.5s, convergence is reached in less than 0.13s on average. For tlostNbr =2.0s and tlostNbr =5.0s, the large convergence times are dominated by tlostNbr . Except for the simulations of Fig. 11, all others configure ATLAS with tlostNbr =0.5s. During convergence, nodes affected by the topology change are observed to operate within 4% of their TLA allocation on average. These numbers are striking. The small convergence times stem from a counterintuitive feature of the TLA allocation: the majority of topology changes do not affect the TLA allocation. A new link only has an effect if the link connects a bidder with an auction that lacks the capacity to support the bidder's claim. Even in heavily loaded networks, many auctions have spare capacity to support a new bidder. For these scenarios, convergence is instantaneous. D. Scalability to Large Networks We now turn to results demonstrating ATLAS's scalability. We simulate 10 network sizes with the x-dimension ranging from 600m (2.4 hops) to 6000m (24 hops) in 600m increments; the y -dimension is held constant at 300m. The number of nodes is selected to keep the average neighbourhood density constant across all network sizes. Fig. 8 reports convergence times for 4000 network scenarios, 100 of each traffic load and network size combination. The convergence of ATLAS in large networks is striking. In networks spanning 24 hops,

9

(a) Convergence time after a demand change. Fig. 6.

(b) Relative persistence error after a demand change.

Convergence time and relative persistence error during convergence following a single demand change.

(a) Convergence time after a topology change. Fig. 7.

(b) Relative persistence error after a topology change.

Convergence time and relative persistence error following a single topology change.

Fig. 8.

Convergence times as the width of the network grows.

0.1s to 15.0s. Node speeds are fixed at 30 m/s. Degraded performance is observed for large timeouts, tlostNbr  0.5s, but also for small timeouts, tlostNbr = 0.1s. In networks loaded with 10 large demands, tlostNbr = 0.1s causes nodes to falsely identify lost neighbours that must be rediscovered. The remaining simulations are run with tlostNbr = 0.5s. Fig. 12 reports packet delay for ATLAS and IEEE 802.11 for the 200 network scenarios of Fig. 10 with node speeds equal to 30 m/s. IEEE 802.11 is configured with a maximum packet retry count of seven for RTS, CTS, and ACKs and four for data packets [11], a mini-slot length of 20µs, and minimum and maximum contention window sizes of 32 and 1024 slots, respectively. Each point in the scatter plot reports the average packet delay (x-coordinate) and variation in packet delay (y coordinate) for a single node. The largest reported average delay is 0.047s for ATLAS and 0.058s for IEEE 802.11. The largest reported variation in delay for ATLAS is 0.0016s2 , just 3.6% of the 0.0444s2 reported for IEEE 802.11. This impressive reduction in delay variance is crucial to the support of TCP, which we evaluate next. F. Multi-hop TCP Flows To this point, we have used MAC layer traffic to simulate a diverse set of network scenarios. We now evaluate the performance of ATLAS using multi-hop TCP flows. To accommodate the dynamic nature of these flows, each node estimates its own demand by monitoring queue behaviour. Demand is estimated as the sum of two parts: wenqueue and wlevel . wenqueue

Fig. 9.

Average range of impact (in hops) for a demand or topology change.

10

(a) Relative persistence error. Fig. 10. Relative persistence error and total MAC throughput for varying levels of node mobility.

(b) Total MAC throughput.

(a) Relative persistence error. Fig. 11. Relative persistence error and total MAC throughput for varying neighbour timeouts.

(b) Total MAC throughput.

Fig. 12.

Delays for ATLAS and IEEE 802.11 with node speeds of 30 m/s.

is the percentage of channel required to keep up with the current enqueue packet rate, wenqueue = (packet enqueue rate) × (slot length). wlevel is the percentage of channel required to transmit all packets in the queue within 0.2s (i.e., 25 slots), wlevel = [(# packets in queue)/0.02s] × (slot length). To avoid cross-layer interactions between the MAC and routing protocols, Dijkstra's shortest path algorithm [28] using accurate knowledge of the global topology computes the next hop address for all packet transmissions. FTP agents emulate transfer of infinite size files to create flows with throughput limited only by the performance of the network. Transfers start at time zero and run for 20s. Nodes are statically placed at random locations in a 300 × 1500m2 simulation area. The source and destination nodes for each file transfer are selected

at random. Each FTP transfer is transported over TCP Reno configured for selective acknowledgements, the extensions of RFC 1323 [1], and 900 byte TCP segments. The return ACKs are not combined with each other or with other data packets. Consequently, the transmission of a single 40-byte TCP ACK consumes an entire transmission slot in ATLAS. The maximum congestion window size is 32 packets. Network scenarios are simulated for three traffic loads: networks with 2, 8, and 25 TCP flows. The number of replicates per traffic load are chosen so that 3000 TCP flows are simulated for each. Fifteen hundred scenarios are simulated with two TCP flows, 375 with eight TCP flows, and 120 with 25 TCP flows. We simulate TCP traffic on five MAC protocols: the four configurations of ATLAS and IEEE 802.11. The configurations of ATLAS use pdefault = 0.05, tlostNbr = 0.5s, and pmin = 0.01. IEEE 802.11 parameters match those described in Section VI-E. Each node dynamically sets its bidder weight to one or the number of outgoing TCP flows it services, whichever is larger. The 15 sub-plots in Fig. 13 show the percentage of flows (y -axis) achieving a minimum throughput (x-axis). The distinguishing characteristics of the three unweighted ATLAS configurations are seen in the throughput curves for networks with two flows. These networks are loaded lightly enough for the auctions at non-receiver nodes to make a difference in the allocation, improving throughput for 2- and 4-hop flows. These networks also demonstrate how the longer initial packet delays of the Lazy Persistences configuration increase round trip time

11

Networks with 2 Flows

Networks with 8 Flows

Networks with 25 Flows

All flows

y -axis shows % of TCP flows achieving minimum required throughput.

1-hop flows 2-hop flows 3-hop flows 4- and 5-hop flows

x-axis shows minimum required throughput for TCP flows in packets/second.
Fig. 13. Percent of TCP flows (y -axis) achieving a minimum throughput (x-axis). Plots in the left, center, and right columns report on flows from simulations of 2, 8, and 25 flows, respectively. The plots in the top row report on all flows, regardless of hop count. Plots in the second, third, and fourth rows report on 1-hop, 2-hop, and 3-hop flows, respectively. Plots in the fifth row report on 4- and 5-hop flows.

for 4- and 5-hop flows, preventing TCP from achieving its best throughput. The Weighted Bidders configuration performs well for multi-hop flows in networks with eight and 25 flows by allocating more to multi-hop flows at the expense of singlehop flows. Because one-hop flows tend to achieve higher throughput, the configuration maintains a tighter variation in flow throughputs as indicated by the steeper slope of the Weighted Bidders curve in the top right plot of Fig. 13. Regardless of configuration, ATLAS surpasses IEEE 802.11

in support of concurrent multi-hop flows. The interaction between the IEEE 802.11 back-off algorithm and TCP's congestion control is well known [9]. In testbed experiments, a single TCP flow with no competition has difficulty reaching a destination four hops away [16]. Our simulations corroborate these findings, as approximately 50% of the 4- and 5-hop flows report a throughput of zero. For networks with 25 demands, nearly 75% of 2-hop flows are non-functional; 3-, 4-, and 5hop flows are almost completely shut out. The throughput of

12

ATLAS is achieved in spite of channel wasted transmitting 40 byte TCP ACKs in their own slots.

VII. D ISCUSSION In this section we discuss open issues and suggest potential applications for REACT and ATLAS. A. Improved Reliable Transport TCP's congestion control algorithm is known to suffer cross-layer interactions with binary exponential back-off (BEB) employed by IEEE 802.11 [9]. BEB is short term unfair, allowing a single node to capture the channel at the expense of its neighbours [2], [10] causing high variation in packet delay and making it difficult for TCP to estimate roundtrip delay. Many modifications have been proposed to improve TCP performance over wireless networks [17]; common approaches are detection of packet loss (differentiating it from congestion) and improved estimation of round trip time. An alternative is to minimize packet loss and control variation in packet delay at the MAC layer. ATLAS demonstrates a remarkable control of variation in delay (Fig. 12) enabling TCP to reliably support 3-, 4-, and 5-hop flows over heavily loaded networks (Fig. 13). However, TCP throughput still degrades considerably as the number of hops grows. Potential areas for future work include the integration of ATLAS into a cross-layer solution for reliable transport over wireless networks and the use of REACT to inform TCP's congestion window size. B. Selection of Configurable Parameters ATLAS has three configurable parameters: pdefault , tlostNbr , and pmin . Based on our simulations, [0.01­0.2] is an acceptable range for pdefault (Fig. 5) and [0.1s­2s] is an acceptable range for tlostNbr (Figs. 11a, 11b). In [20], pmin =0.1s is found to be acceptable for a protocol that enforces pmin at all nodes and at all times. Because ATLAS employs pmin temporarily, and only when needed, it is less sensitive to the selection of pmin . Although results show ATLAS to be robust to parameter selection, tuning may be required in other scenarios or in a hardware implementation. C. Dynamic Selection of Auction Capacity ATLAS targets 100% channel allocation by setting auction capacities in REACT to one. Although simulation results show this to be an adequate choice, it is not clear whether performance can be improved by under- or over-allocating the channel. Indeed, optimal auction capacities (however optimal is defined) are dependent on network topology and quality of the communication channel. We leave a thorough analysis of auction capacity selection to future work, pointing out here that REACT adapts continuously, allowing auction capacities to be adjusted dynamically, if necessary. D. Potential Applications for REACT The weighted TLA allocation opens doors for several potential uses. In the simulations of Section VI-F, a bidder's weight is set according to the number of flows it services. It may be desirable to set weights according to queue levels,

G. Comparison with other Scheduled MAC Protocols Here, we compare the adaptation of ATLAS with several other scheduled protocols including DRAND, Z-MAC, FPRP, and SEEDEX. Although the first three compute conflict-free schedules, an NP-hard problem [7], a comparison highlights the agility of ATLAS. 1) Adaptation to Topology Changes: For the simulations of Section VI-E, the number of neighbour changes (i.e., gained or lost neighbours) per second experienced by a node is correlated to the node speed. When the nodes move at 30 m/s, each node is expected to gain, or lose, a neighbour 2.21 times per second; within 6.3s, the number of neighbour changes is expected to exceed the neighbourhood size. Based on the run times reported in [25, Fig. 10], we estimate DRAND to compute schedules for the networks in Section VI-E in approximately 4.9s (adjusting for data rate and a two-hop neighbourhood size of 27). In this time, the topology changes caused by nodes moving at 30 m/s are expected to invalidate the computed schedule. Z-MAC has the same limitation and, although it compensates by running CSMA/CA to resolve collisions, it does not benefit from its TDMA schedule when nodes are mobile. In [25], the run times reported for FPRP schedule generation are comparable to DRAND. For SEEDEX, nodes discover their two-hop neighbours using a fan-in/fan-out procedure described in [26]. However, a practical integration of the procedure into the MAC protocol is not described or evaluated, preventing a comparison of its agility with other MAC protocols. In contrast to the slow schedule computation times of DRAND, Z-MAC, and FPRP, ATLAS is shown to handle node speeds of up to 120 m/s with only moderate degradation to MAC throughput. 2) Adaptation to Changes in Traffic Load: The persistences achieved by DRAND and SEEDEX are dependent on topology alone; neither adapts to traffic load. Although Z-MAC adapts to load, it does so by deviating from its underlying schedule, which does not adapt. FPRP can adapt to load by scheduling a variable number of slots per node; this is done at the expense of both longer frame lengths and longer run times for schedule computation. In contrast, ATLAS adapts to traffic load, responding quickly enough to establish and maintain multi-hop TCP flows. 3) Continuous Adaptation: Common to the scheduled schemes mentioned here is the use of a distinct phase for schedule computation (or neighbour discovery for SEEDEX). The schedules must be updated in order for the MAC to adapt. Any fixed period between schedule updates must be selected a priori; it cannot be adjusted for variations in network mobility. If schedules are to be updated when needed, a mechanism is required to trigger the schedule update. This coordination, by itself, is a challenge in an ad hoc network. In contrast, ATLAS does not employ a schedule computation (or a neighbour discovery) phase and adapts continuously to changes in both topology and traffic load.

13

demand magnitudes, neighbourhood sizes, node betweenness [8], distance from a point of interest (i.e., an access point or a common sink), position in a multicast/broadcast tree, or path hop count. The key observation is that ATLAS maintains flexibility by allowing nodes to define bidder weights arbitrarily to suit the needs of the network. While computation of persistences is the primary motivation for this work, REACT is not limited to this purpose. Consider the Physical Receivers configuration with node demands set to one. The resulting allocation is independent of actions taken by the upper network layers and, therefore, can inform decisions made by those layers. It can serve as a measure of potential network congestion--small allocations are assigned in dense neighbourhoods containing many potentially active neighbours. The routing protocol can use the allocation to discover alternate routes around congestion. An intriguing application is the implementation of differentiated service at the MAC layer. IEEE 802.11e [12] enhances the distributed coordination function by implementing four access categories; an instance of the back-off algorithm is run per access category, each with its own queue. The probability of transmission of each access category is manipulated independently through selection of contention window size and inter-frame space. This permits higher priority traffic to capture the channel from lower priority traffic. Similar results can be achieved by four instances of REACT, each computing the allocation for a single access category. Prioritization is achieved through dynamic coordination of the four auction capacities at each node. A potential strategy sets the capacity for each access category equal to one minus the allocation to higher priority access categories. As a result, higher priority auctions are permitted to starve lower priority auctions of capacity, effectively distributing channel access to high priority traffic. Alternatively, auction capacities can be selected to ensure a minimum or maximum percentage of the channel is offered to an access category. A network can run multiple instances of REACT. For example, an instance of the Physical Receivers configuration with all demands set to one can be run concurrently with four instances configured to support differentiated service. Alternatively, multiple instances of REACT can be used to allocate more than one set of resources concurrently. E. Assumptions Made by ATLAS Two key assumptions are made by ATLAS in its computation of the TLA allocation using REACT: (1) The offers and claims received by a node are accurate. (2) The offers and claims of a node are eventually received by all neighbouring nodes. The first assumption is reasonable, provided received packets are checked for errors by the link layer. The second assumption is almost certainly invalid; asymmetric communication, interference beyond the range of transmission, and signal fading are common in wireless communication and can prevent the delivery of offers and claims. Under realistic conditions, REACT may not converge on the TLA allocation, risking overallocation of the channel. In practice, auctions can adjust their capacities to mitigate the over-allocation. Every node knows

the persistences of its neighbours (from bidder claims) and can compute the expectation for collisions on the channel. Significant deviations above this expectation can trigger the auction to lower its capacity. An evaluation in a testbed of real radios is necessary to understand the sensitivity to anomalies on the wireless channel and the effectiveness of adjusting auction capacities to accommodate channel conditions. The evaluation of ATLAS in Section VI assumes both slot and frame synchronization; ATLAS does not require either. The computation of the TLA allocation by REACT does not rely on a frame structure and the expected performance of the random schedules is not affected by loss of frame synchronization. Even without slot synchronization, REACT can compute the TLA allocation; however, loss of slot synchronization may reduce channel capacity by 50% (see Aloha vs. slotted Aloha in [28]). ATLAS can accommodate the lower channel capacity by reducing auction capacity. This technique may allow ATLAS to be run on commodity IEEE 802.11 hardware [29] that lacks native support for slot synchronization. This is a subject of our current research. F. Enhancing Existing MAC Protocols We have used REACT to compute persistences to be employed within ATLAS, a slotted MAC protocol. Alternatively, REACT can be run on top of the IEEE 802.11 MAC by embedding claims and offers in the headers of existing control and data messages. The TLA allocation can be used to inform the selection of contention window sizes, eliminating the need for (and negative side effects of) binary exponential back off. We are currently working to integrate REACT into IEEE 802.11. Another alternative (and more ambitious) approach is to implement TLA persistences in a topology-dependent MAC that computes conflict-free schedules. Only a few topologydependent schemes allow a node to reserve more than one slot in a frame (i.e., [14], [30]), and those do not define how many slots a node should reserve. The TLA allocation can establish a permissible number of slots to be reserved by each node, given the current topology and traffic load. VIII. C ONCLUSION We have proposed REACT, a distributed auction that converges continuously on the TLA allocation, adapting to changes in both topology and traffic load. The utility of REACT is demonstrated through integration into ATLAS which we simulate under a wide variety of network scenarios. The results presented suggest that REACT can effectively inform the selection of transmitter persistences, and that ATLAS can provide robust, reliable, and scalable services. The application of REACT is not restricted to the computation of transmitter persistences. It has the potential to inform routing and admission control decisions, to enable differentiation of service at the MAC layer, and even to allocate other node resources. In this context, the REACT algorithm provides a potential solution to the immediate challenge of medium access control, but also shows promise as a tool for use in network protocol design in general.

14

ACKNOWLEDGEMENT The authors appreciate the useful comments provided by the anonymous reviewers. R EFERENCES
[1] RFC 1323: TCP Extentions for High Performance, 1992. [2] V. Bharghavan, A. Demers, S. Shenker, and L. Zhang. MACAW: A medium access protocol for wireless LANs. In Proceedings of the ACM Conference on Communications Architectures, Protocols and Applications (SIGCOMM'94), pages 212­225, 1994. [3] I. Chlamtac and A. Farag´ o. Making transmission schedules immune to topology changes in multi-hop packet radio networks. IEEE/ACM Transactions on Networking, 2(1):23­29, 1994. [4] I. Chlamtac, A. Farag´ o, A. D. Myers, V. R. Syrotiuk, and G. Z´ aruba. ADAPT: A dynamically self-adjusting media access control protocol for ad hoc networks. In Proceedings of the IEEE Global Telecommunications Conference (GLOBECOM'99), pages 11­15, 1999. [5] I. Chlamtac and S. S. Pinter. Distributed nodes organization algorithm for channel access in a multihop dynamic radio network. IEEE Transactions on Computers, C-36(6):728­737, June 1987. [6] C. J. Colbourn and V. R. Syrotiuk. Scheduled persistence for medium access control in sensor networks. In Proceedings from the First IEEE International Conference on Mobile Ad hoc and Sensor Systems (MASS'04), pages 264­273, 2004. [7] S. Even, O. Goldreich, S. Moran, and P. Tong. On the NP-completeness of certain network testing problems. Networks, 14(1):1­24, 1984. [8] L. C. Freeman. A set of measures of centrality based on betweenness. Sociometry, 40(1):35­41, 1977. [9] M. Gerla, R. Bagrodia, L. Zhang, K. Tang, and L. Wang. TCP over wireless multi-hop protocols: Simulation and experiments. In Proceedings of the 1999 IEEE International Conference on Communication (ICC'99), pages 1089­1094, 1999. [10] J. Hastad, T. Leighton, and B. Rogoff. Analysis of backoff protocols for multiple access channels. In Proceedings of the 19th annual ACM Symposium on Theory of Computing (STOC'87), pages 740­744, 1987. [11] IEEE. IEEE 802.11, Wireless LAN medium access control (MAC) and physical layer (PHY) specifications, 1997. [12] IEEE. IEEE 802.11e, Enhancements: QoS, including packet bursting, 2007. [13] J. Boleng, N. Bauer, T. Camp, and W. Navidi. Random Waypoint Steady State Mobility Generator (mobgen-ss). http://toilers.mines.edu/. [14] G. Jakllari, M. Neufeld, and R. Ramanathan. A framework for frameless TDMA using slot chains. In Proceedings of the 9th IEEE International Conference on Mobile Ad hoc and Sensor Systems (MASS'12), 2012. [15] J. Ju and V. O. K. Li. An optimal topology-transparent scheduling method in multihop packet radio networks. IEEE/ACM Transactions on Networking, 6(3):298­305, 1998. [16] D. Koutsonikolas, J. Dyaberi, P. Garimella, S. Fahmy, and Y. C. Hu. On TCP throughput and window size in a multihop wireless network testbed. In Proceedings of the 2nd ACM International Workshop on Wireless network testbeds, experimental evaluation and characterization (WiNTECH'07), 2007. [17] K. Leung and V. O. K. Li. Transmission control protocol (TCP) in wireless networks: Issues, approaches, and challenges. IEEE Communications Surveys & Tutorials, 8:64­79, 2006. [18] J. Lutz, C. J. Colbourn, and V. R. Syrotiuk. Apples and oranges: Comparing schedule- and contention-based medium access control. In Proceedings of the 13th ACM International Conference on Modeling, Analysis and Simulation of Wireless and Mobile Systems (MSWiM'10), pages 319­326, 2010. [19] J. Lutz, C. J. Colbourn, and V. R. Syrotiuk. Variable weight sequences for adaptive scheduled access in MANETs. In Proceedings of Sequences and their Applications (SETA'12), pages 53­64, 2012. [20] J. Lutz, C. J. Colbourn, and V. R. Syrotiuk. Topological persistence for medium access control. IEEE Transactions on Mobile Computing, 12(8):1598­1612, 2013. [21] The Network Simulator ns-2. http://www.isi.edu/nsnam/ns/. [22] M. Pi´ oro and D. Medhi. Routing, Flow, and Capacity Design in Communication and Computer Networks. Elsevier Inc., 2004. [23] R. Ramanathan. A unified framework and algorithm for (T/F/C)DMA channel assignment in wireless networks. In Proceedings of the 16th Annual Joint Conference of the IEEE Computer and Communications Societies (INFOCOM'97), pages 900­907, 1997.

[24] I. Rhee, A. Warrier, M. Aia, J. Min, and M. L. Sichitiu. Z-MAC: A hybrid MAC for wireless sensor networks. IEEE Transactions on Networking, 16(3):511­524, 2008. [25] I. Rhee, A. Warrier, J. Min, and L. Xu. DRAND: Distributed randomized TDMA scheduling for wireless ad-hoc networks. IEEE Transactions on Mobile Computing, 8(10):1384­1396, 2009. [26] R. Rozovsky and P. R. Kumar. SEEDEX: A MAC protocol for ad hoc networks. In Proceedings of the 2nd ACM International Symposium on Mobile Ad Hoc Networking and Computing (MOBIHOC'01), pages 67­75, 2001. [27] V. R. Syrotiuk, C. J. Colbourn, and S. Yellamraju. Rateless forward error correction for topology-transparent scheduling. IEEE/ACM Transactions on Networking, 16(2):464­472, 2008. [28] A. S. Tanenbaum. Computer Networks. McGraw Hill, fourth edition, 2003. [29] I. Tinnirello, G. Bianchi, P. Gallo, D. Garlisi, F. Giuliano, and F. Gringoli. Wireless MAC processors: Programming MAC protocols on commodity hardware. In Proceedings of the 31st Annual Joint Conference of the IEEE Computer and Communications Societies (INFOCOM'12), pages 1269­1277, 2012. [30] C. Zhu and M. S. Corson. A five-phase reservation protocol (FPRP) for mobile ad hoc networks. Wireless Networks, 7(4):371­384, 2001.

Jonathan Lutz earned his B.S. in Electrical Engineering from Arizona State University, Tempe, Arizona, in 2000 and his M.S. in Computer Engineering from the University of Waterloo, Waterloo, Canada, in 2003. He is currently working on his Ph.D. in Computer Science at Arizona State University. His research interests include medium access control in mobile ad hoc networks.

Charles J. Colbourn earned his Ph.D. in 1980 from the University of Toronto, and is a Professor of Computer Science and Engineering at Arizona State University. He is the author of The Combinatorics of Network Reliability (Oxford), Triple Systems (Oxford), and 320 refereed journal papers focussing on combinatorial designs and graphs with applications in networking, computing, and communications. In 2004, he was awarded the Euler Medal for Lifetime Research Achievement by the Institute for Combinatorics and its Applications.

Violet R. Syrotiuk earned her Ph.D. in Computer Science from the University of Waterloo (Canada). She is an Associate Professor of Computer Science and Engineering at Arizona State University. Her research has been supported by grants from NSF, ONR, and DSTO, and contracts with LANL, Raytheon, General Dynamics, and ATC. She serves on the editorial boards of Computer Networks and Computer Communications, as well as on the technical program and organizing committees of several major conferences sponsored by ACM and IEEE.

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

1

A Combinatorial Approach to X-Tolerant
Compaction Circuits

arXiv:1508.00481v1 [cs.IT] 3 Aug 2015

Yuichiro Fujiwara and Charles J. Colbourn

Abstractâ€”Test response compaction for integrated circuits
(ICs) with scan-based design-for-testability (DFT) support in
the presence of unknown logic values (Xs) is investigated from
a combinatorial viewpoint. The theoretical foundations of Xcodes, employed in an X-tolerant compaction technique called
X-compact, are examined. Through the formulation of a combinatorial model of X-compact, novel design techniques are developed
for X-codes to detect a specified maximum number of errors in
the presence of a specified maximum number of unknown logic
values, while requiring only small fan-out. The special class of
X-codes that results leads to an avoidance problem for configurations in combinatorial designs. General design methods and
nonconstructive existence theorems to estimate the compaction
ratio of an optimal X-compactor are also derived.
Index Termsâ€”Circuit testing, built-in self-test (BIST), compaction, X-compact, test compression, X-code, superimposed
code, Steiner system, configuration.

I. I NTRODUCTION

T

HIS work discusses a class of codes that arise in data
volume compaction of responses from integrated circuits (ICs) under scan-based test. We first recall briefly the
background of the X-tolerant compaction technique in digital
circuit testing.
Digital circuit testing applies test patterns to a circuit under
test and monitors the circuitâ€™s responses to the applied patterns.
A tester compares the observed response to a test pattern to
the expected response and, if there is a mismatch, declares
the circuit chip defective. Usually the expected responses are
obtained through fault-free simulation of the chip.
Test cost for traditional scan-based testing is dominated
by test data volume and test time [1]. Therefore various test
compression techniques have been developed to reduce test
cost. One way to achieve this is to reduce test application time
and the number of test patterns by employing automatic test
pattern generation (ATPG) (see [2]â€“[5] and references therein).
We are interested in the other kind of technique, using methods
to hash responses while maintaining test quality. Signature
analyzers (e.g., [6]â€“[10]) are vulnerable to error masking
caused by unknown logic values (Xs) [11]. X-compact has
been proposed in order to conduct reliable testing in the
presence of Xs [12]. A response compaction circuit based
This work was supported in part by JSPS Research Fellowships for Young
Scientists (YF) and by DOD grants N00014-08-1-1069 and N00014-08-11070 (CJC).
Y. Fujiwara is with the Department of Mathematical Sciences, Michigan
Technological University, Houghton, MI 49931 USA. yfujiwar@mtu.edu.
C. J. Colbourn is with the School of Computing, Informatics, and Decision
Systems Engineering, Arizona State University, Tempe, AZ 85287-8809 USA.
charles.colbourn@asu.edu

on X-compact is an X-compactor. X-compactors have proved
their high error detection ability in actual systems [11], [13].
An X-compactor can be written in matrix form as an X-code
[14]. Basic properties of X-codes have been studied [14], [15].
Graph theoretic techniques have been employed to minimize
fan-out of inputs [16]; in general an X-compactor tolerates the
presence of Xs in exchange for large fan-out. These studies
focus on particular classes of X-codes rather than the general
coding theoretic aspects.
The purpose of the present paper is to investigate theoretical
foundations of X-codes and to provide general construction
techniques. In Section II we outline the combinatorial requirements for the X-compact technique and present an equivalent
definition of X-codes in order to investigate X-compactors
as codes and combinatorial designs. Some basic properties
of X-codes are also presented. In Section III we investigate
X-codes that require only small fan-out and have good error detectability and X-tolerance. We prove the equivalence
between a class of Steiner t-designs and particular X-codes
having the maximum number of codewords and the minimum
fan-out. This allows us to give constructions and to show
existence of such X-codes. Section IV deals with existence of
X-codes in the more general situation. Both constructive and
nonconstructive theorems are provided. Finally we conclude
in Section V.
II. C OMBINATORIAL R EQUIREMENTS

AND

X-C ODES

We do not describe scan-based testing and response compaction in detail here, instead referring the reader to [11], [12].
Scan-based testing repeatedly applies vectors of test inputs
to the circuit, and for each test captures a vector from {0, 1}n
as the test output. Naturally it is important that the test output
be the correct one. To determine this, the function of the circuit
is simulated (in a fault-free manner) to produce a reference
output. When the test and reference outputs agree, no fault
has been detected. The first major obstacle is that fault-free
simulation may be unable to determine whether a specific
output is 0 or 1, and hence it is an unknown logic value X.
The second is that if each output requires a separate pin on the
chip, the number of tests that can be accommodated is quite
restricted. We deal with these two problems in turn.
We define an algebraic system to describe the behavior of
Xs. The X-algebra X2 = ({0, 1, X}, +, Â·) over the field F2
is the set {0, 1} of elements of F2 and a third element X,
equipped with two binary operations â€œ+â€ (addition) and â€œÂ·â€
(multiplication) satisfying:
1) a + b and a Â· b are performed in F2 for a, b âˆˆ F2 ;

2

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

2) a + X = X + a = X for a âˆˆ F2 ;
3) 0 Â· X = X Â· 0 = 0 for the additive identity 0;
4) 1 Â· X = X Â· 1 = X.
The element X is termed an unknown logic value.
Now consider a test output b = (b1 , . . . , bn ) âˆˆ {0, 1}n
and a reference output c = (c1 , . . . , cn ) âˆˆ {0, 1, X}n . When
ci âˆˆ {0, 1}, the test and reference outputs agree on the ith
bit when bi = ci ; otherwise the ith bit is an error bit. When
ci = X, whatever the value of bi , no error is detected. Thus
the ith bit is (known to be) in error if and only if bi + ci = 1,
using addition in X2 .
Turning to the second problem, an X-compact matrix is an
n Ã— m matrix H with elements from {0, 1}. The compaction
ratio of H is n/m. The number of 1s in the ith row is the
weight, or fan-out, of row i. Output (or response) compaction
is performed by computing the vector d = (d1 , . . . , dm ) = bH
for output (arithmetic is in X2 ). In the same way, the reference
output can be compacted using the same matrix to form r =
(r1 , . . . , rm ) = cH. As before, if di 6= ri and ri 6= X (that is,
if di + ri = 1), an error is detected.
To be of practical value, an X-compact matrix H should
detect the presence of error bits in b with respect to c given
the compacted vectors d and r under â€˜reasonableâ€™ restrictions
on the number of errors and number of unknown logic values.
Suppose that bâ„“ + câ„“ = 1 (so that there is a fault to be
detected). In principle, whenever hâ„“j = 1, the fault could be
observed on output j. Let L =P
{j : hâ„“j = 1}. Suppose then
n
that j âˆˆ L. If it happens that i=1 ci hij = X, the error at
position â„“ is masked for output j (that is, dj + rj = X, and
no error is observed). On the other hand, if
dj + rj =

n
X
i=1

bi hij +

n
X
i=1

ci hij =

n
X

(bi + ci )hij = 0

i=1

then no error is observed. This occurs when there are an even
number of values of i for which hij = 1 and bi + ci = 1;
because this holds when i = â„“ by hypothesis, the error at
position â„“ is canceled for output j when the number of such
errors is even. When an error is masked or canceled for every
output j âˆˆ L, it is not detected. Otherwise, it is detected by
an output that is neither masked nor canceled.
Treating Xâ€™s as erasures and using traditional codes can
increase the error detectability of an X-compactor [17]. Unfortunately, this involves postprocessing test responses and
cannot be easily implemented [12]. Therefore, we focus on
X-compaction in which an error is only detected by the simple
comparison described here.
There are numerous criteria in defining a â€œgoodâ€ X-compact
matrix. It should have a high compaction ratio and be able to
detect any faulty circuit behavior anticipated in actual testing.
Power requirements, compactor delay, and wireability dictate
that the weight of each row in a matrix be small to meet
practical limitations on fan-in and fan-out [11], [16].
The fundamental problem in X-tolerant response compaction is to design an X-compact matrix with large compaction ratio that detects faulty circuit behavior. To achieve
this, X-codes (which represent X-compact matrices) were
introduced [14]. In this section, we discuss basic properties

of X-codes. In order to investigate X-codes from coding and
design theoretic views, we introduce an equivalent definition.
Consider two m-dimensional vectors s1
=
(1) (1)
(1)
(2) (2)
(2)
(s1 , s2 , . . . , sm ) and s2 = (s1 , s2 , . . . , sm ), where
(j)
si âˆˆ F2 . The addition of s1 and s2 is bit-by-bit addition,
denoted by s1 âŠ• s2 ; that is,
(1)

(2)

(1)

(2)

(2)
s1 âŠ• s2 = (s1 + s1 , s2 + s2 , . . . , s(1)
m + sm ).

The superimposed sum of s1 and s2 , denoted s1 âˆ¨ s2 , is
(1)

(2)

(1)

(2)

(2)
s1 âˆ¨ s2 = (s1 âˆ¨ s1 , s2 âˆ¨ s2 , . . . , s(1)
m âˆ¨ sm ),
(j)

(l)

(j)

(l)

where si âˆ¨ sk = 0 if si = sk = 0, otherwise 1. An
m-dimensional vector s1 covers an m-dimensional vector s2
if s1 âˆ¨ s2 = s1 .
For a finite set S = {s1 , . . . , ss } of m-dimensional vectors,
define
M
_
S = s1 âŠ• Â· Â· Â· âŠ• ss and
S = s1 âˆ¨ Â· Â· Â· âˆ¨ ss .
L
W
When S =L
{s1 } isWa singleton,
S = S = s1 . For S = âˆ…
we define
S = S = 0, the zero vector.
Let d be a positive integer and x a nonnegative integer.
An (m, n, d, x) X-code X = {s1 , s2 , . . . , sn } is a set of mdimensional vectors over F2 such that |X | = n and
_
M
_
( S1 ) âˆ¨ (
S2 ) 6=
S1 .

for any pair of mutually disjoint subsets S1 and S2 of X with
|S1 | = x and 1 â‰¤ |S2 | â‰¤ d. A vector si âˆˆ X is a codeword.
(i)
(i)
The weight of a codeword si is |{sj 6= 0 : sj âˆˆ si }|. The
ratio n/m is the compaction ratio of X . An X-code forming
an orthonormal basis of the m-dimensional linear space over
F2 is trivial.
Roughly speaking, an X-code is a set of codewords such
that for every positive integer dâ€² â‰¤ d no superimposed sum of
any x codewords covers the vector obtained by adding up any
dâ€² codewords chosen from the rest of the n âˆ’ x codewords.
Now we present a method of designing an X-compact matrix
from an X-code.
Proposition 1: There exists an (m, n, d, x) X-code X if and
only if there exists an n Ã— m X-compact matrix H which
detects any combination of dâ€² faults (1 â‰¤ dâ€² â‰¤ d) in the
presence of at most x unknown logic values.
Proof: First we prove necessity. Assume that X is an
(m, n, d, x) X-code. Write X = {s1 , s2 , . . . , sn }, where si =
(i) (i)
(i)
(s1 , s2 , . . . , sm ) for 1 â‰¤ i â‰¤ n. Define an n Ã— m matrix
(i)
H = (hi,j ) as hi,j = sj . We show that H forms an Xcompact matrix that detects a fault if the test output b contains
dâ€² error bits, 1 â‰¤ dâ€² â‰¤ d, and up to x Xs.
Let E = {k : bk + ck = 1}, the set of indices of error bits,
have cardinality dâ€² . Let X = {k : ck = X}, the set of indices
of unknown logic values, have cardinality x. Now comparing
dâ„“ and râ„“ ,
X
X
dâ„“ + râ„“ =
bk Â· hk,â„“ +
ck Â· hk,â„“
k

=

k

X

(bk + ck ) Â· hk,â„“

kâˆˆE,X

=

X

kâˆˆE

1 Â· hk,â„“ +

X

kâˆˆX

X Â· hk,â„“ ,

(1)

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

with operations performed in X2 . Because the set of rows of
H forms the set of codewords of X , no superimposed sum of
x rows covers the vector obtained by an addition of any dâ€²
rows. Hence there exists a column c such that
X
X
1 Â· hk,c = 1 and
X Â· hk,c = 0.
(2)
kâˆˆE

kâˆˆX

Then (1) and (2) imply dc + rc = 1, that is, H detects a fault.
Because (2) holds if and only if the right hand side of (1)
equals one for l = c, sufficiency is straightforward.
By virtue of this equivalence, we can employ various
known results and techniques in coding theory to design an
X-compactor with good error detection ability, X-tolerance,
and a high compaction ratio. For the case when x = 0, an
(m, n, d, 0) X-code forms an n Ã— m X-compact matrix which
is a parity-check matrix of a binary linear code of length n and
minimum distance d. In fact, since the condition that x = 0
implies the absence of Xs, this special case is reduced to
traditional space compaction. Because our focus is compaction
in the presence of unknown logic values, we assume that x â‰¥ 1
henceforth unless otherwise stated. In the absence of Xs, see
[18], [19].
By definition, an (m, n, d, x) X-code, d â‰¥ 2, is also an
(m, n, d âˆ’ 1, x) X-code. Also an (m, n, d, x) X-code forms an
(m, n, d, x âˆ’ 1) X-code. Moreover, an (m, n, d, x) X-code is
an (m, n, d + 1, x âˆ’ 1) X-code [14].
It can be difficult to design an X-compactor having both the
necessary error detectability and the exact number of inputs
needed. One trivial solution is to discard codewords from a
larger X-code with sufficient error detection ability and Xtolerance. The following is another simple way to adjust the
number of inputs.
Proposition 2: If
an
(m, n, d, x)
X-code
and
an (mâ€² , nâ€² , dâ€² , xâ€² ) X-code exist, there exists an
(m + mâ€² , n + nâ€² , min{d, dâ€² }, min{x, xâ€² }) X-code.
Proof: Let X = {s1 , . . . , sn } be an (m, n, d, x) Xcode and Y = {t1 , . . . , tnâ€² } an (mâ€² , nâ€² , dâ€² , xâ€² ) X-code.
(i)
(i)
Extend each codeword si = (s1 , . . . , sm ) of X by appending mâ€² 0â€™s so that extended vectors have the form
(i)
(i)
sâ€²i = (s1 , . . . , sm , 0, . . . , 0). Similarly extend each codeword
(j)
(j)
tj = (t1 , . . . , tmâ€² ) of Y by appending m 0s so that extended
(j)
(j)
vectors have the form tâ€²j = (0, . . . , 0, t1 , . . . , tmâ€² ). The
extended (m + mâ€² )-dimensional vectors form an (m + mâ€² , n +
nâ€² , min{d, dâ€² }, min{x, xâ€² }) X-code.
Proposition 2 says that given an (m, n, d, x) X-code, a
codeword of weight less than or equal to x does not essentially
contribute to the compaction ratio (see also [14]). In fact,
(i)
(i)
if X contains such a codeword si = (s1 , . . . , sm ), there
(i)
exists at least one coordinate mâ€² such that smâ€² = 1 and
(j)
smâ€² = 0 for any other codeword sj âˆˆ X . Hence we can
delete si and coordinate mâ€² from X while keeping d and x.
By applying Proposition 2 and combining a trivial X-code
and another X-code, we can obtain an X-code having the
same number of codewords with compaction ratio no smaller.
For this reason, when constructing an (m, n, d, x) X-code
explicitly, we assume that every codeword has weight greater
than x.

3

Let M (m, d, x) be the maximum number n of codewords
for which there exists an (m, n, d, x) X-code. More codewords
means a higher compaction ratio. Hence an (m, n, d, x) Xcode satisfying n = M (m, d, x) is optimal.
Determining the exact value of M (m, d, x) seems difficult
except for M (m, 1, 1). As pointed out in [14], a special case
of M (m, d, x) has been extensively studied in the context of
superimposed codes [20]. An (1, x)-superimposed code of size
m Ã— n is an m Ã— n matrix S with entries in F2 such that no
superimposed sum of any x columns of S covers any other
column of S. Superimposed codes are also called cover-free
families and disjunct matrices.
By definition, a (1, x)-superimposed code of size m Ã— n is
equivalent to the transpose of an X-compact matrix obtained
from an (m, n, 1, x) X-code. Hence known results on the
maximum ratio n/m for superimposed codes immediately
give information about M (m, 1, x). For completeness, we list
useful results on M (m, 1, x).
By Spernerâ€™s theorem,
Theorem 2.1: (see [21], [22]) For m â‰¥ 2 an integer,


m
M (m, 1, 1) â‰¤
.
âŒŠm/2âŒ‹

Indeed by taking all the m-dimensional vectors of weight
âŒŠm/2âŒ‹ as codewords, we attain the bound. The same argument
is also found in [14].
The following is a simple upper bound on M (m, 1, x):
Theorem 2.2: [22] For any x â‰¥ 2,
log2 M (m, 1, x) â‰¤

cm log2 x
x2

for some constant c.
Several different proofs of Theorem 2.2 are known. Bounds
on the constant c are approximately two in [23], approximately
four in [24], and approximately eight in [25].
The asymptotic behavior of the maximum possible number
of codewords has been also investigated for superimposed
codes. Define the ratio R(x) as
log2 M (m, 1, x)
.
mâ†’âˆ
m
The best lower bound R(x) â‰¤ R(x) can be found in
[26] and the best upper bound R(x) â‰¥ R(x) in [23]. The
descriptive asymptotic form of the best bounds as x â†’ âˆ is
R(x) = lim

R(x) âˆ¼

1
2 log2 x
,
and R(x) âˆ¼
x2 log2 e
x2

where e is Napierâ€™s constant. For a detailed summary of the
known lower and upper bounds, see [27]. Constructions with
many codewords have been studied in [28], [29]. See also
[30]â€“[33] and references therein.
III. X-C OMPACTORS

WITH

S MALL FAN -O UT

In this section we consider an X-compactor having sufficient
tolerance for errors and Xs, a high compaction ratio, and small
fan-out. This section is divided into four parts. Subsection
III-A deals with background and known results of the fanout problem in X-compactors. Then in Subsection III-B we
investigate X-codes that tolerate up to two Xâ€™s and have

4

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

the minimum fan-out. X-Codes with further error detection
ability and X-tolerance are investigated in Subsection III-C. In
Subsection III-D we give a brief overview of the performance
of our X-codes given in this section and compare them with
other codes.
A. Background: Fan-Out in X-Codes
X-compact reduces the number of bits in the compacted
output while keeping error detection ability by propagating
each single bit to many signal lines. In fact, each output of
the X-compactor in [12] connects to about half of all inputs.
However, larger fan-in increases power requirements, area, and
delay [16]. When these disadvantages are concerns, fan-out of
inputs of a compactor should be small to reduce fan-in values.
In terms of X-codes, the required fan-out of input i in an
X-compactor is the weight of codeword si of the X-code.
Hence, in order to address the fan-out problem, it is desirable
for a codeword to have small weight. However, as mentioned
in Section I, an (m, n, d, x) X-code containing a codeword
with weight at most x is not essential in the sense of the
compaction ratio. Hence, throughout this section, we restrict
ourselves to (m, n, d, x) X-codes in which every codeword has
weight precisely x + 1, namely constant weight codes.
When a compactor is required to tolerate only a single unknown logic value, fan-out is minimized when every codeword
of an X-code has constant weight two. This extreme case was
addressed in [16] by considering a simple graph. We briefly
restate their theorems in terms of X-codes.
A graph G is a pair (V, E) such that V is a finite set and
E is a set of pairs of distinct elements of V . An element of
V is called a vertex, and an element of E is called an edge.
The girth g of G is the minimal size |C| of a subset C âŠ‚ E
such that each vertex appearing in C is contained in exactly
two edges.
The edge-vertex incidence matrix H of a graph G = (V, E)
is a |E| Ã— |V | binary matrix H = (hi,j ) such that rows and
columns are indexed by edges and vertices respectively and
hi,j = 1 if the ith edge contains the jth vertex, otherwise 0.
By considering the edge-vertex incidence matrix of a graph
and Proposition 1, we obtain:
Theorem 3.1: [16] There exists a graph G = (V, E) of girth
g if and only if there exists a (|V |, |E|, g âˆ’ 2, 1) X-code of
constant weight two.
Theorem 3.2: [16] A set X of m-dimensional vectors is an
(m, n, d âˆ’ 1, 1) X-code of constant weight two if and only if
it is an (m, n, d, 0) X-code of weight two.
These two theorems say that in order to design an Xcompactor with high error detection ability, we only need to
find a graph with large girth. The same argument is also found
in [14]. For existence of such graphs and more details on Xcodes of constant weight two, see [16] and references therein.
B. Two Xâ€™s and Fan-Out Three
Multiple Xâ€™s can occur; here we present X-codes that are
tolerant to two Xâ€™s and have the maximum compaction ratio.
To accept up to two unknown logic values, we need an X-code

of constant weight three. We employ a well-known class of
combinatorial designs.
A set system is an ordered pair (V, B) such that V is a finite
set of points, and B is a family of subsets (blocks) of V . A
Steiner t-design S(t, k, v) is a set system (V, B), where V is
a finite set of cardinality v and B is a family of k-subsets of
V such that each t-subset of V is contained in exactly one
block. Parameters v and k are the order and block size of a
Steiner t-design. When t = 2 and k = 3, an S(2, 3, v) is a
Steiner triple system of order v, STS(v). An STS(v) exists if
and only if v â‰¡ 1, 3 (mod 6) [34]. A triple packing of order
v is a set system (V, B) such that B is a family of triples of
a finite set V of cardinality v and any pair of elements of V
appear in B at most once. An STS(v) is a triple packing of
order v â‰¡ 1, 3 (mod 6) containing the maximum number of
triples.
The point-block incidence matrix of a set system (V, B)
is the binary |V | Ã— |B| matrix H = (hi,j ) such that rows
are indexed by points, columns are indexed by blocks, and
hi,j = 1 if the ith point is contained in the jth block, otherwise
0. The block-point incidence matrix is its transpose.
When d = 1, an (m, n, 1, 2) X-code of constant weight three
is equivalent to a (1, 2)-superimposed code of size m Ã— n of
constant column weight three. It is well known that the pointblock incidence matrix of an S(t, k, v) forms
 an (1, âŒˆk/(t âˆ’
1)âŒ‰ âˆ’ 1)-superimposed code of size v Ã— vt / kt . Hence, by
using an STS(v), we obtain for every v â‰¡ 1, 3 (mod 6) a
(v, v(v âˆ’ 1)/6, 1, 2) X-code. An upper bound on the number
of codewords of (1, 2)-superimposed codes of constant weight
k is available:
Theorem 3.3: [35] Let nk (m) denote the maximum number
of columns of a (1, 2)-superimposed code such that and every
column is of length m and has constant weight k. Then,

m
n2tâˆ’1 (m) â‰¤ n2t (m + 1) â‰¤

t

2tâˆ’1
t

with equality if and only if there exists a Steiner t-design
S(t, 2t âˆ’ 1, m).
The following is an immediate consequence:
Theorem 3.4: For any (m, n, 1, 2) X-code of constant
with equality if and only if there
weight three, n â‰¤ m(mâˆ’1)
6
exists an STS(m).
Hence for d = 1, x = 2, and fan-out three, an X-code from
any STS(v) has the maximum compaction ratio (v âˆ’ 1)/6.
One may ask for larger error detectability of an (m, n, 1, 2)
X-code when one (or zero) unknown logic value is assumed.
An (m, n, d, x) X-code is also an (m, n, d + 1, x âˆ’ 1) X-code,
and hence any (m, n, 1, 2) X-code from an STS(m) is also an
(m, n, 2, 1) X-code. However, a careful choice of Steiner triple
systems gives higher error detectability while maintaining the
compaction ratio.
A configuration C in a triple packing, (V, B), is a subset
C âŠ† B. The set of points appearing in at least one block of a
configuration C is denoted by V (C). Two configurations C and
C â€² are isomorphic, denoted C âˆ¼
= C â€² , if there exists a bijection
â€²
Ï† : V (C) â†’ V (C ) such that for each block B âˆˆ C, the image
Ï†(B) is a block in C â€² . When |C| = i, a configuration C is an
i-configuration. A configuration C is even if for every point

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

a appearing in C the number |{B : a âˆˆ B âˆˆ C}| of blocks
containing a is even. Because every block in a triple packing
has three points, no i-configuration for i odd is even.
A triple packing is r-even-free if for every integer i satisfying 1 â‰¤ i â‰¤ r it contains no even i-configurations. By
definition every r-even-free triple packing, r â‰¥ 2, is also
(r âˆ’ 1)-even-free. For an even integer r, an r-even-free triple
packing is also (r + 1)-even-free. Every triple packing is
trivially 3-even-free. For v > 3 an STS(v) may or may not be
4-even-free. Up to isomorphism, the only even 4-configuration
is the Pasch configuration. It can be written on six points
and four blocks: {{a, b, c}, {a, d, e}, {f, b, d}, {f, c, e}}. For
the list of all the small configurations in a triple packing and
more complete treatments, we refer the reader to [34] and [36].
Because a 4-even-free STS is 5-even-free, an STS is 5-evenfree if and only if it contains no Pasch configuration.
Lemma 3.5: If there exists a 5-even-free STS(v), there
exists a (v, v(v âˆ’ 1)/6, 3, 1) X-code of constant weight three.
The code is a (v, v(v âˆ’ 1)/6, 5, 0) X-code of constant weight
three.
Proof: Let (V, B) be a 5-even-free STS(v). For every
Bi âˆˆ B define a v-dimensional vector si such that each
(i)
coordinate sj âˆˆ si is indexed by a distinct point j âˆˆ V
(i)
and sj = 1 if j âˆˆ Bi , otherwise 0. Then we obtain a
(v, v(v âˆ’ 1)/6, 1, 2) X-code S = {si : Bi âˆˆ B} of constant
weight three. We prove that S is a (v, v(v âˆ’ 1)/6, 3, 1) X-code
that is also a (v, v(v âˆ’ 1)/6, 5, 0) X-code. By definition, for
1 â‰¤ i â‰¤ 5 no i-configuration C âŠ† B is even. Hence
M
{si : Bi âˆˆ C} =
6 0.
This implies that S is a (v, v(v âˆ’ 1)/6, 5, 0) X-code. On the
other hand, since no pair of points appears twice, for any
mutually distinct blocks Bi , Bj , Bk âˆˆ B,
si 6= sj and si âˆ¨ (sj âŠ• sk ) 6= si .
It remains to show that no codeword in S covers addition
of three others. Suppose to the contrary that there exist four
distinct codewords si , sj , sk , and sl such that
si âˆ¨ (sj âŠ• sk âŠ• sl ) = si .
Because no pair of points appears twice and every block has
exactly three points, the only possible case is that the 4configuration {Bi , Bj , Bk , Bl } forms a Pasch configuration,
and hence it is even, a contradiction.
Steiner triple systems avoiding Pasch configurations have
been long studied as anti-Pasch STSs [34].
Theorem 3.6: [37] There exists a 5-even-free STS(v) if and
only if v â‰¡ 1, 3 (mod 6) and v 6âˆˆ {7, 13}.
By combining Theorem 3.6 and Lemma 3.5, we obtain:
Theorem 3.7: For every v â‰¡ 1, 3 (mod 6) and v 6âˆˆ {7, 13},
there exists a (v, v(v âˆ’ 1)/6, 1, 2) X-code of constant weight
three that is a (v, v(v âˆ’ 1)/6, 3, 1) X-code and a (v, v(v âˆ’
1)/6, 5, 0) X-code.
An X-compactor designed from these can detect any odd
number of errors unless there is an unknown logic value. One
may want to take advantage of the high compaction ratio of
the optimal (m, n, 1, 2) X-codes arising from 4-even-free STSs

5

when there is only a small possibility that more than two Xs
occur or multiple errors happen with multiple Xs. Our X-codes
from 4-even-free STSs also have high performance in such
situations:
Theorem 3.8: The probability that a (v, v(v âˆ’ 1)/6, 1, 2) Xcode from a 4-even-free STS(v) fails to detect a single error
162(vâˆ’3)2
when there are exactly three Xs is (v+2)(v+3)(vâˆ’4)(v
2 âˆ’vâˆ’18) .
Proof: Because there is only one error, an X-code fails
to detect this error when all three points in the block that
corresponds to the error are contained in at least one block
corresponding to an X. The number of occurrences of each 4configuration in an STS(v) is determined by v and the number
of Pasch configurations (see [34], for example). A simple
calculation proves the assertion.
Theorem 3.9: The probability that a (v, v(v âˆ’ 1)/6, 1, 2)
X-code from a 4-even-free STS(v) fails to detect errors
when there are exactly two Xs and exactly two errors is
1296
(v+2)(v+3)(vâˆ’4)(v 2 âˆ’vâˆ’18) .
Proof: A (v, v(v âˆ’ 1)/6, 1, 2) X-code from a 4-evenfree STS(v) fails to detect errors when there are exactly two Xs and exactly two errors only when corresponding four blocks form a 4-configuration isomorphic to
{{a, b, c}, {d, e, f }, {a, e, g}, {c, f, g}} where the first two
blocks represent Xs and the other two blocks correspond to
errors. The number of occurrences of the 4-configuration in
a 4-even-free STS(v) is v(vâˆ’1)(vâˆ’3)
, and the total number of
4
v(vâˆ’1) 
6
occurrences of all 4-configurations is
[34]. Divide
4
 v(vâˆ’1) 
v(vâˆ’1)(vâˆ’3)
4
6
to obtain the probability that the
by 2
4
4
X-code fails to detect the two errors.
Hence when a 4-even-free STS of sufficiently large order
is used, the probability that the corresponding X-code fails to
detect errors when the sum of the numbers of errors and Xs
is at most four is close to zero. A more complicated counting
argument is necessary to calculate the performance of X-codes
from STSs when the sum of the numbers of errors and Xs is
greater than four. For more complete treatments and current
research results on counting configurations in Steiner triple
systems, we refer the reader to [36] and references therein.
Useful explicit constructions for 5-even-free STS(v) can be
found in [34], [37]â€“[41]. The cyclic 5-sparse Steiner triple
systems in [42] provide examples of 5-even-free STS(v) for
v â‰¤ 97, because cyclic 5-sparse systems are all anti-Pasch.
Further r-even-freeness improves the error detectability of the
resulting X-code:
Theorem 3.10: For r â‰¥ 4, if there exists an r-even-free
triple packing (V, B), there exists a (|V |, |B|, 1, 2) X-code of
constant weight three that is also a (|V |, |B|, 3, 1) X-code and
a (|V |, |B|, r, 0) X-code.
Proof: Let (V, B) be an r-even-free triple packing of order
v. For every Bi âˆˆ B define a v-dimensional vector si such
(i)
that each coordinate sj âˆˆ si is indexed by a distinct point
(i)
j âˆˆ V and sj = 1 if j âˆˆ Bi , otherwise 0. Then we obtain a
(|V |, |B|, 1, 2) X-code S = {si : Bi âˆˆ B} of constant weight
three. It suffices to prove that S forms a (|V |, |B|, r, 0) X-code.
Suppose to the contrary that S is not a (|V |, |B|, r, 0) X-code.
Then for some râ€² â‰¤ r there exists a set of râ€² codewords si ,

6

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

sj . . . , sk such that
si âŠ• sj Â· Â· Â· âŠ• sk = 0.
However, the set of the corresponding blocks Bi , Bj ,. . . ,Bk
forms an even râ€² -configuration, a contradiction.
One may want an r-even-free STS with large r to obtain
higher error detection ability while keeping the maximum
compaction ratio. Although it is known that every Steiner triple
system has a configuration with seven or fewer blocks so that
every element of the configuration belongs to at least two [36],
it may happen that none of these are even. Nevertheless, the
following gives an upper bound of even-freeness of Steiner
triple systems.
Theorem 3.11: For v > 3 there exists no 8-even-free
STS(v).
Proof: Suppose to the contrary that there exists an
STS(v), S, that is 8-even free. Consider a 4-configuration
C isomorphic to {{a, b, e}, {c, d, e}, {a, c, f }, {b, d, g}}; the
points f and g are each contained in exactly one block.
For any anti-Pasch STS(v) the number of occurrences of
configurations isomorphic to C is v(v âˆ’ 1)(v âˆ’ 3)/4 [43] (see

also [44]). Because v â‰¥ 7, we have v(v âˆ’ 1)(v âˆ’ 3)/4 > v2 .
Hence there is a pair of configurations A and B such that
A âˆ¼
= C and they share the two points contained in
= B âˆ¼
exactly one block. In other words, there exists a pair A and
B having the form {{a, b, e}, {c, d, e}, {a, c, f }, {b, d, g}} and
{{aâ€² , bâ€² , eâ€² }, {câ€² , dâ€² , eâ€² }, {aâ€² , câ€² , f }, {bâ€² , dâ€² , g}} respectively. If
there is no common block between A and B, then the merged
configuration A âˆª B forms an even configuration consisting
of eight blocks, a contradiction. Otherwise, there is at least
one block contained in both A and B. Removing blocks
shared between A and B from their union, we obtain an even
configuration on four or six blocks, a contradiction.
By combining Theorems 3.4, 3.10, and 3.11, we have:
Theorem 3.12: There exists no (m, n, 1, 2) X-code that
achieves the maximum compaction ratio (m âˆ’ 1)/6 and is
also an (m, n, 3, 1) X-code and an (m, n, 8, 0) X-code.
An STS is 7-even-free if and only if it is 6-evenfree. Up to isomorphism, there are two kinds of even 6configurations which may appear in an STS. One is called
the grid and the other is the double triangle. Both 6configurations are described by nine points and six blocks:
{{a, b, c}, {d, e, f }, {g, h, i}, {a, d, g}, {b, e, h}, {c, f, i}} and
{{a, b, c}, {a, d, e}, {c, f, e}, {b, g, h}, {d, h, i}, {f, g, i}} respectively. By definition, an STS is 6-even-free if it simultaneously avoids Pasches, grids, and double triangles. We do
not know whether there exists a 6-even-free STS(v) for any
v > 3. However, a moderately large number of triples can be
included while keeping 6-even-freeness:
Theorem 3.13: There exists a constant c > 0 such that for
sufficiently large v there exists a 6-even-free triple packing of
order v with cv 1.8 triples.
Proof: Let C â€² be a set of representatives of all of the
nonisomorphic even configurations on six or fewer triples and
let C â€²â€² be a configuration consisting of pair of distinct triples
sharing a pair of elements. Let C = C â€² âˆª C â€²â€² . Pick uniformly
6
at random triples from V with probability p = câ€² v âˆ’ 5 inde1
10
â€²
â€²
pendently, where c satisfies 0 < c < ( 41Â·79Â·83 ) 5 . Let bC be

a random variable counting the configurations isomorphic to
a member of C in the resulting set of triples. Define E(bC ) as
its expected value. Then

E(bC ) â‰¤
=

  4
  6
  9
v
v
v
2
4
3
3
3
p +
p +
p6
2
4
4
6
6
9
 9 â€²6 1.8
c v
3
+ f (v),
9!
6

where f (v) = O(v 1.6 ). By Markovâ€™s Inequality,
P (bC â‰¥ 2E(bC )) â‰¤

1
.
2

Hence,


 9 â€²6 1.8
1
c v
3
+ 2f (v) â‰¥ .
P bC â‰¤ 2
9!
2
6
Let t be a random variable counting the triples and E(t) its
expected value. Then
 
câ€²
v
E(t) = p
= v 1.8 âˆ’ g(v),
6
3
where g(v) = O(v 0.8 ). Because t is a binomial random
variable, by Chernoffâ€™s inequality, for sufficiently large v


E(t)
1
E(t)
< eâˆ’ 8 < .
P t<
2
2
Hence, if v is sufficiently large, then with positive probability we have a set B of triples with the property that |B| > E(t)
2
and the number of configurations in B isomorphic to a member
of C is at most
 9 â€²6 1.8
c v
2 3
+ 2f (v).
6
9!
Let ex(v, r) be the maximum cardinality |B| such that there
exists an r-even-free triple packing. By deleting a triple from
each configuration isomorphic to a member of C, we obtain
 9 â€²6 1.8
c v
E(t)
âˆ’2 3
+ h(v),
ex(v, 6) â‰¥
2
9!
6
where h(v) = O(v 1.6 ). Then for some positive constant c and
sufficiently large v, it holds that ex(v, 6) â‰¥ cv 1.8 .
Hence we have:
Theorem 3.14: There exists a constant c > 0 such that for
sufficiently large v there exists a (v, cv 1.8 , 1, 2) X-code that is
also a (v, cv 1.8 , 3, 1) X-code and a (v, cv 1.8 , 6, 0) X-code.
An STS(v) has approximately v 2 /6 triples. The same technique can be used to obtain a lower bound on ex(v, r) for
12
r â‰¥ 8. In fact, ex(v, 8) is at least O(v 7 ), and hence for
sufficiently large v there exists a constant c > 0 such that
12
12
there exists a (v, cv 7 , 1, 2) X-code that is also a (v, cv 7 , 3, 1)
12
X-code and a (v, cv 7 , 8, 0) X-code.

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

C. Higher X-Tolerance with the Minimum Fan-Out
In general, the probability that a defective digital circuit
produces an error at a specific signal output line is quite small.
In fact, several errors are unlikely happen simultaneously [11],
[45]. Also, multiple Xs with errors are rare [12]. Therefore, Xcodes given in Theorems 3.7 and 3.13 are particularly useful
for relatively simple scan-based testing such as built-in selftest (BIST) where the tester is only required to detect defective
chips. Nonetheless, more sophisticated X-codes are also useful
to improve test quality and/or to identify or narrow down the
error sources by taking advantage of more detailed information
about when incorrect responses are produced [12]. Hence, for
use in higher quality testing and error diagnosis support, it
is of theoretical and practical interest to consider (m, n, d, x)
X-codes of constant weight x + 1, where x â‰¥ 3 or d â‰¥ 6.
For (m, n, 1, 2) X-codes of constant weight three, we employed Theorem 3.3 to obtain an upper bound on the number
of codewords. The following theorem gives a generalized
upper bound:
Theorem 3.15: [46] Let n(x, m, k) denote the maximum
number of columns of a (1, x)-superimposed code such that
every column is of length m and has constant weight k. Then,
for every x, t and i = 0, 1 or i â‰¤ x/2t2 ,

 

mâˆ’i
kâˆ’i
n(x, m, x(t âˆ’ 1) + 1 + i) â‰¤
/
t
t
for all sufficiently large m, with equality if and only if there
exists a Steiner t-design S(t, x(t âˆ’ 1) + 1, m âˆ’ i).
By putting t = 2 and i = 0, we obtain:
Corollary 3.16: For an (m, n, 1, x) X-code of constant
weight x + 1,
  

m
x+1
nâ‰¤
/
2
2
for all sufficiently large m, with equality if and only if there
is an S(2, x + 1, m).
Because the set of columns of the block-point incidence
matrix of any S(2, x + 1, m) forms an (m, m(m âˆ’ 1)/x(x +
1), 1, x) X-code of constant weight x + 1, the existence of
Steiner 2-designs is our next interest. For k âˆˆ {4, 5}, necessary
and sufficient conditions for existence of an S(2, k, v) are
known:
Theorem 3.17: [47] There exists an S(2, 4, v) if and only
if v â‰¡ 1, 4 (mod 12).
Theorem 3.18: [48] There exists an S(2, 5, v) if and only
if v â‰¡ 1, 5 (mod 20).
For k â‰¥ 6, the necessary and sufficient conditions on v
for existence of an S(2, k, v) are not known in general; the
existence of a Steiner 2-design is solved only in an asymptotic
sense [49], although for â€˜smallâ€™ values of k substantial results
are known. For a comprehensive table of known Steiner 2designs, see [50].
As with X-codes from Steiner triple systems, the error
detectability can be improved by considering avoidance of
even configurations.
An S(2, k, v), (V, B), is r-even-free if for 1 â‰¤ i â‰¤ r
it contains no subset C âŠ† B such that |C| = i and each
point appearing in C is contained in exactly an even number

7

of blocks in C. A generalized Pasch configuration in an
S(2, k, v), (V, B), is a subset C âŠ‚ B such that |C| = k + 1 and
each point appearing in C is contained exactly two blocks of
C. As with triple systems, an S(2, k, v) is (k + 1)-even-free if
and only if it contains no generalized Pasch configurations.
Theorem 3.19: If an r-even-free S(2, k, v) for r â‰¥ k + 1
exists, there exists a (v, v(v âˆ’ 1)/k(k âˆ’ 1), 1, k âˆ’ 1) X-code
of constant weight k that is also a (v, v(v âˆ’ 1)/k(k âˆ’ 1), k, 1)
X-code and a (v, v(v âˆ’ 1)/k(k âˆ’ 1), r, 0) X-code.
Proof: Let (V, B) be an r-even-free S(2, k, v). For every
Bi âˆˆ B define a v-dimensional vector si such that each
(i)
coordinate sj âˆˆ si is indexed by a distinct point j âˆˆ V
(i)
and sj = 1 if j âˆˆ Bi , otherwise 0. Then we obtain a
(v, v(v âˆ’ 1)/k(k âˆ’ 1), 1, k âˆ’ 1) X-code S = {si : Bi âˆˆ B} of
constant weight k. By definition of an r-even-free S(2, k, v),
it is straightforward to see that S is also a (v, v(v âˆ’ 1)/k(k âˆ’
1), r, 0) X-code. It suffices to prove that S can also be used
as a (v, v(v âˆ’ 1)/k(k âˆ’ 1), k, 1) X-code. Assume that this is
not the case. Then, by following the argument in the proof of
Lemma 3.5, B contains a generalized Pasch configuration, a
contradiction.
Existence of an r-even-free design has been investigated
in the study of erasure-resilient codes for redundant array of
independent disks (RAID) [51]. In fact, infinitely many r-evenfree S(2, k, v)s can be obtained from affine spaces over Fq
[52].
Theorem 3.20: [52] For any odd prime power q and positive
integer n â‰¥ 2 the points and lines of AG(n, q) form a (2qâˆ’1)even-free S(2, q, q n ).
By combining Theorems 3.19 and 3.20, we obtain:
Theorem 3.21: For any odd prime power q and positive
integer n â‰¥ 2, there exists a (q n , q nâˆ’1 (q n âˆ’1)/(qâˆ’1), 1, qâˆ’1)
X-code of constant weight q that is also an (q n , q nâˆ’1 (q n âˆ’
1)/(q âˆ’ 1), q, 1) X-code and a (q n , q nâˆ’1 (q n âˆ’ 1)/(q âˆ’ 1), 2q âˆ’
1, 0) X-code.
D. Characteristics of X-Codes from Combinatorial Designs
We have given tight upper bounds of compaction ratio for
(m, n, 1, x) X-codes with the minimum fan-out and presented
explicit construction methods for X-codes that attain the
bounds. As far as the authors are aware, these are the first
mathematical bounds and construction techniques for this type
of optimal X-code with constant weight greater than two.
Optimal X-codes given in Theorems 3.7 and 3.21 in particular
have higher error detection ability when the number of Xs
is smaller than x. The known construction technique using
hypergraphs, briefly mentioned in [16], can not guarantee the
same error detection ability.
To illustrate the usefulness of our X-codes, here we compare
the error detection ability of an example X-code that can
be generated using Theorem 3.7 with characteristics of Xcodes proposed in [11]. The probability that the example
(50, 500, 1, 1) X-code in Table 5 in [11] fails to detect a single
error when there are exactly two Xs is around 4.2 Ã— 10âˆ’6 .
The fan-out of this code is 11. Our X-code from Theorem
3.7, which has the same compaction ratio, has parameters
(61, 610, 1, 2). The probability that this X-code fails to detect

8

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

a single error in the same situation is exactly 0. Its fan-out
is 3, which is significantly smaller. While the multiple error
detection ability of the (50, 500, 1, 1) X-code is not specified
in [11], our code can always detect up to three errors when
there is only one X, and up to five errors when there is no X.
By Theorem 3.9 the probability that our (61, 610, 1, 2) X-code
fails to detect errors when there are exactly two Xs and two
errors is 1.5 Ã— 10âˆ’6 . Therefore, our X-code is ideal when the
fan-out problem is critical and/or fault-free simulation rarely
produces three or more Xs in an expected response.
Very large optimal X-codes with very high error detecting
ability and compaction ratio can be easily constructed by the
same method. For example, Theorem 3.7 and known results
on anti-Pasch STSs immediately give a (601, 60100, 1, 2) Xcode with fan-out 3 and compaction ratio 100. This code is
also a (601, 60100, 3, 1) X-code and a (601, 60100, 5, 0) Xcode. Moreover, the probability that it fails to detect errors
when there are exactly two Xs and two errors (or exactly
three Xs and a single error) is around 1.6 Ã— 10âˆ’11 (or 7.3âˆ’7
respectively). As far as the authors know, there have been no
X-codes available that guarantee as high error detection ability
and have very small fan-out.
As Theorems 3.7, 3.8, and 3.9 indicate, larger X-codes
designed with this method have an even higher compaction
ratio and better error detection rate. Because discarding codewords does not affect error detection ability, one may use
part of a large X-code to achieve very high test quality when
compaction ratio can be compromised to an extent.
IV. X-C ODES

OF

A RBITRARY W EIGHT

The restriction to low-weight codewords severely limits the
compaction ratio of an X-code. Hence, when fan-in and fanout are not of concern, it is desirable to use X-codes with
arbitrary weight. In this section we study the compaction ratio
and construction methods of such general X-codes.
For d = x = 2, a (âŒˆlog2 nâŒ‰(âŒˆlog2 nâŒ‰ + 1), n, 2, 2) X-code
was constructed for any integer n â‰¥ 2 [14].
Theorem 4.1: [14] For any optimal (m, n, 2, 2) X-code,
m â‰¤ âŒˆlog2 nâŒ‰(âŒˆlog2 nâŒ‰ + 1).
They also gave an explicit construction method of a
(3âŒˆlog3 nâŒ‰, n, 1, 3) X-code. In order to give a more general
construction, we employ design theoretic techniques for arrays. Let n â‰¥ w â‰¥ 2. A perfect hash family, PHF(N ; u, n, w),
is a set F of N functions f : Y â†’ X where |Y | = u and
|X| = n, such that, for any C âŠ† Y with |C| = w, there exists
at least one function f âˆˆ F such that f |C is one-to-one. A
PHF(N ; u, n, w) can be described by a u Ã— N matrix with
entries from a set of n symbols such that for any w rows there
exists at least one column in which each element is distinct.
Theorem 4.2: If an (m, n, d, x) X-code and a
PHF(N ; u, n, max{d, x} + 1) exist, there exists an
(mN, u, d, x) X-code.
Proof: Let H be a u Ã— N n-ary matrix representing
a PHF(N ; u, n, max{d, x} + 1). Assign each codeword of
an (m, n, d, x) X-code to a distinct symbol of the PHF and
replace each entry of H by the m-dimensional row vector
representing the assigned codeword. Then we obtain a uÃ—mN

binary matrix H â€² . Taking each row of H â€² as a codeword,
we obtain a set X of mN -dimensional vectors. It suffices to
show that for any two arbitrary subsets D, X âŠ† X satisfying
|D| = dâ€² â‰¤ d, |X| = xâ€² â‰¤ x, and D âˆ© X = âˆ…, it holds that
_
M
_
( X) âˆ¨ (
D) 6=
X.
(3)

By considering a one-to-one function in the PHF, for any
max{d, x} + 1 codewords of X at least one set of m
coordinates forms max{d, x} + 1 distinct codewords of the
original (m, n, d, x) X-code. Hence, for any choice of D
and X there exists a subset Y âŠ† X of cardinality |Y | =
max{0, dâ€² + xâ€² âˆ’ (max{d, x} + 1)} such that at least one set
of m coordinates in D âˆª (X \ Y ) forms distinct codewords of
the original (m, n, d, x) X-code. Because |Y | â‰¤ dâ€² âˆ’ 1 < |D|,
(3) holds for any D and X. Hence, the resulting set X forms
an (mN, u, d, x) X-code.
Since their introduction in [53], much progress has been
made on existence and construction techniques for perfect hash
families (see [54]â€“[58] for recent results). A concise list of
known results on perfect hash families is available in [50].
We can use perfect hash families from algebraic curves over
finite fields:
Theorem 4.3: [59] For positive integers n â‰¥ w, there
exists an explicit construction for an infinite family of
PHF(N ; u, n, w) such that N is O(log u).
Indeed when n is fixed, a perfect hash family with O(log u)
rows can be determined in polynomial time by a greedy
method [60].
By combining Theorems 4.2 and 4.3, we can construct
infinitely many (m, n, d, x) X-codes where m is O(log n).
Theorem 4.4: For any positive integer d and nonnegative
integer x, there exists an explicit construction for an infinite
family of (m, n, d, x) X-codes, where m is O(log n).
The following is a combinatorial recursion for X-codes.
Theorem
  4.5: If an (m, n, d, x) X-code and an
(â„“, n, d2 , x) X-code exist, there exists an (â„“ + m, 2n, d, x)
X-code.
Proof: Let X = {s1 , . . . , sn }be an (m, n, d, x) X-code
and Y = {t1 , . . . , tn } an (â„“, n, d2 , x) X-code. Extend each
(i)
(i)
codeword si = (s1 , . . . , sm ) of X by appending â„“ 0â€™s so that
(i)
(i)
extended vectors have the form sâ€²i = (s1 , . . . , sm , 0, . . . , 0).
(i)
(i)
Extend each codeword ti = (t1 , . . . , tl ) of Y by combining si so that extended vectors have the form tâ€²i =
(i)
(i) (i)
(i)
(s1 , . . . , sm , t1 , . . . , tl ). Define A = {sâ€²1 , . . . , sâ€²n }, B =
{tâ€²1 , . . . , tâ€²n }, and C = A âˆª B. We prove that C is an
(â„“ + m, 2n, d, x) X-code.
Take two subsets D, X âŠ† C satisfying |D| = dâ€² â‰¤ d,
|X| = xâ€² â‰¤ x, and Dâˆ©X = âˆ…. As in the proof of Theorem 4.2,
it suffices to show that for any choice of D and X the vector
obtained by adding all the codewords in D is not covered by
the superimposed sum of X, that is, (3) holds. Define a sur(i)
(i)
(i)
(i)
jection f of C to X as f : (c1 , . . . , câ„“+m ) 7â†’ (c1 , . . . , cm ).
Mapping all codewords of C under f generates two copies
of X ; one is from A and the other is from B. Define a
(i)
(i)
surjection g of C to Y âˆª {0} as g : (c1 , . . . , câ„“+m ) 7â†’
(i)
(cm + 1(i) , . . . , câ„“+m ). By definition, {g(c) : c âˆˆ B} = Y
and for any c âˆˆ A the image g(c) is an â„“-dimensional zero

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

vector. Let
 a = |D âˆ© A| and b =
 |D âˆ© B|. Because Y is an
(â„“, n, d2 , x) X-code, if b â‰¤ d2 ,
_
M
_
g( X) âˆ¨ g(
D) 6= g( X).
(4)
d
Hence, we only need to consider the case when b > 2 .
Suppose to the contrary that (3) does not hold. Then,
_
M
_
f ( X) âˆ¨ f (
D) = f ( X).
(5)

Let

aâ€² = |{c âˆˆ X : f (c) = f (d), d âˆˆ D âˆ© B}|
and
aâ€²â€² = |{c âˆˆ D âˆ© A : f (c) = f (d), d âˆˆ D âˆ© B}|.
Because {f (c) : c âˆˆ A} = {f (c) : c âˆˆ B} = X and (5)
holds, b = aâ€² + aâ€²â€² . As a + b = dâ€² and b > d2 ,
b

â€²
â‰¤ a
 +a
d
â‰¤
+ aâ€² .
2

(6)

On the
hand, |X âˆ© B| â‰¤ x âˆ’ aâ€² . Because Y is also an
 dother

â€²
(â„“, n, 2 + a , x âˆ’ aâ€² ) X-code, (4) holds, a contradiction.
Next, we present a simple nonconstructive existence result
for (m, n, d, x) X-codes.
Theorem 4.6: Let d, x be a positive integers. For n â‰¥
max{2d, d + x}, if
m â‰¥ 2x+1 (d + x) log n,
there exists an (m, n, d, x) X-code.
Proof: Let X = {s1 , s2 , . . . , sn } be a set of n m(i) (i)
(i)
dimensional vectors si = (s1 , s2 , . . . , sm , ) in which each
(i)
entry sj is defined to be 1 with probability p = 1/2. Let X
be a set of x vectors of X and Di a set of i vectors in X \ X.
Define
A(Di , X) =

(

W
L
W
0 if ( X) âˆ¨ ( Di ) 6= X,
1 otherwise,

and let E(A(Di , X)) be its expected value. Then

E(A(Di , X)) =
=
Let
AX =

ï£«

ï£¬
âˆ’x
ï£­1 âˆ’ 2

j

odd

(1 âˆ’ 2âˆ’xâˆ’1 )m .

d
X X
X

XâŠ†X
|X|=x

ï£¶m
X  i ï£·
2âˆ’i ï£¸
j
1â‰¤jâ‰¤i

i=1

A(Di , X)

Di
Di âˆªX=âˆ…

=

d
X X
X

XâŠ†X
|X|=x

i=1

If E(AX ) < 1, there exists an (m, n, d, x) X-code. Taking
logarithms,
m>

âˆ’(d + x) log n
.
log (1 âˆ’ 2âˆ’xâˆ’1 )

Hence, if
m â‰¥ 2x+1 (d + x) log n >

E(A(Di , X))

Di
Di âˆªX=âˆ…


d  
X
n nâˆ’x
(1 âˆ’ 2âˆ’xâˆ’1 )m
=
x
i
i=1

< nd+x (1 âˆ’ 2âˆ’xâˆ’1 )m .

âˆ’(d + x) log n
,
log (1 âˆ’ 2âˆ’xâˆ’1 )

there exists an (m, n, d, x) X-code.
Hence, for any optimal (m, n, d, x) X-code with n â‰¥
max{2d, d + x}, m is at most O(log n). For example, by
putting d = x = 2 we know that there exists an (m, n, d, x)
X-code if m â‰¥ 32 log n. This significantly improves the upper
bound in Theorem 4.1 proved in [14].
V. C ONCLUSIONS
By formulating X-tolerant space compaction of test responses combinatorially, an equivalent, alternative definition
of X-codes has been introduced. This combinatorial approach
gives general design methods for X-codes and bounds on
the compaction ratio. Using this model with restricted fanout leads to well-studied objects, the Steiner 2-designs. These
provide constructions for X-codes having sufficient error
detectability, X-tolerance, maximum compaction ratio, and
minimum fan-out. Constant weight X-codes with high error
detectability profit from a deep connection with configurations, particularly the Pasch configuration. The combinatorial
formulation of X-tolerant compaction can also be applied in
conjunction with another compaction technique (such as time
compaction). If a tester wants an X-compactor with additional
properties, the necessary structure of the compactor may be
expressed in design theoretic terms.
Our formulation can also be useful for the study of higher
error detectability and error diagnosis support employing the
appropriate assistance from an Automatic Test Equipment
(ATE) [12]. For example, the compaction technique called iCompact can be understood in terms of the model in Section
II [17].
The essential idea underlying Theorem 4.6 is the stochastic
coding technique for X-tolerant signature analysis [61]. We
used a naive value 1/2 as the probability p in the proof of
Theorem 4.6. To obtain a better constant coefficient, p should
be chosen so that it minimizes the expected value E(AX ), that
is, it should minimize
ï£¶m
ï£«




d
X i
X nâˆ’x ï£¬
ï£·
pj (1 âˆ’ p)iâˆ’j+x ï£¸ .
ï£­1 âˆ’
i
j
1â‰¤jâ‰¤i
i=1
j

and E(AX ) its expected value. Then

E(AX )

9

odd

While this optimization does not affect the logarithmic order
in Theorem 4.6, it may help a tester determine the target compaction ratio and estimate the error cancellation and masking
rate of an X-tolerant Multiple Input Signature Register (XMISR) based on stochastic coding [61].
In this paper we focused on space compaction. Nevertheless,
time compaction is of great importance as well. We expect the
combinatorial formulation developed here to provide a useful
framework for exploring time compaction as well.

10

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

ACKNOWLEDGMENT
A substantial part of the research was done while the
first author was visiting the Department of Computer Science
and Engineering of Arizona State University. He thanks the
department for its hospitality. The authors thank an anonymous
referee and the editor for helpful comments and valuable
suggestions.
R EFERENCES
[1] E. J. McCluskey, D. Burek, B. Koenemann, S. Mitra, J. H. Patel,
J. Rajski, and J. A. Waicukauski, â€œTest compression roundtable,â€ IEEE
Des. Test. Comput., vol. 20, pp. 76â€“87, Mar./Apr. 2003.
[2] A. Lempel and M. Cohn, â€œDesign of universal test sequences for VLSI,â€
IEEE Trans. Inf. Theory, vol. 31, pp. 10â€“17, Jan. 1985.
[3] G. Seroussi and N. H. Bshouty, â€œVector sets for exhaustive testing of
logic circuits,â€ IEEE Trans. Inf. Theory, vol. 34, pp. 513â€“522, May 1988.
[4] H. Hollmann, â€œDesign of test sequences for VLSI self-testing using
LFSR,â€ IEEE Trans. Inf. Theory, vol. 36, pp. 386â€“392, Mar. 1990.
[5] G. D. Cohen and G. Zemor, â€œIntersecting codes and independent
families,â€ IEEE Trans. Inf. Theory, vol. 40, pp. 1872â€“1881, Nov. 1994.
[6] N. Benowitz, D. F. Calhoun, G. E. Alderson, J. E. Bauer, and C. T.
Joeckel, â€œAn advanced fault isolation system for digital logic,â€ IEEE
Trans. Comput., vol. C-24, pp. 489â€“497, May 1975.
[7] E. J. McCluskey, Logic Design Principles with Emphasis on Testable
Semi-Custom Circuits. Englewood Cliffs, NJ: Prentice-Hall, 1986.
[8] N. R. Saxena and E. J. McCluskey, â€œParallel signature analysis design
with bounds on aliasing,â€ IEEE Trans. Comput., vol. 46, pp. 425â€“438,
Apr. 1997.
[9] C. Barnhart, V. Brunkhorst, F. Distler, O. Farnsworth, B. Keller, and
B. Koenemann, â€œOPMISR: The foundation for compressed ATPG vectors,â€ in Proc. Int. Test Conf., 2001, pp. 748â€“757.
[10] C. Barnhart, V. Brunkhorst, F. Distler, O. Farnsworth, A. Ferko,
B. Keller, D. Scott, B. Koenemann, and T. Onodera, â€œExtending OPMISR beyond 10x scan test efficiency,â€ IEEE Design Test Comput.,
vol. 19, pp. 65â€“73, Sep. 2002.
[11] S. Mitra, S. S. Lumetta, M. Mitzenmacher, and N. Patil, â€œX-tolerant test
response compaction,â€ IEEE Des. Test. Comput., vol. 22, pp. 566â€“574,
Nov. 2005.
[12] S. Mitra and K. S. Kim, â€œX-compact: An efficient response compaction
technique,â€ IEEE Trans. Comput.-Aided Design Integr. Circuits Syst.,
vol. 23, pp. 421â€“432, Mar. 2004.
[13] S. Mitra, S. Kallepalli, and K. S. Kim, â€œAnalysis of X-compact for
industrial designs,â€ Intel Corp., 2003.
[14] S. S. Lumetta and S. Mitra, â€œX-codes: Theory and applications of
unknowable inputs,â€ Center for Reliable and High-Performance Computing, Univ. of Illinois at Urbana Champaign, Tech. Rep. CRHC-03-08
(also UILU-ENG-03-2217), Aug. 2003.
[15] â€”â€”, â€œX-codes: Error control with unknowable inputs,â€ in Proc. IEEE
Intl. Symp. Information Theory, Yokohama, Japan, June 2003, p. 102.
[16] P. Wohl and L. Huisman, â€œAnalysis and design of optimal combinational
compactors,â€ in Proc. 21st IEEE VLSI Test Symp., April/May 2003, pp.
101â€“106.
[17] J. H. Patel, S. S. Lumetta, and S. M. Reddy, â€œApplication of SalujaKarpovsky compactors to test responses with many unknowns,â€ in Proc.
21st IEEE VLSI Test Symp., 2003, pp. 107â€“112.
[18] T. R. N. Rao and E. Fujiwara, Error-Control Coding for Computer
Systems. Englewood Cliffs, NJ: Prentice-Hall, 1989.
[19] K. K. Saluja and M. Karpovsky, â€œTesting computer hardware through
data compression in space and time,â€ in Proc. Int. Test Conf., 1983, pp.
83â€“93.
[20] W. H. Kautz and R. R. Singleton, â€œNonrandom binary superimposed
codes,â€ IEEE Trans. Inf. Theory, vol. 10, pp. 363â€“377, Jul. 1964.
[21] E. Sperner, â€œEin satz uÌˆber Untermengen einer endlichen Menge,â€ Math.
Z., vol. 27, pp. 544â€“548, 1928.
[22] D. R. Stinson and R. Wei, â€œSome new upper bounds for cover-free
families,â€ J. Combin. Theory Ser. A, vol. 90, pp. 224â€“234, 2000.
[23] A. G. Dâ€™yachkov and V. V. Rykov, â€œBounds on the length of disjunctive
codes,â€ Probl. Contr. Inform. Theory, vol. 11, pp. 7â€“33, 1982, in Russian.
[24] Z. FuÌˆredi, â€œOn r-cover-free families,â€ J. Combin. Theory, Ser. A, vol. 73,
pp. 172â€“173, 1996.
[25] M. RuszinkoÌ, â€œOn the upper bound of the size of the r-cover-free
families,â€ J. Combin. Theory, Ser. A, vol. 66, pp. 302â€“310, 1994.

[26] A. G. Dâ€™yachkov, V. V. Rykov, and A. M. Rashad, â€œSuperimposed
distance codes,â€ Probl. Contr. Inform. Theory, vol. 18, pp. 237â€“250,
1989.
[27] D. Z. Du and F. K. Hwang, Combinatorial Group Testing and Its
Applications, 2nd ed. Singapore: World Scientific, 2000.
[28] H. L. Fu and F. K. Hwang, â€œA novel use of t-packings to construct
d-disjunct matrices,â€ Discrete Appl. Math., vol. 154, pp. 1759â€“1762,
2006.
[29] A. G. Dâ€™yachkov, A. J. Macula, and V. V. Rykov, â€œNew constructions
of superimposed codes,â€ IEEE Trans. Inf. Theory, vol. 46, pp. 284â€“290,
Jan. 2000.
[30] A. J. Macula, â€œA simple construction of d-disjunct matrices with certain
constant weights,â€ Discrete Math., vol. 162, pp. 311â€“312, 1996.
[31] â€”â€”, â€œError-correcting nonadaptive group testing with de -disjunct matrices,â€ Discrete Appl. Math., vol. 80, pp. 217â€“222, 1997.
[32] H. G. Yeh, â€œd-Disjunct matrices: bounds and LovaÌsz Local Lemma,â€
Discrete Math., vol. 253, pp. 97â€“107, 2002.
[33] A. De Bonis and U. Vaccaro, â€œConstructions of generalized superimposed codes with applications to group testing and conflict resolution in
multiple access channels,â€ Theor. Comput. Sci., vol. 306, pp. 223â€“243,
2003.
[34] C. J. Colbourn and A. Rosa, Triple Systems. Oxford: Oxford Univ.
Press, 1999.
[35] P. ErdoÌ‹s, P. Frankl, and Z. FuÌˆredi, â€œFamilies of finite sets in which no
set is covered by the union of two others,â€ J. Combin. Theory, Ser. A,
vol. 33, pp. 158â€“166, 1982.
[36] C. J. Colbourn and Y. Fujiwara, â€œSmall stopping sets in Steiner triple
systems,â€ Cryptography and Communications, vol. 1, no. 1, pp. 31â€“46,
2009.
[37] M. J. Grannell, T. S. Griggs, and C. A. Whitehead, â€œThe resolution of
the anti-Pasch conjecture,â€ J. Combin. Des., vol. 8, pp. 300â€“309, 2000.
[38] A. C. H. Ling, C. J. Colbourn, M. J. Grannell, and T. S. Griggs,
â€œConstruction techniques for anti-Pasch Steiner triple systems,â€ J. Lond.
Math. Soc. (2), vol. 61, pp. 641â€“657, 2000.
[39] D. R. Stinson and Y. J. Wei, â€œSome results on quadrilaterals in Steiner
triple systems,â€ Discrete Math., vol. 105, pp. 207â€“219, 1992.
[40] M. J. Grannell, T. S. Griggs, and J. S. Phelan, â€œA new look at an old
construction for Steiner triple systems,â€ Ars Combinat., vol. 25A, pp.
55â€“60, 1988.
[41] A. E. Brouwer, â€œSteiner triple systems without forbidden subconfigurations,â€ Mathematisch Centrum Amsterdam, ZW 104/77, 1977.
[42] C. J. Colbourn, E. Mendelsohn, A. Rosa, and J. SÌŒiraÌnÌŒ, â€œAnti-Mitre
Steiner triple systems,â€ Graphs Combin., vol. 10, pp. 215â€“224, 1994.
[43] M. J. Grannell, T. S. Griggs, and E. Mendelsohn, â€œA small basis for fourline configurations in Steiner triple systems,â€ J. Combin. Des., vol. 3,
pp. 51â€“59, 1995.
[44] C. J. Colbourn, â€œThe configuration polytope of â„“-line configurations in
Steiner triple systems,â€ Mathematica Slovaca, vol. 59, no. 1, pp. 77â€“108,
2009.
[45] P. Wohl, J. A. Waicukauski, and T. W. Williams, â€œDesign of compactors
for signature-analyzers in built-in-self-test,â€ in Proc. Int. Test Conf.,
2001, pp. 54â€“63.
[46] P. ErdoÌ‹s, P. Frankl, and Z. FuÌˆredi, â€œFamilies of finite sets in which no
set is covered by the union of r others,â€ Israel J. Math., vol. 51, pp.
75â€“89, 1985.
[47] H. Hanani, â€œThe existence and construction of balanced imcomplete
block designs,â€ Ann. Math. Statist., vol. 32, pp. 361â€“386, 1961.
[48] â€”â€”, â€œOn balanced incomplete block designs with blocks having five
elements,â€ J. Combin. Theory Ser. A, vol. 12, pp. 184â€“201, 1972.
[49] R. M. Wilson, â€œAn existence theory for pairwise balanced designs. III.
Proof of the existence conjectures,â€ J. Combin. Theory Ser. A, vol. 18,
pp. 71â€“79, 1975.
[50] C. J. Colbourn and J. H. Dinitz, Eds., Handbook of Combinatorial
Designs. Boca Raton, FL: Chapman & Hall/CRC, 2007.
[51] Y. M. Chee, C. J. Colbourn, and A. C. H. Ling, â€œAsymptotically optimal
erasure-resilient codes for large disk arrays,â€ Discrete Appl. Math., vol.
102, pp. 3â€“36, 2000.
[52] M. MuÌˆller and M. Jimbo, â€œErasure-resilient codes from affine spaces,â€
Discrete Appl. Math., vol. 143, pp. 292â€“297, 2004.
[53] K. Mehlhorn, Data Structures and Algorithms 1. Berlin, Germany:
Springer, 1984.
[54] D. Tonien and R. Safavi-Naini, â€œRecursive constructions of secure codes
and hash families using difference function families,â€ J. Combin. Theory
Ser. A, vol. 113, pp. 664â€“674, 2006.
[55] Tran van Trung and S. S. Martirosyan, â€œNew constructions for IPP
codes,â€ Des. Codes Cryptgr., vol. 32, pp. 227â€“239, 2005.

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

[56] D. Deng, D. R. Stinson, and R. Wei, â€œThe LovaÌsz local lemma and its
applications to some combinatorial arrays,â€ Des. Codes Cryptgr., vol. 32,
pp. 121â€“134, 2004.
[57] R. A. Walker II and C. J. Colbourn, â€œPerfect hash families: Construction
and existence,â€ Journal of Mathematical Cryptology, vol. 1, pp. 125â€“
150, 2007.
[58] S. S. Martirosyan and Tran van Trung, â€œExplicit constructions for perfect
hash families,â€ Des. Codes Cryptogr., vol. 46, no. 1, pp. 97â€“112, 2008.
[59] H. Wang and C. Xing, â€œExplicit constructions of perfect hash families
from algebraic curves over finite fields,â€ J. Combin. Theory Ser. A,
vol. 93, pp. 112â€“124, 2001.
[60] C. J. Colbourn, â€œConstructing perfect hash families using a greedy algorithm,â€ in Coding and Cryptology, Y. Li, S. Zhang, S. Ling, H. Wang,
C. Xing, and H. Niederreiter, Eds. Singapore: World Scientific, 2008.
[61] S. Mitra, S. S. Lumetta, and M. Mitzenmacher, â€œX-tolerant signature
analysis,â€ in Proc. Int. Test Conf., 2004, pp. 432â€“441.

11

Perspectives

The BioIntelligence Framework: a new computational
platform for biomedical knowledge computing
Toni Farley,1 Jeff Kiefer,1 Preston Lee,1 Daniel Von Hoff,1 Jeffrey M Trent,1
Charles Colbourn,2 Spyro Mousses1
< Additional material are

published online only. To view
these files please visit the
journal online (http://dx.doi.org/
10.1136/amiajnl-2011-000646).
1

The Translational Genomics
Research Institute (TGen),
Center for BioIntelligence,
Phoenix, Arizona, USA
2
School of Computing,
Informatics, Decision Systems
Engineering, Arizona State
University, Tempe, Arizona, USA
Correspondence to
Dr Spyro Mousses, The
Translational Genomics
Research Institute (TGen),
Center for BioIntelligence, 445
N. Fifth Street, Phoenix, AZ
85004, USA;
smousses@tgen.org
Received 19 October 2011
Accepted 7 July 2012
Published Online First
2 August 2012

ABSTRACT
Breakthroughs in molecular profiling technologies are
enabling a new data-intensive approach to biomedical
research, with the potential to revolutionize how we
study, manage, and treat complex diseases. The next
great challenge for clinical applications of these
innovations will be to create scalable computational
solutions for intelligently linking complex biomedical
patient data to clinically actionable knowledge.
Traditional database management systems (DBMS) are
not well suited to representing complex syntactic and
semantic relationships in unstructured biomedical
information, introducing barriers to realizing such
solutions. We propose a scalable computational
framework for addressing this need, which leverages
a hypergraph-based data model and query language that
may be better suited for representing complex multilateral, multi-scalar, and multi-dimensional relationships.
We also discuss how this framework can be used
to create rapid learning knowledge base systems to
intelligently capture and relate complex patient data to
biomedical knowledge in order to automate the recovery
of clinically actionable information.

genomic interpretation require: (a) a fundamentally
different computational framework for storing and
representing disparate data types with complex
relationships, and (b) advanced software applications that leverage this framework to structure the
representation of prior knowledge so that it can be
intelligently linked to patient data. We propose
a framework conceptually based on requirements
and cognitive strategies for knowledge computing,
previously introduced as the BioIntelligence
Framework.2
Our framework is compatible with future directions toward computational intelligence. Since it
can support the capturing and querying of multilateral and multi-scalar relations among genomes,
phenotype, environment, lifestyle, medical history,
and clinical outcome data, our platform can
support systems with higher order functions such
as inference and learning. This will ultimately
allow genomic data to be intelligently repurposed
beyond personalized medicine to support more
sophisticated translational research and highly
iterative knowledge discovery.

BIOINTELLIGENCE FRAMEWORK
INTRODUCTION
Next generation genomic proï¬ling technologies are
generating deep and detailed characterizations of
patients and disease states. This data-intensive
approach is providing unprecedented insights that
can be used to resolve mechanistic complexity and
clinical heterogeneity, thereby revolutionizing how
we study, manage, and treat complex diseases. To
support this revolution, bioinformatics tools are
rapidly emerging to process and analyze large-scale
complex molecular data sets for discovery research
applications. Unfortunately, when it comes to
clinical (nÂ¼1) applications of genomics, the data
deluge is rapidly outpacing our capacity to interpret
rich data sets to extract medically useful and
meaningful knowledge. The next great challenge
will be to address the manual interpretation
bottleneck through the development of computational solutions for intelligently linking complex
patient data to actionable biomedical knowledge.
This illuminates a need to represent and query
large-scale complex relationships distributed across
disparate types of biomedical knowledge. A recent
report states a goal for the community is to transition from traditional database management to
managing potentially unstructured data across
many repositories.1
We propose the key challenges for intelligently
linking prior knowledge to partially automate
128

Systems biology is concerned with emergent
properties in complex interactions of systems of
systems involving disparate data elements.
Extracting useful information requires syntactic
and semantic linking of data within and across
large data sets. Systems modeled as networks based
on binary graphs (where edges connect node pairs)
are suited to capturing bilateral relationships and
interactions. To represent multilateral relationships
requires a fundamental change in how we model
systems. We generalize the binary graph model to
a hypergraph model, an approach which has been
previously suggested,3 and introduce a hypergraphbased solution for representing multilateral relations and multi-scalar networks.
Biological systems may beneï¬t from a ï¬‚exible
data model that supports nesting of data elements
and concept abstraction in a more natural manner
than functionally equivalent relational counterparts, and the ability to readily query across
multiple systems and abstraction layers representing complex relationships, leading to systems
compatible with learning, reasoning, and inferencing. Following a model for human intelligence,
information lives in different levels of the
neocortex: from highly variable data inputs, to
patterns, to patterns of patterns, to invariant
concepts.4 Inspired by this model of intelligence,
we extend the notion of a hypergraph to allow

J Am Med Inform Assoc 2013;20:128â€“133. doi:10.1136/amiajnl-2011-000646

Perspectives
links among edges to capture relationships that cross bounds of
scale and dimension, and develop a novel generic framework for
capturing information that can beneï¬t systems biology and
other areas.
We desire a solution that is ï¬‚exible to include various types of
data from disparate sources, extensible to scale to massive stores
of information, and accessible to permit the efï¬cient extraction
of patient-centric knowledge. Figure 1 outlines the architecture of
our BioIntelligence Framework, the components of which are:
1. A public hypergraph-based network for representing knowledge, including
a. A scalable hypergraph-like model for representing a knowledge network
b. Processes to automate populating and updating the
network with public domain knowledge from multiple sources
c. An efï¬cient database solution for storing the network
platform
2. A Patient Data Locker application built on top of the
knowledge network, including:

d. An accessible web-based solution for storing patient-centric
knowledge
e. Processes for structuring and formatting patient genomic
and health data, inducing patient-centric subgraphs on the
public hypergraph, and stratifying patients based on information in their lockers
3. A process for structuring and formatting analyst interpretation to facilitate feedback and rapid automated learning in
the system.

Public hypergraph
A graph is deï¬ned G(V,E) where V is a set of vertices (nodes) and
E is a set of edges (links) between two vertices. A hypergraph is
a generalization of a graph in which an edge can connect any
number of vertices. Biological networks have traditionally been
modeled as graphs/networks. These graph models capture
bilateral relationships among node pairs. Using hypergraphs as
a modeling paradigm supports the characterization of multilateral relationships and processes.3 For example, in a general graph,

Figure 1 A BioIntelligence Framework for creating a hypergraph-like store of public knowledge and using this, along with an individualâ€™s genomic and
other patient information, to derive a personalized genome-based knowledge store for clinical translation and discovery research.
J Am Med Inform Assoc 2013;20:128â€“133. doi:10.1136/amiajnl-2011-000646

129

Perspectives
an edge might represent a relationship between a gene and
disease state. That relationship may change in the context of
a drug, and a hypergraph can represent this contextual knowledge with an edge containing all three elements.
Hypergraphs are proving useful for capturing semantic and
biomedical information in semantic web technologies for biological knowledge management and semantic knowledge
networks.5 6 Approaches to extracting knowledge from biological research literature to store in a hypergraph have been
proposed,7 8 with similar techniques used for population stratiï¬cation.9
To support a data intelligent system, we wish for information
to be stored not only explicitly in the data itself, but implicitly
in how the data are linked, and capture this in our model.
Abstraction is permitted by allowing hyperedges to contain
other edges, forming a nested graph structure. A machine
learning model called hierarchical temporal memory (HTM)
mimics the human neocortex.4 10 Inspired by this design, we
store knowledge at different levels of granularity, from single
points of data, to collections of points, abstracting out to
collections of collections. Thus, we perceive edges in lower levels
of abstraction as nodes in higher levels, thereby permitting the
network to be viewed and operated on within and across
different scales of abstraction. Information is stored on the
nodes and edges of the network and accessed via processes.
Populating the network platform requires processes to pull
different types of information from multiple sources, such as the
Cancer Biomedical Informatics Grid (caBIG) and BioWarehouse,11 12 and other tools and techniques.13e15 The processes
are represented as S0. Sm in ï¬gure 1, and take unstructured,
public domain knowledge as input and structures it as input to
the public network.
The most common database management system (DBMS) is
based on the relational data model, which is best suited to
capturing data structured in a predeï¬ned schema, and presents
limitations in handling complexity and scalability.16 Our solution handles unstructured and semi-structured data, and is in
the scope of non-relational (NoSQL) databases, which do not
require pre-deï¬ned schemas.16 17 Such databases include those
based on graph models and triplestores (eg, RDF), and are best
suited to capturing binary relationships between two elements
(a triplestore is even more restrictive as it is essentially
a â€˜directedâ€™ binary graph). To effectively represent multilateral
relationships and interactions present in biomedical data,
hypergraph-based approaches have been suggested.3 18e21 The
ï¬‚exibility granted by allowing elements to contain elements
allows processing knowledge at different levels of abstraction, in
a less restrictive way than hierarchical models that restrict the
networkâ€™s topology. HyperGraphDB resembles our model, but
uses a directed hypergraph and is built on a traditional database.22 Our model is more general, based on less restrictive
undirected edges, and intended to be implemented natively,
although a DBMS using our model may use other DB solutions
as a persistent data store. In fact, other solutions can be built on
our model as it is a generalization of other models.

Data model
An example use of our model is shown in ï¬gure 2. This example
captures multilateral, multiscalar, and multidimensional relationships using a general model (A), and viewing the network at
different levels of abstraction (B and C). The elements shown in
this solution are described in the legend of the image, and based
on a real-world problem we are exploring. A possible schema
model for a relational database capturing these same data can be
130

found in the online supplementary material, and demonstrates
the added effort involved in capturing these complex relationships in a SQL database.
The entities in ï¬gure 2 are represented as elements in our
database. An attribute is a key/value pair, and a list of attributes
(Attribute Set) is stored with each element. In this way, we can
arbitrarily add any type of attribute to an element without
changing the database structure. In the relational model, each
entity type requires its own table, with pre-deï¬ned ï¬elds for
attributes, and adding an attribute requires adding a ï¬eld to the
table, and migrating the database. A characteristic of the relational model is that all entities of the same type are stored in the
same table. Our model is ï¬‚exible in that it does not require prestructuring data, but we can certainly mimic this behavior if
desired by enforcing a rule that requires all elements to have an
attribute with keyÂ¼â€˜type.â€™ Using our model, this decision is left
to the database designer, and not enforced by the model itself.
An element in our model can contain an arbitrary number of
other elements (allowing â€˜has-aâ€™ and â€˜has-manyâ€™ relationships).
These elements are referenced in the â€˜Internal Element Setâ€™ of
the element. For example, in ï¬gure 2, element g is a gene and
contains three gene variant elements (v1, v2, v3) in its internal
element set. This is an example of a multi-lateral relationship as
g can be viewed as a â€˜hyperedgeâ€™ in a hypergraph, connecting
three â€˜nodes.â€™ While this behavior is easy to model in a relational
database, using a one-to-many relation, it becomes more
complex when an element contains an arbitrary number of
arbitrary types of elements. For example, the element R2
represents a molecular state, which in this case is a protein p
associated with a protein state ps1. In a relational database, we
can capture these relationships in a â€˜molecular stateâ€™ table with
foreign key ï¬elds pointing to a â€˜proteinâ€™ table and a â€˜protein
stateâ€™ table. Now consider a molecular state that exists in the
context of a modiï¬er drug (ie, the state of the protein is
perturbed by a modiï¬er drug in a laboratory experiment). To
capture this in our model, we can simply create a new element
with three internal elements: p, ps1 and the element representing the modiï¬er drug. To capture this in the relational
database, we would need to either add a ï¬eld to the â€˜molecular
stateâ€™ table, which may be blank for many records, or create
a new table with the three related ï¬elds. Both of the later
options require a change to the underlying data structure, and
migrating the database.
The elements shown in ï¬gure 2C represent higher-level
concepts and recursive nesting of elements at different levels,
illustrating the ability to ï¬‚exibly and efï¬ciently capture multiscalar relationships among elements, the motivation behind our
data model. For example, note that R1 captures the relationship
that gene g codes for protein p. C1 captures the genetic event
concept, where gene variant v2 changes the state of the protein
(the molecular state represented by R3). C2 is a drug response
concept representing the higher-level concept that C1, in the
context of the pharmaceutical ph, leads to a changed molecular
state, R2. These combined biological and pharmacologic effects
lead to a change in disease state to ds2, captured by the clinical
response concept C3. The internal (nested) element sets contain
the topology of the network, that is, they deï¬ne how entities are
related, and capture meaning in those relationships. Further
details to describe the nature of the relationships can always be
stored in attributes of the elements.
The â€˜External Element Setâ€™ of an element is the inverse of
internal element relations. For example, v1, v2, and v3 all have g
in their external element set. While it is not necessary to store
this information (the network of relationships among elements
J Am Med Inform Assoc 2013;20:128â€“133. doi:10.1136/amiajnl-2011-000646

Perspectives
Figure 2 An illustrative example of storing
biomedical information in our proposed
knowledge base: a component of the
BioIntelligence Framework. A shows our data
model, and describes its components. B and C
show the elements described in the legend (at
the bottom of the figure) at two different levels
of abstraction.

can be constructed via the internal element sets alone), it does
aid in querying the database. We have deï¬ned three new types of
queries associated with our model: recover, context, and expand.
The expand and context queries retrieve all internal and external
elements of an element, respectively, optionally limited by
modiï¬ers presented with the query. Recover is a combination of
these, and returns both internal and external elements. All three
query actions have an optional level constraint, deï¬ning how
deep to traverse the graph when retrieving related elements. For
instance, expand n will retrieve all internal elements, and their
internal elements, recursively up to n times. For instance, we can
view C3 as an abstraction; a clinical effect that we can relate to
patients and other concepts. Expanding C3 by one level shows
us that it represents a disease state ds2, triggered by a pharmacologic effect C2. Expanding C3 by two levels shows us the
J Am Med Inform Assoc 2013;20:128â€“133. doi:10.1136/amiajnl-2011-000646

details of the pharmacologic effect, and so on. In this way, we
can choose which level of abstraction we wish to view and
compute over, and we can create new concepts that cross these
layers of abstraction (multi-scalar relationships).
By the deï¬nition of the internal and external element sets, it
follows that our model naturally handles many-to-many relationships as well. In summary, we do not argue that a relational
database is incapable of capturing the types of relationships we
discuss here, rather that it requires more work, and added layers
of complexity to the underlying structure of the database, which
makes capturing and querying complex biomedical relationships
more difï¬cult. Our model is an abstraction of other models,
including relational, graph, hierarchical, and object-oriented, and
can therefore be used to model data represented using any and
all of these models at once. The potential beneï¬t of our model is
131

Perspectives
Figure 3 The network on the left is an
example knowledge network platform. The
darkened nodes represent gene variants
present in an individual genome. The network
on the top right is a genome-induced subgraph
of the network. The network on the bottom
right is a genome-induced subgraph, expanded
out to include additional knowledge stored on
edges in the connected-component each data
element is contained in.

that it provides levels of scalability and ï¬‚exibility that are
difï¬cult to achieve with existing models. We are currently
developing a solution based on this model and will present
additional details of the model and related query language in
future publications.

Patient data locker
Given a patientâ€™s data (genomic, health, etc), we wish to recover
related knowledge from our network using BioIntelligence Tools
(BIT). The ï¬rst step (BIT1 in ï¬gure 1) is a process to structure
data as input to a process for inducing patient-relevant
subgraphs of the knowledge network (BIT2 in ï¬gure 1). BIT1
integrates many types of data sets across multiple databases to
support electronic medical and health records (EMR/EHRs), and
is designed as a modular based system to provide metadata and
indexing for queries.
The next step (BIT2 in ï¬gure 1) is a process to extract relevant
knowledge from the network based on individual patient
information. An induced subgraph H(S,T) of network G(V,E)
has the properties S3V, and for every vertex set Si of H, the set
Si is an edge of H if and only if it is an edge in G. That is, H has
the same edges that appear in G over the same set of nodes. We
say that H is an induced subgraph of G, and H is induced by S. In
our system, V is the set of nodes in the platform network G, and
S is the set of nodes that map to an individualâ€™s genomic and
health information. Thus, a subgraph is induced by an individual
genome. The architecture in ï¬gure 1 shows an example public
hypergraph, and private subgraphs stored in a data locker. These
networks are detailed in ï¬gure 3, where the network on the left
is a public knowledge network, and the darkened nodes are
elements relevant to an individualâ€™s information (based on input
patient data). The network on the top right is induced by this
information, and contains all of the darkened nodes, and edges
incident to them. Alternately, we can expand the information
retrieved to include all connected components of the induced
subgraph. An example of a connected-component induced
subgraph is shown on the bottom right of ï¬gure 3.
The most important characteristic of the data locker is that it
contains all relevant knowledge to facilitate clinical translation.
Induced subgraphs can be used to transform a large set of
patient-relevant data to smaller, task-tailored formats void of
extraneous detail. The patient data locker is linked to the public
132

knowledge store, and automatically updated to contain only
a subset of information related to the patient. Thus, an expert
need not develop their own intricate search queries and perform
the tedious task of progressively reducing the amount of irrelevant data returned by the query. Any query that can be run on
the entire knowledge network, can be run on the subgraph in
a patientâ€™s locker, leading to a more precise subset of knowledge
returned, and potentially faster querying speeds as the search
space is reduced.
The expert analyst is provided with knowledge tailored to
a particular patient, partially automating the interpretation
process, and a process (BIT3 in ï¬gure 1) allows the analyst to
input new interpretation knowledge into the public network. We
envision this type of feedback mechanism will support the
inclusion of a learning model for our system, and allow the
community to contribute to its growth. The system is diverse,
providing framework and template libraries, allowing users to
integrate their own tools for analysis, data collection, and beyond.

CONCLUSION
A deluge of biomedical data generated from next-generation
sequencing (NGS) and clinical applications is overwhelming our
ability to efï¬ciently extract value from it. Existing bioinformatics tools were not developed to support clinical translation
for an individual patient, causing an nÂ¼1 translation bottleneck.
A new architecture for managing biomedical data is desired, and
we present the BioIntelligence Framework as a genomecompatible biomedical knowledge representation platform. Our
future efforts to achieve the goals outlined in this paper include
ensuring that we develop algorithms on this framework that
minimally meet the performance expectations of existing solutions in practice.
Contributors All authors contributed to the ideas behind the framework. TF, CC, and
PL contributed computer science expertise. JK, DVH, JMT, and SM contributed
expertise in clinical genomics and translational research.
Competing interests None.
Provenance and peer review Commissioned; externally peer reviewed.

Open Access This is an Open Access article distributed in accordance with the
Creative Commons Attribution Non Commercial (CC BY-NC 3.0) license, which permits
others to distribute, remix, adapt, build upon this work non-commercially, and license
their derivative works on different terms, provided the original work is properly cited
and the use is non-commercial. See: http://creativecommons.org/licenses/by-nc/3.0/
J Am Med Inform Assoc 2013;20:128â€“133. doi:10.1136/amiajnl-2011-000646

Perspectives
REFERENCES
1.
2.
3.
4.
5.
6.
7.
8.
9.
10.
11.

Agrawal R, Ailamaki A, Bernstein PA, et al. The Claremont report on database
research. ACM SIGMOD Record 2008;37:9e19.
Mousses S, Kiefer J, Von Hoff D, et al. Using biointelligence to search the cancer
genome: an epistemological perspective on knowledge recovery strategies to enable
precision medical genomics. Oncogene 2008;27:S58e66.
Klamt S, Haus U, Theis F. Hypergraphs and cellular networks. PLoS Comput Biol
2009;5:e1000385.
Hawkins J, Blakeslee S. On Intelligence. New York: Times Books, 2004.
Antezana E, Kuiper M, Mironov V. Biological knowledge management: the emerging
role of the semantic web technologies. Brief Bioinform 2009;10:392e407.
Zhen L, Jiang Z. Hy-SN: hyper-graph based semantic network. Knowledge-Based
Systems 2010;23:809e16.
Vailaya A, Bluvas P, Kincaid R, et al. An architecture for biological information
extraction and representation. Bioinformatics 2005;21:430e8.
Mukhopadhyay S, Palakal M, Maddu K. Multi-way association extraction and
visualization from biological text documents using hyper-graphs: applications to
genetic association studies for diseases. Artif Intell Med 2010;49:145e54.
Vazquez A. Population stratification using a statistical model on hypergraphs. Phys
Rev E Stat Nonlin Soft Matter Phys 2008;77:1e7.
George D. How the Brain Might Work: a Hierarchical and Temporal Model for
Learning and Recognition [dissertation]. Palo Alto, California: Stanford University,
2008.
NCI. Cancer Biomedical Informatics Grid (caBIG). https://cabig.nci.nih.gov/ (accessed
16 Sep 2011).

J Am Med Inform Assoc 2013;20:128â€“133. doi:10.1136/amiajnl-2011-000646

12.
13.
14.
15.
16.
17.
18.
19.
20.
21.
22.

Karp P. Biowarehouse database integration for bioinformatics. http://biowarehouse
ai.sri.com/ (accessed 16 Sept 2011).
Chen H, Ding L, Wu Z, et al. Semantic web for integrated network analysis in
biomedicine. Components 2009;10:177e92.
Tudor CO, Schmidt CJ, Vijay-Shanker K. eGIFT: mining gene information from the
literature. BMC Bioinformatics 2010;11:418.
Valentin F, Squizzato S, Goujon M, et al. Fast and efficient searching of biological
data resourceseusing EB-eye. Brief Bioinform 2010;11:375e84.
Leavitt N. Will NoSQL databases live up to their promise? Computer
2010;43:12e14.
Angles R, Gutierrez C. Survey of graph database models. ACM Computing Surveys
2008;40:1e39.
Olken F. Graph data management for molecular biology. OMICS 2003;7:75e8.
Hu Z, Mellor J, Wu J, et al. Towards zoomable multidimensional maps of the cell.
Nat Biotechnol 2007;25:547e55.
Spreckelsen C, Spitzer K. Formalising and acquiring model-based hypertext in
medicine: an integrative approach. Methods Inform Med 1998;37:239e46.
Wu G, Li J, Hu J, et al. System: a native RDF repository based on the
hypergraph representation for RDF data model. J Comput Sci Technol
2009;24:652e64.
Iordanov B. HyperGraphDB: a generalized graph database. Proceedings of the 2010
International Conference on Web-age Information Management 2010:25e36.

PAGE fraction trail=5.5

133

Complex engineered systems arise throughout computing, communications, and networking. Many factors, each having a finite number of levels, impact the behaviour of the system either singly or in interaction with one another. Testing or evaluating such a system involves formulating a set of tests, when executed, responses or outcomes from the tests are analyzed. A single round of testing is conducted. To witness the effect of an interaction, some test must cover it, this does not suffice in general to locate the interaction or to measure its effect. When there are few factors or many tests, experimental designs can measure (and hence locate) the interactions. When there are many factors and few tests, can we locate the interaction(s)? Can we efficiently detect them?Combinatorial arrays, locating and detecting arrays, are introduced to address such location and detection in the context of combinatorial testing. Locating and detecting arrays are contrasted with covering arrays and with experimental designs. An application to a 75 factor protocol stack for file transfer is given to demonstrate their practical use. Finally, their place in the literature of combinatorial testing is discussed and some directions are outlined.Locating Arrays: A New Experimental Design for
Screening Complex Engineered Systems
Abraham N. Aldaco, Charles J. Colbourn, and Violet R. Syrotiuk
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, Tempe, AZ, U.S.A. 85287-8809

{aaldacog, colbourn, syrotiuk}@asu.edu
ABSTRACT

Screening: Which factors and interactions are most inï¬‚uential on
a response?

The purpose of a screening experiment is to identify signiï¬cant factors and interactions on a response for a system. Engineered systems are complex in part due to their size. To apply traditional
experimental designs for screening in complex engineered systems
requires either restricting the factors considered, which automatically restricts the interactions to those in the set, or restricting interest to main effects, which fails to consider any possible interactions. To address this problem we propose a locating array (LA) as
a screening design. Locating arrays exhibit logarithmic growth in
the number of factors because their focus is on identiï¬cation rather
than on measurement. This makes practical the consideration of an
order of magnitude more factors in experimentation than traditional
screening designs. We present preliminary results applying an LA
for screening the response of TCP throughput in a simulation model
of a mobile wireless network. The full-factorial design for this system is infeasible (over 1043 design points!) yet an LA has only 421
design points. We validate the signiï¬cance of the identiï¬ed factors
and interactions independently using the statistical software JMP.
Screening using locating arrays is viable and yields useful models.

Conï¬rmation: Is the system currently performing in the same way
as it did in the past?
Discovery: What happens when new operating conditions, materials, factors, etc., are explored?
Robustness: Under what conditions does a response degrade?
Stability: How can variability in a response be reduced?
Our focus is on screening using techniques from statistical design
of experiments (DoE). DoE refers to the process of planning an
experiment so that appropriate data are collected and analyzed by
statistical methods, in order to result in valid and objective conclusions. Hence any experimental problem includes both the design of
the experiment and the statistical analysis of the data.
Suppose that there are k factors, F1 , . . . , Fk , and that each factor
Fj has a set Lj = {vj,1 , . . . , vj,j }, of j possible levels (or values). A design point is an assignment of a level from Lj to Fj ,
for each factor j = 1, . . . , k. An experimental design is a collection of design points. When a design has N design points, it
can be represented by an N Ã— k array A = (ai,j ) in which each
row i corresponds to a design point and each column j to a factor;
the entry ai,j gives the level assigned to factor j in the ith design
point. When run, a design point results in one or more observable
responses.

Categories and Subject Descriptors
General and reference [Cross-computing tools and techniques]:
Experimentation; Mathematics of computing [Discrete mathematics]: Combinatorics

General Terms
Experimentation

A t-way interaction (or interaction of strength t) in A is a choice
of t columns i1 , . . . , it , and the selection of a level Î½ij âˆˆ Lij for
1 â‰¤ j â‰¤ t, represented as T = {(ij , Î½ij ) : 1 â‰¤ j â‰¤ t}. Every
 
design point in A covers kt interactions of strength t.

Keywords
Screening experiments, Locating arrays

1.

INTRODUCTION

Computer and networked systems are examples of complex engineered systems (CESs). The complexity of an engineered system is
not just due to its size, but also arises from its structure, operation
(including control and management), evolution over time, and that
people are involved in its design and operation [35].

When the objective of experimentation is screening, it is often recommended to keep the number of factors low. It has been considered impractical to experiment with â€œmanyâ€ factors; about ten
factors is a suggested maximum [23, 31]. Generally, two levels for
each factor is considered to work well in screening experiments.

Experimentation is often used to study the performance of CESs.
At its most basic, a system may be viewed as transforming some
input variables, or factors, into one or more observable output variables, or responses. Some factors of a system are controllable,
whereas others are not.

Methods for screening seek to reduce the number of design points
required because the exhaustive full-factorial design [9, 31] is too
large. For k factors each with two levels it has 2k design points.
An analysis of variance (ANOVA) allows the signiï¬cant factors
and interactions on the response to be identiï¬ed.

Objectives of experimentation include:

A fractional factorial design 2kâˆ’p
is a 21p fraction of a full factoR
rial design with k two-level factors. The design is described by p

Copyright is held by the authors.

31

tential to transform experimentation in huge factor spaces such as
those found in CESs.

generators, expressions of factors that are confounded; the generators determine the alias structure. A design is of resolution R if no
m-factor effect is aliased with another effect containing fewer than
R âˆ’ m factors.

The rest of this paper is organized as follows. Â§2 deï¬nes a locating
array, and gives an example of how a design is used for location.
Â§3 presents preliminary results applying an LA for screening the response of TCP throughput in a simulation model of a mobile wireless network. The full-factorial design for this system is infeasible
â€” it has over 1043 design points! Yet there is an LA with only 421
design points. We develop an algorithm using the LA to identify
the signiï¬cant factors and interactions from the data collected, providing a small example. In Â§4 we validate the signiï¬cance of the
identiï¬ed factors and interactions independently using the statistical software JMP. Finally, in Â§5 we summarize, discuss potential
threats to our approach, directions for this research, and conclude.

A D-optimal design is a popular experimental design among those
using optimality criteria. A model to ï¬t, and a bound N on the
number of design points, must be speciï¬ed a priori; this restricts
the factors to be analyzed to those in the model. The size of a Doptimal design is bounded by the size of a full-factorial design.
Some designs aggregate the factors into groups, e.g., sequential bifurcation [24], to improve design efï¬ciency. Grouping requires care
to ensure that factor effects do not cancel. This presents a â€œchicken
and eggâ€ problem: we need to know how to group in order to group.
Often, a domain expert is expected to make such grouping decisions. While such experts may have considerable knowledge, it
is doubtful whether an expert knows the importance of a speciï¬c
factor or interaction in a CES.

2.

An interaction graph depicts how a change in the level of one factor
affects the other factor with respect to a response. Figure 1 shows
an interaction graph for the factors of routing and medium access
control (MAC) protocol on average delay in a network. The choice
of MAC protocol (EDCF or IEEE 802.11) has little impact on the
average delay in the AODV routing protocol, while for the DSR
routing protocol the impact is very large; see [53]. If MAC protocols were aggregated, this signiï¬cant interaction would be lost.

LAs differ from standard designed experiments, which are used to
measure interactions and to develop a model for the response as a
function of these [31]. â€œSearch designsâ€ [17, 48, 49] also attempt
to locate interactions of higher strength, but their focus remains on
measurement and hence on balanced designs. Rao [20] shows that
the number of design points in a balanced design must be at least
as large as the number of interactions considered. Thus if t-way
interactions among k factors each having v levels are to be examined, balanced designs only reduce the v k exhaustive design points
to O(kt ). The selection of few factors from hundreds of candidates by this reduction is not viable. By lessening the requirement
from measurement to identiï¬cation, LAs are not subject to the Rao
bound.

Log10(Average delay)

-0.2
-0.3
-0.4
-0.5
-0.6
-0.7
-0.8
-0.9

LOCATING ARRAYS

Reducing the number of design points required relies on a sparsity
of effects assumption, that interactions of interest involve at most
a small, known number t of interacting factors. As one means of
reduction, we deï¬ne locating arrays (LAs) [8]. For a set of factors
each taking on a number of levels, an LA permits the identiï¬cation
of a small number of signiï¬cant interactions among small sets of
(factor, level) combinations.

EDCF
IEEE 802.11

Fortunately LAs behave more like covering arrays, experimental
designs in which every t-way interaction among factors appears in
at least one design point. Unlike designed experiments, the number
of design points in a covering array for k factors grows as a logarithmic function of k (see [43], for example). In [8], a construction
of LAs using covering arrays of higher strength is given, and hence
LAs also exhibit this logarithmic growth, making them asymptotically much more efï¬cient than balanced designs. This motivates
the consideration of covering arrays, which have been the subject
of extensive study [4, 5, 19, 36]. They are used in testing software
[10,13,25,26], hardware [46,50], composite materials [3], biological networks [44, 47], and others. Their use to facilitate location of
interactions is examined in [29, 56], and measurement in [21, 22].
Covering arrays form the basis for combinatorial methods to learn
an unknown classiï¬cation function using few evaluations â€” these
arise in computational learning and classiï¬cation, and hinge on locating the relevant attributes (factors) [11]. Algorithms for generating covering arrays range from greedy (e.g., [2, 16]) through
heuristic search (e.g., [38, 52]). However, combinatorial constructions (see [5]) provide the only available deterministic means of
producing covering arrays with more than a few hundred factors.

-1
AODV
DSR
Routing protocol

Figure 1: Interaction of routing and MAC protocols on delay [53].
A fractional factorial design is saturated when it investigates k =
N âˆ’ 1 factors in N design points [31]. In a supersaturated design,
the number of factors k > N âˆ’ 1; such designs contain more factors than design points. These designs are only able to estimate a
main effects model [27, 31]. Thus they cannot consider possible
interactions at all.
Even with substantial and detailed domain knowledge, it is imperative not to eliminate or aggregate factors a priori. Our goal,
therefore, is an automatic and objective approach to screening. To
address this problem we have formulated the deï¬nition of a locating array (LA) [8]. Locating arrays exhibit logarithmic growth in
the number of factors because their focus is on identiï¬cation rather
than on measurement. This makes practical the consideration of an
order of magnitude more factors in experimentation, removing the
need for the elimination of factors. As a result, LAs have the po-

A design point, when run, yields one or more responses. For ease
of exposition, we classify the responses in two groups, those that

32

exceed a speciï¬ed threshold and those that do not. So we suppose
that the outcome of a run of a design point is a single binary response (â€œpassâ€ or â€œfailâ€). A fault is caused by one or more t-way
interactions, and is evidenced by a run failing.

uous, we can select a threshold on the responses so as to limit the
number of design points yielding a â€œfailâ€ outcome to locate those
that make the most substantial contribution to the response. We
exploit this fact later in Â§3.2.

Given an experimental design and the set of interactions that cause
faults, the outcomes can be easily calculated: A run fails exactly
when it contains one or more of the faulty interactions, and does
not fail otherwise. In order to observe a fault, the interaction must
be covered by at least one design point. With no restriction on the
interactions that can cause faults, every interaction
 must be covered. Then the best one can do is to form all kj=1 j possible
design points, the exhaustive design. Using sparsity of effects, an
upper bound t is placed on the strength of interactions that may be
faulty. Then we require that every t-way interaction be covered; in
other words, the design is a covering array of strength t.

2.1

A Small Example

An example is provided to demonstrate fault location, and show
the limitations of covering arrays for this purpose. Suppose that
we use the experimental design for ï¬ve binaryfactors
in Table 1.

It is a covering array in which each of the 22 52 = 40 two-way
interactions is covered. A response for each design point run is
listed in the adjacent column.
Table 1: Experimental design and response for each run.

Let A = (ai,j ) be an experimental design, an N Ã— k array where in
each row i, levels in the jth column are chosen from a set Lj of size
j . For array A and t-way interaction T = {(ij , Î½ij ) : 1 â‰¤ j â‰¤ t},
deï¬ne Ï(A, T ) = {r : ar,ij = Î½ij , 1 â‰¤ j â‰¤ t} as the set of
rows of A in which T is covered. For a set T of interactions,
Ï(A, T ) = âˆªT âˆˆT Ï(A, T ). Locating faults requires that T be
recovered from Ï(A, T ), whenever T is a possible set of faults.

Design Points

1
2
3
4
5
6

1
0
1
0
1
0
1

2
1
0
1
0
0
1

Factors
3 4
1 1
1 0
0 0
0 1
0 0
0 1

5
1
0
0
1
1
0

Response
Fail
Pass
Fail
Pass
Pass
Pass

First, let us locate faults due to main effects (i.e., the individual factors or one-way interactions). The second design point run passes,
so all (factor, level) pairs in it are known not to be faulty. Therefore
in Table 2(a), that considers only the second design point, when
factor 1 is set to one, the run is not faulty. Similarly, for factors
2, 3, 4, and 5 set to zero, one, zero, and zero, respectively. This
is indicated by a check-mark () in the table. Repeating to check
coverage of each one-way interaction for each successful run, no
single (factor, level) error accounts for the faults; see Table 2(b).

Let It be the set of all t-way interactions for an array, and let It
be the set of all interactions of strength at most t. Consider an
interaction T âˆˆ It of strength less than t. Any interaction T 
of strength t that contains T necessarily has Ï(A, T  ) âŠ† Ï(A, T ).
In this case, when T is faulty we are unable to determine whether
or not T  is also faulty. Call a subset T  of interactions in It
independent if there do not exist T, T  âˆˆ T  with T âŠ† T  . In
general, some interactions in It (or perhaps It ) are believed to
be faulty, but their number and identity are unknown. The faulty
interactions cannot be identiï¬ed precisely from the outcomes, even
if the full factorial design is employed, without some restriction
on their number. (Consider the situation in which every design
point run fails.) We therefore suppose that a maximum number d
of faulty interactions is speciï¬ed.

Table 2: Locating faults due to main effects.
(a) Run 2
Factors 0
1
1

2

3

4

5


D EFINITION 2.1 ( [8]). An array A is (d, t)-locating if whenever T1 , T2 âŠ† It and T1 âˆª T2 is independent, |T1 | â‰¤ d, and
|T2 | â‰¤ d, it holds that Ï(A, T1 ) = Ï(A, T2 ) â‡” T1 = T2 .

(b) All Runs
Factors 0
1
1
 
2
 
3
 
4
 
5
 

Computing Ï(T ) for every one-way interaction, we obtain the sets
in Table 3. Because no two sets are equal, the array is (1, 1)locating and when there is a single faulty one-way interaction it can
be located. However, because {1, 3, 5} âˆª {2, 3, 5} = {1, 3, 5} âˆª
{1, 2}, when rows 1, 3, and 5 fail and 2, 4, and 6 pass, we cannot determine the two faulty interactions â€” the array is not (2, 1)locating.

If there is any set of d interactions of strength t that produce exactly the outcomes obtained when using a (d, t)-locating array A
to conduct experiments, then there is exactly one such set of interactions. To avoid enumeration of all sets of d interactions of
strength t, one can employ a stronger condition that for every interaction T of strength at most T and every set T1 âŠ† It that does
not contain T and for which T1 âˆª {T } is independent, it holds that
Ï(A, T ) = Ï(A, T1 ) â‡” T âˆˆ T1 . A locating array meeting this
stronger condition is termed a detecting array in [8]. When using
a detecting array, if there are at most d independent faulty interactions each of strength at most t, they are characterized precisely
as the interactions that appear in no run that passes. We typically
employ the term locating array to refer to both, but for reasons of
computational efï¬ciency the locating arrays that we use are, in fact,
detecting arrays.

Table 3: Ï(T ) for one-way interactions T = {(c, Î½)}.
Î½â†“câ†’
0
1

1
{1,3,5}
{2,4,6}

2
{2,4,5}
{1,3,6}

3
{3,4,5,6}
{1,2}

4
{2,3,5}
{1,4,6}

5
{2,3,6}
{1,4,6}

Now, let us try to locate faults due to two-way interactions. Because the second design point run passes, all two-way interactions
in it are known not to be faulty; Table 4(a) records the results. Repeating to check for coverage of each two-way interaction for each
successful run, those interactions not found to pass in this way in

In practice, one does not know a priori how many interactions are
faulty, or their strengths. Nevertheless, when responses are contin-

33

Table 4: Locating faults due to two-way interactions.

Factors
1, 2
1, 3
1, 4
1, 5
2, 3
2, 4
2, 5
3, 4
3, 5
4, 5

(a) Run 2
00 01 10














11


Factors
1, 2
1, 3
1, 4
1, 5
2, 3
2, 4
2, 5
3, 4
3, 5
4, 5

(b) All Runs
00 01 10






 
  
 
  
  
  
  

of levels for each factor to the desired number, eliminating rows in
the process and forming an array C with 143 design points. The
resulting array provides coverage of two-way interactions but does
not support location. When T and T  are interactions, to distinguish them we require that Ï(T ) = Ï(T  ), but we ask for more,
namely that |Ï(T ) \ Ï(T  )| â‰¥ 2 and |Ï(T  ) \ Ï(T )| â‰¥ 2; this ensures that for every two interactions of interest, there are at least
two design points containing one but not the other. To accomplish
this, we formed three copies of C, randomly permuted their symbols within each column, and formed their union (so that every
two-way interaction is covered at least three times). The resulting
array B with 429 rows turned out to be (1, 2)-detecting. Three rows
were selected by a greedy method to ensure the stronger condition
that |Ï(T ) \ Ï(T  )| â‰¥ 2 for every pair T, T  of interactions; then
eleven rows were deleted by a greedy algorithm to remove redundant rows, ultimately producing a design with 421 rows. Appendix
A gives a pointer to the locating array used as the experimental
design. Our objective was not to ï¬nd the smallest possible array,
because a fair evaluation of the efï¬cacy of locating arrays should
not rely on substantial additional structure being present.

11








Table 4(b) form a set of candidate faults. In this example, there are
nine interactions in the set of candidate faults. Now for the twoway interaction {(1, 0), (2, 1)}, Ï({(1, 0), (2, 1)}) = {1, 3}, and
it is the only two-way interaction for which this holds; and, no oneway interaction T has Ï(T ) = {1, 3}. Hence if there is a single
fault, it must be {(1, 0), (2, 1)}, and we have located the fault.

Ten replicates of each design point in the LA are run in ns-2;
for each a response of TCP throughput is measured. These are
averaged for each design point resulting in a vector with 421 entries
of observed average TCP throughput obsT h.

Our success for one response is not sufï¬cient, however. Because
Ï({(1, 0), (2, 1)}) = {1} = Ï({(2, 1), (3, 1)}), if only run 1 fails,
there are at least two equally plausible explanations using only a
single two-way interaction. Indeed A is not (1, 2)-locating. Thus
the ability to locate is more than simply coverage!

3.

3.2

SCREENING AN ENGINEERED SYSTEM

We now apply locating arrays for screening in a complex engineered system. One example of a CES for which it has been particularly difï¬cult to develop models is a mobile ad hoc network
(MANET). A MANET is a collection of mobile wireless nodes that
self-organize without the use of any ï¬xed infrastructure or centralized control. We seek to use a locating array to screen for the inï¬‚uential factors and interactions on average transport control protocol
(TCP) throughput in a simulation model of a MANET.

3.1

Screening Algorithm

We describe an algorithm for screening at a high level to facilitate
understanding. In each iteration of the algorithm the most signiï¬cant main effect or two-way interaction is identiï¬ed. These terms
are accumulated in a screening model of average TCP throughput. However, this screening model is not intended as a predictive
model; the quality of its current estimate allows the algorithm to
select the next most signiï¬cant term. The screening model is used
only to identify inï¬‚uential main effects and two-way interactions.
With its output, a predictive model can be built; see Â§4.
Initially, the screening model has no terms. With no other information, it should estimate the average TCP throughput to be the
average of the vector of observed average throughput. This is unlikely to be a very good estimation!

Designing the Experiment

We use the ns-2 simulator [37], version 2.34, for our experimentation. Since our response of interest is average TCP throughput,
we select the ï¬le transfer protocol (FTP) as our application because
it uses TCP for reliability. We select the internet protocol (IP), the
Ad hoc On-demand Distance Vector routing protocol (AODV) [42],
and IEEE 802.11b direct sequence spread spectrum (DSSS) as protocols at the network, data link, and physical layers of the protocol
stack. We also use the mobility, energy, error, and propagation
models in ns-2. From these protocols and models we identify
75 controllable factors. The region of interest for each factor, i.e.,
the range over which the factor is varied, ranges from two to ten
levels, with some set according to recommendations in [33]. See
Appendix A for a pointer to details of the factors and their levels.

Our strategy to identify the most signiï¬cant factor or interaction
as the term to add to the screening model is as follows. Suppose
that factor Fj , 1 â‰¤ k â‰¤ 75, has j levels Lj = {vj,1 , . . . vj,j }.
For each level , 1 â‰¤  â‰¤ j , of factor Fj iterate through each
of the 421 design points of the locating array A. For each design
point i, 1 â‰¤ i â‰¤ 421, partition the contribution of the (factor Fj ,
level vj, ) combination into one of two sets: S or S. If the design
point has the factor Fj set to level , i.e., ai,j = vj, , then add
the throughput measured for design point i, obsT h[i], to S; otherwise add obsT h[i] to S. Then, compute the (absolute) difference
of the average of sets S and S. (Of course, metrics other than the
difference of averages could be used.) Either the difference is zero
(i.e., the average TCP throughput collected in the sets S and S is
the same), or it is non-zero. If the difference is non-zero, then one
possible explanation is that the (factor Fj , level vj, ) combination
is responsible for the difference.

The full-factorial design for this factor space is infeasible; it has
over 1043 design points! In contrast, the locating array constructed
and checked manually has only 421 design points. Except for small
locating arrays [51], no general construction methods have been
published. We adopted a heuristic approach to construct the LA.

Our hypothesis is that the (factor Fj , level vj, ) combination over
all combinations for which the difference between the sets is the
greatest is the most signiï¬cant one. If this is correct, then a term
of the form c Â· (Fj , vj, ) is added to the screening model. The

Initially we selected a covering array with 75 factors and 10 levels
per factor, constructed using a standard product construction [7].
We applied a post-optimization method [34] to reduce the number

34

coefï¬cient c is equal to the difference in average TCP throughput
of each set. When this term is added to the screening model, it
makes the same estimation for average TCP throughput for sets S
and S.

design point with this initial ï¬tted value.
Now, we iterate over each (factor,level) combination. Factor
1 is set

to its low level in design points 1â€“4. Therefore S = 14 41 resT h[i] =

âˆ’134347
= âˆ’33586 and S = 14 85 resT h[i] = 134344
= 33586.
4
4
The absolute difference, |S âˆ’ S| = |-33586 âˆ’ 33586| = 67172.

In the ï¬rst iteration of this algorithm, the estimate (i.e., the average
of the vector of observed average TCP throughput) is used to determine deviations from each entry in the vector obsT h. We now
have a screening model that apparently includes the most signiï¬cant factor. It is now used to produce a new estimate of average
TCP throughput and update the vector of residual throughput. The
algorithm can be applied repeatedly to the residuals to identify the
next most important factor or interaction.

Repeating for each (factor, level) combination, as well as all twoway interactions, we ï¬nd that it is a main effect that has highest
absolute difference with a value of 131255. It occurs when factor
3 is set to its lowest level, namely when the number of ï¬‚ows at
the application layer is only one. Hence we attribute this as the
explanation for the largest difference and add the term c Â· (F3 , v3,0 )
to the model. The method of ordinary least squares (OLS) is used
to ï¬t the intercept and coefï¬cient c of the new term. This results
in an updated model of T = 12410 + 131255 Â· (F3 , v3,0 ). Its
coefï¬cient of determination is R2 = 0.33.

While this algorithm is described for (factor, level) combinations,
we actually iterate over all one-way (i.e., all (factor, level) combinations) and all two-way interactions (i.e., all pairs of (factor, level)
combinations) to identify the main effect or two-way interaction of
highest signiï¬cance. Any number of stopping conditions may be
used to decide when to terminate the model development. We use
the R2 , the coefï¬cient of determination, indicating how well data
ï¬ts a line or curve; when it shows marginal improvement, we stop.

Using this updated model, the residuals can be recomputed as input
to the next iteration of the algorithm.
Next, we describe some of the obstacles arising in the practical
application of the screening algorithm.

The locating array constructed for our CES is a (d = 1, t = 2)locating array, meaning it only guarantees to be able to locate (identify) at most one (d = 1) main effect or two-way (i.e., up to t = 2way) interaction. It is interesting that the LA may be used iteratively to identify subsequent signiï¬cant main effects or interactions. In this sense, the algorithm uses a â€œheavy-hittersâ€ approach
as in compressive sensing [6].

Applying the Screening Algorithm

In applying the screening algorithm to our CES, several obstacles
arose. The ï¬rst is that the measured average TCP throughput is not
normally distributed, as Figure 2 shows; this is not uncommon in
systems experimentation [12]. The best transformation of the data
is a natural logarithm (Figure 3a). From the normal probability plot
(Figure 3b), we ï¬nd that the transformed data are still not normally
distributed; nevertheless, we work with this transformation of the
data.

Example of the Screening Algorithm

A small example is provided to step through one iteration of the
screening algorithm. Suppose that we use the experimental design for four binary factors in Table 5. It is a covering array of
strength three and therefore also a (2, 1)-detecting array. Factor 1
corresponds to the distribution function used for introducing errors
(uniformly or exponentially distributed), factor 2 to the error rate
(10âˆ’7 or 10âˆ’5 ), factor 3 to the number of ï¬‚ows at the application
layer (1 or 18), and factor 4 to the TCP packet size (64 or 2048);
the levels are taken as â€œbinaryâ€ for this example. All remaining factors are set to their default levels for experimentation. A response
of observed TCP throughput for each design point, averaged over
ten replicates, is listed in the column obsT h. (All measures are
truncated to integers for simplicity.)

72%

0.985

300

0.94
250

0.88
Normal Probability

3.3

3.4

Frequency

200

150

0.75

0.5

0.25
0.12

100

0.06
18%

0.015

50
5%
2%

0

50,000

0.003
1%

100,000
obsTH1

1%

0%

150,000

0%

0%

200,000

(a) Throughput distribution.

0%

0

50,000

100,000

150,000

200,000

250,000

250,00

obsTH1

(b) Normal probability plot.

Design Points

Table 5: Experimental design and average TCP throughput.

1
2
3
4
5
6
7
8

1
0
0
0
0
1
1
1
1

Factors
2 3
0 0
0 1
1 0
1 1
0 0
0 1
1 0
1 1

Figure 2: Distribution of the original observed average throughput,
and corresponding normal probability plot.
4
0
1
1
0
1
0
0
1

obsT h
63339
29860
80801
3804
373866
3879
56656
12095

resT h
-14699
-48178
2764
-74234
295828
-74159
-21382
-65943

A much larger problem arises from the fact that the LA does not
cover each main effect and two-way interaction the same number
of times. Indeed, binary factors are covered much more frequently
(some as many as two hundred times in the 421 row LA) compared
to two-way interactions of factors with ten levels (only a handful
of times). This is unavoidable when one-way and two-way interactions are compared, and when factors have a different numbers of
levels.

The overall mean of the obsT h is 78038. Therefore, the screening
model initially estimates this value for average TCP throughput,
i.e., T = 78038. The residuals (resT h) are computed in Table 5 by
taking the difference of the observed average throughput for each

Consider the behaviour of the screening algorithm. For a binary
factor the sets S and S have the same or nearly the same size and,
as a result, the average of each set has small variance. In the example in Â§3.3, each (factor, level) combination is covered four times

35

0.985

26%

0.985

24%

100

100

0.94

0.94
21%

0.88

0.88
80

Normal Probability

14%

18%

0.75
Frequency

60

Normal Probability

Frequency

80

0.5

60
13%
12%

0.25
40

40

0.12

8%

0.75

0.5

0.25
0.12

9%

8%
7%

0.06
5%

20

5%

20

0.015

4%

1%

1%

1%

4

6
8
ln(obsTH1)

10

(a) ln transformation.

0.015

0.003

1%

2
2

0.06

6%

6%
5%

4

6

8

10

1%

1%

0%

12

ln(obsTH1

-4
-2
0
2
Residuals of TCP throughput after iteration 1

-4

-2

0

2

4

4

(a) Distribution of ï¬rst residual.

(b) Normal probability plot.

0.003
-6

-6

12

Residuals of TCP throughput after iteration 1

(b) Normal probability plot.

Figure 3: Natural logarithm transformation of the original observed
throughput, and corresponding normal probability plot.

Figure 4: Distribution of residuals after the ï¬rst iteration of the
screening algorithm, and corresponding normal probability plot.

(each column of the array has four zeros and four ones). However
in general, as the number of levels for a factor increases, the size of
the sets S and S may become markedly different, and the variance
of the average of each set may increase greatly. Returning to the
example in Â§3.3, the two-way interactions are not covered equally.
Consider the two-way interaction {(1, 0), (2, 0)}. It is covered in
only two rows of the array, namely |Ï({(1, 0), (2, 0)})| = |{1, 2}| =
2 (this is true for all two-way interactions in this example). Even
in this small array, the coverage of two-way interactions is unbalanced resulting in S accumulating two values and S accumulating
six values. This makes any direct comparison among (factor, level)
combinations and/or two-way interactions impossible.

intercept and Î²i is the coefï¬cient of term i, 1 â‰¤ i â‰¤ 12.
Table 6: Screening model with twelve terms.

To address this problem, factors are grouped according to the number of times each level is covered in the LA; see Appendix A for
a pointer to the details on how groups are formed. Now, in each
iteration of the screening algorithm, the ï¬rst step is to select the
most signiï¬cant factor or interaction from each group. Then from
these candidates, the most signiï¬cant factor or interaction overall
is selected.
The Figure 4 shows the graphical tests for normality of the residuals after the ï¬rst iteration of the screening algorithm. (Similar behaviour of the residuals is observed after each iteration.) While the
ï¬gures indicate that the residuals are close to normally distributed,
we check using the non-parametric Shapiro-Wilk test. This test indicates that the residuals are still not normally distributed. Hence,
we use the Wilcoxon rank sum test and the Mann-Whitney U test [14, 28, 54] to select the most signiï¬cant factor or two-way
interaction within each group. Then, to select the most signiï¬cant factor or interaction over all groups, the Akaike information
criterion (AICC ) [1] is used.

Î²i
5.6
4.4
4.0
-4.7

-11.8
-12.1
-9.3
6.5

-1.6
-1.5
-1.2
0.9

6.6
8.4

0.7
1.1

6.3

1.1

5.5
5.2

0.7
0.5

Factor or interaction, and level(s)
ErrorModel_ranvar_ U nif orm
ErrorModel_unit_ pkt)
(ErrorModel_ranvar_ U nif orm) *
(ErrorModel_unit_ pkt)
TCP_packetSize_ 64
MAC_RTSThreshold_ 0
TCP_packetSize_ 128
(TCP_RTTvar_exp_ 2) *
(TCP_min_max_RTO_ 0.1)
TCP_min_max_RTO_ 0.2
(ErrorModel_unit_ pkt ) *
(ErrorModel_rate_ 1.0E-07)
(ErrorModel_ranvar_ U nif orm) *
(MAC_RTSThreshold_ 0)
APP_flows_ 1
RWP_Area_ 8

The ï¬rst notable observation about this screening model is that it
contains both main effects and two-way interactions. Moreover, it
contains factors from across the layers of the protocol stack (application, transport, and MAC) and not just the transport layer; in
addition, it includes factors from the error model and the mobility model. Aside from these differences with other models of TCP
throughput (such as [15, 18, 30, 39â€“41, 55, 57, 58]), the screening
model includes not just which factors or two-way interactions are
signiï¬cant, but the level at which each is signiï¬cant.

We still need to ï¬t the intercept and the coefï¬cients of the terms.
For a linear model with the assumptions of expected error of zero
and expected variance in the error to be equal, the method of ordinary least squares (OLS) is used. However, if the expected variance
in the error is unequal, OLS is no longer appropriate [32]. In this
case, the method of weighted least squares (WLS) is used to ï¬t the
intercept and coefï¬cients of the terms in the screening model.

3.4.1

t-Test
52.6
34.5
32.8
-29.1

From the statistical point of view, Table 7 shows a strong correlation among the regressors and the response of average TCP
throughput. The F statistic indicates that the model is signiï¬cant
to the response.
Table 7: Summary statistics of the screening model in Table 6.
R2 and Adjusted R2 : 0.84
Standard deviation: 0.92
F statistic: 180.6 on 12 and 408 df, p-value < 7.89e-155

The Resulting Screening Model

Table 6 gives the screening model for average TCP throughput developed in twelve iterations of the screening algorithm; Table 8 lists
its unique factors. A Studentâ€™s t-test was run on each term in the
screening model and each was found to be signiï¬cant; Î²0 is the

We are encouraged by the factors and interactions identiï¬ed. This
includes how and into what unit errors are introduced (using a uni-

36

Table 9: Partial results of a 29 full-factorial screening experiment
using JMP 11.0 on the nine factors in Table 8.

form distribution into packets rather than bit errors), and their interaction. Smaller sized packets (64 and 128 bytes) tend to reduce
throughput. When RTS/CTS is always on (i.e., the threshold is
zero bytes), there is a negative impact on throughput compared to
when it is conï¬gured to 1500 or 3000 bytes (always off). The retransmission timeout (RTO) and round trip time (RTT) are part of
TCPâ€™s congestion control mechanism; the RTO infers packet loss
by observing duplicate acknowledgements and the RTT is related
to the propagation delay. The RTO is signiï¬cant by itself, and in
its interaction with the RTT as they work to correct and prevent
network congestion. The synthetic error model of the simulator
drops packets comparing them with data from an uniform distribution at a steady-state loss event rate of 1.0E-07; this is the lowest
error rate used and naturally it corresponds with higher throughput. Smaller simulation areas also result in higher throughput; a
larger area has longer average shortest-hop path lengths and average higher network partition rates both of which negatively affect
throughput. The throughput response is higher with fewer ï¬‚ows
because increasing the number of ï¬‚ows not only may overload the
network but more ï¬‚ows are more challenging to route in a MANET.

4.

Term
ErrorModel_ranvar_*ErrorModel_unit_
ErrorModel_ranvar_
ErrorModel_unit_
TCP_packetSize_
APP_ï¬‚ows_
TCP_min_max_RTO_
RWP_Area_
MAC_RTSThreshold_
ErrorModel_unit_*TCP_packetSize_
ErrorModel_rate_
ErrorModel_ranvar_*MAC_RTSThreshold_
APP_ï¬‚ows_*RWP_Area_
ErrorModel_unit_*ErrorModel_rate_
TCP_packetSize_*ErrorModel_rate_
ErrorModel_unit_*MAC_RTSThreshold_
ErrorModel_ranvar_*APP_ï¬‚ows_
APP_ï¬‚ows_*TCP_min_max_RTO_
ErrorModel_unit_*APP_ï¬‚ows_
ErrorModel_ranvar_*TCP_min_max_RTO_
ErrorModel_ranvar_*TCP_packetSize_
TCP_packetSize_*APP_ï¬‚ows_
TCP_min_max_RTO_*RWP_Area_
ErrorModel_ranvar_*RWP_Area_
MAC_RTSThreshold_*ErrorModel_rate_
TCP_min_max_RTO_*ErrorModel_rate_
TCP_min_max_RTO_*TCP_rttvar_exp_
ErrorModel_unit_*TCP_min_max_RTO_
APP_ï¬‚ows_*ErrorModel_rate_
RWP_Area_*MAC_RTSThreshold_
ErrorModel_unit_*RWP_Area_
TCP_rttvar_exp_
TCP_packetSize_*RWP_Area_
APP_ï¬‚ows_*MAC_RTSThreshold_
RWP_Area_*ErrorModel_rate_
ErrorModel_ranvar_*TCP_rttvar_exp_

VALIDATION AND VERIFICATION

From the 75 controllable factors used in experimentation, nine unique
factors are present in the twelve terms in the screening model in Table 6; these are listed in Table 8.
Table 8: Unique factors in the screening model in Table 6.
Level
Factor
TCP_RTTvar_exp_
ErrorModel_ranvar_
ErrorModel_unit_
MAC_RTSThreshold_
ErrorModel_rate_
RWP_Area_
TCP_min_max_RTO_
APP_flows_
TCP_packetSize_

Minimum
2
U nif orm
pkt
0
1.0E-07
8
0.1
1
64

Maximum
4
Exponential
bit
3000
1.0E-05
40
40
18
2048

In order to validate the factors and interactions identiï¬ed, we ï¬rst
conduct a full-factorial experiment for these nine factors using the
extremes of their region of interest, using the statistical software
JMP to analyze the results. From this, we produce a predictive
model of average TCP throughput. We then examine the quality
of this predictive model by comparing how it performs on random
design points (i.e., a design point in which the level of each factor
is selected at random).

in Table 6. Indeed, both models have the same four most signiï¬cant
terms (though in a different order), and all factors and interactions
in Table 6 are a subset of the terms in Table 9. Appendix A gives
a pointer to the details of the predictive model for average TCP
throughput that was ï¬t using a subset of the signiï¬cant terms in
Table 9.
Figure 5 shows the results of evaluating the JMP predictive model
as a function of the TCP packet size, for the three levels of error
rate. As in the experimentation, all remaining factors are ï¬xed at
their default levels. As expected, the results show that the highest
TCP throughput is achieved when the error rate is at the lowest level
(1.0E-07). For a given error rate the TCP throughput increases as
a function of packet size, after which it decreases. An exception
is for packet size 1024. Aside from this exception, these results
also conï¬rm our intuition of TCP throughput behaviour. The reason for this exception deserves further study but may be related to
the default settings used for the other 66 factors not varied in this
screening experiment.

We present our validation results next.

4.1

p-Value
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
<.0001*
0.0001
0.0001
0.0003
0.0006
0.001
0.0012
0.002
0.0116
0.0444
0.0515

Full-Factorial Screening in JMP

We conduct an independent 29 full-factorial experiment on the nine
factors in Table 8. All remaining 75 âˆ’ 9 = 66 factors are ï¬xed to
their default levels. Ten replicates of each of the 29 design points is
run, and TCP throughput measured. The results of the experimentation are input to the JMP statistical software, version 11.0 [45].
The results from the full-factorial screening experiment are given in
Table 9. It includes only the main effects and two-way interactions
sorted in increasing order by the p-value. The results indicate high
commonality with the main effects and two-factor interactions selected by the screening algorithm that formed the screening model

We now examine the predictive accuracy of the JMP model for random design points.

37

120000

TCP throughput (bps)

factors and two-way interactions, the screening model developed
also reï¬‚ects the actual behaviour well.

JMP 1.0e-7
JMP 1.0e-6
JMP 1.0e-5

100000

80000

Despite this, the method aims only to deal with many factors and
their interactions to identify the signiï¬cant ones. We advocate that
further experimentation is necessary after the screening is completed, both to conï¬rm the screening results and to build a predictive model. One must be cautious not to over-ï¬t the experimental results and claim unwarranted conï¬dence; conï¬rmation is
needed. This is particularly a concern if the stopping criterion chosen locates too many or too few signiï¬cant interactions; while our
choice of R2 appears to have worked well, future effort should address the impact of different stopping criteria. A second concern is
the selection criterion for the next factor or interaction to include.
Subsequent selections depend upon selections already made, so our
method could in principle be misdirected by a bad selection. Our
criterion of using the differences between responses for S and those
for S has also worked well, but we cannot be certain that such a
simple selection sufï¬ces in general. Finally, we have employed
only a few locating arrays; while they have worked well in our
analyses, constructing a suitable locating array remains a challenging problem that merits further research.

60000

40000

20000

0
64 128

256

512

768

1024

1280

1536

1792

2048

Packet size (bytes)

Figure 5: TCP throughput as a function of packet size as predicted
the by JMP model; all other factors are at their default levels.

4.2

Predictive Accuracy of JMP Model

In order to test the predictive accuracy of the JMP model, a new
experimental design of one hundred random design points is constructed. In constructing each design point, for each of factor Fj ,
1 â‰¤ j â‰¤ 75, a random level from Lj is selected. New mobility
scenarios are also generated. Ten replicates of each of the random
design points are run in the ns-2 simulator, and the TCP throughput measured. In addition, for each experiment in the design, the
JMP model is evaluated generating a new data set of ï¬tted TCP
throughput.

Certainly further experimentation is needed to assess the merit of
screening using LAs, in particular on physical not just simulated
complex engineered systems, and draw ï¬rm conclusions. What we
can conclude is that in a challenging CES arising from a MANET,
screening using locating arrays is viable and yields useful models.

Figure 6 shows the average TCP throughput from simulation, and
the ï¬tted throughput from the JMP model corresponding to this
random design. The mean TCP throughput from the simulations is
20,892 bps whereas the mean from the JMP model is lower, only
13,946 bps. However, the standard deviation of the results from the
JMP model is smaller than the standard deviation from the simulations. Both models exhibit a few outliers. Approximately 94%
of the results predicted for TCP throughput from the JMP model
are in one standard deviation of the simulation results. Considering
the size of the factor space, we conclude that the predicted average
TCP throughput of the JMP model is similar to the average TCP
throughput measured in simulation.

4.3

Acknowledgment
Thanks to Doug Montgomery for his advice on all things statistical.
This material is based in part upon work supported by the National
Science Foundation under Grant No. 1421058.

6.

Predictive Accuracy of Screening Model

While the model developed in applying the screening algorithm
based on the LA (Table 6) is not intended to be used as a predictive
model, we were curious about its predictive accuracy. Appendix
A gives a pointer to a summary of results similar to those in this
section for the screening model. To our surprise, the predictive accuracy of the screening model is reasonably good. The screening
model does appear to have more variability than the model developed in JMP.

5.

REFERENCES

[1] H. Akaike. A new look at the statistical model identiï¬cation.
IEEE Transactions on Automatic Control, 19(6):716â€“723,
1974.
[2] R. C. Bryce and C. J. Colbourn. A density-based greedy
algorithm for higher strength covering arrays. Software
Testing, Veriï¬cation, and Reliability, 19:37â€“53, 2009.
[3] J. N. Cawse. Experimental design for combinatorial and high
throughput materials development. GE Global Research
Technical Report, 29(9):769â€“781, 2002.
[4] C. J. Colbourn. Combinatorial aspects of covering arrays. Le
Matematiche (Catania), 58:121â€“167, 2004.
[5] C. J. Colbourn. Covering arrays and hash families. In
Information Security and Related Combinatorics, NATO
Peace and Information Security, pages 99â€“136. IOS Press,
2011.
[6] C. J. Colbourn, D. Horsley, and V. R. Syrotiuk. Frameproof
codes and compressive sensing. In Proc. 48th Annual
Allerton Conference on Communication, Control, and
Computing, 2010.
[7] C. J. Colbourn, S. S. Martirosyan, G. L. Mullen, D. E.
Shasha, G. B. Sherwood, and J. L. Yucas. Products of mixed
covering arrays of strength two. Journal of Combinatorial
Designs, 14(2):124â€“138, 2006.
[8] C. J. Colbourn and D. W. McClary. Locating and detecting
arrays for interaction faults. Journal of Combinatorial
Optimization, 15:17â€“48, 2008.
[9] C. Croarkin, P. Tobias, J. J. Filliben, B. Hembree,

CONCLUSIONS

Locating arrays capture the intuition that in order to see the effect of
a main effect or interaction, some design point must cover it; and in
order to distinguish it, the responses for the set of design points that
cover it must not be equally explained by another small set of main
effects or interactions. In a complex engineered system, many main
effects and interactions may be signiï¬cant, but our method identiï¬es them one at a time, iteratively improving a screening model. In
this way, an experimental design must be able to repeatedly locate
a single â€œmost signiï¬cantâ€ main effect or interaction. Our results
show that using locating arrays for screening appears promising.
Indeed while the screening targeted the identiï¬cation of signiï¬cant

38

160000
Fitted JMP model
Mean fitted JMP model

140000

TCP throughput (bps)

120000
100000
80000
60000
40000
+StDev Fitted JMP model

20000
0

-StDev Fitted JMP model

0

10

20

30

40

50

60

70

80

90

100

60

70

80

90

100

Random tests

(a) Predictions by JMP.
160000
Simulated
Mean Simulated

140000

TCP throughput (bps)

120000
100000
80000
60000
+StDev Simulated

40000
20000
0
-StDev Simulated

0

10

20

30

40

50
Random tests

(b) Simulation results.

Figure 6: Predictions by the JMP model and simulation results for random design points.

[10]

[11]

[12]

[13]

[14]

[15]

[16]

W. Guthrie, L. Trutna, and J. Prins, editors.
NIST/SEMATECH e-Handbook of Statistical Methods.
NIST/SEMATECH, 2012.
S. R. Dalal, A. J. N. Karunanithi, J. M. L. Leaton, G. C. P.
Patton, and B. M. Horowitz. Model-based testing in practice.
In Proc. Intl. Conf. on Software Engineering (ICSE â€™99),
pages 285â€“294, 1999.
P. Damaschke. Adaptive versus nonadaptive
attribute-efï¬cient learning. Machine Learning, 41:197â€“215,
2000.
A. B. de Oliveira, S. Fischmeister, A. Diwan, M. Hauswirth,
and P. F. Sweeney. Why you should care about quantile
regression. In Proc. of the ACM Conf. on Architectural
Support for Programming Languages and Operating Systems
(ASPLOS), March 2013.
S. Dunietz, W. K. Ehrlich, B. D. Szablak, C. L. Mallows, and
A. Iannino. Applying design of experiments to software
testing. In Proc. Intl. Conf. on Software Engineering (ICSE
â€™97), pages 205â€“215, Los Alamitos, CA, 1997. IEEE.
M. Fay and M. Proschan. Wilcoxon-Mann-Whitney or t-test?
On assumptions for hypothesis tests and multiple
interpretations of decision rules. Statistics Surveys, 4:1â€“39,
2010.
S. Floyd, M. Handley, J. Padhye, and J. Widmer.
Equation-based congestion control for unicast applications:
The extended version. SIGCOMM Computing
Communications Review, 30:43â€“56, 2000.
M. Forbes, J. Lawrence, Y. Lei, R. N. Kacker, and D. R.

[17]

[18]

[19]

[20]
[21]

[22]

[23]

[24]

39

Kuhn. Reï¬ning the in-parameter-order strategy for
constructing covering arrays. J. Res. Nat. Inst. Stand. Tech.,
113:287â€“297, 2008.
S. Ghosh and C. Burns. Comparison of four new general
classes of search designs. Austral. New Zealand J. Stat.,
44:357â€“366, 2002.
K.-J. Grinnemo and A. Brunstrom. A simulation based
performance analysis of a TCP extension for best-effort
multimedia applications. In Proceedings of the 35th Annual
Simulation Symposium, 2002.
A. Hartman. Software and hardware testing using
combinatorial covering suites. In M. C. Golumbic and
I. B.-A. Hartman, editors, Interdisciplinary Applications of
Graph Theory, Combinatorics, and Algorithms, pages
237â€“266. Springer, Norwell, MA, 2005.
A. S. Hedayat, N. J. A. Sloane, and J. Stufken. Orthogonal
Arrays. Springer-Verlag, New York, 1999.
D. S. Hoskins, C. J. Colbourn, and M. Kulahci. Truncated
D-optimal designs for screening experiments. American
Journal of Mathematical and Management Sciences,
28:359â€“383, 2008.
D. S. Hoskins, C. J. Colbourn, and D. C. Montgomery.
D-optimal designs with interaction coverage. Journal of
Statistical Theory and Practice, 3:817â€“830, 2009.
J. P. C. Kleijnen. An overview of the design and analysis of
simulation experiments for sensitivity analysis. European
Journal of Operational Research, 164:287â€“300, 2005.
J. P. C. Kleijnen, B. Bettonvil, and F. Persson. Screening for

[25]

[26]

[27]

[28]

[29]

[30]

[31]
[32]

[33]

[34]

[35]

[36]
[37]
[38]

[39]

[40]

[41]

[42]

[43]

the important factors in large discrete-even simulation
models: Sequential bifurcation and its applications. In A. M.
Dean and S. M. Lewis, editors, Screening: Methods for
Experimentation in Industry, Drug Discovery and Genetics,
chapter 13, pages 287â€“307. Springer-Verlag, 2006.
D. Kuhn and M. Reilly. An investigation of the applicability
of design of experiments to software testing. In Proc. 27th
Annual NASA Goddard/IEEE Software Engineering
Workshop, pages 91â€“95, Los Alamitos, CA, 2002. IEEE.
D. R. Kuhn, D. R. Wallace, and A. M. Gallo. Software fault
interactions and implications for software testing. IEEE
Trans. Software Engineering, 30(6):418â€“421, 2004.
R. Li and D. K. J. Lin. Analysis methods for supersaturated
designs: Some comparisons. Journal of Data Science, pages
249â€“260, 2003.
H. B. Mann and D. R. Whitney. On a test of whether one of
two random variables is stochastically larger than the other.
Annals of Mathematical Statistics, 18:50â€“60, 1947.
C. MartÃ­nez, L. Moura, D. Panario, and B. Stevens. Locating
errors using ELAs, covering arrays, and adaptive testing
algorithms. SIAM J. Discrete Math., 23:1776â€“1799, 2009/10.
M. Mathis, J. Semke, J. Mahdavi, and T. Ott. The
macroscopic behavior of the TCP congestion avoidance
algorithm. SIGCOMM Comput. Commun. Rev., 27:67â€“82,
1997.
D. C. Montgomery. Design and Analysis of Experiments.
John Wiley & Sons, Inc., 8 edition, 2012.
D. C. Montgomery, E. A. Peck, and C. G. Vining.
Introduction to Linear Regression Analysis. John Wiley &
Sons, Inc., 4th edition, 2006.
A. Munjal, T. Camp, and W. Navidi. Constructing rigorous
MANET simulation scenarios with realistic mobility. In
European Wireless Conference (EW), pages 817â€“824, 2010.
P. Nayeri, C. J. Colbourn, and G. Konjevod. Randomized
postoptimization of covering arrays. European Journal of
Combinatorics, 34:91â€“103, 2013.
Networking and information technology research and
development (NITRD) large scale networking (LSN)
workshop report on complex engineered networks, 2012.
C. Nie and H. Leung. A survey of combinatorial testing.
ACM Computing Surveys, 43, 2011.
The Network Simulator - ns-2.
http://www.isi.edu/nsnam/ns.
K. Nurmela. Upper bounds for covering arrays by tabu
search. Discrete Applied Mathematics, 138(9):143â€“152,
2004.
J. Padhye, V. Firoiu, D. Towsley, and J. Kurose. Modeling
TCP throughput: a simple model and its empirical validation.
SIGCOMM Computing Communications Review,
28:303â€“314, 1998.
J. Padhye, V. Firoiu, D. F. Towsley, and J. F. Kurose.
Modeling TCP Reno performance: A simple model and its
empirical validation. IEEE/ACM Transactions on
Networking, 8:133â€“145, 2000.
N. Parvez, A. Mahanti, and C. Williamson. An analytic
throughput model for TCP NewReno. IEEE/ACM
Transactions on Networking, 18:448â€“461, 2010.
C. E. Perkins and E. M. Royer. Ad hoc on-demand distance
vector routing. In Proc. Second IEEE Workshop on Mobile
Computing Systems and Applications, pages 90â€“100, 1999.
S. Poljak, A. Pultr, and V. RÃ¶dl. On qualitatively independent

[44]

[45]
[46]

[47]

[48]

[49]

[50]

[51]

[52]

[53]

[54]
[55]

[56]

[57]

[58]

partitions and related problems. Discrete Applied Math.,
6:193â€“205, 1983.
A. H. Ronneseth and C. J. Colbourn. Merging covering
arrays and compressing multiple sequence alignments.
Discrete Applied Mathematics, 157:2177â€“2190, 2009.
JMP statistical software from SAS.
http://www.jmp.com.
G. Seroussi and N. H. Bshouty. Vector sets for exhaustive
testing of logic circuits. IEEE Transactions on Information
Theory, 34:513â€“522, 1988.
D. E. Shasha, A. Y. Kouranov, L. V. Lejay, M. F. Chou, and
G. M. Coruzzi. Using combinatorial design to study
regulation by multiple input signals: A tool for parsimony in
the post-genomics era. Plant Physiology, 127:1590â€“1594,
2001.
T. Shirakura, T. Takahashi, and J. N. Srivastava. Searching
probabilities for nonzero effects in search designs for the
noisy case. Ann. Statist., 24:2560â€“2568, 1996.
J. N. Srivastava. Designs for searching non-negligible effects.
In J. N. Srivastava, editor, A Survey of Statistical Design and
Linear Models, pages 507â€“519. Northâ€“Holland, 1975.
D. T. Tang and C. L. Chen. Iterative exhaustive pattern
generation for logic testing. IBM Journal Research and
Development, 28:212â€“219, 1984.
Y. Tang, C. J. Colbourn, and J. Yin. Optimality and
constructions of locating arrays. J. Stat. Theory Pract.,
6(1):20â€“29, 2012.
J. Torres-Jimenez and E. Rodriguez-Tello. New upper
bounds for binary covering arrays using simulated annealing.
Information Sciences, 185:137â€“152, 2012.
K. K. Vadde and V. R. Syrotiuk. Factor interaction on service
delivery in mobile ad hoc networks. IEEE Journal on
Selected Areas in Communications, 22:1335â€“1346, 2004.
F. Wilcoxon. Individual comparisons by ranking methods.
Biometrics Bulletin, 1:80â€“83, 1945.
I. Yeom and A. L. N. Reddy. Modeling TCP behavior in a
differentiated services network. IEEE/ACM Transactions on
Networking, 9:31â€“46, 1999.
C. Yilmaz, M. B. Cohen, and A. Porter. Covering arrays for
efï¬cient fault characterization in complex conï¬guration
spaces. IEEE Transactions on Software Engineering,
31:20â€“34, 2006.
B. Zhou, C. P. Fu, D.-M. Chiu, C. T. Lau, and L. H. Ngoh. A
simple throughput model for TCP Reno. In Proceedings of
the IEEE International Communications Conference
(ICCâ€™06), 2006.
M. Zorzi, A. Chockalingam, and R. R. Rao. Throughput
analysis of TCP on channels with memory. IEEE Journal on
Selected Areas in Communications, 18:1289â€“1300, 2000.

APPENDIX
A. GITHUB REPOSITORY
A GitHub repository provides supplementary material at:
https://github.com/locatingarray/screening.git
Speciï¬cally, it includes the 75 controllable factors in the ns-2 simulator used in experimentation and their levels (Â§3.1), the 421 Ã— 75
LA used as the experimental design (Â§3.1), a description of how
factors are grouped (Â§3.4), the JMP model for TCP throughput,
along with some statistical analysis (Â§4.1), and some analysis of
the predictive capability of the screening model (Â§4.3).

40

Covering subsequences by sets of permutations arises in numerous applications. Given a set of permutations that cover a specific set of subsequences, it is of interest not just to know how few permutations can be used, but also to find a set of size equal to or close to the minimum. These permutation construction problems have proved to be computationally challenging; few explicit constructions have been found for small sets of permutations of intermediate length, mostly arising from greedy algorithms. A different strategy is developed here. Starting with a set that covers the specific subsequences required, we determine local changes that can be made in the permutations without losing the required coverage. By selecting these local changes (using linear extensions) so as to make one or more permutations less â€˜importantâ€™ for coverage, the method attempts to make a permutation redundant so that it can be removed and the set size reduced. A post-optimization method to do this is developed, and preliminary results on sequence covering arrays show that it is surprisingly effective.Des. Codes Cryptogr. (2015) 77:479â€“491
DOI 10.1007/s10623-015-0084-4

Optimal low-power coding for error correction
and crosstalk avoidance in on-chip data buses
Yeow Meng Chee1 Â· Charles J. Colbourn2 Â·
Alan Chi Hung Ling3 Â· Hui Zhang1 Â· Xiande Zhang1

Received: 30 October 2014 / Revised: 12 April 2015 / Accepted: 16 April 2015 /
Published online: 8 May 2015
Â© Springer Science+Business Media New York 2015

Abstract Coupled switched capacitance causes crosstalk in ultra deep submicron/nanometer
VLSI fabrication, which leads to power dissipation, delay faults, and logical malfunctions. We
present the first memoryless transition bus-encoding technique for power minimization, errorcorrection, and elimination of crosstalk simultaneously. To accomplish this, we generalize
balanced sampling plans avoiding adjacent units, which are widely used in the statistical
design of experiments. Optimal or asymptotically optimal constant weight codes eliminating
each kind of crosstalk are constructed.
Keywords Constant weight codes Â· Packing sampling plan avoiding adjacent units Â·
Crosstalk avoidance Â· Low power code Â· Packing by triples Â· Balanced sampling plan
Mathematics Subject Classification

94B25 Â· 05B40 Â· 05B07 Â· 62K10

This is one of several papers published in Designs, Codes and Cryptography comprising the â€œSpecial Issue
on Cryptography, Codes, Designs and Finite Fields: In Memory of Scott A. Vanstoneâ€.

B

Charles J. Colbourn
charles.colbourn@asu.edu
Yeow Meng Chee
ymchee@ntu.edu.sg
Alan Chi Hung Ling
aling@emba.uvm.edu
Hui Zhang
huizhang@ntu.edu.sg
Xiande Zhang
xiandezhang@ntu.edu.sg

1

Division of Mathematical Sciences, School of Physical and Mathematical Sciences,
Nanyang Technological University, Singapore 637371, Singapore

2

School of Computing, Informatics, and Decision Systems Engineering,
Arizona State University, Tempe, AZ 85287, USA

3

Department of Computer Science, University of Vermont, Burlington, VT 05405, USA

123

480

Y. M. Chee et al.

1 Introduction
The ever-decreasing feature size of VLSI fabrication process has led to many challenges in
VLSI circuit design. One of the most important issues concerns the characteristics of onchip wires [11]. The wiresâ€™ cross-sectional areas and spacings have fallen dramatically with
the move into the ultra deep submicron/nanometer (UDSM) regime. This has increased the
resistance and capacitance of wires. To help reduce resistance, wires today are taller than
they are wide, and they are poised to grow even taller as technology continues to scale.
The resulting growth of side-to-side capacitance between long parallel wires causes coupled
switch capacitance to dominate the wire-to-substrate capacitance in UDSM circuits by several
orders of magnitude [21]. Coupled switched capacitance in turn leads to crosstalks, which
result in power dissipation, delay faults, and logical malfunctions. The problem of eliminating
or minimising crosstalks is considered the biggest signal integrity challenge for long on-chip
buses implemented in UDSM CMOS technology [12].
The worst crosstalk couplings have been classified into four types [6,12], as described in
Table 1. The coupled switched capacitance resulting from type-1, -2, -3, and -4 crosstalks is
in the ratio of 1:2:3:4. Hence, it is particularly important to avoid crosstalks of higher types.
Type-1 crosstalks cannot be avoided in any useful communication channel. However, type-1
crosstalks give rise to power dissipation and must be limited, because low power is a critical
design objective in recent years.
Another factor that has emerged as a new challenge for VLSI circuit designers is UDSM
noise, caused by high-leakage transistors, power-grid fluctuations, ground bounce, IR drops,
clock jitter, and electromagnetic radiation. The effects of such noise are difficult to predict
or prevent. For example, noise in radiation-hardened circuits for satellite communication
systems is random and does not correlate with particular switching patterns on the buses.
A further source of faults is manufacturing defects. In nanotechnology, circuits are manufactured with a significant proportion of faults, and occasional errors may be unavoidable.
Hence, preventive techniques are insufficient, and active error correction is required.
Various researchers have proposed coding techniques to encode data on a bus for crosstalk
avoidance [6,17,28], for low power dissipation [3,15,19,22,26], and for error correction
[1,8]. Coding schemes that simultaneously satisfy two of these three criteria have also been
investigated:
â€¢ crosstalk avoidance and low power dissipation [12,27];
â€¢ crosstalk avoidance and error correction [14]; and
â€¢ low power dissipation and error correction [2,16,18].

Table 1 Types of worst crosstalk couplings
Type-1

Type-2

Type-3

Type-4

0 â†â†’ 1

001 â†â†’ 110
011 â†â†’ 100

001 â†â†’ 010
010 â†â†’ 100
011 â†â†’ 101
101 â†â†’ 110

010 â†â†’ 101

Single wire undergoes
transition. Adjacent
wires maintain previous states

Center wire in opposite
transition to an adjacent wire.
The other wire in same transition as center wire

Center wire in opposite
transition to an adjacent
wire. The other wire maintains previous state

123

All three adjacent
wires undergo opposite transitions

Optimal low-power coding for crosstalk avoidance

481

Despite many efforts, the only families of optimal codes known are those for low power
dissipation [3]. Many of the results on the comparative performance of existing codes are
based on simulations rather than rigorous mathematical analysis.
In this paper, we begin the study of codes for UDSM buses that simultaneously provide
for low power dissipation, crosstalk avoidance, and error correction. In particular, we exhibit
the first infinite families of such codes that are provably optimal.
The paper is organized as follows. Section 2 establishes necessary terminology and gives a
mathematical formulation of the problem of designing low-power codes that avoid crosstalks
and correct errors. In Sect. 3, we present the relation of codes of each type with packing
sampling plans avoiding adjacent units. In Sect. 4, we focus on optimal solutions for k = 3
for all positive integer n. In Sect. 5, the sizes of optimal codes of all types with small lengths
are determined by computer search, and brief conclusion is given.

2 Background
2.1 Coding framework
A coding framework for data buses was introduced by Ramprasad et al. [15]. A bus interconnecting two embedded systems on a systems-on-chip (SoC) platform can be modelled
generically as in Fig. 1. The source encoder (decoder) compresses (decompresses) the input
data so that the number of bits required in the representation of the source is minimised. While
the source encoder removes redundancy, the channel encoder adds redundancy to combat
errors that may arise due to noise in the bus.
Ramprasad et al. [15] considered various combinations of source-channel encoder-decoder
pairs and presented simulation results for their power dissipation. Their approach is what
is known as joint source-channel coding in the information theory literature. Shannonâ€™s
information separation theorem [20] states that reliable transmission can be accomplished
by separate source and channel coding, where the source encoder and decoder need not take
into account the channel statistics and the channel encoder and decoder need not take into
account the source statistics. This applies, however, only for point-to-point transmissions
and for infinite sequence length. The first condition (point-to-point transmission) holds for
a UDSM bus but the second requirement for infinite sequence length is clearly undesirable
for bus coding, because it could give rise to circuits of unbounded delay. Moreover, joint
source-channel coding is useful only when we know the statistics of the source and channel.
In the absence of such statistics, one can only fall back on optimising the source and channel
separately. Indeed, Ramprasad et al. [15] considered coding schemes and simulations on
certain source data with better understood statistics (for example, pop music, classical music,
video, and speech).

noisy channel
source
encoder

channel
encoder

transmitter

channel
decoder

source
decoder

receiver

Fig. 1 Framework for systems-on-chip

123

482

Y. M. Chee et al.

In many systems, the behaviour of source data is hard to predict and so the joint sourcechannel coding approach loses its power. Many researchers have therefore fallen back on
addressing the source coding and channel coding problems separately. This is also the
approach taken in this paper. We focus on designing optimal channel coding schemes for the
scenario where the source statistics are unknown.

2.2 Codes
The Hamming n-space is the set H(n) = {0, 1}n , endowed with the (Hamming) distance
dH (Â·, Â·) defined as follows: for u, v âˆˆ H(n), dH (u, v) is the number of positions where u and
v differ. The (Hamming) weight of a vector u âˆˆ H(n) is the number of positions in u with
nonzero value, and is denoted wH (u). The ith component of u is denoted ui . The support of
a vector u âˆˆ H(n), denoted supp(u), is the set {i : ui = 1}.
A (binary) code of length n is a subset C âŠ† H(n). C is said to be of constant weight w if
wH (u) = w for all u âˆˆ C . The elements of a code are called codewords and the size of a code
is the number of codewords it contains. The support of C is supp(C ) = {supp(u) : u âˆˆ C }. The
minimum distance of C is dmin (C ) = min{dH (u, v) : u, v âˆˆ C and u  = v}. A constant-weight
code of length n, minimum distance d, and weight w is denoted as an (n, d, w) code.
A code that is capable of correcting any occurrence of e or fewer symbol errors is said to
be e-error-correcting. A code C is e-error-correcting if and only if dmin (C ) â‰¥ 2e + 1 [9].

2.3 Set systems and graphs
For integers i < j, the set {i, i + 1, . . . , j} is abbreviated as [i, j]. We further abbreviate
[1, j] to [ j]. For a finite set X and k â‰¤ |X |, we define
 
X
X
2 = {B : B âŠ† X }, and
= {B âŠ† X : |B| = k}.
k
A set system is a pair S = (X, B), where X is a finite set of points and B âŠ† 2 X . The
elements of B are called blocks. The order of S is the number of points, |X |, and the size
 of
S is the number of blocks, |B|. A set system (X, B) is said to be k-uniform if B âŠ† Xk . A
graph is a 2-uniform set system and it is common to refer to the points and blocks of a graph
as vertices and edges, respectively. A path of length n is an alternating sequence of vertices
and edges W = v0 , e1 , v1 , e2 , . . . , en , vn , such that all the vertices vi , i âˆˆ [0, n] and edges
ei , i âˆˆ [n] are all distinct from one another, except possibly the first and last vertices. A cycle
is a path in which the first and last vertices are the same.
Let (X, B) be a set system of order n. The incidence vector of a block B âˆˆ B is the vector
Î¹(B) âˆˆ H(n) such that

1, if i âˆˆ B
Î¹(B)i =
0, otherwise.
There is a natural correspondence between the Hamming n-space and the complete set system
(X, 2 X ): the positions of vectors in H(n) correspond to points in X , a vector u âˆˆ H(n)
corresponds to the block supp(u), and dH (u, v) = |(supp(u)\supp(v))âˆª(supp(v)\supp(u))|.
From this, it follows that there is a bijection between the set of all codes of length n and the
set of all set systems of order n.
An (n, k,
is a k-uniform set system (X, B) with |X | = n such that every

 Î»)-packing
element of X2 is contained in at most Î» blocks of B. Let D(n, k, Î») denote the largest size
among all (n, k, Î»)-packings. The leave graph of (X, B) is the multigraph (X, E), where E

123

Optimal low-power coding for crosstalk avoidance

483

 
contains each e âˆˆ X2 exactly Î» âˆ’ d(e) times, where d(e) is the number of blocks containing
e. When Î» = 1, we omit Î» in the notation; in this case, the leave is a simple graph. When the
leave contains no edges, the packing is a balanced incomplete block design.
The balanced sampling plan avoiding adjacent units (BSA) was introduced to design
sampling plans that exclude contiguous units in statistical experiments [10,25]; for more
recent work, see [7,29]. In statistical applications, in a circular or linear order of the elements,
elements that are â€œcloseâ€ do not appear together, while those more distant all appear the same
number of times together. A (circular) BSAÎ» (n, k; Î±) is an (n, k, Î»)-packing (X, B) with X =
Zn whose leave graph consists of all the edges {i, j} with i âˆ’ j â‰¡ Â±1, . . . , Â±Î± (mod n), and
every other pair appears in Î» blocks. A (linear) LBSAÎ» (n, k; Î±) is an (n, k, Î»)-packing (X, B)
with X = [0, n âˆ’ 1] whose leave graph consists of all the edges {i, j} with 0 â‰¤ i < j < n
for which j âˆ’ i â‰¤ Î±, and every other pair appears in Î» blocks. We employ these only when
Î» = 1, and so omit Î» in the notation.
We generalize circular and linear BSAs (with Î» = 1) to a packing sampling plan avoiding
adjacent units (PSA). A (circular) CPSA(n, k; Î±) is an (n, k)-packing (X, B) with X = Zn
whose leave graph contains all the edges {i, j} with i âˆ’ j â‰¡ Â±1, . . . , Â±Î± (mod n), and every
other pair appears in at most one block. A (linear) LPSA(n, k; Î±) is an (n, k)-packing (X, B)
with X = [0, n âˆ’ 1] whose leave graph contains all the edges {i, j} with 0 â‰¤ i < j < n
for which j âˆ’ i â‰¤ Î±, and every other pair appears in at most one block. (In this case, every
CPSA(n, k; Î±) is an LPSA(n, k; Î±) but the converse need not hold.) Let B(n, k; Î±) denote
the largest size of any LPSA(n, k; Î±); the LPSA is optimal if its size is B(n, k; Î±). Similarly,
let B â—¦ (n, k; Î±) denote the largest size of any CPSA(n, k; Î±); the CPSA is optimal if its size
is B â—¦ (n, k; Î±).

	


	


Let U (n, k; Î±) =

2

Î±âˆ’1
i=0

nâˆ’Î±âˆ’iâˆ’1
kâˆ’1

+(nâˆ’2Î±)

nâˆ’2Î±âˆ’1
kâˆ’1

k

.

Lemma 2.1 B(n, k; Î±) â‰¤ U (n, k; Î±).
Proof For an LPSA(n, k; Î±) constructed
on
	

 [0, n âˆ’ 1], for each i âˆˆ [0, Î± âˆ’ 1], the points i
nâˆ’Î±âˆ’iâˆ’1
and n âˆ’ 1 âˆ’ i appear in at most
blocks, and all the other points appear in at most
kâˆ’1
	


	


Î±âˆ’1 	 nâˆ’Î±âˆ’iâˆ’1 

nâˆ’2Î±âˆ’1
blocks. Then k B(n, k; Î±) â‰¤ 2 i=0
+ (n âˆ’ 2Î±) nâˆ’2Î±âˆ’1
.


kâˆ’1
kâˆ’1
kâˆ’1
When Î± = 1, we omit it in the notation. If there is an (n, k)-packing with leave graph
containing a path of length n âˆ’ 1, we can always relabel the points to get an LPSA(n, k).
 	 

	


Corollary 2.2 B(n, k) â‰¤

2

nâˆ’2
kâˆ’1

+(nâˆ’2)
k

nâˆ’3
kâˆ’1

.

Theorem 4.1 shows that when k = 3, this inequality is tight.

2.4 Problem formulation
Limited weight codes have been widely exploited for the case of on-chip communication
to achieve crosstalk coupling elimination and energy efficiency [12,23]. We consider an nbit parallel bus in a single metal layer, for which we want memoryless codes to weaken
crosstalk, reduce power consumption, and correct errors. We use constant weight codes with
small weight to achieve low power similarly by reducing the node switching activity, that is,
reducing the total number of transitions occurring between the newly arrived data and the
present data on the bus.

123

484

Y. M. Chee et al.

Assume an n-bit bus, consisting of signals b0 , b1 , b2 , . . . , bnâˆ’1 . Consider a group of three
wires in an on-chip bus, which are driven by signals biâˆ’1 , bi and bi+1 . The delay and energy
consumption are primarily affected by transition patterns based on the bus signals biâˆ’1 , bi
and bi+1 as the crosstalk patterns in Table 1.
The selection of codeword does not depend on previous history, so the environment is
memoryless. Consequently coding must address the possibility that any two codewords can
appear one after the other. Therefore to avoid crosstalk and correct errors, we are interested
in constant weight codes of length n, weight w and minimum distance d â‰¥ 3 satisfying the
condition that there do not exist three consecutive coordinates i âˆ’ 1, i, i + 1 such that the
crosstalk couplings of type-2 (or -3, -4) occur in any two different codewords.
We denote such a code avoiding crosstalk of each type as an (n, d, w)-II (or -III, -IV)
code. The maximum size of these codes are denoted as A I I (n, d, w) (or A I I I (n, d, w),
A I V (n, d, w)), and any code achieving this size is optimal. When S âŠ† {I I, I I I, I V }, the
maximum size of a code that is simultaneously an (n, d, w)-S code for each S âˆˆ S is denoted
by AS (n, d, w).
When d = 2w, the following results are straightforward.
Lemma 2.3 For all positive integers n and w,
 
(i) A I I (n, 2w, w) = A I V (n, 2w, w) = wn ;




(ii) A I I I (n, 2w, w) = wn when w  = 1; A I I I (n, 2, 1) = n+1
2 .
 
Proof The quantity s = wn is an upper bound on the size of the desired code in each case.
We construct codes of size s as follows. The code with support
{{i, s + i, 2s + i, . . . , (w âˆ’ 1)s + i} : i âˆˆ [0, s âˆ’ 1]}
is an optimal (n, 2w, w)-II code. The code with support
{{wi, 1 + wi, . . . , (w âˆ’ 1) + wi} : i âˆˆ [0, s âˆ’ 1]}
is an optimal (n, 2w, w)-IV code, and an optimal

(n, 2w, w)-III code when w  = 1. When
w = 1, the code with support {{2i} : i âˆˆ [0, nâˆ’1


2 ]} is an optimal (n, 2, 1)-III code.
Next we show there is close connection between (n, 2k âˆ’ 2, k) codes of each type and
optimal LPSA(n, k)s. Hence, optimal codes are constructed based on the construction of
optimal LPSA(n, k)s.

3 Codes and LPSA(n, k; Î±)s
In this section, we establish connections between optimal LPSA(n, k; Î±)s and the codes of
each type. We begin with optimal (n, 2k âˆ’ 2, k)-II codes for sufficiently large n.
Theorem 3.1 Let k â‰¥ 3. Then A I I (n, 2k âˆ’ 2, k) â‰¥ B(n, k). Further, if B(n, k) = U (n, k)
and n â‰¥ 3k 2 + 2k âˆ’ 3, then A I I (n, 2k âˆ’ 2, k) = B(n, k).
Proof Whenever (X, B) is an LPSA(n, k), the code with support B is an (n, 2kâˆ’2, k)-II code.
Now suppose that (X, B) is an optimal LPSA(n, k) of size U (n, k). We prove that U (n, k)
is the largest possible size of an (n, 2k âˆ’ 2, k)-II code. Assume that D is an (n, 2k âˆ’ 2, k)-II
code of size M. Partition the code into three parts as follows.
The first part A contains all codewords with at least one segment â€œ11â€. Because n > k, for
each codeword in A, there always exist three adjacent coordinates such that â€œ110â€ or â€œ011â€

123

Optimal low-power coding for crosstalk avoidance

485

appears in these coordinates. Let S = {i : âˆƒu âˆˆ A, s.t.,u has â€œ110 in coordinates i âˆ’ 2, i âˆ’
1, i, or â€œ011â€ in coordinates i, i + 1, i + 2}, and let s = |S|. For each i âˆˆ S, there exist at
most two codewords in A that have â€œ110â€ in i âˆ’ 2, i âˆ’ 1, i or â€œ011â€ in i, i + 1, i + 2. Hence
|A| â‰¤ 2s.
The second part T âŠ† D \ A contains all codewords with â€œ1â€ in at least one coordinate in
S. Without loss of generality, if there exists a codeword in A with â€œ110â€ in the coordinates
i âˆ’ 2, i âˆ’ 1, i for some i, then the codewords in T with â€œ1â€ in i must have segment â€œ101â€
in these coordinates to avoid type-2 crosstalk. Because dmin (D) = 2k âˆ’ 2, there is only one
such codeword. So for each i âˆˆ S, there is at most one codeword in T with â€œ1â€ in i. Hence
|T | â‰¤ s.
Finally, let C = D \ (A âˆª T ). Then M = |A| + |T | + |C |. Because each codeword in C
has â€œ0â€ in all coordinates in S, we can shorten C to a code C  by deleting all coordinates in
S. Then C  is an (n âˆ’ s, 2k âˆ’ 2, k) code, and supp(C  ) is an (n âˆ’ s, k)-packing.
The shortening process partitions the coordinates of C  into at most s +1 classes, separated
in C by the coordinates deleted to form C  . No codeword of C  has â€œ11â€ in consecutive
coordinates of any single class. Let x be the number of isolated coordinates in this partition,
and m be the number of classes with at least two coordinates; then x + m â‰¤ s + 1. We now
estimate the 	
size of C
 using the




	 packing.
	
nâˆ’sâˆ’2
nâˆ’sâˆ’3
Let a0 = nâˆ’sâˆ’1
,
a
,
a
. Then we have:
=
=
1
2
kâˆ’1
kâˆ’1
kâˆ’1


x Â· a0 + 2m Â· a1 + (n âˆ’ s âˆ’ 2m âˆ’ x) Â· a2

| C | = |C | â‰¤
.
k
Because x âˆ’ y âˆ’ 1 â‰¤ x âˆ’ y â‰¤ x âˆ’ y, we have:

 	


	

 	


	

â¥
â¢ 	
â¢ x nâˆ’sâˆ’1 âˆ’ nâˆ’sâˆ’3 +2m nâˆ’sâˆ’2 âˆ’ nâˆ’sâˆ’3 + (n âˆ’ s) nâˆ’sâˆ’3 â¥
â¥
â¢
kâˆ’1
kâˆ’1
kâˆ’1
kâˆ’1
kâˆ’1
â¦
M â‰¤ 3s + â£
k



	



	

â¥
â¢ 	
2
1
nâˆ’sâˆ’3 â¥
â¢x
+
1
+
2m
+
1
+
(n
âˆ’
s)
â¢
â¥
kâˆ’1
kâˆ’1
kâˆ’1
â¦
â‰¤ 3s + â£
k
	
	

â¥

â¥
â¢
â¢
â¢ 2(s + 1) + (n âˆ’ s) nâˆ’sâˆ’3 â¥
â¢ 2x + 2m + (n âˆ’ s) nâˆ’sâˆ’3 â¥
â¢
â¢
â¥
â¥
kâˆ’1
kâˆ’1
â¦ â‰¤ 3s + â£
â¦.
â‰¤ 3s + â£
k
k

Let F(s) = 3s +

	
2(s+1)+(nâˆ’s)

Because F(s) = 3s +


	
2(s+2)+(nâˆ’sâˆ’1)
k

nâˆ’sâˆ’4
kâˆ’1




. We claim that because n â‰¥ 3k 2 + 2k âˆ’ 3,

k

U (n, k) â‰¥ maxsâˆˆ[1,n] F(s).


nâˆ’sâˆ’3
kâˆ’1



	
2(s+1)+(nâˆ’s)

, we have

	

nâˆ’sâˆ’3
kâˆ’1

k
nâˆ’sâˆ’4
kâˆ’1

k




âˆ’2






and F(s + 1) = 3(s + 1) +
	




âˆ’3 â‰¤ F(s)âˆ’F(s+1) â‰¤

nâˆ’sâˆ’4
kâˆ’1

+nâˆ’sâˆ’2

k

âˆ’2. Further, we have:




n âˆ’ 2k âˆ’ s
n âˆ’ 3k 2 âˆ’ 1 âˆ’ s
â‰¤ F(s) âˆ’ F(s + 1) â‰¤
.
k(k âˆ’ 1)
kâˆ’1
So when s â‰¤ n âˆ’ 3k 2 âˆ’ 1, F(s) âˆ’ F(s + 1) â‰¥ 0, i.e., F(s) is decreasing; and when
s â‰¥ n âˆ’ 2k, F(s) âˆ’ F(s + 1) â‰¤ 0, i.e., F(s) is increasing. When s âˆˆ [n âˆ’ 3k 2 , n âˆ’ 2k âˆ’ 1],

123

486

Y. M. Chee et al.

F(s) â‰¤ F(1); because the verification is tedious, we omit it here. We therefore only need to
compare F(1) and F(n) to find the maximum value of F(s).
	

â¥
â¢
â¢ 4+(n âˆ’ 1) nâˆ’4 â¥

 

2
â¢
kâˆ’1 â¥
â¦ âˆ’ 3n âˆ’ 2(n +1) â‰¥ (n âˆ’ 1)(n âˆ’ 3k âˆ’ 1) .
F(1) âˆ’ F(n) = 3+ â£
k
k
k(k âˆ’ 1)

Because n â‰¥ 3k 2 + 2k âˆ’ 3 â‰¥ 3k 2 + 1, F(1) â‰¥ F(n) and maxsâˆˆ[1,n] F(s) = F(1).


	


	

â¥
â¢ 	
â¢ 2 nâˆ’2 + (n âˆ’ 2) nâˆ’3 âˆ’ 4 âˆ’ (n âˆ’ 1) nâˆ’4 â¥
â¢ kâˆ’1
kâˆ’1
kâˆ’1 â¥
â¦âˆ’3
U (n, k) âˆ’ F(1) â‰¥ â£
k


	


	

â¥
â¢	
â¢ nâˆ’2 + (n âˆ’ 1) nâˆ’3 âˆ’ 4 âˆ’ (n âˆ’ 1) nâˆ’4 â¥
â¢ kâˆ’1
kâˆ’1
kâˆ’1 â¥
â¦âˆ’3
â‰¥â£
k


â¢	
â¥
â¢ nâˆ’2 âˆ’ 4 â¥


2
â¢ kâˆ’1
â¥
â¦ âˆ’ 3 â‰¥ n âˆ’ 3k âˆ’ 2k + 3 â‰¥ 0.
â‰¥â£
k
k(k âˆ’ 1)

Hence U (n, k) â‰¥ maxsâˆˆ[1,n] F(s).




For (n, 2k âˆ’ 2, k)-III codes and (n, 2k âˆ’ 2, k)-IV codes, we establish lower bounds.
 
Lemma 3.2 1. A I I I (n, 2k âˆ’ 2, k) â‰¥ A I I,I I I,I V (n, 2k âˆ’ 2, k) â‰¥ D( n2 , k).


2. A I I I (n, 4, 3) â‰¥ A I I,I I I,I V (n, 4, 3) â‰¥ B( n2 , 3) +  nâˆ’1
2 .
 
3. A I I I (n, 4, 3) â‰¥ A I I,I I I,I V (n, 4, 3) â‰¥ B â—¦ ( n2 , 3) +  n2 .
n
Proof For
 n the first inequality, take an ( 2 , k)-packing (X,
 B), and construct a code C of
length 2 by taking supp(C ) = B. View C as an |B| Ã— n2 array. When n â‰¡ 1 (mod 2),
we add one column of all zeroes between every two consecutive columns of C , and when
n â‰¡ 0 (mod 2) we add one further column of all zeroes after C to get an (n, 2k âˆ’ 2, k)-III
code. The verification is straightforward, because every second column is all zeroes.
  
The construction for the second is similar. Apply the same inflation to an LPSA n2 , 3
  
of size B n2 , 3 to obtain a code C1 . In every codeword of C1 , two 1s are separated by three
(or more) coordinates, and different codewords cannot have 1s in adjacent coordinates. Now
form code C2 , consisting of all codewords with support {2i, 2i +1, 2i +2} for 0 â‰¤ i <  nâˆ’1
2 .
No prohibited situation arises from 000 or 111 in three consecutive coordinates of a codeword.
In consecutive coordinates in which two codewords of C2 are neither 000 nor 111, the two
codewords contain 011 and 110, which is permitted. So we consider one codeword from
C1 and one from C2 . The coordinates with indices in {2i + 1 : 0 â‰¤ i <  nâˆ’1
2 } appear in
only one codeword, which is {2i, 2i + 1, 2i + 2}. So in the consecutive coordinates in which
two such codewords are neither 000 nor 111, and are not equal, the two codewords contain
{001, 100}, {010, 011}, or {010, 110}. All are permitted.
 n 
â—¦
2 ,3 =
 The
 n bound
 in the third case is equal to that in the second unless n is even and B
B 2 , 3 . When both occur, use a CPSA to form C1 and C2 as in the second case; one further
codeword can be added with support {0, n âˆ’ 2, n âˆ’ 1}.



123

Optimal low-power coding for crosstalk avoidance

487

Lemma 3.3 A I V (n, 2k âˆ’ 2, k) â‰¥ B(n, k).
Proof Take an LPSA(n, k) (X, B) of size B(n, k). Apply to the points in [0, n âˆ’ 1] the
permutation

i â†’ 2i, if i < n/2, and
i â†’ 2i âˆ’ 2n/2 + 1, if i â‰¥ n/2,
to get (X, B ). The code C  with supp(C  ) = B is an (n, 2k âˆ’ 2, k)-IV code.




We give another construction for an (n, 2kâˆ’2, k)-IV code from an optimal LPSA(n, k; kâˆ’
1). When k = 3, this construction gives a better lower bound than Lemma 3.3.
Lemma 3.4 Let k â‰¥ 3.
1. A I I,I V (n, 2k âˆ’ 2, k) â‰¥ B(n, k; k âˆ’ 1),	


2. A I V (n, 2k âˆ’ 2, k) â‰¥ B(n, k; k âˆ’ 1) + nâˆ’1
, and
	kâˆ’1 

n
I
V
â—¦
3. A (n, 2k âˆ’ 2, k) â‰¥ B (n, k; k âˆ’ 1) + kâˆ’1 .


	
Proof Let s = nâˆ’1
kâˆ’1 , and (X, B ) be an LPSA(n, k; k âˆ’ 1) of size B(n, k; k âˆ’ 1). Then the
code C with supp(C ) = {B : B âˆˆ B} is an (n, 2k âˆ’2, k)-II code and an (n, 2k âˆ’2, k)-IV code.
Further, the code C with supp(C ) = {B : B âˆˆ B}âˆª{{(kâˆ’1)i, (kâˆ’1)i +1, . . . , (kâˆ’1)i +kâˆ’1} :
i âˆˆ [0, s âˆ’1]} is an (n, 2k âˆ’2, k)-IV code. When n  â‰¡ 0 (mod k âˆ’ 1), statement (3) is implied
by statement (2). So suppose that n â‰¡ 0 (mod k âˆ’ 1). Using instead a CPSA(n, k; k âˆ’ 1) of
size B â—¦ (n, k; k âˆ’ 1), adjoin the block {(k âˆ’ 1)s, (k âˆ’ 1)s + 1, . . . , (k âˆ’ 1)s + k âˆ’ 2, 0}. 

Lemma 3.5 A I I,I V (n, 4, 3) â‰¤ U (n, 3; 2) when n â‰¥ 13.
Proof Computational results reported in Table 2 show that A I I,I V (13, 4, 3) = U (13, 3; 2) =
16, A I I,I V (14, 4, 3) = U (14, 3; 2) = 20, A I I,I V (15, 4, 3) = U (15, 3; 2) = 25, and
Table 2 Sizes of optimal codes for n â‰¤ 20
n

3

4

5

6

7

8

9

10

11

12

13

14

15

16

17

18

19

20

D (n, 3)

1

1

2

4

7

8

12

13

17

20

26

28

35

37

44

48

57

60

B (n, 3)

0

0

1

2

4

6

9

10

14

16

21

24

30

32

39

42

50

54

B â—¦ (n, 3)

0

0

0

2

3

5

9

10

13

16

20

23

30

32

38

42

49

53

B (n, 3; 2)

0

0

0

0

1

2

4

6

9

12

16

20

25

28

34

37

45

48

B â—¦ (n, 3; 2)

0

0

0

0

0

0

3

5

8

12

15

18

25

26

34

36

43

46

A I I .(n, 4, 3)

1

1

2

4

5

6

9

10

14

16

21

24

30

32

39

42

50

54

A I I I (n, 4, 3)

1

1

2

3

4

5

6

7

8

9

10

11

13

14

17

18

19

21

A I V (n, 4, 3)

1

1

2

4

6

7

10

12

15

19

23

26

32

35

42

45

54

57

A I I,I I I (n, 4, 3)

1

1

2

3

3

4

5

6

7

8

10

11

13

13

17

18

19

20

A I I,I V (n, 4, 3)

1

1

2

4

4

4

7

8

12

13

16

20

25

28

34

37

45

48

A I I I,I V (n, 4, 3)

1

1

2

3

4

5

6

7

8

9

10

11

13

14

17

18

19

21

A I I,I I I,I V (n, 4, 3)

1

1

2

3

3

4

5

6

7

8

10

11

13

13

17

18

19

20

Lower bounds and exact values

123

488

Y. M. Chee et al.

A I I,I V (16, 4, 3) = U (16, 3; 2) = 32. Suppose to the contrary that A I I,I V (n, 4, 3) >
U (n, 3; 2) for some n â‰¥ 17, and let n be the smallest such value. When n â‰¥ 17 we have
U (n, 3; 2) â‰¥ U (n âˆ’ 1, 3; 2) + 3 and U (n, 3; 2) â‰¥ U (n âˆ’ 3, 3; 2) + 4. (See Table 2 for small
values.)
Let (X, B) be the support of an (n, 4, 3)-{II,IV} code of size A I I,I V (n, 4, 3). Some triple
of B covers a pair of the form {a, b} âˆˆ {{i, i +1}, {i, i +2}} because it is not an LPSA(n, 3; 2).
Case 1 Some element appears in at most one triple. Suppose that element i appears in no
triple. Shorten the code by deleting coordinate i and delete the triples (if any) containing
pairs {i âˆ’ 1, i + 1}, {i âˆ’ 2, i + 1}, and {i âˆ’ 1, i + 2}. The result is a type II and IV code, so
the given code has at most A I I,I V (n âˆ’ 1, 4, 3) + 3 triples, a contradiction. Suppose now that
element i appears in exactly one triple T . Then if i âˆˆ {0, n âˆ’ 1}, delete coordinate i and triple
T to get a contradiction. If i âˆˆ {1, n âˆ’ 2}, delete coordinate i and delete triple T , along with
triples containing {0, 2} and {0, 3} when i = 1 or {n âˆ’ 4, n âˆ’ 1} and {n âˆ’ 3, n âˆ’ 1}, to get a
contradiction. So 2 â‰¤ i â‰¤ nâˆ’3. If T contains neither i âˆ’1 nor i +1, then no triple contains both
i âˆ’1 and i +1, because the code is type IV. Shorten by deleting coordinate i and delete triple T
and the triples (if any) containing pairs {i âˆ’2, i +1} and {i âˆ’1, i +2}, yielding a contradiction.
Otherwise, without loss of generality T also contains i âˆ’ 1 but does not contain i + 1. But
then if some triple T  contains i âˆ’ 2 and i + 1, it cannot contain i. If T  does not also contain
i âˆ’ 1, then we have T âˆ© {i âˆ’ 1, i, i + 1} = {i âˆ’ 1, i} and T  âˆ© {i âˆ’ 1, i, i + 1} = {i + 1},
which cannot happen in a type II code. So T  = {i âˆ’ 2, i, i + 1}. Hence there are at most
two triples among those containing pairs {i âˆ’ 1, i + 1}, {i âˆ’ 2, i + 1}, and {i âˆ’ 1, i + 2}, so
shorten as before.
Case 2 Some triple T satisfies |T âˆ© {i, i + 1, i + 2}| = 2 for some 0 â‰¤ i â‰¤ n âˆ’ 3. Suppose
that {a, b} = T âˆ© {i, i + 1, i + 2} and let {c} = {i, i + 1, i + 2} \ {a, b}. There can be no triple
containing c but neither a nor b, because the code is type II and type IV. So c is in exactly two
triples, T  that contains a and T  that contains b; only T contains both a and b. Applying the
same argument to T  and T  , a and b each appear in exactly two triples. So there are only
three triples (T , T  , and T  ) that contain a, b, or c. Shorten by deleting coordinate i + 1 and
the triples T , T  , and T  to obtain a contradiction.
Case 3 No triple T satisfies |T âˆ© {i, i + 1, i + 2}| = 2 for any 0 â‰¤ i â‰¤ n âˆ’ 3. If a triple
T satisfies |T âˆ© {i, i + 1, i + 2}| = 3 for some 0 â‰¤ i â‰¤ n âˆ’ 3, equivalently it satisfies
|T âˆ© {i + 1, i + 2, i + 3}| = 2 for some 0 â‰¤ i â‰¤ n âˆ’ 4 or |T âˆ© {i âˆ’ 1, i, i + 1}| = 2 for some
1 â‰¤ i â‰¤ n âˆ’ 3. Apply Case 2. Otherwise every triple T satisfies |T âˆ© {i, i + 1, i + 2}| â‰¤ 1 for
0 â‰¤ i â‰¤ n âˆ’ 3. But then (X, B) is an LPSA(n, 3; 2) and hence we have at most B(n, 3; 2) â‰¤
U (n, 3; 2) triples, the final contradiction.



4 Optimal packing sampling plans
By Corollary 2.2, we have the upper bound:

â§ 2
n âˆ’4n+4
âª
,
âª
6
âª
 




âª
2 âˆ’3n
nâˆ’2
nâˆ’3
â¨
n
2 2 + (n âˆ’ 2) 2
6 ,
U (n, 3) =
= n 2 âˆ’4n
âª
3
âª 6 ,
âª
âª
â© n 2 âˆ’3nâˆ’4 ,
6

Theorem 4.1 B(n, 3) = U (n, 3) for all n â‰¥ 0.

123

if n â‰¡ 2 (mod 6),
if n â‰¡ 3 (mod 6),
if n â‰¡ 0, 4 (mod 6),
if n â‰¡ 1, 5 (mod 6).

Optimal low-power coding for crosstalk avoidance

489

Proof When n â‰¡ 3 (mod 6), Colbourn and Rosa [4] (and Colbourn and Ling [5]) construct
2
a BSA(n, 3) of size n âˆ’3n
6 , which is an optimal LPSA(n, 3). Because each point appears in

(nâˆ’1) âˆ’4(nâˆ’1)+4
blocks, we get an LPSA(n âˆ’ 1, 3) of size n âˆ’3n
âˆ’ nâˆ’3
by removing
6
2 =
6
the point n âˆ’ 1 and all blocks containing it, which is optimal.
When n â‰¡ 1, 5 (mod 6), Colbourn and Rosa [4] showed there exists an (n, 3)-packing of
2
size n âˆ’3n+2
, whose leave graph consists of a cycle of length n âˆ’ 1 and one isolated point.
6
Assume n âˆ’1 is the isolated point. Remove the block {x, n âˆ’2, n âˆ’1} for some x âˆˆ [0, n âˆ’3];
the result is an optimal LPSA(n, 3). Now, n âˆ’ 1 appears in nâˆ’3
2 blocks. Removing n âˆ’ 1
and all blocks containing it from the optimal LPSA(n, 3) constructed above, we obtain an
2
(nâˆ’1)2 âˆ’4(nâˆ’1)
LPSA(n âˆ’ 1, 3) of size n âˆ’3nâˆ’4
âˆ’ nâˆ’3
, which is optimal.


6
2 =
6
2

2

nâˆ’3
2

Theorem 4.2 1. B â—¦ (n, 3) = U (n, 3) when n â‰¡ 0, 3, 4 (mod 6).
2. B â—¦ (n, 3) = U (n, 3) âˆ’ 1 when n â‰¡ 1, 2, 5 (mod 6).
blocks when n â‰¡
Proof The constructions in Theorem 4.1 yield a CPSA(n, 3) with n(nâˆ’3)
6
blocks
when
n
â‰¡
0,
4
(mod
6).
A
CPSA(n,
3)
can have at most
3 (mod 6) and with n(nâˆ’4)
6
 n  nâˆ’3 
blocks,
which
equals
U
(n,
3)
when
n
â‰¡
0,
3,
4
(mod
6),
so
these
are optimal.
3
2
 n  nâˆ’3 
â—¦
When n â‰¡ 2 (mod 6), 3 2
= U (n, 3) âˆ’ 1 so B (n, 3) â‰¤ U (n, 3) âˆ’ 1. When
n â‰¡ 1, 5 (mod 6), if there were a CPSA(n, 3) with U (n, 3) =
n(nâˆ’1)
2

3(n 2 âˆ’3nâˆ’4)

n 2 âˆ’3nâˆ’4
6

codewords, then the

âˆ’
= n + 2. The leave must be an
number of edges in the leave graph is
6
n-cycle with two additional edges, but every vertex in the leave must have even degree, which
cannot occur. So B â—¦ (n, 3) â‰¤ U (n, 3) âˆ’ 1. To establish equality when n â‰¡ 1, 2, 5 (mod 6),
remove the block {0, n âˆ’ 1, x} from an LPSA(n, 3) from Theorem 4.1.


Lemma 4.3 B â—¦ (n, 3; 2) = B(n, 3; 2) = U (n, 3; 2) whenever n â‰¡ 3, 5 (mod 6) and n â‰¥
15. B â—¦ (n, 3; 2) + 2 = B(n, 3; 2) = U (n, 3; 2) whenever n â‰¡ 2, 4 (mod 6) and n â‰¥ 14.
Proof Zhang and Chang [30] establish that whenever n â‰¥ 15 and n â‰¡ 3, 5 (mod 6), there is a
BSA(n, 3; 2) having n(nâˆ’5)
blocks; this is also an optimal CPSA(n, 3; 2) and LPSA(n, 3; 2).
6
Now suppose that n â‰¥ 14 and n â‰¡ 2, 4 (mod 6). When n â‰¡ 2 (mod 6), writing n = 6t + 2,
U (6t + 2, 3; 2) = (2t)(3t âˆ’ 1). Delete element 6t + 2 from a BSA(6t + 3, 3; 2) with
(2t + 1)(3t âˆ’ 1) blocks, removing 3t âˆ’ 1 blocks to obtain an LPSA(6t + 2, 3; 2), which is
therefore optimal. When n â‰¡ 4 (mod 6), writing n = 6t + 4, U (6t + 4, 3; 2) = t (6t + 2).
Delete element 6t + 4 from a BSA(6t + 5, 3; 2) with t (6t + 5) blocks, removing 3t blocks to
obtain an LPSA(6t + 4, 3; 2), which is therefore optimal. Remove the blocks {0, n âˆ’ 2, x},
{1, n âˆ’ 1, y} for some x and y from the optimal LPSA(n, 3; 2) constructed above to obtain
an optimal CPSA(n, 3; 2) when n â‰¡ 2, 4 (mod 6).


	 	



For n = 6t, U (6t, 3) = 6t (t âˆ’ 1) + 1, and 6t3 6tâˆ’5
= 6t (t âˆ’ 1). For n = 6t + 1,
2
 6t+1  6tâˆ’4 
U (6t +1, 3) = t (6t âˆ’3), and 3
= t (6t âˆ’3)âˆ’1. However, if a CPSA(6t +1, 3; 2)
2
were to have t (6t âˆ’ 3) âˆ’ 1 blocks, its leave must have 2(6t + 1) + 1 edges and every such
graph with minimum degree 4 has two vertices of degree 5. Because all vertices in the leave
must have even degree, no CPSA(6t + 1, 3; 2) can exist with more than t (6t âˆ’ 3) âˆ’ 2 blocks.
We provide bounds to apply when n â‰¡ 0, 1 (mod 6).
Lemma 4.4 B(2n, 3; 2) â‰¥ 4B(n, 3), and B(2n + 1, 3; 2) â‰¥ 4B(n, 3) + n âˆ’ 2. In addition,
B â—¦ (2n, 3; 2) â‰¥ 4B â—¦ (n, 3), and B â—¦ (2n + 1, 3; 2) â‰¥ 4B â—¦ (n, 3) + n âˆ’ 3.

123

490

Y. M. Chee et al.

Proof Start with an LPSA(n, 3) on [0, n âˆ’ 1]. We form an LPSA(2n, 3; 2) on [0, 2n âˆ’ 1].
For each block {a, b, c} in the LPSA, form four blocks {{2a + Î±, 2b + Î², 2c + Î³ } : Î±, Î², Î³ âˆˆ
{0, 1}, Î± + Î² + Î³ â‰¡ 0 (mod 2)}. The verification is straightforward. To form an LPSA(2n +
1, 3) on [0, 2n], adjoin {{2i, 2i + 3, 2n} : 0 â‰¤ i â‰¤ n âˆ’ 3}.
The construction for CPSAs is the same, except that one does not adjoin {0, 3, 2n}.



5 Conclusion
Applying Theorem 3.1 with the results in Theorem 4.1, we have optimal (n, 4, 3)-II codes for
all n â‰¥ 30. By computer search (using cliquer [13] and hill-climbing (a variant of [24])),
we determined the sizes of optimal LPSA(n, 3; Î±)s, CPSA(n, 3; Î±)s, and (n, 4, 3) codes of
lengths n â‰¤ 20. The sizes are listed in Table 2 and corresponding optimal codes are available
from the authors; those in slanted font are lower bounds from Theorem 3.1 and Lemma 3.4.
In this paper, we present the first memoryless transition bus-encoding technique for power
minimization, error-correcting and elimination of crosstalk simultaneously. We establish the
connection between codes avoiding crosstalk of each type with packing sampling plans
avoiding adjacent units. Optimal codes of each type are constructed.

References
1. Bertozzi D., Benini L., de Micheli G.: Low power error resilient encoding for on-chip data buses. In:
DATEâ€™02: Proceedings of the Conference on Design. Automation and Test in Europe, pp. 102â€“109. IEEE
Computer Society, Washington, DC (2002).
2. Bertozzi D. Benini L., Ricco B.: Energy-efficient and reliable low-swing signaling for on-chip buses
based on redundant coding. In: ISCASâ€™02: Proceedings of the IEEE International Symposium on Circuits
and Systems, vol. 1, pp. 93â€“96. IEEE Press, Piscataway, NJ (2002).
3. Chee Y.M., Colbourn C.J., Ling A.C.H.: Optimal memoryless encoding for low power off-chip data
buses. In: ICCADâ€™06: Proceedings of the 2006 IEEE/ACM International Conference on Computer-Aided
Design, pp. 369â€“374. ACM Press, New York (2006).
4. Colbourn C.J., Rosa A.: Quadratic leaves of maximal partial triple systems. Graphs Comb. 2(4), 317â€“337
(1986).
5. Colbourn C.J., Ling A.C.H.: A class of partial triple systems with applications in survey sampling.
Commun. Stat. Theory Methods 27(4), 1009â€“1018 (1998).
6. Duan C., Tirumala A., Khatri S.P.: Analysis and avoidance of crosstalk in on-chip buses. In: Hot Interconnectsâ€™01: Proceedings of the 9th Annual Symposium on High-Performance Interconnects, pp. 133â€“138.
IEEE, Piscataway (2001).
7. Dukes P.J., Ling A.C.H.: Existence of balanced sampling plans avoiding cyclic distances. Metrika 70,
131â€“140 (2009).
8. Favalli M., Metra C.: Bus crosstalk fault-detection capabilities of error-detecting codes for on-line testing.
IEEE Trans. Very Large Scale Integr. Syst. 7, 392â€“396 (1999).
9. Hamming R.W.: Error detecting and error correcting codes. Bell Syst. Tech. J. 29(2), 147â€“160 (1950).
10. Hedayat A.S., Rao C.R., Stufken J.: Sampling plans excluding contiguous units. J. Stat. Plan. Inference
19(2), 159â€“170 (1988).
11. Ho R.: On-chip wires: Scaling and efficiency, Ph.D. dissertation, Department of Electrical Engineering,
Stanford University, Palo Alto, CA (2003).
12. Khan Z., Arslan T., Erdogan A.T.: A dual low-power and crosstalk immune encoding scheme for systemon-chip buses. In: PATMOSâ€™04: Proceedings of the 14th International Workshop on Power and Timing
Modeling, Optimization and Simulation. Lecture Notes in Computer Science, vol. 3254, pp. 585â€“592.
Springer, Berlin (2004).
13. Niskanen S., Ã–stergÃ¥rd P.R.J.: Cliquer Userâ€™s Guide, Version 1.0, Communications Laboratory, Helsinki
University of Technology, Espoo. Tech. Rep. T48, (2003).
14. Patel K.N., Markov I.L.: Error-correction and crosstalk avoidance in DSM busses. IEEE Trans. Very
Large Scale Integr. Syst. 12(10), 1076â€“1080 (2004).

123

Optimal low-power coding for crosstalk avoidance

491

15. Ramprasad S., Shanbhag N.R., Hajj I.N.: A coding framework for low-power address and data busses.
IEEE Trans. Very Large Scale Integr. (VLSI) Syst. 7, 212â€“221 (1999).
16. Rossi D., van Dijk E.S., Kleihorst R.P., Nieuwland A.K., Metra C.: Coding scheme for low energy
consumption fault-tolerant bus. In: IOLTWâ€™02: Proceedings of the Eighth IEEE International On-Line
Testing Workshop, pp. 8â€“12. IEEE Computer Society, Washington, DC (2002).
17. Rossi D., Cavallotti S., Metra C.: Error correcting codes for crosstalk effect minimization. In: DFTâ€™03:
Proceedings of the 18th IEEE International Symposium on Defect and Fault-Tolerance in VLSI Systems,
pp. 257â€“266. IEEE Computer Society, Washington, DC (2003).
18. Rossi D., van Dijk E.S., Kleihorst R.P., Nieuwland, A.K., Metra, C.: Power consumption of fault tolerant
codes: the active elements. In IOLTWâ€™03: Proceedings of the Ninth IEEE International On-line Testing
Workshop, pp. 61â€“67. IEEE Computer Society, Washington, DC (2003).
19. Samala N.K., Radhakrishnan D., Izadi B.: A novel deep sub-micron bus coding for low energy. In: ESAâ€™04:
Proceedings of the International Conference on Embedded Systems and Applications, pp. 25â€“30. CSREA
Press, Leuven (2004).
20. Shannon C.E.: A mathematical theory of communications, Bell Syst. Tech. J. 27(3), 379â€“423, 623â€“656
(1948).
21. Sotiriadis P.P., Chandrakasan A.: Bus energy minimization by transition pattern coding (TPC) in deep
sub-micron technologies. In: ICCADâ€™00â€”Proceedings of the 2000 IEEE/ACM International Conference
on Computer-Aided Design, pp. 322â€“327. IEEE, Piscataway, NJ (2000).
22. Stan M.R., Burleson W.P.: Bus-invert coding for low-power I/O. IEEE Trans. Very Large Scale Integr.
Syst. 3(1), 49â€“58 (1995).
23. Stan M.R., Burleson W.P.: Coding a terminated bus for low power. In: Great Lakes Symposium VLSI,
pp. 70â€“73, Buffalo, NY (1995).
24. Stinson D.R.: Hill-climbing algorithms for the construction of combinatorial designs. In: Algorithms in
Combinatorial Design Theory. Annals of Discrete Mathematics, vol. 26, pp. 321â€“334. Elsevier, NorthHolland (1985).
25. Stufken J.: Combinatorial and statistical aspects of sampling plans to avoid the selection of adjacent units.
J. Comb. Inf. Syst. Sci. 18(1â€“2), 149â€“160 (1993).
26. Su C.L., Tsui C.Y., Despain A.M.: Saving power in the control path of embedded processors. IEEE Des.
Comput. 11, 24â€“30 (1994).
27. Subrahmanya P., Manimegalai R., Kamakoti V., Mutyam M.: A bus encoding technique for power and
cross-talk minimization. In: VLSI Design 2004: 17th International Conference on VLSI Design, pp.
443â€“448. IEEE Computer Society, Xiâ€™an (2004).
28. Victor B., Keutzer K.: Bus encoding to prevent crosstalk delay. In: ICCADâ€™01â€”Proceedings of the
2001 IEEE/ACM International Conference on Computer-Aided Design, pp. 57â€“63. IEEE, Piscataway,
NJ (2001).
29. Wright J.H., Stufken J.: New balanced sampling plans excluding adjacent units. J. Stat. Plan. Inference
138, 3326â€“3335 (2008).
30. Zhang J., Chang Y.X.: The spectrum of BSA(v, 3, Î»; Î±) with Î± = 2, 3. J. Comb. Des. 15, 61â€“76 (2007).

123

Test Algebra for Combinatorial Testing
Wei-Tek Tsaiâˆ— , Charles J. Colbournâˆ—â€  , Jie Luoâ€  , Guanqiu Qiâˆ— , Qingyang Liâˆ— , Xiaoying Baiâ€¡
âˆ— School

of Computing, Informatics, and Decision Systems Engineering
Arizona State University, Tempe, AZ, USA
â€  State Key Laboratory of Software Development Environment
School of Computer Science and Engineering,
Beihang University, Beijing, China
â€¡ Department of Computer Science and Technology, INLIST
Tsinghua University, Beijing, China
{wtsai, colbourn}@asu.edu, luojie@nlsde.buaa.edu.cn
{guanqiuq, qingyan2b}@asu.edu, baixy@tsinghua.edu.cn

Abstractâ€”This paper proposes a new algebraic system, Test
Algebra (T A), for identifying faults in combinatorial testing for
SaaS (Software-as-a-Service) applications. SaaS as a part of cloud
computing is a new software delivery model, and mission-critical
applications are composed, deployed, and executed in cloud
platforms. Testing SaaS applications is a challenging task because
new applications need to be tested when they are composed
before they can be deployed for execution. Combinatorial testing
algorithms can be used to identify faulty configurations and
interactions from 2-way all the way to k-way where k is the
number of components in the application. The T A defines rules
to identify faulty configurations and interactions. Using the rules
defined in the T A, a collection of configurations can be tested
concurrently in different servers and in any order and the results
obtained will be still same due to the algebraic constraints.
Index Termsâ€”Combinatorial testing, algebra, SaaS

I. I NTRODUCTION
Software-as-a-Service (SaaS) is a new software delivery
model. SaaS often supports three features: customization,
multi-tenancy architecture (MTA), and scalability. MTA means
using one code base to develop multiple tenant applications,
and each tenant application essentially is a customization
of the base code [12]. A SaaS system often also supports
scalability as it can supply additional computing resources
when the workload is heavy. Tenantsâ€™ applications are often
customized by using components stored in the SaaS database
[14], [1], [11] including GUI, workflow, service, and data
components.
Once tenant applications are composed, they need to be tested, but a SaaS system can have millions of components, and
hundreds of thousands of tenant applications. Testing tenant
applications becomes a challenge as new tenant applications
and components are added into the SaaS system continuously.
New tenant applications are added on a daily basis while other
tenant applications are running on the SaaS platform. As new
tenant applications are composed, new components are added
into the SaaS system. Each tenant application represents a
customer for the SaaS system, and thus it needs to be tested.
Combinatorial testing is a popular testing technique to test
an application with different configurations. It often assumes
that each component within a configuration has been tested

already, but interactions among components in the configuration may cause failures. Traditional combinatorial testing
techniques often focus on test case generation to detect the
presence of faults, but fault location is an active research area.
Each configuration needs to be tested, as each configuration
represents a tenant application. Traditional combinatorial testing such as AETG [2] can reveal the existence of faults by
using few test cases to support t-way coverage for t â‰¥ 2. But
knowing the existence of a fault does not indicate which tway interactions are faulty. When the problem size is small,
an engineer can identify faults. However, when the problem
is large, it can be a challenge to identify faults.
As the IT industries moves to Big Data and cloud computing where hundreds of thousand processors are available,
potentially, a large number of processors with distributed
databases can be used to perform large-scale combinatorial testing. Furthermore, they also provide concurrent and
asynchronous computing mechanisms such as MapReduce,
automated redundancy and recovery management, automated
resource provisioning, and automated migration for scalability.
These capabilities provide significant computing power that
was not available before. One simple way of performing
combinatorial testing in a cloud environment is:
1) Partition the testing tasks;
2) Allocate these testing tasks to different processors in the
cloud platform for test execution;
3) Collect results done by these processors.
However, this is not efficient as while the number of computing
and storage resources have increased significantly, the number
of combinations to be considered is still too high. For example,
a large SaaS system may have millions of components, and
testing all of these combinations can still consume all the
resources in a cloud platform. Two ways to improve this
approach and both are based on learning from the previous
test results:
â€¢

â€¢

Devise a mechanism to merge test results from different
processors so that testing results can be merged quickly,
and detect any inconsistency in testing;
Based on the existing testing results, eliminate any con-

figurations or interactions from future testing.
Due to the asynchronous and autonomous nature of cloud
computing, test results may arrive asynchronously and autonomously, and this necessitates a new testing framework.
This paper proposes a new algebraic system, Test Algebra
(TA), to facilitate concurrent combinatorial testing. The key
feature of TA is that the algebraic rules follow the combinatorial structure, and thus can track the test results obtained. The
TA can then be used to determine whether a tenant application
is faulty, and which interactions need to be tested. The TA
is an algebraic system in which elements and operations are
formally defined. Each element represents a unique component
in the SaaS system, and a set of components represents a
tenant application. Assuming each component has been tested
by developers, testing a tenant application is equivalent to
ensuring that there is no t-way interaction faults for t â‰¥ 2
among the elements in a set.
The TA uses the principle that if a t-way interaction is
faulty, every (t + 1)-way interaction that contains the t-way
interaction as a subset is necessarily faulty. The TA provides
guidance for the testing process based on test results so far.
Each new test result may indicate if additional tests are needed
to test a specific configuration. The TA is an algebraic system,
primarily intended to track the test results without knowing
how these results were obtained. Specifically, it does not
record the execution sequence of previously executed test
cases. Because of this, it is possible to allocate different
configurations to different processors for execution in parallel
or in any order, and the test results are merged following
the TA rules. The execution order and the merge order do
not affect the merged results if the merging follows the TA
operation rules.
This paper is structured as follows: Section II discusses the
related work; Section III proposes TA and shows its details;
and Section IV concludes this paper. Appendix provides proofs
of TA associativity properties.
II. R ELATED W ORK
SaaS testing is a new research topic [14], [4], [10]. Using
policies and metadata, test cases can be generated to test
SaaS applications. Testing can be embedded in the cloud
platform where tenant applications are run [14]. Gao proposed
a framework for testing cloud applications [4], and proposed
a scalability measure for testing cloud application scalability.
Another scalability measure was proposed by [10].
Testing all combinations of inputs and preconditions is not
feasible, even with a simple product [6], [8]. The number
of defects in a software product can be large, and defects
occurring infrequently are difficult to find [15]. Combinatorial
test design is used to identify a small number of tests needed
to get the coverage of important combinations. Combinatorial
test design methods enable one to build structure variation
into test cases for having greater test coverage with fewer test
cases.
Determining the presence of faults caused by a small
number of interacting elements has been extensively studied

in component-based software testing. When interactions are
to be examined, testing involves a combination-based strategy
[5]. When every interaction among t or fewer elements is to
be tested, methods have been developed that provide pairwise
or t-way coverage. Among the early methods, AET G [2]
popularized greedy one-test-at-a-time methods for constructing
test suites. In the literature, the test suite is usually called a
covering array, defined as follows. Suppose that there are k
configurable elements, numbered from 1 to k. Suppose that
for element c, there are vc valid options. A t-way interaction
is a selection of t of the k configurable elements, and a valid
option for each. A test selects a valid option for every element,
and it covers a t-way interaction if, when one restricts the
attention to the t selected elements, each has the same option
in the interaction as it does in the test.
A covering array of strength t is a collection of tests so
that every t-way interaction is covered by at least one of the
tests. Covering arrays reveal faults that arise from improper
interaction of t or fewer elements [9]. There are numerous
computational and mathematical approaches for construction
of covering arrays with a number of tests as small as possible
[3], [7].
If a t-way interaction causes a fault, then executing all
tests of a covering array will reveal the presence of at least
one faulty interaction. SaaS testing is interested in identifying
those interactions that are faulty including their numbers and
locations, as faulty configurations cannot be used in tenant
applications. Furthermore, the number and location of faults
keep on changing as new components can be added into
the SaaS database continuously. By then executing each test,
certain interactions are known not to be faulty, while others
appear only in tests that reveal faults, and hence may be faulty.
At this point, a classification tree analysis builds decision trees
for characterizing possible sets of faults. This classification
analysis is then used either to permit a system developer to
focus on a small collection of possible faults, or to design
additional tests to further restrict the set of possible faults.
In [16], empirical results demonstrate the effectiveness of
this strategy at limiting the possible faulty interactions to a
manageable number. Assuming that interactions of more than
t elements do not produce faults, a covering array can use few
tests to certify that no fault arises from a t-way interaction.
The Adaptive Reasoning algorithm (AR) is a strategy to
detect faults in SaaS [13]. The algorithm uses earlier test
results to generate new test cases to detect faults in tenant
applications. It uses three principles:
â€¢

â€¢

â€¢

Principle 1: When a tenant application (or configuration)
fails the testing, there is at least one fault (but there may
be more) in the tenant configuration.
Principle 2: When a tenant application passes the testing,
there is no fault in the tenant configuration resulting
from a t-way interactions among components in the
configuration.
Principle 3: Whenever a configuration contains one or
more faulty interactions, it is faulty.

III. T EST A LGEBRA
Let C be a finite set of components. A configuration is
a subset T âŠ† C. One is concerned with determining the
operational status of configurations. To do this, one can
execute certain tests; every test is a configuration, but there
may be restrictions on which configurations can be used as
tests. If a certain test can be executed, its execution results in
an outcome of passed (operational) or failed (faulty).
When a test execution yields result, all configurations that
are subsets of the test are operational. However, when a test
execution yields a faulty result, one only knows that at least
one subset causes the fault, but it is unclear which of these
subsets caused the failure. Among a set of configurations that
may be responsible for faults, the objective is to determine,
which cause faults and which do not. To do this, one must
identify the set of candidates to be faulty. Because faults
are expected to arise from an interaction among relatively
few components, one considers t-way interactions. The t-way
interactions are It = {U âŠ† C : |U | = t}. Hence the goal is to
select tests, so that from the execution results of these tests,
one can ascertain the status of all t-way interactions for some
fixed small value of t.
Because interactions and configurations are represented as
subsets, one can use set-theoretic operations such as union, and
their associated algebraic properties such as commutativity,
associativity, and self-absorption. The structure of subsets and
supersets also plays a key role.
To permit this classification, one can use a valuation function V , so that for every subset S of components, V (S)
indicates the current knowledge about the operational status
consistent with the components in S. The focus is on determining V (S) whenever S is an interaction in I1 âˆª Â· Â· Â· âˆª It .
These interactions can have one of five states.
â€¢ Infeasible (X): For certain interactions, it may happen
that no feasible test is permitted to contain this interaction. For example, it may be infeasible to select two GUI
components in one configuration such that one says the
wall is GREEN but the other says RED.
â€¢ Faulty (F): If the interaction has been found to be faulty.
â€¢ Operational (P): Among the rest, if an interaction has
appeared in a test whose execution gave an operational
result, the interaction cannot be faulty.
â€¢ Irrelevant (N): For some feasible interactions, it may
be the case that certain interactions are not expected to
arise, so while it is possible to run a test containing the
interaction, there is no requirement to do so.
â€¢ Unknown (U): If neither of these occurs then the status
of the interaction is required but not currently known.
Any given stage of testing, an interaction has one of five
possible status indicators. These five status indicators are
ordered by X  F  P  N  U under a relation , and
it has a natural interpretation to be explained in a moment.
A. Learning from Previous Test Results
The motivation for developing an algebra is to automate
the deduction of the status of an interaction from the status

of tests and other interactions, particularly in combining the
status of two interactions. Specifically, one is often interested
in determining V (T1 âˆª T2 ) from V (T1 ) and V (T2 ). To do this,
a binary operation âŠ— on {X, F, P, N, U} can be defined, with
operation table as follows:
âŠ—
X
F
P
N
U

X
X
X
X
X
X

F
X
F
F
F
F

P
X
F
U
N
U

N
X
F
N
N
N

U
X
F
U
N
U

Using this definition, one can verify that the binary operation âŠ— has the following properties of commutativity and
associativity.
V (T1 ) âŠ— V (T2 ) = V (T2 ) âŠ— V (T1 ),
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
Using this operation, one observes that V (T1 âˆª T2 ) 
V (T1 ) âŠ— V (T2 ). It follows that
1) Every superset of an infeasible interaction is infeasible.
2) Every superset of a failed interaction is failed or infeasible.
3) Every superset of an irrelevant interaction is irrelevant,
failed, passed, or infeasible.
A set S is an X-implicant if V (S) = X but whenever
S 0 âŠ‚ S, V (S 0 ) â‰º X. The X-implicants provide a compact
representation for all interactions that are infeasible. Indeed
for any interaction T that contains an X-implicant, V (T ) = X.
Furthermore, a set S is an F-implicant if V (S) = F but
whenever S 0 âŠ‚ S, V (S 0 ) â‰º F. For any interaction T that
contains an F-implicant, V (T )  F. In the same way, a set S is
an N-implicant if V (S) = N but whenever S 0 âŠ‚ S, V (S 0 ) = U.
For any interaction T that contains an N-implicant, V (T )  N.
An analogous statement holds for passed interactions, but here
the implication is for subsets. A set S is a P-implicant if
V (S) = P but whenever S 0 âŠƒ S, V (S 0 )  F. For any
interaction T that is contained in a P-implicant, V (T ) = P.
Implicants are defined with respect to the current knowledge
about the status of interactions. When a t-way interaction is
known to be infeasible, failed, or irrelevant, it must contain
an X-, F-, or N-implicant. By repeatedly proceeding from tway to (t + 1)-way interactions, then, one avoids the need
for any tests for (t + 1)-way interactions that contain any
infeasible, failed, or irrelevant t-way interaction. Hence testing
typically proceeds by determining the status of the 1-way
interactions, then proceeding to 2-way, 3-way, and so on.
The operation âŠ— is useful in determining the implied status
of (t + 1)-way interactions from the computed results for
t-way interactions, by examining unions of the t-way and
smaller interactions and determining implications of the rule
that V (T1 âˆª T2 )  V (T1 ) âŠ— V (T2 ). Moreover, when adding
further interactions to consider, all interactions previously
tested that passed are contained in a P-implicant, and every
(t + 1) interaction contained in one of these interactions can
be assigned status P.

For example, V (a, b) = P, V (a, e) = X, and other 2way interactions exist as atomic interaction in TA. Based
on the defined âŠ— operation, values of t-way interactions can
be deduced from the atomic interactions and their contained
interactions, such as V (a, b, e)  V (a, b) âŠ— V (a, e) = X, i.e.
V (a, b, e) = X.
The 3-way interaction (a, b, c) can have inferred results from
2-way interactions (a, b), (a, c), (b, c). If any contained 2-way
interaction has value F, the determining value of 3-way is F,
without further testing needed. But if all values of contained
2-way interactions are P, (a, b, c) the interaction needs to be
tested. In this case, U needs to be changed to non-U such as F
or P, assuming the 3-way is not X or N.
B. Changing Test Result Status
When testing a configuration with n components, one
should test individual components, 2-way interactions, 3-way
interactions, all the way to n-way interactions. Since any
combination of interactions is relevant in this case, the status
of any interaction can be either X, F, P, or U. The status of a
configuration is determined by the status of all interactions.
1) If an interaction has status X (F), the configuration has
status X (F).
2) If all interactions have status P, the configuration has
status P.
3) If some interactions still have status U, further tests are
needed.
It is important to determine when an interaction with status
U can be deduced to have status F or P instead. It can never
obtain status X or N once having had status U.
To change U to P: An interaction is assigned status P if and
only if it is a subset of a test that leads to proper operation.
To change U to F: Consider the candidate T , one can
conclude that V (T ) = F if there is a test containing T that
yields a failed result, but for every other candidate interaction
T 0 that appears in this test, V (T 0 ) = P. In other words, the
only possible explanation for the failure is the failure of T .
C. Matrix Representation
Suppose that each individual component passed the testing.
Then the operation table starts from 2-way interactions, then
enlarges to t-way interactions step by step. During the procedure, many test results can be deduced from the existing results
following TA rules. For example, all possible configurations
of (a, b, c, d, e, f ) can be expressed in the form of matrix, or
operation table. First, we show the operation table for 2-way
interactions. The entries in the operation table are symmetric
and those on the main diagonal are not necessary. So only half
of the entries are shown.
As shown in Figure 1, 3-way interactions can be composed
by using 2-way interactions and components. Thus, following
the TA implication rules, the 3-way interactions operation table
is composed based on the results of 2-way combinations. Here,
(a, b, c, d, e, f ) has more 3-way interactions than 2-way
interactions. As seen in Figure 1, a 3-way interaction can be
obtained through different combinations of 2-way interactions

and components. For example, {a, b, c} = {a} âˆª {b, c} =
{b}âˆª{a, c} = {c}âˆª{a, b} = {a, b}âˆª{a, c} = {a, b}âˆª{b, c} =
{a, c} âˆª {b, c}. V (a) âŠ— V (b, c) = V (c) âŠ— V (a, b) = V (a, b) âŠ—
V (b, c) = PâŠ—P = U. But V (b)âŠ—V (a, c) = V (a, b)âŠ—V (a, c) =
V (b, c) âŠ— V (a, c) = P âŠ— F = F. As TA defines the order of
the five status indicators, the result should be the value with
highest order. So V (a, b, c) = F.
âŠ— a
a
b
c
d
e
f

b
P

c d e
F N X
P X N
F P
F

f
U
F
P
X
U

D. Merging Concurrent Testing Results
One way to achieve efficient testing is to allocate (overlapping or non-overlapping) tenant applications into different
clusters, and each cluster is sent to a different set of servers
for execution. Once each cluster completes its execution, the
test results can be merged. The testing results of a specific
interaction T in different servers should satisfy the following
constraints.
â€¢ If V (T ) = U in one cluster, then in other clusters, the
same V (T ) can be either F, P, N, or U.
â€¢ If V (T ) = N in one cluster, then in other clusters, the
same V (T ) can be either F, P, N, or U.
â€¢ If V (T ) = P in one cluster, then the same V (T ) can be
either P, N, or U in all clusters;
â€¢ If V (T ) = F in one cluster, then in other clusters, the
same V (T ) can be F, N, or U.
â€¢ If V (T ) = X in one cluster, then in other clusters, the
same V (T ) can be X only.
If these constraints are satisfied, then the testing results can
be merged. Otherwise, there must be an error in the testing
results. To represent this situation, a new status indicator, error
(E), is introduced and E  X. We define a binary operation âŠ•
on {E, X, F, P, N, U}, with operation table as follows:
âŠ•
E
X
F
P
N
U

E
E
E
E
E
E
E

X
E
X
E
E
E
E

F
E
E
F
E
F
F

P
E
E
E
P
P
P

N
E
E
F
P
N
U

U
E
E
F
P
U
U

âŠ• also has the properties of commutativity and associativity.
See Appendix for proof of associativity.
Using this operation, merging two testing results from two
different servers can be defined as Vmerged (T ) = Vcluster1 (T ) âŠ•
Vcluster2 (T ). The merge can be performed in any order due to
the commutativity and associativity of âŠ•, and if the constraints
of merge are satisfied and V (T ) = X, F, or P, the results cannot
be changed by any further testing or merging of test results
unless there are some errors in testing. If V (T ) = E, the testing

âˆª
a
b
c
..
.

a
(a)

b
(a, b)
(b)

c
(a, c)
(b, c)
(c)

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

f
(a, f )
(b, f )
(c, f )
..
.

(a, b)
(a, b)
(a, b)
(a, b, c)
..
.

(a, c)
(a, c)
(a, b, c)
(a, c)
..
.

(f )

(a, b, f )
(a, b)

(a, c, f ) Â· Â· Â·
(a, b, c) Â· Â· Â·
(a, c)
Â·Â·Â·
..
.

f
(a, b)
(a, c)
..
.
(b, c)
..
.

(b, c)
(a, b, c)
(b, c)
(b, c)
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

(b, c, f ) Â· Â· Â·
(a, b, c) Â· Â· Â·
(a, b, c) Â· Â· Â·
..
..
.
.
(b, c)
Â·Â·Â·
..
.

(e, f )
(a, e, f )
(b, e, f )
(c, e, f )
..
.
(e, f )
(a, b, e, f )
(a, c, e, f )
..
.
(b, c, e, f )
..
.
(e, f )

(e, f )
âŠ—
a
b
c
..
.
f
(a, b)
(a, c)
..
.

a

b
P

c
F
P

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

f
U
F
P
..
.

(a, b)
U
U
U
..
.

(a, c)
F
F
F
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

(b, c)
U
U
U
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

(e, f )
U
U
U
..
.

U

F
F

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

U
U
F
..
.

Â·Â·Â·
Â·Â·Â·
Â·Â·Â·
..
.

U
U
F
..
.

Â·Â·Â·
..
.

U
..
.

(b, c)
..
.
(e, f )
Fig. 1. 3-way interaction operation table

environment must be corrected and tests executed again after
fixing the error(s) in testing. For example, Vcluster1 (a, c, e) = X
and Vcluster2 (a, c, e) = F, so Vmerged (a, c, e) = X âŠ• F = E.
It means that there is something wrong with the tests of
interaction (a, c, e), and the problem must be fixed before
doing further testing.
Following the âŠ• associative rule, one can derive the following.
V1 (T ) âŠ• V2 (T ) âŠ• V3 (T )
= (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T )
= V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T ))
= V1 (T ) âŠ• V2 (T ) âŠ• V3 (T ) âŠ• V3 (T )
= (V1 (T ) âŠ• V2 (T )) âŠ• (V2 (T ) âŠ• V3 (T ))
= ((V1 (T ) âŠ• V2 (T )) âŠ• V2 (T )) âŠ• V3 (T )
= (V3 (T ) âŠ• V2 (T )) âŠ• (V3 (T ) âŠ• V1 (T ))
Thus the âŠ• rule allows one to partition the configurations
into different sets for different servers to run testing, and
these sets do not need to be non-overlapping. In conventional
cloud computing operations such as MapReduce, data should
not overlap, otherwise incorrect data may be produced. For
example, counting the items in a set can be performed by
MapReduce, but data allocated to different servers cannot
overlap, otherwise items may be counted more than once. In
TA, this is not a concern due to the nature of the TA operations

âŠ•. Once the results are available from each server, the testing
results can be merged either incrementally, in parallel, or in
any order. Furthermore, test results can be merged repeatedly
without changing the final results.
Using this approach, one can test say 10 batches of 100 tenant applications each, for a total of 1, 000 tenant applications.
Once this is done, another batch of 1, 000 tenant applications
can be tested with each 100 tenant application allocated to a
server for execution. In this way, after running 100 batches,
100, 000 tenant applications can be evaluated completely.
The following example illustrates the testing process of
fifteen configurations. According to the definition of configuration testing, all feasible interactions should be tested. For
simplicity, assume that only interaction (c, d, f ) is faulty, and
only interaction (c, d, e) is infeasible, and all other interactions
pass the testing. If one assigns configurations 1, 3, 5, 7, 9, 11,
13, 15 into Server1 , configurations 2, 4, 6, 8, 10, 12, 14 into
Server2 , and 4-11 configurations into Server3 .
If Server1 and Server3 do their own testing first, Server2
can reuse test results of interactions from them to eliminate
interactions that need to be tested. For example, when testing
2-way interactions of configuration (b, c, d, f ) in Server2 ,
it can reuse the test results of (b, c), (b, d) of configuration
(b, c, d, e) from Server3 , (b, f ) of configuration (a, b, c, f )

from Server1 . They are all passed, and it can reuse the test
results of (b, c, d) of configuration (a, b, c, d) from Server1 ,
(b, c, f ) of configuration (a, b, c, f ) from Server1 , (b, d, f )
of configuration (a, b, d, f ) from Server1 , and (c, d, f ) of
configuration (a, c, d, f ) from Server3 . Because (c, d, f ) is
faulty, it can deduce that 4-way interaction (b, c, d, f ) is also
faulty. For the sets of configuration that are overlapping, their
returned test results from different servers are the same. The
merged results of these results also stay the same.
Not only interactions, sets of configurations, CS1 , CS2 ,
. . . , CSK can be allocated to different processors (or clusters)
for testing, and the test results can then be merged. The sets
can be non-overlapping or overlapping, and the merge process
can be arbitrary. For example, say the result of CSi is RCSi ,
the merge process can be (Â· Â· Â· ((((RCS1 + RCS2 ) + RCS3 ) +
RCS4 ) + Â· Â· Â· + RCSK ), or (Â· Â· Â· ((((RCSK + RCSkâˆ’1 ) +
RCSkâˆ’2 ) + Â· Â· Â· + RCS1 ), or any other sequence that includes
all RCSi , for i = 1 to K. This is true because RCS is simply
a set of V (Tj ) for any intercation Tj in the configuration CSi .
(a,b,c,d)
(a,b,c,e)
(a,b,c,f)
(a,b,d,e)
(a,b,d,f)
(a,b,e,f)
(a,c,d,e)
(a,c,d,f)
(a,c,e,f)
(a,d,e,f)
(b,c,d,e)
(b,c,d,f)
(b,c,e,f)
(b,d,e,f)
(c,d,e,f)

Server1
P

Server2

Server3

P
P
P
P
P
X
F
P
P
X
F
P
P
X

P
P
P
X
F
P
P
X

IV. C ONCLUSION
This paper proposes TA to address SaaS combinatorial
testing. The TA provides a foundation for concurrent combinatorial testing. The TA has two operations and test results
can have five states with a priority. By using the TA operations, many combinatorial tests can be eliminated as the
TA identifies those interactions that need not be tested. Also
the TA defines operation rules to merge test results done by
different processors, so that combinatorial tests can be done in
a concurrent manner. The TA rules ensure that either merged
results are consistent or a testing error has been detected so
that retest is needed. In this way, large-scale combinatorial
testing can be carried out in a cloud platform with a large
number of processors to perform test execution in parallel to
identify faulty interactions.
ACKNOWLEDGMENT

This project is sponsored by U.S. National Science Foundation project DUE 0942453 and National Science Foundation
M erged Results China (No.61073003), National Basic Research Program of
China (No.2011CB302505), and the State Key Laboratory of
P
Software Development Environment (No. SKLSDE-2012ZXP
18), and Fujitsu Laboratory.
P
P
R EFERENCES
P
[1] X. Bai, M. Li, B. Chen, W.-T. Tsai, and J. Gao. Cloud Testing Tools. In
P
Proceedings of IEEE 6th International Symposium on Service Oriented
X
System Engineering (SOSE), pages 1â€“12, Irvine, CA, USA, 2011.
F
[2] D. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The
AETG System: An Approach to Testing Based on Combinatorial Design.
P
Journal of IEEE Transactions on Software Engineering, 23:437â€“444,
P
1997.
X
[3] C. J. Colbourn. Covering arrays and hash families. In D. Crnkovic and
V. D. Tonchev, editors, Information Security, Coding Theory and Related
F
Combinatorics, volume 29 of NATO Science for Peace and Security
P
Series - D: Information and Communication Security. IOS Press, 2011.
P
[4] J. Gao, X. Bai, and W.-T. Tsai. SaaS Performance and Scalability
Evaluation in Cloud. In Proceedings of The 6th IEEE International
X

E. Modified Testing Process
Perform 2-way interaction testing first. Before going on to 3way interaction testing, use the results of 2-way testing to eliminate cases. The testing process stops when finishing testing all
t-way interactions. The analysis of t-way interactions is based
on the P T Rs of all (t âˆ’ i)-way interactions for 1 â‰¤ i < t.
The superset of infeasible, irrelevant, and faulty test cases do
not need to be tested. The test results of the superset can be
obtained by TA operations and must be infeasible, irrelevant,
or faulty. But the superset of test cases with unknown indicator
must be tested. In this way, a large repeating testing workload
can be reduced.
For n components, all t-way interactions for t â‰¥ 2
are composed by 2-way, 3-way, ..., t-way interactions. In
n components combinatorial
 testing, the number of 2-way
interactions is equal to n2 . In general, the number of t-way
n
interactions
are treated when

is equal to t . More interactions
n
n
n
.
The
total number
>
,
which
happens
when
t
â‰¤
t
tâˆ’1
 2
Pt
of interactions examined is i=2 ni .

Symposium on Service Oriented System Engineering, SOSE â€™11, 2011.
[5] M. Grindal, J. Offutt, and S. F. Andler. Combination Testing Strategies:
A Survey. Software Testing, Verification, and Reliability, 15:167â€“199,
2005.
[6] C. Kaner, J. Falk, and H. Q. Nguyen. Testing Computer Software, 2nd
Edition. Wiley, New York, NY, USA, 1999.
[7] V. V. Kuliamin and A. A. Petukhov. A Survey of Methods for
Constructing Covering Arrays. Journal of Program Computer Software,
37(3):121â€“146, may 2011.
[8] T. Muller and D. Friedenberg. Certified Tester Foundation Level
Syllabus. Journal of International Software Testing Qualifications
Board.
[9] A. A. Porter, C. Yilmaz, A. M. Memon, D. C. Schmidt, and B. Natarajan.
Skoll: A Process and Infrastructure for Distributed Continuous Quality
Assurance. IEEE Transactions on Software Engineering, 33(8):510â€“525,
2007.
[10] W.-T. Tsai, Y. Huang, X. Bai, and J. Gao. Scalable Architecture for
SaaS. In Proceedings of 15th IEEE International Symposium on Object
Component Service-oriented Real-time Distributed Computing, ISORC
â€™12, Apr. 2012.
[11] W.-T. Tsai, Y. Huang, and Q. Shao. Testing the Scalability of SaaS Applications. In Proceedings of IEEE International Conference on ServiceOriented Computing and Applications (SOCA), pages 1â€“4, Irvine, CA,
USA, 2011.
[12] W.-T. Tsai, Y. Huang, Q. Shao, and X. Bai. Data Partitioning and
Redundancy Management for Robust Multi-Tenancy SaaS. International
Journal of Software and Informatics (IJSI), 4(3):437â€“471, 2010.

[13] W.-T. Tsai, Q. Li, C. J. Colbourn, and X. Bai. Adaptive Fault Detection
for Testing Tenant Applications in Multi-Tenancy SaaS Systems. In
Proceedings of IEEE International Conference on Cloud Engineering
(IC2E), March 2013.
[14] W.-T. Tsai, Q. Shao, and W. Li. OIC: Ontology-based Intelligent
Customization Framework for SaaS. In Proceedings of International
Conference on Service Oriented Computing and Applications(SOCAâ€™10),
Perth, Australia, Dec. 2010.
[15] Wikipedia. Software Testing, 2013.
[16] C. Yilmaz, M. B. Cohen, and A. Porter. Covering Arrays for Efficient
Fault Characterization in Complex Configuration Spaces. In Proceedings
of the 2004 ACM SIGSOFT International Symposium on Software
Testing and Analysis, ISSTA â€™04, pages 45â€“54, New York, NY, USA,
2004. ACM.

A PPENDIX
The associativity of binary operation âŠ—.
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
Proof: We will prove this property in the following cases.
(1) At least one of V (T1 ), V (T2 ), and V (T3 ) is X. Without
loss of generality, suppose that V (T1 ) = X, then according
to the operation table of âŠ—, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) =
X âŠ— (V (T2 ) âŠ— V (T3 )) = X, (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) =
(X âŠ— V (T2 )) âŠ— V (T3 ) = X âŠ— V (T3 ) = X. Thus, in this case,
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
(2) V (T1 ), V (T2 ), and V (T3 ) are not X and at least one
of V (T1 ), V (T2 ), and V (T3 ) is F. Without loss of generality,
suppose that V (T1 ) = F, then according to the operation table
of âŠ—, the value of V (T2 ) âŠ— V (T3 ) can only be F, N or U.
So V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = F âŠ— (V (T2 ) âŠ— V (T3 )) = F,
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) = (F âŠ— V (T2 )) âŠ— V (T3 ) = F âŠ—
V (T3 ) = F. Thus, in this case, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) =
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
(3) V (T1 ), V (T2 ), and V (T3 ) are not X or F and at least one
of V (T1 ), V (T2 ), and V (T3 ) is N. Without loss of generality,
suppose that V (T1 ) = N, then according to the operation table
of âŠ—, the value of V (T2 ) âŠ— V (T3 ) can only be N or U. So
V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = N âŠ— (V (T2 ) âŠ— V (T3 )) = N,
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) = (N âŠ— V (T2 )) âŠ— V (T3 ) = N âŠ—
V (T3 ) = N. Thus, in this case, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) =
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
(4) V (T1 ), V (T2 ), and V (T3 ) are not X, F or N. In this case,
V (T1 ), V (T2 ), and V (T3 ) can only be P or U. According to the
operation table of âŠ—, the value of V (T1 )âŠ—V (T2 ) and V (T2 )âŠ—
V (T3 ) are U. So V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = V (T1 ) âŠ— U = U,
(V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ) = U âŠ— V (T3 ) = U. Thus, in this
case, V (T1 ) âŠ— (V (T2 ) âŠ— V (T3 )) = (V (T1 ) âŠ— V (T2 )) âŠ— V (T3 ).
The associativity of binary operation âŠ•.
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ).
Proof: We will prove this property in the following cases.
(1) One of V1 (T ), V2 (T ), and V3 (T ) is E. Without loss
of generality, suppose that V1 (T ) = E, then according to the
operation table of âŠ•, V1 (T )âŠ•(V2 (T )âŠ•V3 (T )) = EâŠ—(V2 (T )âŠ•
V3 (T )) = E, (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = (E âŠ• V2 (T )) âŠ•
V3 (T ) = EâŠ•V3 (T ) = E. Thus, in this case, V1 (T )âŠ•(V2 (T )âŠ•
V3 (T )) = (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ).

(2) V1 (T ), V2 (T ), and V3 (T ) are not E, and there is a pair
of V1 (T ), V2 (T ), and V3 (T ) does not satisfy the constrains.
Without loss of generality, suppose that V1 (T ) and V2 (T ) does
not satisfy the constrains, then according to the operation table
of âŠ•, V1 (T ) âŠ• V2 (T ) = E. So (V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) =
E âŠ• V3 (T ) = E. Since V1 (T ) and V2 (T ) does not satisfy the
constrains, there can be two cases: (a) one of them is X and
the other is not, or (b) one of them is P and the other is F.
(a) If V1 (T ) = X, then V2 (T ) âŠ• V3 (T ) cannot be X because
V2 (T ) cannot be X. Thus, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E. If
V2 (T ) = X, then V2 (T ) âŠ• V3 (T ) 6= X can only be E or X.
Since V1 (T ) cannot be X, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E.
(b) If V1 (T ) = P and V2 (T ) = F, then V2 (T ) âŠ• V3 (T )
can only be E or F. Thus, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E. If
V1 (T ) = F and V2 (T ) = P, then V2 (T ) âŠ• V3 (T ) can only be
E or P. Thus, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = E.
Thus, in this case, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = (V1 (T ) âŠ•
V2 (T )) âŠ• V3 (T ).
(3) V1 (T ), V2 (T ), and V3 (T ) are not E, and V1 (T ), V2 (T ),
and V3 (T ) satisfy the constrains.
(a) One of V1 (T ), V2 (T ), and V3 (T ) is X. Without loss of
generality, suppose that V1 (T ) = X, then V2 (T ) = V3 (T ) = X.
So V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = X âŠ• (X âŠ• X) = X âŠ• X = X and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = (X âŠ• X) âŠ• X = X âŠ• X = X.
(b) V1 (T ), V2 (T ), and V3 (T ) are not X, and one of V1 (T ),
V2 (T ), and V3 (T ) is F. Without loss of generality, suppose
that V1 (T ) = F, then V2 (T ) and V3 (T ) can only be F, N,
or U. According to operation table of âŠ•, V2 (T ) âŠ• V3 (T ) can
only be F, N, or U, and V1 (T ) âŠ• V2 (T ) can only be F. So
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = F âŠ• (V2 (T ) âŠ• V3 (T )) = F and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = F âŠ• V3 (T ) = F.
(c) V1 (T ), V2 (T ), and V3 (T ) are not X or F, and one of
V1 (T ), V2 (T ), and V3 (T ) is P. Without loss of generality,
suppose that V1 (T ) = P, then V2 (T ) and V3 (T ) can only be
P, N, or U. According to operation table of âŠ•, V2 (T ) âŠ• V3 (T )
can only be P, N, or U, and V1 (T ) âŠ• V2 (T ) can only be F. So
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = P âŠ• (V2 (T ) âŠ• V3 (T )) = P and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = P âŠ• V3 (T ) = P.
(d) V1 (T ), V2 (T ), and V3 (T ) are not X, F or P, and one
of V1 (T ), V2 (T ), and V3 (T ) is U. Without loss of generality,
suppose that V1 (T ) = U, then V2 (T ) and V3 (T ) can only be
N, or U. According to operation table of âŠ•, V2 (T ) âŠ• V3 (T )
can only be N, or U, and V1 (T ) âŠ• V2 (T ) can only be U. So
V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = U âŠ• (V2 (T ) âŠ• V3 (T )) = U and
(V1 (T ) âŠ• V2 (T )) âŠ• V3 (T ) = U âŠ• V3 (T ) = U.
(e) V1 (T ), V2 (T ), and V3 (T ) are N. V1 (T ) âŠ• (V2 (T ) âŠ•
V3 (T )) = N âŠ• (N âŠ• N) = N âŠ• N = N and (V1 (T ) âŠ• V2 (T )) âŠ•
V3 (T ) = (N âŠ• N) âŠ• N = N âŠ• N = N.
Thus, in this case, V1 (T ) âŠ• (V2 (T ) âŠ• V3 (T )) = (V1 (T ) âŠ•
V2 (T )) âŠ• V3 (T ).

Software behavior depends on many factors, and some failures occur only when certain factors interact. This is known as an interaction triggered failure, and the corresponding selection of factor values can be modeled as a Minimal Failure-causing Schema (MFS). (An MFS involving m factors is an m-MFS.) Combinatorial Testing (CT) has been developed to exercise (â€œhitâ€) all MFS with few tests. Adaptive Random Resting (ART) endeavors to make tests as different as possible, ensuring that testing of MFS is not unnecessarily repeated. Random Testing (RT) chooses tests at random without regard to the MFS already treated. CT might be expected to improve on RT for finding interaction triggered faults, and yet some studies report no significant difference. CT can also be expected to be better than ART, and yet other studies report that ART can be much better than RT. In light of these, the relative merits of CT, ART, and RT for finding interaction triggered faults are unclear.Journal of Combinatorial Theory, Series A 120 (2013) 1440â€“1449

Contents lists available at SciVerse ScienceDirect

Journal of Combinatorial Theory,
Series A
www.elsevier.com/locate/jcta

Covering and packing for pairs
Yeow Meng Chee a , Charles J. Colbourn b , Alan C.H. Ling c ,
Richard M. Wilson d
a

Division of Mathematical Sciences, School of Physical and Mathematical Sciences,
Nanyang Technological University, 21 Nanyang Link, Singapore 637371, Singapore
b
Computing, Informatics, and Decision Systems Engineering, Arizona State University,
Tempe, AZ 85287-8809, USA
c
Department of Computer Science, University of Vermont, Burlington, VT 05405, USA
d
Department of Mathematics, 253-37, California Institute of Technology, Pasadena, CA 91125, USA

a r t i c l e

i n f o

Article history:
Received 24 September 2011
Available online xxxx
Keywords:
Balanced incomplete block design
Pair packing
Pair covering
Group divisible design
Pairwise balanced design

a b s t r a c t
When a v-set can be equipped with a set of k-subsets so that
every 2-subset of the v-set appears in exactly (or at most, or
at least) one of the chosen k-subsets, the result is a balanced
incomplete block design (or packing, or covering, respectively).
For each k, balanced incomplete block designs are known to exist
for all suï¬ƒciently large values of v that meet certain divisibility
conditions. When these conditions are not met, one can ask for
the packing with the most blocks and/or the covering with the
fewest blocks. Elementary necessary conditions furnish an upper
bound on the number of blocks in a packing and a lower bound
on the number of blocks in a covering. In this paper it is shown
that for all suï¬ƒciently large values of v, a packing and a covering
on v elements exist whose numbers of blocks differ from the basic
bounds by no more than an additive constant depending only on k.
Â© 2013 Elsevier Inc. All rights reserved.

1. Introduction
Let v, k, and t be integers with v > k > t  2. Let Î» be a positive integer. A (t , Î»)-packing of order v
and blocksize k is a set V of v elements, and a collection B of k-element subsets (blocks) of V , so
that every t-subset of V appears in at most Î» blocks. A (t , Î»)-covering of order v and blocksize k is
a set V of v elements, and a collection B of k-element subsets (blocks) of V , so that every t-subset

E-mail addresses: ymchee@ntu.edu.sg (Y.M. Chee), charles.colbourn@asu.edu (C.J. Colbourn), aling@cems.uvm.edu
(A.C.H. Ling), rmw@caltech.edu (R.M. Wilson).
0097-3165/$ â€“ see front matter Â© 2013 Elsevier Inc. All rights reserved.
http://dx.doi.org/10.1016/j.jcta.2013.04.005

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440â€“1449

1441

of V appears in at least Î» blocks. When Î» = 1, the simpler notation of t-packing or t-covering is used.
When ( V , B ) is both a (t , Î»)-packing and a (t , Î»)-covering with blocksize k, it is a t-( v , k, Î») design.
 v  k
A t-( v , k, Î») design, if one exists, has Î» t / t blocks. When the required number of blocks is
not integral, no such design can exist. Selecting all blocks containing a particular element x âˆˆ V and
deleting x from each forms the derived (t âˆ’ 1)-( v âˆ’ 1, k âˆ’ 1, Î») design (with respect to x). For a design
 v âˆ’i  kâˆ’i 
to exist, evidently the derived design must exist; hence for a t-( v , k, Î») design to exist, Î» t âˆ’i / t âˆ’i
must be integral for every 0  i  t. When these conditions are not all met, one can ask instead for
the largest (t , Î»)-packing, or for the smallest (t , Î»)-covering, of order v and blocksize k. The Johnson
bound [13] states that such a packing can have no more than

 



n
k

Â·Â·Â·



n âˆ’ t + 2 Î»(n âˆ’ t + 1)
kâˆ’t +2





Â·Â·Â·

kâˆ’t +1

blocks, while the SchÃ¶nheim bound [21] states that such a covering can have no fewer than

 
n
k



Â·Â·Â·



n âˆ’ t + 2 Î»(n âˆ’ t + 1)
kâˆ’t +2

kâˆ’t +1





Â·Â·Â·

blocks. Our main result is that when t = 2 and Î» = 1, there exist packings and coverings whose
sizes are within a constant of these bounds. Determining when these bounds are met exactly is a
challenging question.
In 1963, ErdoÌ‹s and Hanani [9] conjectured that, for ï¬xed k and t, with all blocks of size k,
n k
a t-packing on n elements with
/ t (1 âˆ’ o(1)) blocks and a t-covering on n elements with
t

n k
/ t (1 + o(1)) blocks both exist. This was proved by RÃ¶dl [20], and has spawned a large literature
t
(for example, [10,11,14,15,23]). However, even when t = 2, all of these general constructions deviate

from the Johnson and SchÃ¶nheim bounds by an amount that grows as a function of the number of
elements. Wilson [25] established that the necessary divisibility conditions for a 2-( v , k, Î») design to
exist are asymptotically suï¬ƒcient (i.e., for ï¬xed k and Î», and suï¬ƒciently large v). This provides a
different means to establish the ErdoÌ‹sâ€“Hanani conjecture for t = 2, but also does not immediately
imply that one can ï¬nd packings or coverings whose sizes are within a constant of the optimal sizes.
Wilson [24] earlier considered this more challenging problem for packings, but the solution for the
analogous problem for coverings has remained elusive.
We focus on the case when t = 2 and Î» = 1 here. Caro and Yuster state stronger results for covering [3] and packing [2] than we prove here. Their approach relies in an essential manner on a strong
statement by Gustavsson [12]:
Proposition 1.1. Let H be a graph with Î½ vertices and h edges, having degree sequence (d1 , . . . , dÎ½ ). Then
there exist a constant N H and a constant  H > 0, both depending only on H , such that for all n > N H , if
G is a graph on n vertices, m edges, and degree sequence (Î´1 , . . . , Î´n ) so that min(Î´1 , . . . , Î´n )  n(1 âˆ’  H ),
gcd(d1 , . . . , dÎ½ ) | gcd(Î´1 , . . . , Î´n ), and h | m, then G has an edge partition (decomposition) into graphs isomorphic to H .
We have not been able to verify the proof of Proposition 1.1. Indeed, while the result has been
used a number of times in the literature, no satisfactory proof of it appears there. While we expect
that the statement is true, we do not think that the proof in [12] is suï¬ƒcient at this time to employ
the statement as a foundation for further results. Therefore we adopt a strategy that is completely
independent of Proposition 1.1, and independent of the results built on it.
In the remainder of the paper, we ï¬rst recall relevant known results. Then in Section 3, we determine the possible structure of optimal packings and coverings, in order to determine what can remain
uncovered in a packing, and what must be covered more than once in a covering. This is done in general for packings and coverings with a single hole, in order to limit any deviation from the desired
bound to the manner in which a (ï¬xed size) hole is ï¬lled. In Section 4, the most important part of
the proof is established, namely that in each congruence class, one ï¬nite example can be produced.
Finally in Section 5, these single examples are shown to form the required ingredients to establish
asymptotic existence.

1442

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440â€“1449

2. Background
To proceed more formally, we require a number of deï¬nitions and preliminary results from combinatorial design theory; related background material can be found in [1,22]. A balanced incomplete block
design (BIBD) is a 2-( v , k, Î») design. Balanced incomplete block designs have been extensively studied
because of their central role in numerous applications in experimental design, coding and information
theory, communications, and connections with fundamental topics in algebra, ï¬nite geometry, number theory, and combinatorics (see [5,7] for examples). The general divisibility conditions (stated for
v 

k 
general t earlier) require that Î» 2 â‰¡ 0 mod 2 and Î»( v âˆ’ 1) â‰¡ 0 (mod k âˆ’ 1).
A group divisible design ( V , G , B ) is a ï¬nite set V of elements or points; a partition G = {G 1 , . . . , G s }
of V (groups); and a set B of subsets of V (blocks), with the property that every 2-subset of V lying
within a group appears in no block, while every 2-subset of V with elements from different groups
appears in exactly Î» blocks. When K is a set of positive integers for which | B | âˆˆ K whenever B âˆˆ B ,
the design is a ( K , Î»)-GDD. When Î» = 1, we write simply K -GDD. Its order is | V |, its index is Î», and
u
u
its type is Ïƒ1 1 Â· Â· Â· Ïƒ  when the multiset of group sizes {|G i |: 1  i  s} is the same as the multiset
formed by including u j copies of Ïƒ j when Ïƒ j = 0, for all 1  j  . We write (k, Î»)-GDD (or k-GDD
when Î» = 1) when K = {k}. A transversal design TDÎ» (k, n) is a (k, Î»)-GDD of type nk . We write TD(k, n)
when Î» = 1. A transversal design is idempotent if its element set is {1, . . . , k} Ã— {1, . . . , n}, and its block
set contains {{(i , j ): 1  i  k}: 1  j  n}. A pairwise balanced design with blocksizes K and order v
(( K , Î»)-PBD of order v) is a ( K , Î»)-GDD of type 1 v ; we write K -PBD when Î» = 1. Then a balanced
incomplete block design ((k, Î»)-BIBD) is a (k, Î»)-PBD; we write k-BIBD when Î» = 1.
An incomplete pairwise balanced design of order v with holesize h, blocksizes K , and index Î» is a
triple ( V , H , B ) for which | V | = v, | H | = h, H âŠ† V , B contains a set of subsets of V for which | B | âˆˆ K
whenever B âˆˆ B , and for every pair of distinct elements x, y âˆˆ V , the number of blocks in {{x, y } âŠ‚
B âˆˆ B } is 0 if {x, y } âŠ† H and Î» otherwise. The notation ( K , Î»)-IPBD( v , h) is used; we may omit Î»
when it is 1, and write k instead of K when K = {k}.
Let K be a set of positive integers, each at least 2. Then deï¬ne Î± ( K ) = gcd{k âˆ’ 1: k âˆˆ K } and
 
	
Î²( K ) = gcd 2k : k âˆˆ K .
Wilson establishes a crucial asymptotic existence result:
Theorem 2.1. (See [25].) Let K be a set of integers,each
 at least 2. Let Î» be a positive integer. For all suï¬ƒciently
n
large n satisfying Î»(n âˆ’ 1) â‰¡ 0 (mod Î± ( K )) and Î» 2 â‰¡ 0 (mod Î²( K )), there exists a ( K , Î»)-PBD of order n. In
particular for K = {k}, when Î»(n âˆ’ 1) â‰¡ 0 (mod k âˆ’ 1), Î»
exists a (k, Î»)-BIBD of order n.

n
2


 
â‰¡ 0 mod 2k , and n is suï¬ƒciently large, there

Colbourn and RÃ¶dl prove a variant that we use:
Theorem 2.2. (See [6].) 

Let Îµ > 0. Let K = {k1 , . . . , km } be a set of block sizes. Let { p 1 , . . . , pm } be nonm
negative
numbers
with
i =1 p i = 1. For all suï¬ƒciently large v satisfying v âˆ’ 1 â‰¡ 0 (mod Î± ( K )) and
v 
0
(
mod
Î²(
K
))
,
there
is
a K -PBD of order v in which, for each 1  i  m, the fraction of pairs appearing
â‰¡
2
in blocks having size ki is in the range [ p i âˆ’ Îµ , p i + Îµ ].
A stronger version of Theorem 2.2 is given in [26], and a variant for resolvable designs appears
in [8].
Perhaps the most powerful generalization of Theorem 2.1 is due to Lamken and Wilson [16]. We in(r ,Î»)
be a complete digraph on n vertices with exactly Î» edges of color i joining
troduce this next. Let K n
(r ,Î»)
is
any vertex x to any vertex y for every color i in a set of r colors. A family F of subgraphs of K n
(r ,Î»)
(r ,Î»)
if every edge e âˆˆ E ( K n ) belongs to exactly one member in F . Given a fama decomposition of K n
(r ,Î»)
is a decomposition F such that every
ily Î¦ of edge-r-colored digraphs, a Î¦ -decomposition of K n
graph F âˆˆ F is isomorphic to some graph G âˆˆ Î¦ . For a vertex x of an edge-r-colored digraph G, the
degree-vector of x is the 2r-vector d(x) = (in1 (x), out1 (x), in2 (x), out2 (x), . . . , inr (x), outr (x)), where
in j (x) and out j (x) denote the indegree and outdegree of vertex x in the spanning subgraph of G by

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440â€“1449

1443

edges of color j, respectively, for 1  j  r. We denote by Î± (G ) the greatest common divisor of the
integers t such that the 2r-vector (t , t , . . . , t ) is an integral linear combination of the vectors d(x) as
x ranges over the vertex set V (G ) of G. Equivalently, Î± (G ) is the smallest positive integer t 0 such
that (t 0 , t 0 , . . . , t 0 ) is an integral linear combination of the vectors {d(x)}. Let Î¦ be a family of simple
edge-r-colored digraphs and let Î± (Î¦) denote the greatest common divisor of the integers t such that
the 2r-vector (t , t , . . . , t ) is an integral linear combination of the vectors {d(x)} as x ranges over all
vertices of all graphs in Î¦ . For each graph G âˆˆ Î¦ , let Î¼(G ) = (m1 , m2 , . . . , mr ), where mi is the number of edges of color i in G. We denote by Î²(Î¦) the greatest common divisor of the integers m such
that (m, m, . . . , m) is an integral linear combination of the vectors {Î¼(G ): G âˆˆ Î¦}. Equivalently, Î²(Î¦)
is the smallest positive integer m0 such that (m0 , m0 , . . . , m0 ) is an integral linear combination of the
(r ,Î»)
vectors {Î¼(G )}. A graph G 0 âˆˆ Î¦ is useless when it cannot occur in any Î¦ -decomposition of K n .
Î¦ is admissible when no member of Î¦ is useless.
Theorem 2.3. (See [16].) Let Î¦ be an admissible family of simple edge-r-colored digraphs. For all suï¬ƒciently
(r ,Î»)
large n satisfying Î»(n âˆ’ 1) â‰¡ 0 (mod Î± (Î¦)) and Î»n(n âˆ’ 1) â‰¡ 0 (mod Î²(Î¦)), a Î¦ -decomposition of K n
exists.
Theorem 2.3 has numerous consequences for the existence of various classes of combinatorial
designs. Building on Theorem 2.3, Liu establishes the following:
Theorem 2.4. (See [17].) Let K be a set of integers, each at least 2.
nLet m and Î» be positive integers. For all
suï¬ƒciently large n satisfying Î»m(n âˆ’ 1) â‰¡ 0 (mod Î± ( K )) and Î»m2 2 â‰¡ 0 (mod Î²( K )), there exists a ( K , Î»)GDD of order mn .
MohÃ¡csy and Ray-Chaudhuri prove a result for a ï¬xed number of groups when the index is 1.
Theorem 2.5. (See [18,19].) Let k and u be integers with u  k  2. For all suï¬ƒciently large m satisfying
m(u âˆ’ 1) â‰¡ 0 (mod k âˆ’ 1) and m2 u (u âˆ’ 1) â‰¡ 0 (mod k(k âˆ’ 1)), there exists a k-GDD of type mu .
This subsumes a classical result of Chowla, ErdoÌ‹s, and Straus:
Theorem 2.6. (See [4].) Let k  2 be an integer. For all suï¬ƒciently large m, there exists a TD(k, m).
3. Packings, coverings, and the optima
We use known asymptotic existence results to treat asymptotic existence of packings and coverings
in the cases that a k-BIBD does not exist. We require further deï¬nitions, to extend packings and
coverings to have a â€˜holeâ€™.
A packing with blocksize k with a hole ( V , H , B ) is a set V of elements, a subset (hole) H âŠ‚ V , and
 H , there is at most one B âˆˆ B with
a set B of k-subsets of V , so that for every {x, y } âŠ‚ V , {x, y } âŠ‚
{x, y } âŠ‚ B; when {x, y } âŠ‚ H , there is no block B âˆˆ B with {x, y } âŠ‚ B. The leave Î“ of ( V , B ) is a graph
with vertex set V ; pair {x, y } appears as an edge if and only if {x, y }  H and is not a subset of any
block of B .
A covering with blocksize k with a hole ( V , H , B ) is a set V of elements, a subset H âŠ‚ V , and a set B
of k-subsets of V , so that for every {x, y } âŠ‚ V , {x, y } âŠ‚ H , there is at least one B âˆˆ B with {x, y } âŠ‚ B.
The excess Î“ of ( V , B ) is a multigraph with vertex set V ; the number of times pair {x, y } appears
as an edge is exactly Î»xy when {x, y } âŠ‚ H , and Î»xy âˆ’ 1 otherwise, where Î»xy is the number of blocks
of B that contain {x, y }.
A packing with blocksize k ( V , B ) is a packing with blocksize k with a hole ( V , âˆ…, B ), and a covering
with blocksize k ( V , B ) is a covering with blocksize k with a hole ( V , âˆ…, B ). A maximum packing with
blocksize k is a packing with blocksize k ( V , B ) with the most blocks among all packings with blocksize k on | V | elements; equivalently, its leave has the fewest edges. A minimum covering with blocksize

1444

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440â€“1449

k is a covering with blocksize k ( V , B ) with fewest blocks among all coverings with blocksize k on | V |
elements; equivalently, its excess has the fewest edges.
Suppose that ( V , H , B ) is a packing with blocksize k with a hole, with v = | V |, h = | H |, and
n = | V \ H |. Let x be a vertex in V \ H . The number of pairs on V that contain x is congruent to v âˆ’ 1
modulo k âˆ’ 1. The number containing x that appear in blocks of B is congruent to 0 modulo k âˆ’ 1.
Hence x has degree congruent to v âˆ’ 1 modulo k âˆ’ 1 in the leave. When the hole is nonempty,
elements in the hole have degrees congruent to n modulo k âˆ’ 1 in the leave. By the same token, in
the excess of a covering with blocksize k with a hole, x has degree congruent to âˆ’( v âˆ’ 1) modulo
k âˆ’ 1; elements in the hole have degrees congruent to âˆ’n modulo k âˆ’ 1.
We employ speciï¬c types of packings and coverings with holes in which the leave or excess has
all vertices in the hole of degree 0. For an integer n â‰¡ 0 (mod k(k âˆ’ 1)) and an integer h  1, let
Î´ â‰¡ h âˆ’ 1 (mod k âˆ’ 1) and  â‰¡ âˆ’(h âˆ’ 1) (mod k âˆ’ 1) with 0  Î´,  < k âˆ’ 1. Then an optimum packing
with blocksize k with a hole, k-OP(n + h, h), is a packing with blocksize k on n + h elements whose
leave has degree Î´ on each vertex not in the hole, and 0 on each vertex in the hole; and an optimum
covering with blocksize k with a hole, k-OC(n + h, h), is a covering with blocksize k on n + h elements
whose excess has degree  on each vertex not in the hole, and 0 on each vertex in the hole. When
h â‰¡ 1 (mod k âˆ’ 1), Î´ =  = 0. In this case, a k-OP( v , h) and a k-OC( v , h) are the same, and are
equivalent to a k-IPBD( v , h).
In any packing with blocksize k on v = n + h elements with n â‰¡ 0 (mod k(k âˆ’ 1)), no vertex
can have degree smaller than Î´ in the leave; and in any covering with blocksize k on v = n + h
elements with n â‰¡ 0 (mod k(k âˆ’ 1)), no vertex can have degree smaller than  in the excess. Indeed,
choosing  and Î› so that   v âˆ’ 1  Î›; , Î› â‰¡ v âˆ’ 1 (mod k âˆ’ 1); Î› âˆ’  < k âˆ’ 1; and  = Î› when
v
v â‰¡ 1 (mod k âˆ’ 1), every packing with blocksize k on v elements contains at most  v ,k = k(kâˆ’
1)
v
blocks, while every covering with blocksize k on v elements contains at least L v ,k = k(Î›
blocks.
kâˆ’1)
Then  v ,k is at least the Johnson bound, and Î› v ,k is at most the SchÃ¶nheim bound.
The purpose of this paper is to prove the following two results.

Theorem 3.1. There is a constant pk such that for all v  k, the number of blocks in a maximum packing with
blocksize k on v elements is at least  v ,k âˆ’ pk and at most  v ,k .
Theorem 3.2. There is a constant ak such that for all v  k, the number of blocks in a minimum covering with
blocksize k on v elements is at least L v ,k and at most L v ,k + ak .
We establish these results in a number of steps. Treating an arbitrary but ï¬xed value of k, in
Section 4, we show that for every c satisfying 0  c < k(k âˆ’ 1), there exist positive integers nc â‰¡
0 (mod k(k âˆ’ 1)) and hc â‰¡ c (mod k(k âˆ’ 1)) so that a k-OP(nc + hc , hc ) exists; we also show that
for every c satisfying 0  c < k(k âˆ’ 1), there exist positive integers mc â‰¡ 0 (mod k(k âˆ’ 1)) and c â‰¡
c (mod k(k âˆ’ 1)) so that a k-OC(mc + c , c ) exists. This provides a single example for optimal packings
and coverings with a hole in every congruence class modulo k(k âˆ’ 1). In Section 5, we use these
results to establish that there exist integers Îºk and uk , depending only on k, so that whenever v  Îºk ,
there exists an h  uk for which a k-OP( v , h) exists, and there also exists an   uk for which a
k-OC( v , ) exists. From this, because uk is ï¬xed and independent of v, we establish Theorems 3.1
and 3.2 by ï¬lling the holes. The crucial step, particularly for coverings, is producing one example in
each congruence class. We treat this next.
4. One example in each congruence class
In the case when h â‰¡ 1 (mod k âˆ’ 1), a k-OP( v , h) and a k-OC( v , h) coincide with a k-IPBD( v , h),
so we treat this situation ï¬rst; subsequently the packing and covering cases differ.
4.1. Packing and covering: v â‰¡ 1 (mod k âˆ’ 1)
An incomplete transversal design ITD(k, n + Ï†; Ï†) is a set V of k(n + Ï†) elements, of which kÏ† form
a hole H . The elements are partitioned into k groups G 1 , . . . , G k so that |G i âˆ© H | = Ï† for 1  i  k.

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440â€“1449

1445

This set is equipped with a set of k-subsets (blocks) with the property that every pair of elements
that appears in a group or appears in the hole H appears in no block, and every other pair appears
in exactly one block.
Lemma 4.1. Let k  2 be an integer. Let 0  Ï†  k. For all suï¬ƒciently large n, an ITD(k, n + Ï†; Ï†) exists.
Proof. Using Theorem 2.6, choose Ï‰ so that a TD(k + 1, Ï‰), a TD(k + 1, Ï‰ + 1), a TD(k + 1, Ï‰ + 2),
and a TD(k + 1, Ï‰ + 3) all exist. Delete one group in each to form an idempotent TD(k, v ) for each
v âˆˆ {Ï‰, Ï‰ + 1, Ï‰ + 2, Ï‰ + 3}. For n suï¬ƒciently large, there is an {Ï‰ + 1, Ï‰ + 2, Ï‰ + 3}-PBD of order
n + Ï‰ + 1 containing a block of size Ï‰ + 1 by Theorem 2.2. (Because Î± ({Ï‰ + 1, Ï‰ + 2, Ï‰ + 3}) = 1 and
Î²({Ï‰ + 1, Ï‰ + 2, Ï‰ + 3}) = 1, this follows by choosing 0 < Îµ < 14 and choosing the fraction of pairs
in blocks of size Ï‰ + 1 to be 2Îµ .) Delete all but Ï† elements from a block of size Ï‰ + 1, and remove
the block of size Ï† making a hole, to form an {Ï‰, Ï‰ + 1, Ï‰ + 2, Ï‰ + 3}-IPBD(n + Ï†, Ï†). Give every
element weight k, and use the idempotent TDs to inï¬‚ate all blocks. The kÏ† elements arising from the
Ï† elements of hole in the IPBD form the hole of the ITD. 2
Lemma 4.2. Let h be an integer for which h â‰¡ 1 (mod k âˆ’ 1) and k  h  k(k âˆ’ 1) + 1. Then there exist
inï¬nitely many integers Î³ for which a k-IPBD(Î³ k(k âˆ’ 1) + h, h) exists.
âˆ’k
. Choose Î³ so that a k-BIBD of order Î³ (k âˆ’ 1) + k and an ITD(k, Î³ (k âˆ’ 1) + Ï†; Ï†)
Proof. Let Ï† = khâˆ’
1
both exist. (Use Lemma 4.1 for the existence of the ITD.) Start with the ITD on the elements of V
having a hole on the elements in H âŠ‚ V . Add k âˆ’ Ï† new elements N âˆ . For 1  i  k, let N i consist of
the Ï† elements in the ith group of the ITD that appear in H . Place on the elements of the ith group,
together with N âˆ , the blocks of a copy of the k-BIBD, omitting a block on the elements of N i âˆª N âˆ .
On the Î³ k(k âˆ’ 1) + Ï†(k âˆ’ 1) + k = Î³ k(k âˆ’ 1) + h elements of V âˆª N âˆ , all pairs are covered except
k
those within the hole on elements N âˆ âˆª i =1 N i of size h = Ï†(k âˆ’ 1) + k. 2

Corollary 4.3. Whenever c â‰¡ 1 (mod k âˆ’ 1) and 0  c < k(k âˆ’ 1), there are inï¬nitely many integers nc and hc
with nc â‰¡ 0 (mod k(k âˆ’ 1)) and hc â‰¡ c (mod k(k âˆ’ 1)) so that a k-OP(nc + hc , hc ) exists.
Proof. Set hc = c if c  k, and hc = k(k âˆ’ 1) + 1 if c = 1. Apply Lemma 4.2 with h = hc , and set
nc = Î³ k(k âˆ’ 1). 2
The same argument establishes:
Corollary 4.4. Whenever c â‰¡ 1 (mod k âˆ’ 1) and 0  c < k(k âˆ’ 1), there are inï¬nitely many integers mc and c
with mc â‰¡ 0 (mod k(k âˆ’ 1)) and c â‰¡ c (mod k(k âˆ’ 1)) so that a k-OC(mc + c , c ) exists.
4.2. Packing: v â‰¡ 1 (mod k âˆ’ 1)
Lemma 4.5. For every integer c satisfying 0  c < k(k âˆ’ 1), there exist an nc â‰¡ 0 (mod k(k âˆ’ 1)) and an
hc â‰¡ c (mod k(k âˆ’ 1)) for which a k-OP(nc + hc , hc ) exists.
Proof. When c > 0, write c = s(k âˆ’ 1) + d with 1  d < k. When c = 0, set s = d = k âˆ’ 1. If d = 1,
apply Lemma 4.3. Otherwise choose Î± â‰¡ 1 (mod k(k âˆ’ 1)) and N > Î± so that
N â‰¡ Î± (mod k âˆ’ 1);
a k-GDD of type dÎ± exists (Theorem 2.4);
an Î± -BIBD of order N exists (Theorem 2.1); and
an ITD(k, d( N âˆ’ Î± ) + s, s) exists (Lemma 4.1).
Treat the Î± -BIBD as an Î± -GDD of type 1 N âˆ’Î± Î± 1 by removing a block, and inï¬‚ate using the kGDD of type dÎ± to form a k-GDD of type d N âˆ’Î± (dÎ± )1 . Adjoin dÎ± âˆ’ s inï¬nite elements to the

1446

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440â€“1449

ITD(k, d( N âˆ’ Î± ) + s, s). On each group together with the inï¬nite elements, place a copy of the kGDD of type d N âˆ’Î± (dÎ± )1 , aligning the group of size dÎ± on the s elements in the intersection of
the group and the hole of the ITD, together with the dÎ± âˆ’ s inï¬nite elements. The result is a kGDD of type dk( N âˆ’Î± ) (dÎ± + s(k âˆ’ 1))1 . Treat this as a packing. On the dk( N âˆ’ Î± ) points not in the
large hole, the leave has degree d âˆ’ 1, so the result is a k-OP(nc + hc , hc ) with nc = dk( N âˆ’ Î± ) and
hc = dÎ± + s(k âˆ’ 1). Because dk â‰¡ 0 (mod k) and N âˆ’ Î± â‰¡ 0 (mod k âˆ’ 1), nc â‰¡ 0 (mod k(k âˆ’ 1)). Because
dÎ± â‰¡ d (mod k(k âˆ’ 1)), hc â‰¡ c (mod k(k âˆ’ 1)). 2
4.3. Covering: v â‰¡ 1 (mod k âˆ’ 1)
We employ some further, more specialized, combinatorial objects to treat coverings for the remaining congruence classes.
Let V be a set of elements; B be a set of k-subsets of V ; G = {G 1 , . . . , G r } be a partition of V ,
and H = { H 1 , . . . , H t } be a partition of V . Suppose that |G i âˆ© H j | = Î¼ for all 1  i  r, 1  j  t.

r

G 

t

 H 

j
i
Further suppose that for every 2-subset {x, y } âŠ‚ V , either {x, y } âˆˆ
, or there
âˆª
i =1 2
j =1 2
is exactly one B âˆˆ B with {x, y } âŠ‚ B, but not both. Then ( V , G , H, B ) is a double group divisible design
with blocksize k (k-DGDD) of type (Î¼r )t . A holey transversal design with blocksize k (k-HTD) of type Î¼r
is a k-DGDD of type (Î¼r )k .

Theorem 4.6. Let k  2 be an integer. For all suï¬ƒciently large r, there exists a k-HTD of type 2r .
Proof. Choose K = {x1 , . . . , xs } so that Î± ( K ) = Î²( K ) = 1, and so that for each 1  i  s, xi is large
enough to ensure that Theorem 2.6 yields a TD(k + 1, xi ). Remove one group (and rename elements
as needed) to form an idempotent TD(k, xi ). When r is large enough, Theorem 2.4 yields a K -GDD
( V , G , B ) of type 2r with groups G = {G 1 , . . . , G r }. The elements of the k-HTD to be formed are
V Ã— {0, . . . , k âˆ’ 1}. For each block B âˆˆ B , on the elements B Ã— {0, . . . , k âˆ’ 1}, align the k groups on
{ B Ã— {i }: 0  i < k} to place the blocks of an idempotent TD(k, | B |). In the resulting design, one set of
groups is formed by V Ã— {i } for 0  i < k, the other by G j Ã— {0, . . . , k âˆ’ 1} for 1  j  r. 2
Theorem
 t  4.7.
 Let k
2 be an integer. For all suï¬ƒciently large integers r and t satisfying t âˆ’ 1 â‰¡ 0 (mod k âˆ’ 1)
k
and 2 â‰¡ 0 mod 2 , there exists a k-DGDD of type (2r )t .
Proof. Apply Theorem 2.1 to form a k-BIBD ( V , B ) with t elements. Apply Theorem 4.6 to form a
k-HTD of type 2r . To form the k-DGDD, use elements V Ã— {a, b} Ã— {1, . . . , r }. For every B âˆˆ B , place a
copy of the HTD on B Ã— {a, b} Ã— {1, . . . , r }, aligning groups of size 2k on B Ã— {a, b} Ã— {i } for 1  i  r,
and groups of size 2r on {x} Ã— {a, b} Ã— {1, . . . , r } for x âˆˆ B. 2
The key construction follows:
Theorem 4.8. Let t , r , y be positive integers so that r â‰¡ 0 (mod k(k âˆ’ 1)), t â‰¡ 1 (mod k(k âˆ’ 1)), and y â‰¡
2 (mod k âˆ’ 1). Suppose that there exist
(1) a k-DGDD of type (2r )t ;
(2) a k-BIBD on 2t + k âˆ’ 2 elements;
(3) a k-OC(2r + y , y ).
Then there is a k-OC(2rt + k âˆ’ 2 + y , 2r + y + k âˆ’ 2).
Proof. Let V = {ai , j , b i , j : 1  i  r , 1  j  t } be the elements of the k-DGDD, with groups aligned
so that G i = {ai , j , b i , j : 1  j  t } and H j = {ai , j , b i , j : 1  i  r }. Let B be its set of blocks. Adjoin a
set C of k âˆ’ 2 new elements. For 1  i  r, on C âˆª G i , form a k-BIBD on 2t + k âˆ’ 2 elements, aligning
a block on C âˆª {ait , b it }; then delete that block, and call the resulting set of blocks Di . Adjoin a set R

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440â€“1449

1447

with y further new elements. For 1  j < t, on R âˆª H j place a k-OC(2r + y , y ) with the hole aligned
on R, whose block set is E j .
r
We consider the design on the 2rt + k âˆ’ 2 + y elements V âˆª R âˆª C with block set B âˆª i =1 Di âˆª

t âˆ’1

j =1 E j . All blocks have size k because each ingredient contains only blocks of size k. First we show
that the design is a covering with a hole on R âˆª C âˆª H t . Two elements in the hole do not appear
together in a block. An element from G i âˆ© H j with j < t appears in a block with each element
of C âˆª (G i âˆ© H t ) in Di ; it appears in a block with each element of R in E j ; and it appears with
each element of H t \ G i in a block of B . Consider two distinct elements x âˆˆ G i âˆ© H j and y âˆˆ G m âˆ© H n
with j , n < t. If i = m and j = n, then {x, y } = {ai , j , b i , j } appears in a block of Di (and also in at least
one block of E j ). If i = m and j = n, then {x, y } appears in at least one block of E j . If i = m and j = n,
then {x, y } appears in one block of Di . If i = m and j = n, then {x, y } appears in one block of B .
Hence the design is a covering with a hole on R âˆª C âˆª H t .
Secondly, we establish that it has the correct excess degrees to be an optimal covering with a
hole, a k-OC. The design has 2rt + k âˆ’ 2 + y elements. Because r â‰¡ 0 (mod k âˆ’ 1), the number of
elements satisï¬es 2rt + k âˆ’ 2 + y â‰¡ y âˆ’ 1 (mod k âˆ’ 1). The hole has 2r + k âˆ’ 2 + y elements. Because
r â‰¡ 0 (mod k âˆ’ 1), the number of elements in the hole satisï¬es 2r + k âˆ’ 2 + y â‰¡ y âˆ’ 1 (mod k âˆ’ 1).
Let y â‰¡ âˆ’( y âˆ’ 1) (mod k âˆ’ 1) with 0  y < k âˆ’ 1. We must show that every element not in the
hole has degree y + 1 in the excess, and every element in the hole has degree 0 in the excess.
We treat elements in the hole ï¬rst. Each element of C appears only in blocks {Di : 1  i  r }. It
appears in r (t âˆ’ 1) pairs to be covered, and appears in r (t âˆ’ 1)/(k âˆ’ 1) blocks, with (t âˆ’ 1)/(k âˆ’ 1)
blocks arising in each of {Di : 1  i  r } because this was constructed from a BIBD. Each element
of R appears only in blocks {E j : 1  j < t }. Because elements of R have excess degree 0 in the kOC(2r + y , y ) forming E j , they have excess degree 0 in the union. Each element of H t appears only
in blocks of B , and has excess degree 0. Now consider an element x âˆˆ G i âˆ© H j , with j = t so that x
is not in the hole. Then x appears in elements of B , Di and E j . It appears in 2(r âˆ’ 1)(t âˆ’ 1)/(k âˆ’ 1)
blocks of B , because it arises from the DGDD. It appears in (2t + k âˆ’ 3)/(k âˆ’ 1) blocks of Di , because
it arises from a BIBD. Now in E j , x is not in the hole of the k-OC(2r + y , y ), and hence it arises
1
(2rt + k âˆ’ 2 + y + y ) blocks, and
in (2r âˆ’ 1 + y + y )/(k âˆ’ 1) blocks. So in total x appears in kâˆ’
1
because it appears in (2rt + k âˆ’ 2 + y ) âˆ’ 1 pairs, its excess degree is y + 1.
Because 2r + y + k âˆ’ 2 â‰¡ y âˆ’ 1 (mod k âˆ’ 1) and y â‰¡ 2 (mod k âˆ’ 1), the result is the k-OC(2rt +
k âˆ’ 2 + y , 2r + y + k âˆ’ 2). 2

Corollary 4.9. For each 0  c < k(k âˆ’ 1), there exist integers mc and c with mc â‰¡ 0 (mod k(k âˆ’ 1)) and
c â‰¡ c (mod k(k âˆ’ 1)) for which a k-OC(mc + c , c ) exists.
Proof. Let t 0 and r0 be integers with t 0 â‰¡ 1 (mod k(k âˆ’ 1)) and t 0 > 1 so that whenever r  r0 , t  t 0 ,
and t â‰¡ 1 (mod k(k âˆ’ 1)),
(1) there is a k-DGDD of type (2r )t (apply Theorem 4.7), and
(2) there is a k-BIBD on 2t + k âˆ’ 2 (â‰¡ k (mod k(k âˆ’ 1))) elements (apply Theorem 2.1).
When c â‰¡ 1 (mod k âˆ’ 1), apply Corollary 4.4 to choose one k-OC(mc + c , c ) with mc â‰¡
0 (mod k(k âˆ’ 1)) and mc  r0 . In general, when a k-OC(mc + c , c ) with mc â‰¡ 0 (mod k(k âˆ’ 1)) and
mc  r0 exists, Theorem 4.8 produces a k-OC(mc t 0 + k âˆ’ 2 + c , mc + c + k âˆ’ 2). Set mc +kâˆ’2 mod k(kâˆ’1) =
mc (t 0 âˆ’ 1), which exceeds r0 and is a multiple of k(k âˆ’ 1). Set c +kâˆ’2 mod k(kâˆ’1) = mc + c + k âˆ’ 2 â‰¡
c + k âˆ’ 2 (mod k(k âˆ’ 1)). Then k âˆ’ 2 applications of Theorem 4.8 handle all congruence classes. 2
5. Asymptotic existence
Our next task is to handle not just one example for hole size in each congruence class modulo
k(k âˆ’ 1), but to extend to all suï¬ƒciently large orders.
Theorem 5.1. Let k  2 be an integer. Then there are constants Îºk and uk so that whenever v  Îºk , there is a
k-OP( v , h) and a k-OC( v , h) with h  uk .

1448

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440â€“1449

Proof. By Corollary 4.9, for 0  c < k(k âˆ’ 1) there are integers mc â‰¡ 0 (mod k(k âˆ’ 1)) and c â‰¡
c (mod k(k âˆ’ 1)) for which a k-OC(mc + c , c ) exists. By Lemma 4.5, for 0  c < k(k âˆ’ 1) there are
integers nc â‰¡ 0 (mod k(k âˆ’ 1)) and hc â‰¡ c (mod k(k âˆ’ 1)) for which a k-OP(nc + hc , hc ) exists. Set
uk = max{nc + hc , mc + c : 0  c < k(k âˆ’ 1)}.

Using Theorem 2.4, choose an integer x for which k-GDDs of type (k(k âˆ’ 1))x exist for all x âˆˆ
{x, x + 1, x + 2, x + 3}. Again using Theorem 2.4, choose an integer r for which an {x + 1, x + 2, x + 3}GDD of type p g exists for all 1  p  uk and all g  r. Then set Îºk = rk(k âˆ’ 1) + uk , a constant
depending only on k.
We develop the remainder of the proof for packings; that for coverings parallels it very closely. Let
v  Îºk be an integer, and write v = Ï† k(k âˆ’ 1) + c with 0  c < k(k âˆ’ 1). Write v âˆ’ hc = gnc + d so that
d â‰¡ 0 (mod k(k âˆ’ 1)) and d < nc . Let n = nc /(k(k âˆ’ 1)) and d = d/(k(k âˆ’ 1)).
g
Construct a k-GDD of type nc d1 as follows. Form an {x + 1, x + 2, x + 3}-GDD of type (n ) g +1 .

Delete all but d elements in one group to form an {x, x + 1, x + 2, x + 3}-GDD of type (n ) g (d )1 .

Inï¬‚ate using weight k(k âˆ’ 1), employing k-GDDs of type (k(k âˆ’ 1))x for x âˆˆ {x, x + 1, x + 2, x + 3}, to
g 1
form a k-GDD of type nc d . Then add hc new elements, and place a k-OP(nc + hc , hc ) on each group
of size nc together with the hc new elements, aligning the hole on these hc elements. The result is a
k-OP( v , hc + d), and hc + d  uk as required. 2

Îº 

Proof of Theorem 3.1. When v < Îºk , a maximum packing with blocksize k contains at least  v ,k âˆ’ 2k
Îº 
blocks, and 2k is a constant. When v  Îºk , form a k-OP( v , h) with h  uk , which has at least  v ,k âˆ’
u 
u k 
blocks and 2k is a constant. 2
2

Îº 

Proof of Theorem 3.2. When v < Îºk , a minimum covering with blocksize k requires at most 2k
blocks, which is a constant. When v  Îºk , form a k-OC( v , h) with h  uk , which has at most L v ,k
blocks. A covering
on h points in which every block contains some pair that is covered only once has

u
at most 2k blocks, which is a constant independent of v. Use this to ï¬ll the hole. 2
6. Conclusion

For t = 2, our results establish that the elementary Johnson and SchÃ¶nheim bounds are essentially
the correct ones, in that the respective optima cannot differ from them by more than an additive
constant. Unless this constant can be shown to be quite small, the speciï¬c value obtained for the
constant is not of particular interest. Without recourse to Proposition 1.1 or a similar statement, we
see no way at present to obtain differences from the bounds that are bounded by a quantity as small
as (say) k in general, although it is plausible that such bounds hold.
Acknowledgment
We thank an anonymous referee for helpful comments on the presentation.
References
[1] T. Beth, D. Jungnickel, H. Lenz, Design Theory, vol. I, second edition, Encyclopedia Math. Appl., vol. 69, Cambridge University
Press, Cambridge, 1999.
[2] Y. Caro, R. Yuster, Packing graphs: the packing problem solved, Electron. J. Combin. 4 (1) (1997), Research Paper 1, approx.
7 pp. (electronic).
[3] Y. Caro, R. Yuster, Covering graphs: the covering problem solved, J. Combin. Theory Ser. A 83 (2) (1998) 273â€“282.
[4] S. Chowla, P. ErdoÌ‹s, E.G. Straus, On the maximal number of pairwise orthogonal Latin squares of a given order, Canad. J.
Math. 12 (1960) 204â€“208.
[5] C.J. Colbourn, J.H. Dinitz, D.R. Stinson, Applications of combinatorial designs to communications, cryptography, and networking, in: Surveys in Combinatorics, Canterbury, 1999, in: London Math. Soc. Lecture Note Ser., vol. 267, Cambridge
Univ. Press, Cambridge, 1999, pp. 37â€“100.
[6] C.J. Colbourn, V. RÃ¶dl, Percentages in pairwise balanced designs, Discrete Math. 77 (1â€“3) (1989) 57â€“63.
[7] C.J. Colbourn, P.C. van Oorschot, Applications of combinatorial designs in computer science, ACM Comput. Surv. 21 (2)
(1989) 223â€“250.
[8] P. Dukes, A.C.H. Ling, Asymptotic existence of resolvable graph designs, Canad. Math. Bull. 50 (4) (2007) 504â€“518.

Y.M. Chee et al. / Journal of Combinatorial Theory, Series A 120 (2013) 1440â€“1449

[9]
[10]
[11]
[12]
[13]
[14]
[15]
[16]
[17]
[18]
[19]
[20]
[21]
[22]
[23]
[24]

[25]
[26]

1449

P. ErdoÌ‹s, H. Hanani, On a limit theorem in combinatorial analysis, Publ. Math. Debrecen 10 (1963) 10â€“13.
P. Frankl, V. RÃ¶dl, Near perfect coverings in graphs and hypergraphs, European J. Combin. 6 (4) (1985) 317â€“326.
D.A. Grable, More-than-nearly-perfect packings and partial designs, Combinatorica 19 (2) (1999) 221â€“239.
T. Gustavsson, Decompositions of large graphs and digraphs with high minimum degree, PhD thesis, Dept. of Mathematics,
Univ. of Stockholm, 1991.
S.M. Johnson, A new upper bound for error-correcting codes, IRE Trans. IT-8 (1962) 203â€“207.
A.V. Kostochka, V. RÃ¶dl, Partial Steiner systems and matchings in hypergraphs, Random Structures Algorithms 13 (3â€“4)
(1998) 335â€“347.
N.N. Kuzjurin, On the difference between asymptotically good packings and coverings, European J. Combin. 16 (1) (1995)
35â€“40.
E.R. Lamken, R.M. Wilson, Decompositions of edge-colored complete graphs, J. Combin. Theory Ser. A 89 (2) (2000) 149â€“
200.
J. Liu, Asymptotic existence theorems for frames and group divisible designs, J. Combin. Theory Ser. A 114 (3) (2007)
410â€“420.
H. MohÃ¡csy, The asymptotic existence of group divisible designs of large order with index one, J. Combin. Theory
Ser. A 118 (7) (2011) 1915â€“1924.
H. MohÃ¡csy, D.K. Ray-Chaudhuri, An existence theorem for group divisible designs of large order, J. Combin. Theory
Ser. A 98 (1) (2002) 163â€“174.
V. RÃ¶dl, On a packing and covering problem, European J. Combin. 6 (1) (1985) 69â€“78.
J. SchÃ¶nheim, On coverings, Paciï¬c J. Math. 14 (1964) 1405â€“1411.
D.R. Stinson, Combinatorial Designs, Springer-Verlag, New York, 2004.
V.H. Vu, New bounds on nearly perfect matchings in hypergraphs: higher codegrees do help, Random Structures Algorithms 17 (1) (2000) 29â€“63.
R.M. Wilson, The construction of group divisible designs and partial planes having the maximum number of lines of a
given size, in: Proc. Second Chapel Hill Conf. on Combinatorial Mathematics and its Applications, Univ. North Carolina,
Chapel Hill, NC, 1970, Univ. North Carolina, Chapel Hill, NC, 1970, pp. 488â€“497.
R.M. Wilson, An existence theory for pairwise balanced designs. III. Proof of the existence conjectures, J. Combin. Theory
Ser. A 18 (1975) 71â€“79.
R.M. Wilson, The proportion of various graphs in graph designs, in: R.A. Brualdi, S. Hedayat, H. Kharaghani, G. Khosrovshahi,
S. Shahriari (Eds.), Combinatorics and Graphs: The Twentieth Anniversary Conference of IPM Combinatorics, American
Mathematical Society, Providence, RI, 2010, pp. 251â€“255.

The construction of covering arrays with the fewest rows remains a challenging problem. Most computational and recursive constructions result in extensive repetition of coverage. While some is necessary, some is not. By reducing the repeated coverage, metaheuristic search techniques typically outperform simpler computational methods, but they have been applied in a limited set of cases. Time constraints often prevent them from finding an array of competitive size. We examine a different approach. Having used a simple computation or construction to find a covering array, we employ a post-optimization technique that repeatedly adjusts the array in an attempt to reduce its number of rows. At every stage the array retains full coverage. We demonstrate its value on a collection of previously best known arrays by eliminating, in some cases, 10% of their rows. In the well-studied case of strength two with twenty factors having ten values each, post-optimization produces a covering array with only 162 rows, improving on a wide variety of computational and combinatorial methods. We identify certain important features of covering arrays for which post-optimization is successful.SaaS (Software-as-a-Service) often uses multi-tenancy architecture (MTA) where tenant developers compose their applications online using the components stored in the SaaS database. Tenant applications need to be tested, and combinatorial testing can be used. While numerous combinatorial testing techniques are available, most of them produce static sequences of test configurations and their goal is often to provide sufficient coverage such as 2-way interaction coverage. But the goal of SaaS testing is to identify those compositions that are faulty for tenant applications. This paper proposes an adaptive test configuration generation algorithm AR (Adaptive Reasoning) that can rapidly identify those faulty combinations so that those faulty combinations cannot be selected by tenant developers for composition. The AR algorithm has been evaluated by both simulation and real experimentation using a MTA SaaS sample running on GAE (Google App Engine). Both the simulation and experiment showed show that the AR algorithm can identify those faulty combinations rapidly. Whenever a new component is submitted to the SaaS database, the AR algorithm can be applied so that any faulty interactions with new components can be identified to continue to support future tenant applications.1

Hierarchical Recovery in Compressive Sensing
Charles J. Colbourn, Daniel Horsley, and Violet R. Syrotiuk, Senior Member, IEEE

arXiv:1403.1835v1 [cs.IT] 4 Mar 2014

Abstract
A combinatorial approach to compressive sensing based on a deterministic column replacement technique is proposed.
Informally, it takes as input a pattern matrix and ingredient measurement matrices, and results in a larger measurement matrix
by replacing elements of the pattern matrix with columns from the ingredient matrices. This hierarchical technique yields great
flexibility in sparse signal recovery. Specifically, recovery for the resulting measurement matrix does not depend on any fixed
algorithm but rather on the recovery scheme of each ingredient matrix. In this paper, we investigate certain trade-offs for signal
recovery, considering the computational investment required. Coping with noise in signal recovery requires additional conditions,
both on the pattern matrix and on the ingredient measurement matrices.
Index Terms
compressive sensing, hierarchical signal recovery, deterministic column replacement, hash families

I. I NTRODUCTION
Nyquistâ€™s sampling theorem provides a sufficient condition for full recovery of a band-limited signal: sample the signal at a
rate that is twice the band-limit. However, there are cases when full recovery may be achieved with a sub-Nyquist sampling
rate. This occurs with signals that are sparse (or compressible) in some domain, such as those that arise in applications in
sensing, imaging, and communications, and has given rise to the field of compressive sensing [2], [6] (also called compressive
sampling).
Consider the following framework for compressive sensing. An admissible signal of dimension n is a vector in Rn that is
known a priori to be taken from a given set Î¦ âŠ† Rn . A measurement matrix A is a matrix from RmÃ—n . Sampling a signal
x âˆˆ Rn corresponds to computing the product Ax = b. Once sampled, recovery involves determining the unique signal x âˆˆ Î¦
that satisfies Ax = b using only A and b. If Î¦ = Rn , recovery can be accomplished only if A has rank n, and hence m â‰¥ n.
However for more restrictive admissible sets Î¦, recovery may be accomplished when m < n.
Given a measurement matrix A, an equivalence relation â‰¡A is defined so that for signals x, y âˆˆ Rn , we have x â‰¡A y
if and only if Ax = Ay. If for every equivalence class P under â‰¡A , the set P âˆ© Î¦ contains at most one signal then in
principle recovery is possible. Because Ax = Ay ensures that A(x âˆ’ y) = 0, this can be stated more simply: An equivalence
class P of â‰¡A can be represented as {x + y : y âˆˆ N (A)} for any x âˆˆ P , where N (A) is the null space of A, i.e., the set
{x âˆˆ Rn : Ax = 0}. Recoverability is therefore equivalent to requiring that, for every signal x âˆˆ Î¦, there is no y âˆˆ N (A)\{0}
with x + y âˆˆ Î¦.
In order to make use of these observations, a reasonable a priori restriction on the signals to be sampled is identified, suitable
measurement matrices with m â‰ª n are formed, and a reasonably efficient computational strategy for recovering the signal is
provided. A signal is t-sparse if at most t of its n coordinates are nonzero. The recovery of t-sparse signals is the domain
of compressive sensing. An admissible set of signals Î¦ has sparsity t when every signal in Î¦ is t-sparse. An admissible set
of signals Î¦ is t-sparsifiable if there is a full rank matrix B âˆˆ RnÃ—n for which {Bx : x âˆˆ Î¦} has sparsity t. We assume
throughout that when the signals are sparsifiable, a change of basis B is applied so that the admissible signals have sparsity t.
A measurement matrix has (â„“0 , t)-recoverability when it permits exact recovery of all t-sparse signals. A basic problem is
to design measurement matrices with (â„“0 , t)-recoverability where m â‰ª n such that recovery can be accomplished efficiently.
Suppose that measurement matrix A has (â„“0 , t)-recoverability. Then in principle, given A and b, recovery of the signal x can
be accomplished by solving the â„“0 -minimization problem min{||x||0 : Ax = b}. To do so the possible supports of signals
from fewest nonzero entries to most are first listed. For each, reduce A to Aâ€² and x to xâ€² by eliminating coordinates in the
signal assumed to be zero. Examine the now overdetermined system Aâ€² xâ€² = b. When equality holds, a solution is found; we
are guaranteed to find one by considering all possible supports
with at most t nonzero entries. Such an enumerative strategy is

prohibitively time-consuming, examining as many as nt linear systems when the signal has sparsity t. Natarajan [27] showed
that we cannot expect to find a substantially more efficient solution, because the problem is NP-hard.
Instead of the â„“0 -minimization problem, Chen, Donoho, Huo, and Saunders [11], [18] suggest considering the â„“1 -minimization
problem min{||x||1 : Ax = b}. While this can be solved using standard linear programming techniques, to be effective it
is necessary that for each t-sparse signal x, the unique solution to min{||z||1 : Az = Ax} is x. This property is (â„“1 , t)recoverability. A necessary and sufficient condition for (â„“1 , t)-recoverability has been explored, beginning with Donoho and
Huo [18] and subsequently in [19]â€“[21], [24], [30], [31], [33].
C. J. Colbourn and V. R. Syrotiuk are with the School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe,
AZ, U.S.A., 85287-8809, {colbourn,syrotiuk}@asu.edu
D. Horsley is with the School of Mathematical Sciences, Monash University, Vic 3800, Australia, daniel.horsley@monash.edu

2

A measurement matrix A meets the (â„“0 , t)-null space condition if and only if N (A) \ {0} contains no (2t)-sparse vector.
For y âˆˆ Rn and C âŠ‚ {1, . . . , n}, define y|C âˆˆ Rn to be the vector such that (y|C )Î³ = yÎ³ if Î³ âˆˆ C and (y|C )Î³ = 0
otherwise. A measurement matrix A meets the (â„“1 , t)-null space condition if and only if for every y âˆˆ N (A) \ {0} and every
C âŠ‚ {1, . . . , n} with |C| = t, ||y|C ||1 < 12 ||y||1 .
Lemma 1: ([13], for example) Measurement matrix A âˆˆ RmÃ—n has (â„“0 , t)-recoverability if and only if A meets the (â„“0 , t)null space condition.

Lemma 2: ([33], for example) Measurement matrix A âˆˆ RmÃ—n has (â„“1 , t)-recoverability if and only if A meets the (â„“1 , t)null space condition.

To establish (â„“1 , t)-recoverability, and hence also (â„“0 , t)-recoverability, CandeÌ€s and Tao [7], [9] introduced the Restricted
Isometry Property (RIP). For A âˆˆ RmÃ—n , the dth RIP parameter of A, Î´d (A), is the smallest Î´ so that, for some constant R > 0,
(1 âˆ’ Î´)R(||x||2 )2 â‰¤ (||Ax||2 )2 â‰¤ (1 + Î´)R(||x||2 )2 , for all x with ||x||0 â‰¤ d. The dth RIP parameter is better when Î´d (A)
is smaller as the bounds are tighter. The RIP parameters have been employed extensively to establish (â„“1 , t)-recoverability,
particularly for randomly generated measurement
matrices [8]â€“[10], but also for those generated using deterministic conâˆš
structions [12], [17]. Commonly, Î´2t < 2 âˆ’ 1 is required for (â„“1 , t)-recoverability; see [7] for example. The property of
(â„“1 , t)-recoverability in the presence of noise has also been considered. Conditions on the RIP parameters are sufficient but in
general not necessary for recoverability.
Combinatorial approaches to compressive sensing are detailed in [3], [16], [22], [23], [25], [26], [32]. We pursue a different
combinatorial approach here, using a deterministic column replacement technique based on hash families. The use of an
heterogeneous hash family provides an explicit hierarchical construction of a large measurement matrix from a library of
small ingredient matrices. Strengthening hash families provide a means to increase the level of sparsity supporte, allowing the
ingredient matrices to be designed for lower sparsity than the larger measurement matrix produced.
In this paper we show that the heterogeneity extends to signal recovery: it is interesting that the ingredient measurement
matrices need not all employ the same recovery algorithm. This enables hierarchical recovery for the large measurement matrix;
however, this can be computationally prohibitive. By restricting the hash family to be linear, recovery for the large measurement
matrix can be achieved in sublinear time even when computationally intensive methods are used for each ingredient matrix. To
be practical, recovery methods based on hash families must deal with noise in the signal effectively. Suitable restrictions on
the hash family and on each ingredient matrix used in the hierarchical method are shown to be sufficient to permit recovery
in the presence of noise.
The rest of this paper is organized as follows. The results on homogeneous hash families in Section II demonstrate that
a recovery scheme based on (â„“0 , t)- or (â„“1 , t)-recoverability can be â€˜liftedâ€™ from the ingredient measurement matrices to the
matrix resulting from column replacement. Section III considers a generalization of hash families to allow for ingredient
matrices with other recovery algorithms, and the computational investment to recover the signal. Signal recovery without noise
is considered first, and the conditions for a sublinear time recovery algorithm described. Section IV considers the recovery of
almost-sparse signals to deal with noise in the signal. Finally, Section V draws relevant conclusions.
II. H ASH FAMILIES

AND

C OMPRESSIVE S ENSING

A. Column Replacement and Hash Families for Compressive Sensing
Let A âˆˆ RrÃ—k , A = (aij ), be an ingredient matrix. Let P âˆˆ {1, . . . , k}mÃ—n , P = (pij ), be a pattern matrix. The columns
of A are indexed by elements of P . For each row i of P , replace element pij with a copy of column pij of A. The result is
an rm Ã— n matrix B, the column replacement of A into P . Fig. 1 gives an example of column replacement.
ï£¹
ï£®
a11 a12 a13 a11




ï£¯ a21 a22 a23 a21 ï£º
1231
a11 a12 a13
ï£º
B=ï£¯
P =
A=
ï£° a13 a11 a12 a11 ï£»
a21 a22 a23
3121
a23 a21 a22 a21

Fig. 1.

B is the column replacement of A into P .

When the ingredient matrix A is a measurement matrix that meets one of the null space conditions for a given sparsity, our
interest is to ensure that the sparsity supported by B is at least that of A. Not every pattern matrix P suffices for this purpose.
Therefore, we examine the requirements on P .
Let m, n, and k be positive integers. An hash family HF(m; n, k), P = (pij ), is an m Ã— n array, in which each cell contains
one symbol from a set of k symbols. An hash family is perfect of strength t, denoted PHF(m; n, k, t), if in every m Ã— t

3

subarray of P at least one row consists of distinct symbols; see [1], [28]. Fig. 2 gives an example of a perfect hash family
PHF(6; 12, 3, 3). For example, for the 6 Ã— 3 subarray involving columns 4, 5, and 6, only the fourth row consists of distinct
symbols.

â†’
Fig. 2.

0
0
1
2
2
2

1
2
0
0
0
0

â†“
2
0
2
1
1
2

2
1
0
1
2
1

â†“
1
2
2
2
2
1

â†“
2
2
2
0
1
1

2
2
1
2
0
2

0
1
1
0
2
2

1
0
2
1
2
0

1
1
1
1
1
1

0
2
0
2
1
2

0
1
2
1
0
1

A perfect hash family PHF(6; 12, 3, 3).

A perfect hash family has at least one row that separates the t columns into t parts in every m Ã— t subarray. A weaker
conditionP
separates the t columns into classes. A {w1 , . . . , ws }-separating hash family, denoted SHF(m; n, k, {w1 , . . . , ws }),
s
with t = i=1 wi , is an m Ã— n array on k symbols in which for every m Ã— t subarray, and every way to partition the t columns
into classes of sizes w1 , . . . , ws , there is at least one row in which no two classes contain the same symbol; see [4], [29]. A
W-separating hash family, denoted SHF(m; n, k, W), is a {w1 , . . . , ws }-separating hash family for each {w1 , . . . , ws } âˆˆ W.
Fig. 3 gives an example of a {1, 2}-separating hash family SHF(3; 16, 4, {1, 2}). For the 3 Ã— 3 subarray consisting of columns
11, 15, and 16, for example, the last row separates columns {11, 16} from column {15}.

â†’
Fig. 3.

1
1
1

1
2
2

1
3
3

1
4
4

2
1
2

2
2
1

2
3
4

2
4
3

3
1
3

â†“
3
3
1

3
2
4

3
4
2

4
1
4

4
2
3

â†“
4
3
2

â†“
4
4
1

A {1, 2}-separating hash family SHF(3; 16, 4, {1, 2}).

Ps
A distributing hash family DHF(m; n, k, t, s) is an SHF(m; n, k, W) with W = {{w1 , . . . , ws } : t = i=1 wi }. Fig. 4
gives an example of a DHF(10; 13, 9, 5, 2). For the 10 Ã— 5 subarray consisting of columns 8 through 12, row 4 separates
columns {8, 9, 10, 11} from column {12} (a {1, 4}-separation), and row 5 separates columns {8, 9, 12} from columns {10, 11}
(a {2, 3}-separation).

a {1, 4}-separation â†’
a {2, 3}-separation â†’

Fig. 4.

6
3
8
0
0
1
1
1
0
0

7
1
5
2
0
1
0
1
0
0

8
1
1
0
2
2
1
0
3
0

3
7
4
2
1
2
2
1
0
0

4
2
2
2
1
2
0
0
1
0

0
6
3
0
1
0
0
4
0
1

2
8
2
0
2
1
2
2
0
0

â†“
2
4
6
1
0
0
0
0
2
0

â†“
3
3
7
1
0
0
0
2
4
1

â†“
0
0
0
1
2
2
1
0
0
0

â†“
5
2
1
1
2
1
2
1
0
0

â†“
1
0
3
2
0
0
2
0
1
0

1
5
0
0
1
0
1
2
0
1

A distributing hash family DHF(10; 13, 9, 5, 2).

Now, we are in a position to state the requirements on a pattern matrix P that ensure that the sparsity supported by the
matrix B resulting from column replacement is at least that of A.
Theorem 1: [13] Suppose that A is an r Ã— k measurement matrix that meets the (â„“0 , t)-null space condition, that P is an
SHF(m; n, k, {1, t}), and that B is the column replacement of A into P . Then B is an rm Ã— n measurement matrix that meets
the (â„“0 , t)-null space condition.

Theorem 2: [13] Suppose that A is an r Ã— k measurement matrix that meets the (â„“1 , t)-null space condition, that P is a
DHF(m; n, k, t + 1, 2), and that B is the column replacement of A into P . Then B is an rm Ã— n measurement matrix that
meets the (â„“1 , t)-null space condition.

4

B. Exploiting Heterogeneity in Column Replacement
All the standard definitions of hash families may be generalized by replacing k by k = (k1 , . . . , km ), a tuple of positive
integers. Now, an heterogeneous hash family HF(m; n, k), P = (pij ), is an m Ã— n array in which each cell from row i contains
one symbol from a set of ki symbols, 1 â‰¤ i â‰¤ m.
Column replacement may be extended to exploit heterogeneity in an hash family. Let P = (pij ) be an HF(m; n, k) and, for
1 â‰¤ i â‰¤ m, let Ai be an ri Ã— ki ingredient matrix whose columns are indexed by the ki elements P
in row i of P . For each
row i of P , replace the element pij with a copy of column pij of Ai , 1 â‰¤ j â‰¤ n. The result is a ( m
i=1 ri ) Ã— n matrix B,
the column replacement of A1 , . . . , Am into P . Fig. 5 gives an example of column replacement using an heterogeneous hash
family.

P =

Fig. 5.



132123
111222



A1 =



a111 a112 a113
a121 a122 a123

B is the column replacement of A1 , A2 into P .



A2 =



a211 a212
a221 a222



ï£¹
a111 a113 a112 a111 a112 a113
ï£¯ a121 a123 a122 a121 a122 a123 ï£º
ï£º
B=ï£¯
ï£° a211 a211 a211 a212 a212 a212 ï£»
a221 a221 a221 a222 a222 a222
ï£®

An hierarchical method for compressive sensing is obtained using column replacement in an heterogeneous hash family.
Suppose that Ai is a measurement matrix for a signal of dimension ki supporting the recovery of sparsity qi , for 1 â‰¤ i â‰¤ m.
We now describe the properties the pattern matrix needs to satisfy to support recovery of signals of dimension n and sparsity
t.
In Section II-A, we saw that a perfect hash family separates t columns into t parts, and that a separating hash family
separates t columns into classes. We now define a particular type of separating hash family in which the number of symbols
used to accomplish the separations is restricted.
Let d = (d1 , . . . , dm ) be a tuple of positive integers, and let Ï„ be a positive P
integer. Let W = {W1 , . . . , Wr }, where for
si
1 â‰¤ i â‰¤ r, Wi = {wi1 , . . . , wisi } is a multiset of nonnegative integers, and Ïƒi = j=1
wij . An SHF(m; n, k, W), P = (pij ),
is (d, Ï„ )-strengthening if whenever 1 â‰¤ i â‰¤ r,
â€¢ C is a set of Ïƒi columns,
â€¢ C1 , . . . , Csi is a partition of C with |Cj | = wij for 1 â‰¤ j â‰¤ si , and
â€¢ T is a set of Ï„ columns with |C âˆ© T | = min(Ïƒi , Ï„ ),
there exists a row Ï for which pÏx 6= pÏy whenever x âˆˆ Ce , y âˆˆ Cf and e 6= f and the multiset {pÏx : x âˆˆ T } contains no
more than dÏ different symbols. When Ï„ = max{Ïƒi : 1 â‰¤ i â‰¤ r}, we omit Ï„ and write d-strengthening. Because rows of P
can be arbitrarily permuted (while permuting the ingredient matrices in the same manner), the order of elements in k and d is
inconsequential. Hence we often use exponential notation, writing xu1 1 Â· Â· Â· xus s , with ui a non-negative integer for 1 â‰¤ i â‰¤ s,
Pâ„“âˆ’1
Pâ„“
Ps
for a vector (y1 , . . . , yPsj=1 uj ) in which yâ„“ = xj for j=1 uj < â„“ â‰¤ j=1 uj for 1 â‰¤ â„“ â‰¤ j=1 uj .
Fig. 6 gives a heterogeneous d-strengthening DHF(19; 13, k, 5, 2) with k = (56 41 312 ) and d = (46 313 ). This is equivalent
to a d-strengthening SHF(19; 13, k, {{1, 4}, {2, 3}}). Consider the separation of columns {1, 7} from columns {2, 6, 11}. Row
8 accomplishes the required separation because it uses no more than d8 = 3 symbols. Consider instead columns {1, . . . , 5}.
While the first row separates {1, 2, 3} from {4, 5}, it uses 5 symbols instead of d1 = 4 and so does not accomplish the required
separation; this separation is accomplished in row 3.
Next the properties are determined for an heterogeneous hash family to support recovery of signals of dimension n and
sparsity t using a column replacement technique.
Theorem 3: [14] Let k = (k1 , . . . , km ) and q = (q1 , . . . , qm ) be tuples of positive integers. Let d = (2q1 , . . . , 2qm ). For
1 â‰¤ i â‰¤ m, let Ai âˆˆ Rri Ã—ki be a measurement matrix that meets the (â„“0 , qi )-null space condition. Let P be a (d, 2t)strengthening SHF(m; n, k, {1, t}), and let B be the column replacement of A1 , . . . , Am into P . Then B meets the (â„“0 , t)-null
space condition.
Theorem 4: [14] Let k = (k1 , . . . , km ) and q = (q1 , . . . , qm ) be tuples of positive integers. For 1 â‰¤ i â‰¤ m, let Ai âˆˆ Rri Ã—ki
be a measurement matrix that meets the (â„“1 , qi )-null space condition. Let P be a (q, t)-strengthening DHF(m; n, k, t + 1, 2),
and let B be the column replacement of A1 , . . . , Am into P . Then B meets the (â„“1 , t)-null space condition.

Revisiting the d-strengthening DHF(19; 13, k, 5, 2) pattern matrix in Fig. 6, the results of Theorems 3 and 4 indicate that
the number of symbols in each row need not be the same. In general, there may be as many ingredient matrices Ai as there
are rows of the pattern matrix P . Moreover, the strength of each ingredient matrix Ai may be different! In this example, the

5

k1 = . . . = k6 = 5 symbols;
d1 = . . . = d6 = 4 used to separate

k2 = 4 symbols; d2 = 3 used to separate
â†’

k8 = . . . = k19 = 3 symbols;
d8 = . . . = d19 = 3 used to separate

Fig. 6.

â‡“
4
0
0
2
2
3
0
0
1
0
0
0
2
2
0
1
1
2
0

â†“
0
0
2
4
1
4
0
1
0
1
2
1
1
1
0
2
0
2
0

2
1
4
1
2
0
1
0
2
2
2
1
0
2
0
0
2
0
2

1
1
1
0
2
1
0
1
0
2
2
0
1
0
1
1
1
0
1

3
2
1
3
4
0
0
1
0
1
1
1
2
2
0
1
1
1
1

â†“
3
3
2
0
0
3
2
2
2
2
2
2
0
2
1
1
0
2
0

â‡“
0
1
0
3
0
2
2
0
1
1
0
1
1
0
1
2
0
1
1

0
3
1
1
4
4
0
2
1
0
0
0
2
0
2
2
2
0
2

1
2
2
1
0
2
0
0
0
0
1
0
0
1
0
0
0
0
0

4
4
3
4
1
1
1
0
2
0
1
2
2
0
1
0
0
1
2

â†“
2
2
0
2
1
1
3
2
0
1
0
0
0
0
2
0
2
2
2

2
0
3
0
3
2
0
1
2
0
0
2
0
1
2
2
1
1
1

1
4
4
2
3
0
0
2
1
2
1
2
1
1
2
0
0
2
1

A heterogeneous d-strengthening DHF(19; 13, k, 5, 2) with k = (56 41 312 ) and d = (46 313 ).

first 6 rows use 4 symbols to separate, so the corresponding ingredient matrices must have strength at least 4. The remaining
rows use 3 symbols to separate, so the corresponding ingredient matrices must have strength at least 3.
In [14], we showed that heterogeneity gives great flexibility in construction of measurement matrices using column replacement. The hierarchical structure of the measurement matrices produced by column replacement can also aid in recovery, and
be used to support hybrid recovery schemes. We examine this problem next, considering a generalization of hash families that
removes the restriction to those strategies based only on (â„“0 , t)- or (â„“1 , t)-recoverability. We also consider the computational
investment required to recover the signal.
III. H ASH FAMILIES FOR R ECOVERY
In order to tackle signal recovery, we require another generalization of hash families. As before, let k = (k1 , . . . , km ) be a
tuple of positive integers. An HFâ—¦ (m; n, k) is an m Ã— n array, P = (pij ), in which each cell contains one symbol, and for each
row 1 â‰¤ i â‰¤ m, {pij : 1 â‰¤ j â‰¤ n} âŠ† {â—¦, 1, . . . , ki }. The symbol â—¦, when present, is interpreted as representing a â€˜missingâ€™
entry. When the pattern matrix P = (pij ) is an HFâ—¦ (m; n, k), and for 1 â‰¤ i â‰¤ m the ingredient matrix Ai is ri Ã— ki with
columns indexed by the ki symbols in row i of P other than â—¦, the column replacement of A1 , . . . , Am into P is as before,
except that when pij = â—¦, it is replaced with an all zero column vector of length ri . As we will see, the separating properties
of the hash families we use allow us to locate the nonzero coordinates of the signal and hence perform the recovery.
The definition of a W-separating hash family encompasses perfect, {w1 , . . . , ws }-separating, and distributing hash families.
Therefore, we need only extend the definition of W-separating hash families to include the â—¦ symbol. To do so, we allow
some of the elements of the multisets in W to be marked with a â—¦ superscript to form a set of marked multisets W â€² ; the
multisets in W â€² are indexed. Then an HFâ—¦ (m; n, k) is W â€² -separating if, for each {w1 , . . . , ws } âˆˆ W (with some elements
possibly marked),
Ps
â€¢ whenever C is a set of
i=1 wi columns, and
â€¢ C1 , . . . , Cs is an (indexed) partition of C with |Ci | = wi for 1 â‰¤ i â‰¤ s
then there exists a row that separates C1 , . . . , Cs in which, for 1 â‰¤ j â‰¤ s, if â—¦ appears in a column in Cj then wj is marked.
As we will see, to recover the signal, the idea is to effect a separation where a significant coordinate of the signal is present
in one class such that any other class does not prevent its recovery.
A. Signal Recovery without Noise
Theorems 3 and 4 suggest that a recovery scheme based on (â„“0 , t)- or (â„“1 , t)-recoverability can be â€˜liftedâ€™ from the ingredient
measurement matrices A1 , . . . , Am to the larger measurement matrix B obtained from column replacement. However, such a
method appears to have two main drawbacks. First, it is restricted to recovery strategies based on (â„“0 , t)- or (â„“1 , t)-recoverability.
Secondly, and perhaps more importantly, it appears to necessitate a large computational investment to recover the signal, given
B.

6

In order to overcome these problems, we consider two cases. The positive case arises when the signal is known a priori to
be in Rnâ‰¥0 . The general case arises when the signal can be positive, negative, or zero. In each case we develop a recovery
scheme for the matrix B resulting from column replacement that does not depend on any fixed algorithm, but rather on the
recovery schemes for the ingredient matrices A1 , . . . , Am .
We suppose that P = (pij ) is an HFâ—¦ (m; n, k). For each 1 â‰¤ i â‰¤ m, we suppose that Ai is an ri Ã— ki measurement matrix
that has (â„“0 , t)-recoverability, equipped with a recovery algorithm Ri that determines the unique t-sparse vector zi that solves
Ai zi = yi . We further suppose that B is the column replacement of A1 , . . . , Am into P , and that y is the result of sampling
an (unknown) t-sparse vector x = (x1 , . . . , xn ) using B.
For 1 â‰¤ i â‰¤ m, the ith row of P induces a partition {Siâ—¦ , Si1 , . . . , Siki } of the column indices {1, . . . , n}, where SiÏƒ =
{j : pij = Ïƒ, 1 â‰¤ j â‰¤ n} for Ïƒ âˆˆ {â—¦, 1, . . . , ki }. Assume that we have employed the recovery algorithms Ri to find solutions
zi . For 1 â‰¤ i â‰¤ m and Ïƒ âˆˆ {â—¦, 1, . . . , ki }, the partition class SiÏƒ is discarded if Ïƒ = â—¦, insignificant if Ïƒ 6= â—¦ and ziÏƒ = 0,
significant positive if ziÏƒ > 0, and significant negative if ziÏƒ
P< 0.
For 1 â‰¤ i â‰¤ m, let wi = (wi1 , . . . , wiki ) where wiÏƒ = jâˆˆSiÏƒ xj . The vector wi can be considered as a projection of x
induced by the symbol pattern in row i of P . These facts follow:
i
â€¢ For 1 â‰¤ i â‰¤ m, by the definition of B and because Bi x = yi , zi = wi is a solution to A zi = yi .
i
â€¢ For 1 â‰¤ i â‰¤ m, because A has (â„“0 , t)-recoverability and wi is t-sparse (because x is t-sparse), zi = wi is the unique
solution to Ai zi = yi , and so Ri returns wi .
We now consider the positive case and the general case for recovery in succession.
B. Signal Recovery: The Positive Case
We establish that in the positive case with t-sparse signals, it suffices to use a separating hash family of suitable strength,
along with suitable ingredient matrices. An SHFâ—¦ (m; n, k, {1, tâ—¦ }) separates t + 1 columns into two parts, one part of size one
that cannot include the symbol â—¦, and the other of size t that may include â—¦.
Theorem 5: Suppose that P is an SHFâ—¦ (m; n, k, {1, tâ—¦ }). For 1 â‰¤ i â‰¤ m, let Ai âˆˆ Rri Ã—ki be a measurement matrix that
has (â„“0 , t)-recoverability equipped with a recovery algorithm Ri that determines the unique t-sparse vector zi that solves
Ai zi = yi . Further suppose that B is the column replacement of measurement matrices A1 , . . . , Am into P and that y is the
result of sampling an (unknown) t-sparse vector x = (x1 , . . . , xn ) âˆˆ Rnâ‰¥0 using B. Then the t-sparse solution x to Bx = y
can be recovered.

Proof: It suffices to determine whether xi is positive or zero for each 1 â‰¤ i â‰¤ n, because once this is accomplished we
can find the values of the positive xi by solving the overdetermined system that remains. For 1 â‰¤ i â‰¤ m, apply recovery
algorithm Ri to find the unique t-sparse vector zi such that Ai zi = yi . We claim that, for 1 â‰¤ â„“ â‰¤ n, xâ„“ is positive if and
only if for each i âˆˆ {1, . . . , m} the partition class that contains â„“ is either significant positive or discarded.
Suppose
P first that xâ„“ is positive. If SiÏƒ is a partition class that contains â„“, then either Ïƒ = â—¦ and SiÏƒ is insignificant or Ïƒ 6= â—¦,
ziÏƒ = jâˆˆSiÏƒ xj â‰¥ xâ„“ > 0, and SiÏƒ is significant positive. Now suppose that xâ„“ = 0. Let C = {j : xj > 0, 1 â‰¤ j â‰¤ n};
|C| â‰¤ t. There must be a row Ï of P that separates C from {â„“} such that pÏâ„“ 6= â—¦. Let Ïƒ = pÏâ„“ . Then â„“ âˆˆ SÏÏƒ and SÏÏƒ âˆ©C = âˆ…,
so SÏÏƒ is insignificant.
One useful application of Theorem 5 takes the pattern matrix P to be an SHFâ—¦ (m; n, 1, {1, tâ—¦ }), and each Ai to be a 1 Ã— 1
matrix whose only element is 1; in this case, column replacement yields a matrix B isomorphic to P . In P for every column
Î³ and every set C of t columns with Î³ 6âˆˆ C, there is a row in which all columns of C contain â—¦, while column Î³ contains
1. Then the measurement matrices Ai have (â„“0 , t)-recoverability and the recovery algorithms Ri are trivial. Hence in these
cases, a matrix isomorphic to P itself supports recovery.
Theorem 5 leads to a straightforward recovery algorithm. First, Ri is used to solve Ai zi = yi for 1 â‰¤ i â‰¤ m. Then the
classes Sij areP
classified as positive when zij > 0, discarded when j = â—¦, and insignificant when j 6= â—¦ and zij = 0; this can
m
be done in O( i=1 ki ) time. We need only compute, for each row, the complement of the union of the insignificant classes,
and then compute the intersection over all rows of these complements. However, without additional structure this appears to
require the examination of each coordinate; hence, this gives an â„¦(n) lower bound.
It is not difficult, nevertheless, to obtain sublinear recovery times by restricting the hash family; we return to this problem
in Section III-D.
C. Signal Recovery: The General Case
When the signal takes on both positive and negative values, cancellation of positive and negative contributions can yield a
zero measurement despite the presence of a signal. Nevertheless, an additional requirement on the structure of the hash family

7

suffices to address this problem, as we show next.
Theorem 6: Suppose that P is an SHFâ—¦ (m; n, k, {{Ï„, (t + 1 âˆ’ Ï„ )â—¦ } : 1 â‰¤ Ï„ â‰¤ t}). For 1 â‰¤ i â‰¤ m, let Ai âˆˆ Rri Ã—ki be a
measurement matrix that has (â„“0 , t)-recoverability equipped with a recovery algorithm Ri that determines the unique t-sparse
vector zi that solves Ai zi = yi . Further suppose that B is the column replacement of measurement matrices A1 , . . . , Am into
P and that y is the result of sampling an (unknown) t-sparse vector x = (x1 , . . . , xn ) using B. Then the t-sparse solution x
to Bx = y can be recovered.

Proof: As in the proof of Theorem 5, it suffices to determine whether xi is nonzero or zero for each 1 â‰¤ i â‰¤ n,
because once this is accomplished we can find the values of the nonzero xi by solving the overdetermined system that
remains. For 1 â‰¤ i â‰¤ m, apply recovery algorithm Ri to find the unique t-sparse vector zi such that Ai zi = yi . Let
âˆ’
z+
i = (max(0, zij ) : 1 â‰¤ j â‰¤ ki ) and zi = (min(0, zij ) : 1 â‰¤ j â‰¤ ki ).
+
â€²
A row i of P is maximum positive if ||z+
i ||1 â‰¥ ||ziâ€² ||1 for 1 â‰¤ i â‰¤ m. Let M âŠ† {1, . . . , m} index the maximum positive
rows. We claim that a coordinate xâ„“ is positive if and only if, for every Ï âˆˆ M , â„“ is in a significant positive class of the
partition induced by row Ï.
Suppose first that xâ„“ is positive and let Ï âˆˆ M . Because Ï indexes a maximum positive row, the partition class induced
by row Ï that contains â„“ is not discarded and does not contain the index of any negative variable. Thus it is in a significant
positive partition class.
Now suppose that xâ„“ â‰¤ 0. Because P is an SHFâ—¦ (m; n, k, {{Ï„, (t + 1 âˆ’ Ï„ )â—¦ } : 1 â‰¤ Ï„ â‰¤ t}) and x is t-sparse, there is a row
Ï of P that separates {j : xj > 0, 1 â‰¤ j â‰¤ n} from {j : xj < 0, 1 â‰¤ j â‰¤ n} âˆª {â„“} in which the symbol â—¦ only appears in a
subset of the columns indexed by {j : xj < 0, 1 â‰¤ j â‰¤ n} âˆª {â„“}. It follows that Ï is a maximum positive row of P and that
the partition class induced by Ï containing â„“ does not contain the index of any positive coordinate. So Ï âˆˆ M , but â„“ is not in
a significant positive class of the partition induced by row Ï.
In the same manner, all negative coordinates can be identified using maximum negative rows.

Again a straightforward recovery algorithm is given by Theorem 6 but, as in the positive case, it naively involves examining
each of the n coordinates.
D. Sublinear Time Signal Recovery
Recovery can be accomplished in time that is sublinear in k when the hash family has suitable structure; we develop a
general approach, and one example, here. In each case, for some subset M of the rows of P , sets are identified that must
contain the indices of all positive coordinates (the indices of the negative coordinates, if they exist, can be located similarly).
Recall from Section III-A, that the positive case arises when the signal is known a priori to be in Rnâ‰¥0 and the general case
arises when the signal can be positive, negative, or zero. In the positive case, M contains all rows and for Ï âˆˆ M , the candidate
indices are VÏ+ = {â„“ : pÏâ„“ = â—¦, or xâ„“ âˆˆ SÏj and zÏj > 0}. In the general case, M contains all rows that index maximum
positive rows, and for Ï âˆˆ M , the candidate indices are VÏ+ = {â„“ : xâ„“ âˆˆ SÏj and zÏj > 0}. In both cases, we are to determine
T
+
we do not list the members of VÏ+ explicitly, but rather use
ÏâˆˆM VÏ . In order to avoid the examination of each
T coordinate,
+
an implicit representation to list the members of ÏâˆˆM VÏ .
First we give an implicit representation of an hash family HF(q + 1; q Î± , q), P , where q is a prime power and 2 â‰¤ Î± â‰¤ q.
Let {Ï‰0 , . . . , Ï‰qâˆ’1 } be the elements of the finite field of order q, Fq . Index the rows of P by {âˆ} âˆª {Ï‰0 , . . . , Ï‰qâˆ’1 }. Index
the columns of P by the q Î± polynomials of degree less than Î± in indeterminate x, with coefficients in Fq . Now the entry of
P with row index Î² and column indexed by polynomial f (x) is determined as f (Î²) when Î² âˆˆ {Ï‰0 , . . . , Ï‰qâˆ’1 }, and as the
coefficient of xÎ±âˆ’1 in f (x) when Î² = âˆ.
By deleting rows, we form an HF(m; q Î± , q) for some 1 â‰¤ m â‰¤ q + 1. An hash family is linear if it is obtained in this way.
The separation properties of such an hash family are crucial [1], [5]. For our purposes, the observation of interest is from [15]:
if m â‰¥ (Î± âˆ’ 1)w1 w2 + 1, then a linear HF(m; n, q) is {w1 , w2 }-separating. (This can be established by a simple argument:
When two polynomials of degree less than Î± evaluate to the same value at Î± different points, they are the same polynomial.) In
some cases, fewer rows suffice to ensure separation. In particular, Blackburn and Wild [5] establish that when q is sufficiently
large, one needs at most Î±(w1 + w2 âˆ’ 1) rows; and in [15] specific small separations are examined to determine the set of
prime powers for which various numbers of rows less than (Î± âˆ’ 1)w1 w2 + 1 suffice. We proceed with the general statement
so as not to impose additional conditions.
When m â‰¥ (Î± âˆ’ 1)t + 1, P is {1, t}-separating; in addition, every {1, t âˆ’ 1}-separation is accomplished in at least Î± rows.
t+1
When m â‰¥ (Î± âˆ’ 1)âŒŠ t+1
2 âŒ‹âŒˆ 2 âŒ‰+ 1, P is {w, t+ 1 âˆ’ w}-separating for each 1 â‰¤ w â‰¤ t; in addition, every {w, tâˆ’ w}-separation
t+1
t
t
t+1
is accomplished in at least Î± rows, because âŒŠ t+1
2 âŒ‹âŒˆ 2 âŒ‰ = âŒŠ 2 âŒ‹âŒˆ 2 âŒ‰ + âŒŠ 2 âŒ‹. Thus in either case, M contains at least Î± rows
of P .

8

Q
Choose any Î± rows U = {Ïˆ1 , . . . ÏˆÎ± } âŠ† M . Now consider the sets {VÏˆ+ : Ïˆ âˆˆ U }. Define ÏˆâˆˆU |VÏˆ+ | vectors V + =
{(g1 , . . . , gÎ± ) : gi âˆˆ {pÏˆi â„“ : â„“ âˆˆ VÏˆ+i } for 1 â‰¤ i â‰¤ Î±}. Each (g1 , . . . , gÎ± ) âˆˆ V + defines a unique column of the hash family,
corresponding to the unique polynomial L of degree at most Î± âˆ’ 1 satisfying L(Ïˆi ) = gi for 1 â‰¤ i â‰¤ Î±. Any column that does
not arise in this way from a member of V + cannot be the column for a positive coordinate, because in the partition induced
by one of the selected maximum rows it is not in a significant positive class. However, columns arising from vectors in V +
need not arise from positive coordinates, because we may not have examined all of the rows of M . Nevertheless, we can now
generate each of the columns arising from vectors in V + , and check for each whether it occurs in positive classes for all rows
of M , not just the Î± selected.
Now |V + | is O(tÎ± ), so when t is o(q), the size of V + is o(n) (because n = q Î± ). For concreteness, taking q = tÎ² for t a
prime power, we can permit Î± to be as large as tÎ²âˆ’2 . (For the positive case, we can permit Î± to be as large as tÎ²âˆ’1 . ) Hence,
by restricting the hash family to one that is linear, it is possible to obtain recovery of the signal in sublinear time.
In general, a hash family together with its ingredient matrices can be represented more concisely compared to a random
measurement matrix for signal recovery. Furthermore, the hash family is an integer matrix, not a matrix of real numbers,
and may therefore be easier to encode. When the hash family is linear an implicit representation of it may be used, further
compacting its representation.
The results of this section provide some evidence that column replacement enables recoverability conditions to be met. In
Section IV, we show that it also preserves the basic machinery to deal with noise in the signal.
E. Adding Strengthening
As the signal length increases, it is natural to support high sparsity. Yet the techniques developed until this point only
preserve sparsity. Strengthening hash families provide a means to increase the level of sparsity supported.
Theorem 7: Suppose that P is a d-strengthening SHF(m; n, k, {{Ï„, (t + 1 âˆ’ Ï„ )} : 1 â‰¤ Ï„ â‰¤ t}). For each 1 â‰¤ i â‰¤ m, we
suppose that Ai is an ri Ã— ki measurement matrix that has (â„“1 , di )-recoverability, equipped with a recovery algorithm Ri , that
either determines the unique di -sparse vector zi that solves Ai zi = yi or indicates that no such vector exists. Further suppose
that B is the column replacement of A1 , . . . , Am into P , and that y is the result of sampling an (unknown) t-sparse vector
x = (x1 , . . . , xn ) using B. Then the t-sparse solution x to Bx = y can be recovered.

Proof: Again it suffices to locate the nonzero coordinates of x. For 1 â‰¤ i â‰¤ m, if recovery algorithm Ri returns a solution
zi such that ||zi ||1 â‰¥ ||z||1 for any solution z returned by an oracle Rj , then zi is a maximum solution, and row i of P is a
maximum row. Because P is a d-strengthening SHF(m; n, k, {{Ï„, (t + 1 âˆ’ Ï„ )} : 1 â‰¤ Ï„ â‰¤ t}), and x is t-sparse, there is a row
Ï of P that separates {j : xj > 0, 1 â‰¤ j â‰¤ n} from {j : xj < 0, 1 â‰¤ j â‰¤ n} with the property that at most dÏ symbols appear
in the columns indexed by {j : xj 6= 0, 1 â‰¤ j â‰¤ n}. So the projected vector wÏ is dÏ -sparse and it is the solution returned by
RÏ . By the definition of Ï, ||wÏ ||1 = ||x||1 . It follows that the â„“1 -norm of any maximum solution is at least ||x||1 .
We claim that if Ri returns a maximum solution zi , then zi = wi . Suppose otherwise. Then, because zi is a maximum
solution, we have ||zi ||1 â‰¥ ||x||1 . Further, it is clear from the definition of ||wi || that ||wi ||1 â‰¤ ||x||1 . Thus Ai zi = Ai wi , zi
is di -sparse, and ||zi ||1 â‰¥ ||wi ||1 , which is a contradiction to the fact that Ai has (â„“1 , di )-recoverability.
Having established our claim, we can now use arguments similar to those used in the proof of Theorem 6 to show that
a coordinate xâ„“ is positive (negative) if and only if, for every maximum row in P , â„“ is in a significant positive (significant
negative) class of the partition induced by that row.
IV. R ECOVERY

WITH

N OISE

We now treat the recovery ofPsignals with noise. A signal (x1 , . . . , xn ) is (s, t)-almost sparse if there is a set T of at most
t coordinate indices such that iâˆˆ{1...,n}\T |xi | < s.

Theorem 8: Suppose that P is an SHF(m; n, k, {{Ï„, (t + 1 âˆ’ Ï„ )} : 1 â‰¤ Ï„ â‰¤ t}). For each 1 â‰¤ i â‰¤ m, we suppose that Ai
is an ri Ã— ki measurement matrix, equipped with recovery algorithm Ri , which, when applied to the sample obtained from an
(s, t)-almost sparse signal xi , returns a vector zi such that ||zi âˆ’ xi ||1 < Ç«. Further suppose that B is the column replacement
of A1 , . . . , Am into P , and that y is the result of sampling an (unknown) (s, t)-almost sparse vector x = (x1 , . . . , xn ) using B.
Then, a (perfectly) t-sparse vector xâˆ— = (xâˆ—1 , . . . , xâˆ—n ) such that for 1 â‰¤ i â‰¤ n, |xi | < 2(s + Ç«) if xâˆ—i = 0, and |xi âˆ’ xâˆ—i | < s + Ç«
if xâˆ—i > 0, and such that Bxâˆ— = y, can be recovered.
Proof: We provide a sketch first, and then the details. The idea is to write each coordinate of z as a sum of the signal
coordinates in T that contribute to it, and of a noise term e that includes both the small contributions from coordinates outside
T and the error less than Ç« from the recovery algorithm. For each row Ï of P , we then split this sum into two parts: one part

9

containing terms with the same sign as the z coordinate to which they contribute (indexed by sets TÏâ€² and EÏâ€² ), and another part
containing terms with the opposite sign to the z coordinate to which they contribute (indexed by sets TÏâ€²â€² and EÏâ€²â€² ). The key
observation is that the sum of the terms with indices in TÏâ€²â€² can be approximated by 12 (||x|| âˆ’ ||zÏ ||) and hence by 21 (q âˆ’ ||zÏ ||)
â€²â€²
because if TÏâ€²â€² is empty then zÏ has norm close to ||x||, and
Pevery term with index in T+Ï reduces ||zÏ ||.
Let T be a set of at most t coordinate indices such that iâˆˆ{1...,n}\T |xi | < s. Let T = {i âˆˆ T : xi â‰¥ 0}, T âˆ’ = {i âˆˆ T :
P
xi < 0} and q â€  = iâˆˆT |xi |. For 1 â‰¤ i â‰¤ m, apply Ri to yi to find a vector zi such that ||zi âˆ’ wi ||1 < Ç«. For i âˆˆ {1, . . . , m},
call ||zi ||1 the signature of row i of P and let q be the maximum signature of any row of P .
For 1 â‰¤ i â‰¤ n, we calculate upper and lower estimates u(i) and â„“(i) for xi . For each row index Ï âˆˆ {1, . . . , m} and each
symbol Ïƒ âˆˆ {1, . . . , kÏ } we define uÏÏƒ and â„“ÏÏƒ as follows.
1
1
â€¢ If zÏÏƒ â‰¥ 0, then uÏÏƒ = |zÏÏƒ | + 2 (q âˆ’ ||zÏ ||1 ) and â„“ÏÏƒ = âˆ’ 2 (q âˆ’ ||zÏ ||1 ).
1
1
â€¢ If zÏÏƒ < 0, then uÏÏƒ = 2 (q âˆ’ ||zÏ ||1 ) and â„“ÏÏƒ = âˆ’|zÏÏƒ | âˆ’ 2 (q âˆ’ ||zÏ ||1 ).
For each i âˆˆ {1, . . . , n} define uÏ (i) = uÏÏ€ and â„“Ï (i) = â„“ÏÏ€ , where Ï€ is the symbol in row Ï of P such that i âˆˆ SÏÏ€ , and
define u(i) = min{uÏ (i) : 1 â‰¤ Ï â‰¤ m} and â„“(i) = max{â„“Ï (i) : 1 â‰¤ Ï â‰¤ m}. By first examining a row of maximum signature,
we can immediately conclude for each i âˆˆ {1, . . . , n} either that u(i) = 0 or that â„“(i) = 0. Define a vector xâˆ— = (xâˆ—1 , . . . , xâˆ—n )
by setting xâˆ—i = 0 if |ui |, |â„“i | â‰¤ s + Ç«, and otherwise setting xâˆ—i equal to whichever of u(i) or â„“(i) has the greater absolute
value. We claim that xâˆ— satisfies the required conditions.
To establish this claim we prove that, for 1 â‰¤ j â‰¤ n,
(i) for each Ï âˆˆ {1, . . . , m}, â„“Ï (j) âˆ’ (s + Ç«) < xj < uÏ (j) + (s + Ç«);
(ii) there is some Ï âˆˆ {1, . . . , m} such that â„“Ï (j) > âˆ’(s + Ç«) if xj â‰¥ 0 and uÏ (j) < s + Ç« if xj < 0; and
(iii) there is some Ï âˆˆ {1, . . . , m} such that uÏ (j) âˆ’ (s + Ç«) < xj if xj â‰¥ 0 and xj < â„“Ï (j) + (s + Ç«) if xj < 0.
We begin with some observations used throughout the proof. Let Ï be a row of P . For 1 â‰¤ Ïƒ â‰¤ kÏ , we have zÏÏƒ =
PkÏ
P
|eÏÏƒ | â‰¤ s + Ç«. Let TÏâ€² = {i âˆˆ T + : zÏpÏi â‰¥ 0} âˆª {i âˆˆ T âˆ’ : zÏpÏi < 0}
( iâˆˆT âˆ©SÏÏƒ |xi |) + eÏÏƒ for some eÏÏƒ . Note that Ïƒ=1
â€²â€²
â€²
â€²
and let TÏ = T \ TÏ . Further, let EÏ = {Ïƒ âˆˆ {1, . . . , kÏ } : eÏÏƒ , zÏÏƒ â‰¥ 0 or eÏÏƒ , zÏÏƒ < 0} and let EÏâ€²â€² = {1, . . . , kÏ } \ EÏâ€² . For
1 â‰¤ Ï€ â‰¤ kÏ , we have that
ï£«
ï£¶ ï£«
ï£¶
X
X
|zÏÏ€ | = ï£­
|xi |ï£¸ âˆ’ ï£­
|xi |ï£¸ + Î´ÏÏ€ |eÏÏ€ |
(1)
iâˆˆTÏâ€² âˆ©SÏÏ€

iâˆˆTÏâ€²â€² âˆ©SÏÏ€

where Î´ÏÏ€ = 1 if Ï€ âˆˆ EÏâ€² and Î´ÏÏ€ = âˆ’1 if Ï€ âˆˆ EÏâ€²â€² . Summing over the symbols in row Ï of P , we see
ï£«
ï£¶ ï£«
ï£¶ ï£«
ï£¶
X
X
X
||zÏ ||1 = q â€  âˆ’ 2 ï£­
|xi |ï£¸ + ï£­
|eÏÏƒ |ï£¸ âˆ’ ï£­
|eÏÏƒ |ï£¸
iâˆˆTÏâ€²â€²

and it follows that
1 â€ 
2 (q

ï£«

âˆ’ ||zÏ ||1 ) = ï£­

ÏƒâˆˆEÏâ€²

(2)

ÏƒâˆˆEÏâ€²â€²

ï£«
ï£«
ï£¶
ï£¶
X
X
1
1
|eÏÏƒ |ï£¸ + ï£­
|eÏÏƒ |ï£¸ .
|xi |ï£¸ âˆ’ ï£­
2
2
â€²
â€²â€²
â€²â€²
ï£¶

X

ÏƒâˆˆEÏ

iâˆˆTÏ

(3)

ÏƒâˆˆEÏ

Adding (1) to (3), we obtain
ï£«

|zÏÏ€ | + 12 (q â€  âˆ’ ||zÏ ||1 ) = ï£­

X

iâˆˆTÏâ€² âˆ©SÏÏ€

ï£¶

ï£«

|xi |ï£¸ + ï£­

X

iâˆˆTÏâ€²â€² \SÏÏ€

ï£¶

|xi |ï£¸ âˆ’

ï£«

1ï£­
2

X

ÏƒâˆˆEÏâ€²

ï£¶

|eÏÏƒ |ï£¸ +

ï£«

1ï£­
2

X

ÏƒâˆˆEÏâ€²â€²

ï£¶

|eÏÏƒ |ï£¸ + Î´ÏÏ€ |eÏÏ€ |.

(4)

It follows from (2) that each row of P has signature less than q â€  + (s + Ç«) and that any row of P that separates T + from T âˆ’
has signature greater than q â€  âˆ’ (s + Ç«). Thus, q â€  âˆ’ (s + Ç«) < q < q â€  + (s + Ç«) and hence
1
2 (q

âˆ’ ||zÏ ||1 ) âˆ’ 12 (s + Ç«) < 12 (q â€  âˆ’ ||zÏ ||1 ) < 12 (q âˆ’ ||zÏ ||1 ) + 12 (s + Ç«).

(5)

Let j âˆˆ {1, . . . , n}. We next show that (i), (ii) and (iii) hold in the case where xj â‰¥ 0. The proof in the case where xj < 0
is similar.
Proof of (i). Let Ï index any row of P and let SÏÏ€ be the partition class induced by row Ï of P that contains j. Now
â„“Ï (j) âˆ’ (s + Ç«) < xjPbecause â„“Ï (j) â‰¤ 0. If j âˆˆ
/ T , then xj < s and xj < uÏ (j) + (s + Ç«) because uÏ (j) â‰¥ 0. If j âˆˆ T and
zÏÏ€ < 0, then xj â‰¤ iâˆˆT â€²â€² |xi | and we see from (3) and (5) that xj < 21 (q âˆ’ ||zÏ ||1 ) + (s + Ç«). If j âˆˆ T and zÏÏ€ â‰¥ 0, then
Ï
P
xj â‰¤ iâˆˆTÏâ€² âˆ©SÏÏ€ |xi | and we see from (4) and (5) that xj < |zÏÏ€ | + 12 (q âˆ’ ||zÏ ||1 ) + (s + Ç«).
âˆ’
Proof of (ii). Let Ï index a row of P that separates T + âˆª {j} from
induced by row
P T and let SÏÏ€ be the partition class
Ï of P that contains j. If zÏÏ€ â‰¥ 0, then, for 1 â‰¤ Ïƒ â‰¤ kÏ , either iâˆˆT â€²â€² âˆ©SÏÏƒ |xi | â‰¤ |eÏÏƒ | and Ïƒ âˆˆ EÏâ€² or TÏâ€²â€² âˆ© SÏÏƒ = âˆ….
Ï
Using this, it follows from (3) and (5) that â„“Ï (j)P= âˆ’ 21 (q âˆ’ ||zÏ ||1 ) > âˆ’(s + Ç«). If zÏÏ€ < 0, then TÏâ€² âˆ© SÏÏ€ = âˆ… and Ï€ âˆˆ EÏâ€² .
Furthermore, for Ïƒ âˆˆ {1, . . . , kÏ } \ {Ï€}, either iâˆˆT â€²â€² âˆ©SÏÏƒ |xi | < |eÏÏƒ | and Ïƒ âˆˆ EÏâ€² or TÏâ€²â€² âˆ© SÏÏƒ = âˆ…. Using these facts, it
Ï
follows from (4) and (5) that â„“Ï (j) = âˆ’|zÏ€Ï | âˆ’ 12 (q âˆ’ ||zÏ ||1 ) > âˆ’(s + Ç«).

10

Proof of (iii). Let Ï index a row of P thatP
separates T + \ {j} from T âˆ’ âˆª {j} and let SÏÏ€ be the partition class induced by
row Ï of P that contains j. If zÏÏ€ < 0, then iâˆˆT â€²â€² âˆ©SÏÏ€ |xi | â‰¤ xj . Furthermore, for each symbol Ïƒ âˆˆ {1, . . . , kÏ } \ {Ï€}, either
Ï
P
|eÏÏƒ | and Ïƒ âˆˆ EÏâ€² or TÏâ€²â€² âˆ©SÏÏƒ = âˆ…. Then, it follows from (3) and (5) that uÏ (j) = 21 (qâˆ’||zÏ ||1 )âˆ’(s+Ç«) < xj .
iâˆˆTÏâ€²â€² âˆ©SÏÏƒ |xi | â‰¤
P
P
If zÏÏ€ â‰¥ 0, then iâˆˆT â€² âˆ©SÏÏ€ |xi | â‰¤ xj . Furthermore, for each symbol Ïƒ âˆˆ {1, . . . , kÏ } \ {Ï€}, either iâˆˆTÏâ€²â€² âˆ©SÏÏƒ |xi | â‰¤ |eÏÏƒ |
and Ïƒ âˆˆ EÏâ€² or TÏâ€²â€² âˆ© SÏÏƒ = âˆ…. Then, it follows from (4) and (5) that uÏ (j) = |zÏ€Ï | + 12 (q âˆ’ ||zÏ ||1 ) âˆ’ (s + Ç«) < xj .
V. C ONCLUSION
Hierarchical construction of measurement matrices by column replacement permits the explicit construction of large measurement matrices from small ones. The use of heterogeneous hash families supports the use of a library of smaller ingredient
matrices, while the use of strengthening hash families allows the ingredient matrices to be designed for lower sparsity than
the larger measurement matrix produced. Perhaps surprisingly, the ingredient measurement matrices need not all employ
the same recovery algorithm; rather recovery for the large measurement matrix can use arbitrary routines for recovery that
are provided with the ingredient matrices. In this way, computationally intensive recovery methods can be used for the
ingredient matrices, which permits the selection of smaller matrices in general, while still enabling recovery for the large
measurement matrix. Nevertheless, recovery using the large measurement matrix can be computationally prohibitive without
further restrictions. Therefore it is shown that using a standard construction of linear hash families over the finite field, recovery
for the large measurement matrix can be effected in sublinear time. Indeed sublinear recovery time can be obtained even when
computationally intensive methods are used for each ingredient matrix. A practical implementation of these recovery methods
requires that the methods deal effectively with noise in the signal. Suitable restrictions on the hash family and on each ingredient
matrix used in column replacement are shown to be sufficient to permit recovery even in the presence of such noise.
Measurement matrices that result from one column replacement have been studied here. Because recovery does not depend
on the method by which recovery is done for the ingredient matrices, it is possible that the ingredient matrices themselves
are constructed by column replacement from even smaller ingredient matrices. The merits and demerits of repeated column
replacement deserve further study.
ACKNOWLEDGEMENTS
The work of D. Horsley and C. J. Colbourn is supported in part by the Australian Research Council through grant
DP120103067.
R EFERENCES
[1] N. Alon. Explicit construction of exponential sized families of k-independent sets. Discrete Mathematics, 58:191â€“193, 1986.
[2] R. Baraniuk. Compressive sensing. IEEE Signal Processing Magazine, 24:227â€“234, 2007.
[3] R. Berinde, A. C. Gilbert, P. Indyk, H. Karloff, and M. J. Strauss. Combining geometry and combinatorics: A unified approach to sparse signal recovery.
In Proceedings of the 46th Annual Allerton Conference on Communication, Control, and Computing, pages 798â€“805, 2008.
[4] S. R. Blackburn, T. Etzion, D. R. Stinson, and G. M. Zaverucha. A bound on the size of separating hash families. Journal of Combinatorial Theory,
Series A, 115((7):1246â€“1256, 2008.
[5] S. R. Blackburn and P. R. Wild. Optimal linear perfect hash families. Journal of Combinatorial Theory, Series A, 83:233â€“250, 1998.
[6] E. J. CandeÌ€s. Compressive sampling. In International Congress of Mathematicians, volume 3, pages 1433â€“1452, 2006.
[7] E. J. CandeÌ€s. The restricted isometry property and its implications for compressed sensing. Compte Rendus de lâ€™Academie des Sciences, Series I,
346:589â€“592, 2008.
[8] E. J. CandeÌ€s, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE
Transactions on Information Theory, 52:489â€“509, 2006.
[9] E. J. CandeÌ€s and T. Tao. Decoding by linear programming. IEEE Transactions on Information Theory, 51:4203â€“4215, 2005.
[10] E. J. CandeÌ€s and T. Tao. Near optimal signal recovery from random projections: Universal encoding strategies. IEEE Transactions on Information
Theory, 52:5406â€“5425, 2006.
[11] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientific Computing, 20(1):33â€“61, 1998.
[12] A. Cohen, W. Dahmen, and R. A. DeVore. Compressed sensing and best k-term approximation. Journal of the American Mathematical Society,
22:211â€“231, 2009.
[13] C. J. Colbourn, D. Horsley, and C. McLean. Compressive sensing matrices and hash families. IEEE Transactions on Communications, 59(7):1840â€“1845,
2011.
[14] C. J. Colbourn, D. Horsley, and V. R. Syrotiuk. Strengthening hash families and compressive sensing. Journal of Discrete Algorithms, 16:170â€“186,
2012.
[15] C. J. Colbourn and A. C. H. Ling. Linear hash families and forbidden configurations. Designs, Codes and Cryptography, 59:25â€“55, 2009.
[16] G. Cormode and S. Muthukrishnan. Combinatorial algorithms for compressed sensing. In Lecture Notes in Computer Science, volume 4056, pages
280â€“294, 2006.
[17] R. A. DeVore. Deterministic constructions of compressed sensing matrices. Journal of Complexity, 23:918â€“925, 2007.
[18] D. L. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE Transactions on Information Theory, 47:2845â€“2862, 2001.
[19] M. Elad and A. M. Bruckstein. A generalized uncertainty principle and sparse representation in pairs of bases. IEEE Transactions on Information
Theory, 48:2558â€“2567, 2002.
[20] J. J. Fuchs. On sparse representations in arbitrary redundant bases. IEEE Transactions on Information Theory, 50:1341â€“1344, 2004.
[21] J. J. Fuchs. Recovery of exact sparse representations in the presence of bounded noise. IEEE Transactions on Information Theory, 51:3601â€“3608, 2005.
[22] A. C. Gilbert, M. A. Iwen, and M. J. Strauss. Group testing and sparse signal recovery. In Proceedings of the 42nd Asilomar Conference on Signals,
Systems, pages 1059â€“1063, 2008.
[23] A. C. Gilbert, M. J. Strauss, J. Tropp, and R. Vershynin. One sketch for all: Fast algorithms for compressed sensing. In Proceedings of the ACM
Symposium on Theory of Computing, pages 237â€“246, 2007.

11

[24] R. Gribonval and M. Nielsen. Sparse representations in unions of bases. IEEE Transactions on Information Theory, 49:3320â€“3325, 2003.
[25] M. A. Iwen. Combinatorial sublinear-time Fourier algorithms. Foundations of Computational Mathematics, 10:303â€“338, 2010.
[26] S. Jafarpour, W. Xu, B. Hassibi, and R. Calderbank. Efficient and robust compressed sensing using optimized expander graphs. IEEE Transactions on
Information Theory, 55:4299â€“4308, 2009.
[27] B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing, 24:227â€“234, 1995.
[28] D. R. Stinson, Tran Van Trung, and R. Wei. Secure frameproof codes, key distribution patterns, group testing algorithms and related structures. Journal
of Statistical Planning and Inference, 86:595â€“617, 2000.
[29] D. R. Stinson, R. Wei, and K. Chen. On generalized separating hash families. Journal of Combinatorial Theory, Series A, 115:105â€“120, 2008.
[30] M. Stojnic, W. Xu, and B. Hassibi. Compressed sensing-probabilistic analysis of a null-space characterization. In Proceedings of the International
Conference on Acoustics, Speech, and Signal Processing, pages 3377â€“3380, 2008.
[31] J. A. Tropp. Recovery of short, complex linear combinations via l1 minimization. IEEE Transactions on Information Theory, 51:1568â€“1570, 2005.
[32] W. Xu and B. Hassibi. Efficient compressive sensing with deterministic guarantees using expander graphs. In Proceedings of IEEE Information Theory
Workshop, 2007.
[33] Y. Zhang. On theory of compressive sensing via â„“1 -minimization: Simple derivations and extensions. Technical Report Technical Report CAAM
TR08-11, Rice University, 2008.

Necessary and sufficient conditions are established for an integer vector to be the f-vector of some pure simplicial complex of rank three, and also for an integer vector to be thef-vector of some pure simplicial multicomplex of rank three. For specified numbers of sets of cardinality one and cardinality two, an upper bound on the number of sets of cardinality three is established using shifting arguments. Then techniques from combinatorial design theory are used to establish a lower bound. Then it is shown that every number of sets of cardinality three between the lower and the upper bound can be realized. This characterization is restated to determine the precise spectrum of possible numbers of sets of cardinality two for specified numbers of sets of cardinality one and three. For simplicial complexes, these spectra are not always intervals, and the gaps are determined precisely. For simplicial multicomplexes, an alternative proof is given that these spectra are always intervals.IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

575

A Discrete Particle Swarm Optimization for
Covering Array Generation
Huayao Wu, Changhai Nie, Member, IEEE, Fei-Ching Kuo, Member, IEEE,
Hareton Leung, Member, IEEE, and Charles J. Colbourn

Abstractâ€”Software behavior depends on many factors.
Combinatorial testing (CT) aims to generate small sets of test
cases to uncover defects caused by those factors and their
interactions. Covering array generation, a discrete optimization
problem, is the most popular research area in the field of CT.
Particle swarm optimization (PSO), an evolutionary search-based
heuristic technique, has succeeded in generating covering arrays
that are competitive in size. However, current PSO methods for
covering array generation simply round the particleâ€™s position
to an integer to handle the discrete search space. Moreover, no
guidelines are available to effectively set PSOs parameters for
this problem. In this paper, we extend the set-based PSO, an
existing discrete PSO (DPSO) method, to covering array generation. Two auxiliary strategies (particle reinitialization and
additional evaluation of gbest) are proposed to improve performance, and thus a novel DPSO for covering array generation
is developed. Guidelines for parameter settings both for conventional PSO (CPSO) and for DPSO are developed systematically
here. Discrete extensions of four existing PSO variants are developed, in order to further investigate the effectiveness of DPSO
for covering array generation. Experiments show that CPSO can
produce better results using the guidelines for parameter settings,
and that DPSO can generate smaller covering arrays than CPSO
and other existing evolutionary algorithms. DPSO is a promising
improvement on PSO for covering array generation.
Index Termsâ€”Combinatorial testing (CT), covering array
generation, particle swarm optimization (PSO).

I. I NTRODUCTION
S SOFTWARE functions and run-time environments
become more complex, testing of modern software systems is becoming more expensive. Effective detection of

A

Manuscript received May 12, 2013; revised December 29, 2013 and
May 18, 2014; accepted September 28, 2014. Date of publication October 9,
2014; date of current version July 28, 2015. This work was supported in part
by the National Natural Science Foundation of China under Grant 61272079,
in part by the Research Fund for the Doctoral Program of Higher Education
of China under Grant 20130091110032, in part by the Science Fund for
Creative Research Groups of the National Natural Science Foundation of
China under Grant 61321491, in part by the Major Program of National
Natural Science Foundation of China under Grant 91318301, and in part
by the Australian Research Council Linkage under Grant LP100200208.
(Corresponding author: Changhai Nie.)
H. Wu and C. Nie are with the State Key Laboratory for Novel
Software Technology, Nanjing University, Nanjing 210023, China (e-mail:
hywu@outlook.com; changhainie@nju.edu.cn).
F.-C. Kuo is with the Faculty of Information and Communication
Technologies, Swinburne University of Technology, Hawthorn, VIC 3122,
Australia (e-mail: dkuo@swin.edu.au).
H. Leung is with the Department of Computing, Hong Kong Polytechnic
University, Hong Kong (e-mail: hareton.leung@polyu.edu.hk).
C. J. Colbourn is with Arizona State University, Tempe, AZ 85287-8809,
USA (e-mail: colbourn@asu.edu).
Digital Object Identifier 10.1109/TEVC.2014.2362532

failures at a low cost is a key issue for test case generation. Combinatorial testing (CT) is a popular testing method
to detect failures triggered by various factors and their interactions [1]. By employing covering arrays as test suites, the CT
method aims to sample the large combination space with few
test cases to cover different interactions among a fixed number of factors. Kuhn and Reilly [2] shows that more than 70%
of the failures in certain software were caused by the interactions of one or two factors, and almost all the failures could
be detected by checking the interactions among six factors.
Therefore, CT can be an effective method in practice.
Generating a covering array with fewest tests (minimum
size) is a major challenge in CT. In general, the minimum
size of a covering array is unknown; hence, methods have
focused on finding covering arrays that have as few tests as
possible at reasonable search cost. The many methods that
have been proposed can be classified into two main groups:
1) mathematical methods and 2) computational methods [1].
Mathematical (algebraic or combinatorial) methods typically
exploit some known combinatorial structure. Computational
methods primarily use greedy strategies or heuristic-search
techniques to generate covering arrays, due to the size of the
search space.
Mathematical methods yield the best possible covering
arrays in certain cases. For example, orthogonal arrays used
in the design of experiments provide covering arrays with a
number of tests that is provably minimum. However, all known
mathematical methods can be applied only for restrictive sets
of factors. This limitation has led to an emphasis on computational methods. Greedy algorithms have been quite effective
in generating covering arrays, but their accuracy suffers from
becoming trapped in local optima.
In recent years, search-based software engineering (SBSE)
has focused on using search-based optimization algorithms
to find high-quality solutions for software engineering problems. Inspired by SBSE, many artificial intelligence-based
heuristic-search techniques have been applied to software
testing. For example, simulated annealing (SA) [3]â€“[7],
genetic algorithm (GA) [8]â€“[10], and ant colony
optimization (ACO) [9], [11], [12] have all been applied
to covering array generation. These techniques can generate
any types of covering arrays, and the constraint solving
and prioritization techniques can be easily integrated. Their
applications have been shown to be effective, producing
relatively small covering arrays in many cases. Particle swarm
optimization (PSO), a relatively new evolutionary algorithm,

c 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
1089-778X 
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

576

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

has also been used in this area [13]â€“[16]. It is easy to
implement and has fast initial progress.
The conventional PSO (CPSO) algorithm was originally
designed to find optimal or near optimal solutions in a
continuous space. Nevertheless, many discrete PSO (DPSO)
algorithms and frameworks have been developed to solve
discrete problems [17]â€“[22]. For covering array generation,
current discrete methods [13]â€“[16] simply round the particleâ€™s
position to an integer while keeping the velocity as a real
number. They suffer from two main shortcomings. First, the
performance of PSO is significantly impacted by its parameter
settings. In [23], effects of the general parameter selection and
initial population of PSO have been analyzed, but no guidelines on parameter settings have been reported for covering
array generation. Hence, a clear understanding of how to set
these execution parameters is needed. Second, simple rounding
fractional positions to integers introduces a substantial source
of errors in the search. Instead, a specialized DPSO version is
needed. Because current PSO methods show promise for generating covering arrays, these two main shortcomings should
be addressed.
In this paper, we adapt set-based PSO (S-PSO) [18] to
generate covering arrays. S-PSO utilizes set and probability
theories to develop a new representation scheme for combinatorial optimization problems as well as refining the related
evolution operators. In our adaptation of S-PSO, two auxiliary strategies are proposed to enhance performance, and
a novel DPSO algorithm is thus proposed. DPSO has the
same conceptual basis and exhibits similar search behavior to
CPSO, with parameters playing similar roles. Then, we explore
the optimal parameter settings for both CPSO and DPSO to
improve their performance, and identify recommended settings. Furthermore, because many CPSO variants [24]â€“[27]
can be easily extended to discrete versions based on our DPSO,
the performance of these discrete versions is also compared
with their original ones. Finally, we compare CPSO and DPSO
with existing GA and ACO [9], [11] algorithms to generate
covering arrays.
The main contributions of this paper are as follows.
1) Based on the set-based representation, we design a
version of S-PSO [18] for covering array generation.
2) We propose two auxiliary strategies, particle reinitialization, and additional evaluation of gbest, to enhance the
performance of PSO. A novel DPSO for covering array
generation is proposed.
3) We design experiments to explore the optimal parameter settings for CPSO and DPSO for covering array
generation.
4) We implement original and discrete versions of four
representative PSO variants (TVAC [24], CLPSO [25],
APSO [26], and DMS-PSO [27]) to compare their
efficacy for covering array generation.
The rest of this paper is organized as follows. Section II
gives background on CT, covering array generation, and
the CPSO algorithm. Section III summarizes related work.
Section IV presents our DPSO algorithm, including the
representation scheme, related operators, and two auxiliary
strategies. Section V evaluates the performance of CPSO and

TABLE I
E-C OMMERCE S OFTWARE S YSTEM

DPSO, and explores optimal parameter settings. Section VI
gives a comparison among CPSO, DPSO, and original and
discrete variants. Section VII compares CPSO and DPSO
with GA and ACO. Section VIII concludes this paper and
outlines future work.
II. BACKGROUND
A. CT
Suppose that the behavior of the software under test (SUT)
is controlled by n independent factors, which may represent configuration parameters, internal or external events, user
inputs, and the like. The ith factor has i discrete levels, chosen from the finite set Vi of size i . An n-tuple (x1 , x2 , . . . , xn )
forms a test case, where xi âˆˆ Vi for 1 â‰¤ i â‰¤ n.
Consider a simple e-commerce software system [15]. This
system consists of five different components. Each of these five
components can be regarded as a factor, and its configurations
can be regarded as different levels. Table I shows these five
factors and their corresponding levels. In this example, n = 5,
1 = 2 = 3 = 2, 4 = 5 = 3.
System failures are often triggered by interactions among
some factors, which can be represented by the combinations
of factor levels. In order to detect these failures, their combinations should be covered at least once by the test suite. A
t-way schema can be used to represent them.
Definition 1 (t-way schema): The n-tuple (âˆ’, y1 , . . . ,
yt , . . . ) is a t-way schema (t > 0) when some t factors
have fixed levels and the others can take any valid levels,
represented as â€œâˆ’.â€
For example, suppose that when factor Payment Server
takes the level Master and factor Web Server takes the level
Apache, a system failure occurs. To detect this failure, the
2-way schema (Master, â€“, Apache, â€“, â€“) must be covered
at least once by the test suite. To simplify later discussion,
we use the index in the level set of each factor to present
a schema. For example, (0, â€“, 1, â€“, â€“) is used to represent
(Master, â€“, Apache, â€“, â€“).
Exhaustive testing covers all n-way schemas of the system; in our example, it uses 2 Ã— 2 Ã— 2 Ã— 3 Ã— 3 = 72 test
cases. Testing cost becomes prohibitive as the numbers of factors and levels increase. Moreover, because only interactions
among few factors are likely to trigger failures [2], testing
high-way schemas can lead to many uninformative test cases.
At the other extreme, if we only guarantee to cover each 1-way
schema once, only three test cases are needed (a single test
case can cover five 1-way schemas at most). But we may
fail to detect some interaction triggered failures involving two
factors.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

TABLE II
C OVERING A RRAY CA(9; 2, 23 32 )

Instead, CT covers all t-way schemas. Such a test suite is a
t-way covering array, with t being the covering strength. The
value of t determines the depth of coverage. It is a key setting
of CT, and should be decided by the testers. We give a precise
definition.
Definition 2 (Covering Array): If an NÃ—n array, where N is
the number of test cases, has the following properties: 1) each
column i (1 â‰¤ i â‰¤ n) contains only elements from the set Vi
with i = |Vi | and 2) the rows of each N Ã— t sub array cover
all |Vk1 | Ã— |Vk2 | Ã— . . . Ã— |Vkt | combinations of the t columns at
least once, where t â‰¤ n and 1 â‰¤ k1 < . . . < kt â‰¤ n, then it is a
t-way covering array, denoted by CA(N; t, n, (1 , 2 , . . . , n )).
When 1 = 2 = . . . = n = , it is denoted by CA(N; t, n, ).
Reference [2] demonstrated that more than 70% failures can
be detected by a 2-way covering array, and almost all failures
can be detected by a 6-way covering array. Hence, using CT,
we can detect many failures of the system by applying relatively low strength covering arrays. In other words, CT can
greatly reduce the size of test suite while maintaining high
fault detection ability.
In the example of Table I, if we only consider the interactions between any two factors, only nine test cases are
required to construct a 2-way covering array instead of 72
for exhaustive testing. Table II shows a covering array, where
each row represents one test case. This table covers all possible 2-way schemas for any two factors corresponding to the
columns. For example, consider factor Payment Server and
User Browser, all 2Ã—3 = 6 schemas, (Master, â€“, â€“, Firefox, â€“)
(Master, â€“, â€“, Explorer, â€“), (Master, â€“, â€“, Chrome, â€“),
(VISA, â€“, â€“, Firefox, â€“), (VISA, â€“, â€“, Explorer, â€“),
(VISA, â€“, â€“, Chrome, â€“), can be found in the table.
For convenience, if several groups of gi factors (gi < n) have
g
the same number of levels ak , ak i can be used to represent
these factors and their levels. Thus, the coveringarray can
g
g
g
gi = n,
be denoted by CA(N; t, a11 , a22 , . . . , ak k ) where
n
or CA(N; t, a ) when g1 = n and a1 = a. For example, the
covering array in Table II is a CA(9; 2, 23 32 ).
In many software systems, the impacts of the interactions
among factors are not uniform. Some interactions may be more
prone to trigger system failures, while other may have little or no impact on the system. To effectively detect these
different interactions, variable strength (VS) covering arrays
can be applied. This can offer different covering strengths

577

TABLE III
A DDING T HREE T EST C ASES TO C ONSTRUCT
VCA(12; 2, 23 32 , CA(3, 22 31 ))

to different groups of factors, and can therefore provide a
practical approach to test real applications.
Definition 3 (VS Covering Array): A VS covering array,
m

1
denoted by VCA(N; t, a11 . . . akk , CA1 (t1 , bm
. . . bp p ), . . . ,
1
n
CAj (tj , cn11 . . . cqq )), is an N Ã— n covering array of covering
strength t containing one or more sub covering arrays, namely
CA1 , . . . , CAj , each of which has covering strength t1 , . . . , tj
all larger than t.
Consider the e-commerce system shown in Table I. If the
interactions of three factors, Payment Server, Web Server, and
Business Database, have a higher probability to trigger system failures, then a VS covering array can be constructed.
As in Table II, only three more test cases (Table III) are
needed to construct the VCA(12; 2, 23 32 , CA(3, 22 31 )). With
these 12 test cases, not only are all 2-way schemas of all five
factors covered, but also all 3-way schemas of these three factors (Payment Server, Web Server, and Business Database) are
covered.
B. Covering Array Generation
Covering arrays are used as test suites in CT. Covering array
generation is the process of test suite construction. It is the
most active area in CT with more than 50% of research papers
focusing on this field [1]. Due to limited testing resources, all
aim to construct a minimal covering array while still maintaining full coverage of combinations. Computational methods
have been used widely for covering array generation because
they can be applied to any systems. In general, these methods
generate all possible combinations first. Then they generate
test cases to cover these combinations one-by-one. One-testat-a-time is the most widely used strategy among evolutionary
algorithms to generate a covering array.
The one-test-at-a-time strategy was popularized by
AETG [28] and was further used by Bryce and Colbourn [29].
This strategy takes the model of SUT(n, 1 , . . . , n ) where
n is the number of factors and i is the number of valid
levels of factor i, and the covering strength t as input. At
first, an empty test suite TS and a set S of t-way schemas
to be covered are initialized. In each iteration, a test case is
generated with the highest fitness value according to some
heuristic techniques. Then it is added to TS and the t-way
schemas covered by it are removed. When all the t-way
schemas have been covered, the final test suite TS is returned.
This process is shown in Algorithm 1.
In this strategy, a fitness function must be used to evaluate
the quality of a candidate test case (line 6 in Algorithm 1). It is
an important part of all heuristic techniques. In covering array
generation, the fitness function takes the test case as the input
and then outputs a fitness value representing its â€œgoodness.â€
It is defined as follows.

578

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

Algorithm 1 One-Test-at-a-Time Strategy
1: Input: SUT(n, 1 , . . . , n ) and covering strength t
2: Output: covering array TS
3: TS = âˆ…
4: Construct S (all the t-way schemas to be covered) based
on n, 1 , . . . , n and t
5: while S  = âˆ… do
6:
Generate a test case p with the highest fitness value
according to some heuristics
7:
Add p to the test suite TS
8:
Remove the t-way schemas covered by p from S
9: end while
10: return TS

Definition 4 (Fitness Function): Let TS be the generated
test set, and p be a test case. Then fitness(p) is the number of
uncovered t-way schemas in TS that are covered by p.
The fitness function can be formulated as
fitness(p) = |schemat ({p}) âˆ’ schemat (TS)|

(1)

where schemat (TS) represents the set of all t-way schemas
covered by test set TS, and | Â· | stands for cardinality. When
all Cnt t-way schemas covered by p are not covered by TS,
the fitness function reaches the maximum value fitness(p) =
|schemat ({p})| = Cnt .
For example, consider the 2-way covering array generation
of the e-commerce system shown in Table II. Suppose that
TS consists of test cases (0, 0, 0, 0, 0) and (0, 0, 1, 1, 1).
The fitness function computes the fitness value for a candidate test case p = (1, 0, 1, 2, 2) as follows: because p covers
ten 2-way schemas, namely schema2 ({p}) = {(1, 0, â€“, â€“, â€“),
(1, â€“, 1, â€“, â€“,), (1, â€“, â€“, 2, â€“), (1, â€“, â€“, â€“, 2), (â€“, 0, 1, â€“, â€“),
(â€“, 0, â€“, 2, â€“), (â€“, 0, â€“, â€“, 2), (â€“, â€“, 1, 2, â€“), (â€“, â€“, 1, â€“, 2),
(â€“, â€“, â€“, 2, 2)} and TS only covers (â€“, 0, 1, â€“, â€“) in p, the
function returns fitness(p) = 9.
C. PSO
PSO is a swarm-based evolutionary computation technique.
It was developed by Kennedy et al. [30], inspired by the social
behavior of bird flocking and fish schooling. PSO utilizes a
population of particles as a set of candidate solutions. Each
of the particles represents a certain position in the problem
hyperspace with a given velocity. A fitness function is used
to evaluate the quality of each particle. Initially, particles are
distributed in the hyperspace uniformly. Then each particle
repeatedly updates its state according to the individual best
position in its history (pbest) and the current global best position (gbest). Eventually, each particle possibly moves toward
the direction of the individual optimum and global optimum,
and finds an optimal or near optimal solution.
Suppose that the problem domain is a Dâˆ’dimensional
hyperspace. Then the position and velocity of particle i can
be represented by xi âˆˆ RD and vi âˆˆ RD respectively. CPSO
uses the following equations to update a particleâ€™s velocity
and position, where vi,j (k) represents the jth component of the
velocity of particle i at the kth iteration, and xi,j (k) represents

its corresponding position:
vi,j (k + 1) = Ï‰ Ã— vi,j (k) + c1 Ã— r1,j Ã— (pbesti,j âˆ’ xi,j (k))
(2)
+ c2 Ã— r2,j Ã— (gbestj âˆ’ xi,j (k))
xi,j (k + 1) = xi,j (k) + vi,j (k + 1).

(3)

The best position of particle i in its history is pbesti , and
gbest is the best position among all particles. The velocityupdate equation (2) captures the three basic concepts of PSO.
The first is inertia, the tendency of the particle to continue in
the direction it has been moving. The second is memory of
the best position ever found by itself. The third is cooperation
using the best position found by other particles.
The parameter Ï‰ is inertia weight. It controls the balance between exploration (global search state) and exploitation
(local search state). Two positive real numbers c1 and c2
are acceleration constants that control the movement tendency toward the individual and global best position. Most
studies set Ï‰ = 0.9, and c1 = c2 = 2 to get the best
balance [17], [31], [32]. In addition, r1,j and r2,j are two uniformly distributed random numbers in the range of [0.0, 1.0],
used to ensure the diversity of the population.
If the problem domain (the search space of particles) has
bounds, a bound handling strategy is adopted to keep the particles inside the search space. Many different strategies have
been proposed [33]. In the reflecting strategy, when a particle
exceeds the bound of the search space in any dimension, the
particle direction is reversed in this dimension to get back to
the search space. For example, in case of a dimension with
a range of values from 0 to 2, if a particle moves to 3, its
position is reversed to 1.
In addition, as the velocity can increase over time, a limit
is set on velocity to prevent an infinite velocity or invalid
position for the particle. Setting a maximum velocity, which
determines the distance of movement from the current position
to the possible target position, can reduce the likelihood of
explosion of the swarm traveling distance [31]. Generally, the
value of the maximum velocity is selected as i /2, where i
is the range of dimension i.
The pseudocode in Algorithm 2 presents the process of generating a test case by PSO. This algorithm can be invoked
by line 6 of Algorithm 1 to generate a test case for t-way
schemas. The n factors of the test case can be treated as an
n-dimensional hyperspace. A particle pi = (x1 , x2 , . . . , xn ) can
be regarded as a candidate test case. The fitness function is
that of Definition 4, the number of uncovered t-way schemas
in the generated test suite that are covered by particle pi . PSO
employs real numbers but the valid values are integers for covering array generation, so each dimension of particleâ€™s position
can be rounded to an integer while maintaining the velocity
as a real number. This method is used in all prior research
applying PSO to covering array generation [13]â€“[16].
III. R ELATED W ORK
In this section, three different but related aspects are discussed. We first summarize search-based CT, especially the
current applications of PSO for covering array generation.
Then, we introduce prior research on discrete versions of PSO,

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

Algorithm 2 Generate Test Case by PSO
1: Input: SUT(n, 1 , . . . , n ), covering strength t and the
related parameters of PSO
2: Output: the best test case gbest
3: it = 0, gbest = NULL
4: for each particle pi do
5:
Initialize the position xi and velocity vi randomly
6: end for
7: while it < maximum iteration do
8:
for each particle pi do
9:
Compute the fitness value fitness(pi )
10:
if fitness(pi ) > fitness(pbesti ) then
11:
pbesti = pi
12:
end if
13:
if fitness(pi ) > fitness(gbest) then
14:
gbest = pi
15:
end if
16:
end for
17:
for each particle pi do
18:
Update the velocity and position according to
Equations 2 and 3
19:
Apply maximum velocity limitation and bound
handling strategy
20:
end for
21:
it = it + 1
22: end while
23: return gbest

to improve CPSO for discrete problems. Finally, some PSO
variants are introduced. These try to enhance PSO for conventional continuous problems by adopting strategies that can
further improve covering array generation.
A. PSO in Search-Based CT
SBSE has grown quickly in recent years. Many problems
in software engineering have been formulated into searchbased optimization problems, and heuristic techniques have
been used to find solutions. Software testing is a major topic
in software engineering. Many heuristic techniques have also
been applied to this field, including functional testing, mutation testing, stress testing, temporal testing, CT, and regression
testing. Currently, many classic heuristic techniques, such as
SA [3]â€“[7], GA [8]â€“[10], and ACO [9], [11], [12] have been
applied to generate uniform and VS covering arrays
successfully.
PSO has also been applied to software testing.
Windisch et al. [34] applied PSO to structural testing,
and compared its performance with GA. They showed that
PSO outperformed GA for most cases in terms of effectiveness and efficiency. Ganjali [35] proposed a framework for
automatic test partitioning based on PSO, observing that PSO
performed better than other existing heuristic techniques.
PSO has been applied to covering array generation.
Ahmed and Zamli [15] proposed frameworks of PSO to generate different kinds of covering arrays, including t-way
(PSTG [14]) and VS-PSTG. Then they extended PSTG to support VS covering array generation [16]. Chen et al. [18] also

579

applied PSO to 2-way covering array generation. They further
used a test suite minimization algorithm to reduce the size of
the generated covering array.
Current applications of PSO for covering array generation
can yield smaller covering arrays than most greedy algorithms,
but they all apply the same rounding operator to the particleâ€™s
position, and they lack guidelines on the parameter settings.
B. Discrete Versions of PSO
PSO was initially developed to solve problems in continuous
space, but PSO can also be applied to some discrete optimization problems including binary or general integer variables.
Many discrete versions of PSO have been proposed [17]â€“[22].
Chen et al. [18] classify existing algorithms into four
types.
1) Swap operator-based PSO [19] uses a permutation of
numbers as position and a set of swaps as velocity.
2) Space transformation-based PSO [20] uses various techniques to transform the position, defined as a vector of
real numbers, to the corresponding solution in discrete
space.
3) Fuzzy matrix-based PSO [21] defines the position and
velocity as a fuzzy matrix, and decode it to a feasible
solution.
4) Incorporating-based PSO [22] consists of hybrid methods incorporating some heuristic or problem dependent
techniques.
Chen et al. [18] also propose a S-PSO method based on
sets with probabilities, which we later adapt to represent a
particleâ€™s velocity. S-PSO uses a set-based scheme to represent the discrete search space. The velocity is defined as
a set with probabilities, and the operators are all replaced
by procedures defined on the set. They extend some PSO
variants to discrete versions and test them on the traveling
salesman problem and the multidimensional knapsack problem. They show that the discrete version of CLPSO [25] can
perform better than other variants. S-PSO was found to characterize the combinatorial optimization problems very well.
Gong et al. [36] then employed such a set-based representation scheme for solving vehicle routing problems to obtain a
new method, S-PSO-VRPTW.
C. PSO Variants
The original PSO may become trapped in a local optimum.
In order to improve the performance, many variants have been
proposed [17], [24]â€“[27]. Chen et al. [37] classify these variants into four different groups. The first group of variants aims
to adjust the control parameters during the evolution, for example by decreasing inertia weight Ï‰ linearly from 0.9 to 0.4 over
the search process. The second group employs different neighborhood topologies. The basic local version of PSO makes
the particle learn from the local best position lbesti found by
particle iâ€™s neighborhood instead of the global best position
gbest. RPSO and VPSO are two common versions which use
a ring topology and a Von Neumann topology, respectively.
The third group uses hybrid strategies with other search techniques. Many types of operators, such as genetic operators

580

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE IV
D IFFERENT G ROUPS OF DPSO AND PSO VARIANTS

(selection, crossover, and mutation), evolutionary computation
paradigms and biology inspired operators have been used. The
fourth uses multiswarm techniques. Several sets of swarms
optimize different components of the solution concurrently or
cooperatively.
In our experiments, four representative PSO variants are
included, as follows.
1) Ratnaweera et al. [24] proposed a typical variant of
the first group, TVAC, which uses a time varying
inertia weight and acceleration constant to adjust the
parameters.
2) Liang et al. [25] proposed the CLPSO method, a wellknown variant of the second group, which allows the
particle to learn from other particlesâ€™ individual best
positions in different dimensions.
3) Zhan et al. [26] proposed an adaptive PSO (APSO)
that can be seen as a variant of the third group. They
developed an evolutionary state estimation to adaptively control the parameter and used an elitist learning
strategy.
4) DMS-PSO [27], a variant of the fourth group, was proposed by Liang and Suganthan [27]. It is characterized
by a set of swarms with small sizes and these swarms
are frequently regrouped.
In summary, Table IV lists the different groups of discrete
versions of PSO and PSO variants.
IV. DPSO
In this section, a new DPSO for covering array generation
is presented. We firstly illustrate the weakness of CPSO with a
simple example. Then the representation scheme of a particleâ€™s
velocity and the corresponding redefined operators are introduced. Finally, we give two auxiliary strategies to enhance the
performance of DPSO.
A. Example
In CPSO, a particleâ€™s position represents a candidate test
case; its velocity, a real vector, represents the movement tendency of this particle in each dimension. This scheme is
meaningful in a continuous optimization problem, because
an optimal solution may exist near the current best particleâ€™s
position. So it is desirable to move the particle to this area
for further search. This may not hold for covering array
generation.

Here, we use CA(N; 2, 34 ) as an example. Suppose that
three test cases (0, 1, 0, 1), (2, 1, 1, 2), and (1, 2, 0, 0) have
been generated and added to TS in Algorithm 1, and the fourth
one is to be generated according to CPSO. A possible candidate particle pi may have the position (0, 0, 0, 0) and velocity
(0.5, 0.6, âˆ’0.4, 0.2) with fitness value 4, and its individual
best position pbesti may be (0, 0, 1, 1) with fitness value 5.
The global best position gbest may be (0, 2, 2, 2) with fitness
value 6. According to the update (2), if we take Ï‰ = 0.9 and
c1 Ã— r1 = c2 Ã— r2 â‰ˆ 0.65, in the next iteration, pi â€™s velocity
may be transformed to (0.45, 1.84, 1.59, 2.13). Thus, pi moves
to the new position (0, 1, 1, 1) with a limitation on the velocity (the velocity is kept in [âˆ’i /2, i /2]), or (0, 1, 1, 2) with
no limitation. In both cases, pi only has fitness value 2 after
updating. When this occurs, pi evolves to a worse situation,
although its position is closer to the pbesti and gbest than its
original one.
Analyzing the fitness measurement, the main contribution
to the fitness value is the combinations that the test case can
cover, not the concrete â€œpositionâ€ at which it is located. For
example, test case (2, 1, 1, 2) has a larger fitness value than
(0, 0, 0, 0) because it covers six new schemas [(2, 1, â€“, â€“),
(2, â€“, 1, â€“), (2, â€“, â€“, 2) etc.], not because of its relative distance
to other particles.
B. DPSO
To overcome this weakness, the movement of particles
should be modified. Inspired by the set-based representation scheme of velocity, DPSO is designed as a version of
S-PSO [18] to make the particle learn from the individual and
global best more effectively when generating covering arrays.
Unlike S-PSO, the element of the velocity set in DPSO is
designed for covering array generation, and DPSO does not
classify the velocity set into different dimensions to avoid the
inconsistency of different dimensions when updating velocities
in S-PSO.
In DPSO, a particleâ€™s position represents a candidate test
case, while its velocity is changed to a set of t-way schemas
with probabilities. Other than the velocityâ€™s representation and
the newly defined operators, the evolution procedure of DPSO
is the same as CPSO (Algorithm 2).
Definition 5 (Velocity): The velocity is a set of pairs
(s, p(s)), where s is a possible t-way schema of the covering
array and p(s) is a real number between 0 and 1 representing the probability of the selection of schema s to update the
current position.
In the initialization of the swarm, the particleâ€™s position is
randomly assigned. Then Cnt possible different schemas are
selected randomly and each of them is assigned a random
probability p(s) âˆˆ (0, 1). These Cnt pairs form the initial velocity set of this particle; the size of this set changes dynamically
during the evaluation.
We consider the same example CA(N; 2, 34 ). In DPSO,
when particle pi is initialized, its position xi (k) may be
(0, 0, 0, 0) representing a candidate test case as before, and
its velocity vi (k) may be such a set {((1, 1, â€“, â€“), 0.7),
((0, â€“, 0, â€“), 0.3), ((â€“, 0, â€“, 1), 0.8), ((0, â€“, â€“, 2), 0.9),

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

(a)

(b)

(c)

581

(d)

(e)

Fig. 1. Example of pi â€™s velocity updating. (a) 0.9 Ã— vi (k). (b) 2 Ã— r1 Ã— (pbesti âˆ’ xi (k)). (c) 2 Ã— r2 Ã— (gbest âˆ’ xi (k)). (d) vi (k + 1). (e) Final vi (k + 1) where
pro1 = 0.5.

((â€“, 2, 1, â€“), 0.5), ((â€“, â€“, 0, 1), 0.2)} which contains C42 = 6
pairs.
DPSO follows the conventional evolution procedure
(Algorithm 2) that uses (2) and (3) to update the velocity and
position of a particle. To adapt the new scheme for velocity
in Definition 5, the related operators in these equations must
be redefined.
1) Coefficient Ã— Velocity: The coefficient is a real number
which may be a parameter or a random number. It modifies
all the probabilities in the velocity.
Definition 6 (Coefficient Ã— Velocity): Let a be a nonnegative real number and v be a velocity, a Ã— v = {(s, p(s) Ã—
a)|(s, p(s)) âˆˆ v}. (If p(s) Ã— a > 1, p(s) Ã— a = 1.)
For example, Fig. 1(a) shows the result for Ï‰ Ã— vi (k) where
Ï‰ = 0.9.
2) Positionâ€“Position: The difference of two positions gives
the direction on which a particle moves. The results of the
minus operator is a set of (s, p(s)) pairs, as velocity.
Definition 7 (Positionâ€“Position): Let x1 and x2 be two positions. Then x1 âˆ’ x2 = {(s, 0.5)|s is a schema that exists in x1
but not in x2 }.
In the newly generated schema, probability p(s) for s is
set to 0.5 so that the acceleration constants take similar
values in both CPSO and DPSO. As in (2), the result of
positionâ€“position is multiplied by ci Ã— ri . In CPSO, ci is
often set to 2 and ri is a random number between 0 and 1
(recall Section II-C). In DPSO, we want the value of final
probability to have a range between 0 and 1 after multiplying
by ci . Setting p(s) to 0.5 puts the result in [0, 1] when ci = 2.
For example, suppose that xi (k) = (0, 0, 0, 0), pbesti =
(0, 0, 1, 1), and gbest = (0, 2, 2, 2) as before.
We can get pbesti âˆ’ xi (k) = {((0, â€“, 1, â€“, â€“), 0.5),
((0, â€“, â€“, 1), 0.5), ((â€“, 0, 1, â€“), 0.5), ((â€“, 0, â€“, 1), 0.5),
((â€“, â€“, 1, 1), 0.5)}. Fig. 1(b) and (c) shows the results for
2Ã—r1 Ã—(pbesti âˆ’xi (k)) and 2Ã—r2 Ã—(gbestâˆ’xi (k)) respectively.
3) Velocity + Velocity: The addition of velocities gives a
particleâ€™s movement path. The plus operator results in the
union of two velocities.
Definition 8 (Velocity + Velocity): Let v1 and v2 be two
velocities. Then v1 + v2 = {(s, p(s))| if (s, p1 (s)) âˆˆ v1 and
(s, p2 (s)) âˆˆ v2 , p(s) = p1 (s) + p2 (s); if (s, pi (s)) âˆˆ v1
and (s, pi (s)) âˆˆ
/ v2 or (s, pi (s)) âˆˆ v2 and (s, pi (s)) âˆˆ
/ v1 ,
p(s) = pi (s)}. (If p1 (s) + p2 (s) > 1, p1 (s) + p2 (s) = 1.)
For example, Fig. 1(d) shows the results for pi â€™s new
velocity vi (k + 1) after updating velocity.

Algorithm 3 Position Updating
1: Input: position xi (k), velocity vi (k + 1), pro2 and pro3
2: Output: new position xi (k + 1)
3: xi (k + 1) = (â€“, â€“, . . . , â€“)
4: Sort vi (k + 1) in descending order of p(s)
5: for each pair (si ,p(si )) in vi (k + 1) do
6:
Generate a random number Î± âˆˆ [0, 1]
7:
if Î± < p(si ) then
8:
for each fixed level  in si do
9:
Generate a random number Î² âˆˆ [0, 1]
10:
if Î² < pro2 and the corresponding factor
of  has not been fixed in xi (k + 1) then
11:
Update xi (k + 1) with 
12:
end if
13:
end for
14:
end if
15: end for
16: if xi (k + 1) has unfixed factors then
17:
Fill these factors by the same levels of previous
position xi (k)
18: end if
19: Generate a random number Î³ âˆˆ [0, 1]
20: if Î³ < pro3 then
21:
randomly change the level of one factor of xi (k + 1)
22: end if
23: return xi (k + 1)

In the velocity-updating phase of DPSO (2), we introduce
a new parameter pro1 to control the size of the final velocity
set. If p(si ) < pro1 , the pair (si , p(si )) is removed from the
final velocity. For example, if we set pro1 = 0.5, vi (k + 1) is
removed as shown in Fig. 1(e). Here, the velocity has been
sorted in descending order of p(s), where if several p(s) have
the same value, they are in an arbitrary order. If vi (k + 1)
becomes empty, it stays empty until new pairs are added to
it. As long as the velocity is empty, the particleâ€™s position is
not updated and no better solutions can be found from this
particle. In Section IV-C1, we discuss how to reinitialize this
particle.
4) Position+Velocity: Position plus velocity is the position
updating phase. Algorithm 3 gives the pseudo code of this
procedure. Here, two new parameters, pro2 and pro3 , numbers
in the range [0, 1], are introduced. pro2 is used to determine

582

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

the probability of selecting each fixed level in schema s and
pro3 is a mutation probability to make the particle mutate
randomly.
An example helps to describe this procedure. We already
have pi â€™s current position xi (k) = (0, 0, 0, 0), and its updated
velocity vi (k + 1) in descending order of p(s) as shown in
Fig. 1(e). We also assume that pro2 and pro3 are both set to
0.5. Each schema si here is selected to update the position
with probability p(si ). For the first pair ((0, â€“, â€“, 2), 1.0),
suppose that the random number Î± satisfies Î± < 1.0. Then,
for each fixed level of this pair, namely level 0 of the first
factor and level 2 of the fourth factor, its corresponding factor
has not been fixed in xi (k + 1). Suppose that we have the
first Î² < 0.5 but the second Î² > 0.5, the first factor will be
selected to update the position and the second factor will not.
So the new position becomes (0, â€“, â€“, â€“). For the second pair,
we regenerate the random number Î±, and compare it with the
probability 0.9. If Î± < 0.9, the second pair is selected. If we
generate Î² < 0.5 in two rounds, the new position becomes
(0, 2, 2, â€“). Accordingly, if the third pair is selected, and its
second factorâ€™s level 0 is chosen to update, it does not change
position because this factor has been set to a fixed level 2.
This procedure is repeated until all factors in the new position
xi (k+1) are set to fixed levels. If all pairs in velocity have been
considered, unfixed factors of xi (k + 1) are filled by the same
levels of previous position xi (k). For example, after finishing
the For loop in line 15, if the fourth factor of xi (k + 1) has
not been given any level, the fourth factor of xi (k) is used to
update it. Then xi (k + 1) becomes (0, 2, 2, 0).
C. Auxiliary Strategies
Auxiliary strategies can be introduced to improve the performance of PSO. Two of them are added to DPSO: 1) particle
reinitialization to make the search more effective and 2) additional evaluation of gbest to improve the selection of the global
best test case.
1) Particle Reinitialization: The PSO algorithm starts with
a random distribution of particles, which finally converge.
Then the best position that has been found is returned. The
swarm may jump out of a local optimum, but this can not
be guaranteed because CPSO lacks specific strategies for this.
When applying PSO for covering array generation, increasing
the number of iterations does not improve the ability to escape
a local optimum. Hence, particle reinitialization, a widely used
method, is employed to help DPSO to jump out of the local
optimum.
The main issue with particle reinitialization is when to reinitialize the particle. We may select a threshold, so that the
reinitialization is done when the number of iterations exceeds
the threshold. In DPSO, a better method can be applied. Using
the new representation for velocity, when the current particle pi â€™s position equals its individual best position pbesti
and global best position gbest, the size (norm) of pi â€™s velocity reduces gradually, because no pairs are generated from
(pbesti âˆ’ xi ) and (gbest âˆ’ xi ), and the original pairs in velocity
are removed gradually under the influence of Ï‰ Ã— v (reduce
the p(s) of original pairs) and parameter pro1 . After a few
fluctuations around gbest, the particle may stay at gbest, and

TABLE V
T WO D IFFERENT C ONSTRUCTIONS OF CA(N; 2, 34 )

its size (norm) of velocity is zero. We can use this scenario to
trigger reinitialization of the particle. When the reinitialization
is done, each dimension of a particleâ€™s position is randomly
assigned a valid value, and its velocity is regenerated as in the
initialization of the swarm.
2) Additional Evaluation of gbest: Current PSO methods
to generate covering arrays generally employ the fitness function in Definition 4. This fitness function only focuses on
the current candidate, and does not consider the partial test
suite TS.
Consider the generation of CA(N; 2, 34 ). Table V shows two
different construction processes. Both constructions generate
(0, 0, 0, 0) as the first test case. Then they choose different test
cases, but each of the first three reaches the largest number of
newly covered combinations, C42 = 6. The difference between
these two constructions emerges when generating the fourth
test case. In Construction 1, because the combinations with the
same level between any two factors have all been covered, we
cannot find a new test case that can still cover six combinations. However, in Construction 2, such a new test case can be
found, (1, 0, 1, 2). Because the minimum size of CA(N; 2, 34 )
is 9, each test case is required to cover six new combinations.
Thus, Construction 1 cannot generate the minimum test suite,
but Construction 2 can.
In general, there may exist multiple test cases with the same
highest fitness value, which make them equally qualified to be
gbest in Algorithm 2. Instead of arbitrarily selecting one as
gbest, it is better to apply additional distance metric to select
one among them. As shown in Table V, if the new test case
is similar to the existing tests [as (0, 1, 1, 1) is closer to
(0, 0, 0, 0) than (1, 1, 1, 1)], there may be a better chance to
find test cases with larger fitness subsequently.
In order to measure the â€œsimilarityâ€ between a test case t
and an existing test suite TS, we use the average Hamming
distance. The Hamming distance d12 indicates the number of
factors that have different levels between two test cases t1 and
t2 . Hence, the similarity between t and TS can be defined by
the average Hamming distance
H(t, TS) =

1 
dtk .
|TS|

(4)

kâˆˆTS

The new gbest selection strategy selects the best candidate gbest based on two metrics. A test case that has the
minimum average Hamming distance (i.e., the maximum similarity) from the set of test cases that all have the same highest
fitness value is selected as gbest in Algorithm 2. For example, when we generate the second test case for CA(N; 2, 34 )
in Table V, although (1, 1, 1, 1) and (0, 1, 1, 1) both have
the highest fitness value 6, (0, 1, 1, 1) is selected as gbest

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

583

TABLE VI
C OMPARING P ERFORMANCE B ETWEEN CPSO AND DPSO

because its average Hamming distance, 3, is smaller than that
of (1, 1, 1, 1), 4. We expect that this additional evaluation can
enhance the probability of generating a smaller test suite. In
addition, because we still want to make the particle follow the
conventional search behavior on the individual best direction,
this additional distance metric will not be used in updating the
pbest of DPSO.
V. E VALUATION AND PARAMETER T UNING
In this section, we first evaluate the effectiveness of DPSO
in some representative cases, and compare the results against
CPSO. Then the optimal parameter settings for both CPSO
and DPSO are explored. The goal of evaluation and parameter
tuning is to make the size of generated covering array as small
as possible. Five representative cases of covering arrays, listed
below, are selected for our experiments
CA1 (N; 2, 610 )

CA2 (N; 3, 57 )

CA3 (N; 4, 39 )

CA4 (N; 2, 43 53 62 ) VCA(N; 2, 315 , CA(3, 34 )).
We consider four independent parameters, iteration number
(iter), population size (size), inertia weight (Ï‰), and acceleration constant (c), which play similar roles in both CPSO
and DPSO, and three new parameters for DPSO, pro1 , pro2 ,
and pro3 . We carry out a base choice experiment to study the
impact of various values of these parameters on CPSO and
DPSOâ€™s performance and find the recommended settings for
them. First, a base setting is chosen as the basic configuration. Then the value of each parameter is changed to create
a series of configurations, while leaving the other parameters
unchanged. Initially, we set iter = 50, size = 20, Ï‰ = 0.9,
c = 1.3, and pro1 = pro2 = pro3 = 0.5 as a basic configuration based on the previous studies [13], [16] and our
empirical experience. To obtain statistically significant results,
the generation of each case of covering array is executed
30 times.
A. Evaluation of DPSO
We compare the performance between CPSO and DPSO
with the basic configuration. Five classes of covering arrays
are generated by these two algorithms. The sizes obtained and
average execution time per test case are shown as CPSO1 and
DPSO in Table VI. The best and mean array sizes of DPSO are
all better than those of CPSO1 . The new discrete representation scheme and auxiliary strategies improve PSO for covering
array generation.
DPSO can produce smaller covering arrays than CPSO with
the basic configuration. However, DPSO spends more time

during the evolution, because its new operators for updating
are more intricate than the conventional ones. DPSO needs
to deal with many elements of the velocity set, whereas the
conventional scheme only needs simple arithmetic operations.
To compare the performance between CPSO and DPSO given
the same execution time, for each case we let the execution
time per test case for CPSO equal to that for DPSO, so that
CPSO can spend more time in searching. We refer to this
version of CPSO as CPSO2 . In addition, a t-test between
CPSO2 and DPSO is conducted and the corresponding p-value
is presented. A p-value smaller than 0.05 indicates that the performance of these two algorithms is statistically different with
95% confidence.
From CPSO1 , CPSO2 , and DPSO in Table VI, the performance of CPSO is improved with more search time. DPSO,
which must use fewer iterations, still works better than CPSO.
The effectiveness of DPSO comes from the essential improvement of its new representation scheme for velocity and the
auxiliary strategies. The results of the t-test demonstrate the
significance of these differences. Therefore, we can conclude
that DPSO performs better than CPSO with fewer iterations
for covering array generation.
B. Parameter Tuning
In the base choice experiment, because the sizes of the various covering arrays differ, the mean size of each case (si )
obtained is normalized using
si =

si âˆ’ smin
.
smax âˆ’ smin

This normalization enables the graphical representation of
different cases on a common scale.
Because some parameters may not significantly impact the
performance, we use ANOVA (significance level = 0.05) to
test whether there exist significant differences among the mean
results obtained by different parameter settings. When changing the parameter settings have no significant impact on the
generation results, these results will be presented as dotted
lines in the corresponding figures. For example, CA4 is presented as a dotted line in Fig. 2(b). It means that the iteration
number does not significantly impact the generation of this
case of covering array, and so this case will not be further
considered when identifying the optimal settings.
1) Iteration Number (iter): Iteration number determines the
number of updates. PSO typically requires thousands of iterations to evolve. Nevertheless, only a few iterations are required
to generate covering array according to [14]â€“[16]. So we

584

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

(a)
Fig. 2.

(b)

Comparing array sizes under different iteration numbers. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 3.

Comparing array sizes under different population sizes. (a) CPSO. (b) DPSO.

(a)
Fig. 4.

(b)

Comparing array sizes under different settings of population size and iteration number. (a) CPSO. (b) DPSO.

change its value from 50, the base setting, to 1950 incrementally. Much larger values are not used because the execution
time may increase markedly without a commensurate increase
in the quality of the results.
Fig. 2 shows the results for different choices of iteration numbers, where each line represents a covering array.
Performance is improved with increasing iterations for both
CPSO and DPSO. A small number of iterations may not be
appropriate due to insufficient searching. Because the optimal
settings are different among different cases, and several settings can be chosen to generate the minimum covering arrays,
it is open to debate which setting is the â€œbestâ€ one. Given
time constraints, for a population size of 20, a good setting
of iteration number could be approximately 1000 for both
CPSO and DPSO.
2) Population Size (Size): Population size determines the
initial search space. Generally a small value, e.g., 20, is satisfactory for most cases. A large population size may bring a
higher diversity and find a better solution, but it also increases
the evolving time. For the same reason as before, we change
the value of population size from 10 to 100.

Fig. 3 shows the results for different choices of population size. There is no doubt that the mean array size obtained
decreases as population size increases. A large population size
can have more chances to generate smaller covering arrays, but
its execution time can become prohibitive.
In addition, because iteration number and population size
together determine the search effort of PSO, we explore the
combinations between these two parameters. We let iter Ã— size
be a constant 20 000, and generate each case under different
settings of these two parameters. Fig. 4 shows the results,
where PSO prefers a relatively larger population size. In
CPSO, ten particles with 2000 iterations is the worst setting,
and most cases produce good results with 60 or 80 particles.
In DPSO, the best choice of population size is still 80. In
both CPSO and DPSO, the largest population size 100 cannot
produce as good results as that of 80 due to having fewer iterations. So achieving smallest covering arrays requires a good
balance between these two parameters, and a moderately large
population size is necessary for both CPSO and DPSO. Thus,
we can set iteration number to 250, and population size to 80,
as the recommended settings for both CPSO and DPSO.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

(b)

(a)
Fig. 5.

Comparing array sizes under different inertia weights. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 6.

Comparing array sizes under different acceleration constants. (a) CPSO. (b) DPSO.

(a)
Fig. 7.

585

(b)

(c)

Comparing array sizes under different settings of (a) pro1 , (b) pro2 , and (c) pro3 .

3) Inertia Weight (Ï‰): Inertia weight determines the tendency of the particle to continue in the same direction. A
small value help the particle move primarily toward the best
position, while a large one is helpful to continue its previous
movement. A linearly decreasing value is also used as it can
make the swarm gradually narrow the search space. Here, we
investigate both fixed values and a linearly decreasing value
from 0.9 to 0.4 over the whole evolution (presented as â€œdecâ€).
Fig. 5 shows the results for different choices of inertia
weight. In CPSO, most of the smallest covering arrays are
generated by large fixed inertia weights. The decreasing value
does not perform as well as the large values, such as 0.9 for
CA1 , CA2 , and CA4 . So 0.9 can be the recommended choice
for CPSO. In DPSO, 1.1 is the worst choice because the particle may update its position based on its own velocity and
thus fail to learn from individual and global best positions.
The decreasing value can perform reasonably well, but a fixed
value 0.5 may be a better choice in that it keeps the effort of
global search moderate. Thus, we can recommend the fixed
inertia weight of 0.9 for CPSO, and 0.5 for DPSO.
4) Acceleration Constant (c): Acceleration Constants
c1 and c2 control the learning from pbest and gbest, respectively. Generally, they are set to the same value to balance

TABLE VII
R ECOMMENDED PARAMETER S ETTINGS

the influence of these two positions. Setting c1 and c2 to a
large value may make the particle more likely attracted to the
best position ever found, while a small one may make the
search far from the current optimal region. In this paper, we
set c1 = c2 = c, and vary c from 0.1 to 2.9.
Fig. 6 shows the results for different choices of acceleration
constant. Unlike other parameters, there is a consistent trend in
all five cases. In CPSO, the values larger than 0.5 all produce
good results. In DPSO, 1.3 can definitely be regarded as the
optimal value. Thus, we set 1.3 as the recommended value of
acceleration constant for both CPSO and DPSO.
5) pro1 , pro2 , and pro3 : These three parameters are new to
DPSO. Parameter pro1 determines the size of the final velocity. As those pairs whose probabilities are smaller than pro1
are removed from the final velocity set, a small value may
keep more pairs in the velocity for the next evolution, but it

586

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE VIII
S IZES (N) OF C OVERING A RRAYS FOR n FACTORS , E ACH H AVING T HREE L EVELS , W ITH C OVERING S TRENGTH t, CA(N; t, n, 3)

also requires more computation time. Parameter pro2 determines the probability of selecting each fixed level from each
schema when updating positions. A larger value may lead to
a quick construction of new position, but it may also lead
to fast convergence toward a local optimum. Parameter pro3
determines the mutation probability when updating positions.
A larger value may enhance the randomness, but it also lowers
the convergence speed. In this paper, values from 0.1 to 1.0
for these three parameters are investigated.
Fig. 7 shows the results. For pro1 , a large value is not
appropriate because it removes nearly all pairs from the final
velocity set. A medium value 0.5, which appears to lead to the
best result, may be the best choice. For pro2 , the smallest value
0.1 yields the best results. Slower construction of the new position may slow the convergence toward a local optimum, but it
also takes longer. So, in order to balance performance and execution time, we take 0.3 as the recommended value for pro2 .
For pro3 , a larger value may be a good choice. The frequent
mutation of new position may bring better results, but also
takes longer to converge. So, we take 0.7 as the recommended
value for pro3 .
In summary, the recommended parameter settings for PSO
for covering array generation are different from previously
suggested ones [13], [16]. Some parameters may significantly
impact the performance in some cases, and parameter tuning is necessary to enhance heuristic techniques for particular
applications. Naturally, the optimal settings vary for different cases of covering arrays. There may not exist a common
setting that can always lead to the best results. Because it is

impractical to tune parameters for each case in real life, we
suggest two general settings for CPSO and DPSO, as shown
in Table VII, which can typically lead to better performance
within reasonable execution time.
VI. C OMPARING A MONG PSO S
In this section, we compare the best reported array sizes
generated by PSO in [16] with our findings for CPSO, DPSO,
and four representative variants. Because the research in [16]
demonstrated that their generation results typically outperform
greedy algorithms, in this paper, we do not compare CPSO and
DPSO with greedy algorithms.
We implement both the original and discrete versions of
four variants (TVAC [24], CLPSO [25], APSO [26], and
DMS-PSO [27]) to generate covering arrays. Their discrete
versions are extended based on new representation scheme of
velocity and auxiliary strategies. We name them D-TVAC, DCLPSO, D-APSO, and D-DMS-PSO, respectively. In CLPSO,
a particle can learn from different particlesâ€™ pbest in each
dimension whereas we do not distinguish dimensions strictly
in DPSO. So in D-CLPSO, a particle can fully learn from
different particlesâ€™ pbest in all dimensions. That may weaken
the search ability of CLPSO. For the other three variants, they
can be directly extended based on DPSO.
All algorithms are compared using the same number of
fitness function evaluations. CPSO and DPSO use the recommended settings shown in Table VII. For the variants, iter,
size and pro1 , pro2 and pro3 for discrete versions are set to

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

587

TABLE IX
S IZES (N) OF C OVERING A RRAYS FOR S EVEN FACTORS , E ACH H AVING  L EVELS , W ITH C OVERING S TRENGTH t, CA(N; t, 7, )

their recommended values. Ï‰ and c are also set to recommended values unless they are adaptively adjusted during the
evolution, in which case their range is set to [0.4, 0.9] and
[0.8, 1.8], respectively. The new control parameters for these
variants follow their suggested settings.
Tables VIIIâ€“XIII give the results. Because of the execution
time, we only consider covering strengths from 2 to 4, and the
generation of each covering array is repeated 30 times. A t-test
(significance level = 0.05) is also conducted to test whether
there exists a significant difference between the mean sizes
produced by the two algorithms. In the first three columns, we
report the best and mean array sizes obtained from previous
results, CPSO and DPSO, where boldface numbers indicate
that the difference between CPSO and DPSO is significant
based on the t-test. In the last four columns, we report the mean
array sizes from the original and discrete versions of each PSO
variant (presented as meanc and meand respectively), where
boldface numbers indicate that the difference between meanc
and meand of each variant is significant.
A. Uniform Covering Arrays
Tables VIIIâ€“X present the results for uniform covering
arrays. We extend the cases considered in [16], where â€œâ€“â€
indicates the not available cases. In Tables VIII, we report
array sizes for n factors, each having three levels. In
Tables IX and X, we report array sizes for 7 and 10 factors,
each having  levels. Their covering strengths all range from
2 to 4.
Typically, CPSO can produce smaller sizes than those
reported in [16], demonstrating the effectiveness of parameter tuning. Furthermore, DPSO can produce the smallest best
and mean sizes in almost all cases, and usually the performance of DPSO is significantly better than CPSO. When the
covering strength t = 2 is adopted (Table VIII), DPSO does

not generally outperform CPSO. But when covering strength
increases, DPSO performs better. Sometimes the mean sizes
for DPSO are smaller than the best sizes for CPSO. Because
generating a covering array with higher covering strength is
more complicated, DPSO may be more appropriate for generating such covering arrays. In Tables IX and X, we also find
that DPSO is superior for covering arrays with high covering strength. Overall, these results show the effectiveness of
DPSO for uniform covering array generation.
Surprisingly, DPSO does not beat previous results for
CPSO when covering arrays have two levels for each factor
(Table IX). This appears to be a weakness of DPSO. Although
DPSO generates smaller covering arrays than previous results
and CPSO, other techniques may still yield better results than
DPSO (some best known sizes can be found in [38]).
B. VS Covering Array
Tables XIâ€“XIII give the results for VS covering arrays.
Based on CA(N; 2, 315 ), CA(N; 3, 315 ), and CA(N; 2, 42 52 63 ),
some different cases of sub covering arrays conducted in [16]
are examined. Their covering strengths are at most 4.
Generally, we can draw similar conclusions as for uniform
covering arrays. CPSO with the suggested parameter setting
can produce better results than reported sizes in some cases.
DPSO also usually beats them on the best and mean sizes.
In Table XI, often the difference between CPSO and DPSO
is not significant. In part this is because for the CA(3, 33 ),
CA(3, 33 )2 , CA(3, 33 )3 , and CA(4, 34 ), the same best results
are provably the minimum (e.g., the minimum size of the
CA(3, 33 ) is 3 Ã— 3 Ã— 3 = 27). For the other cases, although
sometimes the difference is not significant, DPSO can still generate smaller covering arrays. DPSO remains a good choice
for generating VS covering arrays. In Tables XII and XIII,
similar results can be found.

588

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE X
S IZES (N) OF C OVERING A RRAYS FOR T EN FACTORS , E ACH H AVING  L EVELS , W ITH C OVERING S TRENGTH t, CA(N; t, 10, )

TABLE XI
S IZES (N) OF VS C OVERING A RRAYS VCA(N; 2, 315 , CA)

TABLE XII
S IZES (N) OF VS C OVERING A RRAYS VCA(N; 3, 315 , CA)

For both uniform and variable cases of covering arrays,
parameter tuning can enhance CPSO to generate smaller covering arrays than previous work. DPSO can produce the smallest
covering arrays in nearly all cases, and in general the difference of performance between CPSO and DPSO is significant.
DPSO is an effective discrete version of PSO for covering
array generation.
C. PSO Variants
In order to further investigate the effectiveness of DPSO for
covering array generation, we implement the original versions
of four representative variants of PSO and extend them to their
discrete versions based on DPSO. We apply them to generate

the same cases for covering arrays. The mean sizes obtained
are shown in the last four columns in Tables VIIIâ€“XIII.
We first compare the mean sizes of original PSO variants
with those of CPSO and DPSO. In our experiments, TVACâ€™s
mean sizes are always larger than CPSOâ€™s. The linear adjustment of inertia weight and acceleration constant is not helpful
for CPSO for covering array generation. Typically, APSOâ€™s
mean sizes are also larger than CPSOâ€™s. Because APSO uses
a fuzzy system to classify different evolutionary states, its
ineffectiveness may result from inappropriate parameter settings. CLPSO and DMS-PSO can outperform CPSO typically.
Although on occasion they achieve comparable performance
with DPSO, they cannot perform as well as DPSO in most

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

589

TABLE XIII
S IZES (N) OF VS C OVERING A RRAYS VCA(N; 2, 43 53 62 , CA)

TABLE XIV
C OMPARING CPSO AND DPSO W ITH GA AND ACO

cases. That further demonstrates the effectiveness of DPSO
for covering array generation. Because these four algorithms
are representative PSO variants (as shown in Table IV), designing different neighborhood topologies and using multiswarm
techniques may have potential to improve CPSO for covering
array generation.
We next compare the original and discrete versions of each
variant. Except for CLPSO, the other three variants can be
improved using their discrete versions, and the improvement
is also significant. TVAC cannot outperform CPSO, but it
is enhanced by DPSO so that D-TVAC can produce smaller
mean sizes than CPSO in most cases. The linear adjustment
is helpful for the discrete version. For CLPSO, only in a few
cases is it improved by DPSO. Sometimes D-CLPSO even
leads to worse results (see Table X), due primarily to the
weakened search ability of its discrete version as explained in
Section VI. For APSO, DPSO can enhance its original version,
but D-APSO is still worse than DPSO. That may result from
inappropriate settings as explained before. For DMS-PSO,
sometimes DPSO does not enhance it (see Table XI).
However, in most cases D-DMS-PSO can outperform
DMS-PSO and has comparable performance with D-TVAC.
The multiswarm strategy is also helpful for the discrete
version.

In summary, the comparison study reveals that our suggested parameter settings are more suitable for covering array
generation. DPSO is an effective discrete version of PSO.
It can significantly outperform previous results and CPSO
in nearly all cases for uniform and VS covering arrays.
Furthermore, DPSOâ€™s representation scheme of a particleâ€™s
velocity and auxiliary strategies not only enhance CPSO, but
also typically enhance PSO variants. DPSO is a promising
improvement on PSO for covering array generation.
VII. C OMPARING DPSO W ITH GA AND ACO
Because GAs and ACO [8]â€“[12] have also been successfully
used for covering array generation, we compare CPSO and
DPSO with the reported array sizes in [9] and [11]. There are
no widely accepted benchmarks for the comparison of searchbased covering array generation, so we only consider these
two representative and competitive works.
Shiba et al. [9] applied both GA and ACO to generate
uniform covering arrays (CA1 to CA8 in Table XIV), and
Chen et al. [11] applied ACO to generate VS covering arrays
(VCA9 to VCA12 in Table XIV). They both set algorithm
parameters according to recommendations in related research
fields without parameter tuning, while our CPSO and DPSO

590

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

use the recommended settings shown in Table VII. To compare
these algorithms under the same number of fitness evaluations, for each case of covering array, the population size and
the number of iterations of CPSO and DPSO are modified
accordingly to satisfy the settings in [9] and [11]. Moreover,
Shiba et al. [9] and Chen et al. [11] both applied test minimization algorithms to further reduce the size of generated
covering arrays, while our CPSO and DPSO do not apply any
minimization algorithms.
Table XIV shows the comparison results, where boldface
numbers indicate the best array sizes obtained, and â€œâ€“â€ represents that the corresponding data is not available. The
generation of each case of covering array of CPSO and DPSO
is executed 30 times and the best and average results are
presented. Because we do not implement GA and ACO for
covering array generation, no statistical tests can be conducted
here. In addition, because the platforms used for collecting the
results differ, the comparison of computational time would not
be informative. We nevertheless present the execution times
of our CPSO and DPSO, which can serve as references for
practitioners.
From Table XIV, DPSO can outperform existing GA and
ACO for covering array generation, despite the latter two
applying test minimization algorithms. Because our DPSO is
a version of PSO designed and tuned for covering array generation, it suggests that tuned versions of GAs and ACOs may
be a promising area for further study. In addition, CPSO performs worse than GA and ACO in seven of 12 cases. That
may result from the improvement by minimization algorithms
in [9] and [11]. But for CA2 , CA3 , and CA6 , CPSO can still
achieve smaller covering arrays.
In summary, the results further demonstrate that DPSO is
an effective discrete version of PSO for covering array generation. Further investigations of tuned versions of GA and ACO
should be considered.
VIII. C ONCLUSION
Covering array generation is a key issue in CT. We developed a new DPSO for covering array generation, by adapting
S-PSO to generate covering arrays and incorporating two
auxiliary strategies to improve the performance. Parameter tuning was applied to both CPSO and DPSO to identify their
best parameter settings. The original and discrete versions of
four representative PSO variants were implemented and their
efficacy for covering array generation was compared. The performance of DPSO was also compared with other existing
evolutionary algorithms, GA and ACO.
DPSO can perform better than CPSO with fewer iterations, and the performance of CPSO and DPSO is significantly
impacted by their parameter settings. Different cases require
different parameter settings; there may not exist a single choice
that leads to the best results. After parameter tuning, we identified two recommended settings, which lead to relatively good
performance for CPSO and DPSO. Indeed, CPSO with our recommended parameter settings improves on previously reported
results, and our DPSO with recommended parameter settings usually beats CPSO. Typically, DPSO also significantly

enhances the performance of PSO variants. In addition, DPSO
often outperforms GA and ACO to generate covering arrays.
Consequently, DPSO is a promising improvement of PSO for
covering array generation.
Improvements on the methods here may be possible in
a number of ways. One would be to investigate further
evolution procedures and strategies proposed in PSO, such
as hybridizing with penalty approaches to handle discrete
unknowns [39], and compare the results with some exact
schemes like branch and bound method. A second would be
to examine one-column-at-a-time approaches or methods that
construct the entire array, rather than the one-row-at-a-time
approach adopted here. A third would be to incorporate
DPSO with other methods, in particular with test minimization
methods.
R EFERENCES
[1] C. Nie and H. Leung, â€œA survey of combinatorial testing,â€ ACM Comput.
Surv., vol. 43, no. 2, pp. 11.1â€“11.29, 2011.
[2] D. Kuhn and M. Reilly, â€œAn investigation of the applicability of
design of experiments to software testing,â€ in Proc. 27th Annu. NASA
Goddard/IEEE Softw. Eng. Workshop, Greenbelt, MD, USA, 2002,
pp. 91â€“95.
[3] M. Cohen, P. Gibbons, W. Mugridge, and C. Colbourn, â€œConstructing
test suites for interaction testing,â€ in Proc. 25th Int. Conf. Softw. Eng.,
Portland, OR, USA, 2003, pp. 38â€“48.
[4] M. B. Cohen, C. J. Colbourn, and A. C. Ling, â€œConstructing strength
three covering arrays with augmented annealing,â€ Discrete Math.,
vol. 308, no. 13, pp. 2709â€“2722, 2008.
[5] J. Torres-Jimenez and E. Rodriguez-Tello, â€œSimulated annealing for constructing binary covering arrays of variable strength,â€ in Proc. Congr.
Evol. Comput., Barcelona, Spain, Jul. 2010, pp. 1â€“8.
[6] B. Garvin, M. Cohen, and M. Dwyer, â€œEvaluating improvements to a
meta-heuristic search for constrained interaction testing,â€ Empir. Softw.
Eng., vol. 16, no. 1, pp. 61â€“102, 2011.
[7] J. Torres-Jimenez and E. Rodriguez-Tello, â€œNew bounds for binary
covering arrays using simulated annealing,â€ Inf. Sci., vol. 185, no. 1,
pp. 137â€“152, 2012.
[8] S. Ghazi and M. Ahmed, â€œPair-wise test coverage using genetic algorithms,â€ in Proc. Congr. Evol. Comput., vol. 2. Canberra, ACT, Australia,
2003, pp. 1420â€“1424.
[9] T. Shiba, T. Tsuchiya, and T. Kikuno, â€œUsing artificial life techniques to
generate test cases for combinatorial testing,â€ in Proc. 28th Annu. Int.
Comput. Softw. Appl. Conf., vol. 1. Hong Kong, 2004, pp. 72â€“77.
[10] J. McCaffrey, â€œAn empirical study of pairwise test set generation using
a genetic algorithm,â€ in Proc. 7th Int. Conf. Inf. Technol. New Gener.,
Las Vegas, NV, USA, 2010, pp. 992â€“997.
[11] X. Chen, Q. Gu, A. Li, and D. Chen, â€œVariable strength interaction
testing with an ant colony system approach,â€ in Proc. Asia-Pacific Softw.
Eng. Conf., Penang, Malaysia, 2009, pp. 160â€“167.
[12] X. Chen, Q. Gu, X. Zhang, and D. Chen, â€œBuilding prioritized pairwise
interaction test suites with ant colony optimization,â€ in Proc. 9th Int.
Conf. Qual. Softw., Jeju-do, Korea, 2009, pp. 347â€“352.
[13] X. Chen, Q. Gu, J. Qi, and D. Chen, â€œApplying particle swarm optimization to pairwise testing,â€ in Proc. 34th Annu. Comput. Softw. Appl.
Conf., Seoul, Korea, 2010, pp. 107â€“116.
[14] B. S. Ahmed and K. Z. Zamli, â€œPSTG: A T-way strategy adopting particle swarm optimization,â€ in Proc. 4th Asia Int. Conf. Math. Anal. Model.
Comput. Simulat., Kota Kinabalu, Malaysia, 2010, pp. 1â€“5.
[15] B. S. Ahmed and K. Z. Zamli, â€œA variable strength interaction test suites
generation strategy using particle swarm optimization,â€ J. Syst. Softw.,
vol. 84, no. 12, pp. 2171â€“2185, 2011.
[16] B. S. Ahmed, K. Z. Zamli, and C. P. Lim, â€œApplication of particle
swarm optimization to uniform and variable strength covering array
construction,â€ Appl. Soft Comput., vol. 12, no. 4, pp. 1330â€“1347, 2012.
[17] Y. del Valle, G. Venayagamoorthy, S. Mohagheghi, J.-C. Hernandez, and
R. Harley, â€œParticle swarm optimization: Basic concepts, variants and
applications in power systems,â€ IEEE Trans. Evol. Comput., vol. 12,
no. 2, pp. 171â€“195, Apr. 2008.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

[18] W.-N. Chen et al., â€œA novel set-based particle swarm optimization
method for discrete optimization problems,â€ IEEE Trans. Evol. Comput.,
vol. 14, no. 2, pp. 278â€“300, Apr. 2010.
[19] M. Clerc, Discrete Particle Swarm Optimization (New Optimization
Techniques in Engineering). New York, NY, USA: Springer, 2004.
[20] W. Pang et al., â€œModified particle swarm optimization based on space
transformation for solving traveling salesman problem,â€ in Proc. Int.
Conf. Mach. Learn. Cybern., vol. 4. Shanghai, China, Aug. 2004,
pp. 2342â€“2346.
[21] W. Pang, K.-P. Wang, C.-G. Zhou, and L.-J. Dong, â€œFuzzy discrete particle swarm optimization for solving traveling salesman problem,â€ in Proc.
4th Int. Conf. Comput. Inf. Technol., Wuhan, China, 2004, pp. 796â€“800.
[22] Y. Wang et al., â€œA novel quantum swarm evolutionary algorithm and its
applications,â€ Neurocomputing, vol. 70, nos. 4â€“6, pp. 633â€“640, 2007.
[23] E. Campana, G. Fasano, and A. Pinto, â€œDynamic analysis for the
selection of parameters and initial population, in particle swarm optimization,â€ J. Global Optim., vol. 48, no. 3, pp. 347â€“397, 2010.
[24] A. Ratnaweera, S. Halgamuge, and H. Watson, â€œSelf-organizing hierarchical particle swarm optimizer with time-varying acceleration coefficients,â€ IEEE Trans. Evol. Comput., vol. 8, no. 3, pp. 240â€“255,
Jun. 2004.
[25] J. Liang, A. Qin, P. Suganthan, and S. Baskar, â€œComprehensive learning particle swarm optimizer for global optimization of multimodal
functions,â€ IEEE Trans. Evol. Comput., vol. 10, no. 3, pp. 281â€“295,
Jun. 2006.
[26] Z.-H. Zhan, J. Zhang, Y. Li, and H.-H. Chung, â€œAdaptive particle swarm
optimization,â€ IEEE Trans. Syst., Man, Cybern., B, Cybern., vol. 39,
no. 6, pp. 1362â€“1381, Dec. 2009.
[27] J. Liang and P. Suganthan, â€œDynamic multi-swarm particle swarm optimizer,â€ in Proc. IEEE Swarm Intell. Symp., Pasadena, CA, USA, 2005,
pp. 124â€“129.
[28] D. Cohen, S. Dalal, M. Fredman, and G. Patton, â€œThe AETG system:
An approach to testing based on combinatorial design,â€ IEEE Trans.
Softw. Eng., vol. 23, no. 7, pp. 437â€“444, Jul. 1997.
[29] R. C. Bryce and C. J. Colbourn, â€œOne-test-at-a-time heuristic search for
interaction test suites,â€ in Proc. 9th Annu. Conf. Genet. Evol. Comput.,
London, U.K., 2007, pp. 1082â€“1089.
[30] J. Kennedy and R. Eberhart, â€œParticle swarm optimization,â€ in Proc. Int.
Conf. Neural Netw., vol. 4. Perth, WA, Australia, 1995, pp. 1942â€“1948.
[31] R. Eberhart and Y. Shi, â€œParticle swarm optimization: Developments,
applications and resources,â€ in Proc. Congr. Evol. Comput., vol. 1. Seoul,
Korea, 2001, pp. 81â€“86.
[32] R. Poli, J. Kennedy, and T. Blackwell, â€œParticle swarm optimization,â€
Swarm Intell., vol. 1, no. 1, pp. 33â€“57, 2007.
[33] S. Helwig, J. Branke, and S. Mostaghim, â€œExperimental analysis of
bound handling techniques in particle swarm optimization,â€ IEEE Trans.
Evol. Comput., vol. 17, no. 2, pp. 259â€“271, Apr. 2013.
[34] A. Windisch, S. Wappler, and J. Wegener, â€œApplying particle swarm
optimization to software testing,â€ in Proc. 9th Annu. Conf. Genet. Evol.
Comput., London, U.K., 2007, pp. 1121â€“1128.
[35] A. Ganjali, â€œA requirements-based partition testing framework using particle swarm optimization technique,â€ M.S. thesis, Dept. Electr. Comput.
Eng., Univ. Waterloo, Waterloo, ON, Canada, 2008.
[36] Y.-J. Gong et al., â€œOptimizing the vehicle routing problem with time
windows: A discrete particle swarm optimization approach,â€ IEEE
Trans. Syst., Man, Cybern. C, Appl. Rev., vol. 42, no. 2, pp. 254â€“267,
Mar. 2012.
[37] W.-N. Chen et al., â€œParticle swarm optimization with an aging leader and
challengers,â€ IEEE Trans. Evol. Comput., vol. 17, no. 2, pp. 241â€“258,
Apr. 2013.
[38] C. J. Colbourn. (Apr. 2013). Covering Array Tables for t=2, 3,
4, 5, 6. [Online]. Available: http://www.public.asu.edu/ ccolbou/src/
tabby/catable.html
[39] M. Corazza, G. Fasano, and R. Gusso, â€œParticle swarm optimization
with non-smooth penalty reformulation, for a complex portfolio selection
problem,â€ Appl. Math. Comput., vol. 224, pp. 611â€“624, Nov. 2013.

591

Huayao Wu received the B.S degree from Southeast
University, Nanjing, China and the M.S degree from
Nanjing University, Nanjing, China, where he is
currently working toward the Ph.D. degree from
Nanjing University, Nanjing.
His research interests include software testing,
especially on combinatorial testing and search-based
software testing.

Changhai Nie (Mâ€™12) received the B.S. and M.S.
degrees in mathematics from Harbin Institute of
Technology, Harbin, China, and the Ph.D. degree
in computer science from Southeast University,
Nanjing, China.
He is a Professor of Software Engineering
with State Key Laboratory for Novel Software
Technology, Department of Computer Science
and Technology, Nanjing University, Nanjing. His
research interests include software analysis, testing
and debugging.

Fei-Ching Kuo (Mâ€™06) received the B.Sc. (Hons.)
degree in computer science and the Ph.D. degree in
software engineering from Swinburne University of
Technology, Hawthorn, VIC, Australia.
She was a Lecturer with University of
Wollongong, Wollongong, NSW, Australia. She is
currently a Senior Lecturer with the Swinburne
University of Technology. Her research interests
include software analysis, testing, and debugging.

Hareton Leung (Mâ€™90) received the Ph.D. degree
in computer science from University of Alberta,
Edmonton, AB, Canada.
He is an Associate Professor and a Director
of the Laboratory for Software Development
and Management, Department of Computing,
Hong Kong Polytechnic University, Hong Kong. His
research interests include software testing, project
management, risk management, quality and process
improvement, and software metrics.

Charles J. Colbourn received the Ph.D. degree
from University of Toronto, Toronto, ON, Canada,
in 1980.
He is a Professor of Computer Science and
Engineering with Arizona State University, Tempe,
AZ, USA. He has authored the books The
Combinatorics of Network Reliability (Oxford) and
Triple Systems (Oxford), and also 320 refereed journal papers focussing on combinatorial designs and
graphs with applications in networking, computing,
and communications.
Prof. Colbourn received the Euler Medal for Lifetime Research
Achievement by the Institute for Combinatorics and its Applications in 2004.

Two-stage algorithms for covering array construction

arXiv:1606.06730v1 [cs.DS] 21 Jun 2016

Kaushik Sarkar and Charles J. Colbourn
School of Computing, Informatics, and Decision Systems Engineering
Arizona State University, PO Box 878809
Tempe, Arizona, 85287-8809, U.S.A.
June 23, 2016
Abstract
Modern software systems often consist of many different components, each with a number of options.
Although unit tests may reveal faulty options for individual components, functionally correct components
may interact in unforeseen ways to cause a fault. Covering arrays are used to test for interactions
among components systematically. A two-stage framework, providing a number of concrete algorithms,
is developed for the efficient construction of covering arrays. In the first stage, a time and memory
efficient randomized algorithm covers most of the interactions. In the second stage, a more sophisticated
search covers the remainder in relatively few tests. In this way, the storage limitations of the sophisticated
search algorithms are avoided; hence the range of the number of components for which the algorithm can
be applied is extended, without increasing the number of tests. Many of the framework instantiations
can be tuned to optimize a memory-quality trade-off, so that fewer tests can be achieved using more
memory. The algorithms developed outperform the currently best known methods when the number
of components ranges from 20 to 60, the number of options for each ranges from 3 to 6, and t-way
interactions are covered for t âˆˆ {5, 6}. In some cases a reduction in the number of tests by more than
50% is achieved.

Keywords: Covering array, Software interaction testing, Combinatorial construction algorithm

1

Introduction

Real world software and engineered systems are composed of many different components, each with a number
of options, that are required to work together in a variety of circumstances. Components are factors, and
options for a component form the levels of its factor. Although each level for an individual factor can be tested
in isolation, faults in deployed software can arise from interactions among levels of different factors. When
an interaction involves levels of t different factors, it is a t-way interaction. Testing for faults caused by t-way
interactions for every t is generally infeasible, as a result of a combinatorial explosion. However, empirical
research on real world software systems indicates that testing all possible 2-way (or 3-way) interactions
would detect 70% (or 90%) of all faults [25]. Moreover, testing all possible 6-way interactions is sufficient
for detection of 100% of all faults in the systems examined in [25]. Testing all possible t-way interactions for
some 2 â‰¤ t â‰¤ 6 is pseudo-exhaustive testing [24], and is accomplished with a combinatorial array known as
a covering array.
Formally, let N, t, k, and v be integers with k â‰¥ t â‰¥ 2 and v â‰¥ 2. A covering array CA(N ; t, k, v) is an
N Ã— k array A in which each entry is from a v-ary alphabet Î£, and for every N Ã— t sub-array B of A and
every x âˆˆ Î£t , there is a row of B that equals x. Then t is the strength of the covering array, k is the number
of factors, and v is the number of levels.
When k is a positive integer, [k] denotes the set {1, . . . , k}. A t-way interaction is {(ci , ai ) : 1 â‰¤ i â‰¤
t, ci âˆˆ [k], ci 6= cj for i 6= j, and ai âˆˆ Î£}. So an interaction is an assignment of levels from Î£ to t of the k

factors. It,k,v denotes the set of all kt v t interactions for given t, k and v. An N Ã— k array A covers the
1

interaction Î¹ = {(ci , ai ) : 1 â‰¤ i â‰¤ t, ci âˆˆ [k], ci 6= cj for i 6= j, and ai âˆˆ Î£} if there is a row r in A such that
A(r, ci ) = ai for 1 â‰¤ i â‰¤ t. When there is no such row in A, Î¹ is not covered in A. Hence a CA(N ; t, k, v)
covers all interactions in It,k,v .
Covering arrays are used extensively for interaction testing in complex engineered systems. To ensure
that all possible combinations of options of t components function together correctly, one needs examine
all possible t-way interactions. When the number of components is k, and the number of different options
available for each component is v, each row of CA(N ; t, k, v) represents a test case. The N test cases
collectively test all t-way interactions. For this reason, covering arrays have been used in combinatorial
interaction testing in varied fields like software and hardware engineering, design of composite materials,
and biological networks [8, 24, 26, 32, 34].
The cost of testing is directly related to the number of test cases. Therefore, one is interested in covering
arrays with the fewest rows. The smallest value of N for which CA(N ; t, k, v) exists is denoted by CAN(t, k, v).
Efforts to determine or bound CAN(t, k, v) have been extensive; see [12, 14, 24, 31] for example. Naturally one
would prefer to determine CAN(t, k, v) exactly. Katona [22] and Kleitman and Spencer [23] independently
showed that
 for t = v = 2, the minimum number of rows N in a CA(N ; 2, k, 2) is the smallest N for which
âˆ’1
. Exact determination of CAN(t, k, v) for other values of t and v has remained open. However,
kâ‰¤ N
N
d2e
some progress has been made in determining upper bounds for CAN(t, k, v) in the general case; for recent
results, see [33].
For practical applications such bounds are often unhelpful, because one needs explicit covering arrays to
use as test suites. Explicit constructions can be recursive, producing larger covering arrays using smaller ones
as ingredients (see [14] for a survey), or direct. Direct methods for some specific cases arise from algebraic,
geometric, or number-theoretic techniques; general direct methods are computational in nature. Indeed
when k is relatively small, the best known results arise from computational techniques [13], and these are
in turn essential for the successes of recursive methods. Unfortunately, the existing computational methods
encounter difficulties as k increases, but is still within the range needed for practical applications. Typically
such difficulties arise either as a result of storage or time limitations or by producing covering arrays that
are too big to compete with those arising from simpler recursive methods.
Cohen [11] discusses commercial software where the number of factors often exceeds 50. Aldaco et al. [1]
analyze a complex engineered system having 75 factors, using a variant of covering arrays. Android [3] uses
a Configuration class to describe the device configuration; there are 17 different configuration parameters
with 3 âˆ’ 20 different levels. In each of these cases, while existing techniques are effective when the strength
is small, these moderately large values of k pose concerns for larger strengths.
In this paper, we focus on situations in which every factor has the same number of levels. These cases
have been most extensively studied, and hence provide a basis for making comparisons. In practice, however,
often different components have different number of levels, which is captured by extending the notion of a
covering array. A mixed covering array MCA(N ; t, k, (v1 , v2 , . . . , vk )) is an N Ã— k array in which the ith
column contains vi symbols for 1 â‰¤ i â‰¤ k. When {i1 , . . . , it } âŠ† {1, . . . , k} is
Qat set of t columns, in the N Ã— t
subarray obtained by selecting columns i1 , . . . , it of the MCA, each of the j=1 vij distinct t-tuples appears
as a row at least once. Although we examine the uniform case in which v1 = Â· Â· Â· = vk , the methods developed
here can all be directly applied to mixed covering arrays as well.
Inevitably, when k > max(t + 1, v + 2), a covering array must cover some interactions more than once,
for if not they are orthogonal arrays [20]. Treating the rows of a covering array in a fixed order, each row
covers some number of interactions not covered by any earlier row. For a variety of known constructions,
the initial rows cover many new interactions, while the later ones cover very few [7]. Comparing this rate
of coverage for a purely random method and for one of the sophisticated search techniques, one finds little
difference in the initial rows, but very substantial differences in the final ones. This suggests strategies to
build the covering array in stages, investing more effort as the number of remaining uncovered interactions
declines.
In this paper we propose a new algorithmic framework for covering array construction, the two-stage
framework. In the first stage, a randomized row construction method builds a specified number of rows to
cover all but at most a specified, small number of interactions. As we see later, by dint of being randomized

2

this uses very little memory. The second stage is a more sophisticated search that adds few rows to cover the
remaining uncovered interactions. We choose search algorithms whose requirements depend on the number
of interactions to be covered, to profit from the fact that few interactions remain. By mixing randomized and
deterministic methods, we hope to retain the fast execution and small storage of the randomized methods,
along with the accuracy of the deterministic search techniques.
We introduce a number of algorithms within the two-stage framework. Some improve upon best known
bounds on CAN(t, k, v) (see [33]) in principle. But our focus is on the practical consequences: The twostage algorithms are indeed quite efficient for higher strength (t âˆˆ {5, 6}) and moderate number of levels
(v âˆˆ {3, 4, 5, 6}), when the number of factors k is moderately high (approximately in the range of 20 âˆ’ 80
depending on value of t and v). In fact, for many combination of t, k and v values the two-stage algorithms
beat the previously best known bounds.
Torres-Jimenez et al. [36] explore a related two-stage strategy. In their first stage, an in-parameter-order
greedy strategy (as used in ACTS [24]) adds a column to an existing array; in their second stage, simulated
annealing is applied to cover the remaining interactions. They apply their methods when t = v = 3, when
the storage and time requirements for both stages remain acceptable. In addition to the issues in handling
larger strengths, their methods provide no a priori bound on the size of the resulting array. In contrast with
their methods, ours provide a guarantee prior to execution with much more modest storage and time.
The rest of the paper is organized as follows. Section 2 reviews algorithmic methods of covering array
construction, specifically the randomized algorithm and the density algorithm. This section contrasts these
two methods and points out their limitations. Then it gives an intuitive answer to the question of why a two
stage based strategy might work and introduces the general two-stage framework. Section 3 introduces some
specific two-stage algorithms. Section 3.1 analyzes and evaluates the naÄ±Ìˆve strategy. Section 3.2 describes a
two-stage algorithm that combines the randomized and the density algorithm. Section 3.3 introduces graph
coloring based techniques in the second stage. Section 3.4 examines the effect of group action on the size
of the constructed covering arrays. Section 4 compares the results of various two-stage algorithms with the
presently best known sizes. In Section 5 we discuss the LovaÌsz local lemma (LLL) bounds on CAN(t, k, v)
and provide a Moser-Tardos type randomized algorithm for covering array construction that matches the
bound. Although the bound was known [18], the proof was non-constructive, and a constructive algorithm to
match this bound seems to be absent in the literature. We explore potentially better randomized algorithms
for the first stage using LLL based techniques, We also obtain a two-stage bound that improves the LLL
bound for CAN(t, k, v). We conclude the paper in Section 6.

2

Algorithmic construction of covering arrays

Available algorithms for the construction of covering arrays are primarily heuristic in nature; indeed exact
algorithms have succeeded for very few cases. Computationally intensive metaheuristic search methods such
as simulated annealing, tabu search, constraint programming, and genetic algorithms have been employed
when the strength is relatively small or the number of factors and levels is small. These methods have
established many of the best known bounds on sizes of covering arrays [13], but for many problems of
practical size their time and storage requirements are prohibitive. For larger problems, the best available
methods are greedy. The IPO family of algorithms [24] repeatedly adds one column at a time, and
 then adds
new rows to ensure complete coverage. In this way, at any point in time, the status of v t kâˆ’1
tâˆ’1 interactions
may be stored. AETG [10] pioneered a different method, which greedily selects one row at a time to cover
a large number of as-yet-uncovered interactions. They establish that if a row can be chosen that covers the
maximum number, a good a priori bound on the size of the covering array can be computed. Unfortunately
selecting the maximum is NP-hard, and even if one selects the maximum there is no guarantee that the
covering array is the smallest possible
 [7], so AETG resorts to a good heuristic selection of the next row by
examining the stored status of v t kt interactions. None of the methods so far mentioned therefore guarantee
to reach an a priori bound. An
 extension of the AETG strategy, the density algorithm [5, 6, 15], stores
additional statistics for all v t kt interactions in order to ensure the selection of a good next row, and hence
guarantees to produce an array with at most the precomputed number of rows. Variants of the density
3

Algorithm 1: A randomized algorithm for covering array construction.
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; 
t, k, v)
log (kt)+t log v


1 Set N :=
;
vt
log

2
3

4
5
6
7
8
9
10
11
12

v t âˆ’1

repeat
Construct an N Ã— k array A where each entry is chosen independently and uniformly at random
from a v-ary alphabet;
Set covered := true;
for each interaction Î¹ âˆˆ It,k,v do
if Î¹ is not covered in A then
Set covered := false;
break;
end
end
until covered = true;
Output A;

algorithm have proved to be most effective for problems of moderately large size. For even larger problems,
pure random approaches have been applied.
To produce methods that provide a guarantee on size, it is natural to focus on the density algorithm
in order to understand its strengths and weaknesses. To do this, we contrast it with a basic randomized
algorithm. Algorithm 1 shows a simple randomized algorithm for covering array construction. The algorithm
constructs an array of a particular size randomly and checks whether all the interactions are covered. It
repeats until it finds an array that covers all the interactions.
log (kt)+t log v

 is guaranteed to exist:
A CA(N ; t, k, v) with N =
vt
log

v t âˆ’1

Theorem 1. [21, 27, 35] (Stein-LovaÌsz-Johnson (SLJ) bound): Let t, k, v be integers with k â‰¥ t â‰¥ 2, and
v â‰¥ 2. Then as k â†’ âˆ,

log kt + t log v


CAN(t, k, v) â‰¤
t
log vtvâˆ’1
In fact, the probability that the N Ã— k array constructed in line 3 of Algorithm 1 is a valid covering array
is high enough that the expected number of times the loop in line 2 is repeated is a small constant.
An alternative strategy is to add rows one by one instead of constructing the full array at the outset. We
start with an empty array, and whenever we add a new row we ensure that it covers at least the expected
number of previously uncovered interactions for a randomly chosen row. The probability that an uncovered
interaction is covered by a random row is 1/v t . If the number of uncovered interactions is u, then by linearity
of expectation, the expected number of newly covered interactions in a randomly chosen row is uv âˆ’t . If each
row added covers exactly this expected number, we obtain the same number of rows as the SLJ bound,
realized in Algorithm 1. But because the actual number of newly covered interactions is always an integer,
each added row covers at least duv âˆ’t e interactions. This is especially helpful towards the end when the
expected number is a small fraction.
Algorithm 2 follows this strategy. Again the probability that a randomly chosen row covers at least the
expected number of previously uncovered interactions is high enough that the expected number of times the
row selection loop in line 6 of Algorithm 2 is repeated is bounded by a small constant.
We can obtain an upper bound on the size produced by Algorithm 2 by assuming that each new row
added covers exactly duv âˆ’t e previously uncovered interactions. This bound is the discrete Stein-LovaÌsz4

Algorithm 2: A randomized algorithm for covering array construction using the discrete SLJ strategy.
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; t, k, v)
1 Let A be an empty array;

k t
2 Initialize a table T indexed by all t v interactions, marking every interaction â€œuncoveredâ€;
3 while there is an interaction marked â€œuncoveredâ€ in T do
4
Let u be the number of interactions marked â€œuncoveredâ€ in T ;
5
Set expectedCoverage := d vut e;
6
repeat
7
Let r be a row of length k where each entry is chosen independently and uniformly at random
from a v-ary alphabet;
8
Let coverage be the number of â€œuncoveredâ€ interactions in T that are covered in row r;
9
until coverage > expectedCoverage;
10
Add r to A;
11
Mark all interactions covered by r as â€œcoveredâ€ in T ;
12 end
13 Output A;

Johnson (discrete SLJ) bound. Figure 1 compares the sizes of covering arrays obtained from the SLJ and
the discrete SLJ bounds for different values of k when t = 6 and v = 3. Consider a concrete example, when
t = 5, k = 20, and v = 3. The SLJ bound guarantees the existence of a covering array with 12499 rows,
whereas the discrete SLJ bound guarantees the existence of a covering array with only 8117 rows.
The density algorithm replaces the loop at line 6 of Algorithm 2 by a conditional expectation derandomized method. For fixed v and t the density algorithm selects a row efficiently (time polynomial in k) and
deterministically that is guaranteed to cover at least duv âˆ’t e previously uncovered interactions. In practice,
for small values of k the density algorithm works quite well, often covering many more interactions than
the minimum. Many of the currently best known CAN(t, k, v) upper bounds are obtained by the density
algorithm in combination with various post-optimization techniques [13].
However, the practical applicability of Algorithm 2 and the density algorithm is limited by the storage
of the table T , representing each of the kt v t interactions. Even when t = 6, v = 3, and k = 54, there are
18,828,003,285 6-way interactions. This huge memory requirement renders the density algorithm impractical
for rather small values of k when t âˆˆ {5, 6} and v â‰¥ 3. We present an idea to circumvent this large
requirement for memory, and develop it in full in Section 3.

2.1

Why does a two stage based strategy make sense?

Compare the two extremes, the density algorithm and Algorithm 1. On one hand, Algorithm 1 does not suffer
from any substantial storage restriction, but appears to generate many more rows than the density algorithm.
On the other hand, the density algorithm constructs fewer rows for small values of k, but becomes impractical
when k is moderately large. One wants algorithms that behave like Algorithm 1 in terms of memory, but
yield a number of rows competitive with the density algorithm.
For t = 6, k = 16, and v = 3, Figure 2 compares the coverage profile for the density algorithm and
Algorithm 1. We plot the number of newly covered interactions for each row in the density algorithm, and
the expected number of newly covered interactions for each row for Algorithm 1. The qualitative features
exhibited by this plot are representative of the rates of coverage for other parameters.
Two key observations are suggested by Figure 2. First, the expected coverage in the initial random rows
is similar to the rows chosen by the density algorithm. In this example, the partial arrays consisting of the
first 1000 rows exhibit similar coverage, yet the randomized algorithm needed no extensive bookkeeping.
Secondly, as later rows are added, the judicious selections of the density algorithm produce much larger
coverage per row than Algorithm 1. Consequently it appears sensible to invest few computational resources
5

4

3.5

x 10

SLJ bound
Discrete SLJ bound

N âˆ’ number of rows

3

2.5

2

1.5

1

0.5
0

100

200

300

400

500
k

600

700

800

900

1000

Figure 1: Comparison of covering array sizes obtained from SLJ bound and discrete SLJ bound for different
values of k, when t = 6 and v = 3.

9000
Density
Basic Random

Number of newly covered interactions

8000
7000
6000
5000
4000
3000
2000
1000
0

0

2000

4000

6000
Row number

8000

10000

12000

Figure 2: For t = 6, k = 16 and v = 3, the actual number of newly covered interactions of the density
algorithm and the expected number of newly covered interactions in a random array.

6

on the initial rows, while making more careful selections in the later ones. This forms the blueprint of our
general two-stage algorithmic framework shown in Algorithm 3.
Algorithm 3: The general two-stage framework for covering array construction.
Input: t : strength of the required covering array, k : number of factors, v : number of levels for each
factor
Output: A : a CA(N ; t, k, v)
1 Choose a number n of rows and a number Ï of interactions;
// First Stage
0
2 Use a randomized algorithm to construct an n Ã— k array A ;
0
3 Ensure that A covers all but at most Ï interactions;
0
4 Make a list L of interactions that are not covered in A (L contains at most Ï interactions);
// Second Stage
0
5 Use a deterministic procedure to add N âˆ’ n rows to A to cover all the interactions in L;
6 Output A;
A specific covering array construction algorithm results by specifying the randomized method in the first
stage, the deterministic method in the second stage, and the computation of n and Ï. Any such algorithm
produces a covering array, but we wish to make selections so that the resulting algorithms are practical while
still providing a guarantee on the size of the array. In Section 3 we describe different algorithms from the
two-stage family, determine the size of the partial array to be constructed in the first stage, and establish
upper bound guarantees. In Section 4 we explore how good the algorithms are in practice.

3

Two-stage framework

For the first stage we consider two methods:
Rand
MT

the basic randomized algorithm
the Moser-Tardos type algorithm

We defer the development of method MT until Section 5. Method Rand uses a simple variant of Algorithm
1, choosing a random n Ã— k array.
For the second stage we consider four methods:
Naive
Greedy
Den
Col

the
the
the
the

naÄ±Ìˆve strategy, one row per uncovered interaction
online greedy coloring strategy
density algorithm
graph coloring algorithm

Using these abbreviations, we adopt a uniform naming convention for the algorithms: TS hA, Bi is the algorithm in which A is used in the first stage, and B is used in the second stage. For example, TS hMT, Greedyi
denotes a two-stage algorithm where the first stage is a Moser-Tardos type randomized algorithm and the
second stage is a greedy coloring algorithm. Later when the need arises we refine these algorithm names.

3.1

One row per uncovered interaction in the second stage (TS hRand, Naivei)

In the second stage each of the uncovered interactions after the first stage is covered using a new row.
Algorithm 4 describes the method in more detail.
This simple strategy improves on the basic randomized strategy when n is chosen judiciously. For
example, when t = 6, k = 54 and v = 3, Algorithm 1 constructs a covering array with 17, 236 rows. Figure
3 plots an upper bound on the size of the covering array against the number n of rows in the partial array.

7

Algorithm 4: NaÄ±Ìˆve two-stage algorithm (TS hRand, Naivei).
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; t, k, v)
 t 
log (kt)+t log v+log log vtvâˆ’1


;
1 Let n :=
vt
log

1

v t âˆ’1

2

Let Ï =

3

repeat
Let A be an n Ã— k array where each entry is chosen independently and uniformly at random from
a v-ary alphabet;
Let uncovNum := 0 and unCovList be an empty list of interactions;
Set covered := true;
for each interaction Î¹ âˆˆ It,k,v do
if Î¹ is not covered in A then
Set uncovNum :=uncovNum+1;
Add Î¹ to unCovList;
if uncovNum > Ï then
Set covered := false;
break;
end
end
end
until covered= true;
for each interaction Î¹ âˆˆuncovList do
Add a row to A that covers Î¹;
end
Output A;

4

5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21

log

vt
v t âˆ’1

;

8

4

Total number of rows in the Covering array

1.75

x 10

1.7
1.65
1.6
1.55
1.5
1.45
1.4
1.35
1.3

1

1.1
1.2
1.3
1.4
1.5
1.6
1.7
n âˆ’âˆ’ number of rows in the partial array of the first stage

1.8
4

x 10

Figure 3: An upper bound on the size of the covering array against n, the size of the partial array constructed
in the first stage when t = 6, k = 54, and v = 3, with one new row added per uncovered interaction in the
second stage. The minimum size of 13, 162 is obtained when n = 12, 402. Algorithm 1 requires 17, 236 rows,
and the currently best known covering array has 17, 197 rows.

9

The smallest covering array is obtained when n = 12, 402 which, when completed, yields a covering array
with at most 13, 162 rowsâ€”a big improvement over Algorithm 1.
A theorem from [33] tells us the optimal value of n in general:
Theorem 2. [33] Let t, k, v be integers with k â‰¥ t â‰¥ 2, and v â‰¥ 2. Then
 t 

log kt + t log v + log log vtvâˆ’1 + 1


.
CAN(t, k, v) â‰¤
t
log vtvâˆ’1
log (kt)+t log v+log log


t
log vtvâˆ’1



vt
v t âˆ’1



. The expected number of uncovered
The bound is obtained by setting n =
 t 
interactions is exactly Ï = 1/ log vtvâˆ’1 .
Figure 4 compares SLJ, discrete SLJ and two-stage bounds for k â‰¤ 100, when t = 6 and v = 3. The
two-stage bound does not deteriorate in comparison to discrete SLJ bound as k increases; it consistently
takes only 307-309 more rows. Thus when k = 12 the two-stage bound requires only 6% more rows and
when k = 100 only 2% more rows than the discrete SLJ bound.
4

2.2

x 10

SLJ bound
Discrete SLJ bound
Twoâˆ’stage bound

2

N âˆ’ number of rows

1.8
1.6
1.4
1.2
1
0.8
0.6
0.4
10

20

30

40

50

60

70

80

90

100

k

Figure 4: Comparison of covering array sizes obtained from SLJ bound, discrete SLJ bound and two-stage
bound for k â‰¤ 100, when t = 6 and v = 3. In this range of k values the two-stage bound requires 307-309
more rows than the discrete SLJ bound, that is, 2-6% more rows.
To ensure that the loop in line 7 of Algorithm 4 does not repeat too many times we need to know the
probability with which a random n Ã— k array leaves at most Ï interactions uncovered. Using Chebyshevâ€™s
inequality and the second moment method developed in [2, Chapter 4], we next show that in a random n Ã—
nk
array the number of uncovered interactions is almost always close to its expectation, i.e. kt v t 1 âˆ’ v1t .
Substituting the value of n from line 1, this expected value is equal to Âµ, as in line 2. Therefore, the probability
that a random n Ã— k array covers the desired number of interactions is constant, and the expected number
of times the loop in line 7 is repeated is also a constant (around 2 in practice).

10

Because the theory of the second moment method is developed
considerable detail in [2], here we briefly
Pin
m
mention the relevant concepts and results. Suppose that X = i=1 Xi , where Xi is the indicator random
variable for event Ai for 1 â‰¤ i â‰¤ m. For indices i, j, we write i âˆ¼ j if i 6= j and the events Ai , Aj are not
independent. Also suppose that X1 , . . . , Xm are symmetric, i.e. for every i 6= j there is a measure
preserving
P
mapping of the underlying probability space that sends event Ai to event Aj . Define âˆ†âˆ— = jâˆ¼i Pr [Aj |Ai ].
Then by [2, Corollary 4.3.4]:
Lemma 3. [2] If E[X] â†’ âˆ and âˆ†âˆ— = o(E[X]) then X âˆ¼ E[X] almost always.
In our case, Ai denotes the event that the ith interaction is not covered in a n Ã— k array where each entry
n
is chosen independently and uniformly at random from a v-ary alphabet. Then Pr[Xi ] = 1 âˆ’ v1t . Because



n
there are kt v t interactions in total, by linearity of expectation, E[X] = kt v t 1 âˆ’ v1t , and E[X] â†’ âˆ as
k â†’ âˆ.
Distinct events Ai and Aj are independent if the ith and jth interactions share no column. Therefore,

P
P
k
the event Ai is not independent of at most t tâˆ’1
other events Aj . So âˆ†âˆ— = jâˆ¼i Pr [Aj |Ai ] â‰¤ jâˆ¼i 1 â‰¤

k
t tâˆ’1
= o(E[X]) when v and t are constants. By Lemma 3, the number of uncovered interactions in a random
n Ã— k array is close to the expected number of uncovered interactions. This guarantees that Algorithm 4 is
an efficient randomized algorithm for constructing covering arrays with a number of rows upper bounded by
Theorem 2.
In keeping with the general two-stage framework, Algorithm 4 does not store the coverage status of
each interaction. We only need store the interactions that are uncovered in A, of which there are at most
1
 â‰ˆ v t . This quantity depends only on v and t and is independent of k, so is effectively a
Ï =
t
log vtvâˆ’1

constant that is much smaller than kt v t , the storage requirement for the density algorithm. Hence the
algorithm can be applied to a higher range of k values.
Although Theorem 5 provides asymptotically tighter bounds than Theorem 2, in a range of k values that
are relevant for practical application, Theorem 2 provides better results. Figure 5 compares the bounds on
CAN(t, k, v) with the currently best known results.
4

3

6

x 10

2.5

Best known
Twoâˆ’stage (simple)
GSS bound
2

2

N âˆ’ number of rows

N âˆ’ number of rows

2.5

1.5

1

1.5

1

0.5

0.5

0
0

x 10

Best known
Twoâˆ’stage (simple)
GSS bound

10

20

30

40

50

60

70

80

90

k

0

5

10

15

20

25

30

35

40

45

50

k

(a) t = 6, v = 3

(b) t = 6, v = 6

Figure 5: Comparison of GSS bound and two-stage bound with the currently best known results

3.2

The density algorithm in the second stage (TS hRand, Deni)

Next we apply the density algorithm in the second stage. Figure 6 plots an upper bound on the size of the
covering array against the size of the partial array constructed in the first stage when the density algorithm
is used in the second stage, and compares it with TS hRand, Naivei. The size of the covering array decreases
11

as n decreases. This is expected because with smaller partial arrays, more interactions remain for the second
stage to be covered by the density algorithm. In fact if we cover all the interactions using the density
algorithm (as when n = 0) we would get an even smaller covering array. However, our motivation was
precisely to avoid doing that. Therefore, we need a â€cut-offâ€ for the first stage.
4

Total number of rows in the Covering array

1.9

x 10

Basic Twoâˆ’stage
Twoâˆ’stage with density in second stage

1.8

1.7

1.6

1.5

1.4

1.3

1.2

1

1.1
1.2
1.3
1.4
1.5
1.6
1.7
n âˆ’âˆ’ number of rows in the partial array of the first stage

1.8
4

x 10

Figure 6: Comparison of covering array sizes from two-stage algorithms with Den and Naive in the second
stage. With Den there is no minimum point in the curve; the size of the covering array keeps decreasing as
we leave more uncovered interactions for the second stage.
We are presented with a trade-off. If we construct a smaller partial array in the first stage, we obtain a
smaller covering array overall. But we then pay for more storage and computation time for the second stage.
To appreciate the nature of this trade-off, look at Figure 7, which plots an upper bound on the covering
array size and the number of uncovered interactions in the first stage against n. The improvement in the
covering array size plateaus after a certain point. The three horizontal lines indicate Ï (â‰ˆ v t ), 2Ï and 3Ï
uncovered interactions in the first stage. (In the naÄ±Ìˆve method of Section 3.1, the partial array after the first
stage leaves at most Ï uncovered interactions.) In Figure 7 the final covering array size appears to plateau
when the number of uncovered interactions left by the first stage is around 2Ï. After that we see diminishing
returns â€” the density algorithm needs to cover more interactions in return for a smaller improvement in the
covering array size.
Let r be the maximum number of interactions allowed to remain uncovered after the first stage. Then r
can be specified in the two-stage algorithm. To accommodate this, we denote by TS hA, B; ri the two-stage
algorithm where A is the first stage strategy, B is the second stage strategy, and r is the maximum number
of uncovered interactions after the first stage. For example, TS hRand, Den; 2Ïi applies the basic randomized
algorithm in the first stage to cover all but at most 2Ï interactions, and the density algorithm to cover the
remaining interactions in the second stage.

3.3

Coloring in the second stage (TS hRand, Coli and TS hRand, Greedyi)

Now we describe strategies using graph coloring in the second stage. Construct a graph G = (V, E), the
incompatibility graph, in which V is the set of uncovered interactions, and there is an edge between two
12

Number of rows / Number of uncovered interactions

18000
16000

Num. of rows in the completed CA
Num. of uncovered interaction in first stage

14000
12000
10000
8000
6000
4000
2000
0
0.8

0.9
1
1.1
1.2
1.3
1.4
1.5
1.6
1.7
1.8
4
n âˆ’âˆ’ number of rows in the partial array of the first stage
x 10

Figure 7: Final covering array size against the number of uncovered interactions after the first stage. As the
size n of the partial array decreases, the number of uncovered interactions in the first stage increases. Den is
used in the second stage. From bottom to top, the green lines denote Ï, 2Ï, and 3Ï uncovered interactions.
interactions exactly when they share a column in which they have different symbols. A single row can cover
a set of interactions if and only if it forms an independent set in G. Hence the minimum number of rows
required to cover all interactions of G is exactly its chromatic number Ï‡(G), the minimum number of colors
in a proper coloring of G. Graph coloring is an NP-hard problem, so we employ heuristics to bound the
chromatic number. Moreover, G only has vertices for the uncovered interactions after the first stage, so is
size is small relative to the total number of interactions.
The expected number of edges in the incompatibility
graph after
n choosing n rows uniformly at random
 k  t Pt

 t
 
t kâˆ’t
1
1 n
1
tâˆ’i
is Î³ = 2 t v
) 1 âˆ’ vt
1 âˆ’ (vt âˆ’vtâˆ’i ) . Using the elementary upper bound on
i=1 i tâˆ’i (v âˆ’ v
q
the chromatic number Ï‡ â‰¤ 12 + 2m + 14 , where m is the number of edges [16, Chapter 5.2], we can surely
q
cover the remaining interactions with at most 12 + 2m + 14 rows.
The actual number of edges m that remain after the first stage is a random variable with mean Î³. In
principle, the first stage could be repeatedly applied until m â‰¤ Î³, so we call m = Î³ the optimistic estimate.
To ensure that the first stage is expected to be run a small constant number of times, we increase the
estimate. With probability more than 1/2 the incompatibility graph has m â‰¤ 2Î³ edges, so m = 2Î³ is the
conservative estimate.
For t = 6, k = 56, and v = 3, Figure 8 shows the effect on the minimum number of rows when the bound
on the chromatic number in the second stage is used, for the conservative or optimistic estimates. The NaÄ±Ìˆve
method is plotted for comparison. Better coloring bounds shift the minima leftward, reducing the number
of rows produced in both stages.
Thus far we have considered bounds on the chromatic number. Better estimation of Ï‡(G) is complicated
by the fact that we do not have much information about the structure of G until the first stage is run. In
practice, however, G is known after the first stage and hence an algorithmic method to bound its chromatic
number can be applied. Because the number of vertices in G equals the number of uncovered interactions

13

4

2.2

x 10

Conservative estimate
Optimistic estimate
Simple

Number of rows required

2

1.8

1.6

1.4

1.2

1

1

1.1

1.2

1.3

1.4
N

1.5

1.6

1.7

1.8
4

x 10

Figure 8: Size of the partial array vs. size of the complete CA. t = 6, k = 56, v = 3. Stein-LovaÌsz-Johnson
bound requires 17, 403 rows, discrete Stein-LovaÌsz-Johnson bound requires 13, 021 rows. Simple estimate
for the two stage algorithm is 13, 328 rows, conservative estimate assuming m = 2Î³ is 12, 159 rows, and
optimistic estimate assuming m = Î³ is 11, 919 rows. Even the conservative estimate beats the discrete
Stein-LovaÌsz-Johnson bound.

14

after the first stage, we encounter the same trade-off between time and storage, and final array size, as seen
earlier for density. Hence we again parametrize by the expected number of uncovered interactions in the
first stage.
We employ two different greedy algorithms to color the incompatibility graph. In method Col we first
construct the incompatibility graph G after the first stage. Then we apply the commonly used smallest last
order heuristic to order the vertices for greedy coloring: At each stage, find a vertex vi of minimum degree
in Gi , order the vertices of Gi âˆ’ vi , and then place vi at the end. More precisely, we order the vertices of G
as v1 , v2 , . . . , vn , such that vi is a vertex of minimum degree in Gi , where Gi = G âˆ’ {vi+1 , . . . , vn }. A graph
is d-degenerate if all of its subgraphs have a vertex with degree at most d. When G is d-degenerate but not
(d âˆ’ 1)-degenerate, the Coloring number col(G) is d + 1. If we then greedily color the vertices with the first
available color, at most col(G) colors are used.
In method Greedy we employ an on-line, greedy approach that colors the interactions as they are discovered in the first stage. In this way, the incompatibility graph is never constructed. We instead maintain a set
of rows. Some entries in rows are fixed to a specific value; others are flexible to take on any value. Whenever
a new interaction is found to be uncovered in the first stage, we check if any of the rows is compatible with
this interaction. If such a row is found then entries in the row are fixed so that the row now covers the
interaction. If no such row exists, a new row with exactly t fixed entries corresponding to the interaction is
added to the set of rows. This method is much faster than method Col in practice.

3.4

Using group action

Covering arrays that are invariant under the action of a permutation group on their symbols can be easier
to construct and are often smaller [15]. Direct and computational constructions using group actions are
explored in [9, 28]. Sarkar et al. [33] establish the asymptotically tightest known bounds on CAN(t, k, v)
using group actions. In this section we explore the implications of group actions on two-stage algorithms.
Let Î“ be a permutation group on the set of symbols. The action of this group partitions the set of
t-way interactions into orbits. We construct an array A such that for every orbit, at least one row covers
an interaction from that orbit. Then we develop the rows of A over Î“ to obtain a covering array that is
invariant under the action of Î“. Effort then focuses on covering all the orbits of t-way interactions, instead
of the individual interactions.
If Î“ acts sharply transitively
on the set of symbols

 (for example, if Î“ is a cyclic group of order v) then
the action of Î“ partitions kt v t interactions into kt v tâˆ’1 orbits of length v each. Following
the lines of the

v tâˆ’1
+1
log (kt)+(tâˆ’1) log v+log log tâˆ’1
âˆ’1

 v
that covers at
proof of Theorem 2, there exists an n Ã— k array with n =
v tâˆ’1
log

v tâˆ’1 âˆ’1

least one interaction from each orbit. Therefore,
log
CAN(t, k, v) â‰¤ v

k
t



+ (t âˆ’ 1) log v + log log


v tâˆ’1
log vtâˆ’1
âˆ’1



v tâˆ’1
v tâˆ’1 âˆ’1



+1
.

(1)

Similarly, we can employ a Frobenius group. When v is a prime power, the Frobenius group is the group
of permutations of Fv of the form {x 7â†’ ax + b : a, b âˆˆ Fv , a 6= 0}. The action of the Frobenius group
tâˆ’1
partitions the set of t-tuples on v symbols into v vâˆ’1âˆ’1 orbits of length v(v âˆ’ 1) (full orbits) each and 1 orbit
of length v (a short orbit). The short orbit consists of tuples of the form (x1 , . . . , xt ) where x1 = . . . = xt .
Therefore, we can obtain a covering array by first constructing an array that covers all the full orbits, and
then developing all the rows over the Frobenius group and adding v constant rows. Using the two stage
strategy in conjunction with the Frobenius group action we obtain:


 tâˆ’1 

v tâˆ’1
log kt + log v vâˆ’1âˆ’1 + log log vtâˆ’1
âˆ’v+1 + 1


CAN(t, k, v) â‰¤ v(v âˆ’ 1)
+ v.
(2)
tâˆ’1
v
log vtâˆ’1
âˆ’v+1

15

4

1.5

x 10

N âˆ’ number of rows

1.45

Twoâˆ’stage (simple)
Twoâˆ’stage (cyclic group action)
Twoâˆ’stage (Frobenius group action)

1.4

1.35

1.3

1.25
50

55

60

65

70

75

k

Figure 9: Comparison of the simple two-stage bound with the cyclic and the Frobenius two-stage bounds.
t = 6, v = 3 and 50 â‰¤ k â‰¤ 75. Group action reduces the required number of rows slightly.
Figure 9 compares the simple two-stage bound with the cyclic and Frobenius two-stage bounds. For
t = 6, v = 3 and 12 â‰¤ k â‰¤ 100, the cyclic bound requires 7-21 (on average 16) fewer rows than the simple
bound. In the same range the Frobenius bound requires 17 âˆ’ 51 (on average 40) fewer rows.
Group action can be applied in other methods for the second stage as well. Colbourn [15] incorporates
group action into the density algorithm, allowing us to apply method Den in the second stage.
Greedy extends easily to use group action, as we do not construct an explicit incompatibility graph.
Whenever we fix entries in a row to cover an uncovered orbit, we commit to a specific orbit representative.
However, applying group action to the incompatibility graph coloring for Col is more complicated. We
need to modify the definition of the incompatibility graph for two reasons. First the vertices no longer
represent uncovered interactions, but rather uncovered orbits of interaction. Secondly, and perhaps more
importantly, pairwise compatibility between every two orbits in a set no longer implies mutual compatibility
among all orbits in the set.
One approach is to form a vertex for each uncovered orbit, placing an edge between two when they share
a column. Rather than the usual coloring, however, one asks for a partition of the vertex set into classes so
that every class induces an acyclic subgraph. Problems of this type are generalized graph coloring problems
[4]. Within each class of such a vertex partition, consistent representatives of each orbit can be selected to
form a row; when a cycle is present, this may not be possible. Unfortunately, heuristics for solving these
types of problems appear to be weak, so we adopt a second approach. As we build the incompatibility
graph, we commit to specific orbit representatives. When a vertex for an uncovered orbit is added, we check
its compatibility with the orbit representatives chosen for the orbits already handled with which it shares
columns; we commit to an orbit representative and add edges to those with which it is now incompatible.
Once completed, we have a (standard) coloring problem for the resulting graph.
Because group action can be applied using each of the methods for the two stages, we extend our naming
to TS hA, B; r, Î“i, where Î“ can be Trivial (i.e. no group action), Cyclic, or Frobenius.

16

4

Computational results

Figure 5 indicates that even a simple two-stage bound can improve on best known covering array numbers.
Therefore we investigate the actual performance of our two-stage algorithms for covering arrays of strength
5 and 6.
First we present results for t = 6, when v âˆˆ {3, 4, 5, 6} and no group action is assumed. Table 1 shows the
results for different v values. In each case we select the range of k values where the two-stage bound predicts
smaller covering arrays than
 previously known best ones, setting the maximum number of uncovered
 tthe
v
interactions as Ï = 1/ log vt âˆ’1 â‰ˆ v t . For each value of k we construct a single partial array and then run
the different second stage algorithms on it consecutively. In this way all the second stage algorithms cover
the same set of uncovered interactions.
The column tab lists the best known CAN(t, k, v) upper bounds from [13]. The column bound shows the
upper bounds obtained from the two-stage bound (2). The columns naÄ±Ìˆve, greedy, col and den show results
obtained from running the TS hRand, Naive; Ï, Triviali, TS hRand, Greedy; Ï, Triviali, TS hRand, Col; Ï, Triviali
and TS hRand, Den; Ï, Triviali algorithms, respectively.
The naÄ±Ìˆve method always finds a covering array that is smaller than the two-stage bound. This happens
because we repeat the first stage of Algorithm 4 until the array has fewer than v t uncovered interactions.
(If the first stage were not repeated, the algorithm still produce covering arrays that are not too far from
the bound.) For v = 3 Greedy and Den have comparable performance. Method Col produces covering arrays
that are smaller. However, for v âˆˆ {4, 5, 6} Den and Col are competitive.
Table 2 shows the results obtained by the different second stage algorithms when the maximum number
of uncovered interactions in the first stage is set to 2Ï and 3Ï respectively. When more interactions are
covered in the second stage, we obtain smaller arrays as expected. However, the improvement in size does
not approach 50%. There is no clear winner.
Next we investigate the covering arrays that are invariant under the action of a cyclic group. In Table 3 the
column bound shows the upper bounds from Equation (1). The columns naÄ±Ìˆve, greedy, col and den show
results obtained from running TS hRand, Naive; Ï, Cyclici, TS hRand, Greedy; Ï, Cyclici, TS hRand, Col; Ï, Cyclici
and TS hRand, Den; Ï, Cyclici, respectively.
Table 4 presents results for cyclic group action based algorithms when the number of maximum uncovered
interactions in the first stage is set to 2Ï and 3Ï respectively.
For the Frobenius group action, we show results only for v âˆˆ {3, 5} in Table 5. The column bound shows
the upper bounds obtained from Equation (2).
Table 6 presents results for Frobenius group action algorithms when the number of maximum uncovered
interactions in the first stage is 2Ï or 3Ï.
Next we present a handful of results when t = 5. In the cases examined, using the trivial group action
is too time consuming to be practical. However, the cyclic or Frobenius cases are feasible. Tables 7 and 8
compare two stage algorithms when the number of uncovered interactions in the first stage is at most 2Ï.
In almost all cases there is no clear winner among the three second stage methods. Methods Den and
Greedy are, however, substantially faster and use less memory than method Col; for practical purposes they
would be preferred.
All code used in this experimentation is available from the github repository
https://github.com/ksarkar/CoveringArray
under an open source GPLv3 license.

5

Limited dependence and the Moser-Tardos algorithm

Here we explore a different randomized algorithm that produces smaller covering arrays than Algorithm 1.
When k > 2t, there are interactions that share no column. The events of coverage of such interactions are
independent. Moser et al. [29, 30] provide an efficient randomized construction method that exploits this

17

k

tab

bound

53
54
55
56
57

13021
14155
17161
19033
20185

13076
13162
13246
13329
13410

39
40
41
42
43
44

68314
71386
86554
94042
99994
104794

65520
66186
66834
67465
68081
68681

31
32
33
34
35
36

233945
258845
281345
293845
306345
356045

226700
229950
233080
236120
239050
241900

17
18
19
20
21
22
23
24
25
26
27

506713
583823
653756
694048
783784
844834
985702
1035310
1112436
1146173
1184697

486310
505230
522940
539580
555280
570130
584240
597660
610460
622700
634430

naÄ±Ìˆve
greedy
t = 6, v = 3
13056
12421
13160
12510
13192
12590
13304
12671
13395
12752
t = 6, v = 4
65452
61913
66125
62573
66740
63209
67408
63819
68064
64438
68556
65021
t = 6, v = 5
226503 213244
229829 216444
232929 219514
235933 222516
238981 225410
241831 228205
t = 6, v = 6
486302 449950
505197 468449
522596 485694
539532 502023
555254 517346
569934 531910
584194 545763
597152 558898
610389 571389
622589 583473
634139 594933

col

den

12415
12503
12581
12665
12748

12423
12512
12591
12674
12757

61862
62826
63160
64077
64935
65739

61886
62835
63186
64082
64907
65703

212942
217479
219215
222242
226379
230202

212940
217326
219241
222244
226270
229942

448922
467206
484434
500788
516083
530728
544547
557917
570316
582333
593857

447864
466438
483820
500194
515584
530242
548307
557316
569911
582028
593546

Table 1: Comparison of different TS hRand, âˆ’; Ï, Triviali algorithms.

18

k
greedy

2Ï
col

53
54
55
56
57

11968
12135
12286
12429
12562

11958
12126
12129
12204
12290

39
40
41
42
43
44

59433
60090
60715
61330
61936
62530

59323
60479
61527
62488
61839
62899

31
32
33
34
35
36

204105
207243
210308
213267
216082
218884

203500
206659
209716
212675
215521
218314

17
18
19
20
21
22
23
24
25
26
27

425053
443236
460315
476456
491570
505966
519611
532612
544967
556821
568135

-

den
t = 6, v
11968
12050
12131
12218
12296
t = 6, v
59326
59976
60615
61242
61836
62428
t = 6, v
203302
206440
209554
212508
215389
218172
t = 6, v
420333
438754
455941
472198
487501
502009
515774
528868
541353
553377
564827

greedy
=3
11716
11804
11877
11961
12044
=4
58095
58742
59369
59974
60575
61158
=5
199230
202342
205386
208285
211118
213872
=6
412275
430402
447198
463071
478269
492425
505980
518746
531042
542788
554052

3Ï
col

den

11705
11787
11875
12055
12211

11708
11790
11872
11950
12034

57951
58583
59867
61000
60407
61004

57888
58544
59187
59796
60393
60978

198361
201490
204548
-

197889
201068
204107
207060
209936
212707

-

405093
423493
440532
456725
471946
486306
500038
513047
525536
537418
548781

Table 2: Comparison of TS hRand, âˆ’; 2Ï, Triviali and TS hRand, âˆ’; 3Ï, Triviali algorithms.

19

k

tab

bound

53
54
55
56
57

13021
14155
17161
19033
20185

13059
13145
13229
13312
13393

k
39
40
41
42
43
44

tab
68314
71386
86554
94042
99994
104794

bound
65498
66163
66811
67442
68057
68658

31
32
33
34
35
36

226000
244715
263145
235835
238705
256935

226680
229920
233050
236090
239020
241870

17
18
19
20
21
22
23
24
25
26
27

506713
583823
653756
694048
783784
844834
985702
1035310
1112436
1146173
1184697

486290
505210
522910
539550
555250
570110
584210
597630
610430
622670
624400

naÄ±Ìˆve
greedy
t = 6, v = 3
13053
12405
13119
12489
13209
12573
13284
12660
13368
12744
t = 6, v = 4
naÄ±Ìˆve
greedy
65452
61896
66080
62516
66740
63184
67408
63800
68032
64408
68556
64988
t = 6, v = 5
226000 213165
229695 216440
233015 219450
235835 222450
238705 225330
241470 228140
t = 6, v = 6
485616 449778
504546 468156
522258 485586
539280 501972
554082 517236
569706 531852
583716 545562
597378 558888
610026 571380
622290 583320
633294 594786

col

den

12405
12543
12663
12651
12744

12411
12546
12663
12663
12750

col
61860
62820
63144
63780
64692
64964

den
61864
62784
63152
63784
64680
64976

212945
217585
221770
222300
225130
229235

212890
217270
221290
222210
225120
229020

448530
467232
490488
500880
521730
530832
549660
557790
575010
582546
598620

447732
466326
488454
500172
519966
530178
548196
557280
573882
582030
597246

Table 3: Comparison of TS hRand, âˆ’; Ï, Cyclicialgorithms.

20

k
greedy

2Ï
col

53
54
55
56
57

11958
12039
12120
12204
12276

11955
12027
12183
12342
12474

39
40
41
42
43
44

59412
60040
60700
61320
61908
62512

59336
59996
61156
62196
63192
64096

31
32
33
34
35
36

204060
207165
207165
213225
216050
218835

203650
209110
209865
212830
217795
218480

17
18
19
20
21
22
23
24
25
26
27

424842
443118
460014
476328
491514
505884
519498
532368
544842
543684
568050

422736
440922
457944
474252
489270
503580
517458
530340
542688
543684
566244

den
t = 6, v
11958
12036
12195
12324
12450
t = 6, v
59304
59964
61032
61976
62852
63672
t = 6, v
203265
208225
209540
212510
217070
218155
t = 6, v
420252
438762
455994
472158
487500
501852
515718
528828
541332
543684
564756

greedy
=3
11700
11790
11862
11949
12027
=4
58076
58716
59356
59932
60568
61152
=5
199180
202255
205380
208225
211080
213770
=6
411954
430506
447186
463062
478038
492372
505824
518700
530754
542664
553704

3Ï
col

den

11691
11874
12057
11937
12021

11694
11868
12027
11943
12024

57976
58616
59252
59840
61124
61048

57864
58520
59160
59760
60904
60988

198455
204495
204720
207790
213425
213185

197870
203250
204080
207025
212040
212695

409158
427638
456468
460164
486180
489336
502806
515754
538056
539922
560820

405018
423468
449148
456630
479970
486264
500040
512940
532662
537396
555756

Table 4: Comparison of TS hRand, âˆ’; 2Ï, Cyclici and TS hRand, âˆ’; 3Ï, Cyclici algorithms.

21

k

tab

bound

53
54
55
56
57

13021
14155
17161
19033
20185

13034
13120
13203
13286
13366

31
32
33
34
35
36

233945
258845
281345
293845
306345
356045

226570
229820
232950
235980
238920
241760

naÄ±Ìˆve
greedy
t = 6, v = 3
13029
12393
13071
12465
13179
12561
13245
12633
13365
12723
t = 6, v = 5
226425 213025
229585 216225
232725 219285
234905 222265
238185 225205
241525 227925

col

den

12387
12513
12549
12627
12717

12393
12531
12567
12639
12735

212865
216085
219205
223445
227445
231145

212865
216065
219145
223265
227065
230645

Table 5: Comparison of TS hRand, âˆ’; Ï, Frobeniusi algorithms.

k
greedy

2Ï
col

53
54
55
56
57
70
75
80
85
90

11931
12021
12105
12171
12255
13167
13473
13773
14031
14289

11919
12087
12237
12171
12249
13155
13473
13767
14025
14283

31
32
33
34
35
36
50
55
60
65

203785
206965
209985
213005
215765
218605
250625
259785
268185
275785

203485
208965
209645
214825
215545
218285
250365
259625
268025
275665

den
greedy
t = 6, v = 3
11931
11700
12087
11790
12231
11862
12183
11949
12255
12027
13179
13479
13779
14037
14301
t = 6, v = 5
203225 198945
208065 201845
209405 205045
214145 208065
215265 210705
218025 213525
250325
259565
267945
275665
-

3Ï
col

den

11691
11874
12057
11937
12021
-

11694
11868
12027
11943
12024
-

198445
204505
209845
207545
210365
213105
-

197825
203105
207865
206985
209885
212645
-

Table 6: Comparison of TS hRand, âˆ’; 2Ï, Frobeniusi and TS hRand, âˆ’; 3Ï, Frobeniusi algorithms.

22

k
67
68
69
70
71

tab
59110
60991
60991
60991
60991

greedy
48325
48565
48765
49005
49245

col
48285
48565
49005
48985
49205

den
48305
48585
48985
49025
49245

Table 7: Comparison of TS hRand, âˆ’; 2Ï, Frobeniusi algorithms. t = 5, v = 5
k
49
50
51
52
53

tab
122718
125520
128637
135745
137713

greedy
108210
109014
109734
110556
111306

col
108072
108894
110394
110436
111180

den
107988
108822
110166
110364
111120

Table 8: Comparison of TS hRand, âˆ’; 2Ï, Cyclici algorithms. t = 5, v = 6
limited dependence. Specializing their method to covering arrays, we obtain Algorithm 5. For the specified
value of N in the algorithm it is guaranteed that the expected number of times the loop in line 3 of Algorithm
5 is repeated is linearly bounded in k (See Theorem 1.2 of [30]).
The upper bound on CAN(t, k, v) guaranteed by Algorithm 5 is obtained by applying the LovaÌsz local
lemma (LLL).
Lemma 4. (LovaÌsz local lemma; symmetric case) (see [2]) Let A1 , A2 , . . . , An events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of all other events Aj except for at
most d, and that Pr[Ai ] â‰¤ p for all 1 â‰¤ i â‰¤ n. If ep(d + 1) â‰¤ 1, then Pr[âˆ©ni=1 AÌ„i ] > 0.
The symmetric version of LovaÌsz local lemma provides an upper bound on the probability of a â€œbadâ€
event in terms of the maximum degree of a bad event in a dependence graph, so that the probability that
all the bad events are avoided is non zero. Godbole, Skipper, and Sunley [18] apply Lemma 4 essentially to
obtain the bound on CAN(t, k, v) in line 1 of Algorithm 5.
Theorem 5. [18] Let t, v and k â‰¥ 2t be integers with t, v â‰¥ 2. Then
n 
o
+ t log v + 1
log kt âˆ’ kâˆ’t
t


CAN (t, k, v) â‰¤
t
log vtvâˆ’1
The bound on the size of covering arrays obtained from Theorem 5 is asymptotically tighter than the
one obtained from Theorem 1. Figure 10 compares the bounds for t = 6 and v = 3.
The original proof of LLL is essentially non-constructive and does not immediately lead to a polynomial
time construction algorithm for covering arrays satisfying the bound of Theorem 5. Indeed no previous
construction algorithms appear to be based on it. However the Moser-Tardos method of Algorithm 5 does
provide a construction algorithm running in expected polynomial time. For sufficiently large values of k
Algorithm 5 produces smaller covering arrays than the Algorithm 1.
But the question remains: Does Algorithm 5 produce smaller covering arrays than the currently best
known results within the range that it can be effectively computed? Perhaps surprisingly, we show that
the answer is affirmative. In Algorithm 5 we do not need to store the coverage information of individual
interactions in memory because each time an uncovered interaction is encountered we re-sample the columns
involved in that interaction and start the check afresh (checking the coverage in interactions in the same
order each time). Consequently, Algorithm 5 can be applied for larger values of k than the density algorithm.
23

Algorithm 5: Moser-Tardos type algorithm for covering array construction.
Input: t : strength of the covering array, k : number of factors, v : number of levels for each factor
Output: A : a CA(N ; t, k, v)
log{(kt)âˆ’(kâˆ’t
log v+1
)}+t.
t

;
1 Let N :=
vt
log

2

3
4
5
6
7
8
9
10
11
12
13

14
15
16

v t âˆ’1

Construct an N Ã— k array A where each entry is chosen independently and uniformly at random from
a v-ary alphabet;
repeat
Set covered := true;
for each interaction Î¹ âˆˆ It,k,v do
if Î¹ is not covered in A then
Set covered := false;
Set missing-interaction := Î¹;
break;
end
end
if covered = false then
Choose all the entries in the t columns involved in missing-interaction independently and
uniformly at random from the v-ary alphabet;
end
until covered = true;
Output A;

Smaller covering arrays can be obtained by exploiting a group action using LLL, as shown in [33]. Table
9 shows the sizes of the covering arrays constructed by a variant of Algorithm 5 that employs cyclic and
Frobenius group actions. While this single stage algorithm produces smaller arrays than the currently best
known [13], these are already superseded by the two-stage based algorithms.
k
56
57
58
59
60

tab
19033
20185
23299
23563
23563

MT
16281
16353
16425
16491
16557

(a) Frobenius. t = 6, v = 3

k
44
45
46
47
48

tab
411373
417581
417581
423523
423523

MT
358125
360125
362065
363965
365805

(b) Frobenius. t = 6, v = 5

k
25
26
27
28
29

tab
1006326
1040063
1082766
1105985
1149037

MT
1020630
1032030
1042902
1053306
1063272

(c) Cyclic. t = 6, v = 6

Table 9: Comparison of covering array size from Algorithm 5 (MT) with the best known results [13] (tab).

5.1

Moser-Tardos type algorithm for the first stage

The linearity of expectation arguments used in the SLJ bounds permit one to consider situations in which
a few of the â€œbadâ€ events are allowed to occur, a fact that we exploited in the first stage of the algorithms
thus far. However, the LovaÌsz local lemma does not address this situation directly. The conditional LovaÌsz
local lemma (LLL) distribution, introduced in [19], is a very useful tool.
Lemma 6. (Conditional LLL distribution; symmetric case) (see [2, 33]) Let A = {A1 , A2 , . . . , Al } be a set
of l events in an arbitrary probability space. Suppose that each event Ai is mutually independent of a set of
all other events Aj except for at most d, and that Pr[Ai ] â‰¤ p for all 1 â‰¤ i â‰¤ l. Also suppose that ep(d+1) â‰¤ 1
(Therefore, by LLL (Lemma 4) Pr[âˆ©li=1 AÌ„i ] > 0). Let B âˆˆ
/ A be another event in the same probability space
24

5

10

N âˆ’ number of rows

SLJ bound
GSS bound

4

10

3

10
1
10

2

3

10

10
k

4

10

5

10

Figure 10: Comparison of SLJ (Theorem 1) and GSS (Theorem 5) bounds for t = 6 and v = 3. The graph
is plotted in log-log scale to highlight the asymptotic difference between the two bounds.
with Pr[B] â‰¤ q, such that B is also mutually independent of a set of all other events Aj âˆˆ A except for at
most d. Then Pr[B| âˆ©li=1 AÌ„i ] â‰¤ eq.
We apply the conditional
LLL distribution to obtain an upper bound on the size of partial array that
 t 
v
leaves at most log vt âˆ’1 â‰ˆ v t interactions uncovered. For a positive integer k, let I = {j1 , . . . , jÏ } âŠ† [k]
where j1 < . . . < jÏ . Let A be an n Ã— k array where each entry is from the set [v]. Let AI denote the n Ã— Ï
array in which AI (i, `) = A(i, j` ) for 1 â‰¤ i â‰¤ N and 1 â‰¤ ` â‰¤ Ï; AI is the projection of A onto the columns
in I.

Let M âŠ† [v]t be a set of m t-tuples of symbols, and C âˆˆ [k]
be a set of t columns. Suppose the
t
entries in the array A are chosen independently from [v] with uniform probability.
 Let BC denote the event
that at least one of the tuples in M is not covered in AC . There are Î· = kt such events, and for all of
n
them Pr[BC ] â‰¤ m 1 âˆ’ v1t . Moreover, when k â‰¥ 2t, each of the events is mutually independent of all



k
other events except for at most Ï = kt âˆ’ kâˆ’t
âˆ’ 1 < t tâˆ’1
. Therefore, by the LovaÌsz local lemma, when
t

1 n
eÏm 1 âˆ’ vt â‰¤ 1, none of the events BC occur. Solving for n, when
nâ‰¥

log(eÏm)


t
log vtvâˆ’1

(3)


there exists an n Ã— k array A over [v] such that for all C âˆˆ [k]
t , AC covers all the m tuples in M . In fact
we can use a Moser-Tardos type algorithm to construct such an array.
Let Î¹ be an interaction whose t-tuple
n of symbols is not in M . Then the probability that Î¹ is not covered
in an n Ã— k array is at most 1 âˆ’ v1t
when each entry of the array is chosen independently from [v] with
uniform probability. Therefore, by the
 conditional LLL distribution the probability that Î¹1 is
n not covered
in the array A where for all C âˆˆ [k]
. Moreover,
t , AC covers all the m tuples in M is at most e 1 âˆ’ v t
there are Î·(v t âˆ’ m) such interactions Î¹. By the linearity of expectation, the expected number of uncovered
25

n
interactions in A is less than v t when Î·(v t âˆ’ m)e 1 âˆ’ v1t â‰¤ v t . Solving for n, we obtain
	

log Î·e 1 âˆ’ vmt

 .
nâ‰¥
t
log vtvâˆ’1

Therefore, there exists an n Ã— k array with n = max

log{Î·e(1âˆ’ vmt )}
log(eÏm)


,

t
t
log vtvâˆ’1
log vtvâˆ’1



(4)

that has at most v t

uncovered interactions. To compute n explicitly, we must choose m. We can select a value of m to minimize
n graphically for given values of t, k and v. For example, Figure 11 plots Equations 3 and 4 against m for
t = 3, k = 350, v = 3, and finds the minimum value of n.
460

445
max(Equation (3), Equation (4))

440

n âˆ’ number of rows in the partial array

n âˆ’ number of rows in the partial array

Equation (3)
Equation (4)

420

400

380

360

340
0

5

10

15
m

20

25

30

(a) Equations 3 and 4 against m.

440

435

430

425

420
0

5

10

15
m

20

25

30

(b) Maximum of the two sizes against m.

Figure 11: The minimum is at n = 422, when m = 16. t = 3, k = 350, v = 3
We compare the size of the partial array from the naÄ±Ìˆve two-stage method (Algorithm 4) with the size
obtained by the graphical methods in Figure 12. The LovaÌsz local lemma based method is asymptotically
better than the simple randomized method. However, except for the small values of t and v, in the range
of k values relevant for practical applications the simple randomized algorithm requires fewer rows than the
LovaÌsz local lemma based method.

5.2

LovaÌsz local lemma based two-stage bound

We can apply the techniques from Section 5.1 to obtain a two-stage bound similar to Theorem 2 using the
LovaÌsz local lemma and conditional LLL distribution. First we extend a result from [33].



Theorem
7. Let t, k, v be integers with k â‰¥ t â‰¥ 2, v â‰¥ 2 and let Î· = kt , and Ï = kt âˆ’ kâˆ’t
t . If


vt
v t âˆ’1

Î·v t log

Ï

â‰¤ v t Then
log
CAN(t, k, v) â‰¤

k
t



+ t log v + log log


t
log vtvâˆ’1



vt
v t âˆ’1



+2

Î·
âˆ’ .
Ï

Proof. Let M âŠ† [v]t be a set of m t-tuples of symbols. Following the arguments of Section 5.1, when


 there exists an n Ã— k array A over [v] such that for all C âˆˆ [k] , AC covers all m tuples in M .
n â‰¥ log(eÏm)
t
vt
log

v t âˆ’1

At most Î·(v t âˆ’ m) interactions are uncovered in such an array. Using the conditional
n LLL distribution,
the probability that one such interaction is not covered in A is at most e 1 âˆ’ v1t . Therefore, by the
26

n âˆ’ number of rows in partial array with vt missing interactions

n âˆ’ number of rows in partial array with vt missing interactions

550
500
450
400
350
300
250
200
150
Randomized (Algorithm 4)
LLL based

100
50

0

200

400

600
k

800

1000

1200

2500

2000

1500

1000

500

0

0

Randomized (Algorithm 4)
LLL based

500

1000

1500

2000

2500

3000

3500

4000

4500

k

(a) t = 3, v = 3.

(b) t = 4, v = 3.

Figure 12: Comparison of the size of the partial array constructed in the first stage. Compares the size of
the partial array specified in Algorithm 4 in Section 3.1, and the size derived in Section 5.1.
 t

n
linearity of expectation, we can find one such array A that leaves at most eÎ·(v t âˆ’ m) 1 âˆ’ v1t = Î·Ï vm âˆ’ 1
interactions uncovered. Adding one row per uncovered interactions to A, we obtain a covering array with at
most N rows, where


log(eÏm)
Î· vt

+
N=
âˆ’
1
t
Ï m
log tv
The value of N is minimized when m =

v âˆ’1
 t 
Î·v t log vtvâˆ’1
Ï

. Because m â‰¤ v t , we obtain the desired bound.

When m = v t this recaptures the bound of Theorem 5.
Figure 13 compares the LLL based two-stage bound from Theorem 7 to the standard two-stage bound
from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1. Although the
LLL based two-stage bound is tighter than the LLL based Godbole et al. bound, even for quite large values
of k the standard two-stage bound is tighter than the LLL based two-stage bound. In practical terms, this
specific LLL based two-stage method does not look very promising, unless the parameters are quite large.

6

Conclusion and open problems

Many concrete algorithms within a two-stage framework for covering array construction have been introduced
and evaluated. The two-stage approach extends the range of parameters for which competitive covering arrays
can be constructed, each meeting an a priori guarantee on its size. Indeed as a consequence a number of
best known covering arrays have been improved upon. Although each of the methods proposed has useful
features, our experimental evaluation suggests that TS hRand, Greedy; 2Ï, Î“i and TS hRand, Den; 2Ï.Î“i with
Î“ âˆˆ {Cyclic, Frobenius} realize a good trade-off between running time and size of the constructed covering
array.
Improvements in the bounds, or in the algorithms that realize them, are certainly of interest. We
mention some specific directions. Establishing tighter bounds on the coloring based methods of Section 3.3
is a challenging problem. Either better estimates of the chromatic number of the incompatibility graph after
a random first stage, or a first stage designed to limit the chromatic number, could lead to improvements in
the bounds.
In Section 5.1 and 5.2 we explored using a Moser-Tardos type algorithm for the first stage. Although this
is useful for asymptotic bounds, practical improvements appear to be limited. Perhaps a different approach of
27

10000

500

9000

450

8000

400

7000

N âˆ’ number of rows

N âˆ’ number of rows

550

350
300
250

SLJ bound
Godbole
LLLâˆ’2âˆ’stage
2âˆ’stage

200
150
100

0

50

100

150

200

250

300

350

400

6000
5000
4000

SLJ bound
Godbole
LLLâˆ’2âˆ’stage
2âˆ’stage

3000
2000

450

k

(a) t = 3, v = 3.

1000
0

1000

2000

3000

4000
k

5000

6000

7000

8000

(b) t = 4, v = 4.

Figure 13: Comparison among the LLL based two-stage bound from Theorem 7, the standard two-stage
bound from Theorem 2, the Godbole et al. bound in Theorem 5, and the SLJ bound in Theorem 1.
reducing the number of bad events to be avoided explicitly by the algorithm may lead to a better algorithm.
A potential approach may look like following: â€œBadâ€ events would denote non-coverage of an interaction
over a t-set of columns. We would select a set of column t-sets such that the dependency graph of the
corresponding bad events have a bounded maximum degree (less than the original dependency graph). We
would devise a Moser-Tardos type algorithm for covering all the interactions on our chosen column t-sets,
and then apply the conditional LLL distribution to obtain an upper bound on the number of uncovered
interactions. However, the difficulty lies in the fact that â€œall vertices have degree â‰¤ Ïâ€ is a non-trivial,
â€œhereditaryâ€ property for induced subgraphs, and for such properties finding a maximum induced subgraph
with the property is an NP-hard optimization problem [17]. There is still hope for a randomized or â€œnibbleâ€
like strategy that may find a reasonably good induced subgraph with a bounded maximum degree. Further
exploration of this idea seems to be a promising research avenue.
In general, one could consider more than two stages. Establishing the benefit (or not) of having more
than two stages is also an interesting open problem. Finally, the application of the methods developed to
mixed covering arrays appears to provide useful techniques for higher strengths; this merits further study as
well.

Acknowledgments
The research was supported in part by the National Science Foundation under Grant No. 1421058.

References
[1] A. N. Aldaco, C. J. Colbourn, and V. R. Syrotiuk. Locating arrays: A new experimental design for
screening complex engineered systems. SIGOPS Oper. Syst. Rev., 49(1):31â€“40, Jan. 2015.
[2] N. Alon and J. H. Spencer. The probabilistic method. Wiley-Interscience Series in Discrete Mathematics
and Optimization. John Wiley & Sons, Inc., Hoboken, NJ, third edition, 2008. With an appendix on
the life and work of Paul ErdoÌ‹s.
[3] Android. Android configuration class, 2016. http://developer.android.com/reference/android/content/
res/Configuration.html.
[4] J. I. Brown. The complexity of generalized graph colorings. Discrete Appl. Math., 69(3):257â€“270, 1996.
28

[5] R. C. Bryce and C. J. Colbourn. The density algorithm for pairwise interaction testing. Software
Testing, Verification, and Reliability, 17:159â€“182, 2007.
[6] R. C. Bryce and C. J. Colbourn. A density-based greedy algorithm for higher strength covering arrays.
Software Testing, Verification, and Reliability, 19:37â€“53, 2009.
[7] R. C. Bryce and C. J. Colbourn. Expected time to detection of interaction faults. Journal of Combinatorial Mathematics and Combinatorial Computing, 86:87â€“110, 2013.
[8] J. N. Cawse. Experimental design for combinatorial and high throughput materials development. GE
Global Research Technical Report, 29:769â€“781, 2002.
[9] M. A. Chateauneuf, C. J. Colbourn, and D. L. Kreher. Covering arrays of strength 3. Des. Codes
Crypt., 16:235â€“242, 1999.
[10] D. M. Cohen, S. R. Dalal, M. L. Fredman, and G. C. Patton. The AETG system: An approach to
testing based on combinatorial design. IEEE Transactions on Software Engineering, 23:437â€“44, 1997.
[11] M. B. Cohen. Designing test suites for software interaction testing. PhD thesis, The University of
Auckland, Department of Computer Science, 2004.
[12] C. J. Colbourn. Combinatorial aspects of covering arrays. Le Matematiche (Catania), 58:121â€“167, 2004.
[13] C. J. Colbourn. Covering array tables, 2005-2015. http://www.public.asu.edu/âˆ¼ccolbou/src/tabby.
[14] C. J. Colbourn. Covering arrays and hash families. In Information Security and Related Combinatorics,
NATO Peace and Information Security, pages 99â€“136. IOS Press, 2011.
[15] C. J. Colbourn. Conditional expectation algorithms for covering arrays. Journal of Combinatorial
Mathematics and Combinatorial Computing, 90:97â€“115, 2014.
[16] R. Diestel. Graph Theory. Graduate Texts in Mathematics. Springer, fourth edition, 2010.
[17] M. R. Garey and D. S. Johnson. Computers and Intractability: A Guide to the Theory of NPCompleteness. W. H. Freeman & Co., New York, NY, USA, 1979.
[18] A. P. Godbole, D. E. Skipper, and R. A. Sunley. t-covering arrays: upper bounds and Poisson approximations. Combinatorics, Probability and Computing, 5:105â€“118, 1996.
[19] B. Haeupler, B. Saha, and A. Srinivasan. New constructive aspects of the LovaÌsz local lemma. J. ACM,
58(6):Art. 28, 28, 2011.
[20] A. S. Hedayat, N. J. A. Sloane, and J. Stufken. Orthogonal Arrays. Springer-Verlag, New York, 1999.
[21] D. S. Johnson. Approximation algorithms for combinatorial problems. J. Comput. System Sci., 9:256â€“
278, 1974.
[22] G. O. H. Katona. Two applications (for search theory and truth functions) of Sperner type theorems.
Periodica Math., 3:19â€“26, 1973.
[23] D. Kleitman and J. Spencer. Families of k-independent sets. Discrete Math., 6:255â€“262, 1973.
[24] D. R. Kuhn, R. Kacker, and Y. Lei. Introduction to Combinatorial Testing. CRC Press, 2013.
[25] D. R. Kuhn and M. Reilly. An investigation of the applicability of design of experiments to software
testing. In Proc. 27th Annual NASA Goddard/IEEE Software Engineering Workshop, pages 91â€“95, Los
Alamitos, CA, 2002. IEEE.

29

[26] D. R. Kuhn, D. R. Wallace, and A. M. Gallo. Software fault interactions and implications for software
testing. IEEE Trans. Software Engineering, 30:418â€“421, 2004.
[27] L. LovaÌsz. On the ratio of optimal integral and fractional covers. Discrete Math., 13(4):383â€“390, 1975.
[28] K. Meagher and B. Stevens. Group construction of covering arrays. J. Combin. Des., 13:70â€“77, 2005.
[29] R. A. Moser. A constructive proof of the LovaÌsz local lemma. In STOCâ€™09â€”Proceedings of the 2009
ACM International Symposium on Theory of Computing, pages 343â€“350. ACM, New York, 2009.
[30] R. A. Moser and G. Tardos. A constructive proof of the general LovaÌsz local lemma. J. ACM, 57(2):Art.
11, 15, 2010.
[31] C. Nie and H. Leung. A survey of combinatorial testing. ACM Computing Surveys, 43(2):#11, 2011.
[32] A. H. Ronneseth and C. J. Colbourn. Merging covering arrays and compressing multiple sequence
alignments. Discrete Appl. Math., 157:2177â€“2190, 2009.
[33] K. Sarkar and C. J. Colbourn. Upper bounds on the size of covering arrays. ArXiv e-prints.
[34] G. Seroussi and N. H. Bshouty. Vector sets for exhaustive testing of logic circuits. IEEE Trans. Inform.
Theory, 34:513â€“522, 1988.
[35] S. K. Stein. Two combinatorial covering theorems. J. Combinatorial Theory Ser. A, 16:391â€“397, 1974.
[36] J. Torres-Jimenez, H. Avila-George, and I. Izquierdo-Marquez. A two-stage algorithm for combinatorial
testing. Optimization Letters, pages 1â€“13, 2016.

30

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

575

A Discrete Particle Swarm Optimization for
Covering Array Generation
Huayao Wu, Changhai Nie, Member, IEEE, Fei-Ching Kuo, Member, IEEE,
Hareton Leung, Member, IEEE, and Charles J. Colbourn

Abstractâ€”Software behavior depends on many factors.
Combinatorial testing (CT) aims to generate small sets of test
cases to uncover defects caused by those factors and their
interactions. Covering array generation, a discrete optimization
problem, is the most popular research area in the field of CT.
Particle swarm optimization (PSO), an evolutionary search-based
heuristic technique, has succeeded in generating covering arrays
that are competitive in size. However, current PSO methods for
covering array generation simply round the particleâ€™s position
to an integer to handle the discrete search space. Moreover, no
guidelines are available to effectively set PSOs parameters for
this problem. In this paper, we extend the set-based PSO, an
existing discrete PSO (DPSO) method, to covering array generation. Two auxiliary strategies (particle reinitialization and
additional evaluation of gbest) are proposed to improve performance, and thus a novel DPSO for covering array generation
is developed. Guidelines for parameter settings both for conventional PSO (CPSO) and for DPSO are developed systematically
here. Discrete extensions of four existing PSO variants are developed, in order to further investigate the effectiveness of DPSO
for covering array generation. Experiments show that CPSO can
produce better results using the guidelines for parameter settings,
and that DPSO can generate smaller covering arrays than CPSO
and other existing evolutionary algorithms. DPSO is a promising
improvement on PSO for covering array generation.
Index Termsâ€”Combinatorial testing (CT), covering array
generation, particle swarm optimization (PSO).

I. I NTRODUCTION
S SOFTWARE functions and run-time environments
become more complex, testing of modern software systems is becoming more expensive. Effective detection of

A

Manuscript received May 12, 2013; revised December 29, 2013 and
May 18, 2014; accepted September 28, 2014. Date of publication October 9,
2014; date of current version July 28, 2015. This work was supported in part
by the National Natural Science Foundation of China under Grant 61272079,
in part by the Research Fund for the Doctoral Program of Higher Education
of China under Grant 20130091110032, in part by the Science Fund for
Creative Research Groups of the National Natural Science Foundation of
China under Grant 61321491, in part by the Major Program of National
Natural Science Foundation of China under Grant 91318301, and in part
by the Australian Research Council Linkage under Grant LP100200208.
(Corresponding author: Changhai Nie.)
H. Wu and C. Nie are with the State Key Laboratory for Novel
Software Technology, Nanjing University, Nanjing 210023, China (e-mail:
hywu@outlook.com; changhainie@nju.edu.cn).
F.-C. Kuo is with the Faculty of Information and Communication
Technologies, Swinburne University of Technology, Hawthorn, VIC 3122,
Australia (e-mail: dkuo@swin.edu.au).
H. Leung is with the Department of Computing, Hong Kong Polytechnic
University, Hong Kong (e-mail: hareton.leung@polyu.edu.hk).
C. J. Colbourn is with Arizona State University, Tempe, AZ 85287-8809,
USA (e-mail: colbourn@asu.edu).
Digital Object Identifier 10.1109/TEVC.2014.2362532

failures at a low cost is a key issue for test case generation. Combinatorial testing (CT) is a popular testing method
to detect failures triggered by various factors and their interactions [1]. By employing covering arrays as test suites, the CT
method aims to sample the large combination space with few
test cases to cover different interactions among a fixed number of factors. Kuhn and Reilly [2] shows that more than 70%
of the failures in certain software were caused by the interactions of one or two factors, and almost all the failures could
be detected by checking the interactions among six factors.
Therefore, CT can be an effective method in practice.
Generating a covering array with fewest tests (minimum
size) is a major challenge in CT. In general, the minimum
size of a covering array is unknown; hence, methods have
focused on finding covering arrays that have as few tests as
possible at reasonable search cost. The many methods that
have been proposed can be classified into two main groups:
1) mathematical methods and 2) computational methods [1].
Mathematical (algebraic or combinatorial) methods typically
exploit some known combinatorial structure. Computational
methods primarily use greedy strategies or heuristic-search
techniques to generate covering arrays, due to the size of the
search space.
Mathematical methods yield the best possible covering
arrays in certain cases. For example, orthogonal arrays used
in the design of experiments provide covering arrays with a
number of tests that is provably minimum. However, all known
mathematical methods can be applied only for restrictive sets
of factors. This limitation has led to an emphasis on computational methods. Greedy algorithms have been quite effective
in generating covering arrays, but their accuracy suffers from
becoming trapped in local optima.
In recent years, search-based software engineering (SBSE)
has focused on using search-based optimization algorithms
to find high-quality solutions for software engineering problems. Inspired by SBSE, many artificial intelligence-based
heuristic-search techniques have been applied to software
testing. For example, simulated annealing (SA) [3]â€“[7],
genetic algorithm (GA) [8]â€“[10], and ant colony
optimization (ACO) [9], [11], [12] have all been applied
to covering array generation. These techniques can generate
any types of covering arrays, and the constraint solving
and prioritization techniques can be easily integrated. Their
applications have been shown to be effective, producing
relatively small covering arrays in many cases. Particle swarm
optimization (PSO), a relatively new evolutionary algorithm,

c 2014 IEEE. Personal use is permitted, but republication/redistribution requires IEEE permission.
1089-778X 
See http://www.ieee.org/publications_standards/publications/rights/index.html for more information.

576

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

has also been used in this area [13]â€“[16]. It is easy to
implement and has fast initial progress.
The conventional PSO (CPSO) algorithm was originally
designed to find optimal or near optimal solutions in a
continuous space. Nevertheless, many discrete PSO (DPSO)
algorithms and frameworks have been developed to solve
discrete problems [17]â€“[22]. For covering array generation,
current discrete methods [13]â€“[16] simply round the particleâ€™s
position to an integer while keeping the velocity as a real
number. They suffer from two main shortcomings. First, the
performance of PSO is significantly impacted by its parameter
settings. In [23], effects of the general parameter selection and
initial population of PSO have been analyzed, but no guidelines on parameter settings have been reported for covering
array generation. Hence, a clear understanding of how to set
these execution parameters is needed. Second, simple rounding
fractional positions to integers introduces a substantial source
of errors in the search. Instead, a specialized DPSO version is
needed. Because current PSO methods show promise for generating covering arrays, these two main shortcomings should
be addressed.
In this paper, we adapt set-based PSO (S-PSO) [18] to
generate covering arrays. S-PSO utilizes set and probability
theories to develop a new representation scheme for combinatorial optimization problems as well as refining the related
evolution operators. In our adaptation of S-PSO, two auxiliary strategies are proposed to enhance performance, and
a novel DPSO algorithm is thus proposed. DPSO has the
same conceptual basis and exhibits similar search behavior to
CPSO, with parameters playing similar roles. Then, we explore
the optimal parameter settings for both CPSO and DPSO to
improve their performance, and identify recommended settings. Furthermore, because many CPSO variants [24]â€“[27]
can be easily extended to discrete versions based on our DPSO,
the performance of these discrete versions is also compared
with their original ones. Finally, we compare CPSO and DPSO
with existing GA and ACO [9], [11] algorithms to generate
covering arrays.
The main contributions of this paper are as follows.
1) Based on the set-based representation, we design a
version of S-PSO [18] for covering array generation.
2) We propose two auxiliary strategies, particle reinitialization, and additional evaluation of gbest, to enhance the
performance of PSO. A novel DPSO for covering array
generation is proposed.
3) We design experiments to explore the optimal parameter settings for CPSO and DPSO for covering array
generation.
4) We implement original and discrete versions of four
representative PSO variants (TVAC [24], CLPSO [25],
APSO [26], and DMS-PSO [27]) to compare their
efficacy for covering array generation.
The rest of this paper is organized as follows. Section II
gives background on CT, covering array generation, and
the CPSO algorithm. Section III summarizes related work.
Section IV presents our DPSO algorithm, including the
representation scheme, related operators, and two auxiliary
strategies. Section V evaluates the performance of CPSO and

TABLE I
E-C OMMERCE S OFTWARE S YSTEM

DPSO, and explores optimal parameter settings. Section VI
gives a comparison among CPSO, DPSO, and original and
discrete variants. Section VII compares CPSO and DPSO
with GA and ACO. Section VIII concludes this paper and
outlines future work.
II. BACKGROUND
A. CT
Suppose that the behavior of the software under test (SUT)
is controlled by n independent factors, which may represent configuration parameters, internal or external events, user
inputs, and the like. The ith factor has i discrete levels, chosen from the finite set Vi of size i . An n-tuple (x1 , x2 , . . . , xn )
forms a test case, where xi âˆˆ Vi for 1 â‰¤ i â‰¤ n.
Consider a simple e-commerce software system [15]. This
system consists of five different components. Each of these five
components can be regarded as a factor, and its configurations
can be regarded as different levels. Table I shows these five
factors and their corresponding levels. In this example, n = 5,
1 = 2 = 3 = 2, 4 = 5 = 3.
System failures are often triggered by interactions among
some factors, which can be represented by the combinations
of factor levels. In order to detect these failures, their combinations should be covered at least once by the test suite. A
t-way schema can be used to represent them.
Definition 1 (t-way schema): The n-tuple (âˆ’, y1 , . . . ,
yt , . . . ) is a t-way schema (t > 0) when some t factors
have fixed levels and the others can take any valid levels,
represented as â€œâˆ’.â€
For example, suppose that when factor Payment Server
takes the level Master and factor Web Server takes the level
Apache, a system failure occurs. To detect this failure, the
2-way schema (Master, â€“, Apache, â€“, â€“) must be covered
at least once by the test suite. To simplify later discussion,
we use the index in the level set of each factor to present
a schema. For example, (0, â€“, 1, â€“, â€“) is used to represent
(Master, â€“, Apache, â€“, â€“).
Exhaustive testing covers all n-way schemas of the system; in our example, it uses 2 Ã— 2 Ã— 2 Ã— 3 Ã— 3 = 72 test
cases. Testing cost becomes prohibitive as the numbers of factors and levels increase. Moreover, because only interactions
among few factors are likely to trigger failures [2], testing
high-way schemas can lead to many uninformative test cases.
At the other extreme, if we only guarantee to cover each 1-way
schema once, only three test cases are needed (a single test
case can cover five 1-way schemas at most). But we may
fail to detect some interaction triggered failures involving two
factors.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

TABLE II
C OVERING A RRAY CA(9; 2, 23 32 )

Instead, CT covers all t-way schemas. Such a test suite is a
t-way covering array, with t being the covering strength. The
value of t determines the depth of coverage. It is a key setting
of CT, and should be decided by the testers. We give a precise
definition.
Definition 2 (Covering Array): If an NÃ—n array, where N is
the number of test cases, has the following properties: 1) each
column i (1 â‰¤ i â‰¤ n) contains only elements from the set Vi
with i = |Vi | and 2) the rows of each N Ã— t sub array cover
all |Vk1 | Ã— |Vk2 | Ã— . . . Ã— |Vkt | combinations of the t columns at
least once, where t â‰¤ n and 1 â‰¤ k1 < . . . < kt â‰¤ n, then it is a
t-way covering array, denoted by CA(N; t, n, (1 , 2 , . . . , n )).
When 1 = 2 = . . . = n = , it is denoted by CA(N; t, n, ).
Reference [2] demonstrated that more than 70% failures can
be detected by a 2-way covering array, and almost all failures
can be detected by a 6-way covering array. Hence, using CT,
we can detect many failures of the system by applying relatively low strength covering arrays. In other words, CT can
greatly reduce the size of test suite while maintaining high
fault detection ability.
In the example of Table I, if we only consider the interactions between any two factors, only nine test cases are
required to construct a 2-way covering array instead of 72
for exhaustive testing. Table II shows a covering array, where
each row represents one test case. This table covers all possible 2-way schemas for any two factors corresponding to the
columns. For example, consider factor Payment Server and
User Browser, all 2Ã—3 = 6 schemas, (Master, â€“, â€“, Firefox, â€“)
(Master, â€“, â€“, Explorer, â€“), (Master, â€“, â€“, Chrome, â€“),
(VISA, â€“, â€“, Firefox, â€“), (VISA, â€“, â€“, Explorer, â€“),
(VISA, â€“, â€“, Chrome, â€“), can be found in the table.
For convenience, if several groups of gi factors (gi < n) have
g
the same number of levels ak , ak i can be used to represent
these factors and their levels. Thus, the coveringarray can
g
g
g
gi = n,
be denoted by CA(N; t, a11 , a22 , . . . , ak k ) where
n
or CA(N; t, a ) when g1 = n and a1 = a. For example, the
covering array in Table II is a CA(9; 2, 23 32 ).
In many software systems, the impacts of the interactions
among factors are not uniform. Some interactions may be more
prone to trigger system failures, while other may have little or no impact on the system. To effectively detect these
different interactions, variable strength (VS) covering arrays
can be applied. This can offer different covering strengths

577

TABLE III
A DDING T HREE T EST C ASES TO C ONSTRUCT
VCA(12; 2, 23 32 , CA(3, 22 31 ))

to different groups of factors, and can therefore provide a
practical approach to test real applications.
Definition 3 (VS Covering Array): A VS covering array,
m

1
denoted by VCA(N; t, a11 . . . akk , CA1 (t1 , bm
. . . bp p ), . . . ,
1
n
CAj (tj , cn11 . . . cqq )), is an N Ã— n covering array of covering
strength t containing one or more sub covering arrays, namely
CA1 , . . . , CAj , each of which has covering strength t1 , . . . , tj
all larger than t.
Consider the e-commerce system shown in Table I. If the
interactions of three factors, Payment Server, Web Server, and
Business Database, have a higher probability to trigger system failures, then a VS covering array can be constructed.
As in Table II, only three more test cases (Table III) are
needed to construct the VCA(12; 2, 23 32 , CA(3, 22 31 )). With
these 12 test cases, not only are all 2-way schemas of all five
factors covered, but also all 3-way schemas of these three factors (Payment Server, Web Server, and Business Database) are
covered.
B. Covering Array Generation
Covering arrays are used as test suites in CT. Covering array
generation is the process of test suite construction. It is the
most active area in CT with more than 50% of research papers
focusing on this field [1]. Due to limited testing resources, all
aim to construct a minimal covering array while still maintaining full coverage of combinations. Computational methods
have been used widely for covering array generation because
they can be applied to any systems. In general, these methods
generate all possible combinations first. Then they generate
test cases to cover these combinations one-by-one. One-testat-a-time is the most widely used strategy among evolutionary
algorithms to generate a covering array.
The one-test-at-a-time strategy was popularized by
AETG [28] and was further used by Bryce and Colbourn [29].
This strategy takes the model of SUT(n, 1 , . . . , n ) where
n is the number of factors and i is the number of valid
levels of factor i, and the covering strength t as input. At
first, an empty test suite TS and a set S of t-way schemas
to be covered are initialized. In each iteration, a test case is
generated with the highest fitness value according to some
heuristic techniques. Then it is added to TS and the t-way
schemas covered by it are removed. When all the t-way
schemas have been covered, the final test suite TS is returned.
This process is shown in Algorithm 1.
In this strategy, a fitness function must be used to evaluate
the quality of a candidate test case (line 6 in Algorithm 1). It is
an important part of all heuristic techniques. In covering array
generation, the fitness function takes the test case as the input
and then outputs a fitness value representing its â€œgoodness.â€
It is defined as follows.

578

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

Algorithm 1 One-Test-at-a-Time Strategy
1: Input: SUT(n, 1 , . . . , n ) and covering strength t
2: Output: covering array TS
3: TS = âˆ…
4: Construct S (all the t-way schemas to be covered) based
on n, 1 , . . . , n and t
5: while S  = âˆ… do
6:
Generate a test case p with the highest fitness value
according to some heuristics
7:
Add p to the test suite TS
8:
Remove the t-way schemas covered by p from S
9: end while
10: return TS

Definition 4 (Fitness Function): Let TS be the generated
test set, and p be a test case. Then fitness(p) is the number of
uncovered t-way schemas in TS that are covered by p.
The fitness function can be formulated as
fitness(p) = |schemat ({p}) âˆ’ schemat (TS)|

(1)

where schemat (TS) represents the set of all t-way schemas
covered by test set TS, and | Â· | stands for cardinality. When
all Cnt t-way schemas covered by p are not covered by TS,
the fitness function reaches the maximum value fitness(p) =
|schemat ({p})| = Cnt .
For example, consider the 2-way covering array generation
of the e-commerce system shown in Table II. Suppose that
TS consists of test cases (0, 0, 0, 0, 0) and (0, 0, 1, 1, 1).
The fitness function computes the fitness value for a candidate test case p = (1, 0, 1, 2, 2) as follows: because p covers
ten 2-way schemas, namely schema2 ({p}) = {(1, 0, â€“, â€“, â€“),
(1, â€“, 1, â€“, â€“,), (1, â€“, â€“, 2, â€“), (1, â€“, â€“, â€“, 2), (â€“, 0, 1, â€“, â€“),
(â€“, 0, â€“, 2, â€“), (â€“, 0, â€“, â€“, 2), (â€“, â€“, 1, 2, â€“), (â€“, â€“, 1, â€“, 2),
(â€“, â€“, â€“, 2, 2)} and TS only covers (â€“, 0, 1, â€“, â€“) in p, the
function returns fitness(p) = 9.
C. PSO
PSO is a swarm-based evolutionary computation technique.
It was developed by Kennedy et al. [30], inspired by the social
behavior of bird flocking and fish schooling. PSO utilizes a
population of particles as a set of candidate solutions. Each
of the particles represents a certain position in the problem
hyperspace with a given velocity. A fitness function is used
to evaluate the quality of each particle. Initially, particles are
distributed in the hyperspace uniformly. Then each particle
repeatedly updates its state according to the individual best
position in its history (pbest) and the current global best position (gbest). Eventually, each particle possibly moves toward
the direction of the individual optimum and global optimum,
and finds an optimal or near optimal solution.
Suppose that the problem domain is a Dâˆ’dimensional
hyperspace. Then the position and velocity of particle i can
be represented by xi âˆˆ RD and vi âˆˆ RD respectively. CPSO
uses the following equations to update a particleâ€™s velocity
and position, where vi,j (k) represents the jth component of the
velocity of particle i at the kth iteration, and xi,j (k) represents

its corresponding position:
vi,j (k + 1) = Ï‰ Ã— vi,j (k) + c1 Ã— r1,j Ã— (pbesti,j âˆ’ xi,j (k))
(2)
+ c2 Ã— r2,j Ã— (gbestj âˆ’ xi,j (k))
xi,j (k + 1) = xi,j (k) + vi,j (k + 1).

(3)

The best position of particle i in its history is pbesti , and
gbest is the best position among all particles. The velocityupdate equation (2) captures the three basic concepts of PSO.
The first is inertia, the tendency of the particle to continue in
the direction it has been moving. The second is memory of
the best position ever found by itself. The third is cooperation
using the best position found by other particles.
The parameter Ï‰ is inertia weight. It controls the balance between exploration (global search state) and exploitation
(local search state). Two positive real numbers c1 and c2
are acceleration constants that control the movement tendency toward the individual and global best position. Most
studies set Ï‰ = 0.9, and c1 = c2 = 2 to get the best
balance [17], [31], [32]. In addition, r1,j and r2,j are two uniformly distributed random numbers in the range of [0.0, 1.0],
used to ensure the diversity of the population.
If the problem domain (the search space of particles) has
bounds, a bound handling strategy is adopted to keep the particles inside the search space. Many different strategies have
been proposed [33]. In the reflecting strategy, when a particle
exceeds the bound of the search space in any dimension, the
particle direction is reversed in this dimension to get back to
the search space. For example, in case of a dimension with
a range of values from 0 to 2, if a particle moves to 3, its
position is reversed to 1.
In addition, as the velocity can increase over time, a limit
is set on velocity to prevent an infinite velocity or invalid
position for the particle. Setting a maximum velocity, which
determines the distance of movement from the current position
to the possible target position, can reduce the likelihood of
explosion of the swarm traveling distance [31]. Generally, the
value of the maximum velocity is selected as i /2, where i
is the range of dimension i.
The pseudocode in Algorithm 2 presents the process of generating a test case by PSO. This algorithm can be invoked
by line 6 of Algorithm 1 to generate a test case for t-way
schemas. The n factors of the test case can be treated as an
n-dimensional hyperspace. A particle pi = (x1 , x2 , . . . , xn ) can
be regarded as a candidate test case. The fitness function is
that of Definition 4, the number of uncovered t-way schemas
in the generated test suite that are covered by particle pi . PSO
employs real numbers but the valid values are integers for covering array generation, so each dimension of particleâ€™s position
can be rounded to an integer while maintaining the velocity
as a real number. This method is used in all prior research
applying PSO to covering array generation [13]â€“[16].
III. R ELATED W ORK
In this section, three different but related aspects are discussed. We first summarize search-based CT, especially the
current applications of PSO for covering array generation.
Then, we introduce prior research on discrete versions of PSO,

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

Algorithm 2 Generate Test Case by PSO
1: Input: SUT(n, 1 , . . . , n ), covering strength t and the
related parameters of PSO
2: Output: the best test case gbest
3: it = 0, gbest = NULL
4: for each particle pi do
5:
Initialize the position xi and velocity vi randomly
6: end for
7: while it < maximum iteration do
8:
for each particle pi do
9:
Compute the fitness value fitness(pi )
10:
if fitness(pi ) > fitness(pbesti ) then
11:
pbesti = pi
12:
end if
13:
if fitness(pi ) > fitness(gbest) then
14:
gbest = pi
15:
end if
16:
end for
17:
for each particle pi do
18:
Update the velocity and position according to
Equations 2 and 3
19:
Apply maximum velocity limitation and bound
handling strategy
20:
end for
21:
it = it + 1
22: end while
23: return gbest

to improve CPSO for discrete problems. Finally, some PSO
variants are introduced. These try to enhance PSO for conventional continuous problems by adopting strategies that can
further improve covering array generation.
A. PSO in Search-Based CT
SBSE has grown quickly in recent years. Many problems
in software engineering have been formulated into searchbased optimization problems, and heuristic techniques have
been used to find solutions. Software testing is a major topic
in software engineering. Many heuristic techniques have also
been applied to this field, including functional testing, mutation testing, stress testing, temporal testing, CT, and regression
testing. Currently, many classic heuristic techniques, such as
SA [3]â€“[7], GA [8]â€“[10], and ACO [9], [11], [12] have been
applied to generate uniform and VS covering arrays
successfully.
PSO has also been applied to software testing.
Windisch et al. [34] applied PSO to structural testing,
and compared its performance with GA. They showed that
PSO outperformed GA for most cases in terms of effectiveness and efficiency. Ganjali [35] proposed a framework for
automatic test partitioning based on PSO, observing that PSO
performed better than other existing heuristic techniques.
PSO has been applied to covering array generation.
Ahmed and Zamli [15] proposed frameworks of PSO to generate different kinds of covering arrays, including t-way
(PSTG [14]) and VS-PSTG. Then they extended PSTG to support VS covering array generation [16]. Chen et al. [18] also

579

applied PSO to 2-way covering array generation. They further
used a test suite minimization algorithm to reduce the size of
the generated covering array.
Current applications of PSO for covering array generation
can yield smaller covering arrays than most greedy algorithms,
but they all apply the same rounding operator to the particleâ€™s
position, and they lack guidelines on the parameter settings.
B. Discrete Versions of PSO
PSO was initially developed to solve problems in continuous
space, but PSO can also be applied to some discrete optimization problems including binary or general integer variables.
Many discrete versions of PSO have been proposed [17]â€“[22].
Chen et al. [18] classify existing algorithms into four
types.
1) Swap operator-based PSO [19] uses a permutation of
numbers as position and a set of swaps as velocity.
2) Space transformation-based PSO [20] uses various techniques to transform the position, defined as a vector of
real numbers, to the corresponding solution in discrete
space.
3) Fuzzy matrix-based PSO [21] defines the position and
velocity as a fuzzy matrix, and decode it to a feasible
solution.
4) Incorporating-based PSO [22] consists of hybrid methods incorporating some heuristic or problem dependent
techniques.
Chen et al. [18] also propose a S-PSO method based on
sets with probabilities, which we later adapt to represent a
particleâ€™s velocity. S-PSO uses a set-based scheme to represent the discrete search space. The velocity is defined as
a set with probabilities, and the operators are all replaced
by procedures defined on the set. They extend some PSO
variants to discrete versions and test them on the traveling
salesman problem and the multidimensional knapsack problem. They show that the discrete version of CLPSO [25] can
perform better than other variants. S-PSO was found to characterize the combinatorial optimization problems very well.
Gong et al. [36] then employed such a set-based representation scheme for solving vehicle routing problems to obtain a
new method, S-PSO-VRPTW.
C. PSO Variants
The original PSO may become trapped in a local optimum.
In order to improve the performance, many variants have been
proposed [17], [24]â€“[27]. Chen et al. [37] classify these variants into four different groups. The first group of variants aims
to adjust the control parameters during the evolution, for example by decreasing inertia weight Ï‰ linearly from 0.9 to 0.4 over
the search process. The second group employs different neighborhood topologies. The basic local version of PSO makes
the particle learn from the local best position lbesti found by
particle iâ€™s neighborhood instead of the global best position
gbest. RPSO and VPSO are two common versions which use
a ring topology and a Von Neumann topology, respectively.
The third group uses hybrid strategies with other search techniques. Many types of operators, such as genetic operators

580

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE IV
D IFFERENT G ROUPS OF DPSO AND PSO VARIANTS

(selection, crossover, and mutation), evolutionary computation
paradigms and biology inspired operators have been used. The
fourth uses multiswarm techniques. Several sets of swarms
optimize different components of the solution concurrently or
cooperatively.
In our experiments, four representative PSO variants are
included, as follows.
1) Ratnaweera et al. [24] proposed a typical variant of
the first group, TVAC, which uses a time varying
inertia weight and acceleration constant to adjust the
parameters.
2) Liang et al. [25] proposed the CLPSO method, a wellknown variant of the second group, which allows the
particle to learn from other particlesâ€™ individual best
positions in different dimensions.
3) Zhan et al. [26] proposed an adaptive PSO (APSO)
that can be seen as a variant of the third group. They
developed an evolutionary state estimation to adaptively control the parameter and used an elitist learning
strategy.
4) DMS-PSO [27], a variant of the fourth group, was proposed by Liang and Suganthan [27]. It is characterized
by a set of swarms with small sizes and these swarms
are frequently regrouped.
In summary, Table IV lists the different groups of discrete
versions of PSO and PSO variants.
IV. DPSO
In this section, a new DPSO for covering array generation
is presented. We firstly illustrate the weakness of CPSO with a
simple example. Then the representation scheme of a particleâ€™s
velocity and the corresponding redefined operators are introduced. Finally, we give two auxiliary strategies to enhance the
performance of DPSO.
A. Example
In CPSO, a particleâ€™s position represents a candidate test
case; its velocity, a real vector, represents the movement tendency of this particle in each dimension. This scheme is
meaningful in a continuous optimization problem, because
an optimal solution may exist near the current best particleâ€™s
position. So it is desirable to move the particle to this area
for further search. This may not hold for covering array
generation.

Here, we use CA(N; 2, 34 ) as an example. Suppose that
three test cases (0, 1, 0, 1), (2, 1, 1, 2), and (1, 2, 0, 0) have
been generated and added to TS in Algorithm 1, and the fourth
one is to be generated according to CPSO. A possible candidate particle pi may have the position (0, 0, 0, 0) and velocity
(0.5, 0.6, âˆ’0.4, 0.2) with fitness value 4, and its individual
best position pbesti may be (0, 0, 1, 1) with fitness value 5.
The global best position gbest may be (0, 2, 2, 2) with fitness
value 6. According to the update (2), if we take Ï‰ = 0.9 and
c1 Ã— r1 = c2 Ã— r2 â‰ˆ 0.65, in the next iteration, pi â€™s velocity
may be transformed to (0.45, 1.84, 1.59, 2.13). Thus, pi moves
to the new position (0, 1, 1, 1) with a limitation on the velocity (the velocity is kept in [âˆ’i /2, i /2]), or (0, 1, 1, 2) with
no limitation. In both cases, pi only has fitness value 2 after
updating. When this occurs, pi evolves to a worse situation,
although its position is closer to the pbesti and gbest than its
original one.
Analyzing the fitness measurement, the main contribution
to the fitness value is the combinations that the test case can
cover, not the concrete â€œpositionâ€ at which it is located. For
example, test case (2, 1, 1, 2) has a larger fitness value than
(0, 0, 0, 0) because it covers six new schemas [(2, 1, â€“, â€“),
(2, â€“, 1, â€“), (2, â€“, â€“, 2) etc.], not because of its relative distance
to other particles.
B. DPSO
To overcome this weakness, the movement of particles
should be modified. Inspired by the set-based representation scheme of velocity, DPSO is designed as a version of
S-PSO [18] to make the particle learn from the individual and
global best more effectively when generating covering arrays.
Unlike S-PSO, the element of the velocity set in DPSO is
designed for covering array generation, and DPSO does not
classify the velocity set into different dimensions to avoid the
inconsistency of different dimensions when updating velocities
in S-PSO.
In DPSO, a particleâ€™s position represents a candidate test
case, while its velocity is changed to a set of t-way schemas
with probabilities. Other than the velocityâ€™s representation and
the newly defined operators, the evolution procedure of DPSO
is the same as CPSO (Algorithm 2).
Definition 5 (Velocity): The velocity is a set of pairs
(s, p(s)), where s is a possible t-way schema of the covering
array and p(s) is a real number between 0 and 1 representing the probability of the selection of schema s to update the
current position.
In the initialization of the swarm, the particleâ€™s position is
randomly assigned. Then Cnt possible different schemas are
selected randomly and each of them is assigned a random
probability p(s) âˆˆ (0, 1). These Cnt pairs form the initial velocity set of this particle; the size of this set changes dynamically
during the evaluation.
We consider the same example CA(N; 2, 34 ). In DPSO,
when particle pi is initialized, its position xi (k) may be
(0, 0, 0, 0) representing a candidate test case as before, and
its velocity vi (k) may be such a set {((1, 1, â€“, â€“), 0.7),
((0, â€“, 0, â€“), 0.3), ((â€“, 0, â€“, 1), 0.8), ((0, â€“, â€“, 2), 0.9),

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

(a)

(b)

(c)

581

(d)

(e)

Fig. 1. Example of pi â€™s velocity updating. (a) 0.9 Ã— vi (k). (b) 2 Ã— r1 Ã— (pbesti âˆ’ xi (k)). (c) 2 Ã— r2 Ã— (gbest âˆ’ xi (k)). (d) vi (k + 1). (e) Final vi (k + 1) where
pro1 = 0.5.

((â€“, 2, 1, â€“), 0.5), ((â€“, â€“, 0, 1), 0.2)} which contains C42 = 6
pairs.
DPSO follows the conventional evolution procedure
(Algorithm 2) that uses (2) and (3) to update the velocity and
position of a particle. To adapt the new scheme for velocity
in Definition 5, the related operators in these equations must
be redefined.
1) Coefficient Ã— Velocity: The coefficient is a real number
which may be a parameter or a random number. It modifies
all the probabilities in the velocity.
Definition 6 (Coefficient Ã— Velocity): Let a be a nonnegative real number and v be a velocity, a Ã— v = {(s, p(s) Ã—
a)|(s, p(s)) âˆˆ v}. (If p(s) Ã— a > 1, p(s) Ã— a = 1.)
For example, Fig. 1(a) shows the result for Ï‰ Ã— vi (k) where
Ï‰ = 0.9.
2) Positionâ€“Position: The difference of two positions gives
the direction on which a particle moves. The results of the
minus operator is a set of (s, p(s)) pairs, as velocity.
Definition 7 (Positionâ€“Position): Let x1 and x2 be two positions. Then x1 âˆ’ x2 = {(s, 0.5)|s is a schema that exists in x1
but not in x2 }.
In the newly generated schema, probability p(s) for s is
set to 0.5 so that the acceleration constants take similar
values in both CPSO and DPSO. As in (2), the result of
positionâ€“position is multiplied by ci Ã— ri . In CPSO, ci is
often set to 2 and ri is a random number between 0 and 1
(recall Section II-C). In DPSO, we want the value of final
probability to have a range between 0 and 1 after multiplying
by ci . Setting p(s) to 0.5 puts the result in [0, 1] when ci = 2.
For example, suppose that xi (k) = (0, 0, 0, 0), pbesti =
(0, 0, 1, 1), and gbest = (0, 2, 2, 2) as before.
We can get pbesti âˆ’ xi (k) = {((0, â€“, 1, â€“, â€“), 0.5),
((0, â€“, â€“, 1), 0.5), ((â€“, 0, 1, â€“), 0.5), ((â€“, 0, â€“, 1), 0.5),
((â€“, â€“, 1, 1), 0.5)}. Fig. 1(b) and (c) shows the results for
2Ã—r1 Ã—(pbesti âˆ’xi (k)) and 2Ã—r2 Ã—(gbestâˆ’xi (k)) respectively.
3) Velocity + Velocity: The addition of velocities gives a
particleâ€™s movement path. The plus operator results in the
union of two velocities.
Definition 8 (Velocity + Velocity): Let v1 and v2 be two
velocities. Then v1 + v2 = {(s, p(s))| if (s, p1 (s)) âˆˆ v1 and
(s, p2 (s)) âˆˆ v2 , p(s) = p1 (s) + p2 (s); if (s, pi (s)) âˆˆ v1
and (s, pi (s)) âˆˆ
/ v2 or (s, pi (s)) âˆˆ v2 and (s, pi (s)) âˆˆ
/ v1 ,
p(s) = pi (s)}. (If p1 (s) + p2 (s) > 1, p1 (s) + p2 (s) = 1.)
For example, Fig. 1(d) shows the results for pi â€™s new
velocity vi (k + 1) after updating velocity.

Algorithm 3 Position Updating
1: Input: position xi (k), velocity vi (k + 1), pro2 and pro3
2: Output: new position xi (k + 1)
3: xi (k + 1) = (â€“, â€“, . . . , â€“)
4: Sort vi (k + 1) in descending order of p(s)
5: for each pair (si ,p(si )) in vi (k + 1) do
6:
Generate a random number Î± âˆˆ [0, 1]
7:
if Î± < p(si ) then
8:
for each fixed level  in si do
9:
Generate a random number Î² âˆˆ [0, 1]
10:
if Î² < pro2 and the corresponding factor
of  has not been fixed in xi (k + 1) then
11:
Update xi (k + 1) with 
12:
end if
13:
end for
14:
end if
15: end for
16: if xi (k + 1) has unfixed factors then
17:
Fill these factors by the same levels of previous
position xi (k)
18: end if
19: Generate a random number Î³ âˆˆ [0, 1]
20: if Î³ < pro3 then
21:
randomly change the level of one factor of xi (k + 1)
22: end if
23: return xi (k + 1)

In the velocity-updating phase of DPSO (2), we introduce
a new parameter pro1 to control the size of the final velocity
set. If p(si ) < pro1 , the pair (si , p(si )) is removed from the
final velocity. For example, if we set pro1 = 0.5, vi (k + 1) is
removed as shown in Fig. 1(e). Here, the velocity has been
sorted in descending order of p(s), where if several p(s) have
the same value, they are in an arbitrary order. If vi (k + 1)
becomes empty, it stays empty until new pairs are added to
it. As long as the velocity is empty, the particleâ€™s position is
not updated and no better solutions can be found from this
particle. In Section IV-C1, we discuss how to reinitialize this
particle.
4) Position+Velocity: Position plus velocity is the position
updating phase. Algorithm 3 gives the pseudo code of this
procedure. Here, two new parameters, pro2 and pro3 , numbers
in the range [0, 1], are introduced. pro2 is used to determine

582

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

the probability of selecting each fixed level in schema s and
pro3 is a mutation probability to make the particle mutate
randomly.
An example helps to describe this procedure. We already
have pi â€™s current position xi (k) = (0, 0, 0, 0), and its updated
velocity vi (k + 1) in descending order of p(s) as shown in
Fig. 1(e). We also assume that pro2 and pro3 are both set to
0.5. Each schema si here is selected to update the position
with probability p(si ). For the first pair ((0, â€“, â€“, 2), 1.0),
suppose that the random number Î± satisfies Î± < 1.0. Then,
for each fixed level of this pair, namely level 0 of the first
factor and level 2 of the fourth factor, its corresponding factor
has not been fixed in xi (k + 1). Suppose that we have the
first Î² < 0.5 but the second Î² > 0.5, the first factor will be
selected to update the position and the second factor will not.
So the new position becomes (0, â€“, â€“, â€“). For the second pair,
we regenerate the random number Î±, and compare it with the
probability 0.9. If Î± < 0.9, the second pair is selected. If we
generate Î² < 0.5 in two rounds, the new position becomes
(0, 2, 2, â€“). Accordingly, if the third pair is selected, and its
second factorâ€™s level 0 is chosen to update, it does not change
position because this factor has been set to a fixed level 2.
This procedure is repeated until all factors in the new position
xi (k+1) are set to fixed levels. If all pairs in velocity have been
considered, unfixed factors of xi (k + 1) are filled by the same
levels of previous position xi (k). For example, after finishing
the For loop in line 15, if the fourth factor of xi (k + 1) has
not been given any level, the fourth factor of xi (k) is used to
update it. Then xi (k + 1) becomes (0, 2, 2, 0).
C. Auxiliary Strategies
Auxiliary strategies can be introduced to improve the performance of PSO. Two of them are added to DPSO: 1) particle
reinitialization to make the search more effective and 2) additional evaluation of gbest to improve the selection of the global
best test case.
1) Particle Reinitialization: The PSO algorithm starts with
a random distribution of particles, which finally converge.
Then the best position that has been found is returned. The
swarm may jump out of a local optimum, but this can not
be guaranteed because CPSO lacks specific strategies for this.
When applying PSO for covering array generation, increasing
the number of iterations does not improve the ability to escape
a local optimum. Hence, particle reinitialization, a widely used
method, is employed to help DPSO to jump out of the local
optimum.
The main issue with particle reinitialization is when to reinitialize the particle. We may select a threshold, so that the
reinitialization is done when the number of iterations exceeds
the threshold. In DPSO, a better method can be applied. Using
the new representation for velocity, when the current particle pi â€™s position equals its individual best position pbesti
and global best position gbest, the size (norm) of pi â€™s velocity reduces gradually, because no pairs are generated from
(pbesti âˆ’ xi ) and (gbest âˆ’ xi ), and the original pairs in velocity
are removed gradually under the influence of Ï‰ Ã— v (reduce
the p(s) of original pairs) and parameter pro1 . After a few
fluctuations around gbest, the particle may stay at gbest, and

TABLE V
T WO D IFFERENT C ONSTRUCTIONS OF CA(N; 2, 34 )

its size (norm) of velocity is zero. We can use this scenario to
trigger reinitialization of the particle. When the reinitialization
is done, each dimension of a particleâ€™s position is randomly
assigned a valid value, and its velocity is regenerated as in the
initialization of the swarm.
2) Additional Evaluation of gbest: Current PSO methods
to generate covering arrays generally employ the fitness function in Definition 4. This fitness function only focuses on
the current candidate, and does not consider the partial test
suite TS.
Consider the generation of CA(N; 2, 34 ). Table V shows two
different construction processes. Both constructions generate
(0, 0, 0, 0) as the first test case. Then they choose different test
cases, but each of the first three reaches the largest number of
newly covered combinations, C42 = 6. The difference between
these two constructions emerges when generating the fourth
test case. In Construction 1, because the combinations with the
same level between any two factors have all been covered, we
cannot find a new test case that can still cover six combinations. However, in Construction 2, such a new test case can be
found, (1, 0, 1, 2). Because the minimum size of CA(N; 2, 34 )
is 9, each test case is required to cover six new combinations.
Thus, Construction 1 cannot generate the minimum test suite,
but Construction 2 can.
In general, there may exist multiple test cases with the same
highest fitness value, which make them equally qualified to be
gbest in Algorithm 2. Instead of arbitrarily selecting one as
gbest, it is better to apply additional distance metric to select
one among them. As shown in Table V, if the new test case
is similar to the existing tests [as (0, 1, 1, 1) is closer to
(0, 0, 0, 0) than (1, 1, 1, 1)], there may be a better chance to
find test cases with larger fitness subsequently.
In order to measure the â€œsimilarityâ€ between a test case t
and an existing test suite TS, we use the average Hamming
distance. The Hamming distance d12 indicates the number of
factors that have different levels between two test cases t1 and
t2 . Hence, the similarity between t and TS can be defined by
the average Hamming distance
H(t, TS) =

1 
dtk .
|TS|

(4)

kâˆˆTS

The new gbest selection strategy selects the best candidate gbest based on two metrics. A test case that has the
minimum average Hamming distance (i.e., the maximum similarity) from the set of test cases that all have the same highest
fitness value is selected as gbest in Algorithm 2. For example, when we generate the second test case for CA(N; 2, 34 )
in Table V, although (1, 1, 1, 1) and (0, 1, 1, 1) both have
the highest fitness value 6, (0, 1, 1, 1) is selected as gbest

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

583

TABLE VI
C OMPARING P ERFORMANCE B ETWEEN CPSO AND DPSO

because its average Hamming distance, 3, is smaller than that
of (1, 1, 1, 1), 4. We expect that this additional evaluation can
enhance the probability of generating a smaller test suite. In
addition, because we still want to make the particle follow the
conventional search behavior on the individual best direction,
this additional distance metric will not be used in updating the
pbest of DPSO.
V. E VALUATION AND PARAMETER T UNING
In this section, we first evaluate the effectiveness of DPSO
in some representative cases, and compare the results against
CPSO. Then the optimal parameter settings for both CPSO
and DPSO are explored. The goal of evaluation and parameter
tuning is to make the size of generated covering array as small
as possible. Five representative cases of covering arrays, listed
below, are selected for our experiments
CA1 (N; 2, 610 )

CA2 (N; 3, 57 )

CA3 (N; 4, 39 )

CA4 (N; 2, 43 53 62 ) VCA(N; 2, 315 , CA(3, 34 )).
We consider four independent parameters, iteration number
(iter), population size (size), inertia weight (Ï‰), and acceleration constant (c), which play similar roles in both CPSO
and DPSO, and three new parameters for DPSO, pro1 , pro2 ,
and pro3 . We carry out a base choice experiment to study the
impact of various values of these parameters on CPSO and
DPSOâ€™s performance and find the recommended settings for
them. First, a base setting is chosen as the basic configuration. Then the value of each parameter is changed to create
a series of configurations, while leaving the other parameters
unchanged. Initially, we set iter = 50, size = 20, Ï‰ = 0.9,
c = 1.3, and pro1 = pro2 = pro3 = 0.5 as a basic configuration based on the previous studies [13], [16] and our
empirical experience. To obtain statistically significant results,
the generation of each case of covering array is executed
30 times.
A. Evaluation of DPSO
We compare the performance between CPSO and DPSO
with the basic configuration. Five classes of covering arrays
are generated by these two algorithms. The sizes obtained and
average execution time per test case are shown as CPSO1 and
DPSO in Table VI. The best and mean array sizes of DPSO are
all better than those of CPSO1 . The new discrete representation scheme and auxiliary strategies improve PSO for covering
array generation.
DPSO can produce smaller covering arrays than CPSO with
the basic configuration. However, DPSO spends more time

during the evolution, because its new operators for updating
are more intricate than the conventional ones. DPSO needs
to deal with many elements of the velocity set, whereas the
conventional scheme only needs simple arithmetic operations.
To compare the performance between CPSO and DPSO given
the same execution time, for each case we let the execution
time per test case for CPSO equal to that for DPSO, so that
CPSO can spend more time in searching. We refer to this
version of CPSO as CPSO2 . In addition, a t-test between
CPSO2 and DPSO is conducted and the corresponding p-value
is presented. A p-value smaller than 0.05 indicates that the performance of these two algorithms is statistically different with
95% confidence.
From CPSO1 , CPSO2 , and DPSO in Table VI, the performance of CPSO is improved with more search time. DPSO,
which must use fewer iterations, still works better than CPSO.
The effectiveness of DPSO comes from the essential improvement of its new representation scheme for velocity and the
auxiliary strategies. The results of the t-test demonstrate the
significance of these differences. Therefore, we can conclude
that DPSO performs better than CPSO with fewer iterations
for covering array generation.
B. Parameter Tuning
In the base choice experiment, because the sizes of the various covering arrays differ, the mean size of each case (si )
obtained is normalized using
si =

si âˆ’ smin
.
smax âˆ’ smin

This normalization enables the graphical representation of
different cases on a common scale.
Because some parameters may not significantly impact the
performance, we use ANOVA (significance level = 0.05) to
test whether there exist significant differences among the mean
results obtained by different parameter settings. When changing the parameter settings have no significant impact on the
generation results, these results will be presented as dotted
lines in the corresponding figures. For example, CA4 is presented as a dotted line in Fig. 2(b). It means that the iteration
number does not significantly impact the generation of this
case of covering array, and so this case will not be further
considered when identifying the optimal settings.
1) Iteration Number (iter): Iteration number determines the
number of updates. PSO typically requires thousands of iterations to evolve. Nevertheless, only a few iterations are required
to generate covering array according to [14]â€“[16]. So we

584

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

(a)
Fig. 2.

(b)

Comparing array sizes under different iteration numbers. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 3.

Comparing array sizes under different population sizes. (a) CPSO. (b) DPSO.

(a)
Fig. 4.

(b)

Comparing array sizes under different settings of population size and iteration number. (a) CPSO. (b) DPSO.

change its value from 50, the base setting, to 1950 incrementally. Much larger values are not used because the execution
time may increase markedly without a commensurate increase
in the quality of the results.
Fig. 2 shows the results for different choices of iteration numbers, where each line represents a covering array.
Performance is improved with increasing iterations for both
CPSO and DPSO. A small number of iterations may not be
appropriate due to insufficient searching. Because the optimal
settings are different among different cases, and several settings can be chosen to generate the minimum covering arrays,
it is open to debate which setting is the â€œbestâ€ one. Given
time constraints, for a population size of 20, a good setting
of iteration number could be approximately 1000 for both
CPSO and DPSO.
2) Population Size (Size): Population size determines the
initial search space. Generally a small value, e.g., 20, is satisfactory for most cases. A large population size may bring a
higher diversity and find a better solution, but it also increases
the evolving time. For the same reason as before, we change
the value of population size from 10 to 100.

Fig. 3 shows the results for different choices of population size. There is no doubt that the mean array size obtained
decreases as population size increases. A large population size
can have more chances to generate smaller covering arrays, but
its execution time can become prohibitive.
In addition, because iteration number and population size
together determine the search effort of PSO, we explore the
combinations between these two parameters. We let iter Ã— size
be a constant 20 000, and generate each case under different
settings of these two parameters. Fig. 4 shows the results,
where PSO prefers a relatively larger population size. In
CPSO, ten particles with 2000 iterations is the worst setting,
and most cases produce good results with 60 or 80 particles.
In DPSO, the best choice of population size is still 80. In
both CPSO and DPSO, the largest population size 100 cannot
produce as good results as that of 80 due to having fewer iterations. So achieving smallest covering arrays requires a good
balance between these two parameters, and a moderately large
population size is necessary for both CPSO and DPSO. Thus,
we can set iteration number to 250, and population size to 80,
as the recommended settings for both CPSO and DPSO.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

(b)

(a)
Fig. 5.

Comparing array sizes under different inertia weights. (a) CPSO. (b) DPSO.

(b)

(a)
Fig. 6.

Comparing array sizes under different acceleration constants. (a) CPSO. (b) DPSO.

(a)
Fig. 7.

585

(b)

(c)

Comparing array sizes under different settings of (a) pro1 , (b) pro2 , and (c) pro3 .

3) Inertia Weight (Ï‰): Inertia weight determines the tendency of the particle to continue in the same direction. A
small value help the particle move primarily toward the best
position, while a large one is helpful to continue its previous
movement. A linearly decreasing value is also used as it can
make the swarm gradually narrow the search space. Here, we
investigate both fixed values and a linearly decreasing value
from 0.9 to 0.4 over the whole evolution (presented as â€œdecâ€).
Fig. 5 shows the results for different choices of inertia
weight. In CPSO, most of the smallest covering arrays are
generated by large fixed inertia weights. The decreasing value
does not perform as well as the large values, such as 0.9 for
CA1 , CA2 , and CA4 . So 0.9 can be the recommended choice
for CPSO. In DPSO, 1.1 is the worst choice because the particle may update its position based on its own velocity and
thus fail to learn from individual and global best positions.
The decreasing value can perform reasonably well, but a fixed
value 0.5 may be a better choice in that it keeps the effort of
global search moderate. Thus, we can recommend the fixed
inertia weight of 0.9 for CPSO, and 0.5 for DPSO.
4) Acceleration Constant (c): Acceleration Constants
c1 and c2 control the learning from pbest and gbest, respectively. Generally, they are set to the same value to balance

TABLE VII
R ECOMMENDED PARAMETER S ETTINGS

the influence of these two positions. Setting c1 and c2 to a
large value may make the particle more likely attracted to the
best position ever found, while a small one may make the
search far from the current optimal region. In this paper, we
set c1 = c2 = c, and vary c from 0.1 to 2.9.
Fig. 6 shows the results for different choices of acceleration
constant. Unlike other parameters, there is a consistent trend in
all five cases. In CPSO, the values larger than 0.5 all produce
good results. In DPSO, 1.3 can definitely be regarded as the
optimal value. Thus, we set 1.3 as the recommended value of
acceleration constant for both CPSO and DPSO.
5) pro1 , pro2 , and pro3 : These three parameters are new to
DPSO. Parameter pro1 determines the size of the final velocity. As those pairs whose probabilities are smaller than pro1
are removed from the final velocity set, a small value may
keep more pairs in the velocity for the next evolution, but it

586

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE VIII
S IZES (N) OF C OVERING A RRAYS FOR n FACTORS , E ACH H AVING T HREE L EVELS , W ITH C OVERING S TRENGTH t, CA(N; t, n, 3)

also requires more computation time. Parameter pro2 determines the probability of selecting each fixed level from each
schema when updating positions. A larger value may lead to
a quick construction of new position, but it may also lead
to fast convergence toward a local optimum. Parameter pro3
determines the mutation probability when updating positions.
A larger value may enhance the randomness, but it also lowers
the convergence speed. In this paper, values from 0.1 to 1.0
for these three parameters are investigated.
Fig. 7 shows the results. For pro1 , a large value is not
appropriate because it removes nearly all pairs from the final
velocity set. A medium value 0.5, which appears to lead to the
best result, may be the best choice. For pro2 , the smallest value
0.1 yields the best results. Slower construction of the new position may slow the convergence toward a local optimum, but it
also takes longer. So, in order to balance performance and execution time, we take 0.3 as the recommended value for pro2 .
For pro3 , a larger value may be a good choice. The frequent
mutation of new position may bring better results, but also
takes longer to converge. So, we take 0.7 as the recommended
value for pro3 .
In summary, the recommended parameter settings for PSO
for covering array generation are different from previously
suggested ones [13], [16]. Some parameters may significantly
impact the performance in some cases, and parameter tuning is necessary to enhance heuristic techniques for particular
applications. Naturally, the optimal settings vary for different cases of covering arrays. There may not exist a common
setting that can always lead to the best results. Because it is

impractical to tune parameters for each case in real life, we
suggest two general settings for CPSO and DPSO, as shown
in Table VII, which can typically lead to better performance
within reasonable execution time.
VI. C OMPARING A MONG PSO S
In this section, we compare the best reported array sizes
generated by PSO in [16] with our findings for CPSO, DPSO,
and four representative variants. Because the research in [16]
demonstrated that their generation results typically outperform
greedy algorithms, in this paper, we do not compare CPSO and
DPSO with greedy algorithms.
We implement both the original and discrete versions of
four variants (TVAC [24], CLPSO [25], APSO [26], and
DMS-PSO [27]) to generate covering arrays. Their discrete
versions are extended based on new representation scheme of
velocity and auxiliary strategies. We name them D-TVAC, DCLPSO, D-APSO, and D-DMS-PSO, respectively. In CLPSO,
a particle can learn from different particlesâ€™ pbest in each
dimension whereas we do not distinguish dimensions strictly
in DPSO. So in D-CLPSO, a particle can fully learn from
different particlesâ€™ pbest in all dimensions. That may weaken
the search ability of CLPSO. For the other three variants, they
can be directly extended based on DPSO.
All algorithms are compared using the same number of
fitness function evaluations. CPSO and DPSO use the recommended settings shown in Table VII. For the variants, iter,
size and pro1 , pro2 and pro3 for discrete versions are set to

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

587

TABLE IX
S IZES (N) OF C OVERING A RRAYS FOR S EVEN FACTORS , E ACH H AVING  L EVELS , W ITH C OVERING S TRENGTH t, CA(N; t, 7, )

their recommended values. Ï‰ and c are also set to recommended values unless they are adaptively adjusted during the
evolution, in which case their range is set to [0.4, 0.9] and
[0.8, 1.8], respectively. The new control parameters for these
variants follow their suggested settings.
Tables VIIIâ€“XIII give the results. Because of the execution
time, we only consider covering strengths from 2 to 4, and the
generation of each covering array is repeated 30 times. A t-test
(significance level = 0.05) is also conducted to test whether
there exists a significant difference between the mean sizes
produced by the two algorithms. In the first three columns, we
report the best and mean array sizes obtained from previous
results, CPSO and DPSO, where boldface numbers indicate
that the difference between CPSO and DPSO is significant
based on the t-test. In the last four columns, we report the mean
array sizes from the original and discrete versions of each PSO
variant (presented as meanc and meand respectively), where
boldface numbers indicate that the difference between meanc
and meand of each variant is significant.
A. Uniform Covering Arrays
Tables VIIIâ€“X present the results for uniform covering
arrays. We extend the cases considered in [16], where â€œâ€“â€
indicates the not available cases. In Tables VIII, we report
array sizes for n factors, each having three levels. In
Tables IX and X, we report array sizes for 7 and 10 factors,
each having  levels. Their covering strengths all range from
2 to 4.
Typically, CPSO can produce smaller sizes than those
reported in [16], demonstrating the effectiveness of parameter tuning. Furthermore, DPSO can produce the smallest best
and mean sizes in almost all cases, and usually the performance of DPSO is significantly better than CPSO. When the
covering strength t = 2 is adopted (Table VIII), DPSO does

not generally outperform CPSO. But when covering strength
increases, DPSO performs better. Sometimes the mean sizes
for DPSO are smaller than the best sizes for CPSO. Because
generating a covering array with higher covering strength is
more complicated, DPSO may be more appropriate for generating such covering arrays. In Tables IX and X, we also find
that DPSO is superior for covering arrays with high covering strength. Overall, these results show the effectiveness of
DPSO for uniform covering array generation.
Surprisingly, DPSO does not beat previous results for
CPSO when covering arrays have two levels for each factor
(Table IX). This appears to be a weakness of DPSO. Although
DPSO generates smaller covering arrays than previous results
and CPSO, other techniques may still yield better results than
DPSO (some best known sizes can be found in [38]).
B. VS Covering Array
Tables XIâ€“XIII give the results for VS covering arrays.
Based on CA(N; 2, 315 ), CA(N; 3, 315 ), and CA(N; 2, 42 52 63 ),
some different cases of sub covering arrays conducted in [16]
are examined. Their covering strengths are at most 4.
Generally, we can draw similar conclusions as for uniform
covering arrays. CPSO with the suggested parameter setting
can produce better results than reported sizes in some cases.
DPSO also usually beats them on the best and mean sizes.
In Table XI, often the difference between CPSO and DPSO
is not significant. In part this is because for the CA(3, 33 ),
CA(3, 33 )2 , CA(3, 33 )3 , and CA(4, 34 ), the same best results
are provably the minimum (e.g., the minimum size of the
CA(3, 33 ) is 3 Ã— 3 Ã— 3 = 27). For the other cases, although
sometimes the difference is not significant, DPSO can still generate smaller covering arrays. DPSO remains a good choice
for generating VS covering arrays. In Tables XII and XIII,
similar results can be found.

588

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

TABLE X
S IZES (N) OF C OVERING A RRAYS FOR T EN FACTORS , E ACH H AVING  L EVELS , W ITH C OVERING S TRENGTH t, CA(N; t, 10, )

TABLE XI
S IZES (N) OF VS C OVERING A RRAYS VCA(N; 2, 315 , CA)

TABLE XII
S IZES (N) OF VS C OVERING A RRAYS VCA(N; 3, 315 , CA)

For both uniform and variable cases of covering arrays,
parameter tuning can enhance CPSO to generate smaller covering arrays than previous work. DPSO can produce the smallest
covering arrays in nearly all cases, and in general the difference of performance between CPSO and DPSO is significant.
DPSO is an effective discrete version of PSO for covering
array generation.
C. PSO Variants
In order to further investigate the effectiveness of DPSO for
covering array generation, we implement the original versions
of four representative variants of PSO and extend them to their
discrete versions based on DPSO. We apply them to generate

the same cases for covering arrays. The mean sizes obtained
are shown in the last four columns in Tables VIIIâ€“XIII.
We first compare the mean sizes of original PSO variants
with those of CPSO and DPSO. In our experiments, TVACâ€™s
mean sizes are always larger than CPSOâ€™s. The linear adjustment of inertia weight and acceleration constant is not helpful
for CPSO for covering array generation. Typically, APSOâ€™s
mean sizes are also larger than CPSOâ€™s. Because APSO uses
a fuzzy system to classify different evolutionary states, its
ineffectiveness may result from inappropriate parameter settings. CLPSO and DMS-PSO can outperform CPSO typically.
Although on occasion they achieve comparable performance
with DPSO, they cannot perform as well as DPSO in most

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

589

TABLE XIII
S IZES (N) OF VS C OVERING A RRAYS VCA(N; 2, 43 53 62 , CA)

TABLE XIV
C OMPARING CPSO AND DPSO W ITH GA AND ACO

cases. That further demonstrates the effectiveness of DPSO
for covering array generation. Because these four algorithms
are representative PSO variants (as shown in Table IV), designing different neighborhood topologies and using multiswarm
techniques may have potential to improve CPSO for covering
array generation.
We next compare the original and discrete versions of each
variant. Except for CLPSO, the other three variants can be
improved using their discrete versions, and the improvement
is also significant. TVAC cannot outperform CPSO, but it
is enhanced by DPSO so that D-TVAC can produce smaller
mean sizes than CPSO in most cases. The linear adjustment
is helpful for the discrete version. For CLPSO, only in a few
cases is it improved by DPSO. Sometimes D-CLPSO even
leads to worse results (see Table X), due primarily to the
weakened search ability of its discrete version as explained in
Section VI. For APSO, DPSO can enhance its original version,
but D-APSO is still worse than DPSO. That may result from
inappropriate settings as explained before. For DMS-PSO,
sometimes DPSO does not enhance it (see Table XI).
However, in most cases D-DMS-PSO can outperform
DMS-PSO and has comparable performance with D-TVAC.
The multiswarm strategy is also helpful for the discrete
version.

In summary, the comparison study reveals that our suggested parameter settings are more suitable for covering array
generation. DPSO is an effective discrete version of PSO.
It can significantly outperform previous results and CPSO
in nearly all cases for uniform and VS covering arrays.
Furthermore, DPSOâ€™s representation scheme of a particleâ€™s
velocity and auxiliary strategies not only enhance CPSO, but
also typically enhance PSO variants. DPSO is a promising
improvement on PSO for covering array generation.
VII. C OMPARING DPSO W ITH GA AND ACO
Because GAs and ACO [8]â€“[12] have also been successfully
used for covering array generation, we compare CPSO and
DPSO with the reported array sizes in [9] and [11]. There are
no widely accepted benchmarks for the comparison of searchbased covering array generation, so we only consider these
two representative and competitive works.
Shiba et al. [9] applied both GA and ACO to generate
uniform covering arrays (CA1 to CA8 in Table XIV), and
Chen et al. [11] applied ACO to generate VS covering arrays
(VCA9 to VCA12 in Table XIV). They both set algorithm
parameters according to recommendations in related research
fields without parameter tuning, while our CPSO and DPSO

590

IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION, VOL. 19, NO. 4, AUGUST 2015

use the recommended settings shown in Table VII. To compare
these algorithms under the same number of fitness evaluations, for each case of covering array, the population size and
the number of iterations of CPSO and DPSO are modified
accordingly to satisfy the settings in [9] and [11]. Moreover,
Shiba et al. [9] and Chen et al. [11] both applied test minimization algorithms to further reduce the size of generated
covering arrays, while our CPSO and DPSO do not apply any
minimization algorithms.
Table XIV shows the comparison results, where boldface
numbers indicate the best array sizes obtained, and â€œâ€“â€ represents that the corresponding data is not available. The
generation of each case of covering array of CPSO and DPSO
is executed 30 times and the best and average results are
presented. Because we do not implement GA and ACO for
covering array generation, no statistical tests can be conducted
here. In addition, because the platforms used for collecting the
results differ, the comparison of computational time would not
be informative. We nevertheless present the execution times
of our CPSO and DPSO, which can serve as references for
practitioners.
From Table XIV, DPSO can outperform existing GA and
ACO for covering array generation, despite the latter two
applying test minimization algorithms. Because our DPSO is
a version of PSO designed and tuned for covering array generation, it suggests that tuned versions of GAs and ACOs may
be a promising area for further study. In addition, CPSO performs worse than GA and ACO in seven of 12 cases. That
may result from the improvement by minimization algorithms
in [9] and [11]. But for CA2 , CA3 , and CA6 , CPSO can still
achieve smaller covering arrays.
In summary, the results further demonstrate that DPSO is
an effective discrete version of PSO for covering array generation. Further investigations of tuned versions of GA and ACO
should be considered.
VIII. C ONCLUSION
Covering array generation is a key issue in CT. We developed a new DPSO for covering array generation, by adapting
S-PSO to generate covering arrays and incorporating two
auxiliary strategies to improve the performance. Parameter tuning was applied to both CPSO and DPSO to identify their
best parameter settings. The original and discrete versions of
four representative PSO variants were implemented and their
efficacy for covering array generation was compared. The performance of DPSO was also compared with other existing
evolutionary algorithms, GA and ACO.
DPSO can perform better than CPSO with fewer iterations, and the performance of CPSO and DPSO is significantly
impacted by their parameter settings. Different cases require
different parameter settings; there may not exist a single choice
that leads to the best results. After parameter tuning, we identified two recommended settings, which lead to relatively good
performance for CPSO and DPSO. Indeed, CPSO with our recommended parameter settings improves on previously reported
results, and our DPSO with recommended parameter settings usually beats CPSO. Typically, DPSO also significantly

enhances the performance of PSO variants. In addition, DPSO
often outperforms GA and ACO to generate covering arrays.
Consequently, DPSO is a promising improvement of PSO for
covering array generation.
Improvements on the methods here may be possible in
a number of ways. One would be to investigate further
evolution procedures and strategies proposed in PSO, such
as hybridizing with penalty approaches to handle discrete
unknowns [39], and compare the results with some exact
schemes like branch and bound method. A second would be
to examine one-column-at-a-time approaches or methods that
construct the entire array, rather than the one-row-at-a-time
approach adopted here. A third would be to incorporate
DPSO with other methods, in particular with test minimization
methods.
R EFERENCES
[1] C. Nie and H. Leung, â€œA survey of combinatorial testing,â€ ACM Comput.
Surv., vol. 43, no. 2, pp. 11.1â€“11.29, 2011.
[2] D. Kuhn and M. Reilly, â€œAn investigation of the applicability of
design of experiments to software testing,â€ in Proc. 27th Annu. NASA
Goddard/IEEE Softw. Eng. Workshop, Greenbelt, MD, USA, 2002,
pp. 91â€“95.
[3] M. Cohen, P. Gibbons, W. Mugridge, and C. Colbourn, â€œConstructing
test suites for interaction testing,â€ in Proc. 25th Int. Conf. Softw. Eng.,
Portland, OR, USA, 2003, pp. 38â€“48.
[4] M. B. Cohen, C. J. Colbourn, and A. C. Ling, â€œConstructing strength
three covering arrays with augmented annealing,â€ Discrete Math.,
vol. 308, no. 13, pp. 2709â€“2722, 2008.
[5] J. Torres-Jimenez and E. Rodriguez-Tello, â€œSimulated annealing for constructing binary covering arrays of variable strength,â€ in Proc. Congr.
Evol. Comput., Barcelona, Spain, Jul. 2010, pp. 1â€“8.
[6] B. Garvin, M. Cohen, and M. Dwyer, â€œEvaluating improvements to a
meta-heuristic search for constrained interaction testing,â€ Empir. Softw.
Eng., vol. 16, no. 1, pp. 61â€“102, 2011.
[7] J. Torres-Jimenez and E. Rodriguez-Tello, â€œNew bounds for binary
covering arrays using simulated annealing,â€ Inf. Sci., vol. 185, no. 1,
pp. 137â€“152, 2012.
[8] S. Ghazi and M. Ahmed, â€œPair-wise test coverage using genetic algorithms,â€ in Proc. Congr. Evol. Comput., vol. 2. Canberra, ACT, Australia,
2003, pp. 1420â€“1424.
[9] T. Shiba, T. Tsuchiya, and T. Kikuno, â€œUsing artificial life techniques to
generate test cases for combinatorial testing,â€ in Proc. 28th Annu. Int.
Comput. Softw. Appl. Conf., vol. 1. Hong Kong, 2004, pp. 72â€“77.
[10] J. McCaffrey, â€œAn empirical study of pairwise test set generation using
a genetic algorithm,â€ in Proc. 7th Int. Conf. Inf. Technol. New Gener.,
Las Vegas, NV, USA, 2010, pp. 992â€“997.
[11] X. Chen, Q. Gu, A. Li, and D. Chen, â€œVariable strength interaction
testing with an ant colony system approach,â€ in Proc. Asia-Pacific Softw.
Eng. Conf., Penang, Malaysia, 2009, pp. 160â€“167.
[12] X. Chen, Q. Gu, X. Zhang, and D. Chen, â€œBuilding prioritized pairwise
interaction test suites with ant colony optimization,â€ in Proc. 9th Int.
Conf. Qual. Softw., Jeju-do, Korea, 2009, pp. 347â€“352.
[13] X. Chen, Q. Gu, J. Qi, and D. Chen, â€œApplying particle swarm optimization to pairwise testing,â€ in Proc. 34th Annu. Comput. Softw. Appl.
Conf., Seoul, Korea, 2010, pp. 107â€“116.
[14] B. S. Ahmed and K. Z. Zamli, â€œPSTG: A T-way strategy adopting particle swarm optimization,â€ in Proc. 4th Asia Int. Conf. Math. Anal. Model.
Comput. Simulat., Kota Kinabalu, Malaysia, 2010, pp. 1â€“5.
[15] B. S. Ahmed and K. Z. Zamli, â€œA variable strength interaction test suites
generation strategy using particle swarm optimization,â€ J. Syst. Softw.,
vol. 84, no. 12, pp. 2171â€“2185, 2011.
[16] B. S. Ahmed, K. Z. Zamli, and C. P. Lim, â€œApplication of particle
swarm optimization to uniform and variable strength covering array
construction,â€ Appl. Soft Comput., vol. 12, no. 4, pp. 1330â€“1347, 2012.
[17] Y. del Valle, G. Venayagamoorthy, S. Mohagheghi, J.-C. Hernandez, and
R. Harley, â€œParticle swarm optimization: Basic concepts, variants and
applications in power systems,â€ IEEE Trans. Evol. Comput., vol. 12,
no. 2, pp. 171â€“195, Apr. 2008.

WU et al.: A DISCRETE PARTICLE SWARM OPTIMIZATION FOR COVERING ARRAY GENERATION

[18] W.-N. Chen et al., â€œA novel set-based particle swarm optimization
method for discrete optimization problems,â€ IEEE Trans. Evol. Comput.,
vol. 14, no. 2, pp. 278â€“300, Apr. 2010.
[19] M. Clerc, Discrete Particle Swarm Optimization (New Optimization
Techniques in Engineering). New York, NY, USA: Springer, 2004.
[20] W. Pang et al., â€œModified particle swarm optimization based on space
transformation for solving traveling salesman problem,â€ in Proc. Int.
Conf. Mach. Learn. Cybern., vol. 4. Shanghai, China, Aug. 2004,
pp. 2342â€“2346.
[21] W. Pang, K.-P. Wang, C.-G. Zhou, and L.-J. Dong, â€œFuzzy discrete particle swarm optimization for solving traveling salesman problem,â€ in Proc.
4th Int. Conf. Comput. Inf. Technol., Wuhan, China, 2004, pp. 796â€“800.
[22] Y. Wang et al., â€œA novel quantum swarm evolutionary algorithm and its
applications,â€ Neurocomputing, vol. 70, nos. 4â€“6, pp. 633â€“640, 2007.
[23] E. Campana, G. Fasano, and A. Pinto, â€œDynamic analysis for the
selection of parameters and initial population, in particle swarm optimization,â€ J. Global Optim., vol. 48, no. 3, pp. 347â€“397, 2010.
[24] A. Ratnaweera, S. Halgamuge, and H. Watson, â€œSelf-organizing hierarchical particle swarm optimizer with time-varying acceleration coefficients,â€ IEEE Trans. Evol. Comput., vol. 8, no. 3, pp. 240â€“255,
Jun. 2004.
[25] J. Liang, A. Qin, P. Suganthan, and S. Baskar, â€œComprehensive learning particle swarm optimizer for global optimization of multimodal
functions,â€ IEEE Trans. Evol. Comput., vol. 10, no. 3, pp. 281â€“295,
Jun. 2006.
[26] Z.-H. Zhan, J. Zhang, Y. Li, and H.-H. Chung, â€œAdaptive particle swarm
optimization,â€ IEEE Trans. Syst., Man, Cybern., B, Cybern., vol. 39,
no. 6, pp. 1362â€“1381, Dec. 2009.
[27] J. Liang and P. Suganthan, â€œDynamic multi-swarm particle swarm optimizer,â€ in Proc. IEEE Swarm Intell. Symp., Pasadena, CA, USA, 2005,
pp. 124â€“129.
[28] D. Cohen, S. Dalal, M. Fredman, and G. Patton, â€œThe AETG system:
An approach to testing based on combinatorial design,â€ IEEE Trans.
Softw. Eng., vol. 23, no. 7, pp. 437â€“444, Jul. 1997.
[29] R. C. Bryce and C. J. Colbourn, â€œOne-test-at-a-time heuristic search for
interaction test suites,â€ in Proc. 9th Annu. Conf. Genet. Evol. Comput.,
London, U.K., 2007, pp. 1082â€“1089.
[30] J. Kennedy and R. Eberhart, â€œParticle swarm optimization,â€ in Proc. Int.
Conf. Neural Netw., vol. 4. Perth, WA, Australia, 1995, pp. 1942â€“1948.
[31] R. Eberhart and Y. Shi, â€œParticle swarm optimization: Developments,
applications and resources,â€ in Proc. Congr. Evol. Comput., vol. 1. Seoul,
Korea, 2001, pp. 81â€“86.
[32] R. Poli, J. Kennedy, and T. Blackwell, â€œParticle swarm optimization,â€
Swarm Intell., vol. 1, no. 1, pp. 33â€“57, 2007.
[33] S. Helwig, J. Branke, and S. Mostaghim, â€œExperimental analysis of
bound handling techniques in particle swarm optimization,â€ IEEE Trans.
Evol. Comput., vol. 17, no. 2, pp. 259â€“271, Apr. 2013.
[34] A. Windisch, S. Wappler, and J. Wegener, â€œApplying particle swarm
optimization to software testing,â€ in Proc. 9th Annu. Conf. Genet. Evol.
Comput., London, U.K., 2007, pp. 1121â€“1128.
[35] A. Ganjali, â€œA requirements-based partition testing framework using particle swarm optimization technique,â€ M.S. thesis, Dept. Electr. Comput.
Eng., Univ. Waterloo, Waterloo, ON, Canada, 2008.
[36] Y.-J. Gong et al., â€œOptimizing the vehicle routing problem with time
windows: A discrete particle swarm optimization approach,â€ IEEE
Trans. Syst., Man, Cybern. C, Appl. Rev., vol. 42, no. 2, pp. 254â€“267,
Mar. 2012.
[37] W.-N. Chen et al., â€œParticle swarm optimization with an aging leader and
challengers,â€ IEEE Trans. Evol. Comput., vol. 17, no. 2, pp. 241â€“258,
Apr. 2013.
[38] C. J. Colbourn. (Apr. 2013). Covering Array Tables for t=2, 3,
4, 5, 6. [Online]. Available: http://www.public.asu.edu/ ccolbou/src/
tabby/catable.html
[39] M. Corazza, G. Fasano, and R. Gusso, â€œParticle swarm optimization
with non-smooth penalty reformulation, for a complex portfolio selection
problem,â€ Appl. Math. Comput., vol. 224, pp. 611â€“624, Nov. 2013.

591

Huayao Wu received the B.S degree from Southeast
University, Nanjing, China and the M.S degree from
Nanjing University, Nanjing, China, where he is
currently working toward the Ph.D. degree from
Nanjing University, Nanjing.
His research interests include software testing,
especially on combinatorial testing and search-based
software testing.

Changhai Nie (Mâ€™12) received the B.S. and M.S.
degrees in mathematics from Harbin Institute of
Technology, Harbin, China, and the Ph.D. degree
in computer science from Southeast University,
Nanjing, China.
He is a Professor of Software Engineering
with State Key Laboratory for Novel Software
Technology, Department of Computer Science
and Technology, Nanjing University, Nanjing. His
research interests include software analysis, testing
and debugging.

Fei-Ching Kuo (Mâ€™06) received the B.Sc. (Hons.)
degree in computer science and the Ph.D. degree in
software engineering from Swinburne University of
Technology, Hawthorn, VIC, Australia.
She was a Lecturer with University of
Wollongong, Wollongong, NSW, Australia. She is
currently a Senior Lecturer with the Swinburne
University of Technology. Her research interests
include software analysis, testing, and debugging.

Hareton Leung (Mâ€™90) received the Ph.D. degree
in computer science from University of Alberta,
Edmonton, AB, Canada.
He is an Associate Professor and a Director
of the Laboratory for Software Development
and Management, Department of Computing,
Hong Kong Polytechnic University, Hong Kong. His
research interests include software testing, project
management, risk management, quality and process
improvement, and software metrics.

Charles J. Colbourn received the Ph.D. degree
from University of Toronto, Toronto, ON, Canada,
in 1980.
He is a Professor of Computer Science and
Engineering with Arizona State University, Tempe,
AZ, USA. He has authored the books The
Combinatorics of Network Reliability (Oxford) and
Triple Systems (Oxford), and also 320 refereed journal papers focussing on combinatorial designs and
graphs with applications in networking, computing,
and communications.
Prof. Colbourn received the Euler Medal for Lifetime Research
Achievement by the Institute for Combinatorics and its Applications in 2004.

Gao SW, Lv JH, Du BL et al. Balancing frequencies and fault detection in the in-parameter-order algorithm. JOURNAL
OF COMPUTER SCIENCE AND TECHNOLOGY 30(5): 957â€“968 Sept. 2015. DOI 10.1007/s11390-015-1574-6

Balancing Frequencies and Fault Detection in the In-Parameter-Order
Algorithm

pÂ­Â•
ÃªÂ­9

Â½s), Bing-Lei Du (ÃšX[), Charles J. Colbourn

Shi-Wei Gao 1 (
), Jiang-Hua Lv 1,âˆ— (
1
)
and Shi-Long Ma (
1
2

1

2

State Key Laboratory of Software Development Environment, Beihang University, Beijing 100191, China
School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe
AZ 85287-8809, U.S.A.

E-mail: ge89@163.com; jhlv@nlsde.buaa.edu.cn; binglei.du@gmail.com; Charles.Colbourn@asu.edu
E-mail: slma@nlsde.buaa.edu.cn
Received March 16, 2015; revised June 29, 2015.
Abstract The In-Parameter-Order (IPO) algorithm is a widely used strategy for the construction of software test suites
for combinatorial testing (CT) whose goal is to reveal faults triggered by interactions among parameters. Variants of IPO
have been shown to produce test suites within reasonable amounts of time that are often not much larger than the smallest
test suites known. When an entire test suite is executed, all faults that arise from t-way interactions for some fixed t are
surely found. However, when tests are executed one at a time, it is desirable to detect a fault as early as possible so that
it can be repaired. The basic IPO strategies of horizontal and vertical growth address test suite size, but not the early
detection of faults. In this paper, the growth strategies in IPO are modified to attempt to evenly distribute the values of
each parameter across the tests. Together with a reordering strategy that we add, this modification to IPO improves the
rate of fault detection dramatically (improved by 31% on average). Moreover, our modifications always reduce generation
time (2 times faster on average) and in some cases also reduce test suite size.
Keywords

1

combinatorial testing, IPO, test suite generation, expected time to fault detection, software under test

Introduction

Modern software systems are highly configurable.
Their behavior is controlled by many parameters. Interactions among these parameters may cause severe
failures, resulting in poor reliability. Therefore, software testing and reliability assessment are crucial in
the design of effective software, as discussed in [1-3] for
reliability and in [4-15] for software testing. Software
testing serves two main purposes: 1) to ensure that
software has as few errors as possible prior to release,
and 2) to detect and isolate faults in the software. A
generic model of such a software system identifies a finite set of parameters, and a finite set of possible values

for each parameter. Faults may arise due to a choice
of a value for a single parameter, interactions among
the values of a subset of the parameters, or a result
of environmental conditions not included in the software model. We focus on the faults that arise from
the parameters identified and the interactions among
them. It is nearly always impractical to exhaustively
test all combinations of parameter values because of resource constraints. Fortunately, this is not necessary
in general: in some real software systems, more than
70 percent of faults are caused by interactions between
two parameters[16], and all known faults are caused by
interactions among six or fewer parameters[17-18] .

Regular Paper
Special Section on Software Systems
This work was supported by the National Natural Science Foundation of China under Grant Nos. 61300007 and 61305054, the
Fundamental Research Funds for the Central Universities of China under Grant Nos. YWF-15-GJSYS-106 and YWF-14-JSJXY-007,
and the Project of the State Key Laboratory of Software Development Environment of China under Grant Nos. SKLSDE-2015ZX-09
and SKLSDE-2014ZX-06.
âˆ— Corresponding Author
Â©2015 Springer Science + Business Media, LLC & Science Press, China

958

For these reasons, combinatorial testing (CT) or tway testing chooses a strength t (the largest number of
parameters interacting to cause a fault), and forms a
software interaction test suite as follows. Every row of
the test suite is a test or a test case. For each parameter
in the system, each test specifies an admissible value for
the parameter. The defining property is that, no matter
how one chooses t parameters and an admissible value
for each (a t-way interaction), at least one test has the
specified parameters set to the indicated values. This
coverage property ensures that every possible interaction among t or fewer parameter values must arise in
at least one of the test cases. CT has proved to be an
efficient testing technique for software[6,9,19]. Indeed,
empirical studies have shown that t-way testing can effectively detect faults in various applications[17-18,20-22] .
A primary objective in producing a test suite is to
minimize the cost of executing the tests; hence minimizing the number of tests is desired. At the same
time, however, the time to produce the test suite is
also crucial. Hence the most effort has been invested
in finding a variety of test suite generation algorithms.
Some invest additional computational resources in minimizing the size of the test suite, while others focus
on fast generation methods for test suites of acceptable but not minimum size. General methods providing fast generation have primarily involved greedy
algorithms[9]. One-test-at-a-time methods start with
an empty test suite, and keep track of the as-yetuncovered t-way interactions. Then repeatedly a test
is selected, which attempts to maximize the number of
such interactions that are covered by the test, until all
interactions are covered. This strategy was pioneered in
AETG[23] , and later proved to be within a constant factor of the optimal size[24-25] . In practice, maintaining
a list of all t-way interactions can be prohibitive when
the number of parameters is large. One-parameter-ata-time methods instead construct a test suite for t of
the parameters (this contains all of the possible tests).
Then it repeatedly adds a new parameter, and chooses
a value for this parameter in each of the existing tests
(horizontal growth). Because it is possible that some
t-way interactions involving the new parameter have
not been covered yet, further tests are selected to cover
all such interactions (vertical growth). This requires
maintaining a list of (t âˆ’ 1)-way interactions, and hence
can involve less bookkeeping. The pioneering example here is IPO[26] and its extensions, IPOG[27] , and
IPOG-F and IPOG-F2[28], which will be discussed in
more detail in Section 2. Both strategies typically pro-

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

duce test suites of acceptable size[26,29] . It has been observed that one-test-at-a-time methods produce slightly
smaller test suites in general, while one-parameter-ata-time methods are somewhat faster at generation[26].
As mentioned earlier, software interaction test suites
serve as two complementary roles[30]: to verify that no
t-way interaction of SUT (software under test) causes a
fault, or to locate such a fault. These two roles are
different: certifying absence of a fault requires running the whole test suite, while locating a fault may
not. Indeed in [30], it is shown that minimum test
suite size is not the correct objective for fault location;
the structure of the test suite can be more important
than its size alone. An improved rate of fault detection can provide faster feedback to testers[31] . Recent
studies have shown that CT is an effective fault detection technique and that early fault detection can
be improved by reordering the generated test suites
using interaction-based prioritization approaches[32-34] .
Many strategies have been proposed to guide prioritization using evaluation measures such as interaction
coverage based prioritization[30,35-39] and incremental
interaction coverage based prioritization[40-41] . In [30],
an evaluation measure of the expected time to fault detection is given.
Test case prioritization techniques have been explored for the one-test-at-a-time methods, but little is known for the one-parameter-at-a-time methods.
Bryce et al.[35-36,42] presented techniques
that combine generation and prioritization. Pure
prioritization[32-34,39] instead reorders an existing interaction test suite, using the metric of normalized average
percentage of faults detected (NAPFD). However, existing pure prioritization techniques use explicit fault
measurements of real systems, and hence are not directly suitable for the IPO algorithm.
The main contributions of our work are:
1) We modify the IPO algorithm in order to accelerate the method and make it effective for fault detection. Our modifications attempt to make the values of
each parameter more evenly distributed during generation. We focus on choosing values for the extension to
an additional parameter during the horizontal growth
of the algorithm and filling values for donâ€™t care positions. (See Section 3.)
2) We develop a pure prioritization technique (a reordering strategy) for the IPO algorithm based on the
evaluation measure presented in [30]. Our method can
reduce the expected time to fault detection effectively.
(See Section 4.)

959

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO

vertical growth. During both horizontal and vertical
growth, it frequently happens that the value for one
or more parameters in a row can be chosen arbitrarily
without affecting the coverage of the row. Such entries
are donâ€™t care positions[26] in the test suite. The IPO
methods exploit the fact that selecting values for donâ€™t
care positions can be deferred; then they can be filled
during horizontal growth when the next parameter is
introduced. Every variant of IPO must therefore deal
with two basic problems:
â€¢ choose values for the new parameter to maximize
the number of uncovered interactions covered during
horizontal growth;
â€¢ assign values for donâ€™t care positions that arise.
In the next section, we explore an implementation
of this IPO framework in which the objective is not
just to ensure coverage, but also to attempt to make
each value appear as equally often as possible for each
parameter. The latter is a balance condition.

3) We conduct experiments to demonstrate the effectiveness of the modifications (see Section 5). We
conclude that the modifications to the IPO strategy result in faster generation (2 times faster on average according to the experimental results in Subsection 5.1),
sometimes in smaller test suites, and together with the
pure prioritization, in less time to detect the first fault
(improved by 31% on average according to the experimental results in Subsection 5.2).
2

Framework of the IPO Algorithm

IPO comprises a family of methods of the oneparameter-at-a-time type. We focus on IPOG as a representative implementation. The basic operation is to
add a new parameter to an existing interaction test
suite of strength t. To initialize the method, whenever
the number of parameters is at most t, all possible rows
are included, which is necessary and sufficient to obtain
a test suite.
Thereafter, to introduce a new parameter, the set Ï€
of all t-way interactions involving the new parameter is
computed. Horizontal growth adds a value of the new
parameter to each existing row so that this extended
row covers the most interactions in Ï€; the interactions
covered are removed from Ï€. Then if Ï€ still contains
uncovered interactions, vertical growth adds new rows
to cover them. This process is outlined in the flowchart
in Fig.1.
Existing variants of the IPO strategy alter the selection of values for the new parameter during horizontal growth and the selection of additional rows during

3

Balance in the IPO Algorithm

A test suite must cover all t-way interactions. Consider a specific parameter and the t-way interactions
that contain it. For each value of the parameter, the
numbers of these t-way interactions with each different
value of the parameter are the same. Now consider the
frequencies of values of the parameter within the tests
of a test suite. Because each value must provide the
coverage of the same number of interactions, it appears
to be reasonable to attempt to make the frequencies

START

Create Set Ï€ of Uncovered t-Way
Combinations of Values Involving
the Next Parameter

Build a t-Way Test Set
for the First t Parameters

Horizontal Growth
(Remove the Covered
Combinatons from Ï€)

Yes
Ï€ Is Empty?

All the Parameters
Are Included in the Set?

No
Vertical Growth
(Remove the Covered
Combinations from Ï€)

Fig.1. Flowchart of IPOG algorithm.

Yes
END

No

960

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

close to equal. The same argument applies to fault detection.
Two issues arise. First, current IPO algorithms do
not make any explicit effort to balance the frequencies
of values. Second, it is not at all clear how such an objective might affect the sizes of test suites produced, or
the time to generate them, or their rates of fault detection. In this section, we develop modifications of IPO
to address frequencies of values. Subsequent sections
treat their impacts.

While shown in Algorithm 1 for IPOG, this simple
strategy can also be used in IPOG-F and IPOG-F2. We
show the modification for IPOG-F. The IPOG-F algorithm greedily selects over both the row and the value
with which the covering array is extended, and the extended row/value pair (i; a) is greedily selected by the
following formula[28] :

3.1

where n is the number of parameters, Tc [i; a] denotes
the t-tuples that have previously been covered by already extended rows, and tn denotes the number of new
t-tuples the row/value pair would cover if we extend row
i with value a. The metric of optimal selection for the
extended row (i; a) is that the extended row (i; a) would
maximize tn .
The original pseudo-code for horizontal growth in
IPOG-F is shown in Algorithm 2. The modification
replaces line 6 to line 10 of Algorithm 2 as shown in
Algorithm 3. Similar modifications can be applied to
IPOG-F2.

Choosing a New Parameterâ€™s Values

During horizontal growth, the IPOG algorithm
chooses to add a value of the new parameter to cover
the greatest number of interactions in Ï€. In many situations, more than one value achieves this goal, and we
must choose one. A naive strategy treats the values as
ordered, and selects the smallest value that covers the
most interactions in Ï€. This introduces a bias towards
the smaller values of each parameter, sometimes resulting in smaller values appearing much more frequently
than larger ones.
Here a different strategy, shown in Algorithm 1, is
proposed. The essential change is to treat the values
as being cyclically ordered, recording the value selected
for the previous row. Then possible values for this row
are considered by starting from the value following the
previous one selected. For this modification, vertical
growth remains unchanged.
Algorithm 1. Modified Horizontal Growth
1. Cov[r; v] is the number of interactions that
the extended row (r; v) covers
2. q â† |P |
3. prev â† q
4. for each row r in the covering array ca do
5.
max â† (prev + 1) mod q
6.
j â† (max + 1) mod q
7.
while j 6= ((prev + 1) mod q) do
8.
if Cov[r, vj ] > Cov[r, vmax ] then
9.
max â† j
10.
end if
11.
j â† (j + 1) mod q
12.
end while
13.
r â† (r, vmax )
14.
prev â† max
15. end for

Algorithm 1 incurs additional time to track the previous value selected, but this small addition is dominated by the computation of coverage, and hence makes
no change in the complexity of the method.



nâˆ’1
tn =
âˆ’ Tc [i; a],
tâˆ’1

Algorithm 2. Horizontal Growth of IPOG-F
1. Tc [r; a] is the number of t-tuples covered by (r; a)
2. Cov[Î›, v] is true if the interaction with column
tuple Î› and value tuple v is covered
false otherwise
3. Tc [i; a] â† 0, âˆ€i, a
4. Cov[Î›, v] â† false, âˆ€Î›, a
5. while some row is non-extended do
6.
Find non-extended row i and value a

7.
so that tn = kâˆ’1
âˆ’ Tc [i; a] is maximum
tâˆ’1
8.
if tn = 0 then
9.
Stop horizontal growth
10.
end if
11.
Extend row i with value a
12.
for all non-extended row j do
13.
S â† set of columns where rows i and j
have identical entries
14.
for all column tuples Î› âŠ‚ S do
15.
v â† the value tuple in row i and
column tuple Î›
16.
if Cov[Î›, v] = false then
17.
Tc [j; a] â† Tc [j; a] + 1
18.
end if
19.
end for
20.
end for
21.
for all column tuples Î› do
22.
v â† the value tuple in row r and
column tuple Î›
23.
if Cov[Î›, v] = false then
24.
Cov[Î›, v] â† true
25.
end if
26.
end for
27. end while

961

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO
Algorithm 3. Modification (Lines 6âˆ¼10)
1. max â† (prev + 1) mod q
2. j â† (max + 1) mod q
3. while j 6= ((prev + 1) mod q) do
4.
if Tc [i; vj ] < Tc [i, vmax ] then
5.
max â† j
6.
j â† (j + 1) mod q
7.
end if
8. end while
9. a â† vmax

10. tn â† kâˆ’1
âˆ’ Tc [Ï„, a]
tâˆ’1
11. if tn = 0 then
12.
Stop horizontal growth
13. end if
14. Extend row i with value a
15. prev â† max

3.2

Addressing donâ€™t care Positions

In horizontal growth, when the maximum number
of interactions that the extended row (r; v) can cover is
0, the value at this position is a donâ€™t care. The donâ€™t
care positions can be addressed using the method of
Subsection 3.1.
In vertical growth, new rows that are created to
cover the t-way combinations in Ï€ not covered by horizontal growth can leave positions not needed to cover
interactions in Ï€ as donâ€™t care. The selection of these
values can influence the extension for the remaining parameters. To exploit these donâ€™t care positions, one
strategy focuses on coverage, and the other on balance.
The balance strategy attempts to make values of
all parameters distributed evenly: as each donâ€™t care
arises, it is filled with a value for this parameter that
currently appears the least often; ties are handled by
taking the next in the cyclic order of values after the
previous selection.
The coverage strategy is greedy. Donâ€™t care positions produced in vertical growth are left unassigned
until the next horizontal growth. Then a value is chosen
so that the row covers the most uncovered interactions,
using the method described in Subsection 3.1.
Focusing on coverage is generally slightly superior in
reducing the size of test suites. However, the balance
strategy reduces the time to generate the test suite.
Because of our interest in fault detection, and the fact
that existing IPO variants use a coverage strategy, we
adopt the balance strategy here. The pseudo-code for
the balance strategy is shown in Algorithm 4.
Vertical growth treating donâ€™t care positions using a
coverage strategy examines all t-way interactions, while

Algorithm 4. Addressing donâ€™t care Positions
1. Number the values of Pi as v1 , v2 , . . . , v|Pi |
2. f req[Pi , j] is the frequency of value vj of Pi
appears in the existing test set
3. e is an entry in column i
4. if e is a donâ€™t care position then
5.
Find min that f req[Pi , min] is minimum
in f req[Pi , 1], . . . , f req[Pi , |Pi |]
6.
Assign e with vmin
7. end if

our balance strategy only examines frequencies. Savings are only incurred with the balance strategy when
donâ€™t care positions arise during vertical growth. In
both cases, the worst-case complexity is dominated by
the cost of horizontal growth, so in principle the two
methods have the same asymptotic complexity. However, in practice, every donâ€™t care position results in a
saving in computation time for the balance strategy.
4

Reducing the Expected Time to Fault
Detection

In [30], a measurement of the goodness of a test suite
at detecting a fault is defined. Suppose that every test
takes the same time to run. Further suppose that faults
are randomly distributed among the t-way interactions,
and that there is no a priori information about their location. For a system with s faults, the expected time to
fault detection is determined by the expected number
of tests to detect the presence of a fault. Î¦s denotes
the expected number of tests to detect the first fault in
a system with s faults.
PN ui 
Î¦s =

i=1 s

Î›
s

.

Here ui is the number of uncovered interactions before executing the i-th row, N is the number of rows
of the test suite, and Î› is the total number of t-way
interactions.
This measure applies to any test suite when faults
arise randomly, and is not intended to examine particular patterns of faults in specific systems. As such, it can
serve as a means to evaluate test suites for use in an
as-yet-unknown application.
Minimizing the expected time to fault detection
means constructing a test suite to minimize Î¦s given
s. Rather than constructing a test suite to minimize
Î¦s directly, we can reorder the rows of a test suite to
reduce Î¦s .
Because all faults of interest are caused by parameter interactions, the more uncovered interactions con-

962

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

tained in the test, the more likely a fault is to be revealed. Hence placing the tests that cover the greatest
number of the uncovered interactions early can increase
the probability of detecting a fault. To see this, we
rewrite the formula as follows:

PN ui 
N
ui
X
i=1 s
s
Î¦s =

=
.
Î›
Î›
s

i=1

s

Then the problem becomes minimizing the average
(ui )
value of Î›s , the likelihood that all faults remain unde(s)
tected after running i tests. The method for reordering
the test suite is Algorithm 5. There may be a tie for
row rj where Tc [rj ] is the largest â€” if there is, the tie
would be broken randomly.
Algorithm 5. Reordering Test Suites
1. n â† N
2. for j from 1 to n do
3.
4.

for each row r1 , . . . , rn
Determine the number Tc [ri ] of t-way
interactions covered in ri
but not covered in r1 , . . . , riâˆ’1

5.

end for

6.

Choose a row rj from ri , . . . , rn for which

7.

if Tc [rj ] = 0 then

Tc [rj ] is the largest
8.
9.
10.

Remove all rows ri , . . . , rn from the suite
n â†iâˆ’1
else

11.
12.

Swap ri and rj in the suite
end if

13. end for

5

Experiments

We employ the tool ACTS-2.8 (Advanced Combinatorial Testing System)[43] , including implementations
of IPOG, IPOG-F and IPOG-F2, etc. We compare the
tool ACTS-2.8 with our variants of IPOG, IPOG-F and
IPOG-F2 in which the handling of donâ€™t care positions
attempts to balance frequencies of values; our versions
are coded in C++. All of the experimental results reported here are performed on a laptop with CoreTM 2
Duo Intelr processor clocked at 2.60 GHz and 4 GB
memory.
5.1

Test Suite Size and Execution Time

First we examine the relative performance for different numbers of values for the parameters. The notation dt indicates that there are t parameters, each with

d values. To start, we vary the number of values. Table 1 shows execution time and test suite sizes when
the strength is 4, and there are five parameters whose
number of values is 5, 10, 15, or 20. As expected, the
execution time for our methods is substantially smaller
(see Fig.2). What is more surprising is that our methods consistently produce test suites no larger than the
original methods, and sometimes produce much smaller
ones.
Now we vary the number of parameters. Table 2
shows results when the strength is 4, the number of parameters is 10, 15, 20, or 25, and the number of values
is 5. Again the execution time for our methods shows
improvements (see Fig.3). However, as the number of
parameters increases, the deferral in filling donâ€™t care
positions by the original methods generally produces
smaller test suite sizes.
Now we vary the strength. Table 3 presents results for 106 when the strength is 2, 3, 4, or 5. Once
again, the execution time for our methods is substantially lower (see Fig.4). Our methods do not fare as
well with respect to test suite size, but appear to be
very effective when the strength is larger.
Our methods appear to improve execution time consistently as expected. Nevertheless, they also improve
on test suite sizes in some cases, especially when the
strength is large or the number of values is large. Real
systems rarely have the same number of values for each
parameter, so we also consider situations in which different parameters can have different numbers of values.
Table 4 presents results with strength 4 for five
different sets of numbers of values for 10 parameters.
Execution time improvements again arise for our algorithms. Moreover, a pattern for test suite sizes is clear:
our methods improve when there is more variation in
numbers of values.
Next we examine the relative performance using
the Traffic Collision Avoidance System (TCAS), which
has been utilized in several other studies of software
testing[27,44-46] . TCAS has 12 parameters: seven parameters have two values, two parameters have three
values, one parameter has four values, and two parameters have 10 values. Table 5 gives the results. (In [46],
similar results for the original IPOG versions are given
for the TCAS system.) While our improvements in execution time are evident, no obvious pattern indicates
which method produces the smallest test suite.
Our methods have simplified the manner in which
donâ€™t care positions are treated in order to balance the
frequencies of values. Our experimental results all con-

963

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO
Table 1. Results for Five Parameters with 5 to 20 Values for 4-Way Testing
Parameter

Our IPOG

IPOG(ACTS)

Our IPOG-F

Size

Size

Time (s)

IPOG-F(ACTS)

55

000 745

0.001

000 790 000.015

000 625 000.000

000625 1 000.047

000 625 000.000

100 788 1 000.031

105

011 990

0.078

012 298 000.827

010 000 000.673

010 000 1 006.109

010 000 000.500

112 394 1 004.859

155

058 410

1.101

061 945 016.329

050 625 018.469

050 625 1 146.730

050 625 012.782

161 615 1 184.450

205

184 680

9.666

191 652 120.220

160 000 200.020

160 000 1 376.000

160 000 209.290

192 082 1 966.200

Time (s)

Our IPOG
IPOG(ACTS)

120

Our IPOG-F
IPOG-F(ACTS)

1200

Time (s)

80
60

800
600

40

400

20

200
5

10
15
Domain Size

0

20

Time (s)

Size

Time (s)

Our IPOG-F2
IPOG-F2(ACTS)

1500

1000

100

Size

2000

1400

140

Time (s)

Size

IPOG-F2(ACTS)

Time (s)

0

Time (s)

Our IPOG-F2

Size

Time (s)

Config.

1000

500

5

10
15
Domain Size

(a)

0

20

5

10
15
Domain Size

20

(c)

(b)

Fig.2. Execution time, varying the number of values (4-way). (a) IPOG. (b) IPOG-F. (c) IPOG-F2.
Table 2. Results for 10 to 25 5-Value Parameters for 4-Way Testing
Parameter

IPOG-F(ACTS)

Our IPOG-F2

IPOG-F2(ACTS)

Config.

Size

Our IPOG
Time (s)

Size

IPOG(ACTS)
Time (s)

Size

Our IPOG-F
Time (s)

Size

Time (s)

Size

Size

510

1 890

0.056

1 859

00.188

1 833

000.625

1 882

001.750

1 965

0.187

1 905

0.297

515

2 584

0.517

2 534

00.954

2 461

007.109

2 454

014.579

2 736

1.282

2 644

1.421

520

3 114

2.140

3 032

04.094

2 951

034.361

2 898

060.987

3 308

4.329

3 180

4.344

525

3 540

7.012

3 434

16.049

3 338

111.150

3 279

176.340

3 763

8.752

3 589

9.188

Time (s)

Time (s)

10
Our IPOG
IPOG(ACTS)

10

5

Our IPOG-F2
IPOG-F2(ACTS)

8
Time (s)

150

Time (s)

Time (s)

15

Our IPOG-F
IPOG-F(ACTS)

100

50

6
4
2

0
10

0

15
20
25
Number of Parameters

10

15
20
25
Number of Parameters

(a)

0

10

15
20
25
Number of Parameters

(b)

(c)

Fig.3. Execution time, increasing the number of parameters (4-way). (a) IPOG. (b) IPOG-F. (c) IPOG-F2.
Table 3. Results for Six 10-Value Parameters for 2âˆ¼5-Way Testing
t

Our IPOG
Size

IPOG(ACTS)

Our IPOG-F

IPOG-F(ACTS)

Our IPOG-F2

IPOG-F2(ACTS)

Time (s)

Size

Time (s)

Size

Time (s)

Size

Time (s)

Size

Time (s)

Size

Time (s)

2 000 149

0.000

000 130

000.005

000 133

00.000

000 134

000.031

000 135

00.000

000 134

000.016

3 001 633

0.010

001 633

000.059

001 577

00.047

001 553

000.266

001 629

00.032

001 625

000.140

4 016 293

0.195

016 496

004.276

015 594

02.704

015 467

018.126

015 631

01.594

016 347

009.297

5 123 060

5.139

130 728

116.470

100 000

88.692

100 000

575.150

100 000

54.971

132 428

449.330

964

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5
Table 4. Results for Five Systems with Different Numbers of Values in 4-Way Testing

Parameter Config.

IPOG(ACTS)

Our IPOG-F

Size

Our IPOG
Time (s)

Size

Time (s)

Size

Time (s)

Size

Time (s)

Size

Time (s)

1010

29 915

1.942

29 466

28.040

28 437

129.57

28 079

359.17

31 744 045.237

30 986

053.440

105 95

23 878

1.295

23 961

14.583

22 521

094.27

22 726

248.04

25 222 031.611

24 741

039.736

153 104 53

41 128

1.734

45 128

13.689

41 505

236.87

43 306

757.68

46 509 162.850

48 295

262.170

161 152 104 52 41

42 913

1.750

47 591

14.532

43 774

249.37

45 693

289.72

48 660 148.510

51 147

149.510

171 161 151 104 51 42 47 248

1.844

52 991

14.860

48 847

235.95

50 287

333.89

54 099 189.290

57 634

199.810

Our IPOG-F2
Size

Time (s)

IPOG-F2(ACTS)

600

120
Our IPOG
IPOG(ACTS)

Our IPOG-F
IPOG-F(ACTS)

500
Time (s)

80
60
40

400
300
200

0

0
2

3
4
Strength of Coverage
(a)

5

300
200
100

100

20

Our IPOG-F2
IPOG-F2(ACTS)

400

Time (s)

100
Time (s)

IPOG-F(ACTS)

2

3
4
Strength of Coverage
(b)

5

0

2

3
4
Strength of Coverage
(c)

5

Fig.4. Execution time, increasing the test strength. (a) IPOG. (b) IPOG-F. (c) IPOG-F2.
Table 5. Results for TCAS
t

IPOG-F(ACTS)

Our IPOG-F2

Size

Our IPOG
Time (s)

Size

IPOG(ACTS)
Time (s)

Size

Our IPOG-F
Time (s)

Size

Time (s)

Size

Time (s)

Size

Time (s)

2

00 100

0.001

00 100

0.002

00 100

00.002

00 100

000.015

00 100

00.004

00 100

00.017

3

00 404

0.009

00 400

0.007

00 400

00.025

00 402

000.087

00 431

00.044

00 438

00.061

4

01 306

0.065

01 359

0.031

01 269

00.323

01 349

001.117

01 639

00.489

01 653

00.572

5

04 464

0.411

04 233

0.219

04 068

04.104

04 245

013.405

05 129

04.133

05 034

04.379

6

11 774

1.463

11 021

3.233

11 381

32.870

11 257

101.330

13 323

18.030

13 379

20.959

firm that this can dramatically reduce the execution
time. One might have expected a substantial degradation in the test suite sizes produced. However, our
results indicate not only that the balancing strategy
is competitive, but also that it can improve test suite
sizes.
Fast methods such as IPO do not generally produce the smallest test suites possible. To illustrate
this, we apply a post-optimization method from [4748] to some of the TCAS results. For strength 4, we
treat the solutions for IPOG-F2; within 10 minutes of
computation, post-optimization reduces the solution by
our method from 1 639 to 1 201 rows, and the solution
by the original method from 1 653 to 1 205 rows. For
strength 5, we treat the solutions for IPOG-F; within
one hour of computation, post-optimization reduces the
solution by our method from 4 068 to 3 600 rows, and
the solution by the original method from 4 245 also
to 3 600 rows. For strength 6, we treat the solutions

IPOG-F2(ACTS)

for IPOG; within 10 hours, post-optimization reduces
the solution by our method from 11 774 to 9 794 rows,
and the solution by the original method from 11 021 to
9 798 rows. By contrast, in a comparison of six different one-parameter-at-a-time methods[46] , the best result has 10 851 rows. While the test suites from oneparameter-at-a-time methods are therefore definitely
not the smallest, post-optimization is much more timeconsuming and it requires a test suite as input. As the
number of parameters increases, the speed with which
an initial test suite can be constructed is crucial.
5.2

Expected Time to Fault Detection

Accelerating the IPO methods, even with a possible loss of accuracy in test suite size, can be worthwhile. However, a second concern is with potential performance in revealing faults. We examine the TCAS
system, using our and the original versions of the three
IPO variants. We examine the time to find the first

965

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO

fault when 1, 2, or 3 faults are present and when the
strength is between 2 and 6. In our model, the time to
execute each test is the same, so the expected time is
directly proportional to the expected number of tests or
rows needed. We consider test suites before and after
our reordering.
Table 6 gives the results. To assess the efficacy of
our modifications, we report two lines for each method
and each strength; the first reports results for our methods, and the second for the original methods. Î¦1 , Î¦2
and Î¦3 denote the expected number of tests to detect
the first fault when there are one, two or three faults
that are randomly chosen.
These results indicate that reordering is effective in
reducing the time to fault detection, both for our met-

hods and for the original ones. Fig.5 shows Î¦2 for each
strength before and after the reordering for our methods, showing a substantial reduction from reordering.
Fig.6 instead shows the expected number of tests when
zero, one, two, or three faults are present. It appears
that the reordering method is the most effective when
the number of faults is small. This should be expected,
because the presence of many faults ensures that one
will be found early no matter what ordering is used.
Our methods, despite often producing larger test
suites, fare well with respect to expected time to fault
detection. Comparing the performance of ours and the
original IPOG when t = 6, for example, although our
test suite is larger, it would yield smaller expected time
to detect faults once reordered. Evidently the size of the

Table 6. Expected Time to Fault Detection for TCAS Before and After Reordering
Algorithm

t

Number of Faults
Î¦1

IPOG

2
3
4
5
6

IPOG-F

2
3
4
5
6

IPOG-F2

2
3
4
5
6

Î¦2

Î¦3

Before

After

Before

After

Before

After

00 24.80

00 19.65

00 10.72

000 9.26

000 6.27

005.83

00 24.81

00 19.65

00 10.73

000 9.26

000 6.27

005.85

0 117.90

00 82.15

00 53.30

00 38.57

00 30.08

023.69

0 117.68

00 82.12

00 53.18

00 38.47

00 30.03

023.56

0 407.20

0 275.38

0 200.45

0 131.93

0 118.33

082.38

0 408.42

0 276.34

0 201.60

0 132.73

0 119.08

082.77

1 348.26

0 850.74

0 707.82

0 421.49

0 436.03

268.03

1 348.14

0 848.20

0 708.24

0 421.30

0 436.40

268.37

3 015.32

2 127.94

1 682.31

1 095.54

1 097.27

719.43

3 007.69

2 140.04

1 680.95

1 106.03

1 096.56

725.45

00 28.36

00 20.43

00 12.73

000 9.58

000 7.29

006.01

00 27.19

00 20.67

00 12.18

000 9.67

000 7.08

006.02

0 120.96

00 81.47

00 55.81

00 38.33

00 31.84

023.71

0 120.94

00 81.34

00 55.99

00 38.18

00 32.13

023.72

0 411.37

0 272.86

0 204.59

0 132.18

0 121.66

082.83

0 411.97

0 269.36

0 204.73

0 129.68

0 121.75

081.77
263.23

1 353.42

0 828.83

0 716.08

0 411.92

0 444.08

1 354.28

0 822.73

0 715.52

0 410.22

0 443.26

261.36

3 076.17

2 090.57

1 722.82

1 065.49

1 129.80

698.08

3 017.29

2 059.33

1 693.94

1 063.99

1 109.05

700.68

00 26.44

00 20.52

00 11.90

000 9.75

000 6.98

006.13

00 26.27

00 20.19

00 11.67

000 9.56

000 6.80

006.00

0 120.61

00 82.75

00 55.26

00 38.36

00 31.49

023.84

0 121.04

00 81.63

00 55.61

00 38.15

00 31.76

023.55

0 419.07

0 275.05

0 207.11

0 130.39

0 123.20

081.95

0 421.75

0 278.10

0 208.20

0 131.82

0 123.79

082.43
263.65

1 378.15

0 844.54

0 724.94

0 412.79

0 449.34

1 377.84

0 838.77

0 725.02

0 409.71

0 449.50

261.35

3 129.21

2 127.62

1 732.44

1 068.73

1 133.78

699.08

3 138.02

2 121.97

1 736.29

1 062.64

1 136.22

695.18

966

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5
2000
Before Reorder
After Reorder

1500
1000
500
0

2

3
4
5
Test Strength
(a)

6

2000
Expected Number of Tests

Expected Number of Tests

Expected Number of Tests

2000

Before Reorder
After Reorder

1500
1000
500
0

2

3
4
5
Test Strength
(b)

6

Before Reorder
After Reorder

1500
1000
500
0

2

3
4
5
Test Strength
(c)

6

Fig.5. Expected number of tests for Î¦2 . (a) IPOG. (b) IPOG-F. (c) IPOG-F2.

Before Reorder
After Reorder
1000

500

0

0

1
2
3
Number of Faults

4

Expected Number of Tests

1500
Expected Number of Tests

Expected Number of Tests

1500

Before Reorder
After Reorder
1000

500

0

0

1
2
3
Number of Faults

(a)

(b)

4

2000
Before Reorder
After Reorder

1500

1000

500

0

0

1
2
3
Number of Faults

4

(c)

Fig.6. Expected number of tests, increasing the number of faults in TCAS (4-way). (a) IPOG. (b) IPOG-F. (c) IPOG-F2.

test suite, while relevant, is not the only factor affecting the expected time. Our results suggest that faster
IPO implementations remain competitive, and hence
that the objective of balancing frequencies of values is
a reasonable one to pursue.
6

Conclusions

We identified three main goals in generating a test
suite: time to generate the test suite, time to execute
the test suite (test suite size), and the rate of fault
detection. Our methods focus on reducing the time
for generation, without severe negative impact on test
suite size and fault detection. We accelerated variants
of the IPO method by simplifying the manner in which
donâ€™t care positions are filled. This results in a consistent improvement in the execution time to construct a
test suite, but sacrifices to some extent the algorithmâ€™s
ability to exploit such positions in repeated horizontal
growth phases. This is reflected in our experimental results. While in numerous cases, our modifications find
smaller test suites, in the others they do not. This
occurs particularly when the number of parameters is
large.

Any method to fill donâ€™t care positions immediately
would be expected to accelerate the methods; however
we devised a simple method that strives to balance the
frequency of values for each parameter. We argued that
such an objective can result in more effective horizontal growth, and that it can permit us to retain effective
rates of fault detection. Both of these motivations are
borne out by the experimental data.
One-test-at-a-time generation methods explicitly
aim for good rates of fault detection by covering interactions early in the test suite, while one-parameterat-a-time methods like IPO do not. Nevertheless, we
showed that a reordering strategy can be applied to
make dramatic improvement on the rate of fault detection.
If test suite size is a primary objective, using our methods together with randomized postoptimization[47-48] appears to be worthwhile. If expected time to fault detection is paramount, extending
reordering to discover and replace donâ€™t care positions
appears to be viable. Both merit further study. We
suggest that both can benefit from balancing frequencies of values, a fast and simple way to generate useful
test suites.

Shi-Wei Gao et al.: Balancing Frequencies and Fault Detection in IPO

References
[1] Birnbaum Z W. On the importance of different components in a multicomponent system. In Multivariate Analysis, Krishnaiah P R (ed.), New York: Academic Press,
1969, pp.591-592.
[2] Kuo W, Zhu X. Relations and generalizations of importance
measures in reliability. IEEE Trans. Rel., 2012, 61(3): 659674.
[3] Kuo W, Zhu X. Some recent advances on importance measures in reliability. IEEE Trans. Rel., 2012, 61(2): 344-360.
[4] Anand S, Burke E K, Chen T Y et al. An orchestrated
survey of methodologies for automated software test case
generation. J. Sys. Software, 2013, 86(8): 1978-2001.
[5] Chen T Y, Kuo F C, Liu H et al. Code coverage of adaptive
random testing. IEEE Trans. Rel., 2013, 62(1): 226-237.
[6] Grindal M, Offutt J, Andler S F. Combination testing
strategies: A survey. Softw. Test. Verif. Rel., 2005, 15(3):
167-199.
[7] Hao D, Zhang L M, Zhang L et al. A unified test-case prioritization approach. ACM Trans. Soft. Eng. Method, 2014,
24(2): 10:1-10:31.
[8] Harman M, McMinn P. A theoretical and empirical study
of search-based testing: Local, global, and hybrid search.
IEEE Trans. Software Eng., 2010, 36(2): 226-247.
[9] Nie C H, Leung H. A survey of combinatorial testing. ACM
Comput. Surv., 2011, 43(2): 11:1-11:29.
[10] Nebut C, Fleurey F, Le Traon Y et al. Automatic test generation: A use case driven approach. IEEE. Trans. Software
Eng., 2006, 32(3): 140-155.
[11] Perrouin G, Oster S, Sen S et al. Pairwise testing for software product lines: Comparison of two approaches. Software. Qual. J., 2012, 20(3/4): 605-643.
[12] Xie T, Zhang L, Xiao X et al. Cooperative software testing
and analysis: Advances and challenges. Journal of Computer Science and Technology, 2014, 29(4): 713-723.
[13] Yoo S, Harman M. Regression testing minimization, selection and prioritization: A survey. Softw. Test. Verif. Rel.,
2012, 22(2): 67-120.
[14] Zhang D M, Xie T. Software analytics: Achievements and
challenges. In Proc. the 35th Int. Conf. Software Eng., May
2013, p.1487.
[15] Yu K, Lin M, Chen J et al. Towards automated debugging
in software evolution: Evaluating delta debugging on real
regression bugs from the developersâ€™ perspectives. J. Sys.
Software, 2012, 85(10): 2305-2317.
[16] Bryce R C, Colbourn C J. One-test-at-a-time heuristic
search for interaction test suites. In Proc. the 9th Annu.
Conf. Genetic and Evolutionary Computation, Jul. 2007,
pp.1082-1089.
[17] Kuhn D R, Reilly M J. An investigation of the applicability
of design of experiments to software testing. In Proc. the
27th Annu. NASA Goddard Workshop on Software Eng.,
Dec. 2002, pp.91-95.
[18] Kuhn D R, Wallace D R, Gallo Jr J A. Software fault interactions and implications for software testing. IEEE Trans.
Software Eng., 2004, 30(6): 418-421.
[19] Cohen M B, Dwyer M B, Shi J. Constructing interaction
test suites for highly-configurable systems in the presence
of constraints: A greedy approach. IEEE Trans. Software
Eng., 2008, 34(5): 633-650.

967

[20] Lei Y, Kacker R, Kuhn D R et al. IPOG/IPOG-D: Efficient
test generation for multi-way combinatorial testing. Softw.
Test. Verif. Rel., 2008, 18(3): 125-148.
[21] Tung Y W, Aldiwan W S. Automating test case generation
for the new generation mission software system. In Proc.
IEEE Aerospace Con., March 2000, pp.431-437.
[22] Wallace D R, Kuhn D R. Failure modes in medical device
software: An analysis of 15 years of recall data. Int. J. Rel.,
Quality and Safety Eng., 2001, 8(4): 351-371.
[23] Cohen D M, Dalal S R, Kajla A et al. The automatic efficient tests generator (AETG) system. In Proc. the 5th Int.
Sympo. Software Rel. Eng., Nov. 1994, pp.303-309.
[24] Bryce R C, Colbourn C J. The density algorithm for pairwise interaction testing. Softw. Test. Verif. Rel., 2007,
17(3): 159-182.
[25] Bryce R C, Colbourn C J. A density-based greedy algorithm
for higher strength covering arrays. Softw. Test. Verif. Rel.,
2009, 19(1): 37-53.
[26] Lei Y, Tai K C. In-parameter-order: A test generation strategy for pairwise testing. In Proc. the 3rd Int. Symp. HighAssurance Sys. Eng., Nov. 1998, pp.254-261.
[27] Lei Y, Kacker R, Kuhn D R et al. IPOG: A general strategy for t-way software testing. In Proc. the 14th Annu. Int.
Conf. Worshop. Eng. Computer-Based Sys., March 2007,
pp.549-556.
[28] Forbes M, Lawrence J, Lei Y, Kacker R N, Kuhn D R. Refining the in-parameter-order strategy for constructing covering arrays. Journal of Research of the National Institute
of Standards and Technology, 2008, 113(5): 287-297.
[29] Cohen M B, Gibbons P B, Mugridge W B et al. Constructing test cases for interaction testing. In Proc. the 25th Int.
Conf. Software Eng., May 2003, pp.38-48.
[30] Bryce R C, Colbourn C J. Expected time to detection of
interaction faults. J. Combin. Mathematics and Combin.
Comput., 2013, 86: 87-110.
[31] Rothermel G, Untch R H, Chu C et al. Prioritizing test
cases for regression testing. IEEE Trans. Software Eng.,
2001, 27(10): 929-948.
[32] Qu X, Cohen M B. A study in prioritization for higher
strength combinatorial testing. In Proc. the 6th Int. Con.
Software Testing, Verification and Validation, the 2nd Int.
Workshops on Combinatorial Testing, March 2013, pp.285294.
[33] Qu X. Configuration aware prioritization techniques in regression testing. In Proc. the 31st Int. Conf. Software Engineering, Companion Volume, May 2009, pp.375-378.
[34] Qu X, Cohen M B, Rothermel G. Configuration-aware regression testing: An empirical study of sampling and prioritization. In Proc. Int. Symp. Software Tesing and Analysis,
July 2008, pp.75-86.
[35] Bryce R C, Colbourn C J. Test prioritization for pairwise
interaction coverage. In Proc. the 1st Int. Workshop on Advances in Model-Based Testing, May 2005.
[36] Bryce R C, Colbourn C J. Prioritized interaction testing
for pair-wise coverage with seeding and constraints. Inform.
Software Tech., 2006, 48(10): 960-970.
[37] Huang R, Chen J, Li Z, Wang R, Lu Y. Adaptive random
prioritization for interaction test suites. In Proc. the 29th
Symp. Appl. Comput., March 2014, pp.1058-1063.

968
[38] Petke J, Yoo S, Cohen M B, Harman M. Efficiency and
early fault detection with lower and higher strength combinatorial interaction testing. In Proc. the 12th Joint Meeting on European Software Engineering Conf. and the
ACM SIGSOFT Symp. the Foundations of Software Eng.
(ESEC/FSE 2013), August 2013, pp.26-36.
[39] Qu X, Cohen M B, Woolf K M. Combinatorial interaction
regression testing: A study of test case generation and prioritization. In Proc. the 23rd Int. Conf. Software Maintenance, Oct. 2007, pp.255-264.
[40] Huang R, Chen J, Zhang T, Wang R, Lu Y. Prioritizing
variable-strength covering array. In Proc. the 37th IEEE
Annu. Computer Software and Applications Conf., July
2013, pp.502-511.
[41] Huang R, Xie X, Towey D, Chen T Y, Lu Y, Chen J. Prioritization of combinatorial test cases by incremental interaction coverage. Int. J. Softw. Eng. Know., 2014, 23(10):
1427-1457.
[42] Bryce R C, Memon A M. Test suite prioritization by interaction coverage. In Proc. Workshop on Domain Specific
Approaches to Software Test Automation, September 2007,
pp.1-7.
[43] Lei Y, Kuhn D R. Advanced combinatorial testing
suite (ACTS). http://csrc.nist.gov/groups/SNS/acts/index.html, Aug. 2015.
[44] Hutchins M, Foster H, Goradia T et al. Experiments of the
effectiveness of dataflow and control-flow-based test adequacy criteria. In Proc. the 16th Int. Conf. Software Eng.,
May 1994, pp.191-200.
[45] Kuhn D R, Okun V. Pseudo-exhaustive testing for software.
In Proc. the 30th Annu. IEEE/NASA Software Engineering Workshop, April 2006, pp.153-158.
[46] Soh Z H C, Abdullah S A C, Zamil K Z. A distributed tway test suite generation using â€œOne-Parameter-at-a-Timeâ€
approach. Int. J. Advance Soft Compu. Appl., 2013, 5(3):
91-103.
[47] Li X, Dong Z, Wu H et al. Refining a randomized postoptimization method for covering arrays. In Proc. the 7th
IEEE Int. Conf. Software Testing, Verification and Validation Workshops (ICSTW), March 31-April 4, 2014, pp.143152.
[48] Nayeri P, Colbourn C J, Konjevod G. Randomized postoptimization of covering arrays. Eur. J. Combin., 2013,
34(1): 91-103.

Shi-Wei Gao received his B.S. degree in computer science and technology
from Dezhou University, Dezhou, in
2007, and M.S. degree in information
science and engineering from Yanshan
University, Qinhuangdao, in 2010. He
is currently a Ph.D. candidate in the
School of Computer Science and Engineering of Beihang University, Beijing. He is a member
of the State Key Laboratory of Software Development
Environment of Beihang University. His research interests
include software testing, software reliability theory, and
formal methods.

J. Comput. Sci. & Technol., Sept. 2015, Vol.30, No.5

Jiang-Hua Lv received her B.S. and
Ph.D. degrees in computer science from
Jilin University, Changchun, in 1998
and 2003, respectively. Currently she
is an assistant professor in the School
of Computer Science and Engineering
of Beihang University, Beijing. She
is a member of the State Key Laboratory of Software
Development Environment of Beihang University. Her
research focuses on formal theory and technology of
software, theory and technology of testing, automatic
testing of safety critical systems, and device collaboration.
Bing-Lei Du is currently an undergraduate in the School of Computer
Science and Engineering of Beihang
University, Beijing, and has been an
intern in the State Key Laboratory of
Software Development Environment
of Beihang University since 2013. His
research interest is software testing.
Charles J. Colbourn earned his
Ph.D. degree in computer science from
the University of Toronto in 1980, and
is a professor of computer science and
engineering at Arizona State University.
He is the author of The Combinatorics
of Network Reliability (Oxford), Triple
Systems (Oxford), and 320 refereed
journal papers focusing on combinatorial designs and
graphs with applications in networking, computing, and
communications. In 2004, he was awarded the Euler Medal
for Lifetime Research Achievement by the Institute for
Combinatorics and its Applications.
Shi-Long Ma is currently a professor and doctor tutor of the School
of Computer Science and Engineering of Beihang University, Beijing.
He is a member of the State Key
Laboratory of Software Development
Environment of Beihang University.
His main research focus is on computation models in networks, logic reasoning and behaviors
in network computing, and the theory of automatic testing.

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

1

A Combinatorial Approach to X-Tolerant
Compaction Circuits

arXiv:1508.00481v1 [cs.IT] 3 Aug 2015

Yuichiro Fujiwara and Charles J. Colbourn

Abstractâ€”Test response compaction for integrated circuits
(ICs) with scan-based design-for-testability (DFT) support in
the presence of unknown logic values (Xs) is investigated from
a combinatorial viewpoint. The theoretical foundations of Xcodes, employed in an X-tolerant compaction technique called
X-compact, are examined. Through the formulation of a combinatorial model of X-compact, novel design techniques are developed
for X-codes to detect a specified maximum number of errors in
the presence of a specified maximum number of unknown logic
values, while requiring only small fan-out. The special class of
X-codes that results leads to an avoidance problem for configurations in combinatorial designs. General design methods and
nonconstructive existence theorems to estimate the compaction
ratio of an optimal X-compactor are also derived.
Index Termsâ€”Circuit testing, built-in self-test (BIST), compaction, X-compact, test compression, X-code, superimposed
code, Steiner system, configuration.

I. I NTRODUCTION

T

HIS work discusses a class of codes that arise in data
volume compaction of responses from integrated circuits (ICs) under scan-based test. We first recall briefly the
background of the X-tolerant compaction technique in digital
circuit testing.
Digital circuit testing applies test patterns to a circuit under
test and monitors the circuitâ€™s responses to the applied patterns.
A tester compares the observed response to a test pattern to
the expected response and, if there is a mismatch, declares
the circuit chip defective. Usually the expected responses are
obtained through fault-free simulation of the chip.
Test cost for traditional scan-based testing is dominated
by test data volume and test time [1]. Therefore various test
compression techniques have been developed to reduce test
cost. One way to achieve this is to reduce test application time
and the number of test patterns by employing automatic test
pattern generation (ATPG) (see [2]â€“[5] and references therein).
We are interested in the other kind of technique, using methods
to hash responses while maintaining test quality. Signature
analyzers (e.g., [6]â€“[10]) are vulnerable to error masking
caused by unknown logic values (Xs) [11]. X-compact has
been proposed in order to conduct reliable testing in the
presence of Xs [12]. A response compaction circuit based
This work was supported in part by JSPS Research Fellowships for Young
Scientists (YF) and by DOD grants N00014-08-1-1069 and N00014-08-11070 (CJC).
Y. Fujiwara is with the Department of Mathematical Sciences, Michigan
Technological University, Houghton, MI 49931 USA. yfujiwar@mtu.edu.
C. J. Colbourn is with the School of Computing, Informatics, and Decision
Systems Engineering, Arizona State University, Tempe, AZ 85287-8809 USA.
charles.colbourn@asu.edu

on X-compact is an X-compactor. X-compactors have proved
their high error detection ability in actual systems [11], [13].
An X-compactor can be written in matrix form as an X-code
[14]. Basic properties of X-codes have been studied [14], [15].
Graph theoretic techniques have been employed to minimize
fan-out of inputs [16]; in general an X-compactor tolerates the
presence of Xs in exchange for large fan-out. These studies
focus on particular classes of X-codes rather than the general
coding theoretic aspects.
The purpose of the present paper is to investigate theoretical
foundations of X-codes and to provide general construction
techniques. In Section II we outline the combinatorial requirements for the X-compact technique and present an equivalent
definition of X-codes in order to investigate X-compactors
as codes and combinatorial designs. Some basic properties
of X-codes are also presented. In Section III we investigate
X-codes that require only small fan-out and have good error detectability and X-tolerance. We prove the equivalence
between a class of Steiner t-designs and particular X-codes
having the maximum number of codewords and the minimum
fan-out. This allows us to give constructions and to show
existence of such X-codes. Section IV deals with existence of
X-codes in the more general situation. Both constructive and
nonconstructive theorems are provided. Finally we conclude
in Section V.
II. C OMBINATORIAL R EQUIREMENTS

AND

X-C ODES

We do not describe scan-based testing and response compaction in detail here, instead referring the reader to [11], [12].
Scan-based testing repeatedly applies vectors of test inputs
to the circuit, and for each test captures a vector from {0, 1}n
as the test output. Naturally it is important that the test output
be the correct one. To determine this, the function of the circuit
is simulated (in a fault-free manner) to produce a reference
output. When the test and reference outputs agree, no fault
has been detected. The first major obstacle is that fault-free
simulation may be unable to determine whether a specific
output is 0 or 1, and hence it is an unknown logic value X.
The second is that if each output requires a separate pin on the
chip, the number of tests that can be accommodated is quite
restricted. We deal with these two problems in turn.
We define an algebraic system to describe the behavior of
Xs. The X-algebra X2 = ({0, 1, X}, +, Â·) over the field F2
is the set {0, 1} of elements of F2 and a third element X,
equipped with two binary operations â€œ+â€ (addition) and â€œÂ·â€
(multiplication) satisfying:
1) a + b and a Â· b are performed in F2 for a, b âˆˆ F2 ;

2

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

2) a + X = X + a = X for a âˆˆ F2 ;
3) 0 Â· X = X Â· 0 = 0 for the additive identity 0;
4) 1 Â· X = X Â· 1 = X.
The element X is termed an unknown logic value.
Now consider a test output b = (b1 , . . . , bn ) âˆˆ {0, 1}n
and a reference output c = (c1 , . . . , cn ) âˆˆ {0, 1, X}n . When
ci âˆˆ {0, 1}, the test and reference outputs agree on the ith
bit when bi = ci ; otherwise the ith bit is an error bit. When
ci = X, whatever the value of bi , no error is detected. Thus
the ith bit is (known to be) in error if and only if bi + ci = 1,
using addition in X2 .
Turning to the second problem, an X-compact matrix is an
n Ã— m matrix H with elements from {0, 1}. The compaction
ratio of H is n/m. The number of 1s in the ith row is the
weight, or fan-out, of row i. Output (or response) compaction
is performed by computing the vector d = (d1 , . . . , dm ) = bH
for output (arithmetic is in X2 ). In the same way, the reference
output can be compacted using the same matrix to form r =
(r1 , . . . , rm ) = cH. As before, if di 6= ri and ri 6= X (that is,
if di + ri = 1), an error is detected.
To be of practical value, an X-compact matrix H should
detect the presence of error bits in b with respect to c given
the compacted vectors d and r under â€˜reasonableâ€™ restrictions
on the number of errors and number of unknown logic values.
Suppose that bâ„“ + câ„“ = 1 (so that there is a fault to be
detected). In principle, whenever hâ„“j = 1, the fault could be
observed on output j. Let L =P
{j : hâ„“j = 1}. Suppose then
n
that j âˆˆ L. If it happens that i=1 ci hij = X, the error at
position â„“ is masked for output j (that is, dj + rj = X, and
no error is observed). On the other hand, if
dj + rj =

n
X
i=1

bi hij +

n
X
i=1

ci hij =

n
X

(bi + ci )hij = 0

i=1

then no error is observed. This occurs when there are an even
number of values of i for which hij = 1 and bi + ci = 1;
because this holds when i = â„“ by hypothesis, the error at
position â„“ is canceled for output j when the number of such
errors is even. When an error is masked or canceled for every
output j âˆˆ L, it is not detected. Otherwise, it is detected by
an output that is neither masked nor canceled.
Treating Xâ€™s as erasures and using traditional codes can
increase the error detectability of an X-compactor [17]. Unfortunately, this involves postprocessing test responses and
cannot be easily implemented [12]. Therefore, we focus on
X-compaction in which an error is only detected by the simple
comparison described here.
There are numerous criteria in defining a â€œgoodâ€ X-compact
matrix. It should have a high compaction ratio and be able to
detect any faulty circuit behavior anticipated in actual testing.
Power requirements, compactor delay, and wireability dictate
that the weight of each row in a matrix be small to meet
practical limitations on fan-in and fan-out [11], [16].
The fundamental problem in X-tolerant response compaction is to design an X-compact matrix with large compaction ratio that detects faulty circuit behavior. To achieve
this, X-codes (which represent X-compact matrices) were
introduced [14]. In this section, we discuss basic properties

of X-codes. In order to investigate X-codes from coding and
design theoretic views, we introduce an equivalent definition.
Consider two m-dimensional vectors s1
=
(1) (1)
(1)
(2) (2)
(2)
(s1 , s2 , . . . , sm ) and s2 = (s1 , s2 , . . . , sm ), where
(j)
si âˆˆ F2 . The addition of s1 and s2 is bit-by-bit addition,
denoted by s1 âŠ• s2 ; that is,
(1)

(2)

(1)

(2)

(2)
s1 âŠ• s2 = (s1 + s1 , s2 + s2 , . . . , s(1)
m + sm ).

The superimposed sum of s1 and s2 , denoted s1 âˆ¨ s2 , is
(1)

(2)

(1)

(2)

(2)
s1 âˆ¨ s2 = (s1 âˆ¨ s1 , s2 âˆ¨ s2 , . . . , s(1)
m âˆ¨ sm ),
(j)

(l)

(j)

(l)

where si âˆ¨ sk = 0 if si = sk = 0, otherwise 1. An
m-dimensional vector s1 covers an m-dimensional vector s2
if s1 âˆ¨ s2 = s1 .
For a finite set S = {s1 , . . . , ss } of m-dimensional vectors,
define
M
_
S = s1 âŠ• Â· Â· Â· âŠ• ss and
S = s1 âˆ¨ Â· Â· Â· âˆ¨ ss .
L
W
When S =L
{s1 } isWa singleton,
S = S = s1 . For S = âˆ…
we define
S = S = 0, the zero vector.
Let d be a positive integer and x a nonnegative integer.
An (m, n, d, x) X-code X = {s1 , s2 , . . . , sn } is a set of mdimensional vectors over F2 such that |X | = n and
_
M
_
( S1 ) âˆ¨ (
S2 ) 6=
S1 .

for any pair of mutually disjoint subsets S1 and S2 of X with
|S1 | = x and 1 â‰¤ |S2 | â‰¤ d. A vector si âˆˆ X is a codeword.
(i)
(i)
The weight of a codeword si is |{sj 6= 0 : sj âˆˆ si }|. The
ratio n/m is the compaction ratio of X . An X-code forming
an orthonormal basis of the m-dimensional linear space over
F2 is trivial.
Roughly speaking, an X-code is a set of codewords such
that for every positive integer dâ€² â‰¤ d no superimposed sum of
any x codewords covers the vector obtained by adding up any
dâ€² codewords chosen from the rest of the n âˆ’ x codewords.
Now we present a method of designing an X-compact matrix
from an X-code.
Proposition 1: There exists an (m, n, d, x) X-code X if and
only if there exists an n Ã— m X-compact matrix H which
detects any combination of dâ€² faults (1 â‰¤ dâ€² â‰¤ d) in the
presence of at most x unknown logic values.
Proof: First we prove necessity. Assume that X is an
(m, n, d, x) X-code. Write X = {s1 , s2 , . . . , sn }, where si =
(i) (i)
(i)
(s1 , s2 , . . . , sm ) for 1 â‰¤ i â‰¤ n. Define an n Ã— m matrix
(i)
H = (hi,j ) as hi,j = sj . We show that H forms an Xcompact matrix that detects a fault if the test output b contains
dâ€² error bits, 1 â‰¤ dâ€² â‰¤ d, and up to x Xs.
Let E = {k : bk + ck = 1}, the set of indices of error bits,
have cardinality dâ€² . Let X = {k : ck = X}, the set of indices
of unknown logic values, have cardinality x. Now comparing
dâ„“ and râ„“ ,
X
X
dâ„“ + râ„“ =
bk Â· hk,â„“ +
ck Â· hk,â„“
k

=

k

X

(bk + ck ) Â· hk,â„“

kâˆˆE,X

=

X

kâˆˆE

1 Â· hk,â„“ +

X

kâˆˆX

X Â· hk,â„“ ,

(1)

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

with operations performed in X2 . Because the set of rows of
H forms the set of codewords of X , no superimposed sum of
x rows covers the vector obtained by an addition of any dâ€²
rows. Hence there exists a column c such that
X
X
1 Â· hk,c = 1 and
X Â· hk,c = 0.
(2)
kâˆˆE

kâˆˆX

Then (1) and (2) imply dc + rc = 1, that is, H detects a fault.
Because (2) holds if and only if the right hand side of (1)
equals one for l = c, sufficiency is straightforward.
By virtue of this equivalence, we can employ various
known results and techniques in coding theory to design an
X-compactor with good error detection ability, X-tolerance,
and a high compaction ratio. For the case when x = 0, an
(m, n, d, 0) X-code forms an n Ã— m X-compact matrix which
is a parity-check matrix of a binary linear code of length n and
minimum distance d. In fact, since the condition that x = 0
implies the absence of Xs, this special case is reduced to
traditional space compaction. Because our focus is compaction
in the presence of unknown logic values, we assume that x â‰¥ 1
henceforth unless otherwise stated. In the absence of Xs, see
[18], [19].
By definition, an (m, n, d, x) X-code, d â‰¥ 2, is also an
(m, n, d âˆ’ 1, x) X-code. Also an (m, n, d, x) X-code forms an
(m, n, d, x âˆ’ 1) X-code. Moreover, an (m, n, d, x) X-code is
an (m, n, d + 1, x âˆ’ 1) X-code [14].
It can be difficult to design an X-compactor having both the
necessary error detectability and the exact number of inputs
needed. One trivial solution is to discard codewords from a
larger X-code with sufficient error detection ability and Xtolerance. The following is another simple way to adjust the
number of inputs.
Proposition 2: If
an
(m, n, d, x)
X-code
and
an (mâ€² , nâ€² , dâ€² , xâ€² ) X-code exist, there exists an
(m + mâ€² , n + nâ€² , min{d, dâ€² }, min{x, xâ€² }) X-code.
Proof: Let X = {s1 , . . . , sn } be an (m, n, d, x) Xcode and Y = {t1 , . . . , tnâ€² } an (mâ€² , nâ€² , dâ€² , xâ€² ) X-code.
(i)
(i)
Extend each codeword si = (s1 , . . . , sm ) of X by appending mâ€² 0â€™s so that extended vectors have the form
(i)
(i)
sâ€²i = (s1 , . . . , sm , 0, . . . , 0). Similarly extend each codeword
(j)
(j)
tj = (t1 , . . . , tmâ€² ) of Y by appending m 0s so that extended
(j)
(j)
vectors have the form tâ€²j = (0, . . . , 0, t1 , . . . , tmâ€² ). The
extended (m + mâ€² )-dimensional vectors form an (m + mâ€² , n +
nâ€² , min{d, dâ€² }, min{x, xâ€² }) X-code.
Proposition 2 says that given an (m, n, d, x) X-code, a
codeword of weight less than or equal to x does not essentially
contribute to the compaction ratio (see also [14]). In fact,
(i)
(i)
if X contains such a codeword si = (s1 , . . . , sm ), there
(i)
exists at least one coordinate mâ€² such that smâ€² = 1 and
(j)
smâ€² = 0 for any other codeword sj âˆˆ X . Hence we can
delete si and coordinate mâ€² from X while keeping d and x.
By applying Proposition 2 and combining a trivial X-code
and another X-code, we can obtain an X-code having the
same number of codewords with compaction ratio no smaller.
For this reason, when constructing an (m, n, d, x) X-code
explicitly, we assume that every codeword has weight greater
than x.

3

Let M (m, d, x) be the maximum number n of codewords
for which there exists an (m, n, d, x) X-code. More codewords
means a higher compaction ratio. Hence an (m, n, d, x) Xcode satisfying n = M (m, d, x) is optimal.
Determining the exact value of M (m, d, x) seems difficult
except for M (m, 1, 1). As pointed out in [14], a special case
of M (m, d, x) has been extensively studied in the context of
superimposed codes [20]. An (1, x)-superimposed code of size
m Ã— n is an m Ã— n matrix S with entries in F2 such that no
superimposed sum of any x columns of S covers any other
column of S. Superimposed codes are also called cover-free
families and disjunct matrices.
By definition, a (1, x)-superimposed code of size m Ã— n is
equivalent to the transpose of an X-compact matrix obtained
from an (m, n, 1, x) X-code. Hence known results on the
maximum ratio n/m for superimposed codes immediately
give information about M (m, 1, x). For completeness, we list
useful results on M (m, 1, x).
By Spernerâ€™s theorem,
Theorem 2.1: (see [21], [22]) For m â‰¥ 2 an integer,


m
M (m, 1, 1) â‰¤
.
âŒŠm/2âŒ‹

Indeed by taking all the m-dimensional vectors of weight
âŒŠm/2âŒ‹ as codewords, we attain the bound. The same argument
is also found in [14].
The following is a simple upper bound on M (m, 1, x):
Theorem 2.2: [22] For any x â‰¥ 2,
log2 M (m, 1, x) â‰¤

cm log2 x
x2

for some constant c.
Several different proofs of Theorem 2.2 are known. Bounds
on the constant c are approximately two in [23], approximately
four in [24], and approximately eight in [25].
The asymptotic behavior of the maximum possible number
of codewords has been also investigated for superimposed
codes. Define the ratio R(x) as
log2 M (m, 1, x)
.
mâ†’âˆ
m
The best lower bound R(x) â‰¤ R(x) can be found in
[26] and the best upper bound R(x) â‰¥ R(x) in [23]. The
descriptive asymptotic form of the best bounds as x â†’ âˆ is
R(x) = lim

R(x) âˆ¼

1
2 log2 x
,
and R(x) âˆ¼
x2 log2 e
x2

where e is Napierâ€™s constant. For a detailed summary of the
known lower and upper bounds, see [27]. Constructions with
many codewords have been studied in [28], [29]. See also
[30]â€“[33] and references therein.
III. X-C OMPACTORS

WITH

S MALL FAN -O UT

In this section we consider an X-compactor having sufficient
tolerance for errors and Xs, a high compaction ratio, and small
fan-out. This section is divided into four parts. Subsection
III-A deals with background and known results of the fanout problem in X-compactors. Then in Subsection III-B we
investigate X-codes that tolerate up to two Xâ€™s and have

4

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

the minimum fan-out. X-Codes with further error detection
ability and X-tolerance are investigated in Subsection III-C. In
Subsection III-D we give a brief overview of the performance
of our X-codes given in this section and compare them with
other codes.
A. Background: Fan-Out in X-Codes
X-compact reduces the number of bits in the compacted
output while keeping error detection ability by propagating
each single bit to many signal lines. In fact, each output of
the X-compactor in [12] connects to about half of all inputs.
However, larger fan-in increases power requirements, area, and
delay [16]. When these disadvantages are concerns, fan-out of
inputs of a compactor should be small to reduce fan-in values.
In terms of X-codes, the required fan-out of input i in an
X-compactor is the weight of codeword si of the X-code.
Hence, in order to address the fan-out problem, it is desirable
for a codeword to have small weight. However, as mentioned
in Section I, an (m, n, d, x) X-code containing a codeword
with weight at most x is not essential in the sense of the
compaction ratio. Hence, throughout this section, we restrict
ourselves to (m, n, d, x) X-codes in which every codeword has
weight precisely x + 1, namely constant weight codes.
When a compactor is required to tolerate only a single unknown logic value, fan-out is minimized when every codeword
of an X-code has constant weight two. This extreme case was
addressed in [16] by considering a simple graph. We briefly
restate their theorems in terms of X-codes.
A graph G is a pair (V, E) such that V is a finite set and
E is a set of pairs of distinct elements of V . An element of
V is called a vertex, and an element of E is called an edge.
The girth g of G is the minimal size |C| of a subset C âŠ‚ E
such that each vertex appearing in C is contained in exactly
two edges.
The edge-vertex incidence matrix H of a graph G = (V, E)
is a |E| Ã— |V | binary matrix H = (hi,j ) such that rows and
columns are indexed by edges and vertices respectively and
hi,j = 1 if the ith edge contains the jth vertex, otherwise 0.
By considering the edge-vertex incidence matrix of a graph
and Proposition 1, we obtain:
Theorem 3.1: [16] There exists a graph G = (V, E) of girth
g if and only if there exists a (|V |, |E|, g âˆ’ 2, 1) X-code of
constant weight two.
Theorem 3.2: [16] A set X of m-dimensional vectors is an
(m, n, d âˆ’ 1, 1) X-code of constant weight two if and only if
it is an (m, n, d, 0) X-code of weight two.
These two theorems say that in order to design an Xcompactor with high error detection ability, we only need to
find a graph with large girth. The same argument is also found
in [14]. For existence of such graphs and more details on Xcodes of constant weight two, see [16] and references therein.
B. Two Xâ€™s and Fan-Out Three
Multiple Xâ€™s can occur; here we present X-codes that are
tolerant to two Xâ€™s and have the maximum compaction ratio.
To accept up to two unknown logic values, we need an X-code

of constant weight three. We employ a well-known class of
combinatorial designs.
A set system is an ordered pair (V, B) such that V is a finite
set of points, and B is a family of subsets (blocks) of V . A
Steiner t-design S(t, k, v) is a set system (V, B), where V is
a finite set of cardinality v and B is a family of k-subsets of
V such that each t-subset of V is contained in exactly one
block. Parameters v and k are the order and block size of a
Steiner t-design. When t = 2 and k = 3, an S(2, 3, v) is a
Steiner triple system of order v, STS(v). An STS(v) exists if
and only if v â‰¡ 1, 3 (mod 6) [34]. A triple packing of order
v is a set system (V, B) such that B is a family of triples of
a finite set V of cardinality v and any pair of elements of V
appear in B at most once. An STS(v) is a triple packing of
order v â‰¡ 1, 3 (mod 6) containing the maximum number of
triples.
The point-block incidence matrix of a set system (V, B)
is the binary |V | Ã— |B| matrix H = (hi,j ) such that rows
are indexed by points, columns are indexed by blocks, and
hi,j = 1 if the ith point is contained in the jth block, otherwise
0. The block-point incidence matrix is its transpose.
When d = 1, an (m, n, 1, 2) X-code of constant weight three
is equivalent to a (1, 2)-superimposed code of size m Ã— n of
constant column weight three. It is well known that the pointblock incidence matrix of an S(t, k, v) forms
 an (1, âŒˆk/(t âˆ’
1)âŒ‰ âˆ’ 1)-superimposed code of size v Ã— vt / kt . Hence, by
using an STS(v), we obtain for every v â‰¡ 1, 3 (mod 6) a
(v, v(v âˆ’ 1)/6, 1, 2) X-code. An upper bound on the number
of codewords of (1, 2)-superimposed codes of constant weight
k is available:
Theorem 3.3: [35] Let nk (m) denote the maximum number
of columns of a (1, 2)-superimposed code such that and every
column is of length m and has constant weight k. Then,

m
n2tâˆ’1 (m) â‰¤ n2t (m + 1) â‰¤

t

2tâˆ’1
t

with equality if and only if there exists a Steiner t-design
S(t, 2t âˆ’ 1, m).
The following is an immediate consequence:
Theorem 3.4: For any (m, n, 1, 2) X-code of constant
with equality if and only if there
weight three, n â‰¤ m(mâˆ’1)
6
exists an STS(m).
Hence for d = 1, x = 2, and fan-out three, an X-code from
any STS(v) has the maximum compaction ratio (v âˆ’ 1)/6.
One may ask for larger error detectability of an (m, n, 1, 2)
X-code when one (or zero) unknown logic value is assumed.
An (m, n, d, x) X-code is also an (m, n, d + 1, x âˆ’ 1) X-code,
and hence any (m, n, 1, 2) X-code from an STS(m) is also an
(m, n, 2, 1) X-code. However, a careful choice of Steiner triple
systems gives higher error detectability while maintaining the
compaction ratio.
A configuration C in a triple packing, (V, B), is a subset
C âŠ† B. The set of points appearing in at least one block of a
configuration C is denoted by V (C). Two configurations C and
C â€² are isomorphic, denoted C âˆ¼
= C â€² , if there exists a bijection
â€²
Ï† : V (C) â†’ V (C ) such that for each block B âˆˆ C, the image
Ï†(B) is a block in C â€² . When |C| = i, a configuration C is an
i-configuration. A configuration C is even if for every point

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

a appearing in C the number |{B : a âˆˆ B âˆˆ C}| of blocks
containing a is even. Because every block in a triple packing
has three points, no i-configuration for i odd is even.
A triple packing is r-even-free if for every integer i satisfying 1 â‰¤ i â‰¤ r it contains no even i-configurations. By
definition every r-even-free triple packing, r â‰¥ 2, is also
(r âˆ’ 1)-even-free. For an even integer r, an r-even-free triple
packing is also (r + 1)-even-free. Every triple packing is
trivially 3-even-free. For v > 3 an STS(v) may or may not be
4-even-free. Up to isomorphism, the only even 4-configuration
is the Pasch configuration. It can be written on six points
and four blocks: {{a, b, c}, {a, d, e}, {f, b, d}, {f, c, e}}. For
the list of all the small configurations in a triple packing and
more complete treatments, we refer the reader to [34] and [36].
Because a 4-even-free STS is 5-even-free, an STS is 5-evenfree if and only if it contains no Pasch configuration.
Lemma 3.5: If there exists a 5-even-free STS(v), there
exists a (v, v(v âˆ’ 1)/6, 3, 1) X-code of constant weight three.
The code is a (v, v(v âˆ’ 1)/6, 5, 0) X-code of constant weight
three.
Proof: Let (V, B) be a 5-even-free STS(v). For every
Bi âˆˆ B define a v-dimensional vector si such that each
(i)
coordinate sj âˆˆ si is indexed by a distinct point j âˆˆ V
(i)
and sj = 1 if j âˆˆ Bi , otherwise 0. Then we obtain a
(v, v(v âˆ’ 1)/6, 1, 2) X-code S = {si : Bi âˆˆ B} of constant
weight three. We prove that S is a (v, v(v âˆ’ 1)/6, 3, 1) X-code
that is also a (v, v(v âˆ’ 1)/6, 5, 0) X-code. By definition, for
1 â‰¤ i â‰¤ 5 no i-configuration C âŠ† B is even. Hence
M
{si : Bi âˆˆ C} =
6 0.
This implies that S is a (v, v(v âˆ’ 1)/6, 5, 0) X-code. On the
other hand, since no pair of points appears twice, for any
mutually distinct blocks Bi , Bj , Bk âˆˆ B,
si 6= sj and si âˆ¨ (sj âŠ• sk ) 6= si .
It remains to show that no codeword in S covers addition
of three others. Suppose to the contrary that there exist four
distinct codewords si , sj , sk , and sl such that
si âˆ¨ (sj âŠ• sk âŠ• sl ) = si .
Because no pair of points appears twice and every block has
exactly three points, the only possible case is that the 4configuration {Bi , Bj , Bk , Bl } forms a Pasch configuration,
and hence it is even, a contradiction.
Steiner triple systems avoiding Pasch configurations have
been long studied as anti-Pasch STSs [34].
Theorem 3.6: [37] There exists a 5-even-free STS(v) if and
only if v â‰¡ 1, 3 (mod 6) and v 6âˆˆ {7, 13}.
By combining Theorem 3.6 and Lemma 3.5, we obtain:
Theorem 3.7: For every v â‰¡ 1, 3 (mod 6) and v 6âˆˆ {7, 13},
there exists a (v, v(v âˆ’ 1)/6, 1, 2) X-code of constant weight
three that is a (v, v(v âˆ’ 1)/6, 3, 1) X-code and a (v, v(v âˆ’
1)/6, 5, 0) X-code.
An X-compactor designed from these can detect any odd
number of errors unless there is an unknown logic value. One
may want to take advantage of the high compaction ratio of
the optimal (m, n, 1, 2) X-codes arising from 4-even-free STSs

5

when there is only a small possibility that more than two Xs
occur or multiple errors happen with multiple Xs. Our X-codes
from 4-even-free STSs also have high performance in such
situations:
Theorem 3.8: The probability that a (v, v(v âˆ’ 1)/6, 1, 2) Xcode from a 4-even-free STS(v) fails to detect a single error
162(vâˆ’3)2
when there are exactly three Xs is (v+2)(v+3)(vâˆ’4)(v
2 âˆ’vâˆ’18) .
Proof: Because there is only one error, an X-code fails
to detect this error when all three points in the block that
corresponds to the error are contained in at least one block
corresponding to an X. The number of occurrences of each 4configuration in an STS(v) is determined by v and the number
of Pasch configurations (see [34], for example). A simple
calculation proves the assertion.
Theorem 3.9: The probability that a (v, v(v âˆ’ 1)/6, 1, 2)
X-code from a 4-even-free STS(v) fails to detect errors
when there are exactly two Xs and exactly two errors is
1296
(v+2)(v+3)(vâˆ’4)(v 2 âˆ’vâˆ’18) .
Proof: A (v, v(v âˆ’ 1)/6, 1, 2) X-code from a 4-evenfree STS(v) fails to detect errors when there are exactly two Xs and exactly two errors only when corresponding four blocks form a 4-configuration isomorphic to
{{a, b, c}, {d, e, f }, {a, e, g}, {c, f, g}} where the first two
blocks represent Xs and the other two blocks correspond to
errors. The number of occurrences of the 4-configuration in
a 4-even-free STS(v) is v(vâˆ’1)(vâˆ’3)
, and the total number of
4
v(vâˆ’1) 
6
occurrences of all 4-configurations is
[34]. Divide
4
 v(vâˆ’1) 
v(vâˆ’1)(vâˆ’3)
4
6
to obtain the probability that the
by 2
4
4
X-code fails to detect the two errors.
Hence when a 4-even-free STS of sufficiently large order
is used, the probability that the corresponding X-code fails to
detect errors when the sum of the numbers of errors and Xs
is at most four is close to zero. A more complicated counting
argument is necessary to calculate the performance of X-codes
from STSs when the sum of the numbers of errors and Xs is
greater than four. For more complete treatments and current
research results on counting configurations in Steiner triple
systems, we refer the reader to [36] and references therein.
Useful explicit constructions for 5-even-free STS(v) can be
found in [34], [37]â€“[41]. The cyclic 5-sparse Steiner triple
systems in [42] provide examples of 5-even-free STS(v) for
v â‰¤ 97, because cyclic 5-sparse systems are all anti-Pasch.
Further r-even-freeness improves the error detectability of the
resulting X-code:
Theorem 3.10: For r â‰¥ 4, if there exists an r-even-free
triple packing (V, B), there exists a (|V |, |B|, 1, 2) X-code of
constant weight three that is also a (|V |, |B|, 3, 1) X-code and
a (|V |, |B|, r, 0) X-code.
Proof: Let (V, B) be an r-even-free triple packing of order
v. For every Bi âˆˆ B define a v-dimensional vector si such
(i)
that each coordinate sj âˆˆ si is indexed by a distinct point
(i)
j âˆˆ V and sj = 1 if j âˆˆ Bi , otherwise 0. Then we obtain a
(|V |, |B|, 1, 2) X-code S = {si : Bi âˆˆ B} of constant weight
three. It suffices to prove that S forms a (|V |, |B|, r, 0) X-code.
Suppose to the contrary that S is not a (|V |, |B|, r, 0) X-code.
Then for some râ€² â‰¤ r there exists a set of râ€² codewords si ,

6

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

sj . . . , sk such that
si âŠ• sj Â· Â· Â· âŠ• sk = 0.
However, the set of the corresponding blocks Bi , Bj ,. . . ,Bk
forms an even râ€² -configuration, a contradiction.
One may want an r-even-free STS with large r to obtain
higher error detection ability while keeping the maximum
compaction ratio. Although it is known that every Steiner triple
system has a configuration with seven or fewer blocks so that
every element of the configuration belongs to at least two [36],
it may happen that none of these are even. Nevertheless, the
following gives an upper bound of even-freeness of Steiner
triple systems.
Theorem 3.11: For v > 3 there exists no 8-even-free
STS(v).
Proof: Suppose to the contrary that there exists an
STS(v), S, that is 8-even free. Consider a 4-configuration
C isomorphic to {{a, b, e}, {c, d, e}, {a, c, f }, {b, d, g}}; the
points f and g are each contained in exactly one block.
For any anti-Pasch STS(v) the number of occurrences of
configurations isomorphic to C is v(v âˆ’ 1)(v âˆ’ 3)/4 [43] (see

also [44]). Because v â‰¥ 7, we have v(v âˆ’ 1)(v âˆ’ 3)/4 > v2 .
Hence there is a pair of configurations A and B such that
A âˆ¼
= C and they share the two points contained in
= B âˆ¼
exactly one block. In other words, there exists a pair A and
B having the form {{a, b, e}, {c, d, e}, {a, c, f }, {b, d, g}} and
{{aâ€² , bâ€² , eâ€² }, {câ€² , dâ€² , eâ€² }, {aâ€² , câ€² , f }, {bâ€² , dâ€² , g}} respectively. If
there is no common block between A and B, then the merged
configuration A âˆª B forms an even configuration consisting
of eight blocks, a contradiction. Otherwise, there is at least
one block contained in both A and B. Removing blocks
shared between A and B from their union, we obtain an even
configuration on four or six blocks, a contradiction.
By combining Theorems 3.4, 3.10, and 3.11, we have:
Theorem 3.12: There exists no (m, n, 1, 2) X-code that
achieves the maximum compaction ratio (m âˆ’ 1)/6 and is
also an (m, n, 3, 1) X-code and an (m, n, 8, 0) X-code.
An STS is 7-even-free if and only if it is 6-evenfree. Up to isomorphism, there are two kinds of even 6configurations which may appear in an STS. One is called
the grid and the other is the double triangle. Both 6configurations are described by nine points and six blocks:
{{a, b, c}, {d, e, f }, {g, h, i}, {a, d, g}, {b, e, h}, {c, f, i}} and
{{a, b, c}, {a, d, e}, {c, f, e}, {b, g, h}, {d, h, i}, {f, g, i}} respectively. By definition, an STS is 6-even-free if it simultaneously avoids Pasches, grids, and double triangles. We do
not know whether there exists a 6-even-free STS(v) for any
v > 3. However, a moderately large number of triples can be
included while keeping 6-even-freeness:
Theorem 3.13: There exists a constant c > 0 such that for
sufficiently large v there exists a 6-even-free triple packing of
order v with cv 1.8 triples.
Proof: Let C â€² be a set of representatives of all of the
nonisomorphic even configurations on six or fewer triples and
let C â€²â€² be a configuration consisting of pair of distinct triples
sharing a pair of elements. Let C = C â€² âˆª C â€²â€² . Pick uniformly
6
at random triples from V with probability p = câ€² v âˆ’ 5 inde1
10
â€²
â€²
pendently, where c satisfies 0 < c < ( 41Â·79Â·83 ) 5 . Let bC be

a random variable counting the configurations isomorphic to
a member of C in the resulting set of triples. Define E(bC ) as
its expected value. Then

E(bC ) â‰¤
=

  4
  6
  9
v
v
v
2
4
3
3
3
p +
p +
p6
2
4
4
6
6
9
 9 â€²6 1.8
c v
3
+ f (v),
9!
6

where f (v) = O(v 1.6 ). By Markovâ€™s Inequality,
P (bC â‰¥ 2E(bC )) â‰¤

1
.
2

Hence,


 9 â€²6 1.8
1
c v
3
+ 2f (v) â‰¥ .
P bC â‰¤ 2
9!
2
6
Let t be a random variable counting the triples and E(t) its
expected value. Then
 
câ€²
v
E(t) = p
= v 1.8 âˆ’ g(v),
6
3
where g(v) = O(v 0.8 ). Because t is a binomial random
variable, by Chernoffâ€™s inequality, for sufficiently large v


E(t)
1
E(t)
< eâˆ’ 8 < .
P t<
2
2
Hence, if v is sufficiently large, then with positive probability we have a set B of triples with the property that |B| > E(t)
2
and the number of configurations in B isomorphic to a member
of C is at most
 9 â€²6 1.8
c v
2 3
+ 2f (v).
6
9!
Let ex(v, r) be the maximum cardinality |B| such that there
exists an r-even-free triple packing. By deleting a triple from
each configuration isomorphic to a member of C, we obtain
 9 â€²6 1.8
c v
E(t)
âˆ’2 3
+ h(v),
ex(v, 6) â‰¥
2
9!
6
where h(v) = O(v 1.6 ). Then for some positive constant c and
sufficiently large v, it holds that ex(v, 6) â‰¥ cv 1.8 .
Hence we have:
Theorem 3.14: There exists a constant c > 0 such that for
sufficiently large v there exists a (v, cv 1.8 , 1, 2) X-code that is
also a (v, cv 1.8 , 3, 1) X-code and a (v, cv 1.8 , 6, 0) X-code.
An STS(v) has approximately v 2 /6 triples. The same technique can be used to obtain a lower bound on ex(v, r) for
12
r â‰¥ 8. In fact, ex(v, 8) is at least O(v 7 ), and hence for
sufficiently large v there exists a constant c > 0 such that
12
12
there exists a (v, cv 7 , 1, 2) X-code that is also a (v, cv 7 , 3, 1)
12
X-code and a (v, cv 7 , 8, 0) X-code.

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

C. Higher X-Tolerance with the Minimum Fan-Out
In general, the probability that a defective digital circuit
produces an error at a specific signal output line is quite small.
In fact, several errors are unlikely happen simultaneously [11],
[45]. Also, multiple Xs with errors are rare [12]. Therefore, Xcodes given in Theorems 3.7 and 3.13 are particularly useful
for relatively simple scan-based testing such as built-in selftest (BIST) where the tester is only required to detect defective
chips. Nonetheless, more sophisticated X-codes are also useful
to improve test quality and/or to identify or narrow down the
error sources by taking advantage of more detailed information
about when incorrect responses are produced [12]. Hence, for
use in higher quality testing and error diagnosis support, it
is of theoretical and practical interest to consider (m, n, d, x)
X-codes of constant weight x + 1, where x â‰¥ 3 or d â‰¥ 6.
For (m, n, 1, 2) X-codes of constant weight three, we employed Theorem 3.3 to obtain an upper bound on the number
of codewords. The following theorem gives a generalized
upper bound:
Theorem 3.15: [46] Let n(x, m, k) denote the maximum
number of columns of a (1, x)-superimposed code such that
every column is of length m and has constant weight k. Then,
for every x, t and i = 0, 1 or i â‰¤ x/2t2 ,

 

mâˆ’i
kâˆ’i
n(x, m, x(t âˆ’ 1) + 1 + i) â‰¤
/
t
t
for all sufficiently large m, with equality if and only if there
exists a Steiner t-design S(t, x(t âˆ’ 1) + 1, m âˆ’ i).
By putting t = 2 and i = 0, we obtain:
Corollary 3.16: For an (m, n, 1, x) X-code of constant
weight x + 1,
  

m
x+1
nâ‰¤
/
2
2
for all sufficiently large m, with equality if and only if there
is an S(2, x + 1, m).
Because the set of columns of the block-point incidence
matrix of any S(2, x + 1, m) forms an (m, m(m âˆ’ 1)/x(x +
1), 1, x) X-code of constant weight x + 1, the existence of
Steiner 2-designs is our next interest. For k âˆˆ {4, 5}, necessary
and sufficient conditions for existence of an S(2, k, v) are
known:
Theorem 3.17: [47] There exists an S(2, 4, v) if and only
if v â‰¡ 1, 4 (mod 12).
Theorem 3.18: [48] There exists an S(2, 5, v) if and only
if v â‰¡ 1, 5 (mod 20).
For k â‰¥ 6, the necessary and sufficient conditions on v
for existence of an S(2, k, v) are not known in general; the
existence of a Steiner 2-design is solved only in an asymptotic
sense [49], although for â€˜smallâ€™ values of k substantial results
are known. For a comprehensive table of known Steiner 2designs, see [50].
As with X-codes from Steiner triple systems, the error
detectability can be improved by considering avoidance of
even configurations.
An S(2, k, v), (V, B), is r-even-free if for 1 â‰¤ i â‰¤ r
it contains no subset C âŠ† B such that |C| = i and each
point appearing in C is contained in exactly an even number

7

of blocks in C. A generalized Pasch configuration in an
S(2, k, v), (V, B), is a subset C âŠ‚ B such that |C| = k + 1 and
each point appearing in C is contained exactly two blocks of
C. As with triple systems, an S(2, k, v) is (k + 1)-even-free if
and only if it contains no generalized Pasch configurations.
Theorem 3.19: If an r-even-free S(2, k, v) for r â‰¥ k + 1
exists, there exists a (v, v(v âˆ’ 1)/k(k âˆ’ 1), 1, k âˆ’ 1) X-code
of constant weight k that is also a (v, v(v âˆ’ 1)/k(k âˆ’ 1), k, 1)
X-code and a (v, v(v âˆ’ 1)/k(k âˆ’ 1), r, 0) X-code.
Proof: Let (V, B) be an r-even-free S(2, k, v). For every
Bi âˆˆ B define a v-dimensional vector si such that each
(i)
coordinate sj âˆˆ si is indexed by a distinct point j âˆˆ V
(i)
and sj = 1 if j âˆˆ Bi , otherwise 0. Then we obtain a
(v, v(v âˆ’ 1)/k(k âˆ’ 1), 1, k âˆ’ 1) X-code S = {si : Bi âˆˆ B} of
constant weight k. By definition of an r-even-free S(2, k, v),
it is straightforward to see that S is also a (v, v(v âˆ’ 1)/k(k âˆ’
1), r, 0) X-code. It suffices to prove that S can also be used
as a (v, v(v âˆ’ 1)/k(k âˆ’ 1), k, 1) X-code. Assume that this is
not the case. Then, by following the argument in the proof of
Lemma 3.5, B contains a generalized Pasch configuration, a
contradiction.
Existence of an r-even-free design has been investigated
in the study of erasure-resilient codes for redundant array of
independent disks (RAID) [51]. In fact, infinitely many r-evenfree S(2, k, v)s can be obtained from affine spaces over Fq
[52].
Theorem 3.20: [52] For any odd prime power q and positive
integer n â‰¥ 2 the points and lines of AG(n, q) form a (2qâˆ’1)even-free S(2, q, q n ).
By combining Theorems 3.19 and 3.20, we obtain:
Theorem 3.21: For any odd prime power q and positive
integer n â‰¥ 2, there exists a (q n , q nâˆ’1 (q n âˆ’1)/(qâˆ’1), 1, qâˆ’1)
X-code of constant weight q that is also an (q n , q nâˆ’1 (q n âˆ’
1)/(q âˆ’ 1), q, 1) X-code and a (q n , q nâˆ’1 (q n âˆ’ 1)/(q âˆ’ 1), 2q âˆ’
1, 0) X-code.
D. Characteristics of X-Codes from Combinatorial Designs
We have given tight upper bounds of compaction ratio for
(m, n, 1, x) X-codes with the minimum fan-out and presented
explicit construction methods for X-codes that attain the
bounds. As far as the authors are aware, these are the first
mathematical bounds and construction techniques for this type
of optimal X-code with constant weight greater than two.
Optimal X-codes given in Theorems 3.7 and 3.21 in particular
have higher error detection ability when the number of Xs
is smaller than x. The known construction technique using
hypergraphs, briefly mentioned in [16], can not guarantee the
same error detection ability.
To illustrate the usefulness of our X-codes, here we compare
the error detection ability of an example X-code that can
be generated using Theorem 3.7 with characteristics of Xcodes proposed in [11]. The probability that the example
(50, 500, 1, 1) X-code in Table 5 in [11] fails to detect a single
error when there are exactly two Xs is around 4.2 Ã— 10âˆ’6 .
The fan-out of this code is 11. Our X-code from Theorem
3.7, which has the same compaction ratio, has parameters
(61, 610, 1, 2). The probability that this X-code fails to detect

8

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

a single error in the same situation is exactly 0. Its fan-out
is 3, which is significantly smaller. While the multiple error
detection ability of the (50, 500, 1, 1) X-code is not specified
in [11], our code can always detect up to three errors when
there is only one X, and up to five errors when there is no X.
By Theorem 3.9 the probability that our (61, 610, 1, 2) X-code
fails to detect errors when there are exactly two Xs and two
errors is 1.5 Ã— 10âˆ’6 . Therefore, our X-code is ideal when the
fan-out problem is critical and/or fault-free simulation rarely
produces three or more Xs in an expected response.
Very large optimal X-codes with very high error detecting
ability and compaction ratio can be easily constructed by the
same method. For example, Theorem 3.7 and known results
on anti-Pasch STSs immediately give a (601, 60100, 1, 2) Xcode with fan-out 3 and compaction ratio 100. This code is
also a (601, 60100, 3, 1) X-code and a (601, 60100, 5, 0) Xcode. Moreover, the probability that it fails to detect errors
when there are exactly two Xs and two errors (or exactly
three Xs and a single error) is around 1.6 Ã— 10âˆ’11 (or 7.3âˆ’7
respectively). As far as the authors know, there have been no
X-codes available that guarantee as high error detection ability
and have very small fan-out.
As Theorems 3.7, 3.8, and 3.9 indicate, larger X-codes
designed with this method have an even higher compaction
ratio and better error detection rate. Because discarding codewords does not affect error detection ability, one may use
part of a large X-code to achieve very high test quality when
compaction ratio can be compromised to an extent.
IV. X-C ODES

OF

A RBITRARY W EIGHT

The restriction to low-weight codewords severely limits the
compaction ratio of an X-code. Hence, when fan-in and fanout are not of concern, it is desirable to use X-codes with
arbitrary weight. In this section we study the compaction ratio
and construction methods of such general X-codes.
For d = x = 2, a (âŒˆlog2 nâŒ‰(âŒˆlog2 nâŒ‰ + 1), n, 2, 2) X-code
was constructed for any integer n â‰¥ 2 [14].
Theorem 4.1: [14] For any optimal (m, n, 2, 2) X-code,
m â‰¤ âŒˆlog2 nâŒ‰(âŒˆlog2 nâŒ‰ + 1).
They also gave an explicit construction method of a
(3âŒˆlog3 nâŒ‰, n, 1, 3) X-code. In order to give a more general
construction, we employ design theoretic techniques for arrays. Let n â‰¥ w â‰¥ 2. A perfect hash family, PHF(N ; u, n, w),
is a set F of N functions f : Y â†’ X where |Y | = u and
|X| = n, such that, for any C âŠ† Y with |C| = w, there exists
at least one function f âˆˆ F such that f |C is one-to-one. A
PHF(N ; u, n, w) can be described by a u Ã— N matrix with
entries from a set of n symbols such that for any w rows there
exists at least one column in which each element is distinct.
Theorem 4.2: If an (m, n, d, x) X-code and a
PHF(N ; u, n, max{d, x} + 1) exist, there exists an
(mN, u, d, x) X-code.
Proof: Let H be a u Ã— N n-ary matrix representing
a PHF(N ; u, n, max{d, x} + 1). Assign each codeword of
an (m, n, d, x) X-code to a distinct symbol of the PHF and
replace each entry of H by the m-dimensional row vector
representing the assigned codeword. Then we obtain a uÃ—mN

binary matrix H â€² . Taking each row of H â€² as a codeword,
we obtain a set X of mN -dimensional vectors. It suffices to
show that for any two arbitrary subsets D, X âŠ† X satisfying
|D| = dâ€² â‰¤ d, |X| = xâ€² â‰¤ x, and D âˆ© X = âˆ…, it holds that
_
M
_
( X) âˆ¨ (
D) 6=
X.
(3)

By considering a one-to-one function in the PHF, for any
max{d, x} + 1 codewords of X at least one set of m
coordinates forms max{d, x} + 1 distinct codewords of the
original (m, n, d, x) X-code. Hence, for any choice of D
and X there exists a subset Y âŠ† X of cardinality |Y | =
max{0, dâ€² + xâ€² âˆ’ (max{d, x} + 1)} such that at least one set
of m coordinates in D âˆª (X \ Y ) forms distinct codewords of
the original (m, n, d, x) X-code. Because |Y | â‰¤ dâ€² âˆ’ 1 < |D|,
(3) holds for any D and X. Hence, the resulting set X forms
an (mN, u, d, x) X-code.
Since their introduction in [53], much progress has been
made on existence and construction techniques for perfect hash
families (see [54]â€“[58] for recent results). A concise list of
known results on perfect hash families is available in [50].
We can use perfect hash families from algebraic curves over
finite fields:
Theorem 4.3: [59] For positive integers n â‰¥ w, there
exists an explicit construction for an infinite family of
PHF(N ; u, n, w) such that N is O(log u).
Indeed when n is fixed, a perfect hash family with O(log u)
rows can be determined in polynomial time by a greedy
method [60].
By combining Theorems 4.2 and 4.3, we can construct
infinitely many (m, n, d, x) X-codes where m is O(log n).
Theorem 4.4: For any positive integer d and nonnegative
integer x, there exists an explicit construction for an infinite
family of (m, n, d, x) X-codes, where m is O(log n).
The following is a combinatorial recursion for X-codes.
Theorem
  4.5: If an (m, n, d, x) X-code and an
(â„“, n, d2 , x) X-code exist, there exists an (â„“ + m, 2n, d, x)
X-code.
Proof: Let X = {s1 , . . . , sn }be an (m, n, d, x) X-code
and Y = {t1 , . . . , tn } an (â„“, n, d2 , x) X-code. Extend each
(i)
(i)
codeword si = (s1 , . . . , sm ) of X by appending â„“ 0â€™s so that
(i)
(i)
extended vectors have the form sâ€²i = (s1 , . . . , sm , 0, . . . , 0).
(i)
(i)
Extend each codeword ti = (t1 , . . . , tl ) of Y by combining si so that extended vectors have the form tâ€²i =
(i)
(i) (i)
(i)
(s1 , . . . , sm , t1 , . . . , tl ). Define A = {sâ€²1 , . . . , sâ€²n }, B =
{tâ€²1 , . . . , tâ€²n }, and C = A âˆª B. We prove that C is an
(â„“ + m, 2n, d, x) X-code.
Take two subsets D, X âŠ† C satisfying |D| = dâ€² â‰¤ d,
|X| = xâ€² â‰¤ x, and Dâˆ©X = âˆ…. As in the proof of Theorem 4.2,
it suffices to show that for any choice of D and X the vector
obtained by adding all the codewords in D is not covered by
the superimposed sum of X, that is, (3) holds. Define a sur(i)
(i)
(i)
(i)
jection f of C to X as f : (c1 , . . . , câ„“+m ) 7â†’ (c1 , . . . , cm ).
Mapping all codewords of C under f generates two copies
of X ; one is from A and the other is from B. Define a
(i)
(i)
surjection g of C to Y âˆª {0} as g : (c1 , . . . , câ„“+m ) 7â†’
(i)
(cm + 1(i) , . . . , câ„“+m ). By definition, {g(c) : c âˆˆ B} = Y
and for any c âˆˆ A the image g(c) is an â„“-dimensional zero

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

vector. Let
 a = |D âˆ© A| and b =
 |D âˆ© B|. Because Y is an
(â„“, n, d2 , x) X-code, if b â‰¤ d2 ,
_
M
_
g( X) âˆ¨ g(
D) 6= g( X).
(4)
d
Hence, we only need to consider the case when b > 2 .
Suppose to the contrary that (3) does not hold. Then,
_
M
_
f ( X) âˆ¨ f (
D) = f ( X).
(5)

Let

aâ€² = |{c âˆˆ X : f (c) = f (d), d âˆˆ D âˆ© B}|
and
aâ€²â€² = |{c âˆˆ D âˆ© A : f (c) = f (d), d âˆˆ D âˆ© B}|.
Because {f (c) : c âˆˆ A} = {f (c) : c âˆˆ B} = X and (5)
holds, b = aâ€² + aâ€²â€² . As a + b = dâ€² and b > d2 ,
b

â€²
â‰¤ a
 +a
d
â‰¤
+ aâ€² .
2

(6)

On the
hand, |X âˆ© B| â‰¤ x âˆ’ aâ€² . Because Y is also an
 dother

â€²
(â„“, n, 2 + a , x âˆ’ aâ€² ) X-code, (4) holds, a contradiction.
Next, we present a simple nonconstructive existence result
for (m, n, d, x) X-codes.
Theorem 4.6: Let d, x be a positive integers. For n â‰¥
max{2d, d + x}, if
m â‰¥ 2x+1 (d + x) log n,
there exists an (m, n, d, x) X-code.
Proof: Let X = {s1 , s2 , . . . , sn } be a set of n m(i) (i)
(i)
dimensional vectors si = (s1 , s2 , . . . , sm , ) in which each
(i)
entry sj is defined to be 1 with probability p = 1/2. Let X
be a set of x vectors of X and Di a set of i vectors in X \ X.
Define
A(Di , X) =

(

W
L
W
0 if ( X) âˆ¨ ( Di ) 6= X,
1 otherwise,

and let E(A(Di , X)) be its expected value. Then

E(A(Di , X)) =
=
Let
AX =

ï£«

ï£¬
âˆ’x
ï£­1 âˆ’ 2

j

odd

(1 âˆ’ 2âˆ’xâˆ’1 )m .

d
X X
X

XâŠ†X
|X|=x

ï£¶m
X  i ï£·
2âˆ’i ï£¸
j
1â‰¤jâ‰¤i

i=1

A(Di , X)

Di
Di âˆªX=âˆ…

=

d
X X
X

XâŠ†X
|X|=x

i=1

If E(AX ) < 1, there exists an (m, n, d, x) X-code. Taking
logarithms,
m>

âˆ’(d + x) log n
.
log (1 âˆ’ 2âˆ’xâˆ’1 )

Hence, if
m â‰¥ 2x+1 (d + x) log n >

E(A(Di , X))

Di
Di âˆªX=âˆ…


d  
X
n nâˆ’x
(1 âˆ’ 2âˆ’xâˆ’1 )m
=
x
i
i=1

< nd+x (1 âˆ’ 2âˆ’xâˆ’1 )m .

âˆ’(d + x) log n
,
log (1 âˆ’ 2âˆ’xâˆ’1 )

there exists an (m, n, d, x) X-code.
Hence, for any optimal (m, n, d, x) X-code with n â‰¥
max{2d, d + x}, m is at most O(log n). For example, by
putting d = x = 2 we know that there exists an (m, n, d, x)
X-code if m â‰¥ 32 log n. This significantly improves the upper
bound in Theorem 4.1 proved in [14].
V. C ONCLUSIONS
By formulating X-tolerant space compaction of test responses combinatorially, an equivalent, alternative definition
of X-codes has been introduced. This combinatorial approach
gives general design methods for X-codes and bounds on
the compaction ratio. Using this model with restricted fanout leads to well-studied objects, the Steiner 2-designs. These
provide constructions for X-codes having sufficient error
detectability, X-tolerance, maximum compaction ratio, and
minimum fan-out. Constant weight X-codes with high error
detectability profit from a deep connection with configurations, particularly the Pasch configuration. The combinatorial
formulation of X-tolerant compaction can also be applied in
conjunction with another compaction technique (such as time
compaction). If a tester wants an X-compactor with additional
properties, the necessary structure of the compactor may be
expressed in design theoretic terms.
Our formulation can also be useful for the study of higher
error detectability and error diagnosis support employing the
appropriate assistance from an Automatic Test Equipment
(ATE) [12]. For example, the compaction technique called iCompact can be understood in terms of the model in Section
II [17].
The essential idea underlying Theorem 4.6 is the stochastic
coding technique for X-tolerant signature analysis [61]. We
used a naive value 1/2 as the probability p in the proof of
Theorem 4.6. To obtain a better constant coefficient, p should
be chosen so that it minimizes the expected value E(AX ), that
is, it should minimize
ï£¶m
ï£«




d
X i
X nâˆ’x ï£¬
ï£·
pj (1 âˆ’ p)iâˆ’j+x ï£¸ .
ï£­1 âˆ’
i
j
1â‰¤jâ‰¤i
i=1
j

and E(AX ) its expected value. Then

E(AX )

9

odd

While this optimization does not affect the logarithmic order
in Theorem 4.6, it may help a tester determine the target compaction ratio and estimate the error cancellation and masking
rate of an X-tolerant Multiple Input Signature Register (XMISR) based on stochastic coding [61].
In this paper we focused on space compaction. Nevertheless,
time compaction is of great importance as well. We expect the
combinatorial formulation developed here to provide a useful
framework for exploring time compaction as well.

10

IEEE TRANSACTIONS ON INFORMATION THEORY, VOL. X, NO. XX, MONTH YEAR

ACKNOWLEDGMENT
A substantial part of the research was done while the
first author was visiting the Department of Computer Science
and Engineering of Arizona State University. He thanks the
department for its hospitality. The authors thank an anonymous
referee and the editor for helpful comments and valuable
suggestions.
R EFERENCES
[1] E. J. McCluskey, D. Burek, B. Koenemann, S. Mitra, J. H. Patel,
J. Rajski, and J. A. Waicukauski, â€œTest compression roundtable,â€ IEEE
Des. Test. Comput., vol. 20, pp. 76â€“87, Mar./Apr. 2003.
[2] A. Lempel and M. Cohn, â€œDesign of universal test sequences for VLSI,â€
IEEE Trans. Inf. Theory, vol. 31, pp. 10â€“17, Jan. 1985.
[3] G. Seroussi and N. H. Bshouty, â€œVector sets for exhaustive testing of
logic circuits,â€ IEEE Trans. Inf. Theory, vol. 34, pp. 513â€“522, May 1988.
[4] H. Hollmann, â€œDesign of test sequences for VLSI self-testing using
LFSR,â€ IEEE Trans. Inf. Theory, vol. 36, pp. 386â€“392, Mar. 1990.
[5] G. D. Cohen and G. Zemor, â€œIntersecting codes and independent
families,â€ IEEE Trans. Inf. Theory, vol. 40, pp. 1872â€“1881, Nov. 1994.
[6] N. Benowitz, D. F. Calhoun, G. E. Alderson, J. E. Bauer, and C. T.
Joeckel, â€œAn advanced fault isolation system for digital logic,â€ IEEE
Trans. Comput., vol. C-24, pp. 489â€“497, May 1975.
[7] E. J. McCluskey, Logic Design Principles with Emphasis on Testable
Semi-Custom Circuits. Englewood Cliffs, NJ: Prentice-Hall, 1986.
[8] N. R. Saxena and E. J. McCluskey, â€œParallel signature analysis design
with bounds on aliasing,â€ IEEE Trans. Comput., vol. 46, pp. 425â€“438,
Apr. 1997.
[9] C. Barnhart, V. Brunkhorst, F. Distler, O. Farnsworth, B. Keller, and
B. Koenemann, â€œOPMISR: The foundation for compressed ATPG vectors,â€ in Proc. Int. Test Conf., 2001, pp. 748â€“757.
[10] C. Barnhart, V. Brunkhorst, F. Distler, O. Farnsworth, A. Ferko,
B. Keller, D. Scott, B. Koenemann, and T. Onodera, â€œExtending OPMISR beyond 10x scan test efficiency,â€ IEEE Design Test Comput.,
vol. 19, pp. 65â€“73, Sep. 2002.
[11] S. Mitra, S. S. Lumetta, M. Mitzenmacher, and N. Patil, â€œX-tolerant test
response compaction,â€ IEEE Des. Test. Comput., vol. 22, pp. 566â€“574,
Nov. 2005.
[12] S. Mitra and K. S. Kim, â€œX-compact: An efficient response compaction
technique,â€ IEEE Trans. Comput.-Aided Design Integr. Circuits Syst.,
vol. 23, pp. 421â€“432, Mar. 2004.
[13] S. Mitra, S. Kallepalli, and K. S. Kim, â€œAnalysis of X-compact for
industrial designs,â€ Intel Corp., 2003.
[14] S. S. Lumetta and S. Mitra, â€œX-codes: Theory and applications of
unknowable inputs,â€ Center for Reliable and High-Performance Computing, Univ. of Illinois at Urbana Champaign, Tech. Rep. CRHC-03-08
(also UILU-ENG-03-2217), Aug. 2003.
[15] â€”â€”, â€œX-codes: Error control with unknowable inputs,â€ in Proc. IEEE
Intl. Symp. Information Theory, Yokohama, Japan, June 2003, p. 102.
[16] P. Wohl and L. Huisman, â€œAnalysis and design of optimal combinational
compactors,â€ in Proc. 21st IEEE VLSI Test Symp., April/May 2003, pp.
101â€“106.
[17] J. H. Patel, S. S. Lumetta, and S. M. Reddy, â€œApplication of SalujaKarpovsky compactors to test responses with many unknowns,â€ in Proc.
21st IEEE VLSI Test Symp., 2003, pp. 107â€“112.
[18] T. R. N. Rao and E. Fujiwara, Error-Control Coding for Computer
Systems. Englewood Cliffs, NJ: Prentice-Hall, 1989.
[19] K. K. Saluja and M. Karpovsky, â€œTesting computer hardware through
data compression in space and time,â€ in Proc. Int. Test Conf., 1983, pp.
83â€“93.
[20] W. H. Kautz and R. R. Singleton, â€œNonrandom binary superimposed
codes,â€ IEEE Trans. Inf. Theory, vol. 10, pp. 363â€“377, Jul. 1964.
[21] E. Sperner, â€œEin satz uÌˆber Untermengen einer endlichen Menge,â€ Math.
Z., vol. 27, pp. 544â€“548, 1928.
[22] D. R. Stinson and R. Wei, â€œSome new upper bounds for cover-free
families,â€ J. Combin. Theory Ser. A, vol. 90, pp. 224â€“234, 2000.
[23] A. G. Dâ€™yachkov and V. V. Rykov, â€œBounds on the length of disjunctive
codes,â€ Probl. Contr. Inform. Theory, vol. 11, pp. 7â€“33, 1982, in Russian.
[24] Z. FuÌˆredi, â€œOn r-cover-free families,â€ J. Combin. Theory, Ser. A, vol. 73,
pp. 172â€“173, 1996.
[25] M. RuszinkoÌ, â€œOn the upper bound of the size of the r-cover-free
families,â€ J. Combin. Theory, Ser. A, vol. 66, pp. 302â€“310, 1994.

[26] A. G. Dâ€™yachkov, V. V. Rykov, and A. M. Rashad, â€œSuperimposed
distance codes,â€ Probl. Contr. Inform. Theory, vol. 18, pp. 237â€“250,
1989.
[27] D. Z. Du and F. K. Hwang, Combinatorial Group Testing and Its
Applications, 2nd ed. Singapore: World Scientific, 2000.
[28] H. L. Fu and F. K. Hwang, â€œA novel use of t-packings to construct
d-disjunct matrices,â€ Discrete Appl. Math., vol. 154, pp. 1759â€“1762,
2006.
[29] A. G. Dâ€™yachkov, A. J. Macula, and V. V. Rykov, â€œNew constructions
of superimposed codes,â€ IEEE Trans. Inf. Theory, vol. 46, pp. 284â€“290,
Jan. 2000.
[30] A. J. Macula, â€œA simple construction of d-disjunct matrices with certain
constant weights,â€ Discrete Math., vol. 162, pp. 311â€“312, 1996.
[31] â€”â€”, â€œError-correcting nonadaptive group testing with de -disjunct matrices,â€ Discrete Appl. Math., vol. 80, pp. 217â€“222, 1997.
[32] H. G. Yeh, â€œd-Disjunct matrices: bounds and LovaÌsz Local Lemma,â€
Discrete Math., vol. 253, pp. 97â€“107, 2002.
[33] A. De Bonis and U. Vaccaro, â€œConstructions of generalized superimposed codes with applications to group testing and conflict resolution in
multiple access channels,â€ Theor. Comput. Sci., vol. 306, pp. 223â€“243,
2003.
[34] C. J. Colbourn and A. Rosa, Triple Systems. Oxford: Oxford Univ.
Press, 1999.
[35] P. ErdoÌ‹s, P. Frankl, and Z. FuÌˆredi, â€œFamilies of finite sets in which no
set is covered by the union of two others,â€ J. Combin. Theory, Ser. A,
vol. 33, pp. 158â€“166, 1982.
[36] C. J. Colbourn and Y. Fujiwara, â€œSmall stopping sets in Steiner triple
systems,â€ Cryptography and Communications, vol. 1, no. 1, pp. 31â€“46,
2009.
[37] M. J. Grannell, T. S. Griggs, and C. A. Whitehead, â€œThe resolution of
the anti-Pasch conjecture,â€ J. Combin. Des., vol. 8, pp. 300â€“309, 2000.
[38] A. C. H. Ling, C. J. Colbourn, M. J. Grannell, and T. S. Griggs,
â€œConstruction techniques for anti-Pasch Steiner triple systems,â€ J. Lond.
Math. Soc. (2), vol. 61, pp. 641â€“657, 2000.
[39] D. R. Stinson and Y. J. Wei, â€œSome results on quadrilaterals in Steiner
triple systems,â€ Discrete Math., vol. 105, pp. 207â€“219, 1992.
[40] M. J. Grannell, T. S. Griggs, and J. S. Phelan, â€œA new look at an old
construction for Steiner triple systems,â€ Ars Combinat., vol. 25A, pp.
55â€“60, 1988.
[41] A. E. Brouwer, â€œSteiner triple systems without forbidden subconfigurations,â€ Mathematisch Centrum Amsterdam, ZW 104/77, 1977.
[42] C. J. Colbourn, E. Mendelsohn, A. Rosa, and J. SÌŒiraÌnÌŒ, â€œAnti-Mitre
Steiner triple systems,â€ Graphs Combin., vol. 10, pp. 215â€“224, 1994.
[43] M. J. Grannell, T. S. Griggs, and E. Mendelsohn, â€œA small basis for fourline configurations in Steiner triple systems,â€ J. Combin. Des., vol. 3,
pp. 51â€“59, 1995.
[44] C. J. Colbourn, â€œThe configuration polytope of â„“-line configurations in
Steiner triple systems,â€ Mathematica Slovaca, vol. 59, no. 1, pp. 77â€“108,
2009.
[45] P. Wohl, J. A. Waicukauski, and T. W. Williams, â€œDesign of compactors
for signature-analyzers in built-in-self-test,â€ in Proc. Int. Test Conf.,
2001, pp. 54â€“63.
[46] P. ErdoÌ‹s, P. Frankl, and Z. FuÌˆredi, â€œFamilies of finite sets in which no
set is covered by the union of r others,â€ Israel J. Math., vol. 51, pp.
75â€“89, 1985.
[47] H. Hanani, â€œThe existence and construction of balanced imcomplete
block designs,â€ Ann. Math. Statist., vol. 32, pp. 361â€“386, 1961.
[48] â€”â€”, â€œOn balanced incomplete block designs with blocks having five
elements,â€ J. Combin. Theory Ser. A, vol. 12, pp. 184â€“201, 1972.
[49] R. M. Wilson, â€œAn existence theory for pairwise balanced designs. III.
Proof of the existence conjectures,â€ J. Combin. Theory Ser. A, vol. 18,
pp. 71â€“79, 1975.
[50] C. J. Colbourn and J. H. Dinitz, Eds., Handbook of Combinatorial
Designs. Boca Raton, FL: Chapman & Hall/CRC, 2007.
[51] Y. M. Chee, C. J. Colbourn, and A. C. H. Ling, â€œAsymptotically optimal
erasure-resilient codes for large disk arrays,â€ Discrete Appl. Math., vol.
102, pp. 3â€“36, 2000.
[52] M. MuÌˆller and M. Jimbo, â€œErasure-resilient codes from affine spaces,â€
Discrete Appl. Math., vol. 143, pp. 292â€“297, 2004.
[53] K. Mehlhorn, Data Structures and Algorithms 1. Berlin, Germany:
Springer, 1984.
[54] D. Tonien and R. Safavi-Naini, â€œRecursive constructions of secure codes
and hash families using difference function families,â€ J. Combin. Theory
Ser. A, vol. 113, pp. 664â€“674, 2006.
[55] Tran van Trung and S. S. Martirosyan, â€œNew constructions for IPP
codes,â€ Des. Codes Cryptgr., vol. 32, pp. 227â€“239, 2005.

FUJIWARA AND COLBOURN: A COMBINATORIAL APPROACH TO X-TOLERANT COMPACTION CIRCUITS

[56] D. Deng, D. R. Stinson, and R. Wei, â€œThe LovaÌsz local lemma and its
applications to some combinatorial arrays,â€ Des. Codes Cryptgr., vol. 32,
pp. 121â€“134, 2004.
[57] R. A. Walker II and C. J. Colbourn, â€œPerfect hash families: Construction
and existence,â€ Journal of Mathematical Cryptology, vol. 1, pp. 125â€“
150, 2007.
[58] S. S. Martirosyan and Tran van Trung, â€œExplicit constructions for perfect
hash families,â€ Des. Codes Cryptogr., vol. 46, no. 1, pp. 97â€“112, 2008.
[59] H. Wang and C. Xing, â€œExplicit constructions of perfect hash families
from algebraic curves over finite fields,â€ J. Combin. Theory Ser. A,
vol. 93, pp. 112â€“124, 2001.
[60] C. J. Colbourn, â€œConstructing perfect hash families using a greedy algorithm,â€ in Coding and Cryptology, Y. Li, S. Zhang, S. Ling, H. Wang,
C. Xing, and H. Niederreiter, Eds. Singapore: World Scientific, 2008.
[61] S. Mitra, S. S. Lumetta, and M. Mitzenmacher, â€œX-tolerant signature
analysis,â€ in Proc. Int. Test Conf., 2004, pp. 432â€“441.

11

ATLAS: Adaptive Topology- and Load-Aware
Scheduling

arXiv:1305.4897v2 [cs.NI] 4 Nov 2013

Jonathan Lutz, Charles J. Colbourn, and Violet R. Syrotiuk
CIDSE, Arizona State University, Tempe, AZ 85287-8809
Email: {jlutz, colbourn, syrotiuk}@asu.edu

Abstractâ€”The largest strength of contention-based MAC protocols is simultaneously the largest weakness of their scheduled counterparts: the ability to adapt to changes in network
conditions. For scheduling to be competitive in mobile wireless
networks, continuous adaptation must be addressed. We propose
ATLAS, an Adaptive Topology- and Load-Aware Scheduling
protocol to address this problem. In ATLAS, each node employs a
random schedule achieving its persistence, the fraction of time a
node is permitted to transmit, that is computed in a topology
and load dependent manner. A distributed auction (REACT)
piggybacks offers and claims onto existing network traffic to
compute a lexicographic max-min channel allocation. A nodeâ€™s
persistence p is related to its allocation. Its schedule achieving p
is updated where and when needed, without waiting for a frame
boundary. We study how ATLAS adapts to controlled changes
in topology and load. Our results show that ATLAS adapts to
most network changes in less than 0.1s, with about 20% relative
error, scaling with network size. We further study ATLAS in
more dynamic networks showing that it keeps up with changes
in topology and load sufficient for TCP to sustain multi-hop flows,
a struggle in IEEE 802.11 networks. The stable performance of
ATLAS supports the design of higher-layer services that inform,
and are informed by, the underlying communication network.
Index Termsâ€”Wireless networks, medium access control,
adaptation.

I. I NTRODUCTION
Despite the well known shortcomings of IEEE 802.11
and other contention-based MAC protocols for mobile wireless networksâ€”such as probabilistic delay guarantees, severe
short-term unfairness, and poor performance at high loadâ€”
they remain the access method of choice. The primary reason
is their ease in adapting to changes in network conditions,
specifically to changes in topology and in load. The lack
of timely adaptation is the most serious limitation facing
scheduled MAC protocols. For scheduling to be competitive,
continuous adaptation is required.
Topology-dependent approaches to adaptation in scheduling
alternate a contention phase with a scheduled phase. In the
contention phase, nodes exchange topology information used
to compute a conflict-free schedule that is followed in the
subsequent scheduled phase (see, as examples, [5], [30]).
However, changes in topology and load do not always align
with the phases of the algorithm resulting in a schedule that
often lags behind the network state.
In contrast, the idea behind topology-transparent scheduling
is to design schedules independent of the detailed network
topology [3], [15]. Specifically, the schedules do not depend
on the identity of a nodeâ€™s neighbours, but rather on how many

of them are transmitting. Even if a nodeâ€™s neighbours change,
its schedule does not; if the number of neighbours does not exceed the designed bound then the schedule guarantees success.
Though such schedules are robust to network conditions that
deviate from the design parameters [27], because the schedules
do not adapt, the technique remains a theoretical curiosity.
In contention-based schemes, such as IEEE 802.11, a node
computes implicitly when to access the channel, basing its decisions on perceived channel contention. We instead compute
a nodeâ€™s persistenceâ€”the fraction of time it is permitted to
transmitâ€”explicitly in a way that tracks the current topology
and load. To achieve this, we propose ATLAS, an Adaptive
Topology- and Load-Aware Scheduling protocol. Channel
allocation is a resource allocation problem where the demands
correspond to transmitters, and the resources to receivers.
ATLAS implements the REsource AlloCaTion computed by
REACT, a distributed auction that runs continuously. REACT
piggybacks offers and claims onto existing network traffic to
compute the lexicographic max-min allocation to transmitters
which we call the TLA allocation, emphasizing that it is
both topology- and load-aware. Each nodeâ€™s random schedule,
achieving a persistence informed by its allocation, is updated
whenever a change in topology or load results in a change
in allocation. While the slots of the schedule are grouped
into frames, this is done only to reduce the variance in delay
[6]; there is no need to wait for a frame boundary to update
the schedule. Even though the random schedules may not be
conflict-free, ATLAS is not contention-based; it does not select
persistences or make scheduling decisions based on perceived
channel contentionâ€”its decisions are based solely on topology
and load. We study how ATLAS adapts to controlled changes
in topology and load, measuring convergence time, relative
error, and scalability. We also assess the ability of ATLAS to
adapt in more dynamic network conditions.
To the best of our knowledge, ATLAS is the first scheduled
MAC protocol able to adapt to changes in topology and
load that is competitive with contention-based protocols in
throughput and delay while realizing superior delay variance.
It achieves this through the continuous computation of the
TLA allocation, and updating the schedule on-the-fly. These
updates occur only where and when needed. By not requiring
phases of execution and by computing persistences rather
than conflict-free schedules, ATLAS eliminates the complexity
of, and lag inherent in, topology-dependent approaches. By
not being dependent on the identity of neighbours, ATLAS
shares the best of topology-transparent schemes (and also their

c
Submitted to IEEE Transactions on Mobile Computing â€“ 2013
IEEE

2

potential for collisions) yet overcomes its weakness by being
adaptive. By not forcing updates to be frame synchronized,
ATLAS shares the critical features of continuous adaptation
with contention-based protocols. As a result, ATLAS achieves
predictable throughput and delay characteristics. Such characteristics and information about localized capacity at the MAC
layer may be used to inform higher layers, while end-toend characteristics at higher layers may be used to inform
ATLAS. This may support the development of an agile, higher
performing protocol stack.
The primary contributions of this paper are twofold: (1) The
REACT algorithm, an asynchronous, adaptive, and distributed
auction that solves a general resource allocation problem to
produce the TLA allocation. (2) ATLAS, a MAC protocol
that uses REACT to solve the specific problem of channel
allocation in a wireless network where each node produces
a random schedule with the number of transmission slots
determined by its allocation.
The sequel is organized as follows: Section II defines a
general resource allocation problem and presents the REACT
algorithm, proving its correctness. Section III expresses channel allocation as a resource allocation problem and defines
ATLAS. Related work is described in Section IV. After
describing the simulation set-up in Section V, Section VI
studies how ATLAS adapts to controlled changes in topology
and load, and to dynamic network conditions. In Section VII,
we discuss open issues and potential applications of REACT,
including the design of higher-layer services that inform, and
are informed by, the underlying communication channel.
II. D ISTRIBUTED R ESOURCE A LLOCATION â€” REACT
We consider a general resource allocation problem. Let R
be a set of N resources with capacity c = (c1 , . . . , cN ). Let D
be a set of M demands with magnitudes w = (w1 , . . . , wM ).
Resource j âˆˆ R is required by demands Dj âŠ† D. Demand
i âˆˆ D consumes capacity at all resources in Ri âŠ† R
simultaneously. The resource allocation s = (s1 , . . . , sM ),
si â‰¥ 0 defines the capacity P
reserved for the demands. Resource
allocation s is feasible if iâˆˆDj si â‰¤ cj for all j âˆˆ R and
si â‰¤ wi for all i âˆˆ D. Demand
P i is satisfied if si â‰¥ wi .
Resource j is saturated if
iâˆˆDj si â‰¥ cj . Throughout,
capacity refers to the magnitude of a resource.
Definition 1: [22] A feasible allocation s is lexicographically max-min if, for every demand i âˆˆ D, either i is satisfied,
or there exists a saturated resource j with i âˆˆ Dj where
si = max(sk : k âˆˆ Dj ).
We now describe REACT, a distributed auction that computes the lexicographic max-min allocation. In it, resources
are represented by auctioneers and demands by bidders. Each
auctioneer maintains an offerâ€”the maximum capacity consumed by any adjacent bidderâ€”and each bidder maintains a
claimâ€”the capacity the bidder intends to consume at adjacent
auctions. The final claim of bidder i defines allocation si .
Auctioneer j satisfies Def. 1 locally by increasing its offer in
an attempt to become saturated while maintaining a feasible
allocation. Bidder i satisfies Def. 1 locally for demand i by
increasing its claim until it is satisfied or has a maximal claim

Algorithm 1 REACT Bidder for Demand i.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:

upon initialization
Ri â† âˆ…
wi â† 0
U PDATE C LAIM ()
end upon
upon receiving a new demand magnitude wi
U PDATE C LAIM ()
end upon
upon receiving offer from auctioneer j
offers[j] â† offer // Remember the offer of auctioneer j.
U PDATE C LAIM ()
end upon
upon bidder i joining auction j
Ri â† Ri âˆª j // Resource j is now required by demand i.
U PDATE C LAIM ()
end upon
upon bidder i leaving auction j
Ri â† Ri \ j // Resource j is no longer required by demand i.
U PDATE C LAIM ()
end upon
procedure U PDATE C LAIM ()
// Select the claim to be no larger than the smallest offer or wi .
claim â† min ({offers[j] : j âˆˆ Ri }, wi )
send claim to all auctions in Ri
end procedure

at an adjacent auction. Through continuous updates of offers
and claims, the auctioneers and bidders eventually converge
on the lexicographic max-min allocation. We give precise
definitions of auction and bidder behaviour next.
Bidder i knows wi and maintains set Ri . Offers are stored in
offers[]; offers[j] holds the offer last received from auctioneer
j. Bidder i constrains its claim to be no larger than wi or the
smallest offer from auctioneers in Ri ,
claim = min ({offers[j] : j âˆˆ Ri }, wi ) .

(1)

Auctioneer j knows cj and maintains set Dj . Bidder claims
are stored in claims[]; claims[i] holds the claim last received
from bidder i. Auctioneer j identifies set Djâˆ— âŠ† Dj containing
bidders with claims strictly smaller than its offer,
Djâˆ— = {b : b âˆˆ Dj , claims[b] < offer}.

(2)

Bidders in Djâˆ— are either satisfied or are constrained by another
auction and cannot increase their claims in response to a larger
offer from auctioneer j. Bidders in Dj \ Djâˆ— are constrained
by auction j. They may increase their claims in response to a
larger offer. Resources left unclaimed by bidders in Djâˆ— ,
P

Aj = cj âˆ’
(3)
iâˆˆD âˆ— claims[i] ,
j

remain available to be offered in equal portions to bidders in
Dj \Djâˆ— . If claims of all bidders in Dj are smaller than the offer
(i.e., Dj = Djâˆ— ), there are no bidders to share the available
resources in Aj . The auctioneer sets its offer to Aj plus the
largest claim, ensuring that any bidder in Dj can increase its
claim to consume resources in Aj :
(
Aj /|Dj \ Djâˆ— |,
if Dj 6= Djâˆ— ,
offer =
(4)
Aj + max (claims[i] : i âˆˆ Dj ) , otherwise.
Alg. 1 and Alg. 2 describe actions taken by the bidders
and auctioneers of REACT in response to externally triggered

3

Algorithm 2 REACT Auctioneer for Resource j.
1:
2:
3:
4:
5:
6:
7:
8:
9:
10:
11:
12:
13:
14:
15:
16:
17:
18:
19:
20:
21:
22:
23:
24:
25:
26:
27:
28:
29:
30:
31:
32:
33:
34:
35:
36:
37:
38:
39:
40:
41:
42:
43:
44:

upon initialization
Dj â† âˆ…
cj â† 0
U PDATE O FFER ()
end upon
upon receiving a new capacity of cj
U PDATE O FFER ()
end upon
upon receiving claim from bidder i
claims[i] â† claim // Remember the claim of bidder i.
U PDATE O FFER ()
end upon
upon bidder i joining auction j
Dj â† Dj âˆª i // Demand i now requires resource j.
U PDATE O FFER ()
end upon
upon bidder i leaving auction j
Dj â† Dj \ i // Demand i no longer requires resource j.
U PDATE O FFER ()
end upon
procedure U PDATE O FFER ()
Djâˆ— â† âˆ…
Aj â† cj
done â† False
while ( done = False ) do
// If Djâˆ— contains all bidders in Dj , then auction j does not
// constrain any of the bidders in Dj .
if ( Djâˆ— = Dj ) then
done â† True
offer â† Aj + max ({claims[i] : i âˆˆ Dj })
// Otherwise, auction j constrains at least one bidder in Dj .
else
done â† True
// What remains available is offered in equal portions to the
// bidders constrained by auction j.
offer â† Aj /|Dj \ Djâˆ— |
// Construct Djâˆ— and compute Aj for the new offer.
for all b âˆˆ {Dj \ Djâˆ— } do
if ( claims[b] < offer ) then
Djâˆ— â† Djâˆ— âˆª b
Aj â† Aj âˆ’ claims[b]
done â† False
send offer to all bidders in Dj
end procedure

events. Collectively, auctioneers and bidders know the inputs
to the allocation problem and bidder claims converge on
the lexicographic max-min allocation; the claim of bidder i
converges on si .
The correctness of Alg. 1 and Alg. 2 is established in
two steps: Lemma 1 establishes forward progress on the
number of auctioneers to have converged on their final offer.
Theorem 1 employs Lemma 1 to show eventual convergence
to the lexicographic max-min allocation. Let claimi denote the
claim of bidder i and offerj the offer of auctioneer j. Assume
that the resource allocation remains constant for the period of
analysis, that bidder i knows Ri and wi , and that auctioneer
j knows Dj and cj . Further assume communication between
adjacent auctioneers and bidders is not delayed indefinitely. A
claim or offer is stable if it has converged on its final value.
Denote by Astable the set of auctioneers whose offers are stable
and remain the smallest among all offers.
Lemma 1: Suppose Astable contains k auctioneers, 0 â‰¤ k <
N . Then, within finite time, at least one auctioneer converges

on the next smallest offer omin . Offers equal to omin are stable
and remain smaller than all other offers not in Astable .
Proof: Wait sufficient time for every bidder i to send a
new claim to auctioneers in Ri and for every auctioneer j to
send a new offer to bidders in Dj . Let omin be the smallest
offer of an auctioneer not in Astable . Assume to the contrary that
offerx for some x âˆˆ
/ Astable is the first to become smaller than
omin . By Eq. 2 and 4, a decrease to offerx can only occur after a
bidder y at auction x with claimy < offerx increases its claim.
By Eq. 1, claimy can increase only after its limiting constraint
starts out smaller than offerx and increases. Constraints in the
system smaller than offerx are maximum claims, offers from
Astable , and offers equal to omin . Maximum claims and offers
from Astable do not change, leaving some x0 with offerx0 = omin
as the only potential limiting constraint for claimy . By Eq. 2
and 4, offerx0 can increase only after one of its bidders y 0
reduces its claim to be smaller than offerx0 . By Eq. 1, claimy0
can get smaller only after one of its auctioneers, say x00 ,
reduces its offer to be offerx00 < omin = offerx0 contradicting
the assumption that offerx is the first to become smaller than
omin . Therefore, offers equal to omin remain smaller than offers
not from Astable .
By Eq. 2 and 4, any j offering omin can change only after
a bidder i at auction j with claimi â‰¤ omin changes. By Eq. 1,
claimi only changes if its limiting constraint changes. Potential
limiting constraints include wi , offers from Astable , and offers
equal to omin . These constraints are stable; therefore, offers
equal to omin are stable.
Theorem 1: Bidders and auctioneers of Alg. 1 and Alg. 2
compute the lexicographic max-min allocation.
Proof: We apply Lemma 1 to show by induction that
every auctioneer eventually computes a stable offer.
Base Case: Consider an allocation problem with arbitrary
wi , cj , Ri , and Dj for 1 â‰¤ i â‰¤ M , 1 â‰¤ j â‰¤ N . Let |Astable | =
0. By Lemma 1, at least one auctioneer eventually converges
on a smallest offer omin . Offers equal to omin are stable and
remain smallest among all offers. Add auctioneers offering
omin to Astable ; |Astable | â‰¥ 1.
Inductive Step: Let |Astable | = k, 1 â‰¤ k < N . Then, by
Lemma 1 a non-empty set of auctioneers A+ with A+ âˆ©
Astable = âˆ… eventually converge on the next smallest offer.
Offers from A+ remain smaller than offers not from A+ or
Astable and are stable. Add A+ to Astable ; |Astable | â‰¥ k + 1.
By induction, all auctioneers are eventually added to Astable .
Wait for auctioneers to send their offers to adjacent bidders.
Bidder claims are now stable. By Eq. 1, bidder i is either
satisfied with its claim (claimi = wi ) or its claim is maximal at
an auction in Ri . By Definition 1, the claims are lexicographic
max-min.
III. T HE ATLAS MAC P ROTOCOL
Channel allocation in wireless networks can be expressed
as a resource allocation problem. In this context, transmitters
correspond to the demands in D and receivers to the resources
in R. Label transmitters {1, . . . , M } and receivers {1, . . . , N }.
A transmitter with a non-zero demand magnitude is active.
Receiver j is in Ri if it is within transmission range of

4

Slot x + 3

pkt #1

pkt #1

ack #1

pkt #2

ack #2

Node B

collision

pkt #1

ack #1

pkt #2

ack #2

Node C

pkt #2

pkt #1

ack #1

pkt #2

ack #2

Header
Fields

claim
offer

Slot x

MAC Header

Slot x + 1

MAC Payload

Header
Fields

claim
offer

Slot x + 2

Node A

Node # 1
w1 = 0.45
s1 = 0.25
sâˆ—1 = 0.20

ACK
Fields

MAC Header

Fig. 1.
Example transmissions in ATLAS of two packets in a network of
three fully connected nodes. The first packet is sent from node A to node
C. The second packet is sent from node C to node B. Transmissions are
coloured white and receptions are shaded grey. The frame structure is shown
for a data packet and an acknowledgement.

Node # 7
w7 = 0.30
s7 = 0.30
sâˆ—7 = 0.20

Node # 3
w3 = 0.50
s3 = 0.25
sâˆ—3 = 0.20

Node # 2
w2 = 0.55
s2 = 0.25
sâˆ—2 = 0.20

Node # 5
w5 = 0.75
s5 = 0.45
sâˆ—5 = 0.55
Node # 4
w4 = 0.40
s4 = 0.25
sâˆ—4 = 0.20

Bidirectional Link
New Bidirectional Link

transmitter i and transmitter i is active. Dj contains the active
transmitters for which receiver j is within transmission range.
Receiver j is adjacent to transmitter i if j âˆˆ Ri and i âˆˆ Dj .
The sets Dj and Ri capture the network topology for active
transmitters. For load, wi is set to the percentage of slots
required to support the demand at transmitter i. Transmitters
with no demand (i.e., wi = 0) receive an allocation of zero
slots: they are not active. Receiver capacities are set to one,
targeting 100% channel allocation. The lexicographic max-min
solution s = (s1 , . . . , sM ) for a given topology and traffic load
is the TLA allocation.
To apply REACT to channel allocation, we integrate it into
ATLAS, a simple random scheduled MAC protocol. Although
REACT could instead augment contention-based schemes, we
choose to work within a scheduled environment, a traditionally
difficult setting for adaptation. In ATLAS, each node runs a
REACT bidder (Alg. 1) and a REACT auctioneer (Alg. 2)
continuously. Auctioneers and bidders discover each other as
they hear from one another and rely on the host node to detect
lost adjacencies. The network topology is implicit in the sets
Ri and Dj . Each node updates its bidderâ€™s demand magnitude
to accurately reflect its traffic load. Offers and claims are encoded using eight bits each and are embedded within the MAC
header of all transmissions to be piggybacked on existing
network traffic. The encoding supports a total of 256 values for
offers, claims, and persistences uniformly distributed between
0 and 1; the error in the representation does not exceed 0.004.
Adding fields for an offer and claim to data packets and
acknowledgements results in a communication overhead of
four bytes per packet. For the slot size and data rate simulated
in Section VI, the overhead is 0.36%. A nodeâ€™s offer and claim
are eventually received by all single-hop neighbours reaching
the bidders and auctioneers that need to know the offer and
claim. In time, the bidder claims in REACT converge on the
TLA allocation s.
Packets are acknowledged within the slot they are transmitted and slots are sized accordingly. Unacknowledged MAC
packets are retransmitted up to ten times before they are
dropped by the sender. Fig. 1 shows that collisions are possible
in ATLAS, and that successful transmissions are acknowledged in the same slot. The transmissions collide in slot x;
they are repeated (successfully) in slots x + 2 and x + 3. Fig. 1
also shows the frame structure.
The TLA allocation can be interpreted directly as a set of

Node # 6
w6 = 0.05
s6 = 0.05
sâˆ—6 = 0.05

Fig. 2. Example network showing the TLA allocation computed by REACT
before and after an added link in the topology. wi identifies a nodeâ€™s demand,
si its initial TLA allocation, and sâˆ—i its TLA allocation after the added link.
Resource capacities are set to one. Double-lined circles identify nodes with
saturated resources.

persistences in a p-persistent MAC [28]. However, we achieve
lower variation in delay by introducing the notion of a frame
[6]. Specifically, ATLAS divides time into slots which are
organized into frames of v slots. Node i operates at persistence
pi = si . At the start of every frame and upon any change to
pi , node i computes ki = bpi vc + 1 with probability Ï€i and
ki = bpi vc with probability 1 âˆ’ Ï€i where Ï€i = pi v âˆ’ bpi vc.
Node i constructs a transmission schedule of ki slots selected
uniformly at random. Over many frames, E[ki ]/v equals pi
where E[ki ] is the expectation for ki .
Fig. 2 shows the TLA allocation in a small example network
before and after a change in topology. Node 7 starts out
disconnected from the other nodes and moves within range
of node 3. In REACT, node 3 starts out offering 0.25 which is
claimed by the bidders of nodes 1, 2, 3, and 4. With the claims
of node 3 and 4 limited by the offer of node 3 and the claim
of node 6 limited by its demand, the auctioneer at node 4 is
free to offer 0.45, which is claimed by node 5. Upon detecting
node 7 as a neighbour, the auctioneer at node 3 decreases its
offer to 0.20. The bidders at nodes 1, 2, 3, 4, and 7 respond
by reducing their claims accordingly. The smaller claims of
the bidders at nodes 3 and 4 allow the auctioneer at node 4
to increase its offer to 0.55. The bidder at node 5 responds by
increasing its claim to 0.55. It can be verified that, before and
after the topology change, the claims of the bidders (i.e., the
values of si and sâˆ—i ) are lexicographically max-min; that is,
every claim is satisfied or is maximal at an adjacent auction.
Consider the topology with node 3 and node 7 connected. The
bidder at node 6 is satisfied. The bidders at nodes 1, 2, 3, 4,
and 7 are maximal at the auction of node 3. The bidder at
node 5 is maximal at the auction of node 4.
There are many implementation choices to be made in applying REACT to channel allocation. We identify three binary
choicesâ€”lazy or eager persistences, physical layer or MAC
layer receivers, and weighted or non-weighted biddersâ€”and
three configurable parametersâ€”pmin , pdefault , and tlostNbr . The
choices are described here; they are evaluated in Section VI.

5

A. Lazy or Eager Persistences
A lazy approach sets persistence pi equal to the claim of
bidder i. Once converged, pi matches the TLA allocation
interpreted as a persistence. There is a potential disadvantage
with being lazy. For many applications, nodes cannot predict
future demand for the channel; they can only estimate demand
based on past events, i.e., packet arrival rate or queue depth.
As a consequence, wi lags the true magnitude of the demand
at node i. If wi is the limiting constraint for the claim
of bidder i, pi can be sluggish in response to increases
in demand. Alternatively, an eager approach sets persistence
pi = min (offers[j] : j âˆˆ Ri ), breaking the direct dependence
on wi . Under stable conditions, a nodeâ€™s channel occupancy,
the fraction of time it spends transmitting, matches its TLA
allocation; its occupancy is limited by the availability of
packets to transmit which is no larger than wi , even when
pi > wi . By allowing pi > wi , the persistence is made more
responsive to sudden increases in demand.

overwhelmed by neighbouring transmitters, a non-zero persistence is needed to quiet the neighbours. To accomplish this, the
node enforces a minimum persistence pmin , creating dummy
packets if necessary, whenever the sum of claims from adjacent
bidders exceeds the auction capacity.
E. Overriding the TLA Allocation with pdefault
There are two conditions where a node constrains its persistence to be no larger than pdefault . The first is when it has
no neighbours. While the TLA allocation permits an isolated
node to consume 100% of the channel, it cannot discover new
neighbours if it does so. The second time a node employs
pdefault is for a short period after the discovery of a new
neighbour. It is possible for several nodes operating with large
persistences to join a neighbourhood at about the same time. If
the persistences are large enough, neighbour discovery can be
hindered. For both scenarios, limiting the persistence to pdefault
facilitates efficient neighbour discovery.

B. Physical Layer or MAC Layer Receivers

F. Adaptation to Topology Changes and tlostNbr

A central objective of the TLA allocation is to ensure
that no receiver is overrun. In a wireless network, receivers
can be defined in terms of physical layer or MAC layer
communication. At the physical layer, every node is a receiver. At the MAC layer, packets are filtered by destination
address; a node is only a receiver if one of its neighbours
has MAC packets destined to it. MAC layer receivers can
increase channel allocation by over-allocating at non-receiving
nodes. However, the overallocation can slow detection of
new receivers. Physical receivers prevent overallocation at any
receiver, making the allocation more responsive to changes in
traffic where nodes become receivers.

Changes in network topology are detected externally to
REACT. In ATLAS, neighbour discovery is performed independently by each node. If a node hears from a new neighbour,
then the node notifies its bidder of the new auction and
its auctioneer of the new bidder. Conversely, if a node has
not heard from a neighbour in more than tlostNbr seconds, it
presumes the node is no longer a neighbour and informs its
auctioneer and bidder accordingly.

C. Weighted or Non-Weighted Bidders
We have described a MAC protocol where transmitters are
represented by equally weighted bidders. For applications requiring multiple demands per transmitter, i.e., nodes servicing
more than one traffic flow, we propose the weighted TLA
allocation. The demands of weighted bidders are comprised
of one or more demand fragments; the number of fragments
accumulated into a demand is the demandâ€™s weight. Let Î³i
be the weight for demand i. Demand fragments in demand i
have magnitude wi /Î³i . The weighted TLA allocation defines
the lexicographically max-min vector u = (u1 , . . . , uN ) where
ui is the allocation to each demand fragment in demand i
for a total allocation of ui Î³i to demand i. REACT can be
extended to compute the weighted TLA allocation. To do
this, each bidder must inform adjacent auctions of its weight.
Sixteen unique weights (with a four-bit representation) may
be sufficient for many applications.
D. Minimum Persistence pmin
A node can maintain a persistence of zero without impacting
the communication requirements of its bidder. For auctioneers,
a persistence of zero is problematic. If a receiver becomes

IV. R ELATED W ORK
This paper focuses on the TLA allocation, its continuous
distributed computation, and its application to setting transmitter persistences. In this section, we review a representative
set of scheduled MAC protocols, observing how each selects
a nodeâ€™s persistence and adapts to topology and load.
Any finite schedule used in a cyclically repeated way can
be generalized as a (k, v)-schedule with k transmission slots
per frame of v slots, producing an effective persistence of
p = k/v. Examples include the random schedules of [6], [18]
where each node selects its k transmission slots randomly
from the set of v slots in the frame. Topology transparent schemes [3], [15], [27] also implement (k, v)-schedules.
These schedules rely on only two design parameters: N , the
number of nodes in the network, and Dmax , the maximum
supported neighbourhood size. These schedules guarantee each
node a collision-free transmission opportunity from each of
its neighbours at least once per frame, provided the nodeâ€™s
neighbourhood size does not exceed Dmax . (k, v)-schedules do
not adapt to variations in neighbourhood size or traffic load.
The combinatorial requirements for variable-weight topology
transparent schedules (variable k) are explored in [19], but no
construction nor protocol using them is given.
A class of topology-dependent scheduled protocols compute
distance-2 vertex colourings of the network graph to achieve
TDMA schedules with spatial reuse. The colourings assign
one transmission slot to each node and do not adapt to

6

TABLE I
ATLAS CONFIGURATIONS SELECTED FOR SIMULATION .
Configuration
Name
Nominal
Lazy Persistences
Physical Receivers
Weighted Bidders

Eager (0)
or Lazy (1)
0
1
0
0

MAC (0) or
Physical (1)
0
0
1
0

Unweighted (0)
or Weighted (1)
0
0
0
1

traffic load. One of the first distributed protocols to bound
the number of colours is proposed in [5]. Distributed-RAND
(DRAND) [25] is a distributed implementation of RAND (a
centralized algorithm for distance-2 colouring [23]). DRAND
runs a series of loosely synchronized rounds. A colour is
assigned in each round to one or more nodes in different
two-hop neighbourhoods. DRAND is employed by ZebraMAC (Z-MAC) [24] to compute schedules over which to run
CSMA/CA. Nodes are given priority access to their own slot,
but also allowed to contend for access in other unused slots, as
is done in [4]. Due to the complexity of DRAND, schedules
are only computed once during network initialization.
Other topology-dependent schemes support variable persistences. The periodic slot chains proposed in [14] are not
limited to the structure of a fixed length frame and can support
variable and arbitrarily precise persistences. A slot chain is
defined by its starting transmission slot and period between
its consecutive transmission slots. By combining multiple
slot chains with different periods, schedules are constructed
targeting any rational persistence in the range [0, 1]. The
computation of slot chains provided in [14] is centralized; a
distributed mechanism to adaptively compute the slot chains
remains an open problem. In [30], a five phase reservation
protocol (FPRP) computes conflict-free schedules where a
node can reserve one or more transmission slots in the frame
to achieve variable persistences. Reservation frames are run
periodically rather than on a demand basis and, therefore, may
not accommodate the current topology and traffic load.
In SEEDEX [26], nodes do not attempt to derive conflictfree schedules. They learn the identities of their two-hop
neighbours and adjust transmission probabilities (i.e., persistences) to improve the likelihood of collision-free transmissions. The transmission probabilities accommodate the number
and identity of neighbours, but not traffic load.
In our earlier work [20], a distributed algorithm for computing the TLA allocation is provided; however, the algorithm
assumes a fixed topology and does not adapt to changes in the
network. REACT solves these limitations by asynchronously
adapting to changes in both topology and traffic demand.

A. Scenario Details
Unless otherwise noted, all four configurations run with
pdefault = 0.05, tlostNbr = 0.5s, and pmin = 0.01. The selection
of pdefault and tlostNbr are justified by results in Figs. 5, 11a,
and 11b. The selection of pmin is based on [20]. Frames
contain v = 100 slots of length 800Âµs (1100 bytes per slot).
Simulations are run using the ns-2 simulator [21]. Each
wireless node is equipped with a single half-duplex transceiver
and omni-directional antenna whose physical properties match
those of the 914 MHz Lucent WaveLAN DSS radio. The data
rate for all simulations is 11 Mbps. The transmission and
carrier sense ranges are 250m.
Each simulation runs a network scenario composed of a
randomly generated topology and a randomly generated traffic
load. Unless specified otherwise, topologies contain 50 randomly placed nodes constrained to a 300 Ã— 1500m2 area. With
the exception of the multi-hop TCP flows in Section VI-F,
each traffic load consists of single-hop constant rate traffic.
Four traffic loads are simulated: 20% and 80% of nodes loaded
with small demands (75 Â± 50 pkts/s), 20% and 80% of nodes
loaded with large demands (500 Â± 50 pkts/s). Nodes loaded
with traffic are selected at random and the demand magnitudes
are selected uniformly at random from the specified range.
The packet destination is selected dynamically from the set
of neighbouring nodes as the packet is passed down to the
MAC layer. For the Weighted Bidders configuration, each
demand is assigned a random integer weight between one
and five. Traffic is generated by constant bit rate generators
and transported over UDP; packets are 900 bytes in length,
leaving room in each slot for header bytes and a MAC layer
acknowledgement. Combined with the random placement of
nodes and the addition of mobility, these four traffic loads
enable simulation of a wide variety of network conditions.
B. Relative Error
A metric of interest is the average relative error for a
nodeâ€™s persistence with respect to the TLA allocation. Error
is reported in two parts: relative excess and deficit persistence
error. Errors are measured per node over 80ms consecutive
intervals in time (equal to the length of one MAC frame).
We compute the average relative excess error and average
relative deficit error for a given sample set of persistence
measurements. The relative errors are ratios, requiring use of
the geometric rather than arithmetic mean. But, the errors are
often zero, preventing direct use of their mean. Instead, we
convert errors into accuracies eliminating zeros from the data
set for a more meaningful geometric average. The average
relative accuracies are converted back to relative errors.

V. S IMULATION S ET- UP
We now describe the simulations used to produce the experimental results presented in Section VI. Table I lists the four
ATLAS configurations simulated. The Nominal configuration
employs eager persistences, defines receivers in terms of MAC
layer communication, and operates with unweighted bidders.
The other three configurations differ from the Nominal case
by a single choice and are named accordingly.

VI. E VALUATION OF ATLAS
Results from [20] show the TLA allocation applied in a
static network to maintain expected delay and throughput
compared to IEEE 802.11, while reducing the variance for
both metrics. The TLA allocation nearly eliminates packets
dropped by the MAC layer. In this section, we build on these
results, focusing on the efficient distributed computation of the

7

Fig. 3.

Convergence time following network initialization.

simulations of 1000 network scenarios, 250 of each traffic
load. The scenarios are simulated eight times each, once per
default persistence: 0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.3, and
0.4. Small default persistences (pdefault â‰¤ 0.01) limit a nodeâ€™s
ability to communicate during neighbour discovery, slowing
convergence. Large default persistences (pdefault â‰¥ 0.3) permit
nodes to transmit with large persistences before they discover
their neighbours. In networks with 40 large demands, the large
persistences can overwhelm the channel preventing neighbour
discovery and delaying convergence. ATLAS is robust to the
selection of pdefault with a suitable range of [0.05â€“0.2]. For the
remaining simulations, pdefault is 0.05.
B. Convergence after a Change in Demand

TLA allocation in the face of changes in topology and load.
The results presented here work to answer four questions:
1) Can ATLAS converge quickly on the TLA allocation?
2) Can ATLAS scale to larger networks?
3) Can ATLAS keep up with changes in a mobile network?
4) Can ATLAS adapt to multi-hop traffic flows?
The first question is addressed Sections VI-A, VI-B, and VI-C.
The second is addressed in Section VI-D. The third and
fourth questions are addressed in Sections VI-E and VI-F,
respectively. Continuing the focus on adaptation, Section VI-G
provides comparisons with several scheduled protocols.
A. Convergence after Network Initialization
Fig. 3 reports average convergence times for all four ATLAS configurations. Error bars denote the arithmetic standard
deviation from the mean for each sample set. Convergence is
measured from network initialization (time = 0) to the time
ATLAS converges on the TLA allocation. Times are collected
from simulations of 1000 network scenarios simulated four
times each, once per configuration. There are 250 scenarios
for each traffic load.
The Physical Receivers configuration converges fastest in
less than 0.4s on average for networks with 40 large demands
and faster for other traffic loads. The extra step of detecting
MAC receivers slows convergence. The Lazy Persistences
configuration is the slowest with an average convergence time
of 0.67s for networks with 40 large demands. The strict
limit on persistences enforced by this configuration slows
convergence compared to the others.
Fig. 4a shows average excess and deficit relative persistence
errors for all four configurations. The averages are computed
for nodes with a non-zero TLA allocation and only during convergence. Nodes are observed to operate within approximately
20% of their TLA allocation regardless of configuration.
Deficit errors are larger than excess errors reflecting a tendency
to converge from below, rather than above, the TLA allocation.
In Fig. 4b, each data point reflects the convergence time (xcoordinate) and total relative persistence error (y-coordinate)
for one simulation of the nominal configuration. The data
shows relative persistence error to be fairly consistent from
network to network with a maximum observed error of 27%.
Fig. 5 reports convergence time for the Nominal configuration while varying pdefault . Convergence is measured for

Fig. 6 reports convergence times and relative persistence
errors for the Nominal configuration following a change to a
single demand magnitude. Four types of demand are simulated: a new small demand, a new large demand, a removed
small demand, and a removed large demand. New demands
start with magnitude zero and change to 75 Â± 50 pkts/s for
small demands and to 500 Â± 50 pkts/s for large demands.
Removed small demands and removed large demands start
at 75 Â± 50 pkts/s and at 500 Â± 50 pkts/s, respectively; both
change to zero. The four demand change types are simulated
under the four traffic loads. REACT is allowed to converge
on the initial TLA allocation prior to the demand change.
Convergence times and error measurements are taken from
simulations of 4000 network scenarios, 250 for each of the 16
demand change and traffic load combinations.
Fig. 6a reports convergence times measured from the time
of the change to the time of convergence on the new TLA
allocation. The largest convergence times of approximately
0.175s are found in networks loaded with 40 demands. The
average convergence time for the other scenarios is 0.125s
or smaller. Fig. 6b shows relative persistence errors measured
during convergence at nodes whose TLA allocation are affected by the demand change. Persistences are observed to be
within 10% of the TLA allocation.
C. Convergence after a Change in Topology
Fig. 7 reports convergence time and relative persistence
error following two types of topology change: the creation
of a link and the removal of a link between a pair of nodes.
Simulations are run on 2000 network scenarios, 250 for each
topology change type and traffic load combination. Networks
that lose a link are simulated once per neighbour timeout
tlostNbr of 0.5s, 2.0s and 5.0s.
Network topologies are generated as follows. A first node
is placed at a random location in the simulation area. For
topologies gaining a link, a second node is placed just outside
the transmission range of the first node with a trajectory
toward the first node. For topologies losing a link, the second
node is placed just inside the transmission range of the first
node with a trajectory away from the first node. The remaining
48 nodes are placed at random locations in the simulation area.
The distance travelled by the second node is constrained to
avoid unintentional topology changes.

8

(a) Relative excess and deficit persistence errors.
Fig. 4.

Fig. 5.

(b) Convergence time vs. error for the Nominal configuration.

Relative persistence error for ATLAS.

Convergence times when run with varying default persistences.

The expected convergence time following the addition of a
new link is 0.025s. For tlostNbr =0.5s, convergence is reached in
less than 0.13s on average. For tlostNbr =2.0s and tlostNbr =5.0s,
the large convergence times are dominated by tlostNbr . Except
for the simulations of Fig. 11, all others configure ATLAS
with tlostNbr =0.5s. During convergence, nodes affected by the
topology change are observed to operate within 4% of their
TLA allocation on average. These numbers are striking. The
small convergence times stem from a counterintuitive feature
of the TLA allocation: the majority of topology changes do
not affect the TLA allocation. A new link only has an effect
if the link connects a bidder with an auction that lacks the
capacity to support the bidderâ€™s claim. Even in heavily loaded
networks, many auctions have spare capacity to support a new
bidder. For these scenarios, convergence is instantaneous.
D. Scalability to Large Networks
We now turn to results demonstrating ATLASâ€™s scalability.
We simulate 10 network sizes with the x-dimension ranging
from 600m (2.4 hops) to 6000m (24 hops) in 600m increments;
the y-dimension is held constant at 300m. The number of
nodes is selected to keep the average neighbourhood density
constant across all network sizes. Fig. 8 reports convergence
times for 4000 network scenarios, 100 of each traffic load
and network size combination. The convergence of ATLAS
in large networks is striking. In networks spanning 24 hops,

convergence is reached in an average of 0.89s, a mere 40%
increase compared to networks spanning 4.8 hops.
The impressive convergence times, particularly those of
networks spanning 12 or more hops, suggest that convergence
happens locally, allowing distant neighbourhoods to converge
in parallel. This local behaviour is captured in Fig. 9 which
reports the average distance between a network change and a
node whose bidder changes its claim in response. Distances
are reported in hops. A node that changes its demand or
gains/loses a neighbour has distance zero. Neighbours of this
node have distance one, and so on. Range of impact is reported
for the six types of change evaluated in Sections VI-B and
VI-C. Each type of change is simulated in 1000 network
scenarios, 250 of each traffic load. The range of impact is
less than 1.75 hops on average.
E. Performance with Node Mobility
Section VI-C addresses the robustness of ATLAS to single topology changes. We now evaluate its performance in
networks with continuous mobility which may not have the
opportunity to converge on the TLA allocation.
Fig. 10a reports persistence error for node speeds ranging
from 0 m/s to 120 m/s with 200 scenarios simulated for each
node speed, 50 of each traffic load. Node movements are
generated using the steady-state mobility model generator of
[13] with a pause time of zero. Simulations are run for 20s.
As node speeds increase, so do deficit persistence errors. The
larger deficit errors are an artifact of lost neighbour detection
which is delayed by tlostNbr = 0.5s. As a result, nodes tend to
think their neighbourhoods are more crowded than they are, a
tendency that gets worse as node speeds increase. In terms of
REACT, auctioneers and bidders unnecessarily constrain their
offers and claims to accommodate lost neighbours. The deficit
persistences translate to degraded throughput. Fig. 10b reports
MAC throughput for the simulations of Fig. 10a. Even with
node speeds of 120 m/s where a node travels its transmission
range in 2.1s, throughput degrades modestly, decreasing by
less than 20% compared to static networks.
Fig. 11 shows that a large tlostNbr exacerbates deficit persistence error and further degrades throughput. Data is collected
from 200 scenarios, 50 of each traffic load. Each scenario is
simulated five times with neighbour timeouts ranging from

9

(a) Convergence time after a demand change.
Fig. 6.

(a) Convergence time after a topology change.
Fig. 7.

Fig. 8.

(b) Relative persistence error after a demand change.

Convergence time and relative persistence error during convergence following a single demand change.

(b) Relative persistence error after a topology change.

Convergence time and relative persistence error following a single topology change.

Convergence times as the width of the network grows.

0.1s to 15.0s. Node speeds are fixed at 30 m/s. Degraded
performance is observed for large timeouts, tlostNbr â‰¥ 0.5s,
but also for small timeouts, tlostNbr = 0.1s. In networks
loaded with 10 large demands, tlostNbr = 0.1s causes nodes
to falsely identify lost neighbours that must be rediscovered.
The remaining simulations are run with tlostNbr = 0.5s.
Fig. 12 reports packet delay for ATLAS and IEEE 802.11
for the 200 network scenarios of Fig. 10 with node speeds
equal to 30 m/s. IEEE 802.11 is configured with a maximum
packet retry count of seven for RTS, CTS, and ACKs and four
for data packets [11], a mini-slot length of 20Âµs, and minimum
and maximum contention window sizes of 32 and 1024 slots,
respectively. Each point in the scatter plot reports the average
packet delay (x-coordinate) and variation in packet delay (ycoordinate) for a single node. The largest reported average
delay is 0.047s for ATLAS and 0.058s for IEEE 802.11. The
largest reported variation in delay for ATLAS is 0.0016s2 ,
just 3.6% of the 0.0444s2 reported for IEEE 802.11. This
impressive reduction in delay variance is crucial to the support
of TCP, which we evaluate next.
F. Multi-hop TCP Flows

Fig. 9.

Average range of impact (in hops) for a demand or topology change.

To this point, we have used MAC layer traffic to simulate
a diverse set of network scenarios. We now evaluate the
performance of ATLAS using multi-hop TCP flows. To accommodate the dynamic nature of these flows, each node estimates
its own demand by monitoring queue behaviour. Demand is
estimated as the sum of two parts: wenqueue and wlevel . wenqueue

10

(a) Relative persistence error.
Fig. 10.

(b) Total MAC throughput.

Relative persistence error and total MAC throughput for varying levels of node mobility.

(a) Relative persistence error.
Fig. 11.

Fig. 12.

(b) Total MAC throughput.

Relative persistence error and total MAC throughput for varying neighbour timeouts.

Delays for ATLAS and IEEE 802.11 with node speeds of 30 m/s.

is the percentage of channel required to keep up with the current enqueue packet rate, wenqueue = (packet enqueue rate) Ã—
(slot length). wlevel is the percentage of channel required to
transmit all packets in the queue within 0.2s (i.e., 25 slots),
wlevel = [(# packets in queue)/0.02s] Ã— (slot length).
To avoid cross-layer interactions between the MAC and
routing protocols, Dijkstraâ€™s shortest path algorithm [28] using
accurate knowledge of the global topology computes the next
hop address for all packet transmissions. FTP agents emulate
transfer of infinite size files to create flows with throughput
limited only by the performance of the network. Transfers start
at time zero and run for 20s. Nodes are statically placed at
random locations in a 300 Ã— 1500m2 simulation area. The
source and destination nodes for each file transfer are selected

at random. Each FTP transfer is transported over TCP Reno
configured for selective acknowledgements, the extensions
of RFC 1323 [1], and 900 byte TCP segments. The return
ACKs are not combined with each other or with other data
packets. Consequently, the transmission of a single 40-byte
TCP ACK consumes an entire transmission slot in ATLAS.
The maximum congestion window size is 32 packets. Network
scenarios are simulated for three traffic loads: networks with
2, 8, and 25 TCP flows. The number of replicates per traffic
load are chosen so that 3000 TCP flows are simulated for each.
Fifteen hundred scenarios are simulated with two TCP flows,
375 with eight TCP flows, and 120 with 25 TCP flows.
We simulate TCP traffic on five MAC protocols: the four
configurations of ATLAS and IEEE 802.11. The configurations of ATLAS use pdefault = 0.05, tlostNbr = 0.5s, and
pmin = 0.01. IEEE 802.11 parameters match those described
in Section VI-E. Each node dynamically sets its bidder weight
to one or the number of outgoing TCP flows it services,
whichever is larger.
The 15 sub-plots in Fig. 13 show the percentage of flows
(y-axis) achieving a minimum throughput (x-axis). The distinguishing characteristics of the three unweighted ATLAS
configurations are seen in the throughput curves for networks
with two flows. These networks are loaded lightly enough for
the auctions at non-receiver nodes to make a difference in the
allocation, improving throughput for 2- and 4-hop flows. These
networks also demonstrate how the longer initial packet delays
of the Lazy Persistences configuration increase round trip time

11

Networks with 2 Flows

Networks with 8 Flows

Networks with 25 Flows

1-hop flows

y-axis shows % of TCP flows achieving minimum required throughput.

All flows
2-hop flows
3-hop flows
4- and 5-hop flows

x-axis shows minimum required throughput for TCP flows in packets/second.
Fig. 13. Percent of TCP flows (y-axis) achieving a minimum throughput (x-axis). Plots in the left, center, and right columns report on flows from simulations
of 2, 8, and 25 flows, respectively. The plots in the top row report on all flows, regardless of hop count. Plots in the second, third, and fourth rows report on
1-hop, 2-hop, and 3-hop flows, respectively. Plots in the fifth row report on 4- and 5-hop flows.

for 4- and 5-hop flows, preventing TCP from achieving its best
throughput.
The Weighted Bidders configuration performs well for
multi-hop flows in networks with eight and 25 flows by
allocating more to multi-hop flows at the expense of singlehop flows. Because one-hop flows tend to achieve higher
throughput, the configuration maintains a tighter variation in
flow throughputs as indicated by the steeper slope of the
Weighted Bidders curve in the top right plot of Fig. 13.
Regardless of configuration, ATLAS surpasses IEEE 802.11

in support of concurrent multi-hop flows. The interaction
between the IEEE 802.11 back-off algorithm and TCPâ€™s congestion control is well known [9]. In testbed experiments, a
single TCP flow with no competition has difficulty reaching a
destination four hops away [16]. Our simulations corroborate
these findings, as approximately 50% of the 4- and 5-hop flows
report a throughput of zero. For networks with 25 demands,
nearly 75% of 2-hop flows are non-functional; 3-, 4-, and 5hop flows are almost completely shut out. The throughput of

12

ATLAS is achieved in spite of channel wasted transmitting 40
byte TCP ACKs in their own slots.

G. Comparison with other Scheduled MAC Protocols
Here, we compare the adaptation of ATLAS with several
other scheduled protocols including DRAND, Z-MAC, FPRP,
and SEEDEX. Although the first three compute conflict-free
schedules, an NP-hard problem [7], a comparison highlights
the agility of ATLAS.
1) Adaptation to Topology Changes: For the simulations of
Section VI-E, the number of neighbour changes (i.e., gained
or lost neighbours) per second experienced by a node is
correlated to the node speed. When the nodes move at 30 m/s,
each node is expected to gain, or lose, a neighbour 2.21 times
per second; within 6.3s, the number of neighbour changes is
expected to exceed the neighbourhood size.
Based on the run times reported in [25, Fig. 10], we
estimate DRAND to compute schedules for the networks in
Section VI-E in approximately 4.9s (adjusting for data rate
and a two-hop neighbourhood size of 27). In this time, the
topology changes caused by nodes moving at 30 m/s are
expected to invalidate the computed schedule. Z-MAC has
the same limitation and, although it compensates by running
CSMA/CA to resolve collisions, it does not benefit from its
TDMA schedule when nodes are mobile. In [25], the run
times reported for FPRP schedule generation are comparable
to DRAND. For SEEDEX, nodes discover their two-hop
neighbours using a fan-in/fan-out procedure described in [26].
However, a practical integration of the procedure into the MAC
protocol is not described or evaluated, preventing a comparison
of its agility with other MAC protocols. In contrast to the slow
schedule computation times of DRAND, Z-MAC, and FPRP,
ATLAS is shown to handle node speeds of up to 120 m/s with
only moderate degradation to MAC throughput.
2) Adaptation to Changes in Traffic Load: The persistences
achieved by DRAND and SEEDEX are dependent on topology
alone; neither adapts to traffic load. Although Z-MAC adapts
to load, it does so by deviating from its underlying schedule,
which does not adapt. FPRP can adapt to load by scheduling
a variable number of slots per node; this is done at the
expense of both longer frame lengths and longer run times for
schedule computation. In contrast, ATLAS adapts to traffic
load, responding quickly enough to establish and maintain
multi-hop TCP flows.
3) Continuous Adaptation: Common to the scheduled
schemes mentioned here is the use of a distinct phase for
schedule computation (or neighbour discovery for SEEDEX).
The schedules must be updated in order for the MAC to adapt.
Any fixed period between schedule updates must be selected a
priori; it cannot be adjusted for variations in network mobility.
If schedules are to be updated when needed, a mechanism is
required to trigger the schedule update. This coordination, by
itself, is a challenge in an ad hoc network. In contrast, ATLAS
does not employ a schedule computation (or a neighbour
discovery) phase and adapts continuously to changes in both
topology and traffic load.

VII. D ISCUSSION
In this section we discuss open issues and suggest potential
applications for REACT and ATLAS.
A. Improved Reliable Transport
TCPâ€™s congestion control algorithm is known to suffer cross-layer interactions with binary exponential back-off
(BEB) employed by IEEE 802.11 [9]. BEB is short term
unfair, allowing a single node to capture the channel at the
expense of its neighbours [2], [10] causing high variation in
packet delay and making it difficult for TCP to estimate roundtrip delay. Many modifications have been proposed to improve
TCP performance over wireless networks [17]; common approaches are detection of packet loss (differentiating it from
congestion) and improved estimation of round trip time. An
alternative is to minimize packet loss and control variation
in packet delay at the MAC layer. ATLAS demonstrates a
remarkable control of variation in delay (Fig. 12) enabling
TCP to reliably support 3-, 4-, and 5-hop flows over heavily
loaded networks (Fig. 13). However, TCP throughput still
degrades considerably as the number of hops grows. Potential
areas for future work include the integration of ATLAS into
a cross-layer solution for reliable transport over wireless
networks and the use of REACT to inform TCPâ€™s congestion
window size.
B. Selection of Configurable Parameters
ATLAS has three configurable parameters: pdefault , tlostNbr ,
and pmin . Based on our simulations, [0.01â€“0.2] is an acceptable
range for pdefault (Fig. 5) and [0.1sâ€“2s] is an acceptable range
for tlostNbr (Figs. 11a, 11b). In [20], pmin =0.1s is found to
be acceptable for a protocol that enforces pmin at all nodes
and at all times. Because ATLAS employs pmin temporarily,
and only when needed, it is less sensitive to the selection of
pmin . Although results show ATLAS to be robust to parameter
selection, tuning may be required in other scenarios or in a
hardware implementation.
C. Dynamic Selection of Auction Capacity
ATLAS targets 100% channel allocation by setting auction
capacities in REACT to one. Although simulation results
show this to be an adequate choice, it is not clear whether
performance can be improved by under- or over-allocating the
channel. Indeed, optimal auction capacities (however optimal
is defined) are dependent on network topology and quality of
the communication channel. We leave a thorough analysis of
auction capacity selection to future work, pointing out here
that REACT adapts continuously, allowing auction capacities
to be adjusted dynamically, if necessary.
D. Potential Applications for REACT
The weighted TLA allocation opens doors for several potential uses. In the simulations of Section VI-F, a bidderâ€™s
weight is set according to the number of flows it services.
It may be desirable to set weights according to queue levels,

13

demand magnitudes, neighbourhood sizes, node betweenness
[8], distance from a point of interest (i.e., an access point or a
common sink), position in a multicast/broadcast tree, or path
hop count. The key observation is that ATLAS maintains flexibility by allowing nodes to define bidder weights arbitrarily
to suit the needs of the network.
While computation of persistences is the primary motivation
for this work, REACT is not limited to this purpose. Consider
the Physical Receivers configuration with node demands set
to one. The resulting allocation is independent of actions
taken by the upper network layers and, therefore, can inform
decisions made by those layers. It can serve as a measure of
potential network congestionâ€”small allocations are assigned
in dense neighbourhoods containing many potentially active
neighbours. The routing protocol can use the allocation to
discover alternate routes around congestion.
An intriguing application is the implementation of differentiated service at the MAC layer. IEEE 802.11e [12] enhances
the distributed coordination function by implementing four
access categories; an instance of the back-off algorithm is run
per access category, each with its own queue. The probability of transmission of each access category is manipulated
independently through selection of contention window size
and inter-frame space. This permits higher priority traffic to
capture the channel from lower priority traffic.
Similar results can be achieved by four instances of REACT,
each computing the allocation for a single access category.
Prioritization is achieved through dynamic coordination of the
four auction capacities at each node. A potential strategy sets
the capacity for each access category equal to one minus the
allocation to higher priority access categories. As a result,
higher priority auctions are permitted to starve lower priority
auctions of capacity, effectively distributing channel access to
high priority traffic. Alternatively, auction capacities can be
selected to ensure a minimum or maximum percentage of the
channel is offered to an access category.
A network can run multiple instances of REACT. For
example, an instance of the Physical Receivers configuration
with all demands set to one can be run concurrently with
four instances configured to support differentiated service.
Alternatively, multiple instances of REACT can be used to
allocate more than one set of resources concurrently.
E. Assumptions Made by ATLAS
Two key assumptions are made by ATLAS in its computation of the TLA allocation using REACT: (1) The offers and
claims received by a node are accurate. (2) The offers and
claims of a node are eventually received by all neighbouring
nodes. The first assumption is reasonable, provided received
packets are checked for errors by the link layer. The second assumption is almost certainly invalid; asymmetric communication, interference beyond the range of transmission, and signal
fading are common in wireless communication and can prevent
the delivery of offers and claims. Under realistic conditions,
REACT may not converge on the TLA allocation, risking overallocation of the channel. In practice, auctions can adjust their
capacities to mitigate the over-allocation. Every node knows

the persistences of its neighbours (from bidder claims) and
can compute the expectation for collisions on the channel.
Significant deviations above this expectation can trigger the
auction to lower its capacity. An evaluation in a testbed of real
radios is necessary to understand the sensitivity to anomalies
on the wireless channel and the effectiveness of adjusting
auction capacities to accommodate channel conditions.
The evaluation of ATLAS in Section VI assumes both slot
and frame synchronization; ATLAS does not require either.
The computation of the TLA allocation by REACT does not
rely on a frame structure and the expected performance of the
random schedules is not affected by loss of frame synchronization. Even without slot synchronization, REACT can compute
the TLA allocation; however, loss of slot synchronization
may reduce channel capacity by 50% (see Aloha vs. slotted
Aloha in [28]). ATLAS can accommodate the lower channel
capacity by reducing auction capacity. This technique may
allow ATLAS to be run on commodity IEEE 802.11 hardware
[29] that lacks native support for slot synchronization. This is
a subject of our current research.
F. Enhancing Existing MAC Protocols
We have used REACT to compute persistences to be employed within ATLAS, a slotted MAC protocol. Alternatively,
REACT can be run on top of the IEEE 802.11 MAC by
embedding claims and offers in the headers of existing control
and data messages. The TLA allocation can be used to inform
the selection of contention window sizes, eliminating the need
for (and negative side effects of) binary exponential back
off. We are currently working to integrate REACT into IEEE
802.11. Another alternative (and more ambitious) approach is
to implement TLA persistences in a topology-dependent MAC
that computes conflict-free schedules. Only a few topologydependent schemes allow a node to reserve more than one
slot in a frame (i.e., [14], [30]), and those do not define how
many slots a node should reserve. The TLA allocation can
establish a permissible number of slots to be reserved by each
node, given the current topology and traffic load.
VIII. C ONCLUSION
We have proposed REACT, a distributed auction that
converges continuously on the TLA allocation, adapting to
changes in both topology and traffic load. The utility of REACT is demonstrated through integration into ATLAS which
we simulate under a wide variety of network scenarios. The
results presented suggest that REACT can effectively inform
the selection of transmitter persistences, and that ATLAS can
provide robust, reliable, and scalable services. The application
of REACT is not restricted to the computation of transmitter
persistences. It has the potential to inform routing and admission control decisions, to enable differentiation of service at
the MAC layer, and even to allocate other node resources.
In this context, the REACT algorithm provides a potential
solution to the immediate challenge of medium access control,
but also shows promise as a tool for use in network protocol
design in general.

14

ACKNOWLEDGEMENT
The authors appreciate the useful comments provided by the
anonymous reviewers.
R EFERENCES
[1] RFC 1323: TCP Extentions for High Performance, 1992.
[2] V. Bharghavan, A. Demers, S. Shenker, and L. Zhang. MACAW:
A medium access protocol for wireless LANs. In Proceedings of
the ACM Conference on Communications Architectures, Protocols and
Applications (SIGCOMMâ€™94), pages 212â€“225, 1994.
[3] I. Chlamtac and A. FaragoÌ. Making transmission schedules immune
to topology changes in multi-hop packet radio networks. IEEE/ACM
Transactions on Networking, 2(1):23â€“29, 1994.
[4] I. Chlamtac, A. FaragoÌ, A. D. Myers, V. R. Syrotiuk, and G. ZaÌruba.
ADAPT: A dynamically self-adjusting media access control protocol for
ad hoc networks. In Proceedings of the IEEE Global Telecommunications Conference (GLOBECOMâ€™99), pages 11â€“15, 1999.
[5] I. Chlamtac and S. S. Pinter. Distributed nodes organization algorithm
for channel access in a multihop dynamic radio network. IEEE
Transactions on Computers, C-36(6):728â€“737, June 1987.
[6] C. J. Colbourn and V. R. Syrotiuk. Scheduled persistence for medium
access control in sensor networks. In Proceedings from the First
IEEE International Conference on Mobile Ad hoc and Sensor Systems
(MASSâ€™04), pages 264â€“273, 2004.
[7] S. Even, O. Goldreich, S. Moran, and P. Tong. On the NP-completeness
of certain network testing problems. Networks, 14(1):1â€“24, 1984.
[8] L. C. Freeman. A set of measures of centrality based on betweenness.
Sociometry, 40(1):35â€“41, 1977.
[9] M. Gerla, R. Bagrodia, L. Zhang, K. Tang, and L. Wang. TCP over wireless multi-hop protocols: Simulation and experiments. In Proceedings of
the 1999 IEEE International Conference on Communication (ICCâ€™99),
pages 1089â€“1094, 1999.
[10] J. Hastad, T. Leighton, and B. Rogoff. Analysis of backoff protocols
for multiple access channels. In Proceedings of the 19th annual ACM
Symposium on Theory of Computing (STOCâ€™87), pages 740â€“744, 1987.
[11] IEEE. IEEE 802.11, Wireless LAN medium access control (MAC) and
physical layer (PHY) specifications, 1997.
[12] IEEE. IEEE 802.11e, Enhancements: QoS, including packet bursting,
2007.
[13] J. Boleng, N. Bauer, T. Camp, and W. Navidi. Random Waypoint Steady
State Mobility Generator (mobgen-ss). http://toilers.mines.edu/.
[14] G. Jakllari, M. Neufeld, and R. Ramanathan. A framework for frameless
TDMA using slot chains. In Proceedings of the 9th IEEE International
Conference on Mobile Ad hoc and Sensor Systems (MASSâ€™12), 2012.
[15] J. Ju and V. O. K. Li. An optimal topology-transparent scheduling
method in multihop packet radio networks. IEEE/ACM Transactions on
Networking, 6(3):298â€“305, 1998.
[16] D. Koutsonikolas, J. Dyaberi, P. Garimella, S. Fahmy, and Y. C. Hu.
On TCP throughput and window size in a multihop wireless network
testbed. In Proceedings of the 2nd ACM International Workshop on
Wireless network testbeds, experimental evaluation and characterization
(WiNTECHâ€™07), 2007.
[17] K. Leung and V. O. K. Li. Transmission control protocol (TCP) in
wireless networks: Issues, approaches, and challenges. IEEE Communications Surveys & Tutorials, 8:64â€“79, 2006.
[18] J. Lutz, C. J. Colbourn, and V. R. Syrotiuk. Apples and oranges:
Comparing schedule- and contention-based medium access control. In
Proceedings of the 13th ACM International Conference on Modeling,
Analysis and Simulation of Wireless and Mobile Systems (MSWiMâ€™10),
pages 319â€“326, 2010.
[19] J. Lutz, C. J. Colbourn, and V. R. Syrotiuk. Variable weight sequences
for adaptive scheduled access in MANETs. In Proceedings of Sequences
and their Applications (SETAâ€™12), pages 53â€“64, 2012.
[20] J. Lutz, C. J. Colbourn, and V. R. Syrotiuk. Topological persistence
for medium access control. IEEE Transactions on Mobile Computing,
12(8):1598â€“1612, 2013.
[21] The Network Simulator ns-2. http://www.isi.edu/nsnam/ns/.
[22] M. PioÌro and D. Medhi. Routing, Flow, and Capacity Design in
Communication and Computer Networks. Elsevier Inc., 2004.
[23] R. Ramanathan. A unified framework and algorithm for (T/F/C)DMA
channel assignment in wireless networks. In Proceedings of the 16th
Annual Joint Conference of the IEEE Computer and Communications
Societies (INFOCOMâ€™97), pages 900â€“907, 1997.

[24] I. Rhee, A. Warrier, M. Aia, J. Min, and M. L. Sichitiu. Z-MAC:
A hybrid MAC for wireless sensor networks. IEEE Transactions on
Networking, 16(3):511â€“524, 2008.
[25] I. Rhee, A. Warrier, J. Min, and L. Xu. DRAND: Distributed randomized
TDMA scheduling for wireless ad-hoc networks. IEEE Transactions on
Mobile Computing, 8(10):1384â€“1396, 2009.
[26] R. Rozovsky and P. R. Kumar. SEEDEX: A MAC protocol for ad hoc
networks. In Proceedings of the 2nd ACM International Symposium
on Mobile Ad Hoc Networking and Computing (MOBIHOCâ€™01), pages
67â€“75, 2001.
[27] V. R. Syrotiuk, C. J. Colbourn, and S. Yellamraju. Rateless forward error
correction for topology-transparent scheduling. IEEE/ACM Transactions
on Networking, 16(2):464â€“472, 2008.
[28] A. S. Tanenbaum. Computer Networks. McGraw Hill, fourth edition,
2003.
[29] I. Tinnirello, G. Bianchi, P. Gallo, D. Garlisi, F. Giuliano, and
F. Gringoli. Wireless MAC processors: Programming MAC protocols
on commodity hardware. In Proceedings of the 31st Annual Joint
Conference of the IEEE Computer and Communications Societies (INFOCOMâ€™12), pages 1269â€“1277, 2012.
[30] C. Zhu and M. S. Corson. A five-phase reservation protocol (FPRP) for
mobile ad hoc networks. Wireless Networks, 7(4):371â€“384, 2001.

Jonathan Lutz earned his B.S. in Electrical Engineering from Arizona State University, Tempe, Arizona, in 2000 and his M.S. in Computer Engineering
from the University of Waterloo, Waterloo, Canada,
in 2003. He is currently working on his Ph.D. in
Computer Science at Arizona State University. His
research interests include medium access control in
mobile ad hoc networks.

Charles J. Colbourn earned his Ph.D. in 1980 from
the University of Toronto, and is a Professor of
Computer Science and Engineering at Arizona State
University. He is the author of The Combinatorics
of Network Reliability (Oxford), Triple Systems (Oxford), and 320 refereed journal papers focussing on
combinatorial designs and graphs with applications
in networking, computing, and communications. In
2004, he was awarded the Euler Medal for Lifetime
Research Achievement by the Institute for Combinatorics and its Applications.

Violet R. Syrotiuk earned her Ph.D. in Computer
Science from the University of Waterloo (Canada).
She is an Associate Professor of Computer Science and Engineering at Arizona State University.
Her research has been supported by grants from
NSF, ONR, and DSTO, and contracts with LANL,
Raytheon, General Dynamics, and ATC. She serves
on the editorial boards of Computer Networks and
Computer Communications, as well as on the technical program and organizing committees of several
major conferences sponsored by ACM and IEEE.

1

Hierarchical Recovery in Compressive Sensing
Charles J. Colbourn, Daniel Horsley, and Violet R. Syrotiuk, Senior Member, IEEE

arXiv:1403.1835v1 [cs.IT] 4 Mar 2014

Abstract
A combinatorial approach to compressive sensing based on a deterministic column replacement technique is proposed.
Informally, it takes as input a pattern matrix and ingredient measurement matrices, and results in a larger measurement matrix
by replacing elements of the pattern matrix with columns from the ingredient matrices. This hierarchical technique yields great
flexibility in sparse signal recovery. Specifically, recovery for the resulting measurement matrix does not depend on any fixed
algorithm but rather on the recovery scheme of each ingredient matrix. In this paper, we investigate certain trade-offs for signal
recovery, considering the computational investment required. Coping with noise in signal recovery requires additional conditions,
both on the pattern matrix and on the ingredient measurement matrices.
Index Terms
compressive sensing, hierarchical signal recovery, deterministic column replacement, hash families

I. I NTRODUCTION
Nyquistâ€™s sampling theorem provides a sufficient condition for full recovery of a band-limited signal: sample the signal at a
rate that is twice the band-limit. However, there are cases when full recovery may be achieved with a sub-Nyquist sampling
rate. This occurs with signals that are sparse (or compressible) in some domain, such as those that arise in applications in
sensing, imaging, and communications, and has given rise to the field of compressive sensing [2], [6] (also called compressive
sampling).
Consider the following framework for compressive sensing. An admissible signal of dimension n is a vector in Rn that is
known a priori to be taken from a given set Î¦ âŠ† Rn . A measurement matrix A is a matrix from RmÃ—n . Sampling a signal
x âˆˆ Rn corresponds to computing the product Ax = b. Once sampled, recovery involves determining the unique signal x âˆˆ Î¦
that satisfies Ax = b using only A and b. If Î¦ = Rn , recovery can be accomplished only if A has rank n, and hence m â‰¥ n.
However for more restrictive admissible sets Î¦, recovery may be accomplished when m < n.
Given a measurement matrix A, an equivalence relation â‰¡A is defined so that for signals x, y âˆˆ Rn , we have x â‰¡A y
if and only if Ax = Ay. If for every equivalence class P under â‰¡A , the set P âˆ© Î¦ contains at most one signal then in
principle recovery is possible. Because Ax = Ay ensures that A(x âˆ’ y) = 0, this can be stated more simply: An equivalence
class P of â‰¡A can be represented as {x + y : y âˆˆ N (A)} for any x âˆˆ P , where N (A) is the null space of A, i.e., the set
{x âˆˆ Rn : Ax = 0}. Recoverability is therefore equivalent to requiring that, for every signal x âˆˆ Î¦, there is no y âˆˆ N (A)\{0}
with x + y âˆˆ Î¦.
In order to make use of these observations, a reasonable a priori restriction on the signals to be sampled is identified, suitable
measurement matrices with m â‰ª n are formed, and a reasonably efficient computational strategy for recovering the signal is
provided. A signal is t-sparse if at most t of its n coordinates are nonzero. The recovery of t-sparse signals is the domain
of compressive sensing. An admissible set of signals Î¦ has sparsity t when every signal in Î¦ is t-sparse. An admissible set
of signals Î¦ is t-sparsifiable if there is a full rank matrix B âˆˆ RnÃ—n for which {Bx : x âˆˆ Î¦} has sparsity t. We assume
throughout that when the signals are sparsifiable, a change of basis B is applied so that the admissible signals have sparsity t.
A measurement matrix has (â„“0 , t)-recoverability when it permits exact recovery of all t-sparse signals. A basic problem is
to design measurement matrices with (â„“0 , t)-recoverability where m â‰ª n such that recovery can be accomplished efficiently.
Suppose that measurement matrix A has (â„“0 , t)-recoverability. Then in principle, given A and b, recovery of the signal x can
be accomplished by solving the â„“0 -minimization problem min{||x||0 : Ax = b}. To do so the possible supports of signals
from fewest nonzero entries to most are first listed. For each, reduce A to Aâ€² and x to xâ€² by eliminating coordinates in the
signal assumed to be zero. Examine the now overdetermined system Aâ€² xâ€² = b. When equality holds, a solution is found; we
are guaranteed to find one by considering all possible supports
with at most t nonzero entries. Such an enumerative strategy is

prohibitively time-consuming, examining as many as nt linear systems when the signal has sparsity t. Natarajan [27] showed
that we cannot expect to find a substantially more efficient solution, because the problem is NP-hard.
Instead of the â„“0 -minimization problem, Chen, Donoho, Huo, and Saunders [11], [18] suggest considering the â„“1 -minimization
problem min{||x||1 : Ax = b}. While this can be solved using standard linear programming techniques, to be effective it
is necessary that for each t-sparse signal x, the unique solution to min{||z||1 : Az = Ax} is x. This property is (â„“1 , t)recoverability. A necessary and sufficient condition for (â„“1 , t)-recoverability has been explored, beginning with Donoho and
Huo [18] and subsequently in [19]â€“[21], [24], [30], [31], [33].
C. J. Colbourn and V. R. Syrotiuk are with the School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe,
AZ, U.S.A., 85287-8809, {colbourn,syrotiuk}@asu.edu
D. Horsley is with the School of Mathematical Sciences, Monash University, Vic 3800, Australia, daniel.horsley@monash.edu

2

A measurement matrix A meets the (â„“0 , t)-null space condition if and only if N (A) \ {0} contains no (2t)-sparse vector.
For y âˆˆ Rn and C âŠ‚ {1, . . . , n}, define y|C âˆˆ Rn to be the vector such that (y|C )Î³ = yÎ³ if Î³ âˆˆ C and (y|C )Î³ = 0
otherwise. A measurement matrix A meets the (â„“1 , t)-null space condition if and only if for every y âˆˆ N (A) \ {0} and every
C âŠ‚ {1, . . . , n} with |C| = t, ||y|C ||1 < 12 ||y||1 .
Lemma 1: ([13], for example) Measurement matrix A âˆˆ RmÃ—n has (â„“0 , t)-recoverability if and only if A meets the (â„“0 , t)null space condition.

Lemma 2: ([33], for example) Measurement matrix A âˆˆ RmÃ—n has (â„“1 , t)-recoverability if and only if A meets the (â„“1 , t)null space condition.

To establish (â„“1 , t)-recoverability, and hence also (â„“0 , t)-recoverability, CandeÌ€s and Tao [7], [9] introduced the Restricted
Isometry Property (RIP). For A âˆˆ RmÃ—n , the dth RIP parameter of A, Î´d (A), is the smallest Î´ so that, for some constant R > 0,
(1 âˆ’ Î´)R(||x||2 )2 â‰¤ (||Ax||2 )2 â‰¤ (1 + Î´)R(||x||2 )2 , for all x with ||x||0 â‰¤ d. The dth RIP parameter is better when Î´d (A)
is smaller as the bounds are tighter. The RIP parameters have been employed extensively to establish (â„“1 , t)-recoverability,
particularly for randomly generated measurement
matrices [8]â€“[10], but also for those generated using deterministic conâˆš
structions [12], [17]. Commonly, Î´2t < 2 âˆ’ 1 is required for (â„“1 , t)-recoverability; see [7] for example. The property of
(â„“1 , t)-recoverability in the presence of noise has also been considered. Conditions on the RIP parameters are sufficient but in
general not necessary for recoverability.
Combinatorial approaches to compressive sensing are detailed in [3], [16], [22], [23], [25], [26], [32]. We pursue a different
combinatorial approach here, using a deterministic column replacement technique based on hash families. The use of an
heterogeneous hash family provides an explicit hierarchical construction of a large measurement matrix from a library of
small ingredient matrices. Strengthening hash families provide a means to increase the level of sparsity supporte, allowing the
ingredient matrices to be designed for lower sparsity than the larger measurement matrix produced.
In this paper we show that the heterogeneity extends to signal recovery: it is interesting that the ingredient measurement
matrices need not all employ the same recovery algorithm. This enables hierarchical recovery for the large measurement matrix;
however, this can be computationally prohibitive. By restricting the hash family to be linear, recovery for the large measurement
matrix can be achieved in sublinear time even when computationally intensive methods are used for each ingredient matrix. To
be practical, recovery methods based on hash families must deal with noise in the signal effectively. Suitable restrictions on
the hash family and on each ingredient matrix used in the hierarchical method are shown to be sufficient to permit recovery
in the presence of noise.
The rest of this paper is organized as follows. The results on homogeneous hash families in Section II demonstrate that
a recovery scheme based on (â„“0 , t)- or (â„“1 , t)-recoverability can be â€˜liftedâ€™ from the ingredient measurement matrices to the
matrix resulting from column replacement. Section III considers a generalization of hash families to allow for ingredient
matrices with other recovery algorithms, and the computational investment to recover the signal. Signal recovery without noise
is considered first, and the conditions for a sublinear time recovery algorithm described. Section IV considers the recovery of
almost-sparse signals to deal with noise in the signal. Finally, Section V draws relevant conclusions.
II. H ASH FAMILIES

AND

C OMPRESSIVE S ENSING

A. Column Replacement and Hash Families for Compressive Sensing
Let A âˆˆ RrÃ—k , A = (aij ), be an ingredient matrix. Let P âˆˆ {1, . . . , k}mÃ—n , P = (pij ), be a pattern matrix. The columns
of A are indexed by elements of P . For each row i of P , replace element pij with a copy of column pij of A. The result is
an rm Ã— n matrix B, the column replacement of A into P . Fig. 1 gives an example of column replacement.
ï£¹
ï£®
a11 a12 a13 a11




ï£¯ a21 a22 a23 a21 ï£º
1231
a11 a12 a13
ï£º
B=ï£¯
P =
A=
ï£° a13 a11 a12 a11 ï£»
a21 a22 a23
3121
a23 a21 a22 a21

Fig. 1.

B is the column replacement of A into P .

When the ingredient matrix A is a measurement matrix that meets one of the null space conditions for a given sparsity, our
interest is to ensure that the sparsity supported by B is at least that of A. Not every pattern matrix P suffices for this purpose.
Therefore, we examine the requirements on P .
Let m, n, and k be positive integers. An hash family HF(m; n, k), P = (pij ), is an m Ã— n array, in which each cell contains
one symbol from a set of k symbols. An hash family is perfect of strength t, denoted PHF(m; n, k, t), if in every m Ã— t

3

subarray of P at least one row consists of distinct symbols; see [1], [28]. Fig. 2 gives an example of a perfect hash family
PHF(6; 12, 3, 3). For example, for the 6 Ã— 3 subarray involving columns 4, 5, and 6, only the fourth row consists of distinct
symbols.

â†’
Fig. 2.

0
0
1
2
2
2

1
2
0
0
0
0

â†“
2
0
2
1
1
2

2
1
0
1
2
1

â†“
1
2
2
2
2
1

â†“
2
2
2
0
1
1

2
2
1
2
0
2

0
1
1
0
2
2

1
0
2
1
2
0

1
1
1
1
1
1

0
2
0
2
1
2

0
1
2
1
0
1

A perfect hash family PHF(6; 12, 3, 3).

A perfect hash family has at least one row that separates the t columns into t parts in every m Ã— t subarray. A weaker
conditionP
separates the t columns into classes. A {w1 , . . . , ws }-separating hash family, denoted SHF(m; n, k, {w1 , . . . , ws }),
s
with t = i=1 wi , is an m Ã— n array on k symbols in which for every m Ã— t subarray, and every way to partition the t columns
into classes of sizes w1 , . . . , ws , there is at least one row in which no two classes contain the same symbol; see [4], [29]. A
W-separating hash family, denoted SHF(m; n, k, W), is a {w1 , . . . , ws }-separating hash family for each {w1 , . . . , ws } âˆˆ W.
Fig. 3 gives an example of a {1, 2}-separating hash family SHF(3; 16, 4, {1, 2}). For the 3 Ã— 3 subarray consisting of columns
11, 15, and 16, for example, the last row separates columns {11, 16} from column {15}.

â†’
Fig. 3.

1
1
1

1
2
2

1
3
3

1
4
4

2
1
2

2
2
1

2
3
4

2
4
3

3
1
3

â†“
3
3
1

3
2
4

3
4
2

4
1
4

4
2
3

â†“
4
3
2

â†“
4
4
1

A {1, 2}-separating hash family SHF(3; 16, 4, {1, 2}).

Ps
A distributing hash family DHF(m; n, k, t, s) is an SHF(m; n, k, W) with W = {{w1 , . . . , ws } : t = i=1 wi }. Fig. 4
gives an example of a DHF(10; 13, 9, 5, 2). For the 10 Ã— 5 subarray consisting of columns 8 through 12, row 4 separates
columns {8, 9, 10, 11} from column {12} (a {1, 4}-separation), and row 5 separates columns {8, 9, 12} from columns {10, 11}
(a {2, 3}-separation).

a {1, 4}-separation â†’
a {2, 3}-separation â†’

Fig. 4.

6
3
8
0
0
1
1
1
0
0

7
1
5
2
0
1
0
1
0
0

8
1
1
0
2
2
1
0
3
0

3
7
4
2
1
2
2
1
0
0

4
2
2
2
1
2
0
0
1
0

0
6
3
0
1
0
0
4
0
1

2
8
2
0
2
1
2
2
0
0

â†“
2
4
6
1
0
0
0
0
2
0

â†“
3
3
7
1
0
0
0
2
4
1

â†“
0
0
0
1
2
2
1
0
0
0

â†“
5
2
1
1
2
1
2
1
0
0

â†“
1
0
3
2
0
0
2
0
1
0

1
5
0
0
1
0
1
2
0
1

A distributing hash family DHF(10; 13, 9, 5, 2).

Now, we are in a position to state the requirements on a pattern matrix P that ensure that the sparsity supported by the
matrix B resulting from column replacement is at least that of A.
Theorem 1: [13] Suppose that A is an r Ã— k measurement matrix that meets the (â„“0 , t)-null space condition, that P is an
SHF(m; n, k, {1, t}), and that B is the column replacement of A into P . Then B is an rm Ã— n measurement matrix that meets
the (â„“0 , t)-null space condition.

Theorem 2: [13] Suppose that A is an r Ã— k measurement matrix that meets the (â„“1 , t)-null space condition, that P is a
DHF(m; n, k, t + 1, 2), and that B is the column replacement of A into P . Then B is an rm Ã— n measurement matrix that
meets the (â„“1 , t)-null space condition.

4

B. Exploiting Heterogeneity in Column Replacement
All the standard definitions of hash families may be generalized by replacing k by k = (k1 , . . . , km ), a tuple of positive
integers. Now, an heterogeneous hash family HF(m; n, k), P = (pij ), is an m Ã— n array in which each cell from row i contains
one symbol from a set of ki symbols, 1 â‰¤ i â‰¤ m.
Column replacement may be extended to exploit heterogeneity in an hash family. Let P = (pij ) be an HF(m; n, k) and, for
1 â‰¤ i â‰¤ m, let Ai be an ri Ã— ki ingredient matrix whose columns are indexed by the ki elements P
in row i of P . For each
row i of P , replace the element pij with a copy of column pij of Ai , 1 â‰¤ j â‰¤ n. The result is a ( m
i=1 ri ) Ã— n matrix B,
the column replacement of A1 , . . . , Am into P . Fig. 5 gives an example of column replacement using an heterogeneous hash
family.

P =

Fig. 5.



132123
111222



A1 =



a111 a112 a113
a121 a122 a123

B is the column replacement of A1 , A2 into P .



A2 =



a211 a212
a221 a222



ï£¹
a111 a113 a112 a111 a112 a113
ï£¯ a121 a123 a122 a121 a122 a123 ï£º
ï£º
B=ï£¯
ï£° a211 a211 a211 a212 a212 a212 ï£»
a221 a221 a221 a222 a222 a222
ï£®

An hierarchical method for compressive sensing is obtained using column replacement in an heterogeneous hash family.
Suppose that Ai is a measurement matrix for a signal of dimension ki supporting the recovery of sparsity qi , for 1 â‰¤ i â‰¤ m.
We now describe the properties the pattern matrix needs to satisfy to support recovery of signals of dimension n and sparsity
t.
In Section II-A, we saw that a perfect hash family separates t columns into t parts, and that a separating hash family
separates t columns into classes. We now define a particular type of separating hash family in which the number of symbols
used to accomplish the separations is restricted.
Let d = (d1 , . . . , dm ) be a tuple of positive integers, and let Ï„ be a positive P
integer. Let W = {W1 , . . . , Wr }, where for
si
1 â‰¤ i â‰¤ r, Wi = {wi1 , . . . , wisi } is a multiset of nonnegative integers, and Ïƒi = j=1
wij . An SHF(m; n, k, W), P = (pij ),
is (d, Ï„ )-strengthening if whenever 1 â‰¤ i â‰¤ r,
â€¢ C is a set of Ïƒi columns,
â€¢ C1 , . . . , Csi is a partition of C with |Cj | = wij for 1 â‰¤ j â‰¤ si , and
â€¢ T is a set of Ï„ columns with |C âˆ© T | = min(Ïƒi , Ï„ ),
there exists a row Ï for which pÏx 6= pÏy whenever x âˆˆ Ce , y âˆˆ Cf and e 6= f and the multiset {pÏx : x âˆˆ T } contains no
more than dÏ different symbols. When Ï„ = max{Ïƒi : 1 â‰¤ i â‰¤ r}, we omit Ï„ and write d-strengthening. Because rows of P
can be arbitrarily permuted (while permuting the ingredient matrices in the same manner), the order of elements in k and d is
inconsequential. Hence we often use exponential notation, writing xu1 1 Â· Â· Â· xus s , with ui a non-negative integer for 1 â‰¤ i â‰¤ s,
Pâ„“âˆ’1
Pâ„“
Ps
for a vector (y1 , . . . , yPsj=1 uj ) in which yâ„“ = xj for j=1 uj < â„“ â‰¤ j=1 uj for 1 â‰¤ â„“ â‰¤ j=1 uj .
Fig. 6 gives a heterogeneous d-strengthening DHF(19; 13, k, 5, 2) with k = (56 41 312 ) and d = (46 313 ). This is equivalent
to a d-strengthening SHF(19; 13, k, {{1, 4}, {2, 3}}). Consider the separation of columns {1, 7} from columns {2, 6, 11}. Row
8 accomplishes the required separation because it uses no more than d8 = 3 symbols. Consider instead columns {1, . . . , 5}.
While the first row separates {1, 2, 3} from {4, 5}, it uses 5 symbols instead of d1 = 4 and so does not accomplish the required
separation; this separation is accomplished in row 3.
Next the properties are determined for an heterogeneous hash family to support recovery of signals of dimension n and
sparsity t using a column replacement technique.
Theorem 3: [14] Let k = (k1 , . . . , km ) and q = (q1 , . . . , qm ) be tuples of positive integers. Let d = (2q1 , . . . , 2qm ). For
1 â‰¤ i â‰¤ m, let Ai âˆˆ Rri Ã—ki be a measurement matrix that meets the (â„“0 , qi )-null space condition. Let P be a (d, 2t)strengthening SHF(m; n, k, {1, t}), and let B be the column replacement of A1 , . . . , Am into P . Then B meets the (â„“0 , t)-null
space condition.
Theorem 4: [14] Let k = (k1 , . . . , km ) and q = (q1 , . . . , qm ) be tuples of positive integers. For 1 â‰¤ i â‰¤ m, let Ai âˆˆ Rri Ã—ki
be a measurement matrix that meets the (â„“1 , qi )-null space condition. Let P be a (q, t)-strengthening DHF(m; n, k, t + 1, 2),
and let B be the column replacement of A1 , . . . , Am into P . Then B meets the (â„“1 , t)-null space condition.

Revisiting the d-strengthening DHF(19; 13, k, 5, 2) pattern matrix in Fig. 6, the results of Theorems 3 and 4 indicate that
the number of symbols in each row need not be the same. In general, there may be as many ingredient matrices Ai as there
are rows of the pattern matrix P . Moreover, the strength of each ingredient matrix Ai may be different! In this example, the

5

k1 = . . . = k6 = 5 symbols;
d1 = . . . = d6 = 4 used to separate

k2 = 4 symbols; d2 = 3 used to separate
â†’

k8 = . . . = k19 = 3 symbols;
d8 = . . . = d19 = 3 used to separate

Fig. 6.

â‡“
4
0
0
2
2
3
0
0
1
0
0
0
2
2
0
1
1
2
0

â†“
0
0
2
4
1
4
0
1
0
1
2
1
1
1
0
2
0
2
0

2
1
4
1
2
0
1
0
2
2
2
1
0
2
0
0
2
0
2

1
1
1
0
2
1
0
1
0
2
2
0
1
0
1
1
1
0
1

3
2
1
3
4
0
0
1
0
1
1
1
2
2
0
1
1
1
1

â†“
3
3
2
0
0
3
2
2
2
2
2
2
0
2
1
1
0
2
0

â‡“
0
1
0
3
0
2
2
0
1
1
0
1
1
0
1
2
0
1
1

0
3
1
1
4
4
0
2
1
0
0
0
2
0
2
2
2
0
2

1
2
2
1
0
2
0
0
0
0
1
0
0
1
0
0
0
0
0

4
4
3
4
1
1
1
0
2
0
1
2
2
0
1
0
0
1
2

â†“
2
2
0
2
1
1
3
2
0
1
0
0
0
0
2
0
2
2
2

2
0
3
0
3
2
0
1
2
0
0
2
0
1
2
2
1
1
1

1
4
4
2
3
0
0
2
1
2
1
2
1
1
2
0
0
2
1

A heterogeneous d-strengthening DHF(19; 13, k, 5, 2) with k = (56 41 312 ) and d = (46 313 ).

first 6 rows use 4 symbols to separate, so the corresponding ingredient matrices must have strength at least 4. The remaining
rows use 3 symbols to separate, so the corresponding ingredient matrices must have strength at least 3.
In [14], we showed that heterogeneity gives great flexibility in construction of measurement matrices using column replacement. The hierarchical structure of the measurement matrices produced by column replacement can also aid in recovery, and
be used to support hybrid recovery schemes. We examine this problem next, considering a generalization of hash families that
removes the restriction to those strategies based only on (â„“0 , t)- or (â„“1 , t)-recoverability. We also consider the computational
investment required to recover the signal.
III. H ASH FAMILIES FOR R ECOVERY
In order to tackle signal recovery, we require another generalization of hash families. As before, let k = (k1 , . . . , km ) be a
tuple of positive integers. An HFâ—¦ (m; n, k) is an m Ã— n array, P = (pij ), in which each cell contains one symbol, and for each
row 1 â‰¤ i â‰¤ m, {pij : 1 â‰¤ j â‰¤ n} âŠ† {â—¦, 1, . . . , ki }. The symbol â—¦, when present, is interpreted as representing a â€˜missingâ€™
entry. When the pattern matrix P = (pij ) is an HFâ—¦ (m; n, k), and for 1 â‰¤ i â‰¤ m the ingredient matrix Ai is ri Ã— ki with
columns indexed by the ki symbols in row i of P other than â—¦, the column replacement of A1 , . . . , Am into P is as before,
except that when pij = â—¦, it is replaced with an all zero column vector of length ri . As we will see, the separating properties
of the hash families we use allow us to locate the nonzero coordinates of the signal and hence perform the recovery.
The definition of a W-separating hash family encompasses perfect, {w1 , . . . , ws }-separating, and distributing hash families.
Therefore, we need only extend the definition of W-separating hash families to include the â—¦ symbol. To do so, we allow
some of the elements of the multisets in W to be marked with a â—¦ superscript to form a set of marked multisets W â€² ; the
multisets in W â€² are indexed. Then an HFâ—¦ (m; n, k) is W â€² -separating if, for each {w1 , . . . , ws } âˆˆ W (with some elements
possibly marked),
Ps
â€¢ whenever C is a set of
i=1 wi columns, and
â€¢ C1 , . . . , Cs is an (indexed) partition of C with |Ci | = wi for 1 â‰¤ i â‰¤ s
then there exists a row that separates C1 , . . . , Cs in which, for 1 â‰¤ j â‰¤ s, if â—¦ appears in a column in Cj then wj is marked.
As we will see, to recover the signal, the idea is to effect a separation where a significant coordinate of the signal is present
in one class such that any other class does not prevent its recovery.
A. Signal Recovery without Noise
Theorems 3 and 4 suggest that a recovery scheme based on (â„“0 , t)- or (â„“1 , t)-recoverability can be â€˜liftedâ€™ from the ingredient
measurement matrices A1 , . . . , Am to the larger measurement matrix B obtained from column replacement. However, such a
method appears to have two main drawbacks. First, it is restricted to recovery strategies based on (â„“0 , t)- or (â„“1 , t)-recoverability.
Secondly, and perhaps more importantly, it appears to necessitate a large computational investment to recover the signal, given
B.

6

In order to overcome these problems, we consider two cases. The positive case arises when the signal is known a priori to
be in Rnâ‰¥0 . The general case arises when the signal can be positive, negative, or zero. In each case we develop a recovery
scheme for the matrix B resulting from column replacement that does not depend on any fixed algorithm, but rather on the
recovery schemes for the ingredient matrices A1 , . . . , Am .
We suppose that P = (pij ) is an HFâ—¦ (m; n, k). For each 1 â‰¤ i â‰¤ m, we suppose that Ai is an ri Ã— ki measurement matrix
that has (â„“0 , t)-recoverability, equipped with a recovery algorithm Ri that determines the unique t-sparse vector zi that solves
Ai zi = yi . We further suppose that B is the column replacement of A1 , . . . , Am into P , and that y is the result of sampling
an (unknown) t-sparse vector x = (x1 , . . . , xn ) using B.
For 1 â‰¤ i â‰¤ m, the ith row of P induces a partition {Siâ—¦ , Si1 , . . . , Siki } of the column indices {1, . . . , n}, where SiÏƒ =
{j : pij = Ïƒ, 1 â‰¤ j â‰¤ n} for Ïƒ âˆˆ {â—¦, 1, . . . , ki }. Assume that we have employed the recovery algorithms Ri to find solutions
zi . For 1 â‰¤ i â‰¤ m and Ïƒ âˆˆ {â—¦, 1, . . . , ki }, the partition class SiÏƒ is discarded if Ïƒ = â—¦, insignificant if Ïƒ 6= â—¦ and ziÏƒ = 0,
significant positive if ziÏƒ > 0, and significant negative if ziÏƒ
P< 0.
For 1 â‰¤ i â‰¤ m, let wi = (wi1 , . . . , wiki ) where wiÏƒ = jâˆˆSiÏƒ xj . The vector wi can be considered as a projection of x
induced by the symbol pattern in row i of P . These facts follow:
i
â€¢ For 1 â‰¤ i â‰¤ m, by the definition of B and because Bi x = yi , zi = wi is a solution to A zi = yi .
i
â€¢ For 1 â‰¤ i â‰¤ m, because A has (â„“0 , t)-recoverability and wi is t-sparse (because x is t-sparse), zi = wi is the unique
solution to Ai zi = yi , and so Ri returns wi .
We now consider the positive case and the general case for recovery in succession.
B. Signal Recovery: The Positive Case
We establish that in the positive case with t-sparse signals, it suffices to use a separating hash family of suitable strength,
along with suitable ingredient matrices. An SHFâ—¦ (m; n, k, {1, tâ—¦ }) separates t + 1 columns into two parts, one part of size one
that cannot include the symbol â—¦, and the other of size t that may include â—¦.
Theorem 5: Suppose that P is an SHFâ—¦ (m; n, k, {1, tâ—¦ }). For 1 â‰¤ i â‰¤ m, let Ai âˆˆ Rri Ã—ki be a measurement matrix that
has (â„“0 , t)-recoverability equipped with a recovery algorithm Ri that determines the unique t-sparse vector zi that solves
Ai zi = yi . Further suppose that B is the column replacement of measurement matrices A1 , . . . , Am into P and that y is the
result of sampling an (unknown) t-sparse vector x = (x1 , . . . , xn ) âˆˆ Rnâ‰¥0 using B. Then the t-sparse solution x to Bx = y
can be recovered.

Proof: It suffices to determine whether xi is positive or zero for each 1 â‰¤ i â‰¤ n, because once this is accomplished we
can find the values of the positive xi by solving the overdetermined system that remains. For 1 â‰¤ i â‰¤ m, apply recovery
algorithm Ri to find the unique t-sparse vector zi such that Ai zi = yi . We claim that, for 1 â‰¤ â„“ â‰¤ n, xâ„“ is positive if and
only if for each i âˆˆ {1, . . . , m} the partition class that contains â„“ is either significant positive or discarded.
Suppose
P first that xâ„“ is positive. If SiÏƒ is a partition class that contains â„“, then either Ïƒ = â—¦ and SiÏƒ is insignificant or Ïƒ 6= â—¦,
ziÏƒ = jâˆˆSiÏƒ xj â‰¥ xâ„“ > 0, and SiÏƒ is significant positive. Now suppose that xâ„“ = 0. Let C = {j : xj > 0, 1 â‰¤ j â‰¤ n};
|C| â‰¤ t. There must be a row Ï of P that separates C from {â„“} such that pÏâ„“ 6= â—¦. Let Ïƒ = pÏâ„“ . Then â„“ âˆˆ SÏÏƒ and SÏÏƒ âˆ©C = âˆ…,
so SÏÏƒ is insignificant.
One useful application of Theorem 5 takes the pattern matrix P to be an SHFâ—¦ (m; n, 1, {1, tâ—¦ }), and each Ai to be a 1 Ã— 1
matrix whose only element is 1; in this case, column replacement yields a matrix B isomorphic to P . In P for every column
Î³ and every set C of t columns with Î³ 6âˆˆ C, there is a row in which all columns of C contain â—¦, while column Î³ contains
1. Then the measurement matrices Ai have (â„“0 , t)-recoverability and the recovery algorithms Ri are trivial. Hence in these
cases, a matrix isomorphic to P itself supports recovery.
Theorem 5 leads to a straightforward recovery algorithm. First, Ri is used to solve Ai zi = yi for 1 â‰¤ i â‰¤ m. Then the
classes Sij areP
classified as positive when zij > 0, discarded when j = â—¦, and insignificant when j 6= â—¦ and zij = 0; this can
m
be done in O( i=1 ki ) time. We need only compute, for each row, the complement of the union of the insignificant classes,
and then compute the intersection over all rows of these complements. However, without additional structure this appears to
require the examination of each coordinate; hence, this gives an â„¦(n) lower bound.
It is not difficult, nevertheless, to obtain sublinear recovery times by restricting the hash family; we return to this problem
in Section III-D.
C. Signal Recovery: The General Case
When the signal takes on both positive and negative values, cancellation of positive and negative contributions can yield a
zero measurement despite the presence of a signal. Nevertheless, an additional requirement on the structure of the hash family

7

suffices to address this problem, as we show next.
Theorem 6: Suppose that P is an SHFâ—¦ (m; n, k, {{Ï„, (t + 1 âˆ’ Ï„ )â—¦ } : 1 â‰¤ Ï„ â‰¤ t}). For 1 â‰¤ i â‰¤ m, let Ai âˆˆ Rri Ã—ki be a
measurement matrix that has (â„“0 , t)-recoverability equipped with a recovery algorithm Ri that determines the unique t-sparse
vector zi that solves Ai zi = yi . Further suppose that B is the column replacement of measurement matrices A1 , . . . , Am into
P and that y is the result of sampling an (unknown) t-sparse vector x = (x1 , . . . , xn ) using B. Then the t-sparse solution x
to Bx = y can be recovered.

Proof: As in the proof of Theorem 5, it suffices to determine whether xi is nonzero or zero for each 1 â‰¤ i â‰¤ n,
because once this is accomplished we can find the values of the nonzero xi by solving the overdetermined system that
remains. For 1 â‰¤ i â‰¤ m, apply recovery algorithm Ri to find the unique t-sparse vector zi such that Ai zi = yi . Let
âˆ’
z+
i = (max(0, zij ) : 1 â‰¤ j â‰¤ ki ) and zi = (min(0, zij ) : 1 â‰¤ j â‰¤ ki ).
+
â€²
A row i of P is maximum positive if ||z+
i ||1 â‰¥ ||ziâ€² ||1 for 1 â‰¤ i â‰¤ m. Let M âŠ† {1, . . . , m} index the maximum positive
rows. We claim that a coordinate xâ„“ is positive if and only if, for every Ï âˆˆ M , â„“ is in a significant positive class of the
partition induced by row Ï.
Suppose first that xâ„“ is positive and let Ï âˆˆ M . Because Ï indexes a maximum positive row, the partition class induced
by row Ï that contains â„“ is not discarded and does not contain the index of any negative variable. Thus it is in a significant
positive partition class.
Now suppose that xâ„“ â‰¤ 0. Because P is an SHFâ—¦ (m; n, k, {{Ï„, (t + 1 âˆ’ Ï„ )â—¦ } : 1 â‰¤ Ï„ â‰¤ t}) and x is t-sparse, there is a row
Ï of P that separates {j : xj > 0, 1 â‰¤ j â‰¤ n} from {j : xj < 0, 1 â‰¤ j â‰¤ n} âˆª {â„“} in which the symbol â—¦ only appears in a
subset of the columns indexed by {j : xj < 0, 1 â‰¤ j â‰¤ n} âˆª {â„“}. It follows that Ï is a maximum positive row of P and that
the partition class induced by Ï containing â„“ does not contain the index of any positive coordinate. So Ï âˆˆ M , but â„“ is not in
a significant positive class of the partition induced by row Ï.
In the same manner, all negative coordinates can be identified using maximum negative rows.

Again a straightforward recovery algorithm is given by Theorem 6 but, as in the positive case, it naively involves examining
each of the n coordinates.
D. Sublinear Time Signal Recovery
Recovery can be accomplished in time that is sublinear in k when the hash family has suitable structure; we develop a
general approach, and one example, here. In each case, for some subset M of the rows of P , sets are identified that must
contain the indices of all positive coordinates (the indices of the negative coordinates, if they exist, can be located similarly).
Recall from Section III-A, that the positive case arises when the signal is known a priori to be in Rnâ‰¥0 and the general case
arises when the signal can be positive, negative, or zero. In the positive case, M contains all rows and for Ï âˆˆ M , the candidate
indices are VÏ+ = {â„“ : pÏâ„“ = â—¦, or xâ„“ âˆˆ SÏj and zÏj > 0}. In the general case, M contains all rows that index maximum
positive rows, and for Ï âˆˆ M , the candidate indices are VÏ+ = {â„“ : xâ„“ âˆˆ SÏj and zÏj > 0}. In both cases, we are to determine
T
+
we do not list the members of VÏ+ explicitly, but rather use
ÏâˆˆM VÏ . In order to avoid the examination of each
T coordinate,
+
an implicit representation to list the members of ÏâˆˆM VÏ .
First we give an implicit representation of an hash family HF(q + 1; q Î± , q), P , where q is a prime power and 2 â‰¤ Î± â‰¤ q.
Let {Ï‰0 , . . . , Ï‰qâˆ’1 } be the elements of the finite field of order q, Fq . Index the rows of P by {âˆ} âˆª {Ï‰0 , . . . , Ï‰qâˆ’1 }. Index
the columns of P by the q Î± polynomials of degree less than Î± in indeterminate x, with coefficients in Fq . Now the entry of
P with row index Î² and column indexed by polynomial f (x) is determined as f (Î²) when Î² âˆˆ {Ï‰0 , . . . , Ï‰qâˆ’1 }, and as the
coefficient of xÎ±âˆ’1 in f (x) when Î² = âˆ.
By deleting rows, we form an HF(m; q Î± , q) for some 1 â‰¤ m â‰¤ q + 1. An hash family is linear if it is obtained in this way.
The separation properties of such an hash family are crucial [1], [5]. For our purposes, the observation of interest is from [15]:
if m â‰¥ (Î± âˆ’ 1)w1 w2 + 1, then a linear HF(m; n, q) is {w1 , w2 }-separating. (This can be established by a simple argument:
When two polynomials of degree less than Î± evaluate to the same value at Î± different points, they are the same polynomial.) In
some cases, fewer rows suffice to ensure separation. In particular, Blackburn and Wild [5] establish that when q is sufficiently
large, one needs at most Î±(w1 + w2 âˆ’ 1) rows; and in [15] specific small separations are examined to determine the set of
prime powers for which various numbers of rows less than (Î± âˆ’ 1)w1 w2 + 1 suffice. We proceed with the general statement
so as not to impose additional conditions.
When m â‰¥ (Î± âˆ’ 1)t + 1, P is {1, t}-separating; in addition, every {1, t âˆ’ 1}-separation is accomplished in at least Î± rows.
t+1
When m â‰¥ (Î± âˆ’ 1)âŒŠ t+1
2 âŒ‹âŒˆ 2 âŒ‰+ 1, P is {w, t+ 1 âˆ’ w}-separating for each 1 â‰¤ w â‰¤ t; in addition, every {w, tâˆ’ w}-separation
t+1
t
t
t+1
is accomplished in at least Î± rows, because âŒŠ t+1
2 âŒ‹âŒˆ 2 âŒ‰ = âŒŠ 2 âŒ‹âŒˆ 2 âŒ‰ + âŒŠ 2 âŒ‹. Thus in either case, M contains at least Î± rows
of P .

8

Q
Choose any Î± rows U = {Ïˆ1 , . . . ÏˆÎ± } âŠ† M . Now consider the sets {VÏˆ+ : Ïˆ âˆˆ U }. Define ÏˆâˆˆU |VÏˆ+ | vectors V + =
{(g1 , . . . , gÎ± ) : gi âˆˆ {pÏˆi â„“ : â„“ âˆˆ VÏˆ+i } for 1 â‰¤ i â‰¤ Î±}. Each (g1 , . . . , gÎ± ) âˆˆ V + defines a unique column of the hash family,
corresponding to the unique polynomial L of degree at most Î± âˆ’ 1 satisfying L(Ïˆi ) = gi for 1 â‰¤ i â‰¤ Î±. Any column that does
not arise in this way from a member of V + cannot be the column for a positive coordinate, because in the partition induced
by one of the selected maximum rows it is not in a significant positive class. However, columns arising from vectors in V +
need not arise from positive coordinates, because we may not have examined all of the rows of M . Nevertheless, we can now
generate each of the columns arising from vectors in V + , and check for each whether it occurs in positive classes for all rows
of M , not just the Î± selected.
Now |V + | is O(tÎ± ), so when t is o(q), the size of V + is o(n) (because n = q Î± ). For concreteness, taking q = tÎ² for t a
prime power, we can permit Î± to be as large as tÎ²âˆ’2 . (For the positive case, we can permit Î± to be as large as tÎ²âˆ’1 . ) Hence,
by restricting the hash family to one that is linear, it is possible to obtain recovery of the signal in sublinear time.
In general, a hash family together with its ingredient matrices can be represented more concisely compared to a random
measurement matrix for signal recovery. Furthermore, the hash family is an integer matrix, not a matrix of real numbers,
and may therefore be easier to encode. When the hash family is linear an implicit representation of it may be used, further
compacting its representation.
The results of this section provide some evidence that column replacement enables recoverability conditions to be met. In
Section IV, we show that it also preserves the basic machinery to deal with noise in the signal.
E. Adding Strengthening
As the signal length increases, it is natural to support high sparsity. Yet the techniques developed until this point only
preserve sparsity. Strengthening hash families provide a means to increase the level of sparsity supported.
Theorem 7: Suppose that P is a d-strengthening SHF(m; n, k, {{Ï„, (t + 1 âˆ’ Ï„ )} : 1 â‰¤ Ï„ â‰¤ t}). For each 1 â‰¤ i â‰¤ m, we
suppose that Ai is an ri Ã— ki measurement matrix that has (â„“1 , di )-recoverability, equipped with a recovery algorithm Ri , that
either determines the unique di -sparse vector zi that solves Ai zi = yi or indicates that no such vector exists. Further suppose
that B is the column replacement of A1 , . . . , Am into P , and that y is the result of sampling an (unknown) t-sparse vector
x = (x1 , . . . , xn ) using B. Then the t-sparse solution x to Bx = y can be recovered.

Proof: Again it suffices to locate the nonzero coordinates of x. For 1 â‰¤ i â‰¤ m, if recovery algorithm Ri returns a solution
zi such that ||zi ||1 â‰¥ ||z||1 for any solution z returned by an oracle Rj , then zi is a maximum solution, and row i of P is a
maximum row. Because P is a d-strengthening SHF(m; n, k, {{Ï„, (t + 1 âˆ’ Ï„ )} : 1 â‰¤ Ï„ â‰¤ t}), and x is t-sparse, there is a row
Ï of P that separates {j : xj > 0, 1 â‰¤ j â‰¤ n} from {j : xj < 0, 1 â‰¤ j â‰¤ n} with the property that at most dÏ symbols appear
in the columns indexed by {j : xj 6= 0, 1 â‰¤ j â‰¤ n}. So the projected vector wÏ is dÏ -sparse and it is the solution returned by
RÏ . By the definition of Ï, ||wÏ ||1 = ||x||1 . It follows that the â„“1 -norm of any maximum solution is at least ||x||1 .
We claim that if Ri returns a maximum solution zi , then zi = wi . Suppose otherwise. Then, because zi is a maximum
solution, we have ||zi ||1 â‰¥ ||x||1 . Further, it is clear from the definition of ||wi || that ||wi ||1 â‰¤ ||x||1 . Thus Ai zi = Ai wi , zi
is di -sparse, and ||zi ||1 â‰¥ ||wi ||1 , which is a contradiction to the fact that Ai has (â„“1 , di )-recoverability.
Having established our claim, we can now use arguments similar to those used in the proof of Theorem 6 to show that
a coordinate xâ„“ is positive (negative) if and only if, for every maximum row in P , â„“ is in a significant positive (significant
negative) class of the partition induced by that row.
IV. R ECOVERY

WITH

N OISE

We now treat the recovery ofPsignals with noise. A signal (x1 , . . . , xn ) is (s, t)-almost sparse if there is a set T of at most
t coordinate indices such that iâˆˆ{1...,n}\T |xi | < s.

Theorem 8: Suppose that P is an SHF(m; n, k, {{Ï„, (t + 1 âˆ’ Ï„ )} : 1 â‰¤ Ï„ â‰¤ t}). For each 1 â‰¤ i â‰¤ m, we suppose that Ai
is an ri Ã— ki measurement matrix, equipped with recovery algorithm Ri , which, when applied to the sample obtained from an
(s, t)-almost sparse signal xi , returns a vector zi such that ||zi âˆ’ xi ||1 < Ç«. Further suppose that B is the column replacement
of A1 , . . . , Am into P , and that y is the result of sampling an (unknown) (s, t)-almost sparse vector x = (x1 , . . . , xn ) using B.
Then, a (perfectly) t-sparse vector xâˆ— = (xâˆ—1 , . . . , xâˆ—n ) such that for 1 â‰¤ i â‰¤ n, |xi | < 2(s + Ç«) if xâˆ—i = 0, and |xi âˆ’ xâˆ—i | < s + Ç«
if xâˆ—i > 0, and such that Bxâˆ— = y, can be recovered.
Proof: We provide a sketch first, and then the details. The idea is to write each coordinate of z as a sum of the signal
coordinates in T that contribute to it, and of a noise term e that includes both the small contributions from coordinates outside
T and the error less than Ç« from the recovery algorithm. For each row Ï of P , we then split this sum into two parts: one part

9

containing terms with the same sign as the z coordinate to which they contribute (indexed by sets TÏâ€² and EÏâ€² ), and another part
containing terms with the opposite sign to the z coordinate to which they contribute (indexed by sets TÏâ€²â€² and EÏâ€²â€² ). The key
observation is that the sum of the terms with indices in TÏâ€²â€² can be approximated by 12 (||x|| âˆ’ ||zÏ ||) and hence by 21 (q âˆ’ ||zÏ ||)
â€²â€²
because if TÏâ€²â€² is empty then zÏ has norm close to ||x||, and
Pevery term with index in T+Ï reduces ||zÏ ||.
Let T be a set of at most t coordinate indices such that iâˆˆ{1...,n}\T |xi | < s. Let T = {i âˆˆ T : xi â‰¥ 0}, T âˆ’ = {i âˆˆ T :
P
xi < 0} and q â€  = iâˆˆT |xi |. For 1 â‰¤ i â‰¤ m, apply Ri to yi to find a vector zi such that ||zi âˆ’ wi ||1 < Ç«. For i âˆˆ {1, . . . , m},
call ||zi ||1 the signature of row i of P and let q be the maximum signature of any row of P .
For 1 â‰¤ i â‰¤ n, we calculate upper and lower estimates u(i) and â„“(i) for xi . For each row index Ï âˆˆ {1, . . . , m} and each
symbol Ïƒ âˆˆ {1, . . . , kÏ } we define uÏÏƒ and â„“ÏÏƒ as follows.
1
1
â€¢ If zÏÏƒ â‰¥ 0, then uÏÏƒ = |zÏÏƒ | + 2 (q âˆ’ ||zÏ ||1 ) and â„“ÏÏƒ = âˆ’ 2 (q âˆ’ ||zÏ ||1 ).
1
1
â€¢ If zÏÏƒ < 0, then uÏÏƒ = 2 (q âˆ’ ||zÏ ||1 ) and â„“ÏÏƒ = âˆ’|zÏÏƒ | âˆ’ 2 (q âˆ’ ||zÏ ||1 ).
For each i âˆˆ {1, . . . , n} define uÏ (i) = uÏÏ€ and â„“Ï (i) = â„“ÏÏ€ , where Ï€ is the symbol in row Ï of P such that i âˆˆ SÏÏ€ , and
define u(i) = min{uÏ (i) : 1 â‰¤ Ï â‰¤ m} and â„“(i) = max{â„“Ï (i) : 1 â‰¤ Ï â‰¤ m}. By first examining a row of maximum signature,
we can immediately conclude for each i âˆˆ {1, . . . , n} either that u(i) = 0 or that â„“(i) = 0. Define a vector xâˆ— = (xâˆ—1 , . . . , xâˆ—n )
by setting xâˆ—i = 0 if |ui |, |â„“i | â‰¤ s + Ç«, and otherwise setting xâˆ—i equal to whichever of u(i) or â„“(i) has the greater absolute
value. We claim that xâˆ— satisfies the required conditions.
To establish this claim we prove that, for 1 â‰¤ j â‰¤ n,
(i) for each Ï âˆˆ {1, . . . , m}, â„“Ï (j) âˆ’ (s + Ç«) < xj < uÏ (j) + (s + Ç«);
(ii) there is some Ï âˆˆ {1, . . . , m} such that â„“Ï (j) > âˆ’(s + Ç«) if xj â‰¥ 0 and uÏ (j) < s + Ç« if xj < 0; and
(iii) there is some Ï âˆˆ {1, . . . , m} such that uÏ (j) âˆ’ (s + Ç«) < xj if xj â‰¥ 0 and xj < â„“Ï (j) + (s + Ç«) if xj < 0.
We begin with some observations used throughout the proof. Let Ï be a row of P . For 1 â‰¤ Ïƒ â‰¤ kÏ , we have zÏÏƒ =
PkÏ
P
|eÏÏƒ | â‰¤ s + Ç«. Let TÏâ€² = {i âˆˆ T + : zÏpÏi â‰¥ 0} âˆª {i âˆˆ T âˆ’ : zÏpÏi < 0}
( iâˆˆT âˆ©SÏÏƒ |xi |) + eÏÏƒ for some eÏÏƒ . Note that Ïƒ=1
â€²â€²
â€²
â€²
and let TÏ = T \ TÏ . Further, let EÏ = {Ïƒ âˆˆ {1, . . . , kÏ } : eÏÏƒ , zÏÏƒ â‰¥ 0 or eÏÏƒ , zÏÏƒ < 0} and let EÏâ€²â€² = {1, . . . , kÏ } \ EÏâ€² . For
1 â‰¤ Ï€ â‰¤ kÏ , we have that
ï£«
ï£¶ ï£«
ï£¶
X
X
|zÏÏ€ | = ï£­
|xi |ï£¸ âˆ’ ï£­
|xi |ï£¸ + Î´ÏÏ€ |eÏÏ€ |
(1)
iâˆˆTÏâ€² âˆ©SÏÏ€

iâˆˆTÏâ€²â€² âˆ©SÏÏ€

where Î´ÏÏ€ = 1 if Ï€ âˆˆ EÏâ€² and Î´ÏÏ€ = âˆ’1 if Ï€ âˆˆ EÏâ€²â€² . Summing over the symbols in row Ï of P , we see
ï£«
ï£¶ ï£«
ï£¶ ï£«
ï£¶
X
X
X
||zÏ ||1 = q â€  âˆ’ 2 ï£­
|xi |ï£¸ + ï£­
|eÏÏƒ |ï£¸ âˆ’ ï£­
|eÏÏƒ |ï£¸
iâˆˆTÏâ€²â€²

and it follows that
1 â€ 
2 (q

ï£«

âˆ’ ||zÏ ||1 ) = ï£­

ÏƒâˆˆEÏâ€²

(2)

ÏƒâˆˆEÏâ€²â€²

ï£«
ï£«
ï£¶
ï£¶
X
X
1
1
|eÏÏƒ |ï£¸ + ï£­
|eÏÏƒ |ï£¸ .
|xi |ï£¸ âˆ’ ï£­
2
2
â€²
â€²â€²
â€²â€²
ï£¶

X

ÏƒâˆˆEÏ

iâˆˆTÏ

(3)

ÏƒâˆˆEÏ

Adding (1) to (3), we obtain
ï£«

|zÏÏ€ | + 12 (q â€  âˆ’ ||zÏ ||1 ) = ï£­

X

iâˆˆTÏâ€² âˆ©SÏÏ€

ï£¶

ï£«

|xi |ï£¸ + ï£­

X

iâˆˆTÏâ€²â€² \SÏÏ€

ï£¶

|xi |ï£¸ âˆ’

ï£«

1ï£­
2

X

ÏƒâˆˆEÏâ€²

ï£¶

|eÏÏƒ |ï£¸ +

ï£«

1ï£­
2

X

ÏƒâˆˆEÏâ€²â€²

ï£¶

|eÏÏƒ |ï£¸ + Î´ÏÏ€ |eÏÏ€ |.

(4)

It follows from (2) that each row of P has signature less than q â€  + (s + Ç«) and that any row of P that separates T + from T âˆ’
has signature greater than q â€  âˆ’ (s + Ç«). Thus, q â€  âˆ’ (s + Ç«) < q < q â€  + (s + Ç«) and hence
1
2 (q

âˆ’ ||zÏ ||1 ) âˆ’ 12 (s + Ç«) < 12 (q â€  âˆ’ ||zÏ ||1 ) < 12 (q âˆ’ ||zÏ ||1 ) + 12 (s + Ç«).

(5)

Let j âˆˆ {1, . . . , n}. We next show that (i), (ii) and (iii) hold in the case where xj â‰¥ 0. The proof in the case where xj < 0
is similar.
Proof of (i). Let Ï index any row of P and let SÏÏ€ be the partition class induced by row Ï of P that contains j. Now
â„“Ï (j) âˆ’ (s + Ç«) < xjPbecause â„“Ï (j) â‰¤ 0. If j âˆˆ
/ T , then xj < s and xj < uÏ (j) + (s + Ç«) because uÏ (j) â‰¥ 0. If j âˆˆ T and
zÏÏ€ < 0, then xj â‰¤ iâˆˆT â€²â€² |xi | and we see from (3) and (5) that xj < 21 (q âˆ’ ||zÏ ||1 ) + (s + Ç«). If j âˆˆ T and zÏÏ€ â‰¥ 0, then
Ï
P
xj â‰¤ iâˆˆTÏâ€² âˆ©SÏÏ€ |xi | and we see from (4) and (5) that xj < |zÏÏ€ | + 12 (q âˆ’ ||zÏ ||1 ) + (s + Ç«).
âˆ’
Proof of (ii). Let Ï index a row of P that separates T + âˆª {j} from
induced by row
P T and let SÏÏ€ be the partition class
Ï of P that contains j. If zÏÏ€ â‰¥ 0, then, for 1 â‰¤ Ïƒ â‰¤ kÏ , either iâˆˆT â€²â€² âˆ©SÏÏƒ |xi | â‰¤ |eÏÏƒ | and Ïƒ âˆˆ EÏâ€² or TÏâ€²â€² âˆ© SÏÏƒ = âˆ….
Ï
Using this, it follows from (3) and (5) that â„“Ï (j)P= âˆ’ 21 (q âˆ’ ||zÏ ||1 ) > âˆ’(s + Ç«). If zÏÏ€ < 0, then TÏâ€² âˆ© SÏÏ€ = âˆ… and Ï€ âˆˆ EÏâ€² .
Furthermore, for Ïƒ âˆˆ {1, . . . , kÏ } \ {Ï€}, either iâˆˆT â€²â€² âˆ©SÏÏƒ |xi | < |eÏÏƒ | and Ïƒ âˆˆ EÏâ€² or TÏâ€²â€² âˆ© SÏÏƒ = âˆ…. Using these facts, it
Ï
follows from (4) and (5) that â„“Ï (j) = âˆ’|zÏ€Ï | âˆ’ 12 (q âˆ’ ||zÏ ||1 ) > âˆ’(s + Ç«).

10

Proof of (iii). Let Ï index a row of P thatP
separates T + \ {j} from T âˆ’ âˆª {j} and let SÏÏ€ be the partition class induced by
row Ï of P that contains j. If zÏÏ€ < 0, then iâˆˆT â€²â€² âˆ©SÏÏ€ |xi | â‰¤ xj . Furthermore, for each symbol Ïƒ âˆˆ {1, . . . , kÏ } \ {Ï€}, either
Ï
P
|eÏÏƒ | and Ïƒ âˆˆ EÏâ€² or TÏâ€²â€² âˆ©SÏÏƒ = âˆ…. Then, it follows from (3) and (5) that uÏ (j) = 21 (qâˆ’||zÏ ||1 )âˆ’(s+Ç«) < xj .
iâˆˆTÏâ€²â€² âˆ©SÏÏƒ |xi | â‰¤
P
P
If zÏÏ€ â‰¥ 0, then iâˆˆT â€² âˆ©SÏÏ€ |xi | â‰¤ xj . Furthermore, for each symbol Ïƒ âˆˆ {1, . . . , kÏ } \ {Ï€}, either iâˆˆTÏâ€²â€² âˆ©SÏÏƒ |xi | â‰¤ |eÏÏƒ |
and Ïƒ âˆˆ EÏâ€² or TÏâ€²â€² âˆ© SÏÏƒ = âˆ…. Then, it follows from (4) and (5) that uÏ (j) = |zÏ€Ï | + 12 (q âˆ’ ||zÏ ||1 ) âˆ’ (s + Ç«) < xj .
V. C ONCLUSION
Hierarchical construction of measurement matrices by column replacement permits the explicit construction of large measurement matrices from small ones. The use of heterogeneous hash families supports the use of a library of smaller ingredient
matrices, while the use of strengthening hash families allows the ingredient matrices to be designed for lower sparsity than
the larger measurement matrix produced. Perhaps surprisingly, the ingredient measurement matrices need not all employ
the same recovery algorithm; rather recovery for the large measurement matrix can use arbitrary routines for recovery that
are provided with the ingredient matrices. In this way, computationally intensive recovery methods can be used for the
ingredient matrices, which permits the selection of smaller matrices in general, while still enabling recovery for the large
measurement matrix. Nevertheless, recovery using the large measurement matrix can be computationally prohibitive without
further restrictions. Therefore it is shown that using a standard construction of linear hash families over the finite field, recovery
for the large measurement matrix can be effected in sublinear time. Indeed sublinear recovery time can be obtained even when
computationally intensive methods are used for each ingredient matrix. A practical implementation of these recovery methods
requires that the methods deal effectively with noise in the signal. Suitable restrictions on the hash family and on each ingredient
matrix used in column replacement are shown to be sufficient to permit recovery even in the presence of such noise.
Measurement matrices that result from one column replacement have been studied here. Because recovery does not depend
on the method by which recovery is done for the ingredient matrices, it is possible that the ingredient matrices themselves
are constructed by column replacement from even smaller ingredient matrices. The merits and demerits of repeated column
replacement deserve further study.
ACKNOWLEDGEMENTS
The work of D. Horsley and C. J. Colbourn is supported in part by the Australian Research Council through grant
DP120103067.
R EFERENCES
[1] N. Alon. Explicit construction of exponential sized families of k-independent sets. Discrete Mathematics, 58:191â€“193, 1986.
[2] R. Baraniuk. Compressive sensing. IEEE Signal Processing Magazine, 24:227â€“234, 2007.
[3] R. Berinde, A. C. Gilbert, P. Indyk, H. Karloff, and M. J. Strauss. Combining geometry and combinatorics: A unified approach to sparse signal recovery.
In Proceedings of the 46th Annual Allerton Conference on Communication, Control, and Computing, pages 798â€“805, 2008.
[4] S. R. Blackburn, T. Etzion, D. R. Stinson, and G. M. Zaverucha. A bound on the size of separating hash families. Journal of Combinatorial Theory,
Series A, 115((7):1246â€“1256, 2008.
[5] S. R. Blackburn and P. R. Wild. Optimal linear perfect hash families. Journal of Combinatorial Theory, Series A, 83:233â€“250, 1998.
[6] E. J. CandeÌ€s. Compressive sampling. In International Congress of Mathematicians, volume 3, pages 1433â€“1452, 2006.
[7] E. J. CandeÌ€s. The restricted isometry property and its implications for compressed sensing. Compte Rendus de lâ€™Academie des Sciences, Series I,
346:589â€“592, 2008.
[8] E. J. CandeÌ€s, J. Romberg, and T. Tao. Robust uncertainty principles: Exact signal reconstruction from highly incomplete frequency information. IEEE
Transactions on Information Theory, 52:489â€“509, 2006.
[9] E. J. CandeÌ€s and T. Tao. Decoding by linear programming. IEEE Transactions on Information Theory, 51:4203â€“4215, 2005.
[10] E. J. CandeÌ€s and T. Tao. Near optimal signal recovery from random projections: Universal encoding strategies. IEEE Transactions on Information
Theory, 52:5406â€“5425, 2006.
[11] S. S. Chen, D. L. Donoho, and M. A. Saunders. Atomic decomposition by basis pursuit. SIAM Journal on Scientific Computing, 20(1):33â€“61, 1998.
[12] A. Cohen, W. Dahmen, and R. A. DeVore. Compressed sensing and best k-term approximation. Journal of the American Mathematical Society,
22:211â€“231, 2009.
[13] C. J. Colbourn, D. Horsley, and C. McLean. Compressive sensing matrices and hash families. IEEE Transactions on Communications, 59(7):1840â€“1845,
2011.
[14] C. J. Colbourn, D. Horsley, and V. R. Syrotiuk. Strengthening hash families and compressive sensing. Journal of Discrete Algorithms, 16:170â€“186,
2012.
[15] C. J. Colbourn and A. C. H. Ling. Linear hash families and forbidden configurations. Designs, Codes and Cryptography, 59:25â€“55, 2009.
[16] G. Cormode and S. Muthukrishnan. Combinatorial algorithms for compressed sensing. In Lecture Notes in Computer Science, volume 4056, pages
280â€“294, 2006.
[17] R. A. DeVore. Deterministic constructions of compressed sensing matrices. Journal of Complexity, 23:918â€“925, 2007.
[18] D. L. Donoho and X. Huo. Uncertainty principles and ideal atomic decomposition. IEEE Transactions on Information Theory, 47:2845â€“2862, 2001.
[19] M. Elad and A. M. Bruckstein. A generalized uncertainty principle and sparse representation in pairs of bases. IEEE Transactions on Information
Theory, 48:2558â€“2567, 2002.
[20] J. J. Fuchs. On sparse representations in arbitrary redundant bases. IEEE Transactions on Information Theory, 50:1341â€“1344, 2004.
[21] J. J. Fuchs. Recovery of exact sparse representations in the presence of bounded noise. IEEE Transactions on Information Theory, 51:3601â€“3608, 2005.
[22] A. C. Gilbert, M. A. Iwen, and M. J. Strauss. Group testing and sparse signal recovery. In Proceedings of the 42nd Asilomar Conference on Signals,
Systems, pages 1059â€“1063, 2008.
[23] A. C. Gilbert, M. J. Strauss, J. Tropp, and R. Vershynin. One sketch for all: Fast algorithms for compressed sensing. In Proceedings of the ACM
Symposium on Theory of Computing, pages 237â€“246, 2007.

11

[24] R. Gribonval and M. Nielsen. Sparse representations in unions of bases. IEEE Transactions on Information Theory, 49:3320â€“3325, 2003.
[25] M. A. Iwen. Combinatorial sublinear-time Fourier algorithms. Foundations of Computational Mathematics, 10:303â€“338, 2010.
[26] S. Jafarpour, W. Xu, B. Hassibi, and R. Calderbank. Efficient and robust compressed sensing using optimized expander graphs. IEEE Transactions on
Information Theory, 55:4299â€“4308, 2009.
[27] B. K. Natarajan. Sparse approximate solutions to linear systems. SIAM Journal on Computing, 24:227â€“234, 1995.
[28] D. R. Stinson, Tran Van Trung, and R. Wei. Secure frameproof codes, key distribution patterns, group testing algorithms and related structures. Journal
of Statistical Planning and Inference, 86:595â€“617, 2000.
[29] D. R. Stinson, R. Wei, and K. Chen. On generalized separating hash families. Journal of Combinatorial Theory, Series A, 115:105â€“120, 2008.
[30] M. Stojnic, W. Xu, and B. Hassibi. Compressed sensing-probabilistic analysis of a null-space characterization. In Proceedings of the International
Conference on Acoustics, Speech, and Signal Processing, pages 3377â€“3380, 2008.
[31] J. A. Tropp. Recovery of short, complex linear combinations via l1 minimization. IEEE Transactions on Information Theory, 51:1568â€“1570, 2005.
[32] W. Xu and B. Hassibi. Efficient compressive sensing with deterministic guarantees using expander graphs. In Proceedings of IEEE Information Theory
Workshop, 2007.
[33] Y. Zhang. On theory of compressive sensing via â„“1 -minimization: Simple derivations and extensions. Technical Report Technical Report CAAM
TR08-11, Rice University, 2008.

SIAM J. DISCRETE MATH.
Vol. 27, No. 4, pp. 1844â€“1861

c 2013 Society for Industrial and Applied Mathematics
âƒ

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

SEQUENCE COVERING ARRAYSâˆ—
YEOW MENG CHEEâ€  , CHARLES J. COLBOURNâ€¡ , DANIEL HORSLEYÂ§ , AND
JUNLING ZHOUÂ¶
Abstract. Sequential processes can encounter faults as a result of improper ordering of subsets
of the events. In order to reveal faults caused by the relative ordering of t or fewer of v events, for
some fixed t, a test suite must provide tests so that every ordering of every set of t or fewer events
is exercised. Such a test suite is equivalent to a sequence covering array, a set of permutations on
v events for which every subsequence of t or fewer events arises in at least one of the permutations.
Equivalently it is a (diï¬€erent) set of permutations, a completely t-scrambling set of permutations,
in which the images of every set of t chosen events include each of the t! possible â€œpatterns.â€ In
event sequence testing, minimizing the number of permutations used is the principal objective. By
developing a connection with covering arrays, lower bounds on this minimum in terms of the minimum
number of rows in covering arrays are obtained. An existing bound on the largest v for which the
minimum can equal t! is improved. A conditional expectation algorithm is developed to generate
sequence covering arrays whose number of permutations never exceeds a specified logarithmic function
of v when t is fixed, and this method is shown to operate in polynomial time. A recursive product
construction is established when t = 3 to construct sequence covering arrays on vw events from ones
on v and w events. Finally computational results are given for t âˆˆ {3, 4, 5} to demonstrate the utility
of the conditional expectation algorithm and the product construction.
Key words. sequence covering array, completely scrambling set of permutations, covering array,
directed t-design
AMS subject classifications. 05B40, 05B15, 05B30, 05A05
DOI. 10.1137/120894099

1. Introduction. A set of permutations {Ï€1 , . . . , Ï€N } of a v-element set X is
completely t-scrambling if for every ordered t-set (x1 , . . . , xt ) with xi âˆˆ X for 1 â‰¤ i â‰¤ t,
there is some Ï (1 â‰¤ Ï â‰¤ N ) for which Ï€Ï (xi ) < Ï€Ï (xj ) if and only if i < j. Spencer
[32] first explored the existence of completely t-scrambling sets of permutations in
generalizing a question of Dushnik [15] on linear extensions. Recently Kuhn et al.
[23, 24] examined an equivalent combinatorial object, the sequence covering array.
For parameters N , t, and v, such an array is a set of n permutations of v letters so
that every permutation of every t of the v letters appearsâ€”in the specified orderâ€”in
at least one of the n permutations. The motivation for finding sequence covering
arrays with small values of n arises in event sequence testing. Suppose that a process
involves a sequence of v tasks or events. The operator may, unfortunately, fail to
do the tasks in the correct sequence. When this happens, errors may occur. But
we anticipate that errors can be attributed to the (improper) ordering of a small
âˆ— Received by the editors October 8, 2012; accepted for publication (in revised form) September
4, 2013; published electronically October 28, 2013.
http://www.siam.org/journals/sidma/27-4/89409.html
â€  School of Physical and Mathematical Sciences, Nanyang Technological University 637371,
Singapore (YMChee@ntu.edu.sg).
â€¡ School of Computing, Informatics, and Decision Systems Engineering, Arizona State University, Tempe, AZ 85257, and State Key Laboratory of Software Development Environment, Beihang
University, Beijing 100191, China (colbourn@asu.edu). This author was supported by Australian
Research Council grant DP120103067.
Â§ School of Mathematical Sciences, Monash University, Melbourne, Australia (danhorsley@
gmail.com). This author was supported by Australian Research Council grants DP120103067 and
DE120100040.
Â¶ Department of Mathematics, Beijing Jiaotong University, Beijing, China (jlzhou@bjtu.edu.cn).

1844

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

SEQUENCE COVERING ARRAYS

1845

subset of tasks. When each permutation of a sequence covering array is used in
turn to specify a task order, every potential ordering of t or fewer tasks will be
tried and hence all errors found that result solely from the improper ordering of t
or fewer tasks. Applications are discussed further in [19, 23, 39, 40]; related event
sequence testing problems in which tasks can be repeated are discussed in [42, 43, 44].
While the application of these combinatorial structures is of much practical concern,
our interest is in bounds on the size of sequence covering arrays and their explicit
construction.
We first state the problem formally. Let Î£ = {0, . . . , v âˆ’ 1} be symbols that
represent the v tasks or events. A t-subsequence of Î£ is a t-tuple (x1 , . . . , xt ) with
xi âˆˆ Î£ for 1 â‰¤ i â‰¤ t, and xi Ì¸= xj when i Ì¸= j. A permutation Ï€ of Î£ covers the
t-subsequence (x1 , . . . , xt ) if Ï€ âˆ’1 (xi ) < Ï€ âˆ’1 (xj ) whenever i < j. For example, with
v = 5 and t = 3, (4, 0, 3) is a 3-subsequence that is covered by the permutation 4 2
0 3 1. A sequence covering array of order v and strength t, or SeqCA(N ; t, v), is a set
Î  = {Ï€1 , . . . , Ï€N }, where Ï€i is a permutation of Î£, and every t-subsequence of Î£ is
covered by at least one of the permutations {Ï€1 , . . . , Ï€N }. Often the permutations are
written as an N Ã— v array.
We use an array representation for completely t-scrambling sets of permutations
as well. An N Ã— v array is a completely t-scrambling set of permutations of strength t
on v symbols, or CSSP(N ; t, v), when the columns are indexed by Î£ and the symbols
by Î£, and for every way c1 , . . . , ct to choose t distinct columns and every permutation
Ï† of {1, . . . , t}, there is a row Ï for which, for every 1 â‰¤ a < b â‰¤ t, the entry in cell
(Ï, cÏ†(a) ) is less than the entry in cell (Ï, cÏ†(b) ).
Lemma 1.1. A CSSP(N ; t, v) is equivalent to a SeqCA(N ; t, v).
Proof. If Ï€1 , . . . , Ï€N are the N permutations of a SeqCA(N ; t, v), form an N Ã— v
array A in which cell (i, j) contains Ï€iâˆ’1 (j). Then A is a CSSP(N ; t, v).
In the opposite direction, if A is a CSSP(N ; t, v), define permutation Ï€i by setting
Ï€i (aij ) = j for 1 â‰¤ i â‰¤ N and 0 â‰¤ j < v. Then Ï€1 , . . . , Ï€N form the N permutations
of a SeqCA(N ; t, v).
In the CSSP(8;3,5) of Table 1.1, the symbols {1, 2, 3} appear as 123 once, 132
once, 213 once, 231 zero times, 312 four times, and 321 once. Hence the rows of a
completely t-scrambling set of permutations do not necessarily produce a sequence
covering array; nevertheless they are conjugates, obtained by interchanging the roles
of columns and symbols.
Permutation problems concerning the avoidance of specified patterns of subsequences have been extensively studied in algebraic and probabilistic combinatorics;
see [33] for an excellent survey. (Here two subsequences (x1 , . . . , xt ) and (y1 , . . . , yt )
have the same pattern when, for 1 â‰¤ i < t, xi < xi+1 if and only if yi < yi+1 .)
Table 1.1
Example: SeqCA(8;3,5) â€“ t = 3, v = 5, N = 8.
SeqCA
4203
1430
3120
0241
2134
0341
3021
4120

1
2
4
3
0
2
4
3

CSSP
2413
3042
3120
0314
4102
0341
1320
3124

0
1
4
2
3
2
4
0

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

1846

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

While cosmetically similar to pattern avoidance problems, the existence problem for
sequence covering arrays requires coverage rather than avoidance and requires that
all subsequences be covered and not simply every pattern.
The question of principal concern in this paper is as follows: Given t and v, what
is the smallest N for which a CSSP(N ; t, v) (equivalently, a SeqCA(N ; t, v)) exists?
Call this number SeqCAN(t, v). In the vernacular of completely t-scrambling sets
of permutations, Spencer [32] did the foundational work, and FuÌˆredi [17], Ishigami
[20, 21], Radhakrishnan [31], and Tarui [36] made improvements.
In a sequence covering
every t symbols must appear in each of the t! possible
"
! array,
orderings, and there are vt t! t-subsequences in total, so
t! â‰¤ SeqCAN(t, v) â‰¤

# $
v
t!
t

Both bounds are trivial, but the lower bound is the correct one when t = 2.
Lemma 1.2. SeqCAN(2, v) = 2 for all v â‰¥ 2.
Proof. Any permutation on v symbols and its reversal form a SeqCA(2; 2, v).
When t â‰¥ 3, neither bound is correct as v increases. Indeed the growth as a
function of v for fixed t is logarithmic.
Theorem 1.3 (see [31, 32]). For t â‰¥ 3,
#
$
#
$
2
v
t log(v)
&.
%
1+
(t âˆ’ 1)!
log2 (v âˆ’ t + 2) â‰¤ SeqCAN(t, v) â‰¤
t!
log2 (e)
2v âˆ’ t + 1
log t!âˆ’1

The question of when SeqCAN(t, v) = t! is of independent interest in yet another
setting. Let V be a finite set; an element of V is a vertex. A transitive tournament on
V is a directed graph in which (1) for all x âˆˆ V , (x, x) is not an arc; (2) for distinct
x, y âˆˆ V , (x, y) is an arc if and only if (y, x) is not an arc; and (3) whenever (x, y)
and (y, z) are arcs, so is (x, z). A transitive tournament T = (V, A) has transitive
tournament T â€² = (W, B) as a subdigraph, denoted T â€² â‰º T , whenever W âŠ† V and
B âŠ† A. Let (V, T ) be a finite set V of cardinality v and a collection T with every
T âˆˆ T being a transitive tournament on k of the vertices in V ; members of T are blocks.
Then (V, T ) is a (t, Î»)-directed packing of blocksize k and order v, or DPÎ» (t, k, v), if
for every X âŠ† V with |X| = t and every transitive tournament T â€² on vertex set X,
|{T âˆˆ T : T â€² â‰º T }| â‰¤ Î».
On the other hand, (V, T ) is a (t, Î»)-directed covering of blocksize k and order v, or
DCÎ» (t, k, v)), if for every X âŠ† V with |X| = t and every transitive tournament T â€² on
vertex set X,
|{T âˆˆ T : T â€² â‰º T }| â‰¥ Î».
When (V, T ) is a DPÎ» (t, k, v) and also a DCÎ» (t, k, v), it is a (t, Î»)-directed design
of blocksize k and order v, or DDÎ» (t, k, v). In this notation, the subscript is often
omitted when Î» = 1.
Directed designs with t = 2 have been extensively studied as generalizations of
balanced incomplete block designs. The study of (t, 1)-directed packings has also
been extensive as a result of their equivalence to â€œdeletion-correcting codesâ€ (see
Levenshtein [25]). The connection with our investigation follows.

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

SEQUENCE COVERING ARRAYS

1847

Lemma 1.4. A CSSP(N ; t, v) is equivalent to a DC(t, v, v) with N blocks. Moreover, a DD(t, v, v) exists if and only if SeqCAN(t, v) = t!.
Proof. For each row (a0 , . . . , avâˆ’1 ) of the CSSP, form a transitive tournament on
vertex set {0, . . . , v âˆ’ 1} by including, for 0 â‰¤ i < j < v, arc (i, j) if ai < aj and arc
(j, i) otherwise. Each transitive tournament on t of these vertices is a subdigraph of
at least one of these N tournaments. The other direction is similar. When N = t!,
every t-subsequence is covered exactly once, and every transitive tournament of order
t arises as a subdigraph exactly once.
Theorem 1.5. Suï¬ƒcient conditions for a DD(t, v, v) to exist include
1. t â‰¥ 3 and t â‰¤ v â‰¤ t + 1 [25] and
2. t = 4 and v = 6 [28].
Necessary conditions for a DD(t, v, v) to exist include
1. v â‰¤ t + 1 for t âˆˆ {3, 5, 6} [28],
2. v â‰¤ t! + 2" for t = 4 [28], and
âˆ’ 1 for t â‰¥ 7 [28].
3. v â‰¤ t+1
2
Levenshtein [25] had conjectured that v â‰¤ t + 1 whenever a DD(t, v, v) exists for
t â‰¥ 3. As stated in Theorem 1.5, this does not hold for t = 4, but this is the only
known exception to Levenshteinâ€™s conjecture. In the next section we significantly
reduce the upper bound on the largest v for which SeqCAN(t, v) can equal t!.
2. Lower bounds. Here we extend a technique used in [17, Theorem 5.1], improving on a method of Ishigami [21]. We require a number of previous results on
covering arrays, introduced next. See [8] for a more thorough introduction to them.
Let N , k, t, and v be positive integers. Let C be an N Ã— k array with entries from
an alphabet Î£ of size v; we typically take Î£ = {0, . . . , v âˆ’ 1}. When (Î½1 , . . . , Î½t )
is a t-tuple with Î½i âˆˆ Î£ for 1 â‰¤ i â‰¤ t, (c1 , . . . , ct ) is a tuple of t column indices
(ci âˆˆ {1, . . . , k}), and ci Ì¸= cj whenever Î½i Ì¸= Î½j , the t-tuple {(ci , Î½i ) : 1 â‰¤ i â‰¤ t} is a
t-way interaction. The array covers the t-way interaction {(ci , Î½i ) : 1 â‰¤ i â‰¤ t} if, in
at least one row Ï of C, the entry in row Ï and column ci is Î½i for 1 â‰¤ i â‰¤ t. Array
C is a covering array CA(N ; t, k, v) of strength t if it covers every t-way interaction.
CAN(t, k, v) is the minimum N for which a CA(N ; t, k, v) exists. The basic goal is to
minimize the number of rows (tests) required and hence to determine CAN(t, k, v).
When t â‰¥ 2 and v â‰¥ 2 are both fixed, CAN(t, k, v) is Î˜(log k) (see, for example, [8]).
We strengthen this standard definition somewhat. For a t-way interaction T =
{(ci , Î½i ) : 1 â‰¤ i â‰¤ t} with symbols chosen from Î£ = {0, . . . , v âˆ’ 1}, let Ï„Ïƒ (T ) =
'
|{i : Î½i = Ïƒ}|. Then define Âµ(T ) = vâˆ’1
Ïƒ=0 Ï„Ïƒ (T )!. The natural interpretation is that
Âµ(T ) is the number of ways to permute the columns (c1 , . . . , ct ) so that the symbols
appear in the same order. A covering array provides excess coverage when every tway interaction T is covered by at least Âµ(T ) rows; such a covering array is denoted
by CAX (N ; t, k, v). More generally, the subscript X is used to extend notation from
covering arrays to those having excess coverage.
Theorem 2.1. Let v, t, and a be integers satisfying v â‰¥ t â‰¥ 3 and t â‰¥ a â‰¥ 0.
Then SeqCAN(t, v) â‰¥ a!CANX (t âˆ’ a, v âˆ’ a, a + 1).
Proof. Let S be a CSSP(N ; t, v). Choose any a columns of S, {e1 , . . . , ea }. For
each ordering Ï€ of these columns, form a matrix CÏ€ that contains all rows of S in
which the entry in column Ï€(ei ) is less than that in column Ï€(ei+1 ) for 1 â‰¤ i < a.
Because there are a! orderings and every row of S appears in exactly one of the {CÏ€ },
it suï¬ƒces to show that for every choice of Ï€ the number n of rows in CÏ€ is at least
CANX (t âˆ’ a, v âˆ’ a, a + 1). To do this, form an n Ã— (v âˆ’ a) array AÏ€ whose columns
are the columns of CÏ€ that are not among the a selected. To determine the content of

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

1848

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

cell (r, c) of AÏ€ , examine the symbol Ïƒ in column c and row r of CÏ€ . If Ïƒ is less than
the symbol in row r and column Ï€(e1 ), the entry is set to 0. If Ïƒ is greater than the
symbol in row r and column Ï€(ea ), the entry is set to a. Otherwise find the unique j
for which Ïƒ is greater than the symbol in row r and column Ï€(ej ) but less than the
symbol in row r and column Ï€(ej+1 ), and set the entry to j.
Now we claim that AÏ€ is a CAX (n; t âˆ’ a, v âˆ’ a, a + 1). The verification requires
demonstrating that every (t âˆ’ a)-way interaction T is covered at least Âµ(T ) times. So
let T = {(fi , Î½i ) : 1 â‰¤ i â‰¤ t âˆ’ a}, noting that Î½i âˆˆ {0, . . . , a} and fi indexes a column
of A. We form permutations of {e1 , . . . , ea } âˆª {f1 , . . . , ftâˆ’a } that are consistent with
Ï€ on {e1 , . . . , ea } in that these columns appear in the order prescribed by Ï€. To
do this, there are Ï„0 (T ) columns with entry 0; place one of the Ï„0 (T )! orderings of
these columns so that all appear before Ï€(e1 ). There are Ï„a (T ) columns with entry
a; place one of the Ï„a (T )! orderings of these columns so that all appear after Ï€(ea ).
For 1 â‰¤ j < a, there are Ï„j (T ) columns with entry j; place one of the Ï„j (T )! orderings
of these columns so that all appear before Ï€(ej ) and after Ï€(ej+1 ). In this way we
can form Âµ(T ) permutations of {e1 , . . . , ea } âˆª {f1 , . . . , ftâˆ’a }, each consistent with Ï€.
Because each is consistent with Ï€, it appears in CÏ€ . But each such appearance in CÏ€
results in a diï¬€erent row of A that covers T , and hence T is indeed covered at least
Âµ(T ) times.
The easiest applications of Theorem 2.1 result from using CAN(t, k, v) as a lower
bound for CANX (t, k, v). Apply it with a = t âˆ’ 1, noting that CAN(1, k, t) = t for
all k â‰¥ 1, to recover the trivial lower bound that SeqCAN(t, v) â‰¥ t!. Apply it with
a = t âˆ’ 2 to establish that SeqCAN(t, v) â‰¥ (t âˆ’ 2)!CAN(2, v âˆ’ t + 2, t âˆ’ 1).
Now we return to the question of when SeqCAN(t, v) can equal t!, or equivalently when a DD(t, v, v) can exist. Theorem 2.1 ensures that SeqCAN(t, v) â‰¥
(t âˆ’ 2)!CANX (2, v âˆ’ t + 2, t âˆ’ 1), so SeqCAN(t, v) = t! can hold only when CANX (2, v âˆ’
t + 2, t âˆ’ 1) â‰¤ t(t âˆ’ 1). The 2-way interaction T = {(c1 , Î½1 ), (c2 , Î½2 )} has Âµ(T ) = 2
exactly when Î½1 = Î½2 (called a constant pair) and has Âµ(T ) = 1 otherwise (a nonconstant pair). Because, for each pair of columns, t âˆ’ 1 constant pairs must be covered
twice each, and (t âˆ’ 1)(t âˆ’ 2) nonconstant pairs must be covered at least once each,
CANX (2, v âˆ’ t + 2, t âˆ’ 1) â‰¥ t(t âˆ’ 1). So we are concerned with when equality can hold.
Lemma 2.2. For v â‰¥ 4, CANX (2, k, v) = v(v + 1) only if k â‰¤ v + 2.
Proof. Suppose that a CAX (v(v + 1); 2, k, v) exists with columns indexed by
{1, . . . , k} and symbols by {0, . . . , v âˆ’ 1}. We form sets on symbols V = ({1, . . . , k} Ã—
{0, . . . , v âˆ’ 1}) âˆª {âˆ}. The system of sets (blocks) B is formed as follows. For every
row (x1 , . . . , xk ) of the covering array, a set {(i, xi ) : 1 â‰¤ i â‰¤ k} is placed in B.
Then for every 1 â‰¤ i â‰¤ k, a set {(i, j) : 0 â‰¤ j < v} âˆª {âˆ} is placed in B. The set
system (V, B) has kv + 1 symbols and v(v + 1) + k blocks. By construction, every two
diï¬€erent symbols appear together in exactly one block, unless the pair is of the form
{(i, j), (iâ€² , j)} corresponding to a constant pair and therefore occurring in exactly two
blocks.
Now form a (kv + 1) Ã— (v(v + 1) + k) matrix A, which is the symbol-block incidence
matrix, as follows. Rows are indexed by symbols, columns by blocks. The matrix
contains the entry 1 in row r and column c when symbol r appears in block c and 0
otherwise. Now examine B = AAT , which has rows and columns indexed by V . Its
diagonal entries are k in entry (âˆ, âˆ) and v + 2 elsewhere. Its oï¬€-diagonal entries
are 2 in cells indexed by ((i, j), (iâ€² , j)) with i Ì¸= iâ€² and 1 otherwise.
The rank of B cannot exceed the number of columns in A, namely, v(v + 1) + k.
So in order to bound k, we bound the rank of B.

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

SEQUENCE COVERING ARRAYS

1849

Write D = B âˆ’ J. The rows indexed by V \ {âˆ} can be partitioned into v parts;
a part is formed by including all rows and columns with indices {(i, j) : 1 â‰¤ i â‰¤ k}
for some j with 0 â‰¤ j < v. Then D can be written as a block diagonal matrix with v
k Ã— k block matrices each equal to X = vI + J and a single 1 Ã— 1 block matrix with
entry k âˆ’ 1. Now det(D) = (k âˆ’ 1)(det(X))v , and det(X) = v k (1 + kv ) by the matrix
determinant lemma. Hence rank(D) = kv + 1. Because B is obtained from D by a
rank one update, rank(B) â‰¥ kv.
Consequently, kv â‰¤ v(v + 1) + k, or k â‰¤ v(v+1)
vâˆ’1 . Thus k â‰¤ v + 2 when v â‰¥ 4,
because k is an integer.
This enables us to establish a substantial improvement on the bound of Mathon
and Tran Van Trung [28] stated in Theorem 1.5.
Theorem 2.3. If t â‰¥ 3 and SeqCAN(t, v) = t! (or equivalently, a DD(t, v, v)
exists), then v â‰¤ 2t âˆ’ 1.
Proof. If 3 â‰¤ t â‰¤ 6, this follows from Theorem 1.5. By Theorem 2.1, t! =
SeqCAN(t, v) â‰¥ (t âˆ’ 2)!CANX (2, v âˆ’ t + 2, t âˆ’ 1) and thus CANX (2, v âˆ’ t + 2, t âˆ’ 1) =
t(t âˆ’ 1). By Lemma 2.2, v âˆ’ t âˆ’ 2 â‰¤ t âˆ’ 1 + 2 and hence v â‰¤ 2t âˆ’ 1 as required.
It appears plausible that the bound should be t+2 rather than 2tâˆ’1; nevertheless,
the method here gives the first bound that is linear in t.
3. Upper bounds from probabilistic methods. Spencer [32] analyzed a
method that selects a set of N permutations on v symbols uniformly at random;
we explore this first.
v!
t!
))/(log( t!âˆ’1
)).
Lemma 3.1. For fixed t â‰¥ 3, SeqCAN(t, v) â‰¤ 1 + (log( (vâˆ’t)!
Proof. A permutation of {0, . . . , v âˆ’ 1} chosen uniformly at random covers any
specific t-subsequence with probability t!1 and so fails to cover it with probability
t!âˆ’1
t! . Then N permutations of {0, . . . , v âˆ’ 1} chosen uniformly at random and in!
"N
dependently together fail to cover a specific t-subsequence with probability t!âˆ’1
.
t!
v!
There are (vâˆ’t)!
t-subsequences. When N permutations are chosen, each subsequence
"N
!
. Thus the expected number of uncovered
is not covered with probability t!âˆ’1
t!
!
"
! t!âˆ’1 "N
v!
t!âˆ’1 N
v!
t-subsequences is (vâˆ’t)!
.
When
< 1, a SeqCA(N ; t, v) must
t!
(vâˆ’t)!
t!
v!
t!
exist. This holds whenever N > (log( (vâˆ’t)!
))/(log( t!âˆ’1
)).

3.1. One permutation at a time. Lemma 3.1 provides a useful upper bound
on the size of completely t-scrambling sets of permutations but does not provide
an eï¬€ective method to find such arrays. Stein [34], LovaÌsz [26], and Johnson [22]
develop a general strategy for finding solutions to covering problems; this algorithm
has been shown to lead to polynomial time methods in many combinatorial covering
problems [3, 4, 7, 9, 10]. We extend that strategy here to treat sequence covering
arrays.
The basic approach is greedy. Repeatedly select one permutation to add that
covers a large number of as-yet-uncovered t-subsequences, until all are covered. Stein
[34], LovaÌsz [26], and Johnson [22] each suggest selecting to maximize the number of
newly covered elements, but their analyses require only that the next selection cover
at least the average. If after i permutations are selected there remain Ui uncovered
t-subsequences, then a permutation selected uniformly at random is expected to cover
Ui t!1 t-subsequences for the first time. Provided that we select the (i+1)st permutation
to cover at least Ui t!1 t-subsequences for the first time, we have that Ui+1 â‰¤ Ui t!âˆ’1
t! .
v!
v!
i
, we have that Ui â‰¤ (vâˆ’t)!
( t!âˆ’1
)
.
Choose
N
to
be
the
smallest
Because U0 = (vâˆ’t)!
t!

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

1850

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

value for which UN < 1; then there must be a sequence covering array with N
permutations.
This simply restates the argument of Lemma 3.1, but with two important improvements. It derandomizes the method by ensuring that appropriate selection of
each permutation guarantees that the bound is met, rather than asserting the existence of a set of permutations that meets it. More importantly, the time to construct
the sequence covering array is polynomial in the number of permutations and the time
to select a permutation that covers at least the average.
For fixed t the number of permutations is logarithmic in v, and so an algorithm
with running time polynomial in v will result if we can select the next permutation
in time polynomial in v. Because the details are quite similar to earlier approaches,
we merely outline how this can be done.
Suppose that U consists of the as-yet-uncovered t-subsequences. For U âˆˆ U,
let cov(Ï€, U ) = 1 when Ï€ covers U , and cov(Ï€, U ) = 0 otherwise. Let R be an rsubsequence; the r symbols in R are fixed, and the remaining v âˆ’ r are free. There
is a set PR of v!
r! permutations that cover R. We focus on the expected number of
members of U that are covered by a member of PR chosen uniformly at random;
this is
r! ( (
ec(R) =
cov(Ï€, U ).
v!
Ï€âˆˆPR UâˆˆU

The strategy is to find a sequence of subsequences P0 , . . . , Pv , so that Pi is an
i-subsequence, Pi+1 covers Pi for 0 â‰¤ i < v, symbol i is free in Pi but fixed in Pi+1 ,
and ec(Pi+1 ) â‰¥ ec(Pi ) for 0 â‰¤ i < v. Because ec(P0 ) = t!1 |U|, it follows that Pv is a
permutation that covers at least t!1 |U| of the as-yet-uncovered t-subsequences. Given
a selection of Pi , there are precisely i + 1 candidates {C1 , . . . , Ci+1 } for Pi+1 obtained
by placing symbol i in one of the i + 1 positions of Pi . Our task is to choose one for
which ec(Cj ) â‰¥ ec(Pi ), in order to set Pi+1 = Cj .
A naive computation of ec(Cj ) would enumerate members of U and of PCj ,
but)the latter may have size exponential in v. Instead, for U âˆˆ U let ec(U, R) =
r!
Ï€âˆˆPR cov(Ï€, U ), and observe that
v!
(
ec(U, R).
ec(R) =
UâˆˆU

When t is fixed, U contains fewer than v t subsequences, which is polynomial
in v. Therefore it suï¬ƒces to compute ec(U, R) eï¬ƒciently, given a t-subsequence U
and an r-subsequence R. Let Ï„ be the number of symbols appearing in both U and
R. When the Ï„ symbols in common do not appear in the same order in U and R,
ec(U, R) = 0. Otherwise let T be the Ï„ -subsequence that they have in common. Then
ec(U, R) = ec(U, T ) = Ï„t!! .
)i+1
1
The key observation in selecting Pi+1 is that ec(Pi ) = i+1
j=1 ec(Cj ). Computing ec(Cj ) for 1 â‰¤ j â‰¤ i + 1, and selecting Pi+1 to be the one that maximizes ec(Cj ),
we are then sure that ec(Pi+1 ) â‰¥ ec(Pi ).
Combining all of these arguments, we have established the next theorem.
Theorem 3.2. For fixed t and input v, there is an algorithm to construct a
v!
t!
))/(log( t!âˆ’1
)) permutations in time that is
SeqCA(N ; t, v) having N â‰¤ 1 + (log( (vâˆ’t)!
polynomial in v.
This algorithm can be easily implemented, and we report results from it in
section 5. One immediate improvement results from observing that the counts of

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

SEQUENCE COVERING ARRAYS

1851

as-yet-uncovered t-subsequences (R0 , R1 , . . . , RN ) must be integers. Hence we have
that Ri+1 â‰¤ âŒŠRi t!âˆ’1
t! âŒ‹. In specific cases this improves on the bound, without the need
to construct the sequence covering array. If, however, the sequence covering array
is explicitly constructed, at each selection of Pi+1 from Pi , we can choose Pi+1 to
maximize ec() among the i + 1 candidates.
3.2. Greedy methods with reversals. The methods developed are greedy in
that they attempt to cover the largest number of as-yet-uncovered t-subsequences.
The very first permutation chosen is arbitrary; all are equally eï¬€ective at coverage.
Once one is selected, however, there is a genuine choice for the second. Greedy
selection indicates that we should choose one that covers no t-subsequence already
covered by the first. Indeed when t = 2, choosing the reversal of this first covers all
remaining t-subsequences.
For t â‰¥ 3, suppose that we have chosen 2s permutations Ï€1 , . . . , Ï€2s , and suppose
further that Ï€2i is the reverse of Ï€2iâˆ’1 for 1 â‰¤ i â‰¤ s. It follows that the number of
as-yet-uncovered t-subsequences covered by a permutation Ï€ is precisely the same as
the number covered by the reverse of Ï€. Yet Ï€ and its reverse never cover the same
t-subsequence. Hence if the algorithm were to select Ï€ next, the reverse of Ï€ remains
an equally beneficial choice immediately thereafter. Therefore a useful variant of the
algorithm developed, after adding a permutation Ï€ to the array, always adds the
reverse of Ï€ as well. Pursuing this, we obtain the following.
Theorem 3.3. For fixed t and input v, there is an algorithm to construct a
v!
t!
))/(log( t!âˆ’2
)) permutations in time that is
SeqCA(N ; t, v) having N â‰¤ 2(log( (vâˆ’t)!
polynomial in v.
Naturally, we could again obtain small improvements in practice because every
count of as-yet-uncovered t-subsequences is an integer.
In principle, always including reversals improves slightly on the bound (that is,
Theorem 3.3 improves on Theorem 3.2). Whether this is a practical improvement
remains to be seen; we return to this point.
4. Product constructions. Product (or â€œcut-and-pasteâ€ or â€œRoux-typeâ€) constructions are well studied for covering arrays; see, for example, [11, 6]. We develop
a product construction for completely 3-scrambling sets of permutations. To do this,
we first introduce an auxiliary property. A signing of a CSSP(N ; t, v) A = (aij ) is an
N Ã— v matrix S = (sij ) with entries {â†‘, â†“}. A CSSP(N ; t, v) A is properly signed by
an N Ã— v matrix S with entries {â†‘, â†“}. When for every set of t âˆ’ 1 distinct columns
c1 , . . . , ctâˆ’1 , each sign s âˆˆ {â†‘, â†“}, and every permutation Ï€ of 1, . . . , t âˆ’ 1, there exists
a row Ï of A for which, for every 1 â‰¤ a < b < t, the entry in cell (Ï, cÏ€(a) ) is less than
the entry in row (Ï, cÏ€(b) ), and the sign sÏ,c1 = s. A properly signed CSSP(7;3,5) is
shown in Table 4.1.
We defer for the moment the question of how to sign a completely t-scrambling
set of permutations.
4.1. Products for strength three.
Theorem 4.1. If a properly signed CSSP(N ; 3, v) and a properly signed
CSSP(M ; 3, w) both exist, so does a properly signed CSSP(N + M ; 3, vw).
Proof. Let A = (aij ) be a CSSP(N ; 3, v) having sign matrix S = (sij ) with columns
indexed by {0, . . . , v âˆ’ 1}. Let B = (bij ) be a CSSP(M ; 3, w) having sign matrix T =
(tij ) with columns indexed by {0, . . . , w âˆ’ 1}. Form an array C = (cÏ,(i,j) ) on N + M
rows and vw columns with columns indexed by {0, . . . , v âˆ’ 1} Ã— {0, . . . , w âˆ’ 1}. In row
Ï for 1 â‰¤ Ï â‰¤ N , in column (i, j), place the entry aÏi w + j if sÏi =â†‘, aÏi w + (w âˆ’ 1 âˆ’ j)

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

1852

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Table 4.1
Properly signed CSSP(7; 3, 5) â€“ t = 3, v = 5, N = 7. All signs not specified can be selected
arbitrarily.
0â†‘
1â†“
2â†“
2â†“
2
4â†‘
4

4â†‘
2â†“
0â†‘
1â†“
3â†“
1
3

2â†“
3â†“
4â†‘
0â†‘
0â†“
2
2
(7;3,5)

3â†“
0â†‘
3â†“
3
4â†‘
3
0â†“

1â†“
4â†‘
1â†“
4â†“
1
0â†‘
1

if sÏi =â†“. In row N + Ï for 1 â‰¤ Ï â‰¤ M , in column (i, j), place the entry bÏj v + i if
tÏj =â†‘, bÏj v + (v âˆ’ 1 âˆ’ i) if tÏj =â†“.
To show that C is a CSSP(N + M ; 3, vw), we must establish that every 3subsequence is covered. Consider three columns (i1 , j1 ), (i2 , j2 ), (i3 , j3 ), in this order. If i1 , i2 , and i3 are all distinct, there is a row Ï of A in which aÏi1 < aÏi2 < aÏi3 .
Then in C row Ï has the three specified columns in the chosen order. By the same
token, if j1 , j2 , and j3 are all distinct, there is a row Ï of B in which bÏj1 < bÏj2 < bÏj3 .
Then in C row Ï + N has the three specified columns in the chosen order. If {i1 , i2 , i3 }
contains only one element, {j1 , j2 , j3 } contains three distinct elements; symmetrically,
if {j1 , j2 , j3 } contains only one element, {i1 , i2 , i3 } contains three distinct elements.
So it remains only to treat cases in which both {i1 , i2 , i3 } and {j1 , j2 , j3 } contain two
distinct elements.
Now suppose that the three columns are {(i1 , j1 ), (i1 , j2 ), (i2 , j1 )}; we are concerned with the six orderings of the elements {c(i1 ,j1 ) , c(i1 ,j2 ) , c(i2 ,j1 ) }, which we represent by giving the indices in the sorted order for {c(i1 ,j1 ) , c(i1 ,j2 ) , c(i2 ,j1 ) }, as shown
next.
j2
j1
i1 c(i1 ,j1 ) c(i1 ,j2 )
1 2
1 3
2 1
2 3
3 1
3 2
i2 c(i2 ,j1 )
3
2
3
1
2
1
Table 4.2 gives the rows in which each of the six orderings is covered; we use
sgn(x âˆ’ y) to be â†“ if x < y, â†‘ if x > y. Two of the orderings are covered at least
twice.
To sign C properly, assign â†‘ to each entry in each of the first N rows and â†“ to
each entry in each of the last M rows.
A small example, combining two CSSP(6;3,4)s to form a CSSP(12;3,16), is shown
in Table 4.3.
Use the strategy in the proof of Theorem 4.1, taking B and T from
â›
â
0â†‘ 1â†‘
âœ 0â†“ 1â†“ âŸ
âœ
âŸ
â 1 â†‘ 0 â†‘ â ,
1â†“ 0â†“

to establish the next theorem.
Theorem 4.2. If a properly signed CSSP(N ; 3, v) exists, so does a properly signed
CSSP(N + 4; 3, 2v).

4.2. Signing a completely t-scrambling set of permutations. We first give
one technique for signing that applies for all strengths.

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

1853

SEQUENCE COVERING ARRAYS

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Table 4.2
Verification for six orderings.
Row in C

Condition on Ï

Sign condition

Ï

aÏi1 < aÏi2

sÏi1 = sgn(j2 âˆ’ j1 )

Ï

aÏi1 < aÏi2

sÏi1 = sgn(j1 âˆ’ j2 )

Ï

aÏi1 > aÏi2

sÏi1 = sgn(j2 âˆ’ j1 )

Ï

aÏi1 > aÏi2

sÏi1 = sgn(j1 âˆ’ j2 )

Ï+N

bÏj1 < bÏj2

tÏj1 = sgn(i2 âˆ’ i1 )

Ï+N

bÏj1 < bÏj2

tÏj1 = sgn(i1 âˆ’ i2 )

Ï+N

bÏj1 > bÏj2

tÏj1 = sgn(i2 âˆ’ i1 )

Ï+N

bÏj1 > bÏj2

tÏj1 = sgn(i1 âˆ’ i2 )

Ordering
1
3
2
3
2
1
3
1
1
2
2
1
2
3
3
2

2
1
3
2
3
3
1
1

Table 4.3
Example: Properly signed CSSP(6; 3, 4) and its product with itself, a CSSP(12; 3, 16).

0â†“
1â†‘
1â†‘
3â†‘
1â†‘
3â†“

1â†‘
0â†“
3â†“
1â†‘
3â†‘
1â†‘

2â†‘
3â†“
0â†“
0â†‘
2â†‘
2â†‘

3â†“
2â†‘
2â†‘
2â†‘
0â†‘
0â†“

3
4
4
12
4
15
3
4
4
12
4
15

2
5
5
13
5
14
4
3
15
4
12
4

1
6
6
14
6
13
8
15
3
0
8
8

0
7
7
15
7
12
15
8
8
8
0
3

4
3
15
4
12
4
2
5
5
13
5
14

5
2
14
5
13
5
5
2
14
5
13
5

6
1
13
6
14
6
9
14
2
1
9
9

7
0
12
7
15
7
14
9
9
9
1
2

8
15
3
0
8
8
1
6
6
14
6
13

9
14
2
1
9
9
6
1
13
6
14
6

10
13
1
2
10
10
10
13
1
2
10
10

11
12
0
3
11
11
13
10
10
10
2
1

15
8
8
8
0
3
0
7
7
15
7
12

14
9
9
9
1
2
7
0
12
7
15
7

13
10
10
10
2
1
11
12
0
3
11
11

12
11
11
11
3
0
12
11
11
11
3
0

Lemma 4.3. Whenever a CSSP(N ; t, v) exists, a properly signed CSSP(N ; t, v âˆ’1)
exists.
Proof. Let A = (aij ) be a CSSP(N ; t, v). Form an N Ã— (v âˆ’ 1) array S = (sij )
with sij = sgn(aij âˆ’ ai,vâˆ’1 ). Form an N Ã— (v âˆ’ 1) array B = (bij ) with bij = aij if
aij < ai,vâˆ’1 and bij = aij âˆ’ 1 otherwise. Then B is a CSSP(N ; t, v âˆ’ 1) that is properly
signed by S.
Let A be a CSSP(N ; t, v) and A1 , A2 be arrays that partition the rows of A. When,
for i = 1, 2, Ai is a CSSP(Ni ; t âˆ’ 1, v), A is a partitionable CSSP.
Lemma 4.4. Whenever a partitionable CSSP(N ; t, v) exists, a properly signed
CSSP(N ; t, v) exists.
Proof. Let A be a CSSP(N ; t, v) with partition A1 , A2 . Assign sign â†‘ to every
entry of A1 and â†“ to every entry of A2 .
Corollary 4.5. Whenever a CSSP(N ; 3, v) contains a row and its reverse, it is
partitionable and hence can be properly signed.
Proof. Place the row and its reverse in A1 and all other rows in A2 . Then A1 is a
CSSP(N ; 2, v). Moreover, A2 is a CSSP(N ; 2, v) because for every i, j âˆˆ {0, . . . , v âˆ’ 1}
with i Ì¸= j, in A there are at least three rows in which the entry in column i is less
than that in column j. Then A is partitionable, so apply Lemma 4.4.

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

1854

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Lemma 4.4 provides a suï¬ƒcient condition for a CSSP(N ; 3, v) to have a proper
signing but considers only signings in which all entries in each row receive the same
sign. Column c is properly signed when, for every set of t âˆ’ 1 distinct columns
c1 , c2 , . . . , ctâˆ’1 with c1 = c, each sign s âˆˆ {+, âˆ’}, and every permutation Ï€ of
1, . . . , t âˆ’ 1, there exists a row Ï of A for which, for every 1 â‰¤ a < b < t, the entry in cell (Ï, cÏ€(a) ) is less than the entry in row (Ï, cÏ€(b) ), and the sign sÏ,c = s.
Properly signing the N Ã— v array A is equivalent to properly signing each column of
A; the important fact is that signs assigned in one column are unrelated to signs in
any other, and so one can (hope to) proceed by signing each column separately.
Consider the case of strength t = 3. What does it mean to properly sign a
specific column c? For every column i other than c we form two sets: Ai contains
the row indices in which the entry in column i is larger than that in column c, and
Bi (= {1, . . . , N }\Ai ) contains the row indices in which the entry in column i is smaller
than that in column c. We can consider these sets as the edges of a hypergraph H on
vertex set {1, . . . , N }. Then H has 2v âˆ’ 2 edges each containing at least three vertices
and a proper 2-coloring of H corresponds to a proper signing of c. Lemma 4.4 and
Corollary 4.5 give proper 2-colorings. In all examples that we have examined, each
column can be properly signed by finding a suitable 2-coloring. Hence it is plausible
that every CSSP(N ; 3, v) can be properly signed, but if this is true the proof is elusive
at the moment.
5. Computational results. In [23], a simple greedy method is used to compute
upper bounds on SeqCAN(t, v) for t âˆˆ {3, 4} and small values of v. These are reported
in column K in Tables 5.1 and 5.2. Results from a more sophisticated greedy method
by Erdem et al. [16] are reported in column ER. Using techniques from constraint
satisfaction, in particular answer set programming, much more sophisticated search
methods have been applied to strengths three and four [1, 2, 16]. Banbara, Tamura,
and Inoue [1] implement an answer set programming method and show bounds for
SeqCAN(3, v) for v â‰¤ 80 and for SeqCAN(4, v) for v â‰¤ 23. These bounds appear in
column BTI in Tables 5.1 and 5.2. Results by Brain et al. [2] are reported in column
BR in Tables 5.1 and 5.2. In [18], bounds for t âˆˆ {3, 4, 5} and v â‰¤ 10 are reported
from a method called the â€œbees algorithmâ€; these oï¬€er modest improvements on the
greedy method in [23]. We do not report them for t âˆˆ {3, 4} because they are not
competitive with the results in [1]; we do report them for t = 5 in column BA, because
they are the only published computational results. When t = 3, Tarui [36] establishes
"
!
) â‰¤ q for all q â‰¥ 4; these are reported in
by direct construction that SeqCAN(3, âŒŠq/2âŒ‹
âŒŠq/4âŒ‹
column TA.
The bound U is obtained by computing the number Ui of as-yet-uncovered tsubsequences using Ui+1 = âŒŠ t!âˆ’1
t! Ui âŒ‹ and terminating with the first value N for which
UN = 0. (This does not explicitly construct the array but rather yields a number of
rows for which it can surely be produced.) In the same manner, bound UR is obtained
by including reversals, so that Ui+2 = Ui âˆ’ 2âŒˆ t!1 Ui âŒ‰ when i is even. The bound D is
obtained by applying the algorithm that establishes Theorem 3.2 and that of DR by
applying the algorithm that establishes Theorem 3.3.
Table 5.1 reports results for t = 3. The theoretical results indicate that including
reversals accelerates coverage, and so the bound UR improves on the bound U. Neither
is competitive with greedy bounds from [23], given by K. In turn these are improved
upon by the greedy method from [16], given by ER. Implementing our greedy approach
nevertheless results in useful improvement to the two earlier greedy bounds, whether
reversals are included or not. It comes as no surprise that the answer set programming

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

1855

SEQUENCE COVERING ARRAYS

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Table 5.1
Upper bounds on SeqCAN(3, v).
Events
v
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
40
50
60
70
80
90

TA
8
8
8
10
10
10
10
12
12
12
12
12
12
12
12
12
12
14
14
14
14
14
14
14
14
14
14
16
16
16
16
18
18

U
12
17
20
23
26
28
30
32
33
35
36
37
39
40
41
42
42
43
44
45
46
46
47
48
48
49
49
54
58
61
64
66
68

UR
12
16
18
22
24
26
28
30
30
32
34
34
36
36
38
38
38
40
40
40
42
42
42
44
44
44
46
50
52
56
58
60
62

t=3
K
ER
8
10
12
12
14
14
11
14
16
16
16
18
18
20
20
22
22
19
22
22
24
24
24
24
26
26
26
26
23
32
27
34
31
38
34
40
36
42
38
44

DR
8
10
10
12
12
12
14
14
14
16
16
16
16
18
18
18
18
18
20
20
20
20
20
20
20
22
22
24
26
26
28
30
30

D
6
8
8
9
10
11
12
12
13
13
14
14
15
15
16
16
16
17
17
17
17
18
18
18
18
19
19
21
23
24
25
26
27

BTI

BR

7
8
8
8
9
9
10
10
10
10
10
11
11
12
12
12
12
13
14
14
14
14
14
14
15
15
17
19
21
22
24

7
8
8
8
9
9
10
10
10
10
10
10
11
12
12
12
12
12
13
13
14
14
14
14
14
15
17
18
20
22
23

methods from [1, 2] (BTI, BR) yield consistent improvements on all the greedy methods
for strength three. However, the direct construction of Tarui [36] provides better
results at this time whenever v â‰¥ 30.
Perhaps the most perplexing pattern is the regularity with which D yields a better
bound than does DR . Remarkably, we consistently produce smaller sequence covering
arrays when we do not automatically include reversals! The reasons for this are quite
unclear at the present time.
Table 5.2 gives results for strengths four and five. For strength four, our improvements on the method from [23] are more dramatic than for strength three. Surprisingly, the answer set programming technique from [1] obtains a better result than our
greedy methods only when v â‰¤ 8. For 9 â‰¤ v â‰¤ 23, our greedy method yields much
smaller arrays. (For v = 23, we employ 98 permutations as opposed to the 112 in
BTI.) A similar comparison applies with the results from [2, 16] reported in column BR.
Of course, we expect that given enough time, the answer set programming techniques
would improve upon our greedy bounds.
However, our methods require polynomial time in theory and are eï¬€ective in
practice for larger problems than those considered in [2, 1]; despite these â€œlimitations,â€
our methods appear to yield better results within the time available.

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

1856

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Table 5.2
Upper bounds on SeqCAN(t, v) for t âˆˆ {4, 5}.
Events
v
4
5
6
7
8
9
10
11
12
13
14
15
16
17
18
19
20
21
22
23
24
25
26
27
28
29
30
40
50
60
70
80
90

U
24
54
79
98
114
128
140
151
160
169
177
184
191
197
203
209
214
219
224
228
232
236
240
244
248
251
255
283
305
322
337
350
361

UR
24
54
78
96
112
126
138
148
158
166
174
180
188
194
200
204
210
214
220
224
228
232
236
240
242
246
250
278
298
316
330
342
354

K
24
29
38
50
56
68
72
78
86
92
100
108
112
118
122
128
134
134
140
146
146
152
158
160
162
166
166
198
214
238
250
264
-

t=4
DR
24
24
32
40
44
50
56
60
64
70
74
78
80
84
86
90
92
96
98
98
102
104
106
108
110
112
114
132
146
154
166
174
180

D
24
26
34
41
47
52
57
61
66
71
73
78
81
84
86
91
92
95
97
99
101
104
105
107
110
111
113
128
141
151
160
168
176

BTI
24
24
38
44
52
58
65
69
77
81
84
89
91
97
100
105
104
111
112

BR

55

104

149
181

U

UR

120
294
437
552
648
731
803
868
926
978
1027
1072
1113
1152
1189
1223
1256
1286
1316
1344
1370
1396
1420
1444
1466
1488
1671
1811
1924
2019
2101
2173

120
294
436
550
646
728
800
864
922
976
1024
1068
1110
1148
1184
1218
1252
1282
1310
1338
1366
1390
1416
1438
1460
1482
1664
1804
1916
2012
2092
2164

t=5
DR
120
148
198
242
282
318
354
384
416
446
470
496
518
540
560
582
600
622
636
654
674
688
706
718
734
748

D
120
149
200
243
284
322
356
386
419
448
475
501
521
547
570
590
610
629
646
665
682
698
715
732
746
760

BA

159
212
271
329
383

A somewhat diï¬€erent pattern with respect to reversals is evident for strength
four: The theoretical bound profits by including reversals throughout, but the implemented construction method appears first to benefit from reversals (for v â‰¤ 20)
but later no longer benefit (for 40 â‰¤ v â‰¤ 90). Again the reasons for this are unclear.
When v = 90, our methods track the coverage of 61,324,560 4-subsequences; thus,
while the methods scale polynomially with v, the computations are nonetheless quite
extensive. There is a CSSP(24;4,6) [28], but our methods do not yield fewer than 32
permutations.
For strength five, none of the published methods in [1, 16, 23] report computational results, so it is diï¬ƒcult to make any comments about relative accuracy. However, the answer set programming methods do appear to require substantially more
storage, which limits to a degree their eï¬€ective range. To apply our methods would
require tracking the coverage of 78,960,960 5-subsequences for v = 40; despite the
eï¬ƒciency of our methods, a straightforward implementation encounters both storage
and time limitations. The method BA [18] is not competitive with our greedy methods.
Within the range computed, including reversals improves our results. The pattern
thereafter is unknown. Again, there is a CSSP(120;5,6) [25], but our methods do not
yield fewer than 148 permutations.

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

SEQUENCE COVERING ARRAYS

1857

6. Using the product construction. For strength three, Theorem 4.1 provides
substantial improvements on the computational results from the greedy methods. We
properly signed a CSSP(6;3,4) in Table 4.3 and a CSSP(7;3,5) in Table 4.1. Table 6.1
shows proper signings for further arrays from [1].
We obtain CSSP(N ; 3, v) for (v, N ) âˆˆ {(40, 15), (80, 17), (128, 18), (160, 19),
(256, 20), (288, 21)} by using these in Theorem 4.1. These improve upon all the computational results! For example, while in [1] it is shown that SeqCAN(3, 80) â‰¤ 24
and in [2] that SeqCAN(3, 80) â‰¤ 23, here it is shown that SeqCAN(3, 80) â‰¤ 17. The
examples given also provide better bounds than those of Tarui [36], but Theorem 4.1
does not outperform the direct construction asymptotically.
7. Constraints. In the testing application, it may happen that not every permutation of the events can in fact be executed; see [2, 23, 24]. It is therefore reasonable to ask how constraints on the execution order aï¬€ect the number of permutations needed and how they aï¬€ect the diï¬ƒculty of finding a sequence covering array. We briefly consider the latter, in order to examine connections with further
problems.
Let Î£ = {0, . . . , v âˆ’ 1}. Let C be a set of subpermutations of Î£, called constraints.
A constrained sequence covering array SeqCA(N ; t, v, C) is a set Î  = {Ï€1 , . . . , Ï€N }
where Ï€i is a permutation of Î£ that does not cover any subpermutation in C, and
every t-subsequence of Î£ that does not cover any subpermutation in C is covered by
at least one of the permutations {Ï€1 , . . . , Ï€N }.
Even in the easiest case, when t = 2 and all constraints are 2-subpermutations,
the nature of the problem changes dramatically. Imposing the subpermutation constraint that b cannot precede a is the same as enforcing the precedence constraint that
a precede b. When the precedence constraints contain a cycle, it is impossible to meet
all constraints. This can be easily checked. When the constraints are acyclic, there is
a permutation that covers no constraint. However, covering all 2-subpermutations not
in C requires more. Let C r be the set of 2-subpermutations obtained by reversing each
2-subpermutation in C. Suppose that a SeqCA(N ; 2, v, C) exists. Every permutation
in the sequence covering array covers every 2-subpermutation in C r . Equivalently,
treating C r as a partial order, every permutation gives a linear extension of the partial order. When (a, b) âˆˆ C, (b, a) must be covered by every permutation in the
sequence covering array. When {(a, b), (b, a)} âˆ© C = âˆ…, some but not all permutations
in the sequence covering array cover (a, b)â€”and the rest cover (b, a). Hence the set
of 2-subpermutations covered by every permutation in the sequence covering array
is exactly C r . This establishes a connection with the theory of partial orders. The
dimension of a partial order is the smallest number of linear extensions whose intersection is the partial order [37, 38]. Our discussion establishes that a SeqCA(N ; 2, v, C)
exists if and only if the dimension of the partial order induced by C r is at most N .
Hence we have the next lemma.
Lemma 7.1. Deciding whether a SeqCA(N ; 2, v, C) exists is NP-complete, even
when C is an acyclic set of 2-subpermutations.
Proof. Yannakakis [41] shows that determining whether a partial order has dimension at most 3 is NP-complete.
Brain et al. [2] establish the NP-completeness of a related problem in which the
subsequences to be covered, the constraints, and the permutations allowed are all
specified. Lemma 7.1 is in stark contrast with the existence of sequence covering
arrays of strength two without constraints. Nevertheless, the complexity arises in
determining whether a small sequence covering array exists in these cases, not in

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

1858

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

Table 6.1
Small properly signed CSSP(N ; 3, v)s. Signs not shown can be chosen arbitrarily.
8â†‘ 5â†‘ 6â†“ 2â†‘ 3â†“ 7â†‘ 1â†‘ 4â†“
1â†‘ 2â†‘ 8â†‘ 4â†‘ 5â†“ 7â†‘ 6â†“ 3â†“
6â†“ 5â†“ 7 8â†‘ 3 2â†“ 4â†“ 1â†‘
5â†“ 7â†‘ 4â†“ 3â†“ 8â†‘ 1â†‘ 6 2
3â†“ 7â†“ 4 2â†“ 1â†‘ 5â†“ 8â†‘ 6â†“
6 2â†“ 1â†‘ 3 8 7â†“ 4â†“ 5
2 7 3â†“ 6 4 5 1 8â†‘
5 1 3 8â†“ 4â†“ 2 6 7
(8;3,8)

10â†‘ 4â†“ 5â†‘ 6â†‘ 8â†“ 1â†‘ 3â†‘ 2â†“ 9â†‘ 7â†‘
2â†‘ 10â†‘ 6â†‘ 5â†‘ 4â†“ 8â†‘ 7 1â†‘ 9â†‘ 3â†‘
8 6 1â†“ 5â†‘ 10â†‘ 3 9â†‘ 7â†“ 4â†“ 2â†“
5â†‘ 3â†“ 9â†“ 10â†“ 1â†‘ 8â†‘ 4â†‘ 6â†“ 7â†“ 2â†“
2â†“ 5 7â†‘ 4â†“ 8 9â†“ 3â†“ 10â†‘ 1â†‘ 6
6 8 1 10 7 5â†“ 2â†“ 4 3 9
9â†“ 10â†“ 8â†“ 3 5â†“ 2 4 7 1 6â†‘
5â†“ 1â†‘ 7 3â†“ 2 4â†“ 9 8 6â†“ 10â†“
3 5 1 2 9 7 10â†“ 4 6 8
(9;3,10)

2â†‘ 14â†‘ 15â†‘ 5â†‘ 8â†‘ 3â†“ 1â†‘ 11â†‘ 9â†‘ 7â†‘ 13â†‘ 12â†‘ 16â†‘ 6â†‘ 4â†‘ 10â†‘
14â†‘ 8â†“ 3â†‘ 6â†“ 7â†“ 11 1â†“ 16â†‘ 12â†“ 13â†“ 2â†‘ 10â†“ 15â†“ 5â†“ 9 4â†“
12â†“ 16â†“ 4 6â†‘ 3â†‘ 5 7 2â†“ 1â†‘ 9 15â†“ 11â†“ 8 14â†“ 10 13â†“
12â†“ 2â†“ 13â†“ 4â†“ 3â†“ 14â†“ 9 6 11 1â†‘ 5â†“ 15 10â†“ 7â†“ 16â†‘ 8â†“
16 4 10â†‘ 5 12 9â†“ 11 14â†“ 1â†“ 8 13 2â†‘ 7 15â†‘ 6â†‘ 3â†‘
6 4â†‘ 12 15 3 1â†‘ 11â†“ 13 8 14â†‘ 10 5 2â†“ 9 16â†“ 7
2â†“ 15 12â†“ 11 6 8 13â†“ 5 14â†“ 7 3â†“ 4â†“ 9 16 10 1
5 9â†‘ 7 14â†“ 16â†‘ 13 6 10â†“ 12 1â†“ 11 2 8 3â†‘ 4â†“ 15
5 6 1â†“ 15â†‘ 12 16â†‘ 10 2â†‘ 3 13â†“ 4 11 9 8 7â†“ 14
11â†‘ 7 8 4 15â†“ 5 16â†‘ 6 14â†‘ 12 9â†‘ 13â†‘ 1â†‘ 2 3 10â†‘
(10;3,16)
1â†‘ 2â†‘ 3â†‘ 4â†‘ 5â†‘ 6â†‘ 7â†‘ 8â†‘ 9â†‘ 10â†“ 11â†‘ 12â†“ 13 14â†‘ 15â†‘ 16â†“ 17â†‘ 18â†‘
4â†“ 14â†‘ 11â†‘ 10â†‘ 6â†‘ 9â†‘ 13â†“ 7â†“ 17â†‘ 1â†‘ 18â†‘ 3â†“ 2â†‘ 12â†“ 16â†‘ 5â†“ 8â†“ 15â†“
4â†“ 17â†“ 3 2 18â†“ 16â†“ 14â†‘ 7 9 13 11â†“ 12 10â†‘ 8â†“ 1â†‘ 6 15â†“ 5â†‘
6 13 7â†“ 8â†“ 12 10 14 4â†‘ 16â†“ 1 2â†“ 18â†‘ 17â†“ 15â†‘ 9 11 3â†‘ 5â†‘
7 18 6â†“ 16â†“ 4â†“ 5 1â†“ 17â†‘ 13 11 10 12 14 15 8â†“ 2 9â†‘ 3
8 3â†“ 15â†“ 4â†“ 9 17 2 16 11â†“ 12 14 13 10â†“ 7 6 1â†‘ 5 18
8 11 3 14 6â†“ 15 13 5â†“ 2â†“ 16â†“ 17â†“ 1â†‘ 18â†‘ 7 10 9 4 12
10 7â†“ 16 4 5 9â†“ 13 15â†“ 1 18â†‘ 3 17â†“ 2 12 14 8 6 11
13 9â†‘ 16 14 15 11â†‘ 7 12 5â†‘ 4â†“ 2â†‘ 1 3â†“ 10â†“ 6â†“ 18â†‘ 17 8â†“
18â†‘ 4 15â†‘ 17â†‘ 16â†‘ 2â†“ 8â†‘ 1 11 13 9 12 10 3 6 14 7 5â†“
18â†“ 7 2 5 4 3 17â†“ 10 15 11 14 12 13 1â†‘ 16â†“ 9â†“ 8â†“ 6
(11;3,18)
19â†‘ 3â†‘ 5â†‘ 2â†‘ 6â†‘ 18â†‘ 9 10 14â†‘ 21â†‘ 20â†‘ 1â†‘ 15â†“ 4â†‘ 12 22â†‘ 16â†‘ 7 13 8 11â†‘ 23â†‘ 17â†‘
2â†“ 22â†‘ 13â†“ 21â†‘ 7â†‘ 4â†“ 1â†‘ 6â†“ 18â†“ 23â†“ 15â†“ 12 8 10â†‘ 19â†‘ 16â†‘ 17 11â†‘ 20â†‘ 3â†‘ 5â†“ 14â†‘ 9â†“
1â†‘ 21â†“ 6â†‘ 7 4â†“ 20 16â†“ 18 10â†“ 23 3â†‘ 13 17 9â†“ 11â†‘ 14 2â†‘ 19â†“ 8â†‘ 22â†‘ 12 15â†“ 5
10 12â†‘ 5â†“ 15 8â†“ 13 23â†‘ 17â†“ 22â†‘ 11 18 14 9 3â†“ 4â†“ 1â†‘ 16 2â†‘ 20â†“ 21 19â†“ 6 7â†“
3 17â†“ 22 4â†“ 14 2â†“ 7 13 1â†‘ 10â†‘ 9 16â†‘ 11â†“ 12 23â†“ 8 20â†‘ 19 18â†“ 15 21 5 6â†‘
10 1â†“ 19â†“ 15â†‘ 3 14â†‘ 23â†“ 4 12 11 8 18â†“ 2 21â†‘ 16 6 17 20â†‘ 7 5â†“ 9 13 22â†“
20â†‘ 13 3 17â†“ 18 7 14 10 22 9â†“ 1â†“ 16 11â†“ 21 15 8â†“ 4 6 2 5â†“ 23â†‘ 19â†“ 12â†‘
8 18 12 3 22â†“ 11 14 1â†‘ 15 6 21â†“ 5 10 19â†“ 9 13 4â†“ 2â†“ 20 17 16 7 23
21â†“ 2 12 15 22â†‘ 23â†“ 3â†“ 19â†“ 8â†“ 5â†“ 16 17â†‘ 1â†‘ 20 6â†‘ 18 13â†“ 7 9â†‘ 11 14â†“ 4â†“ 10
18 10 9 14â†“ 7 17 8 6 5 15 16â†‘ 13 23â†‘ 11 4â†“ 12 20â†“ 21 1â†“ 22â†“ 3â†‘ 2â†‘ 19
16 10 14 9 21 12 7 23â†‘ 13 2â†‘ 4 19â†“ 20 1 22 3â†“ 8 17â†“ 18 6â†‘ 5 15 11
15â†“ 16 23â†‘ 21 12 3â†‘ 19 17 4 9 13 1â†“ 18 14 5 22â†“ 7 11 10 8 6 20 2
(12;3,23)

determining whether a sequence covering array exists. The situation is worse when
constraints have strength three.
Consider a collection T of ordered triples of distinct elements of Î£, and associate with (a, b, c) the constraints {(b, a, c), (b, c, a), (a, c, b), (c, a, b)}. Meeting these
constraints requires that b lie between a and c, and a collection of constraints of this
type forms an instance of the betweenness problem [5] in which one is required to

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

SEQUENCE COVERING ARRAYS

1859

order all items so that for every triple (a, b, c) âˆˆ T , b lies between a and c. Forming
C = {(b, a, c), (b, c, a), (a, c, b), (c, a, b) : (a, b, c) âˆˆ T }, even finding a single permutation that covers no 3-subpermutation in C appears hard:
Lemma 7.2 (see [30]). Determining whether an instance of the betweenness problem has a solution is NP-complete.
These complexity results suggest that constraints pose severe additional challenges
in the construction of sequence covering arrays. Checking feasibility can become diï¬ƒcult; even when feasibility is easily checked, the minimization problem is substantially
more complicated.
8. Conclusions. The close connection between sequence covering arrays and
covering arrays has proved useful in establishing bounds on the sizes of sequence covering arrays. The eï¬ƒcient conditional expectation algorithm for generating sequence
covering arrays and the product construction for strength three parallel analogous
results for covering arrays. Unfortunately, while sequence covering arrays lead to
covering arrays with excess coverage, additional conditions on such a covering array
would be required in order to recover a sequence covering array. Hence the parallels
between the extensive literature on covering arrays and the existence problem for
sequence covering arrays are primarily by analogy.
We have examined numerous formulations for sequence covering arrays. In closing, we indicate one more (see also [27]). A perfect hash family PHF(N ; k, w, t) is an
N Ã— k array on w symbols in which in every N Ã— t subarray, at least one row consists
of distinct symbols. Mehlhorn [29] introduced perfect hash families as an eï¬ƒcient tool
for compact storage and fast retrieval of frequently used information; see also [14].
Stinson et al. [35] establish that perfect hash families can be used to construct separating systems, key distribution patterns, group testing algorithms, cover-free families,
and secure frameproof codes. They are also used extensively in product constructions
for covering arrays [8, 12, 13]. Completely t-scrambling sets of permutations can be
viewed as an ordered analogue of perfect hash families in which k = w, no element
appears twice in a row, and for every way to select t distinct columns in order there is
a row in which the elements in these columns are in increasing order. In particular, a
completely t-scrambling set of permutations provides a perfect hash family in which,
for every set of t columns, there are at least t! rows containing distinct symbols in the
chosen columns, and at least one for each of the t! symbol orderings. For this reason,
it appears reasonable to expect that constructions for perfect hash families may also
prove to be useful for sequence covering arrays.
Acknowledgments. Thanks to two anonymous referees for pointing out relevant
references. Special thanks to Mutsunori Banbara and Johannes Oetsch for providing
explicit solutions for small sequence covering arrays with t = 3.
REFERENCES
[1] M. Banbara, N. Tamura, and K. Inoue, Generating event-sequence test cases by answer
set programming with the incidence matrix, in Technical Communications of the 28th
International Conference on Logic Programming (ICLP12), 2012, pp. 86â€“97.
[2] M. Brain, E. Erdem, K. Inoue, J. Oetsch, J. PuÌˆhrer, H. Tompits, and C. Yilmaz, Eventsequence testing using answer-set programming, Internat. J. Advances Software, 5 (2012),
pp. 237â€“251.
[3] R. C. Bryce and C. J. Colbourn, The density algorithm for pairwise interaction testing,
Software Testing, Verification, and Reliability, 17 (2007), pp. 159â€“182.

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

1860

Y. M. CHEE, C. J. COLBOURN, D. HORSLEY, AND J. ZHOU

[4] R. C. Bryce and C. J. Colbourn, A density-based greedy algorithm for higher strength covering arrays, Software Testing Verification Reliability, 19 (2009), pp. 37â€“53.
[5] B. Chor and M. Sudan, A geometric approach to betweenness, SIAM J. Discrete Math., 11
(1998), pp. 511â€“523.
[6] M. B. Cohen, C. J. Colbourn, and A. C. H. Ling, Constructing strength three covering
arrays with augmented annealing, Discrete Math., 308 (2008), pp. 2709â€“2722.
[7] C. J. Colbourn, Constructing perfect hash families using a greedy algorithm, in Coding and
Cryptology, Y. Li, S. Zhang, S. Ling, H. Wang, C. Xing, and H. Niederreiter, eds., World
Scientific, Singapore, 2008, pp. 109â€“118.
[8] C. J. Colbourn, Covering arrays and hash families, in Information Security and Related
Combinatorics, NATO Peace and Information Security, IOS Press, Amsterdam, 2011,
pp. 99â€“136.
[9] C. J. Colbourn, Eï¬ƒcient conditional expectation algorithms for constructing hash families, in
Combinatorial Algorithms, Lecture Notes in Comput. Sci., 7056, Springer-Verlag, Berlin,
2011, pp. 144â€“155.
[10] C. J. Colbourn, D. Horsley, and V. R. Syrotiuk, Strengthening hash families and compressive sensing, J. Discrete Algorithms, 16 (2012), pp. 170â€“186.
[11] C. J. Colbourn, S. S. Martirosyan, Tran Van Trung, and R. A. Walker II, Roux-type
constructions for covering arrays of strengths three and four, Des. Codes Cryptogr., 41
(2006), pp. 33â€“57.
[12] C. J. Colbourn and J. Torres-JimeÌnez, Heterogeneous hash families and covering arrays,
Contemp. Math., 523 (2010), pp. 3â€“15.
[13] C. J. Colbourn and J. Zhou, Improving two recursive constructions for covering arrays, J.
Statist. Theory Practice, 6 (2012), pp. 30â€“47.
[14] Z. J. Czech, G. Havas, and B. S. Majewski, Perfect hashing, Theoret. Comput. Sci., 182
(1997), pp. 1â€“143.
[15] B. Dushnik, Concerning a certain set of arrangements, Proc. Amer. Math. Soc., 1 (1950),
pp. 788â€“796.
[16] E. Erdem, K. Inoue, J. Oetsch, J. PuÌˆhrer, H. Tompits, and C. Yilmaz, Answer-set programming as a new approach to event-sequence testing, in Proceedings of the 2nd International Conference on Advances in System Testing and Validation Lifecycle, Xpert
Publishing Services, 2011, pp. 25â€“34.
[17] Z. FuÌˆredi, Scrambling permutations and entropy of hypergraphs, Random Structures
Algorithms, 8 (1996), pp. 97â€“104.
[18] M. M. Z. Hazli, K. Z. Zamli, and R. R. Othman, Sequence-based interaction testing implementation using bees algorithm, in Proceedings of the IEEE Symposium on Computers
and Informatics, 2012, pp. 81â€“85.
[19] S. Huang, M. B. Cohen, and A. M. Memon, Repairing GUI test suites using a genetic algorithm, in Proceedings of the 3rd International Conference on Software Testing, Verification
and Validation (ICST), 2010, pp. 245â€“254.
[20] Y. Ishigami, Containment problems in high-dimensional spaces, Graphs Combin., 11 (1995),
pp. 327â€“335.
[21] Y. Ishigami, An extremal problem of d permutations containing every permutation of every t
elements, Discrete Math., 159 (1996), pp. 279â€“283.
[22] D. S. Johnson, Approximation algorithms for combinatorial problems, J. Comput. System Sci.,
9 (1974), pp. 256â€“278.
[23] D. R. Kuhn, J. M. Higdon, J. F. Lawrence, R. N. Kacker, and Y. Lei, Combinatorial methods for event sequence testing, in Proceedings of the IEEE 5th International Conference
on Software Testing, Verification and Validation (ICST), 2012, pp. 601â€“609.
[24] D. R. Kuhn, J. M. Higdon, J. F. Lawrence, R. N. Kacker, and Y. Lei, Combinatorial
methods for event sequence testing, CrossTalk J. Defense Software Engineering, 25 (2012),
pp. 15â€“18.
[25] V. I. LevenshteÄ±n, Perfect codes in the metric of deletions and insertions, Diskret. Mat., 3
(1991), pp. 3â€“20.
[26] L. LovaÌsz, On the ratio of optimal integral and fractional covers, Discrete Math., 13 (1975),
pp. 383â€“390.
[27] O. Margalit, Better bounds for event sequence testing, in Proceedings of the 2nd International
Workshop on Combinatorial Testing, 2013.
[28] R. Mathon and Tran Van Trung, Directed t-packings and directed t-Steiner systems, Des.
Codes Cryptogr., 18 (1999), pp. 187â€“198.
[29] K. Mehlhorn, Data Structures and Algorithms 1: Sorting and Searching, Springer-Verlag,
Berlin, 1984.

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

Downloaded 12/11/13 to 155.69.4.4. Redistribution subject to SIAM license or copyright; see http://www.siam.org/journals/ojsa.php

SEQUENCE COVERING ARRAYS

1861

[30] J. OpatrnyÌ, Total ordering problem, SIAM J. Comput., 8 (1979), pp. 111â€“114.
[31] J. Radhakrishnan, A note on scrambling permutations, Random Structures Algorithms, 22
(2003), pp. 435â€“439.
[32] J. Spencer, Minimal scrambling sets of simple orders, Acta Math. Acad. Sci. Hungar., 22
(1971/72), pp. 349â€“353.
[33] R. P. Stanley, Increasing and decreasing subsequences and their variants, in Proceedings of
the International Congress of Mathematicians, vol. I, Madrid, 2007, pp. 545â€“579.
[34] S. K. Stein, Two combinatorial covering theorems, J. Combin. Theory Ser. A, 16 (1974),
pp. 391â€“397.
[35] D. R. Stinson, Tran Van Trung, and R. Wei, Secure frameproof codes, key distribution
patterns, group testing algorithms and related structures, J. Statist. Plann. Inference, 86
(2000), pp. 595â€“617.
[36] J. Tarui, On the minimum number of completely 3-scrambling permutations, Discrete Math.,
308 (2008), pp. 1350â€“1354.
[37] W. T. Trotter, Jr., Some combinatorial problems for permutations, in Proceedings of the 8th
Southeastern Conference on Combinatorics, Graph Theory and Computing, Baton Rouge,
La., 1977, Utilitas Mathematica, Winnipeg, pp. 619â€“632.
[38] W. T. Trotter, Jr., Combinatorics and partially ordered sets, in Dimension Theory, Johns
Hopkins Ser. Math. Sci., Johns Hopkins University Press, Baltimore, MD, 1992.
[39] W. Wang, Y. Lei, S. Sampath, R. Kacker, D. Kuhn, and J. Lawrence, A combinatorial
approach to building navigation graphs for dynamic web applications, in Proceedings of
the 25th International Conference on Software Maintenance, 2009, pp. 211â€“220.
[40] W. Wang, S. Sampath, Y. Lei, and R. Kacker, An interaction-based test sequence generation
approach for testing web applications, in 11th IEEE High Assurance Systems Engineering
Symposium, 2008, pp. 209â€“218.
[41] M. Yannakakis, The complexity of the partial order dimension problem, SIAM J. Algebraic
Discrete Methods, 3 (1982), pp. 351â€“358.
[42] X. Yuan, M. B. Cohen, and A. M. Memon, Towards dynamic adaptive automated test generation for graphical user interfaces, in International Conference on Software Testing, Verification and Validation Workshops, 2009, pp. 263â€“266.
[43] X. Yuan, M. B. Cohen, and A. M. Memon, GUI interaction testing: Incorporating event
context, IEEE Trans. Software Engrg., 37 (2011), pp. 559â€“574.
[44] X. Yuan and A. M. Memon, Generating event sequence-based test cases using GUI runtime
state feedback, IEEE Trans. Software Engrg., 36 (2010), pp. 81â€“95.

Copyright Â© by SIAM. Unauthorized reproduction of this article is prohibited.

